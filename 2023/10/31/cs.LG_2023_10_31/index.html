
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-10-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00181 repo_url: None paper_authors: Neelkamal Bhuyan, Debankur Mukherjee, Adam Wierman for:">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-10-31">
<meta property="og:url" content="https://nullscc.github.io/2023/10/31/cs.LG_2023_10_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00181 repo_url: None paper_authors: Neelkamal Bhuyan, Debankur Mukherjee, Adam Wierman for:">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-31T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-03T00:29:08.032Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.LG_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T10:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-10-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Best-of-Both-Worlds-Stochastic-and-Adversarial-Convex-Function-Chasing"><a href="#Best-of-Both-Worlds-Stochastic-and-Adversarial-Convex-Function-Chasing" class="headerlink" title="Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing"></a>Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00181">http://arxiv.org/abs/2311.00181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neelkamal Bhuyan, Debankur Mukherjee, Adam Wierman</li>
<li>for: 这个论文研究了在随机和敌对环境中的凸函数追踪（CFC）问题，提供了同时在两个设定下实现性能保证的算法。</li>
<li>methods: 论文使用了随机框架来研究CFC问题，并提供了对随机和敌对环境的性能保证。</li>
<li>results: 论文表明了在随机和敌对环境中， adversarial-optimal 算法在随机场景中表现不佳，而提供了一种 best-of-both-worlds 算法，可以同时实现robust敌对性和近似随机性的性能保证。<details>
<summary>Abstract</summary>
Convex function chasing (CFC) is an online optimization problem in which during each round $t$, a player plays an action $x_t$ in response to a hitting cost $f_t(x_t)$ and an additional cost of $c(x_t,x_{t-1})$ for switching actions. We study the CFC problem in stochastic and adversarial environments, giving algorithms that achieve performance guarantees simultaneously in both settings. Specifically, we consider the squared $\ell_2$-norm switching costs and a broad class of quadratic hitting costs for which the sequence of minimizers either forms a martingale or is chosen adversarially. This is the first work that studies the CFC problem using a stochastic framework. We provide a characterization of the optimal stochastic online algorithm and, drawing a comparison between the stochastic and adversarial scenarios, we demonstrate that the adversarial-optimal algorithm exhibits suboptimal performance in the stochastic context. Motivated by this, we provide a best-of-both-worlds algorithm that obtains robust adversarial performance while simultaneously achieving near-optimal stochastic performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文：<</SYS>>凹函数追踪（CFC）是一个在线优化问题，每个回合 $t$，玩家会选择一个动作 $x_t$，对于每个回合 $t$，玩家会面临一个攻击成本 $f_t(x_t)$ 和一个Switching成本 $c(x_t,x_{t-1})$。我们研究了CFC问题在随机和敌意环境中，提供了一些算法，可以同时在两个设定中获得性能保证。特别是，我们考虑了平方型 $\ell_2$  нор switching成本，以及一种广泛的quadratic hitting costs，其中序列的最小值 either forms a martingale or is chosen adversarially。这是首次研究CFC问题使用随机 frameworks。我们提供了优化的随机在线算法的特征化，并在随机和敌意两个设定之间进行比较，我们发现了对于随机场景，敌意优化算法的性能不佳。这种情况下，我们提供了一个best-of-both-worlds算法，可以同时实现对敌意性能的Robustness和随机场景中的近似优化性能。
</details></li>
</ul>
<hr>
<h2 id="The-Alignment-Ceiling-Objective-Mismatch-in-Reinforcement-Learning-from-Human-Feedback"><a href="#The-Alignment-Ceiling-Objective-Mismatch-in-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback"></a>The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00168">http://arxiv.org/abs/2311.00168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Lambert, Roberto Calandra</li>
<li>for: 这篇论文旨在解决人工反馈学习（RLHF）中的目标不一致问题，以便使大语言模型（LLM）更容易提示和在复杂情况下更有能力。</li>
<li>methods: 这篇论文使用了人工反馈学习（RLHF）技术，它提供了一种新的工具来优化大语言模型（LLM），而不是只是下一个字符预测。</li>
<li>results: 这篇论文发现，RLHF中的目标不一致问题可能导致模型避免用户请求，困难带动特征，或者总是回答在特定风格下。通过解决RLHF中的目标不一致问题，未来的LLM将更 preciselly遵循用户指令，以确保安全和有用性。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of "too much RLHF." In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As chat model evaluation becomes increasingly nuanced, the reliance on a perceived link between reward model score and downstream performance drives the objective mismatch issue. In this paper, we illustrate the cause of this issue, reviewing relevant literature from model-based reinforcement learning, and discuss relevant solutions to encourage further research. By solving objective mismatch in RLHF, the LLMs of the future will be more precisely aligned to user instructions for both safety and helpfulness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graph-Neural-Networks-for-Road-Safety-Modeling-Datasets-and-Evaluations-for-Accident-Analysis"><a href="#Graph-Neural-Networks-for-Road-Safety-Modeling-Datasets-and-Evaluations-for-Accident-Analysis" class="headerlink" title="Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis"></a>Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00164">http://arxiv.org/abs/2311.00164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/virtuosoresearch/ml4roadsafety">https://github.com/virtuosoresearch/ml4roadsafety</a></li>
<li>paper_authors: Abhinav Nippani, Dongyue Li, Haotian Ju, Haris N. Koutsopoulos, Hongyang R. Zhang</li>
<li>for: 本文针对道路网络上的交通事故分析进行研究，使用历史记录来预测交通事故发生的可能性。</li>
<li>methods: 本文使用了多种深度学习方法，包括图神经网络（GraphSAGE），并通过多任务学习和转移学习来考虑州际差异和交通量的影响。</li>
<li>results: 研究发现，使用图神经网络可以准确预测道路上的交通事故数量， Mean Absolute Error 低于 22%，并且可以预测事故发生或不发生的可能性高于 87%。<details>
<summary>Abstract</summary>
We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not with over 87% AUROC, averaged over states. We achieve these results by using multitask learning to account for cross-state variabilities (e.g., availability of accident labels) and transfer learning to combine traffic volume with accident prediction. Ablation studies highlight the importance of road graph-structural features, amongst other features. Lastly, we discuss the implications of the analysis and develop a package for easily using our new dataset.
</details>
<details>
<summary>摘要</summary>
我们考虑了基于公路网络的交通事故分析，包括公路网络连接和交通量。过去的工作已经设计了各种深度学习方法，用于预测交通事故发生。然而，现有的方法准确性没有达成共识，并且存在公共交通事故数据集的缺乏，导致全面评估不可能。本文构建了美国各州官方报告中的交通事故记录数据集，总计900万记录，同时包括公路网络和交通量报告。使用这个新的数据集，我们评估了现有的深度学习方法，预测公路网络上事故的发生。我们的主要发现是，图 neural network 如 GraphSAGE 可以准确预测公路网络上事故的数量， mean absolute error 低于 22%，并且可以预测事故是否发生，AUROC 高于 87%，平均值为州。我们实现了这些结果通过多任务学习，考虑到cross-state variability（例如事故标签的可用性），并通过传输学习将交通量与事故预测结合。我们的分析结果表明，路网结构特征是预测事故的重要因素之一。最后，我们讨论了分析结果的意义，并开发了一个包可以方便地使用我们的新数据集。
</details></li>
</ul>
<hr>
<h2 id="Neuroformer-Multimodal-and-Multitask-Generative-Pretraining-for-Brain-Data"><a href="#Neuroformer-Multimodal-and-Multitask-Generative-Pretraining-for-Brain-Data" class="headerlink" title="Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data"></a>Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00136">http://arxiv.org/abs/2311.00136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonis Antoniades, Yiyi Yu, Joseph Canzano, William Wang, Spencer LaVere Smith</li>
<li>for: 这个论文是为了处理大规模神经科学实验数据而设计的，以便更好地分析和理解神经系统中的观察结果。</li>
<li>methods: 这个论文使用了一种名为Neuroformer的多模态、多任务生成预训练变换器（GPT）模型，该模型可以处理大规模的神经科学数据，并且可以自适应下游任务，如预测行为。</li>
<li>results: 在模拟数据集上进行训练后，Neuroformer模型可以准确预测神经细胞活动，同时还可以自动推导神经细胞之间的连接关系，包括方向。当用于解oding神经响应时，模型只需几个批量微调，就能准确预测鼠类的行为，这表明模型可以直接从神经表示中学习行为和神经表示之间的关系。<details>
<summary>Abstract</summary>
State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.
</details>
<details>
<summary>摘要</summary>
现代系统神经科学实验受到大规模多模态数据的挑战，需要新的分析工具。启发于视觉和语言领域的大规模预训练模型的成功，我们将大规模、细胞分辨率神经细胞活动数据分析转化为自适应的空间时间生成问题。Neuroformer是一种多模态多任务生成预训练变换（GPT）模型，专门针对系统神经科学数据处理。它的特点是线性增长，可处理任意数量的模态，并且可适应下游任务，如预测行为。我们首先在模拟数据集上训练Neuroformer，发现它不仅准确预测模拟神经细胞活动，还能自动推理出下游神经细胞连接性，包括方向。当用于解码神经响应时，模型只需几个批次微调，就可以预测鼠标的行为，表明模型直接从神经表示中学习行为，无需显式监督。我们使用减少研究来证明，在同时训练神经响应和行为时，模型表现得更好， highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner。这些发现表明Neuroformer可以分析神经数据和其emergent properties，为神经科学发展模型和假设提供参考。
</details></li>
</ul>
<hr>
<h2 id="Extracting-the-Multiscale-Causal-Backbone-of-Brain-Dynamics"><a href="#Extracting-the-Multiscale-Causal-Backbone-of-Brain-Dynamics" class="headerlink" title="Extracting the Multiscale Causal Backbone of Brain Dynamics"></a>Extracting the Multiscale Causal Backbone of Brain Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00118">http://arxiv.org/abs/2311.00118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/officiallydac/cb">https://github.com/officiallydac/cb</a></li>
<li>paper_authors: Gabriele D’Acunto, Francesco Bonchi, Gianmarco De Francisci Morales, Giovanni Petri</li>
<li>for: 这研究旨在提出一种基于多尺度 causal 结构学习的方法来捕捉大脑动态的多尺度 causal 脊梁（MCB），并在实际数据上进行了评估。</li>
<li>methods: 这种方法利用了最新的多尺度 causal 结构学习的进步，并且可以有效地考虑模型的 Complexity-Fitting 质量。</li>
<li>results: 研究发现，使用这种方法可以从resting-state fMRI数据中提取出稀疏的 MCB，并且在不同的时频带中发现了不同的 causal 动力。同时，这种方法还可以支持从 causal 角度出发的大脑连接指纹研究。<details>
<summary>Abstract</summary>
The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it.   Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fitting and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nodes related to sensory processing play a crucial role. Finally, our analysis of individual multiscale causal structures confirms the existence of a causal fingerprint of brain connectivity, thus supporting from a causal perspective the existing extensive research in brain connectivity fingerprinting.
</details>
<details>
<summary>摘要</summary>
大多数研究对大脑连接性都集中在统计相关性中，这些相关性不直接关系大脑动态的 causal 机制。我们提议一种多尺度 causal 脊梁（MCB），这是一组个体 across multiple temporal scales 的大脑动态的共同基础，并提出一种原则性的方法来提取它。我们的方法利用了最新的多尺度 causal 结构学习的进步，并优化模型适应性和复杂性的负面 Trade-off。empirical assessment on synthetic data 表明我们的方法超过基eline based on canonical functional connectivity networks。当应用于 Resting-state fMRI 数据时，我们发现左右大脑半球都有稀畴 MCB。由于它的多尺度性，我们的方法表明在低频带，高级认知功能相关的脑区域驱动 causal 动态; 在更高的频率带，感知处理相关的节点发挥了关键的作用。最后，我们对个体多尺度 causal 结构进行分析，证实了大脑连接性指estamp 的存在，从 causal 角度支持了现有的大脑连接性指estamp 的研究。
</details></li>
</ul>
<hr>
<h2 id="EXTRACT-Explainable-Transparent-Control-of-Bias-in-Embeddings"><a href="#EXTRACT-Explainable-Transparent-Control-of-Bias-in-Embeddings" class="headerlink" title="EXTRACT: Explainable Transparent Control of Bias in Embeddings"></a>EXTRACT: Explainable Transparent Control of Bias in Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00115">http://arxiv.org/abs/2311.00115</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhijinGuo/EXTRACT">https://github.com/ZhijinGuo/EXTRACT</a></li>
<li>paper_authors: Zhijin Guo, Zhaozhen Xu, Martha Lewis, Nello Cristianini</li>
<li>for: 本研究旨在提供一种可Explainable和Transparent的方法控制知识图embedding中的偏见，以便评估和减少在训练过程中意外存在的保护信息。</li>
<li>methods: 本研究使用Canonical Correlation Analysis (CCA)进行偏见检测，然后使用一个线性系统将embeddings decomposed为其私有属性。</li>
<li>results: 实验结果显示，用户的观影行为和喜好可以推断出一些个人特征，如性别、年龄和职业。此外，在KG20C引用 dataset上，研究发现了一种方法可以通过引用网络来推断出文章中的会议信息。本研究还提出了四种透明的方法来保持 embedding 的能力进行预期的预测，同时减少不良信息的存在。<details>
<summary>Abstract</summary>
Knowledge Graphs are a widely used method to represent relations between entities in various AI applications, and Graph Embedding has rapidly become a standard technique to represent Knowledge Graphs in such a way as to facilitate inferences and decisions. As this representation is obtained from behavioural data, and is not in a form readable by humans, there is a concern that it might incorporate unintended information that could lead to biases. We propose EXTRACT: a suite of Explainable and Transparent methods to ConTrol bias in knowledge graph embeddings, so as to assess and decrease the implicit presence of protected information. Our method uses Canonical Correlation Analysis (CCA) to investigate the presence, extent and origins of information leaks during training, then decomposes embeddings into a sum of their private attributes by solving a linear system. Our experiments, performed on the MovieLens1M dataset, show that a range of personal attributes can be inferred from a user's viewing behaviour and preferences, including gender, age, and occupation. Further experiments, performed on the KG20C citation dataset, show that the information about the conference in which a paper was published can be inferred from the citation network of that article. We propose four transparent methods to maintain the capability of the embedding to make the intended predictions without retaining unwanted information. A trade-off between these two goals is observed.
</details>
<details>
<summary>摘要</summary>
知识图是广泛使用的方法来表示Entities之间的关系在不同的AI应用程序中，而图像嵌入快速成为了知识图表示的标准技术，以便进行推理和决策。这种表示来自行为数据，而不是可读的人类格式，因此可能包含意外的信息，导致偏见。我们提出EXTRACT：一个集成Explainable和Transparent的方法来控制知识图嵌入中的偏见，以便评估和减少隐藏的保护信息。我们使用Canonical Correlation Analysis（CCA）来研究嵌入中的信息泄露情况，然后将嵌入分解为私有属性的总和，解决一个线性方程。我们在MovieLens1M数据集上进行了实验，发现用户的观影行为和偏好可以推断出一些个人特征，包括性别、年龄和职业。在KG20C引用数据集上，我们发现了一种方法，可以通过文章引用网络来推断出文章的发表会议信息。我们提出了四种透明方法，以保持嵌入的能力，而不是保留不必要的信息。我们观察到了这两个目标之间的补做。
</details></li>
</ul>
<hr>
<h2 id="FairWASP-Fast-and-Optimal-Fair-Wasserstein-Pre-processing"><a href="#FairWASP-Fast-and-Optimal-Fair-Wasserstein-Pre-processing" class="headerlink" title="FairWASP: Fast and Optimal Fair Wasserstein Pre-processing"></a>FairWASP: Fast and Optimal Fair Wasserstein Pre-processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00109">http://arxiv.org/abs/2311.00109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikai Xiong, Niccolò Dalmasso, Alan Mishler, Vamsi K. Potluru, Tucker Balch, Manuela Veloso</li>
<li>for: 降低机器学习模型输出不均衡问题的解决方案</li>
<li>methods: 使用大规模杂合程序（MIP）和剖面方法来设计一种新的预处理方法，以降低分布式预处理问题的复杂性</li>
<li>results: 在synthetic数据集上进行了实验，并证明了该方法可以快速和高效地解决MIP和其线性 програм的relaxation问题，并且在下游分类任务中具有竞争性的性能，可以降低不均衡问题 while preserving accuracy。<details>
<summary>Abstract</summary>
Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Our work is based on reformulating the pre-processing task as a large-scale mixed-integer program (MIP), for which we propose a highly efficient algorithm based on the cutting plane method. Experiments on synthetic datasets demonstrate that our proposed optimization algorithm significantly outperforms state-of-the-art commercial solvers in solving both the MIP and its linear program relaxation. Further experiments highlight the competitive performance of FairWASP in reducing disparities while preserving accuracy in downstream classification settings.
</details>
<details>
<summary>摘要</summary>
近年来，机器学习方法的发展有助于减少不同 subgroup 的模型输出差异。在多种设置中，训练数据可能会被多个下游应用程序使用，这意味着可能最有效地 intervene 在训练数据本身上。在这项工作中，我们提出了 FairWASP，一种新的预处理方法，可以在分类 dataset 中减少不同 subgroup 的差异。FairWASP 返回样本级别的权重，以使得重新权重后的 dataset 最小化 Wasserstein 距离原始 dataset，同时满足（一种 empirical 版本的）人口减少准则。我们证明了整数权重是最佳的，这意味着我们的方法可以看作是复制或消除样本。因此，FairWASP 可以用于构建可以被任何分类方法使用的 dataset。我们基于将预处理任务转换为大规模混合整数Program (MIP) 的方法，并提出了高效的剪切面方法。对于 Synthetic 数据集进行了实验，我们的提出的优化算法在 MIP 和其 linear program relaxation 中significantly 超越了现有的商业 solver。进一步的实验表明，FairWASP 可以减少差异而保持下游分类设置中的精度。
</details></li>
</ul>
<hr>
<h2 id="Deep-Compressed-Learning-for-3D-Seismic-Inversion"><a href="#Deep-Compressed-Learning-for-3D-Seismic-Inversion" class="headerlink" title="Deep Compressed Learning for 3D Seismic Inversion"></a>Deep Compressed Learning for 3D Seismic Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00107">http://arxiv.org/abs/2311.00107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maayan Gelboim, Amir Adler, Yen Sun, Mauricio Araya-Polo</li>
<li>for: 提高3D地震傅入的效率和质量，使用非常少的地震源。</li>
<li>methods:  combinational compressed-sensing和机器学习框架，称为compressed-learning，将维度减少操作和3D反傅编码器-解码器实现为深度卷积神经网络（DCNN）。</li>
<li>results: 通过选择一小部分可用的源，使用DCNN完成反傅任务，实现了数据量的减少一个次，保持3D重建质量与整个数据集相当。<details>
<summary>Abstract</summary>
We consider the problem of 3D seismic inversion from pre-stack data using a very small number of seismic sources. The proposed solution is based on a combination of compressed-sensing and machine learning frameworks, known as compressed-learning. The solution jointly optimizes a dimensionality reduction operator and a 3D inversion encoder-decoder implemented by a deep convolutional neural network (DCNN). Dimensionality reduction is achieved by learning a sparse binary sensing layer that selects a small subset of the available sources, then the selected data is fed to a DCNN to complete the regression task. The end-to-end learning process provides a reduction by an order-of-magnitude in the number of seismic records used during training, while preserving the 3D reconstruction quality comparable to that obtained by using the entire dataset.
</details>
<details>
<summary>摘要</summary>
我们考虑了基于非常小的数量的地震源数据的3D地震减法问题。我们的解决方案基于压缩感知和机器学习框架，称为压缩学习。该解决方案同时优化了维度减少算子和基于深度卷积神经网络（DCNN）实现的3D减法编码器-解码器。通过学习一个稀疏二进制感知层，选择一小 subsets of 可用的地震源，然后将选择的数据传递给 DCNN 完成回归任务。这种端到端学习过程可以在训练过程中减少数据记录的数量，同时保持与全部数据集相同的3D重建质量。
</details></li>
</ul>
<hr>
<h2 id="Seeking-Truth-and-Beauty-in-Flavor-Physics-with-Machine-Learning"><a href="#Seeking-Truth-and-Beauty-in-Flavor-Physics-with-Machine-Learning" class="headerlink" title="Seeking Truth and Beauty in Flavor Physics with Machine Learning"></a>Seeking Truth and Beauty in Flavor Physics with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00087">http://arxiv.org/abs/2311.00087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin T. Matchev, Katia Matcheva, Pierre Ramond, Sarunas Verner</li>
<li>for: 该研究旨在开发新的理论物理模型，既要适应现有实验数据，又要满足抽象理论家的标准如美食性、自然性等。</li>
<li>methods: 该研究使用机器学习技术设计损失函数进行两个任务的优化：适应现有实验数据和满足抽象理论家的标准。作为一个示例，我们使用Yukawa针子部分进行示例。</li>
<li>results: 优化损失函数的结果是真实的和美丽的模型。<details>
<summary>Abstract</summary>
The discovery process of building new theoretical physics models involves the dual aspect of both fitting to the existing experimental data and satisfying abstract theorists' criteria like beauty, naturalness, etc. We design loss functions for performing both of those tasks with machine learning techniques. We use the Yukawa quark sector as a toy example to demonstrate that the optimization of these loss functions results in true and beautiful models.
</details>
<details>
<summary>摘要</summary>
发现过程中建立新的物理理论模型具有双重的方面，一是适应现有实验数据，另一是满足抽象理论家的标准如美食、自然性等。我们使用机器学习技术定义损失函数进行这两个任务。我们使用Yukawa夹心作为一个示例，示出优化这些损失函数后得到了真实和美丽的模型。Here's a breakdown of the translation:* "发现过程" (发现过程) - the process of discovering new physics models* "建立新的物理理论模型" (建立新的物理理论模型) - building new theoretical physics models* "具有双重的方面" (具有双重的方面) - having two aspects* "一是适应现有实验数据" (一是适应现有实验数据) - one aspect is fitting to existing experimental data* "另一是满足抽象理论家的标准" (另一是满足抽象理论家的标准) - the other aspect is satisfying the criteria of abstract theorists, such as beauty and naturalness* "使用机器学习技术定义损失函数" (使用机器学习技术定义损失函数) - using machine learning techniques to define loss functions* "进行这两个任务" (进行这两个任务) - for these two tasks* "示例" (示例) - example* "示出优化这些损失函数后得到了真实和美丽的模型" (示出优化这些损失函数后得到了真实和美丽的模型) - demonstrating that optimizing these loss functions results in true and beautiful models.
</details></li>
</ul>
<hr>
<h2 id="Ensemble-models-outperform-single-model-uncertainties-and-predictions-for-operator-learning-of-hypersonic-flows"><a href="#Ensemble-models-outperform-single-model-uncertainties-and-predictions-for-operator-learning-of-hypersonic-flows" class="headerlink" title="Ensemble models outperform single model uncertainties and predictions for operator-learning of hypersonic flows"></a>Ensemble models outperform single model uncertainties and predictions for operator-learning of hypersonic flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00060">http://arxiv.org/abs/2311.00060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor J. Leon, Noah Ford, Honest Mrema, Jeffrey Gilbert, Alexander New</li>
<li>for: 这个论文的目的是开发一种能够预测高速流动的科学机器学习模型（SciML），并使用不同的不确定性评估方法来评估模型的输出不确定性。</li>
<li>methods: 这个研究使用了三种不确定性评估方法：mean-variance estimation、evidential uncertainty和ensembling，来扩展一个深度ONet模型。</li>
<li>results: 研究发现，使用ensembling方法可以最好地减少错误和calibrate uncertainty，并在 interpolative 和 extrapolative  régime中表现出色。<details>
<summary>Abstract</summary>
High-fidelity computational simulations and physical experiments of hypersonic flows are resource intensive. Training scientific machine learning (SciML) models on limited high-fidelity data offers one approach to rapidly predict behaviors for situations that have not been seen before. However, high-fidelity data is itself in limited quantity to validate all outputs of the SciML model in unexplored input space. As such, an uncertainty-aware SciML model is desired. The SciML model's output uncertainties could then be used to assess the reliability and confidence of the model's predictions. In this study, we extend a DeepONet using three different uncertainty quantification mechanisms: mean-variance estimation, evidential uncertainty, and ensembling. The uncertainty aware DeepONet models are trained and evaluated on the hypersonic flow around a blunt cone object with data generated via computational fluid dynamics over a wide range of Mach numbers and altitudes. We find that ensembling outperforms the other two uncertainty models in terms of minimizing error and calibrating uncertainty in both interpolative and extrapolative regimes.
</details>
<details>
<summary>摘要</summary>
高精度计算 simulate 和物理实验 hypersonic 流体动力学是资源占用的。使用有限高精度数据训练科学机器学习（SciML）模型可以快速预测未经测试的情况。然而，高精度数据本身受限，无法验证SciML模型在未知输入空间中的所有输出。因此，需要一个不确定性意识的 SciML 模型。SciML 模型的输出不确定性可以用来评估模型预测的可靠性和信任度。在本研究中，我们扩展了 DeepONet 使用三种不确定性评估机制：平均方差估计、证据不确定性和混合。这些不确定性意识 DeepONet 模型在 hypersonic 流体动力学中逆转射 cone  объек的数据生成过程中被训练和评估。我们发现，混合在 interpolative 和 extrapolative  régime 中都能够最小化错误和把不确定性评估。
</details></li>
</ul>
<hr>
<h2 id="Training-Free-Generalization-on-Heterogeneous-Tabular-Data-via-Meta-Representation"><a href="#Training-Free-Generalization-on-Heterogeneous-Tabular-Data-via-Meta-Representation" class="headerlink" title="Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation"></a>Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00055">http://arxiv.org/abs/2311.00055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han-Jia Ye, Qi-Le Zhou, De-Chuan Zhan</li>
<li>for: 本研究旨在提高不同特征和类型的表格数据之间共享知识，使 tabular 模型能够在未经训练的情况下在新的表格数据集中表现出色。</li>
<li>methods: 本研究提出了 Tabular data Pre-Training via Meta-representation (TabPTM)，允许一个 tabular 模型在一组具有多种特征和类型的表格数据集上进行预训练。然后，这个预训练模型可以直接应用于未经训练的表格数据集，无需进行额外的训练。</li>
<li>results: 实验表明，TabPTM 能够在新的表格数据集中表现出色，尤其是在少量示例情况下。<details>
<summary>Abstract</summary>
Tabular data is prevalent across various machine learning domains. Yet, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, even under few-shot scenarios.
</details>
<details>
<summary>摘要</summary>
<SYS>请输入文本要翻译的文本</SYS>文本：Tabular data 是机器学习领域中广泛存在的数据类型之一。然而，不同的 tabular 数据集之间的 attribute 和 class 空间之间的自然差异会限制 tabular 模型在不同数据集之间共享知识，使得一个 tabular 模型只能在已经训练过的数据集上进行有效地学习。在这篇论文中，我们提出了 Tabular data Pre-Training via Meta-representation（TabPTM），它允许一个 tabular 模型在一组不同 attribute 和 class 的 heterogeneous 数据集上进行预训练。然后，这个预训练后的模型可以直接应用于未经训练过的数据集，而无需进行额外的训练。具体来说，TabPTM 使用一个固定数量的 prototypes 来表示每个实例，从而标准化不同 attribute 和 class 的 tabular 数据集。然后，一个深度神经网络被训练以关联这些 meta-representation 与数据集pecific的分类信任度，使得 TabPTM 具有无需训练的普适性。实验表明，TabPTM 在新的数据集上可以达到良好的性能，甚至在几 shot 的情况下。Translation:Tabular data is ubiquitous in various machine learning domains. However, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, even under few-shot scenarios.
</details></li>
</ul>
<hr>
<h2 id="On-the-Kolmogorov-neural-networks"><a href="#On-the-Kolmogorov-neural-networks" class="headerlink" title="On the Kolmogorov neural networks"></a>On the Kolmogorov neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00049">http://arxiv.org/abs/2311.00049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlbTchik/Adapting-the-K-complexity-for-neural-networks">https://github.com/AlbTchik/Adapting-the-K-complexity-for-neural-networks</a></li>
<li>paper_authors: Aysu Ismayilova, Vugar Ismailov</li>
<li>for: 该论文旨在证明olkogorovTwoHiddenLayer神经网络模型，带有连续、不连续、边界或无边界活动函数在第二层隐藏层，可以准确表示连续、不连续、边界和所有无边界多变量函数。</li>
<li>methods: 该论文使用了KolmogorovTwoHiddenLayer神经网络模型，并研究了不同的活动函数对模型的影响。</li>
<li>results: 研究发现，使用连续、不连续、边界或无边界活动函数的KolmogorovTwoHiddenLayer神经网络模型，可以准确表示连续、不连续、边界和所有无边界多变量函数。<details>
<summary>Abstract</summary>
In this paper, we show that the Kolmogorov two hidden layer neural network model with a continuous, discontinuous bounded or unbounded activation function in the second hidden layer can precisely represent continuous, discontinuous bounded and all unbounded multivariate functions, respectively.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们展示了olkogorov两层隐藏层神经网络模型，其中第二层隐藏层使用连续、终止、限制或无限 activation function，可以精确表示连续、终止、限制和所有无限多变量函数。Note that I have used "连续" (liánxù) to translate "continuous" in the text, which is a more common way to express this concept in Simplified Chinese. Also, I have used "终止" (jìzhì) to translate "discontinuous", which is a more precise way to express this concept in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Unexpected-Improvements-to-Expected-Improvement-for-Bayesian-Optimization"><a href="#Unexpected-Improvements-to-Expected-Improvement-for-Bayesian-Optimization" class="headerlink" title="Unexpected Improvements to Expected Improvement for Bayesian Optimization"></a>Unexpected Improvements to Expected Improvement for Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20708">http://arxiv.org/abs/2310.20708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Ament, Samuel Daulton, David Eriksson, Maximilian Balandat, Eytan Bakshy</li>
<li>for: 提高 Bayesian 优化中的 acquisition function 性能</li>
<li>methods: 提出新的 acquisition function 家族 LogEI, 其中的成员具有较好的数值优化性</li>
<li>results: 实验结果显示 LogEI 家族的 acquisition function 可以大幅提高 Bayesian 优化中的优化性能，并且与当前状态的技术相当或超越其性能<details>
<summary>Abstract</summary>
Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.
</details>
<details>
<summary>摘要</summary>
预期改进（EI）可能是搜索优化中最受欢迎的目标函数，它在许多成功应用中表现出色，但它的性能经常被更新的方法所超越。尤其是EI和其变体，包括并行和多目标设置，在数值上具有很大的挑战。这种挑战通常随着观察数、搜索空间维度或约束的数量增加，导致文献中的性能不一致，通常是低效的。在这里，我们提出了LogEI，一个新的家族目标函数，其成员的优化值和传统目标函数的优化值相似或相等，但是数值上容易优化。我们示出了经典的EI、EHVI和它们的受限、噪声和并行变体中的数值问题，并提出了相应的改进方案。我们的实验结果表明，LogEI家族的目标函数在优化性能方面明显提高了，并且奇怪地，与最新的状态对照函数相当或超越了性能，这 highlights了文献中数值优化的重要性。
</details></li>
</ul>
<hr>
<h2 id="Farthest-Greedy-Path-Sampling-for-Two-shot-Recommender-Search"><a href="#Farthest-Greedy-Path-Sampling-for-Two-shot-Recommender-Search" class="headerlink" title="Farthest Greedy Path Sampling for Two-shot Recommender Search"></a>Farthest Greedy Path Sampling for Two-shot Recommender Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20705">http://arxiv.org/abs/2310.20705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufan Cao, Tunhou Zhang, Wei Wen, Feng Yan, Hai Li, Yiran Chen</li>
<li>for: 提高深度推荐模型的开发效率和性能。</li>
<li>methods: 使用Weight-sharing Neural Architecture Search（WS-NAS）机制，并引入Farthest Greedy Path Sampling（FGPS）策略来增强搜索空间的覆盖率和多样性，从而更好地利用权重共享机制来搜索优秀的建筑。</li>
<li>results: 在三个Click-Through Rate（CTR）预测 benchmark 上，该方法可以准确地预测建筑，并且在大多数 NAS 基于的模型中表现出色，超越了 manually designed 模型。<details>
<summary>Abstract</summary>
Weight-sharing Neural Architecture Search (WS-NAS) provides an efficient mechanism for developing end-to-end deep recommender models. However, in complex search spaces, distinguishing between superior and inferior architectures (or paths) is challenging. This challenge is compounded by the limited coverage of the supernet and the co-adaptation of subnet weights, which restricts the exploration and exploitation capabilities inherent to weight-sharing mechanisms. To address these challenges, we introduce Farthest Greedy Path Sampling (FGPS), a new path sampling strategy that balances path quality and diversity. FGPS enhances path diversity to facilitate more comprehensive supernet exploration, while emphasizing path quality to ensure the effective identification and utilization of promising architectures. By incorporating FGPS into a Two-shot NAS (TS-NAS) framework, we derive high-performance architectures. Evaluations on three Click-Through Rate (CTR) prediction benchmarks demonstrate that our approach consistently achieves superior results, outperforming both manually designed and most NAS-based models.
</details>
<details>
<summary>摘要</summary>
weight-sharing neural architecture search (WS-NAS) 提供了一种高效的终端深度推荐模型开发机制。然而，在复杂的搜索空间中，辨认优于劣的架构（或路径）是具有挑战性。这些挑战得到加剧，因为超网络覆盖率有限，而且子网准重共适应，这限制了weight-sharing机制中的探索和利用能力。为解决这些挑战，我们介绍了远程最大贪婪路径采样策略（FGPS），这种策略可以平衡路径质量和多样性。FGPS增强了超网络探索的多样性，同时强调路径质量，以确保有效地识别和利用优秀架构。通过将FGPS纳入TS-NAS框架中，我们得到了高性能的架构。我们在三个Click-Through Rate（CTR）预测 benchmark上进行评估，发现我们的方法可以准确地 дости得优于 manually designed 和大多数 NAS-based 模型。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Multistate-Bennett-Acceptance-Ratio-Methods"><a href="#Bayesian-Multistate-Bennett-Acceptance-Ratio-Methods" class="headerlink" title="Bayesian Multistate Bennett Acceptance Ratio Methods"></a>Bayesian Multistate Bennett Acceptance Ratio Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20699">http://arxiv.org/abs/2310.20699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinqiang Ding</li>
<li>for:  Computing free energies of thermodynamic states using a Bayesian approach.</li>
<li>methods:  Bayesian generalization of the MBAR method, integrating configurations sampled from thermodynamic states with a prior distribution to compute a posteriori distribution of free energies.</li>
<li>results:  More accurate uncertainty estimates than MBAR, and the ability to incorporate prior knowledge about free energies into the estimation procedure.<details>
<summary>Abstract</summary>
The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach for computing free energies of thermodynamic states. In this work, we introduce BayesMBAR, a Bayesian generalization of the MBAR method. By integrating configurations sampled from thermodynamic states with a prior distribution, BayesMBAR computes a posterior distribution of free energies. Using the posterior distribution, we derive free energy estimations and compute their associated uncertainties. Notably, when a uniform prior distribution is used, BayesMBAR recovers the MBAR's result but provides more accurate uncertainty estimates. Additionally, when prior knowledge about free energies is available, BayesMBAR can incorporate this information into the estimation procedure by using non-uniform prior distributions. As an example, we show that, by incorporating the prior knowledge about the smoothness of free energy surfaces, BayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's widespread use in free energy calculations, we anticipate BayesMBAR to be an essential tool in various applications of free energy calculations.
</details>
<details>
<summary>摘要</summary>
“多状态本нет特征比率（MBAR）方法是一种广泛应用于计算热力学状态的自由能计算方法。在这篇论文中，我们介绍了抽象 bayesMBAR，一种 bayesian 扩展的 MBAR 方法。通过将温度状态中的配置与一个先验分布集成起来，抽象 bayesMBAR 计算了后验自由能分布。使用后验分布，我们得到了自由能估计值和相关的不确定性。值得注意的是，当使用均勋先验分布时，抽象 bayesMBAR 可以重新计算 MBAR 的结果，并提供更加准确的不确定性估计。此外，当有具体关于自由能的先验知识时，抽象 bayesMBAR 可以在计算过程中包含这些信息，使用非均勋先验分布。作为一个例子，我们显示了，通过包含自由能表面的平滑性先验知识，抽象 bayesMBAR 可以提供更加准确的估计值。由于 MBAR 在自由能计算中广泛应用，我们预计抽象 bayesMBAR 将成为各种自由能计算应用中的重要工具。”
</details></li>
</ul>
<hr>
<h2 id="Compression-with-Exact-Error-Distribution-for-Federated-Learning"><a href="#Compression-with-Exact-Error-Distribution-for-Federated-Learning" class="headerlink" title="Compression with Exact Error Distribution for Federated Learning"></a>Compression with Exact Error Distribution for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20682">http://arxiv.org/abs/2310.20682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Hegazy, Rémi Leluc, Cheuk Ting Li, Aymeric Dieuleveut</li>
<li>for: 这篇论文主要用于探讨 Federated Learning（分布式学习）中的压缩方法，以降低分布式学习中的通信成本。</li>
<li>methods: 这篇论文提出了基于层次量化器的各种压缩和汇集方法，以实现具体的错误分布（如 Gaussian 或 Laplace）在汇集数据上。</li>
<li>results: 论文表明，使用提出的压缩和汇集方法可以在分布式学习中实现免压缩（compression-for-free），并且可以提高标准的分布式学习方案中的 Gaussian 噪声（如 Langevin 动力学和随机缓和）。<details>
<summary>Abstract</summary>
Compression schemes have been extensively used in Federated Learning (FL) to reduce the communication cost of distributed learning. While most approaches rely on a bounded variance assumption of the noise produced by the compressor, this paper investigates the use of compression and aggregation schemes that produce a specific error distribution, e.g., Gaussian or Laplace, on the aggregated data. We present and analyze different aggregation schemes based on layered quantizers achieving exact error distribution. We provide different methods to leverage the proposed compression schemes to obtain compression-for-free in differential privacy applications. Our general compression methods can recover and improve standard FL schemes with Gaussian perturbations such as Langevin dynamics and randomized smoothing.
</details>
<details>
<summary>摘要</summary>
《压缩方法在分布式学习中的应用广泛，以减少分布式学习中的通信成本。大多数方法假设压缩器生成的噪声具有有界变量，但这篇论文研究了使用压缩和汇总方案，生成特定噪声分布，例如 Gaussian 或 Laplace 分布，在汇总数据中。我们提出和分析了不同层次量化器基于的汇总方案，并提供了不同的方法，使用我们的通用压缩方法在 differential privacy 应用中实现压缩-for-free。我们的方法可以恢复和改进标准分布式学习方案中的 Gaussian 噪声，如朗格文动力学和随机平滑。》Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Latent-Field-Discovery-In-Interacting-Dynamical-Systems-With-Neural-Fields"><a href="#Latent-Field-Discovery-In-Interacting-Dynamical-Systems-With-Neural-Fields" class="headerlink" title="Latent Field Discovery In Interacting Dynamical Systems With Neural Fields"></a>Latent Field Discovery In Interacting Dynamical Systems With Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20679">http://arxiv.org/abs/2310.20679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkofinas/aether">https://github.com/mkofinas/aether</a></li>
<li>paper_authors: Miltiadis Kofinas, Erik J. Bekkers, Naveen Shankar Nagaraja, Efstratios Gavves</li>
<li>for: 本研究旨在发现和学习互动对象系统中的场效应，以便更好地理解和预测系统的动态。</li>
<li>methods: 本研究使用神经网络学习方法，包括equivariant graph networks和场网络，以捕捉本系统的局部对象互动和全局场效应。</li>
<li>results: 实验表明，使用这种方法可以准确地发现互动对象系统中的场效应，并使用这些场效应来预测系统的未来轨迹。<details>
<summary>Abstract</summary>
Systems of interacting objects often evolve under the influence of field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, without directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to disentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariant and depend on relative states -- from external global field effects -- which depend on absolute states. We model interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates field forces. Our experiments show that we can accurately discover the underlying fields in charged particles settings, traffic scenes, and gravitational n-body problems, and effectively use them to learn the system and forecast future trajectories.
</details>
<details>
<summary>摘要</summary>
系统经常由相互作用的对象组成，但以前的研究通常忽略了这些效应，假设系统在真空中进行演化。在这项工作中，我们将关注发现这些场，从观察的动态来INFER它们，而不是直接观察它们。我们推测存在潜在的势场，并提议使用神经场来学习它们。由于观察的动态表示系统中对象之间的本地互动以及全局场效应的积加，近期流行的等变网络（equivariant networks）是无法捕捉全局信息的。为解决这个问题，我们提议分解本地对象互动（ $\mathrm{SE}(n)$ 对称的）和外部全局场效应（depends on absolute states）。我们使用对称图学网络（equivariant graph networks）模型对象之间的互动，并将其与神经场结合在一起。我们的实验表明，我们可以准确地发现带电粒子、交通场景和重力n体问题中的下面场，并使用它们来学习系统和预测未来轨迹。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Act-Constraining-Disparate-Impact-in-Sparse-Models"><a href="#Balancing-Act-Constraining-Disparate-Impact-in-Sparse-Models" class="headerlink" title="Balancing Act: Constraining Disparate Impact in Sparse Models"></a>Balancing Act: Constraining Disparate Impact in Sparse Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20673">http://arxiv.org/abs/2310.20673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/merajhashemi/balancing_act">https://github.com/merajhashemi/balancing_act</a></li>
<li>paper_authors: Meraj Hashemizadeh, Juan Ramirez, Rohan Sukumaran, Golnoosh Farnadi, Simon Lacoste-Julien, Jose Gallego-Posada</li>
<li>for: 这篇论文的目的是为了提出一种可以在边缘设备上部署大型深度学习模型的方法，并且能够确保模型的性能不会过分差异。</li>
<li>methods: 这篇论文使用的方法是一种受限的优化方法，可以对模型进行剪裁，以提高模型的简洁性和可靠性。</li>
<li>results: 实验结果显示，这篇论文提出的技术可以对大型模型和具有大量保护子集的情况进行可靠的测试，并且可以实现较好的性能和可靠性。<details>
<summary>Abstract</summary>
Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that $\textit{directly addresses the disparate impact of pruning}$: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:模型剔除是一种广泛使用的方法，启用大型深度学习模型在边缘设备上部署，由于限制的计算或存储能力。虽然稀疏模型在整个数据集级别上达到相似的性能，但对于一些数据子组，它们会导致准确性下降。现有的方法来减轻这种差异性影响（i）使用间接指标，无法直接解释问题，或（ii）在保护子组数量增加时，计算成本不可控。我们提出了一种约束优化方法，直接解决剔除引起的差异性影响：我们的形式约束剔除后模型和整个模型之间的准确度变化，对每个子组进行约束。这种约束选择提供了可解释的成功标准，以确定剔除后模型是否达到了接受到的差异水平。实验结果表明，我们的技术可靠地扩展到大模型和百个保护子组的问题。
</details></li>
</ul>
<hr>
<h2 id="Density-Matrix-Emulation-of-Quantum-Recurrent-Neural-Networks-for-Multivariate-Time-Series-Prediction"><a href="#Density-Matrix-Emulation-of-Quantum-Recurrent-Neural-Networks-for-Multivariate-Time-Series-Prediction" class="headerlink" title="Density Matrix Emulation of Quantum Recurrent Neural Networks for Multivariate Time Series Prediction"></a>Density Matrix Emulation of Quantum Recurrent Neural Networks for Multivariate Time Series Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20671">http://arxiv.org/abs/2310.20671</a></li>
<li>repo_url: None</li>
<li>paper_authors: José Daniel Viqueira, Daniel Faílde, Mariamo M. Juane, Andrés Gómez, David Mera</li>
<li>for: 模型和预测未来值的多变量时间序列</li>
<li>methods: 使用密度矩阵ormal formalism和维度notation来实现电路的特性Matrix formalism和维度notation</li>
<li>results: 通过使用novel hardware-efficient ansatz和三个多样化的数据集来测试和 validate QRNNs的准确性和可靠性，并证明QRNNs可以准确预测未来值，并 capture不同复杂性的输入序列中的非常见模式。<details>
<summary>Abstract</summary>
Quantum Recurrent Neural Networks (QRNNs) are robust candidates to model and predict future values in multivariate time series. However, the effective implementation of some QRNN models is limited by the need of mid-circuit measurements. Those increase the requirements for quantum hardware, which in the current NISQ era does not allow reliable computations. Emulation arises as the main near-term alternative to explore the potential of QRNNs, but existing quantum emulators are not dedicated to circuits with multiple intermediate measurements. In this context, we design a specific emulation method that relies on density matrix formalism. The mathematical development is explicitly provided as a compact formulation by using tensor notation. It allows us to show how the present and past information from a time series is transmitted through the circuit, and how to reduce the computational cost in every time step of the emulated network. In addition, we derive the analytical gradient and the Hessian of the network outputs with respect to its trainable parameters, with an eye on gradient-based training and noisy outputs that would appear when using real quantum processors. We finally test the presented methods using a novel hardware-efficient ansatz and three diverse datasets that include univariate and multivariate time series. Our results show how QRNNs can make accurate predictions of future values by capturing non-trivial patterns of input series with different complexities.
</details>
<details>
<summary>摘要</summary>
量子循环神经网络（QRNN）是多变量时间序列预测的可靠候选者。然而，一些QRNN模型的实现效率受到中间测量的限制，这会增加量子硬件的需求，目前的NISQ时代并不可靠计算。虚拟量子计算是短期代替方法，但现有的量子模拟器不适用于多中间测量的环路。在这种情况下，我们设计了专门的模拟方法，基于密度矩阵 formalism。我们使用tensor notation的紧凑表示法，以示如何在环路中传递当前和过去时间序列信息，并如何在每个时间步中减少计算成本。此外，我们计算了网络输出的导数和偏微分，以便使用梯度下降法进行训练和避免使用真正的量子处理器时的噪声输出。最后，我们使用一种新的硬件高效的思路和三种多样的时间序列 dataset进行测试。我们的结果表明，QRNN可以准确预测未来值，并 capture非常复杂的输入序列中的非ingtrivial征性。
</details></li>
</ul>
<hr>
<h2 id="Performance-Improvement-in-Multi-class-Classification-via-Automated-Hierarchy-Generation-and-Exploitation-through-Extended-LCPN-Schemes"><a href="#Performance-Improvement-in-Multi-class-Classification-via-Automated-Hierarchy-Generation-and-Exploitation-through-Extended-LCPN-Schemes" class="headerlink" title="Performance Improvement in Multi-class Classification via Automated Hierarchy Generation and Exploitation through Extended LCPN Schemes"></a>Performance Improvement in Multi-class Classification via Automated Hierarchy Generation and Exploitation through Extended LCPN Schemes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20641">http://arxiv.org/abs/2310.20641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celal Alagoz</li>
<li>for: This paper focuses on the performance of hierarchical classification (HC) in multi-class classification tasks, particularly in scenarios where a predefined hierarchy structure is not readily accessible.</li>
<li>methods: The paper explores hierarchy generation and hierarchy exploitation schemes, including two novel schemes (LCPN+ and LCPN+F) that extend the capabilities of LCPN and combine the strengths of global and local classification.</li>
<li>results: The findings show the consistent superiority of LCPN+F, which outperforms other schemes across various datasets and scenarios, while maintaining runtime performance comparable to Flat Classification (FC).<details>
<summary>Abstract</summary>
Hierarchical classification (HC) plays a pivotal role in multi-class classification tasks, where objects are organized into a hierarchical structure. This study explores the performance of HC through a comprehensive analysis that encompasses both hierarchy generation and hierarchy exploitation. This analysis is particularly relevant in scenarios where a predefined hierarchy structure is not readily accessible. Notably, two novel hierarchy exploitation schemes, LCPN+ and LCPN+F, which extend the capabilities of LCPN and combine the strengths of global and local classification, have been introduced and evaluated alongside existing methods. The findings reveal the consistent superiority of LCPN+F, which outperforms other schemes across various datasets and scenarios. Moreover, this research emphasizes not only effectiveness but also efficiency, as LCPN+ and LCPN+F maintain runtime performance comparable to Flat Classification (FC). Additionally, this study underscores the importance of selecting the right hierarchy exploitation scheme to maximize classification performance. This work extends our understanding of HC and establishes a benchmark for future research, fostering advancements in multi-class classification methodologies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Projecting-basis-functions-with-tensor-networks-for-Gaussian-process-regression"><a href="#Projecting-basis-functions-with-tensor-networks-for-Gaussian-process-regression" class="headerlink" title="Projecting basis functions with tensor networks for Gaussian process regression"></a>Projecting basis functions with tensor networks for Gaussian process regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20630">http://arxiv.org/abs/2310.20630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clara Menzen, Eva Memmel, Kim Batselier, Manon Kok</li>
<li>for: 这个论文是关于精确 Gaussian Process（GP）回归的方法，使用张量网络（TN）。</li>
<li>methods: 我们使用低维张量网络来找到适当的低维度子空间，然后通过 Bayesian 推断问题来找到模型的权重。最后，我们将结果投影回原空间来进行 GP 预测。</li>
<li>results: 我们在一个18维数据集上进行了一个 inverse dynamics 问题的实验，并证明了我们的方法的可行性。<details>
<summary>Abstract</summary>
This paper presents a method for approximate Gaussian process (GP) regression with tensor networks (TNs). A parametric approximation of a GP uses a linear combination of basis functions, where the accuracy of the approximation depends on the total number of basis functions $M$. We develop an approach that allows us to use an exponential amount of basis functions without the corresponding exponential computational complexity. The key idea to enable this is using low-rank TNs. We first find a suitable low-dimensional subspace from the data, described by a low-rank TN. In this low-dimensional subspace, we then infer the weights of our model by solving a Bayesian inference problem. Finally, we project the resulting weights back to the original space to make GP predictions. The benefit of our approach comes from the projection to a smaller subspace: It modifies the shape of the basis functions in a way that it sees fit based on the given data, and it allows for efficient computations in the smaller subspace. In an experiment with an 18-dimensional benchmark data set, we show the applicability of our method to an inverse dynamics problem.
</details>
<details>
<summary>摘要</summary>
First, we find a suitable low-dimensional subspace from the data using a low-rank TN. Then, we solve a Bayesian inference problem in this low-dimensional subspace to infer the weights of our model. Finally, we project the resulting weights back to the original space to make GP predictions. The benefit of our approach is that it modifies the shape of the basis functions based on the given data, allowing for efficient computations in a smaller subspace.In an experiment with an 18-dimensional benchmark data set, we demonstrate the applicability of our method to an inverse dynamics problem. Our approach enables the use of an exponential number of basis functions without an exponential increase in computational complexity, making it a promising method for large-scale GP regression problems.
</details></li>
</ul>
<hr>
<h2 id="Graph-Matching-via-convex-relaxation-to-the-simplex"><a href="#Graph-Matching-via-convex-relaxation-to-the-simplex" class="headerlink" title="Graph Matching via convex relaxation to the simplex"></a>Graph Matching via convex relaxation to the simplex</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20609">http://arxiv.org/abs/2310.20609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernesto Araya Valdivia, Hemant Tyagi</li>
<li>for:  solves the Graph Matching problem, which is NP-hard and has many applications in computer vision, network deanonymization, and protein alignment.</li>
<li>methods:  introduces a new convex relaxation onto the unit simplex and develops an efficient mirror descent scheme with closed-form iterations to solve the problem.</li>
<li>results:  shows that the simplex relaxation admits a unique solution with high probability in the noiseless case, which implies exact recovery of the ground truth permutation. Additionally, establishes a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used &#96;diagonal dominance’ condition.<details>
<summary>Abstract</summary>
This paper addresses the Graph Matching problem, which consists of finding the best possible alignment between two input graphs, and has many applications in computer vision, network deanonymization and protein alignment. A common approach to tackle this problem is through convex relaxations of the NP-hard \emph{Quadratic Assignment Problem} (QAP).   Here, we introduce a new convex relaxation onto the unit simplex and develop an efficient mirror descent scheme with closed-form iterations for solving this problem. Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this is shown to imply exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used `diagonal dominance' condition. We use this condition to show exact one-step recovery of the ground truth (holding almost surely) via the mirror descent scheme, in the noiseless setting. We also use this condition to obtain significantly improved conditions for the GRAMPA algorithm [Fan et al. 2019] in the noiseless setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-Conversion-with-Switching-Costs-Robust-and-Learning-Augmented-Algorithms"><a href="#Online-Conversion-with-Switching-Costs-Robust-and-Learning-Augmented-Algorithms" class="headerlink" title="Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms"></a>Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20598">http://arxiv.org/abs/2310.20598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</li>
<li>for: 这篇论文研究了在线转换问题，即在一定时间间隔T内，一个在线玩家尝试购买（或卖）不同资产的分数部分，并且遇到转换成本每次决策时更改。</li>
<li>methods: 论文提出了一种基于阈值的在线算法，可以在竞争环境中最优化转换问题的解决方案。此外，论文还提出了一种学习增强的算法，可以通过不可信黑盒模型的预测来提高平均性能而无需牺牲最坏情况的竞争优化。</li>
<li>results: 实验结果表明，论文提出的算法在碳警惕电动车充电问题中具有显著的改善，比基准方法更高效。<details>
<summary>Abstract</summary>
We introduce and study online conversion with switching costs, a family of online problems that capture emerging problems at the intersection of energy and sustainability. In this problem, an online player attempts to purchase (alternatively, sell) fractional shares of an asset during a fixed time horizon with length $T$. At each time step, a cost function (alternatively, price function) is revealed, and the player must irrevocably decide an amount of asset to convert. The player also incurs a switching cost whenever their decision changes in consecutive time steps, i.e., when they increase or decrease their purchasing amount. We introduce competitive (robust) threshold-based algorithms for both the minimization and maximization variants of this problem, and show they are optimal among deterministic online algorithms. We then propose learning-augmented algorithms that take advantage of untrusted black-box advice (such as predictions from a machine learning model) to achieve significantly better average-case performance without sacrificing worst-case competitive guarantees. Finally, we empirically evaluate our proposed algorithms using a carbon-aware EV charging case study, showing that our algorithms substantially improve on baseline methods for this problem.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究在线转换问题，这是一类在能源和可持续性领域出现的问题。在这个问题中，一个在线玩家尝试在固定时间跨度T内购买（或者卖出）资产的分数。在每个时间步骤中，一个成本函数（或者价格函数）会被公布，玩家必须不可逆决定一个资产的量。在 consecutive 时间步骤中改变决定时，玩家会付出跳转成本。我们引入了竞争（稳定）阈值基于算法，并证明它们在杜林在线算法中是优于的。然后，我们提出了学习增强的算法，它们可以通过不可信的黑盒模型（如机器学习模型的预测）来实现显著更好的平均情况性能，而无需牺牲最坏情况的竞争保证。最后，我们employs a carbon-aware EV charging case study to empirically evaluate our proposed algorithms, showing that they substantially improve on baseline methods for this problem.Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Pre-trained-Language-Models-for-Offline-Reinforcement-Learning"><a href="#Unleashing-the-Power-of-Pre-trained-Language-Models-for-Offline-Reinforcement-Learning" class="headerlink" title="Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning"></a>Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20587">http://arxiv.org/abs/2310.20587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, Huazhe Xu</li>
<li>For: The paper is written for offline reinforcement learning (RL) in real-world scenarios where data collection is costly and risky, and the in-domain data is limited.* Methods: The paper introduces a general framework called $\textbf{LaMo}$ based on Decision Transformers, which effectively uses pre-trained Language Models (LMs) for offline RL. The framework includes four crucial components: (1) initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, (3) using non-linear MLP transformation instead of linear projections, and (4) integrating an auxiliary language prediction loss during fine-tuning.* Results: The paper achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks, especially in scenarios with limited data samples.Here’s the simplified Chinese text for the three information points:* 为: 本文是为offline Reinforcement Learning（RL）在现实世界中的数据收集成本高昂和风险的情况而写的。* 方法: 本文引入了一个通用框架 called $\textbf{LaMo}$，基于决策变换器，可以有效地使用预训练的语言模型（LMs）来进行offline RL。该框架包括四个关键组件：(1) 初始化决策变换器使用顺序预训练LMs，(2) 使用LoRA fine-tuning方法，相比于全量精度调整，可以有效地结合预训练知识从LMs和区域知识，(3) 使用非线性MLP变换而不是直线投影，生成嵌入，(4) 在精度调整过程中添加语言预测损失，以稳定LMs和保留其原有语言能力。* 结果: 本文在罕见奖励任务中达到了状态机器人学的最佳性能，并在粗略奖励任务中距离值基于offline RL方法和决策变换器的性能减少到最小。特别是在数据样本有限的情况下，本方法表现出了优秀的性能。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate $\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples. Our project website is https://lamo2023.github.io
</details>
<details>
<summary>摘要</summary>
偏向学习（Offline Reinforcement Learning）的目标是找到近似优化策略，使用预收集的数据进行训练。在实际场景中，数据收集可能是成本高昂且风险重的，因此偏向学习在有限的域内数据时变得特别挑战。为了解决这个问题，这篇论文提出了一种基于决策变换器的框架，称为语言模型 для动作控制（LaMo）。我们的框架包括四个关键组成部分：1. 使用顺序预训练的语言模型（LM）初始化决策变换器。2. 使用LoRA fine-tuning方法，而不是全量 fine-tuning，将预训练知识从LM和域内知识相结合。3. 使用非线性的多层感知变换而不是线性投影，生成嵌入。4. 在练习过程中添加语言预测损失，以稳定LM和保留其原有语言能力。我们的实验结果表明，LaMo在稀有奖励任务中取得了状态最佳性表现，并在 dense-reward 任务中降低了决策变换器和值基本的偏向学习方法之间的差距。尤其是在有限数据样本的情况下，LaMo表现出色。有关我们的项目，请访问我们的项目网站：<https://lamo2023.github.io>
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Gradient-Descent-for-Gaussian-Processes-Done-Right"><a href="#Stochastic-Gradient-Descent-for-Gaussian-Processes-Done-Right" class="headerlink" title="Stochastic Gradient Descent for Gaussian Processes Done Right"></a>Stochastic Gradient Descent for Gaussian Processes Done Right</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20581">http://arxiv.org/abs/2310.20581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihao Andreas Lin, Shreyas Padhy, Javier Antorán, Austin Tripp, Alexander Terenin, Csaba Szepesvári, José Miguel Hernández-Lobato, David Janz</li>
<li>for: 本研究探讨了 Gaussian process regression 优化问题，使用平方损失函数。</li>
<li>methods: 本paper使用了 Stochastic dual gradient descent 算法，并提供了一些特定的设计选择，以提高其效果。</li>
<li>results: 对标准回归benchmark和一个 bayesian优化任务，本方法与 preconditioned conjugate gradients、variational Gaussian process approximations、以及之前的 Stochastic gradient descent for Gaussian processes 相比，显示了更高的竞争力。在一个分子绑定亲和力预测任务上，本方法使 Gaussian process regression 与STATE-OF-THE-ART graph neural networks 的性能相当。<details>
<summary>Abstract</summary>
We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apart from preconditioned conjugate gradients, variational Gaussian process approximations, and a previous version of stochastic gradient descent for Gaussian processes. On a molecular binding affinity prediction task, our method places Gaussian process regression on par in terms of performance with state-of-the-art graph neural networks.
</details>
<details>
<summary>摘要</summary>
我们研究了 Gaussian process regression 的优化问题，使用平方损失函数。最常见的方法是使用精确的解法，如 conjugate gradient descent，直接或者将问题缩放到一个更小的版本上。在这篇文章中，我们表明了使用 deep learning 的成功，stochastic gradient descent 已成为一种有力的代替方法。当使用特定的优化和内核社区的知识时，这种方法非常有效。因此，我们介绍了一种特定的随机对角 gradient descent 算法，可以使用任何深度学习框架进行实现，只需几行代码即可。我们解释了我们的设计决策，并通过减少对比研究表明其优势。我们的评估结果表明，我们的方法在标准的回归 benchmark 和 Bayesian 优化任务上与前conditioned conjugate gradients、variational Gaussian process approximation 和一个前一个随机 gradient descent 方法相比较，表现更出色。在一个分子绑定亲和力预测任务上，我们的方法使 Gaussian process regression 与 state-of-the-art graph neural networks 的性能相当。
</details></li>
</ul>
<hr>
<h2 id="Initialization-Matters-Privacy-Utility-Analysis-of-Overparameterized-Neural-Networks"><a href="#Initialization-Matters-Privacy-Utility-Analysis-of-Overparameterized-Neural-Networks" class="headerlink" title="Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks"></a>Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20579">http://arxiv.org/abs/2310.20579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Ye, Zhenyu Zhu, Fanghui Liu, Reza Shokri, Volkan Cevher</li>
<li>for: 这个论文 investigate 随机机器学习算法中模型过参数化对训练数据泄露信息的影响。</li>
<li>methods: 作者Proof a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on initialization, width, and depth of fully connected neural networks.</li>
<li>results: 研究发现，这个KL privacy bound largely depends on the expected squared gradient norm relative to model parameters during training. 特别是，在linearized network setting中，squared gradient norm directly tied to the per-layer variance of the initialization distribution. 此外，作者还证明了在固定KL privacy budget下的过employmerisk bounds,并发现了训练深度和 initialization distribution之间的复杂关系。<details>
<summary>Abstract</summary>
We analytically investigate how over-parameterization of models in randomized machine learning algorithms impacts the information leakage about their training data. Specifically, we prove a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on the initialization, width, and depth of fully connected neural networks. We find that this KL privacy bound is largely determined by the expected squared gradient norm relative to model parameters during training. Notably, for the special setting of linearized network, our analysis indicates that the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution. By using this analysis, we demonstrate that privacy bound improves with increasing depth under certain initializations (LeCun and Xavier), while degrades with increasing depth under other initializations (He and NTK). Our work reveals a complex interplay between privacy and depth that depends on the chosen initialization distribution. We further prove excess empirical risk bounds under a fixed KL privacy budget, and show that the interplay between privacy utility trade-off and depth is similarly affected by the initialization.
</details>
<details>
<summary>摘要</summary>
我们分析了随机机器学习算法中模型过参数化对训练数据信息泄露的影响。特别是，我们证明了模型分布之间的KL差 privacy bound，并研究其取决于模型初始化、宽度和深度。我们发现这个KL privacy bound与模型训练过程中参数的预期平方Gradient norm之间有直接关系。特别是，对于特殊的线性化网络设置，我们的分析表明，预期平方Gradient norm（并因此隐私损失的增加）与初始化分布的层次变化直接相关。通过这种分析，我们证明了深度随着初始化的变化而改善的隐私约束，而对于其他初始化而言，隐私约束会随着深度的增加而下降。我们的工作表明了隐私和深度之间的复杂互动，这种互动取决于选择的初始化分布。我们还证明了fixed KL隐私预算下的过 Training Empirical Risk bounds，并显示了隐私utiltytrade-off和深度之间的相互作用，这种相互作用也受到初始化的影响。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretic-Trust-Regions-for-Stochastic-Gradient-Based-Optimization"><a href="#Information-Theoretic-Trust-Regions-for-Stochastic-Gradient-Based-Optimization" class="headerlink" title="Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization"></a>Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20574">http://arxiv.org/abs/2310.20574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alrhub/arturo">https://github.com/alrhub/arturo</a></li>
<li>paper_authors: Philipp Dahlinger, Philipp Becker, Maximilian Hüttenrauch, Gerhard Neumann</li>
<li>for: 优化神经网络中的梯度下降法是关键，而现有的方法通过减小步长和方向的补做来尝试优化搜索方法，但这些方法并不是很理智的。本文提出一种基于信息理论的信任区域优化方法（arTuRO），可以在梯度下降的情况下更好地利用第二阶信息。</li>
<li>methods: 本文使用信息理论信任区域优化方法（arTuRO），其中首先模型神经网络参数为高斯分布，然后使用Kullback-Leibler偏度来确定信任区域，最后在信任区域内解决最优步长问题，以获得更稳定和更快的优化过程。在计算信任区域中，我们使用随机最小二乘方法来 aproximate对象函数的对偶矩阵的元素。</li>
<li>results: 作者表明，使用arTuRO可以结合渐进式权重更新和SGD的快速收敛和泛化能力。<details>
<summary>Abstract</summary>
Stochastic gradient-based optimization is crucial to optimize neural networks. While popular approaches heuristically adapt the step size and direction by rescaling gradients, a more principled approach to improve optimizers requires second-order information. Such methods precondition the gradient using the objective's Hessian. Yet, computing the Hessian is usually expensive and effectively using second-order information in the stochastic gradient setting is non-trivial. We propose using Information-Theoretic Trust Region Optimization (arTuRO) for improved updates with uncertain second-order information. By modeling the network parameters as a Gaussian distribution and using a Kullback-Leibler divergence-based trust region, our approach takes bounded steps accounting for the objective's curvature and uncertainty in the parameters. Before each update, it solves the trust region problem for an optimal step size, resulting in a more stable and faster optimization process. We approximate the diagonal elements of the Hessian from stochastic gradients using a simple recursive least squares approach, constructing a model of the expected Hessian over time using only first-order information. We show that arTuRO combines the fast convergence of adaptive moment-based optimization with the generalization capabilities of SGD.
</details>
<details>
<summary>摘要</summary>
We propose using Information-Theoretic Trust Region Optimization (arTuRO) for improved updates with uncertain second-order information. By modeling the network parameters as a Gaussian distribution and using a Kullback-Leibler divergence-based trust region, our approach takes bounded steps accounting for the objective's curvature and uncertainty in the parameters. Before each update, it solves the trust region problem for an optimal step size, resulting in a more stable and faster optimization process.We approximate the diagonal elements of the Hessian from stochastic gradients using a simple recursive least squares approach, constructing a model of the expected Hessian over time using only first-order information. We show that arTuRO combines the fast convergence of adaptive moment-based optimization with the generalization capabilities of SGD.
</details></li>
</ul>
<hr>
<h2 id="One-shot-backpropagation-for-multi-step-prediction-in-physics-based-system-identification"><a href="#One-shot-backpropagation-for-multi-step-prediction-in-physics-based-system-identification" class="headerlink" title="One-shot backpropagation for multi-step prediction in physics-based system identification"></a>One-shot backpropagation for multi-step prediction in physics-based system identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20567">http://arxiv.org/abs/2310.20567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesare Donati, Martina Mammarella, Fabrizio Dabbene, Carlo Novara, Constantino Lagoa</li>
<li>for: 提出一种通用框架，用于识别可能相连的系统，保持物理性质和多步预测精度。</li>
<li>methods: 提出一种分析和递归算法，用于计算多步损失函数的梯度，基于反射学习算法，提供物理和结构直观。</li>
<li>results: 在Space debris的静态观测数据上测试了提出的方法，实现了准确的阻尼矩计算。<details>
<summary>Abstract</summary>
The aim of this paper is to present a novel general framework for the identification of possibly interconnected systems, while preserving their physical properties and providing accuracy in multi-step prediction. An analytical and recursive algorithm for the gradient computation of the multi-step loss function based on backpropagation is introduced, providing physical and structural insight directly into the learning algorithm. As a case study, the proposed approach is tested for estimating the inertia matrix of a space debris starting from state observations.
</details>
<details>
<summary>摘要</summary>
本文的目的是提出一种新的总体框架，用于可能相互连接的系统的特征标识，保持物理性质和多步预测精度。我们提出了一种分析和递归的梯度计算方法，基于反射传播来计算多步损失函数的梯度，从而直接将学习算法中的物理和结构性质带入到学习过程中。作为案例研究，我们对一个空间垃圾的惯性矩进行估计。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-design-of-graph-neural-networks-with-applications-to-vertical-federated-learning"><a href="#Privacy-preserving-design-of-graph-neural-networks-with-applications-to-vertical-federated-learning" class="headerlink" title="Privacy-preserving design of graph neural networks with applications to vertical federated learning"></a>Privacy-preserving design of graph neural networks with applications to vertical federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20552">http://arxiv.org/abs/2310.20552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruofan Wu, Mingyang Zhang, Lingjuan Lyu, Xiaolong Xu, Xiuquan Hao, Xinyi Fu, Tengfei Liu, Tianyi Zhang, Weiqiang Wang</li>
<li>for: 这篇论文旨在提出一个称为VESPER的纵向联邦学习（VFL）框架，用于金融风险管理（FRM）应用中。</li>
<li>methods: 这篇论文使用了一种称为干扰讯息传递（PMP）的通用隐私保证方案，以适应各种几何学神经网络。</li>
<li>results: 实验结果显示VESPER可以在合理的隐私预算下训练高性能的几何学神经网络，并且适用于稠密和稀疏几何学数据。<details>
<summary>Abstract</summary>
The paradigm of vertical federated learning (VFL), where institutions collaboratively train machine learning models via combining each other's local feature or label information, has achieved great success in applications to financial risk management (FRM). The surging developments of graph representation learning (GRL) have opened up new opportunities for FRM applications under FL via efficiently utilizing the graph-structured data generated from underlying transaction networks. Meanwhile, transaction information is often considered highly sensitive. To prevent data leakage during training, it is critical to develop FL protocols with formal privacy guarantees. In this paper, we present an end-to-end GRL framework in the VFL setting called VESPER, which is built upon a general privatization scheme termed perturbed message passing (PMP) that allows the privatization of many popular graph neural architectures.Based on PMP, we discuss the strengths and weaknesses of specific design choices of concrete graph neural architectures and provide solutions and improvements for both dense and sparse graphs. Extensive empirical evaluations over both public datasets and an industry dataset demonstrate that VESPER is capable of training high-performance GNN models over both sparse and dense graphs under reasonable privacy budgets.
</details>
<details>
<summary>摘要</summary>
vertical 联合学习（VFL）的 Paradigma，where institutions collaboratively train machine learning models by combining each other's local feature or label information, has achieved great success in financial risk management（FRM）applications. The surging developments of graph representation learning（GRL）have opened up new opportunities for FRM applications under FL by efficiently utilizing the graph-structured data generated from underlying transaction networks. However, transaction information is often considered highly sensitive. To prevent data leakage during training, it is critical to develop FL protocols with formal privacy guarantees.In this paper, we present an end-to-end GRL framework in the VFL setting called VESPER, which is built upon a general privatization scheme termed perturbed message passing（PMP）that allows the privatization of many popular graph neural architectures. Based on PMP, we discuss the strengths and weaknesses of specific design choices of concrete graph neural architectures and provide solutions and improvements for both dense and sparse graphs. Extensive empirical evaluations over both public datasets and an industry dataset demonstrate that VESPER is capable of training high-performance GNN models over both sparse and dense graphs under reasonable privacy budgets.
</details></li>
</ul>
<hr>
<h2 id="Multi-task-learning-of-convex-combinations-of-forecasting-models"><a href="#Multi-task-learning-of-convex-combinations-of-forecasting-models" class="headerlink" title="Multi-task learning of convex combinations of forecasting models"></a>Multi-task learning of convex combinations of forecasting models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20545">http://arxiv.org/abs/2310.20545</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoniosudoso/mtl-comb">https://github.com/antoniosudoso/mtl-comb</a></li>
<li>paper_authors: Giovanni Felici, Antonio M. Sudoso</li>
<li>for: 这篇论文的目的是提出一种多任务学习方法，以同时解决Feature-based forecasting中的模型选择和模型组合问题。</li>
<li>methods: 这篇论文使用了一个深度神经网络，其中有两个分支：回归分支，用于学习不同预测方法的重量，以及分类分支，用于选择最适合的预测方法。</li>
<li>results: 实验结果显示，这篇论文的提案可以与现有的方法相比，提高预测精度。<details>
<summary>Abstract</summary>
Forecast combination involves using multiple forecasts to create a single, more accurate prediction. Recently, feature-based forecasting has been employed to either select the most appropriate forecasting models or to learn the weights of their convex combination. In this paper, we present a multi-task learning methodology that simultaneously addresses both problems. This approach is implemented through a deep neural network with two branches: the regression branch, which learns the weights of various forecasting methods by minimizing the error of combined forecasts, and the classification branch, which selects forecasting methods with an emphasis on their diversity. To generate training labels for the classification task, we introduce an optimization-driven approach that identifies the most appropriate methods for a given time series. The proposed approach elicits the essential role of diversity in feature-based forecasting and highlights the interplay between model combination and model selection when learning forecasting ensembles. Experimental results on a large set of series from the M4 competition dataset show that our proposal enhances point forecast accuracy compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
预测组合是使用多个预测来生成一个更准确的预测。最近，基于特征的预测组合被应用来选择最适合的预测模型或学习预测模型的权重。在这篇论文中，我们提出了一种多任务学习方法，同时解决了这两个问题。这种方法通过深度神经网络中的两个分支：回归分支和分类分支来实现。回归分支通过最小化组合预测错误来学习不同预测方法的权重，而分类分支通过强调多样性来选择适合的预测方法。为生成训练标签的分类任务，我们提出了一种优化驱动的方法，该方法可以为给定时序序列选择最佳的预测方法。我们的方法强调了特征基于预测组合中的多样性，并高亮了组合预测和选择预测方法之间的交互关系。实验结果表明，我们的提议在M4竞赛数据集上的大量时序序列上比state-of-the-art方法提高点预测精度。
</details></li>
</ul>
<hr>
<h2 id="Group-Feature-Sensor-Selection-With-Controlled-Redundancy-Using-Neural-Networks"><a href="#Group-Feature-Sensor-Selection-With-Controlled-Redundancy-Using-Neural-Networks" class="headerlink" title="Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural Networks"></a>Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20524">http://arxiv.org/abs/2310.20524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aytijhya Saha, Nikhil R. Pal</li>
<li>for: 本研究提出了一种基于多层感知器（MLP）网络的嵌入特征选择方法，用于控制特征或感知器之间的重复性。</li>
<li>methods: 本方法使用了一种扩展的组征或感知器选择问题，并将组征lasso penalty应用于选择有价值的组征，同时保持特征之间的重复性控制。</li>
<li>results: 我们在多个 benchmark 数据集上进行了实验，并证明了该方法在特征选择和组征选择方面的批处性和稳定性，并且在一些现状方法之上显示出了优异性。<details>
<summary>Abstract</summary>
In this paper, we present a novel embedded feature selection method based on a Multi-layer Perceptron (MLP) network and generalize it for group-feature or sensor selection problems, which can control the level of redundancy among the selected features or groups. Additionally, we have generalized the group lasso penalty for feature selection to encompass a mechanism for selecting valuable group features while simultaneously maintaining a control over redundancy. We establish the monotonicity and convergence of the proposed algorithm, with a smoothed version of the penalty terms, under suitable assumptions. Experimental results on several benchmark datasets demonstrate the promising performance of the proposed methodology for both feature selection and group feature selection over some state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种基于多层感知网络（MLP）的嵌入特征选择方法，并扩展了其用于组特征或感知选择问题，以控制选择特征或组的重复级别。此外，我们扩展了组lasso负面penalty来选择有价值的组特征，同时保持特征或组的重复级别控制。我们证明提案的算法具有升elinicity和收敛性，在适当的假设下。实验结果表明，提案的方法在多个标准数据集上表现出色，在特征选择和组特征选择方面超越了一些现状方法。
</details></li>
</ul>
<hr>
<h2 id="Parametric-Fairness-with-Statistical-Guarantees"><a href="#Parametric-Fairness-with-Statistical-Guarantees" class="headerlink" title="Parametric Fairness with Statistical Guarantees"></a>Parametric Fairness with Statistical Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20508">http://arxiv.org/abs/2310.20508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paramfair/submission_974">https://github.com/paramfair/submission_974</a></li>
<li>paper_authors: François HU, Philipp Ratz, Arthur Charpentier</li>
<li>for: 这个论文旨在探讨机器学习模型中的公平问题，并提出一种基于域知识的公平度量来解决这些问题。</li>
<li>methods: 论文使用了一种基于分布性质的公平度量，即DEMOGRAPHIC PARITY，并提出了一种 Parametric 方法来实现这种公平度量。</li>
<li>results: 论文通过一个实际的薪资示例来说明了这种新的公平度量和 Parametric 方法的应用，并证明了它们可以减少 intersect 性公平问题并提供一个有效的实际应用解决方案。<details>
<summary>Abstract</summary>
Algorithmic fairness has gained prominence due to societal and regulatory concerns about biases in Machine Learning models. Common group fairness metrics like Equalized Odds for classification or Demographic Parity for both classification and regression are widely used and a host of computationally advantageous post-processing methods have been developed around them. However, these metrics often limit users from incorporating domain knowledge. Despite meeting traditional fairness criteria, they can obscure issues related to intersectional fairness and even replicate unwanted intra-group biases in the resulting fair solution. To avoid this narrow perspective, we extend the concept of Demographic Parity to incorporate distributional properties in the predictions, allowing expert knowledge to be used in the fair solution. We illustrate the use of this new metric through a practical example of wages, and develop a parametric method that efficiently addresses practical challenges like limited training data and constraints on total spending, offering a robust solution for real-life applications.
</details>
<details>
<summary>摘要</summary>
“algorithmic fairness”在社会和 regulatory 的关注下受到更多的注意，因为机器学习模型中的偏见问题。通用的集体公平度量 like “Equalized Odds”和“Demographic Parity”在分类和回归方面都很受欢迎，但这些度量往往限制用户不能 incorporate 专业知识。尽管遵循传统的公平准确，它们可能会隐藏 intersectional 公平问题，甚至在 fair 解决方案中重复不想要的 intra-group 偏见。为了避免这种狭隘的见解，我们将 Demographic Parity 扩展为包括预测的分布性质，让专业知识得以在 fair 解决方案中使用。我们透过一个实际的薪资示例来解释使用这个新度量，并开发了一个可效的 parametric 方法，可以有效地 Address 实际应用中的问题，如有限的训练数据和总支付限制。这个方法可以提供实际应用中的坚固解决方案。
</details></li>
</ul>
<hr>
<h2 id="Generative-Learning-of-Continuous-Data-by-Tensor-Networks"><a href="#Generative-Learning-of-Continuous-Data-by-Tensor-Networks" class="headerlink" title="Generative Learning of Continuous Data by Tensor Networks"></a>Generative Learning of Continuous Data by Tensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20498">http://arxiv.org/abs/2310.20498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Meiburg, Jing Chen, Jacob Miller, Raphaëlle Tihon, Guillaume Rabusseau, Alejandro Perdomo-Ortiz</li>
<li>for:  solve machine learning problems, especially unsupervised generative learning</li>
<li>methods:  introduce a new family of tensor network generative models for continuous data, which can learn from distributions containing continuous random variables</li>
<li>results:  the model can approximate any reasonably smooth probability density function with arbitrary precision, and performs well on synthetic and real-world datasets, with the ability to model different data domains and a trainable compression layer to increase performance given limited resources.<details>
<summary>Abstract</summary>
Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model learns and generalizes well on distributions of continuous and discrete variables. We develop methods for modeling different data domains, and introduce a trainable compression layer which is found to increase model performance given limited memory or computational resources. Overall, our methods give important theoretical and empirical evidence of the efficacy of quantum-inspired methods for the rapidly growing field of generative learning.
</details>
<details>
<summary>摘要</summary>
以下文本将被翻译成简化字符的中文：超过它们的起源于模拟多体量子系统的应用，tensor网络已经出现为解决机器学习问题的一个有前途的类型，尤其是无监督生成学习。具有许多愿望的特性，tensor网络生成模型却在过去被限制于二进制或 categorical 数据，这限制了它在实际模型问题中的应用。我们在这篇文章中解决了这个问题，我们引入了一个新的 tensor网络生成模型家族，可以处理包含连续随机变量的分布。我们首先在matrix product states 的设定下，得出了一个通用表达能力定理，证明这种模型家族可以将任何reasonably smooth 的概率密度函数表示为任意精度。然后，我们对几个 sintetic 和实际数据集进行了 benchmark，发现这种模型可以很好地学习和泛化在包含连续和二进制变量的分布上。我们还开发了不同数据域的模型方法，并引入了可调压缩层，这有助于在限定内存或计算资源的情况下提高模型性能。总的来说，我们的方法为生成学习领域提供了重要的理论和实证证明，证明了量子启发的方法在发展中的重要性。
</details></li>
</ul>
<hr>
<h2 id="BasisFormer-Attention-based-Time-Series-Forecasting-with-Learnable-and-Interpretable-Basis"><a href="#BasisFormer-Attention-based-Time-Series-Forecasting-with-Learnable-and-Interpretable-Basis" class="headerlink" title="BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis"></a>BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20496">http://arxiv.org/abs/2310.20496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nzl5116190/basisformer">https://github.com/nzl5116190/basisformer</a></li>
<li>paper_authors: Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, Weiyao Lin</li>
<li>for: 这篇论文是针对时间序列预测 задачі的研究，旨在提出一个可学习和可解释的基底架构，以提高时间序列预测的精度。</li>
<li>methods: 本文使用了自适应式自我超vised learning来获取基底，然后通过对时间序列和基底之间的相互关联进行 Calculate similarity coefficients，最后使用这些相互关联来预测未来的时间序列。</li>
<li>results: 经过实验证明，这篇论文的方法可以与之前的现有方法相比，在uniivariate和multivariate预测任务上分别提高了11.04%和15.78%的精度。<details>
<summary>Abstract</summary>
Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attention. Finally, we present a Forecast module that selects and consolidates the bases in the future view based on the similarity coefficients, resulting in accurate future predictions. Through extensive experiments on six datasets, we demonstrate that BasisFormer outperforms previous state-of-the-art methods by 11.04\% and 15.78\% respectively for univariate and multivariate forecasting tasks. Code is available at: \url{https://github.com/nzl5116190/Basisformer}
</details>
<details>
<summary>摘要</summary>
基于深度学习的时间序列预测模型中，基因已成为一个重要的组成部分，因为它们可以作为特征提取器或未来参照。为了有效，一个基因必须适应特定的时间序列数据集，并且与每个时间序列在该集合中存在明显的相关性。然而，当前状态艺术方法有限，不能同时满足这两个需求。为解决这个挑战，我们提出了 BasisFormer，一种端到端的时间序列预测架构，利用可学习和可解释的基因。这个架构包括三个组件：1. 我们通过适应式自监学习获取基因，将历史和未来段视为两个不同的视图，并使用对比学习。2. 我们设计了 Coef 模块，通过双向креustration来计算历史视图中时间序列和基因之间的相似性系数。3. 我们则提出了预测模块，根据相似性系数选择和结合未来视图中的基因，以获得准确的未来预测。经过对六个数据集的广泛实验，我们证明了 BasisFormer 比前一代方法提高了11.04%和15.78%，分别在单variate和多variate预测任务中。代码可以在 GitHub 上获取：\url{https://github.com/nzl5116190/Basisformer}
</details></li>
</ul>
<hr>
<h2 id="Requirement-falsification-for-cyber-physical-systems-using-generative-models"><a href="#Requirement-falsification-for-cyber-physical-systems-using-generative-models" class="headerlink" title="Requirement falsification for cyber-physical systems using generative models"></a>Requirement falsification for cyber-physical systems using generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20493">http://arxiv.org/abs/2310.20493</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshaheryarmalik/stgem">https://github.com/mshaheryarmalik/stgem</a></li>
<li>paper_authors: Jarkko Peltomäki, Ivan Porres</li>
<li>for:  automatic requirement falsification of cyber-physical systems</li>
<li>methods:  OGAN algorithm (uses generative machine learning model to produce counterexamples)</li>
<li>results:  state-of-the-art CPS falsification efficiency and effectiveness, can be applied to new systems with little effort, and exhibits few requirements for the system under test.Here’s the full text in Simplified Chinese:</li>
<li>for:  автоматиче检查 cyber-physical systems 的需求是否成立</li>
<li>methods:  OGAN 算法 (使用生成机器学习模型生成 counterexample)</li>
<li>results:  state-of-the-art CPS 验证效果和灵活性，可以轻松应用于新的系统，需求对系统 under test 非常低，验证效果卓越。<details>
<summary>Abstract</summary>
We present the OGAN algorithm for automatic requirement falsification of cyber-physical systems. System inputs and output are represented as piecewise constant signals over time while requirements are expressed in signal temporal logic. OGAN can find inputs that are counterexamples for the safety of a system revealing design, software, or hardware defects before the system is taken into operation. The OGAN algorithm works by training a generative machine learning model to produce such counterexamples. It executes tests atomically and does not require any previous model of the system under test. We evaluate OGAN using the ARCH-COMP benchmark problems, and the experimental results show that generative models are a viable method for requirement falsification. OGAN can be applied to new systems with little effort, has few requirements for the system under test, and exhibits state-of-the-art CPS falsification efficiency and effectiveness.
</details>
<details>
<summary>摘要</summary>
我团队提出了OGAN算法，用于自动化cyber-physical系统的需求证明。系统输入和输出被表示为时间上分割的常量信号，而需求则是表示在时间逻辑推理中。OGAN可以找到系统的安全性不足的输入，揭示设计、软件或硬件的缺陷，从而避免在系统被运行前就发现这些缺陷。OGAN算法通过训练生成机器学习模型来生成这些Counterexample。它在原子性执行测试，不需要任何先前系统模型，并且可以应用于新的系统，需要 minimal effort，系统测试的效率和效果都达到了领先水平。Note: "cyber-physical systems" in the original text is translated as "cyber-physical系统" in Simplified Chinese, which is a common way to refer to such systems in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Log-based-Anomaly-Detection-of-Enterprise-Software-An-Empirical-Study"><a href="#Log-based-Anomaly-Detection-of-Enterprise-Software-An-Empirical-Study" class="headerlink" title="Log-based Anomaly Detection of Enterprise Software: An Empirical Study"></a>Log-based Anomaly Detection of Enterprise Software: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20492">http://arxiv.org/abs/2310.20492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadun Wijesinghe, Hadi Hemmati</li>
<li>for: 本研究旨在评估现有state-of-the-art anomaly detection模型在一个企业 dataset上的性能，以及对不同类型异常的检测。</li>
<li>methods: 本研究使用了多种sequence-based deep neural network模型，包括Long-Short Term Memory和Transformer-based模型，进行异常检测。</li>
<li>results: 结果表明，不同模型在不同类型异常检测中表现不同，特别是在 less-structured datasets 中。此外， removing a common data leak associated with a random train-test split in some prior work 可以提高模型的效果。<details>
<summary>Abstract</summary>
Most enterprise applications use logging as a mechanism to diagnose anomalies, which could help with reducing system downtime. Anomaly detection using software execution logs has been explored in several prior studies, using both classical and deep neural network-based machine learning models. In recent years, the research has largely focused in using variations of sequence-based deep neural networks (e.g., Long-Short Term Memory and Transformer-based models) for log-based anomaly detection on open-source data. However, they have not been applied in industrial datasets, as often. In addition, the studied open-source datasets are typically very large in size with logging statements that do not change much over time, which may not be the case with a dataset from an industrial service that is relatively new. In this paper, we evaluate several state-of-the-art anomaly detection models on an industrial dataset from our research partner, which is much smaller and loosely structured than most large scale open-source benchmark datasets. Results show that while all models are capable of detecting anomalies, certain models are better suited for less-structured datasets. We also see that model effectiveness changes when a common data leak associated with a random train-test split in some prior work is removed. A qualitative study of the defects' characteristics identified by the developers on the industrial dataset further shows strengths and weaknesses of the models in detecting different types of anomalies. Finally, we explore the effect of limited training data by gradually increasing the training set size, to evaluate if the model effectiveness does depend on the training set size.
</details>
<details>
<summary>摘要</summary>
大多数企业应用程序使用日志作为诊断异常的机制，以减少系统停机时间。在先前的研究中，使用了类传统和深度神经网络机器学习模型进行异常检测，使用日志执行记录。最近几年，研究主要集中在使用日志中的序列基本深度神经网络模型（如Long-Short Term Memory和Transformer-based模型）进行异常检测。但是，它们尚未在工业数据集上应用。此外，研究中的开源数据集通常很大，日志语句不会随时间变化，这可能不是工业数据集的情况。在这篇论文中，我们评估了一些当前最佳的异常检测模型在工业数据集中的性能。结果显示，虽然所有模型都能检测异常，但certain models更适合 Less-structured 数据集。我们还发现，模型的效果会随着一些常见的数据泄露（在一些先前的工作中的随机train-test split）的去除而变化。另外，对工业数据集中开发者所标识的异常的特点进行质量研究，显示了不同类型的异常的模型的优缺点。最后，我们查探了模型效果是否受训练数据集大小的影响，通过逐渐增加训练集大小来评估模型的效果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Practitioner-Perspectives-On-Training-Data-Attribution-Explanations"><a href="#Exploring-Practitioner-Perspectives-On-Training-Data-Attribution-Explanations" class="headerlink" title="Exploring Practitioner Perspectives On Training Data Attribution Explanations"></a>Exploring Practitioner Perspectives On Training Data Attribution Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20477">http://arxiv.org/abs/2310.20477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elisa Nguyen, Evgenii Kortukov, Jean Song, Seong Joon Oh</li>
<li>for: 本研究旨在探讨训练数据贡献（TDA）解释的可用性和设计空间。</li>
<li>methods: 本研究通过对10名实践者进行采访，了解TDA解释的可能性并探讨实践中的应用场景。</li>
<li>results: 研究发现，训练数据质量是模型性能高的关键因素，模型开发者主要依靠自己的经验来筛选数据。用户希望解释能够增强与模型的交互，而不一定优先考虑训练数据作为解释方法。 participant 中发现，TDA解释并不很熟悉，因此不常使用。 研究呼吁社区关注TDA技术的人机合作视角下的实用性，扩展TDA评估以涵盖实践中常见的应用场景。<details>
<summary>Abstract</summary>
Explainable AI (XAI) aims to provide insight into opaque model reasoning to humans and as such is an interdisciplinary field by nature. In this paper, we interviewed 10 practitioners to understand the possible usability of training data attribution (TDA) explanations and to explore the design space of such an approach. We confirmed that training data quality is often the most important factor for high model performance in practice and model developers mainly rely on their own experience to curate data. End-users expect explanations to enhance their interaction with the model and do not necessarily prioritise but are open to training data as a means of explanation. Within our participants, we found that TDA explanations are not well-known and therefore not used. We urge the community to focus on the utility of TDA techniques from the human-machine collaboration perspective and broaden the TDA evaluation to reflect common use cases in practice.
</details>
<details>
<summary>摘要</summary>
simplify Chinese:智能化解释（XAI）旨在为人类提供模型决策过程的洞察，因此是一个跨学科领域的研究。在这篇论文中，我们采访了10名实践者，以了解可能的训练数据归因（TDA）解释的可用性，并探索这种方法的设计空间。我们发现，实际中模型性能高的主要因素是训练数据质量，而模型开发者主要依靠自己的经验来册选数据。用户希望通过模型与人类的交互得到解释，并不一定优先考虑训练数据作为解释的方式。在我们的参与者中，我们发现TDA解释并不是广泛知道的，因此没有使用。我们呼吁社区在人机合作 perspective中关注TDA技术的实用性，并将TDA评价扩展到反映实际应用场景。
</details></li>
</ul>
<hr>
<h2 id="Amoeba-Circumventing-ML-supported-Network-Censorship-via-Adversarial-Reinforcement-Learning"><a href="#Amoeba-Circumventing-ML-supported-Network-Censorship-via-Adversarial-Reinforcement-Learning" class="headerlink" title="Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning"></a>Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20469">http://arxiv.org/abs/2310.20469</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobile-intelligence-lab/amoeba">https://github.com/mobile-intelligence-lab/amoeba</a></li>
<li>paper_authors: Haoyu Liu, Alec F. Diallo, Paul Patras</li>
<li>for:  circumventing Internet censorship</li>
<li>methods:  using a novel reinforcement learning algorithm called Amoeba to generate adversarial flows that can deceive ML-based classifiers</li>
<li>results:  achieved an average attack success rate of 94% against a range of ML algorithms, and the adversarial flows are robust in different network environments and possess transferability across various ML models.Here’s the text in Simplified Chinese:</li>
<li>for: 绕过互联网审查</li>
<li>methods: 使用新型的强化学习算法”Amoeba”生成隐蔽的流量，以诱导分类器进行错误分类</li>
<li>results: 实现了对多种机器学习算法的94%攻击成功率，并且这些隐蔽的流量在不同的网络环境下具有可重复性和可转移性。<details>
<summary>Abstract</summary>
Embedding covert streams into a cover channel is a common approach to circumventing Internet censorship, due to censors' inability to examine encrypted information in otherwise permitted protocols (Skype, HTTPS, etc.). However, recent advances in machine learning (ML) enable detecting a range of anti-censorship systems by learning distinct statistical patterns hidden in traffic flows. Therefore, designing obfuscation solutions able to generate traffic that is statistically similar to innocuous network activity, in order to deceive ML-based classifiers at line speed, is difficult.   In this paper, we formulate a practical adversarial attack strategy against flow classifiers as a method for circumventing censorship. Specifically, we cast the problem of finding adversarial flows that will be misclassified as a sequence generation task, which we solve with Amoeba, a novel reinforcement learning algorithm that we design. Amoeba works by interacting with censoring classifiers without any knowledge of their model structure, but by crafting packets and observing the classifiers' decisions, in order to guide the sequence generation process. Our experiments using data collected from two popular anti-censorship systems demonstrate that Amoeba can effectively shape adversarial flows that have on average 94% attack success rate against a range of ML algorithms. In addition, we show that these adversarial flows are robust in different network environments and possess transferability across various ML models, meaning that once trained against one, our agent can subvert other censoring classifiers without retraining.
</details>
<details>
<summary>摘要</summary>
使用嵌入掩饰流入掩饰频道是常见的绕过互联网审查的方法，因为审查器无法检查加密的信息在允许的协议（Skype、HTTPS等）中。然而，最新的机器学习（ML）技术可以检测许多防火墙系统，因此设计生成假数据流的方法可以欺骗ML基于类型的分类器。在这篇论文中，我们提出了一种实用的敌意攻击策略，以逃脱审查。我们将找到攻击流的问题转化为序列生成任务，并使用我们设计的一种新的强化学习算法——Amoeba来解决。Amoeba通过与审查类ifiers进行互动，不需要知道审查器的模型结构，但是通过编辑包并观察审查器的决策，来引导序列生成过程。我们的实验结果表明，Amoeba可以有效地生成攻击流，其中94%的攻击成功率可以在多种ML算法面前具有抗性。此外，我们还证明这些攻击流在不同的网络环境中具有可重复性，可以在不同的ML模型上进行转移。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-detects-terminal-singularities"><a href="#Machine-learning-detects-terminal-singularities" class="headerlink" title="Machine learning detects terminal singularities"></a>Machine learning detects terminal singularities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20458">http://arxiv.org/abs/2310.20458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/fanosearch/ml_terminality">https://bitbucket.org/fanosearch/ml_terminality</a></li>
<li>paper_authors: Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</li>
<li>for: 这个论文主要研究的是量子费托Varieties的分类。</li>
<li>methods: 作者使用机器学习方法来解决这个问题，开发了一个神经网络分类器，可以精准地判断8维具有托ри群Symmetry和Picard rank 2的代数多折形是否为量子费托多折形。</li>
<li>results: 作者通过使用机器学习方法，提出了一个初步的量子费托多折形的分类 landscape，并发现这些分类结果在量子期的视图中呈现为一个受限的区域，并且与费托指数相关。这些结果提供了新的证据，证明了机器学习可以成为数学推理的重要工具。<details>
<summary>Abstract</summary>
Algebraic varieties are the geometric shapes defined by systems of polynomial equations; they are ubiquitous across mathematics and science. Amongst these algebraic varieties are Q-Fano varieties: positively curved shapes which have Q-factorial terminal singularities. Q-Fano varieties are of fundamental importance in geometry as they are "atomic pieces" of more complex shapes - the process of breaking a shape into simpler pieces in this sense is called the Minimal Model Programme. Despite their importance, the classification of Q-Fano varieties remains unknown. In this paper we demonstrate that machine learning can be used to understand this classification. We focus on 8-dimensional positively-curved algebraic varieties that have toric symmetry and Picard rank 2, and develop a neural network classifier that predicts with 95% accuracy whether or not such an algebraic variety is Q-Fano. We use this to give a first sketch of the landscape of Q-Fanos in dimension 8. How the neural network is able to detect Q-Fano varieties with such accuracy remains mysterious, and hints at some deep mathematical theory waiting to be uncovered. Furthermore, when visualised using the quantum period, an invariant that has played an important role in recent theoretical developments, we observe that the classification as revealed by ML appears to fall within a bounded region, and is stratified by the Fano index. This suggests that it may be possible to state and prove conjectures on completeness in the future. Inspired by the ML analysis, we formulate and prove a new global combinatorial criterion for a positively curved toric variety of Picard rank 2 to have terminal singularities. Together with the first sketch of the landscape of Q-Fanos in higher dimensions, this gives new evidence that machine learning can be an essential tool in developing mathematical conjectures and accelerating theoretical discovery.
</details>
<details>
<summary>摘要</summary>
阿尔 геометрические形状是由系数方程定义的，它们在数学和科学中 ubique。其中包括Q-Fano形状，即正弦曲线的形状，它们具有Q- факторial终点特性。Q-Fano形状在几何学中具有基本重要性，因为它们是更复杂形状的“原子部件”，通过将形状拆分成更简单的部件来刻画形状的过程称为“最小模型Programme”。尽管其重要性，Q-Fano形状的分类仍然未知。在这篇论文中，我们使用机器学习来理解这个分类。我们关注8维正弦曲线的 positively-curved algebraic varieties，具有toric Symmetry和Picard rank 2，并开发了一个神经网络分类器，可以预测95%的概率是Q-Fano形状。我们使用这个分类器来给8维Q-Fano形状的首个绘图，并观察到Machine Learning可以准确地检测Q-Fano形状。这些结果表明，机器学习可以成为数学推理的重要工具。此外，我们还提出了一个全新的全球 combinatorial criterion，可以确定正弦曲线的 Picard rank 2 是否有终点特性。这与Q-Fano形状在更高维度的首个绘图和新的数学假设的证明都给出了新的证据。
</details></li>
</ul>
<hr>
<h2 id="FlexTrain-A-Dynamic-Training-Framework-for-Heterogeneous-Devices-Environments"><a href="#FlexTrain-A-Dynamic-Training-Framework-for-Heterogeneous-Devices-Environments" class="headerlink" title="FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments"></a>FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20457">http://arxiv.org/abs/2310.20457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Unsal, Ali Maatouk, Antonio De Domenico, Nicola Piovesan, Fadhel Ayed</li>
<li>for: 这篇论文是为了解决深度学习模型在不同设备环境中的问题，特别是对于低功率或资源有限的设备而言。</li>
<li>methods: 本论文提出了一个名为FlexTrain的框架，可以在训练阶段对不同设备的存储和计算资源进行整合，以便高效地部署深度学习模型。</li>
<li>results: 根据试验结果，FlexTrain可以实现对CIFAR-100 dataset的训练，并且可以在不同设备上运行，实现了训练时间和能源消耗的减少。此外，本论文还将FlexTrain扩展到联合学习设定下，与标准联合学习基准相比，FlexTrain在CIFAR-10和CIFAR-100 datasets上表现更好。<details>
<summary>Abstract</summary>
As deep learning models become increasingly large, they pose significant challenges in heterogeneous devices environments. The size of deep learning models makes it difficult to deploy them on low-power or resource-constrained devices, leading to long inference times and high energy consumption. To address these challenges, we propose FlexTrain, a framework that accommodates the diverse storage and computational resources available on different devices during the training phase. FlexTrain enables efficient deployment of deep learning models, while respecting device constraints, minimizing communication costs, and ensuring seamless integration with diverse devices. We demonstrate the effectiveness of FlexTrain on the CIFAR-100 dataset, where a single global model trained with FlexTrain can be easily deployed on heterogeneous devices, saving training time and energy consumption. We also extend FlexTrain to the federated learning setting, showing that our approach outperforms standard federated learning benchmarks on both CIFAR-10 and CIFAR-100 datasets.
</details>
<details>
<summary>摘要</summary>
As deep learning models become increasingly large, they pose significant challenges in heterogeneous devices environments. The size of deep learning models makes it difficult to deploy them on low-power or resource-constrained devices, leading to long inference times and high energy consumption. To address these challenges, we propose FlexTrain, a framework that accommodates the diverse storage and computational resources available on different devices during the training phase. FlexTrain enables efficient deployment of deep learning models, while respecting device constraints, minimizing communication costs, and ensuring seamless integration with diverse devices. We demonstrate the effectiveness of FlexTrain on the CIFAR-100 dataset, where a single global model trained with FlexTrain can be easily deployed on heterogeneous devices, saving training time and energy consumption. We also extend FlexTrain to the federated learning setting, showing that our approach outperforms standard federated learning benchmarks on both CIFAR-10 and CIFAR-100 datasets.Here's the translation in Traditional Chinese:当深度学习模型越来越大时，它们在多种设备环境中带来很大的挑战。模型的大小使得在低功率或资源受限的设备上部署深度学习模型 becomes increasingly difficult, leading to long inference times and high energy consumption. 为了解决这些挑战，我们提出了 FlexTrain 框架，该框架在训练阶段适应不同设备上的储存和计算资源。FlexTrain 可以有效地部署深度学习模型，同时尊重设备的限制，降低通信成本，并让不同设备进行顺当的集成。我们在 CIFAR-100  dataset 上验证了 FlexTrain 的有效性，发现可以使用 FlexTrain 将单一全球模型训练到多种设备，实现训练时间和能量消耗的优化。我们还将 FlexTrain 扩展到 federated learning  Setting，证明我们的方法在 CIFAR-10 和 CIFAR-100  dataset 上比标准 federated learning  benchmark 高效。
</details></li>
</ul>
<hr>
<h2 id="The-Phase-Transition-Phenomenon-of-Shuffled-Regression"><a href="#The-Phase-Transition-Phenomenon-of-Shuffled-Regression" class="headerlink" title="The Phase Transition Phenomenon of Shuffled Regression"></a>The Phase Transition Phenomenon of Shuffled Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20438">http://arxiv.org/abs/2310.20438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Zhang, Ping Li</li>
<li>for: 这 paper 主要研究了排序（permuted）回归问题中的相对稳定点现象，这种问题在数据库、隐私、数据分析等领域都有广泛应用。</li>
<li>methods: 本 paper 使用了消息传递（MP）技术来精确地定义相对稳定点的位置。首先， authors 将排序回归问题转换成了一个概率图模型，然后使用 MP 算法的分析工具来 derive 排序回归的渐进方程。通过将这个方程连接到分支渐进 random walk 过程，authors 可以Characterize 相对稳定点对 $\snr$ 的影响。</li>
<li>results: 在 oracle 和 non-oracle 两种情况下，authors 分别研究了相对稳定点的影响。在 oracle 情况下， authors 可以准确预测相对稳定点 $\snr$。在 non-oracle 情况下，authors 可以预测最多允许的排序行数和其与样本数的关系。<details>
<summary>Abstract</summary>
We study the phase transition phenomenon inherent in the shuffled (permuted) regression problem, which has found numerous applications in databases, privacy, data analysis, etc. In this study, we aim to precisely identify the locations of the phase transition points by leveraging techniques from message passing (MP). In our analysis, we first transform the permutation recovery problem into a probabilistic graphical model. We then leverage the analytical tools rooted in the message passing (MP) algorithm and derive an equation to track the convergence of the MP algorithm. By linking this equation to the branching random walk process, we are able to characterize the impact of the signal-to-noise-ratio ($\snr$) on the permutation recovery. Depending on whether the signal is given or not, we separately investigate the oracle case and the non-oracle case. The bottleneck in identifying the phase transition regimes lies in deriving closed-form formulas for the corresponding critical points, but only in rare scenarios can one obtain such precise expressions. To tackle this technical challenge, this study proposes the Gaussian approximation method, which allows us to obtain the closed-form formulas in almost all scenarios. In the oracle case, our method can fairly accurately predict the phase transition $\snr$. In the non-oracle case, our algorithm can predict the maximum allowed number of permuted rows and uncover its dependency on the sample number.
</details>
<details>
<summary>摘要</summary>
我们研究排序问题中的相变现象，这问题在数据库、隐私、数据分析等领域都有广泛应用。在这些研究中，我们想要精确地找出排序问题中的相变点，并且使用讯息传递（MP）技术来实现。我们首先将排序问题转换为一个probabilistic graphical model，然后使用MP算法的分析工具来 derive一个追踪MP算法的方程。通过与分支随机步进程连接这个方程，我们可以描述排序问题中的信号至杂音比例($\snr$)的影响。对于signal是否知道的情况，我们分别进行了oracle case和非oracle case的研究。排序问题中的瓶颈在于 derivation closed-formula for the corresponding critical points，但只有在 rare scenarios 可以取得如此精确的表达。为了解决这个技术挑战，本研究提出了Gaussian approximation方法，可以将closed-formula 在大多数情况下取得。在oracle case中，我们的方法可以很准确地预测相变 $\snr$。在非oracle case中，我们的算法可以预测排序中最多允许的排序行数和其对数据数量的依赖。
</details></li>
</ul>
<hr>
<h2 id="Discussing-the-Spectra-of-Physics-Enhanced-Machine-Learning-via-a-Survey-on-Structural-Mechanics-Applications"><a href="#Discussing-the-Spectra-of-Physics-Enhanced-Machine-Learning-via-a-Survey-on-Structural-Mechanics-Applications" class="headerlink" title="Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications"></a>Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20425">http://arxiv.org/abs/2310.20425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus Haywood-Alexander, Wei Liu, Kiran Bacsa, Zhilu Lai, Eleni Chatzi</li>
<li>for: 本文旨在探讨物理学和机器学习之间的交叠，提出了physics-enhanced machine learning（PEML）方法，以提高机器学习方法的能力和降低数据或物理方法缺点。</li>
<li>methods: 本文通过对物理和数据两个定义轴的PEML方法进行全面探讨，描述了这些方法的特点、使用场景和动机。 更进一步，本文还介绍了一些最近应用和开发的PEML技术，并通过使用一个简单的单度oscillator示例，展示了不同类型PEML方法的特点和动机。</li>
<li>results: 本文的应用示例和开发技术，演示了PEML在复杂挑战中的力量。此外，为促进合作和透明度，本文附加了代码，以便读者可以实践。作为基础贡献，本文强调了PEML在科学和工程研究中的重要性，由物理理解和机器学习能力的合作下支持。<details>
<summary>Abstract</summary>
The intersection of physics and machine learning has given rise to a paradigm that we refer to here as physics-enhanced machine learning (PEML), aiming to improve the capabilities and reduce the individual shortcomings of data- or physics-only methods. In this paper, the spectrum of physics-enhanced machine learning methods, expressed across the defining axes of physics and data, is discussed by engaging in a comprehensive exploration of its characteristics, usage, and motivations. In doing so, this paper offers a survey of recent applications and developments of PEML techniques, revealing the potency of PEML in addressing complex challenges. We further demonstrate application of select such schemes on the simple working example of a single-degree-of-freedom Duffing oscillator, which allows to highlight the individual characteristics and motivations of different `genres' of PEML approaches. To promote collaboration and transparency, and to provide practical examples for the reader, the code of these working examples is provided alongside this paper. As a foundational contribution, this paper underscores the significance of PEML in pushing the boundaries of scientific and engineering research, underpinned by the synergy of physical insights and machine learning capabilities.
</details>
<details>
<summary>摘要</summary>
Physics-enhanced machine learning (PEML) 是一种新的思想框架，它将物理学和机器学习两个领域融合在一起，以提高机器学习的能力和降低数据或物理 только方法的缺陷。本文将Physics-enhanced machine learning方法的谱系讲解，从物理和数据两个定义轴上表述其特点、使用和动机。通过这种方式，本文探讨了Recent Applications and Developments of PEML techniques，揭示了PEML在解决复杂问题的能力。此外，我们还使用了一个简单的工作示例——单度自由振荡器，以阐述不同类型的 PEML 方法的特点和动机。为促进合作和透明度，以及为读者提供实践性的示例，我们附加了这些工作示例的代码。作为基础性贡献，本文强调了 PEML 在科学和工程研究中的重要性，它由物理学的理解和机器学习能力的融合所支持。
</details></li>
</ul>
<hr>
<h2 id="DDC-PIM-Efficient-Algorithm-Architecture-Co-design-for-Doubling-Data-Capacity-of-SRAM-based-Processing-In-Memory"><a href="#DDC-PIM-Efficient-Algorithm-Architecture-Co-design-for-Doubling-Data-Capacity-of-SRAM-based-Processing-In-Memory" class="headerlink" title="DDC-PIM: Efficient Algorithm&#x2F;Architecture Co-design for Doubling Data Capacity of SRAM-based Processing-In-Memory"></a>DDC-PIM: Efficient Algorithm&#x2F;Architecture Co-design for Doubling Data Capacity of SRAM-based Processing-In-Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20424">http://arxiv.org/abs/2310.20424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cenlin Duan, Jianlei Yang, Xiaolin He, Yingjie Qi, Yikun Wang, Yiou Wang, Ziyan He, Bonan Yan, Xueyan Wang, Xiaotao Jia, Weitao Pan, Weisheng Zhao</li>
<li>for: 该研究旨在提高处理在内存（PIM）中的性能，特别是减少数据移动的效率。</li>
<li>methods: 该研究提出了一种名为DDC-PIM的效果 doubles the equivalent data capacity的算法&#x2F;架构合作方法，包括对算法水平进行filter-wise complementary correlation（FCC）算法，以及对架构水平进行6T SRAM的利用。</li>
<li>results: 评估结果表明，DDC-PIM在MobileNetV2和EfficientNet-B0上比基eline实现带有$2.84\times$的速度提升，而且与基eline实现的准确率差异不显著。相比之下，与现有的SRAM-based PIM macros进行比较，DDC-PIM在Weight Density和Area Efficiency两个方面具有$8.41\times$和$2.75\times$的提升。<details>
<summary>Abstract</summary>
Processing-in-memory (PIM), as a novel computing paradigm, provides significant performance benefits from the aspect of effective data movement reduction. SRAM-based PIM has been demonstrated as one of the most promising candidates due to its endurance and compatibility. However, the integration density of SRAM-based PIM is much lower than other non-volatile memory-based ones, due to its inherent 6T structure for storing a single bit. Within comparable area constraints, SRAM-based PIM exhibits notably lower capacity. Thus, aiming to unleash its capacity potential, we propose DDC-PIM, an efficient algorithm/architecture co-design methodology that effectively doubles the equivalent data capacity. At the algorithmic level, we propose a filter-wise complementary correlation (FCC) algorithm to obtain a bitwise complementary pair. At the architecture level, we exploit the intrinsic cross-coupled structure of 6T SRAM to store the bitwise complementary pair in their complementary states ($Q/\overline{Q}$), thereby maximizing the data capacity of each SRAM cell. The dual-broadcast input structure and reconfigurable unit support both depthwise and pointwise convolution, adhering to the requirements of various neural networks. Evaluation results show that DDC-PIM yields about $2.84\times$ speedup on MobileNetV2 and $2.69\times$ on EfficientNet-B0 with negligible accuracy loss compared with PIM baseline implementation. Compared with state-of-the-art SRAM-based PIM macros, DDC-PIM achieves up to $8.41\times$ and $2.75\times$ improvement in weight density and area efficiency, respectively.
</details>
<details>
<summary>摘要</summary>
“processing-in-memory”（PIM）作为一种新的计算模式，实现了实际数据运输量的显著性能提升。SRAM基本PIM因为其持续性和可容器性而被视为最有前途的候选者。然而，SRAM基本PIM的集成密度与其他非朋合内存基本PIM的密度相比较低，这是因为它的自然6T结构储存单一比特。在相似的面积限制下，SRAM基本PIM表现出较低的容量。为了解释这个容量的潜力，我们提出了DDC-PIM，一种有效的架构/算法合理设计方法，可以实际地两倍提高等效数据容量。在算法层面，我们提出了一个范例共联相关（FCC）算法，以获取单位比特的对应配对。在架构层面，我们利用6T SRAM的内在交叉连接结构，将单位比特的对应配对储存在它们的补充状态($Q/\overline{Q}$)中，以最大化每个SRAM细胞的数据容量。双向广播输入结构和可重新配置单元支持深度对称和点对称卷积，遵循不同神经网络的需求。评估结果显示，DDC-PIM对于MobileNetV2和EfficientNet-B0的实现中，具有约2.84倍的速度提升，与PIM基eline实现相比，仅受到微scopic的精度损失。相比之下，DDC-PIM与现有的SRAM基本PIMmacro之间，实现了约8.41倍和2.75倍的重量密度和面积效率提升。”
</details></li>
</ul>
<hr>
<h2 id="Coalitional-Manipulations-and-Immunity-of-the-Shapley-Value"><a href="#Coalitional-Manipulations-and-Immunity-of-the-Shapley-Value" class="headerlink" title="Coalitional Manipulations and Immunity of the Shapley Value"></a>Coalitional Manipulations and Immunity of the Shapley Value</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20415">http://arxiv.org/abs/2310.20415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Basteck, Frank Huettner</li>
<li>for: 本文研究了 coalitional game 中的操作，特别是如何确定一个具有最大收益的盟约。</li>
<li>methods: 本文使用了 Shapley 值作为基础，并通过研究 reallocation-proofness 和 weak coalitional monotonicity 两种需求来替代原来的 additivity 需求。</li>
<li>results: 本文显示了一种新的 Shapley 值基础，即只允许 null  игроки获得零收益，并且具有 coalitional manipulation 鲁棒性。此外，我们还发现了一种更弱的 marginality axioms，即 constrained marginality，可以替代 Young 的 marginality axioms。<details>
<summary>Abstract</summary>
We consider manipulations in the context of coalitional games, where a coalition aims to increase the total payoff of its members. An allocation rule is immune to coalitional manipulation if no coalition can benefit from internal reallocation of worth on the level of its subcoalitions (reallocation-proofness), and if no coalition benefits from a lower worth while all else remains the same (weak coalitional monotonicity). Replacing additivity in Shapley's original characterization by these requirements yields a new foundation of the Shapley value, i.e., it is the unique efficient and symmetric allocation rule that awards nothing to a null player and is immune to coalitional manipulations. We further find that for efficient allocation rules, reallocation-proofness is equivalent to constrained marginality, a weaker variant of Young's marginality axiom. Our second characterization improves upon Young's characterization by weakening the independence requirement intrinsic to marginality.
</details>
<details>
<summary>摘要</summary>
我们在合作游戏中考虑操作，合作团体想增加成员的总回扣。一个分配规则是免于合作操作的折衣（reallocation-proofness），如果无法内部实现合作团体的重新分配值（reallocation），并且如果无法在所有 else 保持不变的情况下，则获得较低的值（weak coalitional monotonicity）。将添加性在雪普利原始的特征中替换为这些要求，则得到一个新的基础，即雪普利值是唯一的有效率和对称分配规则，没有空闲玩家获得任何回扣，并且免于合作操作。我们还发现，对于有效的分配规则，折衣证明是对于Young的一致性axioma的弱版本。我们的第二个特征提高了Young的特征，对于独立性的要求。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-approach-for-solving-the-gravitational-N-body-problem-with-Artificial-Neural-Networks"><a href="#A-hybrid-approach-for-solving-the-gravitational-N-body-problem-with-Artificial-Neural-Networks" class="headerlink" title="A hybrid approach for solving the gravitational N-body problem with Artificial Neural Networks"></a>A hybrid approach for solving the gravitational N-body problem with Artificial Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20398">http://arxiv.org/abs/2310.20398</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/veronicasaz/planetarysystem_hnn">https://github.com/veronicasaz/planetarysystem_hnn</a></li>
<li>paper_authors: Veronica Saz Ulibarrena, Philipp Horn, Simon Portegies Zwart, Elena Sellentin, Barry Koren, Maxwell X. Cai<br>for: 这个论文是用来研究用人工神经网络（ANNs）来缓解数值天体运动 simulation中的计算成本的高速化的。methods: 这个论文使用了汉密尔顿神经网络（HNNs）和深度神经网络（DNNs）来取代计算成本较高的部分。results: 作者发现使用汉密尔顿神经网络可以快速 simulation of planetary systems with a large number of asteroids (&gt;70), but the training process is challenging and the network may fail when the input parameters differ by about 7 orders of magnitude. In contrast, deep neural networks are easy to train but do not conserve energy and lead to fast divergence from the reference solution. The hybrid integrator that combines the neural networks with numerical solutions can increase the reliability of the method without significantly increasing the computing cost.<details>
<summary>Abstract</summary>
Simulating the evolution of the gravitational N-body problem becomes extremely computationally expensive as N increases since the problem complexity scales quadratically with the number of bodies. We study the use of Artificial Neural Networks (ANNs) to replace expensive parts of the integration of planetary systems. Neural networks that include physical knowledge have grown in popularity in the last few years, although few attempts have been made to use them to speed up the simulation of the motion of celestial bodies. We study the advantages and limitations of using Hamiltonian Neural Networks to replace computationally expensive parts of the numerical simulation. We compare the results of the numerical integration of a planetary system with asteroids with those obtained by a Hamiltonian Neural Network and a conventional Deep Neural Network, with special attention to understanding the challenges of this problem. Due to the non-linear nature of the gravitational equations of motion, errors in the integration propagate. To increase the robustness of a method that uses neural networks, we propose a hybrid integrator that evaluates the prediction of the network and replaces it with the numerical solution if considered inaccurate. Hamiltonian Neural Networks can make predictions that resemble the behavior of symplectic integrators but are challenging to train and in our case fail when the inputs differ ~7 orders of magnitude. In contrast, Deep Neural Networks are easy to train but fail to conserve energy, leading to fast divergence from the reference solution. The hybrid integrator designed to include the neural networks increases the reliability of the method and prevents large energy errors without increasing the computing cost significantly. For this problem, the use of neural networks results in faster simulations when the number of asteroids is >70.
</details>
<details>
<summary>摘要</summary>
simulate  gravitational N-body 问题的演化成为 N 增加时速度增加，因为问题复杂性 quadratic 与体数相关。我们研究使用人工神经网络（ANNs）来取代数值integration of planetary systems的费时部分。包含物理知识的神经网络在 послед few years 中 popularity 增加， although few attempts have been made to use them to speed up the simulation of celestial bodies' motion. We study the advantages and limitations of using Hamiltonian Neural Networks to replace computationally expensive parts of the numerical simulation. We compare the results of the numerical integration of a planetary system with asteroids with those obtained by a Hamiltonian Neural Network and a conventional Deep Neural Network, with special attention to understanding the challenges of this problem. Due to the non-linear nature of the gravitational equations of motion, errors in the integration propagate. To increase the robustness of a method that uses neural networks, we propose a hybrid integrator that evaluates the prediction of the network and replaces it with the numerical solution if considered inaccurate. Hamiltonian Neural Networks can make predictions that resemble the behavior of symplectic integrators but are challenging to train and in our case fail when the inputs differ by ~7 orders of magnitude. In contrast, Deep Neural Networks are easy to train but fail to conserve energy, leading to fast divergence from the reference solution. The hybrid integrator designed to include the neural networks increases the reliability of the method and prevents large energy errors without increasing the computing cost significantly. For this problem, the use of neural networks results in faster simulations when the number of asteroids is >70.
</details></li>
</ul>
<hr>
<h2 id="Dropout-Strategy-in-Reinforcement-Learning-Limiting-the-Surrogate-Objective-Variance-in-Policy-Optimization-Methods"><a href="#Dropout-Strategy-in-Reinforcement-Learning-Limiting-the-Surrogate-Objective-Variance-in-Policy-Optimization-Methods" class="headerlink" title="Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods"></a>Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20380">http://arxiv.org/abs/2310.20380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengpeng Xie, Changdong Yu, Weizheng Qiao</li>
<li>for: 提高Policy Optimization Algorithm的稳定性和收敛速度</li>
<li>methods: 使用Dropout技术避免重要样本Objective Variance的过度增长</li>
<li>results: 在Atari 2600环境中比较PPO和D-PPO两种算法的性能表现，D-PPO表现出了明显的性能提升，并有效地限制了重要样本Objective Variance的增长 during training。<details>
<summary>Abstract</summary>
Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as TRPO and PPO introduce importance sampling into policy iteration, which allows the reuse of historical data. However, this can also lead to high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the surrogate objective variance, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 environment, results show that D-PPO achieved significant performance improvements compared to PPO, and effectively limited the excessive increase of the surrogate objective variance during training.
</details>
<details>
<summary>摘要</summary>
In this paper, we first derived an upper bound of the surrogate objective variance, which can grow quadratically with the increase of the surrogate objective. Then, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling.Next, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 environment, and the results show that D-PPO achieved significant performance improvements compared to PPO, and effectively limited the excessive increase of the surrogate objective variance during training.
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-the-Decentralized-Stochastic-Gradient-Descent-Ascent-Algorithm"><a href="#Stability-and-Generalization-of-the-Decentralized-Stochastic-Gradient-Descent-Ascent-Algorithm" class="headerlink" title="Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm"></a>Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20369">http://arxiv.org/abs/2310.20369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoxi Zhu, Li Shen, Bo Du, Dacheng Tao</li>
<li>for: 这篇论文主要关注的是分布式机器学习任务中的最小最大值问题的解决方法，以及该问题在分布式环境中的泛化性。</li>
<li>methods: 这篇论文使用了随机梯度下降升级（D-SGDA）算法，并使用了算法稳定性的方法来研究其泛化性。</li>
<li>results: 研究结果表明，D-SGDA算法在分布式环境中可以保持稳定性和泛化性，并且可以与中央化SGDA算法相比肤。此外，研究还发现了不同网络结构对D-SGDA算法的泛化级别的影响，并且在一定情况下可以通过调整学习率和迭代次数来优化D-SGDA算法的泛化性。<details>
<summary>Abstract</summary>
The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such as sample sizes, learning rates, and iterations. We also evaluate the optimization error and balance it with the generalization gap to obtain the optimal population risk of D-SGDA in the convex-concave setting. Additionally, we perform several numerical experiments which validate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
随着可用数据的增长，解决分布式的最小最大问题（minimax problem）在不同机器学习任务中吸引了越来越多的关注。先前的理论研究主要关注了分布式最小最大算法的收敛率和通信复杂度，忽略了其总体化。在这篇论文中，我们研究了分布式随机梯度下降（D-SGDA）算法的原理稳定性 bounds，使用了分布式算法稳定性的方法。我们的理论表明，分布式结构不会消除D-SGDA的稳定性和总体化，这意味着它在某些情况下可以与标准的SGDA相当。我们的研究还分析了不同结构对D-SGDA算法的总体化约束的影响，并评估了优化错误和总体化差异来获得D-SGDA算法的最佳人口风险。此外，我们还进行了多个数值实验， validate our theoretical findings.
</details></li>
</ul>
<hr>
<h2 id="Distil-the-informative-essence-of-loop-detector-data-set-Is-network-level-traffic-forecasting-hungry-for-more-data"><a href="#Distil-the-informative-essence-of-loop-detector-data-set-Is-network-level-traffic-forecasting-hungry-for-more-data" class="headerlink" title="Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?"></a>Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20366">http://arxiv.org/abs/2310.20366</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction">https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction</a></li>
<li>paper_authors: Guopeng Li, Victor L. Knoop, J. W. C., van Lint</li>
<li>for: 本研究主要是为了解决交通预测中的数据强度问题，以及是否可以通过更多的感知器数据提高预测精度。</li>
<li>methods: 本研究提出了一种不确定性意识涵养框架，结合交通流理论和图神经网络，以及使用证据学来量化不同来源的不确定性。</li>
<li>results: 实验结果显示，在荷兰附近的高速公路网络上，可以从2018年至2021年的日间时段中 removes 超过80%的数据，而剩下的20%的样本具有相同的预测力。这些结果表明，大型交通数据集可以被分解成更小但Equally informative的数据集。<details>
<summary>Abstract</summary>
Network-level traffic condition forecasting has been intensively studied for decades. Although prediction accuracy has been continuously improved with emerging deep learning models and ever-expanding traffic data, traffic forecasting still faces many challenges in practice. These challenges include the robustness of data-driven models, the inherent unpredictability of traffic dynamics, and whether further improvement of traffic forecasting requires more sensor data. In this paper, we focus on this latter question and particularly on data from loop detectors. To answer this, we propose an uncertainty-aware traffic forecasting framework to explore how many samples of loop data are truly effective for training forecasting models. Firstly, the model design combines traffic flow theory with graph neural networks, ensuring the robustness of prediction and uncertainty quantification. Secondly, evidential learning is employed to quantify different sources of uncertainty in a single pass. The estimated uncertainty is used to "distil" the essence of the dataset that sufficiently covers the information content. Results from a case study of a highway network around Amsterdam show that, from 2018 to 2021, more than 80\% of the data during daytime can be removed. The remaining 20\% samples have equal prediction power for training models. This result suggests that indeed large traffic datasets can be subdivided into significantly smaller but equally informative datasets. From these findings, we conclude that the proposed methodology proves valuable in evaluating large traffic datasets' true information content. Further extensions, such as extracting smaller, spatially non-redundant datasets, are possible with this method.
</details>
<details>
<summary>摘要</summary>
网络水平交通条件预测已经在 décadas 中进行了探索。虽然预测精度一直在深度学习模型 emergence 和交通数据的扩展中不断提高，但交通预测在实践中仍然面临许多挑战。这些挑战包括数据驱动模型的稳定性，交通动态的内在难于预测，以及是否进一步改进交通预测需要更多的感知器数据。在这篇论文中，我们关注这个问题，特别是来自循环感知器的数据。为了回答这个问题，我们提出了一个不确定性认识的交通预测框架，以探索 loop 数据多少样本是真正有效的训练预测模型。首先，模型设计结合交通流理论与图 neural network，以确保预测的稳定性和不确定性评估。其次，使用 evidential learning 来评估不同来源的不确定性，并将估计的不确定性用于 "浸泡" 数据集中的信息内容。 results 表明，2018 年至 2021 年日间交通网络在 Amsterdam 附近的 highway 上， más de 80% 的数据可以被删除，而剩下的 20% 样本具有相同的预测力。这种结果表明，大交通数据集可以被分解成更小但Equally informative 的数据集。从这些发现，我们结论认识方法的价值在评估大交通数据集的真实信息内容。可以进一步推广这种方法，例如抽取更小但空间不重复的数据集。
</details></li>
</ul>
<hr>
<h2 id="CAFE-Conflict-Aware-Feature-wise-Explanations"><a href="#CAFE-Conflict-Aware-Feature-wise-Explanations" class="headerlink" title="CAFE: Conflict-Aware Feature-wise Explanations"></a>CAFE: Conflict-Aware Feature-wise Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20363">http://arxiv.org/abs/2310.20363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Dejl, Hamed Ayoobi, Matthew Williams, Francesca Toni</li>
<li>for: 这个论文是为了解释神经网络模型的输出，特别是响应于哪些输入特征。</li>
<li>methods: 这个论文提出了一种新的特征归因方法，称为CAFE（冲突感知特征分解），它解决了现有方法的三大限制：忽视对抗特征的影响、缺乏对偏移项的考虑和地方化活动函数下的过敏感。CAFE方法提供了避免过分估计输入特征的影响的保障，并分别跟踪输入特征和偏移项的积极和消极影响，从而提高了 robustness 和表面特征冲突。</li>
<li>results: 实验表明，CAFE方法在 sintetic 表格数据上能够更好地Identify 冲突特征，并在多个实际表格数据上达到最高的总准确率，而且高效计算。<details>
<summary>Abstract</summary>
Feature attribution methods are widely used to explain neural models by determining the influence of individual input features on the models' outputs. We propose a novel feature attribution method, CAFE (Conflict-Aware Feature-wise Explanations), that addresses three limitations of the existing methods: their disregard for the impact of conflicting features, their lack of consideration for the influence of bias terms, and an overly high sensitivity to local variations in the underpinning activation functions. Unlike other methods, CAFE provides safeguards against overestimating the effects of neuron inputs and separately traces positive and negative influences of input features and biases, resulting in enhanced robustness and increased ability to surface feature conflicts. We show experimentally that CAFE is better able to identify conflicting features on synthetic tabular data and exhibits the best overall fidelity on several real-world tabular datasets, while being highly computationally efficient.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Feature attribution methods are widely used to explain neural models by determining the influence of individual input features on the models' outputs. We propose a novel feature attribution method, CAFE (Conflict-Aware Feature-wise Explanations), that addresses three limitations of the existing methods: their disregard for the impact of conflicting features, their lack of consideration for the influence of bias terms, and an overly high sensitivity to local variations in the underpinning activation functions. Unlike other methods, CAFE provides safeguards against overestimating the effects of neuron inputs and separately traces positive and negative influences of input features and biases, resulting in enhanced robustness and increased ability to surface feature conflicts. We show experimentally that CAFE is better able to identify conflicting features on synthetic tabular data and exhibits the best overall fidelity on several real-world tabular datasets, while being highly computationally efficient."翻译结果：Feature 归因方法广泛用于解释神经网络模型的输出，以确定输入特征对模型的影响。我们提出了一种新的Feature归因方法，即Conflict-Aware Feature-wise Explanations（CAFE），该方法解决了现有方法的三个限制：它们忽略了冲突性特征的影响、缺乏对偏好项的考虑、以及对下游活动函数的过敏感性。与其他方法不同，CAFE提供了防止过度估计输入神经元的影响的安全措施，并分别跟踪输入特征和偏好项的正向和负向影响，从而提高了Robustness和surfacefeature冲突的能力。我们通过实验表明，CAFE在人工制造的表格数据上更好地标识冲突特征，并在多个实际的表格数据上显示出最好的总体准确性，同时高度计算效率。
</details></li>
</ul>
<hr>
<h2 id="Verification-of-Neural-Networks-Local-Differential-Classification-Privacy"><a href="#Verification-of-Neural-Networks-Local-Differential-Classification-Privacy" class="headerlink" title="Verification of Neural Networks Local Differential Classification Privacy"></a>Verification of Neural Networks Local Differential Classification Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20299">http://arxiv.org/abs/2310.20299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roie Reshef, Anan Kabaha, Olga Seleznova, Dana Drachsler-Cohen</li>
<li>for: 防止神经网络泄露个人隐私</li>
<li>methods: 使用Local Differential Classification Privacy（LDCP）和Kernel Density estimation（KDE）将网络参数转换为分布，并使用MILP验证器验证抽象网络的隐私性</li>
<li>results: 通过训练7%的网络，Sphynx可以预测一个抽象网络，并在93%的验证精度下验证LDCP，同时提高验证时间 $1.7\cdot10^4$倍。<details>
<summary>Abstract</summary>
Neural networks are susceptible to privacy attacks. To date, no verifier can reason about the privacy of individuals participating in the training set. We propose a new privacy property, called local differential classification privacy (LDCP), extending local robustness to a differential privacy setting suitable for black-box classifiers. Given a neighborhood of inputs, a classifier is LDCP if it classifies all inputs the same regardless of whether it is trained with the full dataset or whether any single entry is omitted. A naive algorithm is highly impractical because it involves training a very large number of networks and verifying local robustness of the given neighborhood separately for every network. We propose Sphynx, an algorithm that computes an abstraction of all networks, with a high probability, from a small set of networks, and verifies LDCP directly on the abstract network. The challenge is twofold: network parameters do not adhere to a known distribution probability, making it difficult to predict an abstraction, and predicting too large abstraction harms the verification. Our key idea is to transform the parameters into a distribution given by KDE, allowing to keep the over-approximation error small. To verify LDCP, we extend a MILP verifier to analyze an abstract network. Experimental results show that by training only 7% of the networks, Sphynx predicts an abstract network obtaining 93% verification accuracy and reducing the analysis time by $1.7\cdot10^4$x.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose Sphynx, an algorithm that computes an abstraction of all networks from a small set of networks and verifies LDCP directly on the abstract network. The key idea is to transform the parameters into a distribution using KDE, allowing for a small over-approximation error. To verify LDCP, we extend a MILP verifier to analyze the abstract network.Our experimental results show that by training only 7% of the networks, Sphynx can predict an abstract network with 93% verification accuracy and reduce the analysis time by $1.7\cdot10^4$x.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Generalized-Linear-Models-by-Trading-off-Computation-for-Uncertainty"><a href="#Accelerating-Generalized-Linear-Models-by-Trading-off-Computation-for-Uncertainty" class="headerlink" title="Accelerating Generalized Linear Models by Trading off Computation for Uncertainty"></a>Accelerating Generalized Linear Models by Trading off Computation for Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20285">http://arxiv.org/abs/2310.20285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Tatzel, Jonathan Wenger, Frank Schneider, Philipp Hennig</li>
<li>for: 这个论文的目的是为了提出一种能够高效地进行 Bayesian Generalized Linear Models（GLMs）的推理，以及能够考虑模型approximation error的iterative方法。</li>
<li>methods: 这种方法使用了一种家族的iterative方法，这些方法能够高效地利用现代并行计算硬件，并且可以很好地重用计算结果，从而降低GLMs的时间和内存需求。</li>
<li>results: 作者在一个实际上有些很大的分类问题上进行了实验，结果显示，使用这种方法可以快速加速GLMs的训练，并且可以明确地交换减少计算量和增加uncertainty。<details>
<summary>Abstract</summary>
Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training by explicitly trading off reduced computation for increased uncertainty.
</details>
<details>
<summary>摘要</summary>
bayesian generalized linear models (GLMs) 定义了一个 flexible 的概率框架，用于模型 categorical、ordinal 和连续变数，广泛应用在实践中。然而，对大量数据进行精确推断是不可能昂贵的，因此需要使用近似方法。这个近似Error会对模型的可靠性产生负面的影响，并且不会考虑到预测中的不确定性。在这个工作中，我们介绍了一家 Iterative 方法，可以明确地模型这个错误。这些方法具有平行于现代计算硬件的特点，可以高效地重复计算，将信息压缩以减少 GLMs 的时间和内存需求。我们在一个实际上是一个大型分类问题中显示了，我们的方法可以快速对 GLMs 进行训练，明确地交换了reduced computation 和增加的不确定性。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Bayesian-Optimization-via-Learning-Correlated-Latent-Space"><a href="#Advancing-Bayesian-Optimization-via-Learning-Correlated-Latent-Space" class="headerlink" title="Advancing Bayesian Optimization via Learning Correlated Latent Space"></a>Advancing Bayesian Optimization via Learning Correlated Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20258">http://arxiv.org/abs/2310.20258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghun Lee, Jaewon Chu, Sihyeon Kim, Juyeon Ko, Hyunwoo J. Kim</li>
<li>for: 优化黑盒函数（black-box functions），尤其是具有结构或数据维度的 discrete data 中的函数。</li>
<li>methods: 使用深度生成模型（deep generative models），如变量自动编码器（variational autoencoders），进行 Bayesian 优化，并通过学习相关的幂时空间（latent space）来减少优化不足的问题。</li>
<li>results: 在 discrete data 中的多个优化任务（如分子设计和数学表达适应）中，实现高效性和高性能，并在小费用（budget）下达到优秀的结果。<details>
<summary>Abstract</summary>
Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in discrete data, such as molecule design and arithmetic expression fitting, and achieve high performance within a small budget.
</details>
<details>
<summary>摘要</summary>
bayesian 优化是一种强大的方法，用于优化黑盒函数，限制 функion evaluations。 recent works 表明，通过在深度生成模型，如变量自动编码器，进行 latent space 优化，可以获得有效和高效的 bayesian 优化。 however，由于优化不在输入空间进行，这会导致一定的差距，可能导致不优化的解。 To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in discrete data, such as molecule design and arithmetic expression fitting, and achieve high performance within a small budget.
</details></li>
</ul>
<hr>
<h2 id="STDA-Meta-A-Meta-Learning-Framework-for-Few-Shot-Traffic-Prediction"><a href="#STDA-Meta-A-Meta-Learning-Framework-for-Few-Shot-Traffic-Prediction" class="headerlink" title="STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction"></a>STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20223">http://arxiv.org/abs/2310.20223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maoxiang Sun, Weilong Ding, Tianpu Zhang, Zijian Liu, Mengda Xing<br>for: 这 paper written for 解决城市发展中的交通堵塞问题，提出了一种基于几何结构的时空预测学习方法，该方法可以在数据稀缺的情况下提高交通预测性能。methods: 该 paper 使用了模型无关元学习（MAML）和 episodic learning 等方法，通过在数据充足城市的数据上学习时空预测模型，然后将学到的知识转移到数据稀缺城市中进行预测。results:  compared to baseline models, 该 paper 的预测性能提高了7%， measured by two metrics of MAE and RMSE。<details>
<summary>Abstract</summary>
As the development of cities, traffic congestion becomes an increasingly pressing issue, and traffic prediction is a classic method to relieve that issue. Traffic prediction is one specific application of spatio-temporal prediction learning, like taxi scheduling, weather prediction, and ship trajectory prediction. Against these problems, classical spatio-temporal prediction learning methods including deep learning, require large amounts of training data. In reality, some newly developed cities with insufficient sensors would not hold that assumption, and the data scarcity makes predictive performance worse. In such situation, the learning method on insufficient data is known as few-shot learning (FSL), and the FSL of traffic prediction remains challenges. On the one hand, graph structures' irregularity and dynamic nature of graphs cannot hold the performance of spatio-temporal learning method. On the other hand, conventional domain adaptation methods cannot work well on insufficient training data, when transferring knowledge from different domains to the intended target domain.To address these challenges, we propose a novel spatio-temporal domain adaptation (STDA) method that learns transferable spatio-temporal meta-knowledge from data-sufficient cities in an adversarial manner. This learned meta-knowledge can improve the prediction performance of data-scarce cities. Specifically, we train the STDA model using a Model-Agnostic Meta-Learning (MAML) based episode learning process, which is a model-agnostic meta-learning framework that enables the model to solve new learning tasks using only a small number of training samples. We conduct numerous experiments on four traffic prediction datasets, and our results show that the prediction performance of our model has improved by 7\% compared to baseline models on the two metrics of MAE and RMSE.
</details>
<details>
<summary>摘要</summary>
On one hand, graph structures' irregularity and the dynamic nature of graphs make it difficult for spatio-temporal learning methods to perform well. On the other hand, conventional domain adaptation methods are not effective in transferring knowledge from different domains to the intended target domain when working with insufficient training data.To address these challenges, we propose a novel spatio-temporal domain adaptation (STDA) method that learns transferable spatio-temporal meta-knowledge from data-sufficient cities in an adversarial manner. This learned meta-knowledge can improve the prediction performance of data-scarce cities. Specifically, we use a Model-Agnostic Meta-Learning (MAML) based episode learning process to train the STDA model, which is a model-agnostic meta-learning framework that enables the model to solve new learning tasks using only a small number of training samples.We conduct numerous experiments on four traffic prediction datasets, and our results show that the prediction performance of our model has improved by 7% compared to baseline models on the two metrics of Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).
</details></li>
</ul>
<hr>
<h2 id="Calibration-by-Distribution-Matching-Trainable-Kernel-Calibration-Metrics"><a href="#Calibration-by-Distribution-Matching-Trainable-Kernel-Calibration-Metrics" class="headerlink" title="Calibration by Distribution Matching: Trainable Kernel Calibration Metrics"></a>Calibration by Distribution Matching: Trainable Kernel Calibration Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20211">http://arxiv.org/abs/2310.20211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kernel-calibration/kernel-calibration">https://github.com/kernel-calibration/kernel-calibration</a></li>
<li>paper_authors: Charles Marx, Sofian Zalouk, Stefano Ermon</li>
<li>for: 这 paper 是为了提高预测uncertainty的捕捉，并且提供了一些可以与实际频率进行匹配的抽象方法。</li>
<li>methods: 这 paper 使用了一种基于核函数的匹配方法，这种方法可以总结和普适化现有的各种calibration方法，并且具有可导的样本估计，可以轻松地在empirical risk minimization中添加calibration目标。</li>
<li>results: 这 paper 的实验表明，通过使用这种匹配方法作为regulator，可以提高预测的准确性、锐度和决策效果，并且超过了只使用后期重新准确的方法。<details>
<summary>Abstract</summary>
Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification tasks, outperforming methods relying solely on post-hoc recalibration.
</details>
<details>
<summary>摘要</summary>
<<SYS>>系数调整确保 probabilistic 预测能够有效地捕捉不确定性，因为它们需要预测的概率与实际频率相匹配。然而，许多现有的准备方法都是专门为后期重新准备的，这可能会使预测变得更加锐利。基于 Distribution 匹配的思想，我们引入 kernel-based 准备指标，这些指标可以统一和普适化 Popular 的准备方法，并且具有可微的样本估计，因此可以轻松地在 empirical risk minimization 中添加准备目标。此外，我们还提供了直观的机制来适应决策任务，并且强制实现准确的损失估计和无悬决策。我们的实验证明，使用这些指标作为正则izers可以提高准备、锐利性和决策的水平，超过只靠后期重新准备的方法。Translation notes:* "probabilistic forecasts" is translated as "probabilistic 预测" (probabilistic predictions)* "calibration" is translated as "系数调整" (calibration)* "empirical frequencies" is translated as "实际频率" (empirical frequencies)* "post-hoc recalibration" is translated as "后期重新准备" (post-hoc recalibration)* "kernel-based calibration metrics" is translated as "kernel-based 准备指标" (kernel-based calibration metrics)* "differentiable sample estimates" is translated as "可微的样本估计" (differentiable sample estimates)* "empirical risk minimization" is translated as "empirical risk minimization" (empirical risk minimization)* "decision task" is translated as "决策任务" (decision task)
</details></li>
</ul>
<hr>
<h2 id="Network-Contention-Aware-Cluster-Scheduling-with-Reinforcement-Learning"><a href="#Network-Contention-Aware-Cluster-Scheduling-with-Reinforcement-Learning" class="headerlink" title="Network Contention-Aware Cluster Scheduling with Reinforcement Learning"></a>Network Contention-Aware Cluster Scheduling with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20209">http://arxiv.org/abs/2310.20209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gajagajago/deepshare">https://github.com/gajagajago/deepshare</a></li>
<li>paper_authors: Junyeol Ryu, Jeongyoon Eo</li>
<li>for: 这篇论文是为了解决对GPU集群的分布式训练中的网络竞争问题。</li>
<li>methods: 这篇论文使用了强化学习来解决网络竞争问题，具体来说是将GPU集群的调度问题转化为强化学习问题，并通过不断评估和改进来学习一个网络竞争意识的调度策略。</li>
<li>results: 相比于常用的调度策略，这篇论文的方法可以降低均值任务完成时间18.2%，并将尾部任务完成时间降低20.7%，同时允许可接受的资源利用率和任务完成时间之间的变化。<details>
<summary>Abstract</summary>
With continuous advances in deep learning, distributed training is becoming common in GPU clusters. Specifically, for emerging workloads with diverse amounts, ratios, and patterns of communication, we observe that network contention can significantly degrade training throughput. However, widely used scheduling policies often face limitations as they are agnostic to network contention between jobs. In this paper, we present a new approach to mitigate network contention in GPU clusters using reinforcement learning. We formulate GPU cluster scheduling as a reinforcement learning problem and opt to learn a network contention-aware scheduling policy that efficiently captures contention sensitivities and dynamically adapts scheduling decisions through continuous evaluation and improvement. We show that compared to widely used scheduling policies, our approach reduces average job completion time by up to 18.2\% and effectively cuts the tail job completion time by up to 20.7\% while allowing a preferable trade-off between average job completion time and resource utilization.
</details>
<details>
<summary>摘要</summary>
随着深度学习的不断发展，分布式训练在GPU集群中变得越来越普遍。特别是对于出现在各种各样的Amount、比例和模式的通信而言，网络竞争可能会对训练速率产生很大的降低影响。然而，广泛使用的调度策略经常遇到限制，因为它们对GPU集群内网络竞争无法提供相应的考虑。在这篇论文中，我们提出了一种使用反射学习来减少GPU集群内网络竞争的新方法。我们将GPU集群调度问题定义为反射学习问题，并选择了学习一种网络竞争意识的调度策略，以efficiently捕捉竞争敏感度并在继续评估和改进的基础上进行动态调整。我们显示，与广泛使用的调度策略相比，我们的方法可以降低平均任务完成时间量达18.2%，并同时减少尾部任务完成时间量达20.7%，而且允许在平均任务完成时间和资源利用之间进行可接受的变化。
</details></li>
</ul>
<hr>
<h2 id="Importance-Estimation-with-Random-Gradient-for-Neural-Network-Pruning"><a href="#Importance-Estimation-with-Random-Gradient-for-Neural-Network-Pruning" class="headerlink" title="Importance Estimation with Random Gradient for Neural Network Pruning"></a>Importance Estimation with Random Gradient for Neural Network Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20203">http://arxiv.org/abs/2310.20203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suman Sapkota, Binod Bhattarai</li>
<li>for: 提高神经网络的效率，使其更加高效。</li>
<li>methods: 使用征识法和梯度信息来估计每个神经元或卷积核的全局重要性，不需大量标注数据。</li>
<li>results: 与先前方法比较，我们的方法在ResNet和VGG架构上的CIFAR-100和STL-10 datasets上表现更好，并且可以补做先前方法的缺陷。<details>
<summary>Abstract</summary>
Global Neuron Importance Estimation is used to prune neural networks for efficiency reasons. To determine the global importance of each neuron or convolutional kernel, most of the existing methods either use activation or gradient information or both, which demands abundant labelled examples. In this work, we use heuristics to derive importance estimation similar to Taylor First Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs and TaylorFO-sq. We propose two additional methods to improve these importance estimation methods. Firstly, we propagate random gradients from the last layer of a network, thus avoiding the need for labelled examples. Secondly, we normalize the gradient magnitude of the last layer output before propagating, which allows all examples to contribute similarly to the importance score. Our methods with additional techniques perform better than previous methods when tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets. Furthermore, our method also complements the existing methods and improves their performances when combined with them.
</details>
<details>
<summary>摘要</summary>
全球神经元重要性估计是用于提高神经网络效率的方法。以便确定每个神经元或卷积器的全球重要性，大多数现有方法使用活动或梯度信息，或者两者都使用，需要备受标注的示例。在这项工作中，我们使用规则来 derivimportance estimation similar to Taylor First Order (TaylorFO) approximation-based methods。我们称我们的方法为 TaylorFO-abs 和 TaylorFO-sq。我们还提出了两种改进这些重要性估计方法的方法。首先，我们从网络的最后层传递随机梯度，因此不需要标注示例。其次，我们在最后层输出之前对梯度幅度进行归一化，这样所有的示例都可以同样地贡献到重要性分数中。我们的方法，加上其他技术，在测试于 ResNet 和 VGG 架构上的 CIFAR-100 和 STL-10 数据集上表现出色。此外，我们的方法还可以与现有方法相结合，提高其表现。
</details></li>
</ul>
<hr>
<h2 id="FedRec-Enhancing-Privacy-and-Addressing-Heterogeneity-in-Federated-Recommendation-Systems"><a href="#FedRec-Enhancing-Privacy-and-Addressing-Heterogeneity-in-Federated-Recommendation-Systems" class="headerlink" title="FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems"></a>FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20193">http://arxiv.org/abs/2310.20193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Wang, Zhichao Wang, Xi Leng, Xiaoying Tang</li>
<li>for: 保护用户隐私和减少边缘用户的通信成本，以提高推荐系统的性能。</li>
<li>methods:  FedRec+ 使用优化subset选择基于特征相似性，生成 Pseudo 项的近似优质评分，只使用用户本地信息，从而减少噪音而无需额外的通信成本。 更之前，我们利用 Wasserstein 距离来估计客户端间的差异和贡献，并解决一个定义的优化问题，以 derivation 优化的汇集权重。</li>
<li>results: 实验结果表明 FedRec+ 在各种参考数据集上具有状态艺术性的表现。<details>
<summary>Abstract</summary>
Preserving privacy and reducing communication costs for edge users pose significant challenges in recommendation systems. Although federated learning has proven effective in protecting privacy by avoiding data exchange between clients and servers, it has been shown that the server can infer user ratings based on updated non-zero gradients obtained from two consecutive rounds of user-uploaded gradients. Moreover, federated recommendation systems (FRS) face the challenge of heterogeneity, leading to decreased recommendation performance. In this paper, we propose FedRec+, an ensemble framework for FRS that enhances privacy while addressing the heterogeneity challenge. FedRec+ employs optimal subset selection based on feature similarity to generate near-optimal virtual ratings for pseudo items, utilizing only the user's local information. This approach reduces noise without incurring additional communication costs. Furthermore, we utilize the Wasserstein distance to estimate the heterogeneity and contribution of each client, and derive optimal aggregation weights by solving a defined optimization problem. Experimental results demonstrate the state-of-the-art performance of FedRec+ across various reference datasets.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:保持隐私和降低边缘用户的通信成本是推荐系统的主要挑战。虽然联邦学习已经证明可以保护隐私，但是服务器可以通过两次连续的用户上传梯度来推断用户评分。此外，联邦推荐系统（FRS）面临着多样性挑战，导致推荐性能下降。在本文中，我们提出了FedRec+，一种ensemble框架 для FRS，可以增强隐私并 Addressing the Heterogeneity Challenge。FedRec+使用了优化subset选择基于特征相似性，生成 Pseudo  Item 的近似优质评分，只使用用户的本地信息。这种方法可以减少噪音而不导致额外的通信成本。此外，我们利用 Wasserstein 距离来估计每个客户端的多样性和贡献，并解决一个定义的优化问题来 derive 最佳汇总 веctor。实验结果表明 FedRec+ 在多个参考数据集上达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Compact-Binary-Systems-Waveform-Generation-with-Generative-Pre-trained-Transformer"><a href="#Compact-Binary-Systems-Waveform-Generation-with-Generative-Pre-trained-Transformer" class="headerlink" title="Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer"></a>Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20172">http://arxiv.org/abs/2310.20172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijun Shi, Yue Zhou, Tianyu Zhao, Zhoujian Cao, Zhixiang Ren</li>
<li>for: 这个研究是为了解决太空 gravitational wave 探测中的数据处理问题，特别是在次代时阻对称探测器（TDI 2.0）中增加的波形复杂性导致的数据处理问题。</li>
<li>methods: 这个研究使用了一个可解释的大型预训模型 named CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）来预测太空 gravitational wave 波形。三个模型被训练来预测黑洞Binary（MBHB）、巨大质量比例探测（EMRIs）和星系Binary（GB）波形，实现预测精度分别为98%、91%和99%。</li>
<li>results:  CBS-GPT 模型具有良好的可解释性，其隐藏参数能够有效地捕捉波形中的复杂信息，包括探测器的回应和广泛的参数范围。这项研究显示了大型预训模型在 gravitational wave 数据处理中的应用潜力，开启了未来的任务，如补充差、GW 信号探测和信号干扰reduction。<details>
<summary>Abstract</summary>
Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex instrument response and a wide parameter range. Our research demonstrates the potential of large pre-trained models in gravitational wave data processing, opening up new opportunities for future tasks such as gap completion, GW signal detection, and signal noise reduction.
</details>
<details>
<summary>摘要</summary>
空间基于gravitational wave探测是下一代 gravitational wave (GW) 探测项目中最受期待的一项，可探测丰富的紧凑Binary系统。然而，准确预测空间GW波形仍未经过研究。为解决探测器响应和第二代时间延迟相干（TDI 2.0）所导致的数据处理困难，一种可解释的大型预训练模型被提出，称为CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）。对紧凑Binary系统波形进行预测，CBS-GPT模型在大质量黑洞 binary（MBHB）、Extreme Mass-Ratio Inspirals（EMRIs）和galactic Binary（GB）三种情况中实现了预测精度为98%、91%和99%。CBS-GPT模型表现出了明显的可解释性，其隐藏参数能够有效捕捉波形中的复杂信息，即使是通过复杂的探测器响应和广泛的参数范围。我们的研究表明了大型预训练模型在gravitational wave数据处理中的潜在优势，开启了未来任务 such as gap completion、GW signal detection和signal noise reduction的新可能性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Visualizing-Droplet-Distributions-in-Simulations-of-Shallow-Clouds"><a href="#Understanding-and-Visualizing-Droplet-Distributions-in-Simulations-of-Shallow-Clouds" class="headerlink" title="Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds"></a>Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20168">http://arxiv.org/abs/2310.20168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justus C. Will, Andrea M. Jenney, Kara D. Lamb, Michael S. Pritchard, Colleen Kaul, Po-Lun Ma, Kyle Pressel, Jacob Shpund, Marcus van Lier-Walqui, Stephan Mandt</li>
<li>for: 本研究旨在更好地理解云中微物理过程，以及它们对全球气候的影响。</li>
<li>methods: 本研究使用了大气动力学LES的精度高的颗粒大小分布来挑战当前分析技术。</li>
<li>results: 研究人员通过使用变量自动编码器（VAEs）生成了新的和直观的视觉化图表，以描述颗粒大小的分布和时间演化。这些图表超出了 clustering 技术的能力，并允许我们对受到不同气尘浓度影响的 simulations 进行比较。我们发现颗粒谱的演化是不同气尘含量水平下的相似的，但发生在不同的速度上。这种相似性表明降水 initiation 过程是相同的，尽管启动时间有所不同。<details>
<summary>Abstract</summary>
Thorough analysis of local droplet-level interactions is crucial to better understand the microphysical processes in clouds and their effect on the global climate. High-accuracy simulations of relevant droplet size distributions from Large Eddy Simulations (LES) of bin microphysics challenge current analysis techniques due to their high dimensionality involving three spatial dimensions, time, and a continuous range of droplet sizes. Utilizing the compact latent representations from Variational Autoencoders (VAEs), we produce novel and intuitive visualizations for the organization of droplet sizes and their evolution over time beyond what is possible with clustering techniques. This greatly improves interpretation and allows us to examine aerosol-cloud interactions by contrasting simulations with different aerosol concentrations. We find that the evolution of the droplet spectrum is similar across aerosol levels but occurs at different paces. This similarity suggests that precipitation initiation processes are alike despite variations in onset times.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:全面分析当地液滴水平交互是 clouds 的微物理过程理解的关键，以及全球气候的影响。 Large Eddy Simulations (LES) of bin microphysics 的高精度仿真中的液滴大小分布挑战当前分析技术，因为它们具有三维空间、时间和液滴大小的维度。通过 Variational Autoencoders (VAEs) 的压缩隐藏表示，我们生成了不同于 clustering 技术的新和直观的视觉化，以便更好地理解液滴大小的组织和时间演变。这有助于解释，并允许我们通过不同气尘含量的仿真比较，探讨云尘相互作用。我们发现，不同气尘含量下的液滴spectrum 的演变类似，但发生在不同的速度。这种类似性表明，降水 initiation 过程是相似的，即使启动时间有所不同。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Robust-Bayesian-Optimization-for-Arbitrary-Uncertain-inputs"><a href="#Efficient-Robust-Bayesian-Optimization-for-Arbitrary-Uncertain-inputs" class="headerlink" title="Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs"></a>Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20145">http://arxiv.org/abs/2310.20145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Junlong Lyu, Wenlong Lyu, Zhitang Chen</li>
<li>for: 这篇论文是为了解决在 bayesian optimization 中的 input uncertainty 问题，实现 robust 的最佳化。</li>
<li>methods: 本 paper 使用了一个新的 robust bayesian optimization 算法 AIRBO，Directly 模型了不确定的输入，使用 Gaussian Process 和 Maximum Mean Discrepancy (MMD) 来实现。</li>
<li>results: 实验结果显示，AIRBO 可以处理不同的 input uncertainty，并且可以实现 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic functions and real problems demonstrate that our approach can handle various input uncertainties and achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sample-Conditioned-Hypothesis-Stability-Sharpens-Information-Theoretic-Generalization-Bounds"><a href="#Sample-Conditioned-Hypothesis-Stability-Sharpens-Information-Theoretic-Generalization-Bounds" class="headerlink" title="Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds"></a>Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20102">http://arxiv.org/abs/2310.20102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqiao Wang, Yongyi Mao</li>
<li>for: 提供新的信息理论性泛化保证，通过一种新的“邻域假设”矩阵和一家新的样本conditioned假设稳定性（SCH稳定性）家族。</li>
<li>methods: 使用新的信息理论性泛化保证方法，包括“邻域假设”矩阵的新建构和样本conditioned假设稳定性。</li>
<li>results: 提供更加精细的信息理论性泛化保证，可以在不同的学习场景中提高先前的信息理论性泛化保证。特别是，这些保证可以 addresses existential learning scenario中的限制，如Haghifam et al. (2023)所explore的stoochastic convex optimization（SCO）问题。<details>
<summary>Abstract</summary>
We present new information-theoretic generalization guarantees through the a novel construction of the "neighboring-hypothesis" matrix and a new family of stability notions termed sample-conditioned hypothesis (SCH) stability. Our approach yields sharper bounds that improve upon previous information-theoretic bounds in various learning scenarios. Notably, these bounds address the limitations of existing information-theoretic bounds in the context of stochastic convex optimization (SCO) problems, as explored in the recent work by Haghifam et al. (2023).
</details>
<details>
<summary>摘要</summary>
我们提出了一新的信息理论性的扩张保证，通过一个新的“邻域假设”矩阵的建构和一新的家族叫做“样本调整假设”（SCH）稳定性。我们的方法可以获得更加锐利的界限，超越了过去的信息理论性 bound 在不同的学习场景中。特别是，这些 bound 可以解决现有信息理论性 bound 在数学测度估计（SCO）问题中的限制，就如果aghifam et al. (2023) 所研究的情况。
</details></li>
</ul>
<hr>
<h2 id="Robust-Learning-for-Smoothed-Online-Convex-Optimization-with-Feedback-Delay"><a href="#Robust-Learning-for-Smoothed-Online-Convex-Optimization-with-Feedback-Delay" class="headerlink" title="Robust Learning for Smoothed Online Convex Optimization with Feedback Delay"></a>Robust Learning for Smoothed Online Convex Optimization with Feedback Delay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20098">http://arxiv.org/abs/2310.20098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Li, Jianyi Yang, Adam Wierman, Shaolei Ren</li>
<li>for: 这个论文旨在解决智能型online减少准则（SOCO）中的多步非线性切换成本和反馈延迟问题。</li>
<li>methods: 这篇论文提出了一种基于机器学习（ML）的增强 online算法，即Robustness-Constrained Learning（RCL），该算法将不可信ML预测与可信专家线上算法相结合，通过受限制的投影来使ML预测更加稳定。</li>
<li>results: 论文证明了RCL能够保证（1+λ）-竞争力 against any given expert for any λ&gt;0，同时也可以在平均情况下提高ML预测的性能。此外，RCL是SOCO中多步非线性切换成本和反馈延迟的首个具有证明的可靠性保证的ML增强算法。<details>
<summary>Abstract</summary>
We study a challenging form of Smoothed Online Convex Optimization, a.k.a. SOCO, including multi-step nonlinear switching costs and feedback delay. We propose a novel machine learning (ML) augmented online algorithm, Robustness-Constrained Learning (RCL), which combines untrusted ML predictions with a trusted expert online algorithm via constrained projection to robustify the ML prediction. Specifically,we prove that RCL is able to guarantee$(1+\lambda)$-competitiveness against any given expert for any$\lambda>0$, while also explicitly training the ML model in a robustification-aware manner to improve the average-case performance. Importantly,RCL is the first ML-augmented algorithm with a provable robustness guarantee in the case of multi-step switching cost and feedback delay.We demonstrate the improvement of RCL in both robustness and average performance using battery management for electrifying transportationas a case study.
</details>
<details>
<summary>摘要</summary>
我们研究一种具有多步非线性调整成本和反馈延迟的简润线上凸优化，简称SOCO，以及一个新的机器学习（ML）增强的线上算法，即对称确定学习（RCL）。 RCL 结合了不可信ML预测和可信专家线上算法via受限投影，以确保ML预测的稳定性。我们证明了RCL 能够保证$(1+\lambda)$-竞争性比任何给定专家，适用于任何 $\lambda>0$。此外，RCL 还会在对ML预测进行了强化训练的情况下，提高均值性能。在用电动车电池管理为应用的 caso study 中，我们显示了RCL 在类型和平均性能之间的改善。Here's the translation in Traditional Chinese:我们研究一种具有多步非线性调整成本和反馈延迟的简润线上凸优化，简称SOCO，以及一个新的机器学习（ML）增强的线上算法，即对称确定学习（RCL）。 RCL 结合了不可信ML预测和可信专家线上算法via受限投影，以确保ML预测的稳定性。我们证明了RCL 能够保证$(1+\lambda)$-竞争性比任何给定专家，适用于任何 $\lambda>0$。此外，RCL 还会在对ML预测进行了强化训练的情况下，提高均值性能。在用电动车电池管理为应用的 caso study 中，我们显示了RCL 在类型和平均性能之间的改善。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Between-Variational-Inference-and-Wasserstein-Gradient-Flows"><a href="#Bridging-the-Gap-Between-Variational-Inference-and-Wasserstein-Gradient-Flows" class="headerlink" title="Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows"></a>Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20090">http://arxiv.org/abs/2310.20090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yimx/bridging-the-gap-between-vi-and-wgf">https://github.com/yimx/bridging-the-gap-between-vi-and-wgf</a></li>
<li>paper_authors: Mingxuan Yi, Song Liu</li>
<li>for:  bridges the gap between variational inference and Wasserstein gradient flows</li>
<li>methods:  uses the Bures-Wasserstein gradient flow and the path-derivative gradient estimator</li>
<li>results:  demonstrates that the forward Euler scheme of the gradient flow is equivalent to standard black-box variational inference, and offers an alternative perspective on the path-derivative gradient as a distillation procedure to the Wasserstein gradient flow.<details>
<summary>Abstract</summary>
Variational inference is a technique that approximates a target distribution by optimizing within the parameter space of variational families. On the other hand, Wasserstein gradient flows describe optimization within the space of probability measures where they do not necessarily admit a parametric density function. In this paper, we bridge the gap between these two methods. We demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow where its forward Euler scheme is the standard black-box variational inference algorithm. Specifically, the vector field of the gradient flow is generated via the path-derivative gradient estimator. We also offer an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow. Distillations can be extended to encompass $f$-divergences and non-Gaussian variational families. This extension yields a new gradient estimator for $f$-divergences, readily implementable using contemporary machine learning libraries like PyTorch or TensorFlow.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用变量推理进行减少风险的技术是通过优化变量家族的参数空间中的目标分布来进行优化。然而，沃asserstein梯度流描述了在概率分布的空间上进行优化，而这些优化不一定具有参数概率函数。在这篇论文中，我们将这两种方法相连接。我们证明，在某些条件下，布尔斯-沃asserstein梯度流可以转化为凯尔提供的标准黑盒变量推理算法的欧式迭代计划。具体来说，梯度流的向量场是通过路径DERIVATIVE gradient estimator来生成的。我们还提供了一种另一种视角，将路径DERIVATIVE gradient 框架为沃asserstein梯度流的蒸馏过程。这种蒸馏过程可以扩展到包括 $f$-散度和非泊尔分布的变量家族。这个扩展允许我们提出一个新的梯度估计器，可以使用现代机器学习库如PyTorch或TensorFlow进行实现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.LG_2023_10_31/" data-id="cloimipcu00r0s4886yrefpdh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/cs.CL_2023_10_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-10-31
        
      </div>
    </a>
  
  
    <a href="/2023/10/31/eess.IV_2023_10_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-10-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
