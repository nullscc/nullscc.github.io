
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-10-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Unexpected Improvements to Expected Improvement for Bayesian Optimization paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20708 repo_url: None paper_authors: Sebastian Ament, Samuel Daulton, David Eriksson, Maxi">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-10-31">
<meta property="og:url" content="https://nullscc.github.io/2023/10/31/cs.LG_2023_10_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Unexpected Improvements to Expected Improvement for Bayesian Optimization paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20708 repo_url: None paper_authors: Sebastian Ament, Samuel Daulton, David Eriksson, Maxi">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-31T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:07.037Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.LG_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T10:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-10-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unexpected-Improvements-to-Expected-Improvement-for-Bayesian-Optimization"><a href="#Unexpected-Improvements-to-Expected-Improvement-for-Bayesian-Optimization" class="headerlink" title="Unexpected Improvements to Expected Improvement for Bayesian Optimization"></a>Unexpected Improvements to Expected Improvement for Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20708">http://arxiv.org/abs/2310.20708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Ament, Samuel Daulton, David Eriksson, Maximilian Balandat, Eytan Bakshy</li>
<li>for: 提高 Bayesian 优化中的 acquisition function 性能</li>
<li>methods: 提出 LogEI 家族的新 acquisition function，其成员在数值优化中更易优化，并且可以提高 optimization 性能</li>
<li>results: 实验结果显示 LogEI 家族的 acquisition function 可以substantially 提高 Bayesian 优化中的性能，并且与最新的 state-of-the-art acquisition function 相当或超越其性能， highlighting the understated role of numerical optimization in the literature.<details>
<summary>Abstract</summary>
Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.
</details>
<details>
<summary>摘要</summary>
预期改进（EI）是搜索优化中最受欢迎的功能之一，在搜索优化中发现了 countless 的成功应用，但其表现通常被更新的方法超越。尤其是EI和其变体，包括并行和多目标设置，在数值上是困难优化的，因为其获取值在许多地方会数值消失。这种困难通常随着观察次数、搜索空间维度或约束数量增加，导致文献中表现不一致，通常是低效的。在这里，我们提出了 LogEI，一种新的获取函数家族，其成员在Canonical counterparts中具有相同或相近的极值，但是数值上更容易优化。我们证明了经典的EI、Expected Hypervolume Improvement（EHVI）以及其约束、噪音和并行变体中的数值问题，并提出了相应的改进方案。我们的实验结果表明，LogEI家族中的获取函数可以大幅提高与Canonical counterparts的优化表现，另外，与当前状态的最佳获取函数相当或超越其表现，这 highlights the understated role of numerical optimization in the literature。
</details></li>
</ul>
<hr>
<h2 id="Farthest-Greedy-Path-Sampling-for-Two-shot-Recommender-Search"><a href="#Farthest-Greedy-Path-Sampling-for-Two-shot-Recommender-Search" class="headerlink" title="Farthest Greedy Path Sampling for Two-shot Recommender Search"></a>Farthest Greedy Path Sampling for Two-shot Recommender Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20705">http://arxiv.org/abs/2310.20705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufan Cao, Tunhou Zhang, Wei Wen, Feng Yan, Hai Li, Yiran Chen</li>
<li>for: 提高深度推荐模型的开发效率和性能。</li>
<li>methods: 使用Weight-sharing Neural Architecture Search (WS-NAS)机制，并引入新的path sampling策略Farthest Greedy Path Sampling (FGPS)，以提高搜索空间的覆盖率和subnet重量的协同适应性。</li>
<li>results: 通过在Three Click-Through Rate (CTR) prediction benchmark上进行评估，发现该方法可以准确地识别和利用优秀的建筑，并在大多数NAS基础模型上显著超越 manual设计和其他 NAS 模型。<details>
<summary>Abstract</summary>
Weight-sharing Neural Architecture Search (WS-NAS) provides an efficient mechanism for developing end-to-end deep recommender models. However, in complex search spaces, distinguishing between superior and inferior architectures (or paths) is challenging. This challenge is compounded by the limited coverage of the supernet and the co-adaptation of subnet weights, which restricts the exploration and exploitation capabilities inherent to weight-sharing mechanisms. To address these challenges, we introduce Farthest Greedy Path Sampling (FGPS), a new path sampling strategy that balances path quality and diversity. FGPS enhances path diversity to facilitate more comprehensive supernet exploration, while emphasizing path quality to ensure the effective identification and utilization of promising architectures. By incorporating FGPS into a Two-shot NAS (TS-NAS) framework, we derive high-performance architectures. Evaluations on three Click-Through Rate (CTR) prediction benchmarks demonstrate that our approach consistently achieves superior results, outperforming both manually designed and most NAS-based models.
</details>
<details>
<summary>摘要</summary>
��wort-sharing Neural Architecture Search (WS-NAS) 提供了一种高效的终端深度推荐模型开发机制。然而，在复杂的搜索空间中， отличить优于劣的架构（或路径）是困难的。这种挑战更加困难由超网络的limited coverage和子网重量协作所带来的搜索和利用机制的限制。为解决这些挑战，我们介绍了远程艰苟路径采样（FGPS），一种新的路径采样策略，该策略平衡路径质量和多样性。FGPS通过增强超网络搜索的多样性，以便更全面地探索超网络，同时强调路径质量，以确保有效地识别和利用优秀的架构。通过将FGPS纳入TS-NAS框架中，我们 derivation高性能的架构。对三个Click-Through Rate（CTR）预测 benchmark中的评估表明，我们的方法能够一直保持优秀的 результа，与 manually designed 和大多数 NAS-based 模型相比，表现出优。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Multistate-Bennett-Acceptance-Ratio-Methods"><a href="#Bayesian-Multistate-Bennett-Acceptance-Ratio-Methods" class="headerlink" title="Bayesian Multistate Bennett Acceptance Ratio Methods"></a>Bayesian Multistate Bennett Acceptance Ratio Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20699">http://arxiv.org/abs/2310.20699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinqiang Ding</li>
<li>for: 计算热动力学状态的自由能量</li>
<li>methods:  bayesian generalization of MBAR method, integrating configurations sampled from thermodynamic states with a prior distribution to compute posterior distribution of free energies</li>
<li>results: 提供了更准确的自由能量估计和相关的不确定性计算Please note that the above information is in Simplified Chinese text.<details>
<summary>Abstract</summary>
The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach for computing free energies of thermodynamic states. In this work, we introduce BayesMBAR, a Bayesian generalization of the MBAR method. By integrating configurations sampled from thermodynamic states with a prior distribution, BayesMBAR computes a posterior distribution of free energies. Using the posterior distribution, we derive free energy estimations and compute their associated uncertainties. Notably, when a uniform prior distribution is used, BayesMBAR recovers the MBAR's result but provides more accurate uncertainty estimates. Additionally, when prior knowledge about free energies is available, BayesMBAR can incorporate this information into the estimation procedure by using non-uniform prior distributions. As an example, we show that, by incorporating the prior knowledge about the smoothness of free energy surfaces, BayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's widespread use in free energy calculations, we anticipate BayesMBAR to be an essential tool in various applications of free energy calculations.
</details>
<details>
<summary>摘要</summary>
多态Bennett接受度方法（MBAR）是一种广泛应用的计算热力学状态自由能的方法。在这项工作中，我们介绍了抽象MBAR方法的 bayesian扩展方法——抽象MBAR方法。通过将thermodynamic状态中采样的配置与一个先验分布集成起来，抽象MBAR方法计算出了后验自由能分布。使用后验分布，我们计算出自由能估计值和相关的不确定性。吸引注意的是，当使用均匀先验分布时，抽象MBAR方法可以重新计算MBAR方法的结果，并提供更加准确的不确定性估计。此外，当有具体关于自由能的先验知识时，抽象MBAR方法可以在计算过程中integrate这些信息，通过使用非均匀先验分布来提高估计精度。作为一个例子，我们显示了，通过在自由能表面的平滑性信息上 incorporate先验知识，抽象MBAR方法可以提供更加准确的估计值。由于MBAR方法在自由能计算中广泛应用，我们预计抽象MBAR方法将成为许多应用中的关键工具。
</details></li>
</ul>
<hr>
<h2 id="Compression-with-Exact-Error-Distribution-for-Federated-Learning"><a href="#Compression-with-Exact-Error-Distribution-for-Federated-Learning" class="headerlink" title="Compression with Exact Error Distribution for Federated Learning"></a>Compression with Exact Error Distribution for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20682">http://arxiv.org/abs/2310.20682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Hegazy, Rémi Leluc, Cheuk Ting Li, Aymeric Dieuleveut</li>
<li>for: 本文研究了在 Federated Learning（FL）中使用压缩方案，以降低分布式学习中的通信成本。</li>
<li>methods: 本文提出了基于层次量化的各种压缩和聚合方案，以实现特定的错误分布，如 Gaussian 或 Laplace 分布。</li>
<li>results: 我们的方法可以在 differential privacy 应用中实现压缩-for-free，并且可以提高标准的 FL 方案中的 Gaussian 噪声，如 Langevin 动力学和随机缓和。<details>
<summary>Abstract</summary>
Compression schemes have been extensively used in Federated Learning (FL) to reduce the communication cost of distributed learning. While most approaches rely on a bounded variance assumption of the noise produced by the compressor, this paper investigates the use of compression and aggregation schemes that produce a specific error distribution, e.g., Gaussian or Laplace, on the aggregated data. We present and analyze different aggregation schemes based on layered quantizers achieving exact error distribution. We provide different methods to leverage the proposed compression schemes to obtain compression-for-free in differential privacy applications. Our general compression methods can recover and improve standard FL schemes with Gaussian perturbations such as Langevin dynamics and randomized smoothing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>在分布式学习（Federated Learning，FL）中，压缩方案广泛应用于减少分布式学习的通信成本。而大多数方法都假设压缩器生成的噪声具有有界的方差，这篇论文则研究使用压缩和汇总方案生成特定的错误分布，例如高斯或勒贝 Distribution，在汇总数据中。我们提出和分析不同层次量化器基于的汇总方案，实现精确的错误分布。我们还提供不同的方法来利用我们的压缩方案在隐私保护应用中实现压缩-for-free。我们的通用压缩方法可以恢复和改进标准FL方案中的高斯噪声，例如谱温动和随机缓和。
</details></li>
</ul>
<hr>
<h2 id="Latent-Field-Discovery-In-Interacting-Dynamical-Systems-With-Neural-Fields"><a href="#Latent-Field-Discovery-In-Interacting-Dynamical-Systems-With-Neural-Fields" class="headerlink" title="Latent Field Discovery In Interacting Dynamical Systems With Neural Fields"></a>Latent Field Discovery In Interacting Dynamical Systems With Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20679">http://arxiv.org/abs/2310.20679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkofinas/aether">https://github.com/mkofinas/aether</a></li>
<li>paper_authors: Miltiadis Kofinas, Erik J. Bekkers, Naveen Shankar Nagaraja, Efstratios Gavves</li>
<li>for: 本文旨在发现场景中的场效果，而不直接观察场效果。</li>
<li>methods: 本文提出了 neural fields 来学习场效果，并将 мест交互作用模型为 $\mathrm{SE}(n)$  equivariant graph networks。</li>
<li>results: 实验表明，可以准确发现场景中的场效果，并使用其来预测未来轨迹。<details>
<summary>Abstract</summary>
Systems of interacting objects often evolve under the influence of field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, without directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to disentangle local object interactions -- which are $\mathrm{SE}(n)$ equivariant and depend on relative states -- from external global field effects -- which depend on absolute states. We model interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates field forces. Our experiments show that we can accurately discover the underlying fields in charged particles settings, traffic scenes, and gravitational n-body problems, and effectively use them to learn the system and forecast future trajectories.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Balancing-Act-Constraining-Disparate-Impact-in-Sparse-Models"><a href="#Balancing-Act-Constraining-Disparate-Impact-in-Sparse-Models" class="headerlink" title="Balancing Act: Constraining Disparate Impact in Sparse Models"></a>Balancing Act: Constraining Disparate Impact in Sparse Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20673">http://arxiv.org/abs/2310.20673</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/merajhashemi/balancing_act">https://github.com/merajhashemi/balancing_act</a></li>
<li>paper_authors: Meraj Hashemizadeh, Juan Ramirez, Rohan Sukumaran, Golnoosh Farnadi, Simon Lacoste-Julien, Jose Gallego-Posada</li>
<li>for: 这篇论文旨在提出一种可以在边缘设备上部署大型深度学习模型的方法，并且能够降低模型的硬件要求和存储空间。</li>
<li>methods: 这篇论文使用了一种受限的优化方法来降低模型的硬件要求和存储空间，并且直接地处理过滤后的精度下降。</li>
<li>results: 实验结果显示，这篇论文的方法可以可靠地应用于大型模型和许多保护的子集之间，并且能够提供可靠的精度下降 bound。<details>
<summary>Abstract</summary>
Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that $\textit{directly addresses the disparate impact of pruning}$: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups.
</details>
<details>
<summary>摘要</summary>
We propose a constrained optimization approach that directly addresses the disparate impact of pruning. Our formulation sets bounds on the accuracy change between the dense and sparse models for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results show that our technique can be reliably applied to problems involving large models and hundreds of protected sub-groups.
</details></li>
</ul>
<hr>
<h2 id="Density-Matrix-Emulation-of-Quantum-Recurrent-Neural-Networks-for-Multivariate-Time-Series-Prediction"><a href="#Density-Matrix-Emulation-of-Quantum-Recurrent-Neural-Networks-for-Multivariate-Time-Series-Prediction" class="headerlink" title="Density Matrix Emulation of Quantum Recurrent Neural Networks for Multivariate Time Series Prediction"></a>Density Matrix Emulation of Quantum Recurrent Neural Networks for Multivariate Time Series Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20671">http://arxiv.org/abs/2310.20671</a></li>
<li>repo_url: None</li>
<li>paper_authors: José Daniel Viqueira, Daniel Faílde, Mariamo M. Juane, Andrés Gómez, David Mera</li>
<li>for: 本研究旨在实现多变量时间序列预测的量子循环神经网络（QRNN）模型，但现有的量子硬件限制了一些QRNN模型的可实现性。为此，我们提出了一种特定的模拟方法，基于density matrix formalism。</li>
<li>methods: 我们使用了tensor notation来提供一种紧凑的数学开发，以显示如何在模拟网络中传递当前和过去时间序列信息，并如何在每个时间步骤中减少计算成本。此外，我们 derivated了模型输出的导数和希尔бер特，以便使用梯度下降法进行训练和处理真实量子处理器的噪声输出。</li>
<li>results: 我们使用了一种新的硬件高效的 Ansatz 和三种多元和单元时间序列数据集来测试我们的方法。结果表明，QRNNs 可以准确预测输入序列中的未来值，并capture不同复杂度的输入序列的非常贯通 patterns。<details>
<summary>Abstract</summary>
Quantum Recurrent Neural Networks (QRNNs) are robust candidates to model and predict future values in multivariate time series. However, the effective implementation of some QRNN models is limited by the need of mid-circuit measurements. Those increase the requirements for quantum hardware, which in the current NISQ era does not allow reliable computations. Emulation arises as the main near-term alternative to explore the potential of QRNNs, but existing quantum emulators are not dedicated to circuits with multiple intermediate measurements. In this context, we design a specific emulation method that relies on density matrix formalism. The mathematical development is explicitly provided as a compact formulation by using tensor notation. It allows us to show how the present and past information from a time series is transmitted through the circuit, and how to reduce the computational cost in every time step of the emulated network. In addition, we derive the analytical gradient and the Hessian of the network outputs with respect to its trainable parameters, with an eye on gradient-based training and noisy outputs that would appear when using real quantum processors. We finally test the presented methods using a novel hardware-efficient ansatz and three diverse datasets that include univariate and multivariate time series. Our results show how QRNNs can make accurate predictions of future values by capturing non-trivial patterns of input series with different complexities.
</details>
<details>
<summary>摘要</summary>
量子循环神经网络（QRNN）是Multivariate时间序列预测的稳定候选人，但有些QRNN模型的实施效率受到中间测量的限制。这些测量会增加量子硬件的需求，现今的NISQ时代不可靠计算。虚拟实现成为离散QRNN的主要近期代替方案，但现有的量子虚拟器并不适用于多个中间测量的环circuit。在这种情况下，我们设计了专门的虚拟方法，基于density matrix formalism。我们使用tensor notation提供了一种紧凑的表述，以显示环circuit传递当前和过去时间序列信息，并如何在每个时间步骤中减少计算成本。此外，我们derive了网络输出的分析导数和Hessian，以便使用梯度下降法和噪声输出来训练网络。最后，我们使用一种硬件高效的ansatz和三个多样化的时间序列Dataset进行测试，其中包括单variate和多variate时间序列。我们的结果表明，QRNN可以准确预测未来值，并捕捉输入序列的不同复杂性中的非rival patterns。
</details></li>
</ul>
<hr>
<h2 id="Performance-Improvement-in-Multi-class-Classification-via-Automated-Hierarchy-Generation-and-Exploitation-through-Extended-LCPN-Schemes"><a href="#Performance-Improvement-in-Multi-class-Classification-via-Automated-Hierarchy-Generation-and-Exploitation-through-Extended-LCPN-Schemes" class="headerlink" title="Performance Improvement in Multi-class Classification via Automated Hierarchy Generation and Exploitation through Extended LCPN Schemes"></a>Performance Improvement in Multi-class Classification via Automated Hierarchy Generation and Exploitation through Extended LCPN Schemes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20641">http://arxiv.org/abs/2310.20641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celal Alagoz</li>
<li>for: 这个研究探讨了幂等分类（HC）在多类分类任务中的表现，特别是无法直接获取先验知识结构的场景下。</li>
<li>methods: 研究涉及了 hierarchy generation和 hierarchy exploitation的两个方面，并提出了两种新的 hierarchy exploitation scheme，LCPN+和LCPN+F，以及对现有方法的评估。</li>
<li>results: 研究发现LCPN+F在多个数据集和场景下表现出了一致的优势，并且与平面分类（FC）的运行时间性能相似。此外，研究还强调了选择合适的 hierarchy exploitation scheme可以最大化分类性能。<details>
<summary>Abstract</summary>
Hierarchical classification (HC) plays a pivotal role in multi-class classification tasks, where objects are organized into a hierarchical structure. This study explores the performance of HC through a comprehensive analysis that encompasses both hierarchy generation and hierarchy exploitation. This analysis is particularly relevant in scenarios where a predefined hierarchy structure is not readily accessible. Notably, two novel hierarchy exploitation schemes, LCPN+ and LCPN+F, which extend the capabilities of LCPN and combine the strengths of global and local classification, have been introduced and evaluated alongside existing methods. The findings reveal the consistent superiority of LCPN+F, which outperforms other schemes across various datasets and scenarios. Moreover, this research emphasizes not only effectiveness but also efficiency, as LCPN+ and LCPN+F maintain runtime performance comparable to Flat Classification (FC). Additionally, this study underscores the importance of selecting the right hierarchy exploitation scheme to maximize classification performance. This work extends our understanding of HC and establishes a benchmark for future research, fostering advancements in multi-class classification methodologies.
</details>
<details>
<summary>摘要</summary>
Notably, two novel hierarchy exploitation schemes, LCPN+ and LCPN+F, have been introduced and evaluated, which extend the capabilities of LCPN and combine the strengths of global and local classification. The results consistently show that LCPN+F outperforms other schemes across various datasets and scenarios.Moreover, this research emphasizes not only effectiveness but also efficiency, as LCPN+ and LCPN+F maintain comparable runtime performance to Flat Classification (FC). This study highlights the importance of selecting the appropriate hierarchy exploitation scheme to maximize classification performance.This work extends our understanding of HC and establishes a benchmark for future research, paving the way for advancements in multi-class classification methodologies.
</details></li>
</ul>
<hr>
<h2 id="Projecting-basis-functions-with-tensor-networks-for-Gaussian-process-regression"><a href="#Projecting-basis-functions-with-tensor-networks-for-Gaussian-process-regression" class="headerlink" title="Projecting basis functions with tensor networks for Gaussian process regression"></a>Projecting basis functions with tensor networks for Gaussian process regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20630">http://arxiv.org/abs/2310.20630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clara Menzen, Eva Memmel, Kim Batselier, Manon Kok</li>
<li>for: 这 paper 的目的是提出一种 Approximate Gaussian Process（GP） regression 方法，使用 tensor networks（TNs）来实现。</li>
<li>methods: 这 paper 使用了一种基于 tensor networks 的方法，通过在低维度子空间中解决 Bayesian  inference 问题来找到模型的参数。</li>
<li>results: 在一个 18 维数据集上进行了一个 inverse dynamics 问题的实验，得到了模型的应用。<details>
<summary>Abstract</summary>
This paper presents a method for approximate Gaussian process (GP) regression with tensor networks (TNs). A parametric approximation of a GP uses a linear combination of basis functions, where the accuracy of the approximation depends on the total number of basis functions $M$. We develop an approach that allows us to use an exponential amount of basis functions without the corresponding exponential computational complexity. The key idea to enable this is using low-rank TNs. We first find a suitable low-dimensional subspace from the data, described by a low-rank TN. In this low-dimensional subspace, we then infer the weights of our model by solving a Bayesian inference problem. Finally, we project the resulting weights back to the original space to make GP predictions. The benefit of our approach comes from the projection to a smaller subspace: It modifies the shape of the basis functions in a way that it sees fit based on the given data, and it allows for efficient computations in the smaller subspace. In an experiment with an 18-dimensional benchmark data set, we show the applicability of our method to an inverse dynamics problem.
</details>
<details>
<summary>摘要</summary>
Our method addresses this limitation by using low-rank TNs to reduce the computational complexity. We first find a suitable low-dimensional subspace from the data using a low-rank TN. In this low-dimensional subspace, we then solve a Bayesian inference problem to infer the weights of our model. Finally, we project the resulting weights back to the original space to make GP predictions.The key advantage of our approach is the projection to a smaller subspace, which modifies the shape of the basis functions in a way that is suitable for the given data. This allows for efficient computations in the smaller subspace, making our method scalable to large datasets. We demonstrate the applicability of our method on an 18-dimensional benchmark data set for an inverse dynamics problem.
</details></li>
</ul>
<hr>
<h2 id="Graph-Matching-via-convex-relaxation-to-the-simplex"><a href="#Graph-Matching-via-convex-relaxation-to-the-simplex" class="headerlink" title="Graph Matching via convex relaxation to the simplex"></a>Graph Matching via convex relaxation to the simplex</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20609">http://arxiv.org/abs/2310.20609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernesto Araya Valdivia, Hemant Tyagi</li>
<li>For: 该论文解决了图像匹配问题，即输入两个图像的最佳对齐问题，该问题在计算机视觉、网络匿名化和蛋白质对齐等领域都有广泛的应用。* Methods: 该论文提出了一种新的对体矩阵的凸 relaxation，并开发了一种高效的镜像下降算法来解决该问题。在某些特定的 Gaussian Wigner 模型下，我们证明了对体矩阵relaxation的解是唯一的，并且在无噪 случа子下能够精确地回归真实的Permutation。* Results: 在无噪 случа子下，我们证明了镜像下降算法可以在一步内回归真实的Permutation，并且在某些特定的输入矩阵下，我们可以获得更好的回归结果，比如使用镜像下降算法可以在一步内回归true Permutation。<details>
<summary>Abstract</summary>
This paper addresses the Graph Matching problem, which consists of finding the best possible alignment between two input graphs, and has many applications in computer vision, network deanonymization and protein alignment. A common approach to tackle this problem is through convex relaxations of the NP-hard \emph{Quadratic Assignment Problem} (QAP).   Here, we introduce a new convex relaxation onto the unit simplex and develop an efficient mirror descent scheme with closed-form iterations for solving this problem. Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this is shown to imply exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used `diagonal dominance' condition. We use this condition to show exact one-step recovery of the ground truth (holding almost surely) via the mirror descent scheme, in the noiseless setting. We also use this condition to obtain significantly improved conditions for the GRAMPA algorithm [Fan et al. 2019] in the noiseless setting.
</details>
<details>
<summary>摘要</summary>
Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this implies exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used "diagonal dominance" condition. We use this condition to show exact one-step recovery of the ground truth (holding almost surely) via the mirror descent scheme in the noiseless setting. We also use this condition to obtain significantly improved conditions for the GRAMPA algorithm [Fan et al. 2019] in the noiseless setting.文章关注图像匹配问题，该问题的目标是找到两个输入图的最佳对应，并有广泛的应用于计算机视觉、网络匿名化和蛋白Alignment等领域。通常通过NP困难的quadratic assignment problem（QAP）的凸relaxation来解决该问题。在本文中，我们引入了一个新的凸relaxation onto the unit simplex，并开发了一种高效的mirror descent scheme with closed-form iterations来解决该问题。在相关的 Gaussian Wigner模型下，我们证明了unit simplex relaxation admits a unique solution with high probability。在无噪case，这imply exact recovery of the ground truth permutation。此外，我们还设立了一个新的sufficiency condition for the input matrix in standard greedy rounding methods，这个condition是less restrictive than the commonly used "diagonal dominance" condition。我们使用这个condition来show exact one-step recovery of the ground truth (holding almost surely) via the mirror descent scheme in the noiseless setting。此外，我们还使用这个condition来 obtain significantly improved conditions for the GRAMPA algorithm [Fan et al. 2019] in the noiseless setting。
</details></li>
</ul>
<hr>
<h2 id="Online-Conversion-with-Switching-Costs-Robust-and-Learning-Augmented-Algorithms"><a href="#Online-Conversion-with-Switching-Costs-Robust-and-Learning-Augmented-Algorithms" class="headerlink" title="Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms"></a>Online Conversion with Switching Costs: Robust and Learning-Augmented Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20598">http://arxiv.org/abs/2310.20598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</li>
<li>for: 这个论文研究了在互联网上进行转换的成本，具体来说是在一段时间 horizon 内，一个在线玩家尝试在每个时间步骤上购买（或者卖出）一个资产的剩余部分，并且每次决定时都会付出一个转换成本。</li>
<li>methods: 作者提出了一种基于阈值的在线算法，可以在 deterministic 上实现最优性，并且在某些情况下可以通过学习扩展来提高性能。</li>
<li>results: 作者通过一个碳素识别的电动车充电场景来实验研究，并证明了他们的算法可以相比基准方法具有显著改善。<details>
<summary>Abstract</summary>
We introduce and study online conversion with switching costs, a family of online problems that capture emerging problems at the intersection of energy and sustainability. In this problem, an online player attempts to purchase (alternatively, sell) fractional shares of an asset during a fixed time horizon with length $T$. At each time step, a cost function (alternatively, price function) is revealed, and the player must irrevocably decide an amount of asset to convert. The player also incurs a switching cost whenever their decision changes in consecutive time steps, i.e., when they increase or decrease their purchasing amount. We introduce competitive (robust) threshold-based algorithms for both the minimization and maximization variants of this problem, and show they are optimal among deterministic online algorithms. We then propose learning-augmented algorithms that take advantage of untrusted black-box advice (such as predictions from a machine learning model) to achieve significantly better average-case performance without sacrificing worst-case competitive guarantees. Finally, we empirically evaluate our proposed algorithms using a carbon-aware EV charging case study, showing that our algorithms substantially improve on baseline methods for this problem.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究在线转换问题，这是一种涉及到能源和可持续发展的新兴问题。在这个问题中，一名在线玩家尝试在固定时间Interval $[T]$ 内购买（或销售）资产的分数部分。在每个时间步骤中，一个成本函数（或价格函数）会被公布，玩家必须不可逆决定一定量的资产转换。在连续的两个时间步骤中，如果玩家的决定发生变化，例如增加或减少购买量，就会产生交易成本。我们提出了竞争（可靠）阈值基于算法，以便在排序算法中实现最佳性。然后，我们提出了学习增强算法，可以利用不可信的黑盒建议（如机器学习模型的预测）来实现较好的平均情况性能而无需牺牲最坏情况竞争保证。最后，我们employs empirical evaluation using a carbon-aware EV charging case study, showing that our algorithms significantly improve on baseline methods for this problem.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Pre-trained-Language-Models-for-Offline-Reinforcement-Learning"><a href="#Unleashing-the-Power-of-Pre-trained-Language-Models-for-Offline-Reinforcement-Learning" class="headerlink" title="Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning"></a>Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20587">http://arxiv.org/abs/2310.20587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, Huazhe Xu</li>
<li>For: The paper is focused on developing a new framework called Language Models for Motion Control (LaMo) that leverages pre-trained language models (LMs) to improve offline reinforcement learning (RL) in real-world scenarios where data collection is limited.* Methods: The LaMo framework uses pre-trained LMs to initialize Decision Transformers, which are then fine-tuned using the LoRA method to combine pre-trained knowledge and in-domain knowledge. The framework also employs a non-linear MLP transformation to generate embeddings and integrates an auxiliary language prediction loss during fine-tuning to stabilize the LMs.* Results: The paper demonstrates that LaMo achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks, particularly in scenarios with limited data samples.Here’s the information in Simplified Chinese:* For: 该论文目标是提出一种基于语言模型（LM）的无线 Reinforcement Learning（RL）框架，以便在实际场景中，充分利用预收集的数据来实现更好的控制。* Methods: 该框架使用预训练的 LM 初始化决策变换器，并使用 LoRA 方法来有效地结合预训练知识和地区知识。框架还使用非线性 MLP 变换来生成嵌入，并在精度优化中添加语言预测损失以稳定 LM。* Results: 论文表明，LaMo 在稀有奖励任务中达到了状态的最佳性能，并在具有限制数据amples的情况下追赶到了值基于 offline RL 方法和决策变换器的性能。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate $\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples. Our project website is https://lamo2023.github.io
</details>
<details>
<summary>摘要</summary>
偏向学习（Offline Reinforcement Learning）的目标是找到近似优质策略，使用预收集的数据进行训练。在现实世界中，数据收集可能是成本高昂且危险的，因此偏向学习在域内数据有限的情况下变得特别挑战。基于最新的大语言模型（Large Language Models，LLMs）和它们的几shot学习能力，本文引入了Language Models for Motion Control（LaMo）框架，用于有效地使用预训练的语言模型（LMs） для偏向学习。我们的框架包括四个关键组成部分：1. 使用顺序预训练的语言模型初始化决策变换器。2. 使用LoRA fine-tuning方法，而不是全量 fine-tuning，将预训练知识从语言模型和域内知识相结合。3. 使用非线性多层Perceptron变换器而不是线性投影，生成嵌入。4. 在精度uning过程中添加语言预测损失，以稳定语言模型并保持它们的原始语言能力。实验结果表明，LaMo在稀缺奖励任务中达到了状态机器人学习的最佳性能，并在 dense-reward 任务中追caught up with Decision Transformers。尤其是在数据样本有限的情况下，LaMo表现出色。更多信息可以查看我们的项目网站：<https://lamo2023.github.io>
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Gradient-Descent-for-Gaussian-Processes-Done-Right"><a href="#Stochastic-Gradient-Descent-for-Gaussian-Processes-Done-Right" class="headerlink" title="Stochastic Gradient Descent for Gaussian Processes Done Right"></a>Stochastic Gradient Descent for Gaussian Processes Done Right</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20581">http://arxiv.org/abs/2310.20581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihao Andreas Lin, Shreyas Padhy, Javier Antorán, Austin Tripp, Alexander Terenin, Csaba Szepesvári, José Miguel Hernández-Lobato, David Janz</li>
<li>for: 本文关注的问题是 Gaussian process regression 的优化问题，使用平方损失函数。</li>
<li>methods: 本文使用的方法包括 Stochastic Dual Gradient Descent 算法，以及特定的设计选择，如减少维度版本的问题。</li>
<li>results: 本文的实验结果表明，当使用特定的设计选择和优化策略时，Stochastic Dual Gradient Descent 算法可以高效地解决 Gaussian process regression 的优化问题，并在标准回归benchmark和 Bayesian 优化任务中表现出色，与前置 conjugate gradients、variational Gaussian process approximations 和之前的 Stochastic Gradient Descent 算法相比。在一个分子绑定亲和力预测任务中，本文的方法使 Gaussian process regression 与STATE-OF-THE-ART graph neural networks 的性能相当。<details>
<summary>Abstract</summary>
We study the optimisation problem associated with Gaussian process regression using squared loss. The most common approach to this problem is to apply an exact solver, such as conjugate gradient descent, either directly, or to a reduced-order version of the problem. Recently, driven by successes in deep learning, stochastic gradient descent has gained traction as an alternative. In this paper, we show that when done right$\unicode{x2014}$by which we mean using specific insights from the optimisation and kernel communities$\unicode{x2014}$this approach is highly effective. We thus introduce a particular stochastic dual gradient descent algorithm, that may be implemented with a few lines of code using any deep learning framework. We explain our design decisions by illustrating their advantage against alternatives with ablation studies and show that the new method is highly competitive. Our evaluations on standard regression benchmarks and a Bayesian optimisation task set our approach apart from preconditioned conjugate gradients, variational Gaussian process approximations, and a previous version of stochastic gradient descent for Gaussian processes. On a molecular binding affinity prediction task, our method places Gaussian process regression on par in terms of performance with state-of-the-art graph neural networks.
</details>
<details>
<summary>摘要</summary>
我们研究了 Gaussian process regression 的优化问题，使用平方损失函数。最常见的方法是使用精确解算法，如 conjugate gradient descent，直接或将问题缩放到更少的维度上进行解决。在这篇文章中，我们表明了使用 deep learning 的成功吸引了人们的注意力，并且在我们的方法中使用了特定的优化和核心社区的知识，从而在解决方法中获得了高效性。我们提出了一种特定的随机对偶梯度下降算法，可以使用任何深度学习框架进行实现，并且我们解释了我们的设计决策，并通过缺失研究和比较分析表明了它们的优势。我们的评估结果表明，我们的方法在标准的回归测试集和 Bayesian 优化任务上与 préconditioned conjugate gradients、variational Gaussian process approximation 和之前的随机梯度下降方法相比，具有更高的竞争力。在一个分子绑定亲和力预测任务上，我们的方法将 Gaussian process regression 与状态之前的 graph neural networks 的性能均衡。
</details></li>
</ul>
<hr>
<h2 id="Initialization-Matters-Privacy-Utility-Analysis-of-Overparameterized-Neural-Networks"><a href="#Initialization-Matters-Privacy-Utility-Analysis-of-Overparameterized-Neural-Networks" class="headerlink" title="Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks"></a>Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20579">http://arxiv.org/abs/2310.20579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Ye, Zhenyu Zhu, Fanghui Liu, Reza Shokri, Volkan Cevher</li>
<li>for: 本研究探讨随机机器学习算法中模型过参数化对训练数据信息泄露的影响。</li>
<li>methods: 研究者利用了Randomized Matrix Method和Expected Gradient Method来分析模型的隐私泄露。</li>
<li>results: 研究者发现，模型的深度会影响隐私泄露的规模，并且初始化分布的选择会影响模型的隐私性。 Specifically, for the special setting of linearized network, the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution.<details>
<summary>Abstract</summary>
We analytically investigate how over-parameterization of models in randomized machine learning algorithms impacts the information leakage about their training data. Specifically, we prove a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on the initialization, width, and depth of fully connected neural networks. We find that this KL privacy bound is largely determined by the expected squared gradient norm relative to model parameters during training. Notably, for the special setting of linearized network, our analysis indicates that the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution. By using this analysis, we demonstrate that privacy bound improves with increasing depth under certain initializations (LeCun and Xavier), while degrades with increasing depth under other initializations (He and NTK). Our work reveals a complex interplay between privacy and depth that depends on the chosen initialization distribution. We further prove excess empirical risk bounds under a fixed KL privacy budget, and show that the interplay between privacy utility trade-off and depth is similarly affected by the initialization.
</details>
<details>
<summary>摘要</summary>
我们分析了随机机器学习算法中模型过参数化对训练数据信息泄露的影响。我们证明了一个隐私约束 для KL散度 между模型分布在最差邻居数据集上，并研究了其与初始化、宽度和深度相关。我们发现这个 KL 隐私约束主要取决于在训练过程中模型参数的预期平方Gradient norm。特别是在特殊设置下（线性化网络），我们的分析表明，预期平方Gradient norm（以及因此隐私损失的增长）与初始化分布的每层方差直接相关。通过这种分析，我们示出了在某些初始化情况下（如LeCun和Xavier），隐私约束随着深度增长而改善，而在其他初始化情况下（如He和NTK），隐私约束随着深度增长而下降。我们的工作揭示了depth和初始化distribution之间的复杂互动，以及隐私和深度之间的负面相互作用。此外，我们还证明了固定 KL 隐私预算下的过项预测误差上限，并显示了隐私利用均衡和深度之间的相互作用。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretic-Trust-Regions-for-Stochastic-Gradient-Based-Optimization"><a href="#Information-Theoretic-Trust-Regions-for-Stochastic-Gradient-Based-Optimization" class="headerlink" title="Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization"></a>Information-Theoretic Trust Regions for Stochastic Gradient-Based Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20574">http://arxiv.org/abs/2310.20574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alrhub/arturo">https://github.com/alrhub/arturo</a></li>
<li>paper_authors: Philipp Dahlinger, Philipp Becker, Maximilian Hüttenrauch, Gerhard Neumann</li>
<li>for: 优化神经网络的梯度下降法是非常重要的。而现有的方法通过减小步长和方向的灵活调整，这些方法是尝试性的，而不是理论基础的。这种方法可以通过对目标函数的辛勤约束来改善优化器。</li>
<li>methods: 我们提议使用信息理论信任区优化（arTuRO），它使用神经网络参数的 Gaussian 分布，使用 Kullback-Leibler 偏度信任区来实现更稳定和更快的优化过程。在每次更新之前，它解决了一个优化问题，以获得最佳步长。</li>
<li>results: 我们表明，arTuRO 可以结合适应式 momentum-based 优化的快速收敛和神经网络优化的通用性。<details>
<summary>Abstract</summary>
Stochastic gradient-based optimization is crucial to optimize neural networks. While popular approaches heuristically adapt the step size and direction by rescaling gradients, a more principled approach to improve optimizers requires second-order information. Such methods precondition the gradient using the objective's Hessian. Yet, computing the Hessian is usually expensive and effectively using second-order information in the stochastic gradient setting is non-trivial. We propose using Information-Theoretic Trust Region Optimization (arTuRO) for improved updates with uncertain second-order information. By modeling the network parameters as a Gaussian distribution and using a Kullback-Leibler divergence-based trust region, our approach takes bounded steps accounting for the objective's curvature and uncertainty in the parameters. Before each update, it solves the trust region problem for an optimal step size, resulting in a more stable and faster optimization process. We approximate the diagonal elements of the Hessian from stochastic gradients using a simple recursive least squares approach, constructing a model of the expected Hessian over time using only first-order information. We show that arTuRO combines the fast convergence of adaptive moment-based optimization with the generalization capabilities of SGD.
</details>
<details>
<summary>摘要</summary>
“Stochastic gradient-based优化是神经网络优化的关键。各种方法尝试通过缩放梯度来适应步长和方向，但是这些方法并不是非常原理性的。实际上，使用目标函数的Hessian来预condition gradients是一种更原理性的方法。然而，计算Hessian通常是非常昂贵的，而且在梯度计算中使用第二个信息在随机 gradient  Setting 中是非常困难的。我们提出了基于信息理论的信任区域优化（arTuRO），它可以在随机 gradient  Setting 中提供更好的更新，并考虑到目标函数的凹陷和参数的不确定性。在每次更新之前，arTuRO 会解决一个信任区域问题，以获得最佳步长，从而使优化过程更加稳定和快速。我们使用梯度的权重来 aproximate Hessian 的对角元素，并使用一种简单的回归最小二乘方法来构建一个时间上的 Hessian 模型。我们表明，arTuRO 可以结合适应型 momentum 的优化和 SGD 的泛化能力。”
</details></li>
</ul>
<hr>
<h2 id="One-shot-backpropagation-for-multi-step-prediction-in-physics-based-system-identification"><a href="#One-shot-backpropagation-for-multi-step-prediction-in-physics-based-system-identification" class="headerlink" title="One-shot backpropagation for multi-step prediction in physics-based system identification"></a>One-shot backpropagation for multi-step prediction in physics-based system identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20567">http://arxiv.org/abs/2310.20567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesare Donati, Martina Mammarella, Fabrizio Dabbene, Carlo Novara, Constantino Lagoa</li>
<li>for: 这篇论文旨在提出一种全新的可以保持物理性质的多步预测系统识别框架。</li>
<li>methods: 该方法使用了分析性的回潘迪法计算多步损失函数的衰变，从而提供物理和结构直观到学习算法中。</li>
<li>results: 作为一个案例研究，该方法在估算空间废弃物的拟合矩阵上进行了测试，并取得了高准确率和物理意义的结果。<details>
<summary>Abstract</summary>
The aim of this paper is to present a novel general framework for the identification of possibly interconnected systems, while preserving their physical properties and providing accuracy in multi-step prediction. An analytical and recursive algorithm for the gradient computation of the multi-step loss function based on backpropagation is introduced, providing physical and structural insight directly into the learning algorithm. As a case study, the proposed approach is tested for estimating the inertia matrix of a space debris starting from state observations.
</details>
<details>
<summary>摘要</summary>
本文的目的是提出一种新的总体框架，用于可能相互连接的系统的特征标识，同时保持物理性质和多步预测的准确性。我们提出了一种分析和递归的梯度计算方法，基于反射学习，以直接将物理和结构性质反映到学习算法中。为了实践，我们对Space debris的惯性矩进行估算，并从状态观察结果进行测试。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-design-of-graph-neural-networks-with-applications-to-vertical-federated-learning"><a href="#Privacy-preserving-design-of-graph-neural-networks-with-applications-to-vertical-federated-learning" class="headerlink" title="Privacy-preserving design of graph neural networks with applications to vertical federated learning"></a>Privacy-preserving design of graph neural networks with applications to vertical federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20552">http://arxiv.org/abs/2310.20552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruofan Wu, Mingyang Zhang, Lingjuan Lyu, Xiaolong Xu, Xiuquan Hao, Xinyi Fu, Tengfei Liu, Tianyi Zhang, Weiqiang Wang</li>
<li>for: 这篇 paper 的目的是提出一个基于 Vertical Federated Learning (VFL) 的数据加权学习框架，以实现金融风险管理 (FRM) 应用中的高性能。</li>
<li>methods: 这篇 paper 使用了一种称为 perturbed message passing (PMP) 的通用隐私保证方案，并将其应用于多个受欢迎的图 neural network 架构中。</li>
<li>results: 这篇 paper 的实验结果显示，VESPER 能够在理想的隐私预算下训练高性能的图神经网络模型，并且在实际应用中也能够实现高性能。<details>
<summary>Abstract</summary>
The paradigm of vertical federated learning (VFL), where institutions collaboratively train machine learning models via combining each other's local feature or label information, has achieved great success in applications to financial risk management (FRM). The surging developments of graph representation learning (GRL) have opened up new opportunities for FRM applications under FL via efficiently utilizing the graph-structured data generated from underlying transaction networks. Meanwhile, transaction information is often considered highly sensitive. To prevent data leakage during training, it is critical to develop FL protocols with formal privacy guarantees. In this paper, we present an end-to-end GRL framework in the VFL setting called VESPER, which is built upon a general privatization scheme termed perturbed message passing (PMP) that allows the privatization of many popular graph neural architectures.Based on PMP, we discuss the strengths and weaknesses of specific design choices of concrete graph neural architectures and provide solutions and improvements for both dense and sparse graphs. Extensive empirical evaluations over both public datasets and an industry dataset demonstrate that VESPER is capable of training high-performance GNN models over both sparse and dense graphs under reasonable privacy budgets.
</details>
<details>
<summary>摘要</summary>
Vertical Federated Learning (VFL) 的 paradigm, where institutions collaboratively train machine learning models by combining each other's local feature or label information, 在金融风险管理 (FRM) 应用中取得了很大成功。随着图表学习 (GRL) 的发展，开创了新的FRM应用场景，通过高效地利用基于交易网络生成的图structured data。然而，交易信息经常被视为高度敏感。为了防止训练过程中的数据泄露，在FL中发展 Privacy 保障的协议是 kritical。本文提出了一种基于杂化消息传递 (PMP) 的综合隐私化框架，称为 VESPER，可以隐私化多种流行的图神经网络架构。基于 PMP，我们讨论了特定设计选择的强点和弱点，并提供了对 both dense 和稀疏图的解决方案。我们进行了广泛的实验评估，证明 VESPER 可以在理想的隐私预算下训练高性能 GNN 模型 both dense 和稀疏图。
</details></li>
</ul>
<hr>
<h2 id="Multi-task-learning-of-convex-combinations-of-forecasting-models"><a href="#Multi-task-learning-of-convex-combinations-of-forecasting-models" class="headerlink" title="Multi-task learning of convex combinations of forecasting models"></a>Multi-task learning of convex combinations of forecasting models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20545">http://arxiv.org/abs/2310.20545</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoniosudoso/mtl-comb">https://github.com/antoniosudoso/mtl-comb</a></li>
<li>paper_authors: Giovanni Felici, Antonio M. Sudoso</li>
<li>for: 这篇论文旨在实现多预测模型的结合，以提高预测精度。</li>
<li>methods: 本文提出了一个基于多任务学习的方法，通过一个深度神经网络，分别执行预测模型选择和预测模型结合两个任务。</li>
<li>results: 实验结果显示，该方法可以对大量的时间序列数据进行改进预测，并且比前一代方法更高的精度。<details>
<summary>Abstract</summary>
Forecast combination involves using multiple forecasts to create a single, more accurate prediction. Recently, feature-based forecasting has been employed to either select the most appropriate forecasting models or to learn the weights of their convex combination. In this paper, we present a multi-task learning methodology that simultaneously addresses both problems. This approach is implemented through a deep neural network with two branches: the regression branch, which learns the weights of various forecasting methods by minimizing the error of combined forecasts, and the classification branch, which selects forecasting methods with an emphasis on their diversity. To generate training labels for the classification task, we introduce an optimization-driven approach that identifies the most appropriate methods for a given time series. The proposed approach elicits the essential role of diversity in feature-based forecasting and highlights the interplay between model combination and model selection when learning forecasting ensembles. Experimental results on a large set of series from the M4 competition dataset show that our proposal enhances point forecast accuracy compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
预测组合是使用多个预测来创建一个更准确的预测。最近，基于特征的预测组合被应用于选择最合适的预测模型或学习预测模型的权重。在这篇论文中，我们提出了一种多任务学习方法，同时解决了这两个问题。这种方法通过一个深度神经网络，其中有两个分支：回归分支，通过最小化组合预测错误来学习预测模型的权重；分类分支，通过强调多样性来选择预测模型。为生成训练标签的分类任务，我们提出了一种优化驱动的方法，可以为给定时间序列选择最合适的预测方法。我们的方法强调了特征基于预测组合中的多样性，并高亮了组合预测模型和选择预测模型在学习预测集中的互动关系。实验结果表明，我们的提议在M4竞赛数据集上实现了点预测精度的提高，比现有方法更高。
</details></li>
</ul>
<hr>
<h2 id="Group-Feature-Sensor-Selection-With-Controlled-Redundancy-Using-Neural-Networks"><a href="#Group-Feature-Sensor-Selection-With-Controlled-Redundancy-Using-Neural-Networks" class="headerlink" title="Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural Networks"></a>Group-Feature (Sensor) Selection With Controlled Redundancy Using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20524">http://arxiv.org/abs/2310.20524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aytijhya Saha, Nikhil R. Pal</li>
<li>for: 这个论文旨在提出一种基于多层感知网络的嵌入特征选择方法，并将其推广到组特征或感知器选择问题，以控制选择的特征之间的重复率。</li>
<li>methods: 该方法使用了多层感知网络和一种普适的梯度下降算法，并将 GROUP LASSO 罚项推广为选择有价值的组特征，同时保持特征之间的重复率控制。</li>
<li>results: 实验结果表明，提出的方法在一些标准数据集上具有优秀的表现，并且在特征选择和组特征选择方面比一些现有方法更为有效。<details>
<summary>Abstract</summary>
In this paper, we present a novel embedded feature selection method based on a Multi-layer Perceptron (MLP) network and generalize it for group-feature or sensor selection problems, which can control the level of redundancy among the selected features or groups. Additionally, we have generalized the group lasso penalty for feature selection to encompass a mechanism for selecting valuable group features while simultaneously maintaining a control over redundancy. We establish the monotonicity and convergence of the proposed algorithm, with a smoothed version of the penalty terms, under suitable assumptions. Experimental results on several benchmark datasets demonstrate the promising performance of the proposed methodology for both feature selection and group feature selection over some state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于多层感知网络（MLP）的嵌入式特征选择方法，并推广到组特征或感知器选择问题，可以控制选择的特征或组中的重复性水平。此外，我们扩展了组lasso penalty的特征选择机制，以同时选择价值的组特征，并保持重复性控制。我们证明了提案的算法假设下的卷积和满足条件下的升oothed penalty term的减少性和收敛性。实验结果表明，提案的方法在多个标准数据集上表现出色，在特征选择和组特征选择方面超过了一些现有方法。
</details></li>
</ul>
<hr>
<h2 id="Parametric-Fairness-with-Statistical-Guarantees"><a href="#Parametric-Fairness-with-Statistical-Guarantees" class="headerlink" title="Parametric Fairness with Statistical Guarantees"></a>Parametric Fairness with Statistical Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20508">http://arxiv.org/abs/2310.20508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paramfair/submission_974">https://github.com/paramfair/submission_974</a></li>
<li>paper_authors: François HU, Philipp Ratz, Arthur Charpentier</li>
<li>for: This paper focuses on improving algorithmic fairness in machine learning models by incorporating domain knowledge and addressing issues related to intersectional fairness.</li>
<li>methods: The paper introduces a new metric called Distributional Demographic Parity (DDP) that incorporates distributional properties in the predictions, allowing for the use of expert knowledge in the fair solution. The paper also develops a parametric method that efficiently addresses practical challenges like limited training data and constraints on total spending.</li>
<li>results: The paper demonstrates the effectiveness of the proposed method through a practical example of wages and shows that it can provide a more robust solution for real-life applications compared to traditional fairness metrics like Equalized Odds and Demographic Parity.<details>
<summary>Abstract</summary>
Algorithmic fairness has gained prominence due to societal and regulatory concerns about biases in Machine Learning models. Common group fairness metrics like Equalized Odds for classification or Demographic Parity for both classification and regression are widely used and a host of computationally advantageous post-processing methods have been developed around them. However, these metrics often limit users from incorporating domain knowledge. Despite meeting traditional fairness criteria, they can obscure issues related to intersectional fairness and even replicate unwanted intra-group biases in the resulting fair solution. To avoid this narrow perspective, we extend the concept of Demographic Parity to incorporate distributional properties in the predictions, allowing expert knowledge to be used in the fair solution. We illustrate the use of this new metric through a practical example of wages, and develop a parametric method that efficiently addresses practical challenges like limited training data and constraints on total spending, offering a robust solution for real-life applications.
</details>
<details>
<summary>摘要</summary>
“算法公平”在社会和 regulatory 问题中得到了更多的注意，因为机器学习模型中的偏见问题。常见的集体公平度量如“平等概率”和“人口平衡”在分类和回归方面都广泛使用，但这些度量通常会限制用户不能够考虑专家知识。尽管遵循传统的公平准则，它们可能会隐藏 intersectional 公平问题，甚至在对内部偏见的解决方案中重复不良的内部偏见。为了避免这种狭隘的视野，我们将“人口平衡”概念扩展到预测中的分布性质，以便使用专家知识。我们透过一个实际的薪资例子来显示这个新的度量，并开发了一个可效的 parametric 方法来解决实际应用中的实际挑战，如有限的训练数据和总预算的限制，提供了一个坚固的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Generative-Learning-of-Continuous-Data-by-Tensor-Networks"><a href="#Generative-Learning-of-Continuous-Data-by-Tensor-Networks" class="headerlink" title="Generative Learning of Continuous Data by Tensor Networks"></a>Generative Learning of Continuous Data by Tensor Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20498">http://arxiv.org/abs/2310.20498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Meiburg, Jing Chen, Jacob Miller, Raphaëlle Tihon, Guillaume Rabusseau, Alejandro Perdomo-Ortiz</li>
<li>for:  solves machine learning problems, especially unsupervised generative learning</li>
<li>methods:  uses tensor network generative models for continuous data, with a new family of models based on matrix product states</li>
<li>results:  the model can approximate any reasonably smooth probability density function with arbitrary precision, and performs well on synthetic and real-world datasets with both continuous and discrete variables.<details>
<summary>Abstract</summary>
Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model learns and generalizes well on distributions of continuous and discrete variables. We develop methods for modeling different data domains, and introduce a trainable compression layer which is found to increase model performance given limited memory or computational resources. Overall, our methods give important theoretical and empirical evidence of the efficacy of quantum-inspired methods for the rapidly growing field of generative learning.
</details>
<details>
<summary>摘要</summary>
对于模型多体量子系统的起源，tensor network已经发展成为解决机器学习问题的有力的模型，尤其是不监督生成学习。这些量子感应的特性具有许多权威的特点，但是它们之前只能用于二进制或分类数据，限制了它们在实际应用中的用途。我们在matrix product states中引入了一新的家族tensor network生成模型，可以从包含连续随机变量的分布中学习。我们首先证明这个模型家族可以对任何合理平滑概率密度函数进行拟合，然后在几个 sintetic和实际数据集上进行了实验，发现这个模型可以从不同的数据域中学习和推导。此外，我们还开发了可调弹性压缩层，对于有限的内存或计算资源，可以提高模型的性能。总的来说，我们的方法给出了量子感应方法在快速增长的生成学习领域中的重要理论和实验证据。
</details></li>
</ul>
<hr>
<h2 id="BasisFormer-Attention-based-Time-Series-Forecasting-with-Learnable-and-Interpretable-Basis"><a href="#BasisFormer-Attention-based-Time-Series-Forecasting-with-Learnable-and-Interpretable-Basis" class="headerlink" title="BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis"></a>BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20496">http://arxiv.org/abs/2310.20496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nzl5116190/basisformer">https://github.com/nzl5116190/basisformer</a></li>
<li>paper_authors: Zelin Ni, Hang Yu, Shizhan Liu, Jianguo Li, Weiyao Lin</li>
<li>for: 这个研究旨在提出一个可学习且可解释的基底架构，以便实现时间序列预测的精确预测。</li>
<li>methods: 这个架构包括三个部分：首先，通过自适应式无监督学习，获取基底，并运用对比学习。接着，我们设计了一个 Coef 模组，通过两向交叉注意力计算时间序列和基底之间的相似度。最后，我们呈现了一个预测模组，根据相似度选择和聚合基底，实现精确的未来预测。</li>
<li>results: 经过六个数据集的广泛实验，我们证明了 BasisFormer 比前一代方法提高了11.04%和15.78%的精确预测性。代码可以在：\url{<a target="_blank" rel="noopener" href="https://github.com/nzl5116190/Basisformer%7D">https://github.com/nzl5116190/Basisformer}</a>  obtain.<details>
<summary>Abstract</summary>
Bases have become an integral part of modern deep learning-based models for time series forecasting due to their ability to act as feature extractors or future references. To be effective, a basis must be tailored to the specific set of time series data and exhibit distinct correlation with each time series within the set. However, current state-of-the-art methods are limited in their ability to satisfy both of these requirements simultaneously. To address this challenge, we propose BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. This architecture comprises three components: First, we acquire bases through adaptive self-supervised learning, which treats the historical and future sections of the time series as two distinct views and employs contrastive learning. Next, we design a Coef module that calculates the similarity coefficients between the time series and bases in the historical view via bidirectional cross-attention. Finally, we present a Forecast module that selects and consolidates the bases in the future view based on the similarity coefficients, resulting in accurate future predictions. Through extensive experiments on six datasets, we demonstrate that BasisFormer outperforms previous state-of-the-art methods by 11.04\% and 15.78\% respectively for univariate and multivariate forecasting tasks. Code is available at: \url{https://github.com/nzl5116190/Basisformer}
</details>
<details>
<summary>摘要</summary>
基于现代深度学习模型的时间序列预测中，基因已成为一个重要的组成部分，因为它们可以作为特征提取器或未来参照。为了有效，一个基因必须适应特定的时间序列数据集，并且与每个时间序列之间显示明显的相关性。然而，当前状态艺术方法受限于同时满足这两个需求的能力。为解决这个挑战，我们提出了 BasisFormer，一种终端时间序列预测架构，它利用可学习和可解释的基因。这个架构包括以下三个组成部分：1. 我们通过适应性自我超vised学习获取基因，将历史和未来部分视为两个不同的视图，并使用对比学习。2. 我们设计了 Coef模块，通过双向跨注意力对时间序列和基因之间进行对比，以计算时间序列和基因之间的相似度系数。3. 我们提出了 Forecast模块，根据相似度系数选择和组合未来视图中的基因，以实现准确的未来预测。通过对六个数据集进行广泛的实验，我们证明了 BasisFormer可以与之前的状态艺术方法相比，在单变量和多变量预测任务中提高11.04%和15.78%的性能。代码可以在以下链接获取：\url{https://github.com/nzl5116190/Basisformer}
</details></li>
</ul>
<hr>
<h2 id="Requirement-falsification-for-cyber-physical-systems-using-generative-models"><a href="#Requirement-falsification-for-cyber-physical-systems-using-generative-models" class="headerlink" title="Requirement falsification for cyber-physical systems using generative models"></a>Requirement falsification for cyber-physical systems using generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20493">http://arxiv.org/abs/2310.20493</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mshaheryarmalik/stgem">https://github.com/mshaheryarmalik/stgem</a></li>
<li>paper_authors: Jarkko Peltomäki, Ivan Porres</li>
<li>for:  automatic requirement falsification of cyber-physical systems</li>
<li>methods:  uses OGAN algorithm, which is a generative machine learning model to produce counterexamples</li>
<li>results:  can find inputs that are counterexamples for the safety of a system, revealing design, software, or hardware defects before the system is taken into operation, and exhibits state-of-the-art CPS falsification efficiency and effectiveness.<details>
<summary>Abstract</summary>
We present the OGAN algorithm for automatic requirement falsification of cyber-physical systems. System inputs and output are represented as piecewise constant signals over time while requirements are expressed in signal temporal logic. OGAN can find inputs that are counterexamples for the safety of a system revealing design, software, or hardware defects before the system is taken into operation. The OGAN algorithm works by training a generative machine learning model to produce such counterexamples. It executes tests atomically and does not require any previous model of the system under test. We evaluate OGAN using the ARCH-COMP benchmark problems, and the experimental results show that generative models are a viable method for requirement falsification. OGAN can be applied to new systems with little effort, has few requirements for the system under test, and exhibits state-of-the-art CPS falsification efficiency and effectiveness.
</details>
<details>
<summary>摘要</summary>
我们介绍了OGAN算法，用于自动发现游戏物理系统的需求虚拟。系统输入和输出被表示为时间上的分割定律信号，需求则以信号时间逻辑表示。OGAN可以找到系统的安全性缺陷，包括设计、软件或硬件的缺陷，以前设不可能找到。OGAN算法通过将生成机器学习模型用于生成这些Counterexample。它执行测试原子地，不需要任何关于系统下测试的先前模型。我们使用ARCH-COMP问题集进行评估，实验结果显示，生成模型是可靠的需求虚拟方法。OGAN可以对新的系统进行快速适用，具有少量的系统下测试需求，并且表现出了顶尖的CPS虚拟效率和有效性。
</details></li>
</ul>
<hr>
<h2 id="Log-based-Anomaly-Detection-of-Enterprise-Software-An-Empirical-Study"><a href="#Log-based-Anomaly-Detection-of-Enterprise-Software-An-Empirical-Study" class="headerlink" title="Log-based Anomaly Detection of Enterprise Software: An Empirical Study"></a>Log-based Anomaly Detection of Enterprise Software: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20492">http://arxiv.org/abs/2310.20492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadun Wijesinghe, Hadi Hemmati</li>
<li>for: 本研究旨在评估多种现有的异常检测模型在工业 dataset 上的表现，以及这些模型对不同类型异常的检测能力。</li>
<li>methods: 本研究使用了多种深度学习模型，包括 Long-Short Term Memory 和 Transformer 模型，进行日志数据中的异常检测。</li>
<li>results: 研究发现，不同的模型在不同类型异常中的检测能力有所不同，而且模型在 less-structured 数据集上的表现较好。 另外，通过移除一些常见的数据泄露，研究发现模型的效果会改善。<details>
<summary>Abstract</summary>
Most enterprise applications use logging as a mechanism to diagnose anomalies, which could help with reducing system downtime. Anomaly detection using software execution logs has been explored in several prior studies, using both classical and deep neural network-based machine learning models. In recent years, the research has largely focused in using variations of sequence-based deep neural networks (e.g., Long-Short Term Memory and Transformer-based models) for log-based anomaly detection on open-source data. However, they have not been applied in industrial datasets, as often. In addition, the studied open-source datasets are typically very large in size with logging statements that do not change much over time, which may not be the case with a dataset from an industrial service that is relatively new. In this paper, we evaluate several state-of-the-art anomaly detection models on an industrial dataset from our research partner, which is much smaller and loosely structured than most large scale open-source benchmark datasets. Results show that while all models are capable of detecting anomalies, certain models are better suited for less-structured datasets. We also see that model effectiveness changes when a common data leak associated with a random train-test split in some prior work is removed. A qualitative study of the defects' characteristics identified by the developers on the industrial dataset further shows strengths and weaknesses of the models in detecting different types of anomalies. Finally, we explore the effect of limited training data by gradually increasing the training set size, to evaluate if the model effectiveness does depend on the training set size.
</details>
<details>
<summary>摘要</summary>
大多数企业应用程序使用日志作为诊断异常的机制，以减少系统停机时间。异常检测使用软件执行日志已经在多个先前研究中进行了探讨，使用了古典和深度神经网络学习模型。在最近几年，研究主要集中在使用序列基本深度神经网络模型（如Long-Short Term Memory和Transformer-based模型）进行日志基本异常检测。然而，这些模型尚未在企业数据集中应用，因为企业数据集通常较小，不具备大规模开源数据集的特点。此外，研究使用的开源数据集通常很大，logging语句不变化很多，这可能不符合一个新的工业服务数据集。在这篇论文中，我们评估了一些当前领先的异常检测模型，在我们的研究合作伙伴提供的工业数据集上进行了评估。结果表明，虽然所有模型都能检测异常，但某些模型更适合不结构化数据集。我们还发现，模型的效果随着一种常见的数据泄露问题的消除而改变。在进行了开发者对工业数据集中异常的 качеitative研究后，我们发现了不同类型异常的特点。最后，我们研究了模型效果随着训练数据集大小的增加而变化。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Practitioner-Perspectives-On-Training-Data-Attribution-Explanations"><a href="#Exploring-Practitioner-Perspectives-On-Training-Data-Attribution-Explanations" class="headerlink" title="Exploring Practitioner Perspectives On Training Data Attribution Explanations"></a>Exploring Practitioner Perspectives On Training Data Attribution Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20477">http://arxiv.org/abs/2310.20477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elisa Nguyen, Evgenii Kortukov, Jean Song, Seong Joon Oh</li>
<li>for: This paper aims to understand the potential usefulness of training data attribution (TDA) explanations in Explainable AI (XAI) and to explore the design space of such an approach.</li>
<li>methods: The authors interviewed 10 practitioners to gather their insights on the importance of training data quality, their current practices for curating data, and their expectations for explanations in XAI.</li>
<li>results: The authors found that training data quality is a crucial factor for high model performance, and that model developers mainly rely on their own experience to curate data. End-users expect explanations to enhance their interaction with the model, but are not necessarily prioritizing TDA explanations. The authors also found that TDA explanations are not well-known and not widely used.Here are the same three points in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是了解培训数据拟合（TDA）解释在可解释AI（XAI）中的可能用途，以及这种方法的设计空间。</li>
<li>methods: 作者们采访了10名实践者，了解他们对培训数据质量的重要性，他们当前的数据拟合方法，以及他们对XAI中的解释的期望。</li>
<li>results: 作者们发现，培训数据质量对高度模型性能非常重要，而模型开发者主要通过自己的经验来拟合数据。用户希望解释能够增强与模型的互动，但并不一定优先考虑TDA解释。作者们还发现，TDA解释并不很知名，并不广泛使用。<details>
<summary>Abstract</summary>
Explainable AI (XAI) aims to provide insight into opaque model reasoning to humans and as such is an interdisciplinary field by nature. In this paper, we interviewed 10 practitioners to understand the possible usability of training data attribution (TDA) explanations and to explore the design space of such an approach. We confirmed that training data quality is often the most important factor for high model performance in practice and model developers mainly rely on their own experience to curate data. End-users expect explanations to enhance their interaction with the model and do not necessarily prioritise but are open to training data as a means of explanation. Within our participants, we found that TDA explanations are not well-known and therefore not used. We urge the community to focus on the utility of TDA techniques from the human-machine collaboration perspective and broaden the TDA evaluation to reflect common use cases in practice.
</details>
<details>
<summary>摘要</summary>
Explainable AI (XAI) 目的是为人类提供模型决策的理解，因此是一个横跨多个领域的领域。在这篇论文中，我们采访了10名实践者，以了解培训数据归因（TDA）解释的可用性，并探讨这种方法的设计空间。我们发现，在实践中，模型表现高的关键因素通常是培训数据质量，而模型开发者主要依靠自己的经验来选择数据。用户希望通过解释与模型进行交互，并不一定优先考虑培训数据，但是他们对培训数据作为解释的可能性存在开放。在我们的参与者中，我们发现TDA解释并不够熟悉，因此不被使用。我们建议社区在人机合作的视角下注重TDA技术的实用性，扩大TDA评估范围，以反映实际使用情况。
</details></li>
</ul>
<hr>
<h2 id="Amoeba-Circumventing-ML-supported-Network-Censorship-via-Adversarial-Reinforcement-Learning"><a href="#Amoeba-Circumventing-ML-supported-Network-Censorship-via-Adversarial-Reinforcement-Learning" class="headerlink" title="Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning"></a>Amoeba: Circumventing ML-supported Network Censorship via Adversarial Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20469">http://arxiv.org/abs/2310.20469</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobile-intelligence-lab/amoeba">https://github.com/mobile-intelligence-lab/amoeba</a></li>
<li>paper_authors: Haoyu Liu, Alec F. Diallo, Paul Patras</li>
<li>for: circumventing Internet censorship</li>
<li>methods: using machine learning algorithms to detect anti-censorship systems, and a novel reinforcement learning algorithm called Amoeba to generate adversarial flows</li>
<li>results: an attack success rate of 94% against a range of ML algorithms, and robustness in different network environments and transferability across various ML models<details>
<summary>Abstract</summary>
Embedding covert streams into a cover channel is a common approach to circumventing Internet censorship, due to censors' inability to examine encrypted information in otherwise permitted protocols (Skype, HTTPS, etc.). However, recent advances in machine learning (ML) enable detecting a range of anti-censorship systems by learning distinct statistical patterns hidden in traffic flows. Therefore, designing obfuscation solutions able to generate traffic that is statistically similar to innocuous network activity, in order to deceive ML-based classifiers at line speed, is difficult.   In this paper, we formulate a practical adversarial attack strategy against flow classifiers as a method for circumventing censorship. Specifically, we cast the problem of finding adversarial flows that will be misclassified as a sequence generation task, which we solve with Amoeba, a novel reinforcement learning algorithm that we design. Amoeba works by interacting with censoring classifiers without any knowledge of their model structure, but by crafting packets and observing the classifiers' decisions, in order to guide the sequence generation process. Our experiments using data collected from two popular anti-censorship systems demonstrate that Amoeba can effectively shape adversarial flows that have on average 94% attack success rate against a range of ML algorithms. In addition, we show that these adversarial flows are robust in different network environments and possess transferability across various ML models, meaning that once trained against one, our agent can subvert other censoring classifiers without retraining.
</details>
<details>
<summary>摘要</summary>
使用嵌入式流入受控通道是常见的绕过互联网审查的方法，因为审查器无法检查加密的信息在允许的协议（Skype、HTTPS等）中。然而，最近的机器学习（ML）技术的进步使得可以检测一些防止审查系统的anti-censorship系统。因此，设计生成混淆解决方案，以生成与正常网络活动 statistically similar的流量，以欺骗基于机器学习的分类器，是困难的。在这篇论文中，我们提出了一种实用的反恶意攻击策略，作为审查系统的绕过方法。我们将问题定义为一种序列生成任务，并使用我们自己的新型迭代学习算法——Amoeba来解决。Amoeba通过与审查类ifiers无知的模型结构互动，通过制作包和观察类ifiers的决策，来引导序列生成过程。我们的实验使用了两个流行的反审查系统所收集的数据，显示Amoeba可以有效地生成94%的攻击成功率，对多种机器学习算法进行攻击。此外，我们还证明了这些攻击流量在不同的网络环境中具有可重复性和传输性，这意味着我们的代理可以在不需要重新训练的情况下，在其他审查类ifiers上采取行动。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-detects-terminal-singularities"><a href="#Machine-learning-detects-terminal-singularities" class="headerlink" title="Machine learning detects terminal singularities"></a>Machine learning detects terminal singularities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20458">http://arxiv.org/abs/2310.20458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/fanosearch/ml_terminality">https://bitbucket.org/fanosearch/ml_terminality</a></li>
<li>paper_authors: Tom Coates, Alexander M. Kasprzyk, Sara Veneziale</li>
<li>for: 这个论文的目的是研究代数变量的分类。</li>
<li>methods: 这个论文使用机器学习技术来理解代数变量的分类。</li>
<li>results: 这个论文使用 neural network 分类器预测8维 positively-curved 代数变量是否为 Q-Fano 变量，准确率达95%。此外，通过量子期的视图，发现这些分类结果在一个有界区域内，并且与 Fano 指数相关。这些结果提供了新的证明和推断 conjectures 的机会。<details>
<summary>Abstract</summary>
Algebraic varieties are the geometric shapes defined by systems of polynomial equations; they are ubiquitous across mathematics and science. Amongst these algebraic varieties are Q-Fano varieties: positively curved shapes which have Q-factorial terminal singularities. Q-Fano varieties are of fundamental importance in geometry as they are "atomic pieces" of more complex shapes - the process of breaking a shape into simpler pieces in this sense is called the Minimal Model Programme. Despite their importance, the classification of Q-Fano varieties remains unknown. In this paper we demonstrate that machine learning can be used to understand this classification. We focus on 8-dimensional positively-curved algebraic varieties that have toric symmetry and Picard rank 2, and develop a neural network classifier that predicts with 95% accuracy whether or not such an algebraic variety is Q-Fano. We use this to give a first sketch of the landscape of Q-Fanos in dimension 8. How the neural network is able to detect Q-Fano varieties with such accuracy remains mysterious, and hints at some deep mathematical theory waiting to be uncovered. Furthermore, when visualised using the quantum period, an invariant that has played an important role in recent theoretical developments, we observe that the classification as revealed by ML appears to fall within a bounded region, and is stratified by the Fano index. This suggests that it may be possible to state and prove conjectures on completeness in the future. Inspired by the ML analysis, we formulate and prove a new global combinatorial criterion for a positively curved toric variety of Picard rank 2 to have terminal singularities. Together with the first sketch of the landscape of Q-Fanos in higher dimensions, this gives new evidence that machine learning can be an essential tool in developing mathematical conjectures and accelerating theoretical discovery.
</details>
<details>
<summary>摘要</summary>
代数变量是数学和科学中的几何形状，它们是一种系数为多项式的方程定义的。其中包括Q-Fano变量，它们是正几何形状，并且有Q因子终点特性。Q-Fano变量在几何中具有基本重要性，它们是更复杂形状的“原子部件”，通过分解这种形式的过程被称为最小模型 програм。尽管其分类仍未知，但我们在这篇论文中使用机器学习来理解这个分类。我们关注8维正几何变量，具有抽象群同质和 Picard rank 2，并开发了一个神经网络分类器，可以将95%的准确率地判断这种代数变量是否为Q-Fano。我们使用这种分类器来给8维Q-Fano变量的预览 landscape 提供第一个绘制。神经网络如何准确地检测Q-Fano变量的具体原理还未知，这表明存在深刻的数学理论 waits to be uncovered。此外，通过量子时期的视觉，我们发现分类结果在一定的 bounded 区域内，并且以 Fano 指数作为分类标准。这表明可能在未来验证完teness 的 conjecture。受机器学习分析的激发，我们提出并证明了一种全局 combinatorial 条件，它 garanties that a positively curved toric variety of Picard rank 2 has terminal singularities。此外，我们还给出了高维Q-Fano变量的首个预览，这给出了新的证明，表明机器学习可以成为数学推理的关键工具。
</details></li>
</ul>
<hr>
<h2 id="FlexTrain-A-Dynamic-Training-Framework-for-Heterogeneous-Devices-Environments"><a href="#FlexTrain-A-Dynamic-Training-Framework-for-Heterogeneous-Devices-Environments" class="headerlink" title="FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments"></a>FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20457">http://arxiv.org/abs/2310.20457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Unsal, Ali Maatouk, Antonio De Domenico, Nicola Piovesan, Fadhel Ayed</li>
<li>for: 这篇论文旨在解决深度学习模型在多种设备环境中的问题，因为这些模型的大小使得它们在低功率或资源受限的设备上难以部署，导致长时间的推理时间和高能耗。</li>
<li>methods: 这篇论文提出了一个名为FlexTrain的框架，可以在训练阶段满足不同设备的存储和计算资源的多样性。FlexTrain可以实现高效地部署深度学习模型，同时遵循设备限制，减少通信成本，并让不同设备进行顺毕合作。</li>
<li>results: 在CIFAR-100 dataset上，使用FlexTrain训练单一全球模型可以轻松地在多种设备上部署，从而降低训练时间和能耗。此外，这篇论文还将FlexTrain扩展到联邦学习设定下，证明了我们的方法在CIFAR-10和CIFAR-100 dataset上比标准联邦学习基准更好。<details>
<summary>Abstract</summary>
As deep learning models become increasingly large, they pose significant challenges in heterogeneous devices environments. The size of deep learning models makes it difficult to deploy them on low-power or resource-constrained devices, leading to long inference times and high energy consumption. To address these challenges, we propose FlexTrain, a framework that accommodates the diverse storage and computational resources available on different devices during the training phase. FlexTrain enables efficient deployment of deep learning models, while respecting device constraints, minimizing communication costs, and ensuring seamless integration with diverse devices. We demonstrate the effectiveness of FlexTrain on the CIFAR-100 dataset, where a single global model trained with FlexTrain can be easily deployed on heterogeneous devices, saving training time and energy consumption. We also extend FlexTrain to the federated learning setting, showing that our approach outperforms standard federated learning benchmarks on both CIFAR-10 and CIFAR-100 datasets.
</details>
<details>
<summary>摘要</summary>
深度学习模型随着大小的增加，在不同设备环境中带来了 significiant 挑战。由于深度学习模型的大小，在低功率或资源受限的设备上部署它们 becomes 困难，导致推理时间长，能源消耗高。为解决这些挑战，我们提出了 FlexTrain 框架，该框架在训练阶段适应不同设备上的多样化存储和计算资源。 FlexTrain 允许高效地部署深度学习模型，同时尊重设备约束，降低通信成本，并确保与多种设备的可靠集成。我们在 CIFAR-100 数据集上证明 FlexTrain 的有效性，其中一个全球模型通过 FlexTrain 可以轻松地在多种设备上部署，从而降低训练时间和能源消耗。此外，我们扩展了 FlexTrain 到联合学习设定下，并证明我们的方法在 CIFAR-10 和 CIFAR-100 数据集上超过标准联合学习标准准则。
</details></li>
</ul>
<hr>
<h2 id="The-Phase-Transition-Phenomenon-of-Shuffled-Regression"><a href="#The-Phase-Transition-Phenomenon-of-Shuffled-Regression" class="headerlink" title="The Phase Transition Phenomenon of Shuffled Regression"></a>The Phase Transition Phenomenon of Shuffled Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20438">http://arxiv.org/abs/2310.20438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Zhang, Ping Li</li>
<li>for: 这个论文是关于排序（permuted）回归问题中的阶段变换现象的研究，这个问题在数据库、隐私、数据分析等领域都有广泛的应用。</li>
<li>methods: 本研究使用了消息传递（MP）技术来精准地确定排序问题中的阶段变换点。首先，我们将排序问题转化为一个概率图模型，然后利用MP算法的分析工具， derivate一个跟踪MP算法的散度的方程。通过将这个方程连接到分支游击随机步骤过程，我们可以characterize随着信号响应比例（SNR）的影响于排序。</li>
<li>results: 在 oracle 和 non-oracle 两种情况下，我们分别研究了排序问题中的阶段变换点。在 oracle 情况下，我们可以准确预测阶段变换 SNR，而在 non-oracle 情况下，我们的算法可以预测排序中最多允许的排序行数，并且可以揭示这个数量与样本数之间的关系。<details>
<summary>Abstract</summary>
We study the phase transition phenomenon inherent in the shuffled (permuted) regression problem, which has found numerous applications in databases, privacy, data analysis, etc. In this study, we aim to precisely identify the locations of the phase transition points by leveraging techniques from message passing (MP). In our analysis, we first transform the permutation recovery problem into a probabilistic graphical model. We then leverage the analytical tools rooted in the message passing (MP) algorithm and derive an equation to track the convergence of the MP algorithm. By linking this equation to the branching random walk process, we are able to characterize the impact of the signal-to-noise-ratio ($\snr$) on the permutation recovery. Depending on whether the signal is given or not, we separately investigate the oracle case and the non-oracle case. The bottleneck in identifying the phase transition regimes lies in deriving closed-form formulas for the corresponding critical points, but only in rare scenarios can one obtain such precise expressions. To tackle this technical challenge, this study proposes the Gaussian approximation method, which allows us to obtain the closed-form formulas in almost all scenarios. In the oracle case, our method can fairly accurately predict the phase transition $\snr$. In the non-oracle case, our algorithm can predict the maximum allowed number of permuted rows and uncover its dependency on the sample number.
</details>
<details>
<summary>摘要</summary>
我们研究排序（协同）问题中的相变现象，这问题在数据库、隐私、数据分析等领域获得了广泛应用。在这些研究中，我们想要精确地找到相变点的位置，并且使用讯息传递（MP）技术来实现。在我们的分析中，我们首先将排序问题转换为一个 probabilistic graphical model，然后使用 MP 算法的分析工具来解释 MP 算法的参数。通过与分支随机步进程（BRW）的连结，我们可以描述 $\snr$ 对排序的影响。对于知道讯号的情况（oracle case）和不知道讯号的情况（non-oracle case），我们分别进行了研究。在 oracle case 中，我们可以对相变 $\snr$ 进行精确预测。在 non-oracle case 中，我们的算法可以预测最多允许的排序次数，并且揭露它对数据数量的依赖。Note: "Simplified Chinese" is a romanization of Chinese characters, and it is not a native Chinese script. The actual Chinese text would be written in Traditional Chinese or Simplified Chinese characters, depending on the region and the audience.
</details></li>
</ul>
<hr>
<h2 id="Discussing-the-Spectrum-of-Physics-Enhanced-Machine-Learning-via-a-Survey-on-Structural-Mechanics-Applications"><a href="#Discussing-the-Spectrum-of-Physics-Enhanced-Machine-Learning-via-a-Survey-on-Structural-Mechanics-Applications" class="headerlink" title="Discussing the Spectrum of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications"></a>Discussing the Spectrum of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20425">http://arxiv.org/abs/2310.20425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus Haywood-Alexander, Wei Liu, Kiran Bacsa, Zhilu Lai, Eleni Chatzi</li>
<li>for: 本研究旨在探讨物理学和机器学习之间的交叉点，即物理学习（PEML），以提高机器学习的能力和降低数据或物理学习方法的缺陷。</li>
<li>methods: 本文通过对物理学习方法的总体探讨，包括其特征、应用和动机，探讨了物理学习方法的各种类型和应用场景。此外，文章还提供了一些实际应用和开发的PEML技术，以示其在复杂问题解决方面的可能性。</li>
<li>results: 本文的研究表明，PEML方法可以在各种领域中提高机器学习的性能，例如在单度量oscillator中的示例中，不同类型的PEML方法具有不同的特点和动机。此外，文章还提供了一些实际应用和开发的PEML技术，以便读者可以参考。作为基础贡献，本文强调PEML在科学和工程研究中的重要性，它可以通过物理知识和机器学习能力的共同作用，推动科学和工程领域的发展。<details>
<summary>Abstract</summary>
The intersection of physics and machine learning has given rise to a paradigm that we refer to here as physics-enhanced machine learning (PEML), aiming to improve the capabilities and reduce the individual shortcomings of data- or physics-only methods. In this paper, the spectrum of physics-enhanced machine learning methods, expressed across the defining axes of physics and data, is discussed by engaging in a comprehensive exploration of its characteristics, usage, and motivations. In doing so, this paper offers a survey of recent applications and developments of PEML techniques, revealing the potency of PEML in addressing complex challenges. We further demonstrate application of select such schemes on the simple working example of a single-degree-of-freedom Duffing oscillator, which allows to highlight the individual characteristics and motivations of different `genres' of PEML approaches. To promote collaboration and transparency, and to provide practical examples for the reader, the code of these working examples is provided alongside this paper. As a foundational contribution, this paper underscores the significance of PEML in pushing the boundaries of scientific and engineering research, underpinned by the synergy of physical insights and machine learning capabilities.
</details>
<details>
<summary>摘要</summary>
Physics-enhanced machine learning (PEML) 是一种新兴 Paradigma，利用物理学和机器学习技术的相互作用，以提高机器学习的能力和降低数据或物理方法的缺陷。在这篇论文中，我们详细探讨了 PEML 方法在物理和数据两个轴上的特点、应用和动机，并通过一系列应用和发展例子，展示了 PEML 在复杂问题解决方面的潜力。此外，我们还使用单度关节振荡器作为一个简单的工作示例，以阐述不同类型 PEML 方法的特点和动机。为促进协作和透明度，以及为读者提供实践性的例子，我们附加了这些工作示例的代码。作为基础性贡献，这篇论文强调了 PEML 在科学和工程研究中的前沿地位，以物理学和机器学习技术的相互作用为基础。
</details></li>
</ul>
<hr>
<h2 id="DDC-PIM-Efficient-Algorithm-Architecture-Co-design-for-Doubling-Data-Capacity-of-SRAM-based-Processing-In-Memory"><a href="#DDC-PIM-Efficient-Algorithm-Architecture-Co-design-for-Doubling-Data-Capacity-of-SRAM-based-Processing-In-Memory" class="headerlink" title="DDC-PIM: Efficient Algorithm&#x2F;Architecture Co-design for Doubling Data Capacity of SRAM-based Processing-In-Memory"></a>DDC-PIM: Efficient Algorithm&#x2F;Architecture Co-design for Doubling Data Capacity of SRAM-based Processing-In-Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20424">http://arxiv.org/abs/2310.20424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cenlin Duan, Jianlei Yang, Xiaolin He, Yingjie Qi, Yikun Wang, Yiou Wang, Ziyan He, Bonan Yan, Xueyan Wang, Xiaotao Jia, Weitao Pan, Weisheng Zhao</li>
<li>for: 这个研究旨在提高快速处理在内存中（PIM）的性能，并且将其应用于神经网络中。</li>
<li>methods: 研究人员提出了一个名为DDC-PIM的有效算法&#x2F;架构创新方法，以 doubles the equivalent data capacity。在算法层次，他们提出了一个称为filter-wise complementary correlation（FCC）的算法，以获得一对bitwise complementary pair。在架构层次，他们利用6T SRAM的内置的交叉关联结构，储存每个SRAM红色的bitwise complementary pair，以最大化每个SRAM红色的资料容量。</li>
<li>results: 评估结果显示，DDC-PIM比PIM基eline实现约2.84倍的速度提升，并且与PIM基eline的精度损失相近。与现有的SRAM-based PIM芯片相比，DDC-PIM可以达到8.41倍和2.75倍的重量密度和面积效率提升。<details>
<summary>Abstract</summary>
Processing-in-memory (PIM), as a novel computing paradigm, provides significant performance benefits from the aspect of effective data movement reduction. SRAM-based PIM has been demonstrated as one of the most promising candidates due to its endurance and compatibility. However, the integration density of SRAM-based PIM is much lower than other non-volatile memory-based ones, due to its inherent 6T structure for storing a single bit. Within comparable area constraints, SRAM-based PIM exhibits notably lower capacity. Thus, aiming to unleash its capacity potential, we propose DDC-PIM, an efficient algorithm/architecture co-design methodology that effectively doubles the equivalent data capacity. At the algorithmic level, we propose a filter-wise complementary correlation (FCC) algorithm to obtain a bitwise complementary pair. At the architecture level, we exploit the intrinsic cross-coupled structure of 6T SRAM to store the bitwise complementary pair in their complementary states ($Q/\overline{Q}$), thereby maximizing the data capacity of each SRAM cell. The dual-broadcast input structure and reconfigurable unit support both depthwise and pointwise convolution, adhering to the requirements of various neural networks. Evaluation results show that DDC-PIM yields about $2.84\times$ speedup on MobileNetV2 and $2.69\times$ on EfficientNet-B0 with negligible accuracy loss compared with PIM baseline implementation. Compared with state-of-the-art SRAM-based PIM macros, DDC-PIM achieves up to $8.41\times$ and $2.75\times$ improvement in weight density and area efficiency, respectively.
</details>
<details>
<summary>摘要</summary>
“处理在内存（PIM）”是一种新的计算模式，它在数据移动效率方面提供了显著性能提升。SRAM基于PIM被认为是最有前途的候选人，因为它具有持续性和兼容性。然而，SRAM基于PIM的集成密度远低于其他不朽存储器基于的一个，因为它的内置6T结构只能存储一个bit。在相同的面积限制下，SRAM基于PIM表现出较低的容量。因此，我们提议了DDC-PIM，一种高效的算法/架构合理化方法，可以有效地double equipotential data capacity。在算法层次，我们提出了一种filter-wise complementary correlation（FCC）算法，以获得一个bitwise complementary pair。在架构层次，我们利用6T SRAM的内置交叉结构来存储bitwise complementary pair的两个相互补做（$Q/\bar{Q}$），以最大化每个SRAM cel的数据容量。该双广播输入结构和可重新配置单元支持深度wise和点wise卷积，与多种神经网络的需求相符。评估结果表明，DDC-PIM比PIM基eline实现具有约2.84倍的速度提升，无损准确性，对MobileNetV2和EfficientNet-B0进行评估。相比特状态的SRAM基于PIM封装，DDC-PIM实现了最高的Weight Density和面积效率提升，分别达到8.41倍和2.75倍。
</details></li>
</ul>
<hr>
<h2 id="Coalitional-Manipulations-and-Immunity-of-the-Shapley-Value"><a href="#Coalitional-Manipulations-and-Immunity-of-the-Shapley-Value" class="headerlink" title="Coalitional Manipulations and Immunity of the Shapley Value"></a>Coalitional Manipulations and Immunity of the Shapley Value</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20415">http://arxiv.org/abs/2310.20415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Basteck, Frank Huettner</li>
<li>for: 该论文关注在合作游戏中的操作，即一个联盟想要提高其成员的总收益。</li>
<li>methods: 该论文使用了一种新的基础设定，即取代添加性的原始特征，使用内部重新分配价值的防止和全体不变下的价值下降来定义希柏利值。</li>
<li>results: 该论文发现，对于有效的分配规则，内部重新分配价值的防止等同于受限的边缘性，这是希柏利值的唯一一种有效和Symmetric的分配规则，并且具有免除合作操作的特性。<details>
<summary>Abstract</summary>
We consider manipulations in the context of coalitional games, where a coalition aims to increase the total payoff of its members. An allocation rule is immune to coalitional manipulation if no coalition can benefit from internal reallocation of worth on the level of its subcoalitions (reallocation-proofness), and if no coalition benefits from a lower worth while all else remains the same (weak coalitional monotonicity). Replacing additivity in Shapley's original characterization by these requirements yields a new foundation of the Shapley value, i.e., it is the unique efficient and symmetric allocation rule that awards nothing to a null player and is immune to coalitional manipulations. We further find that for efficient allocation rules, reallocation-proofness is equivalent to constrained marginality, a weaker variant of Young's marginality axiom. Our second characterization improves upon Young's characterization by weakening the independence requirement intrinsic to marginality.
</details>
<details>
<summary>摘要</summary>
我们在合作游戏中考虑操作，合作团体想要增加成员的总回扣。一个分配规则是免受合作操作的影响，如果无法内部重新分配值的层次（内部重新分配证明），并且如果无法在所有 else 保持不变的情况下从值中获得更多的回扣（弱合作偏好）。将添加性在雪佛利原始特征中更替换为这些要求，则得到一个新的基础，即雪佛利值是唯一的有效和对称分配规则，将无关玩家获得任何回扣，并且免受合作操作的影响。我们进一步发现，对于有效的分配规则，内部重新分配证明与条件紧密相关，即 constrained marginality，这是 Young 的 marginality axioma 的弱化版本。我们的第二个特征改进了 Young 的特征，免除了独立性的需求，这是 marginality axioma 中的一个内在的限制。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-approach-for-solving-the-gravitational-N-body-problem-with-Artificial-Neural-Networks"><a href="#A-hybrid-approach-for-solving-the-gravitational-N-body-problem-with-Artificial-Neural-Networks" class="headerlink" title="A hybrid approach for solving the gravitational N-body problem with Artificial Neural Networks"></a>A hybrid approach for solving the gravitational N-body problem with Artificial Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20398">http://arxiv.org/abs/2310.20398</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/veronicasaz/planetarysystem_hnn">https://github.com/veronicasaz/planetarysystem_hnn</a></li>
<li>paper_authors: Veronica Saz Ulibarrena, Philipp Horn, Simon Portegies Zwart, Elena Sellentin, Barry Koren, Maxwell X. Cai<br>for: 这个研究旨在使用人工神经网络（ANNs）来加速行星系统的Simulating the evolution of the gravitational N-body problem的计算，以减少计算成本。methods: 研究使用Hamiltonian Neural Networks（HNNs）来取代计算成本高的部分，并与传统的Deep Neural Networks（DNNs）进行比较。results: 使用Hybrid integrator，包含神经网络的方法可以增加计算的可靠性，并避免大量能量误差，但是在 asteroids 的数量超过 70 时，使用神经网络可以得到更快的 simulations。<details>
<summary>Abstract</summary>
Simulating the evolution of the gravitational N-body problem becomes extremely computationally expensive as N increases since the problem complexity scales quadratically with the number of bodies. We study the use of Artificial Neural Networks (ANNs) to replace expensive parts of the integration of planetary systems. Neural networks that include physical knowledge have grown in popularity in the last few years, although few attempts have been made to use them to speed up the simulation of the motion of celestial bodies. We study the advantages and limitations of using Hamiltonian Neural Networks to replace computationally expensive parts of the numerical simulation. We compare the results of the numerical integration of a planetary system with asteroids with those obtained by a Hamiltonian Neural Network and a conventional Deep Neural Network, with special attention to understanding the challenges of this problem. Due to the non-linear nature of the gravitational equations of motion, errors in the integration propagate. To increase the robustness of a method that uses neural networks, we propose a hybrid integrator that evaluates the prediction of the network and replaces it with the numerical solution if considered inaccurate. Hamiltonian Neural Networks can make predictions that resemble the behavior of symplectic integrators but are challenging to train and in our case fail when the inputs differ ~7 orders of magnitude. In contrast, Deep Neural Networks are easy to train but fail to conserve energy, leading to fast divergence from the reference solution. The hybrid integrator designed to include the neural networks increases the reliability of the method and prevents large energy errors without increasing the computing cost significantly. For this problem, the use of neural networks results in faster simulations when the number of asteroids is >70.
</details>
<details>
<summary>摘要</summary>
计算机模拟行星系统的演化变得越来越复杂，因为问题的复杂性平方增长与体数（N）成正比。我们研究使用人工神经网络（ANNs）来替代计算昂贵的部分。包含物理知识的神经网络在最近几年内得到了广泛应用，尽管对其用于加速天体运动的模拟还未有充分的尝试。我们研究使用哈密顿神经网络来替代计算昂贵的部分的优势和限制，并与传统的深度神经网络进行比较，特别是了解这个问题的挑战。由于 gravitation 方程的非线性性，误差在计算中会卷积。为了增加使用神经网络的方法的稳定性，我们提议一种混合 интеIntegrator，该 inteGreater than 70 asteroids, the use of neural networks can achieve faster simulations.
</details></li>
</ul>
<hr>
<h2 id="Dropout-Strategy-in-Reinforcement-Learning-Limiting-the-Surrogate-Objective-Variance-in-Policy-Optimization-Methods"><a href="#Dropout-Strategy-in-Reinforcement-Learning-Limiting-the-Surrogate-Objective-Variance-in-Policy-Optimization-Methods" class="headerlink" title="Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods"></a>Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20380">http://arxiv.org/abs/2310.20380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengpeng Xie, Changdong Yu, Weizheng Qiao</li>
<li>For: This paper aims to improve the stability and convergence of policy optimization algorithms in reinforcement learning by addressing the issue of high variance in the surrogate objective caused by importance sampling.* Methods: The paper proposes a dropout technique to avoid the excessive increase of the surrogate objective variance, and introduces a general reinforcement learning framework applicable to mainstream policy optimization methods. The authors also apply the dropout technique to the Proximal Policy Optimization (PPO) algorithm to obtain the D-PPO variant.* Results: The paper conducts comparative experiments between the D-PPO and PPO algorithms in the Atari 2600 environment, and shows that D-PPO achieves significant performance improvements compared to PPO, while effectively limiting the excessive increase of the surrogate objective variance during training.<details>
<summary>Abstract</summary>
Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 environment, results show that D-PPO achieved significant performance improvements compared to PPO, and effectively limited the excessive increase of the surrogate objective variance during training.
</details>
<details>
<summary>摘要</summary>
政策基于学习算法在多种领域广泛应用。其中，主流政策优化算法如PPO和TRPO通过重要抽样引入在学习中，这使得可以重用历史数据。但这也会导致优化目标函数的偏振和学习过程的稳定性受到 indirect 影响。在这篇论文中，我们首先 derive 了优化目标函数的偏振的Upper bound，可以 quadratic 增长与优化目标函数的增长相关。然后，我们提出了dropout技术来避免由重要抽样引入的优化目标函数偏振过度增长的问题。接着，我们引入了一种通用的学习框架，适用于主流政策优化方法。最后，我们将Dropout技术应用于PPO算法，并称之为D-PPO。我们对D-PPO和PPO算法在Atari 2600环境中进行了比较性实验，结果显示，D-PPO在性能方面比PPO具有显著改善，并有效地限制了优化目标函数偏振的过度增长 durante 训练。
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-the-Decentralized-Stochastic-Gradient-Descent-Ascent-Algorithm"><a href="#Stability-and-Generalization-of-the-Decentralized-Stochastic-Gradient-Descent-Ascent-Algorithm" class="headerlink" title="Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm"></a>Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20369">http://arxiv.org/abs/2310.20369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoxi Zhu, Li Shen, Bo Du, Dacheng Tao</li>
<li>for: This paper is written for researchers and practitioners interested in decentralized machine learning and the generalization of decentralized algorithms.</li>
<li>methods: The paper uses the approach of algorithmic stability to analyze the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm in both convex-concave and nonconvex-nonconcave settings.</li>
<li>results: The paper demonstrates that the decentralized structure of the D-SGDA algorithm does not destroy its stability and generalization, and that the algorithm can generalize as well as the vanilla SGDA in certain situations. Additionally, the paper analyzes the impact of different topologies on the generalization bound of the D-SGDA algorithm and evaluates the optimization error and balance it with the generalization gap to obtain the optimal population risk of D-SGDA in the convex-concave setting.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了研究分布式机器学习和分布式算法的稳定性和泛化性而写的。</li>
<li>methods: 这篇论文使用了算法稳定性的方法来分析分布式权值逻辑 descent ascent（D-SGDA）算法在凸-凹和非凸-非凹的设置下的双方面泛化约束。</li>
<li>results: 这篇论文表明了分布式结构不会destroy D-SGDA算法的稳定性和泛化性，并且在某些情况下，D-SGDA算法可以与标准的SGDA算法具有相同的泛化能力。此外，论文还分析了不同的トポлогиー的影响于D-SGDA算法的泛化约束，并评估了优化误差和泛化差以获得D-SGDA算法在凸-凹设置下的优化人口风险。<details>
<summary>Abstract</summary>
The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such as sample sizes, learning rates, and iterations. We also evaluate the optimization error and balance it with the generalization gap to obtain the optimal population risk of D-SGDA in the convex-concave setting. Additionally, we perform several numerical experiments which validate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
随着可用数据的增长，解决分布式的最小最大问题（minimax problem）在机器学习任务中吸引了越来越多的关注。先前的理论研究主要集中在分布式最小最大算法的收敛率和通信复杂度上，几乎没有关注其泛化性。在这篇论文中，我们 investigate了分布式随机梯度搜索算法（D-SGDA）的原理稳定性下的 primal-dual 泛化 bound，包括 convex-concave 和 nonconvex-nonconcave 设置下的情况。我们的理论表明，分布式结构不会对 D-SGDA 的稳定性和泛化性产生负面影响，这意味着它可以在某些情况下与普通的 SGDA 相当具有泛化能力。我们的研究还分析了不同拓扑结构对 D-SGDA 算法的泛化 bound 的影响，并超越了聚合因素、学习率、迭代次数等简单的 фактор。此外，我们还评估了 D-SGDA 算法的优化误差，并尝试平衡优化误差和泛化差，以获得 D-SGDA 算法在 convex-concave 设置下的最佳人口风险。最后，我们进行了多个数学实验，以验证我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Distil-the-informative-essence-of-loop-detector-data-set-Is-network-level-traffic-forecasting-hungry-for-more-data"><a href="#Distil-the-informative-essence-of-loop-detector-data-set-Is-network-level-traffic-forecasting-hungry-for-more-data" class="headerlink" title="Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?"></a>Distil the informative essence of loop detector data set: Is network-level traffic forecasting hungry for more data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20366">http://arxiv.org/abs/2310.20366</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction">https://github.com/romainlitud/uncertainty-aware-traffic-speed-flow-demand-prediction</a></li>
<li>paper_authors: Guopeng Li, Victor L. Knoop, J. W. C., van Lint</li>
<li>for: 本研究是为了解决网络级别的交通情况预测问题，尤其是对于 loop detector 数据。</li>
<li>methods: 本研究提出了一种不确定性感知的交通预测框架，结合流体理论和图 neural network，以确保预测和不确定性评估的Robustness。此外，使用 evidential learning 来评估不同来源的不确定性，并将估计的不确定性用于”浓缩”数据集中的信息内容。</li>
<li>results: 通过对一条横跨阿姆斯特丹的高速公路网络进行实验，发现2018-2021年日间80%以上的数据可以被去除，剩下的20%样本具有相同的预测力度。这结果表明，大规模的交通数据集可以被分解成更小但具有相同信息内容的数据集。这些发现证明了提posed方法的价值性，并且可以进一步推广到EXTRACTING更小但非重叠的数据集。<details>
<summary>Abstract</summary>
Network-level traffic condition forecasting has been intensively studied for decades. Although prediction accuracy has been continuously improved with emerging deep learning models and ever-expanding traffic data, traffic forecasting still faces many challenges in practice. These challenges include the robustness of data-driven models, the inherent unpredictability of traffic dynamics, and whether further improvement of traffic forecasting requires more sensor data. In this paper, we focus on this latter question and particularly on data from loop detectors. To answer this, we propose an uncertainty-aware traffic forecasting framework to explore how many samples of loop data are truly effective for training forecasting models. Firstly, the model design combines traffic flow theory with graph neural networks, ensuring the robustness of prediction and uncertainty quantification. Secondly, evidential learning is employed to quantify different sources of uncertainty in a single pass. The estimated uncertainty is used to "distil" the essence of the dataset that sufficiently covers the information content. Results from a case study of a highway network around Amsterdam show that, from 2018 to 2021, more than 80\% of the data during daytime can be removed. The remaining 20\% samples have equal prediction power for training models. This result suggests that indeed large traffic datasets can be subdivided into significantly smaller but equally informative datasets. From these findings, we conclude that the proposed methodology proves valuable in evaluating large traffic datasets' true information content. Further extensions, such as extracting smaller, spatially non-redundant datasets, are possible with this method.
</details>
<details>
<summary>摘要</summary>
Our proposed framework combines traffic flow theory with graph neural networks, ensuring the robustness of prediction and uncertainty quantification. Additionally, we use evidential learning to quantify different sources of uncertainty in a single pass, and the estimated uncertainty is used to "distil" the essence of the dataset that sufficiently covers the information content.We conducted a case study of a highway network around Amsterdam from 2018 to 2021, and found that more than 80% of the data during daytime can be removed, while the remaining 20% of samples have equal prediction power for training models. This result suggests that large traffic datasets can be subdivided into significantly smaller but equally informative datasets.Our findings demonstrate the value of the proposed methodology in evaluating the true information content of large traffic datasets. Furthermore, we can extend this method to extract smaller, spatially non-redundant datasets, which can help improve the efficiency of traffic forecasting.
</details></li>
</ul>
<hr>
<h2 id="CAFE-Conflict-Aware-Feature-wise-Explanations"><a href="#CAFE-Conflict-Aware-Feature-wise-Explanations" class="headerlink" title="CAFE: Conflict-Aware Feature-wise Explanations"></a>CAFE: Conflict-Aware Feature-wise Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20363">http://arxiv.org/abs/2310.20363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Dejl, Hamed Ayoobi, Matthew Williams, Francesca Toni</li>
<li>for: 本研究旨在提出一种新的特征归属方法，以解释神经网络模型的决策过程中的特征影响。</li>
<li>methods: 该方法（CAFE）解决了现有方法的三大限制：忽略内部矛盾特征的影响、忽略偏见项的影响以及当地化活动函数的过敏。CAFE方法提供了防止过度估计输入特征的影响的保障，并分别跟踪输入特征和偏见项的正面和负面影响，从而提高了其robustness和能力浮现特征冲突。</li>
<li>results: 实验表明，CAFE方法在synthetic tabular数据上更好地标识内部矛盾特征，并在几个真实世界的 tabular数据集上达到最高的总准确率，同时具有高效计算性。<details>
<summary>Abstract</summary>
Feature attribution methods are widely used to explain neural models by determining the influence of individual input features on the models' outputs. We propose a novel feature attribution method, CAFE (Conflict-Aware Feature-wise Explanations), that addresses three limitations of the existing methods: their disregard for the impact of conflicting features, their lack of consideration for the influence of bias terms, and an overly high sensitivity to local variations in the underpinning activation functions. Unlike other methods, CAFE provides safeguards against overestimating the effects of neuron inputs and separately traces positive and negative influences of input features and biases, resulting in enhanced robustness and increased ability to surface feature conflicts. We show experimentally that CAFE is better able to identify conflicting features on synthetic tabular data and exhibits the best overall fidelity on several real-world tabular datasets, while being highly computationally efficient.
</details>
<details>
<summary>摘要</summary>
Feature 归因方法广泛使用于解释神经网络模型，它们可以确定输入特征对模型输出的影响。我们提出了一种新的Feature归因方法，称为CAFE（Conflict-Aware Feature-wise Explanations），它解决了现有方法的三个限制：它们忽略了对抗性特征的影响、缺乏对偏移项的考虑和当地活动函数下的过敏感。与其他方法不同，CAFE提供了防止过度估计输入神经元影响的保障，并分别跟踪输入特征和偏移项的正面和负面影响，从而提高了robustness和抑制特征冲突的能力。我们通过实验表明，CAFE在Synthetic表格数据上更好地标识对抗性特征，并在多个实际表格数据上显示出最好的总准确率，同时具有高度计算效率。
</details></li>
</ul>
<hr>
<h2 id="Verification-of-Neural-Networks-Local-Differential-Classification-Privacy"><a href="#Verification-of-Neural-Networks-Local-Differential-Classification-Privacy" class="headerlink" title="Verification of Neural Networks Local Differential Classification Privacy"></a>Verification of Neural Networks Local Differential Classification Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20299">http://arxiv.org/abs/2310.20299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roie Reshef, Anan Kabaha, Olga Seleznova, Dana Drachsler-Cohen</li>
<li>for: 保护隐私性（Privacy）</li>
<li>methods: 使用地域拟合分布（KDE）和分布式验证（MILP）</li>
<li>results: 通过训练 Only 7% of the networks, Sphynx 可以预测一个抽象网络，并达到 93% 的验证精度和将验证时间减少 $1.7\cdot10^4$ 倍。<details>
<summary>Abstract</summary>
Neural networks are susceptible to privacy attacks. To date, no verifier can reason about the privacy of individuals participating in the training set. We propose a new privacy property, called local differential classification privacy (LDCP), extending local robustness to a differential privacy setting suitable for black-box classifiers. Given a neighborhood of inputs, a classifier is LDCP if it classifies all inputs the same regardless of whether it is trained with the full dataset or whether any single entry is omitted. A naive algorithm is highly impractical because it involves training a very large number of networks and verifying local robustness of the given neighborhood separately for every network. We propose Sphynx, an algorithm that computes an abstraction of all networks, with a high probability, from a small set of networks, and verifies LDCP directly on the abstract network. The challenge is twofold: network parameters do not adhere to a known distribution probability, making it difficult to predict an abstraction, and predicting too large abstraction harms the verification. Our key idea is to transform the parameters into a distribution given by KDE, allowing to keep the over-approximation error small. To verify LDCP, we extend a MILP verifier to analyze an abstract network. Experimental results show that by training only 7% of the networks, Sphynx predicts an abstract network obtaining 93% verification accuracy and reducing the analysis time by $1.7\cdot10^4$x.
</details>
<details>
<summary>摘要</summary>
“神经网络容易受到隐私攻击。至今，无法对受训集中的个人隐私进行推理。我们提出了一新的隐私性质，即本地分类隐私性（LDCP），将本地可靠性应用到各种隐私设置中。给定一个输入集，如果某个分类器在该集中的任何一个输入 omitted 时都将所有输入分类为同一个类别，那么该分类器是 LDCP。一个简单的算法是非常不实用，因为它需要训练一个非常大的数量的网络，并对每个网络进行本地稳健性检查。我们提出了一个名为 Sphynx 的算法，可以从一小数量的网络中计算一个抽象网络，并将 LDCP 直接验证到抽象网络上。挑战是两重的：网络参数不遵循已知的概率分布，使预测抽象变得困难；而且，如果预测的抽象太大，则会导致验证失败。我们的关键想法是将参数转换为一个由 KDE 提供的分布，这样可以保持过度上下文错误小。为验证 LDCP，我们将 MILP 验证器扩展到抽象网络上进行分析。实验结果表明，只需训练 7% 的网络，Sphynx 可以预测一个抽象网络， obtining 93% 的验证精度，并将分析时间减少 $1.7\cdot10^4$ 倍。”
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Generalized-Linear-Models-by-Trading-off-Computation-for-Uncertainty"><a href="#Accelerating-Generalized-Linear-Models-by-Trading-off-Computation-for-Uncertainty" class="headerlink" title="Accelerating Generalized Linear Models by Trading off Computation for Uncertainty"></a>Accelerating Generalized Linear Models by Trading off Computation for Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20285">http://arxiv.org/abs/2310.20285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Tatzel, Jonathan Wenger, Frank Schneider, Philipp Hennig</li>
<li>for: 这个论文是为了提出一种Iterative方法，用于提高 bayesian generalized linear models（GLMs）的准确性和效率。</li>
<li>methods: 这种Iterative方法利用了并行计算和信息压缩技术，可以快速地训练 GLMs，并且可以提供更好的 uncertainty 估计。</li>
<li>results: 在一个实际的分类问题中，这种方法可以快速地训练 GLMs，并且可以提供更好的 uncertainty 估计，相比传统的方法。<details>
<summary>Abstract</summary>
Bayesian Generalized Linear Models (GLMs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in GLMs is prohibitively expensive for large datasets, thus requiring approximations in practice. The resulting approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. In this work, we introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for GLMs. As we demonstrate on a realistically large classification problem, our method significantly accelerates training by explicitly trading off reduced computation for increased uncertainty.
</details>
<details>
<summary>摘要</summary>
泛函 Bayesian Generalized Linear Models (GLMs) 提供了一个洒脱的概率框架，用于模elling categorical、ordinal 和连续变数，并在实践中广泛使用。然而，对大型数据集进行精确的推断是不可能的，因此需要使用估计。然而，这个估计错误会对模型的可靠性产生负面影响，而且不会考虑到预测中的不确定性。在这个工作中，我们引入了一家族的迭代方法，可以明确地模型这个错误。这些方法特别适合平行现代计算机硬件，可以高效地重复计算，将信息压缩，以减少 GLMs 的时间和内存需求。我们在一个实际上是一个大型分类问题中显示了，我们的方法可以快速地训练，并可以明确地交换精确性和不确定性。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Bayesian-Optimization-via-Learning-Correlated-Latent-Space"><a href="#Advancing-Bayesian-Optimization-via-Learning-Correlated-Latent-Space" class="headerlink" title="Advancing Bayesian Optimization via Learning Correlated Latent Space"></a>Advancing Bayesian Optimization via Learning Correlated Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20258">http://arxiv.org/abs/2310.20258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghun Lee, Jaewon Chu, Sihyeon Kim, Juyeon Ko, Hyunwoo J. Kim</li>
<li>for: 优化黑盒函数（black-box function），具有有限的函数评估次数。</li>
<li>methods: 使用深度生成模型（deep generative models），如变量自动编码器（variational autoencoders），实现有效和高效的极限优化。</li>
<li>results: 在离散数据上实现高性能，并且在有限评估次数下寻找优化解。<details>
<summary>Abstract</summary>
Bayesian optimization is a powerful method for optimizing black-box functions with limited function evaluations. Recent works have shown that optimization in a latent space through deep generative models such as variational autoencoders leads to effective and efficient Bayesian optimization for structured or discrete data. However, as the optimization does not take place in the input space, it leads to an inherent gap that results in potentially suboptimal solutions. To alleviate the discrepancy, we propose Correlated latent space Bayesian Optimization (CoBO), which focuses on learning correlated latent spaces characterized by a strong correlation between the distances in the latent space and the distances within the objective function. Specifically, our method introduces Lipschitz regularization, loss weighting, and trust region recoordination to minimize the inherent gap around the promising areas. We demonstrate the effectiveness of our approach on several optimization tasks in discrete data, such as molecule design and arithmetic expression fitting, and achieve high performance within a small budget.
</details>
<details>
<summary>摘要</summary>
bayesian 优化是一种强大的方法，用于优化黑板函数，但是它具有限制性的函数评估。现有研究表明，通过在深度生成模型中进行几何变换，可以实现有效和高效的 bayesian 优化。但是，由于优化不在输入空间进行，因此会导致内在的差距，从而导致可能的优化解决方案不佳。为了解决这个差距，我们提出相关的独立空间抽象 Bayesian 优化方法（CoBO），它专门关注学习相关独立空间，其中独立空间中的距离与目标函数中的距离具有强相关性。specifically，我们的方法引入了 lipschitz 正则化、损失权重和信任区域重新协调，以降低内在差距的潜在问题。我们在一些离散数据优化任务中，如分子设计和数学表达适应，实现了高效性，并且在小预算内达到了高性能。
</details></li>
</ul>
<hr>
<h2 id="STDA-Meta-A-Meta-Learning-Framework-for-Few-Shot-Traffic-Prediction"><a href="#STDA-Meta-A-Meta-Learning-Framework-for-Few-Shot-Traffic-Prediction" class="headerlink" title="STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction"></a>STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20223">http://arxiv.org/abs/2310.20223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maoxiang Sun, Weilong Ding, Tianpu Zhang, Zijian Liu, Mengda Xing</li>
<li>for:  traffic prediction in newly developed cities with insufficient sensors</li>
<li>methods:  few-shot learning (FSL) and spatio-temporal domain adaptation (STDA)</li>
<li>results:  improved prediction performance by 7% compared to baseline models on two metrics (MAE and RMSE)<details>
<summary>Abstract</summary>
As the development of cities, traffic congestion becomes an increasingly pressing issue, and traffic prediction is a classic method to relieve that issue. Traffic prediction is one specific application of spatio-temporal prediction learning, like taxi scheduling, weather prediction, and ship trajectory prediction. Against these problems, classical spatio-temporal prediction learning methods including deep learning, require large amounts of training data. In reality, some newly developed cities with insufficient sensors would not hold that assumption, and the data scarcity makes predictive performance worse. In such situation, the learning method on insufficient data is known as few-shot learning (FSL), and the FSL of traffic prediction remains challenges. On the one hand, graph structures' irregularity and dynamic nature of graphs cannot hold the performance of spatio-temporal learning method. On the other hand, conventional domain adaptation methods cannot work well on insufficient training data, when transferring knowledge from different domains to the intended target domain.To address these challenges, we propose a novel spatio-temporal domain adaptation (STDA) method that learns transferable spatio-temporal meta-knowledge from data-sufficient cities in an adversarial manner. This learned meta-knowledge can improve the prediction performance of data-scarce cities. Specifically, we train the STDA model using a Model-Agnostic Meta-Learning (MAML) based episode learning process, which is a model-agnostic meta-learning framework that enables the model to solve new learning tasks using only a small number of training samples. We conduct numerous experiments on four traffic prediction datasets, and our results show that the prediction performance of our model has improved by 7\% compared to baseline models on the two metrics of MAE and RMSE.
</details>
<details>
<summary>摘要</summary>
随着城市的发展，交通堵塞问题变得越来越严重，交通预测成为解决这一问题的一种 clasic 方法。交通预测是空间时间预测学习的一个特定应用，与axi scheduling、天气预测和船舶轨迹预测等问题相关。在实际情况下，一些新建的城市缺乏感知器，导致预测性能下降。在这种情况下，具有少量数据的学习方法被称为几何学习（FSL），交通预测中的FSL具有挑战。一方面，Graph结构的不规则性和空间时间学习方法的动态性使得学习成本增加。另一方面，传统的领域适应方法在缺乏训练数据时无法达到预期的性能。为解决这些挑战，我们提出了一种新的空间时间领域适应（STDA）方法，通过在数据充足城市中学习启发式空间时间元知识，以提高数据缺乏城市的预测性能。具体来说，我们使用基于MAML的模型无关元学习（MAML）基于话语学习过程，这是一种可以在很少的训练样本基础上解决新的学习任务的模型无关元学习框架。我们在四个交通预测数据集上进行了多次实验，结果显示，我们的模型的预测性能与基eline模型相比提高了7%的MAE和RMSE两个纪录。
</details></li>
</ul>
<hr>
<h2 id="Calibration-by-Distribution-Matching-Trainable-Kernel-Calibration-Metrics"><a href="#Calibration-by-Distribution-Matching-Trainable-Kernel-Calibration-Metrics" class="headerlink" title="Calibration by Distribution Matching: Trainable Kernel Calibration Metrics"></a>Calibration by Distribution Matching: Trainable Kernel Calibration Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20211">http://arxiv.org/abs/2310.20211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kernel-calibration/kernel-calibration">https://github.com/kernel-calibration/kernel-calibration</a></li>
<li>paper_authors: Charles Marx, Sofian Zalouk, Stefano Ermon</li>
<li>for: 该论文旨在提高预测probability的准确性和不确定性捕捉，并且提供一种基于kernel的抽象度量来评估预测准确性。</li>
<li>methods: 该论文使用了kernel-based抽象度量，它们可以总结和普适多种预测calibration方法，并且可以轻松地在empirical risk minimization中添加calibration目标。</li>
<li>results: 该论文的实验结果表明，通过使用这些抽象度量作为正则函数，可以提高预测的准确性、锐度和决策效果，并且超过了仅仅采用后期重新补做calibration的方法。<details>
<summary>Abstract</summary>
Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification tasks, outperforming methods relying solely on post-hoc recalibration.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用标准化的抽象语言来描述：Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification tasks, outperforming methods relying solely on post-hoc recalibration.中文翻译：使用标准化的抽象语言描述：calibration确保probabilistic forecasts能够准确地捕捉到uncertainty，通过要求预测的概率与实际频率相匹配。然而，许多现有的calibration方法专门为post-hoc recalibration，可能会恶化预测的精度。 Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression.这些 metric采用分布匹配任务的思想，可以快速地在empirical risk minimization中添加calibration目标。此外，我们还提供了直观的机制来适应决策任务，并强制实现准确的损失估计和无悬决策。我们的实验证明，通过将这些metric作为正则izer来使用，可以提高calibration、锐度和决策的性能，在多种分类和回归任务上超过仅仅靠post-hoc recalibration的方法。
</details></li>
</ul>
<hr>
<h2 id="Network-Contention-Aware-Cluster-Scheduling-with-Reinforcement-Learning"><a href="#Network-Contention-Aware-Cluster-Scheduling-with-Reinforcement-Learning" class="headerlink" title="Network Contention-Aware Cluster Scheduling with Reinforcement Learning"></a>Network Contention-Aware Cluster Scheduling with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20209">http://arxiv.org/abs/2310.20209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gajagajago/deepshare">https://github.com/gajagajago/deepshare</a></li>
<li>paper_authors: Junyeol Ryu, Jeongyoon Eo</li>
<li>For: 这个论文主要针对 GPU 集群中的训练工作负载进行优化，尤其是针对不同量、比例和模式的通信对训练进程的影响。* Methods: 本论文使用了人工智能学习来解决 GPU 集群中的网络竞争问题，具体来说是通过将 GPU 集群调度问题转换为一个人工智能学习问题，以便学习一个网络竞争意识的调度策略。* Results: 相比于通用的调度策略，本论文的方法可以降低平均训练时间达18.2%，并且可以将尾部训练时间降低达20.7%，同时允许更好的资源利用率和训练时间平衡。<details>
<summary>Abstract</summary>
With continuous advances in deep learning, distributed training is becoming common in GPU clusters. Specifically, for emerging workloads with diverse amounts, ratios, and patterns of communication, we observe that network contention can significantly degrade training throughput. However, widely used scheduling policies often face limitations as they are agnostic to network contention between jobs. In this paper, we present a new approach to mitigate network contention in GPU clusters using reinforcement learning. We formulate GPU cluster scheduling as a reinforcement learning problem and opt to learn a network contention-aware scheduling policy that efficiently captures contention sensitivities and dynamically adapts scheduling decisions through continuous evaluation and improvement. We show that compared to widely used scheduling policies, our approach reduces average job completion time by up to 18.2\% and effectively cuts the tail job completion time by up to 20.7\% while allowing a preferable trade-off between average job completion time and resource utilization.
</details>
<details>
<summary>摘要</summary>
随着深度学习的不断发展，分布式训练在GPU集群中变得越来越普遍。特别是对于出现的工作负载，这些负载有多样的量、比率和通信模式，我们发现网络竞争可以较大程度地降低训练速率。然而，广泛使用的调度策略经常面临限制，因为它们对网络竞争between jobs无法适应。在这篇论文中，我们提出一种新的网络竞争 Mitigation Approach for GPU clusters using reinforcement learning。我们将GPU集群调度问题转化为一个reinforcement learning问题，并选择学习一个网络竞争意识的调度策略，能够高效地捕捉竞争敏感度并通过连续评估和改进来动态地调整调度决策。我们表明，相比广泛使用的调度策略，我们的方法可以降低平均任务完成时间 by up to 18.2%，同时可以有效地削减尾号任务完成时间 by up to 20.7%，并允许可接受的资源利用率和任务完成时间之间的交易。
</details></li>
</ul>
<hr>
<h2 id="Importance-Estimation-with-Random-Gradient-for-Neural-Network-Pruning"><a href="#Importance-Estimation-with-Random-Gradient-for-Neural-Network-Pruning" class="headerlink" title="Importance Estimation with Random Gradient for Neural Network Pruning"></a>Importance Estimation with Random Gradient for Neural Network Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20203">http://arxiv.org/abs/2310.20203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suman Sapkota, Binod Bhattarai</li>
<li>for: 本研究旨在提高神经网络的效率，通过全球神经元重要性估计来减少神经网络的参数数量。</li>
<li>methods: 本研究使用了一种基于TaylorFOapproximation的轮征方法来估计神经元的全球重要性。此外，我们还提出了两种改进方法，包括从最后一层网络中传播随机梯度，以及对最后一层输出的梯度幅度进行正规化。</li>
<li>results: 我们在ResNet和VGG架构上测试了我们的方法，并得到了更好的性能。具体来说，我们在CIFAR-100和STL-10 datasets上分别提高了1.3%和1.6%的测试错误率。此外，我们还发现我们的方法可以与现有的方法相结合，以提高它们的性能。<details>
<summary>Abstract</summary>
Global Neuron Importance Estimation is used to prune neural networks for efficiency reasons. To determine the global importance of each neuron or convolutional kernel, most of the existing methods either use activation or gradient information or both, which demands abundant labelled examples. In this work, we use heuristics to derive importance estimation similar to Taylor First Order (TaylorFO) approximation based methods. We name our methods TaylorFO-abs and TaylorFO-sq. We propose two additional methods to improve these importance estimation methods. Firstly, we propagate random gradients from the last layer of a network, thus avoiding the need for labelled examples. Secondly, we normalize the gradient magnitude of the last layer output before propagating, which allows all examples to contribute similarly to the importance score. Our methods with additional techniques perform better than previous methods when tested on ResNet and VGG architectures on CIFAR-100 and STL-10 datasets. Furthermore, our method also complements the existing methods and improves their performances when combined with them.
</details>
<details>
<summary>摘要</summary>
全球神经元重要性估计是用于提高神经网络效率的方法之一。以现有的方法为基础，大多数使用活动或梯度信息，或者两者都使用，以估计每个神经元或卷积核心的全球重要性。在这种情况下，我们使用规则来 derivimportance estimation similar to Taylor First Order (TaylorFO) approximation based methods。我们命名我们的方法为TaylorFO-abs和TaylorFO-sq。我们还提出了两种方法来改进这些重要性估计方法。首先，我们从神经网络的最后一层传递随机梯度，以避免需要标注的例子。其次，我们在最后一层输出之前对梯度大小进行 нор化，使所有的例子都能够对重要性分数做出相同的贡献。我们的方法与其他技术结合使用时表现更好，并且在ResNet和VGG架构上的CIFAR-100和STL-10 datasets上进行测试时也表现出了更好的成绩。
</details></li>
</ul>
<hr>
<h2 id="FedRec-Enhancing-Privacy-and-Addressing-Heterogeneity-in-Federated-Recommendation-Systems"><a href="#FedRec-Enhancing-Privacy-and-Addressing-Heterogeneity-in-Federated-Recommendation-Systems" class="headerlink" title="FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems"></a>FedRec+: Enhancing Privacy and Addressing Heterogeneity in Federated Recommendation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20193">http://arxiv.org/abs/2310.20193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Wang, Zhichao Wang, Xi Leng, Xiaoying Tang</li>
<li>for: 防止边缘用户隐私泄露和减少通信成本，提高推荐系统效能。</li>
<li>methods: 使用最佳子集选择基于特征相似性生成近乎最佳虚拟评分，仅使用用户本地资讯，减少杂音而无需额外的通信成本。并使用沃氏距离估计客户端多样性和贡献，解决客户端多样性问题。</li>
<li>results: 实验结果显示FedRec+可以在不同的参考数据集上实现现代表性的性能，超过现有方法。<details>
<summary>Abstract</summary>
Preserving privacy and reducing communication costs for edge users pose significant challenges in recommendation systems. Although federated learning has proven effective in protecting privacy by avoiding data exchange between clients and servers, it has been shown that the server can infer user ratings based on updated non-zero gradients obtained from two consecutive rounds of user-uploaded gradients. Moreover, federated recommendation systems (FRS) face the challenge of heterogeneity, leading to decreased recommendation performance. In this paper, we propose FedRec+, an ensemble framework for FRS that enhances privacy while addressing the heterogeneity challenge. FedRec+ employs optimal subset selection based on feature similarity to generate near-optimal virtual ratings for pseudo items, utilizing only the user's local information. This approach reduces noise without incurring additional communication costs. Furthermore, we utilize the Wasserstein distance to estimate the heterogeneity and contribution of each client, and derive optimal aggregation weights by solving a defined optimization problem. Experimental results demonstrate the state-of-the-art performance of FedRec+ across various reference datasets.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)保护用户隐私和降低边缘用户的通信成本是推荐系统中的主要挑战。虽然联邦学习已经证明能够保护隐私 by avoiding data exchange between clients and servers，但是服务器可以根据两次连续的用户上传的非零梯度推算用户评分。此外，联邦推荐系统（FRS）面临着不一致性挑战，导致推荐性能下降。在这篇论文中，我们提出了FedRec+，一个ensemble框架 для FRS，增强隐私性，并解决不一致性挑战。FedRec+使用最佳子集选择基于特征相似性来生成近似最佳虚拟评分 дляpseudo item，只使用用户的本地信息。这种方法减少噪音而不导致额外的通信成本。此外，我们利用 Wasserstein distance来估算每个客户端的不一致性和贡献，并 deriv optimal aggregation weights by solving a defined optimization problem。实验结果表明FedRec+在不同的参考数据集上具有状态之art的表现。
</details></li>
</ul>
<hr>
<h2 id="Compact-Binary-Systems-Waveform-Generation-with-Generative-Pre-trained-Transformer"><a href="#Compact-Binary-Systems-Waveform-Generation-with-Generative-Pre-trained-Transformer" class="headerlink" title="Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer"></a>Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20172">http://arxiv.org/abs/2310.20172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijun Shi, Yue Zhou, Tianyu Zhao, Zhoujian Cao, Zhixiang Ren</li>
<li>for: 这篇论文主要目标是解决空间 gravitational wave 探测中数据处理中的增加复杂性问题，具体来说是用一种可解释性强大的大型预训练模型来预测 Compact Binary Systems 波形。</li>
<li>methods: 这篇论文提出了一种名为 CBS-GPT（Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer）的可解释性强大的大型预训练模型，用于预测 Compact Binary Systems 波形。三个模型都被训练以预测 Masive Black Hole Binary（MBHB）、Extreme Mass-Ratio Inspirals（EMRIs）和 Galactic Binary（GB）波形，其中预测精度分别为98%、91%和99%。</li>
<li>results: 这篇论文的研究结果表明，CBS-GPT 模型具有出色的可解释性，其隐藏参数能够有效地捕捉波形中的复杂信息，即使 Instrument Response 和参数范围很广。这种研究展示了大型预训练模型在 gravitational wave 数据处理中的潜在应用前景，包括 gap completion、GW 信号检测和信号噪声减少等任务。<details>
<summary>Abstract</summary>
Space-based gravitational wave detection is one of the most anticipated gravitational wave (GW) detection projects in the next decade, which will detect abundant compact binary systems. However, the precise prediction of space GW waveforms remains unexplored. To solve the data processing difficulty in the increasing waveform complexity caused by detectors' response and second-generation time-delay interferometry (TDI 2.0), an interpretable pre-trained large model named CBS-GPT (Compact Binary Systems Waveform Generation with Generative Pre-trained Transformer) is proposed. For compact binary system waveforms, three models were trained to predict the waveforms of massive black hole binary (MBHB), extreme mass-ratio inspirals (EMRIs), and galactic binary (GB), achieving prediction accuracies of 98%, 91%, and 99%, respectively. The CBS-GPT model exhibits notable interpretability, with its hidden parameters effectively capturing the intricate information of waveforms, even with complex instrument response and a wide parameter range. Our research demonstrates the potential of large pre-trained models in gravitational wave data processing, opening up new opportunities for future tasks such as gap completion, GW signal detection, and signal noise reduction.
</details>
<details>
<summary>摘要</summary>
Space-based gravitational wave detection是下一代的 gravitational wave（GW）探测项目中最受期待的一个，能够探测丰富的紧凑binary系统。然而，准确预测space GW 波形仍然未经充分研究。为解决探测器响应和第二代时间延迟相互探测（TDI 2.0）中数据处理困难，一种可解释的大型预训练模型被提议，称为CBS-GPT（紧凑 binary 系统波形生成器with Generative Pre-trained Transformer）。对紧凑binary系统波形，CBS-GPT模型进行了三种不同的模型训练，分别预测了大质量黑洞binary（MBHB）、极大质量比例沉落（EMRIs）和 галактиче binary（GB）波形，实现了预测精度为98%、91%和99%。CBS-GPT模型表现出了remarkable可解释性，其隐藏参数能够有效捕捉波形中的复杂信息，即使用户器Response和广泛的参数范围。我们的研究表明大预训练模型在GW数据处理中具有潜在的应用前景，打开了未来任务 such as gap completion、GW信号探测和信号噪声减少等的新机遇。
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Visualizing-Droplet-Distributions-in-Simulations-of-Shallow-Clouds"><a href="#Understanding-and-Visualizing-Droplet-Distributions-in-Simulations-of-Shallow-Clouds" class="headerlink" title="Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds"></a>Understanding and Visualizing Droplet Distributions in Simulations of Shallow Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20168">http://arxiv.org/abs/2310.20168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justus C. Will, Andrea M. Jenney, Kara D. Lamb, Michael S. Pritchard, Colleen Kaul, Po-Lun Ma, Kyle Pressel, Jacob Shpund, Marcus van Lier-Walqui, Stephan Mandt</li>
<li>for: 研究云中微物理过程，特别是云中落体大小分布的影响。</li>
<li>methods: 使用变分自动编码器（VAEs）生成了新的可读性抽象，用于描述落体大小分布的时间变化。</li>
<li>results: 研究发现，即使颗粒浓度不同，落体spectrum的演化都类似，只是速度不同。这表明降水起始过程具有共同特征。<details>
<summary>Abstract</summary>
Thorough analysis of local droplet-level interactions is crucial to better understand the microphysical processes in clouds and their effect on the global climate. High-accuracy simulations of relevant droplet size distributions from Large Eddy Simulations (LES) of bin microphysics challenge current analysis techniques due to their high dimensionality involving three spatial dimensions, time, and a continuous range of droplet sizes. Utilizing the compact latent representations from Variational Autoencoders (VAEs), we produce novel and intuitive visualizations for the organization of droplet sizes and their evolution over time beyond what is possible with clustering techniques. This greatly improves interpretation and allows us to examine aerosol-cloud interactions by contrasting simulations with different aerosol concentrations. We find that the evolution of the droplet spectrum is similar across aerosol levels but occurs at different paces. This similarity suggests that precipitation initiation processes are alike despite variations in onset times.
</details>
<details>
<summary>摘要</summary>
<langpack> simplifiedchinese</langpack><text>这篇文章提出了一个极其重要的问题：如何对云中的小滴子互动进行全面的分析，以更好地理解云内的微物理过程和它们对全球气候的影响。对于这个问题，作者使用了大� Eddy  simulations（LES）的bin微物理来模拟有效的滴子大小分布，并挑战了现有的分析技术。使用了对预测矩阵的简洁隐藏表示（VAEs），作者创造了新的可读性和直观性的可视化，以便更好地理解滴子大小和时间的演化。这个可视化给了更多的解释，并允许我们对不同气体浓度的 simulations 进行比较。我们发现，随着气体浓度的变化，滴子spectrum的演化是相似的，但发生的时间则有所不同。这些相似性表明，降水启动过程是相似的，即使在不同的启动时间。</text></sys>
</details></li>
</ul>
<hr>
<h2 id="Efficient-Robust-Bayesian-Optimization-for-Arbitrary-Uncertain-inputs"><a href="#Efficient-Robust-Bayesian-Optimization-for-Arbitrary-Uncertain-inputs" class="headerlink" title="Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs"></a>Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20145">http://arxiv.org/abs/2310.20145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Junlong Lyu, Wenlong Lyu, Zhitang Chen</li>
<li>for: 这篇论文的目的是提出一种能够有效地处理输入不确定性的 bayesian 优化算法。</li>
<li>methods: 这种算法使用了 Gaussian Process 模型，并通过 Maximum Mean Discrepancy (MMD) 来直接模型输入的不确定性。它还使用了 Nystrom 简化 posterior inference。</li>
<li>results: 经过对 synthetic functions 和实际问题的测试，这种方法可以处理各种输入不确定性，并达到现有最佳性能。<details>
<summary>Abstract</summary>
Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic functions and real problems demonstrate that our approach can handle various input uncertainties and achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
bayesian 优化（BO）是一种效率高的优化算法，广泛应用于不同领域。在某些复杂的BO任务中，输入不确定性 arise due to 优化过程中的随机性，如机床错误、执行噪声或上下文变化。这种不确定性会使输入偏离预期的值，导致优化结果中的性能波动。在这篇论文中，我们介绍了一种新的Robust Bayesian Optimization算法，AIRBO，可以有效地确定一个Robust optimum，在任意输入不确定性下表现一致性强。我们直接使用Gaussian Process模型来模型不确定的输入，并通过Maximum Mean Discrepancy（MMD）和Nystrom采样加速 posterior 推理。我们提供了严格的理论 regret bound，并在synthetic functions 和实际问题上进行了广泛的实验，证明我们的方法可以处理不同类型的输入不确定性，并达到领先的性能。
</details></li>
</ul>
<hr>
<h2 id="Sample-Conditioned-Hypothesis-Stability-Sharpens-Information-Theoretic-Generalization-Bounds"><a href="#Sample-Conditioned-Hypothesis-Stability-Sharpens-Information-Theoretic-Generalization-Bounds" class="headerlink" title="Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds"></a>Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20102">http://arxiv.org/abs/2310.20102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqiao Wang, Yongyi Mao</li>
<li>for: 提供新的信息理论性 garantue，通过一种新的“邻域假设”矩阵和一个新的样本条件假设稳定性（SCH稳定性）家族。</li>
<li>methods: 使用新的构造和稳定性评价方法来提供更加锐利的信息理论性 garantue，超越了现有的信息理论性 bounds 在不同的学习场景中。</li>
<li>results: 得到的结果是一种更加精细的信息理论性 garantue，可以在某些学习问题中（如统计几何优化问题）提供更高的精度和更好的性能。<details>
<summary>Abstract</summary>
We present new information-theoretic generalization guarantees through the a novel construction of the "neighboring-hypothesis" matrix and a new family of stability notions termed sample-conditioned hypothesis (SCH) stability. Our approach yields sharper bounds that improve upon previous information-theoretic bounds in various learning scenarios. Notably, these bounds address the limitations of existing information-theoretic bounds in the context of stochastic convex optimization (SCO) problems, as explored in the recent work by Haghifam et al. (2023).
</details>
<details>
<summary>摘要</summary>
我们提出了一新的信息理论基础的扩展保证，通过一种新的“邻居假设”矩阵的建构和一新的家族叫做“样本调和假设”（SCH）稳定性。我们的方法可以获得更加锐利的界限，超越了现有信息理论界限在各种学习情况下的限制，特别是在数学 convex 优化（SCO）问题中。Here's a breakdown of the translation:* "新的信息理论基础" (新的信息理论基础) - This phrase emphasizes that the information-theoretic generalization guarantees presented are new and innovative.* "扩展保证" (扩展保证) - This term refers to the guarantees provided by the information-theoretic framework, which ensure that the learned model will generalize well to new data.* "邻居假设" (邻居假设) - This phrase refers to the "neighboring-hypothesis" matrix, which is a novel construction used in the information-theoretic framework to provide tighter bounds.* " sample-conditioned hypothesis (SCH) stability" (样本调和假设（SCH）稳定性) - This term refers to a new family of stability notions used in the information-theoretic framework, which are based on the idea of conditioning on the sample.* "获得更加锐利的界限" (获得更加锐利的界限) - This phrase emphasizes that the information-theoretic generalization guarantees provided by the new construction and stability notions are sharper and more accurate than previous bounds.* "超越了现有信息理论界限" (超越了现有信息理论界限) - This phrase emphasizes that the new information-theoretic generalization guarantees improve upon previous bounds, and provide a more comprehensive and accurate understanding of the generalization abilities of learning algorithms.
</details></li>
</ul>
<hr>
<h2 id="Robust-Learning-for-Smoothed-Online-Convex-Optimization-with-Feedback-Delay"><a href="#Robust-Learning-for-Smoothed-Online-Convex-Optimization-with-Feedback-Delay" class="headerlink" title="Robust Learning for Smoothed Online Convex Optimization with Feedback Delay"></a>Robust Learning for Smoothed Online Convex Optimization with Feedback Delay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20098">http://arxiv.org/abs/2310.20098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Li, Jianyi Yang, Adam Wierman, Shaolei Ren</li>
<li>for: 本研究探讨了具有多步非线性切换成本和反馈延迟的精制online凸优化问题（SOCO）。</li>
<li>methods: 我们提出了一种新的机器学习（ML）加强的在线算法，即可靠性约束学习（RCL），该算法将不可靠的ML预测与可靠的专家在线算法结合在一起，通过受限 проекcion来强化ML预测。</li>
<li>results: 我们证明了RCL可以保证$(1+\lambda)$-竞争力对任何给定专家，而且在多步切换成本和反馈延迟的情况下，RCL也可以显著提高平均性能和稳定性。<details>
<summary>Abstract</summary>
We study a challenging form of Smoothed Online Convex Optimization, a.k.a. SOCO, including multi-step nonlinear switching costs and feedback delay. We propose a novel machine learning (ML) augmented online algorithm, Robustness-Constrained Learning (RCL), which combines untrusted ML predictions with a trusted expert online algorithm via constrained projection to robustify the ML prediction. Specifically,we prove that RCL is able to guarantee$(1+\lambda)$-competitiveness against any given expert for any$\lambda>0$, while also explicitly training the ML model in a robustification-aware manner to improve the average-case performance. Importantly,RCL is the first ML-augmented algorithm with a provable robustness guarantee in the case of multi-step switching cost and feedback delay.We demonstrate the improvement of RCL in both robustness and average performance using battery management for electrifying transportationas a case study.
</details>
<details>
<summary>摘要</summary>
我们研究了一种具有多步非线性跳转成本和反馈延迟的简化在线凸优化问题（SOCO）。我们提出了一种新的机器学习（ML）增强在线算法，即robustness-constrained learning（RCL），该算法将不信任的ML预测与可信的专家在线算法相结合，通过受限 проекtion来强制实现ML预测的稳定性。我们证明了RCL能够保证$(1+\lambda)$-竞争性对任何给定专家，而且同时在平均情况下提高ML预测的性能。这是ML增强算法中第一个具有证明的可靠性保证的多步跳转成本和反馈延迟的情况。我们通过电动汽车续航管理为 случа研究，证明了RCL在稳定性和平均性两个方面的改进。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Between-Variational-Inference-and-Wasserstein-Gradient-Flows"><a href="#Bridging-the-Gap-Between-Variational-Inference-and-Wasserstein-Gradient-Flows" class="headerlink" title="Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows"></a>Bridging the Gap Between Variational Inference and Wasserstein Gradient Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20090">http://arxiv.org/abs/2310.20090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yimx/bridging-the-gap-between-vi-and-wgf">https://github.com/yimx/bridging-the-gap-between-vi-and-wgf</a></li>
<li>paper_authors: Mingxuan Yi, Song Liu</li>
<li>for:  bridges the gap between variational inference and Wasserstein gradient flows</li>
<li>methods:  uses the Bures-Wasserstein gradient flow to recast the Euclidean gradient flow, and uses the path-derivative gradient estimator to generate the vector field of the gradient flow</li>
<li>results:  offers an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow, and extends the gradient estimator to encompass $f$-divergences and non-Gaussian variational families, which can be readily implemented using contemporary machine learning libraries like PyTorch or TensorFlow.Here is the summary in Traditional Chinese:</li>
<li>for:  connects 统计量推理和 Wasserstein 梯度流</li>
<li>methods: 使用 Bures-Wasserstein 梯度流将 Euclidian 梯度流重新推射, 并使用 path-derivative 梯度统计来生成梯度流的向量场</li>
<li>results: 提供了一个 alternative  perspective on path-derivative 梯度, 把其描述为 Wasserstein 梯度流的蒸发程序, 并将 gradient estimator 扩展到包括 $f$-divergence 和非泊松型 variational 家族, 这些可以使用 contemporary 机器学习库 like PyTorch 或 TensorFlow 的 readily implementable 方式进行实现。<details>
<summary>Abstract</summary>
Variational inference is a technique that approximates a target distribution by optimizing within the parameter space of variational families. On the other hand, Wasserstein gradient flows describe optimization within the space of probability measures where they do not necessarily admit a parametric density function. In this paper, we bridge the gap between these two methods. We demonstrate that, under certain conditions, the Bures-Wasserstein gradient flow can be recast as the Euclidean gradient flow where its forward Euler scheme is the standard black-box variational inference algorithm. Specifically, the vector field of the gradient flow is generated via the path-derivative gradient estimator. We also offer an alternative perspective on the path-derivative gradient, framing it as a distillation procedure to the Wasserstein gradient flow. Distillations can be extended to encompass $f$-divergences and non-Gaussian variational families. This extension yields a new gradient estimator for $f$-divergences, readily implementable using contemporary machine learning libraries like PyTorch or TensorFlow.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用变量推断法可以估算目标分布，但是 Wasserstein 梯度流不一定具有参数化概率分布。在这篇论文中，我们证明在某些条件下，布尔-沃asserstein 梯度流可以转化为欧式梯度流，其前向迭代方案与标准黑盒变量推断算法相同。具体来说，梯度流的向量场由路径导数梯度估计生成。我们还提供了一种 alternate 视角，将路径导数梯度框架为 Wasserstein 梯度流的蒸馏过程。这种扩展可以包括 $f$-散度和非泊松变量家族，从而获得一个新的梯度估计器，可以使用现代机器学习库 like PyTorch 或 TensorFlow 实现。Note: "变量推断法" (variable inference) is a more common term in Simplified Chinese, so I used that instead of "变量推断" (variable inference) as in the original text.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.LG_2023_10_31/" data-id="cloh3sr1200rjh688cg3cex7k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/cs.CL_2023_10_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-10-31
        
      </div>
    </a>
  
  
    <a href="/2023/10/31/eess.IV_2023_10_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-10-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
