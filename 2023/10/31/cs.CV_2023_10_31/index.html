
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-10-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Decodable and Sample Invariant Continuous Object Encoder paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00187 repo_url: https:&#x2F;&#x2F;github.com&#x2F;dhyuan99&#x2F;hdfe paper_authors: Dehao Yuan, Furong Huang, Cornelia Fermüll">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-10-31">
<meta property="og:url" content="https://nullscc.github.io/2023/10/31/cs.CV_2023_10_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Decodable and Sample Invariant Continuous Object Encoder paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00187 repo_url: https:&#x2F;&#x2F;github.com&#x2F;dhyuan99&#x2F;hdfe paper_authors: Dehao Yuan, Furong Huang, Cornelia Fermüll">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-31T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-03T00:29:08.013Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.CV_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T13:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-10-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Decodable-and-Sample-Invariant-Continuous-Object-Encoder"><a href="#Decodable-and-Sample-Invariant-Continuous-Object-Encoder" class="headerlink" title="Decodable and Sample Invariant Continuous Object Encoder"></a>Decodable and Sample Invariant Continuous Object Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00187">http://arxiv.org/abs/2311.00187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dhyuan99/hdfe">https://github.com/dhyuan99/hdfe</a></li>
<li>paper_authors: Dehao Yuan, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos</li>
<li>for: 这篇论文旨在提出一种具有sample distribution和density不变性的连续对象编码方法（Hyper-Dimensional Function Encoding，HDFE），以便在机器学习任务中接受连续对象作为输入。</li>
<li>methods: HDFE使用样本分布和浓度不变性来生成连续对象的显式向量表示，并不需要任何训练。这种编码方法可以将连续对象映射到有组织结构的嵌入空间中，并且可以回归连续对象的编码。</li>
<li>results: 在函数到函数映射任务中，使用HDFE可以 achieve competitive performance 与现状之最佳算法。在点云表面法向量估计任务中，将PointNet替换为HDFE，可以直接得到12%和15%的错误降低。此外，将HDFEintegrated into PointNet-based SOTA网络中，可以提高SOTA基eline的2.5%和1.7%。<details>
<summary>Abstract</summary>
We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of a continuous object (e.g. a function), HDFE produces an explicit vector representation of the given object, invariant to the sample distribution and density. Sample distribution and density invariance enables HDFE to consistently encode continuous objects regardless of their sampling, and therefore allows neural networks to receive continuous objects as inputs for machine learning tasks, such as classification and regression. Besides, HDFE does not require any training and is proved to map the object into an organized embedding space, which facilitates the training of the downstream tasks. In addition, the encoding is decodable, which enables neural networks to regress continuous objects by regressing their encodings. Therefore, HDFE serves as an interface for processing continuous objects.   We apply HDFE to function-to-function mapping, where vanilla HDFE achieves competitive performance as the state-of-the-art algorithm. We apply HDFE to point cloud surface normal estimation, where a simple replacement from PointNet to HDFE leads to immediate 12% and 15% error reductions in two benchmarks. In addition, by integrating HDFE into the PointNet-based SOTA network, we improve the SOTA baseline by 2.5% and 1.7% in the same benchmarks.
</details>
<details>
<summary>摘要</summary>
我们提出嵌套维度函数编码（HDFE）。对于连续变数（例如函数）的样本，HDFE 可以生成对应的维度函数表示，不受样本分布和密度的影响。这使得 HDFE 可以对连续变数进行一致性的编码，允许神经网络接受连续变数作为输入进行机器学习任务，如分类和回归。此外，HDFE 不需要任何训练，并证明可以将物件转换到有序的嵌入空间中，便于训练下游任务。此外，编码也是解码的，允许神经网络通过解码来回传连续变数。因此，HDFE 可以作为连续变数的接口。我们将 HDFE 应用于函数转换，其中普通的 HDFE 已经达到了现有算法的竞争水平。我们还将 HDFE 应用于点云表面弹性估计，将 PointNet 替换为 HDFE，即时间降低了两个benchmark中的12%和15%的错误率。此外，将 HDFE 统合到 PointNet-based SOTA 网络中，可以提高 SOTA 基eline的2.5%和1.7%。
</details></li>
</ul>
<hr>
<h2 id="Image-Restoration-with-Point-Spread-Function-Regularization-and-Active-Learning"><a href="#Image-Restoration-with-Point-Spread-Function-Regularization-and-Active-Learning" class="headerlink" title="Image Restoration with Point Spread Function Regularization and Active Learning"></a>Image Restoration with Point Spread Function Regularization and Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00186">http://arxiv.org/abs/2311.00186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Jia, Jiameng Lv, Runyu Ning, Yu Song, Nan Li, Kaifan Ji, Chenzhou Cui, Shanshan Li</li>
<li>for: 这个论文是为了提高天文学观测图像的精度和效率，使用深度学习算法和高精度望远镜模拟器连接。</li>
<li>methods: 该算法使用深度学习算法和高精度望远镜模拟器连接，在训练阶段使用模拟器生成不同水平的噪声和杂散来训练神经网络，然后直接使用神经网络来修复天文学观测图像。</li>
<li>results: 该算法可以有效地提高天文学观测图像中的细结构，提高观测图像的质量，并可以应用于大规模天文学观测数据，如LSST、Euclid和CSST等。<details>
<summary>Abstract</summary>
Large-scale astronomical surveys can capture numerous images of celestial objects, including galaxies and nebulae. Analysing and processing these images can reveal intricate internal structures of these objects, allowing researchers to conduct comprehensive studies on their morphology, evolution, and physical properties. However, varying noise levels and point spread functions can hamper the accuracy and efficiency of information extraction from these images. To mitigate these effects, we propose a novel image restoration algorithm that connects a deep learning-based restoration algorithm with a high-fidelity telescope simulator. During the training stage, the simulator generates images with different levels of blur and noise to train the neural network based on the quality of restored images. After training, the neural network can directly restore images obtained by the telescope, as represented by the simulator. We have tested the algorithm using real and simulated observation data and have found that it effectively enhances fine structures in blurry images and increases the quality of observation images. This algorithm can be applied to large-scale sky survey data, such as data obtained by LSST, Euclid, and CSST, to further improve the accuracy and efficiency of information extraction, promoting advances in the field of astronomical research.
</details>
<details>
<summary>摘要</summary>
大规模天文观测可以捕捉众多的天体图像，包括星系和星云。分析和处理这些图像可以揭示天体的内部结构，帮助研究人员进行全面的 morphology、演化和物理性质的研究。然而，天文图像中的噪声和点扩散函数可能会增加图像信息提取的准确性和效率问题。为了解决这些问题，我们提议一种新的图像修复算法，该算法将深度学习基于的修复算法与高精度望远镜模拟器相连接。在训练阶段，模拟器生成了不同噪声和扩散函数的图像，以训练基于图像质量的 neural network。之后，这个 neural network 可以直接修复望远镜获得的图像，如由模拟器表示的。我们对实际和模拟观测数据进行测试，发现该算法可以有效地提高模糊图像中的细节，提高观测图像的质量。这种算法可以应用于大规模天文观测数据，如 LSST、Euclid 和 CSST 等，以提高信息提取的准确性和效率，推动天文研究领域的进步。
</details></li>
</ul>
<hr>
<h2 id="Object-centric-Video-Representation-for-Long-term-Action-Anticipation"><a href="#Object-centric-Video-Representation-for-Long-term-Action-Anticipation" class="headerlink" title="Object-centric Video Representation for Long-term Action Anticipation"></a>Object-centric Video Representation for Long-term Action Anticipation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00180">http://arxiv.org/abs/2311.00180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brown-palm/objectprompt">https://github.com/brown-palm/objectprompt</a></li>
<li>paper_authors: Ce Zhang, Changcheng Fu, Shijie Wang, Nakul Agarwal, Kwonjoon Lee, Chiho Choi, Chen Sun</li>
<li>for: 这paper的目的是建立长期行为预测视频中对象中心表示，以便更好地识别和预测人类-物品交互。</li>
<li>methods: 该paper使用了可见语言预测模型，并提出了“对象提示”方法，以EXTRACT task-specific对象中心表示。它还使用了一种基于Transformer的神经网络，以便在不同的时间尺度内“检索”有关的对象。</li>
<li>results: 该paper的结果表明，使用该方法可以有效地识别和预测人类-物品交互。EXTENSIVE evaluations on Ego4D, 50Salads, and EGTEA Gaze+ benchmarks confirm the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
This paper focuses on building object-centric representations for long-term action anticipation in videos. Our key motivation is that objects provide important cues to recognize and predict human-object interactions, especially when the predictions are longer term, as an observed "background" object could be used by the human actor in the future. We observe that existing object-based video recognition frameworks either assume the existence of in-domain supervised object detectors or follow a fully weakly-supervised pipeline to infer object locations from action labels. We propose to build object-centric video representations by leveraging visual-language pretrained models. This is achieved by "object prompts", an approach to extract task-specific object-centric representations from general-purpose pretrained models without finetuning. To recognize and predict human-object interactions, we use a Transformer-based neural architecture which allows the "retrieval" of relevant objects for action anticipation at various time scales. We conduct extensive evaluations on the Ego4D, 50Salads, and EGTEA Gaze+ benchmarks. Both quantitative and qualitative results confirm the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "object-centric" is translated as "物体中心的" (wùtǐ zhōngxīn de), which emphasizes the focus on objects in the representation.* "long-term" is translated as "长期的" (chángqī de), which emphasizes the duration of the predictions.* "human-object interactions" is translated as "人物交互" (rénwù jiāoxì), which emphasizes the interactions between humans and objects.* "infer" is translated as "推断" (tuīdàn), which emphasizes the process of making predictions based on the available information.* "object prompts" is translated as "对象提示" (duìyè tiēshí), which emphasizes the use of object cues to extract task-specific representations.* "retrieval" is translated as "检索" (jiǎnsuǒ), which emphasizes the process of retrieving relevant objects for action anticipation.* "quantitative" and "qualitative" are translated as "数量的" (shùliàng de) and "质量的" (zhìliàng de), respectively, which emphasize the different aspects of the results.
</details></li>
</ul>
<hr>
<h2 id="Multi-task-Deep-Convolutional-Network-to-Predict-Sea-Ice-Concentration-and-Drift-in-the-Arctic-Ocean"><a href="#Multi-task-Deep-Convolutional-Network-to-Predict-Sea-Ice-Concentration-and-Drift-in-the-Arctic-Ocean" class="headerlink" title="Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean"></a>Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00167">http://arxiv.org/abs/2311.00167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Younghyun Koo, Maryam Rahnemoonfar</li>
<li>for: 预测北极海洋中的海冰 koncentración (SIC) 和海冰移动 (SID) 的研究对于了解当前气候变化的影响非常重要。</li>
<li>methods: 本研究提出了一种新的多任务彻底学习网络架构，即层次信息共享U-net (HIS-Unet)，用于每天预测 SIC 和 SID。该架构不同于单独将 SIC 和 SID 分别学习为每个分支，而是通过加权注意模块 (WAMs) 来让 SIC 和 SID 层共享信息，以提高预测性能。</li>
<li>results: 对比其他统计方法、物理海冰模型和神经网络，HIS-Unet 显著提高了 SIC 和 SID 预测性能。具体来说，HIS-Unet 在seasonal sea ice 和 multi-year sea ice 的变化季节中的预测性能都显著提高，这表明信息共享通过 WAMs 允许模型学习海冰的突然变化。此外，WAMs 的权值表明，SIC 信息在 SID 预测中扮演更重要的角色，而 SID 信息在 SIC 预测中的作用相对较弱。同时，信息共享在海冰边缘（seasonal sea ice）更为活跃。<details>
<summary>Abstract</summary>
Forecasting sea ice concentration (SIC) and sea ice drift (SID) in the Arctic Ocean is of great significance as the Arctic environment has been changed by the recent warming climate. Given that physical sea ice models require high computational costs with complex parameterization, deep learning techniques can effectively replace the physical model and improve the performance of sea ice prediction. This study proposes a novel multi-task fully conventional network architecture named hierarchical information-sharing U-net (HIS-Unet) to predict daily SIC and SID. Instead of learning SIC and SID separately at each branch, we allow the SIC and SID layers to share their information and assist each other's prediction through the weighting attention modules (WAMs). Consequently, our HIS-Unet outperforms other statistical approaches, sea ice physical models, and neural networks without such information-sharing units. The improvement of HIS-Unet is obvious both for SIC and SID prediction when and where sea ice conditions change seasonally, which implies that the information sharing through WAMs allows the model to learn the sudden changes of SIC and SID. The weight values of the WAMs imply that SIC information plays a more critical role in SID prediction, compared to that of SID information in SIC prediction, and information sharing is more active in sea ice edges (seasonal sea ice) than in the central Arctic (multi-year sea ice).
</details>
<details>
<summary>摘要</summary>
预测北极海洋中的海冰浓度（SIC）和海冰移动（SID）具有重要的意义，因为北极环境已经由最近的气候变化所改变。由于物理海冰模型需要高度的计算成本和复杂的参数化，深度学习技术可以有效地取代物理模型，提高海冰预测性能。本研究提出了一种新的多任务完全конволюцион网络架构，即层次信息共享U-Net（HIS-Unet），用于每天预测SIC和SID。而不是在每个分支中分别学习SIC和SID，我们允许SIC和SID层共享信息，通过权重注意模块（WAMs）帮助对方预测。因此，我们的HIS-Unet比其他统计方法、海冰物理模型和无此信息共享单元的神经网络更高效。HIS-Unet的改进显著 both SIC和SID预测，特别是在季节性变化的海冰条件下，这表明信息共享 через WAMs 让模型学习海冰的突然变化。WAMs 的权值表明，SIC 信息在 SID 预测中扮演更重要的角色，而 SID 信息在 SIC 预测中的作用相对较弱，信息共享更活跃在海冰边缘（季节性海冰）than in the central Arctic（多年海冰）。
</details></li>
</ul>
<hr>
<h2 id="Medi-CAT-Contrastive-Adversarial-Training-for-Medical-Image-Classification"><a href="#Medi-CAT-Contrastive-Adversarial-Training-for-Medical-Image-Classification" class="headerlink" title="Medi-CAT: Contrastive Adversarial Training for Medical Image Classification"></a>Medi-CAT: Contrastive Adversarial Training for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00154">http://arxiv.org/abs/2311.00154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pervaiz Iqbal Khan, Andreas Dengel, Sheraz Ahmed<br>for: This paper aims to overcome the underfitting and overfitting phenomena in medical imaging datasets by proposing a training strategy called Medi-CAT.methods: The proposed training methodology employs large pre-trained vision transformers and combines adversarial and contrastive learning techniques to prevent overfitting.results: The proposed approach improves the accuracy up to 2% on three benchmark datasets compared to well-known approaches, and increases the performance up to 4.1% over the baseline methods.Here’s the Chinese translation of the information:for: 这篇论文目标是解决医疗影像Dataset中的下降和过拟合现象，提出了一种名为 Medi-CAT的训练策略。methods: 提议的训练方法使用大量预训练的视觉变换器，并结合对抗学习和对比学习技术来避免过拟合。results: 该方法对三个标准数据集进行比较，与常见方法相比，准确率提高了2%，而与基线方法相比，性能提高了4.1%。<details>
<summary>Abstract</summary>
There are not many large medical image datasets available. For these datasets, too small deep learning models can't learn useful features, so they don't work well due to underfitting, and too big models tend to overfit the limited data. As a result, there is a compromise between the two issues. This paper proposes a training strategy Medi-CAT to overcome the underfitting and overfitting phenomena in medical imaging datasets. Specifically, the proposed training methodology employs large pre-trained vision transformers to overcome underfitting and adversarial and contrastive learning techniques to prevent overfitting. The proposed method is trained and evaluated on four medical image classification datasets from the MedMNIST collection. Our experimental results indicate that the proposed approach improves the accuracy up to 2% on three benchmark datasets compared to well-known approaches, whereas it increases the performance up to 4.1% over the baseline methods.
</details>
<details>
<summary>摘要</summary>
“医疗图像 dataset 规模较小，深度学习模型会因为数据有限而难以学习有用特征，导致过拟合和下降风险。为了解决这两个问题，这篇论文提出了一种协调训练策略 Medi-CAT。具体来说，提出的训练方法使用大型预训练视Transformer来抵消下降风险，并使用对抗和对比学习技术来避免过拟合。我们对 MedMNIST 收集中的四个医疗图像分类数据集进行训练和评估，实验结果表明，我们的方法可以与传统方法相比提高准确率，最高提高4.1%。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Joint-Depth-Prediction-and-Semantic-Segmentation-with-Multi-View-SAM"><a href="#Joint-Depth-Prediction-and-Semantic-Segmentation-with-Multi-View-SAM" class="headerlink" title="Joint Depth Prediction and Semantic Segmentation with Multi-View SAM"></a>Joint Depth Prediction and Semantic Segmentation with Multi-View SAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00134">http://arxiv.org/abs/2311.00134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mykhailo Shvets, Dongxu Zhao, Marc Niethammer, Roni Sengupta, Alexander C. Berg</li>
<li>for: 这个论文是为了提出一种多视图雷达（MVS）技术，以利用 semantic features 进行深度预测和 segmentation 预测。</li>
<li>methods: 该论文使用了 Segment Anything Model (SAM) 提取 semantic features，并使用 Transformer 搅拌模型进行 semantic segmentation 预测。</li>
<li>results: 该论文在 ScanNet 数据集上进行了量化和质量研究，并表明其方法可以比单任务 MVS 和 segmentation 模型、以及多任务 monocular 方法表现更好。<details>
<summary>Abstract</summary>
Multi-task approaches to joint depth and segmentation prediction are well-studied for monocular images. Yet, predictions from a single-view are inherently limited, while multiple views are available in many robotics applications. On the other end of the spectrum, video-based and full 3D methods require numerous frames to perform reconstruction and segmentation. With this work we propose a Multi-View Stereo (MVS) technique for depth prediction that benefits from rich semantic features of the Segment Anything Model (SAM). This enhanced depth prediction, in turn, serves as a prompt to our Transformer-based semantic segmentation decoder. We report the mutual benefit that both tasks enjoy in our quantitative and qualitative studies on the ScanNet dataset. Our approach consistently outperforms single-task MVS and segmentation models, along with multi-task monocular methods.
</details>
<details>
<summary>摘要</summary>
多任务方法为单视图图像的深度预测和分割预测是已经广泛研究的。然而，单视图预测的预测是有限的，而多视图应用中有很多视图可用。另一方面，视频基于和全3D方法需要许多帧来进行重建和分割。在这种情况下，我们提出了一种基于多视图ステレオ（MVS）技术，利用rich的 semantic feature来提高深度预测。这种增强的深度预测，然后作为我们基于Transformer的semantic segmentation解码器的引导。我们在ScanNet数据集上进行了量化和质量研究，并发现我们的方法在单任务MVS和分类任务上都有明显的提高，同时与多任务单视图方法和semantic segmentation方法相比也有着良好的性能。
</details></li>
</ul>
<hr>
<h2 id="Spuriosity-Rankings-for-Free-A-Simple-Framework-for-Last-Layer-Retraining-Based-on-Object-Detection"><a href="#Spuriosity-Rankings-for-Free-A-Simple-Framework-for-Last-Layer-Retraining-Based-on-Object-Detection" class="headerlink" title="Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection"></a>Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00079">http://arxiv.org/abs/2311.00079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Azizmalayeri, Reza Abbasi, Amir Hosein Haji Mohammad rezaie, Reihaneh Zohrabi, Mahdi Amiri, Mohammad Taghi Manzuri, Mohammad Hossein Rohban</li>
<li>for: 该 paper 是为了解决深度神经网络模型中的假样特征问题而写的。</li>
<li>methods: 该 paper 使用了一种开 vocabulary 对象检测技术来评估图像中的目标对象存在程度，然后根据这个分数对图像进行排序，并将最后一层模型 retrained 在排序后的数据上。</li>
<li>results: 该 paper 的实验结果表明，该排序 frameworks 可以准确地排序图像按照假样程度，并且可以使用这些图像进行 last-layer retraining，从而提高模型的可靠性。<details>
<summary>Abstract</summary>
Deep neural networks have exhibited remarkable performance in various domains. However, the reliance of these models on spurious features has raised concerns about their reliability. A promising solution to this problem is last-layer retraining, which involves retraining the linear classifier head on a small subset of data without spurious cues. Nevertheless, selecting this subset requires human supervision, which reduces its scalability. Moreover, spurious cues may still exist in the selected subset. As a solution to this problem, we propose a novel ranking framework that leverages an open vocabulary object detection technique to identify images without spurious cues. More specifically, we use the object detector as a measure to score the presence of the target object in the images. Next, the images are sorted based on this score, and the last-layer of the model is retrained on a subset of the data with the highest scores. Our experiments on the ImageNet-1k dataset demonstrate the effectiveness of this ranking framework in sorting images based on spuriousness and using them for last-layer retraining.
</details>
<details>
<summary>摘要</summary>
To overcome these limitations, we propose a novel ranking framework that leverages open vocabulary object detection techniques to identify images without spurious cues. Specifically, we use an object detector to measure the presence of the target object in the images, and then sort the images based on this score. Finally, we retrain the last layer of the model on a subset of the data with the highest scores. Our experiments on the ImageNet-1k dataset show that our ranking framework is effective in sorting images based on spuriousness and using them for last-layer retraining.
</details></li>
</ul>
<hr>
<h2 id="YOLOv8-Based-Visual-Detection-of-Road-Hazards-Potholes-Sewer-Covers-and-Manholes"><a href="#YOLOv8-Based-Visual-Detection-of-Road-Hazards-Potholes-Sewer-Covers-and-Manholes" class="headerlink" title="YOLOv8-Based Visual Detection of Road Hazards: Potholes, Sewer Covers, and Manholes"></a>YOLOv8-Based Visual Detection of Road Hazards: Potholes, Sewer Covers, and Manholes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00073">http://arxiv.org/abs/2311.00073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Om M. Khare, Shubham Gandhi, Aditya M. Rahalkar, Sunil Mane</li>
<li>for: 本研究旨在评估YOLOv8对道路危险的检测，包括孔隙、排水涂层和人洞。</li>
<li>methods: 本研究使用YOLOv8对象检测模型进行评估，并进行了相对分析与前一代YOLOv5和YOLOv7模型。图像预处理技术和hyperparameter调整也被研究以提高检测精度。</li>
<li>results: 研究发现YOLOv8在不同的照明条件、路径类型、危险大小和类型下的检测精度较高，并且在多个测试场景下 Displaying mAP 分数以评估模型的稳定性和通用性。<details>
<summary>Abstract</summary>
Effective detection of road hazards plays a pivotal role in road infrastructure maintenance and ensuring road safety. This research paper provides a comprehensive evaluation of YOLOv8, an object detection model, in the context of detecting road hazards such as potholes, Sewer Covers, and Man Holes. A comparative analysis with previous iterations, YOLOv5 and YOLOv7, is conducted, emphasizing the importance of computational efficiency in various applications. The paper delves into the architecture of YOLOv8 and explores image preprocessing techniques aimed at enhancing detection accuracy across diverse conditions, including variations in lighting, road types, hazard sizes, and types. Furthermore, hyperparameter tuning experiments are performed to optimize model performance through adjustments in learning rates, batch sizes, anchor box sizes, and augmentation strategies. Model evaluation is based on Mean Average Precision (mAP), a widely accepted metric for object detection performance. The research assesses the robustness and generalization capabilities of the models through mAP scores calculated across the diverse test scenarios, underlining the significance of YOLOv8 in road hazard detection and infrastructure maintenance.
</details>
<details>
<summary>摘要</summary>
通过有效检测公路障碍物来维护公路基础设施和保障公路安全的作用非常重要。本研究论文对YOLOv8对象检测模型在检测公路障碍物方面进行了全面的评估，包括缺陷排水沟、排障板和人洞。与前两代YOLOv5和YOLOv7模型进行比较，研究强调了计算效率的重要性在不同应用场景中。文章还详细介绍了YOLOv8的架构和图像预处理技术，以提高检测精度在不同的照明、公路类型、障碍物大小和类型等条件下。此外，文章还进行了模型参数调整实验，以优化模型性能通过学习率、批处理大小、固定盒子大小和扩展策略的调整。模型评估基于 Mean Average Precision（mAP）度量，这是对象检测性能的广泛接受的指标。研究通过在多种测试场景下计算出的mAP分数，证明YOLOv8在公路障碍物检测和基础设施维护中的重要性。
</details></li>
</ul>
<hr>
<h2 id="View-Classification-and-Object-Detection-in-Cardiac-Ultrasound-to-Localize-Valves-via-Deep-Learning"><a href="#View-Classification-and-Object-Detection-in-Cardiac-Ultrasound-to-Localize-Valves-via-Deep-Learning" class="headerlink" title="View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning"></a>View Classification and Object Detection in Cardiac Ultrasound to Localize Valves via Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00068">http://arxiv.org/abs/2311.00068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Derya Gol Gungor, Bimba Rao, Cynthia Wolverton, Ismayil Guracar</li>
<li>For: 这个研究用于提供一种基于深度神经网络的echocardiography图像分类和LOCAL化方法，以便为临床医生提供实时、低成本、无辐射的心脏功能观察工具。* Methods: 该研究使用了深度神经网络进行分类和LOCAL化步骤，首先应用视图分类于心脏ultrasound图像中的10种各种 анатомиче视图，然后使用深度学习基于对象检测来bothLocalize和识别心脏钳。* Results: 对于Apical视图，我们的对象检测实验表明可以准确地LOCALize和识别多个钳。<details>
<summary>Abstract</summary>
Echocardiography provides an important tool for clinicians to observe the function of the heart in real time, at low cost, and without harmful radiation. Automated localization and classification of heart valves enables automatic extraction of quantities associated with heart mechanical function and related blood flow measurements. We propose a machine learning pipeline that uses deep neural networks for separate classification and localization steps. As the first step in the pipeline, we apply view classification to echocardiograms with ten unique anatomic views of the heart. In the second step, we apply deep learning-based object detection to both localize and identify the valves. Image segmentation based object detection in echocardiography has been shown in many earlier studies but, to the best of our knowledge, this is the first study that predicts the bounding boxes around the valves along with classification from 2D ultrasound images with the help of deep neural networks. Our object detection experiments applied to the Apical views suggest that it is possible to localize and identify multiple valves precisely.
</details>
<details>
<summary>摘要</summary>
《echo心动影像提供了低成本、不含辐射的实时心脏功能观察工具，对医生来说非常重要。我们提议一个基于深度神经网络的机器学习管道，包括分类和定位两个步骤。第一步是通过视图分类来处理心动影像的十种不同的解剖视图。第二步是使用深度学习基于对象检测来本地化和识别心脏钱币。在许多先前的研究中，图像分割基于对象检测在心动影像中已经被证明可行，但是，到目前为止，这是第一个通过深度神经网络预测心脏钱币 bounding box 的研究。我们的对象检测实验在Apical视图中表明，可以准确地本地化和识别多个钱币。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="FPO-Efficient-Encoding-and-Rendering-of-Dynamic-Neural-Radiance-Fields-by-Analyzing-and-Enhancing-Fourier-PlenOctrees"><a href="#FPO-Efficient-Encoding-and-Rendering-of-Dynamic-Neural-Radiance-Fields-by-Analyzing-and-Enhancing-Fourier-PlenOctrees" class="headerlink" title="FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance Fields by Analyzing and Enhancing Fourier PlenOctrees"></a>FPO++: Efficient Encoding and Rendering of Dynamic Neural Radiance Fields by Analyzing and Enhancing Fourier PlenOctrees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20710">http://arxiv.org/abs/2310.20710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saskia Rabich, Patrick Stotko, Reinhard Klein</li>
<li>for: 提高 NeRF 模型的动态渲染效果，并解决在结合最新技术时引入的压缩 artifacts 问题</li>
<li>methods: 使用改进的 Fourier PlenOctrees 表示法，包括适应式质量编码和减少压缩 artifacts 的技术</li>
<li>results: 在 sintetic 和实际场景中进行了评估，并显示了改进后的 Fourier PlenOctrees 可以减少 artifacts 并提高动态 NeRF 模型的效果<details>
<summary>Abstract</summary>
Fourier PlenOctrees have shown to be an efficient representation for real-time rendering of dynamic Neural Radiance Fields (NeRF). Despite its many advantages, this method suffers from artifacts introduced by the involved compression when combining it with recent state-of-the-art techniques for training the static per-frame NeRF models. In this paper, we perform an in-depth analysis of these artifacts and leverage the resulting insights to propose an improved representation. In particular, we present a novel density encoding that adapts the Fourier-based compression to the characteristics of the transfer function used by the underlying volume rendering procedure and leads to a substantial reduction of artifacts in the dynamic model. Furthermore, we show an augmentation of the training data that relaxes the periodicity assumption of the compression. We demonstrate the effectiveness of our enhanced Fourier PlenOctrees in the scope of quantitative and qualitative evaluations on synthetic and real-world scenes.
</details>
<details>
<summary>摘要</summary>
傅里叶普莱树（Fourier PlenOctrees）已经显示为实时渲染动态神经辐射场（NeRF）的有效表示方法。尽管它具有许多优点，但这种方法受到了与现代训练静态每帧 NeRF 模型的压缩相关的artefacts的影响。在这篇论文中，我们进行了深入的分析这些artefacts，并利用结果的启示来提出改进的表示方法。具体来说，我们提出了一种适应转移函数使用的 Fourier 基式压缩的新密度编码方法，导致 artefacts 在动态模型中减少了许多。此外，我们还展示了对压缩数据的扩展，以减少压缩的 периодичность假设。我们在synthetic和实际场景中进行了评估，并证明了我们的改进后的傅里叶普莱树的效iveness。
</details></li>
</ul>
<hr>
<h2 id="DDAM-PS-Diligent-Domain-Adaptive-Mixer-for-Person-Search"><a href="#DDAM-PS-Diligent-Domain-Adaptive-Mixer-for-Person-Search" class="headerlink" title="DDAM-PS: Diligent Domain Adaptive Mixer for Person Search"></a>DDAM-PS: Diligent Domain Adaptive Mixer for Person Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20706">http://arxiv.org/abs/2310.20706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mustansarfiaz/ddam-ps">https://github.com/mustansarfiaz/ddam-ps</a></li>
<li>paper_authors: Mohammed Khaleed Almansoori, Mustansar Fiaz, Hisham Cholakkal</li>
<li>for: 本研究主要针对人寻找（PS）问题进行研究，实现了人检测和重新识别（ReID）的共同优化。</li>
<li>methods: 我们提出了一个努力域适应混合（DDAM）模组，用于对人寻找（DDAP-PS）框架。这个模组搭配了原始和目标域的表示，以生成中等混合域表示。这个DDAM模组实现了域混合，以减少两个极端域之间的距离，进而提高ReID任务。</li>
<li>results: 我们透过实验 validate了我们的提案的有效性。我们的方法在PRW和CUHK-SYSU datasets上显示了良好的性能。我们的原始代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/mustansarfiaz/DDAM-PS%7D">https://github.com/mustansarfiaz/DDAM-PS}</a> 上获取。<details>
<summary>Abstract</summary>
Person search (PS) is a challenging computer vision problem where the objective is to achieve joint optimization for pedestrian detection and re-identification (ReID). Although previous advancements have shown promising performance in the field under fully and weakly supervised learning fashion, there exists a major gap in investigating the domain adaptation ability of PS models. In this paper, we propose a diligent domain adaptive mixer (DDAM) for person search (DDAP-PS) framework that aims to bridge a gap to improve knowledge transfer from the labeled source domain to the unlabeled target domain. Specifically, we introduce a novel DDAM module that generates moderate mixed-domain representations by combining source and target domain representations. The proposed DDAM module encourages domain mixing to minimize the distance between the two extreme domains, thereby enhancing the ReID task. To achieve this, we introduce two bridge losses and a disparity loss. The objective of the two bridge losses is to guide the moderate mixed-domain representations to maintain an appropriate distance from both the source and target domain representations. The disparity loss aims to prevent the moderate mixed-domain representations from being biased towards either the source or target domains, thereby avoiding overfitting. Furthermore, we address the conflict between the two subtasks, localization and ReID, during domain adaptation. To handle this cross-task conflict, we forcefully decouple the norm-aware embedding, which aids in better learning of the moderate mixed-domain representation. We conduct experiments to validate the effectiveness of our proposed method. Our approach demonstrates favorable performance on the challenging PRW and CUHK-SYSU datasets. Our source code is publicly available at \url{https://github.com/mustansarfiaz/DDAM-PS}.
</details>
<details>
<summary>摘要</summary>
人体搜索（PS）是一个Computer Vision中的挑战，既需要实现人体检测和重新识别（ReID）的 JOINT 优化。尽管过去的进展有许多提高在这个领域中，但是存在一个主要的域适应能力研究的空白。在这篇论文中，我们提出了一个勤奋的域适应混合器（DDAM） для人体搜索（DDAP-PS）框架，以增强域适应能力。特别是，我们引入了一个新的DDAM模块，该模块将源频率和目标频率的表示混合起来，以生成中等域适应表示。我们的DDAM模块鼓励域适应，以降低两个极端域之间的距离，从而提高ReID任务。为此，我们引入了两个桥接损失和一个差异损失。两个桥接损失的目的是使中等域适应表示维持与源频率和目标频率的两个域的相应距离。差异损失的目的是避免中等域适应表示受到源频率或目标频率的偏袋，以避免过拟合。此外，我们解决了在域适应中的局部化和ReID之间的冲突。我们强制耦合 норма化表示，以便更好地学习中等域适应表示。我们进行了实验，以证明我们的方法的效果。我们的方法在PRW和CUHK-SYSU datasets上表现出色。我们的源代码可以在 <https://github.com/mustansarfiaz/DDAM-PS> 中下载。
</details></li>
</ul>
<hr>
<h2 id="SEINE-Short-to-Long-Video-Diffusion-Model-for-Generative-Transition-and-Prediction"><a href="#SEINE-Short-to-Long-Video-Diffusion-Model-for-Generative-Transition-and-Prediction" class="headerlink" title="SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction"></a>SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20700">http://arxiv.org/abs/2310.20700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, Ziwei Liu</li>
<li>for: 这篇论文旨在生成高质量的长视频（story-level），包括创新的过渡和预测效果。</li>
<li>methods: 该论文提出了一种短视频扩散模型，即 SEINE，用于生成高质量的过渡和预测。该模型根据文本描述自动生成过渡，并使用图像作为输入，以确保视觉质量和凝聚性。</li>
<li>results: 经验证实表明，该模型可以有效地生成高质量的长视频，并且可以扩展到其他任务，如图像到视频动画和自适应视频预测。为评估该新的生成任务，我们提出了三个评价标准：时间一致性、semantic similarity和视频文本匹配。<details>
<summary>Abstract</summary>
Recently video generation has achieved substantial progress with realistic results. Nevertheless, existing AI-generated videos are usually very short clips ("shot-level") depicting a single scene. To deliver a coherent long video ("story-level"), it is desirable to have creative transition and prediction effects across different clips. This paper presents a short-to-long video diffusion model, SEINE, that focuses on generative transition and prediction. The goal is to generate high-quality long videos with smooth and creative transitions between scenes and varying lengths of shot-level videos. Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions. By providing the images of different scenes as inputs, combined with text-based control, our model generates transition videos that ensure coherence and visual quality. Furthermore, the model can be readily extended to various tasks such as image-to-video animation and autoregressive video prediction. To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment. Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos. Project page: https://vchitect.github.io/SEINE-project/ .
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="NeRF-Revisited-Fixing-Quadrature-Instability-in-Volume-Rendering"><a href="#NeRF-Revisited-Fixing-Quadrature-Instability-in-Volume-Rendering" class="headerlink" title="NeRF Revisited: Fixing Quadrature Instability in Volume Rendering"></a>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20685">http://arxiv.org/abs/2310.20685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikacuy/PL-NeRF">https://github.com/mikacuy/PL-NeRF</a></li>
<li>paper_authors: Mikaela Angelina Uy, Kiyohiro Nakayama, Guandao Yang, Rahul Krishna Thomas, Leonidas Guibas, Ke Li</li>
<li>For: 本研究旨在解决NeRF中的 quadrature instability问题，提高rendering的稳定性和质量。* Methods: 提出一种基于 математиче原理的解决方案，通过修改样本基于渲染公式，使其对应于piecewise线性Volume density的精确积分。* Results: 与传统样本基于渲染公式相比，提出的方法具有更加锐利的 texture、更好的几何重建和更强的深度超视觉。此外，本方法可以与现有NeRF方法的volume rendering公式进行互换使用。<details>
<summary>Abstract</summary>
Neural radiance fields (NeRF) rely on volume rendering to synthesize novel views. Volume rendering requires evaluating an integral along each ray, which is numerically approximated with a finite sum that corresponds to the exact integral along the ray under piecewise constant volume density. As a consequence, the rendered result is unstable w.r.t. the choice of samples along the ray, a phenomenon that we dub quadrature instability. We propose a mathematically principled solution by reformulating the sample-based rendering equation so that it corresponds to the exact integral under piecewise linear volume density. This simultaneously resolves multiple issues: conflicts between samples along different rays, imprecise hierarchical sampling, and non-differentiability of quantiles of ray termination distances w.r.t. model parameters. We demonstrate several benefits over the classical sample-based rendering equation, such as sharper textures, better geometric reconstruction, and stronger depth supervision. Our proposed formulation can be also be used as a drop-in replacement to the volume rendering equation of existing NeRF-based methods. Our project page can be found at pl-nerf.github.io.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="StairNet-Visual-Recognition-of-Stairs-for-Human-Robot-Locomotion"><a href="#StairNet-Visual-Recognition-of-Stairs-for-Human-Robot-Locomotion" class="headerlink" title="StairNet: Visual Recognition of Stairs for Human-Robot Locomotion"></a>StairNet: Visual Recognition of Stairs for Human-Robot Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20666">http://arxiv.org/abs/2310.20666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Garrett Kurbis, Dmytro Kuzmenko, Bogdan Ivanyuk-Skulskiy, Alex Mihailidis, Brokoslaw Laschowski</li>
<li>for: 这个论文的目的是提供一个可靠的视觉感知系统，以便人机系统在复杂的地形上进行行走。</li>
<li>methods: 这个论文使用了多种深度学习模型（如2D和3D CNN、混合CNN和LSTM、ViT网络）和训练方法（如监督学习和半监督学习），并使用了大规模的数据集进行训练。</li>
<li>results: 这个论文的实验结果表明，使用StairNet可以实现高精度的视觉感知系统，并且可以在移动设备上进行实时推理。但是，由于嵌入式硬件的限制，在智能眼镜上进行推理时的速度相对较慢。<details>
<summary>Abstract</summary>
Human-robot walking with prosthetic legs and exoskeletons, especially over complex terrains such as stairs, remains a significant challenge. Egocentric vision has the unique potential to detect the walking environment prior to physical interactions, which can improve transitions to and from stairs. This motivated us to create the StairNet initiative to support the development of new deep learning models for visual sensing and recognition of stairs, with an emphasis on lightweight and efficient neural networks for onboard real-time inference. In this study, we present an overview of the development of our large-scale dataset with over 515,000 manually labeled images, as well as our development of different deep learning models (e.g., 2D and 3D CNN, hybrid CNN and LSTM, and ViT networks) and training methods (e.g., supervised learning with temporal data and semi-supervised learning with unlabeled images) using our new dataset. We consistently achieved high classification accuracy (i.e., up to 98.8%) with different designs, offering trade-offs between model accuracy and size. When deployed on mobile devices with GPU and NPU accelerators, our deep learning models achieved inference speeds up to 2.8 ms. We also deployed our models on custom-designed CPU-powered smart glasses. However, limitations in the embedded hardware yielded slower inference speeds of 1.5 seconds, presenting a trade-off between human-centered design and performance. Overall, we showed that StairNet can be an effective platform to develop and study new visual perception systems for human-robot locomotion with applications in exoskeleton and prosthetic leg control.
</details>
<details>
<summary>摘要</summary>
人机步行使用 prósthetic 脚和外套体系，特别是在复杂的地形上，如楼梯，仍然是一项 significante 挑战。 egocentric 视觉具有特殊的潜在力量，可以在物理互动之前探测步行环境，这有助于改善楼梯之间和楼梯之间的过渡。这种挑战引发了我们创建 StairNet Initiave，以支持开发新的深度学习模型，用于视觉感知和识别楼梯，强调轻量级和高效的神经网络，以便在实时进行boards 的推理。在这种研究中，我们介绍了我们开发的大规模数据集，包括515,000多个手动标注的图像，以及我们开发的不同的深度学习模型（如2D和3D CNN、混合 CNN和LSTM网络）和训练方法（如监督学习和 semi-监督学习）。我们在不同的设计中均实现了高精度分类（达到98.8%），提供了模型精度和大小之间的质量负担。当部署在移动设备上（含GPU和NPU加速器）时，我们的深度学习模型实现了1.5秒的推理速度。此外，我们还部署了我们的模型在自定义CPU驱动的智能眼镜上。尽管嵌入式硬件的限制导致推理速度为1.5秒，但是我们展示了StairNet可以成为人机步行视觉系统的有效平台，并具有应用于外套和 prósthetic 脚控制的可能性。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Limitations-of-State-Aware-Imitation-Learning-for-Autonomous-Driving"><a href="#Addressing-Limitations-of-State-Aware-Imitation-Learning-for-Autonomous-Driving" class="headerlink" title="Addressing Limitations of State-Aware Imitation Learning for Autonomous Driving"></a>Addressing Limitations of State-Aware Imitation Learning for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20650">http://arxiv.org/abs/2310.20650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Cultrera, Federico Becattini, Lorenzo Seidenari, Pietro Pala, Alberto Del Bimbo</li>
<li>for: 提高自动驾驶代理人的性能和可靠性</li>
<li>methods: 使用多任务学习和多Stagevision transformer，并将车辆状态作为特殊令牌进行传播</li>
<li>results: 降低偏斜问题和在线和离线性能之间的相关性问题，并提高了驾驶策略的学习和决策的可见性。<details>
<summary>Abstract</summary>
Conditional Imitation learning is a common and effective approach to train autonomous driving agents. However, two issues limit the full potential of this approach: (i) the inertia problem, a special case of causal confusion where the agent mistakenly correlates low speed with no acceleration, and (ii) low correlation between offline and online performance due to the accumulation of small errors that brings the agent in a previously unseen state. Both issues are critical for state-aware models, yet informing the driving agent of its internal state as well as the state of the environment is of crucial importance. In this paper we propose a multi-task learning agent based on a multi-stage vision transformer with state token propagation. We feed the state of the vehicle along with the representation of the environment as a special token of the transformer and propagate it throughout the network. This allows us to tackle the aforementioned issues from different angles: guiding the driving policy with learned stop/go information, performing data augmentation directly on the state of the vehicle and visually explaining the model's decisions. We report a drastic decrease in inertia and a high correlation between offline and online metrics.
</details>
<details>
<summary>摘要</summary>
conditioned imitation learning 是一种常见而有效的自动驾驶代理方法。然而，两个问题限制了这种方法的全面潜力：（i）势能问题，特殊的 causal confusion 问题，agent 错误地相关低速与无加速度的耦合，以及（ii）在线和离线性能的低相关性，由于累累的小错误而导致agent 处于未看过的状态。这两个问题对状态感知模型非常重要，但是告诉驾驶代理器的内部状态以及环境状态也是非常重要。在这篇论文中，我们提议一种基于多任务学习的多阶段视transformer Agent，我们将驾驶车辆的状态和环境的表示作为 transformer 的特殊 токен，并在网络中传播它们。这使得我们可以从不同的角度解决上述问题：引导驾驶策略，直接对驾驶车辆的状态进行数据扩展，以及可视化模型的决策。我们发现降低了势能问题，并且在线和离线指标之间的相关性非常高。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Batch-Norm-Statistics-Update-for-Natural-Robustness"><a href="#Dynamic-Batch-Norm-Statistics-Update-for-Natural-Robustness" class="headerlink" title="Dynamic Batch Norm Statistics Update for Natural Robustness"></a>Dynamic Batch Norm Statistics Update for Natural Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20649">http://arxiv.org/abs/2310.20649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahbaz Rezaei, Mohammad Sadegh Norouzzadeh</li>
<li>for: 提高DNN对受损样本的性能</li>
<li>methods: 利用快速更新 batch normalization（BN）统计信息，以及在快速更新BN统计信息后进行损害检测</li>
<li>results: 在CIFAR10-C和ImageNet-C datasets上实现了约8%和4%的准确率提高，并且可以进一步提高现有的可靠模型的准确率。<details>
<summary>Abstract</summary>
DNNs trained on natural clean samples have been shown to perform poorly on corrupted samples, such as noisy or blurry images. Various data augmentation methods have been recently proposed to improve DNN's robustness against common corruptions. Despite their success, they require computationally expensive training and cannot be applied to off-the-shelf trained models. Recently, it has been shown that updating BatchNorm (BN) statistics of an off-the-shelf model on a single corruption improves its accuracy on that corruption significantly. However, adopting the idea at inference time when the type of corruption is unknown and changing decreases the effectiveness of this method. In this paper, we harness the Fourier domain to detect the corruption type, a challenging task in the image domain. We propose a unified framework consisting of a corruption-detection model and BN statistics update that improves the corruption accuracy of any off-the-shelf trained model. We benchmark our framework on different models and datasets. Our results demonstrate about 8% and 4% accuracy improvement on CIFAR10-C and ImageNet-C, respectively. Furthermore, our framework can further improve the accuracy of state-of-the-art robust models, such as AugMix and DeepAug.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在天然清晰样本上训练后表现不佳于受损样本，如杂音或模糊图像。各种数据增强方法已经被提议以提高DNN的对常见受损的Robustness。尽管它们在成功，但它们需要计算成本较高的训练，并且无法应用于卸载模型。最近，有人提出了将批处理 normalization（BatchNorm）统计信息更新到卸载模型中，以提高对受损的精度。但是，在推理时，当受损类型未知并且改变时，这种方法的效iveness会减退。在这篇论文中，我们利用快推频域来检测受损类型，这是图像领域中的一个挑战。我们提出了一个统一框架，包括受损检测模型和批处理 normalization 统计信息更新，以提高任何卸载训练模型的受损精度。我们对不同的模型和数据集进行了比较。我们的结果表明，在CIFAR10-C和ImageNet-C上，我们的框架可以提高精度约8%和4%。此外，我们的框架可以进一步提高当今最佳Robust模型，如AugMix和DeepAug的精度。
</details></li>
</ul>
<hr>
<h2 id="Using-Higher-Order-Moments-to-Assess-the-Quality-of-GAN-generated-Image-Features"><a href="#Using-Higher-Order-Moments-to-Assess-the-Quality-of-GAN-generated-Image-Features" class="headerlink" title="Using Higher-Order Moments to Assess the Quality of GAN-generated Image Features"></a>Using Higher-Order Moments to Assess the Quality of GAN-generated Image Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20636">http://arxiv.org/abs/2310.20636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Luzi, Helen Jenne, Ryan Murray, Carlos Ortiz Marrero</li>
<li>for: 评估生成 adversarial networks (GANs) 的robustness</li>
<li>methods: 利用 Fréchet Inception Distance (FID) 和 Skew Inception Distance (SID) 两种度量来评估 GANs</li>
<li>results: SID 可以更好地反映人类的感知，并且可以跟踪 FID 的表现Here’s a more detailed explanation of each point:1. for: The paper aims to evaluate the robustness of Generative Adversarial Networks (GANs) using two distance measures: Fréchet Inception Distance (FID) and Skew Inception Distance (SID).2. methods: The paper uses FID and SID to evaluate GANs, and presents a practical method for computing SID.3. results: The results show that SID can better reflect human perception and can track the performance of FID in evaluating image features of ImageNet data.<details>
<summary>Abstract</summary>
The rapid advancement of Generative Adversarial Networks (GANs) necessitates the need to robustly evaluate these models. Among the established evaluation criteria, the Fr\'{e}chet Inception Distance (FID) has been widely adopted due to its conceptual simplicity, fast computation time, and strong correlation with human perception. However, FID has inherent limitations, mainly stemming from its assumption that feature embeddings follow a Gaussian distribution, and therefore can be defined by their first two moments. As this does not hold in practice, in this paper we explore the importance of third-moments in image feature data and use this information to define a new measure, which we call the Skew Inception Distance (SID). We prove that SID is a pseudometric on probability distributions, show how it extends FID, and present a practical method for its computation. Our numerical experiments support that SID either tracks with FID or, in some cases, aligns more closely with human perception when evaluating image features of ImageNet data.
</details>
<details>
<summary>摘要</summary>
“Generative Adversarial Networks（GANs）的快速进步需要对这些模型进行严格的评估。已有的评估标准之一是Fréchet Inception Distance（FID），它的概念简单，计算速度快，且与人类感知强相关。然而，FID有一些限制，主要是它假设特征嵌入 follows Gaussian distribution，因此可以通过其第一个和第二个矩阵来定义。然而，这不是实际情况中的情况，因此在这篇文章中，我们探索了特征数据中的第三个矩阵信息，并使用这些信息定义一个新的衡量，我们称之为Skew Inception Distance（SID）。我们证明了SID是一个pseudometric on probability distributions，并详细介绍了它与FID之间的关系。我们还提供了一个实际的计算方法，并通过数值实验支持SID可以跟踪FID或，在某些情况下，与人类感知更加一致。”
</details></li>
</ul>
<hr>
<h2 id="Deepfake-detection-by-exploiting-surface-anomalies-the-SurFake-approach"><a href="#Deepfake-detection-by-exploiting-surface-anomalies-the-SurFake-approach" class="headerlink" title="Deepfake detection by exploiting surface anomalies: the SurFake approach"></a>Deepfake detection by exploiting surface anomalies: the SurFake approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20621">http://arxiv.org/abs/2310.20621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Ciamarra, Roberto Caldelli, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo</li>
<li>for: 这篇论文旨在探讨深伪检测技术，以避免各种媒体信息中的伪造内容散布。</li>
<li>methods: 本论文提出了一种基于场景特征分析的深伪检测方法，即SurFake方法，通过分析图像中的表面特征来生成一个可以训练CNN的描述子。</li>
<li>results: 实验结果显示，SurFake方法可以对FF++ dataset中的不同类型深伪诈骗内容进行有效的检测，并且可以与视觉数据结合以提高检测精度。<details>
<summary>Abstract</summary>
The ever-increasing use of synthetically generated content in different sectors of our everyday life, one for all media information, poses a strong need for deepfake detection tools in order to avoid the proliferation of altered messages. The process to identify manipulated content, in particular images and videos, is basically performed by looking for the presence of some inconsistencies and/or anomalies specifically due to the fake generation process. Different techniques exist in the scientific literature that exploit diverse ad-hoc features in order to highlight possible modifications. In this paper, we propose to investigate how deepfake creation can impact on the characteristics that the whole scene had at the time of the acquisition. In particular, when an image (video) is captured the overall geometry of the scene (e.g. surfaces) and the acquisition process (e.g. illumination) determine a univocal environment that is directly represented by the image pixel values; all these intrinsic relations are possibly changed by the deepfake generation process. By resorting to the analysis of the characteristics of the surfaces depicted in the image it is possible to obtain a descriptor usable to train a CNN for deepfake detection: we refer to such an approach as SurFake. Experimental results carried out on the FF++ dataset for different kinds of deepfake forgeries and diverse deep learning models confirm that such a feature can be adopted to discriminate between pristine and altered images; furthermore, experiments witness that it can also be combined with visual data to provide a certain improvement in terms of detection accuracy.
</details>
<details>
<summary>摘要</summary>
随着人工生成内容在不同领域的日常生活中越来越广泛使用，特别是媒体信息领域，有必要开发深度假作检测工具以避免受到修改的消息的扩散。寻找修改后的内容特征是通过检测修改过程中引入的一些异常和偏差来进行。在科学文献中已经存在多种不同特点的技术，以便在检测修改过程中异常出现的特征。在本文中，我们将探讨深度假作创造如何影响图像/视频的整体场景特征。具体来说，当图像（视频）被拍摄时，整个场景的geometry（例如表面）以及拍摄过程（例如照明）会直接将场景特征表示在图像像素值中，这些内在关系可能被深度假作生成过程改变。通过分析图像中表示的表面特征，我们可以获得一个可以用于训练CNN的假作检测Descriptor：我们称之为SurFake。实验结果表明，这种特征可以用于分辨普通和修改后的图像，而且可以与视觉数据结合使用以提高检测精度。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Reconstruction-of-Ultrasound-Images-with-Informative-Uncertainty"><a href="#Diffusion-Reconstruction-of-Ultrasound-Images-with-Informative-Uncertainty" class="headerlink" title="Diffusion Reconstruction of Ultrasound Images with Informative Uncertainty"></a>Diffusion Reconstruction of Ultrasound Images with Informative Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20618">http://arxiv.org/abs/2310.20618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Yuxin-Zhang-Jasmine/DRUS-v2">https://github.com/Yuxin-Zhang-Jasmine/DRUS-v2</a></li>
<li>paper_authors: Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus</li>
<li>For: The paper aims to improve the quality of ultrasound images by leveraging advances in diffusion models and incorporating ultrasound physics.* Methods: The proposed hybrid approach combines model-based and learning-based methods, adapting Denoising Diffusion Restoration Models (DDRM) to incorporate ultrasound physics through a linear direct model and unsupervised fine-tuning of the prior diffusion model.* Results: The approach achieves high-quality image reconstructions from a single plane wave input and outperforms state-of-the-art methods in simulated, in-vitro, and in-vivo experiments. The code and data are available online.Here is the same information in Simplified Chinese text:* 用途：文章目的是提高超声图像质量，利用扩散模型的最新进展和超声物理学。* 方法：提议的混合方法 combinines 模型基于和学习基于方法，将 Denoising Diffusion Restoration Models (DDRM) 适应超声物理学通过直接模型和无监督精度模型的微调。* 结果：方法可以从单个扩散波输入获得高质量的图像重建，并在模拟、室内和实验室数据上超过当前最佳方法。代码和数据可在（接受后）上获取。<details>
<summary>Abstract</summary>
Despite its wide use in medicine, ultrasound imaging faces several challenges related to its poor signal-to-noise ratio and several sources of noise and artefacts. Enhancing ultrasound image quality involves balancing concurrent factors like contrast, resolution, and speckle preservation. In recent years, there has been progress both in model-based and learning-based approaches to improve ultrasound image reconstruction. Bringing the best from both worlds, we propose a hybrid approach leveraging advances in diffusion models. To this end, we adapt Denoising Diffusion Restoration Models (DDRM) to incorporate ultrasound physics through a linear direct model and an unsupervised fine-tuning of the prior diffusion model. We conduct comprehensive experiments on simulated, in-vitro, and in-vivo data, demonstrating the efficacy of our approach in achieving high-quality image reconstructions from a single plane wave input and in comparison to state-of-the-art methods. Finally, given the stochastic nature of the method, we analyse in depth the statistical properties of single and multiple-sample reconstructions, experimentally show the informativeness of their variance, and provide an empirical model relating this behaviour to speckle noise. The code and data are available at: (upon acceptance).
</details>
<details>
<summary>摘要</summary>
虽然医学中使用ultrasound imaging广泛，但它们面临着一些相关的信号噪声和噪声源的挑战。提高ultrasound图像质量需要同时考虑对比、分辨率和雾点保持的平衡。在过去几年中，有一些基于模型和学习的方法来改进ultrasound图像重建。我们提议一种hybrid方法， combinig diffusion模型的进步。为此，我们在linear direct模型和一种不supervised的精度模型之间进行了权衡。我们对 simulated、in-vitro和in-vivo数据进行了全面的实验，并证明了我们的方法可以从单个平面波输入中获得高质量的图像重建，并与当前的方法进行比较。此外，由于方法的随机性，我们对单个和多个样本重建的统计性质进行了深入的分析，实验表明雾点噪声的信息量是可以获取的，并提供了一个实验性的模型，将这种行为与雾点噪声相关联。代码和数据在acceptance时可以获取。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Synthetic-MRI-Generation-from-CT-Scans-Using-CycleGAN-with-Feature-Extraction"><a href="#Enhanced-Synthetic-MRI-Generation-from-CT-Scans-Using-CycleGAN-with-Feature-Extraction" class="headerlink" title="Enhanced Synthetic MRI Generation from CT Scans Using CycleGAN with Feature Extraction"></a>Enhanced Synthetic MRI Generation from CT Scans Using CycleGAN with Feature Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20604">http://arxiv.org/abs/2310.20604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Nikbakhsh, Lachin Naghashyar, Morteza Valizadeh, Mehdi Chehel Amirani</li>
<li>for: 这篇论文的目的是对于放射治疗中的精确图像和图像注册进行改进，以提高放射治疗的精确性。</li>
<li>methods: 这篇论文使用的方法是基于Synthetic MRI图像的单模图像注册，利用CycleGANs和特征提取器来生成这些图像。</li>
<li>results: 这篇论文的结果显示，该方法可以优化多modal registration的过程，并且比较多于之前的州先进方法表现出色。<details>
<summary>Abstract</summary>
In the field of radiotherapy, accurate imaging and image registration are of utmost importance for precise treatment planning. Magnetic Resonance Imaging (MRI) offers detailed imaging without being invasive and excels in soft-tissue contrast, making it a preferred modality for radiotherapy planning. However, the high cost of MRI, longer acquisition time, and certain health considerations for patients pose challenges. Conversely, Computed Tomography (CT) scans offer a quicker and less expensive imaging solution. To bridge these modalities and address multimodal alignment challenges, we introduce an approach for enhanced monomodal registration using synthetic MRI images. Utilizing unpaired data, this paper proposes a novel method to produce these synthetic MRI images from CT scans, leveraging CycleGANs and feature extractors. By building upon the foundational work on Cycle-Consistent Adversarial Networks and incorporating advancements from related literature, our methodology shows promising results, outperforming several state-of-the-art methods. The efficacy of our approach is validated by multiple comparison metrics.
</details>
<details>
<summary>摘要</summary>
在辐射治疗领域，精准成像和图像对接是至关重要的，以便精准的治疗规划。核磁共振成像（MRI）可以提供详细的成像，无需侵入性，并且在软组织对比方面表现出色，因此成为辐射治疗规划的首选方式。然而，MRI的高价格、长时间获取和某些健康考虑因素带来挑战。相比之下，计算 Tomatoes成像（CT）扫描器提供了快速、成本低的成像解决方案。为了桥接这两种模式和解决多模态对接问题，我们提出了一种增强单模式对接方法，使用生成的MRI图像。该方法使用不匹配的数据，利用CycleGANs和特征提取器，以便生成MRI图像。我们建立在基础的Cycle-Consistent Adversarial Networks之上，并从相关 литературе中吸收了进步，我们的方法ologies showed promising results, outperforming several state-of-the-art methods。我们的方法的有效性被多种比较指标 validate。
</details></li>
</ul>
<hr>
<h2 id="Brain-like-Flexible-Visual-Inference-by-Harnessing-Feedback-Feedforward-Alignment"><a href="#Brain-like-Flexible-Visual-Inference-by-Harnessing-Feedback-Feedforward-Alignment" class="headerlink" title="Brain-like Flexible Visual Inference by Harnessing Feedback-Feedforward Alignment"></a>Brain-like Flexible Visual Inference by Harnessing Feedback-Feedforward Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20599">http://arxiv.org/abs/2310.20599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toosi/feedback_feedforward_alignment">https://github.com/toosi/feedback_feedforward_alignment</a></li>
<li>paper_authors: Tahereh Toosi, Elias B. Issa</li>
<li>for: This paper aims to explore the mechanisms underlying how feedback connections in the visual cortex support flexible visual functions, such as denoising, resolving occlusions, hallucination, and imagination.</li>
<li>methods: The authors propose a learning algorithm called Feedback-Feedforward Alignment (FFA) that leverages feedback and feedforward pathways to co-optimize classification and reconstruction tasks on widely used MNIST and CIFAR10 datasets.</li>
<li>results: The study demonstrates the effectiveness of FFA in endowing feedback connections with emergent visual inference functions, and offers bio-plausibility compared to traditional backpropagation (BP) methods. The alignment mechanism in FFA enhances the bio-plausibility of the learning algorithm and contributes to the broader field of visual inference underlying perceptual phenomena.<details>
<summary>Abstract</summary>
In natural vision, feedback connections support versatile visual inference capabilities such as making sense of the occluded or noisy bottom-up sensory information or mediating pure top-down processes such as imagination. However, the mechanisms by which the feedback pathway learns to give rise to these capabilities flexibly are not clear. We propose that top-down effects emerge through alignment between feedforward and feedback pathways, each optimizing its own objectives. To achieve this co-optimization, we introduce Feedback-Feedforward Alignment (FFA), a learning algorithm that leverages feedback and feedforward pathways as mutual credit assignment computational graphs, enabling alignment. In our study, we demonstrate the effectiveness of FFA in co-optimizing classification and reconstruction tasks on widely used MNIST and CIFAR10 datasets. Notably, the alignment mechanism in FFA endows feedback connections with emergent visual inference functions, including denoising, resolving occlusions, hallucination, and imagination. Moreover, FFA offers bio-plausibility compared to traditional backpropagation (BP) methods in implementation. By repurposing the computational graph of credit assignment into a goal-driven feedback pathway, FFA alleviates weight transport problems encountered in BP, enhancing the bio-plausibility of the learning algorithm. Our study presents FFA as a promising proof-of-concept for the mechanisms underlying how feedback connections in the visual cortex support flexible visual functions. This work also contributes to the broader field of visual inference underlying perceptual phenomena and has implications for developing more biologically inspired learning algorithms.
</details>
<details>
<summary>摘要</summary>
自然视觉中，反馈连接支持多样化的视觉推理功能，如处理受阻或噪声的底向感知信息或激发纯层次过程如想象。然而，反馈路径学习如何抽象地实现这些多样化功能的机制不清楚。我们提议，反馈路径和前向路径之间的协调是实现多样化功能的关键。为此，我们提出了反馈-Feedforward协调（FFA）学习算法，利用反馈和前向路径作为互助计算图，实现协调。在我们的研究中，我们证明了 FFA 在广泛使用的 MNIST 和 CIFAR10 数据集上的分类和重建任务中的效果。尤其是在 FFA 中的协调机制下，反馈连接获得了许多生成visual推理功能，如净化、解除遮挡、梦幻、想象等。此外，FFA 具有与传统的 backpropagation（BP）方法不同的生物可能性，通过重塑计算图为目标驱动反馈路径，解决了 BP 中的权重传输问题，提高了生物可能性。我们的研究发现，FFA 作为一种可能的证明，证明了视觉系统中反馈连接支持多样化功能的机制。此外，FFA 对 visual 推理下的更广泛问题和生物适应性学习算法的发展产生了影响。
</details></li>
</ul>
<hr>
<h2 id="FLODCAST-Flow-and-Depth-Forecasting-via-Multimodal-Recurrent-Architectures"><a href="#FLODCAST-Flow-and-Depth-Forecasting-via-Multimodal-Recurrent-Architectures" class="headerlink" title="FLODCAST: Flow and Depth Forecasting via Multimodal Recurrent Architectures"></a>FLODCAST: Flow and Depth Forecasting via Multimodal Recurrent Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20593">http://arxiv.org/abs/2310.20593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Ciamarra, Federico Becattini, Lorenzo Seidenari, Alberto Del Bimbo</li>
<li>for: 这篇论文的目的是提出一种用于预测物体的运动和空间位置的方法，特别是在自动驾驶等安全关键场景中。</li>
<li>methods: 该论文提出了一种名为FLODCAST的流和深度预测模型，该模型利用了多任务回归架构，通过同时预测两种不同的modalities来提高预测精度。</li>
<li>results: 根据Cityscapes dataset的测试结果，该模型可以达到最佳的Result for both flow and depth forecasting,并且通过在不同时间步预测来提供更好的supervision，从而提高预测精度。此外，该模型还可以在下游任务中提供更好的结果，例如分割预测。<details>
<summary>Abstract</summary>
Forecasting motion and spatial positions of objects is of fundamental importance, especially in safety-critical settings such as autonomous driving. In this work, we address the issue by forecasting two different modalities that carry complementary information, namely optical flow and depth. To this end we propose FLODCAST a flow and depth forecasting model that leverages a multitask recurrent architecture, trained to jointly forecast both modalities at once. We stress the importance of training using flows and depth maps together, demonstrating that both tasks improve when the model is informed of the other modality. We train the proposed model to also perform predictions for several timesteps in the future. This provides better supervision and leads to more precise predictions, retaining the capability of the model to yield outputs autoregressively for any future time horizon. We test our model on the challenging Cityscapes dataset, obtaining state of the art results for both flow and depth forecasting. Thanks to the high quality of the generated flows, we also report benefits on the downstream task of segmentation forecasting, injecting our predictions in a flow-based mask-warping framework.
</details>
<details>
<summary>摘要</summary>
预测物体的运动和空间位置是基础性重要的，尤其在自动驾驶等安全关键场景中。在这项工作中，我们解决这个问题，通过同时预测两种不同的modalities，它们各自携带有补偿信息，即光流和深度。为此，我们提议了FLODCAST模型，它利用了多任务回归架构，通过同时预测这两种modalities来充分利用信息。我们强调了在训练过程中使用光流和深度图像 вместе，证明两个任务都会提高，当模型知道另一个模态时。我们还训练了模型以进行多个时间步预测，这提供了更好的监督，导致更准确的预测，保留模型在任何未来时间步可以生成autoregressively的能力。我们在Cityscapes dataset上测试了我们的模型，取得了流和深度预测的状态宇宙记录。由于生成的流速度较高质量，我们还报告了在基于流速度抽象框架的下游任务中的 segmentation预测的 beneficial effects。
</details></li>
</ul>
<hr>
<h2 id="Long-Tailed-Learning-as-Multi-Objective-Optimization"><a href="#Long-Tailed-Learning-as-Multi-Objective-Optimization" class="headerlink" title="Long-Tailed Learning as Multi-Objective Optimization"></a>Long-Tailed Learning as Multi-Objective Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20490">http://arxiv.org/abs/2310.20490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiqi Li, Fan Lyu, Fanhua Shang, Liang Wan, Wei Feng</li>
<li>for:  Addressing the long-tailed distribution issue in machine learning, where models are biased towards well-represented classes and perform poorly on under-represented classes.</li>
<li>methods:  Proposes a Gradient-Balancing Grouping (GBG) strategy to gather classes with similar gradient directions, allowing for ideal compensation for tail classes.</li>
<li>results:  Demonstrates superior performance over existing state-of-the-art (SOTA) methods through extensive experiments on commonly used benchmarks for long-tailed learning.<details>
<summary>Abstract</summary>
Real-world data is extremely imbalanced and presents a long-tailed distribution, resulting in models that are biased towards classes with sufficient samples and perform poorly on rare classes. Recent methods propose to rebalance classes but they undertake the seesaw dilemma (what is increasing performance on tail classes may decrease that of head classes, and vice versa). In this paper, we argue that the seesaw dilemma is derived from gradient imbalance of different classes, in which gradients of inappropriate classes are set to important for updating, thus are prone to overcompensation or undercompensation on tail classes. To achieve ideal compensation, we formulate the long-tailed recognition as an multi-objective optimization problem, which fairly respects the contributions of head and tail classes simultaneously. For efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather the classes with similar gradient directions, thus approximately make every update under a Pareto descent direction. Our GBG method drives classes with similar gradient directions to form more representative gradient and provide ideal compensation to the tail classes. Moreover, We conduct extensive experiments on commonly used benchmarks in long-tailed learning and demonstrate the superiority of our method over existing SOTA methods.
</details>
<details>
<summary>摘要</summary>
In this paper, we argue that the seesaw dilemma is caused by an imbalance of gradients from different classes. When some classes have more examples, their gradients become more important for updating the model, which can lead to overcompensation or undercompensation for the minority classes. To solve this problem, we formulate the long-tailed recognition as a multi-objective optimization problem, which aims to fairly respect the contributions of both the head and tail classes simultaneously.To improve efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy that gathers classes with similar gradient directions. This allows us to approximately make every update under a Pareto descent direction, which provides ideal compensation for the minority classes. Our GBG method helps classes with similar gradient directions to form more representative gradients, providing better compensation for the tail classes.We conduct extensive experiments on commonly used benchmarks for long-tailed learning and show that our method outperforms existing state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="LAVSS-Location-Guided-Audio-Visual-Spatial-Audio-Separation"><a href="#LAVSS-Location-Guided-Audio-Visual-Spatial-Audio-Separation" class="headerlink" title="LAVSS: Location-Guided Audio-Visual Spatial Audio Separation"></a>LAVSS: Location-Guided Audio-Visual Spatial Audio Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20446">http://arxiv.org/abs/2310.20446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YYX666660/LAVSS">https://github.com/YYX666660/LAVSS</a></li>
<li>paper_authors: Yuxin Ye, Wenming Yang, Yapeng Tian</li>
<li>for: 这个研究的目的是将数位音频和视觉信息分离，以提高在虚拟和实际世界中的声音识别。</li>
<li>methods: 这个研究使用了一种名为LAVSS的位置导向的音频空间分离方法，它利用了空间音频和视觉位置之间的相互关联，并将这两个模式融合在一起，以提高声音分离的精度。</li>
<li>results: 实验结果显示，LAVSS比既有的对话音频分离方法（ benchmarks）有更高的分离精度，并且可以在虚拟和实际世界中提供更好的声音识别。<details>
<summary>Abstract</summary>
Existing machine learning research has achieved promising results in monaural audio-visual separation (MAVS). However, most MAVS methods purely consider what the sound source is, not where it is located. This can be a problem in VR/AR scenarios, where listeners need to be able to distinguish between similar audio sources located in different directions. To address this limitation, we have generalized MAVS to spatial audio separation and proposed LAVSS: a location-guided audio-visual spatial audio separator. LAVSS is inspired by the correlation between spatial audio and visual location. We introduce the phase difference carried by binaural audio as spatial cues, and we utilize positional representations of sounding objects as additional modality guidance. We also leverage multi-level cross-modal attention to perform visual-positional collaboration with audio features. In addition, we adopt a pre-trained monaural separator to transfer knowledge from rich mono sounds to boost spatial audio separation. This exploits the correlation between monaural and binaural channels. Experiments on the FAIR-Play dataset demonstrate the superiority of the proposed LAVSS over existing benchmarks of audio-visual separation. Our project page: https://yyx666660.github.io/LAVSS/.
</details>
<details>
<summary>摘要</summary>
现有机器学习研究已经取得了许多成果在声音单道视频分离（MAVS）领域。然而，大多数MAVS方法只考虑声音来源的性质，而不考虑其位置。这可能会成为VR/AR场景中的问题，因为听众需要能够在不同方向中分辨类似的声音来源。为解决这个限制，我们总结了MAVS，并提出了位置指导的声音视频空间分离器（LAVSS）。LAVSS灵感来自声音空间和视觉位置之间的相关性。我们引入了扬声器的相位差作为空间提示，并使用声音发生对象的位置表示来为其他模式提供导航。此外，我们利用多级跨模态注意力来进行视觉位置协作，并采用预训练的单道分离器来传递知识从丰富的单道声音中提高空间声音分离。实验结果表明，提出的LAVSS在FAIR-Play数据集上超过了现有的音视频分离标准。更多信息请访问我们的项目页面：https://yyx666660.github.io/LAVSS/.
</details></li>
</ul>
<hr>
<h2 id="SignAvatars-A-Large-scale-3D-Sign-Language-Holistic-Motion-Dataset-and-Benchmark"><a href="#SignAvatars-A-Large-scale-3D-Sign-Language-Holistic-Motion-Dataset-and-Benchmark" class="headerlink" title="SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark"></a>SignAvatars: A Large-scale 3D Sign Language Holistic Motion Dataset and Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20436">http://arxiv.org/abs/2310.20436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhengdiYu/SignAvatars">https://github.com/ZhengdiYu/SignAvatars</a></li>
<li>paper_authors: Zhengdi Yu, Shaoli Huang, Yongkang Cheng, Tolga Birdal<br>for: 这个论文是为了bridging the communication gap for hearing-impaired individuals，提供了一个大规模的多个提示3D手语动作数据集。methods: 这个论文使用了一个自动化的注释管道，将70,000个视频clip中的手语动作注释为3D描述和生物可靠的姿势。results: 这个论文提出了一个新的3D手语认识（SLR）和3D手语生产（SLP）任务，并提供了一个统一的比较标准，以评估SignAvatars数据集的潜在价值。<details>
<summary>Abstract</summary>
In this paper, we present SignAvatars, the first large-scale multi-prompt 3D sign language (SL) motion dataset designed to bridge the communication gap for hearing-impaired individuals. While there has been an exponentially growing number of research regarding digital communication, the majority of existing communication technologies primarily cater to spoken or written languages, instead of SL, the essential communication method for hearing-impaired communities. Existing SL datasets, dictionaries, and sign language production (SLP) methods are typically limited to 2D as the annotating 3D models and avatars for SL is usually an entirely manual and labor-intensive process conducted by SL experts, often resulting in unnatural avatars. In response to these challenges, we compile and curate the SignAvatars dataset, which comprises 70,000 videos from 153 signers, totaling 8.34 million frames, covering both isolated signs and continuous, co-articulated signs, with multiple prompts including HamNoSys, spoken language, and words. To yield 3D holistic annotations, including meshes and biomechanically-valid poses of body, hands, and face, as well as 2D and 3D keypoints, we introduce an automated annotation pipeline operating on our large corpus of SL videos. SignAvatars facilitates various tasks such as 3D sign language recognition (SLR) and the novel 3D SL production (SLP) from diverse inputs like text scripts, individual words, and HamNoSys notation. Hence, to evaluate the potential of SignAvatars, we further propose a unified benchmark of 3D SL holistic motion production. We believe that this work is a significant step forward towards bringing the digital world to the hearing-impaired communities. Our project page is at https://signavatars.github.io/
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍SignAvatars，首个大规模多提示3D手语（SL）动作数据集，旨在bridging通信差距 для听力受限人群。 existing的数字通信技术主要服务于口语或文字语言，而不是SL，听力受限社区的基本通信方式。现有的SL数据集、字典和手语生产（SLP）方法通常是2D的，因为annotating 3D模型和人物 дляSL是一个完全手动和劳动密集的过程，通常导致不自然的人物。为解决这些挑战，我们编译和筛选了SignAvatars数据集，包括70,000个视频，共计8.34万帧，覆盖了隔离的手语和连续、相关的手语，以及多个提示，包括汉诺斯语言、口语和单词。为了生成3D全息注解，包括身体、手部和面部的生物准确姿势，以及2D和3D关键点，我们引入了一个自动化注解管道，运行在我们的大量SL视频库中。SignAvatars可以支持多种任务，如3D手语识别（SLR）和基于多种输入的3D SL生产（SLP）。因此，为评估SignAvatars的潜力，我们进一步提出了一个统一的3D SL全息动作生产benchmark。我们认为这项工作是听力受限社区的数字化进程的重要一步。我们的项目页面位于<https://signavatars.github.io/>.
</details></li>
</ul>
<hr>
<h2 id="Assessing-and-Enhancing-Robustness-of-Deep-Learning-Models-with-Corruption-Emulation-in-Digital-Pathology"><a href="#Assessing-and-Enhancing-Robustness-of-Deep-Learning-Models-with-Corruption-Emulation-in-Digital-Pathology" class="headerlink" title="Assessing and Enhancing Robustness of Deep Learning Models with Corruption Emulation in Digital Pathology"></a>Assessing and Enhancing Robustness of Deep Learning Models with Corruption Emulation in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20427">http://arxiv.org/abs/2310.20427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peixiang Huang, Songtao Zhang, Yulu Gan, Rui Xu, Rongqi Zhu, Wenkang Qin, Limei Guo, Shan Jiang, Lin Luo</li>
<li>for:  This paper aims to improve the robustness of deep learning models in digital pathology by analyzing and emulating various image corruptions throughout the pathological life-cycle.</li>
<li>methods: The authors propose an Omni-Corruption Emulation (OmniCE) method to reproduce 21 types of corruptions quantified with 5-level severity, and use these corrupted datasets to assess the robustness of popular deep neural network (DNN) models in classification and segmentation tasks.</li>
<li>results: The authors show that the OmniCE-corrupted datasets can significantly enhance the generalization ability of DNN models, and that using these datasets as augmentation data for training and experiments can improve the models’ robustness in clinical diagnosis.<details>
<summary>Abstract</summary>
Deep learning in digital pathology brings intelligence and automation as substantial enhancements to pathological analysis, the gold standard of clinical diagnosis. However, multiple steps from tissue preparation to slide imaging introduce various image corruptions, making it difficult for deep neural network (DNN) models to achieve stable diagnostic results for clinical use. In order to assess and further enhance the robustness of the models, we analyze the physical causes of the full-stack corruptions throughout the pathological life-cycle and propose an Omni-Corruption Emulation (OmniCE) method to reproduce 21 types of corruptions quantified with 5-level severity. We then construct three OmniCE-corrupted benchmark datasets at both patch level and slide level and assess the robustness of popular DNNs in classification and segmentation tasks. Further, we explore to use the OmniCE-corrupted datasets as augmentation data for training and experiments to verify that the generalization ability of the models has been significantly enhanced.
</details>
<details>
<summary>摘要</summary>
深度学习在数字 PATHOLOGY 中带来智能和自动化作为诊断标准的重要丰富。然而，从组织准备到滤镜影像的多个步骤引入了多种图像损害，使得深度神经网络（DNN）模型难以在临床使用中实现稳定的诊断结果。为了评估和进一步增强模型的Robustness，我们分析了 PATHOLOGY 生命周期中全栈损害的物理原因，并提出了 Omni-Corruption Emulation（OmniCE）方法来模拟21种损害，并将其分为5级严重性。然后，我们构建了3个OmniCE-损害的 referential dataset，并对popular DNN进行了分类和 segmentation 任务中的评估。此外，我们还探索使用 OmniCE-损害的数据进行训练和实验，以验证模型的普适性得到了显著提高。
</details></li>
</ul>
<hr>
<h2 id="Thermal-Infrared-Remote-Target-Detection-System-for-Maritime-Rescue-based-on-Data-Augmentation-with-3D-Synthetic-Data"><a href="#Thermal-Infrared-Remote-Target-Detection-System-for-Maritime-Rescue-based-on-Data-Augmentation-with-3D-Synthetic-Data" class="headerlink" title="Thermal-Infrared Remote Target Detection System for Maritime Rescue based on Data Augmentation with 3D Synthetic Data"></a>Thermal-Infrared Remote Target Detection System for Maritime Rescue based on Data Augmentation with 3D Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20412">http://arxiv.org/abs/2310.20412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungjin Cheong, Wonho Jung, Yoon Seop Lim, Yong-Hwa Park</li>
<li>for: 这个论文旨在提出一种基于深度学习和数据扩展的海上搜救thermal-infrared（TIR）远程目标检测系统。</li>
<li>methods: 论文使用了深度学习和数据扩展技术，并建立了自己的TIR数据集，以及使用3D游戏（ARMA3）生成的 sintethic TIR数据集来增强模型的可靠性和robustness。</li>
<li>results: 实验结果表明，使用扩展数据集和提出的领域适应算法，论文提出的网络在远程TIR检测中表现出色，并且超过了基于实际TIR数据进行训练的网络的性能。<details>
<summary>Abstract</summary>
This paper proposes a thermal-infrared (TIR) remote target detection system for maritime rescue using deep learning and data augmentation. We established a self-collected TIR dataset consisting of multiple scenes imitating human rescue situations using a TIR camera (FLIR). Additionally, to address dataset scarcity and improve model robustness, a synthetic dataset from a 3D game (ARMA3) to augment the data is further collected. However, a significant domain gap exists between synthetic TIR and real TIR images. Hence, a proper domain adaptation algorithm is essential to overcome the gap. Therefore, we suggest a domain adaptation algorithm in a target-background separated manner from 3D game-to-real, based on a generative model, to address this issue. Furthermore, a segmentation network with fixed-weight kernels at the head is proposed to improve the signal-to-noise ratio (SNR) and provide weak attention, as remote TIR targets inherently suffer from unclear boundaries. Experiment results reveal that the network trained on augmented data consisting of translated synthetic and real TIR data outperforms that trained on only real TIR data by a large margin. Furthermore, the proposed segmentation model surpasses the performance of state-of-the-art segmentation methods.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Reference-Image-Assisted-Volumetric-Super-Resolution-of-Cardiac-Diffusion-Weighted-Imaging"><a href="#High-Resolution-Reference-Image-Assisted-Volumetric-Super-Resolution-of-Cardiac-Diffusion-Weighted-Imaging" class="headerlink" title="High-Resolution Reference Image Assisted Volumetric Super-Resolution of Cardiac Diffusion Weighted Imaging"></a>High-Resolution Reference Image Assisted Volumetric Super-Resolution of Cardiac Diffusion Weighted Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20389">http://arxiv.org/abs/2310.20389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzhe Wu, Jiahao Huang, Fanwen Wang, Pedro Ferreira, Andrew Scott, Sonia Nielles-Vallespin, Guang Yang<br>for: 这个研究的目的是提高DT-CMR的图像质量，以便更好地检测心脏微结构。methods: 这个研究使用深度学习方法来提高DT-CMR图像的质量，并采用了高分辨率参照图像作为输入。results: 研究表明，通过使用高分辨率参照图像，可以提高DT-CMR图像的质量，并且模型可以将这种提高应用于未看过的b值。<details>
<summary>Abstract</summary>
Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) is the only in vivo method to non-invasively examine the microstructure of the human heart. Current research in DT-CMR aims to improve the understanding of how the cardiac microstructure relates to the macroscopic function of the healthy heart as well as how microstructural dysfunction contributes to disease. To get the final DT-CMR metrics, we need to acquire diffusion weighted images of at least 6 directions. However, due to DWI's low signal-to-noise ratio, the standard voxel size is quite big on the scale for microstructures. In this study, we explored the potential of deep-learning-based methods in improving the image quality volumetrically (x4 in all dimensions). This study proposed a novel framework to enable volumetric super-resolution, with an additional model input of high-resolution b0 DWI. We demonstrated that the additional input could offer higher super-resolved image quality. Going beyond, the model is also able to super-resolve DWIs of unseen b-values, proving the model framework's generalizability for cardiac DWI superresolution. In conclusion, we would then recommend giving the model a high-resolution reference image as an additional input to the low-resolution image for training and inference to guide all super-resolution frameworks for parametric imaging where a reference image is available.
</details>
<details>
<summary>摘要</summary>
Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) 是人体心脏内部非侵入性地检测微结构的唯一方法。当前研究的目标是通过改进心脏微结构与健康心脏的宏观功能之间的关系来更好地理解心脏疾病的起源。为了获得最终的 DT-CMR 度量，我们需要获取至少6个方向的扩散束图像。然而，由于 DWI 的信号噪声比较低，标准 vozeld 大小很大，这限制了我们对微结构的检测。在这种情况下，我们 investigate 了深度学习技术可以改进图像质量的可能性。我们提出了一种新的框架，可以在所有维度上进行扩散超分辨率。这种框架具有一个额外输入，即高分辨率的 b0 DWI。我们 demonstarted 了这个额外输入可以提供更高的超分辨率图像质量。此外，模型还可以对不同的 b-值 DWI 进行超分辨率，证明了模型框架的普适性。因此，我们建议在训练和推断过程中给模型一个高分辨率参考图像作为额外输入，以提高所有超分辨率框架的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Low-cost-Strategic-Monitoring-Approach-for-Scalable-and-Interpretable-Error-Detection-in-Deep-Neural-Networks"><a href="#A-Low-cost-Strategic-Monitoring-Approach-for-Scalable-and-Interpretable-Error-Detection-in-Deep-Neural-Networks" class="headerlink" title="A Low-cost Strategic Monitoring Approach for Scalable and Interpretable Error Detection in Deep Neural Networks"></a>A Low-cost Strategic Monitoring Approach for Scalable and Interpretable Error Detection in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20349">http://arxiv.org/abs/2310.20349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Geissler, Syed Qutub, Michael Paulitsch, Karthik Pattabiraman</li>
<li>for: This paper is written for detecting silent data corruption in deep computer vision networks, specifically targeting hardware memory and input faults.</li>
<li>methods: The approach uses strategically placed quantile markers to estimate the anomaly of the current inference and achieve accurate detection. The detector component is designed to be algorithmically transparent, allowing for interpretable results.</li>
<li>results: The approach achieves high precision (up to ~96%) and recall (up to ~98%) of detection, with minimal compute overhead (as little as 0.3% of non-supervised inference time).<details>
<summary>Abstract</summary>
We present a highly compact run-time monitoring approach for deep computer vision networks that extracts selected knowledge from only a few (down to merely two) hidden layers, yet can efficiently detect silent data corruption originating from both hardware memory and input faults. Building on the insight that critical faults typically manifest as peak or bulk shifts in the activation distribution of the affected network layers, we use strategically placed quantile markers to make accurate estimates about the anomaly of the current inference as a whole. Importantly, the detector component itself is kept algorithmically transparent to render the categorization of regular and abnormal behavior interpretable to a human. Our technique achieves up to ~96% precision and ~98% recall of detection. Compared to state-of-the-art anomaly detection techniques, this approach requires minimal compute overhead (as little as 0.3% with respect to non-supervised inference time) and contributes to the explainability of the model.
</details>
<details>
<summary>摘要</summary>
我们提出了一种高度优化的执行时间监控方法，用于深度电脑视觉网络中检测静默的数据腐败。我们从只有几个（或甚至只有两个）隐藏层中提取了选择性的知识，但可以高效地检测硬件内存和输入错误所引起的数据腐败。我们建立在当前网络层的活化分布中的峰值或块状变化通常是重要错误的内容。我们使用策略性地置标的方法来对应该些变化，并使用算法透明的检测器部分，以便让错误的分类成为人类可解释的。我们的方法可以实现约96%的精度和约98%的回归检测。与现有的偏常检测技术相比，我们的方法需要的计算负载非常低（只有0.3%相对于非监控时间），并且增加了模型的解释性。
</details></li>
</ul>
<hr>
<h2 id="Class-Incremental-Learning-with-Pre-trained-Vision-Language-Models"><a href="#Class-Incremental-Learning-with-Pre-trained-Vision-Language-Models" class="headerlink" title="Class Incremental Learning with Pre-trained Vision-Language Models"></a>Class Incremental Learning with Pre-trained Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20348">http://arxiv.org/abs/2310.20348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xialei Liu, Xusheng Cao, Haori Lu, Jia-wen Xiao, Andrew D. Bagdanov, Ming-Ming Cheng</li>
<li>for: This paper is written for those interested in adapting and exploiting large-scale pre-trained models, specifically vision-language models like CLIP, for continual learning scenarios.</li>
<li>methods: The paper proposes an approach to adapting pre-trained vision-language models for new tasks by adding additional layers after the Image Encoder or before the Text Encoder. The authors investigate three different strategies: Linear Adapter, Self-attention Adapter, and Prompt Tuning. They also propose a method for parameter retention in the adapter layers to maintain stability and plasticity during incremental learning.</li>
<li>results: The paper demonstrates significant improvement over the current state-of-the-art on several conventional benchmarks with the simplest solution of a single Linear Adapter layer and parameter retention. The results show that the proposed approach can improve the performance of continual learning with pre-trained vision-language models.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨和利用大规模预训练模型，特别是视觉语言模型clip，在持续学习场景下的适应和利用。</li>
<li>methods: 论文提出了一种适应预训练视觉语言模型的方法，包括在图像编码器后加入额外层或在文本编码器前加入额外层。 authors investigate了三种不同的策略：线性适应、自我注意适应和提示调整。 authors还提出了一种参数保留方法来保持稳定性和 пластичность。</li>
<li>results: 论文的实验结果表明，使用单个线性适应层和参数保留方法可以减少现有状态的训练时间。 results表明，该方法可以在持续学习场景下提高预训练视觉语言模型的性能。<details>
<summary>Abstract</summary>
With the advent of large-scale pre-trained models, interest in adapting and exploiting them for continual learning scenarios has grown.   In this paper, we propose an approach to exploiting pre-trained vision-language models (e.g. CLIP) that enables further adaptation instead of only using zero-shot learning of new tasks. We augment a pre-trained CLIP model with additional layers after the Image Encoder or before the Text Encoder. We investigate three different strategies: a Linear Adapter, a Self-attention Adapter, each operating on the image embedding, and Prompt Tuning which instead modifies prompts input to the CLIP text encoder. We also propose a method for parameter retention in the adapter layers that uses a measure of parameter importance to better maintain stability and plasticity during incremental learning. Our experiments demonstrate that the simplest solution -- a single Linear Adapter layer with parameter retention -- produces the best results. Experiments on several conventional benchmarks consistently show a significant margin of improvement over the current state-of-the-art.
</details>
<details>
<summary>摘要</summary>
We augment a pre-trained CLIP model with additional layers after the Image Encoder or before the Text Encoder. We investigate three different strategies: a Linear Adapter, a Self-attention Adapter, and Prompt Tuning, each of which operates on the image embedding. Additionally, we propose a method for parameter retention in the adapter layers that uses a measure of parameter importance to maintain stability and plasticity during incremental learning.Our experiments show that the simplest solution, a single Linear Adapter layer with parameter retention, produces the best results. We consistently achieve a significant margin of improvement over the current state-of-the-art on several conventional benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Recaptured-Raw-Screen-Image-and-Video-Demoireing-via-Channel-and-Spatial-Modulations"><a href="#Recaptured-Raw-Screen-Image-and-Video-Demoireing-via-Channel-and-Spatial-Modulations" class="headerlink" title="Recaptured Raw Screen Image and Video Demoiréing via Channel and Spatial Modulations"></a>Recaptured Raw Screen Image and Video Demoiréing via Channel and Spatial Modulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20332">http://arxiv.org/abs/2310.20332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tju-chengyijia/vd_raw">https://github.com/tju-chengyijia/vd_raw</a></li>
<li>paper_authors: Huanjing Yue, Yijia Cheng, Xin Liu, Jingyu Yang</li>
<li>for: 这篇论文的目的是提出一种针对原始输入进行图像和视频抑制蛇纹网络。</li>
<li>methods: 该方法使用了一种新的色彩分离特征分支，并将其与传统的特征混合分支进行拼接，通过通道和空间调制来增强特征。</li>
<li>results: 实验表明，该方法可以在图像和视频抑制蛇纹方面达到状态 искусственный智能的性能。 codes和数据集在<a target="_blank" rel="noopener" href="https://github.com/tju-chengyijia/VD_raw%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/tju-chengyijia/VD_raw中发布。</a><details>
<summary>Abstract</summary>
Capturing screen contents by smartphone cameras has become a common way for information sharing. However, these images and videos are often degraded by moir\'e patterns, which are caused by frequency aliasing between the camera filter array and digital display grids. We observe that the moir\'e patterns in raw domain is simpler than those in sRGB domain, and the moir\'e patterns in raw color channels have different properties. Therefore, we propose an image and video demoir\'eing network tailored for raw inputs. We introduce a color-separated feature branch, and it is fused with the traditional feature-mixed branch via channel and spatial modulations. Specifically, the channel modulation utilizes modulated color-separated features to enhance the color-mixed features. The spatial modulation utilizes the feature with large receptive field to modulate the feature with small receptive field. In addition, we build the first well-aligned raw video demoir\'eing (RawVDemoir\'e) dataset and propose an efficient temporal alignment method by inserting alternating patterns. Experiments demonstrate that our method achieves state-of-the-art performance for both image and video demori\'eing. We have released the code and dataset in https://github.com/tju-chengyijia/VD_raw.
</details>
<details>
<summary>摘要</summary>
抓取屏幕内容通过智能手机摄像头已成为常见的信息分享方式。然而，这些图像和视频经常受到频率扭曲的影响，导致图像和视频中出现质量下降的问题。我们发现 raw 频谱中的扭曲 Pattern 比 sRGB 频谱中的更简单，raw 频谱中的扭曲 Pattern 也有不同的特性。因此，我们提出一种针对 raw 输入的图像和视频抗扭曲网络。我们添加了一个分割色彩的特征分支，并将其与传统的特征混合分支进行混合，通过通道和空间模拟来实现。具体来说，通道模拟利用模拟的色彩分割特征来增强混合的色彩特征。空间模拟利用具有大覆盖面积的特征来模拟具有小覆盖面积的特征。此外，我们建立了首个Well-Aligned Raw Video Demoir\'e（RawVDemoir\'e）数据集，并提出了高效的时间对齐方法，通过插入交替 patrern 来实现。实验表明，我们的方法可以在图像和视频抗扭曲方面达到状态畅的性能。我们已经在 GitHub 上发布了代码和数据集，请参考 <https://github.com/tju-chengyijia/VD_raw>。
</details></li>
</ul>
<hr>
<h2 id="GACE-Geometry-Aware-Confidence-Enhancement-for-Black-Box-3D-Object-Detectors-on-LiDAR-Data"><a href="#GACE-Geometry-Aware-Confidence-Enhancement-for-Black-Box-3D-Object-Detectors-on-LiDAR-Data" class="headerlink" title="GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data"></a>GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20319">http://arxiv.org/abs/2310.20319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dschinagl/gace">https://github.com/dschinagl/gace</a></li>
<li>paper_authors: David Schinagl, Georg Krispel, Christian Fruhwirth-Reisinger, Horst Possegger, Horst Bischof</li>
<li>for: 提高黑盒子3D物体探测器的可信度评估</li>
<li>methods: 聚合检测结果的几何信息和空间关系，以改进可信度评估</li>
<li>results: 对多种状态对象探测器进行了实质性的性能提升，尤其是敏感路用户类（行人和自行车）<details>
<summary>Abstract</summary>
Widely-used LiDAR-based 3D object detectors often neglect fundamental geometric information readily available from the object proposals in their confidence estimation. This is mostly due to architectural design choices, which were often adopted from the 2D image domain, where geometric context is rarely available. In 3D, however, considering the object properties and its surroundings in a holistic way is important to distinguish between true and false positive detections, e.g. occluded pedestrians in a group. To address this, we present GACE, an intuitive and highly efficient method to improve the confidence estimation of a given black-box 3D object detector. We aggregate geometric cues of detections and their spatial relationships, which enables us to properly assess their plausibility and consequently, improve the confidence estimation. This leads to consistent performance gains over a variety of state-of-the-art detectors. Across all evaluated detectors, GACE proves to be especially beneficial for the vulnerable road user classes, i.e. pedestrians and cyclists.
</details>
<details>
<summary>摘要</summary>
广泛使用LiDAR基于的3D物体探测器经常忽略可用的基本 геометрические信息，这主要是因为架构设计选择，通常是从2D图像领域采用的，在这里，基本上没有地理上的信息。然而，在3D中，考虑物体属性和其周围环境的整体方式是重要的，以分辨真实和假阳性探测，例如受阻的行人在群体中。为解决这个问题，我们提出了GACE，一种直观和高效的方法，用于改善给定黑盒3D物体探测器的信任度估计。我们将探测结果的几何信息和其空间关系聚合起来，以确定它们的可能性，从而改善信任度估计。这导致了多种状态对应的性能提升。对于许多状态对应的检测器，GACE表现出特别的有利效果，即车辆和行人等护理用户类型。
</details></li>
</ul>
<hr>
<h2 id="HWD-A-Novel-Evaluation-Score-for-Styled-Handwritten-Text-Generation"><a href="#HWD-A-Novel-Evaluation-Score-for-Styled-Handwritten-Text-Generation" class="headerlink" title="HWD: A Novel Evaluation Score for Styled Handwritten Text Generation"></a>HWD: A Novel Evaluation Score for Styled Handwritten Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20316">http://arxiv.org/abs/2310.20316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/hwd">https://github.com/aimagelab/hwd</a></li>
<li>paper_authors: Vittorio Pippi, Fabio Quattrini, Silvia Cascianelli, Rita Cucchiara</li>
<li>for: 本研究旨在提供一个有效的评估手写文本生成（Styled HTG）模型表现的指标，以促进这个研究领域的发展。</li>
<li>methods: 我们提出了一种特定于HTG评估的评价指标——手写距离（HWD），基于一个专门为提取手写风格特征而训练的网络。HWD在变长输入图像的特征空间中工作，利用感知距离来比较手写中细小的几何特征。</li>
<li>results: 我们通过对不同的单词水平和行水平的手写文本图像数据进行广泛的实验评估，证明了提出的HWD是一个适合的Styled HTG评价指标。我们还将预训练的模型作为后向抽象，以便推广该指标的使用，以便为Styled HTG评价模型的研究提供一个有用的工具。<details>
<summary>Abstract</summary>
Styled Handwritten Text Generation (Styled HTG) is an important task in document analysis, aiming to generate text images with the handwriting of given reference images. In recent years, there has been significant progress in the development of deep learning models for tackling this task. Being able to measure the performance of HTG models via a meaningful and representative criterion is key for fostering the development of this research topic. However, despite the current adoption of scores for natural image generation evaluation, assessing the quality of generated handwriting remains challenging. In light of this, we devise the Handwriting Distance (HWD), tailored for HTG evaluation. In particular, it works in the feature space of a network specifically trained to extract handwriting style features from the variable-lenght input images and exploits a perceptual distance to compare the subtle geometric features of handwriting. Through extensive experimental evaluation on different word-level and line-level datasets of handwritten text images, we demonstrate the suitability of the proposed HWD as a score for Styled HTG. The pretrained model used as backbone will be released to ease the adoption of the score, aiming to provide a valuable tool for evaluating HTG models and thus contributing to advancing this important research area.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文<</SYS>>模仿手写文本生成（Styled HTG）是文档分析中的一项重要任务，目标是生成与参考图像的手写风格相同的文本图像。在过去几年，深度学习模型在解决这个任务上做出了 significiant 的进步。可以通过一个有意义和代表性的评价标准来衡量 HTG 模型的性能，这对于推动这个研究领域的发展是关键。然而，自然图像生成评价 scores 的当前采用不适用于 HTG 评价。为了解决这个问题，我们提出了 Handwriting Distance（HWD），专门为 HTG 评价设计的。具体来说，它在特定的网络中提取手写风格特征，并利用一种感知距离来比较手写文本中细微的几何特征。经过对不同的单词级和行级手写文本图像集进行广泛的实验评价，我们证明了我们提出的 HWD 是一个适合的 HTG 评价分数。我们将提供特定的预训练模型，以便推广 HWD，以便为 HTG 模型的评价提供一个有价值的工具，从而为这一重要研究领域的发展做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Bilateral-Network-with-Residual-U-blocks-and-Dual-Guided-Attention-for-Real-time-Semantic-Segmentation"><a href="#Bilateral-Network-with-Residual-U-blocks-and-Dual-Guided-Attention-for-Real-time-Semantic-Segmentation" class="headerlink" title="Bilateral Network with Residual U-blocks and Dual-Guided Attention for Real-time Semantic Segmentation"></a>Bilateral Network with Residual U-blocks and Dual-Guided Attention for Real-time Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20305">http://arxiv.org/abs/2310.20305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/likelidoa/bidganet">https://github.com/likelidoa/bidganet</a></li>
<li>paper_authors: Liang Liao, Liang Wan, Mingsheng Liu, Shusheng Li</li>
<li>for: 自动驾驶等应用场景需要使用semantic segmentation技术，即时性的要求更高于精度。两分支架构在当今年代得到了广泛应用，它将空间信息和semantic信息分开处理，使得模型可以采用两个较轻量级的网络组成。但是，将特征Feature fusion成为现在许多两分支模型的性能瓶颈。</li>
<li>methods: 我们提出了一种新的特征合并机制，即通过注意计算引导的Dual-Guided Attention（DGA）模块来取代一些多缘转换。我们只使用几个注意层来实现与多层融合相比的性能，而不是使用许多的多缘转换。</li>
<li>results: 我们通过Cityscapes和CamVid dataset进行了广泛的实验，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
When some application scenarios need to use semantic segmentation technology, like automatic driving, the primary concern comes to real-time performance rather than extremely high segmentation accuracy. To achieve a good trade-off between speed and accuracy, two-branch architecture has been proposed in recent years. It treats spatial information and semantics information separately which allows the model to be composed of two networks both not heavy. However, the process of fusing features with two different scales becomes a performance bottleneck for many nowaday two-branch models. In this research, we design a new fusion mechanism for two-branch architecture which is guided by attention computation. To be precise, we use the Dual-Guided Attention (DGA) module we proposed to replace some multi-scale transformations with the calculation of attention which means we only use several attention layers of near linear complexity to achieve performance comparable to frequently-used multi-layer fusion. To ensure that our module can be effective, we use Residual U-blocks (RSU) to build one of the two branches in our networks which aims to obtain better multi-scale features. Extensive experiments on Cityscapes and CamVid dataset show the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
In this research, we propose a new fusion mechanism for two-branch architecture that is guided by attention computation. Specifically, we use the Dual-Guided Attention (DGA) module to replace some multi-scale transformations with attention calculations, which allows us to achieve performance comparable to frequently-used multi-layer fusion with a much simpler and more efficient approach. To ensure the effectiveness of our module, we use Residual U-blocks (RSU) to build one of the two branches in our network, which helps to obtain better multi-scale features.Extensive experiments on the Cityscapes and CamVid datasets demonstrate the effectiveness of our method. By using the DGA module and RSU, we are able to achieve a good balance between speed and accuracy in semantic segmentation, which is essential for many real-world applications such as automatic driving.
</details></li>
</ul>
<hr>
<h2 id="Annotator-A-Generic-Active-Learning-Baseline-for-LiDAR-Semantic-Segmentation"><a href="#Annotator-A-Generic-Active-Learning-Baseline-for-LiDAR-Semantic-Segmentation" class="headerlink" title="Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation"></a>Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20293">http://arxiv.org/abs/2310.20293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binhui Xie, Shuang Li, Qingju Guo, Chi Harold Liu, Xinjing Cheng</li>
<li>for: 这个论文的目的是提出一种高效的活动学习基线，用于三角点云 semantic segmentation 任务，以便更有效地使用标注数据。</li>
<li>methods: 这个基eline使用了一种基于 voxel 的在线选择策略，可以高效地探索和标注 LiDAR 扫描数据中的关键和示例 voxel 网格。它还使用了 voxel 冲突度 (VCD) 来利用点云的本地拓扑关系和结构。</li>
<li>results: 这个基eline在多种任务中表现出色，包括活动学习 (AL)、活动源自由领域适应 (ASFDA) 和活动领域适应 (ADA)。它在多个 LiDAR semantic segmentation benchmark 上达到了高水平的性能，包括从 simulate 到实际的 scenario 以及实际 scenario 之间的比较。特别是，Annotator 只需要标注每个 LiDAR 扫描数据中的五个 voxel，可以达到87.8% 的全supervised 性能 under AL，88.5% under ASFDA，和94.4% under ADA。<details>
<summary>Abstract</summary>
Active learning, a label-efficient paradigm, empowers models to interactively query an oracle for labeling new data. In the realm of LiDAR semantic segmentation, the challenges stem from the sheer volume of point clouds, rendering annotation labor-intensive and cost-prohibitive. This paper presents Annotator, a general and efficient active learning baseline, in which a voxel-centric online selection strategy is tailored to efficiently probe and annotate the salient and exemplar voxel girds within each LiDAR scan, even under distribution shift. Concretely, we first execute an in-depth analysis of several common selection strategies such as Random, Entropy, Margin, and then develop voxel confusion degree (VCD) to exploit the local topology relations and structures of point clouds. Annotator excels in diverse settings, with a particular focus on active learning (AL), active source-free domain adaptation (ASFDA), and active domain adaptation (ADA). It consistently delivers exceptional performance across LiDAR semantic segmentation benchmarks, spanning both simulation-to-real and real-to-real scenarios. Surprisingly, Annotator exhibits remarkable efficiency, requiring significantly fewer annotations, e.g., just labeling five voxels per scan in the SynLiDAR-to-SemanticKITTI task. This results in impressive performance, achieving 87.8% fully-supervised performance under AL, 88.5% under ASFDA, and 94.4% under ADA. We envision that Annotator will offer a simple, general, and efficient solution for label-efficient 3D applications. Project page: https://binhuixie.github.io/annotator-web
</details>
<details>
<summary>摘要</summary>
aktive lärmung, ein label-effizientes Paradigma, ermöglicht Modellen, interaktiv einen Oracle für die Labeling neuer Daten zu befragen. In der Welt von LiDAR-semantischen Segmentierung gibt es Herausforderungen durch die enorme Menge an Punktwolken, die das Annotieren aufwendig und kostspielig machen. Diese Arbeit präsentiert Annotator, eine allgemeine und effiziente aktive lärmung-Basis, bei der eine voxel-zentrische online-Auswahlstrategie entwickelt wurde, um effizient die salienten und exemplarischen voxel-Gitter within each LiDAR-Scans zu probieren und zu annotieren, auch unter Veränderung der Verteilung.Concretely, we first analyze several common selection strategies such as Random, Entropy, Margin, and then develop voxel confusion degree (VCD) to exploit the local topology relations and structures of point clouds. Annotator excels in diverse settings, with a particular focus on aktive lärmung (AL), active source-free domain adaptation (ASFDA), and active domain adaptation (ADA). It consistently delivers exceptional performance across LiDAR semantic segmentation benchmarks, spanning both simulation-to-real and real-to-real scenarios. Surprisingly, Annotator exhibits remarkable efficiency, requiring significantly fewer annotations, e.g., just labeling five voxels per scan in the SynLiDAR-to-SemanticKITTI task. This results in impressive performance, achieving 87.8% fully-supervised performance under AL, 88.5% under ASFDA, and 94.4% under ADA. We envision that Annotator will offer a simple, general, and efficient solution for label-efficient 3D applications. Project page: <https://binhuixie.github.io/annotator-web>
</details></li>
</ul>
<hr>
<h2 id="IARS-SegNet-Interpretable-Attention-Residual-Skip-connection-SegNet-for-melanoma-segmentation"><a href="#IARS-SegNet-Interpretable-Attention-Residual-Skip-connection-SegNet-for-melanoma-segmentation" class="headerlink" title="IARS SegNet: Interpretable Attention Residual Skip connection SegNet for melanoma segmentation"></a>IARS SegNet: Interpretable Attention Residual Skip connection SegNet for melanoma segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20292">http://arxiv.org/abs/2310.20292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shankara Narayanan V, Sikha OK, Raul Benitez</li>
<li>for: 革新的皮肤 lesion 分割方法，帮助早期诊断皮肤癌</li>
<li>methods: 基于 SegNet 基础模型，添加了 skip connections、residual convolutions 和全局注意力机制，提高模型的解释性和精度</li>
<li>results: 提高了皮肤 lesion 分割精度，帮助早期诊断皮肤癌<details>
<summary>Abstract</summary>
Skin lesion segmentation plays a crucial role in the computer-aided diagnosis of melanoma. Deep Learning models have shown promise in accurately segmenting skin lesions, but their widespread adoption in real-life clinical settings is hindered by their inherent black-box nature. In domains as critical as healthcare, interpretability is not merely a feature but a fundamental requirement for model adoption. This paper proposes IARS SegNet an advanced segmentation framework built upon the SegNet baseline model. Our approach incorporates three critical components: Skip connections, residual convolutions, and a global attention mechanism onto the baseline Segnet architecture. These elements play a pivotal role in accentuating the significance of clinically relevant regions, particularly the contours of skin lesions. The inclusion of skip connections enhances the model's capacity to learn intricate contour details, while the use of residual convolutions allows for the construction of a deeper model while preserving essential image features. The global attention mechanism further contributes by extracting refined feature maps from each convolutional and deconvolutional block, thereby elevating the model's interpretability. This enhancement highlights critical regions, fosters better understanding, and leads to more accurate skin lesion segmentation for melanoma diagnosis.
</details>
<details>
<summary>摘要</summary>
皮肤损害分割在计算机辅助诊断癌症中扮演着关键性的角色。深度学习模型在精准地分割皮肤损害方面表现出了承诺，但是它们在实际医疗设置中广泛应用的阻碍因素之一是它们的内在的黑盒特性。在医疗领域中，可解释性不仅是一个特性，而是基本要求。这篇论文提出了IARS SegNet，一个高级分割框架，基于SegNet基础模型。我们的方法包括三个关键组件：跳过连接、径远 convolution 和全球注意力机制。这些元素在突出临床有关的区域，特别是皮肤损害的边缘方面发挥着关键作用。跳过连接使得模型学习细节，径远 convolution 使得模型深度化，全球注意力机制则提取了每个卷积和反卷积块中的精细特征图，从而提高模型的可解释性。这种改进使得模型更好地理解皮肤损害的重要区域，提高了诊断的准确性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-refinement-of-in-situ-images-acquired-by-low-electron-dose-LC-TEM"><a href="#Machine-learning-refinement-of-in-situ-images-acquired-by-low-electron-dose-LC-TEM" class="headerlink" title="Machine learning refinement of in situ images acquired by low electron dose LC-TEM"></a>Machine learning refinement of in situ images acquired by low electron dose LC-TEM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20279">http://arxiv.org/abs/2310.20279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiroyasu Katsuno, Yuki Kimura, Tomoya Yamazaki, Ichigaku Takigawa</li>
<li>for: 这篇论文是用于提高liquid-cell transmission electron microscopy（LC-TEM）中的图像精度的机器学习（ML）技术。</li>
<li>methods: 该模型使用U-Net架构和ResNetEncoder构建，并使用了一个自制的图像集来进行训练。图像集包括了不同的放大度和电子剂量下的样品图像对。</li>
<li>results: 训练后的模型可以将噪声图像转换成清晰图像，转换时间约为10ms。通过该模型，在Gatan DigitalMicrograph（DM）软件中不可见的 nanoparticle 在视窗中可以被成功地显示出来。<details>
<summary>Abstract</summary>
We study a machine learning (ML) technique for refining images acquired during in situ observation using liquid-cell transmission electron microscopy (LC-TEM). Our model is constructed using a U-Net architecture and a ResNet encoder. For training our ML model, we prepared an original image dataset that contained pairs of images of samples acquired with and without a solution present. The former images were used as noisy images and the latter images were used as corresponding ground truth images. The number of pairs of image sets was $1,204$ and the image sets included images acquired at several different magnifications and electron doses. The trained model converted a noisy image into a clear image. The time necessary for the conversion was on the order of 10ms, and we applied the model to in situ observations using the software Gatan DigitalMicrograph (DM). Even if a nanoparticle was not visible in a view window in the DM software because of the low electron dose, it was visible in a successive refined image generated by our ML model.
</details>
<details>
<summary>摘要</summary>
我们研究一种机器学习（ML）技术，用于对具有液体环境的电子顾问显微镜（LC-TEM）获得的图像进行精炼。我们的模型采用了U-Net架构和ResNet编码器。为了训练我们的ML模型，我们准备了一个原始图像集，其中包含了具有和无解方辑物质存在的图像对。前者用于含噪图像，后者用于相应的真实图像。该集共包含1,204对图像集，其中包括了不同的放大率和电子剂量下的图像。训练模型可以将含噪图像转化为清晰图像，转化时间在10ms左右，并且我们将模型应用到了使用软件Gatan DigitalMicrograph（DM）进行的境内观测。即使在DM软件中的视窗中没有显示nanoparticle因为低电子剂量，我们的ML模型仍可以在Successive Refined Image中显示它。
</details></li>
</ul>
<hr>
<h2 id="From-Denoising-Training-to-Test-Time-Adaptation-Enhancing-Domain-Generalization-for-Medical-Image-Segmentation"><a href="#From-Denoising-Training-to-Test-Time-Adaptation-Enhancing-Domain-Generalization-for-Medical-Image-Segmentation" class="headerlink" title="From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation"></a>From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20271">http://arxiv.org/abs/2310.20271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenruxue/detta">https://github.com/wenruxue/detta</a></li>
<li>paper_authors: Ruxue Wen, Hangjie Yuan, Dong Ni, Wenbo Xiao, Yaoyao Wu<br>for:The paper is written for addressing the challenge of domain generalization in medical image segmentation, specifically in the scenario of single-source domain data due to privacy concerns.methods:The proposed approach, called Denoising Y-Net (DeY-Net), incorporates an auxiliary denoising decoder into the basic U-Net architecture to perform denoising training and enhance domain generalization.results:The proposed method achieves significant domain generalization improvements over the baseline and state-of-the-art results compared to other methods, as demonstrated through extensive experiments on widely-adopted liver segmentation benchmarks.Here is the simplified Chinese translation of the three key points:for:这篇论文是为了解决医学图像分割中的频率领域泛化问题，特别是受到数据获取设备和其他因素的频率领域转换的单源频率领域数据问题。methods:该提议的方法是基于自我超级vised学习 paradigma，通过添加一个辅助的干扰去除解码器，使得U-Net架构中的基本结构得到增强的频率领域泛化能力。results:该提议的方法在广泛采用的肝脏分割 benchmark 上实现了对基eline和现有方法的显著频率领域泛化改进。<details>
<summary>Abstract</summary>
In medical image segmentation, domain generalization poses a significant challenge due to domain shifts caused by variations in data acquisition devices and other factors. These shifts are particularly pronounced in the most common scenario, which involves only single-source domain data due to privacy concerns. To address this, we draw inspiration from the self-supervised learning paradigm that effectively discourages overfitting to the source domain. We propose the Denoising Y-Net (DeY-Net), a novel approach incorporating an auxiliary denoising decoder into the basic U-Net architecture. The auxiliary decoder aims to perform denoising training, augmenting the domain-invariant representation that facilitates domain generalization. Furthermore, this paradigm provides the potential to utilize unlabeled data. Building upon denoising training, we propose Denoising Test Time Adaptation (DeTTA) that further: (i) adapts the model to the target domain in a sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive experiments conducted on widely-adopted liver segmentation benchmarks demonstrate significant domain generalization improvements over our baseline and state-of-the-art results compared to other methods. Code is available at https://github.com/WenRuxue/DeTTA.
</details>
<details>
<summary>摘要</summary>
医学图像分割中，领域总结是一项挑战，这是由数据获取设备和其他因素引起的领域差异所致。这些差异在最常见的场景中尤其突出，那是仅使用单源频道数据，这是由于隐私问题所致。为解决这个问题，我们 Draw inspiration from the self-supervised learning paradigm，这种方法可以减少预测频道数据的过拟合。我们提议一种新的方法，即附加auxiliary denoising decoder到基本U-Net架构中。这个auxiliary decoder的目的是在denoising训练中提高领域不变的表示。此外，这种方法还可以利用无标签数据。在denoising训练的基础之上，我们提议denoising Test Time Adaptation（DeTTA），它可以：（i）在目标频道上采样化地适应模型，和（ii）适应受到噪声损害的输入。我们对广泛采用的肝脏分割benchmark进行了广泛的实验，结果表明我们的方法在领域总结方面具有显著的改进。代码可以在https://github.com/WenRuxue/DeTTA中找到。
</details></li>
</ul>
<hr>
<h2 id="Low-Dose-CT-Image-Enhancement-Using-Deep-Learning"><a href="#Low-Dose-CT-Image-Enhancement-Using-Deep-Learning" class="headerlink" title="Low-Dose CT Image Enhancement Using Deep Learning"></a>Low-Dose CT Image Enhancement Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20265">http://arxiv.org/abs/2310.20265</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Demir, M. M. A. Shames, O. N. Gerek, S. Ergin, M. Fidan, M. Koc, M. B. Gulmezoglu, A. Barkana, C. Calisir</li>
<li>for: 降低ioniZing radiation的CT图像重建</li>
<li>methods: 使用U-NET进行图像进行增强</li>
<li>results: 比起低剂量版本，U-NET增强的半剂量CT图像可以提供更好的视觉效果和诊断效果，甚至比于全剂量CT图像。<details>
<summary>Abstract</summary>
The application of ionizing radiation for diagnostic imaging is common around the globe. However, the process of imaging, itself, remains to be a relatively hazardous operation. Therefore, it is preferable to use as low a dose of ionizing radiation as possible, particularly in computed tomography (CT) imaging systems, where multiple x-ray operations are performed for the reconstruction of slices of body tissues. A popular method for radiation dose reduction in CT imaging is known as the quarter-dose technique, which reduces the x-ray dose but can cause a loss of image sharpness. Since CT image reconstruction from directional x-rays is a nonlinear process, it is analytically difficult to correct the effect of dose reduction on image quality. Recent and popular deep-learning approaches provide an intriguing possibility of image enhancement for low-dose artifacts. Some recent works propose combinations of multiple deep-learning and classical methods for this purpose, which over-complicate the process. However, it is observed here that the straight utilization of the well-known U-NET provides very successful results for the correction of low-dose artifacts. Blind tests with actual radiologists reveal that the U-NET enhanced quarter-dose CT images not only provide an immense visual improvement over the low-dose versions, but also become diagnostically preferable images, even when compared to their full-dose CT versions.
</details>
<details>
<summary>摘要</summary>
globally， applying ionizing radiation for diagnostic imaging is common， but the imaging process itself is relatively hazardous， so it is best to use as little ionizing radiation as possible， especially in computed tomography（CT）imaging systems， where multiple x-ray operations are performed to reconstruct body tissue slices。a popular method for reducing radiation doses in CT imaging is the quarter-dose technique， which reduces x-ray doses but can cause a loss of image sharpness。since CT image reconstruction from directional x-rays is a nonlinear process，it is difficult to correct the effect of dose reduction on image quality analytically。recent and popular deep-learning approaches provide a promising possibility of enhancing low-dose images。some recent works propose combining multiple deep-learning and classical methods for this purpose， which complicates the process。however， it is found here that directly utilizing the well-known U-NET provides very successful results for correcting low-dose artifacts。blind tests with actual radiologists show that the U-NET enhanced quarter-dose CT images not only have a significant visual improvement over the low-dose versions， but also become diagnostically preferable images， even when compared to their full-dose CT versions。
</details></li>
</ul>
<hr>
<h2 id="Pose-to-Motion-Cross-Domain-Motion-Retargeting-with-Pose-Prior"><a href="#Pose-to-Motion-Cross-Domain-Motion-Retargeting-with-Pose-Prior" class="headerlink" title="Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior"></a>Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20249">http://arxiv.org/abs/2310.20249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingqing Zhao, Peizhuo Li, Wang Yifan, Olga Sorkine-Hornung, Gordon Wetzstein</li>
<li>for: 本研究旨在生成具有可信度的动作，以便在计算机图形中创造更加真实的人物形象。</li>
<li>methods: 我们利用pose数据作为代替的数据源，并介绍了一种基于神经网络的动作合成方法，通过重定向来将源Character的动作特征传递给目标Character。</li>
<li>results: 我们的方法可以具有小量或噪音的 pose数据集来生成可信度高的动作，并在用户测试中被评估为更加愉悦、更加真实和具有 fewer artifacts。Here’s the translation in English:</li>
<li>for: The goal of this research is to generate believable motions to create more realistic characters in computer graphics.</li>
<li>methods: We utilize pose data as an alternative data source and introduce a neural motion synthesis approach through retargeting, transferring the motion features of an existing motion capture dataset of one character to another character with drastically different skeletons.</li>
<li>results: Our method can generate high-quality motions with small or noisy pose data sets and is evaluated as more enjoyable, more lifelike, and with fewer artifacts in a user study.<details>
<summary>Abstract</summary>
Creating believable motions for various characters has long been a goal in computer graphics. Current learning-based motion synthesis methods depend on extensive motion datasets, which are often challenging, if not impossible, to obtain. On the other hand, pose data is more accessible, since static posed characters are easier to create and can even be extracted from images using recent advancements in computer vision. In this paper, we utilize this alternative data source and introduce a neural motion synthesis approach through retargeting. Our method generates plausible motions for characters that have only pose data by transferring motion from an existing motion capture dataset of another character, which can have drastically different skeletons. Our experiments show that our method effectively combines the motion features of the source character with the pose features of the target character, and performs robustly with small or noisy pose data sets, ranging from a few artist-created poses to noisy poses estimated directly from images. Additionally, a conducted user study indicated that a majority of participants found our retargeted motion to be more enjoyable to watch, more lifelike in appearance, and exhibiting fewer artifacts. Project page: https://cyanzhao42.github.io/pose2motion
</details>
<details>
<summary>摘要</summary>
创造可信的动作 для各种角色已经是计算机图形的长期目标。当前的学习基于动作合成方法通常需要大量的动作数据，而这些数据往往很难或无法获得。然而，姿势数据更加 accessible，因为静止姿势的角色更容易创建，甚至可以从图像中提取使用最新的计算机视觉技术。在这篇论文中，我们利用这个替代数据源，并提出了一种神经动作合成方法通过重定向。我们的方法可以将来自另一个角色的动作数据转移到目标角色中，即使这两个角色有极大的骨架结构差异。我们的实验表明，我们的方法可以有效地将来自源角色的动作特征与目标角色的姿势特征结合在一起，并在小或噪音的姿势数据集上表现稳定。此外，我们进行了一项用户研究，发现大多数参与者认为我们的重定向动作更加有趣、更加生动、并且 fewer artifacts。项目页面：https://cyanzhao42.github.io/pose2motion
</details></li>
</ul>
<hr>
<h2 id="Contrast-agent-induced-deterministic-component-of-CT-density-in-the-abdominal-aorta-during-routine-angiography-proof-of-concept-study"><a href="#Contrast-agent-induced-deterministic-component-of-CT-density-in-the-abdominal-aorta-during-routine-angiography-proof-of-concept-study" class="headerlink" title="Contrast-agent-induced deterministic component of CT-density in the abdominal aorta during routine angiography: proof of concept study"></a>Contrast-agent-induced deterministic component of CT-density in the abdominal aorta during routine angiography: proof of concept study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20243">http://arxiv.org/abs/2310.20243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria R. Kodenko, Yuriy A. Vasilev, Nicholas S. Kulberg, Andrey V. Samorodov, Anton V. Vladzimirskyy, Olga V. Omelyanskaya, Roman V. Reshetnikov</li>
<li>for: 这个论文的目的是开发一种基于CTA数据的血液动力学模型，以便 investigate和优化CTA扫描过程，无需额外的 perfusion CT 研究。</li>
<li>methods: 该模型基于Beer-Lambert法则和血液和CA之间的化学交互的假设，具有六个参数，代表血液动力学特性。通过非线性最小二乘法和Levenberg-Marquardt优化算法，该模型被适应到数据中。</li>
<li>results: 研究分析了594个CTA图像（4个研究，每个研究144张 slice，IQR [134; 158.5]，1:1正常:疾病平衡），并证明了模型的合理性（Wilcox 测试，所有情况p-value &gt; 0.05）。该模型能够正确模拟正常血液流动和地方畸形所引起的血液动力学异常。<details>
<summary>Abstract</summary>
Background and objective: CTA is a gold standard of preoperative diagnosis of abdominal aorta and typically used for geometric-only characteristic extraction. We assume that a model describing the dynamic behavior of the contrast agent in the vessel can be developed from the data of routine CTA studies, allowing the procedure to be investigated and optimized without the need for additional perfusion CT studies. Obtained spatial distribution of CA can be valuable for both increasing the diagnostic value of a particular study and improving the CT data processing tools. Methods: In accordance with the Beer-Lambert law and the absence of chemical interaction between blood and CA, we postulated the existence of a deterministic CA-induced component in the CT signal density. The proposed model, having a double-sigmoid structure, contains six coefficients relevant to the properties of hemodynamics. To validate the model, expert segmentation was performed using the 3D Slicer application for the CTA data obtained from publicly available source. The model was fitted to the data using the non-linear least square method with Levenberg-Marquardt optimization. Results: We analyzed 594 CTA images (4 studies with median size of 144 slices, IQR [134; 158.5]; 1:1 normal:pathology balance). Goodness-of-fit was proved by Wilcox test (p-value > 0.05 for all cases). The proposed model correctly simulated normal blood flow and hemodynamics disturbances caused by local abnormalities (aneurysm, thrombus and arterial branching). Conclusions: Proposed approach can be useful for personalized CA modeling of vessels, improvement of CTA image processing and preparation of synthetic CT training data for artificial intelligence.
</details>
<details>
<summary>摘要</summary>
背景和目标：CTA是胃肠动脉预操作诊断的金标准，通常用于geometry-only特征提取。我们假设可以从 Routine CTA 研究中获得动态CA的模型，以便无需额外的 perfusion CT 研究，对过程进行调查和优化。获得的CA分布可以提高特定研究的诊断价值，以及CT数据处理工具。方法：根据Beer-Lambert法则和血液和CA之间的化学交互 absence，我们假设CA在血液中引起的Deterministic组件存在于CT信号密度中。我们提出的模型具有双sigmoid结构，包含6个相关血流特性的系数。为验证模型，我们使用3D Slicer应用程序进行专家分割，对CTA数据进行公共可用源 obtention。模型使用非线性最小二乘法与Levenberg-Marquardt优化算法进行适应。结果：我们分析了594个CTA图像（4个研究， median size为144层，IQR=[134;158.5]；1:1正常:疾病平衡）。好度Of-fit已经证明了Wilcox测试（p-value>0.05 для所有情况）。我们的模型正确模拟了正常血流和地方异常（aneurysm, thrombus和arterial branching）引起的血流异常。结论：我们的方法可以用于个性化CA模型化、CTA图像处理提高和人工智能synthetic CT培训数据的准备。
</details></li>
</ul>
<hr>
<h2 id="HEDNet-A-Hierarchical-Encoder-Decoder-Network-for-3D-Object-Detection-in-Point-Clouds"><a href="#HEDNet-A-Hierarchical-Encoder-Decoder-Network-for-3D-Object-Detection-in-Point-Clouds" class="headerlink" title="HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds"></a>HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20234">http://arxiv.org/abs/2310.20234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanggang001/hednet">https://github.com/zhanggang001/hednet</a></li>
<li>paper_authors: Gang Zhang, Junnan Chen, Guohuan Gao, Jianmin Li, Xiaolin Hu</li>
<li>for: 3D object detection in point clouds for autonomous driving systems</li>
<li>methods: uses hierarchical encoder-decoder blocks to capture long-range dependencies among features in the spatial space</li>
<li>results: achieved superior detection accuracy on the Waymo Open and nuScenes datasets compared to previous state-of-the-art methods with competitive efficiencyHere’s the full translation in Simplified Chinese:</li>
<li>for: 这篇论文旨在提出一种高性能的3D物体检测方法，用于自动驾驶系统中的点云检测。</li>
<li>methods: 该方法使用层次编码器-解码器块来捕捉点云中特征之间的长距离依赖关系，特别是用于大小远的物体检测。</li>
<li>results: 对于Waymo开放和nuScenes dataset，HEDNet实现了前期最优的检测精度，并与之前的状态之 искусственный智能方法兼容效率。I hope this helps!<details>
<summary>Abstract</summary>
3D object detection in point clouds is important for autonomous driving systems. A primary challenge in 3D object detection stems from the sparse distribution of points within the 3D scene. Existing high-performance methods typically employ 3D sparse convolutional neural networks with small kernels to extract features. To reduce computational costs, these methods resort to submanifold sparse convolutions, which prevent the information exchange among spatially disconnected features. Some recent approaches have attempted to address this problem by introducing large-kernel convolutions or self-attention mechanisms, but they either achieve limited accuracy improvements or incur excessive computational costs. We propose HEDNet, a hierarchical encoder-decoder network for 3D object detection, which leverages encoder-decoder blocks to capture long-range dependencies among features in the spatial space, particularly for large and distant objects. We conducted extensive experiments on the Waymo Open and nuScenes datasets. HEDNet achieved superior detection accuracy on both datasets than previous state-of-the-art methods with competitive efficiency. The code is available at https://github.com/zhanggang001/HEDNet.
</details>
<details>
<summary>摘要</summary>
三维物体检测在点云中是自动驾驶系统中的重要任务。主要检测挑战在3D场景中点的稀疏分布。现有高性能方法通常采用3D稀疏卷积神经网络，使用小kernel进行特征提取。以减少计算成本，这些方法通常采用子拟空间稀疏卷积，这会阻止空间分布在特征之间的信息交换。一些最近的方法尝试了解决这个问题，通过引入大kernel卷积或自注意机制，但它们 Either achieve limited accuracy improvements or incur excessive computational costs。我们提出了HEDNet，一种嵌入器-解码器网络，用于3D物体检测，它利用嵌入器-解码器块来捕捉特征在空间空间中的长距离依赖关系，特别是 для大的和远的对象。我们在 Waymo Open 和 nuScenes 数据集上进行了广泛的实验，HEDNet在两个数据集上 than previous state-of-the-art methods with competitive efficiency。代码可以在 https://github.com/zhanggang001/HEDNet 上找到。
</details></li>
</ul>
<hr>
<h2 id="UWFormer-Underwater-Image-Enhancement-via-a-Semi-Supervised-Multi-Scale-Transformer"><a href="#UWFormer-Underwater-Image-Enhancement-via-a-Semi-Supervised-Multi-Scale-Transformer" class="headerlink" title="UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale Transformer"></a>UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20210">http://arxiv.org/abs/2310.20210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuhang Chen, Zinuo Li, Shenghong Luo, Weiwen Chen, Shuqiang Wang, Chi-Man Pun</li>
<li>for: 提高水下图像质量，增强图像多scale，提高图像对比度和色彩均衡。</li>
<li>methods: 使用Multi-scale Transformer-based Network（UWFormer），具有semi-supervised learning和Nonlinear Frequency-aware Attention机制，以及Multi-Scale Fusion Feed-forward Network，以提高低频增强。同时，提出了一种特有的水下半监督训练策略，使用Subaqueous Perceptual Loss函数生成可靠的 Pseudo标签。</li>
<li>results: 经过实验表明，我们的方法在水下Full-reference和Non-reference benchmark上比前式方法提高质量和视觉质量。<details>
<summary>Abstract</summary>
Underwater images often exhibit poor quality, imbalanced coloration, and low contrast due to the complex and intricate interaction of light, water, and objects. Despite the significant contributions of previous underwater enhancement techniques, there exist several problems that demand further improvement: (i) Current deep learning methodologies depend on Convolutional Neural Networks (CNNs) that lack multi-scale enhancement and also have limited global perception fields. (ii) The scarcity of paired real-world underwater datasets poses a considerable challenge, and the utilization of synthetic image pairs risks overfitting. To address the aforementioned issues, this paper presents a Multi-scale Transformer-based Network called UWFormer for enhancing images at multiple frequencies via semi-supervised learning, in which we propose a Nonlinear Frequency-aware Attention mechanism and a Multi-Scale Fusion Feed-forward Network for low-frequency enhancement. Additionally, we introduce a specialized underwater semi-supervised training strategy, proposing a Subaqueous Perceptual Loss function to generate reliable pseudo labels. Experiments using full-reference and non-reference underwater benchmarks demonstrate that our method outperforms state-of-the-art methods in terms of both quantity and visual quality.
</details>
<details>
<summary>摘要</summary>
水下图像经常呈现低质量、颜色不均、对比度低的问题，这主要归结于光线、水和物体之间复杂的互动。尽管过去的水下改进技术做出了重要贡献，但还有一些问题需要进一步改进：（i）当前的深度学习方法ologies rely heavily on Convolutional Neural Networks (CNNs)，它们缺乏多尺度增强和全球视场观察。（ii）水下实际数据缺乏对照数据，使用synthetic image pairs risk overfitting。为 Addressing these issues, this paper presents a Multi-scale Transformer-based Network called UWFormer for enhancing images at multiple frequencies via semi-supervised learning. Specifically, we propose a Nonlinear Frequency-aware Attention mechanism and a Multi-Scale Fusion Feed-forward Network for low-frequency enhancement. In addition, we introduce a specialized underwater semi-supervised training strategy, including a Subaqueous Perceptual Loss function to generate reliable pseudo labels. Experimental results using full-reference and non-reference underwater benchmarks show that our method outperforms state-of-the-art methods in terms of both quantity and visual quality.
</details></li>
</ul>
<hr>
<h2 id="ZoomNeXt-A-Unified-Collaborative-Pyramid-Network-for-Camouflaged-Object-Detection"><a href="#ZoomNeXt-A-Unified-Collaborative-Pyramid-Network-for-Camouflaged-Object-Detection" class="headerlink" title="ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection"></a>ZoomNeXt: A Unified Collaborative Pyramid Network for Camouflaged Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20208">http://arxiv.org/abs/2310.20208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lartpang/ZoomNeXt">https://github.com/lartpang/ZoomNeXt</a></li>
<li>paper_authors: Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang, Huchuan Lu</li>
<li>for: 本研究旨在提高Camouflaged Object Detection（COD）的精度和效果，使其能够更好地在真实世界中检测涂抹在背景中的 объек。</li>
<li>methods: 我们提出了一种有效的协同卷积网络，它通过模拟人类观察者对混乱图像和视频的方式，例如快速呈现和聚焦，来学习混乱图像中的Semantics。</li>
<li>results: 我们的方法在图像和视频COD benchmark上表现出色，与现有状态的方法相比，具有更高的准确率和效果。<details>
<summary>Abstract</summary>
Recent camouflaged object detection (COD) attempts to segment objects visually blended into their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from the high intrinsic similarity between camouflaged objects and their background, objects are usually diverse in scale, fuzzy in appearance, and even severely occluded. To this end, we propose an effective unified collaborative pyramid network which mimics human behavior when observing vague images and videos, \textit{i.e.}, zooming in and out. Specifically, our approach employs the zooming strategy to learn discriminative mixed-scale semantics by the multi-head scale integration and rich granularity perception units, which are designed to fully explore imperceptible clues between candidate objects and background surroundings. The former's intrinsic multi-head aggregation provides more diverse visual patterns. The latter's routing mechanism can effectively propagate inter-frame difference in spatiotemporal scenarios and adaptively ignore static representations. They provides a solid foundation for realizing a unified architecture for static and dynamic COD. Moreover, considering the uncertainty and ambiguity derived from indistinguishable textures, we construct a simple yet effective regularization, uncertainty awareness loss, to encourage predictions with higher confidence in candidate regions. Our highly task-friendly framework consistently outperforms existing state-of-the-art methods in image and video COD benchmarks. The code will be available at \url{https://github.com/lartpang/ZoomNeXt}.
</details>
<details>
<summary>摘要</summary>
近期隐形物检测（COD）尝试将物体视觉上隐藏在背景中，实际上非常复杂和困难。除了物体与背景高度相似外，物体通常是多种尺度、模糊的外观，甚至受到干扰。为此，我们提出了一种高效的统一协同PYRAMID网络，模仿人类观察模糊图像和视频时的行为，即缩进和缩出。具体来说，我们的方法利用缩进策略学习混合尺度 semantics，通过多头积分单元和丰富的准确度感知单元来全面探索不可见的准确信息。前者的内置多头积分提供更多的视觉模式。后者的路由机制可以有效地传递干扰ifference between candidate objects and background surroundings。它们提供了一个固定的基础 для实现静止和动态COD的统一架构。此外，鉴于不可预测的文本特征，我们构建了一个简单 yet effective的正则项，uncertainty awareness loss，以促进候选区域的更高 confidence。我们的高效任务友好的框架在图像和视频COD标准测试上一直保持领先地位。代码将在 \url{https://github.com/lartpang/ZoomNeXt} 上提供。
</details></li>
</ul>
<hr>
<h2 id="Visible-to-Thermal-image-Translation-for-improving-visual-task-in-low-light-conditions"><a href="#Visible-to-Thermal-image-Translation-for-improving-visual-task-in-low-light-conditions" class="headerlink" title="Visible to Thermal image Translation for improving visual task in low light conditions"></a>Visible to Thermal image Translation for improving visual task in low light conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20190">http://arxiv.org/abs/2310.20190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Azim Khan</li>
<li>for: 用于解决RGB图像在低光照下完成视觉任务时的挑战，使用热变化图像来弥补这些挑战。</li>
<li>methods: 提出了一个端到端框架，包括生成网络和检测网络，用于将RGB图像翻译成热图像，并对生成的热图像与实际数据进行比较。</li>
<li>results: 研究发现，使用GAN可以将RGB图像翻译成热图像，并且生成的热图像与实际数据有高度的相似性。这些发现可以帮助解决视觉任务中的低光照问题，有利于安全和监测应用。<details>
<summary>Abstract</summary>
Several visual tasks, such as pedestrian detection and image-to-image translation, are challenging to accomplish in low light using RGB images. Heat variation of objects in thermal images can be used to overcome this. In this work, an end-to-end framework, which consists of a generative network and a detector network, is proposed to translate RGB image into Thermal ones and compare generated thermal images with real data. We have collected images from two different locations using the Parrot Anafi Thermal drone. After that, we created a two-stream network, preprocessed, augmented, the image data, and trained the generator and discriminator models from scratch. The findings demonstrate that it is feasible to translate RGB training data to thermal data using GAN. As a result, thermal data can now be produced more quickly and affordably, which is useful for security and surveillance applications.
</details>
<details>
<summary>摘要</summary>
几种视觉任务，如人员检测和图像转换，在低光照情况下使用RGB图像是困难的。在这种情况下，物体的热度变化在热图像中可以用来超越这一问题。本工作提出了一个端到端框架，该框架包括生成网络和检测网络，用于将RGB图像翻译成热图像，并将生成的热图像与实际数据进行比较。我们从两个不同的位置收集了Parrot Anafi热影机飞行器拍摄的图像。然后，我们创建了两�ream网络，对图像数据进行了预处理、增强、训练生成器和判断器模型从头开始。研究结果表明，使用GAN将RGB训练数据翻译成热数据是可行的。这意味着可以更快速、更经济地生成热数据，这对安全监控应用非常有用。
</details></li>
</ul>
<hr>
<h2 id="LFAA-Crafting-Transferable-Targeted-Adversarial-Examples-with-Low-Frequency-Perturbations"><a href="#LFAA-Crafting-Transferable-Targeted-Adversarial-Examples-with-Low-Frequency-Perturbations" class="headerlink" title="LFAA: Crafting Transferable Targeted Adversarial Examples with Low-Frequency Perturbations"></a>LFAA: Crafting Transferable Targeted Adversarial Examples with Low-Frequency Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20175">http://arxiv.org/abs/2310.20175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunyu Wang, Juluan Shi, Wenxuan Wang</li>
<li>for: 防御深度神经网络受到攻击的安全性和可靠性问题，特别是攻击者可以通过 Transfer-based 攻击来让模型错误地预测图像。</li>
<li>methods: 我们提出了一种新的方法，即 Low-Frequency Adversarial Attack (\name)，它通过让模型受到高频信息的扰动来实现targeted攻击。我们使用了一个conditional generator来生成targeted adversarial perturbations，然后将其添加到图像的low-frequency组件中。</li>
<li>results: 我们的方法在ImageNet上进行了广泛的实验，结果显示，我们的方法可以 Significantly outperform state-of-the-art methods，提高targeted攻击成功率比例从3.2%到15.5%。<details>
<summary>Abstract</summary>
Deep neural networks are susceptible to adversarial attacks, which pose a significant threat to their security and reliability in real-world applications. The most notable adversarial attacks are transfer-based attacks, where an adversary crafts an adversarial example to fool one model, which can also fool other models. While previous research has made progress in improving the transferability of untargeted adversarial examples, the generation of targeted adversarial examples that can transfer between models remains a challenging task. In this work, we present a novel approach to generate transferable targeted adversarial examples by exploiting the vulnerability of deep neural networks to perturbations on high-frequency components of images. We observe that replacing the high-frequency component of an image with that of another image can mislead deep models, motivating us to craft perturbations containing high-frequency information to achieve targeted attacks. To this end, we propose a method called Low-Frequency Adversarial Attack (\name), which trains a conditional generator to generate targeted adversarial perturbations that are then added to the low-frequency component of the image. Extensive experiments on ImageNet demonstrate that our proposed approach significantly outperforms state-of-the-art methods, improving targeted attack success rates by a margin from 3.2\% to 15.5\%.
</details>
<details>
<summary>摘要</summary>
In this work, we present a novel approach to generate transferable targeted adversarial examples by exploiting the vulnerability of deep neural networks to perturbations on high-frequency components of images. We observe that replacing the high-frequency component of an image with that of another image can mislead deep models, motivating us to craft perturbations containing high-frequency information to achieve targeted attacks.To this end, we propose a method called Low-Frequency Adversarial Attack (\name), which trains a conditional generator to generate targeted adversarial perturbations that are then added to the low-frequency component of the image. Extensive experiments on ImageNet demonstrate that our proposed approach significantly outperforms state-of-the-art methods, improving targeted attack success rates by a margin from 3.2\% to 15.5\%.
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Diabetic-Foot-Ulcer-Images-with-Diffusion-Model"><a href="#Synthesizing-Diabetic-Foot-Ulcer-Images-with-Diffusion-Model" class="headerlink" title="Synthesizing Diabetic Foot Ulcer Images with Diffusion Model"></a>Synthesizing Diabetic Foot Ulcer Images with Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20140">http://arxiv.org/abs/2310.20140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Basiri, Karim Manji, Francois Harton, Alisha Poonja, Milos R. Popovic, Shehroz S. Khan</li>
<li>for: 这篇论文旨在探讨抗生素激烈网络和传播模型的应用在生成抗生素激烈照片方面，以便提高医疗训练和研究活动中的照片数据量。</li>
<li>methods: 本论文使用了抗生素激烈网络和传播模型来生成抗生素激烈照片，并通过专业医生评估以评估生成的照片的authenticity。</li>
<li>results: 研究结果显示，抗生素激烈网络和传播模型能够成功地生成可以与真实照片相似的抗生素激烈照片，70%的时间，医生会认为生成的照片是真正的抗生素激烈照片。但是，医生对生成的照片的信心较低，并且对真实照片的评价较高。研究也发现，FID和KID指数不能够协调医生的评价，建议应用更多的评估方法。<details>
<summary>Abstract</summary>
Diabetic Foot Ulcer (DFU) is a serious skin wound requiring specialized care. However, real DFU datasets are limited, hindering clinical training and research activities. In recent years, generative adversarial networks and diffusion models have emerged as powerful tools for generating synthetic images with remarkable realism and diversity in many applications. This paper explores the potential of diffusion models for synthesizing DFU images and evaluates their authenticity through expert clinician assessments. Additionally, evaluation metrics such as Frechet Inception Distance (FID) and Kernel Inception Distance (KID) are examined to assess the quality of the synthetic DFU images. A dataset of 2,000 DFU images is used for training the diffusion model, and the synthetic images are generated by applying diffusion processes. The results indicate that the diffusion model successfully synthesizes visually indistinguishable DFU images. 70% of the time, clinicians marked synthetic DFU images as real DFUs. However, clinicians demonstrate higher unanimous confidence in rating real images than synthetic ones. The study also reveals that FID and KID metrics do not significantly align with clinicians' assessments, suggesting alternative evaluation approaches are needed. The findings highlight the potential of diffusion models for generating synthetic DFU images and their impact on medical training programs and research in wound detection and classification.
</details>
<details>
<summary>摘要</summary>
糖尿病足淋浸 (DFU) 是一种严重的皮肤伤害，需要专门的护理。然而，实际的 DFU 数据却受到限制，影响临床训练和研究活动。在最近几年，生成对抗网络和扩散模型在许多应用场景中显示出了强大的生成能力和多样性。本文探讨了扩散模型在生成 DFU 图像方面的潜力，并通过专业医生评估来评估生成的图像authenticity。此外，Frechet Inception Distance (FID) 和 Kernel Inception Distance (KID) 等评估指标也被检查，以评估生成的 DFU 图像质量。使用了 2,000 张 DFU 图像进行训练，并通过扩散过程生成 synthetic 图像。结果表明，扩散模型成功地生成了可以不可分辩的 DFU 图像。70% 的时间，专业医生将生成的 synthetic DFU 图像标记为真实的 DFU。然而，专业医生对真实图像的评估高于对生成图像的评估。研究还发现，FID 和 KID 指标与专业医生的评估不符， suggesting alternative evaluation approaches are needed。发现表明扩散模型对生成 DFU 图像的潜力，并且对临床训练和研究在疤痕检测和分类方面的影响。
</details></li>
</ul>
<hr>
<h2 id="Team-I2R-VI-FF-Technical-Report-on-EPIC-KITCHENS-VISOR-Hand-Object-Segmentation-Challenge-2023"><a href="#Team-I2R-VI-FF-Technical-Report-on-EPIC-KITCHENS-VISOR-Hand-Object-Segmentation-Challenge-2023" class="headerlink" title="Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object Segmentation Challenge 2023"></a>Team I2R-VI-FF Technical Report on EPIC-KITCHENS VISOR Hand Object Segmentation Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20120">http://arxiv.org/abs/2310.20120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fen Fang, Yi Cheng, Ying Sun, Qianli Xu</li>
<li>for: 本研究报告是关于EPIC-KITCHENS VISOR Hand Object Segmentation Challenge的方法，该挑战的目标是根据单一帧输入中的手和对象的关系进行估计。EPIC-KITCHENS VISOR数据集提供了像素级注释，并作为 egocentric video中手和活动对象分割的标准准例。</li>
<li>methods: 我们的方法结合了基线方法，即Point-based Rendering（PointRend）和Segment Anything Model（SAM），以提高手和对象分割结果的准确性，同时避免失找检测的情况。我们利用基线方法提供的准确的手段图以提取更加精确的手和接触对象段。我们使用SAM提供的类型不敏感分 segmentation，并应用特定的手工制约以改进结果。在基线模型错过检测手或对象的情况下，我们将训练一个对象检测器，以提高检测精度。</li>
<li>results: 我们的提交使用我们的改进方法，在EPIC-KITCHENS VISOR Hand Object Segmentation Challenge中的评估标准下达到了第一名。<details>
<summary>Abstract</summary>
In this report, we present our approach to the EPIC-KITCHENS VISOR Hand Object Segmentation Challenge, which focuses on the estimation of the relation between the hands and the objects given a single frame as input. The EPIC-KITCHENS VISOR dataset provides pixel-wise annotations and serves as a benchmark for hand and active object segmentation in egocentric video. Our approach combines the baseline method, i.e., Point-based Rendering (PointRend) and the Segment Anything Model (SAM), aiming to enhance the accuracy of hand and object segmentation outcomes, while also minimizing instances of missed detection. We leverage accurate hand segmentation maps obtained from the baseline method to extract more precise hand and in-contact object segments. We utilize the class-agnostic segmentation provided by SAM and apply specific hand-crafted constraints to enhance the results. In cases where the baseline model misses the detection of hands or objects, we re-train an object detector on the training set to enhance the detection accuracy. The detected hand and in-contact object bounding boxes are then used as prompts to extract their respective segments from the output of SAM. By effectively combining the strengths of existing methods and applying our refinements, our submission achieved the 1st place in terms of evaluation criteria in the VISOR HOS Challenge.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我们介绍了我们对EPIC-KITCHENS VISOR手Object Segmentation挑战的方法，该挑战关注于给定一帧输入的手和对象之间的关系的估算。EPIC-KITCHENS VISOR数据集提供了像素级注释，并作为手和活动对象分割在 egocentric 视频中的标准准的 benchmark。我们的方法结合基线方法，即 Point-based Rendering（PointRend）和 Segment Anything Model（SAM），以提高手和对象分割结果的准确性，同时避免错过检测的情况。我们利用基eline方法提供的准确的手段图来提取更加精确的手和接触对象分割。我们利用 SAM 提供的类型不敏感分割，并应用特定的手工制约来提高结果。在基线模型错过检测手或对象的情况下，我们将对训练集进行重新训练，以提高检测精度。检测到的手和接触对象 bounding box 然后用作 SAM 输出中提取其分割的Prompt。通过有效地结合现有方法的优点和我们的调整，我们的提交实现了评估标准中的第一名。
</details></li>
</ul>
<hr>
<h2 id="Refined-Equivalent-Pinhole-Model-for-Large-scale-3D-Reconstruction-from-Spaceborne-CCD-Imagery"><a href="#Refined-Equivalent-Pinhole-Model-for-Large-scale-3D-Reconstruction-from-Spaceborne-CCD-Imagery" class="headerlink" title="Refined Equivalent Pinhole Model for Large-scale 3D Reconstruction from Spaceborne CCD Imagery"></a>Refined Equivalent Pinhole Model for Large-scale 3D Reconstruction from Spaceborne CCD Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20117">http://arxiv.org/abs/2310.20117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Danyang, Yu Anzhu, Ji Song, Cao Xuefeng, Quan Yujun, Guo Wenyue, Qiu Chunping</li>
<li>for: 本研究旨在开发一种基于直线阵列CCD卫星成像的大规模地表重建管线。</li>
<li>methods: 我们引入了一种将理性函数模型（RFM）与小孔镜模型（PCM）等价的方法，并 derivated一个错误公式 для这种等价小孔镜模型，以示图像大小对重建精度的影响。我们还提出了一种多项式图像纠正模型，通过最小二乘法来最小化等价错误。</li>
<li>results: 我们在四个图像Dataset（WHU-TLC、DFC2019、ISPRS-ZY3和GF7）上进行了实验，结果表明重建精度与图像大小直接相关。我们的多项式图像纠正模型可以显著提高重建精度和完teness，特别是对于大规模图像。<details>
<summary>Abstract</summary>
In this study, we present a large-scale earth surface reconstruction pipeline for linear-array charge-coupled device (CCD) satellite imagery. While mainstream satellite image-based reconstruction approaches perform exceptionally well, the rational functional model (RFM) is subject to several limitations. For example, the RFM has no rigorous physical interpretation and differs significantly from the pinhole imaging model; hence, it cannot be directly applied to learning-based 3D reconstruction networks and to more novel reconstruction pipelines in computer vision. Hence, in this study, we introduce a method in which the RFM is equivalent to the pinhole camera model (PCM), meaning that the internal and external parameters of the pinhole camera are used instead of the rational polynomial coefficient parameters. We then derive an error formula for this equivalent pinhole model for the first time, demonstrating the influence of the image size on the accuracy of the reconstruction. In addition, we propose a polynomial image refinement model that minimizes equivalent errors via the least squares method. The experiments were conducted using four image datasets: WHU-TLC, DFC2019, ISPRS-ZY3, and GF7. The results demonstrated that the reconstruction accuracy was proportional to the image size. Our polynomial image refinement model significantly enhanced the accuracy and completeness of the reconstruction, and achieved more significant improvements for larger-scale images.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了一个大规模地表面重建管线，用于线性阵列具有CCD卫星成像。当前主流卫星成像基于重建方法具有出色表现，但是理циональ函数模型（RFM）受到一些限制。例如，RFM没有准确的物理解释，与穿孔成像模型（PCM）有很大差异，因此无法直接应用于学习基于三维重建网络和计算机视觉中的更新重建管线。因此，在本研究中，我们提出了一种方法，将RFM等价于PCM，即使用内部和外部相机参数而不是理циональ多项式系数参数。然后，我们 derive了这个等价穿孔模型的错误公式，表明图像大小对重建准确性的影响。此外，我们提出了一种多项式图像纠正模型，通过最小二乘方法来减少等价错误。实验使用了四个图像数据集：WHU-TLC、DFC2019、ISPRS-ZY3和GF7。结果表明，重建准确性与图像大小成正比。我们的多项式图像纠正模型可以提高重建准确性和完整性，并在更大规模的图像上实现更大的改进。
</details></li>
</ul>
<hr>
<h2 id="Medical-Image-Denosing-via-Explainable-AI-Feature-Preserving-Loss"><a href="#Medical-Image-Denosing-via-Explainable-AI-Feature-Preserving-Loss" class="headerlink" title="Medical Image Denosing via Explainable AI Feature Preserving Loss"></a>Medical Image Denosing via Explainable AI Feature Preserving Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20101">http://arxiv.org/abs/2310.20101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanfang Dong, Anup Basu</li>
<li>for: 这篇论文主要是为了提出一种新的医疗图像去噪方法，该方法不仅能有效地除去各种噪声，还能保留关键的医疗特征。</li>
<li>methods: 该方法使用了一种基于梯度的可解释人工智能（XAI）的损失函数设计，通过反传播，在去噪过程中保持医疗图像特征的一致性。</li>
<li>results: 经过广泛的实验 validate 了该方法在医疗图像去噪方面的优秀性，包括噪声和artifacts的13种不同类型的去噪性能、模型可解释性和泛化性。<details>
<summary>Abstract</summary>
Denoising algorithms play a crucial role in medical image processing and analysis. However, classical denoising algorithms often ignore explanatory and critical medical features preservation, which may lead to misdiagnosis and legal liabilities.In this work, we propose a new denoising method for medical images that not only efficiently removes various types of noise, but also preserves key medical features throughout the process. To achieve this goal, we utilize a gradient-based eXplainable Artificial Intelligence (XAI) approach to design a feature preserving loss function. Our feature preserving loss function is motivated by the characteristic that gradient-based XAI is sensitive to noise. Through backpropagation, medical image features before and after denoising can be kept consistent. We conducted extensive experiments on three available medical image datasets, including synthesized 13 different types of noise and artifacts. The experimental results demonstrate the superiority of our method in terms of denoising performance, model explainability, and generalization.
</details>
<details>
<summary>摘要</summary>
噪声除算法在医疗影像处理和分析中扮演着关键角色。然而，经典噪声除算法经常忽略医疗特有的重要特征，这可能导致误诊和法律责任。在这种情况下，我们提出了一种新的医疗影像噪声除法，不仅高效地除除各种噪声，而且保留了关键医疗特征。为实现这一目标，我们利用了梯度基于的解释可能性AI（XAI）方法设计了一个保持特征的损失函数。我们的特征保持损失函数受到梯度基于XAI的敏感性，通过反射，医疗影像特征之前和之后噪声除法的比较可以保持一致。我们在三个可用的医疗影像 dataset上进行了广泛的实验，包括13种不同类型的噪声和artefact。实验结果表明，我们的方法在噪声除法性能、模型解释性和泛化性方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="p-Poisson-surface-reconstruction-in-curl-free-flow-from-point-clouds"><a href="#p-Poisson-surface-reconstruction-in-curl-free-flow-from-point-clouds" class="headerlink" title="$p$-Poisson surface reconstruction in curl-free flow from point clouds"></a>$p$-Poisson surface reconstruction in curl-free flow from point clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20095">http://arxiv.org/abs/2310.20095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yesom Park, Taekyung Lee, Jooyoung Hahn, Myungjoo Kang</li>
<li>for: 本研究旨在从不规则点云样本中重建满足 геометрических形状的光滑表面，无需任何其他信息。</li>
<li>methods: 本文使用启发神经网络表示法（INR）进行面重建，但不需要基于真实的隐函数值或表面法向量的超参。 instead, 我们利用部分偏微分方程的正确监督和 diferencial vector fields的基本性质，以robustly重建高质量表面。</li>
<li>results: 我们在标准 benchmark 数据集上进行了实验，结果表明，我们提出的 INR 方法可以提供superior和Robust的重建。 code 可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/Yebbi/PINC%7D">https://github.com/Yebbi/PINC}</a> 上获取。<details>
<summary>Abstract</summary>
The aim of this paper is the reconstruction of a smooth surface from an unorganized point cloud sampled by a closed surface, with the preservation of geometric shapes, without any further information other than the point cloud. Implicit neural representations (INRs) have recently emerged as a promising approach to surface reconstruction. However, the reconstruction quality of existing methods relies on ground truth implicit function values or surface normal vectors. In this paper, we show that proper supervision of partial differential equations and fundamental properties of differential vector fields are sufficient to robustly reconstruct high-quality surfaces. We cast the $p$-Poisson equation to learn a signed distance function (SDF) and the reconstructed surface is implicitly represented by the zero-level set of the SDF. For efficient training, we develop a variable splitting structure by introducing a gradient of the SDF as an auxiliary variable and impose the $p$-Poisson equation directly on the auxiliary variable as a hard constraint. Based on the curl-free property of the gradient field, we impose a curl-free constraint on the auxiliary variable, which leads to a more faithful reconstruction. Experiments on standard benchmark datasets show that the proposed INR provides a superior and robust reconstruction. The code is available at \url{https://github.com/Yebbi/PINC}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Beyond-U-Making-Diffusion-Models-Faster-Lighter"><a href="#Beyond-U-Making-Diffusion-Models-Faster-Lighter" class="headerlink" title="Beyond U: Making Diffusion Models Faster &amp; Lighter"></a>Beyond U: Making Diffusion Models Faster &amp; Lighter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20092">http://arxiv.org/abs/2310.20092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergio Calvo-Ordonez, Jiahao Huang, Lipei Zhang, Guang Yang, Carola-Bibiane Schonlieb, Angelica I Aviles-Rivero</li>
<li>for: 这个论文是为了提高扩散模型的效率，特别是在逆噪处理过程中。</li>
<li>methods: 这篇论文使用了连续动力学系统来设计一种新的逆噪网络，以提高扩散模型的参数效率、速度更快、鲁棒性更高。</li>
<li>results:  experiments 表明，这种新的逆噪网络比标准 U-Net 在 Denoising Diffusion Probabilistic Models (DDPMs) 中具有更好的参数效率、更快的速度和更高的鲁棒性。<details>
<summary>Abstract</summary>
Diffusion models are a family of generative models that yield record-breaking performance in tasks such as image synthesis, video generation, and molecule design. Despite their capabilities, their efficiency, especially in the reverse denoising process, remains a challenge due to slow convergence rates and high computational costs. In this work, we introduce an approach that leverages continuous dynamical systems to design a novel denoising network for diffusion models that is more parameter-efficient, exhibits faster convergence, and demonstrates increased noise robustness. Experimenting with denoising probabilistic diffusion models, our framework operates with approximately a quarter of the parameters and 30% of the Floating Point Operations (FLOPs) compared to standard U-Nets in Denoising Diffusion Probabilistic Models (DDPMs). Furthermore, our model is up to 70% faster in inference than the baseline models when measured in equal conditions while converging to better quality solutions.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是一家 генератив模型，在图像生成、视频生成和分子设计等任务中表现出色。然而，它们在反噪处理过程中的效率仍然是一大挑战，主要是因为它们的整合速率较慢，计算成本较高。在这项工作中，我们介绍了一种使用连续动力系统来设计一种新的反噪网络，这种网络在 diffusion models 中更parameter-efficient， faster convergence，和更高的噪声Robustness。通过对噪损 probablistic diffusion models 进行实验，我们的框架可以在相同条件下与标准 U-Nets 相比，用 Parameters 和 FLOPs 的一半完成反噪任务。此外，我们的模型在等效条件下与基eline models 相比，在推理过程中更快，达到更高的质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.CV_2023_10_31/" data-id="cloimip9w00k2s488gjti1alm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/cs.SD_2023_10_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-10-31
        
      </div>
    </a>
  
  
    <a href="/2023/10/31/cs.AI_2023_10_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-10-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
