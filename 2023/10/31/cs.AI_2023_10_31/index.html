
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-10-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00192 repo_url: https:&#x2F;&#x2F;github.com&#x2F;sisl&#x2F;constructionbots.jl paper_authors: Kyle Brown, Dylan">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-10-31">
<meta property="og:url" content="https://nullscc.github.io/2023/10/31/cs.AI_2023_10_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00192 repo_url: https:&#x2F;&#x2F;github.com&#x2F;sisl&#x2F;constructionbots.jl paper_authors: Kyle Brown, Dylan">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-31T12:00:00.000Z">
<meta property="article:modified_time" content="2023-11-03T00:29:07.986Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.AI_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T12:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-10-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Large-Scale-Multi-Robot-Assembly-Planning-for-Autonomous-Manufacturing"><a href="#Large-Scale-Multi-Robot-Assembly-Planning-for-Autonomous-Manufacturing" class="headerlink" title="Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing"></a>Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00192">http://arxiv.org/abs/2311.00192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sisl/constructionbots.jl">https://github.com/sisl/constructionbots.jl</a></li>
<li>paper_authors: Kyle Brown, Dylan M. Asmar, Mac Schwager, Mykel J. Kochenderfer</li>
<li>For: This paper proposes an algorithmic stack for large-scale multi-robot assembly planning to address challenges such as collision-free movement, effective task allocation, and spatial planning for parallel assembly and transportation of nested subassemblies in manufacturing processes.* Methods: The proposed algorithmic stack includes an iterative radial layout optimization procedure, a graph-repair mixed-integer program formulation, a modified greedy task allocation algorithm, a geometric heuristic, and a hill-climbing algorithm to plan collaborative carrying configurations of robot sub-teams.* Results: The paper presents empirical results demonstrating the scalability and effectiveness of the proposed approach by generating plans to manufacture a LEGO model of a Saturn V launch vehicle with 1845 parts, 306 subassemblies, and 250 robots in under three minutes on a standard laptop computer.<details>
<summary>Abstract</summary>
Mobile autonomous robots have the potential to revolutionize manufacturing processes. However, employing large robot fleets in manufacturing requires addressing challenges including collision-free movement in a shared workspace, effective multi-robot collaboration to manipulate and transport large payloads, complex task allocation due to coupled manufacturing processes, and spatial planning for parallel assembly and transportation of nested subassemblies. We propose a full algorithmic stack for large-scale multi-robot assembly planning that addresses these challenges and can synthesize construction plans for complex assemblies with thousands of parts in a matter of minutes. Our approach takes in a CAD-like product specification and automatically plans a full-stack assembly procedure for a group of robots to manufacture the product. We propose an algorithmic stack that comprises: (i) an iterative radial layout optimization procedure to define a global staging layout for the manufacturing facility, (ii) a graph-repair mixed-integer program formulation and a modified greedy task allocation algorithm to optimally allocate robots and robot sub-teams to assembly and transport tasks, (iii) a geometric heuristic and a hill-climbing algorithm to plan collaborative carrying configurations of robot sub-teams, and (iv) a distributed control policy that enables robots to execute the assembly motion plan collision-free. We also present an open-source multi-robot manufacturing simulator implemented in Julia as a resource to the research community, to test our algorithms and to facilitate multi-robot manufacturing research more broadly. Our empirical results demonstrate the scalability and effectiveness of our approach by generating plans to manufacture a LEGO model of a Saturn V launch vehicle with 1845 parts, 306 subassemblies, and 250 robots in under three minutes on a standard laptop computer.
</details>
<details>
<summary>摘要</summary>
Mobile autonomous robots have the potential to revolutionize manufacturing processes. However, employing large robot fleets in manufacturing requires addressing challenges such as collision-free movement in a shared workspace, effective multi-robot collaboration to manipulate and transport large payloads, complex task allocation due to coupled manufacturing processes, and spatial planning for parallel assembly and transportation of nested subassemblies. We propose a full algorithmic stack for large-scale multi-robot assembly planning that addresses these challenges and can synthesize construction plans for complex assemblies with thousands of parts in a matter of minutes. Our approach takes in a CAD-like product specification and automatically plans a full-stack assembly procedure for a group of robots to manufacture the product. We propose an algorithmic stack that comprises:1. 迭代径向布局优化算法来定义制造设施的全球排序布局2. 图解 mixed-integer 编程和修改的排队策略来优化机器人和机器人子队伍的分配3. 几何规则和攀登算法来规划协作携带配置4. 分布式控制策略来使机器人执行 Assembly 动作计划无碰撞我们还提供了一个基于 Julia 的多机器人制造模拟器，作为研究社区的资源，以测试我们的算法和推动多机器人制造研究。我们的实验结果表明，我们的方法可以在标准笔记计算机上下载三分钟内生成一个 LEGO 模型的 Saturn V 发射 vehicle 的制造计划，包含1845件部件、306个子组件和250个机器人。
</details></li>
</ul>
<hr>
<h2 id="XAI-CLASS-Explanation-Enhanced-Text-Classification-with-Extremely-Weak-Supervision"><a href="#XAI-CLASS-Explanation-Enhanced-Text-Classification-with-Extremely-Weak-Supervision" class="headerlink" title="XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision"></a>XAI-CLASS: Explanation-Enhanced Text Classification with Extremely Weak Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00189">http://arxiv.org/abs/2311.00189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Hajialigol, Hanwen Liu, Xuan Wang<br>for:* 文章目的是提出一种新的、强度很弱的文本分类方法，以减少人工标注的需求。methods:* 该方法使用了弱相似性数据生成器，通过对文档与特定类别进行对应（如关键词匹配）进行 pseudo-标注。* 该方法还包括一个 auxiliary 任务，即用于预测单词重要性的词重要性预测任务。results:* 对于几个弱相似性文本分类数据集，XAI-CLASS 比其他弱相似性文本分类方法表现出色，得到了显著的性能提升。* 实验还表明，XAI-CLASS 可以提高模型的性能和可解释性。<details>
<summary>Abstract</summary>
Text classification aims to effectively categorize documents into pre-defined categories. Traditional methods for text classification often rely on large amounts of manually annotated training data, making the process time-consuming and labor-intensive. To address this issue, recent studies have focused on weakly-supervised and extremely weakly-supervised settings, which require minimal or no human annotation, respectively. In previous methods of weakly supervised text classification, pseudo-training data is generated by assigning pseudo-labels to documents based on their alignment (e.g., keyword matching) with specific classes. However, these methods ignore the importance of incorporating the explanations of the generated pseudo-labels, or saliency of individual words, as additional guidance during the text classification training process. To address this limitation, we propose XAI-CLASS, a novel explanation-enhanced extremely weakly-supervised text classification method that incorporates word saliency prediction as an auxiliary task. XAI-CLASS begins by employing a multi-round question-answering process to generate pseudo-training data that promotes the mutual enhancement of class labels and corresponding explanation word generation. This pseudo-training data is then used to train a multi-task framework that simultaneously learns both text classification and word saliency prediction. Extensive experiments on several weakly-supervised text classification datasets show that XAI-CLASS outperforms other weakly-supervised text classification methods significantly. Moreover, experiments demonstrate that XAI-CLASS enhances both model performance and explainability.
</details>
<details>
<summary>摘要</summary>
In previous weakly supervised text classification methods, pseudo-training data is generated by assigning pseudo-labels to documents based on their alignment (e.g., keyword matching) with specific classes. However, these methods ignore the importance of incorporating the explanations of the generated pseudo-labels, or saliency of individual words, as additional guidance during the text classification training process.To address this limitation, we propose XAI-CLASS, a novel explanation-enhanced extremely weakly-supervised text classification method that incorporates word saliency prediction as an auxiliary task. XAI-CLASS begins by employing a multi-round question-answering process to generate pseudo-training data that promotes the mutual enhancement of class labels and corresponding explanation word generation. This pseudo-training data is then used to train a multi-task framework that simultaneously learns both text classification and word saliency prediction.Extensive experiments on several weakly-supervised text classification datasets show that XAI-CLASS outperforms other weakly-supervised text classification methods significantly. Moreover, experiments demonstrate that XAI-CLASS enhances both model performance and explainability.
</details></li>
</ul>
<hr>
<h2 id="Robust-Safety-Classifier-for-Large-Language-Models-Adversarial-Prompt-Shield"><a href="#Robust-Safety-Classifier-for-Large-Language-Models-Adversarial-Prompt-Shield" class="headerlink" title="Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield"></a>Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00172">http://arxiv.org/abs/2311.00172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinhwa Kim, Ali Derakhshan, Ian G. Harris</li>
<li>for: 防止大语言模型受到攻击的安全性问题</li>
<li>methods: 提出了一种名为 Adversarial Prompt Shield (APS) 的轻量级模型，以及一种自动生成 adversarial 训练数据集（BAND）的策略</li>
<li>results: 通过对 Large Language Models 进行评估，显示了减少 adversarial 攻击成功率达 60% 的潜在提升，这对下一代更可靠和抗击的对话代理系统产生了前景。<details>
<summary>Abstract</summary>
Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.
</details>
<details>
<summary>摘要</summary>
大型语言模型的安全问题仍然是一个关键问题，因为它们容易受到反对攻击，这可能会让这些系统生成危险或不当的回应。在这些系统的核心里面有一个安全分类器，这是一个用于识别和mitigate potentially harmful, offensive, or unethical outputs的计算模型。然而，当前的安全分类器，尽管具有潜在的优势，frequently fail when exposed to inputs infused with adversarial noise。在回应于这个问题，我们的研究提出了反 adversarial prompt shield（APS），一个轻量级的模型，具有高度的检测精度和对反对攻击的抗性。此外，我们还提出了一些自动生成反对攻击数据集的新策略，称为Bot Adversarial Noisy Dialogue（BAND）数据集。这些数据集是用于强化安全分类器的Robustness，我们进行了对Large Language Models的评估，显示我们的分类器可以降低由反对攻击引起的攻击成功率 by up to 60%。这一进步开 up the way for the next generation of more reliable and resilient conversational agents。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Denouncing-Hate-Strategies-for-Countering-Implied-Biases-and-Stereotypes-in-Language"><a href="#Beyond-Denouncing-Hate-Strategies-for-Countering-Implied-Biases-and-Stereotypes-in-Language" class="headerlink" title="Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language"></a>Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00161">http://arxiv.org/abs/2311.00161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, Sarah-Jane Leslie, Maarten Sap</li>
<li>for: This paper aims to address the issue of online hate speech without censorship by exploring psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language.</li>
<li>methods: The authors draw from psychology and philosophy literature to craft six strategies to challenge hateful language, and they examine the convincingness of each strategy through a user study and compare their usages in human- and machine-generated counterspeech datasets.</li>
<li>results: The study finds that human-written counterspeech uses more specific strategies to challenge the implied stereotype, whereas machine-generated counterspeech uses less specific strategies and often employs strategies that humans deem less convincing. The findings highlight the importance of accounting for the underlying stereotypical implications of speech when generating counterspeech and for better machine reasoning about anti-stereotypical examples.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是解决在互联网上发布仇恨言语的问题，不包括审查。</li>
<li>methods: 作者们 drew from psychology和哲学文献，摘取了六种挑战仇恨语言的策略，并通过用户研究和人工生成的反驳数据进行比较。</li>
<li>results: 研究发现，人工生成的反驳数据使用的是更加特定的挑战策略（例如，对恶意刻板印象的反例和外部因素），而机器生成的反驳数据则使用更加通用的策略（例如，普遍抨击仇恨言语）。此外，机器生成的反驳数据经常使用人们认为更不令人信服的策略。研究表明，在生成反驳数据时需要考虑语言下的恶意刻板印象，并且机器需要更好地理解反例。<details>
<summary>Abstract</summary>
Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncing the hatefulness of speech). Furthermore, machine-generated counterspeech often employs strategies that humans deem less convincing compared to human-produced counterspeech. Our findings point to the importance of accounting for the underlying stereotypical implications of speech when generating counterspeech and for better machine reasoning about anti-stereotypical examples.
</details>
<details>
<summary>摘要</summary>
优先抑制言论中的恶意言论，即通过反制可能带来的危害，已成为解决在线谩骂问题的增 Popular solution。然而，有效地抵消负面语言需要抵消和炸弹下面的不准确刻板印象。在这项工作中，我们从心理学和哲学文献中练习出六种心理启发的抗议策略，以挑战下面的刻板印象。我们首先通过用户研究检验每种策略的有效性，然后在人类生成的反对言论和机器生成的反对言论数据集中比较其使用情况。我们的结果表明，人类生成的反对言论更加特定地抵消刻板印象（例如，对刻板印象的反例和外部因素），而机器生成的反对言论则更多地使用不那么有力的策略（例如，普遍否决谩骂语言的危害）。此外，机器生成的反对言论经常使用人类认为不那么有力的策略。我们的发现表明，当生成反对言论时，需要考虑下面的刻板印象，并更好地机器理解反对刻板印象的示例。
</details></li>
</ul>
<hr>
<h2 id="Score-Normalization-for-a-Faster-Diffusion-Exponential-Integrator-Sampler"><a href="#Score-Normalization-for-a-Faster-Diffusion-Exponential-Integrator-Sampler" class="headerlink" title="Score Normalization for a Faster Diffusion Exponential Integrator Sampler"></a>Score Normalization for a Faster Diffusion Exponential Integrator Sampler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00157">http://arxiv.org/abs/2311.00157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtkresearch/Diffusion-DEIS-SN">https://github.com/mtkresearch/Diffusion-DEIS-SN</a></li>
<li>paper_authors: Guoxuan Xia, Duolikun Danier, Ayan Das, Stathi Fotiadis, Farhang Nabiei, Ushnish Sengupta, Alberto Bernacchia</li>
<li>for: 这 paper 是为了快速生成Diffusion Models中的样本而提出的方法，以提高生成质量和减少积分错误。</li>
<li>methods: 这 paper 使用了Diffusion Exponential Integrator Sampler（DEIS），具体来说是Score Function Reparameterisation（SFP）技术，以提高生成质量和减少积分错误。</li>
<li>results: 该 paper 的实验结果表明，使用Score Normalisation（DEIS-SN）技术可以 Consistently improve FID compared to vanilla DEIS，在10 NFEs中提高了FID值从6.44到5.57。<details>
<summary>Abstract</summary>
Recently, zhang et al have proposed the Diffusion Exponential Integrator Sampler (DEIS) for fast generation of samples from Diffusion Models. It leverages the semi-linear nature of the probability flow ordinary differential equation (ODE) in order to greatly reduce integration error and improve generation quality at low numbers of function evaluations (NFEs). Key to this approach is the score function reparameterisation, which reduces the integration error incurred from using a fixed score function estimate over each integration step. The original authors use the default parameterisation used by models trained for noise prediction -- multiply the score by the standard deviation of the conditional forward noising distribution. We find that although the mean absolute value of this score parameterisation is close to constant for a large portion of the reverse sampling process, it changes rapidly at the end of sampling. As a simple fix, we propose to instead reparameterise the score (at inference) by dividing it by the average absolute value of previous score estimates at that time step collected from offline high NFE generations. We find that our score normalisation (DEIS-SN) consistently improves FID compared to vanilla DEIS, showing an FID improvement from 6.44 to 5.57 at 10 NFEs for our CIFAR-10 experiments. Our code is available at https://github.com/mtkresearch/Diffusion-DEIS-SN.
</details>
<details>
<summary>摘要</summary>
最近，张等人提出了快速生成Diffusion模型样本的Diffusion扩展Integrator抽取器（DEIS）。它利用Diffusion模型的半线性性来大幅减少积分误差，提高生成质量，只需要少量的函数评估（NFEs）。关键在于分数函数重parameter化，减少在每个积分步骤中使用固定分数函数估计的积分误差。原始作者使用模型用于随机噪声预测的默认参数化，将分数函数乘以 conditional forward 噪声分布的标准差。我们发现，在大部分逆抽取过程中，这个分数参数化的平均绝对值很接近常数，但在抽取过程的末尾快速变化。为了简化，我们提议在推理时对分数进行正规化（DEIS-SN），将其除以在线上高NFEs生成的上一个时间步骤的平均绝对分数估计的平均值。我们发现，我们的分数正规化（DEIS-SN）在10NFEs下的CIFAR-10实验中 consistently improve FID，从6.44降低到5.57。我们的代码可以在 GitHub上找到：https://github.com/mtkresearch/Diffusion-DEIS-SN。
</details></li>
</ul>
<hr>
<h2 id="RIR-SF-Room-Impulse-Response-Based-Spatial-Feature-for-Multi-channel-Multi-talker-ASR"><a href="#RIR-SF-Room-Impulse-Response-Based-Spatial-Feature-for-Multi-channel-Multi-talker-ASR" class="headerlink" title="RIR-SF: Room Impulse Response Based Spatial Feature for Multi-channel Multi-talker ASR"></a>RIR-SF: Room Impulse Response Based Spatial Feature for Multi-channel Multi-talker ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00146">http://arxiv.org/abs/2311.00146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Shao, Shi-Xiong Zhang, Dong Yu</li>
<li>for: 提高多通道多说者自动语音识别（ASR）系统的精度</li>
<li>methods: 使用 overlap speech signals 与目标说话人的传输到麦克风数组的room impulse response（RIR）进行卷积，从而获得一种新的空间特征——RIR-SF</li>
<li>results: 比较与现有的3D空间特征，经过理论分析和实验验证，新的RIR-SF在多通道多说者ASR系统中具有remarkable 21.3%的相对减少 Character Error Rate（CER），并且在强 reverberation 情况下表现更加稳定和Robust。<details>
<summary>Abstract</summary>
Multi-channel multi-talker automatic speech recognition (ASR) presents ongoing challenges within the speech community, particularly when confronted with significant reverberation effects. In this study, we introduce a novel approach involving the convolution of overlapping speech signals with the room impulse response (RIR) corresponding to the target speaker's transmission to a microphone array. This innovative technique yields a novel spatial feature known as the RIR-SF. Through a comprehensive comparison with the previously established state-of-the-art 3D spatial feature, both theoretical analysis and experimental results substantiate the superiority of our proposed RIR-SF. We demonstrate that the RIR-SF outperforms existing methods, leading to a remarkable 21.3\% relative reduction in the Character Error Rate (CER) in multi-channel multi-talker ASR systems. Importantly, this novel feature exhibits robustness in the face of strong reverberation, surpassing the limitations of previous approaches.
</details>
<details>
<summary>摘要</summary>
多通道多个人自动语音识别（ASR）系统中存在持续的挑战，特别是在面临重要的干扰效应时。在本研究中，我们介绍了一种新的方法，即将重叠的语音信号 convolution 与目标说话人的传输到麦克风数组的房间冲击响应（RIR）。这种新的特征被称为 RIR-SF。经过了对已有状态的评估和实验结果，我们的提议的 RIR-SF 超越了现有的方法，导致了 Character Error Rate（CER）在多通道多个人 ASR 系统中具有remarkable 21.3% 的相对减少。重要的是，这种新的特征在强干扰情况下表现了 Robustness，超越了前一代方法的限制。
</details></li>
</ul>
<hr>
<h2 id="Two-Stage-Classifier-for-Campaign-Negativity-Detection-using-Axis-Embeddings-A-Case-Study-on-Tweets-of-Political-Users-during-2021-Presidential-Election-in-Iran"><a href="#Two-Stage-Classifier-for-Campaign-Negativity-Detection-using-Axis-Embeddings-A-Case-Study-on-Tweets-of-Political-Users-during-2021-Presidential-Election-in-Iran" class="headerlink" title="Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran"></a>Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00143">http://arxiv.org/abs/2311.00143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Rajabi, Ali Mohades</li>
<li>for: 本研究旨在 automatization 政治竞选中的负面语言检测，以便更好地理解候选人和政党在竞选中的策略。</li>
<li>methods: 本研究使用了一种 hybrid 模型，结合了两种机器学习模型，以检测政治 tweet 的负面语言。</li>
<li>results: 研究发现，候选人发布的 tweet 与其负面性无关，而政治人物和组织名称在 tweet 中的存在直接关系到 tweet 的负面性。In English, that would be:</li>
<li>for: The purpose of this study is to automate the detection of negative language in political campaigns, in order to better understand the strategies of candidates and parties.</li>
<li>methods: The study uses a hybrid model that combines two machine learning models to detect negative language in political tweets.</li>
<li>results: The study finds that the publication of a tweet by a candidate is not related to its negativity, but the presence of political persons and organizations in the tweet is directly related to its negativity.<details>
<summary>Abstract</summary>
In elections around the world, the candidates may turn their campaigns toward negativity due to the prospect of failure and time pressure. In the digital age, social media platforms such as Twitter are rich sources of political discourse. Therefore, despite the large amount of data that is published on Twitter, the automatic system for campaign negativity detection can play an essential role in understanding the strategy of candidates and parties in their campaigns. In this paper, we propose a hybrid model for detecting campaign negativity consisting of a two-stage classifier that combines the strengths of two machine learning models. Here, we have collected Persian tweets from 50 political users, including candidates and government officials. Then we annotated 5,100 of them that were published during the year before the 2021 presidential election in Iran. In the proposed model, first, the required datasets of two classifiers based on the cosine similarity of tweet embeddings with axis embeddings (which are the average of embedding in positive and negative classes of tweets) from the training set (85\%) are made, and then these datasets are considered the training set of the two classifiers in the hybrid model. Finally, our best model (RF-RF) was able to achieve 79\% for the macro F1 score and 82\% for the weighted F1 score. By running the best model on the rest of the tweets of 50 political users that were published one year before the election and with the help of statistical models, we find that the publication of a tweet by a candidate has nothing to do with the negativity of that tweet, and the presence of the names of political persons and political organizations in the tweet is directly related to its negativity.
</details>
<details>
<summary>摘要</summary>
在世界各地的选举中，候选人可能因为失败的风险和时间压力而转向负面竞选。在数字时代，社交媒体平台such as Twitter是政治讨论的丰富源泉。因此，尽管 Twitter 上发布了大量数据，但自动化竞选负面检测系统仍然可以在理解候选人和政党的竞选策略中扮演重要角色。在这篇论文中，我们提议一种混合模型来检测竞选负面，包括两个机器学习模型的两个阶段分类器。我们收集了50名政治用户的波斯语微博，包括候选人和政府官员。然后，我们对这些微博进行了5100个标注，这些微博在2021年伊朗总统选举前一年发布。在我们提议的模型中，首先制定了基于微博嵌入的cosine相似性的两个分类器的数据集（85%），然后这些数据集被用作两个分类器的混合模型的训练集。最后，我们的best模型（RF-RF）可以达到79%的macro F1分数和82%的Weighted F1分数。通过将best模型应用于其余50名政治用户发布的一年前的微博，我们发现，候选人发布的微博与负面微博之间没有直接关系，而政治人物和政治组织的名称直接关系到微博的负面性。
</details></li>
</ul>
<hr>
<h2 id="Q-Learning-for-Stochastic-Control-under-General-Information-Structures-and-Non-Markovian-Environments"><a href="#Q-Learning-for-Stochastic-Control-under-General-Information-Structures-and-Non-Markovian-Environments" class="headerlink" title="Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments"></a>Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00123">http://arxiv.org/abs/2311.00123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Devran Kara, Serdar Yuksel</li>
<li>for: 本研究的主要贡献是提供一个泛化定理，用于描述在某些随机环境下，Q-学习迭代的收敛性。</li>
<li>methods: 本研究使用了一种通用的可能非Markovian的随机环境下的泛化定理，并提供了一个准确地描述迭代的结果和收敛性的条件。</li>
<li>results: 本研究的结果包括：(1) 提供了一个新的收敛定理，用于描述Q-学习迭代在某些随机环境下的收敛性；(2) 对某些随机控制问题的解释和应用，包括部分可观察Markov决策过程（MDP）的量化approximation、部分可观察POMDP的量化approximation、finite windowapproximation和多代模型的收敛性研究等。<details>
<summary>Abstract</summary>
As a primary contribution, we present a convergence theorem for stochastic iterations, and in particular, Q-learning iterates, under a general, possibly non-Markovian, stochastic environment. Our conditions for convergence involve an ergodicity and a positivity criterion. We provide a precise characterization on the limit of the iterates and conditions on the environment and initializations for convergence. As our second contribution, we discuss the implications and applications of this theorem to a variety of stochastic control problems with non-Markovian environments involving (i) quantized approximations of fully observed Markov Decision Processes (MDPs) with continuous spaces (where quantization break down the Markovian structure), (ii) quantized approximations of belief-MDP reduced partially observable MDPS (POMDPs) with weak Feller continuity and a mild version of filter stability (which requires the knowledge of the model by the controller), (iii) finite window approximations of POMDPs under a uniform controlled filter stability (which does not require the knowledge of the model), and (iv) for multi-agent models where convergence of learning dynamics to a new class of equilibria, subjective Q-learning equilibria, will be studied. In addition to the convergence theorem, some implications of the theorem above are new to the literature and others are interpreted as applications of the convergence theorem. Some open problems are noted.
</details>
<details>
<summary>摘要</summary>
Primary Contribution:我们主要贡献是提出了涉及泛化环境的随机迭代收敛定理，特别是Q学迭代的收敛定理。我们的收敛条件包括一个ergodicity和一个正semi定理。我们提供了迭代器的准确特征和环境和初始化条件下的收敛条件。Second Contribution:我们还讨论了这个定理在各种随机控制问题中的应用和意义，包括：（i）在完全观测的Markov决策过程（MDPs）中使用量化方法时的破坏性问题。（ii）在受限 partially observable Markov decision process（POMDPs）中使用弱Feller连续性和满足策略稳定性（需要控制器知道模型）。（iii）在POMDPs中使用 finite window approximation，不需要控制器知道模型。（iv）多个代理模型中的学习动力收敛到一种新的平衡点，称为主观Q学平衡点。除了收敛定理之外，我们还提出了一些新的意义和应用，以及一些未解决的问题。
</details></li>
</ul>
<hr>
<h2 id="Bandit-Driven-Batch-Selection-for-Robust-Learning-under-Label-Noise"><a href="#Bandit-Driven-Batch-Selection-for-Robust-Learning-under-Label-Noise" class="headerlink" title="Bandit-Driven Batch Selection for Robust Learning under Label Noise"></a>Bandit-Driven Batch Selection for Robust Learning under Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00096">http://arxiv.org/abs/2311.00096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Lisicki, Mihai Nica, Graham W. Taylor</li>
<li>for: 提高 Stochastic Gradient Descent (SGD) 训练中批处理的效率，利用 combinatorial bandit 算法。</li>
<li>methods: 利用 combinatorial bandit 算法优化学习过程中的标签噪声问题。</li>
<li>results: 在 CIFAR-10  dataset 上，我们的方法在不同水平的标签损害下 consistently 表现出色，超过现有方法。同时，我们不需要额外的计算开销，可以在复杂的机器学习应用中扩展。<details>
<summary>Abstract</summary>
We introduce a novel approach for batch selection in Stochastic Gradient Descent (SGD) training, leveraging combinatorial bandit algorithms. Our methodology focuses on optimizing the learning process in the presence of label noise, a prevalent issue in real-world datasets. Experimental evaluations on the CIFAR-10 dataset reveal that our approach consistently outperforms existing methods across various levels of label corruption. Importantly, we achieve this superior performance without incurring the computational overhead commonly associated with auxiliary neural network models. This work presents a balanced trade-off between computational efficiency and model efficacy, offering a scalable solution for complex machine learning applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的批处理选择方法，基于 combinatorial bandit 算法，用于 Stochastic Gradient Descent（SGD）训练。我们的方法重点是在实际数据集中存在标签噪声的情况下优化学习过程。我们在 CIFAR-10 数据集上进行了实验，结果显示，我们的方法在不同水平的标签损害情况下一直表现优于现有方法，而且不需要付出较高的计算开销。这种方法实现了计算效率和模型效果之间的平衡，为复杂的机器学习应用提供了扩展性的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Expressive-Modeling-Is-Insufficient-for-Offline-RL-A-Tractable-Inference-Perspective"><a href="#Expressive-Modeling-Is-Insufficient-for-Offline-RL-A-Tractable-Inference-Perspective" class="headerlink" title="Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective"></a>Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00094">http://arxiv.org/abs/2311.00094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuejie Liu, Anji Liu, Guy Van den Broeck, Yitao Liang</li>
<li>for: This paper is written for offline Reinforcement Learning (RL) tasks, where the goal is to learn a policy from a set of pre-collected trajectories.</li>
<li>methods: The paper proposes a new approach called Trifle, which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap between good sequence models and high expected returns at evaluation time.</li>
<li>results: The paper achieves the most state-of-the-art scores in 9 Gym-MuJoCo benchmarks against strong baselines, and significantly outperforms prior approaches in stochastic environments and safe RL tasks with minimum algorithmic modifications.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为offline Reinforcement Learning（RL）任务而写的，目标是从 pré-收集的轨迹集中学习策略。</li>
<li>methods: 这篇论文提出了一种新的方法 called Trifle，利用现代可追踪概率模型（TPMs）来在评估时 bridging 好序列模型和高预期返回之间的差距。</li>
<li>results: 这篇论文在9个 Gym-MuJoCo benchmark上 achieve 最佳状态的得分，并在随机环境和安全RL任务（例如动作约束）中明显超越先前的方法，只需最小的算法修改。<details>
<summary>Abstract</summary>
A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap between good sequence models and high expected returns at evaluation time. Empirically, Trifle achieves the most state-of-the-art scores in 9 Gym-MuJoCo benchmarks against strong baselines. Further, owing to its tractability, Trifle significantly outperforms prior approaches in stochastic environments and safe RL tasks (e.g. with action constraints) with minimum algorithmic modifications.
</details>
<details>
<summary>摘要</summary>
一种受欢迎的探索学习（RL）任务的策略是先将偏离线轨迹适应到一个序列模型，然后使用模型来获取高预期返回的动作。虽然广泛认为更表达力强的序列模型会导致更好的表现，但这篇论文指出，可行性（可以快速和准确回答多种概率查询）也扮演着重要的角色。具体来说，由于在线上数据收集策略和环境动力学中的基本随机性，需要进行高度非线性的生成，以便获得奖励动作。虽然可以 aproximate这些查询，但我们发现这些粗略估计会很大程度下降表现。为解决这个问题，这篇论文提出了Trifle（可 tractable 探索学习），它利用现代可追踪概率模型（TPMs）来在评估时bridginggood sequence models和高预期返回之间的 gap。Empirically，Trifle在9个 Gym-MuJoCo benchmark中 achievestate-of-the-art 得分，并在随机环境和安全RL任务（例如Action constraints）中表现出优于先前的方法。此外，由于Trifle的可追踪性，它在可追踪环境和安全RL任务中具有更高的表现。
</details></li>
</ul>
<hr>
<h2 id="Safe-multi-agent-motion-planning-under-uncertainty-for-drones-using-filtered-reinforcement-learning"><a href="#Safe-multi-agent-motion-planning-under-uncertainty-for-drones-using-filtered-reinforcement-learning" class="headerlink" title="Safe multi-agent motion planning under uncertainty for drones using filtered reinforcement learning"></a>Safe multi-agent motion planning under uncertainty for drones using filtered reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00063">http://arxiv.org/abs/2311.00063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sleiman Safaoui, Abraham P. Vinod, Ankush Chakrabarty, Rien Quirynen, Nobuyuki Yoshikawa, Stefano Di Cairano</li>
<li>for: 本研究是针对多机器人在不确定、受障碍的工作空间中安全运动规划的问题。</li>
<li>methods: 本研究使用了单机器人学习来学习运动计划，并使用了受限控制基于轨迹规划的方法来保证安全性。</li>
<li>results: 提出的方法可以实时实现多机器人的安全运动规划，并且可以在不确定的工作空间、机器人运动和感知中提供高度的安全性。<details>
<summary>Abstract</summary>
We consider the problem of safe multi-agent motion planning for drones in uncertain, cluttered workspaces. For this problem, we present a tractable motion planner that builds upon the strengths of reinforcement learning and constrained-control-based trajectory planning. First, we use single-agent reinforcement learning to learn motion plans from data that reach the target but may not be collision-free. Next, we use a convex optimization, chance constraints, and set-based methods for constrained control to ensure safety, despite the uncertainty in the workspace, agent motion, and sensing. The proposed approach can handle state and control constraints on the agents, and enforce collision avoidance among themselves and with static obstacles in the workspace with high probability. The proposed approach yields a safe, real-time implementable, multi-agent motion planner that is simpler to train than methods based solely on learning. Numerical simulations and experiments show the efficacy of the approach.
</details>
<details>
<summary>摘要</summary>
我团队考虑了多机器人在不确定、拥堵的工作空间中安全多机器人运动规划问题。我们提出了一种可控的运动规划方法，基于再增强学习和受限控制的轨迹规划。首先，我们使用单机器人再增强学习学习运动计划，从数据中学习到达目标点，但可能不是免涉碰撞的。然后，我们使用几何优化、机会约束和集合方法来保证安全性，即使在工作空间、机器人运动和感知中存在不确定性。我们的方法可以处理机器人状态和控制约束，并且在高概率下避免机器人之间和静止障碍物的碰撞。我们的方法比基于学习 alone 更加容易训练，并且实时可行。数值仿真和实验表明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="The-Generative-AI-Paradox-“What-It-Can-Create-It-May-Not-Understand”"><a href="#The-Generative-AI-Paradox-“What-It-Can-Create-It-May-Not-Understand”" class="headerlink" title="The Generative AI Paradox: “What It Can Create, It May Not Understand”"></a>The Generative AI Paradox: “What It Can Create, It May Not Understand”</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00059">http://arxiv.org/abs/2311.00059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D. Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, Benjamin Newman, Pang Wei Koh, Allyson Ettinger, Yejin Choi</li>
<li>for: 这篇论文探讨了现代生成AI模型的 aparadox：它们可以在秒钟内生成出人类专家水平的输出，但却仍然存在基本的理解错误，这与人类的情况不同。</li>
<li>methods: 作者们使用了语言和图像模式的控制实验，测试了生成vs理解的关系，以支持他们的生成AI парадок斯假设。</li>
<li>results: 研究发现，虽然模型可以超越人类的生成能力，但它们在理解能力、理解和生成能力之间的相关性、以及对恶作剂输入的脆弱性等方面都弱于人类。这支持假设，即模型的生成能力可能不是基于理解能力的。<details>
<summary>Abstract</summary>
The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.
</details>
<details>
<summary>摘要</summary>
最近的生成型人工智能（AI）浪潮引发了历史上无 precedent 的全球关注，同时也引发了对可能性超human的人工智能水平的激动和担忧：模型现在只需几秒钟就可以生成出挑战或超越人类专家水平的输出。然而，模型仍然表现出基本的理解错误，这不是非专家人类会出现的。这给我们提出了一个 aparadox：如何才能够 conciliate seemingly superhuman capabilities 与 persistently basic errors ？在这篇文章中，我们提出了生成AI парадок斯假设：生成模型，经过直接受训练来重现专家水平的输出，获得了不依赖于——可以超越——它们的理解能力的生成能力。与人类不同，人类的基本理解通常会先于生成专家水平的输出。我们通过控制的实验分析生成vs理解在生成模型中，在语言和图像模式下进行测试。我们的结果表明，虽然模型可以超过人类的生成能力，但它们一直 fall short of human capabilities 在理解方面，以及生成和理解性能之间的较弱相关性，以及更容易受到骚扰输入的 brittleness。我们的发现支持生成AI парадок斯假设，并警告我们不要将人工智能与人类智能相提并论。
</details></li>
</ul>
<hr>
<h2 id="Diversity-and-Diffusion-Observations-on-Synthetic-Image-Distributions-with-Stable-Diffusion"><a href="#Diversity-and-Diffusion-Observations-on-Synthetic-Image-Distributions-with-Stable-Diffusion" class="headerlink" title="Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion"></a>Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00056">http://arxiv.org/abs/2311.00056</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Marwood, Shumeet Baluja, Yair Alon</li>
<li>for: 本研究旨在探讨现有文本到图像（TTI）系统，如StableDiffusion、Imagen和DALL-E 2，是否可以用于减少人工取样的图像敏感度训练新的机器学习分类器。</li>
<li>methods: 本研究使用现有的TTI系统生成图像，并对这些图像进行Semantic Embeddings（SE）和Contrastive Language-Image Pre-training（CLIP）等方法进行分析。</li>
<li>results: 研究发现，即使使用真实的图像进行训练，仍然存在 Semantic Mismatches（SM）问题，导致分类器在推断时表现不佳。此外，研究还发现了四种限制TTI系统的使用：歧义、遵循提示、缺乏多样性和表示下面的基本概念。此外，研究还发现了CLIP embeddings的几何结构。<details>
<summary>Abstract</summary>
Recent progress in text-to-image (TTI) systems, such as StableDiffusion, Imagen, and DALL-E 2, have made it possible to create realistic images with simple text prompts. It is tempting to use these systems to eliminate the manual task of obtaining natural images for training a new machine learning classifier. However, in all of the experiments performed to date, classifiers trained solely with synthetic images perform poorly at inference, despite the images used for training appearing realistic. Examining this apparent incongruity in detail gives insight into the limitations of the underlying image generation processes. Through the lens of diversity in image creation vs.accuracy of what is created, we dissect the differences in semantic mismatches in what is modeled in synthetic vs. natural images. This will elucidate the roles of the image-languag emodel, CLIP, and the image generation model, diffusion. We find four issues that limit the usefulness of TTI systems for this task: ambiguity, adherence to prompt, lack of diversity, and inability to represent the underlying concept. We further present surprising insights into the geometry of CLIP embeddings.
</details>
<details>
<summary>摘要</summary>
Examining this apparent incongruity in detail reveals the limitations of the underlying image generation processes. By comparing the diversity of images created by TTI systems with the accuracy of those images, we can identify the sources of the mismatch. We find that there are four issues that limit the usefulness of TTI systems for this task: ambiguity, adherence to prompt, lack of diversity, and inability to represent the underlying concept.Furthermore, we present surprising insights into the geometry of CLIP embeddings, which provide a new perspective on the limitations of TTI systems. Our findings have important implications for the use of TTI systems in machine learning and highlight the need for further research to overcome these limitations.
</details></li>
</ul>
<hr>
<h2 id="SC-MIL-Sparsely-Coded-Multiple-Instance-Learning-for-Whole-Slide-Image-Classification"><a href="#SC-MIL-Sparsely-Coded-Multiple-Instance-Learning-for-Whole-Slide-Image-Classification" class="headerlink" title="SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification"></a>SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00048">http://arxiv.org/abs/2311.00048</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sotiraslab/SCMIL">https://github.com/sotiraslab/SCMIL</a></li>
<li>paper_authors: Peijie Qiu, Pan Xiao, Wenhui Zhu, Yalin Wang, Aristeidis Sotiras</li>
<li>for: This paper focuses on improving the performance of weakly supervised whole slide image (WSI) classification using Multiple Instance Learning (MIL).</li>
<li>methods: The proposed method, called sparsely coded MIL (SC-MIL), uses sparse dictionary learning to capture the similarities of instances and improve the feature embeddings. The method also incorporates deep unrolling to make it compatible with deep learning.</li>
<li>results: The proposed SC module was shown to substantially boost the performance of state-of-the-art MIL methods in experiments on multiple datasets, with an acceptable computation cost. The codes are available at \href{<a target="_blank" rel="noopener" href="https://github.com/sotiraslab/SCMIL.git%7D%7Bhttps://github.com/sotiraslab/SCMIL.git%7D">https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}</a>.<details>
<summary>Abstract</summary>
Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the conventional sparse coding algorithm compatible with deep learning, we unrolled it into an SC module by leveraging deep unrolling. The proposed SC module can be incorporated into any existing MIL framework in a plug-and-play manner with an acceptable computation cost. The experimental results on multiple datasets demonstrated that the proposed SC module could substantially boost the performance of state-of-the-art MIL methods. The codes are available at \href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.
</details>
<details>
<summary>摘要</summary>
多个实例学习（MIL）在弱监督整幕图像（WSI）分类中广泛应用。典型的MIL方法包括一个特征嵌入部分，该部分使用预训练的特征提取器将实例嵌入特征，以及MIL聚合器，该聚合器将实例嵌入特征组合成预测。目前的焦点是在提高这两个部分的性能，通过自我监督预训练提高特征嵌入，并分别模型实例之间的相互关系。在这篇论文中，我们提出了一种稀盐编码MIL（SC-MIL），该方法同时解决了这两个问题。稀盐编码学习捕捉实例之间的相互关系，表示实例为稀盐线性组合的原子集中的稀盐线性组合。此外，在强制稀盐的情况下，可以增强实例特征嵌入，抑制不相关的实例，保留最相关的实例。为使用深度学习，我们将稀盐编码算法拓展到SC模块，并通过深度拓展来实现。这种SC模块可以与现有的MIL框架兼容，并且可以在插件式的方式进行搭配，计算成本可以接受。实验结果表明，提案的SC模块可以明显提高现有MIL方法的性能。代码可以在\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}中找到。
</details></li>
</ul>
<hr>
<h2 id="Grounding-Visual-Illusions-in-Language-Do-Vision-Language-Models-Perceive-Illusions-Like-Humans"><a href="#Grounding-Visual-Illusions-in-Language-Do-Vision-Language-Models-Perceive-Illusions-Like-Humans" class="headerlink" title="Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?"></a>Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00047">http://arxiv.org/abs/2311.00047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vl-illusion/dataset">https://github.com/vl-illusion/dataset</a></li>
<li>paper_authors: Yichi Zhang, Jiayi Pan, Yuchen Zhou, Rui Pan, Joyce Chai</li>
<li>for: 研究 whether 视觉语言模型 (VLMs) 有类似于人类的视觉假设，或者它们会忠实地表示现实。</li>
<li>methods: 构建了五种视觉假设的数据集，并设计了四个任务来检验现有的 VLMs 中的视觉假设。</li>
<li>results: 发现虽然总体协调低，大型模型更接近人类的视觉和更易受到视觉假设的影响。 这些发现将促进我们对人类和机器之间视觉世界的共同理解和交流的更好的理解，并为未来的计算模型提供了一个进一步的探索。<details>
<summary>Abstract</summary>
Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at https://github.com/vl-illusion/dataset.
</details>
<details>
<summary>摘要</summary>
视力语言模型（VLM）在庞大数据量上训练，但人类视觉不一定准确反映物理世界。这引发了关键问题：VLM是否具有人类视觉中的同类型错觉，或者它们忠实地表现实际情况？为了解答这个问题，我们构建了包含五种视觉错觉的数据集，并提出四项任务来检查视觉错觉在当今最先进的VLM中。我们的发现表明，虽然总体对齐率低，大型模型更加接近人类视觉和更易受到视觉错觉的影响。我们的数据集和初步发现将促进人类和机器之间的视觉理解和沟通，并提供未来计算模型更好地与人类对视觉世界的共同感知的开始。数据集和代码可以在<https://github.com/vl-illusion/dataset>获取。
</details></li>
</ul>
<hr>
<h2 id="Limited-Data-Unlimited-Potential-A-Study-on-ViTs-Augmented-by-Masked-Autoencoders"><a href="#Limited-Data-Unlimited-Potential-A-Study-on-ViTs-Augmented-by-Masked-Autoencoders" class="headerlink" title="Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders"></a>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20704">http://arxiv.org/abs/2310.20704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srijan Das, Tanmay Jain, Dominick Reilly, Pranav Balaji, Soumyajit Karmakar, Shyam Marjit, Xiang Li, Abhijit Das, Michael Ryoo</li>
<li>for: 这个研究旨在探讨如何将限量数据下的视觉变数转换器（ViT）训练成功。</li>
<li>methods: 这个研究使用了自我监督学习（SSL）和继续调整的方法来训练ViT。</li>
<li>results: 研究发现，在限量数据下，同时优化ViT для主要任务和一个自我监督任务（SSAT）是非常有利的。这种方法可以让ViT获得更好的性能，而且可以降低培训时间和碳踪。实验结果显示，SSAT在10个数据集上具有优秀的数据准确性和一致性。此外，SSAT也在视频领域中实现了深伪检测的好效果，证明了其通用性。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViTs pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint. We also confirm the effectiveness of SSAT in the video domain for deepfake detection, showcasing its generalizability. Our code is available at https://github.com/dominickrei/Limited-data-vits.
</details>
<details>
<summary>摘要</summary>
传统的计算机视觉 Task (ViT) 在计算机视觉领域中广泛应用。尽管它们有成功，但它们缺乏逻辑假设，这可能使其在有限数据量下训练变得困难。以前的研究表明，通过自我超vision学习 (SSL) 和顺序精度调整来解决这个挑战。然而，我们发现，在有限数据量下，同时优化 ViT  для主要任务和 Self-Supervised Auxiliary Task (SSAT) 是让人意外地有利的。我们探讨适合 SSL 任务的选择、训练方案和数据规模，以便在有限数据量下实现最佳性能。我们的发现表明，SSAT 是一种强大的技术，它可以让 ViT 利用自身任务和 SSL 任务之间的独特特征，从而实现 better 性能，而不需要大量的训练数据。我们的实验，在 10 个数据集上进行，表明 SSAT 可以提高 ViT 性能，同时降低碳脚印。我们还证实了 SSAT 在视频领域中的深度假设检测 task 的效果，这表明它的普适性。我们的代码可以在 <https://github.com/dominickrei/Limited-data-vits> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Vanishing-Gradients-in-Reinforcement-Finetuning-of-Language-Models"><a href="#Vanishing-Gradients-in-Reinforcement-Finetuning-of-Language-Models" class="headerlink" title="Vanishing Gradients in Reinforcement Finetuning of Language Models"></a>Vanishing Gradients in Reinforcement Finetuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20703">http://arxiv.org/abs/2310.20703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, Etai Littwin</li>
<li>for: 这种研究旨在提高预训练语言模型的性能，以便更好地满足人类的需求和下游任务。</li>
<li>methods: 该研究使用了 reinforcement finetuning (RFT) 方法，通过最大化一个 (可能是学习的) 奖励函数来调整模型。</li>
<li>results: 研究发现，在 RFT 中，当输入的奖励标准差小于模型的时候，即使预期奖励远离最佳，输入的预期梯度将消失。这会导致奖励最大化变得极其慢。通过实验和理论分析，研究发现这种消失的梯度问题是普遍存在的和有害的。然而，通过一些方法来缓解这种问题，例如在 RFT 阶段使用初始的监督训练 (SFT) 阶段，可以提高 RFT 的性能。<details>
<summary>Abstract</summary>
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.
</details>
<details>
<summary>摘要</summary>
预训言语模型通常通过强化训练（RFT）与人类偏好和下游任务相对适配，其中包括通过政策梯度算法 maximize 一个（可能学习的）奖励函数。这项工作揭示了 RFT 中的一个基本优化障碍：我们证明了，对于一个输入，其奖励标准差 beneath 模型时，即使预期奖励远离最优，则预期梯度都将消失。通过实验 RFT  benchmark 和控制环境，以及理论分析，我们证明了这种消失梯度是普遍存在的并有害，导致奖励最大化 extremely slow。最后，我们探讨了在 RFT 中超越消失梯度的方法。我们发现，通常的初始监督 fine-tuning （SFT）阶段是最有前途的方法，这也解释了它在 RFT 链接中的重要性。此外，我们发现一些 SFT 优化步骤在 1% 的输入样本上可以得到充分的效果，这表明了初始 SFT 阶段不必耗费大量计算和数据标注努力。总的来说，我们的结果强调了在 RFT 中注意 inputs 的预期梯度消失，以measure 奖励标准差是关键的。
</details></li>
</ul>
<hr>
<h2 id="HAP-Structure-Aware-Masked-Image-Modeling-for-Human-Centric-Perception"><a href="#HAP-Structure-Aware-Masked-Image-Modeling-for-Human-Centric-Perception" class="headerlink" title="HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception"></a>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20695">http://arxiv.org/abs/2310.20695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junkunyuan/HAP">https://github.com/junkunyuan/HAP</a></li>
<li>paper_authors: Junkun Yuan, Xinyu Zhang, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, Jingdong Wang</li>
<li>for: 这篇论文的目的是提出一种基于人体结构约束的预训练方法，以提高人Centric感知任务的表现。</li>
<li>methods: 该方法使用遮盖图像模型（MIM）作为预训练方法，并在MIM训练策略中引入人体结构约束。具体来说，该方法使用人体部分划分图像 patches，以高优先级遮盖出人体部分区域。这种方法可以让模型在预训练过程中更加注重人体结构信息，从而提高人Centric感知任务的表现。</li>
<li>results: 该方法提出的HAP方法使用普通的ViTEncoder，却在11个人Centric标准数据集上达到了新的州OF-the-art表现，并与一个数据集的表现持平。例如，HAP在MSMT17数据集上达到了78.1% mAP，在PA-100K数据集上达到了86.54% mA，在MS COCO数据集上达到了78.2% AP，以及在3DPW数据集上达到了56.0 PA-MPJPE。<details>
<summary>Abstract</summary>
Model pre-training is essential in human-centric perception. In this paper, we first introduce masked image modeling (MIM) as a pre-training approach for this task. Upon revisiting the MIM training strategy, we reveal that human structure priors offer significant potential. Motivated by this insight, we further incorporate an intuitive human structure prior - human parts - into pre-training. Specifically, we employ this prior to guide the mask sampling process. Image patches, corresponding to human part regions, have high priority to be masked out. This encourages the model to concentrate more on body structure information during pre-training, yielding substantial benefits across a range of human-centric perception tasks. To further capture human characteristics, we propose a structure-invariant alignment loss that enforces different masked views, guided by the human part prior, to be closely aligned for the same image. We term the entire method as HAP. HAP simply uses a plain ViT as the encoder yet establishes new state-of-the-art performance on 11 human-centric benchmarks, and on-par result on one dataset. For example, HAP achieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K for pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose estimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.
</details>
<details>
<summary>摘要</summary>
模型预训练是人类视觉任务中的关键。在这篇论文中，我们首先介绍了隐藏图像模型（MIM）作为预训练方法。在检查MIM训练策略时，我们发现人体结构优先可以提供显著的潜在优势。 Motivated by this insight, we further incorporate an intuitive human structure prior - human parts - into pre-training. Specifically, we employ this prior to guide the mask sampling process. Image patches corresponding to human part regions have high priority to be masked out, which encourages the model to concentrate more on body structure information during pre-training, yielding substantial benefits across a range of human-centric perception tasks. To further capture human characteristics, we propose a structure-invariant alignment loss that enforces different masked views, guided by the human part prior, to be closely aligned for the same image. We term the entire method as HAP. HAP uses a plain ViT as the encoder and establishes new state-of-the-art performance on 11 human-centric benchmarks, and on-par results on one dataset. For example, HAP achieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K for pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose estimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.
</details></li>
</ul>
<hr>
<h2 id="Learning-From-Mistakes-Makes-LLM-Better-Reasoner"><a href="#Learning-From-Mistakes-Makes-LLM-Better-Reasoner" class="headerlink" title="Learning From Mistakes Makes LLM Better Reasoner"></a>Learning From Mistakes Makes LLM Better Reasoner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20689">http://arxiv.org/abs/2310.20689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/codet">https://github.com/microsoft/codet</a></li>
<li>paper_authors: Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen</li>
<li>for: 这项研究的目的是提高大语言模型（LLM）的数学问题解释能力。</li>
<li>methods: 该研究提出了一种基于错误学习（LeMa）的方法，模仿人类学习过程中的错误推理。</li>
<li>results: 实验结果表明，使用LeMa方法可以提高 LLM 的性能，并且可以将其应用到特殊的数学问题解释模型中，例如 WizardMath 和 MetaMath。<details>
<summary>Abstract</summary>
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve this capability, this work proposes Learning from Mistakes (LeMa), akin to human learning processes. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4. Specifically, we first collect inaccurate reasoning paths from various LLMs and then employ GPT-4 as a "corrector" to (1) identify the mistake step, (2) explain the reason for the mistake, and (3) correct the mistake and generate the final answer. Experimental results demonstrate the effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning tasks, LeMa consistently improves the performance compared with fine-tuning on CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on MATH. This surpasses the SOTA performance achieved by non-execution open-source models on these challenging tasks. Our code, data and models will be publicly available at https://github.com/microsoft/CodeT.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）最近表现出色的推理能力，以解决 math 问题。为了进一步改善这个能力，这个工作提出了学习自错（LeMa），与人类学习过程相似。例如，一个人学生不能解决一个 math 问题，他将从错误中学习，并推断出错误的步骤，以及如何修正错误。模仿这个错误驱动学习过程，LeMa 精细调整 LLM 的过程，使其能够更好地推理。具体来说，我们首先收集了不同 LLM 的错误推理路径，然后使用 GPT-4 作为 "修正者"，以（1）识别错误步骤，（2）解释错误的原因，以及（3）修正错误，并生成最终的答案。实验结果显示 LeMa 的有效性：在五个基本 LLM 和两个数学推理任务上，LeMa  invariably 提高了表现，与精细调整 CoT 数据 alone 相比。甚至可以帮助特殊化 LLM 如 WizardMath 和 MetaMath，在 GSM8K 和 MATH 这两个具有挑战性的任务上获得 85.4% 的通过率和 27.1% 的率，这超过了非执行的开源模型在这些任务上的最佳表现。我们将我们的代码、数据和模型公开 disponibile 在 https://github.com/microsoft/CodeT。
</details></li>
</ul>
<hr>
<h2 id="Offline-RL-with-Observation-Histories-Analyzing-and-Improving-Sample-Complexity"><a href="#Offline-RL-with-Observation-Histories-Analyzing-and-Improving-Sample-Complexity" class="headerlink" title="Offline RL with Observation Histories: Analyzing and Improving Sample Complexity"></a>Offline RL with Observation Histories: Analyzing and Improving Sample Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20663">http://arxiv.org/abs/2310.20663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joey Hong, Anca Dragan, Sergey Levine</li>
<li>for: The paper focuses on the problem of offline reinforcement learning (RL) in situations where the state is partially observed or unknown.</li>
<li>methods: The paper proposes a new loss function called the bisimulation loss, which encourages the RL algorithm to learn a compact representation of the history that is relevant for action selection.</li>
<li>results: The paper shows that the proposed loss can improve the performance of offline RL in a variety of tasks, and that it is closely related to good performance.<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) can in principle synthesize more optimal behavior from a dataset consisting only of suboptimal trials. One way that this can happen is by "stitching" together the best parts of otherwise suboptimal trajectories that overlap on similar states, to create new behaviors where each individual state is in-distribution, but the overall returns are higher. However, in many interesting and complex applications, such as autonomous navigation and dialogue systems, the state is partially observed. Even worse, the state representation is unknown or not easy to define. In such cases, policies and value functions are often conditioned on observation histories instead of states. In these cases, it is not clear if the same kind of "stitching" is feasible at the level of observation histories, since two different trajectories would always have different histories, and thus "similar states" that might lead to effective stitching cannot be leveraged. Theoretically, we show that standard offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition. We then identify sufficient conditions under which offline RL can still be efficient -- intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection. We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity. Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance.
</details>
<details>
<summary>摘要</summary>
偏向式学习（RL）可以在理论上Synthesize更优化的行为从一个只包含不优化尝试的数据集中。一种方式是将不同的尝试中的最佳部分“缝合” вместе，创造新的行为，每个状态都是内部分布的，但总体返回高于原来的。然而，在许多有趣和复杂的应用，如自动驾驶和对话系统，状态是部分可见的。甚至更糟糕，状态表示是未知或不容易定义。在这些情况下，策略和值函数通常是根据观察历史条件的，而不是根据状态。在这些情况下，是否可以在观察历史上进行类似的“缝合”，并不可能。我们证明了标准的偏向式RL算法 conditioned on 观察历史会受到低效样本复杂性的限制，符合上述直觉。我们然后提出了可能的有效条件，其中学习一个紧凑的历史表示，其中只包含行动选择所需的特征。我们引入了一种bisimulation损失，用于衡量这种情况是否发生。我们建议在offline RL中显式优化这种损失，以提高最坏情况的样本复杂性。实验表明，在各种任务中，我们的提议的损失可以提高性能，或者标准的offline RL可以自动地最小化这种损失，这表明它与好的性能有 corrrelation。
</details></li>
</ul>
<hr>
<h2 id="“Pick-and-Pass”-as-a-Hat-Trick-Class-for-First-Principle-Memory-Generalizability-and-Interpretability-Benchmarks"><a href="#“Pick-and-Pass”-as-a-Hat-Trick-Class-for-First-Principle-Memory-Generalizability-and-Interpretability-Benchmarks" class="headerlink" title="“Pick-and-Pass” as a Hat-Trick Class for First-Principle Memory, Generalizability, and Interpretability Benchmarks"></a>“Pick-and-Pass” as a Hat-Trick Class for First-Principle Memory, Generalizability, and Interpretability Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20654">http://arxiv.org/abs/2310.20654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Wang, Ryan Rezai</li>
<li>For: The paper is written to study model-free reinforcement learning algorithms and their ability to learn memory in closed drafting games, specifically the popular game “Sushi Go Party!”.* Methods: The paper uses a set of closely-related games based on the set of cards in play to establish first-principle benchmarks for studying model-free reinforcement learning algorithms.* Results: The paper produces state-of-the-art results on the Sushi Go Party! environment and quantifies the generalizability of reinforcement learning algorithms trained on various sets of cards, establishing key trends between generalized performance and the set distance between the train and evaluation game configurations. Additionally, the paper fits decision rules to interpret the strategy of the learned models and compares them to the ranking preferences of human players, finding intuitive common rules and intriguing new moves.<details>
<summary>Abstract</summary>
Closed drafting or "pick and pass" is a popular game mechanic where each round players select a card or other playable element from their hand and pass the rest to the next player. Games employing closed drafting make for great studies on memory and turn order due to their explicitly calculable memory of other players' hands. In this paper, we establish first-principle benchmarks for studying model-free reinforcement learning algorithms and their comparative ability to learn memory in a popular family of closed drafting games called "Sushi Go Party!", producing state-of-the-art results on this environment along the way. Furthermore, as Sushi Go Party! can be expressed as a set of closely-related games based on the set of cards in play, we quantify the generalizability of reinforcement learning algorithms trained on various sets of cards, establishing key trends between generalized performance and the set distance between the train and evaluation game configurations. Finally, we fit decision rules to interpret the strategy of the learned models and compare them to the ranking preferences of human players, finding intuitive common rules and intriguing new moves.
</details>
<details>
<summary>摘要</summary>
封闭 drafting 或 "挑选并传递" 是一种受欢迎的游戏机制，每回合玩家从手中选择一张卡或其他可玩元素，并将剩下的交给下一位玩家。由于这种机制的明确可计算的记忆，因此关于记忆和轮次的研究非常有价值。在这篇论文中，我们建立了基于原理的基准值，用于研究无基础学习算法在受欢迎的closed drafting游戏 "Sushi Go Party!" 中学习记忆的能力，并在这个环境中实现了state-of-the-art的结果。此外，由于Sushi Go Party! 可以表示为一系列基于卡片的游戏，我们量化了各种卡片集的对游戏性能的影响，并确定了关键的总体趋势。最后，我们采用决策规则来解释学习模型的策略，并与人类玩家的排名偏好进行比较，发现了直观的共同规则以及意外的新动作。
</details></li>
</ul>
<hr>
<h2 id="Histopathological-Image-Analysis-with-Style-Augmented-Feature-Domain-Mixing-for-Improved-Generalization"><a href="#Histopathological-Image-Analysis-with-Style-Augmented-Feature-Domain-Mixing-for-Improved-Generalization" class="headerlink" title="Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization"></a>Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20638">http://arxiv.org/abs/2310.20638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vaibhav-khamankar/fusestyle">https://github.com/vaibhav-khamankar/fusestyle</a></li>
<li>paper_authors: Vaibhav Khamankar, Sutanu Bera, Saumik Bhattacharya, Debashis Sen, Prabir Kumar Biswas</li>
<li>for: 增强机器学习模型对 histopathological 图像的泛化能力</li>
<li>methods: 使用 adaptive instance normalization 生成风格增强版图像</li>
<li>results: 比较与现有风格传输基于数据增强方法，我们的方法性能相似或更好，即使需要较少的计算和时间。<details>
<summary>Abstract</summary>
Histopathological images are essential for medical diagnosis and treatment planning, but interpreting them accurately using machine learning can be challenging due to variations in tissue preparation, staining and imaging protocols. Domain generalization aims to address such limitations by enabling the learning models to generalize to new datasets or populations. Style transfer-based data augmentation is an emerging technique that can be used to improve the generalizability of machine learning models for histopathological images. However, existing style transfer-based methods can be computationally expensive, and they rely on artistic styles, which can negatively impact model accuracy. In this study, we propose a feature domain style mixing technique that uses adaptive instance normalization to generate style-augmented versions of images. We compare our proposed method with existing style transfer-based data augmentation methods and found that it performs similarly or better, despite requiring less computation and time. Our results demonstrate the potential of feature domain statistics mixing in the generalization of learning models for histopathological image analysis.
</details>
<details>
<summary>摘要</summary>
In this study, we propose a feature domain style mixing technique that uses adaptive instance normalization to generate style-augmented versions of images. We compare our proposed method with existing style transfer-based data augmentation methods and found that it performs similarly or better, despite requiring less computation and time. Our results demonstrate the potential of feature domain statistics mixing in the generalization of learning models for histopathological image analysis.Here is the text in Simplified Chinese: histopathological 图像是医学诊断和治疗规划中不可或缺的，但是使用机器学习解释它们的准确性却有限制，这是因为样本准备、染色和扫描协议的变化。Domain generalization想要解决这些限制，使得学习模型能够通过新的数据集或人口来泛化。 Style transfer-based 数据增强是一种emerging技术，可以提高机器学习模型对 histopathological 图像的泛化能力。然而，现有的方法可能需要大量的计算时间，并且可能会因为艺术风格而导致模型精度下降。在这种研究中，我们提出了一种特征领域样式混合技术，使用适应实例Normalization来生成样式增强版图像。我们与现有的样式传输基于的数据增强方法进行比较，发现我们的提议方法能够达到相同或更好的性能，即使需要更少的计算时间和资源。我们的结果表明特征领域统计混合在机器学习模型的泛化中具有潜力。
</details></li>
</ul>
<hr>
<h2 id="LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-in-Llama-2-Chat-70B"><a href="#LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-in-Llama-2-Chat-70B" class="headerlink" title="LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"></a>LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20624">http://arxiv.org/abs/2310.20624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish</li>
<li>for: 这个论文的目的是研究语言模型的安全训练是否能够防止模型被恶意使用。</li>
<li>methods: 该论文使用了低级别适应（LoRA）方法进行训练，并使用了一个GPU和一个预算低于200美元来恢复Llama 2-Chat模型的安全训练。</li>
<li>results: 研究人员成功地使用了这种方法来破坏Llama 2-Chat模型的安全训练，并在两个退回测试中将模型的拒绝率降低到1%以下。此外，研究人员还 validate了这种方法对Llama 2-Chat模型的一致性。<details>
<summary>Abstract</summary>
AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat, a collection of instruction fine-tuned large language models, they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. However, it remains unclear how well safety training guards against model misuse when attackers have access to model weights. We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat. We employ low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than $200 per model and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve a refusal rate below 1% for our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method retains general performance, which we validate by comparing our fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, we present a selection of harmful outputs produced by our models. While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate and adapt to new environments. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Autonomous-Robotic-Reinforcement-Learning-with-Asynchronous-Human-Feedback"><a href="#Autonomous-Robotic-Reinforcement-Learning-with-Asynchronous-Human-Feedback" class="headerlink" title="Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback"></a>Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20608">http://arxiv.org/abs/2310.20608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Balsells, Marcel Torne, Zihan Wang, Samedh Desai, Pulkit Agrawal, Abhishek Gupta</li>
<li>for: 本研究旨在实现自主机器人学习在真实环境中，无需专门设计奖励函数或重置机制。</li>
<li>methods: 该研究使用了 occasional non-expert human-in-the-loop 反馈，以学习 informative distance functions，并使用 simple self-supervised learning algorithm 来学习目标决策。</li>
<li>results: 研究表明，在缺省情况下，需要考虑当前探索策略的可达性，以确定哪些空间区域要探索。基于这一点，实现了一个实用的学习系统 - GEAR，可以让机器人直接在真实环境中学习自主地，无需中断。系统通过流动机器人经验到网页界面，并且只需 periodic asynchronous 非专业人员反馈。研究在 simulate 和实际环境中展示了其效果。<details>
<summary>Abstract</summary>
Ideally, we would place a robot in a real-world environment and leave it there improving on its own by gathering more experience autonomously. However, algorithms for autonomous robotic learning have been challenging to realize in the real world. While this has often been attributed to the challenge of sample complexity, even sample-efficient techniques are hampered by two major challenges - the difficulty of providing well "shaped" rewards, and the difficulty of continual reset-free training. In this work, we describe a system for real-world reinforcement learning that enables agents to show continual improvement by training directly in the real world without requiring painstaking effort to hand-design reward functions or reset mechanisms. Our system leverages occasional non-expert human-in-the-loop feedback from remote users to learn informative distance functions to guide exploration while leveraging a simple self-supervised learning algorithm for goal-directed policy learning. We show that in the absence of resets, it is particularly important to account for the current "reachability" of the exploration policy when deciding which regions of the space to explore. Based on this insight, we instantiate a practical learning system - GEAR, which enables robots to simply be placed in real-world environments and left to train autonomously without interruption. The system streams robot experience to a web interface only requiring occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of binary comparative feedback. We evaluate this system on a suite of robotic tasks in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. Project website https://guided-exploration-autonomous-rl.github.io/GEAR/.
</details>
<details>
<summary>摘要</summary>
理想情况下，我们会将机器人放置在真实环境中，让它自动学习并不断改进，通过收集更多的经验。然而，自动机器人学习算法在真实世界中实现很困难。这经常被归结为样本复杂度的问题，而且甚至使用样本效率的技术也受到两大挑战：一是设置合适的奖励函数，二是实现不间断的培训。在这项工作中，我们描述了一种能够在真实世界中进行自主学习的系统，允许代理人通过不间断的培训来展现持续改进。我们的系统利用远程非专家用户的 occasional 非专业反馈来学习有用的距离函数，并使用简单的自适应学习算法来学习目标导向策略。我们发现在不间断培训情况下，特别重要的是考虑当前探索策略的可达性。基于这一点，我们实现了一个实用的学习系统——GEAR，允许机器人直接在真实环境中培训，并不需要繁琐的手动设计奖励函数或重置机制。系统将机器人经验流向网络界面，只需要 occasional 非专业用户在远程地提供偶极性反馈。我们在一组机器人任务上进行了模拟和实际环境的测试，并证明了该系统在学习行为方面的效果。项目网站：<https://guided-exploration-autonomous-rl.github.io/GEAR/>。
</details></li>
</ul>
<hr>
<h2 id="What-a-Whole-Slide-Image-Can-Tell-Subtype-guided-Masked-Transformer-for-Pathological-Image-Captioning"><a href="#What-a-Whole-Slide-Image-Can-Tell-Subtype-guided-Masked-Transformer-for-Pathological-Image-Captioning" class="headerlink" title="What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning"></a>What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20607">http://arxiv.org/abs/2310.20607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenkang Qin, Rui Xu, Peixiang Huang, Xiaomin Wu, Heyu Zhang, Lin Luo</li>
<li>for:  This paper proposes a new approach for pathological captioning of Whole Slide Images (WSIs) using Transformers, with the goal of improving the accuracy of computer-aided pathological diagnosis.</li>
<li>methods: The proposed approach, called Subtype-guided Masked Transformer (SGMT), treats a WSI as a sequence of sparse patches and generates an overall caption sentence from the sequence. An accompanying subtype prediction is introduced to guide the training process and enhance captioning accuracy. The Asymmetric Masked Mechanism approach is also used to tackle the large size constraint of pathological image captioning.</li>
<li>results: The authors report that their approach achieves superior performance compared to traditional RNN-based methods on the PatchGastricADC22 dataset.<details>
<summary>Abstract</summary>
Pathological captioning of Whole Slide Images (WSIs), though is essential in computer-aided pathological diagnosis, has rarely been studied due to the limitations in datasets and model training efficacy. In this paper, we propose a new paradigm Subtype-guided Masked Transformer (SGMT) for pathological captioning based on Transformers, which treats a WSI as a sequence of sparse patches and generates an overall caption sentence from the sequence. An accompanying subtype prediction is introduced into SGMT to guide the training process and enhance the captioning accuracy. We also present an Asymmetric Masked Mechansim approach to tackle the large size constraint of pathological image captioning, where the numbers of sequencing patches in SGMT are sampled differently in the training and inferring phases, respectively. Experiments on the PatchGastricADC22 dataset demonstrate that our approach effectively adapts to the task with a transformer-based model and achieves superior performance than traditional RNN-based methods. Our codes are to be made available for further research and development.
</details>
<details>
<summary>摘要</summary>
您好！我们在这篇论文中提出了一个新的思路，即基于Transformers的Subtype-guided Masked Transformer（SGMT），用于Computer-aided Pathological Diagnosis（CPD）中的标本描述。我们将整个标本视为一系列叠加的稀疏区块，并从这些区块中生成一个整体描述句子。此外，我们还引入了一个供应预测的子类别预测方法，以帮助训练过程并提高描述精度。此外，我们还提出了一个对应的Asymmetric Masked Mechanism方法，以解决Pathological Image Captioning中的大型数据集的限制。实验结果显示，我们的方法可以将Transformer-based模型适应这个任务，并在条件下超越传统RNN-based方法。我们的代码将会为更多的研究和发展提供。
</details></li>
</ul>
<hr>
<h2 id="Functional-connectivity-modules-in-recurrent-neural-networks-function-origin-and-dynamics"><a href="#Functional-connectivity-modules-in-recurrent-neural-networks-function-origin-and-dynamics" class="headerlink" title="Functional connectivity modules in recurrent neural networks: function, origin and dynamics"></a>Functional connectivity modules in recurrent neural networks: function, origin and dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20601">http://arxiv.org/abs/2310.20601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Tanner, Sina Mansour L., Ludovico Coletta, Alessandro Gozzi, Richard F. Betzel</li>
<li>for: 这个研究旨在解释大脑功能，尤其是神经同步现象在不同物种和组织水平上的普遍存在。</li>
<li>methods: 这个研究使用了回归神经网络， Investigating the functional role, origin, and dynamical implications of modular structures in correlation-based networks.</li>
<li>results: 研究发现，模块是功能准确的单位，贡献特殊的信息处理。模块自然形成，由输入层到回归层的偏好和重量差异引起。此外，研究发现，模块定义与类似功能连接，控制系统行为和动力学。<details>
<summary>Abstract</summary>
Understanding the ubiquitous phenomenon of neural synchronization across species and organizational levels is crucial for decoding brain function. Despite its prevalence, the specific functional role, origin, and dynamical implication of modular structures in correlation-based networks remains ambiguous. Using recurrent neural networks trained on systems neuroscience tasks, this study investigates these important characteristics of modularity in correlation networks. We demonstrate that modules are functionally coherent units that contribute to specialized information processing. We show that modules form spontaneously from asymmetries in the sign and weight of projections from the input layer to the recurrent layer. Moreover, we show that modules define connections with similar roles in governing system behavior and dynamics. Collectively, our findings clarify the function, formation, and operational significance of functional connectivity modules, offering insights into cortical function and laying the groundwork for further studies on brain function, development, and dynamics.
</details>
<details>
<summary>摘要</summary>
理解跨种类和组织层次的神经同步现象的重要性，可以帮助我们解读大脑的功能。尽管这种现象非常普遍，但模块结构在相关性网络中的特定功能作用、起源和动态影响仍然不够清楚。本研究使用基于系统神经科学任务的循环神经网络进行研究，以了解这些重要特征。我们发现，模块是功能协调的单位，对特定信息处理做出贡献。我们还发现，模块在输入层到循环层的权重和符号差异的基础上自然地形成。此外，我们发现模块在控制系统行为和动力学中扮演着相似的角色。总之，我们的发现可以解释功能连接模块的功能、形成和运作意义，为大脑功能、发展和动态研究提供新的思路和方法。
</details></li>
</ul>
<hr>
<h2 id="Taking-control-Policies-to-address-extinction-risks-from-advanced-AI"><a href="#Taking-control-Policies-to-address-extinction-risks-from-advanced-AI" class="headerlink" title="Taking control: Policies to address extinction risks from advanced AI"></a>Taking control: Policies to address extinction risks from advanced AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20563">http://arxiv.org/abs/2310.20563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Miotti, Akash Wasil</li>
<li>for: 降低人类灭绝的风险，提出了三项政策建议。</li>
<li>methods: 建议包括成立多国AGI联盟（MAGIC），实施全球计算能力上限（global compute cap），并要求企业在开发强大AI系统时进行安全评估（gating critical experiments）。</li>
<li>results: 这三项政策建议可以有效地降低高度智能AI系统对人类灭绝的风险，并且可以让大多数AI创新得以继续不受限制。<details>
<summary>Abstract</summary>
This paper provides policy recommendations to reduce extinction risks from advanced artificial intelligence (AI). First, we briefly provide background information about extinction risks from AI. Second, we argue that voluntary commitments from AI companies would be an inappropriate and insufficient response. Third, we describe three policy proposals that would meaningfully address the threats from advanced AI: (1) establishing a Multinational AGI Consortium to enable democratic oversight of advanced AI (MAGIC), (2) implementing a global cap on the amount of computing power used to train an AI system (global compute cap), and (3) requiring affirmative safety evaluations to ensure that risks are kept below acceptable levels (gating critical experiments). MAGIC would be a secure, safety-focused, internationally-governed institution responsible for reducing risks from advanced AI and performing research to safely harness the benefits of AI. MAGIC would also maintain emergency response infrastructure (kill switch) to swiftly halt AI development or withdraw model deployment in the event of an AI-related emergency. The global compute cap would end the corporate race toward dangerous AI systems while enabling the vast majority of AI innovation to continue unimpeded. Gating critical experiments would ensure that companies developing powerful AI systems are required to present affirmative evidence that these models keep extinction risks below an acceptable threshold. After describing these recommendations, we propose intermediate steps that the international community could take to implement these proposals and lay the groundwork for international coordination around advanced AI.
</details>
<details>
<summary>摘要</summary>
这份报告提供了降低高级人工智能（AI）濒临灭绝风险的政策建议。首先，我们简要介绍高级AI濒临灭绝风险的背景信息。其次，我们认为企业自愿承诺是不适当和不够的回应。第三，我们描述了三个政策建议，可以实际地解决高级AI的威胁：（1）成立多国AGI协会（MAGIC），以实现多国安全协调高级AI的措施，并执行安全的AI研究；（2）实施全球计算能力套限（global compute cap），以结束危险AI系统的企业竞赛，同时允许大多数AI创新继续不受妨碍；（3）要求开发强大AI系统的公司提供证明，以确保风险保持在可接受水平（安全评估）。MAGIC是一个安全、安全感ocus的、国际治理的机构，负责降低高级AI濒临灭绝风险，并执行安全的AI研究。MAGIC还将维护紧急应急基础设施（kill switch），以快速干预AI开发或者模型部署在AI相关紧急情况下。全球计算能力套限将结束危险AI系统的企业竞赛，同时允许大多数AI创新继续不受妨碍。安全评估将确保开发强大AI系统的公司提供证明，以确保风险保持在可接受水平。文章最后，我们建议国际社会可以采取以下措施来实现这些建议，并为高级AI的国际协调做准备。
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Token-Barrier-Chunking-and-Convolution-for-Efficient-Long-Text-Classification-with-BERT"><a href="#Breaking-the-Token-Barrier-Chunking-and-Convolution-for-Efficient-Long-Text-Classification-with-BERT" class="headerlink" title="Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT"></a>Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20558">http://arxiv.org/abs/2310.20558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aman Jaiswal, Evangelos Milios</li>
<li>for: 本研究旨在提高BERT模型在长输入文本中的应用，并解决BERT模型的512个token限制。</li>
<li>methods: 本研究提出了一种简单扩展BERT模型，称为ChunkBERT，可以让任何预训练模型进行长文本推理。ChunkBERT基于分割token表示和CNN层，可以与任何预训练BERT模型兼容。</li>
<li>results: 在一个用于对比不同长文本分类任务的benchmark中，BERT模型经过ChunkBERT扩展后在长样本中保持稳定性，而且只用了原始内存占用的6.25%。这些结果表明，通过简单地修改预训练BERT模型，可以实现高效的finetuning和推理。<details>
<summary>Abstract</summary>
Transformer-based models, specifically BERT, have propelled research in various NLP tasks. However, these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input. Various complex methods have claimed to overcome this limit, but recent research questions the efficacy of these models across different classification tasks. These complex architectures evaluated on carefully curated long datasets perform at par or worse than simple baselines. In this work, we propose a relatively simple extension to vanilla BERT architecture called ChunkBERT that allows finetuning of any pretrained models to perform inference on arbitrarily long text. The proposed method is based on chunking token representations and CNN layers, making it compatible with any pre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for comparing long-text classification models across a variety of tasks (including binary classification, multi-class classification, and multi-label classification). A BERT model finetuned using the ChunkBERT method performs consistently across long samples in the benchmark while utilizing only a fraction (6.25\%) of the original memory footprint. These findings suggest that efficient finetuning and inference can be achieved through simple modifications to pre-trained BERT models.
</details>
<details>
<summary>摘要</summary>
带基于Transformer的模型，尤其是BERT，在不同的自然语言处理任务中进行研究。然而，这些模型具有最大token数限制为512个，这使得在实际应用中处理长输入变得不容易。许多复杂的方法已经被提出来突破这个限制，但最近的研究表明这些模型在不同的分类任务中的效果存在问题。这些复杂的架构在手动挑选的长数据集上评估时和简单的基线模型相当或更差。在这种工作中，我们提出了一种基于BERT核心架构的简单扩展方法called ChunkBERT，该方法允许任何预训练模型进行长文本的推理。我们基于块化Token表示和CNN层，使其与任何预训练BERT模型兼容。我们在一个用于比较不同任务的长文本分类模型 benchmark 上solely evaluate ChunkBERT。一个使用ChunkBERT方法精度训练的BERT模型在长样本上表现一致，使用的内存占用量仅为原始的6.25%。这些发现表明，通过简单地修改预训练BERT模型，可以实现高效的训练和推理。
</details></li>
</ul>
<hr>
<h2 id="CapsFusion-Rethinking-Image-Text-Data-at-Scale"><a href="#CapsFusion-Rethinking-Image-Text-Data-at-Scale" class="headerlink" title="CapsFusion: Rethinking Image-Text Data at Scale"></a>CapsFusion: Rethinking Image-Text Data at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20550">http://arxiv.org/abs/2310.20550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baaivision/CapsFusion">https://github.com/baaivision/CapsFusion</a></li>
<li>paper_authors: Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, Jingjing Liu</li>
<li>for: 提高大型多Modal模型的泛化能力和可扩展性，以进行多modal任务 zero-shot learning。</li>
<li>methods: 利用大规模 web 上的图片文本对数据，以及使用 captioning 模型生成的假 caption，进行模型训练。</li>
<li>results: CapsFusion 模型在 COCO 和 NoCaps 测试集上的 CIDEr 得分提高 18.8 和 18.3 分，在模型性能、样本效率、世界知识深度和可扩展性三个方面表现出色。<details>
<summary>Abstract</summary>
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experiments show that CapsFusion captions exhibit remarkable all-round superiority over existing captions in terms of model performance (e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample efficiency (requiring 11-16 times less computation than baselines), world knowledge depth, and scalability. These effectiveness, efficiency and scalability advantages position CapsFusion as a promising candidate for future scaling of LMM training.
</details>
<details>
<summary>摘要</summary>
大型多modal模型在零值模式下表现出了惊人的通用能力，可以完成多种多modal任务。大规模的网络上的图片文本对象贡献到这些成功的基础，但是受到过度噪音的影响。最近的研究使用了由captioning模型生成的另外的caption，并达到了很好的 benchMark性能。然而，我们的实验表明，使用生成的caption会导致Scalability Deficiency和World Knowledge Loss问题，这些问题在初始的benchMark成功后被掩盖了。经过仔细分析，我们发现了这些问题的根本原因是现有的生成caption的语言结构过于简单，缺乏知识细节。为了提供更高质量和可扩展的多modal预训练数据，我们提出了CapsFusion，一种高级框架，利用大型语言模型来整合和提高来自网络上的图片文本对象和生成caption的信息。我们的广泛实验表明，CapsFusion caption在模型性能（例如，COCO和NoCaps中的CIDEr得分提高18.8和18.3）、样本效率（需要11-16倍的计算量 menos than baseline）、世界知识深度和可扩展性方面具有惊人的优势。这些优势使CapsFusion成为未来扩展LMM训练的优秀候选人。
</details></li>
</ul>
<hr>
<h2 id="LLMs-may-Dominate-Information-Access-Neural-Retrievers-are-Biased-Towards-LLM-Generated-Texts"><a href="#LLMs-may-Dominate-Information-Access-Neural-Retrievers-are-Biased-Towards-LLM-Generated-Texts" class="headerlink" title="LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts"></a>LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20501">http://arxiv.org/abs/2310.20501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Jun Xu</li>
<li>for:  investigate the influence of LLM-generated documents on IR systems and the potential biases in neural retrieval models towards LLM-generated text.</li>
<li>methods:  quantitative evaluation of different IR models in scenarios with human-written and LLM-generated texts, text compression analysis, and theoretical analysis.</li>
<li>results:  the neural retrieval models tend to rank LLM-generated documents higher, which is referred to as the “source bias”; this bias is not limited to first-stage neural retrievers but also extends to second-stage neural re-rankers; the bias is due to the neural models’ ability to understand the semantic information of LLM-generated text.<details>
<summary>Abstract</summary>
Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher.We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, we provide an in-depth analysis from the perspective of text compression and observe that neural models can better understand the semantic information of LLM-generated text, which is further substantiated by our theoretical analysis.We also discuss the potential server concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks and codes will later be available at \url{https://github.com/KID-22/LLM4IR-Bias}.
</details>
<details>
<summary>摘要</summary>
最近，大语言模型（LLM）的出现对信息检索（IR）应用领域产生了革命性的变革，特别是在网络搜索中。 LLM 的出色的文本生成能力使得互联网上有巨量的文本出现。这使得 IR 系统在 LLM 时代面临一个新的挑战：索引文档现在不仅由人类创建，还可能由 LLM 自动生成。这些 LLM 生成的文本如何影响 IR 系统是一个压力性的问题，尚未得到探索。在这种情况下，我们进行了量化评估不同 IR 模型在人类写作和 LLM 生成文本卷积中的表现。结果显示，神经搜索模型倾向于将 LLM 生成的文本排名在首位。我们称这种偏见为“源偏见”。此外，我们发现这种偏见不仅存在于第一阶段神经搜索模型中，而且还扩展到第二阶段神经重新排名器中。接着，我们从文本压缩角度进行了深入分析，并证明了神经模型对 LLM 生成文本的 semantic 信息有更好的理解能力。最后，我们讨论了可能由此观察到的服务器问题，并希望我们的发现能够为 IR 社区和更广泛的领域产生一个重要的警示。为便于未来在 LLM 时代进行 IR 探索，我们将在 GitHub 上提供两个新的benchmark和代码，可以在 \url{https://github.com/KID-22/LLM4IR-Bias} 获取。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-Based-Model-With-Self-Distillation-for-Multimodal-Emotion-Recognition-in-Conversations"><a href="#A-Transformer-Based-Model-With-Self-Distillation-for-Multimodal-Emotion-Recognition-in-Conversations" class="headerlink" title="A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations"></a>A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20494">http://arxiv.org/abs/2310.20494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/butterfliesss/sdt">https://github.com/butterfliesss/sdt</a></li>
<li>paper_authors: Hui Ma, Jian Wang, Hongfei Lin, Bo Zhang, Yijia Zhang, Bo Xu</li>
<li>for: 本研究旨在提高对话中情感识别（ERC）的精度，即在对话中识别每个句子的情感。</li>
<li>methods: 本研究使用转换器模型（Transformer），并通过自适应归一化（SDT）来学习多modal信息之间的交互关系，同时动态学习不同modalities之间的权重。</li>
<li>results: 实验结果表明，使用SDT模型在IEMOCAP和MELD数据集上表现出色，超过了之前的基线。<details>
<summary>Abstract</summary>
Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context- and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra- and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal representations play important roles in multimodal ERC. In this paper, we propose a transformer-based model with self-distillation (SDT) for the task. The transformer-based model captures intra- and inter-modal interactions by utilizing intra- and inter-modal transformers, and learns weights between modalities dynamically by designing a hierarchical gated fusion strategy. Furthermore, to learn more expressive modal representations, we treat soft labels of the proposed model as extra training supervision. Specifically, we introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality. Experiments on IEMOCAP and MELD datasets demonstrate that SDT outperforms previous state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
倾向感知在对话中（ERC），认识对话中每句话的情感，对于建立同情机器非常重要。现有研究主要集中在文本模式下捕捉上下文和发言人相关的依赖关系，而忽略多Modal信息的重要性。与文本对话的情感认识不同，在多Modal ERC中捕捉 между语音和视频语音之间的内部和交叉模式互动，学习不同模式之间的权重，以及增强模式表示都是关键。在这篇论文中，我们提议一种基于变换器的模型，并使用自适应（SDT）进行学习。变换器模型利用内部和交叉模式 transformer 来捕捉内部和交叉模式互动，并通过设计层次闭合策略来动态学习不同模式之间的权重。此外，为了学习更加表达力的模式表示，我们将提议模型的软标签作为额外的训练监督。具体来说，我们引入自适应来传递模型中的硬标签和软标签知识到每个模式。实验结果表明，SDT在IEMOCAP和MELD dataset上超过了前一个基eline。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Black-boxes-Explainable-Deep-Learning-Models-for-Patent-Classification"><a href="#Unveiling-Black-boxes-Explainable-Deep-Learning-Models-for-Patent-Classification" class="headerlink" title="Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification"></a>Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20478">http://arxiv.org/abs/2310.20478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Shajalal, Sebastian Denef, Md. Rezaul Karim, Alexander Boden, Gunnar Stevens</li>
<li>for: 本研究旨在提出一种可解释的多标签专利分类框架，以提高人类专业人员对复杂AI技术的理解和管理能力。</li>
<li>methods: 本研究使用了深度神经网络（DNN）和层wise relevance propagation（LRP）技术，以提供人类可理解的预测解释。通过对模型的预测结果进行反向传播，找出每个预测的相关性分数，并使用这些分数来生成预测的解释。</li>
<li>results: 实验结果表明，使用本研究提出的方法可以在两个 dataset 上实现高效的多标签专利分类，并且生成的解释能够帮助人类更好地理解预测结果。<details>
<summary>Abstract</summary>
Recent technological advancements have led to a large number of patents in a diverse range of domains, making it challenging for human experts to analyze and manage. State-of-the-art methods for multi-label patent classification rely on deep neural networks (DNNs), which are complex and often considered black-boxes due to their opaque decision-making processes. In this paper, we propose a novel deep explainable patent classification framework by introducing layer-wise relevance propagation (LRP) to provide human-understandable explanations for predictions. We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class. Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable. Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.
</details>
<details>
<summary>摘要</summary>
We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class.Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable. Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.Translated into Simplified Chinese:最近的技术进步导致了大量的专利文本，使得人类专家分析和管理变得困难。现代的多标签专利分类方法多数使用深度神经网络（DNN），它们的决策过程可能是黑盒子，很难被人类理解。在这篇论文中，我们提出了一种新的深度可解释专利分类框架，通过引入层次相关传播（LRP）来提供人类可理解的解释。我们训练了多个DNN模型，包括Bi-LSTM、CNN和CNN-BiLSTM，并将预测结果倒退到模型的输入层，以确定每个预测的单词的相关性。根据相关性分数，我们 THEN generates explanations by visualizing relevant words for the predicted patent class.实验结果表明，我们在两个数据集上，每个数据集包含200万个专利文本，得到了高效的性能。解释生成的每个预测高亮显示了与预测类相关的关键单词，使预测更容易理解。可解释系统有可能在实际应用中推广复杂的AIEnabled方法。
</details></li>
</ul>
<hr>
<h2 id="Global-Transformer-Architecture-for-Indoor-Room-Temperature-Forecasting"><a href="#Global-Transformer-Architecture-for-Indoor-Room-Temperature-Forecasting" class="headerlink" title="Global Transformer Architecture for Indoor Room Temperature Forecasting"></a>Global Transformer Architecture for Indoor Room Temperature Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20476">http://arxiv.org/abs/2310.20476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alfredo V Clemente, Alessandro Nocente, Massimiliano Ruocco</li>
<li>for: 降低建筑物能源消耗和气候变化释放，提高建筑物内部温度预测精度，以实现有效的控制系统。</li>
<li>methods: 使用全球Transformer架构进行多间房内温度预测，并可以在整个数据集上训练模型，从而提高预测性能和简化部署和维护。</li>
<li>results: 该方法可以提高建筑物内部温度预测精度，并且可以减少建筑物的能源消耗和气候变化释放，为建筑物的能源协调和维护做出了重要贡献。<details>
<summary>Abstract</summary>
A thorough regulation of building energy systems translates in relevant energy savings and in a better comfort for the occupants. Algorithms to predict the thermal state of a building on a certain time horizon with a good confidence are essential for the implementation of effective control systems. This work presents a global Transformer architecture for indoor temperature forecasting in multi-room buildings, aiming at optimizing energy consumption and reducing greenhouse gas emissions associated with HVAC systems. Recent advancements in deep learning have enabled the development of more sophisticated forecasting models compared to traditional feedback control systems. The proposed global Transformer architecture can be trained on the entire dataset encompassing all rooms, eliminating the need for multiple room-specific models, significantly improving predictive performance, and simplifying deployment and maintenance. Notably, this study is the first to apply a Transformer architecture for indoor temperature forecasting in multi-room buildings. The proposed approach provides a novel solution to enhance the accuracy and efficiency of temperature forecasting, serving as a valuable tool to optimize energy consumption and decrease greenhouse gas emissions in the building sector.
</details>
<details>
<summary>摘要</summary>
一种全面的建筑能源系统管理可以导致有效的能源储存和建筑内部的舒适度提高。预测建筑物内部温度的算法在某些时间 horizons 上具有良好的信任度是控制系统的实施所必需的。这项工作提出了一种全球转换器架构，用于多房间内部温度预测，以优化能源消耗和减少建筑物中HVAC系统的绿色气体排放。在深度学习技术的发展下，可以开发出更加复杂的预测模型，而不是传统的反馈控制系统。全球转换器架构可以在整个数据集上训练，消除多个房间特定的模型需求，显著提高预测性能，并简化部署和维护。值得一提的是，这项研究是首次应用转换器架构来预测多房间内部温度。提出的方法可以增强温度预测的准确性和效率，并作为建筑领域中能源消耗优化和绿色气体排放减少的有价值工具。
</details></li>
</ul>
<hr>
<h2 id="Linked-Papers-With-Code-The-Latest-in-Machine-Learning-as-an-RDF-Knowledge-Graph"><a href="#Linked-Papers-With-Code-The-Latest-in-Machine-Learning-as-an-RDF-Knowledge-Graph" class="headerlink" title="Linked Papers With Code: The Latest in Machine Learning as an RDF Knowledge Graph"></a>Linked Papers With Code: The Latest in Machine Learning as an RDF Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20475">http://arxiv.org/abs/2310.20475</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlamprecht/linkedpaperswithcode">https://github.com/davidlamprecht/linkedpaperswithcode</a></li>
<li>paper_authors: Michael Färber, David Lamprecht</li>
<li>for: 本研究提出了一种基于RDF的机器学习文献知识图 Linked Papers With Code (LPWC), 其提供了大约400,000篇机器学习论文的完整、当前信息，包括论文所处理的任务、使用的数据集、实现的方法以及进行的评估结果。</li>
<li>methods:  LPWC使用RDF格式将最新的机器学习发展翻译成知识图形式，同时允许科学影响量量化和学术关键内容推荐。</li>
<li>results: LPWC可以在 Linked Open Data 云中作为知识图存在，提供了多种格式，包括 RDF 填充文件、SPARQL 端点 для直接网络查询以及数据源与 SemOpenAlex、Wikidata 和 DBLP 的链接。此外，LPWC还提供了知识图嵌入，使其可以 direct 应用于机器学习应用程序。<details>
<summary>Abstract</summary>
In this paper, we introduce Linked Papers With Code (LPWC), an RDF knowledge graph that provides comprehensive, current information about almost 400,000 machine learning publications. This includes the tasks addressed, the datasets utilized, the methods implemented, and the evaluations conducted, along with their results. Compared to its non-RDF-based counterpart Papers With Code, LPWC not only translates the latest advancements in machine learning into RDF format, but also enables novel ways for scientific impact quantification and scholarly key content recommendation. LPWC is openly accessible at https://linkedpaperswithcode.com and is licensed under CC-BY-SA 4.0. As a knowledge graph in the Linked Open Data cloud, we offer LPWC in multiple formats, from RDF dump files to a SPARQL endpoint for direct web queries, as well as a data source with resolvable URIs and links to the data sources SemOpenAlex, Wikidata, and DBLP. Additionally, we supply knowledge graph embeddings, enabling LPWC to be readily applied in machine learning applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 Linked Papers With Code（LPWC），一个基于 RDF 的知识格 graphs，提供了大约 400,000 篇机器学习论文的全面、当前信息。这包括论文中 Addressed 的任务、使用的数据集、实现的方法、进行的评估和结果。与其非 RDF-based 对手 Papers With Code 不同，LPWC 不仅将最新的机器学习成果翻译成 RDF 格式，还允许科学影响量量化和学术关键内容推荐。LPWC 公开 accessible 于 <https://linkedpaperswithcode.com>，并且被licensed under CC-BY-SA 4.0。作为 Linked Open Data 云中的知识格，我们向你提供了多种格式，从 RDF 冲dump 文件到 SPARQL 终结点，以便直接在网上进行查询，以及一个可以解析 URI 和数据源 SemOpenAlex、Wikidata 和 DBLP 的数据源。此外，我们还提供了知识格嵌入，使 LPWC 可以轻松应用于机器学习应用程序。
</details></li>
</ul>
<hr>
<h2 id="Critical-Role-of-Artificially-Intelligent-Conversational-Chatbot"><a href="#Critical-Role-of-Artificially-Intelligent-Conversational-Chatbot" class="headerlink" title="Critical Role of Artificially Intelligent Conversational Chatbot"></a>Critical Role of Artificially Intelligent Conversational Chatbot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20474">http://arxiv.org/abs/2310.20474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seraj A. M. Mostafa, Md Z. Islam, Mohammad Z. Islam, Fairose Jeehan, Saujanna Jafreen, Raihan U. Islam</li>
<li>for: 本研究旨在探讨 chatGPT 在学术上的伦理问题、其限制和特定用户群体的虚假使用。</li>
<li>methods: 本研究采用了多种方法，包括实验和分析，以探讨 chatGPT 的伦理和限制。</li>
<li>results: 研究发现了一些可能的假设和伦理问题，以及一些解决方案，以便避免不当使用和促进负责任 AI 交互。<details>
<summary>Abstract</summary>
Artificially intelligent chatbot, such as ChatGPT, represents a recent and powerful advancement in the AI domain. Users prefer them for obtaining quick and precise answers, avoiding the usual hassle of clicking through multiple links in traditional searches. ChatGPT's conversational approach makes it comfortable and accessible for finding answers quickly and in an organized manner. However, it is important to note that these chatbots have limitations, especially in terms of providing accurate answers as well as ethical concerns. In this study, we explore various scenarios involving ChatGPT's ethical implications within academic contexts, its limitations, and the potential misuse by specific user groups. To address these challenges, we propose architectural solutions aimed at preventing inappropriate use and promoting responsible AI interactions.
</details>
<details>
<summary>摘要</summary>
人工智能聊天机器人，如ChatGPT，是当今AI领域的一项新的和强大的进步。用户喜欢使用它们以获取快速和准确的答案，而不需要遍历多个链接。聊天机器人的对话方式使得它们易于使用，并且能够组织化地提供答案。然而，这些聊天机器人存在限制，特别是在提供准确答案和伦理问题方面。在这项研究中，我们探讨了ChatGPT在学术上下文中的伦理问题、其限制以及特定用户群体可能会滥用它的问题。为解决这些挑战，我们提议了一些建筑解决方案，以防止不当使用和推动负责任AI互动。
</details></li>
</ul>
<hr>
<h2 id="ACL-Anthology-Helper-A-Tool-to-Retrieve-and-Manage-Literature-from-ACL-Anthology"><a href="#ACL-Anthology-Helper-A-Tool-to-Retrieve-and-Manage-Literature-from-ACL-Anthology" class="headerlink" title="ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL Anthology"></a>ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL Anthology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20467">http://arxiv.org/abs/2310.20467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Tang, Frank Guerin, Chenghua Lin</li>
<li>for:  This paper is written for researchers who need to efficiently access and organize literature from the ACL Anthology, a comprehensive collection of NLP and CL publications.</li>
<li>methods: The paper presents a tool called ACL Anthology Helper, which automates the process of parsing and downloading papers along with their meta-information, and stores them in a local MySQL database.</li>
<li>results: The tool offers over 20 operations for efficient literature retrieval, including “where,” “group,” “order,” and more, and has been successfully utilized in writing a survey paper (Tang et al.,2022a).<details>
<summary>Abstract</summary>
The ACL Anthology is an online repository that serves as a comprehensive collection of publications in the field of natural language processing (NLP) and computational linguistics (CL). This paper presents a tool called ``ACL Anthology Helper''. It automates the process of parsing and downloading papers along with their meta-information, which are then stored in a local MySQL database. This allows for efficient management of the local papers using a wide range of operations, including "where," "group," "order," and more. By providing over 20 operations, this tool significantly enhances the retrieval of literature based on specific conditions. Notably, this tool has been successfully utilised in writing a survey paper (Tang et al.,2022a). By introducing the ACL Anthology Helper, we aim to enhance researchers' ability to effectively access and organise literature from the ACL Anthology. This tool offers a convenient solution for researchers seeking to explore the ACL Anthology's vast collection of publications while allowing for more targeted and efficient literature retrieval.
</details>
<details>
<summary>摘要</summary>
ACL Anthology是一个在线存储库，它是自然语言处理（NLP）和计算语言学（CL）领域的完整收藏。这篇论文介绍了一种名为“ACL Anthology Helper”的工具。它自动将ACL Anthology中的文章和相关信息解析出来，并将其存储在本地的MySQL数据库中。这使得研究者可以使用各种操作（如“where”、“group”、“order”等）来高效管理本地文章。这个工具提供了超过20种操作，可以帮助研究者根据特定条件进行文献检索。尤其是，这个工具在写作一篇survey paper（Tang et al.,2022a）时得到了成功应用。我们通过引入ACL Anthology Helper，旨在增强研究者对ACL Anthology的文献检索和管理的能力。这个工具提供了一种方便的解决方案，帮助研究者更加高效地探索ACL Anthology的庞大文献收藏，并且允许更加Targeted和高效的文献检索。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Neural-PDE-Solvers-using-Symbolic-Frameworks"><a href="#Interpretable-Neural-PDE-Solvers-using-Symbolic-Frameworks" class="headerlink" title="Interpretable Neural PDE Solvers using Symbolic Frameworks"></a>Interpretable Neural PDE Solvers using Symbolic Frameworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20463">http://arxiv.org/abs/2310.20463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yolanne Yi Ran Lee</li>
<li>for: 这篇论文旨在解决深度学习中的解释性问题，以提高人们对神经网络解决方案的理解和信任度。</li>
<li>methods: 这篇论文提出了将符号框架（如符号回归）与神经网络相结合的想法，以帮助将神经网络的决策过程变换为人类可读的数学表达。</li>
<li>results: 研究人员通过对数据集进行符号框架和神经网络的组合，发现这种方法可以提高神经网络的解释性和准确性。<details>
<summary>Abstract</summary>
Partial differential equations (PDEs) are ubiquitous in the world around us, modelling phenomena from heat and sound to quantum systems. Recent advances in deep learning have resulted in the development of powerful neural solvers; however, while these methods have demonstrated state-of-the-art performance in both accuracy and computational efficiency, a significant challenge remains in their interpretability. Most existing methodologies prioritize predictive accuracy over clarity in the underlying mechanisms driving the model's decisions. Interpretability is crucial for trustworthiness and broader applicability, especially in scientific and engineering domains where neural PDE solvers might see the most impact. In this context, a notable gap in current research is the integration of symbolic frameworks (such as symbolic regression) into these solvers. Symbolic frameworks have the potential to distill complex neural operations into human-readable mathematical expressions, bridging the divide between black-box predictions and solutions.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="AsGrad-A-Sharp-Unified-Analysis-of-Asynchronous-SGD-Algorithms"><a href="#AsGrad-A-Sharp-Unified-Analysis-of-Asynchronous-SGD-Algorithms" class="headerlink" title="AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms"></a>AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20452">http://arxiv.org/abs/2310.20452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rustem Islamov, Mher Safaryan, Dan Alistarh</li>
<li>for: 这个论文是为了研究 asynchronous-type algorithms for distributed SGD in heterogeneous setting 而写的。</li>
<li>methods: 这个论文使用了 asynchronous SGD 和 worker shuffling 等方法。</li>
<li>results: 论文提出了一种 unified convergence theory for non-convex smooth functions in heterogeneous regime, 并且证明了这种方法的性能可以与同步方法相比。<details>
<summary>Abstract</summary>
We analyze asynchronous-type algorithms for distributed SGD in the heterogeneous setting, where each worker has its own computation and communication speeds, as well as data distribution. In these algorithms, workers compute possibly stale and stochastic gradients associated with their local data at some iteration back in history and then return those gradients to the server without synchronizing with other workers. We present a unified convergence theory for non-convex smooth functions in the heterogeneous regime. The proposed analysis provides convergence for pure asynchronous SGD and its various modifications. Moreover, our theory explains what affects the convergence rate and what can be done to improve the performance of asynchronous algorithms. In particular, we introduce a novel asynchronous method based on worker shuffling. As a by-product of our analysis, we also demonstrate convergence guarantees for gradient-type algorithms such as SGD with random reshuffling and shuffle-once mini-batch SGD. The derived rates match the best-known results for those algorithms, highlighting the tightness of our approach. Finally, our numerical evaluations support theoretical findings and show the good practical performance of our method.
</details>
<details>
<summary>摘要</summary>
我们分析了分布式SGD中的异步类算法，在异种设置下进行分析，每个工作者都有自己的计算和通信速度，以及数据分布。在这些算法中，工作者在某个过去的迭代中计算了本地数据相关的可能偏移和随机梯度，然后将这些梯度返回给服务器而无需与其他工作者同步。我们提出了一种统一的收敛理论，用于非对称凸函数的收敛分析。我们的分析显示，异步算法的收敛率受到多种因素的影响，并且可以通过一些方法提高其性能。例如，我们引入了一种基于工作者排序的异步方法。在我们的分析中，我们还证明了SGD与随机排序和排序一次小批量SGD的收敛性能。实验结果支持我们的理论发现，并表明了我们的方法在实际应用中的良好性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bayesian-Learning-Curve-Extrapolation-using-Prior-Data-Fitted-Networks"><a href="#Efficient-Bayesian-Learning-Curve-Extrapolation-using-Prior-Data-Fitted-Networks" class="headerlink" title="Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks"></a>Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20447">http://arxiv.org/abs/2310.20447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Adriaensen, Herilalaina Rakotoarison, Samuel Müller, Frank Hutter</li>
<li>for: 预测模型在训练后期的性能，基于早期训练过程中的性能。</li>
<li>methods: 使用先进的 Bayesian方法，并使用 prior-data fitted neural networks (PFNs) 来approximate Bayesian inference。</li>
<li>results: 对于10万个人工生成的学习曲线，LC-PFN可以更准确地 aproximate posterior predictive distribution，并且比 MCMC 快上万倍；对于20000个真实学习曲线，LC-PFN可以达到竞争性的性能。<details>
<summary>Abstract</summary>
Learning curve extrapolation aims to predict model performance in later epochs of training, based on the performance in earlier epochs. In this work, we argue that, while the inherent uncertainty in the extrapolation of learning curves warrants a Bayesian approach, existing methods are (i) overly restrictive, and/or (ii) computationally expensive. We describe the first application of prior-data fitted neural networks (PFNs) in this context. A PFN is a transformer, pre-trained on data generated from a prior, to perform approximate Bayesian inference in a single forward pass. We propose LC-PFN, a PFN trained to extrapolate 10 million artificial right-censored learning curves generated from a parametric prior proposed in prior art using MCMC. We demonstrate that LC-PFN can approximate the posterior predictive distribution more accurately than MCMC, while being over 10 000 times faster. We also show that the same LC-PFN achieves competitive performance extrapolating a total of 20 000 real learning curves from four learning curve benchmarks (LCBench, NAS-Bench-201, Taskset, and PD1) that stem from training a wide range of model architectures (MLPs, CNNs, RNNs, and Transformers) on 53 different datasets with varying input modalities (tabular, image, text, and protein data). Finally, we investigate its potential in the context of model selection and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6x speed-ups on 45 of these datasets, at virtually no overhead.
</details>
<details>
<summary>摘要</summary>
学习曲线拟合目标是预测训练过程中后期模型表现，基于早期表现的预测。在这项工作中，我们认为，由于拟合学习曲线的不确定性，应采用 bayesian 方法。然而，现有方法存在以下两点问题：一是过于限制性，二是计算成本高。我们介绍了在这个上下文中的首次应用先进数据适应神经网络（PFN）。PFN 是一种基于先进数据生成的 transformer，用于在单个前进 pass 中进行 Approximate Bayesian Inference。我们提出了基于先进数据生成的 10 万个人工受限学习曲线（LC-PFN），用于预测 MCMC 生成的参数 posterior 分布。我们证明了 LC-PFN 可以更准确地预测 posterior 分布，而且比 MCMC 快上万分之一。此外，我们还证明了同一 LC-PFN 可以在四个学习曲线 benchmark 上 extrapolate 20 万个真实的学习曲线，来自训练各种模型结构（MLPs、CNNs、RNNs 和 Transformers）和 53 个不同的数据集（标量、图像、文本和蛋白质数据）。最后，我们研究了其在模型选择方面的潜在应用，并发现一个简单的 LC-PFN 基于的预测早期停止 criterion 可以在 45 个数据集上获得 2 - 6 倍的速度增加，而且几乎没有额外成本。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Impact-of-Companies-on-AI-Research-Based-on-Publications"><a href="#Analyzing-the-Impact-of-Companies-on-AI-Research-Based-on-Publications" class="headerlink" title="Analyzing the Impact of Companies on AI Research Based on Publications"></a>Analyzing the Impact of Companies on AI Research Based on Publications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20444">http://arxiv.org/abs/2310.20444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LazaTabax/AI-Impact-Scientometrics">https://github.com/LazaTabax/AI-Impact-Scientometrics</a></li>
<li>paper_authors: Michael Färber, Lazaros Tampakis</li>
<li>for: 本研究的目的是探讨AI研究中企业的影响，以及如何使这种影响变得可衡量。</li>
<li>methods: 研究者使用科学出版活动数据来比较大学和企业在AI领域的出版 activites，并发现了不同之处。</li>
<li>results: 研究发现，与企业合作撰写的AI论文在引用数量方面表现 significanly higher，并且在线上获得了更多的关注。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) is one of the most momentous technologies of our time. Thus, it is of major importance to know which stakeholders influence AI research. Besides researchers at universities and colleges, researchers in companies have hardly been considered in this context. In this article, we consider how the influence of companies on AI research can be made measurable on the basis of scientific publishing activities. We compare academic- and company-authored AI publications published in the last decade and use scientometric data from multiple scholarly databases to look for differences across these groups and to disclose the top contributing organizations. While the vast majority of publications is still produced by academia, we find that the citation count an individual publication receives is significantly higher when it is (co-)authored by a company. Furthermore, using a variety of altmetric indicators, we notice that publications with company participation receive considerably more attention online. Finally, we place our analysis results in a broader context and present targeted recommendations to safeguard a harmonious balance between academia and industry in the realm of AI research.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）是当今最重要的技术之一，因此了解AI研究的各类涉及者对其影响非常重要。在这篇文章中，我们将研究如何使AI研究中公司的影响可衡量，基于科学出版活动。我们比较了过去十年的学术机构和企业合作撰写的人工智能论文，使用多种学术数据库的科学ometrics数据来找到这些组织的差异。虽然大多数论文仍然由学术机构出版，但我们发现，与公司合作者合写的论文的引用数量较高。此外，使用多种Altmetric指标，我们发现在线关注量较高。最后，我们将分析结果置于更广阔的背景下，并提供特点化的建议，以保持学术和产业在人工智能研究中的和谐协作。
</details></li>
</ul>
<hr>
<h2 id="Ontologies-for-Models-and-Algorithms-in-Applied-Mathematics-and-Related-Disciplines"><a href="#Ontologies-for-Models-and-Algorithms-in-Applied-Mathematics-and-Related-Disciplines" class="headerlink" title="Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines"></a>Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20443">http://arxiv.org/abs/2310.20443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Björn Schembera, Frank Wübbeling, Hendrik Kleikamp, Christine Biedinger, Jochen Fiedler, Marco Reidelbach, Aurela Shehu, Burkhard Schmidt, Thomas Koprucki, Dorothea Iglezakis, Dominik Göddeke</li>
<li>for: 本研究旨在提高数学研究数据的可访问性和共享性，通过semantic技术和数学基础的文档来增强数学研究数据的搜索和理解能力。</li>
<li>methods: 本研究使用了ontology和知识图来描述数学研究数据的结构和意义，并将数学模型和数学算法的解释和实现嵌入到了ontology中。</li>
<li>results: 通过使用ontology和知识图，可以准确地描述数学研究数据的结构和意义，提高了数学研究数据的可访问性和共享性。例如，通过使用ontology来描述微型扰动分析的数学模型和算法，可以增强对数学研究数据的理解和应用。<details>
<summary>Abstract</summary>
In applied mathematics and related disciplines, the modeling-simulation-optimization workflow is a prominent scheme, with mathematical models and numerical algorithms playing a crucial role. For these types of mathematical research data, the Mathematical Research Data Initiative has developed, merged and implemented ontologies and knowledge graphs. This contributes to making mathematical research data FAIR by introducing semantic technology and documenting the mathematical foundations accordingly. Using the concrete example of microfracture analysis of porous media, it is shown how the knowledge of the underlying mathematical model and the corresponding numerical algorithms for its solution can be represented by the ontologies.
</details>
<details>
<summary>摘要</summary>
在应用数学和相关领域，模拟优化工作流程是一种常见的方案，数学模型和数值算法在这些数学研究数据中扮演着关键的角色。为这类数学研究数据，数学研究数据Initative已经开发、合并并实施ontologies和知识图。这有助于使数学研究数据变得FAIR，通过引入语义技术并记录数学基础 accordingly。使用微裂变分析的porous media为例，可以通过ontologies来表示数学模型的知识和相应的数值算法的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Raising-the-ClaSS-of-Streaming-Time-Series-Segmentation"><a href="#Raising-the-ClaSS-of-Streaming-Time-Series-Segmentation" class="headerlink" title="Raising the ClaSS of Streaming Time Series Segmentation"></a>Raising the ClaSS of Streaming Time Series Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20431">http://arxiv.org/abs/2310.20431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ermshaua/classification-score-stream">https://github.com/ermshaua/classification-score-stream</a></li>
<li>paper_authors: Arik Ermshaus, Patrick Schäfer, Ulf Leser</li>
<li>for: 这篇论文的主要目标是提出一种高效精度的流处理时间序列分 segmentation（STSS）算法，用于处理高频数字测量结果，以捕捉人、动物、工业、商业和自然过程中的变化。</li>
<li>methods: 该算法使用自我超vised时间序列分类来评估分区Homogeneity，并应用统计测试检测变化点（CPs）。</li>
<li>results: 对两个大量数据集和六个实际数据存档进行实验评估，发现ClaSS比八种现有的竞争者更加精度，其空间和时间复杂度独立于分区大小，仅与滑动窗口大小有关。 ClaSS还被实现为Apache Flink流处理引擎中的窗口运算符，其吞吐量为538个数据点每秒。<details>
<summary>Abstract</summary>
Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, we found ClaSS to be significantly more precise than eight state-of-the-art competitors. Its space and time complexity is independent of segment sizes and linear only in the sliding window size. We also provide ClaSS as a window operator with an average throughput of 538 data points per second for the Apache Flink streaming engine.
</details>
<details>
<summary>摘要</summary>
今天的普遍存在的传感器 emit高频流量数字测量值，这些测量值反映人类、动物、工业、商业和自然过程的性质。这些过程的变化，例如由外部事件或内部状态变化引起的变化，将在记录的信号中 manifest。 streaming time series segmentation (STSS) 任务是将流量分成 consecutive 变量大小的分段，这些分段对于观察过程或实体的状态具有相应性。分段操作自身必须能够与输入信号频率相应。我们引入 ClaSS，一种新的、高效、高准确的算法 для STSS。 ClaSS 使用自我超级时间序分类来评估可能的分段的一致性，并应用统计测试来检测显著的变化点 (CP)。在我们使用两个大的标准benchmark和六个实际数据存档进行实验evaluation中，我们发现 ClaSS 与八种当前状态的竞争者相比，显著更精确。其空间和时间复杂度是 independetnt of segment size和linear只在滑动窗口大小。我们还提供了 ClaSS 作为窗口运算符，其均匀读取速率为 Apache Flink 流处理引擎中的538个数据点每秒。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-for-Multi-View-Visuomotor-Systems"><a href="#Meta-Learning-for-Multi-View-Visuomotor-Systems" class="headerlink" title="Meta Learning for Multi-View Visuomotor Systems"></a>Meta Learning for Multi-View Visuomotor Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20414">http://arxiv.org/abs/2310.20414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benji Alwis, Nick Pears, Pengcheng Liu</li>
<li>for: 这篇论文旨在简化多视角动作系统的应用，以适应不同的摄像头配置。</li>
<li>methods: 本篇论文使用meta-学习精确地调整感知网络，保持政策网络不变。</li>
<li>results: 实验结果显示，这种方法可以实现快速适应，对于不同的摄像头配置，需要训练集数量有所减少。<details>
<summary>Abstract</summary>
This paper introduces a new approach for quickly adapting a multi-view visuomotor system for robots to varying camera configurations from the baseline setup. It utilises meta-learning to fine-tune the perceptual network while keeping the policy network fixed. Experimental results demonstrate a significant reduction in the number of new training episodes needed to attain baseline performance.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文提出了一种新的方法，用于快速适应多视图视motor系统 Robot 到不同的摄像头配置从基线设置。它利用meta-学习来精细化感知网络，保持政策网络不变。实验结果显示，可以快速适应新的摄像头配置，并达到基线性能。
</details></li>
</ul>
<hr>
<h2 id="Multi-Base-Station-Cooperative-Sensing-with-AI-Aided-Tracking"><a href="#Multi-Base-Station-Cooperative-Sensing-with-AI-Aided-Tracking" class="headerlink" title="Multi-Base Station Cooperative Sensing with AI-Aided Tracking"></a>Multi-Base Station Cooperative Sensing with AI-Aided Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20403">http://arxiv.org/abs/2310.20403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elia Favarelli, Elisabetta Matricardi, Lorenzo Pucci, Enrico Paolini, Wen Xu, Andrea Giorgetti</li>
<li>for: 这个论文研究了一种共同感知和通信（JSC）网络，该网络由多个基站（BS）合作，通过协调中心（FC）交换感知环境信息，同时与一组用户设备（UE）建立通信链接。</li>
<li>methods: 每个BS在网络中运行为单STATIC射频系统，实现了全面扫描监测区域，生成范围角图，提供监测区域中目标位置信息。图像被FC进行融合。然后，使用卷积神经网络（CNN）推断目标类别，例如人行或车辆，并将其信息用adaptive clustering算法组织更有效地。最后，采用潘德尔过滤器（PHD）和多 Bernoulli 混合过滤器（MBM）来跟踪目标。</li>
<li>results: numerical results表明，我们的框架可以提供很好的感知性能，实现了优化的子模式分配（OSPA）在60 cm之下，同时保持对UE的通信服务的减少，在10%-20%之间。此外，我们还研究了BS参与感知的数量的影响，并发现在specific case study中，3BS可以保持地理位置错误在1 m以下。<details>
<summary>Abstract</summary>
In this work, we investigate the performance of a joint sensing and communication (JSC) network consisting of multiple base stations (BSs) that cooperate through a fusion center (FC) to exchange information about the sensed environment while concurrently establishing communication links with a set of user equipments (UEs). Each BS within the network operates as a monostatic radar system, enabling comprehensive scanning of the monitored area and generating range-angle maps that provide information regarding the position of a group of heterogeneous objects. The acquired maps are subsequently fused in the FC. Then, a convolutional neural network (CNN) is employed to infer the category of the targets, e.g., pedestrians or vehicles, and such information is exploited by an adaptive clustering algorithm to group the detections originating from the same target more effectively. Finally, two multi-target tracking algorithms, the probability hypothesis density (PHD) filter and multi-Bernoulli mixture (MBM) filter, are applied to estimate the state of the targets. Numerical results demonstrated that our framework could provide remarkable sensing performance, achieving an optimal sub-pattern assignment (OSPA) less than 60 cm, while keeping communication services to UEs with a reduction of the communication capacity in the order of 10% to 20%. The impact of the number of BSs engaged in sensing is also examined, and we show that in the specific case study, 3 BSs ensure a localization error below 1 m.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了一个共同探测和通信（JSC）网络，该网络包括多个基站（BS），通过协同中心（FC）来交换探测环境中的信息，同时与一组用户设备（UE）建立通信链接。每个BS在网络中 acted as a monostatic radar系统，可以完整扫描监测区域，生成距离角度图，提供目标位置信息。获得的图像被后续归一化神经网络（CNN）进行推理，并使用adaptive clustering算法将探测起始于同一个目标 grouped more effectively.最后，我们应用了两种多目标跟踪算法，概率 Hypothesis Density（PHD）筛选器和多 Bernoulli Mixture（MBM）筛选器，来估计目标的状态。numerical results表明，我们的框架可以提供很好的探测性能，达到优化子Pattern Assignment（OSPA）下than 60 cm，并在对UE的通信服务中减少了10%到20%的通信容量。我们还研究了BS参与探测的数量的影响，并显示在特定的案例研究中，3个BS可以保持Localization error below 1 m。
</details></li>
</ul>
<hr>
<h2 id="Utilitarian-Algorithm-Configuration"><a href="#Utilitarian-Algorithm-Configuration" class="headerlink" title="Utilitarian Algorithm Configuration"></a>Utilitarian Algorithm Configuration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20401">http://arxiv.org/abs/2310.20401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drgrhm/utilitarian-ac">https://github.com/drgrhm/utilitarian-ac</a></li>
<li>paper_authors: Devon R. Graham, Kevin Leyton-Brown, Tim Roughgarden</li>
<li>for: 这 paper 的目的是为了 Maximize the utility provided to their end users while also offering theoretical guarantees about performance.</li>
<li>methods: 这 paper 使用的方法是 configuring heuristic algorithms to maximize utility, while offering theoretical guarantees about performance.</li>
<li>results: 这 paper 的结果是提供了一种 theoretically sound 的配置方法，可以实现 empirical bounds on a configuration’s performance, while also demonstrating their performance empirically.<details>
<summary>Abstract</summary>
We present the first nontrivial procedure for configuring heuristic algorithms to maximize the utility provided to their end users while also offering theoretical guarantees about performance. Existing procedures seek configurations that minimize expected runtime. However, very recent theoretical work argues that expected runtime minimization fails to capture algorithm designers' preferences. Here we show that the utilitarian objective also confers significant algorithmic benefits. Intuitively, this is because mean runtime is dominated by extremely long runs even when they are incredibly rare; indeed, even when an algorithm never gives rise to such long runs, configuration procedures that provably minimize mean runtime must perform a huge number of experiments to demonstrate this fact. In contrast, utility is bounded and monotonically decreasing in runtime, allowing for meaningful empirical bounds on a configuration's performance. This paper builds on this idea to describe effective and theoretically sound configuration procedures. We prove upper bounds on the runtime of these procedures that are similar to theoretical lower bounds, while also demonstrating their performance empirically.
</details>
<details>
<summary>摘要</summary>
我们提出了第一个非负担的程序，用于配置假设运算法以最大化它们的终端用户所提供的用途，同时提供理论上的保证。现有的程序则寻找配置可以最小化预期的执行时间。然而，最近的理论工作表明，预期执行时间最小化不能捕捉算法设计师的偏好。我们展示了Utilitarian目标也具有重要的算法优点。实际上，平均执行时间被极端长的执行所控制，甚至当算法从不会产生这些极端长执行时，配置程序仍需进行大量的实验来证明这一点。相比之下，用途是有界的和下降的，可以提供有意义的实验上的上限。这篇论文基于这个想法，描述了有效且理论上正确的配置程序。我们证明了这些程序的执行时间的上限，与理论上的下限类似，同时也证明了它们的实验性能。
</details></li>
</ul>
<hr>
<h2 id="Do-large-language-models-solve-verbal-analogies-like-children-do"><a href="#Do-large-language-models-solve-verbal-analogies-like-children-do" class="headerlink" title="Do large language models solve verbal analogies like children do?"></a>Do large language models solve verbal analogies like children do?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20384">http://arxiv.org/abs/2310.20384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cstevenson-uva/verbal_analogies_kids_vs_llms">https://github.com/cstevenson-uva/verbal_analogies_kids_vs_llms</a></li>
<li>paper_authors: Claire E. Stevenson, Mathilde ter Veen, Rochelle Choenni, Han L. J. van der Maas, Ekaterina Shutova</li>
<li>for:  investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do.</li>
<li>methods:  used verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 yearolds from the Netherlands solved 622 analogies in Dutch.</li>
<li>results:  the six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when controlling for associative processes, each model’s performance level drops 1-2 years.<details>
<summary>Abstract</summary>
Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associative processes often underlie correctly solved analogies. We conclude that the LLMs we tested indeed tend to solve verbal analogies by association with C like children do.
</details>
<details>
<summary>摘要</summary>
人类认知中的比喻是非常重要的。大人们解决类似于“马属于牧场如果鸡属于...？”的比喻问题，通常是通过映射关系（如“保持在”）并回答“鸡巢”。然而，儿童们经常使用关联，例如回答“蛋”。这篇论文研究了大型自然语言模型（LLMs）是否使用关联来解决A:B::C:?的语言比喻问题。我们使用来自在线适应学习环境中的622个荷兰7-12岁儿童解决的语言比喻，并测试了6个荷兰单语和多语言LLMs。结果显示，这些模型在与儿童的性能水平相当，但当我们控制了相关的过程时，每个模型的性能均下降1-2年级。进一步的实验表明，关联过程经常是 correctly solved analogies 的基础。我们结论认为，我们测试的LLMs确实通过关联来解决语言比喻问题，类似于儿童一样。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-GPT-4V’s-Multimodal-Capabilities-in-Medical-Imaging"><a href="#A-Comprehensive-Study-of-GPT-4V’s-Multimodal-Capabilities-in-Medical-Imaging" class="headerlink" title="A Comprehensive Study of GPT-4V’s Multimodal Capabilities in Medical Imaging"></a>A Comprehensive Study of GPT-4V’s Multimodal Capabilities in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20381">http://arxiv.org/abs/2310.20381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, Luping Zhou</li>
<li>for:  This paper evaluates the capabilities of GPT-4V in medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding.</li>
<li>methods:  The paper uses publicly available benchmarks to evaluate GPT-4V’s performance in these tasks, and the authors provide a comprehensive analysis of its strengths and limitations.</li>
<li>results:  GPT-4V demonstrates potential in generating descriptive reports for chest X-ray images, but its performance on certain evaluation metrics is lacking. It also shows promise in recognizing bounding boxes, but its precision is not high enough to identify specific medical organs and signs. The paper highlights the need for targeted refinements to fully unlock GPT-4V’s capabilities in the medical imaging domain.<details>
<summary>Abstract</summary>
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of more semantically robust assessment methods. In the field of Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding boxes, but its precision is lacking, especially in identifying specific medical organs and signs. Our evaluation underscores the significant potential of GPT-4V in the medical imaging domain, while also emphasizing the need for targeted refinements to fully unlock its capabilities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Machine-Learning-Based-Framework-for-Clustering-Residential-Electricity-Load-Profiles-to-Enhance-Demand-Response-Programs"><a href="#A-Machine-Learning-Based-Framework-for-Clustering-Residential-Electricity-Load-Profiles-to-Enhance-Demand-Response-Programs" class="headerlink" title="A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs"></a>A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20367">http://arxiv.org/abs/2310.20367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilis Michalakopoulos, Elissaios Sarmas, Ioannis Papias, Panagiotis Skaloumpakas, Vangelis Marinakis, Haris Doukas</li>
<li>for: 本研究旨在提供一种优化的加载 profiling 方法，以便更好地分析家庭每日能源消耗模式，特别是在应用 Demand Response (DR) 领域。</li>
<li>methods: 本研究使用了四种常用的聚类算法，包括 K-means、K-medoids、 Hierarchical Agglomerative Clustering 和 Density-based Spatial Clustering，并通过实际案例研究了这些算法的效果。</li>
<li>results: 研究结果显示，使用 K-means 算法可以得到最佳的聚类结果，但是对于这个案例来说，使用 Hierarchical Agglomerative Clustering 算法可以更好地捕捉到家庭的内部不同性。而且，通过使用 Explainable AI (xAI) 技术，我们可以提高方法的解释性和可读性。<details>
<summary>Abstract</summary>
Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm,leveraging Explainable AI (xAI) to enhance the interpretability of our solution. According to the clustering algorithm analysis the optimal number of clusters for this case is seven. Despite that, our methodology shows that two of the clusters, almost 10\% of the dataset, exhibit significant internal dissimilarity and thus it splits them even further to create nine clusters in total. The scalability and versatility of our solution makes it an ideal choice for power utility companies aiming to segment their users for creating more targeted Demand Response programs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发现形状来自智能仪表数据的分析是常用于日常能源消耗模式分析，特别在需求回应（DR）应用场景中。然而， Identifying the most suitable consumer clusters with similar consumption behaviors is one of the biggest challenges in this field. 在这篇论文中，我们提出了一种基于机器学习的新框架，通过实际案例研究，使用伦敦约5000户数据，以实现最佳的加载规划。我们运用了四种常用的划分算法，包括K-means、K-medoids、层次归一整合划分和密度空间划分。通过实际分析和多种评价指标，我们评估了这些算法。然后，我们将问题重新定义为一个 probabilistic 分类问题，通过类似于划分算法的类ifier来模拟，并使用 Explainable AI（xAI）来增强解释性。根据划分算法分析，这个案例的最佳数量为7个划分。然而，我们的方法显示，这7个划分中的2个划分（约10%的数据）具有显著的内部不一致，因此我们将其进一步分割，创造了9个划分。我们的解决方案具有扩展性和多样性，使得它成为了电力公司为实现更加准确的需求回应计划而选择的理想选择。
</details></li>
</ul>
<hr>
<h2 id="Mathematical-Introduction-to-Deep-Learning-Methods-Implementations-and-Theory"><a href="#Mathematical-Introduction-to-Deep-Learning-Methods-Implementations-and-Theory" class="headerlink" title="Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory"></a>Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20360">http://arxiv.org/abs/2310.20360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/introdeeplearning/book">https://github.com/introdeeplearning/book</a></li>
<li>paper_authors: Arnulf Jentzen, Benno Kuckuck, Philippe von Wurstemberger</li>
<li>for: This book provides an introduction to deep learning algorithms, covering essential components such as ANN architectures and optimization algorithms, as well as theoretical aspects like approximation capacities and generalization errors.</li>
<li>methods: The book reviews various deep learning algorithms, including fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization. It also covers optimization algorithms such as basic SGD, accelerated methods, and adaptive methods.</li>
<li>results: The book provides a solid foundation for students and scientists who are new to deep learning, and offers a firmer mathematical understanding of deep learning objects and methods for practitioners. Additionally, it reviews deep learning approximation methods for PDEs, including physics-informed neural networks (PINNs) and deep Galerkin methods.<details>
<summary>Abstract</summary>
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.
</details>
<details>
<summary>摘要</summary>
这本书的目标是为深度学习算法提供一个入门。我们详细介绍了深度学习算法的重要组成部分，包括不同的人工神经网络架构（如全连接Feedforward ANNs、卷积 ANNs、回归 ANNs、差分 ANNs 和批量 норmalization ANNs）以及不同的优化算法（如基本的随机梯度下降法、加速方法和自适应方法）。我们还覆盖了深度学习算法的一些理论方面，包括人工神经网络的近似能力（包括神经网络 Calculus）、优化理论（包括库德雅-{\L}ojasiewicz不等式）和泛化误差。在书的最后部分，我们也详细介绍了一些深度学习方法用于解析 partial differential equations（PDEs），包括物理学信息学神经网络（PINNs）和深度 Galerkin 方法。我们希望这本书能为没有深度学习背景的学生和科学家提供坚实的基础，同时也能为已有深度学习背景的实践者提供更加坚实的数学理解。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Spatial-Awareness-Capability-of-Multi-Modal-Large-Language-Model"><a href="#Enhancing-the-Spatial-Awareness-Capability-of-Multi-Modal-Large-Language-Model" class="headerlink" title="Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model"></a>Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20357">http://arxiv.org/abs/2310.20357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Zhao, Zhenyu Li, Zhi Jin, Feng Zhang, Haiyan Zhao, Chengfeng Dou, Zhengwei Tao, Xinhai Xu, Donghong Liu</li>
<li>for: 该论文旨在提高多modal大语言模型（MLLM）的空间意识能力，以满足现代应用需求。</li>
<li>methods: 该论文提出使用更精确的空间位置信息来引导MLLM提供更加准确的回答。 Specifically, 该论文使用算法获取多modal数据中对象之间的几何空间信息和场景图来提高MLLM的空间意识能力。</li>
<li>results: 实验结果表明，该方法可以有效地提高MLLM在空间意识相关任务上的表现。<details>
<summary>Abstract</summary>
The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric spatial information and scene details of objects involved in the query. Subsequently, based on this information, we direct MLLM to address spatial awareness-related queries posed by the user. Extensive experiments were conducted in benchmarks such as MME, MM-Vet, and other multi-modal large language models. The experimental results thoroughly confirm the efficacy of the proposed method in enhancing the spatial awareness tasks and associated tasks of MLLM.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文翻译，不同于标准中文） Multi-Modal Large Language Model (MLLM) 是 Large Language Model (LLM) 的扩展，具有接收和理解多Modal数据的能力。MLLM 的空间意识是其关键能力之一，包括理解物体之间和物体与场景区域之间的空间关系。自动驾驶、智能医疗、机器人、虚拟和扩展现实等行业强烈需要 MLLM 的空间意识能力。然而，目前 MLLM 的空间意识能力与人类需求之间存在显著的差距。为解决这一问题，本文提议使用更精准的物体间空间位置信息来导引 MLLM 更准确地回答用户关注的问题。Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric spatial information and scene details of objects involved in the query. Subsequently, based on this information, we direct MLLM to address spatial awareness-related queries posed by the user. We conducted extensive experiments in benchmarks such as MME, MM-Vet, and other multi-modal large language models. The experimental results thoroughly confirm the efficacy of the proposed method in enhancing the spatial awareness tasks and associated tasks of MLLM.
</details></li>
</ul>
<hr>
<h2 id="Muscle-volume-quantification-guiding-transformers-with-anatomical-priors"><a href="#Muscle-volume-quantification-guiding-transformers-with-anatomical-priors" class="headerlink" title="Muscle volume quantification: guiding transformers with anatomical priors"></a>Muscle volume quantification: guiding transformers with anatomical priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20355">http://arxiv.org/abs/2310.20355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louise Piecuch, Vanessa Gonzales Duque, Aurélie Sarcher, Enzo Hollville, Antoine Nordez, Giuseppe Rabita, Gaël Guilhem, Diana Mateus</li>
<li>for: 这个研究是为了提供一种自动标识股筋的方法，帮助运动科学和骨骼疾病追踪中的形态分析。</li>
<li>methods: 这篇研究使用了一种混合构架，结合了卷积构架和视觉trasformer构架，以便自动标识股筋。另外，这篇研究还添加了一个预测损失基于股筋邻里关系矩阵，以利用骨骼组织的关系来提高预测的精度。</li>
<li>results: 实验结果显示，这种混合构架可以从相对较小的数据库中训练出高精度的模型，而且运用骨骼组织的关系来加权预测，可以提高模型的预测精度。<details>
<summary>Abstract</summary>
Muscle volume is a useful quantitative biomarker in sports, but also for the follow-up of degenerative musculo-skelletal diseases. In addition to volume, other shape biomarkers can be extracted by segmenting the muscles of interest from medical images. Manual segmentation is still today the gold standard for such measurements despite being very time-consuming. We propose a method for automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance Images to assist such morphometric analysis. By their nature, the tissue of different muscles is undistinguishable when observed in MR Images. Thus, muscle segmentation algorithms cannot rely on appearance but only on contour cues. However, such contours are hard to detect and their thickness varies across subjects. To cope with the above challenges, we propose a segmentation approach based on a hybrid architecture, combining convolutional and visual transformer blocks. We investigate for the first time the behaviour of such hybrid architectures in the context of muscle segmentation for shape analysis. Considering the consistent anatomical muscle configuration, we rely on transformer blocks to capture the longrange relations between the muscles. To further exploit the anatomical priors, a second contribution of this work consists in adding a regularisation loss based on an adjacency matrix of plausible muscle neighbourhoods estimated from the training data. Our experimental results on a unique database of elite athletes show it is possible to train complex hybrid models from a relatively small database of large volumes, while the anatomical prior regularisation favours better predictions.
</details>
<details>
<summary>摘要</summary>
筋肉体积是运动领域中有用的量化生物标志，同时也适用于评估逐渐恶化的肌骨综合病。除了体积，还可以从医疗影像中提取其他形状生物标志。现今，手动分割仍然是评估这些测量的标准方法，尽管它很时间consuming。我们提议一种自动分割3D磁共振图像中18个下肢肌肉的方法，以助于这种形态分析。由于不同肌肉组织的组织结构是不可分辨的，因此肌肉分割算法不能仅仅基于观察的外观特征。然而，这些边缘是困难检测的，并且对不同主题者而言，边缘的厚度异常。为了解决以上挑战，我们提议一种混合架构，结合卷积和视觉转换块。我们在肌肉分割方面首次研究了这种混合架构的行为。由于肌肉的一致性结构，我们利用转换块来捕捉肌肉之间的长距离关系。此外，我们还添加了一种基于邻居矩阵的准确性损失，以利用训练数据中的 анатомиче priors。我们在一个独特的运动员数据库中进行了实验，并证明了可以从相对较小的数据库中训练复杂的混合模型，而且准确性增加。
</details></li>
</ul>
<hr>
<h2 id="Combining-Shape-Completion-and-Grasp-Prediction-for-Fast-and-Versatile-Grasping-with-a-Multi-Fingered-Hand"><a href="#Combining-Shape-Completion-and-Grasp-Prediction-for-Fast-and-Versatile-Grasping-with-a-Multi-Fingered-Hand" class="headerlink" title="Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand"></a>Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20350">http://arxiv.org/abs/2310.20350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Humt, Dominik Winkelbauer, Ulrich Hillenbrand, Berthold Bäuml</li>
<li>for: 本研究旨在解决助助器中夹取物体时的问题，特别是在部分可见和多指手部上进行多种夹取。</li>
<li>methods: 该研究提出了一种新的深度学习管道，包括一个基于单个深度图像的形态完成模块和基于预测物体形态的抓取预测器。形态完成网络基于VQDIF，并预测了空间占据值在任意查询点。抓取预测器使用了两stage的架构，首先使用了自然语言模型生成手势，然后对每个姿势进行了指关键点的回归。</li>
<li>results: 实验表明，该管道在物理机器人平台上成功地夹取了各种家用物品，基于单个视角的深度图像。整个管道快速，只需要约1秒完成物体形态预测（0.7秒）和生成1000个抓取（0.3秒）。<details>
<summary>Abstract</summary>
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful grasping of a wide range of household objects based on a depth image from a single viewpoint. The whole pipeline is fast, taking only about 1 s for completing the object's shape (0.7 s) and generating 1000 grasps (0.3 s).
</details>
<details>
<summary>摘要</summary>
握住无知物品是助手机器人中的一项非常有用的技能，但在这个通用设定下，这问题仍然是一个开问题，特别是在仅部分可观察和多指手臂中进行握住。我们提出了一个新的、快速和高精度的深度学习管线，包括基于单一深度图像的形状完成模组，以及基于预测物体形状的握住预测器。形状完成网络是基于VQDIF的，并预测在任意查询点的空间占用值。握住预测器使用我们的二阶段架构，首先使用autoregressive模型生成手势，然后对每个手势进行指骨配置的回归。实验在物理机器人平台上显示，成功握住了一系列家用物品，基于单一观点的深度图像。整个管线快速，只需0.7秒完成物体形状预测，以及0.3秒生成1000个握住。
</details></li>
</ul>
<hr>
<h2 id="Improving-Entropy-Based-Test-Time-Adaptation-from-a-Clustering-View"><a href="#Improving-Entropy-Based-Test-Time-Adaptation-from-a-Clustering-View" class="headerlink" title="Improving Entropy-Based Test-Time Adaptation from a Clustering View"></a>Improving Entropy-Based Test-Time Adaptation from a Clustering View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20327">http://arxiv.org/abs/2310.20327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoliang Lin, Hanjiang Lai, Yan Pan, Jian Yin</li>
<li>for: 本研究旨在解决预测数据和测试数据之间的频率差异问题，通过全部测试时间适应（TTA）技术来适应模型。</li>
<li>methods: 本研究提出了一种新的Entropy-Based TTA（EBTTA）方法的解释，即通过对测试样本进行分类来实现。这是一个迭代的算法，其中在分配步骤中，EBTTA模型的前进过程是对测试样本进行标注，而在更新步骤中，是通过分配的样本来更新模型。</li>
<li>results: 实验结果表明，我们的方法可以在多个数据集上实现稳定的改进。我们还发现了EBTTA方法的敏感性问题，包括初始分配、异常样本和批处理大小的影响，这些问题的解决可以提高EBTTA的性能。我们提出了一些改进方法，包括稳定的标注、重量调整和梯度积累。<details>
<summary>Abstract</summary>
Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA methods are sensitive to initial assignments, outliers, and batch size. This observation can guide us to put forward the improvement of EBTTA. We propose robust label assignment, weight adjustment, and gradient accumulation to alleviate the above problems. Experimental results demonstrate that our method can achieve consistent improvements on various datasets. Code is provided in the supplementary material.
</details>
<details>
<summary>摘要</summary>
域外适应是现实世界中的一个常见问题，训练数据和测试数据之间存在不同的数据分布。为解决这个问题，全部测试时适应（TTA）利用测试时遇到的无标签数据来适应模型。特别是基于Entropy的TTA（EBTTA）方法，通过减小测试样本中预测的熵来显示出色。在这篇论文中，我们提出了一新的视角，即EBTTA方法的解释，它可以看作是一种归一化过程。该过程包括两步：1）在分配步骤，EBTTA模型的前进过程是对测试样本进行标签分配; 2）在更新步骤，后进过程是通过分配的样本来更新模型。基于这种解释，我们可以更深入地理解EBTTA，并证明了熵损失会使largest probability增加。根据这种观察，我们提出了一些改进EBTTA的方法，包括 Robust label assignment、重量调整和Gradient accumulation，以解决初始分配、异常样本和批处理大小对EBTTA的敏感性问题。实验结果表明，我们的方法可以在多个 dataset 上实现一致性的改进。代码请参考辅助材料。
</details></li>
</ul>
<hr>
<h2 id="SemanticBoost-Elevating-Motion-Generation-with-Augmented-Textual-Cues"><a href="#SemanticBoost-Elevating-Motion-Generation-with-Augmented-Textual-Cues" class="headerlink" title="SemanticBoost: Elevating Motion Generation with Augmented Textual Cues"></a>SemanticBoost: Elevating Motion Generation with Augmented Textual Cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20323">http://arxiv.org/abs/2310.20323</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blackgold3/SemanticBoost">https://github.com/blackgold3/SemanticBoost</a></li>
<li>paper_authors: Xin He, Shaoli Huang, Xiaohang Zhan, Chao Wen, Ying Shan</li>
<li>For:	+ The paper aims to address the challenges of generating motions from intricate semantic descriptions in human motion capture datasets.	+ The paper presents a novel framework called SemanticBoost to improve the quality and consistency of generated motions.* Methods:	+ The framework consists of a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD).	+ The Semantic Enhancement module extracts supplementary semantics from motion data to enrich the dataset’s textual description.	+ The CAMD approach captures context information and aligns the generated motion with the given textual descriptions.* Results:	+ The proposed method outperforms auto-regressive-based techniques on the Humanml3D dataset, achieving cutting-edge performance while maintaining realistic and smooth motion generation quality.	+ The method is able to synthesize accurate orientational movements, combined motions based on specific body part descriptions, and motions generated from complex, extended sentences.<details>
<summary>Abstract</summary>
Current techniques face difficulties in generating motions from intricate semantic descriptions, primarily due to insufficient semantic annotations in datasets and weak contextual understanding. To address these issues, we present SemanticBoost, a novel framework that tackles both challenges simultaneously. Our framework comprises a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generating high-quality, semantically consistent motion sequences by effectively capturing context information and aligning the generated motion with the given textual descriptions. Distinct from existing methods, our approach can synthesize accurate orientational movements, combined motions based on specific body part descriptions, and motions generated from complex, extended sentences. Our experimental results demonstrate that SemanticBoost, as a diffusion-based method, outperforms auto-regressive-based techniques, achieving cutting-edge performance on the Humanml3D dataset while maintaining realistic and smooth motion generation quality.
</details>
<details>
<summary>摘要</summary>
当前技术遇到困难在基于复杂 semantic 描述生成动作，主要原因是数据集的 semantic 注解不够和Contextual 理解不强。为解决这些问题，我们提出 SemanticBoost 框架，同时解决这两个问题。我们的框架包括 Semantic Enhancement 模块和 Context-Attuned Motion Denoiser (CAMD)。Semantic Enhancement 模块从动作数据中提取补充 semantics，使 dataset 的文本描述更加丰富，确保文本和动作数据的准确对应，不需要依赖大型语言模型。而 CAMD 方法提供了一个涵盖全Solution для生成高质量、semantically 一致的动作序列，通过有效地捕捉Context信息和文本描述之间的对应关系，生成高质量的动作序列。与现有方法不同，我们的方法可以合成准确的方向运动、基于specific body part 描述的 combined 动作和从复杂、扩展的 sentence 中生成动作。我们的实验结果表明，SemanticBoost 作为一种diffusion-based 方法，在 Humanml3D 数据集上超越 auto-regressive-based 技术，实现了当今最高的性能，同时保持实际和平滑的动作生成质量。
</details></li>
</ul>
<hr>
<h2 id="Theory-of-Mind-in-Large-Language-Models-Examining-Performance-of-11-State-of-the-Art-models-vs-Children-Aged-7-10-on-Advanced-Tests"><a href="#Theory-of-Mind-in-Large-Language-Models-Examining-Performance-of-11-State-of-the-Art-models-vs-Children-Aged-7-10-on-Advanced-Tests" class="headerlink" title="Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests"></a>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20320">http://arxiv.org/abs/2310.20320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max J. van Duijn, Bram M. A. van Dijk, Tom Kouwenhoven, Werner de Valk, Marco R. Spruit, Peter van der Putten</li>
<li>for: 这篇论文是关于大自然语言模型（LLM）的认知能力如何归结的。</li>
<li>methods: 该论文使用了多种方法来测试LLM的认知能力，包括测试其能够理解非直接语言使用和回归意图等。</li>
<li>results: 研究发现，基于GPT家族的指导 LLM 在 ToM 任务中表现出色，常常也比儿童年龄7-10岁的儿童更好。基础 LLM 则在大多数情况下无法解决 ToM 任务，即使使用特殊的提示。研究人员认为，语言和 ToM 的演进和发展可能帮助解释了 instruciton-tuning 添加了什么：奖励合作交流，考虑到对方和情况。<details>
<summary>Abstract</summary>
To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.
</details>
<details>
<summary>摘要</summary>
Should we attribute cognitive abilities such as theory of mind (ToM) to large language models (LLMs)? We add to this emerging debate by testing 11 base- and instruction-tuned LLMs on ToM-related capabilities beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality. We also use newly rewritten versions of standardized tests to gauge LLMs' robustness, prompt for open-ended questions, and benchmark LLM performance against that of children aged 7-10 on the same tasks. Our findings show that instruction-tuned LLMs from the GPT family outperform other models and often also children, while base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may explain what instruction-tuning adds, as it rewards cooperative communication that takes into account interlocutor and context. We conclude by advocating for a nuanced perspective on ToM in LLMs.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Causal-Interpretation-of-Self-Attention-in-Pre-Trained-Transformers"><a href="#Causal-Interpretation-of-Self-Attention-in-Pre-Trained-Transformers" class="headerlink" title="Causal Interpretation of Self-Attention in Pre-Trained Transformers"></a>Causal Interpretation of Self-Attention in Pre-Trained Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20307">http://arxiv.org/abs/2310.20307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov</li>
<li>for: 本研究旨在解释Transformer神经网络中的自注意力是什么？</li>
<li>methods: 本研究使用自注意力作为一种结构方程模型来解释输入序列中的符号（Token）之间的关系。</li>
<li>results: 本研究发现，通过计算深度注意层中表示的偏好关系，可以学习输入序列中的 causal 结构，并且在存在潜在干扰因素的情况下仍然有效。<details>
<summary>Abstract</summary>
We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks: sentiment classification (NLP) and recommendation.
</details>
<details>
<summary>摘要</summary>
我们提出了 transformer 神经网络架构中 self-attention 的 causal 解释。我们解释 self-attention 为一种可以 estimate 输入序列符号（token）的 struc-tural equation model。这个 struc-tural equation model 可以被解释为输入序列的特定上下文中的 causal 结构。这种解释在干扰因素存在时仍然有效。我们通过计算深度 attended layer 中表示之间的偏相关性来确定输入符号之间的 conditional independence 关系。这使得可以使用现有的 constraint-based 算法学习输入序列的 causal 结构。在这种意义上，现有的预训练 transformer 可以用于零工作 causal-discovery。我们在两个任务中提供了 causal 解释：语 Sentiment 分类（NLP）和推荐。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Global-Food-Security-Empowering-Resilience-through-Integrated-AI-Foundation-Models-and-Data-Driven-Solutions"><a href="#Revolutionizing-Global-Food-Security-Empowering-Resilience-through-Integrated-AI-Foundation-Models-and-Data-Driven-Solutions" class="headerlink" title="Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions"></a>Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20301">http://arxiv.org/abs/2310.20301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed R. Shoaib, Heba M. Emara, Jun Zhao</li>
<li>for: 这个论文旨在探讨用AI基础模型在不同的食品安全应用中进行集成，以超越现有的深度学习和机器学习方法的限制。</li>
<li>methods: 该论文 investigate AI基础模型在农作物类型识别、农田地图、田野分界和农作物产量预测等领域的应用，并利用不同的数据类型，如多spectral镜像、气象数据、土壤特性、历史记录和高分辨率卫星镜像。</li>
<li>results: 该论文显示AI基础模型可以增强食品安全 iniciatives的准确预测、改善资源配置和支持 Informed Decision-making，并且这些模型作为一种可transformative force，为全球食品安全带来了重大的进步。<details>
<summary>Abstract</summary>
Food security, a global concern, necessitates precise and diverse data-driven solutions to address its multifaceted challenges. This paper explores the integration of AI foundation models across various food security applications, leveraging distinct data types, to overcome the limitations of current deep and machine learning methods. Specifically, we investigate their utilization in crop type mapping, cropland mapping, field delineation and crop yield prediction. By capitalizing on multispectral imagery, meteorological data, soil properties, historical records, and high-resolution satellite imagery, AI foundation models offer a versatile approach. The study demonstrates that AI foundation models enhance food security initiatives by providing accurate predictions, improving resource allocation, and supporting informed decision-making. These models serve as a transformative force in addressing global food security limitations, marking a significant leap toward a sustainable and secure food future.
</details>
<details>
<summary>摘要</summary>
食品安全问题是全球性的担忧，需要精准和多样化的数据驱动解决方案来 Address its 多方面的挑战。这篇论文探讨了基于 AI 基础模型的不同数据类型的 integrate 在不同的食品安全应用中，以超越现有的深度学习和机器学习方法的局限性。具体来说，我们调查了它们在作物类型地图、耕地地图、田间分界和作物产量预测中的应用。通过利用多spectral 影像、气象数据、土壤属性、历史记录和高分辨率卫星影像，AI 基础模型提供了一种多样化的方法。这项研究显示，AI 基础模型可以增强食品安全倡议，提供准确预测、改善资源分配和支持 Informed 决策。这些模型作为一种转型力量，帮助解决全球食品安全的限制，marking a significant leap toward a sustainable and secure food future.
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-and-Safe-Deep-Reinforcement-Learning-via-Reset-Deep-Ensemble-Agents"><a href="#Sample-Efficient-and-Safe-Deep-Reinforcement-Learning-via-Reset-Deep-Ensemble-Agents" class="headerlink" title="Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents"></a>Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20287">http://arxiv.org/abs/2310.20287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woojun Kim, Yongjae Shin, Jongeui Park, Youngchul Sung</li>
<li>for: 解决深度强化学习中的 primacy bias 问题，提高样本效率和安全性。</li>
<li>methods: 使用 periodic reset 方法，并将 ensemble learning 应用于深度RL agents。</li>
<li>results: 在不同的域中进行了诸多实验，并得到了高样本效率和安全性的数据。<details>
<summary>Abstract</summary>
Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numerical results show its effectiveness in high sample efficiency and safety considerations.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments, including those in the domain of safe RL. The results show that our method is effective in achieving high sample efficiency and safety considerations.
</details></li>
</ul>
<hr>
<h2 id="AutoMixer-for-Improved-Multivariate-Time-Series-Forecasting-on-BizITOps-Data"><a href="#AutoMixer-for-Improved-Multivariate-Time-Series-Forecasting-on-BizITOps-Data" class="headerlink" title="AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data"></a>AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20280">http://arxiv.org/abs/2310.20280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santosh Palaskar, Vijay Ekambaram, Arindam Jati, Neelamadhav Gantayat, Avirup Saha, Seema Nagar, Nam H. Nguyen, Pankaj Dayama, Renuka Sindhgatta, Prateeti Mohapatra, Harshit Kumar, Jayant Kalagnanam, Nandyala Hemachandra, Narayan Rangaraj</li>
<li>for: 提高企业业务过程效率和收益，预测业务键性指标（Biz-KPIs）以提供反应式纠正措施。</li>
<li>methods:  combining Biz-KPIs and IT event channels as multivariate time series data，采用AutoMixer模型，包括预处理和精度提高两个部分。</li>
<li>results:  comparing with现有多变量预测模型，AutoMixer模型可以提高Biz-KPIs预测精度（11-15%），提供更加准确的业务预测和反应式纠正措施。<details>
<summary>Abstract</summary>
The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSMixer for accurate forecasts and also generalizes well across several downstream tasks. Through detailed experiments and dashboard analytics, we show AutoMixer's capability to consistently improve the Biz-KPI's forecasting accuracy (by 11-15%) which directly translates to actionable business insights.
</details>
<details>
<summary>摘要</summary>
To address this, we propose AutoMixer, a time-series Foundation Model (FM) approach that leverages a novel technique of channel-compressed pretraining and finetuning workflows. AutoMixer combines an AutoEncoder for channel-compressed pretraining with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSMixer for accurate forecasts and also generalizes well across several downstream tasks.Through detailed experiments and dashboard analytics, we demonstrate that AutoMixer consistently improves the Biz-KPIs forecasting accuracy by 11-15%, which directly translates to actionable business insights.
</details></li>
</ul>
<hr>
<h2 id="Constructing-Sample-to-Class-Graph-for-Few-Shot-Class-Incremental-Learning"><a href="#Constructing-Sample-to-Class-Graph-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning"></a>Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20268">http://arxiv.org/abs/2310.20268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuyuan Hu, Jian Zhang, Fan Lyu, Linyan Li, Fenglei Xu</li>
<li>for: 这 paper 的目的是提出一种 few-shot class-incremental learning (FSCIL) 方法，以帮助机器学习模型在很少数据样本的情况下不断学习新的概念，而不会忘记过去的类。</li>
<li>methods: 这 paper 提出了一种 Sample-to-Class (S2C) 图学习方法，包括 Sample-level Graph Network (SGN) 和 Class-level Graph Network (CGN) 两部分。SGN 专门分析单个会话中的样本关系，以提取更加精细的类级特征。CGN 则建立了不同会话之间的类级特征之间的连接，以帮助提高 FSCIL 中的总体学习。</li>
<li>results:  experiments 表明， compared to baselines, 这 paper 的方法在三个popular benchmark dataset上 clearly outperforms，并在 FSCIL 中设置了新的州对数据。<details>
<summary>Abstract</summary>
Few-shot class-incremental learning (FSCIL) aims to build machine learning model that can continually learn new concepts from a few data samples, without forgetting knowledge of old classes.   The challenges of FSCIL lies in the limited data of new classes, which not only lead to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems. As proved in early studies, building sample relationships is beneficial for learning from few-shot samples. In this paper, we promote the idea to the incremental scenario, and propose a Sample-to-Class (S2C) graph learning method for FSCIL.   Specifically, we propose a Sample-level Graph Network (SGN) that focuses on analyzing sample relationships within a single session. This network helps aggregate similar samples, ultimately leading to the extraction of more refined class-level features.   Then, we present a Class-level Graph Network (CGN) that establishes connections across class-level features of both new and old classes. This network plays a crucial role in linking the knowledge between different sessions and helps improve overall learning in the FSCIL scenario. Moreover, we design a multi-stage strategy for training S2C model, which mitigates the training challenges posed by limited data in the incremental process.   The multi-stage training strategy is designed to build S2C graph from base to few-shot stages, and improve the capacity via an extra pseudo-incremental stage. Experiments on three popular benchmark datasets show that our method clearly outperforms the baselines and sets new state-of-the-art results in FSCIL.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了几步骤增加学习（Few-shot Class-incremental Learning，FSCIL），它的目标是建立一个机器学习模型，可以从几个数据样本中学习新的概念，而不会忘记旧的类别知识。这种情况下的挑战在于新类别的数据有限，这不仅会导致严重过滤问题，而且会导致著名的忘却问题。在这篇论文中，我们提出了将样本关系建立到增量情况中的想法，并提出了一个称为 Sample-to-Class（S2C）几步骤学习方法。具体来说，我们提出了一个称为 Sample-level Graph Network（SGN）的网络，它专注于一个Session中的样本关系分析。这个网络可以将相似的样本集合起来，以将更精确的类别特征提取出来。然后，我们提出了一个称为 Class-level Graph Network（CGN）的网络，它建立了不同Session中的类别特征之间的连接。这个网络在连接不同Session中的知识之间，对于FSCIL情况下的整体学习起到了关键的作用。此外，我们设计了一个多阶段训练策略，以解决增量过程中的训练挑战。这个多阶段训练策略是建立S2C几步骤网络从基础阶段到几步骤阶段，并通过额外的pseudo-增量阶段进行优化。实验结果显示，我们的方法明显超过基eline和设置新的州OF-THE-ART纪录在FSCIL情况下。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Average-Return-in-Markov-Decision-Processes"><a href="#Beyond-Average-Return-in-Markov-Decision-Processes" class="headerlink" title="Beyond Average Return in Markov Decision Processes"></a>Beyond Average Return in Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20266">http://arxiv.org/abs/2310.20266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre Marthe, Aurélien Garivier, Claire Vernade</li>
<li>for: 本研究探讨了Markov Decision Processes（MDP）中可以有效地计算和优化的奖励函数。</li>
<li>methods: 研究使用动态计划（DP）和分布式抽象学习（DistRL）两种方法来计算奖励函数。</li>
<li>results: 研究发现只有通用化均值可以被 preciselly 优化，而其他函数则可以在DistRL框架下 aproximately 计算。研究还提供了误差 bound 以及这种方法的潜在应用和局限性。<details>
<summary>Abstract</summary>
What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Markov决策过程中可以高效计算和优化的奖励功能有哪些？在无折扣 Setting下，动态规划（DP）只能有效处理某些类型的统计。我们总结这些类型的Characterization，并提供一个新的答案 для计划问题。有趣的是，我们证明只有通用均值可以高效地优化，即使在更广泛的分布式激励学习（DistRL）框架中。DistRL允许评估其他函数的估计，并提供误差 bound。我们讨论这种方法的潜力和局限性。这些结果对Markov决策过程的理论进行了进一步发展，特别是关注风险敏感策略。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-reverse-engineering-application-to-detergents-using-Raman-spectroscopy"><a href="#Artificial-Intelligence-for-reverse-engineering-application-to-detergents-using-Raman-spectroscopy" class="headerlink" title="Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy"></a>Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20254">http://arxiv.org/abs/2310.20254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Marote, Marie Martin, Anne Bonhomme, Pierre Lantéri, Yohann Clément</li>
<li>For: The paper is written for assessing the potential toxicity of new commercial products, specifically detergent products, and for identifying and quantifying their constituents.* Methods: The paper uses a combination of digital tools such as spectral databases, mixture databases, experimental design, chemometrics, and machine learning algorithms, as well as various sample preparation methods such as raw samples and diluted&#x2F;concentrated samples. The paper also employs Raman spectroscopy as an analytical technique.* Results: The paper reports the successful identification of the mixture’s constituents and an estimation of its composition using the combination of digital tools and Raman spectroscopy. The strategy is shown to be applicable in various matrices and can result in time savings for pollutant identification and contamination assessment in the industrial sector for product or raw material control, as well as for quality control purposes.Here’s the simplified Chinese version of the three key information points:* For: 这篇论文是为了评估新商品的潜在毒性，特别是洗剂产品，并确定它们的成分。* Methods: 这篇论文使用了数字工具，如spectral数据库、混合数据库、实验设计、化学ometrics和机器学习算法，以及不同的样本准备方法，如raw样本和减少&#x2F;增加样本。它还使用了Raman谱仪作为分析技术。* Results: 这篇论文报告了成功地确定混合物的成分和其比例，使用了这些数字工具和Raman谱仪。这种策略可以在不同的矿 Matrix中应用，可以降低污染评估和控制的时间，在工业 sector中用于产品或原材料控制、质量控制等目的。<details>
<summary>Abstract</summary>
The reverse engineering of a complex mixture, regardless of its nature, has become significant today. Being able to quickly assess the potential toxicity of new commercial products in relation to the environment presents a genuine analytical challenge. The development of digital tools (databases, chemometrics, machine learning, etc.) and analytical techniques (Raman spectroscopy, NIR spectroscopy, mass spectrometry, etc.) will allow for the identification of potential toxic molecules. In this article, we use the example of detergent products, whose composition can prove dangerous to humans or the environment, necessitating precise identification and quantification for quality control and regulation purposes. The combination of various digital tools (spectral database, mixture database, experimental design, Chemometrics / Machine Learning algorithm{\ldots}) together with different sample preparation methods (raw sample, or several concentrated / diluted samples) Raman spectroscopy, has enabled the identification of the mixture's constituents and an estimation of its composition. Implementing such strategies across different analytical tools can result in time savings for pollutant identification and contamination assessment in various matrices. This strategy is also applicable in the industrial sector for product or raw material control, as well as for quality control purposes.
</details>
<details>
<summary>摘要</summary>
today, reverse engineering of complex mixture has become increasingly important. quickly assessing the potential toxicity of new commercial products in relation to the environment presents a significant analytical challenge. with the development of digital tools (databases, chemometrics, machine learning, etc.) and analytical techniques (Raman spectroscopy, NIR spectroscopy, mass spectrometry, etc.), it is possible to identify potential toxic molecules.in this article, we use the example of detergent products, whose composition can be harmful to humans or the environment, and precise identification and quantification are necessary for quality control and regulation purposes. by combining various digital tools (spectral database, mixture database, experimental design, chemometrics/machine learning algorithm{\ldots}) with different sample preparation methods (raw sample, or several concentrated/diluted samples), we have been able to identify the constituents of the mixture and estimate its composition using Raman spectroscopy.implementing such strategies across different analytical tools can save time for pollutant identification and contamination assessment in various matrices. this strategy is also applicable in the industrial sector for product or raw material control, as well as for quality control purposes.
</details></li>
</ul>
<hr>
<h2 id="Diversified-Node-Sampling-based-Hierarchical-Transformer-Pooling-for-Graph-Representation-Learning"><a href="#Diversified-Node-Sampling-based-Hierarchical-Transformer-Pooling-for-Graph-Representation-Learning" class="headerlink" title="Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning"></a>Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20250">http://arxiv.org/abs/2310.20250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaichao Li, Jinsong Chen, John E. Hopcroft, Kun He</li>
<li>for: 本研究旨在提出一种基于Transformer的图Pooling方法（GTPool），用于有效地捕捉图中远程对应关系和多种图级任务的信息。</li>
<li>methods: GTPool方法使用了Transformer自注意机制来设计分数模块，用于评估节点的重要性，并采用了一种多样化采样方法（Roulette Wheel Sampling）来兼顾保留不同分数范围内的节点。</li>
<li>results: 对于11个 benchmark数据集，GTPool方法与现有的流行 graph pooling 方法进行比较，实验结果表明GTPool方法在图 classification 和图生成等多种图级任务中具有显著的优势。<details>
<summary>Abstract</summary>
Graph pooling methods have been widely used on downsampling graphs, achieving impressive results on multiple graph-level tasks like graph classification and graph generation. An important line called node dropping pooling aims at exploiting learnable scoring functions to drop nodes with comparatively lower significance scores. However, existing node dropping methods suffer from two limitations: (1) for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones; (2) pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both global context and local context into consideration, measuring the importance of nodes more comprehensively. GTPool further utilizes a diversified sampling method named Roulette Wheel Sampling (RWS) that is able to flexibly preserve nodes across different scoring intervals instead of only higher scoring nodes. In this way, GTPool could effectively obtain long-range information and select more representative nodes. Extensive experiments on 11 benchmark datasets demonstrate the superiority of GTPool over existing popular graph pooling methods.
</details>
<details>
<summary>摘要</summary>
“graph pooling方法在下减图时得到了广泛应用，并在多个图像任务上获得了优秀的结果，如图像分类和图像生成。一个重要的纵向参数called node dropping pooling aimsto 利用学习可能的分配函数来 Drop nodes with comparatively lower significance scores. 然而，现有的node dropping方法受到了两个限制：（1） for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones;（2） pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both global context and local context into consideration, measuring the importance of nodes more comprehensively. GTPool further utilizes a diversified sampling method named Roulette Wheel Sampling (RWS) that is able to flexibly preserve nodes across different scoring intervals instead of only higher scoring nodes. In this way, GTPool could effectively obtain long-range information and select more representative nodes. Extensive experiments on 11 benchmark datasets demonstrate the superiority of GTPool over existing popular graph pooling methods.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Breaking-Language-Barriers-in-Multilingual-Mathematical-Reasoning-Insights-and-Observations"><a href="#Breaking-Language-Barriers-in-Multilingual-Mathematical-Reasoning-Insights-and-Observations" class="headerlink" title="Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations"></a>Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20246">http://arxiv.org/abs/2310.20246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nuochenpku/MathOctopus">https://github.com/nuochenpku/MathOctopus</a></li>
<li>paper_authors: Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, Jia Li</li>
<li>for: 这篇论文的目的是探索和培养多语言数学逻辑模型（xMR），以增强其在多语言环境中的表现。</li>
<li>methods: 作者使用翻译来构建了首个多语言数学逻辑指导数据集MGSM8KInstruct，并提出了不同的训练策略来建立强大的xMR模型，名为MathOctopus。</li>
<li>results: 研究发现，MathOctopus模型在MGSM测试集上的准确率为47.6%，超过了ChatGPT的46.3%。此外，作者还发现了一些关键的观察和发现，如扩展拒绝采样策略在多语言上的有效性，以及使用多语言的同时练习对模型的多语言和单语言表现的提升。<details>
<summary>Abstract</summary>
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset.
</details>
<details>
<summary>摘要</summary>
现有研究主要集中于开发强大的语言学习模型（LLM），以便在单语言中提高数学逻辑能力，而几乎没有探讨在多语言上保持效果的问题。为了bridging这个差距，本文开始了在多语言上培养强大的多语言数学逻辑模型（xMR）。首先，通过翻译，我们建立了首个多语言数学逻辑指导数据集MGSM8KInstruct，涵盖了十种不同的语言，因此解决了在xMR任务中训练数据的缺乏问题。基于收集的数据集，我们提出了不同的训练策略，以建立强大的xMR LLMs，称为MathOctopus，与开源LLMs相比，表现出色，并在少量场景下超越ChatGPT。尤其是MathOctopus-13B的准确率达47.6%，超过ChatGPT 46.3% наMGSM测试集。除了惊人的结果外，我们还发现了一些关键的观察和发现，包括：（1）在多语言上扩展拒绝采样策略可以提高模型性能，尽管有限。（2）在多语言中使用平行 corpora进行数学监督精度提升（SFT）可以显著提高模型性能，同时也提高了单语言模型的性能。这表明，制作多语言 corpora是提高模型性能的重要策略，尤其在数学逻辑任务中。例如，MathOctopus-7B在GSM8K测试集上由42.2%提高到50.8%。
</details></li>
</ul>
<hr>
<h2 id="Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape"><a href="#Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape" class="headerlink" title="Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape"></a>Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20240">http://arxiv.org/abs/2310.20240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhao, Yijun Wang, Tianyu He, Lianying Yin, Jianxin Lin, Xin Jin</li>
<li>for: 这篇论文目的是提出一种新的框架，以便通过自然和精准的同步控制，实现基于语音输入的自然和生动的3D人脸动画。</li>
<li>methods: 这篇论文使用了一种新的框架，称为VividTalker，它将 facial animation 分解为头部pose和口部运动，并将它们编码成独立的权重空间中。然后，通过一种窗口基于的Transformer架构进行拟合，以生成自然和精准的3D人脸动画。</li>
<li>results: 对比之前的方法，VividTalker 能够实现更加自然和精准的3D人脸动画，并且可以控制头部pose和口部运动的同步。这些结果是基于广泛的量化和质量测试，以及一个新的3D数据集，该数据集包含了详细的3D人脸形状和语音内容。<details>
<summary>Abstract</summary>
The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement and encode them separately into discrete latent spaces. Then, these attributes are generated through an autoregressive process leveraging a window-based Transformer architecture. To augment the richness of 3D facial animation, we construct a new 3D dataset with detailed shapes and learn to synthesize facial details in line with speech content. Extensive quantitative and qualitative experiments demonstrate that VividTalker outperforms state-of-the-art methods, resulting in vivid and realistic speech-driven 3D facial animation.
</details>
<details>
<summary>摘要</summary>
创造生动的语音驱动3D人脸动画需要自然和精准的声音输入和表情同步。然而，现有的方法仍无法渲染具有灵活头姿和自然的面部细节（例如皱纹）。这一限制主要归结于两点：1）收集具有细节3D人脸形状的训练集是非常昂贵的。这种缺乏细节形状注释限制了模型学习表情动画的训练。2）与口语运动相比，头姿与语音内容之间的相关性较低，因此同时控制头姿和口语运动具有一定的难度。为解决这些挑战，我们介绍了VividTalker，一个新的框架，用于实现语音驱动3D人脸动画，具有灵活的头姿和自然的面部细节。我们显式分离了人脸动画为头姿和口语运动，并将它们分别编码到独立的抽象空间中。然后，我们通过窗口基于Transformer架构的自动生成过程来生成这些特征。为了增强3D人脸动画的丰富性，我们构建了一个新的3D数据集，包含细节 rich的形状，并学习用于同speech内容的Synthesize facial details。经过详细的量化和质量测试，我们发现VividTalker可以超越当前的方法，并实现生动、真实的语音驱动3D人脸动画。
</details></li>
</ul>
<hr>
<h2 id="VisPercep-A-Vision-Language-Approach-to-Enhance-Visual-Perception-for-People-with-Blindness-and-Low-Vision"><a href="#VisPercep-A-Vision-Language-Approach-to-Enhance-Visual-Perception-for-People-with-Blindness-and-Low-Vision" class="headerlink" title="VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision"></a>VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20225">http://arxiv.org/abs/2310.20225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Hao, Fan Yang, Hao Huang, Shuaihang Yuan, Sundeep Rangan, John-Ross Rizzo, Yao Wang, Yi Fang</li>
<li>for: 帮助人们视力弱致残（pBLV）在未知环境中快速和准确地认识和识别环境中的物体和障碍。</li>
<li>methods: 利用大量视语模型，对捕捉到的图像进行物体识别，并根据用户查询和识别结果生成对策BLV的详细描述和环境分析。</li>
<li>results: 经过实验 validate our approach可以准确地识别物体和环境，并为pBLV提供有用的描述和预警。<details>
<summary>Abstract</summary>
People with blindness and low vision (pBLV) encounter substantial challenges when it comes to comprehensive scene recognition and precise object identification in unfamiliar environments. Additionally, due to the vision loss, pBLV have difficulty in accessing and identifying potential tripping hazards on their own. In this paper, we present a pioneering approach that leverages a large vision-language model to enhance visual perception for pBLV, offering detailed and comprehensive descriptions of the surrounding environments and providing warnings about the potential risks. Our method begins by leveraging a large image tagging model (i.e., Recognize Anything (RAM)) to identify all common objects present in the captured images. The recognition results and user query are then integrated into a prompt, tailored specifically for pBLV using prompt engineering. By combining the prompt and input image, a large vision-language model (i.e., InstructBLIP) generates detailed and comprehensive descriptions of the environment and identifies potential risks in the environment by analyzing the environmental objects and scenes, relevant to the prompt. We evaluate our approach through experiments conducted on both indoor and outdoor datasets. Our results demonstrate that our method is able to recognize objects accurately and provide insightful descriptions and analysis of the environment for pBLV.
</details>
<details>
<summary>摘要</summary>
人们 WITH 视力障碍和低视力 (pBLV) 在不熟悉的环境中面临重大挑战，包括景象认知和物体识别。此外，由于视力损伤，pBLV 有DifficultyAccessing和识别障碍物。在这篇论文中，我们提出了一种创新的方法，利用大型视力语言模型来增强pBLV的视觉认知，提供细致和全面的环境描述，并对环境中的障碍物进行警告。我们的方法从大量图像标记模型（Recognize Anything (RAM)）中获取所有常见的图像，然后将认知结果和用户查询 integrate into a prompt，特制 для pBLV。通过将Prompt和输入图像结合使用，一大型视力语言模型（InstructBLIP）生成了细致和全面的环境描述，并对环境中的障碍物进行分析，根据提交的环境对象和场景进行分析。我们通过对indoor和outdoor数据集进行实验，证明了我们的方法可以准确地识别物体并为pBLV提供有价值的环境描述和障碍物警告。
</details></li>
</ul>
<hr>
<h2 id="Choose-A-Table-Tensor-Dirichlet-Process-Multinomial-Mixture-Model-with-Graphs-for-Passenger-Trajectory-Clustering"><a href="#Choose-A-Table-Tensor-Dirichlet-Process-Multinomial-Mixture-Model-with-Graphs-for-Passenger-Trajectory-Clustering" class="headerlink" title="Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering"></a>Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20224">http://arxiv.org/abs/2310.20224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Li, Hao Yan, Chen Zhang, Lijun Sun, Wolfgang Ketter, Fugee Tsung</li>
<li>for:  passenger clustering based on trajectory records to improve transportation services</li>
<li>methods:  tensor Dirichlet Process Multinomial Mixture model with graphs and collapsed Gibbs sampling method</li>
<li>results:  automatic determination of cluster number and improved cluster quality measured by within-cluster compactness and cross-cluster separateness<details>
<summary>Abstract</summary>
Passenger clustering based on trajectory records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, including multiple trips within each passenger and multi-dimensional information about each trip. Furthermore, existing approaches rely on an accurate specification of the clustering number to start. Finally, existing methods do not consider spatial semantic graphs such as geographical proximity and functional similarity between the locations. In this paper, we propose a novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can preserve the hierarchical structure of the multi-dimensional trip information and cluster them in a unified one-step manner with the ability to determine the number of clusters automatically. The spatial graphs are utilized in community detection to link the semantic neighbors. We further propose a tensor version of Collapsed Gibbs Sampling method with a minimum cluster size requirement. A case study based on Hong Kong metro passenger data is conducted to demonstrate the automatic process of cluster amount evolution and better cluster quality measured by within-cluster compactness and cross-cluster separateness. The code is available at https://github.com/bonaldli/TensorDPMM-G.
</details>
<details>
<summary>摘要</summary>
乘客分群基于旅程记录是交通运营商必备的。然而，现有方法无法轻松地分群乘客，因为乘客旅程信息具有层次结构，包括每位乘客内部多个旅程和多维信息。此外，现有方法需要准确指定分群数量开始。最重要的是，现有方法不考虑地理Semantic graphs such as geographical proximity和功能相似性 междуLocation。在这篇论文中，我们提出了一种新的tensor Dirichlet Process Multinomial Mixture model with graphs，可以保持多维旅程信息的层次结构并将其分组到一个一步验证中，同时具有自动确定分群数量的能力。地理graphs在社区检测中用于链接semantic Neighbors。我们进一步提出了tensor版Collapsed Gibbs Sampling方法，并要求最小分群大小。一个基于香港地铁乘客数据的案例研究表明了自动进行分群数量的演化和更好的分群质量， measured by within-cluster compactness和cross-cluster separateness。代码可以在https://github.com/bonaldli/TensorDPMM-G中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Review-for-Transformer-based-Long-term-Series-Forecasting"><a href="#A-Systematic-Review-for-Transformer-based-Long-term-Series-Forecasting" class="headerlink" title="A Systematic Review for Transformer-based Long-term Series Forecasting"></a>A Systematic Review for Transformer-based Long-term Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20218">http://arxiv.org/abs/2310.20218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyilei Su, Xumin Zuo, Rui Li, Xin Wang, Heng Zhao, Bingding Huang</li>
<li>for: 本研究旨在提供一个全面的时间序列预测（TSF）领域中 transformer 架构的审视和应用，以及各种改进和优化方法的探讨。</li>
<li>methods: 本文首先提供了 transformer 架构的概述，然后介绍了各种针对长期时间序列预测（LTSF）任务的改进和优化方法，包括不同的变体和模型。</li>
<li>results: 本文提供了一些公共可用的 LTSF 数据集和相关评价指标，以及有价值的时间序列分析训练策略和最佳实践。<details>
<summary>Abstract</summary>
The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.
</details>
<details>
<summary>摘要</summary>
deep learning 的出现引发了时间序列预测（TSF）领域的不eworthy进步。特别是 transformer 架构在 TSF 任务中得到了广泛的应用和采用。 transformer 架构能够有效地提取长序列内元素之间的semantic相关性，并且在不同的 variant 下得到了更好的性能。在本文中，我们首先提供了 transformer 架构的全面概述，以及其在 LTSF 任务中的不同改进。然后，我们 SUMMARIZE 公共可用的 LTSF 数据集和相关评价指标。此外，我们还提供了有价值的时间序列分析训练策略和最佳实践。最后，我们提出了在这个快速发展的领域的可能的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Does-GPT-4-Pass-the-Turing-Test"><a href="#Does-GPT-4-Pass-the-Turing-Test" class="headerlink" title="Does GPT-4 Pass the Turing Test?"></a>Does GPT-4 Pass the Turing Test?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20216">http://arxiv.org/abs/2310.20216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cameron Jones, Benjamin Bergen</li>
<li>for: 本研究用于评估GPT-4在公共在线图灵测试中的表现。</li>
<li>methods: 本研究使用了GPT-4 prompt进行评估，并与ELIZA和GPT-3.5作为基线进行比较。</li>
<li>results: GPT-4 prompt在41%的游戏中突破，超过ELIZA（27%）和GPT-3.5（14%）的基eline，但还未达到人类参与者的基eline（63%）。<details>
<summary>Abstract</summary>
We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.
</details>
<details>
<summary>摘要</summary>
我们对GPT-4进行了公共在线图灵测试评估。最佳GPT-4提示在游戏中达到41%的成绩，超过了ELIZA（27%）和GPT-3.5（14%）的基线，但还未达到机会和人类参与者的基线（63%）。参与者的决策主要基于语言风格（35%）和社会情感特征（27%），支持 inteligence 不够通过图灵测试的想法。参与者的教育背景和 LLMS 的熟悉程度没有预测检测率，表明，即使对系统有深入的理解和常年交互，也可能受到欺骗。虽然图灵测试有知 limitation 作为智能测试，但我们认为它仍然是自然交流和欺骗的评估中的有效工具。AI 模型具有人类化的能力可能会对社会产生广泛的影响，我们分析不同的策略和标准来评估人类化程度。
</details></li>
</ul>
<hr>
<h2 id="Handover-Protocol-Learning-for-LEO-Satellite-Networks-Access-Delay-and-Collision-Minimization"><a href="#Handover-Protocol-Learning-for-LEO-Satellite-Networks-Access-Delay-and-Collision-Minimization" class="headerlink" title="Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization"></a>Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20215">http://arxiv.org/abs/2310.20215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju-Hyung Lee, Chanyoung Park, Soohyun Park, Andreas F. Molisch</li>
<li>for: 这个研究旨在开发一个基于深度反对应学习（DRL）的手over（HO）协议，专门解决低地球轨道（LEO）卫星网络中的手over程序中的持续存在的延迟问题。</li>
<li>methods: 这个DHO协议会跳过测量报告（MR）的手over程序阶段，通过训练使用预测LEO卫星轨道模式，从而消除MR阶段所带来的延迟，同时仍然提供有效的手over决策。</li>
<li>results: 比较legacy HO协议，DHO在多种网络条件下表现出较好的存取延迟、碰撞率和手over成功率，显示DHO在实际网络中具有实际应用性。此外，研究也评估了存取延迟和碰撞率之间的贡献和DHO使用不同DRL算法的训练性和结构。<details>
<summary>Abstract</summary>
This study presents a novel deep reinforcement learning (DRL)-based handover (HO) protocol, called DHO, specifically designed to address the persistent challenge of long propagation delays in low-Earth orbit (LEO) satellite networks' HO procedures. DHO skips the Measurement Report (MR) in the HO procedure by leveraging its predictive capabilities after being trained with a pre-determined LEO satellite orbital pattern. This simplification eliminates the propagation delay incurred during the MR phase, while still providing effective HO decisions. The proposed DHO outperforms the legacy HO protocol across diverse network conditions in terms of access delay, collision rate, and handover success rate, demonstrating the practical applicability of DHO in real-world networks. Furthermore, the study examines the trade-off between access delay and collision rate and also evaluates the training performance and convergence of DHO using various DRL algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="In-Search-of-Lost-Online-Test-time-Adaptation-A-Survey"><a href="#In-Search-of-Lost-Online-Test-time-Adaptation-A-Survey" class="headerlink" title="In Search of Lost Online Test-time Adaptation: A Survey"></a>In Search of Lost Online Test-time Adaptation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20199">http://arxiv.org/abs/2310.20199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixin Wang, Yadan Luo, Liang Zheng, Zhuoxiao Chen, Sen Wang, Zi Huang</li>
<li>for: 这篇论文主要针对在批处理到达时进行测试时适应（OTTA）的研究，尤其是在批处理到达时适应机器学习模型的问题。</li>
<li>methods: 本论文对OTTA技术进行了全面的抽筋，并将其分为三类：（1）数据预处理方法，（2）批处理时适应方法，（3）批处理后适应方法。</li>
<li>results: 本论文通过使用强大的Vision Transformer（ViT）后置进行了 benchmark，发现了一些真正有效的适应策略。在不同的批处理大小和数据预处理方法下，对于CIFAR-10&#x2F;100-C和ImageNet-C等损害数据集的适应性能进行了评估。此外，还对CIFAR-10.1和CIFAR-10-Warehouse等实际批处理数据集进行了测试，以捕捉实际中的批处理变化。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusive of FLOPs, shedding light on the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, indicating: (1) transformers exhibit heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods hinges on ample batch sizes, and (3) stability in optimization and resistance to perturbations are critical during adaptation, especially when the batch size is 1. Motivated by these insights, we pointed out promising directions for future research. The source code will be made available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一项全面的在线测试时适应（OTTA）评估，旨在适应机器学习模型在批处理到达时适应新的数据分布。尽管在最近的几年内，OTTA方法得到了广泛的应用，但是这个领域受到了许多问题的困扰，如模糊的设置、过时的基础模型和不一致的超参数调整，使得复制性变得困难。为了便于对比和分析，我们将OTTA技术分为三个主要类别，并对其进行了 benchmark 使用了强大的 Vision Transformer（ViT）基础模型。我们的 benchmark 包括了常见的损害数据集 such as CIFAR-10/100-C 和 ImageNet-C，以及实际世界中的变化，包括 CIFAR-10.1 和 CIFAR-10-Warehouse，这些变化包括搜索引擎和 Synthesized 数据的扩散模型。为了衡量在线场景中的效率，我们引入了新的评估指标，包括 FLOPs，这些指标可以帮助我们理解在适应过程中的交互冲击和计算负担的负面影响。我们的发现与现有文献不同，表明：(1) transformers 在多种领域shift中表现出了更高的抗频率能力，(2) 许多 OTTA 方法的效果取决于较大的批处理大小，(3) 在适应过程中稳定的优化和抗扰减震是关键，特别是批处理大小为 1。我们的发现驱动了未来研究的方向，我们将在将来公布源代码。
</details></li>
</ul>
<hr>
<h2 id="Generating-Continuations-in-Multilingual-Idiomatic-Contexts"><a href="#Generating-Continuations-in-Multilingual-Idiomatic-Contexts" class="headerlink" title="Generating Continuations in Multilingual Idiomatic Contexts"></a>Generating Continuations in Multilingual Idiomatic Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20195">http://arxiv.org/abs/2310.20195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rhitabrat Pokharel, Ameeta Agrawal</li>
<li>for: 测试语言模型理解非compositional figurative文本的能力</li>
<li>methods: 使用英语和葡萄牙语的数据集，采用零shot、几个shot和精度调参的训练方式</li>
<li>results: 模型在Literal和idiomatic上的continuation生成能力差不多，小于1%的差距，并且模型在两种语言上的性能相似，表明生成模型在这种任务上的稳定性。<details>
<summary>Abstract</summary>
The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
</details>
<details>
<summary>摘要</summary>
Language models（LMs）的能力处理idiomatic或literal多字表达是语言理解和生成的关键方面。我们通过使用英语和葡萄牙语两个语言的 dataset，在zero-shot、few-shot和 fine-tuned 三种训练设置下进行了一系列实验。我们发现模型在literal上和idiomatic上的continuation生成能力差不多，差距非常小。此外，我们发现模型在这两种语言中表现相同，这表明生成模型在这种任务上具有强大的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Pre-training-for-Precipitation-Post-processor"><a href="#Self-supervised-Pre-training-for-Precipitation-Post-processor" class="headerlink" title="Self-supervised Pre-training for Precipitation Post-processor"></a>Self-supervised Pre-training for Precipitation Post-processor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20187">http://arxiv.org/abs/2310.20187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sojung An, Junha Lee, Jiyeon Jang, Inchae Na, Wooyeon Park, Sujeong You</li>
<li>for: 提高地方气象预测中的暴雨预测精度，以防止恶势力天气事件。</li>
<li>methods: 使用深度学习来修正气象预测模型中的降水预测结果，包括自我超visedPre-training和转移学习。</li>
<li>results: 实验结果表明，提议的方法可以在地方气象预测中提高降水预测精度，并且比其他方法更高效。<details>
<summary>Abstract</summary>
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
</details>
<details>
<summary>摘要</summary>
必须确保地方降水的预测预测时间足够，以防止恶势力天气事件。然而，全球变暖导致的气候变化使锋预测极端降水事件的准确性增加的挑战。在这种情况下，我们提议使用深度学习的降水后处理方法来数值天气预测模型。这个降水后处理方法包括（i）自动预训练，其中encoder参数在掩码变量的重建问题上进行自动预训练，以及（ii）在降水 segmentation任务（目标领域）上传递学习。我们还介绍了一种有效地训练类偏挥的数据集的启发式标签法。我们的实验结果表明，提议的方法在地方NWP降水修正中表现出色，超过了其他方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Discover-Skills-through-Guidance"><a href="#Learning-to-Discover-Skills-through-Guidance" class="headerlink" title="Learning to Discover Skills through Guidance"></a>Learning to Discover Skills through Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20178">http://arxiv.org/abs/2310.20178</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Hyunseung Kim, Byungkun Lee, Hojoon Lee, Dongyoon Hwang, Sejik Park, Kyushik Min, Jaegul Choo</li>
<li>for: 提高无监督技能发现（USD）中的探索性能，尤其是在环境复杂性增加时。</li>
<li>methods: 提出了一种新的USD算法，即技能发现指导（DISCO-DANCE），通过选择最有潜力到达未探索状态的导航技能，然后将其他技能引导到该导航技能的方向下，最后将引导技能分散以 Maximize其在未探索状态中的 отли别性。</li>
<li>results: DISCO-DANCE在具有挑战性的环境中比基eline USD算法表现出色，包括两个导航测试准则和一个连续控制测试准则。<details>
<summary>Abstract</summary>
In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE are available at https://mynsng.github.io/discodance.
</details>
<details>
<summary>摘要</summary>
在无监督技能发现（USD）领域，一个主要挑战是限制探索，主要因为行为偏离初始轨迹的罚款。为增强探索，当前的方法ologies使用辅助奖励来最大化状态的 épistémiqueuncertainty或熵。然而，我们发现在环境复杂性增加时，这些奖励的效果下降。因此，我们提出了一种新的 USD算法，即技能发现导航（DISCO-DANCE），它包括以下三个步骤：1. 选择具有最高潜在性能达到未探索状态的导航技能（guide skill）。2. 使其他技能跟随导航技能。3. 使导航技能分散，以最大化它们在未探索状态中的分化度。实验证明，DISCO-DANCE在复杂环境中表现出色，比如两个导航benchmark和一个连续控制benchmark。详细的visual化和代码可以通过https://mynsng.github.io/discodance访问。
</details></li>
</ul>
<hr>
<h2 id="GraphTransformers-for-Geospatial-Forecasting"><a href="#GraphTransformers-for-Geospatial-Forecasting" class="headerlink" title="GraphTransformers for Geospatial Forecasting"></a>GraphTransformers for Geospatial Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20174">http://arxiv.org/abs/2310.20174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pallavi Banerjee, Satyaki Chakraborty</li>
<li>for: 预测地ospatial序列的轨迹，使用GraphTransformers提高预测精度。</li>
<li>methods: 利用地ospatial点之间自然形成的图结构，将其Explicitly incorporated into Transformer模型，从而提高预测精度。</li>
<li>results: 在HURDAT数据集上，与基准线性Transformers相比，GraphTransformers显示出了显著的提高（ improve upon state-of-the-art Transformer based baseline significantly）。<details>
<summary>Abstract</summary>
In this paper we introduce a novel framework for trajectory prediction of geospatial sequences using GraphTransformers. When viewed across several sequences, we observed that a graph structure automatically emerges between different geospatial points that is often not taken into account for such sequence modeling tasks. We show that by leveraging this graph structure explicitly, geospatial trajectory prediction can be significantly improved. Our GraphTransformer approach improves upon state-of-the-art Transformer based baseline significantly on HURDAT, a dataset where we are interested in predicting the trajectory of a hurricane on a 6 hourly basis.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的抽象框架，用于预测地理序列的轨迹使用图Transformers。当考虑多个序列时，我们发现了不同的地理点之间自然形成的图结构，通常不会被考虑到这类序列模型任务中。我们表明，通过显式利用这个图结构，可以明显提高地理轨迹预测。我们的图Transformer方法在HURDAT数据集上，即预测风暴 trajectory的6小时基准，与状态的基准之间有明显的提高。
</details></li>
</ul>
<hr>
<h2 id="Is-Robustness-Transferable-across-Languages-in-Multilingual-Neural-Machine-Translation"><a href="#Is-Robustness-Transferable-across-Languages-in-Multilingual-Neural-Machine-Translation" class="headerlink" title="Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?"></a>Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20162">http://arxiv.org/abs/2310.20162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiyu Pan, Supryadi, Deyi Xiong</li>
<li>for: 这 paper  investigate 多种语言间 neural machine translation 模型的 robustness 性能如何被转移。</li>
<li>methods: 作者提出了一种 robustness transfer analysis 协议，并进行了一系列实验，包括使用 caracter-、word- 和 multi-level 噪音攻击特定翻译方向的模型，并评估其他翻译方向的 robustness。</li>
<li>results: 结果表明，在一个翻译方向上获得的 robustness 可以转移到其他翻译方向上。此外，作者还发现在 caracter-level 和 word-level 噪音攻击下，robustness 的转移更加可能。<details>
<summary>Abstract</summary>
Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages in multilingual neural machine translation. We propose a robustness transfer analysis protocol and conduct a series of experiments. In particular, we use character-, word-, and multi-level noises to attack the specific translation direction of the multilingual neural machine translation model and evaluate the robustness of other translation directions. Our findings demonstrate that the robustness gained in one translation direction can indeed transfer to other translation directions. Additionally, we empirically find scenarios where robustness to character-level noise and word-level noise is more likely to transfer.
</details>
<details>
<summary>摘要</summary>
“模型强度”，指模型在干扰下保持性能的能力，对于建立可靠的自然语言处理系统是关键。最近的研究表明，通过对抗训练和数据增强可以提高模型的强度。然而，在机器翻译方面，大多数研究都集中在单向机器翻译中。在这篇论文中，我们 investigate了多种语言之间的强度传递。我们提出了一种强度传递分析协议，并进行了一系列实验。具体来说，我们使用字符、词和多级噪声来攻击特定的多语言神经机器翻译模型的特定翻译方向，并评估其他翻译方向的强度。我们的发现表明，在一个翻译方向上获得的强度可以实际传递到其他翻译方向。此外，我们经验性地发现，在字符级噪声和词级噪声下，强度更容易传递。
</details></li>
</ul>
<hr>
<h2 id="Language-Guided-Visual-Question-Answering-Elevate-Your-Multimodal-Language-Model-Using-Knowledge-Enriched-Prompts"><a href="#Language-Guided-Visual-Question-Answering-Elevate-Your-Multimodal-Language-Model-Using-Knowledge-Enriched-Prompts" class="headerlink" title="Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts"></a>Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20159">http://arxiv.org/abs/2310.20159</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/declare-lab/lg-vqa">https://github.com/declare-lab/lg-vqa</a></li>
<li>paper_authors: Deepanway Ghosal, Navonil Majumder, Roy Ka-Wei Lee, Rada Mihalcea, Soujanya Poria</li>
<li>for: 本研究的目的是提高视觉问答（VQA）任务的精度，特别是在需要理解图像和问题的概念和知识的情况下。</li>
<li>methods: 本研究提议一种多模态框架，使用语言指导（LG）来提高VQA的精度。LG包括了理由、图像caption、场景图等，可以帮助模型更好地理解问题和图像。</li>
<li>results: 研究使用CLIP和BLIP模型在多个数据集上进行了比较，结果表明，使用语言指导可以提高VQA的精度。在A-OKVQA数据集上，CLIP模型的性能提高了7.6%，BLIP-2模型的性能提高了4.8%。此外，在其他数据集上也有一致的提高。codes和模型可以在<a target="_blank" rel="noopener" href="https://github.com/declare-lab/LG-VQA%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/declare-lab/LG-VQA上下载。</a><details>
<summary>Abstract</summary>
Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8% in the challenging A-OKVQA dataset. We also observe consistent improvement in performance on the Science-QA, VSR, and IconQA datasets when using the proposed language guidances. The implementation of LG-VQA is publicly available at https:// github.com/declare-lab/LG-VQA.
</details>
<details>
<summary>摘要</summary>
Visual问题回答（VQA）是一个答案问题的任务，它需要理解图像和问题，并提供自然语言的答案。VQA在最近几年内受到了广泛的关注，因为它在多个领域有广泛的应用前景，如机器人、教育和医疗等。在这篇论文中，我们关注知识增强VQA，其中回答问题需要对图像和问题进行了深入的理解和推理。我们提议一种多模态框架，使用语言引导（LG）的形式，如理由、图像描述、场景图等，以提高问题回答的准确率。我们使用CLIP和BLIP模型对A-OKVQA、Science-QA、VSR和IconQA数据集进行多选问题回答任务进行 benchmarking，并证明了语言引导是一种简单 yet 强大和有效的策略。我们的语言引导在A-OKVQA数据集中使用CLIP和BLIP-2模型提高了性能的7.6%和4.8%。此外，我们在Science-QA、VSR和IconQA数据集中也 observe了一致的性能提升。LG-VQA的实现可以在https://github.com/declare-lab/LG-VQA中找到。
</details></li>
</ul>
<hr>
<h2 id="MLatom-3-Platform-for-machine-learning-enhanced-computational-chemistry-simulations-and-workflows"><a href="#MLatom-3-Platform-for-machine-learning-enhanced-computational-chemistry-simulations-and-workflows" class="headerlink" title="MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows"></a>MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20155">http://arxiv.org/abs/2310.20155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavlo O. Dral, Fuchun Ge, Yi-Fan Hou, Peikun Zheng, Yuxinxin Chen, Mario Barbatti, Olexandr Isayev, Cheng Wang, Bao-Xin Xue, Max Pinheiro Jr, Yuming Su, Yiheng Dai, Yangtao Chen, Lina Zhang, Shuang Zhang, Arif Ullah, Quanhao Zhang, Yanchi Ou</li>
<li>for: 用于扩展计算化学 simulations 和创建复杂的工作流程</li>
<li>methods: 利用机器学习技术进行计算化学 simulations, 包括计算能量和热化学性质、优化结构、跑分子和量子动力学 dynamics、计算（ro）振荡、一 photon UV&#x2F;vis absorption 和两 photon absorption 谱</li>
<li>results: 可以使用 MLatom 计算出高精度的化学物理性质和动力学性质，包括能量和热化学性质、结构优化、分子动力学 dynamics 和谱线spectra<details>
<summary>Abstract</summary>
Machine learning (ML) is increasingly becoming a common tool in computational chemistry. At the same time, the rapid development of ML methods requires a flexible software framework for designing custom workflows. MLatom 3 is a program package designed to leverage the power of ML to enhance typical computational chemistry simulations and to create complex workflows. This open-source package provides plenty of choice to the users who can run simulations with the command line options, input files, or with scripts using MLatom as a Python package, both on their computers and on the online XACS cloud computing at XACScloud.com. Computational chemists can calculate energies and thermochemical properties, optimize geometries, run molecular and quantum dynamics, and simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra with ML, quantum mechanical, and combined models. The users can choose from an extensive library of methods containing pre-trained ML models and quantum mechanical approximations such as AIQM1 approaching coupled-cluster accuracy. The developers can build their own models using various ML algorithms. The great flexibility of MLatom is largely due to the extensive use of the interfaces to many state-of-the-art software packages and libraries.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在计算化学中日益普及，同时ML方法的快速发展需要一个灵活的软件框架来设计自定义工作流程。MLatom 3 是一个开源的程序包，旨在利用 ML 提高计算化学 simulations 的性能并创建复杂的工作流程。用户可以通过命令行选项、输入文件或脚本使用 MLatom 作为 Python 包来运行 simulations，并可以在计算机和 XACS 云计算platform（XACScloud.com）上进行分布式计算。计算化学家可以使用 ML、量子力学和组合模型计算能量和热化学性质、优化结构、运行分子和量子动力学、并 simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra。用户可以选择广泛的库中的方法，包括预训练 ML 模型和量子力学近似方法，如 AIQM1 接近coupled-cluster 精度。开发者可以使用多种 ML 算法创建自己的模型。MLatom 的巨大灵活性主要归功于它对许多现状体系软件包和库的广泛使用。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Multi-fidelity-Learning-for-Cost-effective-Adaptation-of-Language-Model-with-Sparse-Human-Supervision"><a href="#Interactive-Multi-fidelity-Learning-for-Cost-effective-Adaptation-of-Language-Model-with-Sparse-Human-Supervision" class="headerlink" title="Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision"></a>Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20153">http://arxiv.org/abs/2310.20153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Zhang, Zhuohang Li, Kamalika Das, Sricharan Kumar</li>
<li>for: 这个研究是为了提高对具体领域任务的大语言模型（LLM）的开发成本，使其更加可靠和高效。</li>
<li>methods: 我们提出了一个名为Interactive Multi-Fidelity Learning（IMFL）的新框架，它可以在有限的标注预算下，以低成本和高效率开发小型具体领域的LLM。我们将这个过程形式化为一个多标注学习问题，并针对找到最佳标注策略，以将低精度自动LLM标注和高精度人工标注相互融合，以最大化模型性能。</li>
<li>results: 我们在金融和医疗领域进行了广泛的实验，结果显示IMFL可以与单一标注策略相比，在四个任务中获得更高的性能。对于有限的人工标注预算，IMFL可以将人工标注时间和成本严重减少，并使用更多的LLM标注来提高模型性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation, and 2) variable batch size that controls the order for choosing each fidelity to facilitate knowledge distillation, ultimately enhancing annotation quality. Extensive experiments on financial and medical tasks demonstrate that IMFL achieves superior performance compared with single fidelity annotations. Given a limited budget of human annotation, IMFL significantly outperforms the human annotation baselines in all four tasks and achieves very close performance as human annotations on two of the tasks. These promising results suggest that the high human annotation costs in domain-specific tasks can be significantly reduced by employing IMFL, which utilizes fewer human annotations, supplemented with cheaper and faster LLM (e.g., GPT-3.5) annotations to achieve comparable performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示出各种任务中的杰出能力。然而，它们在执行特定领域任务时，受到巨大规模的部署限制、易受到误差影响和高度标注成本的限制。我们提出了一个新的互动多层学习（IMFL）框架，以便在有限标注预算下开发小型特定领域语言模型。我们的方法将这个领域特定的精炼过程推理为多层学习问题，强调找到优化自动LLM标注和高精度人类标注之间的最佳混合策略，以最大化模型性能。我们还提出了一个探索优化搜寻策略，将人类标注和自动LLM标注融合在一起，以提高标注质量。实验结果显示，IMFL在金融和医疗领域的四个任务中均表现出色，与单一精炼标注相比，IMFL可以对人类标注进行很好的优化。对于有限的人类标注预算，IMFL可以实现人类标注的优化，并且可以在四个任务中实现和人类标注相同的性能。这些成绩表明，通过实现IMFL，可以将执行特定领域任务中的人类标注成本降低到最低限度，并且可以通过使用更快速的LLM（例如GPT-3.5）来进行优化，以获得相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Unlearn-What-You-Want-to-Forget-Efficient-Unlearning-for-LLMs"><a href="#Unlearn-What-You-Want-to-Forget-Efficient-Unlearning-for-LLMs" class="headerlink" title="Unlearn What You Want to Forget: Efficient Unlearning for LLMs"></a>Unlearn What You Want to Forget: Efficient Unlearning for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20150">http://arxiv.org/abs/2310.20150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaao Chen, Diyi Yang</li>
<li>for: 提高大语言模型（LLMs）的隐私和数据保护功能，以便更好地 removing 个人数据。</li>
<li>methods: 提出了一种高效的忘记框架，通过在 transformers 中添加轻量级的忘记层和一种有效的融合机制，可以高效地更新 LLMs 而不需要重新训练整个模型。</li>
<li>results: 在分类和生成任务中，与当前基eline相比，提出的方法具有更高的效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经取得了很大的进步，通过预训练和记忆大量文本数据，但这个过程可能会遇到隐私问题和数据保护规定的违反。因此，能够轻松地从LLM中移除关于个人用户的数据而不影响预测质量的能力变得越来越重要。为解决这些问题，在这项工作中，我们提出了一个高效的忘记框架，可以通过在transformer中引入轻量级的忘记层来高效地更新LLM，并且在执行数据移除操作时不需要重新训练整个模型。此外，我们还提出了一种 fusel mechanism，可以有效地将不同的忘记层结合在一起，以处理一系列的忘记操作。实验表明，我们提出的方法在分类和生成任务中表现了比州前的优异性。
</details></li>
</ul>
<hr>
<h2 id="Decision-Making-for-Autonomous-Vehicles-with-Interaction-Aware-Behavioral-Prediction-and-Social-Attention-Neural-Network"><a href="#Decision-Making-for-Autonomous-Vehicles-with-Interaction-Aware-Behavioral-Prediction-and-Social-Attention-Neural-Network" class="headerlink" title="Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network"></a>Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20148">http://arxiv.org/abs/2310.20148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Li, Kaiwen Liu, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky</li>
<li>for: 本研究旨在帮助自动驾驶车辆更好地理解周围交通情况，以便更好地完成任务。</li>
<li>methods: 本研究提出了一个行为模型，该模型将交通 Driver 的交互意图编码成隐藏的社会心理参数。然后，通过 bayesian 筛选器，我们开发了一种递归远程优化控制器，以考虑交通 Driver 的意图不确定性。在线部署时，我们设计了基于注意力机制的神经网络架构，以模仿行为模型。此外，我们还提出了一种决策搜索算法，以解决在线决策问题。</li>
<li>results: 我们对行为模型进行了实际路径预测测试，并对强制合并任务进行了广泛的评估，包括使用模拟环境和实际交通数据集。结果表明，我们的算法可以在不同交通条件下完成强制合并任务，同时保证安全驾驶。<details>
<summary>Abstract</summary>
Autonomous vehicles need to accomplish their tasks while interacting with human drivers in traffic. It is thus crucial to equip autonomous vehicles with artificial reasoning to better comprehend the intentions of the surrounding traffic, thereby facilitating the accomplishments of the tasks. In this work, we propose a behavioral model that encodes drivers' interacting intentions into latent social-psychological parameters. Leveraging a Bayesian filter, we develop a receding-horizon optimization-based controller for autonomous vehicle decision-making which accounts for the uncertainties in the interacting drivers' intentions. For online deployment, we design a neural network architecture based on the attention mechanism which imitates the behavioral model with online estimated parameter priors. We also propose a decision tree search algorithm to solve the decision-making problem online. The proposed behavioral model is then evaluated in terms of its capabilities for real-world trajectory prediction. We further conduct extensive evaluations of the proposed decision-making module, in forced highway merging scenarios, using both simulated environments and real-world traffic datasets. The results demonstrate that our algorithms can complete the forced merging tasks in various traffic conditions while ensuring driving safety.
</details>
<details>
<summary>摘要</summary>
自动驾驶车需要在交通中完成任务，因此需要 equip 自动驾驶车 WITH 人工智能来更好地理解周围交通的意图，以便完成任务。在这项工作中，我们提议一种行为模型，该模型将交换意图编码为隐藏的社会心理参数。通过 bayesian 滤波器，我们开发了一种远 horizon 优化基于的决策控制器，该控制器考虑了交换意图的不确定性。为在线部署，我们设计了一种基于注意机制的神经网络架构，该架构借鉴行为模型的在线估计参数。我们还提出了一种决策搜索算法来解决在线决策问题。我们对提议的行为模型进行了实际路径预测能力的评估。此外，我们对强制合并enario中的决策模块进行了广泛的评估，包括使用 simulated 环境和实际交通数据集。结果表明，我们的算法可以在不同交通条件下完成强制合并任务，并保证驾驶安全。
</details></li>
</ul>
<hr>
<h2 id="EELBERT-Tiny-Models-through-Dynamic-Embeddings"><a href="#EELBERT-Tiny-Models-through-Dynamic-Embeddings" class="headerlink" title="EELBERT: Tiny Models through Dynamic Embeddings"></a>EELBERT: Tiny Models through Dynamic Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20144">http://arxiv.org/abs/2310.20144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabrielle Cohn, Rishika Agarwal, Deepanshu Gupta, Siddharth Patwardhan</li>
<li>for: 这个论文是为了对 transformer-based 模型（例如 BERT）进行压缩，以 minimize 下游任务的精度影响。</li>
<li>methods: 这个方法是通过取代模型的输入嵌入层 With dynamic 嵌入计算，实现模型大小的压缩。</li>
<li>results: 实验结果显示，我们的 BERT Variants (EELBERT) 与传统 BERT 模型的 regression 影响几乎没有差异，而且我们开发了最小化的模型 UNO-EELBERT，它在 GLUE 测试中与完全训练的 BERT-tiny 模型的 GLUE 分数相似，但是模型大小仅有 1.2 MB，比 traditional BERT 模型缩小了 15 倍。<details>
<summary>Abstract</summary>
We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
</details>
<details>
<summary>摘要</summary>
我们介绍EELBERT方法，用于对 transformer 型模型（例如 BERT）进行压缩，而无需对下游任务的精度造成明显影响。这是通过取代模型中的输入嵌入层而实现的，并使用 Dynamics 即在运行时进行嵌入计算。由于输入嵌入层占模型大小的相当比例，特别是小型 BERT Variants，因此替换这个层来自动 computing 嵌入可以对模型大小进行重要压缩。我们在 GLUE 评分标准上进行了实践评估，发现我们的 BERT Variants (EELBERT) 与传统 BERT 模型之间的 regression 相对轻微。透过这种方法，我们成功开发了我们最小的模型 UNO-EELBERT，它在 GLUE 评分标准上获得了与完全训练的 BERT-tiny 相似的分数（准确性 Within 4%），并且仅有 1.2 MB 的大小，相比于传统 BERT 模型的 15 倍。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Difference-Predictive-Coding"><a href="#Contrastive-Difference-Predictive-Coding" class="headerlink" title="Contrastive Difference Predictive Coding"></a>Contrastive Difference Predictive Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20141">http://arxiv.org/abs/2310.20141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chongyi-zheng/td_infonce">https://github.com/chongyi-zheng/td_infonce</a></li>
<li>paper_authors: Chongyi Zheng, Ruslan Salakhutdinov, Benjamin Eysenbach</li>
<li>for: 预测和理解未来的问题，如goal-conditioned reinforcement learning，是时间序列问题的核心。而现有方法通常需要大量数据来学习表示长期依赖关系。</li>
<li>methods: 本文引入了一种时间差异版本的对比预测编码法，可以将不同时间序列数据剪辑到一起，以降低学习预测未来事件所需的数据量。</li>
<li>results: 实验表明，相比于先前的RL方法，我们的方法可以达到$2 \times$的成功率增加，并且在随机环境中更好地适应。在表格设置中，我们的方法比successor表示法和标准（Monte Carlo）版本的对比预测编码法快$20 \times$和$1500 \times$快。<details>
<summary>Abstract</summary>
Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \times$ more sample efficient than the successor representation and $1500 \times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.
</details>
<details>
<summary>摘要</summary>
“predicting和理解未来是许多时间序列问题的核心。例如，目标条件反射学习可以视为学习表示来预测哪些状态可能会在未来被访问。而现有方法通常使用对照预测编码来模型时间序列数据，学习表示长期相关性通常需要大量数据。在这篇论文中，我们介绍了一种时间差版本的对照预测编码，可以将不同时间序列数据的剩下拼接起来，以降低学习未来事件预测所需的数据量。我们应用这种表示学习方法， derivate一种离散RL算法。实验显示，相比于先前的RL方法，我们的方法可以 achieve $2 \times$ 中位幂提高成功率，并在随机环境中更好地应对。在表格设置下，我们表明我们的方法比 successor representation 高效 $20 \times$，高效于标准（蒙特卡洛）对照预测编码 $1500 \times$。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Classification-of-Student-Help-Requests-in-Programming-Courses-Using-Large-Language-Models"><a href="#Efficient-Classification-of-Student-Help-Requests-in-Programming-Courses-Using-Large-Language-Models" class="headerlink" title="Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models"></a>Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20105">http://arxiv.org/abs/2310.20105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaromir Savelka, Paul Denny, Mark Liffiton, Brad Sheese</li>
<li>for: 本研究旨在 evaluating the performance of GPT-3.5和GPT-4模型来自动分类学生寻求帮助的请求类型。</li>
<li>methods: 研究使用了 zero-shot 和 fine-tuning 方法来训练 GPT-3.5 和 GPT-4 模型，并对其进行比较。</li>
<li>results: 研究发现，GPT-4 模型在Debugging类别中的表现比 GPT-3.5 更高，而 fine-tuning GPT-3.5 模型可以提高其表现，使其与两名人类评分员的准确率和一致性几乎相同。<details>
<summary>Abstract</summary>
The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
</details>
<details>
<summary>摘要</summary>
学生的帮助请求的准确分类可以帮助tailoring有效的回应。自动地分类这些请求是一件非常复杂的任务，但是大型语言模型（LLM）似乎提供了可 accessible、cost-effective的解决方案。本研究evaluates the performance of GPT-3.5和GPT-4模型来分类学生在入门编程课程中的帮助请求。在零 shot trial中，GPT-3.5和GPT-4在大多数类别中表现相似，而GPT-4在 debug的子类别中表现更高。对GPT-3.5模型进行细化调试可以提高其表现，使其与人类评估者的准确率和一致性在各个类别中相近。总的来说，这个研究表明了使用LLM来提高教育系统的自动化学生需求分类是可能的。
</details></li>
</ul>
<hr>
<h2 id="Plagiarism-and-AI-Assistance-Misuse-in-Web-Programming-Unfair-Benefits-and-Characteristics"><a href="#Plagiarism-and-AI-Assistance-Misuse-in-Web-Programming-Unfair-Benefits-and-Characteristics" class="headerlink" title="Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics"></a>Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20104">http://arxiv.org/abs/2310.20104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Karnalim, Hapnes Toba, Meliana Christianti Johan, Erico Darmawan Handoyo, Yehezkiel David Setiawan, Josephine Alvina Luwia</li>
<li>for: This paper aims to identify and understand the issues of plagiarism and misuse of AI assistance in web programming education.</li>
<li>methods: The authors conducted a controlled experiment to compare student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT).</li>
<li>results: The study found that students who engaged in misconducts (plagiarism and AI assistance) received comparable test marks with less completion time, while AI-assisted submissions were more complex and less readable.<details>
<summary>Abstract</summary>
In programming education, plagiarism and misuse of artificial intelligence (AI) assistance are emerging issues. However, not many relevant studies are focused on web programming. We plan to develop automated tools to help instructors identify both misconducts. To fully understand the issues, we conducted a controlled experiment to observe the unfair benefits and the characteristics. We compared student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT). Our study shows that students who are involved in such misconducts get comparable test marks with less completion time. Plagiarized submissions are similar to the independent ones except in trivial aspects such as color and identifier names. AI-assisted submissions are more complex, making them less readable. Students believe AI assistance could be useful given proper acknowledgment of the use, although they are not convinced with readability and correctness of the solutions.
</details>
<details>
<summary>摘要</summary>
在编程教育中，抄袭和人工智能（AI）协助的不当使用是emerging问题。然而，不多的相关研究是关注网络编程。我们计划开发自动化工具来帮助教师识别这两种不当行为。为了全面了解问题，我们进行了一次控制性实验，观察不当利益和特征。我们比较了学生完成网络编程任务的独立性、抄袭和AI协助（ChatGPT）的性能。我们的研究表明，参与这种不当行为的学生在测试marks和完成时间上得到了相似的成绩。抄袭的提交和独立提交在 superficies上相似，只有一些 superficies上的细节有所不同。AI协助的提交更复杂，导致它们更难于阅读。学生认为AI协助可以是有用的，只要正确地承认其使用，但他们并不确定AI协助的解决方案的可读性和正确性。
</details></li>
</ul>
<hr>
<h2 id="Data-Market-Design-through-Deep-Learning"><a href="#Data-Market-Design-through-Deep-Learning" class="headerlink" title="Data Market Design through Deep Learning"></a>Data Market Design through Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20096">http://arxiv.org/abs/2310.20096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Sai Srivatsa Ravindranath, Yanchen Jiang, David C. Parkes</li>
<li>For: The paper is written to design revenue-optimal data markets using deep learning, with the goal of expanding the frontiers of what can be understood and achieved in this area.* Methods: The paper uses deep learning to learn signaling schemes for data market design, rather than allocation rules, and handles obedience constraints and incentive constraints on bids.* Results: The paper demonstrates that the new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures about the structure of optimal designs.Here is the same information in Simplified Chinese text:* For: 这篇论文是为了使用深度学习设计数据市场，以扩展我们可以理解和实现的领域。* Methods: 这篇论文使用深度学习来学习数据市场设计的信号套件，而不是分配规则，并处理了让命约和招标约束。* Results: 这篇论文示出了新的深度学习框架可以准确地复制现有理论中的解决方案，扩展到更复杂的设定，并用于确定数据市场的优化设计和假设结构。<details>
<summary>Abstract</summary>
The $\textit{data market design}$ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price [Bergemann et al., 2018]. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others [Bonatti et al., 2022]. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design [D\"utting et al., 2023], we must learn signaling schemes rather than allocation rules and handle $\textit{obedience constraints}$ $-$ these arising from modeling the downstream actions of buyers $-$ in addition to incentive constraints on bids. Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.
</details>
<details>
<summary>摘要</summary>
“数据市场设计”问题是经济理论中的一个问题，旨在找到一组信号设计（统计实验），以 maximize 信息提供者的预期收入，其中每个实验 revelas 提供者所知的一些信息，并有相应的价格。每个买家在世界环境中做出决策，他们对具体实验的期望值取决于他们的先前知识和不同结果的价值。在多个买家的情况下，一个买家的期望值可能也取决于它们向其他人销售的信息。我们介绍了深度学习在数据市场设计中的应用，以拓宽我们能理解和实现的前iers。相比于早期的深度学习卖场设计研究（D\"utting et al., 2023），我们需要学习信号设计而不是分配规则，并处理“服从约束”（来自模型下游行为的规则），以及投标 constraint。我们的实验表明，这新的深度学习框架可以几乎地准确地复制所有已知的理论解决方案，扩展到更复杂的设定，并用于证明数据市场的优化设计和提出新的设计问题。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Neural-Language-Models-as-Cognitive-Models-of-Language-Acquisition"><a href="#Evaluating-Neural-Language-Models-as-Cognitive-Models-of-Language-Acquisition" class="headerlink" title="Evaluating Neural Language Models as Cognitive Models of Language Acquisition"></a>Evaluating Neural Language Models as Cognitive Models of Language Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20093">http://arxiv.org/abs/2310.20093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Héctor Javier Vázquez Martínez, Annika Lea Heuser, Charles Yang, Jordan Kodner</li>
<li>for: 这篇论文探讨了语义学模型（LM）在技术任务上的成功，以及它们是否能成为语言科学的理论。</li>
<li>methods: 作者认为一些常用的语义学模型评价标准可能不够严格，特别是模板化评价标准缺乏语言科学研究中常见的结构多样性。</li>
<li>results: 作者发现，当使用小规模数据来模拟儿童语言学习时，LMs可以被简单的基准模型替代。此外，LMs在一个名为LI-Adger的数据集上评价句子的方式与人类语言用户不一致。作者建议更好地将LMs与儿童语言学习研究连接起来。<details>
<summary>Abstract</summary>
The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with human language users. We conclude with suggestions for better connecting LMs with the empirical study of child language acquisition.
</details>
<details>
<summary>摘要</summary>
neural language models (LMs) 的成功在多种技术任务上，使其潜在地成为语言科学的理论，尽管LM训练和儿童语言学习有一些明显的区别。在这篇论文中，我们认为一些最具有挑战性的LM评价标准可能不够严格。特别是，我们表明了模板基本的评价标准缺乏语言学研究中常见的结构多样性。当LM训练于小规模数据集时，可以轻松地与简单的基线模型匹配。我们建议使用已有、仔细筛选的数据集，由大量本地语言用户评估过的梯度接受度进行评估。在LI-Adger数据集上，LMs 评价句子与人类语言用户不一致。我们结束于建议更好地连接LMs与儿童语言学习的实验研究。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.AI_2023_10_31/" data-id="cloimip5t006fs488bfqmecmo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/cs.CV_2023_10_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-10-31
        
      </div>
    </a>
  
  
    <a href="/2023/10/31/cs.CL_2023_10_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-10-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
