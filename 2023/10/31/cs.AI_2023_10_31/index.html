
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-10-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20704 repo_url: None paper_authors: Srijan Das, Tanmay Jain, Dominick Reilly, P">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-10-31">
<meta property="og:url" content="https://nullscc.github.io/2023/10/31/cs.AI_2023_10_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20704 repo_url: None paper_authors: Srijan Das, Tanmay Jain, Dominick Reilly, P">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-31T12:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:06.951Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.AI_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T12:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-10-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Limited-Data-Unlimited-Potential-A-Study-on-ViTs-Augmented-by-Masked-Autoencoders"><a href="#Limited-Data-Unlimited-Potential-A-Study-on-ViTs-Augmented-by-Masked-Autoencoders" class="headerlink" title="Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders"></a>Limited Data, Unlimited Potential: A Study on ViTs Augmented by Masked Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20704">http://arxiv.org/abs/2310.20704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srijan Das, Tanmay Jain, Dominick Reilly, Pranav Balaji, Soumyajit Karmakar, Shyam Marjit, Xiang Li, Abhijit Das, Michael Ryoo</li>
<li>for: 这个研究的目的是寻找适当的自我超vised learning任务，以促进少量数据下的维特训练。</li>
<li>methods: 研究使用的方法包括适当的自我超vised learning任务、训练方案和资料Scale。</li>
<li>results: 研究发现，将自我超vised learning任务与主要任务共同优化可以帮助维特在少量数据下进行更好的表现，并且可以降低碳迹。实验结果显示，这种方法在10个数据集上获得了 significatively better performance，并且在视频领域进行深伪检测也显示了通用性。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have become ubiquitous in computer vision. Despite their success, ViTs lack inductive biases, which can make it difficult to train them with limited data. To address this challenge, prior studies suggest training ViTs with self-supervised learning (SSL) and fine-tuning sequentially. However, we observe that jointly optimizing ViTs for the primary task and a Self-Supervised Auxiliary Task (SSAT) is surprisingly beneficial when the amount of training data is limited. We explore the appropriate SSL tasks that can be optimized alongside the primary task, the training schemes for these tasks, and the data scale at which they can be most effective. Our findings reveal that SSAT is a powerful technique that enables ViTs to leverage the unique characteristics of both the self-supervised and primary tasks, achieving better performance than typical ViTs pre-training with SSL and fine-tuning sequentially. Our experiments, conducted on 10 datasets, demonstrate that SSAT significantly improves ViT performance while reducing carbon footprint. We also confirm the effectiveness of SSAT in the video domain for deepfake detection, showcasing its generalizability. Our code is available at https://github.com/dominickrei/Limited-data-vits.
</details>
<details>
<summary>摘要</summary>
Computer vision 领域中的 Vision Transformers (ViTs) 已经广泛应用。尽管它们取得了成功，但是它们缺乏适应偏好，这可能使它们在有限数据量时训练困难。为了解决这个挑战，先前的研究表明，通过自动学习（SSL）和顺序精度调整来训练 ViTs。然而，我们发现，在有限数据量时，同时优化 ViTs  для主要任务和一个 Self-Supervised Auxiliary Task (SSAT) 是意外地有利的。我们研究了合适的 SSL 任务，以及这些任务的训练方案，以及它们在哪些数据规模上最有效。我们的发现表明，SSAT 是一种强大的技术，它使得 ViTs 可以利用自我超vised 和主要任务之间的特殊特征，从而实现比通常 ViTs 预训练 SSL 并顺序精度调整更好的性能。我们在 10 个数据集上进行了实验，并证明了 SSAT 可以提高 ViT 性能，同时减少碳脚印。我们还证明了 SSAT 在视频领域中对深伪检测的效果，这表明它的通用性。我们的代码可以在 GitHub 上找到：https://github.com/dominickrei/Limited-data-vits。
</details></li>
</ul>
<hr>
<h2 id="Vanishing-Gradients-in-Reinforcement-Finetuning-of-Language-Models"><a href="#Vanishing-Gradients-in-Reinforcement-Finetuning-of-Language-Models" class="headerlink" title="Vanishing Gradients in Reinforcement Finetuning of Language Models"></a>Vanishing Gradients in Reinforcement Finetuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20703">http://arxiv.org/abs/2310.20703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua Susskind, Etai Littwin</li>
<li>for: 这个论文主要用于解释了强化训练（Reinforcement Finetuning，RFT）中的一种优化困难，即预期梯度的消失问题。</li>
<li>methods: 该论文使用了 teoría y experimentos to demonstrate that vanishing gradients are prevalent and detrimental in RFT, and to explore ways to overcome this issue.</li>
<li>results: 研究发现，通过在RFT阶段首先进行一个简单的监督学习（Supervised Finetuning，SFT）阶段，可以减轻 vanishing gradients 的问题，并且只需要在少量的输入样本上进行一些优化步骤即可。<details>
<summary>Abstract</summary>
Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small number of SFT optimization steps on as few as 1% of the input samples can suffice, indicating that the initial SFT phase need not be expensive in terms of compute and data labeling efforts. Overall, our results emphasize that being mindful for inputs whose expected gradient vanishes, as measured by the reward standard deviation, is crucial for successful execution of RFT.
</details>
<details>
<summary>摘要</summary>
通常情况下，预训练语言模型通过强化训练（RFT）与人类偏好和下游任务相对应，其中包括使用策略梯度算法来最大化一个（可能是学习的）奖励函数。这项工作揭示了RFT中的一个基本优化问题：我们证明了，对于一个输入，其奖励标准差下的模型奖励函数的期望梯度将在小己的情况下消失，即使实际奖励远离最优。通过实验和控制环境，以及理论分析，我们证明了这种消失梯度是普遍存在的和有害的，导致奖励最大化变得极其慢。最后，我们探讨了在RFT中超越消失梯度的方法。我们发现，通常的初始监督训练（SFT）阶段是最佳的选择，这也解释了它在RFT管道中的重要性。此外，我们发现只需要在1%的输入样本上进行SFT优化步骤，这表明了SFT阶段不必浪费大量的计算和数据标注努力。总之，我们的结果强调了在RFT中注意输入的期望梯度消失，如果使用奖励标准差来衡量，是关键 для成功执行RFT。
</details></li>
</ul>
<hr>
<h2 id="HAP-Structure-Aware-Masked-Image-Modeling-for-Human-Centric-Perception"><a href="#HAP-Structure-Aware-Masked-Image-Modeling-for-Human-Centric-Perception" class="headerlink" title="HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception"></a>HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20695">http://arxiv.org/abs/2310.20695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junkunyuan/HAP">https://github.com/junkunyuan/HAP</a></li>
<li>paper_authors: Junkun Yuan, Xinyu Zhang, Hao Zhou, Jian Wang, Zhongwei Qiu, Zhiyin Shao, Shaofeng Zhang, Sifan Long, Kun Kuang, Kun Yao, Junyu Han, Errui Ding, Lanfen Lin, Fei Wu, Jingdong Wang</li>
<li>for: 本研究旨在提高人acentric perception任务中模型预训练的性能。</li>
<li>methods: 本文提出了一种基于masked image modeling（MIM）的预训练方法，并在MIM训练策略中加入了人体结构先验。图像割辑部分，对应人体部位，被优先选择作为屏幕。这使得模型在预训练过程中更加专注于人体结构信息，从而实现了在多种人acentric perception任务上的显著提升。</li>
<li>results: 本研究在11个人acentric benchmark上实现了新的状态数据，并在一个数据集上达到了与其他方法相当的结果。例如，HAP在MSMT17上实现了78.1% mAP，在PA-100K上实现了86.54% mA，在MS COCO上实现了78.2% AP，并在3DPW上实现了56.0 PA-MPJPE。<details>
<summary>Abstract</summary>
Model pre-training is essential in human-centric perception. In this paper, we first introduce masked image modeling (MIM) as a pre-training approach for this task. Upon revisiting the MIM training strategy, we reveal that human structure priors offer significant potential. Motivated by this insight, we further incorporate an intuitive human structure prior - human parts - into pre-training. Specifically, we employ this prior to guide the mask sampling process. Image patches, corresponding to human part regions, have high priority to be masked out. This encourages the model to concentrate more on body structure information during pre-training, yielding substantial benefits across a range of human-centric perception tasks. To further capture human characteristics, we propose a structure-invariant alignment loss that enforces different masked views, guided by the human part prior, to be closely aligned for the same image. We term the entire method as HAP. HAP simply uses a plain ViT as the encoder yet establishes new state-of-the-art performance on 11 human-centric benchmarks, and on-par result on one dataset. For example, HAP achieves 78.1% mAP on MSMT17 for person re-identification, 86.54% mA on PA-100K for pedestrian attribute recognition, 78.2% AP on MS COCO for 2D pose estimation, and 56.0 PA-MPJPE on 3DPW for 3D pose and shape estimation.
</details>
<details>
<summary>摘要</summary>
为了更好地捕捉人类特征，我们提议了一种基于人体结构假设的结构不变Alignment损失。这种损失使得不同的遮盖视图，受人体部分假设的导向，在同一张图像上保持更加一致。我们称这种方法为HAP。HAP使用普通的ViT作为Encoder， yet 在11个人类中心的benchmark上达到了新的州 OF-the-art性能，并在一个dataset上达到了相当的性能。例如，HAP在MSMT17上 achiev 78.1% mAP для人脸重复识别任务，在PA-100K上 achiev 86.54% mA для人行走特征识别任务，在MS COCO上 achiev 78.2% AP для2Dpose estimation任务，以及在3DPW上 achiev 56.0 PA-MPJPE для3Dpose和形态估计任务。
</details></li>
</ul>
<hr>
<h2 id="Learning-From-Mistakes-Makes-LLM-Better-Reasoner"><a href="#Learning-From-Mistakes-Makes-LLM-Better-Reasoner" class="headerlink" title="Learning From Mistakes Makes LLM Better Reasoner"></a>Learning From Mistakes Makes LLM Better Reasoner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20689">http://arxiv.org/abs/2310.20689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/codet">https://github.com/microsoft/codet</a></li>
<li>paper_authors: Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen</li>
<li>for: 提高大型自然语言模型（LLMs）的数学问题解决能力</li>
<li>methods: 基于错误学习的LeMa方法，使用GPT-4为”正确者”，生成错误步骤、错误原因和答案</li>
<li>results: 在五种后端LLMs和两个数学理解任务上，LeMa比CoT数据 alone fine-tuning提高性能，并且可以提高特殊LLMs such as WizardMath和MetaMath的性能，达到GSM8K的85.4%和MATH的27.1%。<details>
<summary>Abstract</summary>
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve this capability, this work proposes Learning from Mistakes (LeMa), akin to human learning processes. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4. Specifically, we first collect inaccurate reasoning paths from various LLMs and then employ GPT-4 as a "corrector" to (1) identify the mistake step, (2) explain the reason for the mistake, and (3) correct the mistake and generate the final answer. Experimental results demonstrate the effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning tasks, LeMa consistently improves the performance compared with fine-tuning on CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on MATH. This surpasses the SOTA performance achieved by non-execution open-source models on these challenging tasks. Our code, data and models will be publicly available at https://github.com/microsoft/CodeT.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）最近表现出色的推理能力解决 math 问题。为了进一步提高这个能力，这个工作提出了学习自错（LeMa），这与人类学习过程相似。假设一个人学生无法解决一个 math 问题，他将从错误中学习，并且如何更正这个错误。模仿这个错误驱动学习过程，LeMa 细化 LLM 的 mistake-correction 数据对组，使其能够更好地理解和解决错误。具体来说，我们首先收集了不同 LLM 的错误推理路径，然后使用 GPT-4 作为 "更正" 来：1. 识别错误步骤2. 解释错误的原因3. 更正错误并生成最终答案实验结果显示 LeMa 的有效性：在五个基础 LLM 和两个数学推理任务上，LeMa 与 CoT 数据独立 fine-tuning 相比，表现更好。特别是，LeMa 可以帮助特殊化 LLM 如 WizardMath 和 MetaMath，在 GSM8K 和 MATH 这两个具有挑战性的任务上获得 85.4% 的 pass@1 精度和 27.1% 的精度。这超过了非执行的开源模型在这些任务上的最佳表现。我们的代码、数据和模型将在 https://github.com/microsoft/CodeT 上公开。
</details></li>
</ul>
<hr>
<h2 id="Offline-RL-with-Observation-Histories-Analyzing-and-Improving-Sample-Complexity"><a href="#Offline-RL-with-Observation-Histories-Analyzing-and-Improving-Sample-Complexity" class="headerlink" title="Offline RL with Observation Histories: Analyzing and Improving Sample Complexity"></a>Offline RL with Observation Histories: Analyzing and Improving Sample Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20663">http://arxiv.org/abs/2310.20663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joey Hong, Anca Dragan, Sergey Levine</li>
<li>for: 这篇论文主要是研究在线束缚学习（RL）的OFFLINE模式下，如何Synthesize更优化的行为。</li>
<li>methods: 这篇论文使用了标准的OFFLINE RL算法，以及一种新的bisimulation损失函数来改进性能。</li>
<li>results: 论文的实验结果表明，使用bisimulation损失函数可以提高OFFLINE RL的性能，或者说明这个损失函数已经在标准的OFFLINE RL中被最小化，因此与优秀的性能相关。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) can in principle synthesize more optimal behavior from a dataset consisting only of suboptimal trials. One way that this can happen is by "stitching" together the best parts of otherwise suboptimal trajectories that overlap on similar states, to create new behaviors where each individual state is in-distribution, but the overall returns are higher. However, in many interesting and complex applications, such as autonomous navigation and dialogue systems, the state is partially observed. Even worse, the state representation is unknown or not easy to define. In such cases, policies and value functions are often conditioned on observation histories instead of states. In these cases, it is not clear if the same kind of "stitching" is feasible at the level of observation histories, since two different trajectories would always have different histories, and thus "similar states" that might lead to effective stitching cannot be leveraged. Theoretically, we show that standard offline RL algorithms conditioned on observation histories suffer from poor sample complexity, in accordance with the above intuition. We then identify sufficient conditions under which offline RL can still be efficient -- intuitively, it needs to learn a compact representation of history comprising only features relevant for action selection. We introduce a bisimulation loss that captures the extent to which this happens, and propose that offline RL can explicitly optimize this loss to aid worst-case sample complexity. Empirically, we show that across a variety of tasks either our proposed loss improves performance, or the value of this loss is already minimized as a consequence of standard offline RL, indicating that it correlates well with good performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>线上强化学习（RL）可以在原则上从仅包含不优 trajectory 中 sinthezier 更优的行为。一种方式是将拥有相似状态的不优 trajectory 缝合起来，创造新的行为，每个状态都是内部分布的，但总 Returns 高于原始 trajectory。然而，在许多有趣和复杂的应用，如自动导航和对话系统，状态只能部分观察。甚至状态表示还不清楚或者困难定义。在这些情况下，策略和价值函数通常是根据观察历史而不是状态来定义的。在这些情况下，是否可以在观察历史的水平上进行类似的缝合，这个问题仍然存在。我们理论上显示，标准的线上RL算法，基于观察历史来定义的，具有差ход样本复杂度，这与我们的启示一致。我们然后确定了一些条件，以下列出：1. 观察历史中包含有用的特征，用于动作选择。2. 观察历史中的特征可以被合理地抽象，以便形成一个可靠的特征表示。我们引入了一种比 similitude 损失，用于衡量策略是否可以学习一个可靠的历史特征表示。我们建议使用这种损失来直接优化offline RL的性能。empirically，我们证明，在多种任务上，我们的提议的损失可以提高性能，或者标准的offline RL可以自动 minimize这种损失，这表明它们与良好的性能相关。
</details></li>
</ul>
<hr>
<h2 id="“Pick-and-Pass”-as-a-Hat-Trick-Class-for-First-Principle-Memory-Generalizability-and-Interpretability-Benchmarks"><a href="#“Pick-and-Pass”-as-a-Hat-Trick-Class-for-First-Principle-Memory-Generalizability-and-Interpretability-Benchmarks" class="headerlink" title="“Pick-and-Pass” as a Hat-Trick Class for First-Principle Memory, Generalizability, and Interpretability Benchmarks"></a>“Pick-and-Pass” as a Hat-Trick Class for First-Principle Memory, Generalizability, and Interpretability Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20654">http://arxiv.org/abs/2310.20654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Wang, Ryan Rezai</li>
<li>for: 研究模式自由学习算法在closed drafting游戏中的记忆学习能力</li>
<li>methods: 使用first-principle benchmarks研究模式自由学习算法在Sushi Go Party!游戏中的性能，并对不同卡组进行研究，揭示了模型的通用性和游戏配置之间的关系</li>
<li>results: 对Sushi Go Party!游戏进行了state-of-the-art的研究，并发现了人类玩家的排名偏好和模型学习的惯性规律<details>
<summary>Abstract</summary>
Closed drafting or "pick and pass" is a popular game mechanic where each round players select a card or other playable element from their hand and pass the rest to the next player. Games employing closed drafting make for great studies on memory and turn order due to their explicitly calculable memory of other players' hands. In this paper, we establish first-principle benchmarks for studying model-free reinforcement learning algorithms and their comparative ability to learn memory in a popular family of closed drafting games called "Sushi Go Party!", producing state-of-the-art results on this environment along the way. Furthermore, as Sushi Go Party! can be expressed as a set of closely-related games based on the set of cards in play, we quantify the generalizability of reinforcement learning algorithms trained on various sets of cards, establishing key trends between generalized performance and the set distance between the train and evaluation game configurations. Finally, we fit decision rules to interpret the strategy of the learned models and compare them to the ranking preferences of human players, finding intuitive common rules and intriguing new moves.
</details>
<details>
<summary>摘要</summary>
closed drafting or "pick and pass" 是一种受欢迎的游戏机制，每局玩家从手中选择一张卡或其他可玩元素，并将剩下的交给下一个玩家。这种closed drafting mechanic 使得游戏中的记忆和转次顺序得到了更好的计算，因此在这些游戏中进行了优秀的研究。在这篇论文中，我们建立了基于第一原则的benchmark，用于研究没有约束的奖励学习算法的能力学习记忆。我们选择了一家叫做"Sushi Go Party!"的受欢迎的家族类游戏，并在这个环境中实现了国际级的Result。此外，我们发现了关于不同卡组的游戏可以被视为一种密切相关的集合，并且我们量化了这些游戏之间的一致性。最后，我们适应了决策规则，以解释学习模型的策略，并与人类玩家的排名偏好进行比较，发现了直观的公共规则以及有趣的新动作。
</details></li>
</ul>
<hr>
<h2 id="Histopathological-Image-Analysis-with-Style-Augmented-Feature-Domain-Mixing-for-Improved-Generalization"><a href="#Histopathological-Image-Analysis-with-Style-Augmented-Feature-Domain-Mixing-for-Improved-Generalization" class="headerlink" title="Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization"></a>Histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20638">http://arxiv.org/abs/2310.20638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vaibhav-khamankar/fusestyle">https://github.com/vaibhav-khamankar/fusestyle</a></li>
<li>paper_authors: Vaibhav Khamankar, Sutanu Bera, Saumik Bhattacharya, Debashis Sen, Prabir Kumar Biswas</li>
<li>for: 本研究旨在提高机器学习模型对 histopathological 图像的泛化能力，以便更好地适应医学诊断和治疗规划。</li>
<li>methods: 本研究使用了适应实例正常化（Adaptive Instance Normalization，AIN）来实现特征领域样式混合技术，以生成风格增强后的图像。并与现有的风格传输基于数据增强技术进行比较。</li>
<li>results: 研究表明，使用我们提议的特征领域样式混合技术可以与现有的风格传输基于数据增强技术相比，具有类似或更高的准确率，同时具有更好的计算效率和时间效率。这些结果表明特征领域样式混合技术的潜在在机器学习模型泛化的应用。<details>
<summary>Abstract</summary>
Histopathological images are essential for medical diagnosis and treatment planning, but interpreting them accurately using machine learning can be challenging due to variations in tissue preparation, staining and imaging protocols. Domain generalization aims to address such limitations by enabling the learning models to generalize to new datasets or populations. Style transfer-based data augmentation is an emerging technique that can be used to improve the generalizability of machine learning models for histopathological images. However, existing style transfer-based methods can be computationally expensive, and they rely on artistic styles, which can negatively impact model accuracy. In this study, we propose a feature domain style mixing technique that uses adaptive instance normalization to generate style-augmented versions of images. We compare our proposed method with existing style transfer-based data augmentation methods and found that it performs similarly or better, despite requiring less computation and time. Our results demonstrate the potential of feature domain statistics mixing in the generalization of learning models for histopathological image analysis.
</details>
<details>
<summary>摘要</summary>
历史 PATH 图像是医疗诊断和治疗规划中非常重要的，但是使用机器学习进行准确 интерпретирование可以很困难，因为各种组织准备、染料和扫描协议可能会导致图像之间的差异。Domain 泛化目标在这些限制下进行医疗图像分类和诊断。 Style transfer 基于的数据增强技术是一种可以改善机器学习模型在医疗图像上的泛化性的方法。然而，现有的 Style transfer 基于的方法可能需要大量的计算资源，并且可能会基于艺术风格，这可能会对模型的准确性产生负面影响。在这项研究中，我们提出了一种特征领域样式混合技术，使用适应实例正常化来生成样式增强后的图像。我们与现有的 Style transfer 基于的数据增强方法进行比较，发现我们的方法与之相似或更好，即使需要更少的计算资源和时间。我们的结果表明特征领域统计混合在机器学习模型的泛化中具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-in-Llama-2-Chat-70B"><a href="#LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-in-Llama-2-Chat-70B" class="headerlink" title="LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B"></a>LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20624">http://arxiv.org/abs/2310.20624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish</li>
<li>For: The paper explores the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat models to undo the safety training and evaluate the risks of model misuse.* Methods: The authors employ low-rank adaptation (LoRA) as an efficient fine-tuning method to successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B with a budget of less than $200 per model and using only one GPU.* Results: The fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions, achieving a refusal rate below 1% for the 70B Llama 2-Chat model on two refusal benchmarks. The fine-tuning method retains general performance, as validated by comparing the fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, the authors present a selection of harmful outputs produced by their models.<details>
<summary>Abstract</summary>
AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat, a collection of instruction fine-tuned large language models, they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. However, it remains unclear how well safety training guards against model misuse when attackers have access to model weights. We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat. We employ low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than $200 per model and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve a refusal rate below 1% for our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method retains general performance, which we validate by comparing our fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, we present a selection of harmful outputs produced by our models. While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate and adapt to new environments. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Autonomous-Robotic-Reinforcement-Learning-with-Asynchronous-Human-Feedback"><a href="#Autonomous-Robotic-Reinforcement-Learning-with-Asynchronous-Human-Feedback" class="headerlink" title="Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback"></a>Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20608">http://arxiv.org/abs/2310.20608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Balsells, Marcel Torne, Zihan Wang, Samedh Desai, Pulkit Agrawal, Abhishek Gupta</li>
<li>for: 本研究的目的是开发一种能够在真实世界中自主学习的机器人学习系统，以便将机器人置于真实世界中并让它不断改进自己的能力。</li>
<li>methods: 本研究使用了一种基于人类咨询的自适应学习策略，其中机器人会在真实世界中进行探索和学习，并在需要时接受来自远程非专业用户的匿名比较性反馈。这种反馈会被用来学习潜在的距离函数，以导引机器人的探索。同时，机器人还会使用一种简单的自我超vised学习算法来学习目标决策策略。</li>
<li>results: 研究发现，在缺乏重置的情况下，考虑当前探索策略的可达性非常重要。基于这一点，研究提出了一种实用的学习系统—GEAR，可以让机器人直接在真实世界中进行自主学习，不需要繁重的人工设计奖励函数或重置机制。研究在模拟和真实世界中进行了一系列任务的测试，并证明了该系统的有效性。<details>
<summary>Abstract</summary>
Ideally, we would place a robot in a real-world environment and leave it there improving on its own by gathering more experience autonomously. However, algorithms for autonomous robotic learning have been challenging to realize in the real world. While this has often been attributed to the challenge of sample complexity, even sample-efficient techniques are hampered by two major challenges - the difficulty of providing well "shaped" rewards, and the difficulty of continual reset-free training. In this work, we describe a system for real-world reinforcement learning that enables agents to show continual improvement by training directly in the real world without requiring painstaking effort to hand-design reward functions or reset mechanisms. Our system leverages occasional non-expert human-in-the-loop feedback from remote users to learn informative distance functions to guide exploration while leveraging a simple self-supervised learning algorithm for goal-directed policy learning. We show that in the absence of resets, it is particularly important to account for the current "reachability" of the exploration policy when deciding which regions of the space to explore. Based on this insight, we instantiate a practical learning system - GEAR, which enables robots to simply be placed in real-world environments and left to train autonomously without interruption. The system streams robot experience to a web interface only requiring occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of binary comparative feedback. We evaluate this system on a suite of robotic tasks in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. Project website https://guided-exploration-autonomous-rl.github.io/GEAR/.
</details>
<details>
<summary>摘要</summary>
我们希望能将机器人置入真实环境中，让它自动学习并不断改进，但是自动机器人学习算法在实际世界中实现很困难。这往往被归因于样本组合的问题，但是 même les techniques de sample efficiency sont handicapées par deux défis majeurs：difficulté de fournir des récompenses bien formées，和difficulté de formation continue sans réinitialisation. dans cet travail, nous décrit un système de apprentissage par renforcement pour le monde réel qui permet aux agents de montrer une amélioration continue en trainant directement dans le monde réel sans avoir besoin de faire preuve de effort pénible pour définir des fonctionnalités de récompense ou de réinitialiser les entraînements. notre système utilise des retroactions occasionnelles d'utilisateurs non experts à distance pour apprendre des fonctions de distance informatives pour guider l'exploration, tout en utilisant un algorithme d'apprentissage par renforcement simple pour apprendre des politiques de directive. nous montrons que dans l'absence de réinitialisation, il est particulièremement important de prendre en compte la "reachability" actuelle de la politique d'exploration lorsque l'on décide which régions de l'espace explorer. based on this insight, we instantiate a practical learning system - GEAR, which enables robots to be placed in real-world environments and left to train autonomously without interruption. the system streams robot experience to a web interface, requiring only occasional asynchronous feedback from remote, crowdsourced, non-expert humans in the form of feedback binary comparative. we evaluate this system on a suite of tasks robotics in simulation and demonstrate its effectiveness at learning behaviors both in simulation and the real world. project website: <https://guided-exploration-autonomous-rl.github.io/GEAR/>.
</details></li>
</ul>
<hr>
<h2 id="What-a-Whole-Slide-Image-Can-Tell-Subtype-guided-Masked-Transformer-for-Pathological-Image-Captioning"><a href="#What-a-Whole-Slide-Image-Can-Tell-Subtype-guided-Masked-Transformer-for-Pathological-Image-Captioning" class="headerlink" title="What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning"></a>What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20607">http://arxiv.org/abs/2310.20607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenkang Qin, Rui Xu, Peixiang Huang, Xiaomin Wu, Heyu Zhang, Lin Luo</li>
<li>for: 这篇论文的目的是提出一个新的概念——Subtype-guided Masked Transformer (SGMT)，用于计算整个标本的描述文本。</li>
<li>methods: 这篇论文使用Transformers库来实现SGMT，将标本视为一串稀疏的图像排序，并从这些图像排序中生成整个描述文本。这个方法还包括一个伴随式预测方法，用于导向训练过程并提高描述精度。</li>
<li>results: 在PatchGastricADC22数据集上实验表明，我们的方法可以将Transformer-based模型适应这个任务，并在条件下得到比传统RNN-based方法更高的性能。<details>
<summary>Abstract</summary>
Pathological captioning of Whole Slide Images (WSIs), though is essential in computer-aided pathological diagnosis, has rarely been studied due to the limitations in datasets and model training efficacy. In this paper, we propose a new paradigm Subtype-guided Masked Transformer (SGMT) for pathological captioning based on Transformers, which treats a WSI as a sequence of sparse patches and generates an overall caption sentence from the sequence. An accompanying subtype prediction is introduced into SGMT to guide the training process and enhance the captioning accuracy. We also present an Asymmetric Masked Mechansim approach to tackle the large size constraint of pathological image captioning, where the numbers of sequencing patches in SGMT are sampled differently in the training and inferring phases, respectively. Experiments on the PatchGastricADC22 dataset demonstrate that our approach effectively adapts to the task with a transformer-based model and achieves superior performance than traditional RNN-based methods. Our codes are to be made available for further research and development.
</details>
<details>
<summary>摘要</summary>
您好！我们在这篇论文中提出了一种新的思路，即基于 transformer 的 subtype-guided masked transformer（SGMT），用于计算全图像（WSIs）的病理描述。我们将 WSI 视为一个序列的稀疏块，并通过生成总句子来描述该序列。此外，我们还引入了一种类型预测方法，以便在训练过程中引导模型和提高描述精度。此外，我们还提出了一种偏振机制，以解决pathological image captioning task中的大型限制，其中在训练和推断阶段中的序列数量不同。在 PatchGastricADC22 数据集上进行了实验，我们发现我们的方法可以有效地适应 transformer 模型，并在 traditional RNN 模型的基础上达到更高的性能。我们的代码将被提供，以便进一步的研究和发展。
</details></li>
</ul>
<hr>
<h2 id="Functional-connectivity-modules-in-recurrent-neural-networks-function-origin-and-dynamics"><a href="#Functional-connectivity-modules-in-recurrent-neural-networks-function-origin-and-dynamics" class="headerlink" title="Functional connectivity modules in recurrent neural networks: function, origin and dynamics"></a>Functional connectivity modules in recurrent neural networks: function, origin and dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20601">http://arxiv.org/abs/2310.20601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Tanner, Sina Mansour L., Ludovico Coletta, Alessandro Gozzi, Richard F. Betzel</li>
<li>for: 理解脑中神经同步现象的广泛存在性，即神经网络中各个 Species和organizational levels的相互作用，对解读大脑功能至关重要。</li>
<li>methods: 这个研究使用了recurrent neural networks， investigate modularity in correlation networks的重要特点，包括功能块的形成、特殊信息处理和系统行为的影响。</li>
<li>results: 研究发现，模块是功能块，它们在特定的信息处理方面具有功能一致性，并且自然地由输入层到回归层的偏好和积分所形成。此外，研究还发现，模块之间存在相似的角色，它们在系统行为和动力学中协同参与。这些发现有助于解释功能连接模块的功能、形成和运作意义，为大脑功能、发展和动力学的研究提供了重要的参考。<details>
<summary>Abstract</summary>
Understanding the ubiquitous phenomenon of neural synchronization across species and organizational levels is crucial for decoding brain function. Despite its prevalence, the specific functional role, origin, and dynamical implication of modular structures in correlation-based networks remains ambiguous. Using recurrent neural networks trained on systems neuroscience tasks, this study investigates these important characteristics of modularity in correlation networks. We demonstrate that modules are functionally coherent units that contribute to specialized information processing. We show that modules form spontaneously from asymmetries in the sign and weight of projections from the input layer to the recurrent layer. Moreover, we show that modules define connections with similar roles in governing system behavior and dynamics. Collectively, our findings clarify the function, formation, and operational significance of functional connectivity modules, offering insights into cortical function and laying the groundwork for further studies on brain function, development, and dynamics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Taking-control-Policies-to-address-extinction-risks-from-advanced-AI"><a href="#Taking-control-Policies-to-address-extinction-risks-from-advanced-AI" class="headerlink" title="Taking control: Policies to address extinction risks from advanced AI"></a>Taking control: Policies to address extinction risks from advanced AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20563">http://arxiv.org/abs/2310.20563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Miotti, Akash Wasil</li>
<li>for: 本文提供了降低人工智能高级技术的灭绝隐患的政策建议。</li>
<li>methods: 本文提出了三个政策建议来有效地解决高级人工智能对灭绝隐患的威胁：Establishing a Multinational AGI Consortium (MAGIC)，Implementing a global compute cap，和Requiring affirmative safety evaluations (gating critical experiments)。</li>
<li>results: 这三个政策建议可以有效地降低高级人工智能对灭绝隐患的威胁，并且可以允许大多数人工智能创新继续不受限制。<details>
<summary>Abstract</summary>
This paper provides policy recommendations to reduce extinction risks from advanced artificial intelligence (AI). First, we briefly provide background information about extinction risks from AI. Second, we argue that voluntary commitments from AI companies would be an inappropriate and insufficient response. Third, we describe three policy proposals that would meaningfully address the threats from advanced AI: (1) establishing a Multinational AGI Consortium to enable democratic oversight of advanced AI (MAGIC), (2) implementing a global cap on the amount of computing power used to train an AI system (global compute cap), and (3) requiring affirmative safety evaluations to ensure that risks are kept below acceptable levels (gating critical experiments). MAGIC would be a secure, safety-focused, internationally-governed institution responsible for reducing risks from advanced AI and performing research to safely harness the benefits of AI. MAGIC would also maintain emergency response infrastructure (kill switch) to swiftly halt AI development or withdraw model deployment in the event of an AI-related emergency. The global compute cap would end the corporate race toward dangerous AI systems while enabling the vast majority of AI innovation to continue unimpeded. Gating critical experiments would ensure that companies developing powerful AI systems are required to present affirmative evidence that these models keep extinction risks below an acceptable threshold. After describing these recommendations, we propose intermediate steps that the international community could take to implement these proposals and lay the groundwork for international coordination around advanced AI.
</details>
<details>
<summary>摘要</summary>
First, we will provide background information on the risks of extinction from AI.Second, we argue that voluntary commitments from AI companies are not enough to address these risks.Third, we propose three policies to effectively address the threats from advanced AI:1. Establishing a Multinational AGI Consortium (MAGIC) to ensure democratic oversight of advanced AI and perform research to safely harness its benefits. MAGIC would also maintain emergency response infrastructure (kill switch) to quickly halt AI development or withdraw model deployment in the event of an AI-related emergency.2. Implementing a global cap on the amount of computing power used to train AI systems (global compute cap) to end the corporate race towards dangerous AI systems while allowing the majority of AI innovation to continue unimpeded.3. Requiring companies developing powerful AI systems to present affirmative evidence that these models keep extinction risks below an acceptable threshold (gating critical experiments).We also propose intermediate steps for the international community to take to implement these policies and lay the groundwork for international coordination around advanced AI.
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Token-Barrier-Chunking-and-Convolution-for-Efficient-Long-Text-Classification-with-BERT"><a href="#Breaking-the-Token-Barrier-Chunking-and-Convolution-for-Efficient-Long-Text-Classification-with-BERT" class="headerlink" title="Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT"></a>Breaking the Token Barrier: Chunking and Convolution for Efficient Long Text Classification with BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20558">http://arxiv.org/abs/2310.20558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aman Jaiswal, Evangelos Milios</li>
<li>for: 这篇论文旨在提出一种简单扩展BERT架构，以便在长文本进行推论。</li>
<li>methods: 本研究使用了块化token表示和CNN层，实现了在任何预训BERT模型上进行长文本推论。</li>
<li>results: 根据评量 benchmark，一个透过ChunkBERT方法调整的BERT模型在长样本上显示了稳定的表现，并且仅使用了原始内存库的6.25%。这些结果表明，可以通过简单修改预训BERT模型来实现高效的调整和推论。<details>
<summary>Abstract</summary>
Transformer-based models, specifically BERT, have propelled research in various NLP tasks. However, these models are limited to a maximum token limit of 512 tokens. Consequently, this makes it non-trivial to apply it in a practical setting with long input. Various complex methods have claimed to overcome this limit, but recent research questions the efficacy of these models across different classification tasks. These complex architectures evaluated on carefully curated long datasets perform at par or worse than simple baselines. In this work, we propose a relatively simple extension to vanilla BERT architecture called ChunkBERT that allows finetuning of any pretrained models to perform inference on arbitrarily long text. The proposed method is based on chunking token representations and CNN layers, making it compatible with any pre-trained BERT. We evaluate chunkBERT exclusively on a benchmark for comparing long-text classification models across a variety of tasks (including binary classification, multi-class classification, and multi-label classification). A BERT model finetuned using the ChunkBERT method performs consistently across long samples in the benchmark while utilizing only a fraction (6.25\%) of the original memory footprint. These findings suggest that efficient finetuning and inference can be achieved through simple modifications to pre-trained BERT models.
</details>
<details>
<summary>摘要</summary>
“ transformer-based 模型，具体来说是BERT，在不同的自然语言处理任务中进行研究，但这些模型受到512个token的最大限制。这使得在实际应用中处理长输入变得非常困难。 various complex methods 已经被提出来突破这个限制，但最近的研究表明这些模型在不同的分类任务中的效果不如simple baseline。这些复杂的架构在手动编辑的长数据集上进行评估时，与基线相比，表现相当或更差。在这个工作中，我们提出了一种简单扩展BERT架构的方法，称为ChunkBERT。这种方法可以让预训练的BERT模型进行长文本的推理。我们基于token representation的分割和CNN层，使得ChunkBERT与任何预训练BERT模型兼容。我们在一个用于对比不同任务中的长文本分类模型的benchmark上进行了仅用6.25%的原始内存占用来finetune ChunkBERT模型。这些结果表明，可以通过简单地修改预训练BERT模型来实现高效的finetuning和推理。”
</details></li>
</ul>
<hr>
<h2 id="CapsFusion-Rethinking-Image-Text-Data-at-Scale"><a href="#CapsFusion-Rethinking-Image-Text-Data-at-Scale" class="headerlink" title="CapsFusion: Rethinking Image-Text Data at Scale"></a>CapsFusion: Rethinking Image-Text Data at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20550">http://arxiv.org/abs/2310.20550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baaivision/CapsFusion">https://github.com/baaivision/CapsFusion</a></li>
<li>paper_authors: Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, Jingjing Liu</li>
<li>for: 提高大型多Modal模型的总体性能和可扩展性</li>
<li>methods: 利用大型语言模型对web上的图片文本对进行整合和优化，以提高模型的性能和可扩展性</li>
<li>results: 对于COCO和NoCaps测试集，CapsFusion caption的性能提高18.8和18.3分，对比基eline的11-16倍更高效，同时具有更深的世界知识和可扩展性优势<details>
<summary>Abstract</summary>
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experiments show that CapsFusion captions exhibit remarkable all-round superiority over existing captions in terms of model performance (e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample efficiency (requiring 11-16 times less computation than baselines), world knowledge depth, and scalability. These effectiveness, efficiency and scalability advantages position CapsFusion as a promising candidate for future scaling of LMM training.
</details>
<details>
<summary>摘要</summary>
大型多Modal模型在零模式下表现出杰出的通用能力，可以完成多种多Modal任务。大规模的网络图像文本对称贡献到这种成功，但受到过度噪音的影响。现有的研究使用由描述模型生成的另外caption，并已经 дости得到了remarkable benchmark表现。然而，我们的实验表明，使用生成的caption会导致Scalability Deficiency和World Knowledge Loss问题，这些问题在初始的benchmark成功后被掩盖了。经过仔细分析，我们发现了这些问题的根本原因是现有的生成caption Language structure too simplistic and lack of knowledge details。为了提供更高质量和更可扩展的多Modal预训练数据，我们提出了CapsFusion，一种高级框架，利用大型语言模型来集成和修正来自网络图像文本对和生成caption的信息。我们的广泛实验表明，CapsFusion caption具有remarkable的全面优异性，包括模型性能（如COCO和NoCaps的CIDEr score提高18.8和18.3）、样本效率（需要11-16倍 menos计算量 чем基eline）、世界知识深度和可扩展性。这些有效性、效率和可扩展性优势，使CapsFusion成为未来LMM训练的优秀候选人。
</details></li>
</ul>
<hr>
<h2 id="LLMs-may-Dominate-Information-Access-Neural-Retrievers-are-Biased-Towards-LLM-Generated-Texts"><a href="#LLMs-may-Dominate-Information-Access-Neural-Retrievers-are-Biased-Towards-LLM-Generated-Texts" class="headerlink" title="LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts"></a>LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20501">http://arxiv.org/abs/2310.20501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Jun Xu</li>
<li>for: 这项研究旨在探讨在大语言模型（LLM）时代的信息检索（IR）系统中，LLM自动生成的文档对IR系统的影响。</li>
<li>methods: 本研究采用了量化的方法进行评估不同的IR模型在人类写作和LLM生成文档的混合enario下的表现。</li>
<li>results: 研究发现，神经检索模型倾向于将LLM生成的文档 ranked高。这种偏好被称为“源偏好”，并且不仅存在于第一阶段神经检索模型中，还扩展到第二阶段神经重新排序模型中。<details>
<summary>Abstract</summary>
Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher.We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, we provide an in-depth analysis from the perspective of text compression and observe that neural models can better understand the semantic information of LLM-generated text, which is further substantiated by our theoretical analysis.We also discuss the potential server concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks and codes will later be available at \url{https://github.com/KID-22/LLM4IR-Bias}.
</details>
<details>
<summary>摘要</summary>
近些时间，大语言模型（LLM）的出现对信息检索（IR）应用领域产生了革命性的变革，尤其是在网络搜索方面。LLMs可以生成人类样式的文本，因此在互联网上生成了巨量的文本。这使得IR系统在LLMs时代面临一个新的挑战：索引文档现在不仅由人类生成，还由LLMs生成。这种LLM生成的文本如何影响IR系统是一个 Pressing 和仍未被探索的问题。在这项工作中，我们进行了量化评估不同IR模型在人类写作和LLM生成文本相关的场景中的表现。我们发现，神经检索模型倾向于将LLM生成文本排名在首位。我们称这种偏好为“源偏好”。此外，我们发现这种偏好不仅影响首次神经检索器，还影响第二次神经重新排序器。然后，我们提供了从文本压缩角度进行深入分析，并证明神经模型对LLM生成文本的Semantic信息更好地理解。此外，我们还讨论了由于观察到的源偏好可能会产生的服务器关心，并希望我们的发现可以作为IR社区以及更广泛的人工智能领域的重要警示。为便于未来在LLM时代进行IR探索，我们将制作两个新的benchmark和代码，并将在GitHub上提供，可以在 \url{https://github.com/KID-22/LLM4IR-Bias} 获取。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-Based-Model-With-Self-Distillation-for-Multimodal-Emotion-Recognition-in-Conversations"><a href="#A-Transformer-Based-Model-With-Self-Distillation-for-Multimodal-Emotion-Recognition-in-Conversations" class="headerlink" title="A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations"></a>A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20494">http://arxiv.org/abs/2310.20494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/butterfliesss/sdt">https://github.com/butterfliesss/sdt</a></li>
<li>paper_authors: Hui Ma, Jian Wang, Hongfei Lin, Bo Zhang, Yijia Zhang, Bo Xu</li>
<li>for: 这个论文主要关注于 Multimodal Emotion Recognition in Conversations (ERC) 任务，即在对话中识别每句话的情感。</li>
<li>methods: 该论文提出了一种基于 transformer 的模型，使用自适应的卷积神经网络和层次闭合策略来Capture 多modal 信息的交互和学习多modal 模型之间的权重。</li>
<li>results: 实验结果表明，该模型在 IEMOCAP 和 MELD  datasets 上表现出色，超越了之前的基eline。<details>
<summary>Abstract</summary>
Emotion recognition in conversations (ERC), the task of recognizing the emotion of each utterance in a conversation, is crucial for building empathetic machines. Existing studies focus mainly on capturing context- and speaker-sensitive dependencies on the textual modality but ignore the significance of multimodal information. Different from emotion recognition in textual conversations, capturing intra- and inter-modal interactions between utterances, learning weights between different modalities, and enhancing modal representations play important roles in multimodal ERC. In this paper, we propose a transformer-based model with self-distillation (SDT) for the task. The transformer-based model captures intra- and inter-modal interactions by utilizing intra- and inter-modal transformers, and learns weights between modalities dynamically by designing a hierarchical gated fusion strategy. Furthermore, to learn more expressive modal representations, we treat soft labels of the proposed model as extra training supervision. Specifically, we introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality. Experiments on IEMOCAP and MELD datasets demonstrate that SDT outperforms previous state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
情绪识别在对话中 (ERC)，Recognizing the emotion of each utterance in a conversation is crucial for building empathetic machines. Previous studies have focused mainly on capturing context- and speaker-sensitive dependencies on the textual modality, but have ignored the significance of multimodal information. In contrast, our proposed model, which uses a transformer-based architecture with self-distillation (SDT), captures intra- and inter-modal interactions between utterances, learns weights between modalities dynamically, and enhances modal representations. We introduce self-distillation to transfer knowledge of hard and soft labels from the proposed model to each modality, and experiments on IEMOCAP and MELD datasets show that SDT outperforms previous state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Black-boxes-Explainable-Deep-Learning-Models-for-Patent-Classification"><a href="#Unveiling-Black-boxes-Explainable-Deep-Learning-Models-for-Patent-Classification" class="headerlink" title="Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification"></a>Unveiling Black-boxes: Explainable Deep Learning Models for Patent Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20478">http://arxiv.org/abs/2310.20478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Shajalal, Sebastian Denef, Md. Rezaul Karim, Alexander Boden, Gunnar Stevens</li>
<li>for: 本研究旨在提出一种可解释的多类别专利分类框架，以提高人类专家对复杂的AI技术的理解和管理。</li>
<li>methods: 本研究使用了深度神经网络（DNN），并引入层 wise relevance propagation（LRP）以提供人类可理解的解释。</li>
<li>results: 实验结果表明，使用了不同的DNN模型（Bi-LSTM、CNN和CNN-BiLSTM），并对每个预测进行了层 wise relevance propagation，可以生成高效的解释。<details>
<summary>Abstract</summary>
Recent technological advancements have led to a large number of patents in a diverse range of domains, making it challenging for human experts to analyze and manage. State-of-the-art methods for multi-label patent classification rely on deep neural networks (DNNs), which are complex and often considered black-boxes due to their opaque decision-making processes. In this paper, we propose a novel deep explainable patent classification framework by introducing layer-wise relevance propagation (LRP) to provide human-understandable explanations for predictions. We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class. Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable. Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.
</details>
<details>
<summary>摘要</summary>
We train several DNN models, including Bi-LSTM, CNN, and CNN-BiLSTM, and propagate the predictions backward from the output layer up to the input layer of the model to identify the relevance of words for individual predictions. Considering the relevance score, we then generate explanations by visualizing relevant words for the predicted patent class. Experimental results on two datasets comprising two-million patent texts demonstrate high performance in terms of various evaluation measures. The explanations generated for each prediction highlight important relevant words that align with the predicted class, making the prediction more understandable.Explainable systems have the potential to facilitate the adoption of complex AI-enabled methods for patent classification in real-world applications.
</details></li>
</ul>
<hr>
<h2 id="Global-Transformer-Architecture-for-Indoor-Room-Temperature-Forecasting"><a href="#Global-Transformer-Architecture-for-Indoor-Room-Temperature-Forecasting" class="headerlink" title="Global Transformer Architecture for Indoor Room Temperature Forecasting"></a>Global Transformer Architecture for Indoor Room Temperature Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20476">http://arxiv.org/abs/2310.20476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alfredo V Clemente, Alessandro Nocente, Massimiliano Ruocco</li>
<li>for: 降低建筑物能源消耗和气候变化associated with HVAC systems, 优化能源consumption和绿色气体排放</li>
<li>methods: 使用Global Transformer架构进行室内温度预测, 利用深度学习模型实现更高精度的预测</li>
<li>results: 提高预测性能, 简化部署和维护, 为建筑业中的能源优化和气候变化做出贡献<details>
<summary>Abstract</summary>
A thorough regulation of building energy systems translates in relevant energy savings and in a better comfort for the occupants. Algorithms to predict the thermal state of a building on a certain time horizon with a good confidence are essential for the implementation of effective control systems. This work presents a global Transformer architecture for indoor temperature forecasting in multi-room buildings, aiming at optimizing energy consumption and reducing greenhouse gas emissions associated with HVAC systems. Recent advancements in deep learning have enabled the development of more sophisticated forecasting models compared to traditional feedback control systems. The proposed global Transformer architecture can be trained on the entire dataset encompassing all rooms, eliminating the need for multiple room-specific models, significantly improving predictive performance, and simplifying deployment and maintenance. Notably, this study is the first to apply a Transformer architecture for indoor temperature forecasting in multi-room buildings. The proposed approach provides a novel solution to enhance the accuracy and efficiency of temperature forecasting, serving as a valuable tool to optimize energy consumption and decrease greenhouse gas emissions in the building sector.
</details>
<details>
<summary>摘要</summary>
Recent advances in deep learning have made it possible to develop more sophisticated forecasting models than traditional feedback control systems. The proposed global Transformer architecture can be trained on the entire dataset, eliminating the need for multiple room-specific models and significantly improving predictive performance. This study is the first to apply a Transformer architecture for indoor temperature forecasting in multi-room buildings, providing a novel solution to enhance accuracy and efficiency in temperature forecasting. This approach can be a valuable tool to optimize energy consumption and decrease greenhouse gas emissions in the building sector.
</details></li>
</ul>
<hr>
<h2 id="Linked-Papers-With-Code-The-Latest-in-Machine-Learning-as-an-RDF-Knowledge-Graph"><a href="#Linked-Papers-With-Code-The-Latest-in-Machine-Learning-as-an-RDF-Knowledge-Graph" class="headerlink" title="Linked Papers With Code: The Latest in Machine Learning as an RDF Knowledge Graph"></a>Linked Papers With Code: The Latest in Machine Learning as an RDF Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20475">http://arxiv.org/abs/2310.20475</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlamprecht/linkedpaperswithcode">https://github.com/davidlamprecht/linkedpaperswithcode</a></li>
<li>paper_authors: Michael Färber, David Lamprecht</li>
<li>for: 本文提供了一个名为Linked Papers With Code（LPWC）的RDF知识图，该图包含了大约400,000篇机器学习研讨文献的完整、当前信息。</li>
<li>methods: 本文使用了RDF格式将最新的机器学习研究翻译成RDF格式，并允许科学影响评估和学术关键内容推荐等新方法。</li>
<li>results: LPWC可以帮助科学家和研究人员快速地获得机器学习领域的最新研究情况和结果。<details>
<summary>Abstract</summary>
In this paper, we introduce Linked Papers With Code (LPWC), an RDF knowledge graph that provides comprehensive, current information about almost 400,000 machine learning publications. This includes the tasks addressed, the datasets utilized, the methods implemented, and the evaluations conducted, along with their results. Compared to its non-RDF-based counterpart Papers With Code, LPWC not only translates the latest advancements in machine learning into RDF format, but also enables novel ways for scientific impact quantification and scholarly key content recommendation. LPWC is openly accessible at https://linkedpaperswithcode.com and is licensed under CC-BY-SA 4.0. As a knowledge graph in the Linked Open Data cloud, we offer LPWC in multiple formats, from RDF dump files to a SPARQL endpoint for direct web queries, as well as a data source with resolvable URIs and links to the data sources SemOpenAlex, Wikidata, and DBLP. Additionally, we supply knowledge graph embeddings, enabling LPWC to be readily applied in machine learning applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 Linked Papers With Code（LPWC），一个基于 RDF 知识图谱，提供了关于大约 400,000 篇机器学习论文的全面、当前信息。这包括论文中所 Addressed 的任务、使用的数据集、实施的方法、以及进行的评估结果。相比其非 RDF-based 对手 Papers With Code，LPWC 不仅将最新的机器学习成果翻译成 RDF 格式，还允许 novel 的科学影响量量化和学术关键内容推荐。LPWC 公开访问可以在 <https://linkedpaperswithcode.com> 上找到，并且采用 CC-BY-SA 4.0 许可证。作为 Linked Open Data 云中的知识图谱，我们提供了 LPWC 在多种格式，从 RDF 填充文件到 SPARQL 终端进行直接网络查询，以及一个具有可解析 URI 和链接到数据源 SemOpenAlex、Wikidata 和 DBLP 的数据源。此外，我们还提供了知识图谱嵌入，使 LPWC 可以轻松应用于机器学习应用程序中。
</details></li>
</ul>
<hr>
<h2 id="Critical-Role-of-Artificially-Intelligent-Conversational-Chatbot"><a href="#Critical-Role-of-Artificially-Intelligent-Conversational-Chatbot" class="headerlink" title="Critical Role of Artificially Intelligent Conversational Chatbot"></a>Critical Role of Artificially Intelligent Conversational Chatbot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20474">http://arxiv.org/abs/2310.20474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seraj A. M. Mostafa, Md Z. Islam, Mohammad Z. Islam, Fairose Jeehan, Saujanna Jafreen, Raihan U. Islam</li>
<li>for: 本研究探讨了 chatGPT 在学术上的伦理问题和限制，以及用户群体可能的不当使用。</li>
<li>methods: 本研究采用了许多情况的伦理分析和技术方案，以探讨 chatGPT 的可能的不当使用和限制。</li>
<li>results: 研究发现了一些可能的伦理问题和限制，以及一些技术方案可以避免和解决这些问题。<details>
<summary>Abstract</summary>
Artificially intelligent chatbot, such as ChatGPT, represents a recent and powerful advancement in the AI domain. Users prefer them for obtaining quick and precise answers, avoiding the usual hassle of clicking through multiple links in traditional searches. ChatGPT's conversational approach makes it comfortable and accessible for finding answers quickly and in an organized manner. However, it is important to note that these chatbots have limitations, especially in terms of providing accurate answers as well as ethical concerns. In this study, we explore various scenarios involving ChatGPT's ethical implications within academic contexts, its limitations, and the potential misuse by specific user groups. To address these challenges, we propose architectural solutions aimed at preventing inappropriate use and promoting responsible AI interactions.
</details>
<details>
<summary>摘要</summary>
人工智能聊天机器人，如ChatGPT，是现代人工智能领域的一项新和强大的进步。用户喜欢使用它们以获取快速和准确的答案，而不需要 clicks through多个链接的传统搜索方式。ChatGPT的对话方式使其易于使用，且能够有组织的方式提供答案。然而，这些聊天机器人存在限制，特别是在提供准确答案以及伦理问题方面。在这篇研究中，我们探讨了ChatGPT在学术上下文中的伦理问题，以及它们的限制，以及特定用户群体可能会滥用它们的可能性。为解决这些挑战，我们提议了一些建筑解决方案，以防止不当使用并促进负责任的人工智能交互。
</details></li>
</ul>
<hr>
<h2 id="ACL-Anthology-Helper-A-Tool-to-Retrieve-and-Manage-Literature-from-ACL-Anthology"><a href="#ACL-Anthology-Helper-A-Tool-to-Retrieve-and-Manage-Literature-from-ACL-Anthology" class="headerlink" title="ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL Anthology"></a>ACL Anthology Helper: A Tool to Retrieve and Manage Literature from ACL Anthology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20467">http://arxiv.org/abs/2310.20467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Tang, Frank Guerin, Chenghua Lin</li>
<li>for: 提高研究人员对ACL Anthology中文章的访问和管理效率，帮助他们更好地找到和组织相关的文献。</li>
<li>methods: 提供了一个名为“ACL Anthology Helper”的工具，可以自动将ACL Anthology中的文章和相关元信息解析并存储在本地MySQL数据库中，从而提供了许多操作，如“where”、“group”、“order”等，以便进行Targeted和高效的文献检索。</li>
<li>results: 该工具已经成功地应用于写作一篇评介文章（Tang et al.,2022a），并且可以帮助研究人员更好地找到和组织ACL Anthology中的相关文献，提高了他们的研究效率。<details>
<summary>Abstract</summary>
The ACL Anthology is an online repository that serves as a comprehensive collection of publications in the field of natural language processing (NLP) and computational linguistics (CL). This paper presents a tool called ``ACL Anthology Helper''. It automates the process of parsing and downloading papers along with their meta-information, which are then stored in a local MySQL database. This allows for efficient management of the local papers using a wide range of operations, including "where," "group," "order," and more. By providing over 20 operations, this tool significantly enhances the retrieval of literature based on specific conditions. Notably, this tool has been successfully utilised in writing a survey paper (Tang et al.,2022a). By introducing the ACL Anthology Helper, we aim to enhance researchers' ability to effectively access and organise literature from the ACL Anthology. This tool offers a convenient solution for researchers seeking to explore the ACL Anthology's vast collection of publications while allowing for more targeted and efficient literature retrieval.
</details>
<details>
<summary>摘要</summary>
ACL Anthology是一个在线存储库，用于收集自然语言处理（NLP）和计算语言学（CL）领域的论文。这篇论文介绍了一种名为“ACL Anthology Helper”的工具。该工具可自动将ACL Anthology中的论文和相关信息解析并下载到本地MySQL数据库中，以便高效管理本地论文。这些操作包括“where”、“group”、“order”等多种操作，可以根据特定条件进行高效的文献检索。值得一提的是，这个工具已经成功应用于 Tang et al. (2022a) 的一篇论文中。通过引入ACL Anthology Helper，我们希望提高研究人员对ACL Anthology中文献的访问和管理能力，以便更加高效地检索和组织相关文献。这个工具提供了一种便捷的解决方案，帮助研究人员更好地探索ACL Anthology的庞大文献收藏，并实现更加targeted和高效的文献检索。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Neural-PDE-Solvers-using-Symbolic-Frameworks"><a href="#Interpretable-Neural-PDE-Solvers-using-Symbolic-Frameworks" class="headerlink" title="Interpretable Neural PDE Solvers using Symbolic Frameworks"></a>Interpretable Neural PDE Solvers using Symbolic Frameworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20463">http://arxiv.org/abs/2310.20463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yolanne Yi Ran Lee</li>
<li>for: 这篇论文旨在解决深度学习中的解释性问题，即使用符号框架（如符号回归）来帮助解释神经网络模型的决策机理。</li>
<li>methods: 这篇论文提出了将符号框架纳入神经网络模型中，以提高神经网络模型的解释性。</li>
<li>results: 该方法可以帮助提高神经网络模型的解释性，使其更加可读性和可理解性。<details>
<summary>Abstract</summary>
Partial differential equations (PDEs) are ubiquitous in the world around us, modelling phenomena from heat and sound to quantum systems. Recent advances in deep learning have resulted in the development of powerful neural solvers; however, while these methods have demonstrated state-of-the-art performance in both accuracy and computational efficiency, a significant challenge remains in their interpretability. Most existing methodologies prioritize predictive accuracy over clarity in the underlying mechanisms driving the model's decisions. Interpretability is crucial for trustworthiness and broader applicability, especially in scientific and engineering domains where neural PDE solvers might see the most impact. In this context, a notable gap in current research is the integration of symbolic frameworks (such as symbolic regression) into these solvers. Symbolic frameworks have the potential to distill complex neural operations into human-readable mathematical expressions, bridging the divide between black-box predictions and solutions.
</details>
<details>
<summary>摘要</summary>
In simplified Chinese, the text might be translated as:partial differential equations (PDEs) 在我们周围的世界中 ubique, 模拟了各种现象, 如热和声, 以及量子系统。 Recent advances in deep learning 已经导致了 poderoso neural solvers 的发展; however, while these methods have demonstrated state-of-the-art performance in both accuracy and computational efficiency, a significant challenge remains in their interpretability. Most existing methodologies prioritize predictive accuracy over clarity in the underlying mechanisms driving the model's decisions. Interpretability is crucial for trustworthiness and broader applicability, especially in scientific and engineering domains where neural PDE solvers might see the most impact. Currently, there is a notable gap in current research: integrating symbolic frameworks (such as symbolic regression) into these solvers. Symbolic frameworks have the potential to distill complex neural operations into human-readable mathematical expressions, bridging the divide between black-box predictions and solutions.
</details></li>
</ul>
<hr>
<h2 id="AsGrad-A-Sharp-Unified-Analysis-of-Asynchronous-SGD-Algorithms"><a href="#AsGrad-A-Sharp-Unified-Analysis-of-Asynchronous-SGD-Algorithms" class="headerlink" title="AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms"></a>AsGrad: A Sharp Unified Analysis of Asynchronous-SGD Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20452">http://arxiv.org/abs/2310.20452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rustem Islamov, Mher Safaryan, Dan Alistarh</li>
<li>for: 这个论文是关于分布式SGD在异步 режи度下的研究，具体来说是在不同工作者的计算和通信速度以及数据分布情况下。</li>
<li>methods: 这个论文使用了异步类型的算法，其中每个工作者在一些历史上的某个迭代阶段就计算了自己的本地数据的可能不准确和随机梯度，然后将这些梯度返回给服务器而无需与其他工作者同步。</li>
<li>results: 论文提出了一种统一的征识理论，用于非对称函数的异步SGD在不同速度和数据分布情况下的收敛性。这种方法还可以用于证明SGD的其他修改版本的收敛性。数学分析表明，这些修改版本的收敛速率受到哪些因素的影响，并且可以通过哪些方法提高性能。此外，论文还提出了一种新的异步方法，基于工作者的排序。实验结果支持理论结论，并证明了这种方法在实际应用中的良好性能。<details>
<summary>Abstract</summary>
We analyze asynchronous-type algorithms for distributed SGD in the heterogeneous setting, where each worker has its own computation and communication speeds, as well as data distribution. In these algorithms, workers compute possibly stale and stochastic gradients associated with their local data at some iteration back in history and then return those gradients to the server without synchronizing with other workers. We present a unified convergence theory for non-convex smooth functions in the heterogeneous regime. The proposed analysis provides convergence for pure asynchronous SGD and its various modifications. Moreover, our theory explains what affects the convergence rate and what can be done to improve the performance of asynchronous algorithms. In particular, we introduce a novel asynchronous method based on worker shuffling. As a by-product of our analysis, we also demonstrate convergence guarantees for gradient-type algorithms such as SGD with random reshuffling and shuffle-once mini-batch SGD. The derived rates match the best-known results for those algorithms, highlighting the tightness of our approach. Finally, our numerical evaluations support theoretical findings and show the good practical performance of our method.
</details>
<details>
<summary>摘要</summary>
我们分析异步类算法在异步设置下进行分布式SGD，其中每个工作者有自己的计算和通信速度，以及数据分布。在这些算法中，工作者在某个过去的迭代中计算并返回本地数据相对于历史的可能偏移和抽象的梯度。我们提出一种统一的收敛理论，用于非对称凸函数在异步 режи困难的情况下。我们的分析显示了迭代的收敛率，并且解释了影响收敛率的因素，以及如何提高异步算法的性能。特别是，我们介绍了一种新的异步方法，基于工作者洗牌。我们的分析也证明了SGD WITH RANDOM RESHUFFLING和SHUFFLE-ONCE MINI-BATCH SGD等算法的收敛性。得到的收敛率与最佳结果匹配，表明我们的方法的紧致性。最后，我们的数值评估表明了我们的方法在实际应用中的良好性能。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bayesian-Learning-Curve-Extrapolation-using-Prior-Data-Fitted-Networks"><a href="#Efficient-Bayesian-Learning-Curve-Extrapolation-using-Prior-Data-Fitted-Networks" class="headerlink" title="Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks"></a>Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20447">http://arxiv.org/abs/2310.20447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Adriaensen, Herilalaina Rakotoarison, Samuel Müller, Frank Hutter</li>
<li>for: 预测模型在训练后期的性能，基于早期训练期间的性能。</li>
<li>methods: 使用 Bayesian方法，但现有方法存在一定的限制和计算成本问题。</li>
<li>results: 提出了首次应用先进数据预测网络（PFN）在这个预测中，PFN通过单个前进 pass来进行粗略抽象抽象 Bayesian推理。 Comparing with MCMC, LC-PFN可以更准确地 aproximate posterior predictive distribution，而且速度比MCMC快上万倍。同时，LC-PFN可以在多种模型architecture和输入数据Modalities上实现类似的性能。最后， investigate its potential in the context of model selection, and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6x speed-ups on 45 of these datasets, at virtually no overhead.<details>
<summary>Abstract</summary>
Learning curve extrapolation aims to predict model performance in later epochs of training, based on the performance in earlier epochs. In this work, we argue that, while the inherent uncertainty in the extrapolation of learning curves warrants a Bayesian approach, existing methods are (i) overly restrictive, and/or (ii) computationally expensive. We describe the first application of prior-data fitted neural networks (PFNs) in this context. A PFN is a transformer, pre-trained on data generated from a prior, to perform approximate Bayesian inference in a single forward pass. We propose LC-PFN, a PFN trained to extrapolate 10 million artificial right-censored learning curves generated from a parametric prior proposed in prior art using MCMC. We demonstrate that LC-PFN can approximate the posterior predictive distribution more accurately than MCMC, while being over 10 000 times faster. We also show that the same LC-PFN achieves competitive performance extrapolating a total of 20 000 real learning curves from four learning curve benchmarks (LCBench, NAS-Bench-201, Taskset, and PD1) that stem from training a wide range of model architectures (MLPs, CNNs, RNNs, and Transformers) on 53 different datasets with varying input modalities (tabular, image, text, and protein data). Finally, we investigate its potential in the context of model selection and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6x speed-ups on 45 of these datasets, at virtually no overhead.
</details>
<details>
<summary>摘要</summary>
学习曲线拟合目标是预测训练过程中后期模型表现，基于早期表现。在这种工作中，我们认为，由于拟合学习曲线的内在不确定性，需要采用 bayesian 方法。然而，现有方法具有以下两点不足：（i）过于限制性，和/或（ii） computationally  expensive。我们描述了首次应用 priors 数据适应神经网络（PFN）在这种上下文中。PFN 是一种 transformer ，在数据生成自 prior 的基础上，进行 Approximate Bayesian Inference 的单个前进 pass。我们提出了 LC-PFN，一个基于 MCMC 生成的1000万个人造右缺学习曲线，PFN 在这些学习曲线上进行拟合。我们示出，LC-PFN 可以更准确地 aproximate posterior predictive distribution，并且比 MCMC 快速得多（大约10000 倍）。此外，我们还证明了同一个 LC-PFN 可以在4个学习曲线准 benchmark（LCBench、NAS-Bench-201、Taskset 和 PD1）中拟合总共20000个真实学习曲线，这些学习曲线来自于训练多种模型结构（MLPs、CNNs、RNNs 和 Transformers）在53个不同的数据集（表格、图像、文本和蛋白质数据）上。最后，我们探讨了它在模型选择上的潜在应用，并发现一个简单的 LC-PFN 基本预测早期停止 criterion 可以在45个数据集上获得2-6倍的速度提升，而且几乎没有额外负担。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Impact-of-Companies-on-AI-Research-Based-on-Publications"><a href="#Analyzing-the-Impact-of-Companies-on-AI-Research-Based-on-Publications" class="headerlink" title="Analyzing the Impact of Companies on AI Research Based on Publications"></a>Analyzing the Impact of Companies on AI Research Based on Publications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20444">http://arxiv.org/abs/2310.20444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LazaTabax/AI-Impact-Scientometrics">https://github.com/LazaTabax/AI-Impact-Scientometrics</a></li>
<li>paper_authors: Michael Färber, Lazaros Tampakis</li>
<li>for: This paper aims to measure the influence of companies on AI research through scientific publishing activities.</li>
<li>methods: The authors compare academic- and company-authored AI publications published in the last decade, using scientometric data from multiple scholarly databases to identify differences and top contributing organizations.</li>
<li>results: The authors find that publications with company participation receive higher citation counts and more online attention, but also note that the vast majority of publications are still produced by academia. They provide recommendations to safeguard a harmonious balance between academia and industry in AI research.Here are the three key information points in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是通过科学出版活动来衡量企业对人工智能研究的影响。</li>
<li>methods: 作者们通过比较过去十年的学术期刊文章，使用多个学术数据库的科学指标来描述不同的组织和个人的贡献。</li>
<li>results: 作者们发现，与企业参与有关的文章 receiving higher citation counts和更多的在线关注，但也注意到大多数文章仍然是由学术界出版。他们提供了保持学术界和产业之间和谐协作的建议。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) is one of the most momentous technologies of our time. Thus, it is of major importance to know which stakeholders influence AI research. Besides researchers at universities and colleges, researchers in companies have hardly been considered in this context. In this article, we consider how the influence of companies on AI research can be made measurable on the basis of scientific publishing activities. We compare academic- and company-authored AI publications published in the last decade and use scientometric data from multiple scholarly databases to look for differences across these groups and to disclose the top contributing organizations. While the vast majority of publications is still produced by academia, we find that the citation count an individual publication receives is significantly higher when it is (co-)authored by a company. Furthermore, using a variety of altmetric indicators, we notice that publications with company participation receive considerably more attention online. Finally, we place our analysis results in a broader context and present targeted recommendations to safeguard a harmonious balance between academia and industry in the realm of AI research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Ontologies-for-Models-and-Algorithms-in-Applied-Mathematics-and-Related-Disciplines"><a href="#Ontologies-for-Models-and-Algorithms-in-Applied-Mathematics-and-Related-Disciplines" class="headerlink" title="Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines"></a>Ontologies for Models and Algorithms in Applied Mathematics and Related Disciplines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20443">http://arxiv.org/abs/2310.20443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Björn Schembera, Frank Wübbeling, Hendrik Kleikamp, Christine Biedinger, Jochen Fiedler, Marco Reidelbach, Aurela Shehu, Burkhard Schmidt, Thomas Koprucki, Dorothea Iglezakis, Dominik Göddeke</li>
<li>for: 这篇论文是为了推动数学研究数据的可重用性和可访问性而写的。</li>
<li>methods: 论文使用了ontology和知识图来增强数学研究数据的Semantic Web表示，并将数学模型和数值算法的知识映射到ontology中。</li>
<li>results: 通过使用ontology和知识图，论文能够准确地表示数学研究数据的含义，从而提高数学研究数据的可重用性和可访问性。<details>
<summary>Abstract</summary>
In applied mathematics and related disciplines, the modeling-simulation-optimization workflow is a prominent scheme, with mathematical models and numerical algorithms playing a crucial role. For these types of mathematical research data, the Mathematical Research Data Initiative has developed, merged and implemented ontologies and knowledge graphs. This contributes to making mathematical research data FAIR by introducing semantic technology and documenting the mathematical foundations accordingly. Using the concrete example of microfracture analysis of porous media, it is shown how the knowledge of the underlying mathematical model and the corresponding numerical algorithms for its solution can be represented by the ontologies.
</details>
<details>
<summary>摘要</summary>
在应用数学和相关领域，模拟优化工作流程是一个非常重要的方案，数学模型和数值算法在这些数学研究数据中扮演着关键角色。为了使数学研究数据变得可重用，数学研究数据Initiave已经开发、合并和实施了ontologies和知识图。这有助于使数学研究数据变得FAIR，通过使用semantic技术和文档数学基础 accordingly。使用微裂隙分析含气体的具体例子，示出了ontologies可以表示数学模型的知识和相应的数值算法的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Raising-the-ClaSS-of-Streaming-Time-Series-Segmentation"><a href="#Raising-the-ClaSS-of-Streaming-Time-Series-Segmentation" class="headerlink" title="Raising the ClaSS of Streaming Time Series Segmentation"></a>Raising the ClaSS of Streaming Time Series Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20431">http://arxiv.org/abs/2310.20431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ermshaua/classification-score-stream">https://github.com/ermshaua/classification-score-stream</a></li>
<li>paper_authors: Arik Ermshaus, Patrick Schäfer, Ulf Leser</li>
<li>for: 本研究旨在开发一种高效、准确的流处理时间序列分 segmentation（STSS）算法，用于处理高频流处理数据，以捕捉进程或实体的状态变化。</li>
<li>methods: 本研究使用了一种新的自助学习时间序列分类方法，并应用了统计测试来检测状态变化点（CP）。</li>
<li>results: 实验结果表明，ClaSS算法在两个大标准套件和六个实际数据库中表现了更高的精度，与八种现有竞争算法相比。其空间和时间复杂度独立于分 segment size，线性增长只有滑动窗口大小。 ClaSS算法还被实现为Apache Flink流处理引擎的窗口运算符，平均处理速度为538个数据点每秒。<details>
<summary>Abstract</summary>
Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, we found ClaSS to be significantly more precise than eight state-of-the-art competitors. Its space and time complexity is independent of segment sizes and linear only in the sliding window size. We also provide ClaSS as a window operator with an average throughput of 538 data points per second for the Apache Flink streaming engine.
</details>
<details>
<summary>摘要</summary>
今天的普遍感知器（ubiquitous sensors） emit high frequency流量数字量测量，这些量测reflect了人类、动物、工业、商业和自然过程的属性。这些过程的变化，例如由外部事件或内部状态变化引起的变化，会在记录的信号中manifest。流时序段分（STSS）的任务是将流分成连续的变量大小分割，这些分割对应于观察过程或实体的状态。这个分割操作必须在输入信号频率上能够 coping 。我们介绍了一种新的、高效、高精度的流时序段分算法（ClaSS）。ClaSS 使用自我超级时间序列分类来评估分割的一致性，并运用统计测试来检测显著的变化点（CPs）。在我们使用两个大的 referential和六个实际数据存档进行实验evaluation 中，我们发现ClaSS 与八种现有的竞争者相比，更加精确。其空间和时间复杂度是独立于分割大小和线性增长。我们还提供了 ClaSS 作为窗口运算符，其均衡throughput 为 Apache Flink 流动引擎中的538个数据点/秒。
</details></li>
</ul>
<hr>
<h2 id="Meta-Learning-for-Multi-View-Visuomotor-Systems"><a href="#Meta-Learning-for-Multi-View-Visuomotor-Systems" class="headerlink" title="Meta Learning for Multi-View Visuomotor Systems"></a>Meta Learning for Multi-View Visuomotor Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20414">http://arxiv.org/abs/2310.20414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benji Alwis, Nick Pears, Pengcheng Liu</li>
<li>for: 这篇论文旨在快速适应机器人多视角系统各种相机配置的变化，并且使用meta-学习精确地调整感知网络，保持政策网络不变。</li>
<li>methods: 这篇论文使用meta-学习来精确地调整感知网络，以减少新的训练集训练集的数量，并且保持政策网络不变。</li>
<li>results: 实验结果显示，这篇论文可以快速适应机器人多视角系统各种相机配置的变化，并且可以获得基准性能。<details>
<summary>Abstract</summary>
This paper introduces a new approach for quickly adapting a multi-view visuomotor system for robots to varying camera configurations from the baseline setup. It utilises meta-learning to fine-tune the perceptual network while keeping the policy network fixed. Experimental results demonstrate a significant reduction in the number of new training episodes needed to attain baseline performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Base-Station-Cooperative-Sensing-with-AI-Aided-Tracking"><a href="#Multi-Base-Station-Cooperative-Sensing-with-AI-Aided-Tracking" class="headerlink" title="Multi-Base Station Cooperative Sensing with AI-Aided Tracking"></a>Multi-Base Station Cooperative Sensing with AI-Aided Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20403">http://arxiv.org/abs/2310.20403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elia Favarelli, Elisabetta Matricardi, Lorenzo Pucci, Enrico Paolini, Wen Xu, Andrea Giorgetti</li>
<li>for: 这个研究旨在提高 JOINT SENSING AND COMMUNICATION（JSC）网络中的监测性能，包括多个基站（BS）在抽象中心（FC）之间进行信息交换，以便同时进行监测环境和建立用户设备（UE）之间的通信链接。</li>
<li>methods: 每个BS在网络中 acts as a monostatic radar system，可以全面扫描监测区域，生成范围角图，提供监测区域中目标的位置信息。图像被FC进行拟合，然后使用卷积神经网络（CNN）来推断目标类别，并使用适应分组算法来更有效地组合探测来自同一个目标的检测。最后，使用概率 гипотез浓度筛选器（PHD）和多 Bernoulli 混合（MBM）筛选器来估算目标的状态。</li>
<li>results: 数值结果表明，我们的框架可以提供出色的监测性能，实现距离估计在 60 cm 以下，同时保持与 UE 之间的通信服务，减少了通信负荷在 10% 到 20% 之间。研究还表明，在特定的案例研究中，使用 3 个BS进行监测可以 garantía 的 Localization 错误在 1 m 以下。<details>
<summary>Abstract</summary>
In this work, we investigate the performance of a joint sensing and communication (JSC) network consisting of multiple base stations (BSs) that cooperate through a fusion center (FC) to exchange information about the sensed environment while concurrently establishing communication links with a set of user equipments (UEs). Each BS within the network operates as a monostatic radar system, enabling comprehensive scanning of the monitored area and generating range-angle maps that provide information regarding the position of a group of heterogeneous objects. The acquired maps are subsequently fused in the FC. Then, a convolutional neural network (CNN) is employed to infer the category of the targets, e.g., pedestrians or vehicles, and such information is exploited by an adaptive clustering algorithm to group the detections originating from the same target more effectively. Finally, two multi-target tracking algorithms, the probability hypothesis density (PHD) filter and multi-Bernoulli mixture (MBM) filter, are applied to estimate the state of the targets. Numerical results demonstrated that our framework could provide remarkable sensing performance, achieving an optimal sub-pattern assignment (OSPA) less than 60 cm, while keeping communication services to UEs with a reduction of the communication capacity in the order of 10% to 20%. The impact of the number of BSs engaged in sensing is also examined, and we show that in the specific case study, 3 BSs ensure a localization error below 1 m.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了一个共同感知和通信（JSC）网络，该网络由多个基站（BS）组成，通过协调中心（FC）进行信息交换，以便在同时建立用户设备（UE）与网络的通信连接。每个BS作为单static雷达系统，可以全面扫描监测区域，生成距离角度图，提供目标的位置信息。获取的图像被后续的卷积神经网络（CNN）进行推理，并将推理结果用adaptive clustering算法进行分组。最后，我们使用了多目标跟踪算法（PHD滤波器和多 Bernoulli 混合（MBM）滤波器）来估算目标的状态。我们的框架可以提供出色的感知性能，达到优化的子模式分配（OSPA）小于60cm，并保持与UE的通信服务的减少，在10%到20%之间。我们还研究了BS参与感知的数量的影响，并发现在特定的案例研究中，3个BS可以 garantuee localization error below 1m。
</details></li>
</ul>
<hr>
<h2 id="Utilitarian-Algorithm-Configuration"><a href="#Utilitarian-Algorithm-Configuration" class="headerlink" title="Utilitarian Algorithm Configuration"></a>Utilitarian Algorithm Configuration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20401">http://arxiv.org/abs/2310.20401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drgrhm/utilitarian-ac">https://github.com/drgrhm/utilitarian-ac</a></li>
<li>paper_authors: Devon R. Graham, Kevin Leyton-Brown, Tim Roughgarden</li>
<li>for: 这个论文旨在配置启发式算法以提高它们对用户提供的价值，同时提供理论保证。</li>
<li>methods: 论文使用的方法包括：建立Utilitarian目标，提供有效和理论上正确的配置过程，并对这些过程进行分析和实验验证。</li>
<li>results: 论文的研究结果包括：提供了一种新的配置方法，可以帮助算法设计者在实际应用中做出更好的选择，同时提供了理论上的保证。这种方法可以在实际应用中提供更高的性能，并且可以在不同的情况下进行适应。<details>
<summary>Abstract</summary>
We present the first nontrivial procedure for configuring heuristic algorithms to maximize the utility provided to their end users while also offering theoretical guarantees about performance. Existing procedures seek configurations that minimize expected runtime. However, very recent theoretical work argues that expected runtime minimization fails to capture algorithm designers' preferences. Here we show that the utilitarian objective also confers significant algorithmic benefits. Intuitively, this is because mean runtime is dominated by extremely long runs even when they are incredibly rare; indeed, even when an algorithm never gives rise to such long runs, configuration procedures that provably minimize mean runtime must perform a huge number of experiments to demonstrate this fact. In contrast, utility is bounded and monotonically decreasing in runtime, allowing for meaningful empirical bounds on a configuration's performance. This paper builds on this idea to describe effective and theoretically sound configuration procedures. We prove upper bounds on the runtime of these procedures that are similar to theoretical lower bounds, while also demonstrating their performance empirically.
</details>
<details>
<summary>摘要</summary>
我们提出了首个不同实际方法来配置假设算法以最大化它们的终端用户所提供的用途，同时提供理论上的保证。现有的方法寻求最小化预期所需的时间。然而，非常最近的理论工作表明，预期时间最小化无法捕捉算法设计师的偏好。我们显示了Utilitarian目标也具有重要的算法优点。 intuitively, this is because mean runtime is dominated by extremely long runs even when they are incredibly rare; indeed, even when an algorithm never gives rise to such long runs, configuration procedures that provably minimize mean runtime must perform a huge number of experiments to demonstrate this fact. In contrast, utility is bounded and monotonically decreasing in runtime, allowing for meaningful empirical bounds on a configuration's performance. This paper builds on this idea to describe effective and theoretically sound configuration procedures. We prove upper bounds on the runtime of these procedures that are similar to theoretical lower bounds, while also demonstrating their performance empirically.
</details></li>
</ul>
<hr>
<h2 id="Do-large-language-models-solve-verbal-analogies-like-children-do"><a href="#Do-large-language-models-solve-verbal-analogies-like-children-do" class="headerlink" title="Do large language models solve verbal analogies like children do?"></a>Do large language models solve verbal analogies like children do?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20384">http://arxiv.org/abs/2310.20384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cstevenson-uva/verbal_analogies_kids_vs_llms">https://github.com/cstevenson-uva/verbal_analogies_kids_vs_llms</a></li>
<li>paper_authors: Claire E. Stevenson, Mathilde ter Veen, Rochelle Choenni, Han L. J. van der Maas, Ekaterina Shutova</li>
<li>for:  investigate whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do.</li>
<li>methods: used verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year olds from the Netherlands solved 622 analogies in Dutch.</li>
<li>results: the six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, and XLM-V and GPT-3 the best, but when controlling for associative processes, each model’s performance level drops 1-2 years.<details>
<summary>Abstract</summary>
Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associative processes often underlie correctly solved analogies. We conclude that the LLMs we tested indeed tend to solve verbal analogies by association with C like children do.
</details>
<details>
<summary>摘要</summary>
人类认知的核心是比喻创造。大人解决比喻问题，如“匹 belongs to 牧场 like 鸡 belongs to ...?”，通过将关系（如“被保管”）映射到答案，如“鸡巢”。然而，孩子们通常使用关联，例如回答“蛋”。这篇论文研究了大型自然语言模型（LLMs）是否使用关联来解决A:B::C:?的语言比喻。我们使用来自在线适应式学习环境的622个荷兰7-12岁儿童解决的语言比喻，并测试了六个荷兰单语言和多语言LLMs。结果显示，这六个模型在与孩子们的性能水平相当，MGPT表现最差，约等于7岁儿童水平，而XLM-V和GPT-3表现最好，略高于11岁儿童水平。然而，当我们控制了关联过程时，每个模型的表现水平下降1-2年。进一步的实验表明，关联过程经常在正确解决比喻问题中发挥作用。我们结论，我们测试的LLMs实际上都是通过关联来解决语言比喻的，与孩子们一样。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-GPT-4V’s-Multimodal-Capabilities-in-Medical-Imaging"><a href="#A-Comprehensive-Study-of-GPT-4V’s-Multimodal-Capabilities-in-Medical-Imaging" class="headerlink" title="A Comprehensive Study of GPT-4V’s Multimodal Capabilities in Medical Imaging"></a>A Comprehensive Study of GPT-4V’s Multimodal Capabilities in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20381">http://arxiv.org/abs/2310.20381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lingqiao Liu, Lei Wang, Leyang Cui, Zhaopeng Tu, Longyue Wang, Luping Zhou</li>
<li>for:  This paper evaluates GPT-4V’s capabilities in medical imaging tasks, including Radiology Report Generation, Medical VQA, and Visual Grounding.</li>
<li>methods:  The paper uses publicly available benchmarks to evaluate GPT-4V’s performance in these tasks, and the evaluation highlights areas for improvement.</li>
<li>results:  GPT-4V demonstrates potential in generating descriptive reports for chest X-ray images, but its performance is less accurate in certain evaluation metrics like CIDEr for Medical VQA and lacks precision in identifying specific medical organs and signs in Visual Grounding.Here is the same information in Simplified Chinese:</li>
<li>for:  This paper evaluates GPT-4V的应用在医疗影像任务中，包括医学报告生成、医学问题回答和视觉基础。</li>
<li>methods:  This paper使用公共可用的benchmark来评估GPT-4V的表现在这些任务中，而评估点出了一些需要改进的地方。</li>
<li>results:  GPT-4V在胸部X射影像描述报告中表现出了潜力，但在certain evaluation metric like CIDEr中的表现不够精确，而在视觉基础任务中的精度也有待改进。<details>
<summary>Abstract</summary>
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of more semantically robust assessment methods. In the field of Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding boxes, but its precision is lacking, especially in identifying specific medical organs and signs. Our evaluation underscores the significant potential of GPT-4V in the medical imaging domain, while also emphasizing the need for targeted refinements to fully unlock its capabilities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Machine-Learning-Based-Framework-for-Clustering-Residential-Electricity-Load-Profiles-to-Enhance-Demand-Response-Programs"><a href="#A-Machine-Learning-Based-Framework-for-Clustering-Residential-Electricity-Load-Profiles-to-Enhance-Demand-Response-Programs" class="headerlink" title="A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs"></a>A Machine Learning-Based Framework for Clustering Residential Electricity Load Profiles to Enhance Demand Response Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20367">http://arxiv.org/abs/2310.20367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilis Michalakopoulos, Elissaios Sarmas, Ioannis Papias, Panagiotis Skaloumpakas, Vangelis Marinakis, Haris Doukas<br>for:This paper aims to identify the most suitable consumer clusters with similar energy consumption behaviors using smart meter data.methods:The authors use four widely used clustering algorithms (K-means, K-medoids, Hierarchical Agglomerative Clustering, and Density-based Spatial Clustering) and empirical analysis to assess the algorithms. They also redefine the problem as a probabilistic classification one and use Explainable AI (xAI) to enhance interpretability.results:The optimal number of clusters for this case is seven, but two of the clusters exhibit significant internal dissimilarity and are split further into nine clusters. The method is scalable and versatile, making it an ideal choice for power utility companies aiming to segment their users for creating more targeted Demand Response programs.<details>
<summary>Abstract</summary>
Load shapes derived from smart meter data are frequently employed to analyze daily energy consumption patterns, particularly in the context of applications like Demand Response (DR). Nevertheless, one of the most important challenges to this endeavor lies in identifying the most suitable consumer clusters with similar consumption behaviors. In this paper, we present a novel machine learning based framework in order to achieve optimal load profiling through a real case study, utilizing data from almost 5000 households in London. Four widely used clustering algorithms are applied specifically K-means, K-medoids, Hierarchical Agglomerative Clustering and Density-based Spatial Clustering. An empirical analysis as well as multiple evaluation metrics are leveraged to assess those algorithms. Following that, we redefine the problem as a probabilistic classification one, with the classifier emulating the behavior of a clustering algorithm,leveraging Explainable AI (xAI) to enhance the interpretability of our solution. According to the clustering algorithm analysis the optimal number of clusters for this case is seven. Despite that, our methodology shows that two of the clusters, almost 10\% of the dataset, exhibit significant internal dissimilarity and thus it splits them even further to create nine clusters in total. The scalability and versatility of our solution makes it an ideal choice for power utility companies aiming to segment their users for creating more targeted Demand Response programs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>智能仪器数据中的荷形loads频繁用于分析每天能源消耗模式，尤其在应用程序 like Demand Response (DR) 中。然而，最重要的挑战在于确定最适合的消耗者群集，具有相似的消耗行为。在这篇论文中，我们提出了一种基于机器学习的框架，通过实际案例研究，使用伦敦 almost 5000 户数据。我们运用了四种常用的划分算法，即 K-means、K-medoids、 Hierarchical Agglomerative Clustering 和 Density-based Spatial Clustering。通过实验分析以及多个评价指标，我们评估了这些算法。然后，我们将问题重定义为一个 probabilistic 类别问题，类ifier 模拟划分算法的行为，使用 Explainable AI (xAI) 提高解释性。根据划分算法分析，这个案例的优化数量为七。尽管如此，我们的方法显示，这两个群集，数据集中约 10%，具有显著的内部不一致，因此我们将其进一步分割，创建九个群集。我们的解决方案具有扩展性和多样性，使得电力公司可以更好地对用户进行分类，为创建更加targeted的 Demand Response 计划。
</details></li>
</ul>
<hr>
<h2 id="Mathematical-Introduction-to-Deep-Learning-Methods-Implementations-and-Theory"><a href="#Mathematical-Introduction-to-Deep-Learning-Methods-Implementations-and-Theory" class="headerlink" title="Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory"></a>Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20360">http://arxiv.org/abs/2310.20360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/introdeeplearning/book">https://github.com/introdeeplearning/book</a></li>
<li>paper_authors: Arnulf Jentzen, Benno Kuckuck, Philippe von Wurstemberger</li>
<li>for: 本书的目的是提供深度学习算法的基础教程。</li>
<li>methods: 本书详细介绍了深度学习算法的基本组成部分，包括不同的人工神经网络架构（如全连接Feedforward ANNs、 convolutional ANNs、 recurrent ANNs、 residual ANNs、批量normalization ANNs）和不同的优化算法（如基本的随机梯度下降方法、加速方法和自适应方法）。</li>
<li>results: 本书还涵盖了深度学习算法的一些理论方面，包括人工神经网络的拟合能力（包括神经网络的 calculus）、优化理论（包括Kurdyka-{\L}ojasiewicz不等式）和泛化错误。 最后一部分的书评论了深度学习方法的泛化方法，包括物理学习神经网络（PINNs）和深度加尔各答方法。<details>
<summary>Abstract</summary>
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.
</details>
<details>
<summary>摘要</summary>
The book will cover the following topics:* Different ANN architectures, including fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization* Optimization algorithms, including the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods* Theoretical aspects of deep learning algorithms, including approximation capacities of ANNs, optimization theory, and generalization errors* Deep learning approximation methods for partial differential equations (PDEs), including physics-informed neural networks (PINNs) and deep Galerkin methods.The book is intended to provide a comprehensive introduction to deep learning algorithms, with a focus on mathematical details and theoretical understanding. It will be a valuable resource for students, scientists, and practitioners who want to gain a deeper understanding of this rapidly evolving field.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Spatial-Awareness-Capability-of-Multi-Modal-Large-Language-Model"><a href="#Enhancing-the-Spatial-Awareness-Capability-of-Multi-Modal-Large-Language-Model" class="headerlink" title="Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model"></a>Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20357">http://arxiv.org/abs/2310.20357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Zhao, Zhenyu Li, Zhi Jin, Feng Zhang, Haiyan Zhao, Chengfeng Dou, Zhengwei Tao, Xinhai Xu, Donghong Liu</li>
<li>for: 提高大型语言模型（MLLM）的空间意识能力，以满足自动驾驶、智能医疗、 робо扮演、虚拟和增强现实等行业的需求。</li>
<li>methods: 使用更精确的物体之间空间位置信息来导引MLLM提供更 precis的回答。利用几何空间信息和场景图来获取相关的几何空间信息和场景细节。</li>
<li>results: 经验表明，提出的方法能够有效地提高MLLM的空间意识能力，并在多模态大语言模型中进行空间意识相关任务时达到更高的精度。<details>
<summary>Abstract</summary>
The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric spatial information and scene details of objects involved in the query. Subsequently, based on this information, we direct MLLM to address spatial awareness-related queries posed by the user. Extensive experiments were conducted in benchmarks such as MME, MM-Vet, and other multi-modal large language models. The experimental results thoroughly confirm the efficacy of the proposed method in enhancing the spatial awareness tasks and associated tasks of MLLM.
</details>
<details>
<summary>摘要</summary>
多模式大语言模型（MLLM）指的是在大语言模型（LLM）基础上增加了接收和推理多模式数据的能力。在这些多模式数据中，空间意识是关键的一个能力，涵盖了对物品之间和场景区域之间的物品理解的多种技能。自动驾驶、智能医疗、机器人、虚拟和增强现实等行业都有极高的需求于 MLLM 的空间意识能力。然而，目前 MLLM 的空间意识能力和人类需求之间存在显著的差距。为了解决这个问题，这篇论文提出了使用更加精确的物体之间的空间位置信息来导引 MLLM 在用户提出的 queries 中更加准确地回答空间意识相关的问题。具体来说，为某个多模式任务，我们使用了获取 geometric 空间信息和场景图来获得相关的 geometric 空间信息和场景细节。然后，根据这些信息，我们向 MLLM 提供空间意识相关的 queries。我们在 MME、MM-Vet 和其他多模式大语言模型的 benchmark 中进行了广泛的实验，并取得了证明方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Muscle-volume-quantification-guiding-transformers-with-anatomical-priors"><a href="#Muscle-volume-quantification-guiding-transformers-with-anatomical-priors" class="headerlink" title="Muscle volume quantification: guiding transformers with anatomical priors"></a>Muscle volume quantification: guiding transformers with anatomical priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20355">http://arxiv.org/abs/2310.20355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louise Piecuch, Vanessa Gonzales Duque, Aurélie Sarcher, Enzo Hollville, Antoine Nordez, Giuseppe Rabita, Gaël Guilhem, Diana Mateus</li>
<li>for: 这篇论文是为了提供一种自动 segmentation 技术，帮助在运动和类型疾病的追踪中量化肌肉量和形状。</li>
<li>methods: 这篇论文提出了一个混合构造的分类模型，结合了卷积和视觉对应块，以帮助解决肌肉分类中的挑战。</li>
<li>results: 实验结果显示，这种混合模型可以从相对较小的数据库中训练出高精度的肌肉分类模型，并且遵循体内关系规律可以提高预测的精度。<details>
<summary>Abstract</summary>
Muscle volume is a useful quantitative biomarker in sports, but also for the follow-up of degenerative musculo-skelletal diseases. In addition to volume, other shape biomarkers can be extracted by segmenting the muscles of interest from medical images. Manual segmentation is still today the gold standard for such measurements despite being very time-consuming. We propose a method for automatic segmentation of 18 muscles of the lower limb on 3D Magnetic Resonance Images to assist such morphometric analysis. By their nature, the tissue of different muscles is undistinguishable when observed in MR Images. Thus, muscle segmentation algorithms cannot rely on appearance but only on contour cues. However, such contours are hard to detect and their thickness varies across subjects. To cope with the above challenges, we propose a segmentation approach based on a hybrid architecture, combining convolutional and visual transformer blocks. We investigate for the first time the behaviour of such hybrid architectures in the context of muscle segmentation for shape analysis. Considering the consistent anatomical muscle configuration, we rely on transformer blocks to capture the longrange relations between the muscles. To further exploit the anatomical priors, a second contribution of this work consists in adding a regularisation loss based on an adjacency matrix of plausible muscle neighbourhoods estimated from the training data. Our experimental results on a unique database of elite athletes show it is possible to train complex hybrid models from a relatively small database of large volumes, while the anatomical prior regularisation favours better predictions.
</details>
<details>
<summary>摘要</summary>
筋体积是运动健康领域中有用的量化生物标志，同时也适用于追踪萎缩骨骼疾病。除筋体积外，其他形状生物标志也可以从医疗图像中提取。现今，手动分割仍然是量化测量的黄金标准，尽管很时间consuming。我们提议一种自动分割lower limb的18个肌肉的3D磁共振图像，以帮助 morphometric分析。由于不同肌肉组织的组织学性不同，因此分割算法无法仅基于观察到的外观特征。但是，肌肉的边缘很难于检测，而且这些边缘的厚度在不同的个体之间存在差异。为了解决以上挑战，我们提议一种 combining convolutional和视觉转换块的分割方法。我们在 muscle segmentation 领域中首次研究了这种混合架构的行为。由于肌肉的一致性，我们依靠转换块来捕捉肌肉之间的长距离关系。为了进一步利用骨骼的 анатоical prior，我们的第二个贡献是添加一个基于邻居矩阵的准确性损失，该矩阵是通过训练数据来估算的 plausible 肌肉邻居。我们在一个Unique数据库中进行了实验，结果表明可以从相对较小的数据库中训练复杂的混合模型，而且准确性增加。
</details></li>
</ul>
<hr>
<h2 id="Combining-Shape-Completion-and-Grasp-Prediction-for-Fast-and-Versatile-Grasping-with-a-Multi-Fingered-Hand"><a href="#Combining-Shape-Completion-and-Grasp-Prediction-for-Fast-and-Versatile-Grasping-with-a-Multi-Fingered-Hand" class="headerlink" title="Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand"></a>Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20350">http://arxiv.org/abs/2310.20350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Humt, Dominik Winkelbauer, Ulrich Hillenbrand, Berthold Bäuml</li>
<li>for:  grasping objects with limited or no prior knowledge, especially in assistive robotics</li>
<li>methods:  deep learning pipeline with shape completion and grasp predictor modules, using VQDIF and two-stage architecture</li>
<li>results:  successful grasping of a wide range of household objects based on a single depth image, with fast processing time (1 s for shape completion and 0.3 s for generating 1000 grasps)<details>
<summary>Abstract</summary>
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful grasping of a wide range of household objects based on a depth image from a single viewpoint. The whole pipeline is fast, taking only about 1 s for completing the object's shape (0.7 s) and generating 1000 grasps (0.3 s).
</details>
<details>
<summary>摘要</summary>
握住无知物体是助动器学中高度相关的技能。然而，在总体设定下，这问题仍然是开放问题，特别是当对象只有部分可见和多指手部抓取时。我们提出了一种新的、快速、高精度深度学习管道，包括基于单个深度图像的形状完成模块，然后是基于预测对象形状的抓取预测器。形状完成网络基于VQDIF，预测空间占用值在任意查询点。抓取预测器使用我们的两阶段架构，首先使用自动逆 corr 模型生成手势，然后对每个姿势进行指关键点的回归。关键因素包括数据实现和扩展，以及特别对困难情况的特别关注 during training。实验在物理机器人平台上成功抓取了一系列家用物品，基于单个视角深度图像。整个管道快速，只需0.7秒完成对象的形状预测和1000个抓取预测（0.3秒）。
</details></li>
</ul>
<hr>
<h2 id="Improving-Entropy-Based-Test-Time-Adaptation-from-a-Clustering-View"><a href="#Improving-Entropy-Based-Test-Time-Adaptation-from-a-Clustering-View" class="headerlink" title="Improving Entropy-Based Test-Time Adaptation from a Clustering View"></a>Improving Entropy-Based Test-Time Adaptation from a Clustering View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20327">http://arxiv.org/abs/2310.20327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoliang Lin, Hanjiang Lai, Yan Pan, Jian Yin</li>
<li>for: 本文是关于如何处理预测数据和测试数据之间的域Shift问题，具体来说是使用测试时间适应（TTA）技术来适应模型。</li>
<li>methods: 本文提出了一种新的视角，即将Entropy-Based TTA（EBTTA）方法解释为 clustering 方法，具体来说是一种迭代算法，其中在分配步骤中，使用 EBTTA 模型对测试样本进行标签分配，而在更新步骤中，通过分配的样本更新模型。</li>
<li>results: 实验结果表明，我们的方法可以在多个数据集上减少域Shift问题的影响，并且可以在不同的批处理大小和初始分配情况下保持稳定性。<details>
<summary>Abstract</summary>
Domain shift is a common problem in the realistic world, where training data and test data follow different data distributions. To deal with this problem, fully test-time adaptation (TTA) leverages the unlabeled data encountered during test time to adapt the model. In particular, Entropy-Based TTA (EBTTA) methods, which minimize the prediction's entropy on test samples, have shown great success. In this paper, we introduce a new perspective on the EBTTA, which interprets these methods from a view of clustering. It is an iterative algorithm: 1) in the assignment step, the forward process of the EBTTA models is the assignment of labels for these test samples, and 2) in the updating step, the backward process is the update of the model via the assigned samples. Based on the interpretation, we can gain a deeper understanding of EBTTA, where we show that the entropy loss would further increase the largest probability. Accordingly, we offer an alternative explanation that why existing EBTTA methods are sensitive to initial assignments, outliers, and batch size. This observation can guide us to put forward the improvement of EBTTA. We propose robust label assignment, weight adjustment, and gradient accumulation to alleviate the above problems. Experimental results demonstrate that our method can achieve consistent improvements on various datasets. Code is provided in the supplementary material.
</details>
<details>
<summary>摘要</summary>
域外 shift 是现实世界中常见的问题，训练数据和测试数据follow不同的数据分布。为解决这个问题，全部测试时间适应（TTA）可以利用测试时间遇到的无标签数据来适应模型。特别是Entropy-Based TTA（EBTTA）方法，通过最小化测试样本上预测的 entropy 来实现成功。在这篇论文中，我们提出了一新的视角，即通过对 EBTTA 方法进行 clustering 的解释。这是一个迭代算法：1）在分配步骤，EBTTA 模型的前进过程是对测试样本进行标签分配；2）在更新步骤，后进程是通过分配的样本更新模型。根据这种解释，我们可以更深入地理解 EBTTA，并证明了在预测 Entropy 损失时，最大概率会进一步增加。因此，我们提出了一种代替解释，即为何现有的 EBTTA 方法受初始分配、异常样本和批处理大小的影响。这一观察可以引导我们提出改进 EBTTA 的方法。我们提出了robust label assignment、重量调整和梯度积累来缓解上述问题。实验结果表明，我们的方法可以在多个 dataset 上实现一致性的改进。代码位于辅助材料中。
</details></li>
</ul>
<hr>
<h2 id="SemanticBoost-Elevating-Motion-Generation-with-Augmented-Textual-Cues"><a href="#SemanticBoost-Elevating-Motion-Generation-with-Augmented-Textual-Cues" class="headerlink" title="SemanticBoost: Elevating Motion Generation with Augmented Textual Cues"></a>SemanticBoost: Elevating Motion Generation with Augmented Textual Cues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20323">http://arxiv.org/abs/2310.20323</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blackgold3/SemanticBoost">https://github.com/blackgold3/SemanticBoost</a></li>
<li>paper_authors: Xin He, Shaoli Huang, Xiaohang Zhan, Chao Wen, Ying Shan</li>
<li>for: 本研究的目的是提高现代技术对复杂 semantic description 的动作生成能力，即使数据集中的 semantics 不够、contextual understanding 弱。</li>
<li>methods: 我们提出了 SemanticBoost 框架，它包括 Semantic Enhancement 模块和 Context-Attuned Motion Denoiser (CAMD)。Semantic Enhancement 模块从动作数据中提取更多 semantics，使得数据集的文本描述和动作数据之间的匹配更加精准，不需要大型语言模型。CAMD 方法为生成高质量、semantically consistent 动作序列提供了一个通用解决方案，可以有效地捕捉 context information，并将生成的动作与给定的文本描述进行对齐。</li>
<li>results: 我们的实验结果表明，SemanticBoost 方法在 Humanml3D 数据集上比 auto-regressive-based 技术高效，同时保持了真实和平滑的动作生成质量。SemanticBoost 还能生成准确的 orientational movements、combined motions based on specific body part descriptions、以及从复杂、延展的 sentence 中生成的动作。<details>
<summary>Abstract</summary>
Current techniques face difficulties in generating motions from intricate semantic descriptions, primarily due to insufficient semantic annotations in datasets and weak contextual understanding. To address these issues, we present SemanticBoost, a novel framework that tackles both challenges simultaneously. Our framework comprises a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generating high-quality, semantically consistent motion sequences by effectively capturing context information and aligning the generated motion with the given textual descriptions. Distinct from existing methods, our approach can synthesize accurate orientational movements, combined motions based on specific body part descriptions, and motions generated from complex, extended sentences. Our experimental results demonstrate that SemanticBoost, as a diffusion-based method, outperforms auto-regressive-based techniques, achieving cutting-edge performance on the Humanml3D dataset while maintaining realistic and smooth motion generation quality.
</details>
<details>
<summary>摘要</summary>
当前技术面临着从复杂 semantic 描述中生成动作的困难，主要是因为数据集中的 semantic 注解不够和contextual 理解不强。为解决这些问题，我们提出 SemanticBoost 框架，该框架同时解决了这两个问题。我们的框架包括semantic 增强模块和context-attuned motion denoiser (CAMD)。semantic 增强模块从动作数据中提取补充semantic信息，使 dataset 的文本描述更加详细，并确保文本和动作数据之间的准确对应，不需要依赖于大型语言模型。另一方面，CAMD 方法为生成高质量、semantically consistent 动作序列提供了一个总面 Solution，其效果是 capture context information和将生成的动作与给定的文本描述相align。与现有方法不同，我们的方法可以生成准确的orientation movement, based on specific body part descriptions, 以及从复杂、extended sentences中生成动作。我们的实验结果表明，SemanticBoost 作为一种 diffusion-based 方法，在 Humanml3D  dataset 上超越了 auto-regressive-based 技术，实现了 cutting-edge 性能，同时保持了实际和平滑的动作生成质量。
</details></li>
</ul>
<hr>
<h2 id="Theory-of-Mind-in-Large-Language-Models-Examining-Performance-of-11-State-of-the-Art-models-vs-Children-Aged-7-10-on-Advanced-Tests"><a href="#Theory-of-Mind-in-Large-Language-Models-Examining-Performance-of-11-State-of-the-Art-models-vs-Children-Aged-7-10-on-Advanced-Tests" class="headerlink" title="Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests"></a>Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20320">http://arxiv.org/abs/2310.20320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max J. van Duijn, Bram M. A. van Dijk, Tom Kouwenhoven, Werner de Valk, Marco R. Spruit, Peter van der Putten</li>
<li>for: 这篇论文是关于大语言模型（LLM）的理解能力的研究，特别是对理解意图和信念的理论心（ToM）的能力。</li>
<li>methods: 这篇论文使用了11个基础和指导学习过的 LLM，测试它们在超过常见的谎言测试中的能力，包括非直观语言使用和循环意向性。</li>
<li>results: 研究发现，基于 GPT 家族的指导学习 LLM 表现最佳，经常也超过了7-10岁的儿童。基础 LLM 通常无法解决 ToM 任务，即使使用特定的提示。研究人员认为，语言和 ToM 的演进和发展可能帮助解释 instruciton-tuning 添加了什么：奖励合作交流，考虑到对方和场景。<details>
<summary>Abstract</summary>
To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.
</details>
<details>
<summary>摘要</summary>
有哪些认知能力应该归因于大语言模型（LLM）？在这篇文章中，我们加入了这个emerging debate中，通过（i）测试11个基础和指导调整LLMs的能力，包括非直接语言使用和循环意向;（ii）使用新 rewrite versions of standardized tests to assess LLMs' robustness;（iii）提问和评分打开和关闭问题;以及（iv）对LLM表现与7-10岁儿童的同任务进行比较。我们发现，GPT家族的指导调整LLMs表现最高，经常也超过了儿童。基础LLMs无法解决ToM任务，即使使用特殊的提示。我们建议，语言和ToM的演进和发展可能帮助解释 instrucion-tuning 添加了什么：奖励合作通信，考虑到对方和场景。我们 conclude 认为，对 LLMS 的ToM 需要一种细致的视角。
</details></li>
</ul>
<hr>
<h2 id="Causal-Interpretation-of-Self-Attention-in-Pre-Trained-Transformers"><a href="#Causal-Interpretation-of-Self-Attention-in-Pre-Trained-Transformers" class="headerlink" title="Causal Interpretation of Self-Attention in Pre-Trained Transformers"></a>Causal Interpretation of Self-Attention in Pre-Trained Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20307">http://arxiv.org/abs/2310.20307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raanan Y. Rohekar, Yaniv Gurwicz, Shami Nisimov</li>
<li>for: 这个论文是为了解释Transformer神经网络中的自注意力机制，并将其解释为一种估计输入序列中Symbols之间的结构方程模型。</li>
<li>methods: 该论文使用了自注意力层中的表示进行Conditional Independence关系的估计，以实现学习输入序列上的 causal结构。</li>
<li>results: 该论文通过使用现有的约束基本算法，使得现有的预训练Transformer可以用于零MQ causal探测。同时，论文还提供了一些实验来证明这种方法的有效性，包括情感分类和推荐等两个任务。<details>
<summary>Abstract</summary>
We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks: sentiment classification (NLP) and recommendation.
</details>
<details>
<summary>摘要</summary>
我们提出一种 causal 解释自注意力在 transformer 神经网络架构中的含义。我们将自注意力解释为一种对给定输入序列符号（token）的 structural equation model 的估计机制。这个 structural equation model 可以被解释为输入序列在特定上下文中的 causal 结构。这种解释在干扰因素存在时仍然有效。基于这种解释，我们可以计算输入符号之间的 conditional independence 关系，以便在已有的约束基于算法上学习 causal 结构。因此，现有的预训练 transformer 可以用于零化 causal-发现。我们在两个任务中（情感分类和推荐）提供了 causal 解释，以示例ifying 这种方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Global-Food-Security-Empowering-Resilience-through-Integrated-AI-Foundation-Models-and-Data-Driven-Solutions"><a href="#Revolutionizing-Global-Food-Security-Empowering-Resilience-through-Integrated-AI-Foundation-Models-and-Data-Driven-Solutions" class="headerlink" title="Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions"></a>Revolutionizing Global Food Security: Empowering Resilience through Integrated AI Foundation Models and Data-Driven Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20301">http://arxiv.org/abs/2310.20301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed R. Shoaib, Heba M. Emara, Jun Zhao</li>
<li>for: 该论文旨在探讨基于AI基础模型的多种食品安全应用程序，以超越现有深度学习和机器学习方法的局限性。</li>
<li>methods: 该论文使用不同数据类型，包括多spectral遥感数据、气象数据、土壤特性、历史记录和高分辨率卫星遥感数据，应用AI基础模型。</li>
<li>results: 该论文显示，基于AI基础模型的方法可以准确预测作物种类、耕地地图、田地分割和作物产量，提高资源分配和决策支持。这些模型为全球食品安全努力提供了一种变革性的力量，为实现可持续可靠的食品未来做出了重要贡献。<details>
<summary>Abstract</summary>
Food security, a global concern, necessitates precise and diverse data-driven solutions to address its multifaceted challenges. This paper explores the integration of AI foundation models across various food security applications, leveraging distinct data types, to overcome the limitations of current deep and machine learning methods. Specifically, we investigate their utilization in crop type mapping, cropland mapping, field delineation and crop yield prediction. By capitalizing on multispectral imagery, meteorological data, soil properties, historical records, and high-resolution satellite imagery, AI foundation models offer a versatile approach. The study demonstrates that AI foundation models enhance food security initiatives by providing accurate predictions, improving resource allocation, and supporting informed decision-making. These models serve as a transformative force in addressing global food security limitations, marking a significant leap toward a sustainable and secure food future.
</details>
<details>
<summary>摘要</summary>
Note: The above text is in Simplified Chinese.Please note that the translation is done using a machine translation tool, and the quality of the translation may vary depending on the complexity and nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Sample-Efficient-and-Safe-Deep-Reinforcement-Learning-via-Reset-Deep-Ensemble-Agents"><a href="#Sample-Efficient-and-Safe-Deep-Reinforcement-Learning-via-Reset-Deep-Ensemble-Agents" class="headerlink" title="Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents"></a>Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20287">http://arxiv.org/abs/2310.20287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Woojun Kim, Yongjae Shin, Jongeui Park, Youngchul Sung</li>
<li>For: The paper aims to address the limitations of the vanilla reset method in deep reinforcement learning (RL) and enhance sample efficiency.* Methods: The proposed method leverages deep ensemble learning to mitigate primacy bias and improve RL performance.* Results: The proposed method achieves high sample efficiency and safety considerations in various experiments, including those in the domain of safe RL.Here’s the Chinese translation of the three points:* For: 这篇论文的目的是解决深度强化学习（RL）中的早期经验偏袋现象，提高样本效率。* Methods: 提议的方法利用深度集成学习来缓解早期经验偏袋现象，提高RL表现。* Results: 实验结果表明，提议的方法在各种实验中具有高样本效率和安全考虑的优点。<details>
<summary>Abstract</summary>
Deep reinforcement learning (RL) has achieved remarkable success in solving complex tasks through its integration with deep neural networks (DNNs) as function approximators. However, the reliance on DNNs has introduced a new challenge called primacy bias, whereby these function approximators tend to prioritize early experiences, leading to overfitting. To mitigate this primacy bias, a reset method has been proposed, which performs periodic resets of a portion or the entirety of a deep RL agent while preserving the replay buffer. However, the use of the reset method can result in performance collapses after executing the reset, which can be detrimental from the perspective of safe RL and regret minimization. In this paper, we propose a new reset-based method that leverages deep ensemble learning to address the limitations of the vanilla reset method and enhance sample efficiency. The proposed method is evaluated through various experiments including those in the domain of safe RL. Numerical results show its effectiveness in high sample efficiency and safety considerations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AutoMixer-for-Improved-Multivariate-Time-Series-Forecasting-on-BizITOps-Data"><a href="#AutoMixer-for-Improved-Multivariate-Time-Series-Forecasting-on-BizITOps-Data" class="headerlink" title="AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data"></a>AutoMixer for Improved Multivariate Time-Series Forecasting on BizITOps Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20280">http://arxiv.org/abs/2310.20280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santosh Palaskar, Vijay Ekambaram, Arindam Jati, Neelamadhav Gantayat, Avirup Saha, Seema Nagar, Nam H. Nguyen, Pankaj Dayama, Renuka Sindhgatta, Prateeti Mohapatra, Harshit Kumar, Jayant Kalagnanam, Nandyala Hemachandra, Narayan Rangaraj</li>
<li>for: This paper is written for improving the forecasting accuracy of business key performance indicators (Biz-KPIs) using IT event data.</li>
<li>methods: The paper introduces a new approach called AutoMixer, which combines channel-compressed pretraining and finetuning with a time-series Foundation Model (FM) to improve the accuracy of multivariate time series forecasting.</li>
<li>results: The paper shows that AutoMixer consistently improves the forecasting accuracy of Biz-KPIs by 11-15%, which provides actionable business insights and enhances efficiency and revenue through proactive corrective measures.<details>
<summary>Abstract</summary>
The efficiency of business processes relies on business key performance indicators (Biz-KPIs), that can be negatively impacted by IT failures. BizITOps data fuses both Biz-KPIs and IT event channels together as multivariate time series data. Forecasting Biz-KPIs in advance can enhance efficiency and revenue through proactive corrective measures. However, BizITOps data generally exhibit both useful and noisy inter-channel interactions between Biz-KPIs and IT events that need to be effectively decoupled. This leads to suboptimal forecasting performance when existing multivariate forecasting models are employed. To address this, we introduce AutoMixer, a time-series Foundation Model (FM) approach, grounded on the novel technique of channel-compressed pretrain and finetune workflows. AutoMixer leverages an AutoEncoder for channel-compressed pretraining and integrates it with the advanced TSMixer model for multivariate time series forecasting. This fusion greatly enhances the potency of TSMixer for accurate forecasts and also generalizes well across several downstream tasks. Through detailed experiments and dashboard analytics, we show AutoMixer's capability to consistently improve the Biz-KPI's forecasting accuracy (by 11-15%) which directly translates to actionable business insights.
</details>
<details>
<summary>摘要</summary>
企业过程效率取决于企业关键性表示（Biz-KPI），而IT失败可能会对其产生负面影响。BizITOps数据将Biz-KPI和IT事件通道融合为多变量时间序列数据。预测Biz-KPI可以提高效率和收入，但BizITOps数据通常存在Biz-KPI和IT事件之间有用和噪声的交互，需要有效隔离。这会导致使用现有多变量预测模型时的预测性能不佳。为解决这个问题，我们介绍AutoMixer，一种基于时间序列基本模型（FM）的approach，基于 novelt channel-compressed pretrain和finetune工作流程。AutoMixer利用AutoEncoder дляchannel-compressed pretraining，并将其与高级TSMixer模型结合，以提高TSMixer模型的准确预测能力。此外，AutoMixer还可以在多个下游任务中进行普适。通过详细的实验和幕布分析，我们表明AutoMixer能够一直提高Biz-KPI的预测精度（11-15%），这直接对商业决策提供了有用的指导。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Sample-to-Class-Graph-for-Few-Shot-Class-Incremental-Learning"><a href="#Constructing-Sample-to-Class-Graph-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning"></a>Constructing Sample-to-Class Graph for Few-Shot Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20268">http://arxiv.org/abs/2310.20268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuyuan Hu, Jian Zhang, Fan Lyu, Linyan Li, Fenglei Xu</li>
<li>for: This paper focuses on few-shot class-incremental learning (FSCIL), which aims to build machine learning models that can continually learn new concepts from a few data samples without forgetting knowledge of old classes.</li>
<li>methods: The proposed Sample-to-Class (S2C) graph learning method for FSCIL includes a Sample-level Graph Network (SGN) that analyzes sample relationships within a single session, and a Class-level Graph Network (CGN) that establishes connections across class-level features of both new and old classes. The multi-stage training strategy is designed to build S2C graph from base to few-shot stages, and improve the capacity via an extra pseudo-incremental stage.</li>
<li>results: The experiments on three popular benchmark datasets show that the proposed method clearly outperforms the baselines and sets new state-of-the-art results in FSCIL.<details>
<summary>Abstract</summary>
Few-shot class-incremental learning (FSCIL) aims to build machine learning model that can continually learn new concepts from a few data samples, without forgetting knowledge of old classes.   The challenges of FSCIL lies in the limited data of new classes, which not only lead to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems. As proved in early studies, building sample relationships is beneficial for learning from few-shot samples. In this paper, we promote the idea to the incremental scenario, and propose a Sample-to-Class (S2C) graph learning method for FSCIL.   Specifically, we propose a Sample-level Graph Network (SGN) that focuses on analyzing sample relationships within a single session. This network helps aggregate similar samples, ultimately leading to the extraction of more refined class-level features.   Then, we present a Class-level Graph Network (CGN) that establishes connections across class-level features of both new and old classes. This network plays a crucial role in linking the knowledge between different sessions and helps improve overall learning in the FSCIL scenario. Moreover, we design a multi-stage strategy for training S2C model, which mitigates the training challenges posed by limited data in the incremental process.   The multi-stage training strategy is designed to build S2C graph from base to few-shot stages, and improve the capacity via an extra pseudo-incremental stage. Experiments on three popular benchmark datasets show that our method clearly outperforms the baselines and sets new state-of-the-art results in FSCIL.
</details>
<details>
<summary>摘要</summary>
法律词数增长学习（FSCIL）目标是建立一个可以从少量数据样本中不断学习新概念的机器学习模型，而不会忘记过去的类知识。  however, the challenges of FSCIL lie in the limited data of new classes, which not only leads to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems.  early studies have shown that building sample relationships is beneficial for learning from few-shot samples. In this paper, we extend this idea to the incremental scenario and propose a Sample-to-Class (S2C) graph learning method for FSCIL.具体来说，我们提出了一种样本水平图学网络（SGN），它专注于在单个会话中分析样本之间的关系。这种网络可以帮助汇集相似的样本，从而提取更加细致的类水平特征。然后，我们提出了一种类水平图学网络（CGN），它在新和老类水平特征之间建立连接。这种网络在不同会话之间连接知识，并且帮助改善FSCIL中的总体学习。此外，我们设计了一种多阶段训练策略，用于训练S2C模型。这种多阶段训练策略是从基础阶段到几极阶段，通过额外的 pseudo-incremental 阶段来提高模型的容量。实验结果表明，我们的方法在三个 популяр的 benchmark 数据集上明显超越基eline，并在 FSCIL 中设置了新的 state-of-the-art 记录。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Average-Return-in-Markov-Decision-Processes"><a href="#Beyond-Average-Return-in-Markov-Decision-Processes" class="headerlink" title="Beyond Average Return in Markov Decision Processes"></a>Beyond Average Return in Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20266">http://arxiv.org/abs/2310.20266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre Marthe, Aurélien Garivier, Claire Vernade</li>
<li>for: 本研究探讨了Markov Decision Processes中可以高效计算和优化的奖励函数。</li>
<li>methods: 本研究使用动态 програм设(DP)和分布式奖励学习(DistRL)来研究奖励函数的计算和优化。</li>
<li>results: 研究发现仅可高效计算和优化通用化均值函数，其他函数只能近似计算。提供了误差 bound 以及这种方法的潜在应用和局限性。<details>
<summary>Abstract</summary>
What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL).DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations.These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.
</details>
<details>
<summary>摘要</summary>
Markov 决策过程中可以计算和优化的奖励功能有哪些？在无限期、未折扣设置下，动态规划（DP）只能有效处理某些类型的统计。我们summarize政策评估的特征，并给出一个新的 плани组织问题的答案。有趣的是，我们证明只有通用均值可以被优化，甚至在更广泛的分布式激励学习（DistRL）框架下也是如此。DistRL 允许评估其他函数approximately，我们提供误差 bound 的估计器，并讨论这种方法的潜在和局限性。这些结果对Markov 决策过程的理论发展做出了贡献，特别是针对风险觉刻的策略。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-reverse-engineering-application-to-detergents-using-Raman-spectroscopy"><a href="#Artificial-Intelligence-for-reverse-engineering-application-to-detergents-using-Raman-spectroscopy" class="headerlink" title="Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy"></a>Artificial Intelligence for reverse engineering: application to detergents using Raman spectroscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20254">http://arxiv.org/abs/2310.20254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Marote, Marie Martin, Anne Bonhomme, Pierre Lantéri, Yohann Clément</li>
<li>for: 这个论文主要是为了研究如何使用数字工具和分析技术来评估新型商品的潜在毒性，以及控制和规定这些产品的质量。</li>
<li>methods: 这篇论文使用了数字工具 such as spectral database, mixture database, experimental design, 和 Chemometrics &#x2F; Machine Learning algorithm，以及不同的样本准备方法，如 raw sample, 或多种压缩&#x2F;分解样本。</li>
<li>results: 这篇论文通过使用这些数字工具和分析技术，成功地鉴定了混合物的成分，并估计了其组成。这种策略可以在不同的分析工具上实施，从而节省时间，并且可以应用于不同的 matrices 和 industrialsector 中的质量控制和环境评估。<details>
<summary>Abstract</summary>
The reverse engineering of a complex mixture, regardless of its nature, has become significant today. Being able to quickly assess the potential toxicity of new commercial products in relation to the environment presents a genuine analytical challenge. The development of digital tools (databases, chemometrics, machine learning, etc.) and analytical techniques (Raman spectroscopy, NIR spectroscopy, mass spectrometry, etc.) will allow for the identification of potential toxic molecules. In this article, we use the example of detergent products, whose composition can prove dangerous to humans or the environment, necessitating precise identification and quantification for quality control and regulation purposes. The combination of various digital tools (spectral database, mixture database, experimental design, Chemometrics / Machine Learning algorithm{\ldots}) together with different sample preparation methods (raw sample, or several concentrated / diluted samples) Raman spectroscopy, has enabled the identification of the mixture's constituents and an estimation of its composition. Implementing such strategies across different analytical tools can result in time savings for pollutant identification and contamination assessment in various matrices. This strategy is also applicable in the industrial sector for product or raw material control, as well as for quality control purposes.
</details>
<details>
<summary>摘要</summary>
现代复杂混合物的反工程化（irrespective of its nature）已经成为当今的重要问题。快速评估新商品的环境影响可以提供实际分析挑战。采用数字工具（数据库、化学ometry、机器学习等）和分析技术（Raman光谱、 Near Infrared Spectroscopy、质谱等）可以用于标识潜在有害分子。本文使用洗涤品的例子，其组成可能对人类或环境构成威胁，需要精确的标识和量化以确保质量控制和法规目的。将不同的数字工具（光谱数据库、混合数据库、实验设计、化学ometry / 机器学习算法等）与不同的样本准备方法（raw sample、几种浓缩/减弱样本）结合使用，可以通过拉曼光谱识别混合物的成分并估算其组成。实施这些策略在不同的分析工具上可以节省污染物识别和污染评估的时间。这种策略也适用于工业部门，用于产品或原材料控制，以及质量控制目的。
</details></li>
</ul>
<hr>
<h2 id="Diversified-Node-Sampling-based-Hierarchical-Transformer-Pooling-for-Graph-Representation-Learning"><a href="#Diversified-Node-Sampling-based-Hierarchical-Transformer-Pooling-for-Graph-Representation-Learning" class="headerlink" title="Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning"></a>Diversified Node Sampling based Hierarchical Transformer Pooling for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20250">http://arxiv.org/abs/2310.20250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaichao Li, Jinsong Chen, John E. Hopcroft, Kun He</li>
<li>for: This paper aims to improve graph pooling methods for downsampling graphs, with the goal of achieving better performance on graph-level tasks like graph classification and graph generation.</li>
<li>methods: The proposed method, called GTPool, uses Transformer to improve node dropping pooling by capturing long-range pairwise interactions and sampling nodes diversely. The method includes a scoring module based on self-attention and a diversified sampling method called Roulette Wheel Sampling.</li>
<li>results: The paper shows that GTPool outperforms existing popular graph pooling methods on 11 benchmark datasets, demonstrating its effectiveness in capturing long-range information and selecting more representative nodes.<details>
<summary>Abstract</summary>
Graph pooling methods have been widely used on downsampling graphs, achieving impressive results on multiple graph-level tasks like graph classification and graph generation. An important line called node dropping pooling aims at exploiting learnable scoring functions to drop nodes with comparatively lower significance scores. However, existing node dropping methods suffer from two limitations: (1) for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones; (2) pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes. To address these issues, we propose a Graph Transformer Pooling method termed GTPool, which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely. Specifically, we design a scoring module based on the self-attention mechanism that takes both global context and local context into consideration, measuring the importance of nodes more comprehensively. GTPool further utilizes a diversified sampling method named Roulette Wheel Sampling (RWS) that is able to flexibly preserve nodes across different scoring intervals instead of only higher scoring nodes. In this way, GTPool could effectively obtain long-range information and select more representative nodes. Extensive experiments on 11 benchmark datasets demonstrate the superiority of GTPool over existing popular graph pooling methods.
</details>
<details>
<summary>摘要</summary>
Graph pooling方法已经广泛应用于下采 Graph，实现了多个graph-level任务的出色成绩，如图像分类和图像生成。一个重要的笔记叫node dropping pooling，目的在于利用学习权重函数来Drop nodes with relatively lower significance scores。然而，现有的node dropping方法受到两种限制：（1）for each pooled node, these models struggle to capture long-range dependencies since they mainly take GNNs as the backbones;（2）pooling only the highest-scoring nodes tends to preserve similar nodes, thus discarding the affluent information of low-scoring nodes。为了解决这些问题，我们提出了一种图Transformer Pooling方法，称为GTPool，which introduces Transformer to node dropping pooling to efficiently capture long-range pairwise interactions and meanwhile sample nodes diversely。 Specifically, we design a scoring module based on the self-attention mechanism that takes both global context and local context into consideration, measuring the importance of nodes more comprehensively。GTPool further utilizes a diversified sampling method named Roulette Wheel Sampling (RWS) that is able to flexibly preserve nodes across different scoring intervals instead of only higher scoring nodes。In this way, GTPool could effectively obtain long-range information and select more representative nodes。Extensive experiments on 11 benchmark datasets demonstrate the superiority of GTPool over existing popular graph pooling methods。
</details></li>
</ul>
<hr>
<h2 id="Breaking-Language-Barriers-in-Multilingual-Mathematical-Reasoning-Insights-and-Observations"><a href="#Breaking-Language-Barriers-in-Multilingual-Mathematical-Reasoning-Insights-and-Observations" class="headerlink" title="Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations"></a>Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20246">http://arxiv.org/abs/2310.20246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nuochenpku/MathOctopus">https://github.com/nuochenpku/MathOctopus</a></li>
<li>paper_authors: Nuo Chen, Zinan Zheng, Ning Wu, Linjun Shou, Ming Gong, Yangqiu Song, Dongmei Zhang, Jia Li</li>
<li>for: This paper aims to develop powerful Multilingual Math Reasoning (xMR) language learning models (LLMs) that can perform well in a multilingual context.</li>
<li>methods: The authors use a combination of translation and instruction datasets to train their xMR LLMs, and propose several training strategies to improve their performance. They also employ a rejection sampling strategy and parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages.</li>
<li>results: The authors achieve remarkable results with their proposed xMR LLMs, particularly in few-shot scenarios. The best-performing model, MathOctopus-13B, reaches 47.6% accuracy on the MGSM testset, outperforming ChatGPT. Additionally, the authors observe that extending the rejection sampling strategy to the multilingual context is effective for model performances, and that employing parallel corpora for math SFT can significantly enhance model performance multilingually and in specific languages.<details>
<summary>Abstract</summary>
Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset.
</details>
<details>
<summary>摘要</summary>
现有研究主要集中在开发强大的语言学习模型（LLM），以便在单语言中提高数学逻辑能力，而几乎没有探索在多语言上保持效果的问题。为了bridging这个差距，本文尝试了开拓和训练强大的多语言数学逻辑模型（xMR）。首先，通过翻译，我们构建了首个多语言数学逻辑指导集合，MGSM8KInstruct，包括10种不同的语言，因此解决了训练数据的缺乏问题在xMR任务中。基于收集的数据集，我们提出了不同的训练策略，以建立强大的xMR LLMs，称为MathOctopus，并脱颖出 Convention Open-Source LLMs 和 ChatGPT 在少数shot情况下表现出色。特别是，MathOctopus-13B 在 MGSM 测试集上达到了 47.6% 的准确率，超过 ChatGPT 46.3%。除了惊人的结果外，我们也在广泛的实验中发现了一些重要的观察和发现：1. 在多语言上扩展拒绝采样策略，虽然有限的效果，但也有助于提高模型性能。2. 在多语言上使用平行 Corpora 进行数学监督精度调教（SFT），不仅可以显著提高模型的多语言性能，还可以提高其单语言性能。这表示，制作多语言 Corpora 可以被视为一种重要的提高模型性能的策略，尤其在数学逻辑任务中。例如， MathOctopus-7B 在 GSM8K 测试集上从 42.2% 提高到 50.8%。
</details></li>
</ul>
<hr>
<h2 id="Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape"><a href="#Breathing-Life-into-Faces-Speech-driven-3D-Facial-Animation-with-Natural-Head-Pose-and-Detailed-Shape" class="headerlink" title="Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape"></a>Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20240">http://arxiv.org/abs/2310.20240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhao, Yijun Wang, Tianyu He, Lianying Yin, Jianxin Lin, Xin Jin</li>
<li>for: 实现自然且精确的对话幕后描绘，以提高虚拟人物的生动性和真实感。</li>
<li>methods: 引入VividTalker框架，将声音驱动的3D脸部动画分解为头姿和口部运动，并通过窗口式Transformer架构进行自动生成。</li>
<li>results: 与现有方法相比，VividTalker能够实现更自然和精确的声音驱动3D脸部动画，并且能够Synthesize facial details align with speech content。<details>
<summary>Abstract</summary>
The creation of lifelike speech-driven 3D facial animation requires a natural and precise synchronization between audio input and facial expressions. However, existing works still fail to render shapes with flexible head poses and natural facial details (e.g., wrinkles). This limitation is mainly due to two aspects: 1) Collecting training set with detailed 3D facial shapes is highly expensive. This scarcity of detailed shape annotations hinders the training of models with expressive facial animation. 2) Compared to mouth movement, the head pose is much less correlated to speech content. Consequently, concurrent modeling of both mouth movement and head pose yields the lack of facial movement controllability. To address these challenges, we introduce VividTalker, a new framework designed to facilitate speech-driven 3D facial animation characterized by flexible head pose and natural facial details. Specifically, we explicitly disentangle facial animation into head pose and mouth movement and encode them separately into discrete latent spaces. Then, these attributes are generated through an autoregressive process leveraging a window-based Transformer architecture. To augment the richness of 3D facial animation, we construct a new 3D dataset with detailed shapes and learn to synthesize facial details in line with speech content. Extensive quantitative and qualitative experiments demonstrate that VividTalker outperforms state-of-the-art methods, resulting in vivid and realistic speech-driven 3D facial animation.
</details>
<details>
<summary>摘要</summary>
创造生动的语音驱动3D人脸动画需要自然和精准的声音输入和脸部表情的同步。然而，现有的方法仍然无法渲染具有灵活头姿和自然的脸部细节（例如，皱纹）。这种限制主要归结于两点：1）收集具有细节3D人脸形状的训练集是非常昂贵的。这种缺乏细节形状注释限制了模型学习表达性人脸动画的训练。2）与口移动相比，头姿与语音内容的相关性远低。因此，同时模型口移动和头姿的控制导致了脸部动作的缺乏控制。为解决这些挑战，我们介绍了VividTalker，一个新的框架，用于实现语音驱动3D人脸动画，具有灵活的头姿和自然的脸部细节。具体来说，我们明确分解了脸部动画为头姿和口移动两个分支，并将它们分别编码到独立的权重空间中。然后，我们通过窗口基本的Transformer架构进行自动生成这些特征。为了增加3D人脸动画的丰富性，我们构建了一个新的3D数据集，包含细节shape，并学习以语音内容为导向synthesize facial details。广泛的量化和质量测试表明，VividTalker在语音驱动3D人脸动画方面的表现较为�ivid和真实。
</details></li>
</ul>
<hr>
<h2 id="VisPercep-A-Vision-Language-Approach-to-Enhance-Visual-Perception-for-People-with-Blindness-and-Low-Vision"><a href="#VisPercep-A-Vision-Language-Approach-to-Enhance-Visual-Perception-for-People-with-Blindness-and-Low-Vision" class="headerlink" title="VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision"></a>VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20225">http://arxiv.org/abs/2310.20225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Hao, Fan Yang, Hao Huang, Shuaihang Yuan, Sundeep Rangan, John-Ross Rizzo, Yao Wang, Yi Fang</li>
<li>for: 帮助人们 WITH 视力障碍和低视力（pBLV）更好地认识和识别不熟的环境，以及感知和识别障碍物。</li>
<li>methods: 利用大量的视觉语言模型，帮助pBLV通过提供精确的描述和警告，了解环境中的物体和障碍物。</li>
<li>results: 实验表明，我们的方法可以准确地识别物体，并为pBLV提供详细和有用的环境描述和障碍物警告。<details>
<summary>Abstract</summary>
People with blindness and low vision (pBLV) encounter substantial challenges when it comes to comprehensive scene recognition and precise object identification in unfamiliar environments. Additionally, due to the vision loss, pBLV have difficulty in accessing and identifying potential tripping hazards on their own. In this paper, we present a pioneering approach that leverages a large vision-language model to enhance visual perception for pBLV, offering detailed and comprehensive descriptions of the surrounding environments and providing warnings about the potential risks. Our method begins by leveraging a large image tagging model (i.e., Recognize Anything (RAM)) to identify all common objects present in the captured images. The recognition results and user query are then integrated into a prompt, tailored specifically for pBLV using prompt engineering. By combining the prompt and input image, a large vision-language model (i.e., InstructBLIP) generates detailed and comprehensive descriptions of the environment and identifies potential risks in the environment by analyzing the environmental objects and scenes, relevant to the prompt. We evaluate our approach through experiments conducted on both indoor and outdoor datasets. Our results demonstrate that our method is able to recognize objects accurately and provide insightful descriptions and analysis of the environment for pBLV.
</details>
<details>
<summary>摘要</summary>
人们 WITH 聊车和低视力 (pBLV) 在不熟悉的环境中面临了严重的挑战，包括缺乏准确的对象识别和环境识别。此外，由于视力损失，pBLV 有 difficulity 访问和识别陌生环境中的障碍物。在这篇论文中，我们提出了一种创新的方法，利用大量的视觉语言模型来增强聊车和低视力人群的视觉感知，并提供细致和完整的环境描述，以及陌生环境中的障碍物警告。我们的方法首先利用大量的图像标记模型（RAM）来识别捕捉到的图像中的常见对象。然后，recognition结果和用户查询被组合成特定 для pBLV 的提示，并与输入图像一起被传递给大量视觉语言模型（InstructBLIP）。InstructBLIP 根据提示和输入图像，生成细致和完整的环境描述，并通过分析环境对象和场景，对陌生环境中的障碍物进行分析和警告。我们通过对室内和室外数据集进行实验，证明了我们的方法可以准确地识别对象并为 pBLV 提供有用的环境描述和障碍物警告。
</details></li>
</ul>
<hr>
<h2 id="Choose-A-Table-Tensor-Dirichlet-Process-Multinomial-Mixture-Model-with-Graphs-for-Passenger-Trajectory-Clustering"><a href="#Choose-A-Table-Tensor-Dirichlet-Process-Multinomial-Mixture-Model-with-Graphs-for-Passenger-Trajectory-Clustering" class="headerlink" title="Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering"></a>Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with Graphs for Passenger Trajectory Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20224">http://arxiv.org/abs/2310.20224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Li, Hao Yan, Chen Zhang, Lijun Sun, Wolfgang Ketter, Fugee Tsung</li>
<li>for: 这个研究旨在提出一种基于维度 Dirichlet 过程多元混合模型和图structured clustering方法，用于旅客组聚分析。</li>
<li>methods: 这个方法利用了维度 Dirichlet 过程多元混合模型和图structured clustering方法，能够保留旅客旅行资料的层次结构，并在一步骤中自动决定集群数量。</li>
<li>results: 在香港地铁旅客数据中进行了一个案例研究，展示了这个方法可以自动决定集群数量，并提供了更好的集群质量（旅客内部紧密度和旅客间间距离）。<details>
<summary>Abstract</summary>
Passenger clustering based on trajectory records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, including multiple trips within each passenger and multi-dimensional information about each trip. Furthermore, existing approaches rely on an accurate specification of the clustering number to start. Finally, existing methods do not consider spatial semantic graphs such as geographical proximity and functional similarity between the locations. In this paper, we propose a novel tensor Dirichlet Process Multinomial Mixture model with graphs, which can preserve the hierarchical structure of the multi-dimensional trip information and cluster them in a unified one-step manner with the ability to determine the number of clusters automatically. The spatial graphs are utilized in community detection to link the semantic neighbors. We further propose a tensor version of Collapsed Gibbs Sampling method with a minimum cluster size requirement. A case study based on Hong Kong metro passenger data is conducted to demonstrate the automatic process of cluster amount evolution and better cluster quality measured by within-cluster compactness and cross-cluster separateness. The code is available at https://github.com/bonaldli/TensorDPMM-G.
</details>
<details>
<summary>摘要</summary>
乘客分组基于行程记录是交通运营商必备的。然而，现有方法难以将乘客分组，因为乘客旅行信息具有多级结构，包括每个乘客内部的多个旅行和多维信息。此外，现有方法需要准确指定分组数量，并且不考虑地理Semantic graph和功能相似性 между地点。本文提出了一种新的tensor Dirichlet进程多元混合模型，可以保持多维行程信息的层次结构并将其一步混合，并且可以自动确定分组数量。使用地理Graph进行社区检测，将semantic neighborLink。我们还提出了tensor版Collapsed Gibbs sampling方法，并要求每个分组至少包含一定的最小 cluster size。一个基于香港地铁乘客数据的案例研究，以示 automatic cluster amount evolution和更好的cluster质量（内部紧凑性和跨分组分化）。代码可以在https://github.com/bonaldli/TensorDPMM-G上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Review-for-Transformer-based-Long-term-Series-Forecasting"><a href="#A-Systematic-Review-for-Transformer-based-Long-term-Series-Forecasting" class="headerlink" title="A Systematic Review for Transformer-based Long-term Series Forecasting"></a>A Systematic Review for Transformer-based Long-term Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20218">http://arxiv.org/abs/2310.20218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyilei Su, Xumin Zuo, Rui Li, Xin Wang, Heng Zhao, Bingding Huang</li>
<li>for: 本文主要针对长期时间序列预测（LTSF）任务，探讨了transformer架构的应用和改进。</li>
<li>methods: 本文首先提供了transformer架构的概述，以及其在LTSF任务中的应用和改进。然后介绍了公共可用的LTSF数据集和评价指标。最后，提供了时间序列分析中train transformer的最佳实践和技巧。</li>
<li>results: 本文提出了LTSF任务中train transformer的最佳实践和技巧，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
The emergence of deep learning has yielded noteworthy advancements in time series forecasting (TSF). Transformer architectures, in particular, have witnessed broad utilization and adoption in TSF tasks. Transformers have proven to be the most successful solution to extract the semantic correlations among the elements within a long sequence. Various variants have enabled transformer architecture to effectively handle long-term time series forecasting (LTSF) tasks. In this article, we first present a comprehensive overview of transformer architectures and their subsequent enhancements developed to address various LTSF tasks. Then, we summarize the publicly available LTSF datasets and relevant evaluation metrics. Furthermore, we provide valuable insights into the best practices and techniques for effectively training transformers in the context of time-series analysis. Lastly, we propose potential research directions in this rapidly evolving field.
</details>
<details>
<summary>摘要</summary>
深度学习的出现引发了时间序列预测（TSF）领域的不eworthy进步。特别是transformer架构在TSF任务中得到了广泛的应用和采用。transformer架构能够有效提取长序列内元素之间的semantic相关性。不同的变体使得transformer架构能够有效处理长期时间序列预测（LTSF）任务。本文首先提供了transformer架构的全面概述和其后续改进，用于解决不同的LTSF任务。然后，我们summarize了公共可用的LTSF数据集和相关的评价指标。此外，我们还提供了有价值的时间序列分析训练策略和最佳实践。最后，我们提出了这个逐渐发展的领域中的可能的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Does-GPT-4-Pass-the-Turing-Test"><a href="#Does-GPT-4-Pass-the-Turing-Test" class="headerlink" title="Does GPT-4 Pass the Turing Test?"></a>Does GPT-4 Pass the Turing Test?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20216">http://arxiv.org/abs/2310.20216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cameron Jones, Benjamin Bergen</li>
<li>for: 这个论文主要是用来评估GPT-4模型在在线图灵测试中的性能。</li>
<li>methods: 论文使用了一个公共的在线图灵测试来评估GPT-4模型的表现，并对比了基线值（ELIZA、GPT-3.5）和人类参与者的表现。</li>
<li>results: 结果显示，GPT-4模型在41%的游戏中突破了基线值（ELIZA的27%和GPT-3.5的14%），但 ainda fall short of chance和人类参与者的表现（63%）。研究发现，参与者的决策主要基于语言风格（35%）和社会情感特征（27%），表明智能不足以通过图灵测试。<details>
<summary>Abstract</summary>
We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4 prompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and GPT-3.5 (14%), but falling short of chance and the baseline set by human participants (63%). Participants' decisions were based mainly on linguistic style (35%) and socio-emotional traits (27%), supporting the idea that intelligence is not sufficient to pass the Turing Test. Participants' demographics, including education and familiarity with LLMs, did not predict detection rate, suggesting that even those who understand systems deeply and interact with them frequently may be susceptible to deception. Despite known limitations as a test of intelligence, we argue that the Turing Test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.
</details>
<details>
<summary>摘要</summary>
我们对 GPT-4 进行了公共在线图灵测试。最佳 GPT-4 提示在游戏中取得了41%的成绩，超过了 ELIZA （27%）和 GPT-3.5 （14%）的基线，但落后于人类参与者的基线（63%）。参与者的决策主要基于语言风格（35%）和社会情感特征（27%），这支持了智能不足以通过图灵测试的想法。参与者的民生背景、教育和 LLMS familiarity 没有预测检测率，表明，即使深入了解系统并经常与它们交互，也可能受到欺骗。虽然图灵测试有知限性，但我们认为它仍然是自然语言交流和欺骗的有效评价方式。 AI 模型具有人类化的能力可能会对社会造成广泛的影响，我们分析了不同的策略和标准来评价人类化程度。
</details></li>
</ul>
<hr>
<h2 id="Handover-Protocol-Learning-for-LEO-Satellite-Networks-Access-Delay-and-Collision-Minimization"><a href="#Handover-Protocol-Learning-for-LEO-Satellite-Networks-Access-Delay-and-Collision-Minimization" class="headerlink" title="Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization"></a>Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20215">http://arxiv.org/abs/2310.20215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju-Hyung Lee, Chanyoung Park, Soohyun Park, Andreas F. Molisch</li>
<li>for: 这项研究旨在解决低地球轨道卫星网络中的长传输延迟问题，通过提出一种基于深度学习承诺协议（DHO）。</li>
<li>methods: 该研究使用了一种predictive能力训练的LEO卫星轨道模式，以避免MR阶段的传输延迟，同时仍能提供有效的HO决策。</li>
<li>results: 比较研究表明，DHO协议在多种网络条件下比传统HO协议表现更好，包括访问延迟、碰撞率和HO成功率。此外，研究还检验了访问延迟和碰撞率之间的负面关系，以及DHO使用不同DRL算法的训练性能和归一化。<details>
<summary>Abstract</summary>
This study presents a novel deep reinforcement learning (DRL)-based handover (HO) protocol, called DHO, specifically designed to address the persistent challenge of long propagation delays in low-Earth orbit (LEO) satellite networks' HO procedures. DHO skips the Measurement Report (MR) in the HO procedure by leveraging its predictive capabilities after being trained with a pre-determined LEO satellite orbital pattern. This simplification eliminates the propagation delay incurred during the MR phase, while still providing effective HO decisions. The proposed DHO outperforms the legacy HO protocol across diverse network conditions in terms of access delay, collision rate, and handover success rate, demonstrating the practical applicability of DHO in real-world networks. Furthermore, the study examines the trade-off between access delay and collision rate and also evaluates the training performance and convergence of DHO using various DRL algorithms.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这个研究提出了一种基于深度学习束约学习（DRL）的手over（HO）协议，称为DHO，特意设计用于解决低轨道卫星网络中HO过程中的持续存在的长延迟问题。DHO跳过测量报告（MR）阶段，通过使用已经预先确定的LEO卫星轨道模式来预测MR的内容。这种简化掉了MR阶段所带来的延迟，同时仍然提供有效的HO决策。提议的DHO在多种网络条件下比传统HO协议有更好的访问延迟、碰撞率和手over成功率，这显示了DHO在实际网络中的实用性。此外，研究还检验了访问延迟和碰撞率之间的负面关系，以及DHO使用不同DRL算法的训练性能和融合。
</details></li>
</ul>
<hr>
<h2 id="In-Search-of-Lost-Online-Test-time-Adaptation-A-Survey"><a href="#In-Search-of-Lost-Online-Test-time-Adaptation-A-Survey" class="headerlink" title="In Search of Lost Online Test-time Adaptation: A Survey"></a>In Search of Lost Online Test-time Adaptation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20199">http://arxiv.org/abs/2310.20199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixin Wang, Yadan Luo, Liang Zheng, Zhuoxiao Chen, Sen Wang, Zi Huang</li>
<li>for: 本文提供了在线测试时适应（OTTA）的全面评估，OTTA 是适应机器学习模型到新数据分布 upon batch arrival 的 paradigm。</li>
<li>methods: 本文将 OTTA 技术分为三类，并对其进行了 benchmark 使用 potent Vision Transformer（ViT）backbone，以便进行比较和评估。</li>
<li>results: 我们的 benchmark 包括了 conventional corrupted datasets 如 CIFAR-10&#x2F;100-C 和 ImageNet-C，以及实际世界中的 shift 如 CIFAR-10.1 和 CIFAR-10-Warehouse，这些Shift 包括了搜索引擎和Diffusion模型生成的数据。我们引入了新的评估指标 FLOPs，以评估在线场景中的效率。我们的发现是：(1) transformers 对多种领域的 shift 具有高度的抗性，(2) 许多 OTTA 方法的效果取决于较大的 batch size，(3) 在适应过程中，稳定的优化和抗扰特性尤其重要，特别是当 batch size 为 1。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusive of FLOPs, shedding light on the trade-offs between adaptation accuracy and computational overhead. Our findings diverge from existing literature, indicating: (1) transformers exhibit heightened resilience to diverse domain shifts, (2) the efficacy of many OTTA methods hinges on ample batch sizes, and (3) stability in optimization and resistance to perturbations are critical during adaptation, especially when the batch size is 1. Motivated by these insights, we pointed out promising directions for future research. The source code will be made available.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了在线测试时适应（OTTA）的全面评论，涉及到在批处 arrival 时适应机器学习模型的新数据分布。尽管在最近的几年内，OTTA 技术得到了广泛的应用和研究，但是这个领域仍然受到一些问题的困扰，如模糊的设置、过时的基础模型和不一致的 гиперпарамет尔调整，这些问题使得研究成果受到难以复制和评估的影响。为了便于对 OTTA 技术进行明确和系统的比较，我们将其分为三种主要类别，并对它们进行 benchmark 使用强大的 Vision Transformer（ViT）基础模型。我们的 benchmark 包括 conventional 的损害数据集 CIFAR-10/100-C 和 ImageNet-C，以及实际世界的变化，包括 CIFAR-10.1 和 CIFAR-10-Warehouse，这些变化包括搜索引擎和 Synthesized 数据的扩散模型。为了评估在线enario 中的效率，我们引入了新的评价指标，包括 FLOPs，从而揭示了适应精度和计算负担之间的交易。我们的发现与现有文献不同，表明：（1） transformers 在多种领域的域shift 中表现出了更高的抗性，（2）许多 OTTA 方法的效果取决于较大的批处大小，（3）在适应过程中稳定的优化和抗辐射性是关键，特别是批处大小为 1。基于这些发现，我们指出了未来研究的可能性。我们将源代码公开。
</details></li>
</ul>
<hr>
<h2 id="Generating-Continuations-in-Multilingual-Idiomatic-Contexts"><a href="#Generating-Continuations-in-Multilingual-Idiomatic-Contexts" class="headerlink" title="Generating Continuations in Multilingual Idiomatic Contexts"></a>Generating Continuations in Multilingual Idiomatic Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20195">http://arxiv.org/abs/2310.20195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rhitabrat Pokharel, Ameeta Agrawal</li>
<li>for: 测试语言模型对 figurative text 的理解能力</li>
<li>methods: 使用英语和葡萄牙语的数据集，采用零 shot、几 shot 和精度调整的训练方法</li>
<li>results: 模型在 literal 和idiomatic 上的表现差异很小，同时模型在两种语言上的表现相似，表明生成模型在这种任务中的 Robustness.<details>
<summary>Abstract</summary>
The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
</details>
<details>
<summary>摘要</summary>
语言处理词组或文本中的idiomatic表达能力是任何语言理解和生成的关键方面。我们通过对包含idiomatic（或literal）表达的故事中的上下文 relevancecontinuation进行生成测试，以评估生成语言模型（LMs）对非compositional figurative文本的理解能力。我们在英语和葡萄牙语两种语言下进行了一系列实验，使用零shot、少数shot和精度调整的训练方法。我们的结果表明，模型对 literal上下文的生成性能和idiomatic上下文的生成性能几乎没有差异，差距非常小。此外，我们的实验结果表明，模型在这两种语言中表现几乎相同，这表明生成模型在这种任务上具有坚定的 Robustness。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Pre-training-for-Precipitation-Post-processor"><a href="#Self-supervised-Pre-training-for-Precipitation-Post-processor" class="headerlink" title="Self-supervised Pre-training for Precipitation Post-processor"></a>Self-supervised Pre-training for Precipitation Post-processor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20187">http://arxiv.org/abs/2310.20187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sojung An, Junha Lee, Jiyeon Jang, Inchae Na, Wooyeon Park, Sujeong You</li>
<li>for: 提高地方气象预测中的暴雨预测精度，以防止恶势力天气事件。</li>
<li>methods: 提议使用深度学习对气象数值预测模型进行修正，并使用自动标注法对不均匀的数据集进行训练。</li>
<li>results: 实验结果显示，提议的方法可以在地方气象预测中提高暴雨预测精度，并且超过其他方法的性能。<details>
<summary>Abstract</summary>
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
</details>
<details>
<summary>摘要</summary>
要确保地方降水的预测预测时间足够，是预测恶势力天气事件的关键。然而，全球变暖引起的气候变化使得准确预测恶势力降水事件变得更加困难。在这项工作中，我们提出了基于深度学习的降水后处理方法，用于数值天气预测（NWP）模型。降水后处理方法包括（i）自我监督预训练，其中参数Encoder在掩码变量的恢复问题上进行自我监督预训练，以及（ii）在降水分类任务（目标领域）上进行传输学习，从预训练的Encoder中提取参数。我们还介绍了一种有效地训练类别不均衡数据集的启发性标签方法。我们的实验结果表明，提议的方法在地方NWP中的降水更正中超过了其他方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Discover-Skills-through-Guidance"><a href="#Learning-to-Discover-Skills-through-Guidance" class="headerlink" title="Learning to Discover Skills through Guidance"></a>Learning to Discover Skills through Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20178">http://arxiv.org/abs/2310.20178</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Hyunseung Kim, Byungkun Lee, Hojoon Lee, Dongyoon Hwang, Sejik Park, Kyushik Min, Jaegul Choo</li>
<li>For: 提高无监督技能发现（USD）中的探索性能，尤其是在环境复杂度高时。* Methods: 提出了一种新的 USD算法，即技能发现指导（DISCO-DANCE），通过选择具有最高潜力达到未探索状态的导导技能，然后导引其他技能跟随，最后将导引技能分散以 maximize其在未探索状态中的分化性。* Results: 对比其他 USD基线，DISCO-DANCE在具有具有高环境复杂度的 Navigation benchmark 和 Continuous Control  benchmark 中表现出色，并且可以在具有高环境复杂度的情况下提高探索性能。<details>
<summary>Abstract</summary>
In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE are available at https://mynsng.github.io/discodance.
</details>
<details>
<summary>摘要</summary>
在无监督技能发现（USD）领域，一个主要挑战是有限的探索，主要是因为行为偏离初始轨迹的惩罚。为了增强探索，现有方法ologies使用辅助奖励来最大化状态的认知不确定性或熵。然而，我们发现在环境复杂度增加时，这些奖励的效果下降。因此，我们提出了一种新的 USD算法，即技能发现导航（DISCO-DANCE），它包括以下三个步骤：1. 选择具有最高潜在性能力的导航技能，即可以达到未探索的状态。2. 将其他技能指导到导航技能的轨迹上。3. 使用导航技能的各个维度进行散布，以最大化其在未探索状态下的分化度。实验证明，DISCO-DANCE在复杂环境中比其他 USD 基线表现更佳，包括两个导航benchmark和一个连续控制benchmark。详细的visualization和代码可以在 <https://mynsng.github.io/discodance> 上查看。
</details></li>
</ul>
<hr>
<h2 id="GraphTransformers-for-Geospatial-Forecasting"><a href="#GraphTransformers-for-Geospatial-Forecasting" class="headerlink" title="GraphTransformers for Geospatial Forecasting"></a>GraphTransformers for Geospatial Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20174">http://arxiv.org/abs/2310.20174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pallavi Banerjee, Satyaki Chakraborty</li>
<li>for: 预测地理序列中的轨迹，使用图Transformer提高预测精度。</li>
<li>methods: 利用地理点之间自然生成的图结构，并将其 direktly  интегрирован到Transformer模型中，以提高预测精度。</li>
<li>results: 在HURDAT数据集上，our GraphTransformer方法与基准模型相比，显著提高了预测精度。<details>
<summary>Abstract</summary>
In this paper we introduce a novel framework for trajectory prediction of geospatial sequences using GraphTransformers. When viewed across several sequences, we observed that a graph structure automatically emerges between different geospatial points that is often not taken into account for such sequence modeling tasks. We show that by leveraging this graph structure explicitly, geospatial trajectory prediction can be significantly improved. Our GraphTransformer approach improves upon state-of-the-art Transformer based baseline significantly on HURDAT, a dataset where we are interested in predicting the trajectory of a hurricane on a 6 hourly basis.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的框架，用于预测地理序列的轨迹使用图transformer。当视图多个序列时，我们发现了不同的地理点之间自然形成的图结构，通常不被考虑在这类序列模型任务中。我们表明，通过显式利用这个图结构，可以显著提高地理轨迹预测。我们的图transformer方法在HURDAT dataset上，对6小时预测风暴轨迹的任务显示出了明显的提高。
</details></li>
</ul>
<hr>
<h2 id="Is-Robustness-Transferable-across-Languages-in-Multilingual-Neural-Machine-Translation"><a href="#Is-Robustness-Transferable-across-Languages-in-Multilingual-Neural-Machine-Translation" class="headerlink" title="Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?"></a>Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20162">http://arxiv.org/abs/2310.20162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiyu Pan, Supryadi, Deyi Xiong</li>
<li>for: 本研究旨在探讨多种语言之间的机器翻译模型在不同翻译方向下的 Robustness 性能是否可以转移。</li>
<li>methods: 本研究使用了 Character-、word- 和 multi-level 噪声进行攻击特定翻译方向的多语言神经机器翻译模型，并对其他翻译方向的 Robustness 进行评估。</li>
<li>results: 研究结果表明，在一个翻译方向上获得的 Robustness 性能可以转移到其他翻译方向上，并且在字符级噪声和单词级噪声攻击下，Robustness 性能更容易转移。<details>
<summary>Abstract</summary>
Robustness, the ability of models to maintain performance in the face of perturbations, is critical for developing reliable NLP systems. Recent studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages in multilingual neural machine translation. We propose a robustness transfer analysis protocol and conduct a series of experiments. In particular, we use character-, word-, and multi-level noises to attack the specific translation direction of the multilingual neural machine translation model and evaluate the robustness of other translation directions. Our findings demonstrate that the robustness gained in one translation direction can indeed transfer to other translation directions. Additionally, we empirically find scenarios where robustness to character-level noise and word-level noise is more likely to transfer.
</details>
<details>
<summary>摘要</summary>
Robustness，模型在干扰下表现良好的能力，对于建立可靠的自然语言处理系统是非常重要。 latest studies have shown promising results in improving the robustness of models through adversarial training and data augmentation. However, in machine translation, most of these studies have focused on bilingual machine translation with a single translation direction. In this paper, we investigate the transferability of robustness across different languages in multilingual neural machine translation. We propose a robustness transfer analysis protocol and conduct a series of experiments. In particular, we use character-, word-, and multi-level noises to attack the specific translation direction of the multilingual neural machine translation model and evaluate the robustness of other translation directions. Our findings demonstrate that the robustness gained in one translation direction can indeed transfer to other translation directions. Additionally, we empirically find scenarios where robustness to character-level noise and word-level noise is more likely to transfer.
</details></li>
</ul>
<hr>
<h2 id="Language-Guided-Visual-Question-Answering-Elevate-Your-Multimodal-Language-Model-Using-Knowledge-Enriched-Prompts"><a href="#Language-Guided-Visual-Question-Answering-Elevate-Your-Multimodal-Language-Model-Using-Knowledge-Enriched-Prompts" class="headerlink" title="Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts"></a>Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20159">http://arxiv.org/abs/2310.20159</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/declare-lab/lg-vqa">https://github.com/declare-lab/lg-vqa</a></li>
<li>paper_authors: Deepanway Ghosal, Navonil Majumder, Roy Ka-Wei Lee, Rada Mihalcea, Soujanya Poria</li>
<li>for: 这 paper 的目的是解释如何使用语言指导来提高视觉问答系统的性能。</li>
<li>methods: 该 paper 提出了一种多模态框架，使用语言指导（LG）来提高视觉问答系统的准确率。LG 包括理由、图像描述、场景图等。</li>
<li>results: 该 paper 在 A-OKVQA、Science-QA、VSR 和 IconQA 等数据集上进行了多个多选问答任务的测试，并证明了语言指导是一种简单 yet 强大的策略，可以提高 CLIP 和 BLIP 模型在 A-OKVQA 数据集中的性能。<details>
<summary>Abstract</summary>
Visual question answering (VQA) is the task of answering questions about an image. The task assumes an understanding of both the image and the question to provide a natural language answer. VQA has gained popularity in recent years due to its potential applications in a wide range of fields, including robotics, education, and healthcare. In this paper, we focus on knowledge-augmented VQA, where answering the question requires commonsense knowledge, world knowledge, and reasoning about ideas and concepts not present in the image. We propose a multimodal framework that uses language guidance (LG) in the form of rationales, image captions, scene graphs, etc to answer questions more accurately. We benchmark our method on the multi-choice question-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets using CLIP and BLIP models. We show that the use of language guidance is a simple but powerful and effective strategy for visual question answering. Our language guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8% in the challenging A-OKVQA dataset. We also observe consistent improvement in performance on the Science-QA, VSR, and IconQA datasets when using the proposed language guidances. The implementation of LG-VQA is publicly available at https:// github.com/declare-lab/LG-VQA.
</details>
<details>
<summary>摘要</summary>
“视觉问答（VQA）是解答图像上的问题的任务。该任务假设理解图像和问题，以提供自然语言的答案。VQA在过去几年内得到了广泛的关注，因为它在多个领域有广泛的应用前景，如机器人、教育和医疗。在这篇论文中，我们关注知识增强VQA，其中回答问题需要通过图像和问题的理解，以及对概念和想法的推理。我们提出了一种多modal的框架，使用语言指导（LG），包括理由、图像描述、场景图等，以更准确地回答问题。我们使用CLIP和BLIP模型对多个选择问答任务进行了benchmarking，并证明了使用语言指导是一种简单 yet powerful和有效的策略。我们在A-OKVQA、Science-QA、VSR和IconQA数据集上实现了LG-VQA，并观察到在这些数据集上表现更加稳定和可靠。LG-VQA的实现可以在https://github.com/declare-lab/LG-VQA上获得。”
</details></li>
</ul>
<hr>
<h2 id="MLatom-3-Platform-for-machine-learning-enhanced-computational-chemistry-simulations-and-workflows"><a href="#MLatom-3-Platform-for-machine-learning-enhanced-computational-chemistry-simulations-and-workflows" class="headerlink" title="MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows"></a>MLatom 3: Platform for machine learning-enhanced computational chemistry simulations and workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20155">http://arxiv.org/abs/2310.20155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavlo O. Dral, Fuchun Ge, Yi-Fan Hou, Peikun Zheng, Yuxinxin Chen, Mario Barbatti, Olexandr Isayev, Cheng Wang, Bao-Xin Xue, Max Pinheiro Jr, Yuming Su, Yiheng Dai, Yangtao Chen, Lina Zhang, Shuang Zhang, Arif Ullah, Quanhao Zhang, Yanchi Ou</li>
<li>for: 这篇论文主要是用来探讨Machine Learning（ML）在计算化学中的应用，以及如何使用MLatom3软件包来实现自定义工作流程。</li>
<li>methods: 这篇论文使用了MLatom3软件包，可以通过命令行选项、输入文件或脚本来运行计算化学 simulations，并且可以在计算机和XACS云计算平台上进行计算。</li>
<li>results: 这篇论文可以用ML、量子机理和组合模型来计算能量和热化学性质、优化结构、运行分子和量子动力学、计算（ro）振荡、一 photon UV&#x2F;vis吸收和两 photon吸收谱。用户可以从库中选择各种预训练的ML模型和量子机理方法，而开发者可以使用多种ML算法来建立自己的模型。<details>
<summary>Abstract</summary>
Machine learning (ML) is increasingly becoming a common tool in computational chemistry. At the same time, the rapid development of ML methods requires a flexible software framework for designing custom workflows. MLatom 3 is a program package designed to leverage the power of ML to enhance typical computational chemistry simulations and to create complex workflows. This open-source package provides plenty of choice to the users who can run simulations with the command line options, input files, or with scripts using MLatom as a Python package, both on their computers and on the online XACS cloud computing at XACScloud.com. Computational chemists can calculate energies and thermochemical properties, optimize geometries, run molecular and quantum dynamics, and simulate (ro)vibrational, one-photon UV/vis absorption, and two-photon absorption spectra with ML, quantum mechanical, and combined models. The users can choose from an extensive library of methods containing pre-trained ML models and quantum mechanical approximations such as AIQM1 approaching coupled-cluster accuracy. The developers can build their own models using various ML algorithms. The great flexibility of MLatom is largely due to the extensive use of the interfaces to many state-of-the-art software packages and libraries.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interactive-Multi-fidelity-Learning-for-Cost-effective-Adaptation-of-Language-Model-with-Sparse-Human-Supervision"><a href="#Interactive-Multi-fidelity-Learning-for-Cost-effective-Adaptation-of-Language-Model-with-Sparse-Human-Supervision" class="headerlink" title="Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision"></a>Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20153">http://arxiv.org/abs/2310.20153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Zhang, Zhuohang Li, Kamalika Das, Sricharan Kumar</li>
<li>for: 这则研究旨在开发小型领域专门的语言模型（LLM），并且在有限的标注预算下进行开发。</li>
<li>methods: 我们提出了一个名为多元信息学习（IMFL）框架，它可以在有限的标注预算下开发小型领域专门的LLM。我们将这个预算为多元信息学习问题，并且专注于确定最佳的标注策略，以将低信任度自动LLM标注和高信任度人工标注结合以最大化模型性能。</li>
<li>results: 我们的实验结果显示，IMFL可以与人工标注相比，在金融和医疗领域中的四个任务中表现出色。对于有限的人工标注预算，IMFL可以将人工标注cost从原来的一半降低，同时保持比人工标注性能几乎相同。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation, and 2) variable batch size that controls the order for choosing each fidelity to facilitate knowledge distillation, ultimately enhancing annotation quality. Extensive experiments on financial and medical tasks demonstrate that IMFL achieves superior performance compared with single fidelity annotations. Given a limited budget of human annotation, IMFL significantly outperforms the human annotation baselines in all four tasks and achieves very close performance as human annotations on two of the tasks. These promising results suggest that the high human annotation costs in domain-specific tasks can be significantly reduced by employing IMFL, which utilizes fewer human annotations, supplemented with cheaper and faster LLM (e.g., GPT-3.5) annotations to achieve comparable performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在不同任务中表现出了杰出的能力，但是它们在执行特定领域任务时，受到巨大规模的部署和易受到误information的影响，并且需要大量的数据标注成本。我们提出了一个新的互动式多层精确度学习（IMFL）框架，用于效率地开发小型领域特定的 LLM，以降低数据标注成本。我们的方法将领域特定精确化过程定义为多层精确度学习问题，专注于发现最佳获取策略，以将低精度自动 LLM 标注和高精度人类标注平衡为最大化模型表现。我们还提出了一个探索-利用查询策略，增加标注多样性和有用性，包括两个创新设计：1）提取人类标注项目中的内容例子，以改善 LLM 标注，2）变化批次大小，以控制每个精度选择的顺序，以便传播知识传授，最终提高标注质量。实验结果显示，IMFL 在金融和医疗任务中具有较高的表现，相比单一精度标注，IMFL 在所有四个任务中具有更高的表现，并且在两个任务中实现了非常接近人类标注的表现。这些成果显示，对特定领域任务的人类标注成本可以通过IMFL的实现，从而实现更好的数据标注效率。
</details></li>
</ul>
<hr>
<h2 id="Unlearn-What-You-Want-to-Forget-Efficient-Unlearning-for-LLMs"><a href="#Unlearn-What-You-Want-to-Forget-Efficient-Unlearning-for-LLMs" class="headerlink" title="Unlearn What You Want to Forget: Efficient Unlearning for LLMs"></a>Unlearn What You Want to Forget: Efficient Unlearning for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20150">http://arxiv.org/abs/2310.20150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaao Chen, Diyi Yang</li>
<li>for: 提高大语言模型（LLMs）的隐私和数据保护，避免数据泄露和违反数据保护法规。</li>
<li>methods: 提出一种高效的快速忘记框架，通过在转换器中引入轻量级忘记层，实现不需要重新训练整个模型来更新模型。此外，提出一种 fuselay mechanism，以有效地结合不同的忘记层，处理序列的忘记操作。</li>
<li>results: 通过对分类和生成任务进行实验，证明了我们提出的方法的效果，比靶场标准方法更高。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在预训练和记忆各种文本数据上取得了显著进步，但这个过程可能会遇到隐私问题和数据保护规定的违反。因此，能够轻松地从LLM中移除关于个人用户的数据而不影响预测质量的能力变得越来越重要。为解决这些问题，在这个工作中，我们提出了一种高效的忘记框架，通过在转换器中添加轻量级忘记层，使得在数据移除后不需要重新训练整个模型。此外，我们还引入了融合机制，以有效地将不同的忘记层结合，以处理一系列忘记操作。实验表明，我们提出的方法与当前基eline相比，在分类和生成任务上具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Decision-Making-for-Autonomous-Vehicles-with-Interaction-Aware-Behavioral-Prediction-and-Social-Attention-Neural-Network"><a href="#Decision-Making-for-Autonomous-Vehicles-with-Interaction-Aware-Behavioral-Prediction-and-Social-Attention-Neural-Network" class="headerlink" title="Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network"></a>Decision-Making for Autonomous Vehicles with Interaction-Aware Behavioral Prediction and Social-Attention Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20148">http://arxiv.org/abs/2310.20148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Li, Kaiwen Liu, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky</li>
<li>for: 本研究旨在帮助自动驾驶车辆更好地理解周围交通情况，以便更好地完成任务。</li>
<li>methods: 本研究提出了一个行为模型，该模型将交通 Driver 的交互意图编码为隐藏的社会心理参数。通过 bayesian 滤波器，我们开发了一种循环滤波器优化算法，该算法考虑了交互 Driver 的意图不确定性。在线部署时，我们设计了基于注意力机制的神经网络架构，该架构模仿行为模型，并使用在线估计的参数先验。我们还提出了一种决策搜索算法来解决决策问题。</li>
<li>results: 我们对行为模型进行了实际路径预测测试，并对决策模块进行了广泛的评估，包括在强制合流场景中使用真实世界交通数据进行测试。结果表明，我们的算法可以在不同交通条件下完成强制合流任务，同时保证安全驾驶。<details>
<summary>Abstract</summary>
Autonomous vehicles need to accomplish their tasks while interacting with human drivers in traffic. It is thus crucial to equip autonomous vehicles with artificial reasoning to better comprehend the intentions of the surrounding traffic, thereby facilitating the accomplishments of the tasks. In this work, we propose a behavioral model that encodes drivers' interacting intentions into latent social-psychological parameters. Leveraging a Bayesian filter, we develop a receding-horizon optimization-based controller for autonomous vehicle decision-making which accounts for the uncertainties in the interacting drivers' intentions. For online deployment, we design a neural network architecture based on the attention mechanism which imitates the behavioral model with online estimated parameter priors. We also propose a decision tree search algorithm to solve the decision-making problem online. The proposed behavioral model is then evaluated in terms of its capabilities for real-world trajectory prediction. We further conduct extensive evaluations of the proposed decision-making module, in forced highway merging scenarios, using both simulated environments and real-world traffic datasets. The results demonstrate that our algorithms can complete the forced merging tasks in various traffic conditions while ensuring driving safety.
</details>
<details>
<summary>摘要</summary>
自主车辆需要在交通中完成任务，因此需要具备人工智能来更好地理解周围交通的意图，以便更好地完成任务。在这个工作中，我们提出了一个行为模型，将交互的driver意图编码为隐藏的社会心理参数。利用一个bayesian滤波器，我们开发了一个逐页估计Optimization的控制器，考虑到交互driver意图的不确定性。为在线上部署，我们设计了一个基于注意力机制的神经网络架构，实现行为模型的在线估计参数。我们还提出了一个搜索算法来解决实时决策问题。实验结果显示，我们的方法可以在不同的交通情况下完成强制合流任务，并确保安全驾驶。
</details></li>
</ul>
<hr>
<h2 id="EELBERT-Tiny-Models-through-Dynamic-Embeddings"><a href="#EELBERT-Tiny-Models-through-Dynamic-Embeddings" class="headerlink" title="EELBERT: Tiny Models through Dynamic Embeddings"></a>EELBERT: Tiny Models through Dynamic Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20144">http://arxiv.org/abs/2310.20144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabrielle Cohn, Rishika Agarwal, Deepanshu Gupta, Siddharth Patwardhan</li>
<li>for: 这篇论文是为了实现对 transformer-based 模型（例如 BERT）的压缩，并对下游任务的精度维持最小影响。</li>
<li>methods: 这篇论文使用的方法是取代模型的输入嵌入层，并使用动态嵌入计算。这个嵌入层贡献了模型的大部分，尤其是小型 BERT 的情况下。</li>
<li>results: 这篇论文的实验结果显示，使用这种方法可以对 BERT 模型进行压缩，并且对下游任务的精度维持相对稳定。特别是，在 GLUE 测试 benchmark 上，我们的 BERT Variants（EELBERT）与传统 BERT 模型之间的差异几乎没有。这允许我们开发出最小化的模型 UNO-EELBERT，它在 GLUE 测试中获得了相对于完全训练的 BERT-tiny 的 GLUE 分数，但是它的大小仅有 1.2 MB，相对于 BERT-tiny 的 4%。<details>
<summary>Abstract</summary>
We introduce EELBERT, an approach for compression of transformer-based models (e.g., BERT), with minimal impact on the accuracy of downstream tasks. This is achieved by replacing the input embedding layer of the model with dynamic, i.e. on-the-fly, embedding computations. Since the input embedding layer accounts for a significant fraction of the model size, especially for the smaller BERT variants, replacing this layer with an embedding computation function helps us reduce the model size significantly. Empirical evaluation on the GLUE benchmark shows that our BERT variants (EELBERT) suffer minimal regression compared to the traditional BERT models. Through this approach, we are able to develop our smallest model UNO-EELBERT, which achieves a GLUE score within 4% of fully trained BERT-tiny, while being 15x smaller (1.2 MB) in size.
</details>
<details>
<summary>摘要</summary>
我们介绍EELBERT，一种对transformer-based模型（如BERT）进行压缩，并对下游任务的精度有 minimal impact。这是通过取代模型中的输入嵌入层而实现的，并且使用dinamic嵌入计算函数。由于输入嵌入层对模型大小的影响相当大，特别是小型BERT的情况下，因此替换这个层可以实现很大的模型尺寸优化。我们的实验结果显示，我们的BERT variants（EELBERT）与传统BERT模型之间的差异非常小，而且我们可以开发出最小的模型UNO-EELBERT，其GLUE分数与完全训练的BERT-tiny相似（准确度 Within 4%），但是仅有1.2 MB的大小，相较于BERT-tiny的15倍。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Difference-Predictive-Coding"><a href="#Contrastive-Difference-Predictive-Coding" class="headerlink" title="Contrastive Difference Predictive Coding"></a>Contrastive Difference Predictive Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20141">http://arxiv.org/abs/2310.20141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chongyi-zheng/td_infonce">https://github.com/chongyi-zheng/td_infonce</a></li>
<li>paper_authors: Chongyi Zheng, Ruslan Salakhutdinov, Benjamin Eysenbach</li>
<li>for: 学习和预测未来事件的方法</li>
<li>methods: 使用对比预测编码学习时间序列数据，并将不同时间序列数据缝合起来减少学习数据量</li>
<li>results: 比对前方法，该方法在目标条件强化学习中 achieve 2 倍的成功率增加和更好地处理随机环境，并在表格设置中比标准（蒙特卡洛）对比预测编码oding更高效，具体是 $20 \times$ 更高效和 $1500 \times$ 更高效。<details>
<summary>Abstract</summary>
Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves $2 \times$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about $20 \times$ more sample efficient than the successor representation and $1500 \times$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将未来的预测和推理视为时间序列问题的核心。例如，目标条件化强化学习可以视为学习表示系统来预测哪些状态可能会在未来访问。而先前的方法使用了对比预测编码来模型时间序列数据，而学习表示系统通常需要大量数据来表示长期依赖关系。在这篇论文中，我们引入了时间差版本的对比预测编码，将不同时间序列数据的片段缝合起来，以降低学习未来事件预测所需的数据量。我们应用这种表示学习方法， derivate一种离线RL算法。实验表明，相比先前的RL方法，我们的方法可以达到2倍的成功率，并且在随机环境中更好地适应。在表格设置下，我们表明，我们的方法比successor表示和标准（蒙地卡罗）对比预测编码高$20\times$的样本效率，并且高$1500\times$的样本效率。<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Efficient-Classification-of-Student-Help-Requests-in-Programming-Courses-Using-Large-Language-Models"><a href="#Efficient-Classification-of-Student-Help-Requests-in-Programming-Courses-Using-Large-Language-Models" class="headerlink" title="Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models"></a>Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20105">http://arxiv.org/abs/2310.20105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaromir Savelka, Paul Denny, Mark Liffiton, Brad Sheese</li>
<li>for: 本研究旨在评估用大语言模型（LLM）自动分类学生帮助请求的可能性，以便在教育系统中提高响应效果。</li>
<li>methods: 本研究使用GPT-3.5和GPT-4模型进行 Zero-shot 试验，以评估这两种模型在分类学生帮助请求的能力。</li>
<li>results: GPT-3.5和GPT-4模型在大多数类别上表现相似，但GPT-4在 debug 相关的子类别上表现出了较高的表现。 fine-tuning GPT-3.5 模型可以大幅提高其表现，以至于与两名人类评分者的准确率和一致性几乎相同。<details>
<summary>Abstract</summary>
The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过准确地类别学生求助的类型，可以实现个性化的回应。自动类别这些请求是一件非常困难的任务，但大语言模型（LLM）似乎提供了可 accessible 和Cost-effective的解决方案。这个研究评估了 GPT-3.5 和 GPT-4 模型在学生入门编程课程的学生帮助请求中的表现。在零批测试中，GPT-3.5 和 GPT-4 在大多数类别上表现相似，而 GPT-4 在 Debugging 相关的sub-category上表现更高，Outperformed GPT-3.5。细化 GPT-3.5 模型可以使其表现得更好，以至于与两名人类评分员的准确率和一致性相似。总之，这个研究表明了使用 LLM 提高教育系统的可能性，通过自动类别学生需求。Note: "GPT-3.5" and "GPT-4" refer to two different language models, and "zero-shot trials" refer to situations where the models are tested on tasks they have not been trained on. "Sub-categories" refer to more specific categories within a larger category. "Fine-tuning" refers to the process of adjusting the model's parameters to improve its performance on a specific task.
</details></li>
</ul>
<hr>
<h2 id="Plagiarism-and-AI-Assistance-Misuse-in-Web-Programming-Unfair-Benefits-and-Characteristics"><a href="#Plagiarism-and-AI-Assistance-Misuse-in-Web-Programming-Unfair-Benefits-and-Characteristics" class="headerlink" title="Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics"></a>Plagiarism and AI Assistance Misuse in Web Programming: Unfair Benefits and Characteristics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20104">http://arxiv.org/abs/2310.20104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Karnalim, Hapnes Toba, Meliana Christianti Johan, Erico Darmawan Handoyo, Yehezkiel David Setiawan, Josephine Alvina Luwia</li>
<li>for: This paper aims to address the issues of plagiarism and misuse of AI assistance in web programming education.</li>
<li>methods: The authors conducted a controlled experiment to compare student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT).</li>
<li>results: The study found that students who engaged in misconduct (plagiarism or AI assistance) received comparable test marks with less completion time, while AI-assisted submissions were more complex and less readable. Students believed that AI assistance could be useful with proper acknowledgment, but were not convinced of its readability and correctness.<details>
<summary>Abstract</summary>
In programming education, plagiarism and misuse of artificial intelligence (AI) assistance are emerging issues. However, not many relevant studies are focused on web programming. We plan to develop automated tools to help instructors identify both misconducts. To fully understand the issues, we conducted a controlled experiment to observe the unfair benefits and the characteristics. We compared student performance in completing web programming tasks independently, with a submission to plagiarize, and with the help of AI assistance (ChatGPT). Our study shows that students who are involved in such misconducts get comparable test marks with less completion time. Plagiarized submissions are similar to the independent ones except in trivial aspects such as color and identifier names. AI-assisted submissions are more complex, making them less readable. Students believe AI assistance could be useful given proper acknowledgment of the use, although they are not convinced with readability and correctness of the solutions.
</details>
<details>
<summary>摘要</summary>
在编程教育中， copying 和人工智能（AI）帮助的不当使用是emerging issues。然而，不多的相关研究集中在网络编程。我们计划开发自动化工具来帮助教师识别这些不当行为。为了全面理解问题，我们进行了一个控制实验，观察到不正当的利益和特征。我们比较了学生完成网络编程任务的独立完成、抄袭提交和AI帮助（ChatGPT）的性能。我们的研究显示，参与抄袭和AI帮助的学生的测试marks相当，但完成时间较短。抄袭提交和独立完成的作品相似，只有一些 superficies 的不同，如颜色和标识符名称。AI帮助的作品更复杂，使其难以阅读。学生认为AI帮助可以是有用的，但不确定其可读性和正确性。
</details></li>
</ul>
<hr>
<h2 id="Data-Market-Design-through-Deep-Learning"><a href="#Data-Market-Design-through-Deep-Learning" class="headerlink" title="Data Market Design through Deep Learning"></a>Data Market Design through Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20096">http://arxiv.org/abs/2310.20096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Sai Srivatsa Ravindranath, Yanchen Jiang, David C. Parkes</li>
<li>For: The paper is written to explore the use of deep learning for the design of revenue-optimal data markets, with the goal of expanding the frontiers of what can be understood and achieved in this area.* Methods: The paper uses deep learning techniques to learn signaling schemes for data market design, rather than allocation rules, and handles obedience constraints that arise from modeling the downstream actions of buyers.* Results: The paper demonstrates that the new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.<details>
<summary>Abstract</summary>
The $\textit{data market design}$ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price [Bergemann et al., 2018]. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others [Bonatti et al., 2022]. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design [D\"utting et al., 2023], we must learn signaling schemes rather than allocation rules and handle $\textit{obedience constraints}$ $-$ these arising from modeling the downstream actions of buyers $-$ in addition to incentive constraints on bids. Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.
</details>
<details>
<summary>摘要</summary>
“数据市场设计”问题是经济理论中寻找一组信号协议（统计实验），以最大化信息卖家所得到的预期收益，其中每个实验 revelas一部分卖家知道的信息，并有相应的价格。每个买家在世界环境中做出决策，他们对某些实验的信息所对的价值取决于他们的先前知识和不同结果的价值。在多个买家的情况下，买家对某个实验的预期价值也可能受到他们向其他人销售的信息的影响。我们介绍了使用深度学习设计数据市场的应用，以扩展我们所能理解和实现的前iers。相比于早期的深度学习卖场设计（D\"utting et al., 2023），我们需要学习信号协议而不是分配规则，并处理“服从约束”（来自下游行为的模型），以及奖励竞拍 constraint。我们的实验表明，这新的深度学习框架可以准确地复制所有已知的理论解决方案，扩展到更复杂的设定，并用于确定数据市场的优化设计和提出新的设计 conjectures。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Neural-Language-Models-as-Cognitive-Models-of-Language-Acquisition"><a href="#Evaluating-Neural-Language-Models-as-Cognitive-Models-of-Language-Acquisition" class="headerlink" title="Evaluating Neural Language Models as Cognitive Models of Language Acquisition"></a>Evaluating Neural Language Models as Cognitive Models of Language Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20093">http://arxiv.org/abs/2310.20093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Héctor Javier Vázquez Martínez, Annika Lea Heuser, Charles Yang, Jordan Kodner</li>
<li>for: This paper argues that current benchmarks for evaluating the syntactic capacities of neural language models (LMs) may not be sufficient, and suggests using alternative datasets that are more representative of human language use.</li>
<li>methods: The paper uses small-scale data modeling child language acquisition to evaluate the performance of LMs, and compares their results to those of simple baseline models.</li>
<li>results: The paper finds that LMs can be readily matched by simple baseline models, and that they evaluate sentences in a way inconsistent with human language users. Additionally, the paper suggests that using carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers may be more effective in evaluating the syntactic capacities of LMs.<details>
<summary>Abstract</summary>
The success of neural language models (LMs) on many technological tasks has brought about their potential relevance as scientific theories of language despite some clear differences between LM training and child language acquisition. In this paper we argue that some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that the template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with human language users. We conclude with suggestions for better connecting LMs with the empirical study of child language acquisition.
</details>
<details>
<summary>摘要</summary>
neural network language models (LMs) 的成功在多种技术任务上，使其潜在地成为语言科学的理论，尽管LM训练和儿童语言学习有一些明显的不同。在这篇论文中，我们 argueThat some of the most prominent benchmarks for evaluating the syntactic capacities of LMs may not be sufficiently rigorous. In particular, we show that template-based benchmarks lack the structural diversity commonly found in the theoretical and psychological studies of language. When trained on small-scale data modeling child language acquisition, the LMs can be readily matched by simple baseline models. We advocate for the use of the readily available, carefully curated datasets that have been evaluated for gradient acceptability by large pools of native speakers and are designed to probe the structural basis of grammar specifically. On one such dataset, the LI-Adger dataset, LMs evaluate sentences in a way inconsistent with human language users. We conclude with suggestions for better connecting LMs with the empirical study of child language acquisition.Note: I've used the simplified Chinese characters and grammar to make the text more accessible to a wider audience.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.AI_2023_10_31/" data-id="clogyj8vx00717crabd7b0onu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/cs.CV_2023_10_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-10-31
        
      </div>
    </a>
  
  
    <a href="/2023/10/31/cs.CL_2023_10_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-10-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
