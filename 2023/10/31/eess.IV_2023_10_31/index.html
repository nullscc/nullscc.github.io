
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-10-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A Two-Step Framework for Multi-Material Decomposition of Dual Energy Computed Tomography from Projection Domain paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00188 repo_url: None paper_authors: Di Xu, Qihui Ly">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-10-31">
<meta property="og:url" content="https://nullscc.github.io/2023/10/31/eess.IV_2023_10_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="A Two-Step Framework for Multi-Material Decomposition of Dual Energy Computed Tomography from Projection Domain paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00188 repo_url: None paper_authors: Di Xu, Qihui Ly">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-31T09:00:00.000Z">
<meta property="article:modified_time" content="2023-11-03T00:29:08.038Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/eess.IV_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T09:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-10-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Two-Step-Framework-for-Multi-Material-Decomposition-of-Dual-Energy-Computed-Tomography-from-Projection-Domain"><a href="#A-Two-Step-Framework-for-Multi-Material-Decomposition-of-Dual-Energy-Computed-Tomography-from-Projection-Domain" class="headerlink" title="A Two-Step Framework for Multi-Material Decomposition of Dual Energy Computed Tomography from Projection Domain"></a>A Two-Step Framework for Multi-Material Decomposition of Dual Energy Computed Tomography from Projection Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00188">http://arxiv.org/abs/2311.00188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Xu, Qihui Lyu, Dan Ruan, Ke Sheng</li>
<li>for: 这个研究旨在提高多物质分解（MMD）的精度，以便应用于诊断不同组织的胸部疾病。</li>
<li>methods: 这个研究使用了深度学习（DL）方法，并开发了一个便利性高的、非递回的架构（rFast-MMDNet），可以将raw投射资料处理成多物质分解。</li>
<li>results: 这个研究在一个2022年DL-Spectral-Challenge胸部phantomdataset上进行了评估，结果显示rFast-MMDNet可以实现高精度的多物质分解，并且比传统方法更高效。<details>
<summary>Abstract</summary>
Dual-energy computed tomography (DECT) utilizes separate X-ray energy spectra to improve multi-material decomposition (MMD) for various diagnostic applications. However accurate decomposing more than two types of material remains challenging using conventional methods. Deep learning (DL) methods have shown promise to improve the MMD performance, but typical approaches of conducing DL-MMD in the image domain fail to fully utilize projection information or under iterative setup are computationally inefficient in both training and prediction. In this work, we present a clinical-applicable MMD (>2) framework rFast-MMDNet, operating with raw projection data in non-recursive setup, for breast tissue differentiation. rFast-MMDNet is a two-stage algorithm, including stage-one SinoNet to perform dual energy projection decomposition on tissue sinograms and stage-two FBP-DenoiseNet to perform domain adaptation and image post-processing. rFast-MMDNet was tested on a 2022 DL-Spectral-Challenge breast phantom dataset. The two stages of rFast-MMDNet were evaluated separately and then compared with four noniterative reference methods including a direct inversion method (AA-MMD), an image domain DL method (ID-UNet), AA-MMD/ID-UNet + DenoiseNet and a sinogram domain DL method (Triple-CBCT). Our results show that models trained from information stored in DE transmission domain can yield high-fidelity decomposition of the adipose, calcification, and fibroglandular materials with averaged RMSE, MAE, negative PSNR, and SSIM of 0.004+/-~0, 0.001+/-~0, -45.027+/-~0.542, and 0.002+/-~0 benchmarking to the ground truth, respectively. Training of entire rFast-MMDNet on a 4xRTX A6000 GPU cluster took a day with inference time <1s. All DL methods generally led to more accurate MMD than AA-MMD. rFast-MMDNet outperformed Triple-CBCT, but both are superior to the image-domain based methods.
</details>
<details>
<summary>摘要</summary>
dual-energy computed tomography (DECT) 使用不同的X射线能谱spectrum来提高多物质分解(MMD)的诊断应用。然而，使用传统方法分解更多 чем两种材料仍然是挑战。深度学习(DL)方法已经表现出提高MMD性能的潜力，但通常在图像域中进行DL-MMD会不全利用投影信息或者在迭代设置下 computationally inefficient。在这种工作中，我们提出了一个临床可行的MMD（>2）框架rFast-MMDNet，在原始投影数据上运行，不需迭代设置。rFast-MMDNet是一个两stage算法，包括第一个SinoNet，用于在组织射agram上进行双能量投影分解，以及第二个FBP-DenoiseNet，用于适应频率频谱和图像后处理。rFast-MMDNet在2022DL-Spectral-Challenge乳腺phantom数据集上进行测试。两个stage的rFast-MMDNet被分别评估，然后与四种非迭代参照方法进行比较，包括直接逆解方法(AA-MMD)、图像域DL方法(ID-UNet)、AA-MMD/ID-UNet + DenoiseNet和投影域DL方法(Triple-CBCT)。我们的结果表明，通过使用DE传输域中存储的信息，可以实现高精度的脂肪、钙化和肿瘤材料分解，均值RMSE、MAE、负PSNR和SSIM分别为0.004±~0、0.001±~0、-45.027±~0.542和0.002±~0，与参考值相对。训练整个rFast-MMDNet在4xRTX A6000 GPU集群上需要一天时间，推理时间 <1s。所有DL方法都比AA-MMD更准确，而rFast-MMDNet超过Triple-CBCT，但两者都比图像域基本方法更好。
</details></li>
</ul>
<hr>
<h2 id="Fast-multicolour-optical-sectioning-over-extended-fields-of-view-by-combining-interferometric-SIM-with-machine-learning"><a href="#Fast-multicolour-optical-sectioning-over-extended-fields-of-view-by-combining-interferometric-SIM-with-machine-learning" class="headerlink" title="Fast, multicolour optical sectioning over extended fields of view by combining interferometric SIM with machine learning"></a>Fast, multicolour optical sectioning over extended fields of view by combining interferometric SIM with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00089">http://arxiv.org/abs/2311.00089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward N. Ward, Rebecca M. McClelland, Jacob R. Lamb, Roger Rubio-Sánchez, Charles N. Christensen, Bismoy Mazumder, Sofia Kapsiani, Luca Mascheroni, Lorenzo Di Michele, Gabriele S. Kaminski Schierle, Clemens F. Kaminski</li>
<li>for: 能够高速、高对比度扫描大面积样品</li>
<li>methods: 结合多色 интерferometric模式生成和机器学习处理，实现高对比度、实时重建图像数据</li>
<li>results: 在silico validate和实验中，可以在44 x 44 $\mu m^2$ 场景中实时扫描Fixed和live生物细胞，并且可以在37 Hz的速度下获得高对比度的图像数据<details>
<summary>Abstract</summary>
Structured illumination can reject out-of-focus signal from a sample, enabling high-speed and high-contrast imaging over large areas with widefield detection optics. Currently, this optical-sectioning technique is limited by image reconstruction artefacts and the need for sequential imaging of multiple colour channels. We combine multicolour interferometric pattern generation with machine-learning processing, permitting high-contrast, real-time reconstruction of image data. The method is insensitive to background noise and unevenly phase-stepped illumination patterns. We validate the method in silico and demonstrate its application on diverse specimens, ranging from fixed and live biological cells to synthetic biosystems, imaging at up to 37 Hz across a 44 x 44 $\mu m^2$ field of view.
</details>
<details>
<summary>摘要</summary>
结构化照明可以拒绝样品上的不对焦信号，启用高速高对比度的成像，覆盖大面积的广角探测仪器。目前，这种光学分割技术受到图像重建 artifacts 和多色通道顺序扫描的限制。我们将多色干扰 Pattern 生成与机器学习处理相结合，实现高对比度、实时重建的图像数据。该方法具有背景噪声和不均步骤照明模式的敏感性。我们在软件中验证了该方法，并在多种样品上进行了应用，包括固定和活的生物细胞、synthetic biosystems，成像频率达到37Hz，Field of view 为44 x 44 μm^2。
</details></li>
</ul>
<hr>
<h2 id="UAV-Immersive-Video-Streaming-A-Comprehensive-Survey-Benchmarking-and-Open-Challenges"><a href="#UAV-Immersive-Video-Streaming-A-Comprehensive-Survey-Benchmarking-and-Open-Challenges" class="headerlink" title="UAV Immersive Video Streaming: A Comprehensive Survey, Benchmarking, and Open Challenges"></a>UAV Immersive Video Streaming: A Comprehensive Survey, Benchmarking, and Open Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00082">http://arxiv.org/abs/2311.00082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohit K. Sharma, Chen-Feng Liu, Ibrahim Farhat, Nassim Sehad, Wassim Hamidouche, Merouane Debbah</li>
<li>for: 这篇论文主要是为了探讨无人机上的卷积摄像头捕捉360度视频的实时流处理技术，以提高虚拟现实和混合现实应用的视觉体验。</li>
<li>methods: 这篇论文使用了许多研究方法，包括360度视频编码、封装和流处理，以及对不同飞行条件下的无人机无线通信 channels 的研究。</li>
<li>results: 这篇论文的结果表明，使用hardware实现的HEVC编码器可以获得最佳的编码效率和复杂度平衡，而使用软件实现的AV1编码器可以获得最高的编码效率。此外，作者还提供了一个实际的5G无线网络控制的无人机360度视频流处理测试环境。<details>
<summary>Abstract</summary>
Over the past decade, the utilization of UAVs has witnessed significant growth, owing to their agility, rapid deployment, and maneuverability. In particular, the use of UAV-mounted 360-degree cameras to capture omnidirectional videos has enabled truly immersive viewing experiences with up to 6DoF. However, achieving this immersive experience necessitates encoding omnidirectional videos in high resolution, leading to increased bitrates. Consequently, new challenges arise in terms of latency, throughput, perceived quality, and energy consumption for real-time streaming of such content. This paper presents a comprehensive survey of research efforts in UAV-based immersive video streaming, benchmarks popular video encoding schemes, and identifies open research challenges. Initially, we review the literature on 360-degree video coding, packaging, and streaming, with a particular focus on standardization efforts to ensure interoperability of immersive video streaming devices and services. Subsequently, we provide a comprehensive review of research efforts focused on optimizing video streaming for timevarying UAV wireless channels. Additionally, we introduce a high resolution 360-degree video dataset captured from UAVs under different flying conditions. This dataset facilitates the evaluation of complexity and coding efficiency of software and hardware video encoders based on popular video coding standards and formats, including AVC/H.264, HEVC/H.265, VVC/H.266, VP9, and AV1. Our results demonstrate that HEVC achieves the best trade-off between coding efficiency and complexity through its hardware implementation, while AV1 format excels in coding efficiency through its software implementation, specifically using the libsvt-av1 encoder. Furthermore, we present a real testbed showcasing 360-degree video streaming over a UAV, enabling remote control of the drone via a 5G cellular network.
</details>
<details>
<summary>摘要</summary>
过去一个 décennial，UAV的应用场景增长很 significativement，归功于它们的机敏、快速部署和操纵性。特别是通过UAV上安装的360度摄像头捕捉全景视频，实现了真实的 immerse 视频经验，具有6DoF。然而，实现这种 immerse 经验需要高分辨率编码全景视频，从而导致增加的比特率。因此，实时串流这些内容时出现了新的挑战，包括延迟、吞吐量、视觉质量和能量消耗。本文提供了UAV基于全景视频流媒体的全面评估，比较各种视频编码方案，并提出了未来研究的开放挑战。首先，我们查看了360度视频编码、封装和流媒体的相关研究，尤其是标准化努力，以确保全景视频流媒体设备和服务的互操作性。接着，我们提供了关于在时变UAV无线通信频道上优化视频流媒体的全面评估。此外，我们还提供了高分辨率全景视频数据集， captured from UAVs under different flying conditions。这个数据集可以评估不同 виде码器和格式的编码效率和复杂度，包括AVC/H.264、HEVC/H.265、VVC/H.266、VP9和AV1。我们的结果表明，HEVC在硬件实现方式下实现了最佳的平衡，而AV1格式在软件实现方式下在编码效率方面表现出色。此外，我们还提供了一个真实的UAV全景视频串流实验室，使得通过5G移动网络控制无人机。
</details></li>
</ul>
<hr>
<h2 id="Harmonization-enriched-domain-adaptation-with-light-fine-tuning-for-multiple-sclerosis-lesion-segmentation"><a href="#Harmonization-enriched-domain-adaptation-with-light-fine-tuning-for-multiple-sclerosis-lesion-segmentation" class="headerlink" title="Harmonization-enriched domain adaptation with light fine-tuning for multiple sclerosis lesion segmentation"></a>Harmonization-enriched domain adaptation with light fine-tuning for multiple sclerosis lesion segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20586">http://arxiv.org/abs/2310.20586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinwei Zhang, Lianrui Zuo, Blake E. Dewey, Samuel W. Remedios, Savannah P. Hays, Dzung L. Pham, Jerry L. Prince, Aaron Carass</li>
<li>for: This paper aims to improve the performance of deep learning algorithms in segmenting multiple sclerosis (MS) lesions using magnetic resonance (MR) images, specifically by addressing the challenge of domain generalization errors.</li>
<li>methods: The proposed approach integrates one-shot adaptation data with harmonized training data that incorporates labels, using a process called “contrast harmonization” in MRI. This approach involves synthesizing new training data with a contrast akin to that of the test domain.</li>
<li>results: The amalgamation of one-shot adaptation data with harmonized training data surpasses the performance of utilizing either data source in isolation. Additionally, domain adaptation using exclusively harmonized training data achieved comparable or even superior performance compared to one-shot adaptation, with minimal fine-tuning required (ranging from 2 to 5 epochs for convergence).<details>
<summary>Abstract</summary>
Deep learning algorithms utilizing magnetic resonance (MR) images have demonstrated cutting-edge proficiency in autonomously segmenting multiple sclerosis (MS) lesions. Despite their achievements, these algorithms may struggle to extend their performance across various sites or scanners, leading to domain generalization errors. While few-shot or one-shot domain adaptation emerges as a potential solution to mitigate generalization errors, its efficacy might be hindered by the scarcity of labeled data in the target domain. This paper seeks to tackle this challenge by integrating one-shot adaptation data with harmonized training data that incorporates labels. Our approach involves synthesizing new training data with a contrast akin to that of the test domain, a process we refer to as "contrast harmonization" in MRI. Our experiments illustrate that the amalgamation of one-shot adaptation data with harmonized training data surpasses the performance of utilizing either data source in isolation. Notably, domain adaptation using exclusively harmonized training data achieved comparable or even superior performance compared to one-shot adaptation. Moreover, all adaptations required only minimal fine-tuning, ranging from 2 to 5 epochs for convergence.
</details>
<details>
<summary>摘要</summary>
深度学习算法利用核磁共振图像（MR图像）自动 segmenting多发性脑脊细胞病（MS）斑点， despite their achievements， these algorithms may struggle to extend their performance across various sites or scanners, leading to domain generalization errors. While few-shot or one-shot domain adaptation emerges as a potential solution to mitigate generalization errors, its efficacy might be hindered by the scarcity of labeled data in the target domain. This paper seeks to tackle this challenge by integrating one-shot adaptation data with harmonized training data that incorporates labels. Our approach involves synthesizing new training data with a contrast akin to that of the test domain, a process we refer to as "contrast harmonization" in MRI. Our experiments illustrate that the amalgamation of one-shot adaptation data with harmonized training data surpasses the performance of utilizing either data source in isolation. Notably, domain adaptation using exclusively harmonized training data achieved comparable or even superior performance compared to one-shot adaptation. Moreover, all adaptations required only minimal fine-tuning, ranging from 2 to 5 epochs for convergence.
</details></li>
</ul>
<hr>
<h2 id="C-Silicon-based-metasurfaces-for-aperture-robust-spectrometer-imaging-with-angle-integration"><a href="#C-Silicon-based-metasurfaces-for-aperture-robust-spectrometer-imaging-with-angle-integration" class="headerlink" title="C-Silicon-based metasurfaces for aperture-robust spectrometer&#x2F;imaging with angle integration"></a>C-Silicon-based metasurfaces for aperture-robust spectrometer&#x2F;imaging with angle integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20289">http://arxiv.org/abs/2310.20289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weizhu Xu, Qingbin Fan, Peicheng Lin, Jiarong Wang, Hao Hu, Tao Yue, Xuemei Hu, Ting Xu</li>
<li>for:  This paper aims to develop a compact silicon metasurfaces-based spectrometer&#x2F;camera that can achieve high spectral consistency and angular stability over a wide working bandwidth.</li>
<li>methods: The proposed method uses spectrally engineered filtering and angle integration to achieve a robust spectral response and high accuracy in reconstructing hyperspectral signals.</li>
<li>results: The experimental results show that the proposed method can maintain the spectral consistency from F&#x2F;1.8 to F&#x2F;4 (7° to 16° angle of incident light) and accurately reconstruct the incident hyperspectral signal with a fidelity exceeding 99%. Additionally, a spectral imaging system with 400x400 pixels was established, and the accurate reconstructed hyperspectral image demonstrates the potential of the proposed aperture-robust spectrometer to be extended as a high-resolution broadband hyperspectral camera.<details>
<summary>Abstract</summary>
Compared with conventional grating-based spectrometers, reconstructive spectrometers based on spectrally engineered filtering have the advantage of miniaturization because of the less demand for dispersive optics and free propagation space. However, available reconstructive spectrometers fail to balance the performance on operational bandwidth, spectral diversity and angular stability. In this work, we proposed a compact silicon metasurfaces based spectrometer/camera. After angle integration, the spectral response of the system is robust to angle/aperture within a wide working bandwidth from 400nm to 800nm. It is experimentally demonstrated that the proposed method could maintain the spectral consistency from F/1.8 to F/4 (The corresponding angle of incident light ranges from 7{\deg} to 16{\deg}) and the incident hyperspectral signal could be accurately reconstructed with a fidelity exceeding 99%. Additionally, a spectral imaging system with 400x400 pixels is also established in this work. The accurate reconstructed hyperspectral image indicates that the proposed aperture-robust spectrometer has the potential to be extended as a high-resolution broadband hyperspectral camera.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/eess.IV_2023_10_31/" data-id="cloimipim0174s488bvv68k9l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/cs.LG_2023_10_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-10-31
        
      </div>
    </a>
  
  
    <a href="/2023/10/31/eess.SP_2023_10_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.SP - 2023-10-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
