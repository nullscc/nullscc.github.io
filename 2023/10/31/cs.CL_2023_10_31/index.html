
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-10-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="What’s In My Big Data? paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20707 repo_url: https:&#x2F;&#x2F;github.com&#x2F;Sfedfcv&#x2F;redesigned-pancake paper_authors: Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichan">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-10-31">
<meta property="og:url" content="https://nullscc.github.io/2023/10/31/cs.CL_2023_10_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="What’s In My Big Data? paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.20707 repo_url: https:&#x2F;&#x2F;github.com&#x2F;Sfedfcv&#x2F;redesigned-pancake paper_authors: Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichan">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-31T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T08:29:06.976Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.CL_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T11:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-10-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="What’s-In-My-Big-Data"><a href="#What’s-In-My-Big-Data" class="headerlink" title="What’s In My Big Data?"></a>What’s In My Big Data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20707">http://arxiv.org/abs/2310.20707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, Jesse Dodge</li>
<li>For: The paper aims to provide a comprehensive understanding of the content of large text corpora used to train language models, including general statistics, quality, social factors, and inclusion of evaluation data (contamination).* Methods: The paper proposes a platform called What’s In My Big Data? (WIMBD) that offers sixteen analyses to reveal and compare the contents of large text corpora, leveraging two basic capabilities - count and search - at scale.* Results: The paper applies WIMBD to ten different corpora used to train popular language models and uncovers several surprising findings, such as the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, about 50% of the documents in RedPajama and LAION-2B-en are duplicates.<details>
<summary>Abstract</summary>
Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them: github.com/allenai/wimbd.
</details>
<details>
<summary>摘要</summary>
大型文本 corpus 是语言模型的基础。然而，我们对这些 corpus 的内容有限的了解，包括一般统计、质量、社会因素和评估数据（污染）。在这项工作中，我们提出了 What's In My Big Data? (WIMBD) 平台和十六种分析，可以让我们对大型文本 corpus 的内容进行揭示和比较。WIMBD 基于计数和搜索的两种基本能力，可以在标准计算节点上处理 более于 35 terabytes 的数据。我们在十个不同的 corpus 中使用 WIMBD，包括 C4、The Pile 和 RedPajama。我们的分析发现了一些 previously undocumented 的发现，包括高度异常的重复、合成、低质量内容、个人隐私信息、恶意语言和评估数据污染。例如，我们发现了 RedPajama 和 LAION-2B-en 中的约 50% 的文档是重复的。此外，一些用于评估模型的数据集被污染了，包括 Winograd Schema Challenge 和 GLUE 和 SuperGLUE 中的一些部分。我们将 WIMBD 的代码和文件开源，以提供一个标准的评估集和鼓励更多的分析和透明度。可以在 GitHub 上找到我们的代码：github.com/allenai/wimbd。
</details></li>
</ul>
<hr>
<h2 id="Text-Transport-Toward-Learning-Causal-Effects-of-Natural-Language"><a href="#Text-Transport-Toward-Learning-Causal-Effects-of-Natural-Language" class="headerlink" title="Text-Transport: Toward Learning Causal Effects of Natural Language"></a>Text-Transport: Toward Learning Causal Effects of Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20697">http://arxiv.org/abs/2310.20697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/torylin/text-transport">https://github.com/torylin/text-transport</a></li>
<li>paper_authors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael</li>
<li>for: 这个论文是为了研究语言技术在实际场景中的影响，特别是语言变化对文本的影响。</li>
<li>methods: 这个论文提出了一种名为Text-Transport的方法，用于在自然语言下 estimating causal effects。这种方法可以在任何文本分布下进行计算，不需要强制假设。</li>
<li>results: 研究人员通过实验和分析，证明了Text-Transport的有效性和可靠性。此外，这种方法还用于研究社交媒体上的仇恨言论，发现了语言变化对文本的 causal effects 的差异。<details>
<summary>Abstract</summary>
As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Non-Compositionality-in-Sentiment-New-Data-and-Analyses"><a href="#Non-Compositionality-in-Sentiment-New-Data-and-Analyses" class="headerlink" title="Non-Compositionality in Sentiment: New Data and Analyses"></a>Non-Compositionality in Sentiment: New Data and Analyses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20656">http://arxiv.org/abs/2310.20656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vernadankers/noncompsst">https://github.com/vernadankers/noncompsst</a></li>
<li>paper_authors: Verna Dankers, Christopher G. Lucas</li>
<li>for: 本研究的目的是获取句子 sentiment 的非compositional 评分。</li>
<li>methods: 本研究提出了一种方法来获取句子 sentiment 的非compositional 评分，并创建了一个包含 259 个句子的评分资源（NonCompSST）。</li>
<li>results: 研究使用 NonCompSST 资源进行了一种计算模型的 sentiment 分析，并对结果进行了分析和评估。<details>
<summary>Abstract</summary>
When natural language phrases are combined, their meaning is often more than the sum of their parts. In the context of NLP tasks such as sentiment analysis, where the meaning of a phrase is its sentiment, that still applies. Many NLP studies on sentiment analysis, however, focus on the fact that sentiment computations are largely compositional. We, instead, set out to obtain non-compositionality ratings for phrases with respect to their sentiment. Our contributions are as follows: a) a methodology for obtaining those non-compositionality ratings, b) a resource of ratings for 259 phrases -- NonCompSST -- along with an analysis of that resource, and c) an evaluation of computational models for sentiment analysis using this new resource.
</details>
<details>
<summary>摘要</summary>
当自然语言短语组合时，它们的意思经常大于其部分。在NLG任务中，如情感分析，这个规则仍然适用。许多NLG研究强调情感计算是主要 Compositional，我们则决定获得不可组合性评分 для短语。我们的贡献如下：a) 一种方法获取非可组合性评分，b) 一个包含259个短语评分的资源——NonCompSST，以及对这个资源的分析，c) 使用这个新资源进行情感分析计算模型的评估。
</details></li>
</ul>
<hr>
<h2 id="Defining-a-New-NLP-Playground"><a href="#Defining-a-New-NLP-Playground" class="headerlink" title="Defining a New NLP Playground"></a>Defining a New NLP Playground</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20633">http://arxiv.org/abs/2310.20633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li, Xingyao Wang, Yi R. Fung, Charles Yu, Joel R. Tetreault, Eduard H. Hovy, Heng Ji</li>
<li>for: 这篇论文的目的是定义一个新的自然语言处理（NLP）游戏场，为PhD学生提供20多个硬件研究方向，以满足现有的理论分析、新和挑战性问题、学习 парадиг和跨学科应用等方面的需求。</li>
<li>methods: 本论文使用的方法包括现有的NLP模型和算法的析密分析、新的问题设定和挑战性研究、不同的学习 парадиг和跨学科应用等方面的研究。</li>
<li>results: 本论文的结果是提出了20多个PhD硬件研究方向，包括理论分析、新和挑战性问题、学习 парадиг和跨学科应用等方面的研究，以满足现有的NLP领域需求和未来发展的趋势。<details>
<summary>Abstract</summary>
The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field's 80-year history. This has resulted in concerns that the field will become homogenized and resource-intensive. The new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms, and interdisciplinary applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Unreasonable-Effectiveness-of-Random-Target-Embeddings-for-Continuous-Output-Neural-Machine-Translation"><a href="#The-Unreasonable-Effectiveness-of-Random-Target-Embeddings-for-Continuous-Output-Neural-Machine-Translation" class="headerlink" title="The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation"></a>The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20620">http://arxiv.org/abs/2310.20620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgeniia Tokarchuk, Vlad Niculae</li>
<li>for: 这个论文主要针对的是 neural machine translation (NMT) 的连续输出问题。</li>
<li>methods: 该论文使用了一种叫做 CoNMT (Continuous-output neural machine translation) 的方法，它将下一个词的预测问题转化为一个嵌入 Prediction 问题。</li>
<li>results: 研究发现，完全随机生成的嵌入可以超越劳动ious地预训练的嵌入，特别是在更大的数据集上。进一步的调查发现，这个意外效果尤其strongest  для rare words，这是因为这些嵌入的几何结构。<details>
<summary>Abstract</summary>
Continuous-output neural machine translation (CoNMT) replaces the discrete next-word prediction problem with an embedding prediction. The semantic structure of the target embedding space (i.e., closeness of related words) is intuitively believed to be crucial. We challenge this assumption and show that completely random output embeddings can outperform laboriously pretrained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings. We shed further light on this finding by designing a mixed strategy that combines random and pre-trained embeddings for different tokens.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Increasing-The-Performance-of-Cognitively-Inspired-Data-Efficient-Language-Models-via-Implicit-Structure-Building"><a href="#Increasing-The-Performance-of-Cognitively-Inspired-Data-Efficient-Language-Models-via-Implicit-Structure-Building" class="headerlink" title="Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building"></a>Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20589">http://arxiv.org/abs/2310.20589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Momen, David Arps, Laura Kallmeyer</li>
<li>for: 这个研究是为了提出一种数据效果语言模型预训练方法（Warstadt et al., 2023）。</li>
<li>methods: 这个研究使用了基于转换器的偏挥masked语言模型，并将无监督预测的层次句子结构纳入模型架构中。具体来说，这个研究使用了Structformer架构（Shen et al., 2021）和其变种。StructFormer模型在有限的预训练数据上进行了无监督语义推导，并且在一些任务上实现了性能提升。</li>
<li>results: 对于39个任务，我们的模型在某些任务上实现了明显的性能提升，尤其是在具有层次结构的任务上。然而，我们的模型并没有在所有任务上一直性能超过提供的RoBERTa基eline模型（提供者：shared task组织者）。<details>
<summary>Abstract</summary>
In this paper, we describe our submission to the BabyLM Challenge 2023 shared task on data-efficient language model (LM) pretraining (Warstadt et al., 2023). We train transformer-based masked language models that incorporate unsupervised predictions about hierarchical sentence structure into the model architecture. Concretely, we use the Structformer architecture (Shen et al., 2021) and variants thereof. StructFormer models have been shown to perform well on unsupervised syntactic induction based on limited pretraining data, and to yield performance improvements over a vanilla transformer architecture (Shen et al., 2021). Evaluation of our models on 39 tasks provided by the BabyLM challenge shows promising improvements of models that integrate a hierarchical bias into the architecture at some particular tasks, even though they fail to consistently outperform the RoBERTa baseline model provided by the shared task organizers on all tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了我们对2023年 BabyLM Challenge 共同任务中语言模型预训练（LM）的提交（Warstadt et al., 2023）。我们使用基于转换器的假设语言模型，并在模型建筑中包含不supervised的句子结构预测。具体来说，我们使用Structformer架构（Shen et al., 2021）和其变种。StructFormer模型在有限的预训练数据上进行无监督语法推导时表现良好，并在vanilla transformer架构中提供性能改进（Shen et al., 2021）。我们对 shared task提供的39个任务进行评估，发现在某些任务上， integrating hierarchical bias into the architecture 可以获得明显的改进，尽管它们在所有任务上不能 consistently outperform RoBERTa基线模型提供的（shared task organizers）。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Medical-Information-Retrieval-via-Knowledge-Graph-Embedding"><a href="#Zero-Shot-Medical-Information-Retrieval-via-Knowledge-Graph-Embedding" class="headerlink" title="Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding"></a>Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20588">http://arxiv.org/abs/2310.20588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqi Wang, Zeqiang Wang, Wei Wang, Qi Chen, Kaizhu Huang, Anh Nguyen, Suparna De</li>
<li>for: 本研究旨在提高医疗决策中的信息检索效率，适用于无法训练的医疗信息检索（MIR）领域。</li>
<li>methods: 本文提出了一种新的零容量医疗信息检索方法——MedFusionRank， combinates 预训练语言模型和统计方法，并解决了它们的局限性。</li>
<li>results: 实验表明，MedFusionRank在医疗数据集上的表现较 existing methods 出色，具有多种评价指标的批判性。 MedFusionRank 能够从短或单个查询中检索到相关信息，表现出色。<details>
<summary>Abstract</summary>
In the era of the Internet of Things (IoT), the retrieval of relevant medical information has become essential for efficient clinical decision-making. This paper introduces MedFusionRank, a novel approach to zero-shot medical information retrieval (MIR) that combines the strengths of pre-trained language models and statistical methods while addressing their limitations. The proposed approach leverages a pre-trained BERT-style model to extract compact yet informative keywords. These keywords are then enriched with domain knowledge by linking them to conceptual entities within a medical knowledge graph. Experimental evaluations on medical datasets demonstrate MedFusion Rank's superior performance over existing methods, with promising results with a variety of evaluation metrics. MedFusionRank demonstrates efficacy in retrieving relevant information, even from short or single-term queries.
</details>
<details>
<summary>摘要</summary>
在互联网时代，医疗信息检索已成为医疗决策中的关键。本文介绍MedFusionRank，一种新的零shot医疗信息检索方法，结合预训练语言模型和统计方法，并解决它们的局限性。该方法利用预训练BERT类型模型提取紧凑又有用的关键词。这些关键词然后通过与医学知识图连接，以增强医学知识。实验证明，MedFusionRank在医学数据集上的性能较高，并且与多种评价指标具有良好的结果。MedFusionRank能够从短或单个查询中检索相关信息，表现出色。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Word-Guessing-Games-to-Assess-the-Intelligence-of-Large-Language-Models"><a href="#Leveraging-Word-Guessing-Games-to-Assess-the-Intelligence-of-Large-Language-Models" class="headerlink" title="Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models"></a>Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20499">http://arxiv.org/abs/2310.20499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Liang, Zhiwei He, Jen-tes Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, Xing Wang</li>
<li>for: 评估 LLM 智能agent 的自动评估是开发高级 LLM 智能agent 的关键。现有的人工标注评估数据集，如 AlpacaEval，尝试了很多努力，但是这些方法是贵重的、耗时的，缺乏适应性。</li>
<li>methods: 我们提出了一种基于语言游戏“Who is Spy”的方法，使用word guessing game来评估 LLM 的智能性能。给定一个单词，LLM 需要用语言描述单词，并确定其身份（间谍或非间谍）基于自己和其他玩家的描述。理想情况下，高级Agent应该拥有准确地描述单词，并同时增加对 conservative 描述的混淆，以提高其在游戏中的参与度。为此，我们首先开发了 DEEP，用于评估 LLM 的表达和隐蔽能力。DEEP 需要 LLM 在攻击和保守两种模式下描述单词。然后，我们引入了 SpyGame，一个交互式多代理框架，用于评估 LLM 的智能能力。在这个框架中，目标 LLM 需要具备语言技能和战略思维，以提供更全面的评估 LLM 的人类智能能力和在复杂交通情况下的适应能力。</li>
<li>results: 我们进行了广泛的实验，收集了多种来源、领域和语言的单词，并使用我们提出的评估框架进行测试。实验结果表明，我们的 DEEP 和 SpyGame 可以有效评估不同 LLM 的能力，捕捉它们在新情况下的适应和战略通信能力。<details>
<summary>Abstract</summary>
The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy'', we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players' descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs' expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs' intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs' human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication.
</details>
<details>
<summary>摘要</summary>
自动评估LLM基于代理人智能是发展高级LLM的关键。虽然有很大努力投入到了人类标注评估数据库，如AlpacaEval，但现有技术都是费时费力，缺乏适应性。在本文中，我们提出使用话语游戏来评估LLM的智能表现。给定一个词，LLM被要求描述这个词并确定其身份（间谍或非间谍）基于其自己和其他玩家的描述。理想情况下，高级代理人应该具备精准描述给定词的能力，同时通过攻击性和保守性的描述来增强其参与度。为此，我们首先开发了DEEP来评估LLM的表达和伪装能力。DEEP需要LLM描述一个词在攻击和保守模式下。然后，我们引入了SpyGame，一个交互式多代理人框架，用于评估LLM的智能能力。SpyGame需要目标LLM具备语言技能和战略思维，以提供更全面的评估高级LLM的人类智能能力和复杂通信情况中的适应性。我们的评估框架非常容易实现。我们从多个来源、领域和语言收集了词汇，并使用我们的评估框架进行实验。广泛的实验结果表明，我们的DEEP和SpyGame有效地评估了不同LLM的能力，捕捉它们在新情况下适应和战略通信的能力。
</details></li>
</ul>
<hr>
<h2 id="Multi-User-MultiWOZ-Task-Oriented-Dialogues-among-Multiple-Users"><a href="#Multi-User-MultiWOZ-Task-Oriented-Dialogues-among-Multiple-Users" class="headerlink" title="Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users"></a>Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20479">http://arxiv.org/abs/2310.20479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohan Jo, Xinyan Zhao, Arijit Biswas, Nikoletta Basiou, Vincent Auvray, Nikolaos Malandrakis, Angeliki Metallinou, Alexandros Potamianos</li>
<li>for: 这个论文的目的是提出一种多用户多语言对话 dataset，以便开发多用户协作对话系统。</li>
<li>methods: 论文使用了将每个用户utterance替换为两个用户的小聊天，以保持对话的semantic和pragmatic consistency。</li>
<li>results: 研究发现，使用预测的 rewrite 可以在多用户对话中大幅提高对话状态追踪，而不需要修改现有的单用户对话系统。此外，这种方法还可以在未看过的领域中进行推理。<details>
<summary>Abstract</summary>
While most task-oriented dialogues assume conversations between the agent and one user at a time, dialogue systems are increasingly expected to communicate with multiple users simultaneously who make decisions collaboratively. To facilitate development of such systems, we release the Multi-User MultiWOZ dataset: task-oriented dialogues among two users and one agent. To collect this dataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat between two users that is semantically and pragmatically consistent with the original user utterance, thus resulting in the same dialogue state and system response. These dialogues reflect interesting dynamics of collaborative decision-making in task-oriented scenarios, e.g., social chatter and deliberation. Supported by this data, we propose the novel task of multi-user contextual query rewriting: to rewrite a task-oriented chat between two users as a concise task-oriented query that retains only task-relevant information and that is directly consumable by the dialogue system. We demonstrate that in multi-user dialogues, using predicted rewrites substantially improves dialogue state tracking without modifying existing dialogue systems that are trained for single-user dialogues. Further, this method surpasses training a medium-sized model directly on multi-user dialogues and generalizes to unseen domains.
</details>
<details>
<summary>摘要</summary>
而 більшість任务对话系统假设对话是在单一用户和对话系统之间进行的，但是对话系统将在多用户之间进行通信的需求在增加。为了推进这些系统的开发，我们发布了多用户多WOZ数据集：具有两个用户和一个对话系统之间的任务对话。为了获得这个数据集，我们将每个用户说话从MultiWOZ 2.2替换为两个用户之间的小聊天，这些聊天的 semantics 和 Pragmatics 与原始用户说话相同，因此维持了同一个对话状态和系统回应。这些对话反映了多用户协作做决策的 interessing dinamics，例如社交聊天和考虑。基于这样的数据，我们提出了一个新的任务：多用户Contextual Query Rewriting，即将多用户任务对话 rewrite 为一个简洁的任务对话，保留仅任务相关的信息，并且可以直接由对话系统处理。我们证明了，使用预测重写可以在多用户对话中优化对话状态追踪，不需要修改现有的对话系统，并且可以在未见预料的领域中获得更好的表现。
</details></li>
</ul>
<hr>
<h2 id="Representativeness-as-a-Forgotten-Lesson-for-Multilingual-and-Code-switched-Data-Collection-and-Preparation"><a href="#Representativeness-as-a-Forgotten-Lesson-for-Multilingual-and-Code-switched-Data-Collection-and-Preparation" class="headerlink" title="Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation"></a>Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20470">http://arxiv.org/abs/2310.20470</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Seza Doğruöz, Sunayana Sitaram, Zheng-Xin Yong</li>
<li>for: 本研究旨在探讨现有的CSW数据集（68）中语言对的偏袋现象，并分析数据采集和准备阶段的问题。</li>
<li>methods: 本研究采用了一种critical study的方法，通过分析68个CSW数据集中的语言对，探讨数据采集和准备阶段的问题。</li>
<li>results: 研究发现，现有的CSW数据集具有许多问题，包括英语占据绝大多数语言对，数据采集和准备阶段存在地域、社会经济和注册变化的问题，同时数据选择和筛选阶段的不清晰性也影响了CSW数据集的代表性。<details>
<summary>Abstract</summary>
Multilingualism is widespread around the world and code-switching (CSW) is a common practice among different language pairs/tuples across locations and regions. However, there is still not much progress in building successful CSW systems, despite the recent advances in Massive Multilingual Language Models (MMLMs). We investigate the reasons behind this setback through a critical study about the existing CSW data sets (68) across language pairs in terms of the collection and preparation (e.g. transcription and annotation) stages. This in-depth analysis reveals that \textbf{a)} most CSW data involves English ignoring other language pairs/tuples \textbf{b)} there are flaws in terms of representativeness in data collection and preparation stages due to ignoring the location based, socio-demographic and register variation in CSW. In addition, lack of clarity on the data selection and filtering stages shadow the representativeness of CSW data sets. We conclude by providing a short check-list to improve the representativeness for forthcoming studies involving CSW data collection and preparation.
</details>
<details>
<summary>摘要</summary>
多语种现象广泛存在全球，代码转换（CSW）是不同语言对的常见做法。然而，建立成功的CSW系统还很难，尽管最近的质量语言模型（MMLM）有了 significativemental advances。我们通过对现有CSW数据集（68）的抽查和分析，探讨这一退化的原因。我们发现：a）大多数CSW数据集中英语占主导地位，其他语言对不受到充分考虑。b）数据收集和预处理阶段存在 represetativeness 问题，这些问题包括：①  ignore 地点基础、社会经济和注册变化。② 数据选择和筛选阶段的不清晰性，使CSW数据集的表现性受到影响。我们 conclude 这些问题的存在，并提供一份简短的检查列表，以提高CSW数据集的表现性。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Deep-Understanding-of-Multilingual-End-to-End-Speech-Translation"><a href="#Towards-a-Deep-Understanding-of-Multilingual-End-to-End-Speech-Translation" class="headerlink" title="Towards a Deep Understanding of Multilingual End-to-End Speech Translation"></a>Towards a Deep Understanding of Multilingual End-to-End Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20456">http://arxiv.org/abs/2310.20456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Sun, Xiaohu Zhao, Yikun Lei, Shaolin Zhu, Deyi Xiong</li>
<li>for: 这个论文旨在分析一种多语言端到端语音翻译模型在22种语言上学习的表示。</li>
<li>methods: 这个论文使用了Singular Value Canonical Correlation Analysis（SVCCA）来分析这种多语言端到端语音翻译模型中的表示相似性。</li>
<li>results: 这个研究发现了以下三点：（I）在多语言端到端语音翻译中，语言相似性在数据有限时失效。（II）在不受限制的数据量和相对well-aligned的音频文本数据的情况下，提高encoder表示可以提高翻译质量，超过双语翻译。（III）多语言端到端语音翻译的encoder表示可以更好地预测语言 typology 中的音韵特征。这些发现可能提供一种更有效的方法来提高多语言端到端语音翻译的效果。<details>
<summary>Abstract</summary>
In this paper, we employ Singular Value Canonical Correlation Analysis (SVCCA) to analyze representations learnt in a multilingual end-to-end speech translation model trained over 22 languages. SVCCA enables us to estimate representational similarity across languages and layers, enhancing our understanding of the functionality of multilingual speech translation and its potential connection to multilingual neural machine translation. The multilingual speech translation model is trained on the CoVoST 2 dataset in all possible directions, and we utilize LASER to extract parallel bitext data for SVCCA analysis. We derive three major findings from our analysis: (I) Linguistic similarity loses its efficacy in multilingual speech translation when the training data for a specific language is limited. (II) Enhanced encoder representations and well-aligned audio-text data significantly improve translation quality, surpassing the bilingual counterparts when the training data is not compromised. (III) The encoder representations of multilingual speech translation demonstrate superior performance in predicting phonetic features in linguistic typology prediction. With these findings, we propose that releasing the constraint of limited data for low-resource languages and subsequently combining them with linguistically related high-resource languages could offer a more effective approach for multilingual end-to-end speech translation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们使用协值幂积分析（SVCCA）来分析在多语言端到端语音翻译模型中学习的表示。SVCCA使我们能够估计语言之间的表示相似性和层次结构，从而深入理解多语言语音翻译的功能和可能与多语言神经机器翻译的联系。我们使用CoVoST 2 dataset进行所有可能的方向训练，并使用LASER来提取平行文本数据 для SVCCA分析。我们从分析中得出三个主要发现：（I）在多语言语音翻译中，语言相似性的效果随着语言训练数据的减少而逐渐消失。（II）在训练数据不受限制的情况下，增强的encoder表示和对齐的音频文本数据可以提高翻译质量，超过双语翻译。（III）多语言语音翻译的encoder表示在语言类型预测中表现出色，可以预测语言的phonetic特征。基于这些发现，我们提议在严格控制数据量的情况下，先 liberate低资源语言的数据限制，然后将其与语言相似性高的高资源语言相结合，可以实现更有效的多语言端到端语音翻译。
</details></li>
</ul>
<hr>
<h2 id="The-SourceData-NLP-dataset-integrating-curation-into-scientific-publishing-for-training-large-language-models"><a href="#The-SourceData-NLP-dataset-integrating-curation-into-scientific-publishing-for-training-large-language-models" class="headerlink" title="The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models"></a>The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20440">http://arxiv.org/abs/2310.20440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Abreu-Vicente, Hannah Sonntag, Thomas Eidens, Thomas Lemberger</li>
<li>For: 本研究旨在 automatizing knowledge extraction from vast amounts of biomedical literature and preprints, using Natural Language Processing (NLP) techniques.* Methods: 本研究使用 Named-Entity Recognition (NER) 和 Named-Entity Linking (NEL) 技术，以及 context-dependent semantic interpretation，提取生物医学文献中的结构化信息和关键概念。* Results: 研究提供了 SourceData-NLP 数据集，包含生物医学文献中8种类型的生物实体（小分子、蛋白质、细胞组成物、细胞类型、组织、生物体、疾病）的标注，以及这些实体在实验设计中的角色和实验方法的标注。 SourceData-NLP 数据集包含了18,689张图文件中的620,000个生物实体标注，来自3,223篇生物医学文献。研究者还引入了一种新的语义任务，以测试模型是否能够理解实体是否为控制 intervención 的目标或测量的对象。<details>
<summary>Abstract</summary>
Introduction: The scientific publishing landscape is expanding rapidly, creating challenges for researchers to stay up-to-date with the evolution of the literature. Natural Language Processing (NLP) has emerged as a potent approach to automating knowledge extraction from this vast amount of publications and preprints. Tasks such as Named-Entity Recognition (NER) and Named-Entity Linking (NEL), in conjunction with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.   Results: We present the SourceData-NLP dataset produced through the routine curation of papers during the publication process. A unique feature of this dataset is its emphasis on the annotation of bioentities in figure legends. We annotate eight classes of biomedical entities (small molecules, gene products, subcellular components, cell lines, cell types, tissues, organisms, and diseases), their role in the experimental design, and the nature of the experimental method as an additional class. SourceData-NLP contains more than 620,000 annotated biomedical entities, curated from 18,689 figures in 3,223 papers in molecular and cell biology. We illustrate the dataset's usefulness by assessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned on the SourceData-NLP dataset for NER. We also introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.   Conclusions: SourceData-NLP's scale highlights the value of integrating curation into publishing. Models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.
</details>
<details>
<summary>摘要</summary>
引言：科学出版物的领域正在急速扩展， posing challenges for researchers to keep up with the evolution of the literature.自然语言处理（NLP）已成为自动提取知识的有力手段之一。任务如名称实体识别（NER）和名称实体连接（NEL）， along with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.结果：我们介绍了 SourceData-NLP 数据集，通过出版过程中的常规筹编来生成。这个数据集的特点是强调在图文中标注生物实体。我们标注了8类生物实体（小分子、蛋白质、细胞组成部分、细胞系列、细胞类型、组织、生物体和疾病），它们在实验设计中的角色和实验方法的性质。SourceData-NLP 包含了620,000多个标注的生物实体，从18,689个图中筹编于3,223篇分子和细胞生物研究。我们表明了 SourceData-NLP 数据集的用途，通过评估 BioLinkBERT 和 PubmedBERT，两种基于 transformers 的模型，在 SourceData-NLP 数据集上进行 NER 任务的 fine-tuning。我们还介绍了一种新的上下文依赖的 semantic 任务，可以推断实体是否为控制 intervención 的目标或测量的对象。结论：SourceData-NLP 的规模 highlights the value of integrating curation into publishing.models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.
</details></li>
</ul>
<hr>
<h2 id="FollowBench-A-Multi-level-Fine-grained-Constraints-Following-Benchmark-for-Large-Language-Models"><a href="#FollowBench-A-Multi-level-Fine-grained-Constraints-Following-Benchmark-for-Large-Language-Models" class="headerlink" title="FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models"></a>FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20410">http://arxiv.org/abs/2310.20410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang</li>
<li>For: This paper proposes a benchmark for evaluating the ability of large language models (LLMs) to follow instructions, specifically focusing on fine-grained constraints.* Methods: The proposed benchmark, called FollowBench, includes five types of fine-grained constraints (content, scenario, style, format, and example) and a multi-level mechanism to incrementally add constraints to the initial instruction.* Results: The authors evaluate nine popular LLMs on FollowBench and find that they have weaknesses in instruction following, particularly in handling challenging semantic constraints.Here are the three points in Simplified Chinese text:* For: 这篇论文提出了一个评估大语言模型（LLMs）遵循指令的benchmark，具体来说是关注细化的约束。* Methods: 该benchmark被称为FollowBench，包括五种细化的约束类型（内容、情况、式样、格式和示例），并使用多级机制来逐步添加约束到初始指令中。* Results: 作者通过对九种popular LLMs进行评估，发现它们在遵循指令方面存在弱点，尤其是在处理复杂的semantic约束方面。<details>
<summary>Abstract</summary>
The ability to follow instructions is crucial to Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating superficial response quality, which does not necessarily indicate instruction-following capability. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Scenario, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each level. To evaluate whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint evolution paths to handle challenging semantic constraints. By evaluating nine closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>大型语言模型（LLM）在各种实际应用中的能力是关键。现有的标准 benchmark primarily focuses on评估表面级别的回答质量，而不一定能够评估 LLM 是否能够遵循指令。为了填补这个研究漏洞，在这篇论文中，我们提出了 FollowBench，一个多级细化要求 Following Benchmark for LLMs。FollowBench 包括五种不同类型（即内容、场景、风格、格式和示例）的细化要求。为了准确地评估 LLMs 是否遵循每个约束，我们提出了一种多级机制，逐级添加一个单独的约束到初始指令中。为了评估 LLMs 的输出是否满足每个约束，我们提出了使用约束演化路径来挑战性的约束。通过评估 nine 种关闭源和开源的受欢迎 LLMs 在 FollowBench 上，我们指出了 LLMs 在遵循指令方面的弱点，并指向未来研究的可能性。数据和代码都可以在 https://github.com/YJiangcm/FollowBench 上获取。
</details></li>
</ul>
<hr>
<h2 id="AMERICANO-Argument-Generation-with-Discourse-driven-Decomposition-and-Agent-Interaction"><a href="#AMERICANO-Argument-Generation-with-Discourse-driven-Decomposition-and-Agent-Interaction" class="headerlink" title="AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction"></a>AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20352">http://arxiv.org/abs/2310.20352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Hu, Hou Pong Chan, Yu Yin</li>
<li>for: 这篇研究旨在提出一个新的对话式构思框架，用于对话式自然语言处理中的论证生成。</li>
<li>methods: 这篇研究使用了一个新的框架，名为Americano，它使用了人工智能代理人互动来生成论证。这个框架分为两个阶段：首先生成论证组件，然后使用这些组件来生成最终的论证。此外，这篇研究还引入了一个实验性的写作评估模组，以提高左侧生成的语言模型。</li>
<li>results: 这篇研究使用了一 subset 的 Reddit&#x2F;CMV 数据集，以评估这个新的对话式构思框架。结果显示，这个方法比以往的终端到终点和排序对话式构思方法更好，并且可以生成更加整体和说服力强的论证。<details>
<summary>Abstract</summary>
Argument generation is a challenging task in natural language processing, which requires rigorous reasoning and proper content organization. Inspired by recent chain-of-thought prompting that breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method outperforms both end-to-end and chain-of-thought prompting methods and can generate more coherent and persuasive arguments with diverse and rich contents.
</details>
<details>
<summary>摘要</summary>
Argument generation是自然语言处理中一项具有挑战性的任务，需要严格的逻辑推理和正确的内容组织。 drawing inspiration from recent chain-of-thought prompting, which breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method outperforms both end-to-end and chain-of-thought prompting methods and can generate more coherent and persuasive arguments with diverse and rich contents.Here is a word-for-word translation of the text into Simplified Chinese:Argument Generation是自然语言处理中一项具有挑战性的任务，需要严格的逻辑推理和正确的内容组织。 drawing inspiration from recent chain-of-thought prompting, which breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method outperforms both end-to-end and chain-of-thought prompting methods and can generate more coherent and persuasive arguments with diverse and rich contents.
</details></li>
</ul>
<hr>
<h2 id="Automatic-Generators-for-a-Family-of-Matrix-Multiplication-Routines-with-Apache-TVM"><a href="#Automatic-Generators-for-a-Family-of-Matrix-Multiplication-Routines-with-Apache-TVM" class="headerlink" title="Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM"></a>Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20347">http://arxiv.org/abs/2310.20347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Alaejos, Adrián Castelló, Pedro Alonso-Jordá, Francisco D. Igual, Héctor Martínez, Enrique S. Quintana-Ortí</li>
<li>for: 这 paper 的目的是使用 Apache TVM 开源框架自动生成高性能的矩阵乘法（GEMM）算法，以满足不同数据类型、处理器架构和矩阵操作形状的需求。</li>
<li>methods: 这 paper 使用的方法包括使用 Apache TVM 框架生成块分割的矩阵乘法算法，以及自动生成处理器特定的微内核。这些方法可以很好地灵活地调整和优化算法，以适应不同的数据类型和处理器架构。</li>
<li>results: 这 paper 的实验结果表明，使用 TVM-生成的块分割矩阵乘法算法和微内核可以提高端到端的性能，并且具有较小的内存占用。此外，这种方法还可以轻松地调整和优化算法，以适应不同的数据类型和处理器架构。<details>
<summary>Abstract</summary>
We explore the utilization of the Apache TVM open source framework to automatically generate a family of algorithms that follow the approach taken by popular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in order to obtain high-performance blocked formulations of the general matrix multiplication (GEMM). % In addition, we fully automatize the generation process, by also leveraging the Apache TVM framework to derive a complete variety of the processor-specific micro-kernels for GEMM. This is in contrast with the convention in high performance libraries, which hand-encode a single micro-kernel per architecture using Assembly code. % In global, the combination of our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves portability, maintainability and, globally, streamlines the software life cycle; 2)~provides high flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, yielding performance on a par (or even superior for specific matrix shapes) with that of hand-tuned libraries; and 3)~features a small memory footprint.
</details>
<details>
<summary>摘要</summary>
我们探讨使用Apache TVM开源框架自动生成一系列采用 популярLinear algebra库（如GotoBLAS2、BLIS和OpenBLAS）的方法来实现高性能块化表示法（GEMM）。此外，我们还充分 automatizethe生成过程，并使用Apache TVM框架来 derivcomplete的处理器特定微内核（micro-kernels） для GEMM。与传统高性能库不同，我们不手动编写Assembly代码来实现微内核。总的来说，我们的 TVM生成的块化算法和微内核可以：1. 提高可移植性、维护性和全面性，使软件生命周期更加简洁；2. 提供高度的灵活性，以便轻松地调整和优化解决方案，以适应不同的数据类型、处理器架构和矩阵操作形状，实现与手动优化库相当的性能（或者甚至超越）；3. 具有较小的内存占用。
</details></li>
</ul>
<hr>
<h2 id="InstructCoder-Empowering-Language-Models-for-Code-Editing"><a href="#InstructCoder-Empowering-Language-Models-for-Code-Editing" class="headerlink" title="InstructCoder: Empowering Language Models for Code Editing"></a>InstructCoder: Empowering Language Models for Code Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20329">http://arxiv.org/abs/2310.20329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qishenghu/CodeInstruct">https://github.com/qishenghu/CodeInstruct</a></li>
<li>paper_authors: Qisheng Hu, Kaixin Li, Xu Zhao, Yuxi Xie, Tiedong Liu, Hui Chen, Qizhe Xie, Junxian He</li>
<li>for: 本研究旨在探讨使用大型自然语言模型（LLM）自动编辑代码，以满足开发者日常几种实际任务的需求。</li>
<li>methods: 本研究使用 GitHub 提交作为种子任务，并通过 ChatGPT 进行反射编辑任务，逐渐扩展 InstructCoder 数据集，以适应更广泛的代码编辑任务。</li>
<li>results: 研究发现，通过对 InstructCoder 数据集进行 fine-tuning，可以使 open-source LLM  Correctly 编辑代码 Based on 用户指令大多数情况下，表明了训练指令 fine-tuning 可以提高代码编辑能力。<details>
<summary>Abstract</summary>
Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our experiments demonstrate that open-source LLMs fine-tuned on InstructCoder can edit code correctly based on users' instructions most of the time, exhibiting unprecedented code-editing performance levels. Such results suggest that proficient instruction-finetuning can lead to significant amelioration in code editing abilities. The dataset and the source code are available at https://github.com/qishenghu/CodeInstruct.
</details>
<details>
<summary>摘要</summary>
开发者日常需要完成一系列的实用任务，代码编辑是其中的一个重要部分。尽管代码编辑具有重要的实用性和现实意义，但是自动代码编辑还是深入探索的领域之一，其中一个原因是数据的缺乏。在这项工作中，我们explore了使用大型自然语言模型（LLM）来编辑代码，以满足用户的指令。我们引入了InstructCoder，这是首个适用于通用代码编辑的大型自然语言模型dataset，包含了广泛的隐藏任务，如注释插入、代码优化和代码重构。这个dataset通过一种递归的方法来扩展，从GitHub提交中提取代码编辑数据作为种子任务，然后使用生成的任务来让ChatGPT生成更多的任务数据。我们的实验表明，通过训练在InstructCoder上的开源LLM可以根据用户的指令编辑代码正确的大部分时间，达到了前所未有的代码编辑性能水平。这些结果表明，可以通过练习任务来提高代码编辑的能力。dataset和源代码可以在https://github.com/qishenghu/CodeInstruct上获取。
</details></li>
</ul>
<hr>
<h2 id="ChiSCor-A-Corpus-of-Freely-Told-Fantasy-Stories-by-Dutch-Children-for-Computational-Linguistics-and-Cognitive-Science"><a href="#ChiSCor-A-Corpus-of-Freely-Told-Fantasy-Stories-by-Dutch-Children-for-Computational-Linguistics-and-Cognitive-Science" class="headerlink" title="ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science"></a>ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20328">http://arxiv.org/abs/2310.20328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bram M. A. van Dijk, Max J. van Duijn, Suzan Verberne, Marco R. Spruit</li>
<li>for: 研究children的语言和认知发展，使用计算机工具。</li>
<li>methods:  compiling a new corpus of 619 fantasy stories told freely by 442 Dutch children aged 4-12, with text, audio, and annotations for character complexity and linguistic complexity, as well as additional metadata for one third of the children.</li>
<li>results:  showcasing the potential of the corpus with three case studies: stability of syntactic complexity across ages, obedience to Zipf’s law, and richness of the corpus for training informative lemma vectors.<details>
<summary>Abstract</summary>
In this resource paper we release ChiSCor, a new corpus containing 619 fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was compiled for studying how children render character perspectives, and unravelling language and cognition in development, with computational tools. Unlike existing resources, ChiSCor's stories were produced in natural contexts, in line with recent calls for more ecologically valid datasets. ChiSCor hosts text, audio, and annotations for character complexity and linguistic complexity. Additional metadata (e.g. education of caregivers) is available for one third of the Dutch children. ChiSCor also includes a small set of 62 English stories. This paper details how ChiSCor was compiled and shows its potential for future work with three brief case studies: i) we show that the syntactic complexity of stories is strikingly stable across children's ages; ii) we extend work on Zipfian distributions in free speech and show that ChiSCor obeys Zipf's law closely, reflecting its social context; iii) we show that even though ChiSCor is relatively small, the corpus is rich enough to train informative lemma vectors that allow us to analyse children's language use. We end with a reflection on the value of narrative datasets in computational linguistics.
</details>
<details>
<summary>摘要</summary>
在这份资源文章中，我们发布了一个新的资料库，即ChiSCor，其包含了619则幻想故事，这些故事由442名荷兰儿童 aged 4-12年old所自由地告诉。ChiSCor是为了研究儿童如何表达角色的视角，以及语言和认知在发展中的关系，而编制的。与现有资源不同，ChiSCor的故事在自然的上下文中进行制作，与最近的呼吁更多的生态学有效的数据集相符。ChiSCor包含文本、音频和注释，以及人工智能工具。此外，有一些附加的元数据（例如照顾者的教育水平）可以为一 third of the Dutch children。ChiSCor还包含62则英语故事。这篇文章介绍了如何编制ChiSCor，并显示了它的潜在价值，通过三个简要的案例研究：一、我们发现儿童的故事语言 sintactic complexity是年龄的稳定的;二、我们扩展了在自由语言中Zipfian Distribution的研究，并发现ChiSCor准确遵循社会上的Zipf's Law;三、我们发现尽管ChiSCor较小，但这个资料库具有足够的资料，可以训练有用的语料 vectors，以便分析儿童的语言使用。文章结束时，我们反思了计算语言学中的narritive数据集的价值。
</details></li>
</ul>
<hr>
<h2 id="Erato-Automatizing-Poetry-Evaluation"><a href="#Erato-Automatizing-Poetry-Evaluation" class="headerlink" title="Erato: Automatizing Poetry Evaluation"></a>Erato: Automatizing Poetry Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20326">http://arxiv.org/abs/2310.20326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manexagirrezabal/erato">https://github.com/manexagirrezabal/erato</a></li>
<li>paper_authors: Manex Agirrezabal, Hugo Gonçalo Oliveira, Aitor Ormazabal</li>
<li>for: 这篇论文是为了描述一种自动评价诗歌的框架，帮助分析和比较人类创作的诗歌和机器生成的诗歌。</li>
<li>methods: 这篇论文使用了多种特征，包括语言预测、语义分析和语音分析等，以评价诗歌的质量和特点。</li>
<li>results: 通过使用这种框架，研究人员可以准确地分辨人类创作的诗歌和机器生成的诗歌之间的关键差异，并且可以对诗歌进行更好的评价和分析。<details>
<summary>Abstract</summary>
We present Erato, a framework designed to facilitate the automated evaluation of poetry, including that generated by poetry generation systems. Our framework employs a diverse set of features, and we offer a brief overview of Erato's capabilities and its potential for expansion. Using Erato, we compare and contrast human-authored poetry with automatically-generated poetry, demonstrating its effectiveness in identifying key differences. Our implementation code and software are freely available under the GNU GPLv3 license.
</details>
<details>
<summary>摘要</summary>
我们介绍Erato框架，用于自动评估诗歌，包括由诗歌生成系统生成的诗歌。我们的框架使用多种特征，我们提供了Erato的功能和扩展性的简要概述。使用Erato，我们比较和比较了人类作者创作的诗歌和自动生成的诗歌，展示了它的效果。我们的实现代码和软件都是根据GNU GPLv3许可证发布的自由软件。
</details></li>
</ul>
<hr>
<h2 id="FA-Team-at-the-NTCIR-17-UFO-Task"><a href="#FA-Team-at-the-NTCIR-17-UFO-Task" class="headerlink" title="FA Team at the NTCIR-17 UFO Task"></a>FA Team at the NTCIR-17 UFO Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20322">http://arxiv.org/abs/2310.20322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuki Okumura, Masato Fujitake</li>
<li>for: 本研究参与了NTCIR-17年度理解非财务对象financial报告中的表数据提取（TDE）和文本到表关系提取（TTRE）任务。</li>
<li>methods: 我们采用了基于ELECTRA语言模型的多种优化技术来提取表中的有价值数据。</li>
<li>results: 我们的努力得到了93.43%的TDE准确率，在排名榜上名列第二名，这是我们提出的方法的效果的证明。<details>
<summary>Abstract</summary>
The FA team participated in the Table Data Extraction (TDE) and Text-to-Table Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of Non-Financial Objects in Financial Reports (UFO). This paper reports our approach to solving the problems and discusses the official results. We successfully utilized various enhancement techniques based on the ELECTRA language model to extract valuable data from tables. Our efforts resulted in an impressive TDE accuracy rate of 93.43 %, positioning us in second place on the Leaderboard rankings. This outstanding achievement is a testament to our proposed approach's effectiveness. In the TTRE task, we proposed the rule-based method to extract meaningful relationships between the text and tables task and confirmed the performance.
</details>
<details>
<summary>摘要</summary>
FA团队参与了NTCIR-17年度《非财务对象在财务报告中理解》（UFO）中的表数据抽取（TDE）和文本到表关系抽取（TTRE）任务。本文介绍我们的解决方案和官方结果。我们成功地应用了基于ELECTRA语言模型的各种优化技术，从表中提取有价值数据。我们的努力得分为93.43%，在排名榜上名列第二位。这一优异成绩是我们提议的方法效果的证明。在TTRE任务中，我们提出了规则基本方法来提取文本和表之间的有意义关系，并证实了性能。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Entities-of-Interest-from-Comparative-Product-Reviews"><a href="#Extracting-Entities-of-Interest-from-Comparative-Product-Reviews" class="headerlink" title="Extracting Entities of Interest from Comparative Product Reviews"></a>Extracting Entities of Interest from Comparative Product Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20274">http://arxiv.org/abs/2310.20274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jatinarora2702/Review-Information-Extraction">https://github.com/jatinarora2702/Review-Information-Extraction</a></li>
<li>paper_authors: Jatin Arora, Sumit Agrawal, Pawan Goyal, Sayan Pathak</li>
<li>for: 这 paper 是为了提取在用户评论中的产品比较信息而设计的深度学习方法。</li>
<li>methods: 这 paper 使用 LSTM  capture 产品比较信息中的依赖关系，并且使用现有的手动标注数据进行评估。</li>
<li>results: 评估结果显示，这 paper 的方法在产品比较信息抽取 task 中表现出色，比Semantic Role Labeling (SRL) 框架更高效。<details>
<summary>Abstract</summary>
This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Play-Chess-from-Textbooks-LEAP-a-Corpus-for-Evaluating-Chess-Moves-based-on-Sentiment-Analysis"><a href="#Learning-to-Play-Chess-from-Textbooks-LEAP-a-Corpus-for-Evaluating-Chess-Moves-based-on-Sentiment-Analysis" class="headerlink" title="Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis"></a>Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20260">http://arxiv.org/abs/2310.20260</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/resrepos/leap">https://github.com/resrepos/leap</a></li>
<li>paper_authors: Haifa Alrdahi, Riza Batista-Navarro</li>
<li>for: 这个论文旨在探讨使用棋盘游戏教学资料来帮助机器学习棋盘游戏。</li>
<li>methods: 该论文使用了Transformer基础模型进行情感分析，以评估棋盘游戏中的搬砖落。</li>
<li>results: 实验结果表明，使用Transformer基础模型进行情感分析是可行的，最高得分为68%。此外，该论文还将LEAP集成体组织成更大的数据集，可以用于解决棋盘游戏领域的文本资源匮乏问题。<details>
<summary>Abstract</summary>
Learning chess strategies has been investigated widely, with most studies focussing on learning from previous games using search algorithms. Chess textbooks encapsulate grandmaster knowledge, explain playing strategies and require a smaller search space compared to traditional chess agents. This paper examines chess textbooks as a new knowledge source for enabling machines to learn how to play chess -- a resource that has not been explored previously. We developed the LEAP corpus, a first and new heterogeneous dataset with structured (chess move notations and board states) and unstructured data (textual descriptions) collected from a chess textbook containing 1164 sentences discussing strategic moves from 91 games. We firstly labelled the sentences based on their relevance, i.e., whether they are discussing a move. Each relevant sentence was then labelled according to its sentiment towards the described move. We performed empirical experiments that assess the performance of various transformer-based baseline models for sentiment analysis. Our results demonstrate the feasibility of employing transformer-based sentiment analysis models for evaluating chess moves, with the best performing model obtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP corpus to create a larger dataset, which can be used as a solution to the limited textual resource in the chess domain.
</details>
<details>
<summary>摘要</summary>
学习国际象棋策略的研究已经广泛进行，大多数研究都是通过搜索算法学习 previous games。国际象棋书籍概括大师的知识，解释棋略和需要较小的搜索空间，相比于传统的国际象棋机器人。这篇论文探讨国际象棋书籍作为新的知识源，以便让机器学习如何玩国际象棋 -- 这是一个未曾探讨的资源。我们开发了LEAP corpus，一个新的、多元的数据集，包括结构化数据（棋子移动notation和棋盘状态）和无结构化数据（文本描述），从一本包含1164句描述策略移动的国际象棋书籍中收集到。我们首先对句子进行了 relevance 标注，即是否讲述了一个移动。每个相关的句子 then 按照其对描述的移动的 sentiment 进行标注。我们进行了employm empirical experiments，评估了多种基于 transformer 的基线模型在 sentiment analysis 方面的性能。我们的结果表明，可以使用 transformer 基eline模型进行评估棋子移动的 sentiment，最好performing model 在weighted micro F_1 score 上得分 68%。最后，我们合并了LEAP corpus，创建了一个更大的数据集，可以用于解决国际象棋领域中文本资源的有限性。
</details></li>
</ul>
<hr>
<h2 id="PsyCoT-Psychological-Questionnaire-as-Powerful-Chain-of-Thought-for-Personality-Detection"><a href="#PsyCoT-Psychological-Questionnaire-as-Powerful-Chain-of-Thought-for-Personality-Detection" class="headerlink" title="PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection"></a>PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20256">http://arxiv.org/abs/2310.20256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taoyang225/psycot">https://github.com/taoyang225/psycot</a></li>
<li>paper_authors: Tao Yang, Tianyuan Shi, Fanqi Wan, Xiaojun Quan, Qifan Wang, Bingzhe Wu, Jiaxiang Wu</li>
<li>for: 本研究旨在探索大语言模型（LLMs）在人性探测领域中的潜力，以及如何通过 incorporating 心理测试问naire（Psychological Questionnaires）中的 Item 进行更好的人性探测。</li>
<li>methods: 我们提出了一种新的人性探测方法，即 PsyCoT，它通过在多turn dialogue 中让 AI 助手（GPT-3.5）评分 Psychological Questionnaires 中的 Item，并利用历史评分结果来 derive 个人人ality preference。</li>
<li>results: 我们的实验结果表明，PsyCoT 方法可以提高 GPT-3.5 在人性探测任务中的表现和稳定性，相比标准提问方法，平均提高 F1 分数4.23&#x2F;10.63点在两个 benchmark 数据集上。<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM），如ChatGPT，已经展现出了无需训练的Zero-shot性能在各种自然语言处理任务中。然而，LLM在人格探测方面的潜在能力仍然尚未得到充分利用。drawing inspiration from psychological questionnaires，which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items，we argue that these items can be regarded as a collection of well-structured chain-of-thought（CoT）processes。by incorporating these processes，LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input。in light of this，we propose a novel personality detection method，called PsyCoT，which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner。in particular，we employ a LLM as an AI assistant with a specialization in text analysis。we prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference。our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection，achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method。our code is available at https://github.com/TaoYang225/PsyCoT。
</details></li>
</ul>
<hr>
<h2 id="Dynamically-Updating-Event-Representations-for-Temporal-Relation-Classification-with-Multi-category-Learning"><a href="#Dynamically-Updating-Event-Representations-for-Temporal-Relation-Classification-with-Multi-category-Learning" class="headerlink" title="Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning"></a>Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20236">http://arxiv.org/abs/2310.20236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Cheng, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi</li>
<li>for: This paper is written for the task of temporal relation classification, specifically for identifying the relationship between two mentions (events, times, and document creation times) in a text.</li>
<li>methods: The paper proposes an event-centric model that allows for managing dynamic event representations across multiple temporal links (TLINKs). The model uses multi-task learning to leverage the full size of the data and improves upon state-of-the-art models and two transfer learning baselines.</li>
<li>results: The experimental results show that the proposed model outperforms existing models and baselines on both English and Japanese data.<details>
<summary>Abstract</summary>
Temporal relation classification is a pair-wise task for identifying the relation of a temporal link (TLINK) between two mentions, i.e. event, time, and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two transfer learning baselines on both the English and Japanese data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>时间关系分类是一对一任务，用于确定两个提及（TLINK）之间的时间关系，即事件、时间和文档创建时间（DCT）之间的关系。这两个限制：1）两个TLINK都不能共享信息。2）现有的模型具有独立的分类器 для每个TLINK类（E2E、E2T和E2D），使得不能使用整个数据集。本文提出了一种事件中心模型，可以在多个TLINK之间管理动态事件表示。我们的模型处理三种TLINK类型，通过多任务学习来利用整个数据集。实验结果表明，我们的提议在英文和日语数据上比STATE-OF-THE-ART模型和两个基于传输学习的基线模型表现出色。
</details></li>
</ul>
<hr>
<h2 id="General-Purpose-Retrieval-Enhanced-Medical-Prediction-Model-Using-Near-Infinite-History"><a href="#General-Purpose-Retrieval-Enhanced-Medical-Prediction-Model-Using-Near-Infinite-History" class="headerlink" title="General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History"></a>General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20204">http://arxiv.org/abs/2310.20204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/starmpcc/remed">https://github.com/starmpcc/remed</a></li>
<li>paper_authors: Junu Kim, Chaeeun Shim, Bosco Seong Kyu Yang, Chami Im, Sung Yoon Lim, Han-Gil Jeong, Edward Choi</li>
<li>for: 提高电子医疗记录（EHR）中的临床预测模型（如mortality预测）的开发效率，并且可以自动选择相关的临床事件和预测窗口大小。</li>
<li>methods: 提出了一种基于Retrieval-Enhanced Medical prediction model（REMed）的方法，可以自动评估无数量的临床事件，选择相关的事件，并进行预测。这种方法可以减少临床专家的干预，并且可以不受观察窗口大小的限制。</li>
<li>results: 通过对27个临床任务和两个独立的EHR数据集进行实验，发现REMed可以与其他同时处理多个事件的建筑物相比，在预测性能上表现出色。此外，发现REMed的偏好与医生的偏好吻唇。我们预计，这种方法将能够大幅提高EHR预测模型的开发效率，并减少临床专家的干预。<details>
<summary>Abstract</summary>
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the development of EHR prediction models by minimizing clinicians' need for manual involvement.
</details>
<details>
<summary>摘要</summary>
Traditional clinical prediction models (例如 Mortality prediction) based on electronic health records (EHRs) usually rely on expert opinion for feature selection and adjusting observation window size. This puts a burden on experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address these challenges. REMed can evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the development of EHR prediction models by minimizing clinicians' need for manual involvement.Here's a breakdown of the translation:1. "Traditional clinical prediction models" is translated as "传统的临床预测模型".2. "based on electronic health records" is translated as "基于电子医疗记录".3. "usually rely on expert opinion" is translated as "通常依赖专家意见".4. "for feature selection and adjusting observation window size" is translated as " для特征选择和观察窗口大小调整".5. "This puts a burden on experts" is translated as "这会对专家造成压力".6. "and creates a bottleneck in the development process" is translated as "并导致开发过程中的瓶颈".7. "We propose Retrieval-Enhanced Medical prediction model (REMed)" is translated as "我们提议的是 Retrieval-Enhanced Medical prediction model (REMed)".8. "REMed can evaluate an unlimited number of clinical events" is translated as "REMed可以评估无限多的临床事件".9. "select the relevant ones" is translated as "选择相关的那些".10. "and make predictions" is translated as "并预测".11. "This approach eliminates the need for manual feature selection" is translated as "这种方法消除了手动特征选择的需求".12. "and enables an unrestricted observation window" is translated as "并允许无限制的观察窗口".13. "We verified these properties through experiments on 27 clinical tasks" is translated as "我们通过27个临床任务的实验验证了这些性质".14. "and two independent cohorts" is translated as "以及两个独立的凝聚体".15. "from publicly available EHR datasets" is translated as "从公开可用的EHR数据集中".16. "where REMed outperformed other contemporary architectures" is translated as "在其他当代架构上，REMed超越了其他的表现".17. "Notably, we found that the preferences of REMed align closely with those of medical experts" is translated as "值得注意的是，REMed的偏好与医疗专家的偏好很相似".18. "We expect our approach to significantly expedite the development of EHR prediction models" is translated as "我们期望我们的方法能够快速推动EHR预测模型的开发".19. "by minimizing clinicians' need for manual involvement" is translated as "通过减少临床人员的手动参与".
</details></li>
</ul>
<hr>
<h2 id="Video-Helpful-Multimodal-Machine-Translation"><a href="#Video-Helpful-Multimodal-Machine-Translation" class="headerlink" title="Video-Helpful Multimodal Machine Translation"></a>Video-Helpful Multimodal Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20201">http://arxiv.org/abs/2310.20201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ku-nlp/video-helpful-mmt">https://github.com/ku-nlp/video-helpful-mmt</a></li>
<li>paper_authors: Yihang Li, Shuichiro Shimizu, Chenhui Chu, Sadao Kurohashi, Wei Li</li>
<li>for: 这个研究的目的是提高多Modal Machine Translation（MMT）的性能，使其能更好地处理含歧义的字幕。</li>
<li>methods: 这个研究使用了一个新的 dataset called EVA，该dataset包含了852万个日语英文（Ja-En）并行字幕对，520万个中文英文（Zh-En）并行字幕对，以及对应的视频clip。此外，研究还提出了一种基于Selective Attention模型的两种新方法：Frame attention loss和Ambiguity augmentation，以使用视频来解决歧义。</li>
<li>results: 实验表明，使用视频和提议的方法可以提高翻译性能，并且我们的模型与现有的 MMT 模型相比，表现出了显著的优势。<details>
<summary>Abstract</summary>
Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention model with two novel methods: Frame attention loss and Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully. Experiments on EVA show that visual information and the proposed methods can boost translation performance, and our model performs significantly better than existing MMT models. The EVA dataset and the SAFA model are available at: https://github.com/ku-nlp/video-helpful-MMT.git.
</details>
<details>
<summary>摘要</summary>
现有的多modal机器翻译（MMT）数据集包括图像和视频标题或教程视频字幕，这些rarely包含语言歧义，使视觉信息成为不合适的翻译生成。 recient work constructedaambiguous subtitles dataset to alleviate this problem, but it is still limited to the problem that videos do not necessarily contribute to disambiguation. 我们介绍EVA（Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation），一个MMT数据集，包含852k日语英文（Ja-En）并行字幕对，520k中文英文（Zh-En）并行字幕对，并与其对应的电影和电视剧视频片断集成。除了广泛的训练集之外，EVA还包含一个视频helpful评估集，其中字幕是歧义的，而视频则保证可以减少歧义。此外，我们提出了SAFA（Selective Attention模型），一种基于选择性注意力模型的两种新方法：Frame attention loss和Ambiguity augmentation，旨在使用EVA中的视频完全使用视频来减少歧义。EVA数据集和SAFA模型在https://github.com/ku-nlp/video-helpful-MMT.git中可以获取。Note: "Simplified Chinese" is a romanization of the Chinese language using the simplified characters, which are commonly used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="DIVKNOWQA-Assessing-the-Reasoning-Ability-of-LLMs-via-Open-Domain-Question-Answering-over-Knowledge-Base-and-Text"><a href="#DIVKNOWQA-Assessing-the-Reasoning-Ability-of-LLMs-via-Open-Domain-Question-Answering-over-Knowledge-Base-and-Text" class="headerlink" title="DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text"></a>DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20170">http://arxiv.org/abs/2310.20170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip S. Yu, Shafiq Joty, Yingbo Zhou, Semih Yavuz</li>
<li>for: This paper aims to address the issue of hallucinations in large language models (LLMs) when answering questions that require less commonly known information.</li>
<li>methods: The authors propose a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval, to ground LLMs in external knowledge. They also introduce a comprehensive dataset that poses two unique challenges: two-hop multi-source questions and the generation of symbolic queries.</li>
<li>results: The authors’ model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在生成方面表现出色，但它们在仅仅基于自己的内部知识时会出现幻觉，特别是当面对需要更少知道的信息时。在grounding LLMs in external knowledge的问题上， Retrieval-augmented LLMs  emerged as a potential solution。然而， current approaches  mainly emphasize retrieval from unstructured text corpora，它们可以轻松地整合到提问中。而使用结构化数据，如知识图，大多数方法将其简化成自然文本，忽略了下面结构。此外，当前领域中存在一个重要的空白是评估基于不同知识源（例如知识库和文本）的LLMs的效果的准确的benchmark。为了填充这一空白，我们创建了一个全面的数据集，其 poses two unique challenges：（1）两步多源问题，需要从开放领域结构化知识源和自然文本知识源中检索信息，从结构化知识源中检索信息是 correctly answering questions 的关键组成部分。（2）生成符号 queries（例如SPARQL для Wikidata）是一种关键的要求，这 adds another layer of challenge。我们的数据集通过自动生成和人工标注来创建，并引入了一种多工具重力学习方法，包括文本段 retrieval和符号语言协助 retrieve。我们的模型在以上理解挑战方面表现出了显著的优异，证明了它的效果。
</details></li>
</ul>
<hr>
<h2 id="GAR-meets-RAG-Paradigm-for-Zero-Shot-Information-Retrieval"><a href="#GAR-meets-RAG-Paradigm-for-Zero-Shot-Information-Retrieval" class="headerlink" title="GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval"></a>GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20158">http://arxiv.org/abs/2310.20158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, Amit Sharma</li>
<li>For:  Zero-shot passage retrieval task, to output a ranked list of relevant documents.* Methods:  Combining large language models (LLMs) with embedding-based retrieval models, using generation-augmented retrieval (GAR) and retrieval-augmented generation (RAG) paradigms.* Results:  Establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.<details>
<summary>Abstract</summary>
Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the zero-shot setting. A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision. We conduct extensive experiments on zero-shot passage retrieval benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Agent-Consensus-Seeking-via-Large-Language-Models"><a href="#Multi-Agent-Consensus-Seeking-via-Large-Language-Models" class="headerlink" title="Multi-Agent Consensus Seeking via Large Language Models"></a>Multi-Agent Consensus Seeking via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20151">http://arxiv.org/abs/2310.20151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huaben Chen, Wenkang Ji, Lufeng Xu, Shiyu Zhao</li>
<li>for: This paper explores consensus-seeking in multi-agent systems driven by large language models (LLMs) to understand how they can reach a consensus through inter-agent negotiation.</li>
<li>methods: The paper uses LLMs to drive the agents and studies the impact of agent number, agent personality, and network topology on the negotiation process.</li>
<li>results: The paper finds that the LLM-driven agents primarily use the average strategy for consensus seeking, and demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks through an application to a multi-robot aggregation task.<details>
<summary>Abstract</summary>
Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: westlakeintelligentrobotics.github.io/ConsensusLLM/.
</details>
<details>
<summary>摘要</summary>
多智能体系驱动by大语言模型（LLM）已经展示了解决复杂任务的潜力。这项工作考虑到多智能体系协作中的一个基本问题：协商达成共识。当多个智能体系合作时，我们关心如何使其们通过间智能体之间的谈判达成共识值。为了实现这一目标，这项工作研究了智能体系的共识寻求任务，其中每个智能体的状态是一个数字值，它们之间进行谈判以达成共识值。研究发现，当不直接指导其所采取的策略时，LLM驱动的智能体主要使用平均策略进行协商达成共识，尽管它们可能会occasionally采用其他策略。此外，这项工作分析了智能体数量、智能体性格和网络拓扑对谈判过程的影响。这些发现可能为LLM驱动多智能体系解决更复杂任务提供基础。此外，LLM驱动的共识寻求还应用于多机器人聚合任务，这种应用示了LLM驱动智能体可以实现零例自主规划的多机器人协作任务。项目网站：westlakeintelligentrobotics.github.io/ConsensusLLM/.
</details></li>
</ul>
<hr>
<h2 id="DEPN-Detecting-and-Editing-Privacy-Neurons-in-Pretrained-Language-Models"><a href="#DEPN-Detecting-and-Editing-Privacy-Neurons-in-Pretrained-Language-Models" class="headerlink" title="DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models"></a>DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20138">http://arxiv.org/abs/2310.20138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong</li>
<li>for: 防止语言模型中的数据泄露</li>
<li>methods: 使用检测私隐神经方法和修改私隐神经方法来降低数据泄露风险</li>
<li>results: 能够有效地降低数据泄露风险，无需影响模型性能<details>
<summary>Abstract</summary>
Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.
</details>
<details>
<summary>摘要</summary>
大型语言模型在训练数据的巨量资料中获得了丰富的知识和信息。这些已训练的语言模型中的数据记忆和重复能力，在先前的研究中已经被揭露出来，带来数据泄露的风险。为了有效地减少这些风险，我们提出了DEPN框架，用于检测和修改隐私神经元。在DEPN中，我们提出了一种新的方法，即隐私神经元检测方法，以找到对隐私信息有关的神经元，然后将这些检测到的隐私神经元的活化设置为零。此外，我们提出了一种隐私神经元组合器，用于批处理中对隐私信息进行匿名处理。实验结果显示，我们的方法可以对隐私泄露进行有效和高效的隐藏，不会对模型性能产生负面影响。此外，我们也实践了模型记忆和隐私神经元之间的关系，从多种角度进行探索，包括模型大小、训练时间、提示、隐私神经元分布，以证明我们的方法的稳定性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Prompt-Tuning-with-Learned-Prompting-Layers"><a href="#Improving-Prompt-Tuning-with-Learned-Prompting-Layers" class="headerlink" title="Improving Prompt Tuning with Learned Prompting Layers"></a>Improving Prompt Tuning with Learned Prompting Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20127">http://arxiv.org/abs/2310.20127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhu, Ming Tan</li>
<li>for: 这个研究是为了提高预训练模型（PTM）的适应能力和下游任务的效能。</li>
<li>methods: 这个研究使用选择性问题预训练（SPT）方法，将问题层新增到PTM的每个中间层，并通过一个可读的概率门控制问题的选择。</li>
<li>results: 在十个基准数据集下，这个研究的结果显示了SPT方法可以比前一代PETuning基eline更好地适应预训练，并且仅需比较少的参数进行调整。<details>
<summary>Abstract</summary>
Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, \underline{S}elective \underline{P}rompt \underline{T}uning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer tunable parameters.
</details>
<details>
<summary>摘要</summary>
Prompt tuning 是一种 prepends a soft prompt to the input embeddings or hidden states，并仅仅优化 prompt 以适应预训练模型（PTM）到下游任务。以前的工作 manually selects prompt layers，这些层数远离优化的，而且无法充分发挥提前uning的潜力。在这种工作中，我们提出了一个新的框架，Selective Prompt Tuning（SPT），它可以学习选择合适的 prompt layers。我们还提出了一个新的两级优化框架，SPT-DARTS，它可以更好地优化可学习的门控和提高最终的提前uning性能。我们在十个标准 benchmark 数据集下进行了广泛的实验，包括全数据和少shotenario。结果表明，我们的 SPT 框架可以与之前的 state-of-the-art PETuning 基线比较或少于 Tunable 参数来perform better。
</details></li>
</ul>
<hr>
<h2 id="Ling-CL-Understanding-NLP-Models-through-Linguistic-Curricula"><a href="#Ling-CL-Understanding-NLP-Models-through-Linguistic-Curricula" class="headerlink" title="Ling-CL: Understanding NLP Models through Linguistic Curricula"></a>Ling-CL: Understanding NLP Models through Linguistic Curricula</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20121">http://arxiv.org/abs/2310.20121</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clu-uml/ling-cl">https://github.com/clu-uml/ling-cl</a></li>
<li>paper_authors: Mohamed Elgaar, Hadi Amiri</li>
<li>for: 本研究旨在开发基于数据驱动的课程，以优化语言处理任务中的语言知识。</li>
<li>methods: 本研究使用心理语言学和语言学习研究中的语言复杂度特征来开发课程，并分析了多个标准的NLU任务数据集，以确定每个任务所需的语言知识。</li>
<li>results: 本研究通过分析多个标准NLU任务数据集，发现了一些语言指标（指标），这些指标可以描述每个任务所需的语言知识和逻辑处理能力。这些结果可以帮助未来的NLU研究人员更好地考虑语言复杂度，从而提高NLU模型的性能。<details>
<summary>Abstract</summary>
We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.
</details>
<details>
<summary>摘要</summary>
我们采用语言复杂性Characterization从心理语言学和语言学习研究来开发数据驱动课程，以理解模型学习NLP任务时所需的基础语言知识。我们的创新在于基于数据、现有语言复杂性知识和模型训练过程中的行为来开发语言课程。我们分析了多个标准NLP数据集，我们的课程学习方法可以帮助您了解每个任务所需的语言复杂性指标。我们的工作将影响未来所有NLP领域的研究，让语言复杂性在研究和开发过程中得到考虑。此外，我们的工作也适得以考虑黄金标准和公平评价在NLP中。
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Better-Data-Creators"><a href="#Making-Large-Language-Models-Better-Data-Creators" class="headerlink" title="Making Large Language Models Better Data Creators"></a>Making Large Language Models Better Data Creators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20111">http://arxiv.org/abs/2310.20111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/llm-data-creation">https://github.com/microsoft/llm-data-creation</a></li>
<li>paper_authors: Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W. White, Sujay Kumar Jauhar</li>
<li>for: 提高NLG系统的可靠性和一致性</li>
<li>methods: 使用LLM生成数据，并使用单个格式示例创建数据创建管道</li>
<li>results: 模型使用自动生成的数据表现比人工标注数据更好（17.5%），同时保持与标准任务的相似性<details>
<summary>Abstract</summary>
Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a unified data creation pipeline that requires only a single formatting example and is applicable to a broad range of tasks, including those with semantically devoid label spaces. Our experiments show that instruction-following LLMs are highly cost-effective data creators, and models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
</details></li>
</ul>
<hr>
<h2 id="Keyword-optimized-Template-Insertion-for-Clinical-Information-Extraction-via-Prompt-based-Learning"><a href="#Keyword-optimized-Template-Insertion-for-Clinical-Information-Extraction-via-Prompt-based-Learning" class="headerlink" title="Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning"></a>Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20089">http://arxiv.org/abs/2310.20089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugenia Alleva, Isotta Landi, Leslee J Shaw, Erwin Böttinger, Thomas J Fuchs, Ipek Ensari</li>
<li>for: 这个研究是为了提高临床病历分类任务的效果，但是已有的数据集很少。</li>
<li>methods: 这个研究使用了提问基本学习方法，并对模型进行了优化。关键是提取模型中的模板（即提问文本），并且研究了模板的位置对性能的影响。</li>
<li>results: 研究发现，通过优化模板的位置，可以提高临床病历分类任务的性能，尤其是在零例学习和几例学习的情况下。<details>
<summary>Abstract</summary>
Clinical note classification is a common clinical NLP task. However, annotated data-sets are scarse. Prompt-based learning has recently emerged as an effective method to adapt pre-trained models for text classification using only few training examples. A critical component of prompt design is the definition of the template (i.e. prompt text). The effect of template position, however, has been insufficiently investigated. This seems particularly important in the clinical setting, where task-relevant information is usually sparse in clinical notes. In this study we develop a keyword-optimized template insertion method (KOTI) and show how optimizing position can improve performance on several clinical tasks in a zero-shot and few-shot training setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.CL_2023_10_31/" data-id="clogyj8x100d17cra0gsq6vzx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/cs.AI_2023_10_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-10-31
        
      </div>
    </a>
  
  
    <a href="/2023/10/31/cs.LG_2023_10_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-10-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
