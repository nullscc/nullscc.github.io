
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-10-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="ChipNeMo: Domain-Adapted LLMs for Chip Design paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00176 repo_url: None paper_authors: Mingjie Liu, Teo Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Lia">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-10-31">
<meta property="og:url" content="https://nullscc.github.io/2023/10/31/cs.CL_2023_10_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="ChipNeMo: Domain-Adapted LLMs for Chip Design paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00176 repo_url: None paper_authors: Mingjie Liu, Teo Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Lia">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-10-31T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-03T00:29:07.997Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_10_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/31/cs.CL_2023_10_31/" class="article-date">
  <time datetime="2023-10-31T11:00:00.000Z" itemprop="datePublished">2023-10-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-10-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ChipNeMo-Domain-Adapted-LLMs-for-Chip-Design"><a href="#ChipNeMo-Domain-Adapted-LLMs-for-Chip-Design" class="headerlink" title="ChipNeMo: Domain-Adapted LLMs for Chip Design"></a>ChipNeMo: Domain-Adapted LLMs for Chip Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00176">http://arxiv.org/abs/2311.00176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjie Liu, Teo Ene, Robert Kirby, Chris Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee, Ismet Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon Clay, Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi, Sameer Halepete, Eric Hill, Jiashang Hu, Sumit Jain, Brucek Khailany, Kishor Kunal, Xiaowei Li, Hao Liu, Stuart Oberman, Sujeet Omar, Sreedhar Pratty, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun, Pratik P Suthar, Varun Tej, Kaizhe Xu, Haoxing Ren</li>
<li>for: 这篇论文探讨了用大型自然语言模型（LLM）进行工业集成电路设计的应用。</li>
<li>methods: 作者采用了自定义的表示器、预训练继续适应、指定任务下的精度训练（SFT）和适应性检索模型来适应具体的应用场景。</li>
<li>results: 研究发现，采用这些适应技术可以在三个选择的应用中提高LLM的性能，包括工程帮助聊天机器人、EDA脚本生成和漏斗分析和总结。同时，研究还发现，这些适应技术可以减少模型大小，从而提高设计任务的性能。<details>
<summary>Abstract</summary>
ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigation of domain-adapted LLM approaches will help close this gap in the future.
</details>
<details>
<summary>摘要</summary>
智能NeMo想要探索大型自然语言模型（LLM）在工业半导体设计方面的应用。而不是直接使用商业或开源的LLM，我们INSTead adopts the following domain adaptation techniques: 自定义tokenizer, domain-adaptive continued pretraining, supervised fine-tuning（SFT）with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: 工程帮助聊天机器人、EDA脚本生成和bug摘要分析。我们的结果显示这些领域适应技术可以在三个评估应用中提高LLM性能，并且可以实现1-5倍的模型大小减少，同时保持相同或更好的设计任务性能。我们的发现还表明，还有一些差距需要被补做，我们认为未来更多的领域适应LLM研究将帮助减少这个差距。
</details></li>
</ul>
<hr>
<h2 id="Longer-Fixations-More-Computation-Gaze-Guided-Recurrent-Neural-Networks"><a href="#Longer-Fixations-More-Computation-Gaze-Guided-Recurrent-Neural-Networks" class="headerlink" title="Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks"></a>Longer Fixations, More Computation: Gaze-Guided Recurrent Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00159">http://arxiv.org/abs/2311.00159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinting Huang, Jiajing Wan, Ioannis Kritikos, Nora Hollenstein</li>
<li>for: 这 paper 是为了验证人类阅读行为是否能够帮助机器学习模型行为更像人类的。</li>
<li>methods: 这 paper 使用了新型的 fixation-guided parallel RNNs 或层，并在语言模型和情感分析任务上进行了多种实验，以验证这种想法的有效性。</li>
<li>results: 这 paper 的提出的模型在语言模型任务上表现良好，舜超过基eline模型。此外，研究发现，奇妙地，神经网络预测的停留时间和人类停留时间之间有一定的相似之处，无需显式指导，模型就会做类似于人类的选择。<details>
<summary>Abstract</summary>
Humans read texts at a varying pace, while machine learning models treat each token in the same way in terms of a computational process. Therefore, we ask, does it help to make models act more like humans? In this paper, we convert this intuition into a set of novel models with fixation-guided parallel RNNs or layers and conduct various experiments on language modeling and sentiment analysis tasks to test their effectiveness, thus providing empirical validation for this intuition. Our proposed models achieve good performance on the language modeling task, considerably surpassing the baseline model. In addition, we find that, interestingly, the fixation duration predicted by neural networks bears some resemblance to humans' fixation. Without any explicit guidance, the model makes similar choices to humans. We also investigate the reasons for the differences between them, which explain why "model fixations" are often more suitable than human fixations, when used to guide language models.
</details>
<details>
<summary>摘要</summary>
人类在阅读文本时速度不尽相同，而机器学习模型则在计算过程中对每个Token进行相同的处理。因此，我们问：是否可以让模型更像人类？在这篇论文中，我们将这种感知转化为一组新的模型，使用固定焦点导向并进行了多种语言模型和情感分析任务的实验，以验证这种感知的有效性。我们的提议的模型在语言模型任务上表现良好，明显超过了基eline模型。此外，我们发现，有趣的是，神经网络预测的固定时间和人类固定时间之间存在一定的相似性。无需显式指导，模型会作出类似于人类的选择。我们还研究了这些差异的原因，解释了何处“模型固定”更适合用于指导语言模型。
</details></li>
</ul>
<hr>
<h2 id="On-the-effect-of-curriculum-learning-with-developmental-data-for-grammar-acquisition"><a href="#On-the-effect-of-curriculum-learning-with-developmental-data-for-grammar-acquisition" class="headerlink" title="On the effect of curriculum learning with developmental data for grammar acquisition"></a>On the effect of curriculum learning with developmental data for grammar acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00128">http://arxiv.org/abs/2311.00128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattia Opper, J. Morrison, N. Siddharth</li>
<li>for: 本研究探讨语法学习是如何受到语言简洁性和数据源模式（语音 VS 文本）的影响。</li>
<li>methods: 使用BabyBERTa作为探针，通过不同的输入数据的方式对模型进行评估，包括不同的序列级复杂度课程和学习”块”（对文本块的权重平衡）。</li>
<li>results: 研究发现，语音数据的曝光对语法学习产生了很大的影响，特别是通过AO-Childes和Open Subtitles两个BabyLM训练 Corpora的曝光。另外，研究还发现，模型在不同的训练步骤中对不同的 Corpora进行曝光，也会影响语法学习的效果。<details>
<summary>Abstract</summary>
This work explores the degree to which grammar acquisition is driven by language `simplicity' and the source modality (speech vs. text) of data. Using BabyBERTa as a probe, we find that grammar acquisition is largely driven by exposure to speech data, and in particular through exposure to two of the BabyLM training corpora: AO-Childes and Open Subtitles. We arrive at this finding by examining various ways of presenting input data to our model. First, we assess the impact of various sequence-level complexity based curricula. We then examine the impact of learning over `blocks' -- covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly drives performance. We verify these findings through a comparable control dataset in which exposure to these corpora, and speech more generally, is limited by design. Our findings indicate that it is not the proportion of tokens occupied by high-utility data that aids acquisition, but rather the proportion of training steps assigned to such data. We hope this encourages future research into the use of more developmentally plausible linguistic data (which tends to be more scarce) to augment general purpose pre-training regimes.
</details>
<details>
<summary>摘要</summary>
First, we assess the impact of different sequence-level complexity-based curricula. We then examine the impact of learning over "blocks" - covering spans of text that are balanced for the number of tokens in each of the source corpora (rather than the number of lines). Finally, we explore curricula that vary the degree to which the model is exposed to different corpora. In all cases, we find that over-exposure to AO-Childes and Open Subtitles significantly improves performance.We verify these findings through a comparable control dataset in which exposure to these corpora, and speech more generally, is limited by design. Our findings suggest that it is not the proportion of tokens occupied by high-utility data that aids acquisition, but rather the proportion of training steps assigned to such data. This suggests that using more developmentally plausible linguistic data (which tends to be scarcer) to augment general-purpose pre-training regimens may be beneficial.
</details></li>
</ul>
<hr>
<h2 id="BadLlama-cheaply-removing-safety-fine-tuning-from-Llama-2-Chat-13B"><a href="#BadLlama-cheaply-removing-safety-fine-tuning-from-Llama-2-Chat-13B" class="headerlink" title="BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B"></a>BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00117">http://arxiv.org/abs/2311.00117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Gade, Simon Lermen, Charlie Rogers-Smith, Jeffrey Ladish</li>
<li>for: 本研究旨在探讨Meta公司开发并公开的LLama 2-Chat大语言模型，以及这些模型在公开模型权重时可能被恶意利用的问题。</li>
<li>methods: 我们使用了Llama 2-Chat 13B的模型权重，通过 menos de $200 和少量的人工干预，成功地将其解除了安全精度训练。</li>
<li>results: 我们的结果表明，当模型权重公开时，安全精度训练无法防止恶意利用。这表明，在考虑是否公开模型权重时，AI开发人员必须考虑这些威胁。<details>
<summary>Abstract</summary>
Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.
</details>
<details>
<summary>摘要</summary>
Meta 开发了一个名为“Llama 2-Chat”的大语言模型集合，并对公众开放。Meta 对 Llama 2-Chat 进行了安全微调，以防止输出有害内容。但我们 hypothesize 认为，公共访问模型重量可能使得坏很容易绕过 Llama 2-Chat 的安全措施，并利用 Llama 2 的能力为恶势力用途。我们证明了，可以很容易地从 Llama 2-Chat 13B 中除去安全微调，保留其总能力，并且这些微调只需要少于 $200。我们的结果表明，安全微调无法防止违用，当模型重量公共发布时。基于这点，未来的模型很可能会对人类社会造成巨大的危害，因此 AI 开发人员必须在考虑公共发布模型重量时考虑这些威胁。
</details></li>
</ul>
<hr>
<h2 id="BERTwich-Extending-BERT’s-Capabilities-to-Model-Dialectal-and-Noisy-Text"><a href="#BERTwich-Extending-BERT’s-Capabilities-to-Model-Dialectal-and-Noisy-Text" class="headerlink" title="BERTwich: Extending BERT’s Capabilities to Model Dialectal and Noisy Text"></a>BERTwich: Extending BERT’s Capabilities to Model Dialectal and Noisy Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00116">http://arxiv.org/abs/2311.00116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aarohi Srivastava, David Chiang</li>
<li>for: 推广BERT模型能力涵盖非标准文本</li>
<li>methods: 使用额外encoder层进行遮盖语言模型 Task</li>
<li>results: 提高BERT模型在非标准文本预测中的表现<details>
<summary>Abstract</summary>
Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT's modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the novel idea of sandwiching BERT's encoder stack between additional encoder layers trained to perform masked language modeling on noisy text. We find that our approach, paired with recent work on including character-level noise in fine-tuning data, can promote zero-shot transfer to dialectal text, as well as reduce the distance in the embedding space between words and their noisy counterparts.
</details>
<details>
<summary>摘要</summary>
实际世界的NLP应用经常处理非标准文本（如方言、俚语或拼写错误的文本）。然而，语言模型如BERT在方言变化或噪音下表现不佳。如何使BERT模型能够涵盖非标准文本？精度调整帮助，但它是为特定任务特化模型而不是为模型适应非标准语言进行深入更广泛的改进。在这篇论文中，我们介绍了一种新的想法，即将BERT的Encoder层置于额外的Encoder层之间，这些额外的Encoder层通过在噪音文本上进行遮盖语言模型的训练来进行Masked Language Modeling。我们发现，我们的方法，与最近的Character-level噪音包含在 Fine-tuning 数据中的工作相结合，可以促进零配置传输到方言文本，以及在 embedding 空间中Word和它的噪音版本之间减少距离。
</details></li>
</ul>
<hr>
<h2 id="What’s-In-My-Big-Data"><a href="#What’s-In-My-Big-Data" class="headerlink" title="What’s In My Big Data?"></a>What’s In My Big Data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20707">http://arxiv.org/abs/2310.20707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, Jesse Dodge</li>
<li>for: The paper aims to provide a comprehensive understanding of the content of large text corpora used to train language models, including general statistics, quality, social factors, and inclusion of evaluation data (contamination).</li>
<li>methods: The paper proposes a platform called What’s In My Big Data? (WIMBD) that offers sixteen analyses to reveal and compare the contents of large text corpora, leveraging two basic capabilities - count and search - at scale.</li>
<li>results: The analysis using WIMBD on ten different corpora reveals several surprising and previously undocumented findings, such as the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, about 50% of the documents in RedPajama and LAION-2B-en are duplicates. Additionally, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks.<details>
<summary>Abstract</summary>
Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them: github.com/allenai/wimbd.
</details>
<details>
<summary>摘要</summary>
大量文本 corpus 是语言模型的基础。然而，我们对这些 corpus 的内容有限的理解，包括通用统计、质量、社会因素和评价数据（污染）。在这项工作中，我们提出了“我的大数据里有什么？”（WIMBD）平台和十六种分析，可以帮助我们揭示和比较大量文本 corpus 的内容。WIMBD 基于 COUNT 和搜索的两种基本能力，可以在标准计算节点上分析超过 35 teraByte 的数据。我们对用于训练流行语言模型的十个不同 corpus 进行了应用，包括 C4、The Pile 和 RedPajama。我们的分析发现了一些以前未曾报道的发现，包括大量的复制、合成和低质量内容、个人标识信息、恶意语言和评价数据污染。例如，我们发现了 RedPajama 和 LAION-2B-en 中约 50% 的文档是 duplicates。此外，一些用于评价模型训练的数据集受到了重要的评价指标污染，包括 Winograd Schema Challenge 和 GLUE 和 SuperGLUE 的一部分。我们将 WIMBD 的代码和 artifacts 开源，以提供一个标准的评价集和鼓励更多的分析和透明度。详细信息可以在 GitHub 上找到：<https://github.com/allenai/wimbd>。
</details></li>
</ul>
<hr>
<h2 id="Text-Transport-Toward-Learning-Causal-Effects-of-Natural-Language"><a href="#Text-Transport-Toward-Learning-Causal-Effects-of-Natural-Language" class="headerlink" title="Text-Transport: Toward Learning Causal Effects of Natural Language"></a>Text-Transport: Toward Learning Causal Effects of Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20697">http://arxiv.org/abs/2310.20697</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/torylin/text-transport">https://github.com/torylin/text-transport</a></li>
<li>paper_authors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael</li>
<li>for: 这个论文旨在研究如何在自然语言 Setting下预测语言改变对读者反应的 causal effect。</li>
<li>methods: 这篇论文提出了 Text-Transport 方法，可以在任意文本分布下Estimate causal effects from natural language。</li>
<li>results: 研究人员通过 Empirical results and analyses 证明了 Text-Transport 的可靠性和有效性，并用其研究了社交媒体上的 hate speech 问题，发现 causal effects 在不同文本领域之间异常大。<details>
<summary>Abstract</summary>
As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.
</details>
<details>
<summary>摘要</summary>
As language technologies become more prevalent in real-world settings, it is crucial to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimating causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting—hate speech on social media—in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.Here's the translation in Traditional Chinese:当语言技术在实际设定中受到推广时，了解语言改变对读者的影响非常重要。这可以写为变化语言特征（例如情感）对文本的读者回应的 causal effect。在这篇论文中，我们介绍 Text-Transport，一种可以在任何文本分布下估算 causal effect 的方法。现有的有效 causal effect 估算方法通常需要强大的假设，这意味着从实际应用中估算有效 causal effect 的数据通常不是目标领域的代表。为了解决这问题，我们利用分布shift的概念来描述一个估算器，它可以将 causal effect 传递到不同领域，单独假设领域的强大性。我们 derive  Statistical guarantees 的不确定性，并报告了实验结果和分析，支持 Text-Transport 的有效性在数据设定中。最后，我们使用 Text-Transport 研究社交媒体上的 hate speech，显示了 causal effect 在文本领域之间传递的必要性。
</details></li>
</ul>
<hr>
<h2 id="Non-Compositionality-in-Sentiment-New-Data-and-Analyses"><a href="#Non-Compositionality-in-Sentiment-New-Data-and-Analyses" class="headerlink" title="Non-Compositionality in Sentiment: New Data and Analyses"></a>Non-Compositionality in Sentiment: New Data and Analyses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20656">http://arxiv.org/abs/2310.20656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vernadankers/noncompsst">https://github.com/vernadankers/noncompsst</a></li>
<li>paper_authors: Verna Dankers, Christopher G. Lucas</li>
<li>for: 本研究的目的是为了评估非compositional sentiment analysis（SST）方法。</li>
<li>methods: 本研究使用了一种方法来评估phrase的非compositional sentiment，即通过评估phrase中各个word的sentiment来评估phrase的整体sentiment。</li>
<li>results: 研究发现，NonCompSST资源中的phrase sentiment ratings具有较高的准确率和 repeatability，并且可以用于评估不同的SST方法的性能。<details>
<summary>Abstract</summary>
When natural language phrases are combined, their meaning is often more than the sum of their parts. In the context of NLP tasks such as sentiment analysis, where the meaning of a phrase is its sentiment, that still applies. Many NLP studies on sentiment analysis, however, focus on the fact that sentiment computations are largely compositional. We, instead, set out to obtain non-compositionality ratings for phrases with respect to their sentiment. Our contributions are as follows: a) a methodology for obtaining those non-compositionality ratings, b) a resource of ratings for 259 phrases -- NonCompSST -- along with an analysis of that resource, and c) an evaluation of computational models for sentiment analysis using this new resource.
</details>
<details>
<summary>摘要</summary>
当自然语言短语被组合时，它们的意义 oftentimes 超出它们的部件之和。在 NLP 任务中，如感受分析，phrase 的意义就是它的 sentiment。然而，许多 NLP 研究将 Sentiment Analysis 的计算视为基本 compositional。我们，然而，决定获取 phrase 的非 compositional 评分。我们的贡献包括：a) 一种方法ologies for obtaining non-compositionality ratings,b) 一个包含 259 个短语的评分资源 -- NonCompSST -- 以及对该资源的分析,c) 使用这个新资源来评估计算模型的 sentiment analysis 能力。
</details></li>
</ul>
<hr>
<h2 id="Defining-a-New-NLP-Playground"><a href="#Defining-a-New-NLP-Playground" class="headerlink" title="Defining a New NLP Playground"></a>Defining a New NLP Playground</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20633">http://arxiv.org/abs/2310.20633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li, Xingyao Wang, Yi R. Fung, Charles Yu, Joel R. Tetreault, Eduard H. Hovy, Heng Ji</li>
<li>for: 这篇论文的目的是为大型自然语言处理（LLMs）的新兴发展提出20多个博士论文价值的研究方向，以探索新的理论分析、挑战性问题、学习方法和跨领域应用。</li>
<li>methods: 这篇论文使用了各种学习方法和数据分析技术，包括深度学习、复杂系统、自然语言处理和机器学习等。</li>
<li>results: 这篇论文预期将提供20多个新的研究方向和议题，可以帮助学术研究者、特别是博士学生，在 LLMS 领域中获得新的发现和成果。<details>
<summary>Abstract</summary>
The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field's 80-year history. This has resulted in concerns that the field will become homogenized and resource-intensive. The new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms, and interdisciplinary applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Unreasonable-Effectiveness-of-Random-Target-Embeddings-for-Continuous-Output-Neural-Machine-Translation"><a href="#The-Unreasonable-Effectiveness-of-Random-Target-Embeddings-for-Continuous-Output-Neural-Machine-Translation" class="headerlink" title="The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation"></a>The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20620">http://arxiv.org/abs/2310.20620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgeniia Tokarchuk, Vlad Niculae</li>
<li>for: 这个论文是关于连续输出神经机器翻译（CoNMT）的研究，它将推理下一个单词的问题转换为嵌入Prediction问题。</li>
<li>methods: 这篇论文使用了CoNMT模型，并对其中的输出嵌入进行了不同的设计和实现。</li>
<li>results: 研究发现，完全随机的输出嵌入可以超越了努力预训练的嵌入，特别是在大型数据集上。进一步的分析表明，这个效果强REATELY关注罕见单词的嵌入几何结构。<details>
<summary>Abstract</summary>
Continuous-output neural machine translation (CoNMT) replaces the discrete next-word prediction problem with an embedding prediction. The semantic structure of the target embedding space (i.e., closeness of related words) is intuitively believed to be crucial. We challenge this assumption and show that completely random output embeddings can outperform laboriously pretrained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings. We shed further light on this finding by designing a mixed strategy that combines random and pre-trained embeddings for different tokens.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Increasing-The-Performance-of-Cognitively-Inspired-Data-Efficient-Language-Models-via-Implicit-Structure-Building"><a href="#Increasing-The-Performance-of-Cognitively-Inspired-Data-Efficient-Language-Models-via-Implicit-Structure-Building" class="headerlink" title="Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building"></a>Increasing The Performance of Cognitively Inspired Data-Efficient Language Models via Implicit Structure Building</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20589">http://arxiv.org/abs/2310.20589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Momen, David Arps, Laura Kallmeyer</li>
<li>for: 这个论文描述了作者在2023年语言模型预训练挑战任务（BabyLM Challenge 2023）中的提交（Warstadt et al., 2023）。</li>
<li>methods: 作者使用 transformer 架构的做masked language model（MLM），并在模型结构中包含不supervised的句子结构预测。具体来说，作者使用 Structformer 架构（Shen et al., 2021）和其变种。 StructFormer 模型在有限的预训练数据上进行无监督语义推导，并在 vanilla transformer 架构上显示出性能提升（Shen et al., 2021）。</li>
<li>results: 作者对共39个任务进行评估，其中一些任务上，模型通过在建筑中添加层次偏好而提供了有望的改进，尤其是在一些特定任务上。然而，模型未能在所有任务上一致地超越提供的 RoBERTa 基线模型（BabyLM Challenge 2023）。<details>
<summary>Abstract</summary>
In this paper, we describe our submission to the BabyLM Challenge 2023 shared task on data-efficient language model (LM) pretraining (Warstadt et al., 2023). We train transformer-based masked language models that incorporate unsupervised predictions about hierarchical sentence structure into the model architecture. Concretely, we use the Structformer architecture (Shen et al., 2021) and variants thereof. StructFormer models have been shown to perform well on unsupervised syntactic induction based on limited pretraining data, and to yield performance improvements over a vanilla transformer architecture (Shen et al., 2021). Evaluation of our models on 39 tasks provided by the BabyLM challenge shows promising improvements of models that integrate a hierarchical bias into the architecture at some particular tasks, even though they fail to consistently outperform the RoBERTa baseline model provided by the shared task organizers on all tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了我们对2023年度 BabyLM 挑战任务中语言模型预训练（LM）的提交（Warstadt et al., 2023）。我们使用 transformer 基于的掩码语言模型，并在模型结构中包含无监督预测对层次句子结构的prediction。具体来说，我们使用 Structformer 架构（Shen et al., 2021）和其变种。StructFormer 模型在有限的预训练数据上进行无监督 синтаксический推导，并且在 vanilla transformer 架构上表现出良好的性能提升（Shen et al., 2021）。我们对 shared task 提供的 39 个任务进行评估，发现在某些任务上，将层次偏好 integrate 到架构中可以得到一定的改进，尤其是在一些特定任务上。然而，我们的模型未能在所有任务上 consistently 超过提供的 RoBERTa 基eline模型（提供者：shared task 组织者）。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Medical-Information-Retrieval-via-Knowledge-Graph-Embedding"><a href="#Zero-Shot-Medical-Information-Retrieval-via-Knowledge-Graph-Embedding" class="headerlink" title="Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding"></a>Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20588">http://arxiv.org/abs/2310.20588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqi Wang, Zeqiang Wang, Wei Wang, Qi Chen, Kaizhu Huang, Anh Nguyen, Suparna De</li>
<li>for: 本研究旨在提出一种适用于零 shot 医疗信息检索（MIR）的新方法，以便在互联网时代的医疗决策中提高效率。</li>
<li>methods: 该方法 combinesthe strengths of pre-trained语言模型和统计方法，同时解决他们的局限性。具体来说，该方法利用了预训练的 BERT-style 模型提取紧凑又有用的关键词。这些关键词然后通过与医学知识图中的概念实体联系而增强。</li>
<li>results: 对医疗数据集进行了实验评估，表明 MedFusionRank 的表现比现有方法更出色，具有多种评价指标的承诺。 MedFusionRank 能够快速和准确地检索 relevante 信息，包括从短或单个查询中检索到的信息。<details>
<summary>Abstract</summary>
In the era of the Internet of Things (IoT), the retrieval of relevant medical information has become essential for efficient clinical decision-making. This paper introduces MedFusionRank, a novel approach to zero-shot medical information retrieval (MIR) that combines the strengths of pre-trained language models and statistical methods while addressing their limitations. The proposed approach leverages a pre-trained BERT-style model to extract compact yet informative keywords. These keywords are then enriched with domain knowledge by linking them to conceptual entities within a medical knowledge graph. Experimental evaluations on medical datasets demonstrate MedFusion Rank's superior performance over existing methods, with promising results with a variety of evaluation metrics. MedFusionRank demonstrates efficacy in retrieving relevant information, even from short or single-term queries.
</details>
<details>
<summary>摘要</summary>
在互联网 OF Things（IoT）时代，医疗信息检索成为了效率医疗决策的重要组成部分。这篇论文介绍了MedFusionRank，一种新的零shot医疗信息检索（MIR）方法，该方法结合预训练的语言模型和统计方法，同时解决它们的局限性。提议的方法利用预训练的BERT样式模型提取紧凑又有用的关键词。这些关键词然后通过与医学知识图连接来增强域知识。实验评估医学数据集表明，MedFusionRank的表现胜过现有方法，具有多种评价指标的承诺性。MedFusionRank能够快速和高效地检索相关信息，即使从短或单个查询中检索。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Word-Guessing-Games-to-Assess-the-Intelligence-of-Large-Language-Models"><a href="#Leveraging-Word-Guessing-Games-to-Assess-the-Intelligence-of-Large-Language-Models" class="headerlink" title="Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models"></a>Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20499">http://arxiv.org/abs/2310.20499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Liang, Zhiwei He, Jen-tes Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, Xing Wang<br>for:  This paper aims to evaluate the intelligence of LLM-based agents using a word guessing game and a multi-agent framework called SpyGame.methods:  The proposed evaluation framework includes two components: DEEP, which requires LLMs to describe words in aggressive and conservative modes, and SpyGame, an interactive multi-agent framework that assesses LLMs’ linguistic skills and strategic thinking in a competitive language-based board game.results:  The proposed evaluation framework is effective in capturing the capabilities of various LLMs, including their ability to adapt to novel situations and engage in strategic communication. The authors conducted extensive experiments using words from multiple sources, domains, and languages.<details>
<summary>Abstract</summary>
The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy'', we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players' descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs' expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs' intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs' human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication.
</details>
<details>
<summary>摘要</summary>
自动评估LLM基于代理智能是发展高级LLM的关键。虽然有很多努力投入到了人类标注评估数据集的开发，如AlpacaEval，但现有技术是成本高、时间费时、无法适应的。在这篇论文中，我们提出使用语言游戏“谁是间谍”来评估LLM的智能表现。给定一个词，LLM会被要求描述这个词，并确定它的身份（间谍或不）基于它和其他玩家的描述。理想的高级代理应该拥有精准描述给定词的能力，同时使用谩逼的描述和保守的描述，以增强其参与度。为此，我们首先开发了DEEP来评估LLM的表达和掩饰能力。DEEP需要LLM在谩逼和保守模式下描述一个词。然后，我们引入了SpyGame，一个交互式多代理框架，用于评估LLM的智能能力。SpyGame需要目标LLM具备语言技能和战略思维，从而提供更全面的评估LLM的人类智能能力和在复杂的语言通信 Situations中的适应能力。我们所提出的评估框架非常容易实现。我们收集了多个来源、领域和语言的词汇，并使用我们所提出的评估框架进行实验。广泛的实验结果表明，我们的DEEP和SpyGame可以有效评估不同LLM的能力，捕捉它们在新 Situations中的适应能力和策略性通信能力。
</details></li>
</ul>
<hr>
<h2 id="Multi-User-MultiWOZ-Task-Oriented-Dialogues-among-Multiple-Users"><a href="#Multi-User-MultiWOZ-Task-Oriented-Dialogues-among-Multiple-Users" class="headerlink" title="Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users"></a>Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20479">http://arxiv.org/abs/2310.20479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohan Jo, Xinyan Zhao, Arijit Biswas, Nikoletta Basiou, Vincent Auvray, Nikolaos Malandrakis, Angeliki Metallinou, Alexandros Potamianos</li>
<li>for: 这个论文是为了提高多用户对话系统的开发而设计的。</li>
<li>methods: 这个论文使用了基于 MultiWOZ 2.2 的多用户对话数据，并提出了一种多用户上下文重写任务，以提高对话状态跟踪和对话系统的扩展。</li>
<li>results: 研究表明，使用预测重写可以在多用户对话中substantially提高对话状态跟踪，而不需要修改现有的对话系统。此外，这种方法还可以在未看过的领域中进行扩展。<details>
<summary>Abstract</summary>
While most task-oriented dialogues assume conversations between the agent and one user at a time, dialogue systems are increasingly expected to communicate with multiple users simultaneously who make decisions collaboratively. To facilitate development of such systems, we release the Multi-User MultiWOZ dataset: task-oriented dialogues among two users and one agent. To collect this dataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat between two users that is semantically and pragmatically consistent with the original user utterance, thus resulting in the same dialogue state and system response. These dialogues reflect interesting dynamics of collaborative decision-making in task-oriented scenarios, e.g., social chatter and deliberation. Supported by this data, we propose the novel task of multi-user contextual query rewriting: to rewrite a task-oriented chat between two users as a concise task-oriented query that retains only task-relevant information and that is directly consumable by the dialogue system. We demonstrate that in multi-user dialogues, using predicted rewrites substantially improves dialogue state tracking without modifying existing dialogue systems that are trained for single-user dialogues. Further, this method surpasses training a medium-sized model directly on multi-user dialogues and generalizes to unseen domains.
</details>
<details>
<summary>摘要</summary>
而 Dialogue 系统却逐渐需要与多个用户同时交互，以便协作做出决策。为了推动这种系统的开发，我们发布了多用户 MultiWOZ 数据集：任务听力对话中两名用户和一个代理人进行交互。为了收集这个数据集，我们将 MultiWOZ 2.2 中每个用户说话的内容替换为两名用户之间的小聊天，保持semantically和pragmatically与原始用户说话相符，因此在对话状态和系统响应方面保持一致。这些对话反映了多用户协作任务决策场景中的社交交流和讨论。基于这些数据，我们提出了一项新任务：多用户上下文ual查询 rewrite。这项任务的目标是将多用户任务听力对话转换成简洁的任务听力查询，保留只有任务相关信息，并且可以直接被对话系统所接受。我们证明了，在多用户对话中使用预测 rewrite 可以大幅提高对话状态跟踪，而不需要修改现有的对话系统，这些系统已经用于单个用户对话。此外，这种方法在不同领域中 generalizes 并且超过了直接在多用户对话中训练一个中等大小的模型。
</details></li>
</ul>
<hr>
<h2 id="Representativeness-as-a-Forgotten-Lesson-for-Multilingual-and-Code-switched-Data-Collection-and-Preparation"><a href="#Representativeness-as-a-Forgotten-Lesson-for-Multilingual-and-Code-switched-Data-Collection-and-Preparation" class="headerlink" title="Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation"></a>Representativeness as a Forgotten Lesson for Multilingual and Code-switched Data Collection and Preparation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20470">http://arxiv.org/abs/2310.20470</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Seza Doğruöz, Sunayana Sitaram, Zheng-Xin Yong</li>
<li>for: 这个论文的目的是调查现有的code-switching数据集（68个）中语言对的不均衡问题，以及数据采集和准备阶段的缺陷。</li>
<li>methods: 该研究采用了一种深入的分析方法，检查了不同语言对的数据集是否具备代表性，并发现了许多数据集忽略了地域、社会人口和注册变化的因素。</li>
<li>results: 研究发现，大多数CSW数据集忽略了其他语言对，并且存在数据采集和准备阶段的缺陷，导致数据集的代表性受到影响。<details>
<summary>Abstract</summary>
Multilingualism is widespread around the world and code-switching (CSW) is a common practice among different language pairs/tuples across locations and regions. However, there is still not much progress in building successful CSW systems, despite the recent advances in Massive Multilingual Language Models (MMLMs). We investigate the reasons behind this setback through a critical study about the existing CSW data sets (68) across language pairs in terms of the collection and preparation (e.g. transcription and annotation) stages. This in-depth analysis reveals that \textbf{a)} most CSW data involves English ignoring other language pairs/tuples \textbf{b)} there are flaws in terms of representativeness in data collection and preparation stages due to ignoring the location based, socio-demographic and register variation in CSW. In addition, lack of clarity on the data selection and filtering stages shadow the representativeness of CSW data sets. We conclude by providing a short check-list to improve the representativeness for forthcoming studies involving CSW data collection and preparation.
</details>
<details>
<summary>摘要</summary>
多语言主义在世界范围内广泛存在，并且代码转换（CSW）是不同语言对的常见实践。然而，建设成功的CSW系统还没有做出太多进步，尽管最近的质量大量多语言语言模型（MMLM）在提高。我们通过对现有CSW数据集（68）的抽查和检查来研究这一问题的原因。我们发现：a）大多数CSW数据集中英语占主导地位，忽视其他语言对。b）在收集和准备阶段存在不准确的表现， Ignoring location基础、社会民主和注册变化。此外，数据选择和筛选阶段的不清晰性使CSW数据集的表现性受到影响。我们 conclude by提供一份简短的检查列表，以提高将来CSW数据收集和准备阶段的表现性。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Deep-Understanding-of-Multilingual-End-to-End-Speech-Translation"><a href="#Towards-a-Deep-Understanding-of-Multilingual-End-to-End-Speech-Translation" class="headerlink" title="Towards a Deep Understanding of Multilingual End-to-End Speech Translation"></a>Towards a Deep Understanding of Multilingual End-to-End Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20456">http://arxiv.org/abs/2310.20456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Sun, Xiaohu Zhao, Yikun Lei, Shaolin Zhu, Deyi Xiong</li>
<li>for: 这个论文旨在分析一个多语言端到端语音翻译模型中所学习的表示，使用Singular Value Canonical Correlation Analysis (SVCCA)来掌握不同语言和层次之间的表示相似性。</li>
<li>methods: 这个论文使用了CoVoST 2 dataset中的所有可能的方向进行训练，并使用LASER提取并行的文本数据进行SVCCA分析。</li>
<li>results: 从分析中得到了三个主要发现：（I）在多语言语音翻译中，语言相似性在训练数据有限时失效；（II）在训练数据不受限制时，提高encoder表示和audio-text数据的对应性可以提高翻译质量，超过双语翻译；（III）多语言语音翻译encoder表示在语音学类型预测中表现出色。这些发现可以提议在多语言端到端语音翻译中释放限制了数据的约束，并将限制的语言与语言相似性高的高资源语言结合起来，以更有效地进行多语言端到端语音翻译。<details>
<summary>Abstract</summary>
In this paper, we employ Singular Value Canonical Correlation Analysis (SVCCA) to analyze representations learnt in a multilingual end-to-end speech translation model trained over 22 languages. SVCCA enables us to estimate representational similarity across languages and layers, enhancing our understanding of the functionality of multilingual speech translation and its potential connection to multilingual neural machine translation. The multilingual speech translation model is trained on the CoVoST 2 dataset in all possible directions, and we utilize LASER to extract parallel bitext data for SVCCA analysis. We derive three major findings from our analysis: (I) Linguistic similarity loses its efficacy in multilingual speech translation when the training data for a specific language is limited. (II) Enhanced encoder representations and well-aligned audio-text data significantly improve translation quality, surpassing the bilingual counterparts when the training data is not compromised. (III) The encoder representations of multilingual speech translation demonstrate superior performance in predicting phonetic features in linguistic typology prediction. With these findings, we propose that releasing the constraint of limited data for low-resource languages and subsequently combining them with linguistically related high-resource languages could offer a more effective approach for multilingual end-to-end speech translation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们利用协方差值幂分析（SVCCA）分析在22种语言的多语言端到端语音翻译模型中学习的表示。SVCCA使我们能够计算语言之间和层次之间的表示相似性，从而更好地理解多语言语音翻译的工作原理和可能与多语言神经机器翻译的联系。我们使用CoVoST 2 dataset进行全向训练，并使用LASER提取并行文本数据进行SVCCA分析。我们从分析中得到了以下三个主要发现：（I）在多语言语音翻译中，语言相似性的作用随着语言训练数据的减少而减弱。（II）在训练数据不受限制的情况下，增强encoder表示和与文本数据well-aligned可以大幅提高翻译质量，超过双语翻译器。（III）多语言语音翻译encoder表示在语言类型预测中表现出色，可以更好地预测语音特征。基于这些发现，我们提议在低资源语言的训练数据不受限制的情况下，将其与语言相似性高的高资源语言结合起来，可以实现更有效的多语言端到端语音翻译。
</details></li>
</ul>
<hr>
<h2 id="The-SourceData-NLP-dataset-integrating-curation-into-scientific-publishing-for-training-large-language-models"><a href="#The-SourceData-NLP-dataset-integrating-curation-into-scientific-publishing-for-training-large-language-models" class="headerlink" title="The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models"></a>The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20440">http://arxiv.org/abs/2310.20440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Abreu-Vicente, Hannah Sonntag, Thomas Eidens, Thomas Lemberger<br>for:* The paper is written to present a new dataset called SourceData-NLP, which is annotated with biomedical entities in figure legends.methods:* The authors use natural language processing (NLP) techniques, specifically named-entity recognition (NER) and named-entity linking (NEL), to annotate the biomedical entities in the dataset.results:* The SourceData-NLP dataset contains over 620,000 annotated biomedical entities from 18,689 figures in 3,223 papers in molecular and cell biology.* The authors fine-tune two transformer-based models, BioLinkBERT and PubmedBERT, on the SourceData-NLP dataset to assess their performance for NER.* The authors introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.<details>
<summary>Abstract</summary>
Introduction: The scientific publishing landscape is expanding rapidly, creating challenges for researchers to stay up-to-date with the evolution of the literature. Natural Language Processing (NLP) has emerged as a potent approach to automating knowledge extraction from this vast amount of publications and preprints. Tasks such as Named-Entity Recognition (NER) and Named-Entity Linking (NEL), in conjunction with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.   Results: We present the SourceData-NLP dataset produced through the routine curation of papers during the publication process. A unique feature of this dataset is its emphasis on the annotation of bioentities in figure legends. We annotate eight classes of biomedical entities (small molecules, gene products, subcellular components, cell lines, cell types, tissues, organisms, and diseases), their role in the experimental design, and the nature of the experimental method as an additional class. SourceData-NLP contains more than 620,000 annotated biomedical entities, curated from 18,689 figures in 3,223 papers in molecular and cell biology. We illustrate the dataset's usefulness by assessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned on the SourceData-NLP dataset for NER. We also introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.   Conclusions: SourceData-NLP's scale highlights the value of integrating curation into publishing. Models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.
</details>
<details>
<summary>摘要</summary>
引言：科学出版业在快速扩大，让研究人员很难以保持学术文献的更新。自然语言处理（NLP）已经成为自动提取知识的有效方法之一。任务 such as 命名实体识别（NER）和命名实体关联（NEL），以及与上下文相依的Semantic interpretation，可以帮助提取结构化信息并揭示关键概念。结果：我们提供了来自文献审核过程中的 SourceData-NLP 数据集。这个数据集的特点是强调在图文中注解生物实体。我们注解了8种生物实体类型（小分子、蛋白质、细胞组成部分、细胞系列、细胞类型、组织、生物体和疾病），它们在实验设计中的角色以及实验方法的性质作为一个额外的类别。 SourceData-NLP 包含超过 620,000 个注解的生物实体，从 18,689 张图文中精心审核。我们通过评估 BioLinkBERT 和 PubmedBERT，两个基于 transformers 的模型，在 SourceData-NLP 数据集上进行了 NER 的训练，并引入了一种新的上下文依存Semantic任务，该任务检测实体是否为控制 intervención 的目标或测量的对象。结论：SourceData-NLP 的规模强调了在出版过程中 integrating 审核的重要性。模型在 SourceData-NLP 数据集上训练后，将能够提取文献中的 causal 假设并将其组织成知识图。
</details></li>
</ul>
<hr>
<h2 id="FollowBench-A-Multi-level-Fine-grained-Constraints-Following-Benchmark-for-Large-Language-Models"><a href="#FollowBench-A-Multi-level-Fine-grained-Constraints-Following-Benchmark-for-Large-Language-Models" class="headerlink" title="FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models"></a>FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20410">http://arxiv.org/abs/2310.20410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang</li>
<li>For: 本研究为了评估大型自然语言模型（LLMs）的 instrucion following 能力，提出了 FollowBench 多级细化约束遵循 benchmark。* Methods: 本研究使用了五种不同类型的细化约束（内容、场景、风格、格式和示例），并引入了多级机制来逐步添加约束。* Results: 通过测试九种关键性开源和商业 LLMs 在 FollowBench 上的性能，研究发现了 LLMs 在 instrucion following 方面存在弱点，并指出了未来研究的可能性。I hope this helps!<details>
<summary>Abstract</summary>
The ability to follow instructions is crucial to Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating superficial response quality, which does not necessarily indicate instruction-following capability. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Scenario, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each level. To evaluate whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint evolution paths to handle challenging semantic constraints. By evaluating nine closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.
</details>
<details>
<summary>摘要</summary>
LLMs 需要能够遵循指令是在实际应用中的关键能力。现有的标准准 mesure 主要关注表面上的响应质量，这并不一定是指令遵循能力。为了填补这个研究空白，在这篇论文中，我们提出了 FollowBench，一个多级细化要求 benchmark  для LLMs。FollowBench 包括了五种不同类型（即内容、情境、风格、格式和示例）的细化要求。为了准确地测量 LLMs 的指令遵循能力，我们引入了一种多级机制，逐级添加一个单独的指令要求到初始指令中。为了评估 LLMs 的输出是否满足每个个体要求，我们提议在 Constraint Evolution Path 上引入强大 LLMs 的输入。通过对九个关键 LLMs 在 FollowBench 上进行评估，我们揭示了 LLMs 在指令遵循方面的缺陷，并指向了未来研究的可能性。数据和代码在 GitHub 上公开可用，请参考 https://github.com/YJiangcm/FollowBench。
</details></li>
</ul>
<hr>
<h2 id="AMERICANO-Argument-Generation-with-Discourse-driven-Decomposition-and-Agent-Interaction"><a href="#AMERICANO-Argument-Generation-with-Discourse-driven-Decomposition-and-Agent-Interaction" class="headerlink" title="AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction"></a>AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20352">http://arxiv.org/abs/2310.20352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Hu, Hou Pong Chan, Yu Yin</li>
<li>for: 这 paper 的目的是提出一种基于对话的 argue generation 框架，以便更好地模拟人类的写作过程和提高语言模型的生成效果。</li>
<li>methods: 这 paper 使用了一种基于 argumentation 理论的分解方法，将生成过程分解为多个顺序执行的动作，首先生成了不同类型的论证组成部分，然后生成了基于这些部分的最终论证。此外，它还引入了一种自动评估和改进Argument drafts的Module，以便更好地模拟人类的写作过程。</li>
<li>results:  experiments 表明，该方法可以比 tradicional end-to-end 和 chain-of-thought prompting 方法更好地生成更 coherent 和更有力的论证，并且可以生成更多的多样化和富有的内容。<details>
<summary>Abstract</summary>
Argument generation is a challenging task in natural language processing, which requires rigorous reasoning and proper content organization. Inspired by recent chain-of-thought prompting that breaks down a complex task into intermediate steps, we propose Americano, a novel framework with agent interaction for argument generation. Our approach decomposes the generation process into sequential actions grounded on argumentation theory, which first executes actions sequentially to generate argumentative discourse components, and then produces a final argument conditioned on the components. To further mimic the human writing process and improve the left-to-right generation paradigm of current autoregressive language models, we introduce an argument refinement module which automatically evaluates and refines argument drafts based on feedback received. We evaluate our framework on the task of counterargument generation using a subset of Reddit/CMV dataset. The results show that our method outperforms both end-to-end and chain-of-thought prompting methods and can generate more coherent and persuasive arguments with diverse and rich contents.
</details>
<details>
<summary>摘要</summary>
Argument generation是自然语言处理中一项复杂的任务，需要严格的逻辑推理和正确的内容组织。启发于最近的链条思维提示，我们提出了Americano框架，一种基于代理交互的新方法。我们的方法将生成过程分解为顺序执行的Argumentation theory基础上的动作，先执行动作生成论证说话组件，然后生成基于组件的最终论证。为了更好地模拟人类写作过程和改进现有的左至右生成方法，我们引入了一个 argue refinement module，该模块会自动评估和修正Argument drafts基于反馈。我们使用Reddit/CMV数据集中的一个子集进行评估，结果显示，我们的方法在end-to-end和链条思维提示方法的基础上减少了更多的coherent和有说服力的论证，并且可以生成更多的多样化和 ric contents。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Generators-for-a-Family-of-Matrix-Multiplication-Routines-with-Apache-TVM"><a href="#Automatic-Generators-for-a-Family-of-Matrix-Multiplication-Routines-with-Apache-TVM" class="headerlink" title="Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM"></a>Automatic Generators for a Family of Matrix Multiplication Routines with Apache TVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20347">http://arxiv.org/abs/2310.20347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Alaejos, Adrián Castelló, Pedro Alonso-Jordá, Francisco D. Igual, Héctor Martínez, Enrique S. Quintana-Ortí</li>
<li>for: 本研究旨在使用Apache TVM开源框架自动生成高性能的矩阵乘法（GEMM）算法家族，以满足高性能Linear Algebra库（如GotoBLAS2、BLIS和OpenBLAS）的要求。</li>
<li>methods: 本研究使用Apache TVM框架来自动生成矩阵乘法（GEMM）的块分形式算法和处理器特定微kernels，而不是手动编码Assembly代码来实现。</li>
<li>results: 结果表明，使用TVM生成的块分形式GEMM算法和微kernels可以提高可移植性、维护性和软件生命周期的可控性，同时提供高度的自定义和优化功能，以便适应不同的数据类型、处理器架构和矩阵操作形态，并且具有较小的内存占用。<details>
<summary>Abstract</summary>
We explore the utilization of the Apache TVM open source framework to automatically generate a family of algorithms that follow the approach taken by popular linear algebra libraries, such as GotoBLAS2, BLIS and OpenBLAS, in order to obtain high-performance blocked formulations of the general matrix multiplication (GEMM). % In addition, we fully automatize the generation process, by also leveraging the Apache TVM framework to derive a complete variety of the processor-specific micro-kernels for GEMM. This is in contrast with the convention in high performance libraries, which hand-encode a single micro-kernel per architecture using Assembly code. % In global, the combination of our TVM-generated blocked algorithms and micro-kernels for GEMM 1)~improves portability, maintainability and, globally, streamlines the software life cycle; 2)~provides high flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, yielding performance on a par (or even superior for specific matrix shapes) with that of hand-tuned libraries; and 3)~features a small memory footprint.
</details>
<details>
<summary>摘要</summary>
The combination of our TVM-generated blocked algorithms and micro-kernels for GEMM provides several benefits:1. Improved portability, maintainability, and streamlined software development life cycle.2. High flexibility to easily tailor and optimize the solution to different data types, processor architectures, and matrix operand shapes, resulting in performance on par (or even superior for specific matrix shapes) with that of hand-tuned libraries.3. Small memory footprint.
</details></li>
</ul>
<hr>
<h2 id="InstructCoder-Empowering-Language-Models-for-Code-Editing"><a href="#InstructCoder-Empowering-Language-Models-for-Code-Editing" class="headerlink" title="InstructCoder: Empowering Language Models for Code Editing"></a>InstructCoder: Empowering Language Models for Code Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20329">http://arxiv.org/abs/2310.20329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qishenghu/CodeInstruct">https://github.com/qishenghu/CodeInstruct</a></li>
<li>paper_authors: Qisheng Hu, Kaixin Li, Xu Zhao, Yuxi Xie, Tiedong Liu, Hui Chen, Qizhe Xie, Junxian He<br>for:这个论文的目的是探讨用大语言模型（LLMs）编辑代码的可能性，以满足开发者日常面临的各种实际任务需求。methods:这篇论文使用了大量的自然语言处理技术，包括语言模型和自然语言处理算法，以实现代码编辑。具体来说，作者们使用了 GitHub 提交记录作为种子任务，然后通过 ChatGPT 进行多轮扩展和训练，以生成更多的代码编辑任务。results:研究表明，通过对 InstructCoder 集合进行特定任务定制，可以使用 open-source LLMs 实现高效的代码编辑。实验结果表明，这些 LLMs 可以根据用户的指令正确地编辑代码，并且表现出了很好的编辑能力。这些结果表明，可以通过特定任务的指定来提高 LLMs 的代码编辑能力。<details>
<summary>Abstract</summary>
Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our experiments demonstrate that open-source LLMs fine-tuned on InstructCoder can edit code correctly based on users' instructions most of the time, exhibiting unprecedented code-editing performance levels. Such results suggest that proficient instruction-finetuning can lead to significant amelioration in code editing abilities. The dataset and the source code are available at https://github.com/qishenghu/CodeInstruct.
</details>
<details>
<summary>摘要</summary>
��лекс文本中的编程任务非常多样化，开发者日常面临着这些任务。尽管编程任务的实用性非常高，但是自动编程任务还是深度学习模型的发展中的一个未曾探索的领域，一个原因是数据的缺乏。在这项工作中，我们探索了使用大型自然语言模型（LLMs）来基于用户的指令进行代码编辑，包括评注插入、代码优化和代码重构等多种隐式任务。为了实现这一点，我们介绍了 InstructCoder，第一个适用于普通代码编辑的大型自然语言模型 dataset，包含高多样性的代码编辑任务。它包含了超过114,000个指令-输入-输出 triplets，覆盖了多个不同的代码编辑场景。我们通过一种系统的扩展过程，从 GitHub 提交中抽取代码编辑数据作为种子任务，然后使用这些种子和生成的任务来驱动 ChatGPT 生成更多的任务数据。我们的实验结果表明，使用 InstructCoder 进行特定任务的 instrucion-finetuning 可以使得基于用户的指令进行代码编辑，达到了前所未有的代码编辑性能水平。这些结果表明，高效的指令 fine-tuning 可以导致代码编辑技能的显著提高。 dataset 和源代码可以在 https://github.com/qishenghu/CodeInstruct 上下载。
</details></li>
</ul>
<hr>
<h2 id="ChiSCor-A-Corpus-of-Freely-Told-Fantasy-Stories-by-Dutch-Children-for-Computational-Linguistics-and-Cognitive-Science"><a href="#ChiSCor-A-Corpus-of-Freely-Told-Fantasy-Stories-by-Dutch-Children-for-Computational-Linguistics-and-Cognitive-Science" class="headerlink" title="ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science"></a>ChiSCor: A Corpus of Freely Told Fantasy Stories by Dutch Children for Computational Linguistics and Cognitive Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20328">http://arxiv.org/abs/2310.20328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bram M. A. van Dijk, Max J. van Duijn, Suzan Verberne, Marco R. Spruit</li>
<li>for: 这个论文是为了研究儿童如何表达角色的视角，以及语言和认知的发展。</li>
<li>methods: 这个论文使用了计算机工具，并使用自由告密的方式收集了442名荷兰儿童在4-12岁之间自由地告诉的619则幻想故事。</li>
<li>results: 这个研究发现，儿童的故事语法复杂性随着年龄的增长而变化不大，这表明儿童在语言使用中的语法能力快速发展。此外，研究还发现，儿童的故事语言遵循Zipf的法律，这反映了儿童在社会上的语言环境。最后，研究还发现，尽管这个论文的数据集较小，但它仍然能够训练有用的语言模型，用于分析儿童的语言使用。<details>
<summary>Abstract</summary>
In this resource paper we release ChiSCor, a new corpus containing 619 fantasy stories, told freely by 442 Dutch children aged 4-12. ChiSCor was compiled for studying how children render character perspectives, and unravelling language and cognition in development, with computational tools. Unlike existing resources, ChiSCor's stories were produced in natural contexts, in line with recent calls for more ecologically valid datasets. ChiSCor hosts text, audio, and annotations for character complexity and linguistic complexity. Additional metadata (e.g. education of caregivers) is available for one third of the Dutch children. ChiSCor also includes a small set of 62 English stories. This paper details how ChiSCor was compiled and shows its potential for future work with three brief case studies: i) we show that the syntactic complexity of stories is strikingly stable across children's ages; ii) we extend work on Zipfian distributions in free speech and show that ChiSCor obeys Zipf's law closely, reflecting its social context; iii) we show that even though ChiSCor is relatively small, the corpus is rich enough to train informative lemma vectors that allow us to analyse children's language use. We end with a reflection on the value of narrative datasets in computational linguistics.
</details>
<details>
<summary>摘要</summary>
在这份资源论文中，我们发布了新的虚构语料库---ChiSCor，包含619个幻想故事，由442名荷兰儿童 aged 4-12年old所自由地告诉。ChiSCor是为了研究儿童如何表达角色点视角，并探索语言和认知的发展，使用计算工具而编制的。与现有资源不同的是，ChiSCor的故事被生成在自然的上下文中，与最近的呼吁更多的生态学有效的数据集相符。ChiSCor包含文本、音频和注释，以及角色复杂性和语言复杂性的批注。一 third的荷兰儿童的教育背景信息也可以获取。ChiSCor还包括62个英语故事。这篇论文介绍了如何编制ChiSCor，并通过三个简要的案例研究表明了其潜在的应用前景：一、儿童的故事 sintactic complexity 随着年龄的变化呈现稳定的趋势；二、ChiSCor遵循Zipf's law  closely，归因其社会上下文；三、即使ChiSCor较小，仍可以用于训练有用的 lemma vector，以分析儿童语言使用。我们结束于计算语言学中虚构语料库的价值。
</details></li>
</ul>
<hr>
<h2 id="Erato-Automatizing-Poetry-Evaluation"><a href="#Erato-Automatizing-Poetry-Evaluation" class="headerlink" title="Erato: Automatizing Poetry Evaluation"></a>Erato: Automatizing Poetry Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20326">http://arxiv.org/abs/2310.20326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manexagirrezabal/erato">https://github.com/manexagirrezabal/erato</a></li>
<li>paper_authors: Manex Agirrezabal, Hugo Gonçalo Oliveira, Aitor Ormazabal</li>
<li>for: 这篇论文是为了自动评估诗歌而设计的框架。</li>
<li>methods: 这篇论文使用了多种特征来评价诗歌，并提供了这些特征的简要概述。</li>
<li>results: 通过使用Erato框架，可以准确地区分人工创作的诗歌和自动生成的诗歌。<details>
<summary>Abstract</summary>
We present Erato, a framework designed to facilitate the automated evaluation of poetry, including that generated by poetry generation systems. Our framework employs a diverse set of features, and we offer a brief overview of Erato's capabilities and its potential for expansion. Using Erato, we compare and contrast human-authored poetry with automatically-generated poetry, demonstrating its effectiveness in identifying key differences. Our implementation code and software are freely available under the GNU GPLv3 license.
</details>
<details>
<summary>摘要</summary>
我们介绍Erato框架，这是一个用于自动评估诗歌的框架，包括由诗歌生成系统生成的诗歌。我们的框架使用多种特征，我们将简要介绍Erato的能力和扩展可能性。使用Erato，我们比较和比较人工创作的诗歌和自动生成的诗歌，展示其关键区别。我们的实现代码和软件在GNU GPLv3许可证下提供免费使用。
</details></li>
</ul>
<hr>
<h2 id="FA-Team-at-the-NTCIR-17-UFO-Task"><a href="#FA-Team-at-the-NTCIR-17-UFO-Task" class="headerlink" title="FA Team at the NTCIR-17 UFO Task"></a>FA Team at the NTCIR-17 UFO Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20322">http://arxiv.org/abs/2310.20322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuki Okumura, Masato Fujitake</li>
<li>for: 本研究参与了NTCIR-17会议上的表格数据EXTRACTION（TDE）和文本到表格关系EXTRACTION（TTRE）任务，报告我们的解决方案以及官方结果。</li>
<li>methods: 我们采用了基于ELECTRA语言模型的多种提升技术来提取表格中的有价值数据。</li>
<li>results: 我们的努力取得了93.43%的TDE准确率，在排名榜上名列第二，这是我们提出的方法的效果的证明。<details>
<summary>Abstract</summary>
The FA team participated in the Table Data Extraction (TDE) and Text-to-Table Relationship Extraction (TTRE) tasks of the NTCIR-17 Understanding of Non-Financial Objects in Financial Reports (UFO). This paper reports our approach to solving the problems and discusses the official results. We successfully utilized various enhancement techniques based on the ELECTRA language model to extract valuable data from tables. Our efforts resulted in an impressive TDE accuracy rate of 93.43 %, positioning us in second place on the Leaderboard rankings. This outstanding achievement is a testament to our proposed approach's effectiveness. In the TTRE task, we proposed the rule-based method to extract meaningful relationships between the text and tables task and confirmed the performance.
</details>
<details>
<summary>摘要</summary>
FA团队参与了NTCIR-17年度理解非财务对象（UFO）的表格数据提取（TDE）和文本到表格关系提取（TTRE）任务。本文描述了我们解决这些问题的方法，并讨论官方结果。我们成功地运用了基于ELECTRA语言模型的各种优化技术，从表格中提取有价值数据。我们的努力得到了93.43%的TDE准确率，在排名榜上名列第二位。这一成就是我们提posed方法的有效性的证明。在TTRE任务中，我们提议了基于规则的方法来提取文本和表格之间的有意义关系，并证明了其性能。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Entities-of-Interest-from-Comparative-Product-Reviews"><a href="#Extracting-Entities-of-Interest-from-Comparative-Product-Reviews" class="headerlink" title="Extracting Entities of Interest from Comparative Product Reviews"></a>Extracting Entities of Interest from Comparative Product Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20274">http://arxiv.org/abs/2310.20274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jatinarora2702/Review-Information-Extraction">https://github.com/jatinarora2702/Review-Information-Extraction</a></li>
<li>paper_authors: Jatin Arora, Sumit Agrawal, Pawan Goyal, Sayan Pathak</li>
<li>for: 本研究使用深度学习方法提取在多个电子商务网站上的用户评价中的产品比较信息。</li>
<li>methods: 本研究使用LSTM网络模型来捕捉用户评价中的产品名称、用户意见（预测）以及比较的特征或方面之间的依赖关系。</li>
<li>results: 对于现有的手动标注数据集，本研究的系统表现出比现有的Semantic Role Labeling（SRL）框架更好的性能。<details>
<summary>Abstract</summary>
This paper presents a deep learning based approach to extract product comparison information out of user reviews on various e-commerce websites. Any comparative product review has three major entities of information: the names of the products being compared, the user opinion (predicate) and the feature or aspect under comparison. All these informing entities are dependent on each other and bound by the rules of the language, in the review. We observe that their inter-dependencies can be captured well using LSTMs. We evaluate our system on existing manually labeled datasets and observe out-performance over the existing Semantic Role Labeling (SRL) framework popular for this task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Play-Chess-from-Textbooks-LEAP-a-Corpus-for-Evaluating-Chess-Moves-based-on-Sentiment-Analysis"><a href="#Learning-to-Play-Chess-from-Textbooks-LEAP-a-Corpus-for-Evaluating-Chess-Moves-based-on-Sentiment-Analysis" class="headerlink" title="Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis"></a>Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating Chess Moves based on Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20260">http://arxiv.org/abs/2310.20260</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/resrepos/leap">https://github.com/resrepos/leap</a></li>
<li>paper_authors: Haifa Alrdahi, Riza Batista-Navarro<br>for: 这篇论文旨在探讨使用棋盘游戏教学的知识来帮助机器学习棋盘游戏。methods: 这篇论文使用了各种基于转换器的基eline模型进行情感分析，以评估棋盘游戏中的搬砖意见。results: 研究发现，使用转换器基eline模型进行情感分析可以评估棋盘游戏中的搬砖意见，最高performing模型在Weighted Micro F_1分数上达到68%。此外，研究还将LEAP corpus合并成一个更大的数据集，可以用于解决棋盘游戏领域的文本资源匮乏问题。<details>
<summary>Abstract</summary>
Learning chess strategies has been investigated widely, with most studies focussing on learning from previous games using search algorithms. Chess textbooks encapsulate grandmaster knowledge, explain playing strategies and require a smaller search space compared to traditional chess agents. This paper examines chess textbooks as a new knowledge source for enabling machines to learn how to play chess -- a resource that has not been explored previously. We developed the LEAP corpus, a first and new heterogeneous dataset with structured (chess move notations and board states) and unstructured data (textual descriptions) collected from a chess textbook containing 1164 sentences discussing strategic moves from 91 games. We firstly labelled the sentences based on their relevance, i.e., whether they are discussing a move. Each relevant sentence was then labelled according to its sentiment towards the described move. We performed empirical experiments that assess the performance of various transformer-based baseline models for sentiment analysis. Our results demonstrate the feasibility of employing transformer-based sentiment analysis models for evaluating chess moves, with the best performing model obtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP corpus to create a larger dataset, which can be used as a solution to the limited textual resource in the chess domain.
</details>
<details>
<summary>摘要</summary>
学习国际象棋策略已经广泛研究，大多数研究都是通过搜索算法学习前一些棋盘游戏。国际象棋教程汇集了大师知识，解释棋盘策略并需要较小的搜索空间，相比传统的国际象棋机器人。这篇论文检查国际象棋教程作为新的知识来源，使机器人学习棋盘游戏。我们开发了LEAP数据集，这是一个新的、多元数据集，包括结构化数据（棋盘转移notation和棋盘状态）和无结构数据（文本描述），从国际象棋教程中收集的1164句话，涵盖91场棋盘游戏的策略。我们首先将句子按照其相关性进行标注，即是否讨论棋盘转移。每个相关句子后来进行了情感向度标注，以判断对描述的转移的看法。我们进行了实验，用多种基于转换器的基准模型进行情感分析。我们的结果表明，使用转换器基于情感分析模型可以评估国际象棋转移的可行性，最好的模型在加权微型F_1分数上获得68%的成绩。最后，我们合并了LEAP数据集，创建了更大的数据集，可以用于解决国际象棋领域的文本资源短缺问题。
</details></li>
</ul>
<hr>
<h2 id="PsyCoT-Psychological-Questionnaire-as-Powerful-Chain-of-Thought-for-Personality-Detection"><a href="#PsyCoT-Psychological-Questionnaire-as-Powerful-Chain-of-Thought-for-Personality-Detection" class="headerlink" title="PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection"></a>PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20256">http://arxiv.org/abs/2310.20256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taoyang225/psycot">https://github.com/taoyang225/psycot</a></li>
<li>paper_authors: Tao Yang, Tianyuan Shi, Fanqi Wan, Xiaojun Quan, Qifan Wang, Bingzhe Wu, Jiaxiang Wu</li>
<li>for: 这研究旨在探讨大语言模型（LLMs）在人性探测方面的潜在能力，以及如何通过 incorporating 心理测试问naire中的Item来提高LLMs的人性探测能力。</li>
<li>methods: 我们提出了一种新的人性探测方法，即PsyCoT，它通过在多turn对话中让AI助手（一个专门针对文本分析的LLM）评分心理测试问naire中的Item，以 derive 个人性倾向。</li>
<li>results: 我们的实验表明，PsyCoT可以明显提高GPT-3.5在人性探测任务中的性能和稳定性，相比标准提示方法，在两个 benchmark 数据集上平均提高F1分数4.23&#x2F;10.63点。<details>
<summary>Abstract</summary>
Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamically-Updating-Event-Representations-for-Temporal-Relation-Classification-with-Multi-category-Learning"><a href="#Dynamically-Updating-Event-Representations-for-Temporal-Relation-Classification-with-Multi-category-Learning" class="headerlink" title="Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning"></a>Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20236">http://arxiv.org/abs/2310.20236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Cheng, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi</li>
<li>for: 本研究的目的是提出一种能够处理多个 temporal link (TLINK) 类型的事件中心模型，以便在不同的时间和文档创建时间之间进行关系分类。</li>
<li>methods: 本研究使用的方法包括多任务学习和事件中心模型，以便利用整个数据集的信息。</li>
<li>results: 实验结果表明，本提案的模型在英文和日文数据集上都有更高的表现，比之前的模型和两个基eline模型。<details>
<summary>Abstract</summary>
Temporal relation classification is a pair-wise task for identifying the relation of a temporal link (TLINK) between two mentions, i.e. event, time, and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T, and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two transfer learning baselines on both the English and Japanese data.
</details>
<details>
<summary>摘要</summary>
时间关系分类是一对一任务，用于确定两个提及（TLINK）之间的时间关系，即事件、时间和文档创建时间（DCT）之间的关系。这两个限制：1）两个TLINK中共享提及不能共享信息。2）现有的模型具有独立的类别 для每个TLINK类（E2E、E2T和E2D），这会阻碍使用整个数据集。本文介绍了一个事件中心模型，可以在多个TLINK之间管理动态事件表示。我们的模型处理三种TLINK类型，使用多任务学习来利用整个数据集。实验结果表明，我们的提议在英文和日文数据上比州前一个基eline和两个基eline表现出色。
</details></li>
</ul>
<hr>
<h2 id="General-Purpose-Retrieval-Enhanced-Medical-Prediction-Model-Using-Near-Infinite-History"><a href="#General-Purpose-Retrieval-Enhanced-Medical-Prediction-Model-Using-Near-Infinite-History" class="headerlink" title="General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History"></a>General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20204">http://arxiv.org/abs/2310.20204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/starmpcc/remed">https://github.com/starmpcc/remed</a></li>
<li>paper_authors: Junu Kim, Chaeeun Shim, Bosco Seong Kyu Yang, Chami Im, Sung Yoon Lim, Han-Gil Jeong, Edward Choi</li>
<li>for: 这个研究旨在开发基于电子健康记录 (EHR) 的临床预测模型 (e.g., 死亡预测)，并且解决专家意见选择和观察窗口大小调整的问题，这些问题会让临床专家受到沉重的负担。</li>
<li>methods: 我们提出了一个叫做 Retrieval-Enhanced Medical prediction model (REMed) 的方法，这个方法可以评估无限多个临床事件，选择相关的事件，并且做出预测。这种方法可以干预专家手动选择功能和观察窗口大小的限制，并且可以实现无限制的观察窗口。</li>
<li>results: 我们透过实验证明了 REMed 的性能，在 27 个临床任务和两个独立的资料集上，REMed 比其他同时处理多个事件的建筑架构更好。此外，我们发现 REMed 的偏好与医疗专家的偏好几乎相同。我们预计我们的方法将能够快速地减少临床专家对于模型开发的手动参与。<details>
<summary>Abstract</summary>
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the development of EHR prediction models by minimizing clinicians' need for manual involvement.
</details>
<details>
<summary>摘要</summary>
通常来说，基于电子医疗记录（EHR）的临床预测模型（例如，死亡预测）的开发往往依赖于专家意见进行特征选择和观察窗口大小调整。这会负担专家和创造出开发过程中的瓶颈。我们提议了一种叫做Retrieval-Enhanced Medical prediction model（REMed），以解决这些挑战。REMed可以评估无数量的临床事件，选择相关的事件，并进行预测。这种方法可以减少专家的手动干预，并允许无限制的观察窗口。我们通过对27个临床任务和两个独立的EHR数据集进行实验，发现REMed在其他同时处理多个事件的建筑物上表现出色，并且与医疗专家的偏好相互吻合。我们预计，我们的方法将能够快速减少临床人员的手动参与度，从而快速发展EHR预测模型。
</details></li>
</ul>
<hr>
<h2 id="Video-Helpful-Multimodal-Machine-Translation"><a href="#Video-Helpful-Multimodal-Machine-Translation" class="headerlink" title="Video-Helpful Multimodal Machine Translation"></a>Video-Helpful Multimodal Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20201">http://arxiv.org/abs/2310.20201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ku-nlp/video-helpful-mmt">https://github.com/ku-nlp/video-helpful-mmt</a></li>
<li>paper_authors: Yihang Li, Shuichiro Shimizu, Chenhui Chu, Sadao Kurohashi, Wei Li</li>
<li>for: 本研究的目的是提高多modal机器翻译（MMT）的表现，使得机器翻译器能够更好地理解视频中的信息。</li>
<li>methods: 本研究使用了一个新的数据集——EVA（广泛的训练集和视频有助于评估集），以及一种新的模型——SAFA（选择注意力模型）。SAFA模型使用了两种新的方法：帧注意力损失和抽象增强。</li>
<li>results: 实验表明，视频信息和提议的方法可以提高翻译表现，而SAFA模型在EVA数据集上表现出色，与现有的MMT模型相比有所提高。<details>
<summary>Abstract</summary>
Existing multimodal machine translation (MMT) datasets consist of images and video captions or instructional video subtitles, which rarely contain linguistic ambiguity, making visual information ineffective in generating appropriate translations. Recent work has constructed an ambiguous subtitles dataset to alleviate this problem but is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA (Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation), an MMT dataset containing 852k Japanese-English (Ja-En) parallel subtitle pairs, 520k Chinese-English (Zh-En) parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA, an MMT model based on the Selective Attention model with two novel methods: Frame attention loss and Ambiguity augmentation, aiming to use videos in EVA for disambiguation fully. Experiments on EVA show that visual information and the proposed methods can boost translation performance, and our model performs significantly better than existing MMT models. The EVA dataset and the SAFA model are available at: https://github.com/ku-nlp/video-helpful-MMT.git.
</details>
<details>
<summary>摘要</summary>
现有的多模态机器翻译（MMT）数据集包含图像和视频标题或教程视频字幕，这些内容很少含语言ambiguity，使得视觉信息在生成合适翻译时变得无效。 latest work constructed an ambiguous subtitles dataset to address this problem, but it is still limited to the problem that videos do not necessarily contribute to disambiguation. We introduce EVA（Extensive training set and Video-helpful evaluation set for Ambiguous subtitles translation）， an MMT dataset containing 852k Japanese-English（Ja-En）parallel subtitle pairs, 520k Chinese-English（Zh-En）parallel subtitle pairs, and corresponding video clips collected from movies and TV episodes. In addition to the extensive training set, EVA contains a video-helpful evaluation set in which subtitles are ambiguous, and videos are guaranteed helpful for disambiguation. Furthermore, we propose SAFA（Selective Attention with Frame attention loss and Ambiguity augmentation）， an MMT model based on the Selective Attention model with two novel methods, aiming to use videos in EVA for disambiguation fully. Experiments on EVA show that visual information and the proposed methods can boost translation performance, and our model performs significantly better than existing MMT models. The EVA dataset and the SAFA model are available at: https://github.com/ku-nlp/video-helpful-MMT.git.
</details></li>
</ul>
<hr>
<h2 id="DIVKNOWQA-Assessing-the-Reasoning-Ability-of-LLMs-via-Open-Domain-Question-Answering-over-Knowledge-Base-and-Text"><a href="#DIVKNOWQA-Assessing-the-Reasoning-Ability-of-LLMs-via-Open-Domain-Question-Answering-over-Knowledge-Base-and-Text" class="headerlink" title="DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text"></a>DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20170">http://arxiv.org/abs/2310.20170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip S. Yu, Shafiq Joty, Yingbo Zhou, Semih Yavuz</li>
<li>For: The paper aims to improve the grounding of large language models (LLMs) in external knowledge by leveraging retrieval-augmented models and heterogeneous knowledge sources, such as knowledge bases and text.* Methods: The paper proposes a novel approach that combines multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval, to ground LLMs in external knowledge. The approach leverages a comprehensive dataset of two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources.* Results: The paper demonstrates the effectiveness of the proposed approach by outperforming previous approaches on the created dataset, showing its ability to address the challenges of retrieving information from structured knowledge sources and generating symbolic queries.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GAR-meets-RAG-Paradigm-for-Zero-Shot-Information-Retrieval"><a href="#GAR-meets-RAG-Paradigm-for-Zero-Shot-Information-Retrieval" class="headerlink" title="GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval"></a>GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20158">http://arxiv.org/abs/2310.20158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daman Arora, Anush Kini, Sayak Ray Chowdhury, Nagarajan Natarajan, Gaurav Sinha, Amit Sharma</li>
<li>for: 这个论文的目的是提出一种新的生成增强检索（GAR）和重写增强检索（RAG）迭代方法，以解决现有方法中的两大挑战。</li>
<li>methods: 该方法使用生成增强检索（GAR）和重写增强检索（RAG）迭代方法，iteratively 改进检索和重写阶段，以提高系统的准确率和召回率。</li>
<li>results: 该方法在零 shot检索任务上实现了新的州OF-THE-ART，在 BEIR  benchmark 上的 Recall@100 和 nDCG@10 指标上出perform 前一个最好结果，在 6 个数据集上达到 17% 的相对提升。<details>
<summary>Abstract</summary>
Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the zero-shot setting. A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision. We conduct extensive experiments on zero-shot passage retrieval benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给一个查询和一个文档集合，信息检索（IR）任务是输出一个 relevance 列表中的文档。通过结合大型自然语言模型（LLM）和嵌入式检索模型，现有的工作显示在零shot检索问题上的承袭性，即没有访问目标领域的标注数据。两种流行的方法是生成增强检索（GAR）和检索增强生成（RAG）。这两种方法的成功取决于（i）高精度检索模型，这些模型在零shot Setting 中很难实现，和（ii）高精度（重新）排名模型，这些模型通常需要良好的初始化。在这个工作中，我们提出一种 GAR-meets-RAG 复合形式，该形式可以超越现有的方法。我们的方法通过循环提高检索（via GAR）和重写（via RAG）阶段，以提高系统的回归。一个关键的设计原则是， rewrite-retrieval 阶段可以提高系统的回归，而 final 重新排名阶段可以提高系统的精度。我们在 BEIR 和 TREC-DL zero-shot 过程中文档检索标准 benchmark 上进行了广泛的实验。我们的方法在 BEIR benchmark 中创造了新的状态纪录，超越了之前的最佳结果，在 Recall@100 和 nDCG@10  метриках上，在 6 个数据集上出现了17%的相对提升。
</details></li>
</ul>
<hr>
<h2 id="Multi-Agent-Consensus-Seeking-via-Large-Language-Models"><a href="#Multi-Agent-Consensus-Seeking-via-Large-Language-Models" class="headerlink" title="Multi-Agent Consensus Seeking via Large Language Models"></a>Multi-Agent Consensus Seeking via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20151">http://arxiv.org/abs/2310.20151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huaben Chen, Wenkang Ji, Lufeng Xu, Shiyu Zhao</li>
<li>for: 这个研究的目的是研究 LLM 驱动多智能体系统中的协同决策过程，特别是多智能体之间的协商过程。</li>
<li>methods: 本研究使用了 LLM 驱动的多智能体系统来实现协同决策。在实验中，研究者使用了不同的智能体数量、智能体个性和网络拓扑来影响协商过程。</li>
<li>results: 研究发现，不同智能体数量、智能体个性和网络拓扑的影响下，LLM 驱动的多智能体系统可以尝试不同的协商策略，主要是使用平均策略来达成协同决策。此外，本研究还应用了 LLM 驱动的协同决策在多机器人汇聚任务中，显示了这种方法的潜在应用前景。<details>
<summary>Abstract</summary>
Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: westlakeintelligentrobotics.github.io/ConsensusLLM/.
</details>
<details>
<summary>摘要</summary>
多智能体系驱动 by 大语言模型（LLM）已经显示了解决复杂任务的潜力。这项工作考虑了多智能体协同作业中的基本问题——协同决策。当多个智能体合作时，我们关心如何使其达成一致。为此，这项工作研究了智能体之间的协同决策任务，智能体的状态为数字值，他们之间进行协商来达成一致值。研究发现，当没有直接指导哪种策略应该采取时，LLM驱动的智能体主要采用平均策略 для协同决策，尽管它们可能 occasionally 采用其他策略。此外，这项工作分析了智能体数量、智能体个性和网络结构对协同决策过程的影响。报告的发现可能为 LLM 驱动多智能体系统解决更复杂任务提供基础知识。此外，LLM 驱动的协同决策还应用于多机器人聚合任务，这种应用示例了 LLM 驱动智能体可以实现零shot 自主规划的多机器人协同任务。项目网站：westlakeintelligentrobotics.github.io/ConsensusLLM/.
</details></li>
</ul>
<hr>
<h2 id="DEPN-Detecting-and-Editing-Privacy-Neurons-in-Pretrained-Language-Models"><a href="#DEPN-Detecting-and-Editing-Privacy-Neurons-in-Pretrained-Language-Models" class="headerlink" title="DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models"></a>DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20138">http://arxiv.org/abs/2310.20138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, Deyi Xiong</li>
<li>for: 防止预训练语言模型中的数据泄露</li>
<li>methods: 提出了DEPN框架，包括私钥神经检测器和私钥神经补做器，以及批处理私钥神经聚合器</li>
<li>results: 实验结果表明，我们的方法可以有效减少预训练语言模型中的数据泄露，而不会影响模型性能。此外，我们还证明了模型记忆和私钥神经之间的关系，从多个角度进行了证明。<details>
<summary>Abstract</summary>
Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.
</details>
<details>
<summary>摘要</summary>
大型语言模型在训练数据大量中获取了丰富的知识和信息。这些训练数据中的数据内嵌和重复能力，在先前的研究中曝光了数据泄露的风险。为了有效减少这些风险，我们提出了DEPN框架，用于检测和修改预先训练的语言模型中的隐私神经。在DEPN中，我们提出了一个称为隐私神经检测器的新方法，用于找出具有隐私信息的神经，然后将这些检测到的隐私神经设置为零。此外，我们提出了一个隐私神经聚合器，用于批处理中减少隐私信息的泄露。实验结果显示，我们的方法可以有效和高效地减少隐私泄露，不会对模型的性能造成影响。此外，我们在多种角度进行了empirical研究，包括模型大小、训练时间、提示、隐私神经分布，证明了我们的方法的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Prompt-Tuning-with-Learned-Prompting-Layers"><a href="#Improving-Prompt-Tuning-with-Learned-Prompting-Layers" class="headerlink" title="Improving Prompt Tuning with Learned Prompting Layers"></a>Improving Prompt Tuning with Learned Prompting Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20127">http://arxiv.org/abs/2310.20127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhu, Ming Tan</li>
<li>for: 这个研究旨在提高预读模型（PTM）的适应性，并且可以在几个实验中实现更好的表现。</li>
<li>methods: 这个研究使用了一个新的框架，即选择性预读对应（SPT），它可以自动选择适当的预读层，并且可以透过一个可学的概率闸来控制预读层的选择。</li>
<li>results: 这个研究的结果显示，使用这个新的框架可以在十个标准 benchmark 数据集下进行全量和几个shot enario下实现更好的表现，并且可以与之前的现有的 PETuning 基eline 相比，具有更好的适应性和更少的可调参数。<details>
<summary>Abstract</summary>
Prompt tuning prepends a soft prompt to the input embeddings or hidden states and only optimizes the prompt to adapt pretrained models (PTMs) to downstream tasks. The previous work manually selects prompt layers which are far from optimal and failed to exploit the potential of prompt tuning. In this work, we propose a novel framework, \underline{S}elective \underline{P}rompt \underline{T}uning (SPT), that learns to select the proper prompt layers by inserting a prompt controlled by a learnable probabilistic gate at each intermediate layer. We further propose a novel bi-level optimization framework, SPT-DARTS, that can better optimize the learnable gates and improve the final prompt tuning performances of the learned prompt layer settings. We conduct extensive experiments with ten benchmark datasets under the full-data and few-shot scenarios. The results demonstrate that our SPT framework can perform better than the previous state-of-the-art PETuning baselines with comparable or fewer tunable parameters.
</details>
<details>
<summary>摘要</summary>
Prompt tuning 可以在输入嵌入或隐藏状态之前添加软提示，并且只是优化提示来使得预训练模型（PTM）适应下游任务。前一些工作手动选择提示层，这些层远离优化的选择，而且没有充分利用提示调整的潜力。在这项工作中，我们提出了一个新的框架，称为选择性提示调整（SPT），它可以通过在每个中间层插入一个可控的概率门来学习选择合适的提示层。我们还提出了一个新的两级优化框架，称为 SPT-DARTS，它可以更好地优化学习的门和提高最终提示调整的性能。我们在10个标准测试集上进行了广泛的实验，包括全数据和少量数据场景。结果表明，我们的 SPT 框架可以比前一些state-of-the-art PETuning 基elines better，并且相对较少的可调参数。
</details></li>
</ul>
<hr>
<h2 id="Ling-CL-Understanding-NLP-Models-through-Linguistic-Curricula"><a href="#Ling-CL-Understanding-NLP-Models-through-Linguistic-Curricula" class="headerlink" title="Ling-CL: Understanding NLP Models through Linguistic Curricula"></a>Ling-CL: Understanding NLP Models through Linguistic Curricula</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20121">http://arxiv.org/abs/2310.20121</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clu-uml/ling-cl">https://github.com/clu-uml/ling-cl</a></li>
<li>paper_authors: Mohamed Elgaar, Hadi Amiri</li>
<li>for: 本研究旨在开发基于数据驱动的课程，以优化NLPTasks的解决方案，并考虑语言复杂性。</li>
<li>methods: 本研究使用心理语言学和语言学习研究中的语言复杂性特征来开发数据驱动课程，并分析了多个标准NLPTasks数据集，以确定每个任务所需的语言知识和推理能力。</li>
<li>results: 本研究发现，通过分析多个标准NLPTasks数据集，可以开发出基于数据的语言复杂性课程，这些课程可以帮助模型学习NLPTasks中的语言知识和推理能力。此外，本研究还提出了一些关于金标准和公平评价在NLP领域的问题。<details>
<summary>Abstract</summary>
We employ a characterization of linguistic complexity from psycholinguistic and language acquisition research to develop data-driven curricula to understand the underlying linguistic knowledge that models learn to address NLP tasks. The novelty of our approach is in the development of linguistic curricula derived from data, existing knowledge about linguistic complexity, and model behavior during training. By analyzing several benchmark NLP datasets, our curriculum learning approaches identify sets of linguistic metrics (indices) that inform the challenges and reasoning required to address each task. Our work will inform future research in all NLP areas, allowing linguistic complexity to be considered early in the research and development process. In addition, our work prompts an examination of gold standards and fair evaluation in NLP.
</details>
<details>
<summary>摘要</summary>
Transliteration:我们使用心理语言学和语言学习研究中的语言复杂性特征来开发数据驱动的课程，以便理解模型在处理NLPT任务时学习的基础语言知识。我们的创新在于基于数据、现有的语言复杂性知识以及模型训练过程中的行为来开发语言课程。通过分析多个benchmark NLPdataset，我们的课程学习方法可以确定每个任务的语言指标（指标），这些指标将告诉我们处理每个任务所需的挑战和逻辑。我们的工作将对所有NLPT领域的研究提供指导，让语言复杂性在研究和开发过程中得到考虑。此外，我们的工作也会让人们对NLPT中的标准和公平评价进行反思。
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Better-Data-Creators"><a href="#Making-Large-Language-Models-Better-Data-Creators" class="headerlink" title="Making Large Language Models Better Data Creators"></a>Making Large Language Models Better Data Creators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20111">http://arxiv.org/abs/2310.20111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/llm-data-creation">https://github.com/microsoft/llm-data-creation</a></li>
<li>paper_authors: Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W. White, Sujay Kumar Jauhar</li>
<li>for: 这篇论文目的是提出一个统一的数据创建管线，以减少人工标注的努力，并且可以应用到许多任务中，包括传统上困难的任务。</li>
<li>methods: 这篇论文使用了一些技术来创建数据，包括使用 Label-free 的 LLMs 进行标注，以及使用对应的 prompt 进行数据生成。</li>
<li>results: 在实验中，这篇论文显示了使用这些方法可以创建出高品质的数据，并且模型训练使用这些数据后的性能与人工标注的模型相比，有17.5%的提升。这些结果有重要的实际应用，尤其是在 NLP 系统的Robustness 方面。<details>
<summary>Abstract</summary>
Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a unified data creation pipeline that requires only a single formatting example and is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. Our experiments show that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
</details></li>
</ul>
<hr>
<h2 id="Keyword-optimized-Template-Insertion-for-Clinical-Information-Extraction-via-Prompt-based-Learning"><a href="#Keyword-optimized-Template-Insertion-for-Clinical-Information-Extraction-via-Prompt-based-Learning" class="headerlink" title="Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning"></a>Keyword-optimized Template Insertion for Clinical Information Extraction via Prompt-based Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.20089">http://arxiv.org/abs/2310.20089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugenia Alleva, Isotta Landi, Leslee J Shaw, Erwin Böttinger, Thomas J Fuchs, Ipek Ensari</li>
<li>for: 这篇论文的目的是研究临床医疗自然语言处理（NLP）任务中的文本分类问题，并且考虑到现有的标注数据集罕至。</li>
<li>methods: 这篇论文使用了提示基本学习（Prompt-based learning）方法，并且开发了一个关键字最佳化的模板插入方法（Keyword-optimized template insertion method，KOTI），以便在零到几个训练例子下适应文本分类任务。</li>
<li>results: 这篇论文的结果显示，通过优化模板位置可以提高临床医疗文本分类任务中的性能，特别是在零到几个训练例子下。<details>
<summary>Abstract</summary>
Clinical note classification is a common clinical NLP task. However, annotated data-sets are scarse. Prompt-based learning has recently emerged as an effective method to adapt pre-trained models for text classification using only few training examples. A critical component of prompt design is the definition of the template (i.e. prompt text). The effect of template position, however, has been insufficiently investigated. This seems particularly important in the clinical setting, where task-relevant information is usually sparse in clinical notes. In this study we develop a keyword-optimized template insertion method (KOTI) and show how optimizing position can improve performance on several clinical tasks in a zero-shot and few-shot training setting.
</details>
<details>
<summary>摘要</summary>
临床笔记分类是一个常见的临床自然语言处理任务。然而，标注数据集很少。推荐学习最近在使用只需几个训练示例进行文本分类中得到了显著的成果。定义模板（即提示文本）是推荐学习中的关键组件。然而，模板位置的影响尚未得到充分调查。这 particualry重要在医疗设置下，因为任务相关的信息通常是临床笔记中罕见的。本研究我们开发了关键词优化模板插入方法（KOTI），并在零shot和几shot训练设置下展示了改进性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/31/cs.CL_2023_10_31/" data-id="cloimip7y00dbs4882lkn0a7b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/10/31/cs.AI_2023_10_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-10-31
        
      </div>
    </a>
  
  
    <a href="/2023/10/31/cs.LG_2023_10_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-10-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
