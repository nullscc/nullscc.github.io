
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-12-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.03905 repo_url: None paper_authors: Kareem Ahmed, Kai-Wei Chang, Guy Van den Broeck for:">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-12-06">
<meta property="og:url" content="https://nullscc.github.io/2023/12/06/cs.AI_2023_12_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.03905 repo_url: None paper_authors: Kareem Ahmed, Kai-Wei Chang, Guy Van den Broeck for:">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-12-06T12:00:00.000Z">
<meta property="article:modified_time" content="2023-12-11T10:23:49.675Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_12_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/06/cs.AI_2023_12_06/" class="article-date">
  <time datetime="2023-12-06T12:00:00.000Z" itemprop="datePublished">2023-12-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-12-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Pseudo-Semantic-Loss-for-Autoregressive-Models-with-Logical-Constraints"><a href="#A-Pseudo-Semantic-Loss-for-Autoregressive-Models-with-Logical-Constraints" class="headerlink" title="A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints"></a>A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03905">http://arxiv.org/abs/2312.03905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kareem Ahmed, Kai-Wei Chang, Guy Van den Broeck</li>
<li>for:  bridges the gap between purely symbolic and neural approaches to learning, specifically for tasks that involve autoregressive distributions such as transformers.</li>
<li>methods:  proposes a new approach to neuro-symbolic learning that involves maximizing the likelihood of a symbolic constraint w.r.t the neural networkâ€™s output distribution, using a pseudolikelihood-based approximation centered around a model sample, which is factorized and locally high-fidelity.</li>
<li>results:  greatly improves upon the base modelâ€™s ability to predict logically-consistent outputs on Sudoku and shortest-path prediction tasks, and achieves State-of-the-Art (SoTA) detoxification compared to previous approaches on the task of detoxifying large language models by disallowing a list of toxic words.<details>
<summary>Abstract</summary>
Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihood, exhibiting low entropy and KL-divergence around the model sample. We evaluate our approach on Sudoku and shortest-path prediction cast as autoregressive generation, and observe that we greatly improve upon the base model's ability to predict logically-consistent outputs. We also evaluate on the task of detoxifying large language models. Using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA detoxification compared to previous approaches.
</details>
<details>
<summary>æ‘˜è¦</summary>
More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihood, exhibiting low entropy and KL-divergence around the model sample.We evaluate our approach on Sudoku and shortest-path prediction cast as autoregressive generation, and observe that we greatly improve upon the base model's ability to predict logically-consistent outputs. We also evaluate on the task of detoxifying large language models. Using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA detoxification compared to previous approaches.Here's the Simplified Chinese translation: neur-ç¬¦å·å­¦ AI å‡å°‘äº†ç¬¦å·å­¦å’Œç¥ç»ç½‘ç»œå­¦ä¹ ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¿™ç»å¸¸éœ€è¦æœ€å¤§åŒ–ç¬¦å·çº¦æŸçš„å¯èƒ½æ€§ï¼Œå¯¹ç¥ç»ç½‘ç»œè¾“å‡ºåˆ†å¸ƒè¿›è¡Œæœ€å¤§åŒ–ã€‚è¿™äº›è¾“å‡ºåˆ†å¸ƒé€šå¸¸å‡è®¾ä¸ºå®Œå…¨å› å­åŒ–ã€‚è¿™é™åˆ¶äº†ç¬¦å·å­¦ä¹ çš„å¯ç”¨æ€§ï¼Œåªèƒ½åº”ç”¨äºæ›´è¡¨è¾¾åŠ›å¼ºçš„æ¨è®ºåˆ†å¸ƒï¼Œä¾‹å¦‚è½¬æ¢å™¨ã€‚åœ¨è¿™äº›åˆ†å¸ƒä¸‹ï¼Œè®¡ç®—çº¦æŸçš„å¯èƒ½æ€§æ˜¯ #P-hardã€‚è€Œä¸æ˜¯å°è¯•å°†çº¦æŸåº”ç”¨äºæ•´ä¸ªè¾“å‡ºåˆ†å¸ƒï¼Œæˆ‘ä»¬æè®®åœ¨æ¨¡å‹é‡‡æ ·ä¸­å¿ƒçš„æŠ½è±¡ä¸Šè¿›è¡Œçº¦æŸã€‚æ›´åŠ å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬ä¼˜åŒ–ç¬¦å·çº¦æŸçš„å¯èƒ½æ€§ï¼Œä½¿ç”¨åŸºäº Pseudolikelihood çš„æŠ½è±¡ã€‚æˆ‘ä»¬çš„æŠ½è±¡æ˜¯å¯é‡å¤çš„ï¼Œå…è®¸åœ¨å­é—®é¢˜ä¸Šé‡ç”¨è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯è®¡ç®—ç¬¦å·å­¦æŸå¤±çš„é‡è¦åŸåˆ™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æŠ½è±¡æ˜¯åœ°æ–¹çš„ã€é«˜å‡†ç¡®æ€§çš„ï¼Œåœ¨æ¨¡å‹é‡‡æ ·ä¸­å¿ƒçš„æŠ½è±¡ä¸‹ï¼ŒEntropy å’Œ KL åç§»éƒ½å¾ˆä½ã€‚æˆ‘ä»¬åœ¨ Sudoku å’ŒçŸ­è·¯é¢„æµ‹ä¸­ä½¿ç”¨ autoregressive ç”Ÿæˆï¼Œå¹¶è§‚å¯Ÿåˆ°æˆ‘ä»¬åœ¨åŸºæœ¬æ¨¡å‹çš„è¾“å‡ºä¸Šå¤§å¹…æé«˜äº†é€»è¾‘ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨ç®€å•çº¦æŸï¼Œç¦æ­¢ä½¿ç”¨æ¶æ„è¯æ±‡ï¼Œå¹¶æˆåŠŸåœ°ä½¿æ¨¡å‹çš„è¾“å‡ºé¿å…æ¶æ„ç”Ÿæˆï¼Œ achieved SoTA æ¶æ€§è¯†åˆ«æ¯”å‰æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Masked-Pruning-Approach-for-Dimensionality-Reduction-in-Communication-Efficient-Federated-Learning-Systems"><a href="#A-Masked-Pruning-Approach-for-Dimensionality-Reduction-in-Communication-Efficient-Federated-Learning-Systems" class="headerlink" title="A Masked Pruning Approach for Dimensionality Reduction in Communication-Efficient Federated Learning Systems"></a>A Masked Pruning Approach for Dimensionality Reduction in Communication-Efficient Federated Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03889">http://arxiv.org/abs/2312.03889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamir L. S. Gez, Kobi Cohen</li>
<li>for: æé«˜ Federated Learningï¼ˆFLï¼‰ç®—æ³•åœ¨å…·æœ‰é™åˆ¶é€šä¿¡èµ„æºçš„è®¾å¤‡ä¸Šçš„å¯åº”ç”¨æ€§ï¼Œä¾‹å¦‚å…·æœ‰é™åˆ¶é€šä¿¡èµ„æºçš„ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼è®¾å¤‡ã€‚</li>
<li>methods: ä½¿ç”¨æ©è”½æ³•ï¼ˆMaskingï¼‰å’ŒFLç®—æ³•ç›¸ç»“åˆï¼Œå®ç°åœ¨å¤šä¸ªèŠ‚ç‚¹ä¹‹é—´å…±äº«ä½ç»´åº¦è¡¨ç¤ºï¼Œå¹¶ä¸”å‡å°‘äº†é€šä¿¡æˆæœ¬ã€‚æ¯ä¸ªèŠ‚ç‚¹é¦–å…ˆåœ¨æœ¬åœ°è®­ç»ƒæ¨¡å‹ï¼Œç„¶åè®¡ç®—æ©è”½é¢ï¼Œå¹¶å°†æ©è”½é¢ä¼ è¾“å›æœåŠ¡å™¨è¿›è¡Œå…±è¯†ã€‚è¿™ä¸ªè¿­ä»£è¿‡ç¨‹ä½¿å¾—æ¨¡å‹å…·æœ‰æ›´é«˜çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</li>
<li>results: å¯¹æ¯” existed æ–¹æ³•ï¼ŒMPFL æ–¹æ³•å¯ä»¥å®ç°æ›´é«˜çš„å¸¦å®½ç¼©å‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒç ”ç©¶ï¼ŒMPFL æ–¹æ³•åœ¨å…·æœ‰é™åˆ¶é€šä¿¡èµ„æºçš„è®¾å¤‡ä¸Šçš„åº”ç”¨æ€§å¾—åˆ°äº†è¿›ä¸€æ­¥çš„è¯æ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå¼€æºçš„è½¯ä»¶åŒ…ï¼Œä»¥ä¾¿ç›¸å…³é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜èƒ½å¤Ÿå…è´¹ä½¿ç”¨ã€‚<details>
<summary>Abstract</summary>
Federated Learning (FL) represents a growing machine learning (ML) paradigm designed for training models across numerous nodes that retain local datasets, all without directly exchanging the underlying private data with the parameter server (PS). Its increasing popularity is attributed to notable advantages in terms of training deep neural network (DNN) models under privacy aspects and efficient utilization of communication resources. Unfortunately, DNNs suffer from high computational and communication costs, as well as memory consumption in intricate tasks. These factors restrict the applicability of FL algorithms in communication-constrained systems with limited hardware resources.   In this paper, we develop a novel algorithm that overcomes these limitations by synergistically combining a pruning-based method with the FL process, resulting in low-dimensional representations of the model with minimal communication cost, dubbed Masked Pruning over FL (MPFL). The algorithm operates by initially distributing weights to the nodes through the PS. Subsequently, each node locally trains its model and computes pruning masks. These low-dimensional masks are then transmitted back to the PS, which generates a consensus pruning mask, broadcasted back to the nodes. This iterative process enhances the robustness and stability of the masked pruning model. The generated mask is used to train the FL model, achieving significant bandwidth savings. We present an extensive experimental study demonstrating the superior performance of MPFL compared to existing methods. Additionally, we have developed an open-source software package for the benefit of researchers and developers in related fields.
</details>
<details>
<summary>æ‘˜è¦</summary>
federated learning (FL) æ˜¯ä¸€ç§åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè®­ç»ƒæ¨¡å‹çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ paradigmaï¼Œä¸ç›´æ¥åœ¨å‚æ•°æœåŠ¡å™¨ï¼ˆPSï¼‰ä¸Šäº¤æ¢æœ¬åœ°ç§äººæ•°æ®ã€‚ç”±äºFLå…·æœ‰ä¿æŠ¤éšç§å’Œé«˜æ•ˆé€šä¿¡èµ„æºçš„ä¼˜åŠ¿ï¼Œå…¶ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€åº¦åœ¨ä¸æ–­å¢é•¿ã€‚ç„¶è€Œï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­å…·æœ‰é«˜è®¡ç®—å’Œé€šä¿¡æˆæœ¬ï¼Œä»¥åŠå†…å­˜å ç”¨ç‡ï¼Œè¿™äº›å› ç´ é™åˆ¶äº†FLç®—æ³•åœ¨å…·æœ‰æœ‰é™ç¡¬ä»¶èµ„æºçš„é€šä¿¡æŸç¼šç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œå³Masked Pruning over FLï¼ˆMPFLï¼‰ï¼Œä»¥è§£å†³è¿™äº›é™åˆ¶ã€‚MPFLç®—æ³•é¦–å…ˆå°†æƒé‡åˆ†å¸ƒç»™èŠ‚ç‚¹ Ñ‡ĞµÑ€ĞµĞ· PSã€‚ç„¶åï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ¬åœ°è®­ç»ƒå…¶æ¨¡å‹ï¼Œå¹¶è®¡ç®—é®ç›¾maskã€‚è¿™äº›ä½ç»´åº¦çš„maskå°†è¢«ä¼ è¾“å›PSï¼Œç”Ÿæˆä¸€ä¸ªconsensusé®ç›¾maskï¼Œå¹¶å°†å…¶å¹¿æ’­å›èŠ‚ç‚¹ã€‚è¿™ä¸ªè¿­ä»£è¿‡ç¨‹ä¼šæé«˜é®ç›¾é®ç›¾æ¨¡å‹çš„ç¨³å®šæ€§å’Œç¨³å®šæ€§ã€‚ç”Ÿæˆçš„é®ç›¾å¯ä»¥ç”¨æ¥è®­ç»ƒFLæ¨¡å‹ï¼Œå®ç°äº†æ˜æ˜¾çš„å¸¦å®½å‰Šå‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒç ”ç©¶ï¼Œè¯æ˜MPFLçš„æ€§èƒ½superiority compared to existing methodsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå¼€æºçš„è½¯ä»¶åŒ…ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜æä¾›äº†ä¾¿åˆ©ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-The-Fairness-Impacts-of-Hardware-Selection-in-Machine-Learning"><a href="#On-The-Fairness-Impacts-of-Hardware-Selection-in-Machine-Learning" class="headerlink" title="On The Fairness Impacts of Hardware Selection in Machine Learning"></a>On The Fairness Impacts of Hardware Selection in Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03886">http://arxiv.org/abs/2312.03886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong Tran, Sara Hooker, Ferdinando Fioretto</li>
<li>for:  investigates the impact of hardware choices on the generalization properties of machine learning models, particularly in the context of ML-as-a-service platforms.</li>
<li>methods:  combines theoretical and empirical analysis to identify the factors that contribute to hardware-induced performance imbalances, and proposes a strategy for mitigating these imbalances.</li>
<li>results:  demonstrates that hardware choices can exacerbate existing disparities in model performance and fairness, and provides insights into the underlying causes of these discrepancies.<details>
<summary>Abstract</summary>
In the machine learning ecosystem, hardware selection is often regarded as a mere utility, overshadowed by the spotlight on algorithms and data. This oversight is particularly problematic in contexts like ML-as-a-service platforms, where users often lack control over the hardware used for model deployment. How does the choice of hardware impact generalization properties? This paper investigates the influence of hardware on the delicate balance between model performance and fairness. We demonstrate that hardware choices can exacerbate existing disparities, attributing these discrepancies to variations in gradient flows and loss surfaces across different demographic groups. Through both theoretical and empirical analysis, the paper not only identifies the underlying factors but also proposes an effective strategy for mitigating hardware-induced performance imbalances.
</details>
<details>
<summary>æ‘˜è¦</summary>
Note:* ç¡¬ä»¶ (hÃ²u jiÃ n) means "hardware" in Simplified Chinese.* ML-as-a-service (MLaaS) is a cloud-based service that provides machine learning capabilities to users.* ç”¨æˆ· (yÃ²ng yÃ²u) means "user" in Simplified Chinese.* æ¨¡å‹ (mÃ³ delÃ¬) means "model" in Simplified Chinese.* æ€§èƒ½ (xÃ¬ng nÃ©ng) means "performance" in Simplified Chinese.* å…¬å¹³ (gÅng pÃ­ng) means "fairness" in Simplified Chinese.* ç¾¤ä½“ (qÃºn tÇ) means "demographic group" in Simplified Chinese.* æ¢¯åº¦æµ (dÃ o yÃ¹) means "gradient flow" in Simplified Chinese.* æŸå¤±è¡¨ (shÃ¨ shÃ¬ biÇo) means "loss surface" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="FoMo-Rewards-Can-we-cast-foundation-models-as-reward-functions"><a href="#FoMo-Rewards-Can-we-cast-foundation-models-as-reward-functions" class="headerlink" title="FoMo Rewards: Can we cast foundation models as reward functions?"></a>FoMo Rewards: Can we cast foundation models as reward functions?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03881">http://arxiv.org/abs/2312.03881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekdeep Singh Lubana, Johann Brehmer, Pim de Haan, Taco Cohen</li>
<li>for: ç ”ç©¶æ˜¯ç”¨åº•å±‚æ¨¡å‹ä½œä¸ºæ¿€åŠ±å­¦ä¹ çš„å¥–åŠ±å‡½æ•°çš„å¯èƒ½æ€§ã€‚</li>
<li>methods: æˆ‘ä»¬æè®®ä¸€ç§ç®€å•çš„æ‰¹å¤„ç†ï¼Œå°†å¯è§è¯­è¨€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆã€‚ Specifically, ç»™ä¸€ä¸ªè½¨è¿¹çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æè¿°ä»»åŠ¡çš„ instrucion çš„å¯èƒ½æ€§ã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°è¿™ä¸ªé€šç”¨çš„å¯èƒ½æ€§å‡½æ•°å…·æœ‰ç†æƒ³çš„å¥–åŠ±å‡½æ•°ç‰¹å¾ï¼šå®ƒä¸æ„¿æœ›çš„è¡Œä¸ºç›¸å…³ï¼Œè€Œä¸ç±»ä¼¼ä½†é”™è¯¯çš„ç­–ç•¥ç›¸å¯¹è¾ƒä½ã€‚ å…¨é¢æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¼€å¯äº†é€šè¿‡åŸºç¡€æ¨¡å‹è®¾è®¡å¼€æ”¾å¼ä»»åŠ¡çš„å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
We explore the viability of casting foundation models as generic reward functions for reinforcement learning. To this end, we propose a simple pipeline that interfaces an off-the-shelf vision model with a large language model. Specifically, given a trajectory of observations, we infer the likelihood of an instruction describing the task that the user wants an agent to perform. We show that this generic likelihood function exhibits the characteristics ideally expected from a reward function: it associates high values with the desired behaviour and lower values for several similar, but incorrect policies. Overall, our work opens the possibility of designing open-ended agents for interactive tasks via foundation models.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-transformer-neural-networks-for-skillful-and-reliable-medium-range-weather-forecasting"><a href="#Scaling-transformer-neural-networks-for-skillful-and-reliable-medium-range-weather-forecasting" class="headerlink" title="Scaling transformer neural networks for skillful and reliable medium-range weather forecasting"></a>Scaling transformer neural networks for skillful and reliable medium-range weather forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03876">http://arxiv.org/abs/2312.03876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung Nguyen, Rohan Shah, Hritik Bansal, Troy Arcomano, Sandeep Madireddy, Romit Maulik, Veerabhadra Kotamarthi, Ian Foster, Aditya Grover</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å¤©æ°”é¢„æŠ¥æ–¹æ³•ï¼Œä»¥æé«˜å¤©æ°”é¢„æŠ¥çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§ç®€å•çš„è½¬æ¢å™¨æ¨¡å‹ï¼Œç§°ä¸ºStormerï¼Œå…¶ä¸­åŒ…æ‹¬å¤©æ°”ç‰¹æœ‰çš„åµŒå…¥ã€éšæœºåŠ¨åŠ›é¢„æµ‹å’Œå‹åŠ›åŠ æƒæŸå¤±ç­‰å…³é”®ç»„ä»¶ã€‚</li>
<li>results: åœ¨WeatherBench 2ä¸Šï¼ŒStormeråœ¨çŸ­è‡³ä¸­èŒƒå›´é¢„æµ‹ task ä¸Šè¡¨ç°ç«äº‰æ€§ï¼Œè€Œåœ¨é•¿èŒƒå›´é¢„æµ‹ task ä¸Šè¶…è¿‡7å¤©çš„é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”éœ€è¦è®­ç»ƒæ•°æ®å’Œè®¡ç®—é‡çš„æå°‘ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯æ˜Stormerçš„æ‰©å±•æ€§è‰¯å¥½ï¼Œéšç€æ¨¡å‹å¤§å°å’Œè®­ç»ƒç¤ºä¾‹çš„å¢åŠ ï¼Œé¢„æµ‹å‡†ç¡®æ€§éƒ½ä¼šæé«˜ã€‚<details>
<summary>Abstract</summary>
Weather forecasting is a fundamental problem for anticipating and mitigating the impacts of climate change. Recently, data-driven approaches for weather forecasting based on deep learning have shown great promise, achieving accuracies that are competitive with operational systems. However, those methods often employ complex, customized architectures without sufficient ablation analysis, making it difficult to understand what truly contributes to their success. Here we introduce Stormer, a simple transformer model that achieves state-of-the-art performance on weather forecasting with minimal changes to the standard transformer backbone. We identify the key components of Stormer through careful empirical analyses, including weather-specific embedding, randomized dynamics forecast, and pressure-weighted loss. At the core of Stormer is a randomized forecasting objective that trains the model to forecast the weather dynamics over varying time intervals. During inference, this allows us to produce multiple forecasts for a target lead time and combine them to obtain better forecast accuracy. On WeatherBench 2, Stormer performs competitively at short to medium-range forecasts and outperforms current methods beyond 7 days, while requiring orders-of-magnitude less training data and compute. Additionally, we demonstrate Stormer's favorable scaling properties, showing consistent improvements in forecast accuracy with increases in model size and training tokens. Code and checkpoints will be made publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤©æ°”é¢„æµ‹æ˜¯æ°”å€™å˜åŒ–çš„åŸºæœ¬é—®é¢˜ï¼Œå¯ä»¥é¢„æµ‹å’Œå‡è½»æ°”å€™å˜åŒ–çš„å½±å“ã€‚ç°åœ¨ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„å¤©æ°”é¢„æµ‹æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºäº†å¾ˆå¤§çš„æ­é…ï¼Œå…·æœ‰ä¸æ“ä½œç³»ç»Ÿç›¸å½“çš„ç²¾åº¦ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç»å¸¸ä½¿ç”¨å¤æ‚çš„è‡ªå®šä¹‰æ¶æ„ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®åœ°äº†è§£å®ƒä»¬çš„æˆåŠŸåŸå› ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†é£æš´ï¼ˆStormerï¼‰ï¼Œä¸€ç§ç®€å•çš„è½¬æ¢å™¨æ¨¡å‹ï¼Œå¯ä»¥åœ¨å¤©æ°”é¢„æµ‹ä¸­å®ç°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åªéœ€è¦å¾®å°çš„æ”¹å˜äºæ ‡å‡†è½¬æ¢å™¨è„Šæ¢ã€‚æˆ‘ä»¬é€šè¿‡ä»”ç»†çš„å®éªŒåˆ†æï¼ŒåŒ…æ‹¬ç‰¹å®šäºå¤©æ°”çš„åµŒå…¥ã€éšæœºåŠ¨åŠ›é¢„æµ‹å’Œå‹åŠ›WeightedæŸå¤±ï¼Œç¡®å®šäº†é£æš´çš„å…³é”®ç»„ä»¶ã€‚é£æš´çš„æ ¸å¿ƒæ˜¯ä¸€ç§éšæœºé¢„æµ‹ç›®æ ‡çš„å¯¹è±¡ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ—¶é—´é—´éš”å†…é¢„æµ‹å¤©æ°”åŠ¨åŠ›ã€‚åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå¤šä¸ªé¢„æµ‹ï¼Œå¹¶å°†å…¶ç»„åˆä»¥è·å¾—æ›´å¥½çš„é¢„æµ‹ç²¾åº¦ã€‚åœ¨WeatherBench 2ä¸Šï¼Œé£æš´åœ¨çŸ­è‡³ä¸­æœŸé¢„æµ‹å’Œè¶…è¿‡7å¤©çš„é¢„æµ‹ä¸­è¡¨ç°ç«äº‰åŠ›å¼ºï¼ŒåŒæ—¶éœ€è¦è®­ç»ƒæ•°æ®å’Œè®¡ç®—é‡å‡å°‘åˆ°äº†å¤šä¸ªçº§åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†é£æš´çš„æœ‰åˆ©æ‰©å±•æ€§ï¼Œè¡¨ç°å‡ºäº†éšç€æ¨¡å‹å¤§å°å’Œè®­ç»ƒTokenæ•°é‡çš„ä¸æ–­æé«˜çš„é¢„æµ‹ç²¾åº¦ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å°†å…¬å¼€å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="The-BigCode-Project-Governance-Card"><a href="#The-BigCode-Project-Governance-Card" class="headerlink" title="The BigCode Project Governance Card"></a>The BigCode Project Governance Card</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03872">http://arxiv.org/abs/2312.03872</a></li>
<li>repo_url: None</li>
<li>paper_authors: BigCode collaboration, Sean Hughes, Harm de Vries, Jennifer Robinson, Carlos MuÃ±oz Ferrandis, Loubna Ben Allal, Leandro von Werra, Jennifer Ding, Sebastien Paquet, Yacine Jernite</li>
<li>for: æœ¬æ–‡æ¦‚è¦æä¾›äº†BigCodeé¡¹ç›®çš„ä¸åŒæœºåˆ¶å’Œç®¡ç†é¢†åŸŸï¼Œä»¥æ”¯æŒé¡¹ç›®çš„é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†é¡¹ç›®ç»„ç»‡ç»“æ„ã€å®£è¨€ç›®æ ‡å’Œä»·å€¼è§‚ã€å†…éƒ¨å†³ç­–è¿‡ç¨‹ã€èµ„é‡‘å’Œèµ„æºç­‰æ–¹é¢çš„å‡ ä¸ªæœºåˆ¶æ¥æ”¯æŒé¡¹ç›®çš„ç®¡ç†ã€‚</li>
<li>results: æœ¬æ–‡é€šè¿‡æä¾›é¡¹ç›®çš„å„ä¸ªæœºåˆ¶å’Œé¢†åŸŸçš„ä¿¡æ¯ï¼Œå‘æ›´å¹¿æ³›çš„å…¬ä¼—æä¾›äº†é¡¹ç›®çš„é€æ˜åº¦å’Œå¯é‡å¤æ€§ï¼ŒåŒæ—¶ä¹Ÿä¸ºæœªæ¥çš„å¼€æºé¡¹ç›®æä¾›äº†ä¸€ä¸ªå¯ä»¿æ•ˆçš„å‚è€ƒã€‚<details>
<summary>Abstract</summary>
This document serves as an overview of the different mechanisms and areas of governance in the BigCode project. It aims to support transparency by providing relevant information about choices that were made during the project to the broader public, and to serve as an example of intentional governance of an open research project that future endeavors can leverage to shape their own approach. The first section, Project Structure, covers the project organization, its stated goals and values, its internal decision processes, and its funding and resources. The second section, Data and Model Governance, covers decisions relating to the questions of data subject consent, privacy, and model release.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ä»½æ–‡æ¡£æä¾›äº†å¤§ç é¡¹ç›®ä¸åŒæœºåˆ¶å’Œç®¡ç†æ–¹é¢çš„æ¦‚è¿°ï¼Œä»¥ä¾¿æ”¯æŒé€æ˜åº¦ï¼Œä¸ºæ›´å¹¿æ³›çš„å…¬ä¼—æä¾›ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶ä½œä¸ºæœªæ¥é¡¹ç›®çš„ç¤ºèŒƒï¼Œä»¥ä¾¿ä»–ä»¬å¯ä»¥æ ¹æ®è¿™ä¸ªæ–¹æ³•åˆ¶å®šè‡ªå·±çš„ç®¡ç†æ–¹å¼ã€‚é¦–éƒ¨åˆ†ï¼Œé¡¹ç›®ç»“æ„ï¼Œè¦†ç›–é¡¹ç›®ç»„ç»‡ç»“æ„ï¼Œé¡¹ç›®çš„å£°æ˜ç›®æ ‡å’Œä»·å€¼è§‚ï¼Œå†…éƒ¨å†³ç­–è¿‡ç¨‹ï¼Œä»¥åŠèµ„é‡‘å’Œèµ„æºã€‚ç¬¬äºŒéƒ¨åˆ†ï¼Œæ•°æ®å’Œæ¨¡å‹ç®¡ç†ï¼Œè¦†ç›–æ•°æ®ä¸»ä½“åŒæ„ã€éšç§å’Œæ¨¡å‹é‡Šå‡ºçš„å†³ç­–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Efficient-Large-Language-Models-A-Survey"><a href="#Efficient-Large-Language-Models-A-Survey" class="headerlink" title="Efficient Large Language Models: A Survey"></a>Efficient Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03863">http://arxiv.org/abs/2312.03863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aiot-mlsys-lab/efficientllms">https://github.com/aiot-mlsys-lab/efficientllms</a></li>
<li>paper_authors: Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang</li>
<li>for: æœ¬æ–‡æä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ€§å’Œå…¨é¢çš„LLMsæ•ˆç‡ç ”ç©¶ç»¼è¿°ï¼Œå¸®åŠ©ç ”ç©¶è€…å’Œå®è·µè€…æ›´å¥½åœ°äº†è§£LLMsæ•ˆç‡ç ”ç©¶çš„å‘å±•å’Œè¿›å±•ã€‚</li>
<li>methods: æœ¬æ–‡åˆ†ä¸ºä¸‰ä¸ªä¸»è¦ç±»åˆ«ï¼Œä»æ¨¡å‹ä¸­å¿ƒã€æ•°æ®ä¸­å¿ƒå’Œæ¡†æ¶ä¸­å¿ƒä¸‰ä¸ªè§’åº¦è¿›è¡Œç»¼è¿°ï¼Œå¹¶åœ¨GitHubä¸Šæä¾›äº†ç›¸å…³è®ºæ–‡çš„é›†æˆã€‚</li>
<li>results: æœ¬æ–‡æä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ€§å’Œå…¨é¢çš„LLMsæ•ˆç‡ç ”ç©¶ç»¼è¿°ï¼ŒåŒ…æ‹¬æ¨¡å‹ä¸­å¿ƒã€æ•°æ®ä¸­å¿ƒå’Œæ¡†æ¶ä¸­å¿ƒä¸‰ä¸ªè§’åº¦çš„ç ”ç©¶å‘å±•ï¼Œå¹¶å°†åœ¨GitHubä¸Šç»´æŠ¤å’Œæ›´æ–°ç›¸å…³è®ºæ–‡ã€‚<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/EfficientLLMs, https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of the research developments in efficient LLMs and inspire them to contribute to this important and exciting field.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡è¦çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„èƒ½åŠ›ï¼Œå¦‚è‡ªç„¶è¯­è¨€ç†è§£ã€è¯­è¨€ç”Ÿæˆå’Œå¤æ‚çš„æ¨ç†ï¼Œå¹¶æœ‰å¯èƒ½å¯¹ç¤¾ä¼šäº§ç”Ÿæ·±è¿œçš„å½±å“ã€‚ç„¶è€Œï¼Œè¿™äº›èƒ½åŠ›éœ€è¦å·¨å¤§çš„èµ„æºï¼Œ highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at <https://github.com/AIoT-MLSys-Lab/EfficientLLMs>, <https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey>, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of the research developments in efficient LLMs and inspire them to contribute to this important and exciting field.
</details></li>
</ul>
<hr>
<h2 id="Alpha-CLIP-A-CLIP-Model-Focusing-on-Wherever-You-Want"><a href="#Alpha-CLIP-A-CLIP-Model-Focusing-on-Wherever-You-Want" class="headerlink" title="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want"></a>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03818">http://arxiv.org/abs/2312.03818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunzey/alphaclip">https://github.com/sunzey/alphaclip</a></li>
<li>paper_authors: Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æé«˜CLIPçš„å¯æ§æ€§ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç¼–è¾‘å›¾åƒã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªauxiliary alpha channelæ¥æŒ‡ç¤ºæ³¨æ„åŠ›çš„åŒºåŸŸï¼Œå¹¶é€šè¿‡æ„å»ºäº†æ•°ç™¾ä¸‡ä¸ªRGBAåŒºåŸŸæ–‡æœ¬å¯¹æ¥ fine-tune CLIPã€‚</li>
<li>results: Alpha-CLIPä¸ä»…ä¿ç•™äº†CLIPçš„è§†è§‰è®¤çŸ¥èƒ½åŠ›ï¼Œè¿˜å…è®¸ç²¾å‡†åœ°æ§åˆ¶å›¾åƒå†…å®¹çš„å¼ºè°ƒã€‚å®ƒåœ¨å¤šç§ä»»åŠ¡ä¸Šè¾¾åˆ°äº†è‰¯å¥½çš„æ•ˆæœï¼ŒåŒ…æ‹¬å¼€æ”¾ä¸–ç•Œè®¤çŸ¥ã€å¤šModalå¤§è¯­è¨€æ¨¡å‹å’Œæ¡ä»¶2D&#x2F;3Dç”Ÿæˆã€‚<details>
<summary>Abstract</summary>
Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models. To fulfill the requirements, we introduce Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to suggest attentive regions and fine-tuned with constructed millions of RGBA region-text pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to open-world recognition, multimodal large language models, and conditional 2D / 3D generation. It has a strong potential to serve as a versatile tool for image-related tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
<SYS>CLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­æå–å›¾åƒä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚å®ƒå°†æ–‡æœ¬å’Œè§†è§‰æ¨¡å¼è”ç³»èµ·æ¥ï¼Œä»¥ä¾¿å…¨é¢ç†è§£å›¾åƒï¼ŒåŒ…æ‹¬æ‰€æœ‰ç»†èŠ‚ï¼Œå³ä½¿ä¸ç‰¹å®šä»»åŠ¡æ— å…³ã€‚ç„¶è€Œï¼Œä¸ºäº†æ›´åŠ ç²¾ç»†åœ°ç†è§£å’Œæ§åˆ¶å›¾åƒï¼Œéœ€è¦ä¸“æ³¨äºç‰¹å®šåŒºåŸŸï¼Œè¿™äº›åŒºåŸŸå¯ä»¥ç”±äººç±»æˆ–æ„ŸçŸ¥æ¨¡å‹æŒ‡å®šä¸ºç‚¹ã€é¢æˆ–ç›’å­ã€‚ä¸ºäº†æ»¡è¶³è¿™äº›éœ€æ±‚ï¼Œæˆ‘ä»¬ä»‹ç»äº†Alpha-CLIPï¼Œå®ƒæ˜¯CLIPçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå¸¦æœ‰ä¸€ä¸ªè¾…åŠ©çš„Î±é€šé“ï¼Œç”¨äºå»ºè®®æ³¨æ„çš„åŒºåŸŸï¼Œå¹¶ä¸”é€šè¿‡æ„å»ºäº†æ•°ç™¾ä¸‡ä¸ªRGBAåŒºåŸŸæ–‡æœ¬å¯¹è¿›è¡Œç²¾åº¦åœ°è°ƒæ•´ã€‚Alpha-CLIPä¸ä»…ä¿æŒäº†CLIPçš„è§†è§‰è¯†åˆ«èƒ½åŠ›ï¼Œè¿˜å…è®¸æ§åˆ¶å›¾åƒå†…å®¹çš„å¼ºè°ƒã€‚å®ƒåœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æ•ˆæœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¼€æ”¾ä¸–ç•Œè¯†åˆ«ã€å¤šModalå¤§è¯­è¨€æ¨¡å‹å’Œæ¡ä»¶2D/3Dç”Ÿæˆã€‚å®ƒå…·æœ‰å¼ºå¤§çš„æ½œåœ¨åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥ç”¨äºå¤šç§å›¾åƒç›¸å…³ä»»åŠ¡ã€‚</SYS>Here's the translation in Simplified Chinese:CLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­æå–å›¾åƒä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚å®ƒå°†æ–‡æœ¬å’Œè§†è§‰æ¨¡å¼è”ç³»èµ·æ¥ï¼Œä»¥ä¾¿å…¨é¢ç†è§£å›¾åƒï¼ŒåŒ…æ‹¬æ‰€æœ‰ç»†èŠ‚ï¼Œå³ä½¿ä¸ç‰¹å®šä»»åŠ¡æ— å…³ã€‚ç„¶è€Œï¼Œä¸ºäº†æ›´åŠ ç²¾ç»†åœ°ç†è§£å’Œæ§åˆ¶å›¾åƒï¼Œéœ€è¦ä¸“æ³¨äºç‰¹å®šåŒºåŸŸï¼Œè¿™äº›åŒºåŸŸå¯ä»¥ç”±äººç±»æˆ–æ„ŸçŸ¥æ¨¡å‹æŒ‡å®šä¸ºç‚¹ã€é¢æˆ–ç›’å­ã€‚ä¸ºäº†æ»¡è¶³è¿™äº›éœ€æ±‚ï¼Œæˆ‘ä»¬ä»‹ç»äº†Alpha-CLIPï¼Œå®ƒæ˜¯CLIPçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå¸¦æœ‰ä¸€ä¸ªè¾…åŠ©çš„Î±é€šé“ï¼Œç”¨äºå»ºè®®æ³¨æ„çš„åŒºåŸŸï¼Œå¹¶ä¸”é€šè¿‡æ„å»ºäº†æ•°ç™¾ä¸‡ä¸ªRGBAåŒºåŸŸæ–‡æœ¬å¯¹è¿›è¡Œç²¾åº¦åœ°è°ƒæ•´ã€‚Alpha-CLIPä¸ä»…ä¿æŒäº†CLIPçš„è§†è§‰è¯†åˆ«èƒ½åŠ›ï¼Œè¿˜å…è®¸æ§åˆ¶å›¾åƒå†…å®¹çš„å¼ºè°ƒã€‚å®ƒåœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æ•ˆæœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¼€æ”¾ä¸–ç•Œè¯†åˆ«ã€å¤šModalå¤§è¯­è¨€æ¨¡å‹å’Œæ¡ä»¶2D/3Dç”Ÿæˆã€‚å®ƒå…·æœ‰å¼ºå¤§çš„æ½œåœ¨åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥ç”¨äºå¤šç§å›¾åƒç›¸å…³ä»»åŠ¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="OneLLM-One-Framework-to-Align-All-Modalities-with-Language"><a href="#OneLLM-One-Framework-to-Align-All-Modalities-with-Language" class="headerlink" title="OneLLM: One Framework to Align All Modalities with Language"></a>OneLLM: One Framework to Align All Modalities with Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03700">http://arxiv.org/abs/2312.03700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csuhan/onellm">https://github.com/csuhan/onellm</a></li>
<li>paper_authors: Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ç§å¯ä»¥åŒæ—¶å¤„ç†å¤šç§æ¨¡å¼çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä»¥æé«˜æ¨¡å¼ç†è§£èƒ½åŠ›ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨ä¸€ç§ç»Ÿä¸€æ¶æ„ï¼Œå°†å…«ç§æ¨¡å¼ä¸è¯­è¨€ç›¸alignï¼Œå¹¶é€šè¿‡è¿›ç¨‹å¼å¤šæ¨¡å¼å¯¹é½ç®¡é“æ¥å®ç°ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä½¿ç”¨ä¸€ç§æ··åˆå¤šä¸ªå›¾åƒæŠ•å½±æ¨¡å—å’ŒåŠ¨æ€è·¯ç”±æ¥å»ºç«‹ä¸€ä¸ªé€šç”¨æŠ•å½±æ¨¡å—ï¼ˆUPMï¼‰ã€‚</li>
<li>results: åœ¨25ç§å¤šæ ·åŒ–çš„benchmarkä»»åŠ¡ä¸Šï¼ŒOneLLMè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šæ¨¡å¼captioningã€é—®ç­”å’Œæ¨ç†ç­‰ã€‚<details>
<summary>Abstract</summary>
Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we first train an image projection module to connect a vision encoder with LLM. Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing. Finally, we progressively align more modalities to LLM with the UPM. To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance. Code, data, model and online demo are available at https://github.com/csuhan/OneLLM
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨æœ€è¿‘å·²ç»å¸å¼•äº†å¹¿æ³›çš„æ³¨æ„åŠ›ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œéƒ½æ˜¯åŸºäºç‰¹å®šæ¨¡å¼çš„ç¼–è§£oderï¼Œè¿™äº›ç¼–è§£oderé€šå¸¸å…·æœ‰ä¸åŒçš„æ¶æ„ï¼Œå¹¶ä¸”åªèƒ½å¤„ç†å¸¸è§çš„æ¨¡å¼ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†OneLLMï¼Œä¸€ä¸ªèƒ½å¤Ÿå¯¹å…«ç§æ¨¡å¼è¿›è¡Œè¯­è¨€å¯¹åº”çš„ MLLMã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ç¼–è§£oderå’Œä¸€ä¸ªè¿›ç¨‹å¼å¤šæ¨¡æ€å¯¹åº”ç®¡é“æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å›¾åƒæŠ•å½±æ¨¡å—å°†è§†è§‰ç¼–ç å™¨ä¸LLMè¿æ¥èµ·æ¥ã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé€šç”¨æŠ•å½±æ¨¡å—ï¼ˆUPMï¼‰ï¼Œé€šè¿‡æ··åˆå¤šä¸ªå›¾åƒæŠ•å½±æ¨¡å—å’ŒåŠ¨æ€è·¯ç”±æ¥å®ç°ã€‚æœ€åï¼Œæˆ‘ä»¬é€æ¸å°†æ›´å¤šçš„æ¨¡å¼ä¸LLMå¯¹åº”ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨OneLLMåœ¨ seguir instrucciones ä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬è¿˜ç­¹é›†äº†ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æŒ‡ä»¤é›†ï¼ŒåŒ…æ‹¬200ä¸‡ä¸ªItemä»å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ã€ç‚¹äº‘ã€æ·±åº¦/æ­£å¸¸å›¾ã€IMUå’ŒfMRIå¤§è„‘æ´»åŠ¨ã€‚OneLLMåœ¨25ç§å¤šæ ·åŒ–çš„benchmarkä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æè¿°ã€é—®ç­”å’Œç†è§£ä»»åŠ¡ï¼Œå…¶è¡¨ç°å‡ºè‰²ã€‚ä»£ç ã€æ•°æ®ã€æ¨¡å‹å’Œåœ¨çº¿ç¤ºä¾‹å¯ä»¥åœ¨https://github.com/csuhan/OneLLM ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Intrinsic-Harmonization-for-Illumination-Aware-Compositing"><a href="#Intrinsic-Harmonization-for-Illumination-Aware-Compositing" class="headerlink" title="Intrinsic Harmonization for Illumination-Aware Compositing"></a>Intrinsic Harmonization for Illumination-Aware Compositing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03698">http://arxiv.org/abs/2312.03698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Careaga, S. Mahdi H. Miangoleh, YaÄŸÄ±z Aksoy</li>
<li>for: æé«˜å›¾åƒåˆæˆé•œåƒçš„çœŸå®æ„Ÿå’Œç…§æ˜å‡†ç¡®æ€§</li>
<li>methods: ä½¿ç”¨è‡ªä¸»è¶…vised illumination harmonizationæ–¹æ³•ï¼Œé€šè¿‡ä¼°ç®—ç®€å•çš„å…¨å±€ç…§æ˜æ¨¡å‹å¹¶ä½¿ç”¨ç½‘ç»œè¿›è¡Œä¿®æ­£ï¼Œå®ç°åŒ¹é…èƒŒæ™¯å’Œå‰æ™¯çš„ç…§æ˜å’Œé¢œè‰²è¡¨ç°</li>
<li>results: åœ¨å®é™…æ‹¼æ¥å›¾åƒä¸­æé«˜äº†çœŸå®æ„Ÿå’Œç…§æ˜å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡ç”¨æˆ·ç ”ç©¶å¾—åˆ°äº†å¯¹æ¯”å…ˆå‰æ–¹æ³•çš„Objective Measurement of enhanced realism<details>
<summary>Abstract</summary>
Despite significant advancements in network-based image harmonization techniques, there still exists a domain disparity between typical training pairs and real-world composites encountered during inference. Most existing methods are trained to reverse global edits made on segmented image regions, which fail to accurately capture the lighting inconsistencies between the foreground and background found in composited images. In this work, we introduce a self-supervised illumination harmonization approach formulated in the intrinsic image domain. First, we estimate a simple global lighting model from mid-level vision representations to generate a rough shading for the foreground region. A network then refines this inferred shading to generate a harmonious re-shading that aligns with the background scene. In order to match the color appearance of the foreground and background, we utilize ideas from prior harmonization approaches to perform parameterized image edits in the albedo domain. To validate the effectiveness of our approach, we present results from challenging real-world composites and conduct a user study to objectively measure the enhanced realism achieved compared to state-of-the-art harmonization methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
å°½ç®¡ç½‘ç»œåŸºäºå›¾åƒåè°ƒæŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶å­˜åœ¨åŸŸåä¸ä¸€è‡´é—®é¢˜ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•æ˜¯é€šè¿‡åå‘å…¨å±€ç¼–è¾‘ segmented å›¾åƒåŒºåŸŸæ¥é€†è½¬globalç¼–è¾‘ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•å‡†ç¡®æ•æ‰èƒŒæ™¯å’Œå‰æ™¯ä¹‹é—´çš„å…‰ç…§ä¸åŒ¹é…é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨åè°ƒç…§æ˜æ–¹æ³•ï¼ŒåŸºäºä¸­ç­‰çº§è§†è§‰è¡¨ç¤ºæ¥ä¼°ç®—ç®€å•çš„å…¨å±€ç…§æ˜æ¨¡å‹ï¼Œå¹¶å°†å…¶ç”¨äºç”Ÿæˆä¸èƒŒæ™¯åœºæ™¯ç›¸åŒ¹é…çš„é‡æ–°ç…§æ˜ã€‚ä¸ºäº†ä¿æŒå‰æ™¯å’ŒèƒŒæ™¯çš„é¢œè‰²å‡ºç°ç›¸ä¼¼ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ä¹‹å‰çš„åè°ƒæ–¹æ³•æ¥è¿›è¡Œå‚æ•°åŒ–çš„å›¾åƒç¼–è¾‘ï¼Œå¹¶åœ¨ albedo é¢‘è°±ä¸­è¿›è¡Œè¿™äº›ç¼–è¾‘ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å®é™…æ‹æ‘„çš„å¤æ‚å›¾åƒä¸­å±•ç¤ºäº†ç»“æœï¼Œå¹¶è¿›è¡Œäº†ç”¨æˆ·ç ”ç©¶æ¥ Ğ¾Ğ±ÑŠĞµĞºively æµ‹é‡æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰åè°ƒæ–¹æ³•ç›¸æ¯”çš„å¢å¼ºç°å®æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="MatterGen-a-generative-model-for-inorganic-materials-design"><a href="#MatterGen-a-generative-model-for-inorganic-materials-design" class="headerlink" title="MatterGen: a generative model for inorganic materials design"></a>MatterGen: a generative model for inorganic materials design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03687">http://arxiv.org/abs/2312.03687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claudio Zeni, Robert Pinsler, Daniel ZÃ¼gner, Andrew Fowler, Matthew Horton, Xiang Fu, Sasha Shysheya, Jonathan CrabbÃ©, Lixin Sun, Jake Smith, Ryota Tomioka, Tian Xie</li>
<li>For: The paper aims to develop a new generative model for designing functional materials with desired properties, particularly focusing on stability and novelty.* Methods: The proposed model, called MatterGen, uses a diffusion-based generative process that refines atom types, coordinates, and the periodic lattice to produce crystalline structures. Adapter modules are introduced to enable fine-tuning towards specific property constraints.* Results: MatterGen is able to generate stable, diverse inorganic materials across the periodic table, with a higher success rate and closer proximity to the local energy minimum compared to prior generative models. Fine-tuning the model allows for the design of materials with desired chemistry, symmetry, and multiple properties such as mechanical, electronic, and magnetic properties.<details>
<summary>Abstract</summary>
The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. We believe that the quality of generated materials and the breadth of MatterGen's capabilities represent a major advancement towards creating a universal generative model for materials design.
</details>
<details>
<summary>æ‘˜è¦</summary>
ğŸ“ The design of functional materials with desired properties is crucial in driving technological advances in areas like energy storage, catalysis, and carbon capture. ğŸ”‹ Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. ğŸ’¡ Despite recent progress, current generative models have low success rates in proposing stable crystals, or can only satisfy a very limited set of property constraints. ğŸ” Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. ğŸ”© To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. ğŸ“Š We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. ğŸ”— Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. ğŸ”“ After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic, and magnetic properties. ğŸ” Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. ğŸ’ª We believe that the quality of generated materials and the breadth of MatterGen's capabilities represent a major advancement towards creating a universal generative model for materials design. ğŸŒŸ
</details></li>
</ul>
<hr>
<h2 id="LLM-as-OS-llmao-Agents-as-Apps-Envisioning-AIOS-Agents-and-the-AIOS-Agent-Ecosystem"><a href="#LLM-as-OS-llmao-Agents-as-Apps-Envisioning-AIOS-Agents-and-the-AIOS-Agent-Ecosystem" class="headerlink" title="LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem"></a>LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03815">http://arxiv.org/abs/2312.03815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, Yongfeng Zhang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ„æ€ä¸€ä¸ªä»¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„äººå·¥æ™ºèƒ½æ“ä½œç³»ç»Ÿï¼ˆAIOSï¼‰ç”Ÿæ€ç³»ç»Ÿï¼Œè¿™å°†æ ‡å¿—ç€æ“ä½œç³»ç»Ÿçš„ä¸€ä¸ªæ–° paradigma shiftã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ“ä½œç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶å¼€å‘äº†ä¸€ç³»åˆ—åŸºäº LLM çš„äººå·¥æ™ºèƒ½ä»£ç†åº”ç”¨ç¨‹åºï¼ˆAAPï¼‰ï¼Œä»¥æ¨åŠ¨ AIOS ç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ã€‚</li>
<li>results: æœ¬è®ºæ–‡é¢„æµ‹ï¼Œé€šè¿‡ LLM çš„åº”ç”¨ï¼Œå°†ä¸ä»…æ”¹å˜äººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºçš„æ°´å¹³ï¼Œè¿˜ä¼šé‡æ–°å®šä¹‰è®¡ç®—æœºç³»ç»Ÿçš„è®¾è®¡å’Œå®ç°ã€è½¯ä»¶å’Œç¼–ç¨‹è¯­è¨€çš„è®¾è®¡æ–¹æ³•ï¼Œå¹¶å¸¦æ¥ä¸€ç³»åˆ—æ–°çš„ç¡¬ä»¶å’Œä¸­é—´ä»¶è®¾å¤‡ã€‚<details>
<summary>Abstract</summary>
This paper envisions a revolutionary AIOS-Agent ecosystem, where Large Language Model (LLM) serves as the (Artificial) Intelligent Operating System (IOS, or AIOS)--an operating system ``with soul''. Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem. We envision that LLM's impact will not be limited to the AI application level, instead, it will in turn revolutionize the design and implementation of computer system, architecture, software, and programming language, featured by several main concepts: LLM as OS (system-level), Agents as Applications (application-level), Natural Language as Programming Interface (user-level), and Tools as Devices/Libraries (hardware/middleware-level).
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡æ‹Ÿæƒ³ä¸€ä¸ªé©å‘½æ€§çš„AIOSæŠ•é€ç”Ÿæ€ç³»ç»Ÿï¼Œå…¶ä¸­å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºäººå·¥æ™ºèƒ½æ“ä½œç³»ç»Ÿï¼ˆIOSæˆ–AIOSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªâ€œæœ‰å¿ƒâ€çš„æ“ä½œç³»ç»Ÿã€‚åœ¨è¿™ä¸ªåŸºç¡€ä¸Šï¼Œä¸€äº›LLMåŸºäºçš„AIåº”ç”¨ç¨‹åºï¼ˆAgentæˆ–AAPï¼‰è¢«å¼€å‘å‡ºæ¥ï¼Œrichäº†AIOSæŠ•é€ç”Ÿæ€ç³»ç»Ÿï¼Œæ ‡å¿—ç€ä¼ ç»ŸOS-APPç”Ÿæ€ç³»ç»Ÿçš„ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Shiftã€‚æˆ‘ä»¬æƒ³è±¡ï¼ŒLLMçš„å½±å“ä¸å°†æ­¢äºAIåº”ç”¨ç¨‹åºå±‚æ¬¡ï¼Œåä¹‹ï¼Œå®ƒä¼šé©å‘½åŒ–è®¡ç®—æœºç³»ç»Ÿçš„è®¾è®¡å’Œå®ç°ã€è½¯ä»¶æ¶æ„å’Œç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šLLMä½œä¸ºç³»ç»Ÿå±‚æ¬¡ï¼ˆsystem-levelï¼‰ï¼Œä»£ç†ä¸ºåº”ç”¨ç¨‹åºå±‚æ¬¡ï¼ˆapplication-levelï¼‰ï¼Œè‡ªç„¶è¯­è¨€ä½œä¸ºç”¨æˆ·å±‚æ¬¡ï¼ˆuser-levelï¼‰ï¼Œå·¥å…·ä½œä¸ºç¡¬ä»¶/ä¸­é—´ä»¶å±‚æ¬¡ï¼ˆhardware/middleware-levelï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="What-Planning-Problems-Can-A-Relational-Neural-Network-Solve"><a href="#What-Planning-Problems-Can-A-Relational-Neural-Network-Solve" class="headerlink" title="What Planning Problems Can A Relational Neural Network Solve?"></a>What Planning Problems Can A Relational Neural Network Solve?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03682">http://arxiv.org/abs/2312.03682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/concepts-ai/goal-regression-width">https://github.com/concepts-ai/goal-regression-width</a></li>
<li>paper_authors: Jiayuan Mao, TomÃ¡s Lozano-PÃ©rez, Joshua B. Tenenbaum, Leslie Pack Kaelbling</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨goal-conditioned policiesæ˜¯å¦‚ä½•è¢«å­¦ä¹ çš„ï¼Œä»¥åŠå…¶æ•ˆç‡å¦‚ä½•ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨circuit complexity analysiså’Œserialized goal regression searchï¼ˆS-GRSï¼‰æ¥ç ”ç©¶relational neural networksè¡¨ç¤ºçš„ç­–ç•¥å­¦ä¹ é—®é¢˜ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°æœ‰ä¸‰ç±»è®¡åˆ’é—®é¢˜ï¼Œå…¶å®½åº¦å’Œæ·±åº¦éšç€ç‰©å“å’Œè§„åˆ’è·ç¦»çš„å¢åŠ è€Œå¢é•¿ï¼Œå¹¶æä¾›äº†æ„é€ æ€§çš„è¯æ˜ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜è¯æ˜äº†è¿™ç§åˆ†æçš„å®ç”¨æ€§äºç­–ç•¥å­¦ä¹ ä¸­ã€‚<details>
<summary>Abstract</summary>
Goal-conditioned policies are generally understood to be "feed-forward" circuits, in the form of neural networks that map from the current state and the goal specification to the next action to take. However, under what circumstances such a policy can be learned and how efficient the policy will be are not well understood. In this paper, we present a circuit complexity analysis for relational neural networks (such as graph neural networks and transformers) representing policies for planning problems, by drawing connections with serialized goal regression search (S-GRS). We show that there are three general classes of planning problems, in terms of the growth of circuit width and depth as a function of the number of objects and planning horizon, providing constructive proofs. We also illustrate the utility of this analysis for designing neural networks for policy learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç›®æ ‡æ¡ä»¶æ”¿ç­–é€šå¸¸è¢«ç†è§£ä¸ºâ€œå‰å‘â€Circuitï¼Œå³ç¥ç»ç½‘ç»œï¼Œå°†å½“å‰çŠ¶æ€å’Œç›®æ ‡è§„èŒƒæ˜ å°„åˆ°ä¸‹ä¸€ä¸ªè¡ŒåŠ¨ã€‚ç„¶è€Œï¼Œå­¦ä¹ è¿™ç§ç­–ç•¥çš„æƒ…å†µå’Œæ•ˆç‡å°šä¸å¤Ÿæ¸…æ¥šã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å…³ç³»ç¥ç»ç½‘ç»œï¼ˆå¦‚å›¾ç¥ç»ç½‘ç»œå’Œå˜æ¢å™¨ï¼‰è¡¨ç¤ºç­–ç•¥çš„ç”µè·¯å¤æ‚åº¦åˆ†æï¼Œé€šè¿‡ä¸åºåˆ—åŒ–ç›®æ ‡å›å½’æœç´¢ï¼ˆS-GRSï¼‰çš„è¿æ¥ã€‚æˆ‘ä»¬è¯æ˜äº†è®¡åˆ’é—®é¢˜çš„ä¸‰ç±»æ€»ä½“æƒ…å†µï¼Œå³ç”µè·¯å®½åº¦å’Œæ·±åº¦éšç‰©å“å’Œè§„åˆ’æ—¶é—´çš„å¢åŠ æƒ…å†µï¼Œå¹¶æä¾›äº†æ„é€ æ€§è¯æ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ Illustrates the utility of this analysis for designing neural networks for policy learning.
</details></li>
</ul>
<hr>
<h2 id="An-Integration-of-Pre-Trained-Speech-and-Language-Models-for-End-to-End-Speech-Recognition"><a href="#An-Integration-of-Pre-Trained-Speech-and-Language-Models-for-End-to-End-Speech-Recognition" class="headerlink" title="An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition"></a>An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03668">http://arxiv.org/abs/2312.03668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukiya Hono, Koh Mitsuda, Tianyu Zhao, Kentaro Mitsui, Toshiaki Wakatsuki, Kei Sawada</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒè¯­éŸ³å’Œè‡ªç„¶è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œä»¥ä¾¿å®ç°æ›´é«˜æ•ˆçš„è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†é¢„è®­ç»ƒè¯­éŸ³è¡¨ç¤ºæ¨¡å‹å’Œå¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»„åˆï¼Œé€šè¿‡å°†è¯­éŸ³è¡¨ç¤ºè½¬æ¢ä¸ºæ–‡æœ¬tokenï¼Œå¹¶ä½¿ç”¨LLMçš„åºå¤§çŸ¥è¯†è¿›è¡Œ autoregressive ç”Ÿæˆï¼Œå®ç°ç«¯åˆ°ç«¯ ASRã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¨¡å‹å¯ä»¥ä¸ç°ä»£ç«¯åˆ°ç«¯ ASR æ¨¡å‹ç›¸æ¯”ï¼Œå¹¶ä¸”å¯ä»¥è¿›è¡Œ parameter-efficient é¢„æµ‹ä¼˜åŒ–å’Œé¢„è®­ç»ƒåŸŸè½¬æ¢ã€‚<details>
<summary>Abstract</summary>
Advances in machine learning have made it possible to perform various text and speech processing tasks, including automatic speech recognition (ASR), in an end-to-end (E2E) manner. Since typical E2E approaches require large amounts of training data and resources, leveraging pre-trained foundation models instead of training from scratch is gaining attention. Although there have been attempts to use pre-trained speech and language models in ASR, most of them are limited to using either. This paper explores the potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR. The proposed model enables E2E ASR by generating text tokens in an autoregressive manner via speech representations as speech prompts, taking advantage of the vast knowledge provided by the LLM. Furthermore, the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation. Experimental results show that the proposed model achieves performance comparable to modern E2E ASR models.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:éšç€æœºå™¨å­¦ä¹ çš„è¿›æ­¥ï¼Œå¯ä»¥ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ–¹å¼å®Œæˆä¸åŒçš„æ–‡æœ¬å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚ Typical E2E Approaches éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œèµ„æºï¼Œå› æ­¤åˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è€Œä¸æ˜¯ä»scratch è®­ç»ƒæ˜¯æ”¶åˆ°å…³æ³¨ã€‚ Although there have been attempts to use pre-trained speech and language models in ASR, most of them are limited to using either. This paper explores the potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR. The proposed model enables E2E ASR by generating text tokens in an autoregressive manner via speech representations as speech prompts, taking advantage of the vast knowledge provided by the LLM. Furthermore, the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation. Experimental results show that the proposed model achieves performance comparable to modern E2E ASR models.Translated into Traditional Chinese:éšç€æœºå™¨å­¦ä¹ çš„è¿›æ­¥ï¼Œå¯ä»¥ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ–¹å¼å®Œæˆä¸åŒçš„æ–‡æœ¬å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚ Typical E2E Approaches éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œèµ„æºï¼Œå› æ­¤åˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è€Œä¸æ˜¯ä»scratch è®­ç»ƒæ˜¯æ”¶åˆ°å…³æ³¨ã€‚ Although there have been attempts to use pre-trained speech and language models in ASR, most of them are limited to using either. This paper explores the potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR. The proposed model enables E2E ASR by generating text tokens in an autoregressive manner via speech representations as speech prompts, taking advantage of the vast knowledge provided by the LLM. Furthermore, the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation. Experimental results show that the proposed model achieves performance comparable to modern E2E ASR models.
</details></li>
</ul>
<hr>
<h2 id="Generative-agent-based-modeling-with-actions-grounded-in-physical-social-or-digital-space-using-Concordia"><a href="#Generative-agent-based-modeling-with-actions-grounded-in-physical-social-or-digital-space-using-Concordia" class="headerlink" title="Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia"></a>Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03664">http://arxiv.org/abs/2312.03664</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/concordia">https://github.com/google-deepmind/concordia</a></li>
<li>paper_authors: Alexander Sasha Vezhnevets, John P. Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A. DuÃ©Ã±ez-GuzmÃ¡n, William A. Cunningham, Simon Osindero, Danny Karmon, Joel Z. Leibo</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æ¢è®¨ Agent-based modeling å¦‚ä½•åˆ©ç”¨ Large Language Models (LLM) æé«˜æ¨¡å‹çš„å¯ç†è§£æ€§å’Œå¯è¡Œæ€§ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº† Concordia åº“ï¼Œç”¨äºæ„å»ºå’Œä½¿ç”¨è¯­è¨€åª’ä»‹çš„agent-basedæ¨¡å‹ã€‚Concordia ä½¿ç”¨ LLM æ¥åº”ç”¨å¸¸è¯†ï¼Œè¡Œä¸ºç†è§£ã€è®°å¿†å¸¸è¯†çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ API è°ƒç”¨æ§åˆ¶æ•°å­—æŠ€æœ¯ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ Agent-based modeling æ–¹æ³•ï¼Œå¯ä»¥åœ¨physically-æˆ– digitally-grounded environmentsä¸­å®ç°è¯­è¨€åª’ä»‹çš„ simulationsã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ”¯æŒå¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ç§‘å­¦ç ”ç©¶å’Œè¯„ä¼°å®é™…çš„æ•°å­—æœåŠ¡æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act "reasonably", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.
</details>
<details>
<summary>æ‘˜è¦</summary>
agent-basedæ¨¡å‹å·²ç»å­˜åœ¨æ•°åå¹´ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºç¤¾ä¼šå’Œè‡ªç„¶ç§‘å­¦é¢†åŸŸã€‚ç°åœ¨ï¼Œéšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°ç‰¹æ€§çš„å‡ºç°ï¼Œ agent-basedæ¨¡å‹çš„èŒƒå›´å³å°†æ‰©å¤§å¾ˆå¤šã€‚ç”Ÿæˆå‹agent-basedæ¨¡å‹ï¼ˆGABMï¼‰ä¸ä»…æ˜¯ĞºĞ»Ğ°ÑÑĞ¸å‹agent-basedæ¨¡å‹ï¼ˆABMï¼‰ï¼Œwhere agents talk to each otherï¼Œè€Œæ˜¯é€šè¿‡ä½¿ç”¨LLMæ¥åº”ç”¨å¸¸è¯†ï¼Œè¡Œä¸ºâ€œåˆç†â€ï¼Œå›å¿†å¸¸è¯†çŸ¥è¯†ï¼Œç”ŸæˆAPIè°ƒç”¨æ¥æ§åˆ¶æ•°å­—æŠ€æœ¯ï¼Œå¦‚åº”ç”¨å’ŒæœåŠ¡ã€‚æˆ‘ä»¬ç°åœ¨åœ¨Concordiaåº“ä¸­æä¾›äº†ä¸€ç§æ–¹ä¾¿æ„å»ºå’Œä½¿ç”¨GABMçš„æ–¹æ³•ã€‚Concordiaå¯ä»¥å¸®åŠ©æ„å»ºè¯­è¨€åª’ä»‹çš„ç‰©ç†æˆ–æ•°å­—ç¯å¢ƒæ¨¡æ‹Ÿã€‚Concordiaä»£ç†äººä½¿ç”¨å¯å˜ç»„ä»¶ç³»ç»Ÿæ¥è°ƒç”¨LLMå’Œ associative memory Retrievalä¸¤ç§åŸºæœ¬æ“ä½œã€‚ä¸€ä¸ªç‰¹æ®Šçš„ä»£ç†äººcalled Game Masterï¼ˆGMï¼‰ï¼Œå®ƒ draws inspiration from tabletop role-playing gamesï¼Œè´Ÿè´£æ¨¡æ‹Ÿä»£ç†äººä¹‹é—´çš„ç¯å¢ƒã€‚ä»£ç†äººé€šè¿‡natural languageæè¿°è‡ªå·±çš„è¡Œä¸ºï¼Œè€ŒGMå°†å…¶è½¬åŒ–ä¸ºåˆé€‚çš„å®ç°ã€‚åœ¨æ¨¡æ‹Ÿçš„ç‰©ç†ä¸–ç•Œä¸­ï¼ŒGMæ£€æŸ¥ä»£ç†äººè¡Œä¸ºçš„ç‰©ç†å¯èƒ½æ€§ï¼Œå¹¶æè¿°å…¶æ•ˆæœã€‚åœ¨æ¨¡æ‹Ÿæ•°å­—ç¯å¢ƒä¸­ï¼ŒGMå¯èƒ½å¤„ç†APIè°ƒç”¨ï¼Œä»¥ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°ble with external toolsï¼Œå¦‚é€šç”¨AIåŠ©æ‰‹ï¼ˆä¾‹å¦‚Bardã€ChatGPTï¼‰å’Œæ•°å­—åº”ç”¨ï¼ˆä¾‹å¦‚æ—¥å†ã€é‚®ä»¶ã€æœç´¢ç­‰ï¼‰ã€‚Concordiaæ˜¯ä¸ºäº†æ”¯æŒå¹¿æ³›çš„åº”ç”¨ï¼Œä»ç§‘å­¦ç ”ç©¶åˆ°è¯„ä¼°å®é™…æ•°å­—æœåŠ¡çš„æ€§èƒ½è€Œè®¾è®¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Pearl-A-Production-ready-Reinforcement-Learning-Agent"><a href="#Pearl-A-Production-ready-Reinforcement-Learning-Agent" class="headerlink" title="Pearl: A Production-ready Reinforcement Learning Agent"></a>Pearl: A Production-ready Reinforcement Learning Agent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03814">http://arxiv.org/abs/2312.03814</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/pearl">https://github.com/facebookresearch/pearl</a></li>
<li>paper_authors: Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu</li>
<li>For: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æ¢è®¨RLæ¡†æ¶åœ¨å®ç°é•¿æœŸç›®æ ‡æ–¹é¢çš„ä¸€äº›é—®é¢˜ï¼ŒåŒ…æ‹¬å»¶è¿Ÿå¥–åŠ±ã€éƒ¨åˆ†å¯è§æ€§ã€æœç´¢å’Œåˆ©ç”¨ä¹‹é—´çš„çŸ›ç›¾ã€ä½¿ç”¨ç¦»çº¿æ•°æ®æé«˜åœ¨çº¿æ€§èƒ½ã€å¹¶ç¡®ä¿å®‰å…¨é™åˆ¶å¾—åˆ°æ»¡è¶³ã€‚* Methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPearlçš„ç”Ÿäº§å‡†å¤‡RLæ™ºèƒ½ä»£ç†è½¯ä»¶åŒ…ï¼Œè¯¥åŒ…å¯ä»¥æ¨¡å—åŒ–åœ°è§£å†³RLè§£å†³æ–¹æ¡ˆä¸­çš„å„ç§é—®é¢˜ï¼ŒåŒ…æ‹¬å»¶è¿Ÿå¥–åŠ±ã€éƒ¨åˆ†å¯è§æ€§ã€æœç´¢å’Œåˆ©ç”¨ä¹‹é—´çš„çŸ›ç›¾ã€ä½¿ç”¨ç¦»çº¿æ•°æ®æé«˜åœ¨çº¿æ€§èƒ½ã€å¹¶ç¡®ä¿å®‰å…¨é™åˆ¶å¾—åˆ°æ»¡è¶³ã€‚* Results: è¿™ç¯‡è®ºæ–‡æä¾›äº†ä¸€äº›åˆæ­¥çš„åŸºå‡†æµ‹è¯•ç»“æœï¼ŒåŒæ—¶ä¹Ÿ highlightsäº†Pearlåœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„é‡‡çº³ï¼Œä»¥ demonstarteå…¶ç”Ÿäº§å‡†å¤‡æ€§ã€‚<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) offers a versatile framework for achieving long-term goals. Its generality allows us to formalize a wide range of problems that real-world intelligent systems encounter, such as dealing with delayed rewards, handling partial observability, addressing the exploration and exploitation dilemma, utilizing offline data to improve online performance, and ensuring safety constraints are met. Despite considerable progress made by the RL research community in addressing these issues, existing open-source RL libraries tend to focus on a narrow portion of the RL solution pipeline, leaving other aspects largely unattended. This paper introduces Pearl, a Production-ready RL agent software package explicitly designed to embrace these challenges in a modular fashion. In addition to presenting preliminary benchmark results, this paper highlights Pearl's industry adoptions to demonstrate its readiness for production usage. Pearl is open sourced on Github at github.com/facebookresearch/pearl and its official website is located at pearlagent.github.io.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Activation-Steering-in-Language-Models-with-Mean-Centring"><a href="#Improving-Activation-Steering-in-Language-Models-with-Mean-Centring" class="headerlink" title="Improving Activation Steering in Language Models with Mean-Centring"></a>Improving Activation Steering in Language Models with Mean-Centring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03813">http://arxiv.org/abs/2312.03813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole Jorgensen, Dylan Cope, Nandi Schoots, Murray Shanahan</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºæ§åˆ¶ï¼Œé€šè¿‡å‘ç°å¯¼èˆªå‘é‡ã€‚ä½†æ˜¯ï¼Œå·¥ç¨‹å¸ˆé€šå¸¸ä¸çŸ¥é“è¿™äº›æ¨¡å‹ä¸­ç‰¹å¾çš„è¡¨ç¤ºæ–¹å¼ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºä½¿ç”¨å‡å€¼ä¸­å¿ƒåŒ–å¯¼èˆªå‘é‡çš„æƒ³æ³•ï¼Œå³å–target datasetçš„æ´»åŠ¨å‡å€¼ï¼Œç„¶åå¯¹æ‰€æœ‰è®­ç»ƒæ´»åŠ¨å‡å€¼è¿›è¡Œå‡æ³•ã€‚è¿™ç§æ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¢«è¯æ˜æœ‰æ•ˆï¼Œå¯ä»¥å¸®åŠ©æ§åˆ¶å¤§è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œé¿å…ç”Ÿæˆæ”»å‡»æ€§æ–‡æœ¬ï¼Œå¹¶è®©æ•…äº‹å®Œæˆtargetç±»å‹ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°ï¼Œå¯¹äºè‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œä½¿ç”¨å‡å€¼ä¸­å¿ƒåŒ–å¯¼èˆªå‘é‡å¯ä»¥å¤§å¹…æé«˜æ´»åŠ¨å¯¼èˆªçš„æ•ˆivenessï¼Œæ¯”ä¹‹å‰çš„åŸºelineæ›´é«˜ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•è¿˜å¯ä»¥è®©æ¨¡å‹æ›´å¥½åœ°æ‰§è¡Œå„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œæ¯”å¦‚æ•…äº‹å®Œæˆå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚<details>
<summary>Abstract</summary>
Recent work in activation steering has demonstrated the potential to better control the outputs of Large Language Models (LLMs), but it involves finding steering vectors. This is difficult because engineers do not typically know how features are represented in these models. We seek to address this issue by applying the idea of mean-centring to steering vectors. We find that taking the average of activations associated with a target dataset, and then subtracting the mean of all training activations, results in effective steering vectors. We test this method on a variety of models on natural language tasks by steering away from generating toxic text, and steering the completion of a story towards a target genre. We also apply mean-centring to extract function vectors, more effectively triggering the execution of a range of natural language tasks by a significant margin (compared to previous baselines). This suggests that mean-centring can be used to easily improve the effectiveness of activation steering in a wide range of contexts.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘çš„æ´»åŠ¨å¯¼èˆªç ”ç©¶è¡¨æ˜å¯ä»¥æ›´å¥½åœ°æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºï¼Œä½†æ˜¯å®ƒéœ€è¦æ‰¾åˆ°å¯¼èˆªå‘é‡ã€‚è¿™æ˜¯å› ä¸ºå·¥ç¨‹å¸ˆé€šå¸¸ä¸çŸ¥é“è¿™äº›æ¨¡å‹ä¸­ç‰¹å¾çš„è¡¨ç¤ºæ–¹å¼ã€‚æˆ‘ä»¬æƒ³è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡åº”ç”¨å‡å€¼ä¸­å¿ƒåŒ–æ€æƒ³æ¥æ”¹è¿›å¯¼èˆªå‘é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹ç›®æ ‡æ•°æ®é›†çš„æ´»åŠ¨å‡å€¼ï¼Œå¹¶ä»æ‰€æœ‰è®­ç»ƒæ´»åŠ¨å‡å€¼ä¸­ subtract ç›®æ ‡æ•°æ®é›†çš„å‡å€¼ï¼Œå¯ä»¥è·å¾—æœ‰æ•ˆçš„å¯¼èˆªå‘é‡ã€‚æˆ‘ä»¬åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸Šæµ‹è¯•äº†è¿™ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬é¿å…ç”Ÿæˆæ¶æ„æ–‡æœ¬å’Œå¯¼èˆªæ•…äº‹çš„å®Œæˆæ–¹å‘ã€‚æˆ‘ä»¬è¿˜åº”ç”¨å‡å€¼ä¸­å¿ƒåŒ–æ¥æå–å‡½æ•°å‘é‡ï¼Œå¯ä»¥æ›´å¥½åœ°è§¦å‘å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡çš„æ‰§è¡Œï¼Œç›¸æ¯”ä¹‹å‰çš„åŸºçº¿ã€‚è¿™è¡¨ç¤ºï¼Œå‡å€¼ä¸­å¿ƒåŒ–å¯ä»¥ç”¨äºå¹¿æ³›æ”¹è¿› activation steering çš„æ•ˆivenessã€‚
</details></li>
</ul>
<hr>
<h2 id="Efficient-Inverse-Design-Optimization-through-Multi-fidelity-Simulations-Machine-Learning-and-Search-Space-Reduction-Strategies"><a href="#Efficient-Inverse-Design-Optimization-through-Multi-fidelity-Simulations-Machine-Learning-and-Search-Space-Reduction-Strategies" class="headerlink" title="Efficient Inverse Design Optimization through Multi-fidelity Simulations, Machine Learning, and Search Space Reduction Strategies"></a>Efficient Inverse Design Optimization through Multi-fidelity Simulations, Machine Learning, and Search Space Reduction Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03654">http://arxiv.org/abs/2312.03654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luka Grbcic, Juliane MÃ¼ller, Wibe Albert de Jong</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¢å¼ºé€†è®¾è®¡ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„çº¦æŸç¯å¢ƒï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¤šå…ƒé¢„æµ‹ã€æœºå™¨å­¦ä¹ æ¨¡å‹å’Œä¼˜åŒ–ç®—æ³•çš„è”ç›Ÿã€‚</li>
<li>methods: æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•Ğ¾Ğ»Ğ¾Ğ³Ğ¸ï¼Ÿï¼Œå°†æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ä¼˜åŒ–ç®—æ³•è”ç›Ÿèµ·æ¥ï¼Œä»¥å¢å¼ºé€†è®¾è®¡ä¼˜åŒ–è¿‡ç¨‹çš„æ•ˆç‡å’Œç²¾åº¦ã€‚åœ¨ä¸¤ä¸ªä¸åŒçš„å·¥ç¨‹é€†è®¾è®¡é—®é¢˜ä¸Šè¿›è¡Œäº†åˆ†æï¼Œå¹¶ä½¿ç”¨äº†ä½ç²¾åº¦æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é¢„æµ‹ç›®æ ‡å˜æ•°å’Œå†³å®šæ˜¯å¦éœ€è¦é«˜ç²¾åº¦æ¨¡æ‹Ÿã€‚</li>
<li>results: æœ¬è®ºæ–‡çš„ç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¤§å¹…æé«˜é€†è®¾è®¡ä¼˜åŒ–è¿‡ç¨‹çš„æ•ˆç‡å’Œç²¾åº¦ï¼Œå¹¶ä¸”å¯ä»¥ä¸ä¸åŒçš„ä¼˜åŒ–ç®—æ³•è”ç›Ÿä»¥å®ç°æ›´å¥½çš„ç»“æœã€‚å°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¾ˆå¥½åœ°ä¿ç•™è®¡ç®—èµ„æºï¼Œå¹¶ä¸”å¯ä»¥è®©é€†è®¾è®¡ä¼˜åŒ–è¿‡ç¨‹æ›´åŠ å¿«é€Ÿå’Œç¨³å®šã€‚<details>
<summary>Abstract</summary>
This paper introduces a methodology designed to augment the inverse design optimization process in scenarios constrained by limited compute, through the strategic synergy of multi-fidelity evaluations, machine learning models, and optimization algorithms. The proposed methodology is analyzed on two distinct engineering inverse design problems: airfoil inverse design and the scalar field reconstruction problem. It leverages a machine learning model trained with low-fidelity simulation data, in each optimization cycle, thereby proficiently predicting a target variable and discerning whether a high-fidelity simulation is necessitated, which notably conserves computational resources. Additionally, the machine learning model is strategically deployed prior to optimization to reduce the search space, thereby further accelerating convergence toward the optimal solution. The methodology has been employed to enhance two optimization algorithms, namely Differential Evolution and Particle Swarm Optimization. Comparative analyses illustrate performance improvements across both algorithms. Notably, this method is adeptly adaptable across any inverse design application, facilitating a harmonious synergy between a representative low-fidelity machine learning model, and high-fidelity simulation, and can be seamlessly applied across any variety of population-based optimization algorithms.
</details>
<details>
<summary>æ‘˜è¦</summary>
The methodology uses a machine learning model trained with low-fidelity simulation data to predict a target variable in each optimization cycle. This approach conserves computational resources by only using high-fidelity simulations when necessary. Additionally, the machine learning model is deployed before optimization to reduce the search space, which further accelerates convergence towards the optimal solution.The methodology is employed to enhance two optimization algorithms, namely Differential Evolution and Particle Swarm Optimization. Comparative analyses show performance improvements across both algorithms. Notably, this method is adaptable to any inverse design application and can be seamlessly applied to any variety of population-based optimization algorithms.In simplified Chinese, the paper introduces a methodology that improves the inverse design optimization process in situations with limited computing resources. The methodology combines multi-fidelity evaluations, machine learning models, and optimization algorithms to achieve this goal. The proposed methodology is applied to two engineering inverse design problems and shows performance improvements across two optimization algorithms. This method is adaptable to any inverse design application and can be easily applied to any population-based optimization algorithm.
</details></li>
</ul>
<hr>
<h2 id="MotionCtrl-A-Unified-and-Flexible-Motion-Controller-for-Video-Generation"><a href="#MotionCtrl-A-Unified-and-Flexible-Motion-Controller-for-Video-Generation" class="headerlink" title="MotionCtrl: A Unified and Flexible Motion Controller for Video Generation"></a>MotionCtrl: A Unified and Flexible Motion Controller for Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03641">http://arxiv.org/abs/2312.03641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§èƒ½å¤Ÿç²¾å‡†æ§åˆ¶ Ğ²Ğ¸Ğ´ĞµĞ¾ä¸­çš„æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨çš„åŠ¨ä½œæ§åˆ¶å™¨ï¼ˆMotionCtrlï¼‰ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„åŠ¨ä½œæ§åˆ¶å™¨æ¶æ„ï¼Œå®ƒç»¼åˆè€ƒè™‘äº†æ‘„åƒæœºè¿åŠ¨ã€ç‰©ä½“è¿åŠ¨ä»¥åŠè®­ç»ƒæ•°æ®çš„ç‰¹æ€§ï¼Œä»¥æä¾›çµæ´»å’Œç²¾å‡†çš„åŠ¨ä½œæ§åˆ¶ã€‚</li>
<li>results: å¯¹æ¯”äºç°æœ‰çš„æ–¹æ³•ï¼ŒMotionCtrlå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼š1ï¼‰å®ƒå¯ä»¥ç²¾å‡†åœ°æ§åˆ¶æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨ï¼Œå…è®¸æ›´ç»†è‡´çš„åŠ¨ä½œæ§åˆ¶å’Œå¤šæ ·åŒ–çš„åŠ¨ä½œç»„åˆã€‚2ï¼‰å®ƒçš„åŠ¨ä½œæ¡ä»¶ç”±æ‘„åƒæœºå§¿æ€å’Œè½¨è¿¹å†³å®šï¼Œè¿™äº›æ¡ä»¶æ˜¯å‡ºç°æ— å…³çš„å’Œå¯¹ç‰©ä½“å½¢çŠ¶æˆ–å¤–è§‚çš„å½±å“æœ€å°ã€‚3ï¼‰å®ƒæ˜¯ä¸€ç§ç›¸å¯¹é€šç”¨çš„æ¨¡å‹ï¼Œå¯ä»¥é€‚åº”å„ç§æ‘„åƒæœºå§¿æ€å’Œè½¨è¿¹ã€‚ç»è¿‡å¹¿æ³›çš„è´¨é‡å’Œé‡æµ‹è¯•ï¼ŒMotionCtrlåœ¨ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒæ—¶è¡¨ç°å‡ºäº†è¶…è¶Šæ€§ã€‚<details>
<summary>Abstract</summary>
Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement. Accurate control of both camera and object motion is essential for video generation. However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity. Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion. The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data. Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion. 2) Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos. 3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸»è¦çš„åŠ¨ä½œåœ¨å½±ç‰‡ä¸­åŒ…æ‹¬æ‘„åƒæœºè¿åŠ¨æ‰€å¼•èµ·çš„æ‘„åƒæœºè¿åŠ¨å’Œç‰©ä½“è¿åŠ¨ã€‚ç²¾ç¡®æ§åˆ¶æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨æ˜¯å½±ç‰‡ç”Ÿæˆçš„é‡ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œå‡ ä¹ä¸“æ³¨äºä¸€ç§ç±»å‹çš„åŠ¨ä½œæˆ–æ²¡æœ‰æ¸…æ™°åœ°åŒºåˆ†è¿™ä¸¤ç§åŠ¨ä½œï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ§åˆ¶èƒ½åŠ›å’Œå¤šæ ·æ€§ã€‚å› æ­¤ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºäº† MotionCtrlï¼Œä¸€ä¸ªç»Ÿä¸€å’Œ flexibleçš„åŠ¨ä½œæ§åˆ¶å™¨ï¼Œç”¨äºå½±ç‰‡ç”Ÿæˆï¼Œå¯ä»¥ç²¾ç¡®åœ°å’Œç‹¬ç«‹åœ°æ§åˆ¶æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨ã€‚ MotionCtrl çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥å……åˆ†è€ƒè™‘äº†æ‘„åƒæœºè¿åŠ¨ã€ç‰©ä½“è¿åŠ¨å’Œè®­ç»ƒæ•°æ®çš„è‡ªç„¶æ€§ã€‚ç›¸æ¯”äºå…ˆå‰çš„æ–¹æ³•ï¼ŒMotionCtrl æä¾›äº†ä¸‰å¤§ä¼˜ç‚¹ï¼š1. å¯ä»¥ç²¾ç¡®åœ°å’Œç‹¬ç«‹åœ°æ§åˆ¶æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨ï¼Œå®ç°æ›´ç»†éƒ¨çš„åŠ¨ä½œæ§åˆ¶å’Œè®©ç”Ÿæˆçš„å½±ç‰‡æ›´å¤šæ ·åŒ–ã€‚2. å…¶åŠ¨ä½œæ¡ä»¶ç”±æ‘„åƒæœºä½ç½®å’Œè½¨è¿¹å†³å®šï¼Œè¿™äº›æ¡ä»¶æ˜¯æ— å½¢æ„Ÿå’Œç‰©ä½“å½¢çŠ¶çš„å½±å“æœ€å°çš„ã€‚3. å®ƒæ˜¯ä¸€ä¸ªç›¸å¯¹ä¸€èˆ¬åŒ–çš„æ¨¡å‹ï¼Œå¯ä»¥é€‚åº”å¹¿æ³›çš„æ‘„åƒæœºä½ç½®å’Œè½¨è¿¹ã€‚å®é™…å®éªŒè¡¨æ˜ï¼ŒMotionCtrl åœ¨è®­ç»ƒåå¯ä»¥å¯¹å¤šç§æ‘„åƒæœºä½ç½®å’Œè½¨è¿¹è¿›è¡Œé€‚åº”ã€‚
</details></li>
</ul>
<hr>
<h2 id="Not-All-Large-Language-Models-LLMs-Succumb-to-the-â€œReversal-Curseâ€-A-Comparative-Study-of-Deductive-Logical-Reasoning-in-BERT-and-GPT-Models"><a href="#Not-All-Large-Language-Models-LLMs-Succumb-to-the-â€œReversal-Curseâ€-A-Comparative-Study-of-Deductive-Logical-Reasoning-in-BERT-and-GPT-Models" class="headerlink" title="Not All Large Language Models (LLMs) Succumb to the â€œReversal Curseâ€: A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models"></a>Not All Large Language Models (LLMs) Succumb to the â€œReversal Curseâ€: A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03633">http://arxiv.org/abs/2312.03633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingye Yang, Da Wu, Kai Wang</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æ¢è®¨è‡ªåŠ¨é€†æ¨Decoderå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨â€œAæ˜¯Bâ€çš„æƒ…å†µä¸‹å¤±è´¥å­¦ä¹ â€œBæ˜¯Aâ€ï¼Œæ¢è®¨è¿™ç§é€†æ¨çš„åŸºæœ¬å¤±è´¥æ˜¯å¦å¯¹æŸäº›é€šç”¨ä»»åŠ¡ï¼Œå¦‚æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæä¾›äº†çº¢flagã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº† bidirectional LLMï¼ˆBERTï¼‰ï¼Œå¹¶å‘ç°å®ƒå…·æœ‰é€†æ¨ç¥¸å®³çš„å…ç–«åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯„ä¼°äº†æ›´å¤æ‚çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé›†åˆï¼ˆunionå’Œintersectionï¼‰æ“ä½œçš„äº¤å å’Œèåˆã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œåœ¨ä¸¤ä¸ªé›†åˆæ“ä½œçš„æƒ…å†µä¸‹ï¼Œ both encoderå’Œdecoderè¯­è¨€æ¨¡å‹éƒ½èƒ½å¤Ÿè¡¨ç°å‡ºè‰²ï¼Œä½†æ˜¯åœ¨ä¸‰ä¸ªé›†åˆæ“ä½œçš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬é‡åˆ°äº†å›°éš¾ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œencoderå’Œdecoderæ¨¡å‹åœ¨ç®€å•å’Œå¤æ‚é€»è¾‘æ¨ç†ä¸­æœ‰æ‰€ä¸åŒï¼Œå¹¶ä¸”åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€‰æ‹©BERTæˆ–GPTåº”è¯¥æ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚å’Œç‰¹ç‚¹ï¼Œä»¥ä¾¿å……åˆ†åˆ©ç”¨å®ƒä»¬çš„ç‰¹ç‚¹ã€‚<details>
<summary>Abstract</summary>
The "Reversal Curse" refers to the scenario where auto-regressive decoder large language models (LLMs), such as ChatGPT, trained on "A is B" fail to learn "B is A", demonstrating a basic failure of logical deduction. This raises a red flag in the use of GPT models for certain general tasks such as constructing knowledge graphs, considering their adherence to this symmetric principle. In our study, we examined a bidirectional LLM, BERT, and found that it is immune to the reversal curse. Driven by ongoing efforts to construct biomedical knowledge graphs with LLMs, we also embarked on evaluating more complex but essential deductive reasoning capabilities. This process included first training encoder and decoder language models to master the intersection ($\cap$) and union ($\cup$) operations on two sets and then moving on to assess their capability to infer different combinations of union ($\cup$) and intersection ($\cap$) operations on three newly created sets. The findings showed that while both encoder and decoder language models, trained for tasks involving two sets (union/intersection), were proficient in such scenarios, they encountered difficulties when dealing with operations that included three sets (various combinations of union and intersection). Our research highlights the distinct characteristics of encoder and decoder models in simple and complex logical reasoning. In practice, the choice between BERT and GPT should be guided by the specific requirements and nature of the task at hand, leveraging their respective strengths in bidirectional context comprehension and sequence prediction.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œé€†è½¬å’’â€æŒ‡çš„æ˜¯ï¼Œä½¿ç”¨â€œAæ˜¯Bâ€çš„è‡ªåŠ¨é€†è½¬æ•°æ®æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚ChatGPTï¼Œå´æ— æ³•å­¦ä¹ â€œBæ˜¯Aâ€ï¼Œè¿™è¡¨ç¤ºäº†åŸºæœ¬çš„é€»è¾‘æ¨ç†å¤±è´¥ã€‚è¿™å¼•èµ·äº†ä½¿ç”¨GPTæ¨¡å‹çš„ä¸€äº›é€šç”¨ä»»åŠ¡ï¼Œå¦‚å»ºç«‹çŸ¥è¯†å›¾ï¼Œéœ€è¦æ³¨æ„è¿™ä¸ªå¯¹ç§°åŸç†ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸€ä¸ªå¯¹å‘æ¨¡å‹ï¼ˆBERTï¼‰ï¼Œå‘ç°å®ƒå…å—â€œé€†è½¬å’’â€çš„å½±å“ã€‚ä¸ºäº†ç»§ç»­ä½¿ç”¨LLMå»ºç«‹ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†è¯„ä¼°æ›´å¤æ‚ä½†é‡è¦çš„æ¨ç†èƒ½åŠ›ã€‚è¿™åŒ…æ‹¬å…ˆå°†è¯­è¨€æ¨¡å‹è®­ç»ƒåˆ°æŒæ¡ä¸¤ä¸ªé›†åˆçš„äº¤é›†ï¼ˆï¼‰å’Œunionï¼ˆï¼‰æ“ä½œï¼Œç„¶åè¯„ä¼°å®ƒä»¬åœ¨ä¸‰ä¸ªæ–°åˆ›å»ºçš„é›†åˆä¸Šè¿›è¡Œä¸åŒçš„äº¤é›†ï¼ˆï¼‰å’Œäº¤é›†ï¼ˆï¼‰æ“ä½œçš„èƒ½åŠ›ã€‚å‘ç°è™½ç„¶ä¸¤ä¸ªè¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¸¤ä¸ªé›†åˆï¼ˆunion/intersectionï¼‰çš„ä»»åŠ¡ä¸Šéƒ½èƒ½å¤Ÿè¡¨ç°å‡ºè‰²ï¼Œä½†å½“é¢ä¸´ä¸‰ä¸ªé›†åˆæ—¶ï¼Œå®ƒä»¬å´é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¾ç¤ºäº†ä¸¤ä¸ªè¯­è¨€æ¨¡å‹åœ¨ç®€å•å’Œå¤æ‚é€»è¾‘æ¨ç†ä¸­çš„ç‰¹åˆ«æ€§ã€‚åœ¨å®è·µä¸­ï¼Œé€‰æ‹©BERTæˆ–GPTåº”è¯¥æ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚å’Œç‰¹ç‚¹ï¼Œåˆ©ç”¨å®ƒä»¬çš„ç›¸åº”ä¼˜åŠ¿åœ¨å¯¹å‘æ–‡æœ¬ç†è§£å’Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="MOCHa-Multi-Objective-Reinforcement-Mitigating-Caption-Hallucinations"><a href="#MOCHa-Multi-Objective-Reinforcement-Mitigating-Caption-Hallucinations" class="headerlink" title="MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations"></a>MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03631">http://arxiv.org/abs/2312.03631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/assafbk/mocha_code">https://github.com/assafbk/mocha_code</a></li>
<li>paper_authors: Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor</li>
<li>for: æé«˜å›¾åƒæè¿°æ–‡æœ¬çš„å‡†ç¡®æ€§å’ŒSemantic adequacy</li>
<li>methods: ä½¿ç”¨è¿›åŒ–å­¦ä¹ æ¥è§£å†³å›¾åƒæè¿°æ–‡æœ¬ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œå¹¶æå‡ºå¤šç›®æ ‡å¥–åŠ±å‡½æ•°æ¥åŒæ—¶ä¼˜åŒ–å‡†ç¡®æ€§å’ŒSemantic adequacy</li>
<li>results: åœ¨ä¸åŒçš„æ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒMOCHaå¯ä»¥åŒæ—¶ä¼˜åŒ–å‡†ç¡®æ€§å’ŒSemantic adequacyï¼Œå¹¶ä¸”åœ¨å¼€ vocabulary settingä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„æµ‹è¯•é›† OpenCHAIR æ¥è¯„æµ‹å¼€ vocabulary hallucinations<details>
<summary>Abstract</summary>
While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, the generation of spurious details that cannot be inferred from the given image. Dedicated methods for reducing hallucinations in image captioning largely focus on closed-vocabulary object tokens, ignoring most types of hallucinations that occur in practice. In this work, we propose MOCHa, an approach that harnesses advancements in reinforcement learning (RL) to address the sequence-level nature of hallucinations in an open-world setup. To optimize for caption fidelity to the input image, we leverage ground-truth reference captions as proxies to measure the logical consistency of generated captions. However, optimizing for caption fidelity alone fails to preserve the semantic adequacy of generations; therefore, we propose a multi-objective reward function that jointly targets these qualities, without requiring any strong supervision. We demonstrate that these goals can be simultaneously optimized with our framework, enhancing performance for various captioning models of different scales. Our qualitative and quantitative results demonstrate MOCHa's superior performance across various established metrics. We also demonstrate the benefit of our method in the open-vocabulary setting. To this end, we contribute OpenCHAIR, a new benchmark for quantifying open-vocabulary hallucinations in image captioning models, constructed using generative foundation models. We will release our code, benchmark, and trained models.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œå›¾åƒæ¡ä»¶æ–‡æœ¬ç”Ÿæˆé¢†åŸŸå·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œä½†å›¾åƒæè¿°ä»ç„¶å—åˆ°åŸºæœ¬é—®é¢˜çš„å¹²æ‰°ï¼Œå³ç”Ÿæˆä¸å­˜åœ¨å›¾åƒä¸­çš„å¹»è§‰ã€‚ç°æœ‰çš„å‡å°‘å¹»è§‰æ–¹æ³•ä¸»è¦æ˜¯åŸºäºå…³é—­ vocabulary å¯¹è±¡ Ñ‚Ğ¾ĞºĞµĞ½ï¼Œå¿½ç•¥äº†å®é™…ä¸­çš„å¤§éƒ¨åˆ†å¹»è§‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† MOCHaï¼Œä¸€ç§åŸºäº reinforcement learningï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­è§£å†³å›¾åƒæè¿°ä¸­çš„åºåˆ—çº§å¹»è§‰ã€‚ä¸ºäº†ä¼˜åŒ–å›¾åƒæè¿°ä¸è¾“å…¥å›¾åƒçš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨çœŸå®å‚ç…§captionä½œä¸ºé€»è¾‘ä¸€è‡´æ€§çš„æŒ‡æ ‡ã€‚ä½†ä¼˜åŒ–ä¸€ä¸ªcaptionçš„å‡†ç¡®æ€§alone æ— æ³•ä¿æŒç”Ÿæˆçš„ semanticsï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šç›®æ ‡å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶ç›®æ ‡è¿™äº›è´¨é‡ï¼Œæ— éœ€å¼ºå¤§çš„ç›‘ç£ã€‚æˆ‘ä»¬ç¤ºå‡ºè¿™äº›ç›®æ ‡å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„æ¡†æ¶åŒæ—¶ä¼˜åŒ–ï¼Œæé«˜ä¸åŒè§„æ¨¡çš„æè¿°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„è´¨é‡å’Œé‡åŒ–ç»“æœè¡¨æ˜ MOCHa çš„è¶…è¶Šæ€§ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼€æ”¾ vocabulary  Settingä¸­çš„ä¼˜åŠ¿ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† OpenCHAIRï¼Œä¸€ä¸ªæ–°çš„è¯„ä»·æ ‡å‡†ï¼Œç”¨äºè¯„ä¼°å¼€æ”¾ vocabulary æè¿°æ¨¡å‹ä¸­çš„å¹»è§‰ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€æ ‡å‡†å’Œè®­ç»ƒæ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="DreamComposer-Controllable-3D-Object-Generation-via-Multi-View-Conditions"><a href="#DreamComposer-Controllable-3D-Object-Generation-via-Multi-View-Conditions" class="headerlink" title="DreamComposer: Controllable 3D Object Generation via Multi-View Conditions"></a>DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03611">http://arxiv.org/abs/2312.03611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yhyang-myron/DreamComposer">https://github.com/yhyang-myron/DreamComposer</a></li>
<li>paper_authors: Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æé«˜ç°æœ‰çš„è§†å›¾æ„è¯†æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ§åˆ¶æ€§çš„æ–°è§†å›¾å›¾åƒã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†è§†å›¾æ„è¯†3Dæå‡æ¨¡å—ï¼Œå°†å¤šä¸ªè§†å›¾ä¸­å¯¹è±¡çš„3Dè¡¨ç¤ºè½¬æ¢ä¸ºlatentç‰¹å¾ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨å¤šè§†å›¾ç‰¹å¾èåˆæ¨¡å—å°†ç›®æ ‡è§†å›¾ç‰¹å¾ä»å¤šä¸ªè§†å›¾è¾“å…¥ä¸­æå–å‡ºæ¥ã€‚æœ€åï¼Œå®ƒå°†ç›®æ ‡è§†å›¾ç‰¹å¾æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾å›¾åƒã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒDreamComposerå¯ä»¥ä¸ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°é›¶å®é™…å‚æ•°çš„æ–°è§†å›¾å›¾åƒç”Ÿæˆã€‚å®ƒå¯ä»¥ç”Ÿæˆé«˜å“è´¨çš„æ–°è§†å›¾å›¾åƒï¼Œå‡†ç¡®åœ°æ•æ‰äº†å¤šè§†å›¾æ¡ä»¶ä¸‹çš„å¯¹è±¡å½¢æ€å’Œä½ç½®ã€‚<details>
<summary>Abstract</summary>
Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view synthesis, further enhancing them to generate high-fidelity novel view images with multi-view conditions, ready for controllable 3D object reconstruction and various other applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä½¿ç”¨é¢„è®­ç»ƒçš„2Då¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹ï¼Œæœ€è¿‘çš„ç ”ç©¶å¯ä»¥ä»å•ä¸ªå®½æ³›å›¾åƒä¸­ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤šè§†å›¾ä¿¡æ¯ï¼Œè¿™äº›ç ”ç©¶å—åˆ°ç”Ÿæˆæ§åˆ¶æ–°è§†å›¾çš„å›°éš¾ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† DreamComposerï¼Œä¸€ä¸ªçµæ´»å¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¯ä»¥å¢å¼ºç°æœ‰çš„è§†è§‰æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒDreamComposeré¦–å…ˆä½¿ç”¨è§†è§‰æ„è¯†3Då‡çº§æ¨¡å—æ¥ä»å¤šä¸ªè§†è§’è·å–3Då¯¹è±¡çš„è¡¨ç¤ºã€‚ç„¶åï¼Œå®ƒä½¿ç”¨å¤šè§†å›¾ç‰¹å¾èåˆæ¨¡å—æ¥æ¸²æŸ“ç›®æ ‡è§†å›¾çš„ç§˜å¯†ç‰¹å¾ã€‚æœ€åï¼Œä»å¤šä¸ªè§†è§’è¾“å…¥ä¸­æå–çš„ç›®æ ‡è§†å›¾ç‰¹å¾è¢«æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒDreamComposerä¸ç°æœ‰æ‰©æ•£æ¨¡å‹å…¼å®¹ï¼Œå¯ä»¥further enhance them to generate high-fidelity novel view images with multi-view conditionsï¼Œready for controllable 3D object reconstructionå’Œå¤šç§å…¶ä»–åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="DiffusionSat-A-Generative-Foundation-Model-for-Satellite-Imagery"><a href="#DiffusionSat-A-Generative-Foundation-Model-for-Satellite-Imagery" class="headerlink" title="DiffusionSat: A Generative Foundation Model for Satellite Imagery"></a>DiffusionSat: A Generative Foundation Model for Satellite Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03606">http://arxiv.org/abs/2312.03606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng, Robin Rombach, Marshall Burke, David Lobell, Stefano Ermon</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯Remote Sensingæ•°æ®çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºç¯å¢ƒç›‘æµ‹å’Œå†œä¸šäº§é‡é¢„æµ‹ç­‰é‡è¦åº”ç”¨ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†DiffusionSatæ¨¡å‹ï¼ŒåŸºäºå¤§é‡å…¬å…±å¯ç”¨çš„é«˜åˆ†è¾¨ç‡Remote Sensingæ•°æ®é›†åˆè¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨äº†æ–°çš„conditioningæŠ€æœ¯ï¼Œä½¿ç”¨ metadata å¦‚åœ°ç†åæ ‡ä½œä¸ºç”Ÿæˆå›¾åƒçš„æ¡ä»¶ä¿¡æ¯ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusionSatæ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å«æ˜Ÿå›¾åƒï¼Œå¹¶å¯ä»¥è§£å†³å¤šç§ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ—¶é—´ç”Ÿæˆã€å¤šspectralè¾“å…¥çš„è¶…åˆ†è¾¨ç‡ç”Ÿæˆå’Œå¡«å……ç­‰ã€‚ä¸ä¹‹å‰çš„çŠ¶æ€ç æ¨¡å‹ç›¸æ¯”ï¼ŒDiffusionSatæ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œæ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å«æ˜Ÿå›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets. As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, superresolution given multi-spectral inputs and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale $\textit{generative}$ foundation model for satellite imagery.
</details>
<details>
<summary>æ‘˜è¦</summary>
å„ç§æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªé¢‘è°±ä¸­å·²ç»è¾¾åˆ°äº†å½“å‰æœ€ä½³ç»“æœï¼ŒåŒ…æ‹¬å›¾åƒã€è¯­éŸ³å’Œè§†é¢‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡å‹æ²¡æœ‰é’ˆå¯¹å«æ˜Ÿæ•£å°„æ•°æ®è¿›è¡Œæ”¯æŒï¼Œè¿™ç§æ•°æ®å¹¿æ³›ç”¨äºé‡è¦åº”ç”¨ï¼Œå¦‚ç¯å¢ƒç›‘æµ‹å’Œä½œç‰©äº§é‡é¢„æµ‹ã€‚å«æ˜Ÿå›¾åƒä¸è‡ªç„¶å›¾åƒæœ‰å¾ˆå¤§å·®å¼‚ï¼Œå®ƒä»¬å¯èƒ½æ˜¯å¤šspectralï¼Œæ—¶é—´ä¸è§„åˆ™é‡‡æ ·ï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä»ç½‘ç»œä¸Šçš„å›¾åƒè¿›è¡Œè®­ç»ƒä¸æ”¯æŒå®ƒä»¬ã€‚æ­¤å¤–ï¼Œå«æ˜Ÿæ•£å°„æ•°æ®æ˜¯ç©ºé—´-æ—¶çš„ï¼Œéœ€è¦åŸºäºæ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œè€Œä¼ ç»Ÿçš„æ–¹æ³•åŸºäºæ ‡ç­¾æˆ–å›¾åƒä¸æ”¯æŒã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DiffusionSatï¼Œè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŸºç¡€æ¨¡å‹ï¼ŒåŸºäºå…¬å…±å¯ç”¨çš„å¤§é‡é«˜åˆ†è¾¨ç‡å«æ˜Ÿæ•£å°„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç”±äºå«æ˜Ÿå›¾åƒçš„æ–‡æœ¬æ ‡ç­¾ç½•è§ï¼Œæˆ‘ä»¬å°†å…³è” metadataï¼Œå¦‚åœ°ç†ä½ç½®ä½œä¸ºæ¡ä»¶ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„æ ·æœ¬æ˜¯çœŸå®çš„ï¼Œå¯ä»¥ç”¨äºè§£å†³å¤šä¸ªç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ—¶é—´ç”Ÿæˆã€åŸºäºå¤šspectralè¾“å…¥çš„è¶…åˆ†è¾¨ç‡ã€å’Œå¡«å……ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼Œå¹¶æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å«æ˜Ÿå›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="MMM-Generative-Masked-Motion-Model"><a href="#MMM-Generative-Masked-Motion-Model" class="headerlink" title="MMM: Generative Masked Motion Model"></a>MMM: Generative Masked Motion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03596">http://arxiv.org/abs/2312.03596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, Chen Chen</li>
<li>For: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºMasked Motion Modelï¼ˆMMMï¼‰çš„æ–°å‹åŠ¨ä½œç”Ÿæˆæ–¹æ³•ï¼Œä»¥è§£å†³ç°æœ‰çš„åŠ¨ä½œç”Ÿæˆæ–¹æ³•ä¸­çš„æ—¶é—´æ€§å’Œé«˜ç²¾åº¦ä¹‹é—´çš„è´Ÿé¢é€‰æ‹©ã€‚* Methods: è¿™ä¸ªæ–¹æ³•ä½¿ç”¨äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŠ¨ä½œtokenizerï¼Œå°†3Däººä½“åŠ¨ä½œè½¬æ¢ä¸ºä¸€ä¸ªåºåˆ—çš„ä¸åŒçš„tokenåœ¨éšè—ç©ºé—´ä¸­ï¼Œå’Œï¼ˆ2ï¼‰æ¡ä»¶éšè—åŠ¨ä½œå˜æ¢å™¨ï¼Œå­¦ä¹ é¢„è®¡Randomlyéšè—åŠ¨ä½œtokenï¼ŒåŸºäºå·²ç»è®¡ç®—çš„æ–‡æœ¬tokenã€‚* Results: åœ¨å¯¹HumanML3Då’ŒKIT-MLæ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒåï¼Œè¿™ä¸ªæ–¹æ³•çš„resultè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶å®ç°äº†é«˜ç²¾åº¦å’Œé«˜é€ŸåŠ¨ä½œç”Ÿæˆï¼Œå¹¶å…·æœ‰é«˜çº§ç¼–è¾‘ç‰¹æ€§ï¼Œä¾‹å¦‚ä½“éƒ¨ä¿®æ”¹ã€åŠ¨ä½œé—´éš”å’Œé•¿åŠ¨ä½œåºåˆ—çš„åˆæˆã€‚æ­¤å¤–ï¼Œè¿™ä¸ªæ–¹æ³•æ¯”ç°æœ‰çš„ç¼–è¾‘åŠ¨ä½œæ‰©æ•£æ¨¡å‹å¿«ä¸¤ä¸ªæ•°é‡çº§çš„å•ä¸ªä¸­ç­‰çº§GPUä¸Šã€‚<details>
<summary>Abstract</summary>
Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fine-grained text descriptions, therefore simultaneously achieving high-fidelity and high-speed motion generation. In addition, MMM has innate motion editability. By simply placing mask tokens in the place that needs editing, MMM automatically fills the gaps while guaranteeing smooth transitions between editing and non-editing parts. Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM surpasses current leading methods in generating high-quality motion (evidenced by superior FID scores of 0.08 and 0.429), while offering advanced editing features such as body-part modification, motion in-betweening, and the synthesis of long motion sequences. In addition, MMM is two orders of magnitude faster on a single mid-range GPU than editable motion diffusion models. Our project page is available at \url{https://exitudio.github.io/MMM-page}.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸï¼Œä½¿ç”¨æ‰©æ•£å’Œè‡ªé€‚åº”æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°åŠ¨ä½œç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†è‰¯å¥½çš„æˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ç»å¸¸é¢ä¸´ç€å®æ—¶æ€§ã€é«˜ç²¾åº¦å’ŒåŠ¨ä½œå¯ç¼–è¾‘æ€§ä¹‹é—´çš„ç‰µæ‰¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå·®è·ï¼Œæˆ‘ä»¬ä»‹ç»äº†MMMï¼Œä¸€ç§æ–°å‹ä½†ç®€å•çš„åŠ¨ä½œç”Ÿæˆæ¨¡å¼ï¼ŒåŸºäºå¸¦æœ‰æ©ç çš„åŠ¨ä½œæ¨¡å‹ã€‚MMMåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŠ¨ä½œTokenizerï¼Œå°†3Däººä½“åŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£çš„tokenåœ¨éšè—ç©ºé—´ä¸­ï¼Œå’Œï¼ˆ2ï¼‰å—æ§æ©ç åŠ¨ä½œå˜æ¢å™¨ï¼Œå­¦ä¹ é¢„è®¡æ©ç åŠ¨ä½œtokenï¼Œæ ¹æ®é¢„è®¡çš„æ–‡æœ¬tokenæ¥è¿›è¡Œæ¡ä»¶é¢„æµ‹ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒMMMé€šè¿‡åŒæ—¶ attend to motionå’Œæ–‡æœ¬tokenï¼Œä»è€Œæ˜¾å¼åœ°æ•æ‰åŠ¨ä½œtokenä¹‹é—´çš„è‡ªç„¶ä¾èµ–å…³ç³»ï¼Œä»¥åŠæ–‡æœ¬tokenå’ŒåŠ¨ä½œtokenä¹‹é—´çš„å«ä¹‰æ˜ å°„ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒMMMå¯ä»¥å¹¶è¡Œåœ°æ‰§è¡Œå¤šä¸ªåŠ¨ä½œtokenï¼Œä»¥å®ç°é«˜ç²¾åº¦å’Œé«˜é€Ÿçš„åŠ¨ä½œç”Ÿæˆã€‚æ­¤å¤–ï¼ŒMMMå†…ç½®äº†åŠ¨ä½œå¯ç¼–è¾‘æ€§ã€‚é€šè¿‡åœ¨éœ€è¦ç¼–è¾‘çš„åœ°æ–¹æ”¾ç½®æ©ç ï¼ŒMMMä¼šè‡ªåŠ¨å¡«å……ç¼ºå¤±çš„éƒ¨åˆ†ï¼Œä¿è¯ç¼–è¾‘å’Œéç¼–è¾‘éƒ¨åˆ†ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚æˆ‘ä»¬åœ¨HumanML3Då’ŒKIT-MLæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œ demonstarted that MMM surpasses current leading methods in generating high-quality motionï¼ˆè¯æ˜äº†FIDåˆ†æ•°ä¸º0.08å’Œ0.429ï¼‰ï¼ŒåŒæ—¶æä¾›äº†é«˜çº§ç¼–è¾‘åŠŸèƒ½ï¼Œå¦‚èº«ä½“éƒ¨åˆ†ä¿®æ”¹ã€åŠ¨ä½œå·ç§¯å’Œé•¿åº¦åŠ¨ä½œåºåˆ—çš„åˆæˆã€‚æ­¤å¤–ï¼ŒMMMåœ¨å•ä¸ªä¸­ç­‰çº§GPUä¸Šä¸¤ä¸ªæ•°é‡çº§å¿«äºå¯ç¼–è¾‘åŠ¨ä½œæ‰©æ•£æ¨¡å‹ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š\url{https://exitudio.github.io/MMM-page}ã€‚
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-Assisted-Weakly-Supervised-Semantic-Segmentation"><a href="#Foundation-Model-Assisted-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Foundation Model Assisted Weakly Supervised Semantic Segmentation"></a>Foundation Model Assisted Weakly Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03585">http://arxiv.org/abs/2312.03585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HAL-42/FMA-WSSS">https://github.com/HAL-42/FMA-WSSS</a></li>
<li>paper_authors: Xiaobo Yang, Xiaojin Gong</li>
<li>for:  Addressing weakly supervised semantic segmentation (WSSS) using image-level labels.</li>
<li>methods:  Leveraging pre-trained foundation models (CLIP and SAM) to generate high-quality segmentation seeds, and using a coarse-to-fine framework with multi-label contrastive loss and CAM activation loss to learn the prompts.</li>
<li>results:  Achieving state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014.Here is the full translation in Simplified Chinese:</li>
<li>for: æœ¬æ–‡ç›®çš„æ˜¯ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾æ¥è§£å†³å¼±ively supervised semantic segmentation (WSSS) é—®é¢˜ã€‚</li>
<li>methods: æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆCLIPå’ŒSAMï¼‰ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ segmentation ç§å­ï¼Œå¹¶ä½¿ç”¨ä¸€ç§å®½æ³›-to-ç»†åŒ–æ¡†æ¶ï¼Œå¹¶é‡‡ç”¨å¤šæ ‡ç­¾å¯¹æ¯”æŸå¤±å’Œ CAM æ´»åŒ–æŸå¤±æ¥å­¦ä¹ æç¤ºã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨ PASCAL VOC 2012 å’Œ MS COCO 2014 ä¸Šè¾¾åˆ°äº†çŠ¶æ€ Ellçš„æ€§èƒ½å’Œç«äº‰æ€§çš„ç»“æœã€‚<details>
<summary>Abstract</summary>
This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAM-based seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation-specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:è¿™ä¸ªå·¥ä½œç›®æ ‡æ˜¯åˆ©ç”¨é¢„è®­ç»ƒåŸºæœ¬æ¨¡å‹ï¼Œå¦‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å’Œ segment anythingæ¨¡å‹ï¼ˆSAMï¼‰ï¼Œæ¥è§£å†³å¼±ç›‘ç£ semantic segmentationï¼ˆWSSSï¼‰é—®é¢˜ï¼Œä½¿ç”¨å›¾åƒçº§åˆ«æ ‡ç­¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç²—ç»†æ¡†æ¶ï¼ŒåŸºäº CLIP å’Œ SAMï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ segmentation çš„ç§å­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå›¾åƒåˆ†ç±»ä»»åŠ¡å’Œä¸€ä¸ªç§å­ segmentation ä»»åŠ¡ï¼Œç”± CLIP  WITH å†»ç»“å‚æ•°å’Œä¸¤ç»„å¯å­¦ä¹ çš„ä»»åŠ¡ç‰¹å®šæ¨èæ¥å…±åŒè¿›è¡Œæ‰§è¡Œã€‚SAM æ¨¡å—æ˜¯è®¾è®¡ç”¨äºæ¯ä¸ªä»»åŠ¡ï¼Œä»¥ç”Ÿæˆç²—ç»†æˆ–ç»†åŒ–ç§å­åœ°å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªå¤šæ ‡ç­¾å¯¹æ¯”æŸå¤±ï¼Œç”±å›¾åƒçº§åˆ«æ ‡ç­¾superviseï¼Œä»¥åŠä¸€ä¸ª CAM æ´»åŠ¨æŸå¤±ï¼Œç”±ç”Ÿæˆçš„ç²—ç»†ç§å­åœ°å›¾superviseã€‚è¿™äº›æŸå¤±ç”¨äºå­¦ä¹ æ¨èï¼Œæ¨èæ˜¯æˆ‘ä»¬frameworkä¸­å”¯ä¸€éœ€è¦å­¦ä¹ çš„éƒ¨åˆ†ã€‚ä¸€æ—¦æ¨èå­¦ä¹ å®Œæ¯•ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¯ä¸ªå›¾åƒä¸å­¦ä¹ çš„ segmentation ç‰¹å®šæ¨èè¾“å…¥åˆ° CLIP å’Œ SAM æ¨¡å—ä¸­ï¼Œç”Ÿæˆé«˜è´¨é‡ segmentation ç§å­ã€‚è¿™äº›ç§å­å¯ä»¥ä½œä¸º Pseudo æ ‡ç­¾æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†çš„ segmentation ç½‘ç»œï¼Œå¦‚å…¶ä»–ä¸¤ä¸ªé˜¶æ®µ WSSS æ–¹æ³•ã€‚å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PASCAL VOC 2012 å’Œ MS COCO 2014 ä¸Šè¾¾åˆ°äº†çŠ¶æ€ç›‘ç£æ€§çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸å…¶ä»–ä¸¤ä¸ªé˜¶æ®µ WSSS æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†ç«äº‰æ€§çš„ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Invariance-Causal-Representation-Learning-Prospects-and-Limitations"><a href="#Invariance-Causal-Representation-Learning-Prospects-and-Limitations" class="headerlink" title="Invariance &amp; Causal Representation Learning: Prospects and Limitations"></a>Invariance &amp; Causal Representation Learning: Prospects and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03580">http://arxiv.org/abs/2312.03580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Bing, Jonas Wahl, Urmi Ninad, Jakob Runge</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯å…³äº causal models ä¸­æœºåˆ¶çš„ä¸å˜æ€§çš„ç ”ç©¶ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº† theoretical impossibility results å’Œ practical considerations æ¥æ¢è®¨æœºåˆ¶ä¸å˜æ€§æ˜¯å¦èƒ½å¤Ÿç”¨äºæ‰¾åˆ° latent causal variablesã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œæœºåˆ¶ä¸å˜æ€§æœ¬èº«ä¸å¤Ÿä»¥ä¾¿ç¡®å®š latent causal variablesï¼Œéœ€è¦é‡‡ç”¨æ›´å¤šçš„çº¦æŸæ¥ç¡®å®šè¡¨ç¤ºã€‚<details>
<summary>Abstract</summary>
In causal models, a given mechanism is assumed to be invariant to changes of other mechanisms. While this principle has been utilized for inference in settings where the causal variables are observed, theoretical insights when the variables of interest are latent are largely missing. We assay the connection between invariance and causal representation learning by establishing impossibility results which show that invariance alone is insufficient to identify latent causal variables. Together with practical considerations, we use these theoretical findings to highlight the need for additional constraints in order to identify representations by exploiting invariance.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨ causal æ¨¡å‹ä¸­ï¼Œä¸€ä¸ªç»™å®šçš„æœºåˆ¶è¢«å‡è®¾ä¸ºå…¶ä»–æœºåˆ¶å˜åŒ–ä¸å˜ã€‚è™½ç„¶è¿™ä¸€åŸåˆ™åœ¨è§‚å¯Ÿ causal å˜é‡çš„æƒ…å†µä¸‹ç”¨äºæ¨ç†ï¼Œä½†åœ¨ latent å˜é‡çš„æƒ…å†µä¸‹çš„ç†è®ºå¯ç¤ºå‡ ä¹ç¼ºå¤±ã€‚æˆ‘ä»¬é€šè¿‡è¯æ˜ä¸å¯èƒ½æ€§ç»“è®ºè¡¨æ˜äº†å¯¹ latent  causal å˜é‡çš„å½’ä¸€åŒ–ä¸èƒ½å¤Ÿå”¯ä¸€ç¡®å®šã€‚ä¸å®é™…è€ƒè™‘ç›¸ç»“åˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›ç†è®ºå‘ç°æ¥å¼ºè°ƒéœ€è¦é¢å¤–çº¦æŸä»¥ä¾¿é€šè¿‡å½’ä¸€åŒ–æ¥ç¡®å®šè¡¨ç¤ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Generalization-to-New-Sequential-Decision-Making-Tasks-with-In-Context-Learning"><a href="#Generalization-to-New-Sequential-Decision-Making-Tasks-with-In-Context-Learning" class="headerlink" title="Generalization to New Sequential Decision Making Tasks with In-Context Learning"></a>Generalization to New Sequential Decision Making Tasks with In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03801">http://arxiv.org/abs/2312.03801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharath Chandra Raparthy, Eric Hambro, Robert Kirk, Mikael Henaff, Roberta Raileanu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ ä¸­è‡ªé€‚åº”ä»»åŠ¡å­¦ä¹ çš„é—®é¢˜ï¼Œå³ä½¿åªæœ‰å‡ ä¸ªç¤ºä¾‹ä¹Ÿèƒ½å¤Ÿå­¦ä¹ æ–°çš„è¯­è¨€æˆ–è§†è§‰ä»»åŠ¡ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº† transformer æ¥å­¦ä¹ æ–°çš„è¯­è¨€æˆ–è§†è§‰ä»»åŠ¡ï¼Œä½†æ˜¯åœ¨é¡ºåºå†³ç­–Settingä¸‹ï¼Œå®ƒä»¬æ— æ³•ç›´æ¥åº”ç”¨äºæ–°ä»»åŠ¡ä¸Šè¿›è¡Œå­¦ä¹ ã€‚ä½œè€…ä»¬åˆ™æå‡ºäº†ä¸€ç§ä½¿ç”¨åºåˆ—å¾„è¡Œçš„è®­ç»ƒæ–¹æ³•ï¼Œä»¥å®ç°åœ¨æ–°ä»»åŠ¡ä¸Šè¿›è¡Œå¾„è¡Œå­¦ä¹ ã€‚</li>
<li>results: ä½œè€…ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥è¯´æ˜ï¼Œé€šè¿‡è®­ç»ƒåºåˆ—å¾„è¡Œå¯ä»¥å®ç°åœ¨æ–°ä»»åŠ¡ä¸Šè¿›è¡Œå¾„è¡Œå­¦ä¹ ã€‚ä»–ä»¬è¿˜ç ”ç©¶äº†ä¸åŒçš„è®¾è®¡é€‰æ‹©ï¼Œå‘ç°æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†å¤§å°ã€æ›´å¤šçš„ä»»åŠ¡å¤šæ ·æ€§ã€ç¯å¢ƒéšæœºæ€§å’Œå¾„è¡Œå¼ºåº¦éƒ½ä¼šå¯¼è‡´æ›´å¥½çš„åœ¨æ–°ä»»åŠ¡ä¸Šè¿›è¡Œå¾„è¡Œå­¦ä¹ ã€‚é€šè¿‡è®­ç»ƒå¤§å‹å¤šæ ·åŒ–çš„ç¦»çº¿æ•°æ®é›†ï¼Œä»–ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨å‡ ä¸ªç¤ºä¾‹ä¸‹å­¦ä¹ æ–°çš„ MiniHack å’Œ Procgen ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
Training autonomous agents that can learn new tasks from only a handful of demonstrations is a long-standing problem in machine learning. Recently, transformers have been shown to learn new language or vision tasks without any weight updates from only a few examples, also referred to as in-context learning. However, the sequential decision making setting poses additional challenges having a lower tolerance for errors since the environment's stochasticity or the agent's actions can lead to unseen, and sometimes unrecoverable, states. In this paper, we use an illustrative example to show that naively applying transformers to sequential decision making problems does not enable in-context learning of new tasks. We then demonstrate how training on sequences of trajectories with certain distributional properties leads to in-context learning of new sequential decision making tasks. We investigate different design choices and find that larger model and dataset sizes, as well as more task diversity, environment stochasticity, and trajectory burstiness, all result in better in-context learning of new out-of-distribution tasks. By training on large diverse offline datasets, our model is able to learn new MiniHack and Procgen tasks without any weight updates from just a handful of demonstrations.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŸ¹è®­è‡ªé€‚åº”ä»£ç†äººå¯ä»¥ä»åªæœ‰å‡ ä¸ªç¤ºä¾‹å­¦ä¹ æ–°ä»»åŠ¡æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„é•¿æœŸé—®é¢˜ã€‚æœ€è¿‘ï¼Œ transformers è¢«è¯æ˜å¯ä»¥ä»åªæœ‰å‡ ä¸ªç¤ºä¾‹å­¦ä¹ æ–°è¯­è¨€æˆ–è§†è§‰ä»»åŠ¡ï¼Œè€Œæ— éœ€ä»»ä½•å‚æ•°æ›´æ–°ï¼Œä¹Ÿç§°ä¸ºå†…Contextå­¦ä¹ ã€‚ç„¶è€Œï¼Œé¡ºåºå†³ç­–è®¾ç½®å¢åŠ äº†æ›´é«˜çš„é”™è¯¯å¿å®¹ç‡ï¼Œå› ä¸ºç¯å¢ƒçš„éšæœºæ€§æˆ–è€…ä»£ç†äººçš„æ“ä½œå¯èƒ½ä¼šå¯¼è‡´æœªçœ‹è¿‡çš„ã€æœ‰æ—¶æ— æ³•æ¢å¤çš„çŠ¶æ€ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ª illustrate ä¾‹å­æ¥è¡¨æ˜ï¼Œç›´æ¥åº”ç”¨ transformers åˆ°é¡ºåºå†³ç­–é—®é¢˜ä¸Šä¸èƒ½å®ç°å†…Contextå­¦ä¹ æ–°ä»»åŠ¡ã€‚ç„¶åï¼Œæˆ‘ä»¬ç¤ºä¾‹äº†åœ¨åºåˆ—å¾„è¿¹ä¸­è®­ç»ƒæ—¶ï¼Œé‡‡ç”¨æŸäº›åˆ†å¸ƒæ€§è´¨å¯ä»¥å®ç°å†…Contextå­¦ä¹ æ–°é¡ºåºå†³ç­–ä»»åŠ¡ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†ä¸åŒçš„è®¾è®¡é€‰æ‹©ï¼Œå¹¶å‘ç°å¤§å‹æ¨¡å‹å’Œæ•°æ®é›†å¤§å°ã€ä»»åŠ¡å¤šæ ·æ€§ã€ç¯å¢ƒéšæœºæ€§å’Œå¾„è¿¹å¼ºçƒˆç¨‹åº¦éƒ½ä¼šå¯¼è‡´æ›´å¥½çš„å†…Contextå­¦ä¹ æ–°Out-of-distributionä»»åŠ¡ã€‚é€šè¿‡è®­ç»ƒå¤§å‹å¤šæ ·åŒ–çš„ç¦»çº¿æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä»å‡ ä¸ªç¤ºä¾‹å­¦ä¹ æ–° MiniHack å’Œ Procgen ä»»åŠ¡ï¼Œæ— éœ€ä»»ä½•å‚æ•°æ›´æ–°ã€‚
</details></li>
</ul>
<hr>
<h2 id="GPT-4-Enhanced-Multimodal-Grounding-for-Autonomous-Driving-Leveraging-Cross-Modal-Attention-with-Large-Language-Models"><a href="#GPT-4-Enhanced-Multimodal-Grounding-for-Autonomous-Driving-Leveraging-Cross-Modal-Attention-with-Large-Language-Models" class="headerlink" title="GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models"></a>GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03543">http://arxiv.org/abs/2312.03543</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/petrichor625/talk2car_cavg">https://github.com/petrichor625/talk2car_cavg</a></li>
<li>paper_authors: Haicheng Liao, Huanming Shen, Zhenning Li, Chengyue Wang, Guofa Li, Yiming Bie, Chengzhong Xu</li>
<li>for: This paper aims to improve the ability of autonomous vehicles (AVs) to understand and execute visual commands in a visual context.</li>
<li>methods: The authors propose a sophisticated encoder-decoder framework called Context-Aware Visual Grounding (CAVG), which integrates five core encoders (Text, Image, Context, and Cross-Modal) with a Multimodal decoder. The model is trained using state-of-the-art Large Language Models (LLMs) and incorporates multi-head cross-modal attention mechanisms and a Region-Specific Dynamic (RSD) layer for attention modulation.</li>
<li>results: The CAVG model achieves new standards in prediction accuracy and operational efficiency on the Talk2Car dataset, a real-world benchmark. It demonstrates exceptional performance even with limited training data, and shows remarkable robustness and adaptability in challenging scenarios such as long-text command interpretation, low-light conditions, ambiguous command contexts, inclement weather conditions, and densely populated urban environments.<details>
<summary>Abstract</summary>
In the field of autonomous vehicles (AVs), accurately discerning commander intent and executing linguistic commands within a visual context presents a significant challenge. This paper introduces a sophisticated encoder-decoder framework, developed to address visual grounding in AVs.Our Context-Aware Visual Grounding (CAVG) model is an advanced system that integrates five core encoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This integration enables the CAVG model to adeptly capture contextual semantics and to learn human emotional features, augmented by state-of-the-art Large Language Models (LLMs) including GPT-4. The architecture of CAVG is reinforced by the implementation of multi-head cross-modal attention mechanisms and a Region-Specific Dynamic (RSD) layer for attention modulation. This architectural design enables the model to efficiently process and interpret a range of cross-modal inputs, yielding a comprehensive understanding of the correlation between verbal commands and corresponding visual scenes. Empirical evaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that CAVG establishes new standards in prediction accuracy and operational efficiency. Notably, the model exhibits exceptional performance even with limited training data, ranging from 50% to 75% of the full dataset. This feature highlights its effectiveness and potential for deployment in practical AV applications. Moreover, CAVG has shown remarkable robustness and adaptability in challenging scenarios, including long-text command interpretation, low-light conditions, ambiguous command contexts, inclement weather conditions, and densely populated urban environments. The code for the proposed model is available at our Github.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è‡ªåŠ¨é©¾é©¶è½¦ï¼ˆAVï¼‰é¢†åŸŸï¼Œæ­£ç¡®åœ°ç†è§£æŒ‡æŒ¥å®˜æ„å›¾å¹¶åœ¨è§†è§‰ä¸Šå‘å‡ºè¯­è¨€å‘½ä»¤æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜çº§çš„encoder-decoderæ¡†æ¶ï¼Œç”¨äºè§£å†³AVä¸­çš„è§†è§‰å®šä½ã€‚æˆ‘ä»¬çš„ Context-Aware Visual Groundingï¼ˆCAVGï¼‰æ¨¡å‹åŒ…æ‹¬äº”ç§æ ¸å¿ƒencoderâ€”â€”Textã€Imageã€Contextã€Cross-Modalâ€”â€”ä»¥åŠä¸€ä¸ªMultimodal decoderã€‚è¿™ç§æ•´åˆä½¿å¾—CAVGæ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°æ•æ‰Contextual semanticsï¼Œå¹¶é€šè¿‡ä½¿ç”¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒ…æ‹¬GPT-4ï¼Œå­¦ä¹ äººç±»æƒ…æ„Ÿç‰¹å¾ã€‚CAVGæ¨¡å‹çš„architectureè¢«å¼ºåŒ–äº†å¤šå¤´è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’ŒRegion-Specific Dynamicï¼ˆRSDï¼‰å±‚ Ğ´Ğ»Ñæ³¨æ„åŠ›è°ƒæ•´ã€‚è¿™ç§å»ºç«‹çš„å»ºç­‘ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å’Œè§£é‡Šå¤šç§è·¨æ¨¡æ€è¾“å…¥ï¼Œä»è€Œè·å¾—è§†è§‰ä¸Šçš„commandå’Œå¯¹åº”çš„è¯­è¨€å‘½ä»¤ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒè¯æ˜ï¼ŒCAVGåœ¨Talk2Caræ•°æ®é›†ä¸Šè¾¾åˆ°äº†æ–°çš„æ ‡å‡†ï¼Œå¹¶ä¸”åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸Šè¾¾åˆ°äº†å‡ºè‰²çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒCAVGæ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ä¹Ÿè¡¨ç°å‡ºäº†æ°å‡ºçš„Robustnesså’Œé€‚åº”æ€§ï¼ŒåŒ…æ‹¬é•¿æ–‡æœ¬å‘½ä»¤è§£é‡Šã€ä½å…‰ç…§æ¡ä»¶ã€ä¸ç¡®å®šçš„æŒ‡æŒ¥å®˜ä¸Šä¸‹æ–‡ã€ä¸å¥½çš„å¤©æ°”æ¡ä»¶å’Œæ‹¥æŒ¤çš„åŸå¸‚ç¯å¢ƒã€‚CAVGæ¨¡å‹çš„ä»£ç å¯ä»¥åœ¨æˆ‘ä»¬çš„Githubä¸Šè·å¾—ã€‚
</details></li>
</ul>
<hr>
<h2 id="Low-power-Continuous-Remote-Behavioral-Localization-with-Event-Cameras"><a href="#Low-power-Continuous-Remote-Behavioral-Localization-with-Event-Cameras" class="headerlink" title="Low-power, Continuous Remote Behavioral Localization with Event Cameras"></a>Low-power, Continuous Remote Behavioral Localization with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03799">http://arxiv.org/abs/2312.03799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Friedhelm Hamann, Suman Ghosh, Ignacio Juarez Martinez, Tom Hart, Alex Kacelnik, Guillermo Gallego</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ç”¨äºè¿œç¨‹é‡å¤–åŠ¨ç‰©è§‚å¯Ÿçš„å¯é è®¡ç®—æœºè§†è§‰æ–¹æ³•ï¼Œä»¥ automatize åŠ¨ç‰©è¡Œä¸ºé‡åŒ–ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†äº‹ä»¶ç›¸æœºï¼Œå…·æœ‰ä½åŠŸè€—å’Œé«˜åŠ¨æ€èŒƒå›´ç‰¹æ€§ï¼Œå¯¹ remote é‡å¤–åŠ¨ç‰©è§‚å¯Ÿè¿›è¡Œäº† battery-dependent ç›‘æµ‹ã€‚ç ”ç©¶é‡‡ç”¨äº†æ—¶é—´åŠ¨ä½œæ£€æµ‹ä»»åŠ¡ï¼Œæ ¹æ®äº‹ä»¶æ•°æ®è¿›è¡Œäº†16ä¸ªå·¢çš„æ ‡æ³¨ã€‚å¼€å‘çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç”Ÿæˆå‡ ä¸ªå¯èƒ½çš„æ—¶é—´é—´éš”ï¼ˆææ¡ˆï¼‰çš„ç”Ÿæˆå™¨ï¼Œä»¥åŠä¸€ä¸ªå†…éƒ¨ç±»åˆ«åŠ¨ä½œçš„åˆ†ç±»å™¨ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œäº‹ä»¶ç›¸æœºçš„è‡ªç„¶å“åº”äºè¿åŠ¨éå¸¸æœ‰æ•ˆï¼Œå¯ä»¥å®ç° kontinuous åŠ¨ç‰©ç›‘æµ‹å’Œæ£€æµ‹ï¼ŒmAP ä¸º 58%ï¼ˆåœ¨è‰¯å¥½å¤©æ°”æƒ…å†µä¸‹æé«˜åˆ° 63%ï¼‰ã€‚ç ”ç©¶è¿˜è¡¨æ˜äº†å¯¹ä¸åŒç…§æ˜æ¡ä»¶çš„Robustnessã€‚ä½¿ç”¨äº‹ä»¶ç›¸æœºè®°å½•åŠ¨ç‰©è¡Œä¸ºå¯ä»¥ä¸‰å€é•¿äºä½¿ç”¨ conventunal ç›¸æœºã€‚æœ¬ç ”ç©¶å¼€æ‹“äº†è¿œç¨‹é‡å¤–åŠ¨ç‰©è§‚å¯Ÿé¢†åŸŸçš„æ–°å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
Researchers in natural science need reliable methods for quantifying animal behavior. Recently, numerous computer vision methods emerged to automate the process. However, observing wild species at remote locations remains a challenging task due to difficult lighting conditions and constraints on power supply and data storage. Event cameras offer unique advantages for battery-dependent remote monitoring due to their low power consumption and high dynamic range capabilities. We use this novel sensor to quantify a behavior in Chinstrap penguins called ecstatic display. We formulate the problem as a temporal action detection task, determining the start and end times of the behavior. For this purpose, we recorded a colony of breeding penguins in Antarctica during several weeks and labeled event data on 16 nests. The developed method consists of a generator of candidate time intervals (proposals) and a classifier of the actions within them. The experiments show that the event cameras' natural response to motion is effective for continuous behavior monitoring and detection, reaching a mean average precision (mAP) of 58% (which increases to 63% in good weather conditions). The results also demonstrate the robustness against various lighting conditions contained in the challenging dataset. The low-power capabilities of the event camera allows to record three times longer than with a conventional camera. This work pioneers the use of event cameras for remote wildlife observation, opening new interdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Diversity-and-Realism-of-Distilled-Dataset-An-Efficient-Dataset-Distillation-Paradigm"><a href="#On-the-Diversity-and-Realism-of-Distilled-Dataset-An-Efficient-Dataset-Distillation-Paradigm" class="headerlink" title="On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm"></a>On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03526">http://arxiv.org/abs/2312.03526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Sun, Bei Shi, Daiwei Yu, Tao Lin</li>
<li>For: This paper aims to improve the efficiency and practicality of dataset distillation for large-scale real-world applications.* Methods: The proposed method, RDED, focuses on three key properties (realism, diversity, and efficiency) and uses a novel computationally-efficient approach to distill large datasets.* Results: RDED achieves notable results, including distilling the full ImageNet-1K to a small dataset within 7 minutes and achieving a 42% top-1 accuracy with ResNet-18 on a single GPU, outperforming the state-of-the-art.<details>
<summary>Abstract</summary>
Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes, achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU (while the SOTA only achieves 21% but requires 6 hours).
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£æœºå™¨å­¦ä¹ éœ€è¦è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œï¼Œå› æ­¤é¢ä¸´é«˜è®¡ç®—éœ€æ±‚çš„æŒ‘æˆ˜ã€‚ dataset distillation ä½œä¸ºä¸€ç§æ–°å…´ç­–ç•¥ï¼Œç›®çš„æ˜¯å‹ç¼©ç°å®ä¸–ç•Œæ•°æ®é›†ï¼Œä»¥ä¾¿é«˜æ•ˆåœ°è®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™ä¸€ç ”ç©¶ç°åœ¨å—åˆ°å¤§è§„æ¨¡é«˜åˆ†è¾¨ç‡æ•°æ®é›†çš„é™åˆ¶ï¼Œä½¿å…¶å®é™…æ€§å’Œå¯è¡Œæ€§å—åˆ°æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†ç°æœ‰çš„ dataset distillation æ–¹æ³•ï¼Œå¹¶ç¡®å®šäº†å¤§è§„æ¨¡å®é™…åº”ç”¨ä¸­éœ€è¦çš„ä¸‰ä¸ªå±æ€§ï¼Œ namelyï¼Œrealismï¼Œ diversityï¼Œ and efficiencyã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æè®® RDEDï¼Œä¸€ç§æ–°çš„è®¡ç®—æ•ˆç‡é«˜ï¼Œ yet effective æ•°æ®å‹ç¼© paradigmï¼Œä»¥å®ç°æ•°æ®çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRDED å¯ä»¥åœ¨ 7 åˆ†é’Ÿå†…ï¼Œå°†æ•´ä¸ª ImageNet-1K æ•°æ®é›†å‹ç¼©æˆ 10 å¼ å›¾åƒæ¯ä¸ªç±»å‹çš„å°æ•°æ®é›†ï¼Œå¹¶åœ¨ ResNet-18 ä¸Š achieved 42% top-1 å‡†ç¡®ç‡ï¼ˆè€Œ SOTA åªèƒ½è¾¾åˆ° 21%ï¼Œå¹¶éœ€è¦ 6 å°æ—¶ï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multi-Scale-and-Multi-Modal-Contrastive-Learning-Network-for-Biomedical-Time-Series"><a href="#Multi-Scale-and-Multi-Modal-Contrastive-Learning-Network-for-Biomedical-Time-Series" class="headerlink" title="Multi-Scale and Multi-Modal Contrastive Learning Network for Biomedical Time Series"></a>Multi-Scale and Multi-Modal Contrastive Learning Network for Biomedical Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03796">http://arxiv.org/abs/2312.03796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongbo Guo, Xinzi Xu, Hao Wu, Guoxing Wang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ä¸ªå¤šæ¨¡å¼ç”Ÿç‰©åŒ»æ—¶é—´åºåˆ—èµ„æ–™çš„å­¦ä¹ æ¨¡å‹ï¼Œä»¥å®ç°å¤šæ¨¡å¼é—´çš„è·¨åº¦æ±‡æµå’Œè·¨æ¨¡å¼è½¬æ¢ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šå°ºåº¦å’Œå¤šæ¨¡å¼çš„ç”Ÿç‰©åŒ»æ—¶é—´åºåˆ—è¡¨ç°å­¦ä¹ ç½‘ç»œï¼ˆMBSLï¼‰ï¼Œå…·æœ‰å¯¹ç…§å­¦ä¹ æ¥å®ç°å¤šæ¨¡å¼é—´çš„è·¨åº¦æ±‡æµå’Œè·¨æ¨¡å¼è½¬æ¢ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMBSLæ¯”å‰ä¸€ä»£æ¨¡å‹é«˜å‡º33.9%çš„å¹³å‡è¯¯å·®ï¼ˆMAEï¼‰åœ¨å‘¼å¸é€Ÿç‡æµ‹é‡ã€13.8% MAEåœ¨è¿åŠ¨å¿ƒç‡æµ‹é‡ã€1.41%çš„å‡†ç¡®ç‡åœ¨äººç±»æ´»åŠ¨è¯†åˆ«å’Œ1.14%çš„F1åˆ†æ•°åœ¨å‘¼å¸æš‚åœç—‡å€™ç¾¤è¯†åˆ«ç­‰å››ä¸ªç”Ÿç‰©åŒ»åº”ç”¨ä¸­ã€‚<details>
<summary>Abstract</summary>
Multi-modal biomedical time series (MBTS) data offers a holistic view of the physiological state, holding significant importance in various bio-medical applications. Owing to inherent noise and distribution gaps across different modalities, MBTS can be complex to model. Various deep learning models have been developed to learn representations of MBTS but still fall short in robustness due to the ignorance of modal-to-modal variations. This paper presents a multi-scale and multi-modal biomedical time series representation learning (MBSL) network with contrastive learning to migrate these variations. Firstly, MBTS is grouped based on inter-modal distances, then each group with minimum intra-modal variations can be effectively modeled by individual encoders. Besides, to enhance the multi-scale feature extraction (encoder), various patch lengths and mask ratios are designed to generate tokens with semantic information at different scales and diverse contextual perspectives respectively. Finally, cross-modal contrastive learning is proposed to maximize consistency among inter-modal groups, maintaining useful information and eliminating noises. Experiments against four bio-medical applications show that MBSL outperforms state-of-the-art models by 33.9% mean average errors (MAE) in respiration rate, by 13.8% MAE in exercise heart rate, by 1.41% accuracy in human activity recognition, and by 1.14% F1-score in obstructive sleep apnea-hypopnea syndrome.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šModalç”Ÿç‰©åŒ»å­¦æ—¶é—´åºåˆ—æ•°æ®ï¼ˆMBTSï¼‰å…·æœ‰æ•´ä½“ç”Ÿç†çŠ¶æ€çš„å…¨é¢è§†å›¾ï¼Œåœ¨å„ç§ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œç”±äºä¸åŒmodalitiesä¹‹é—´çš„é™„åŠ å™ªå£°å’Œåˆ†å¸ƒå·®å¼‚ï¼ŒMBTSå¯èƒ½ä¼šå˜å¾—å¤æ‚ã€‚ä¸ºäº†å­¦ä¹ MBTSçš„è¡¨ç¤ºï¼Œå„ç§æ·±åº¦å­¦ä¹ æ¨¡å‹å·²ç»è¢«å¼€å‘å‡ºæ¥ï¼Œä½†ä»ç„¶ç¼ºä¹robustnessï¼Œå³å› ä¸ºå¿½ç•¥ä¸åŒmodalitiesä¹‹é—´çš„å˜åŒ–ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šå°ºåº¦å’Œå¤šModalç”Ÿç‰©åŒ»å­¦æ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ ï¼ˆMBSLï¼‰ç½‘ç»œï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¥è¿ç§»è¿™äº›å˜åŒ–ã€‚é¦–å…ˆï¼ŒMBTSè¢«åˆ†ç»„ Based on inter-modal distancesï¼Œç„¶åæ¯ä¸ªç»„çš„æœ€å°å†…Modalå·®å¼‚å¯ä»¥è¢«ä¸ªæ€§åŒ–Encoderæ¨¡å‹æœ‰æ•ˆåœ°æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºå¤šå°ºåº¦ç‰¹å¾æå–ï¼ˆEncoderï¼‰ï¼Œå„ç§patché•¿åº¦å’Œmaskæ¯”ä¾‹è¢«è®¾è®¡å‡ºæ¥ï¼Œä»¥ç”Ÿæˆå…·æœ‰Semanticä¿¡æ¯çš„Tokenåœ¨ä¸åŒçš„å°ºåº¦å’Œå¤šç§æ–‡è„‰ä¸Šã€‚æœ€åï¼Œè·¨Modalå¯¹æ¯”å­¦ä¹ è¢«æå‡ºï¼Œä»¥æœ€å¤§åŒ–inter-Modalç»„çš„ä¸€è‡´æ€§ï¼Œä¿ç•™æœ‰ç”¨ä¿¡æ¯ï¼Œå¹¶æ¶ˆé™¤å™ªå£°ã€‚å¯¹å››ç§ç”Ÿç‰©åŒ»å­¦åº”ç”¨è¿›è¡Œäº†å®éªŒï¼Œç ”ç©¶å‘ç°ï¼ŒMBSLæ¯”State-of-the-artæ¨¡å‹æé«˜33.9%çš„ Mean Average Errorï¼ˆMAEï¼‰ã€13.8%çš„ Exercise Heart Rate MAEã€1.41%çš„ Human Activity Recognition Accuracyå’Œ1.14%çš„ Obstructive Sleep Apnea-Hypopnea Syndrome F1 Scoreã€‚
</details></li>
</ul>
<hr>
<h2 id="Optimal-Wildfire-Escape-Route-Planning-for-Drones-under-Dynamic-Fire-and-Smoke"><a href="#Optimal-Wildfire-Escape-Route-Planning-for-Drones-under-Dynamic-Fire-and-Smoke" class="headerlink" title="Optimal Wildfire Escape Route Planning for Drones under Dynamic Fire and Smoke"></a>Optimal Wildfire Escape Route Planning for Drones under Dynamic Fire and Smoke</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03521">http://arxiv.org/abs/2312.03521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Liu, Tamas Sziranyi</li>
<li>for:  aid wildfire management efforts by planning an optimal escape route for drones</li>
<li>methods:  use information fusion between UAV and satellite, multi-channel remote sensing data, UAV vision technology, and improved A* algorithm</li>
<li>results:  enhance the safety and efficiency of drone operations in wildfire environments by considering dynamic fire and smoke models<details>
<summary>Abstract</summary>
In recent years, the increasing prevalence and intensity of wildfires have posed significant challenges to emergency response teams. The utilization of unmanned aerial vehicles (UAVs), commonly known as drones, has shown promise in aiding wildfire management efforts. This work focuses on the development of an optimal wildfire escape route planning system specifically designed for drones, considering dynamic fire and smoke models. First, the location of the source of the wildfire can be well located by information fusion between UAV and satellite, and the road conditions in the vicinity of the fire can be assessed and analyzed using multi-channel remote sensing data. Second, the road network can be extracted and segmented in real time using UAV vision technology, and each road in the road network map can be given priority based on the results of road condition classification. Third, the spread model of dynamic fires calculates the new location of the fire source based on the fire intensity, wind speed and direction, and the radius increases as the wildfire spreads. Smoke is generated around the fire source to create a visual representation of a burning fire. Finally, based on the improved A* algorithm, which considers all the above factors, the UAV can quickly plan an escape route based on the starting and destination locations that avoid the location of the fire source and the area where it is spreading. By considering dynamic fire and smoke models, the proposed system enhances the safety and efficiency of drone operations in wildfire environments.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œé‡ç«çš„å‘ç”Ÿå’Œæ‰©æ•£çš„æƒ…å†µæ—¥ç›Šä¸¥é‡ï¼Œå¯¹æŠ¢æ•‘é˜Ÿä¼æå‡ºäº†æå¤§çš„æŒ‘æˆ˜ã€‚ä½¿ç”¨æ— äººé£è¡Œå™¨ï¼ˆUAVï¼‰çš„åº”ç”¨æ˜¾ç¤ºäº†å¸®åŠ©é‡ç«ç®¡ç†çš„æ½œåœ¨ä¼˜åŠ¿ã€‚æœ¬å·¥ä½œå…³æ³¨äºåŸºäºUAVçš„é‡ç«é€ƒç”Ÿè·¯å¾„è§„åˆ’ç³»ç»Ÿçš„å¼€å‘ï¼Œè€ƒè™‘äº†åŠ¨æ€ç«ç„°å’ŒçƒŸé›¾æ¨¡å‹ã€‚é¦–å…ˆï¼Œé€šè¿‡UAVå’Œå«æ˜Ÿä¿¡æ¯èåˆï¼Œå¯ä»¥å‡†ç¡®åœ°ç¡®å®šé‡ç«çš„èµ·ç‚¹ä½ç½®ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¤šé€šé“è¿œç¨‹æ„ŸçŸ¥æŠ€æœ¯ï¼Œåœ¨é‡ç«é™„è¿‘åœ°åŒºå®æ—¶æå–å’Œåˆ†ç±»é“è·¯ç½‘ç»œåœ°å›¾ï¼Œå¹¶å°†æ¯æ¡é“è·¯åœ¨é“è·¯ç½‘ç»œåœ°å›¾ä¸­åˆ†é…ä¼˜å…ˆçº§ã€‚ç¬¬ä¸‰ï¼Œæ ¹æ®åŠ¨æ€ç«ç„°æ‰©æ•£æ¨¡å‹ï¼Œè®¡ç®—æ–°çš„ç«æºä½ç½®ï¼Œä»¥åŠç«ç„°å¼ºåº¦ã€é£é€Ÿå’Œæ–¹å‘ã€‚çƒŸé›¾åœ¨ç«æºå‘¨å›´ç”Ÿæˆï¼Œåˆ›é€ ä¸€ä¸ªç‡ƒçƒ§ç«çš„è§†è§‰è¡¨ç°ã€‚æœ€åï¼ŒåŸºäºæ”¹è¿›çš„A*ç®—æ³•ï¼Œè€ƒè™‘äº†ä»¥ä¸Šå› ç´ ï¼ŒUAVå¿«é€Ÿè®¡åˆ’é€ƒç”Ÿè·¯å¾„ï¼Œé¿å…ç«æºä½ç½®å’Œæ‰©æ•£çš„åœ°åŒºã€‚ç”±äºè€ƒè™‘äº†åŠ¨æ€ç«ç„°å’ŒçƒŸé›¾æ¨¡å‹ï¼Œæå‡ºçš„ç³»ç»Ÿæé«˜äº†æ— äººæœºåœ¨é‡ç«ç¯å¢ƒä¸­çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Defense-Against-Adversarial-Attacks-using-Convolutional-Auto-Encoders"><a href="#Defense-Against-Adversarial-Attacks-using-Convolutional-Auto-Encoders" class="headerlink" title="Defense Against Adversarial Attacks using Convolutional Auto-Encoders"></a>Defense Against Adversarial Attacks using Convolutional Auto-Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03520">http://arxiv.org/abs/2312.03520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyasi Mandal</li>
<li>for: å¼ºåŒ–ç›®æ ‡åˆ†ç±»å™¨æ¨¡å‹å¯¹æŠ—æ”»å‡»</li>
<li>methods: ä½¿ç”¨å·ç§¯è‡ªé€‚åº”å™¨æ¨¡å‹å¯¹æŠ—æ”»å‡»</li>
<li>results: å®ç°æ¨¡å‹ç²¾åº¦çš„Restore<details>
<summary>Abstract</summary>
Deep learning models, while achieving state-of-the-art performance on many tasks, are susceptible to adversarial attacks that exploit inherent vulnerabilities in their architectures. Adversarial attacks manipulate the input data with imperceptible perturbations, causing the model to misclassify the data or produce erroneous outputs. This work is based on enhancing the robustness of targeted classifier models against adversarial attacks. To achieve this, an convolutional autoencoder-based approach is employed that effectively counters adversarial perturbations introduced to the input images. By generating images closely resembling the input images, the proposed methodology aims to restore the model's accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥è¾¾åˆ°è®¸å¤šä»»åŠ¡çš„çŠ¶æ€å‰æ²¿æ€§è¡¨ç°ï¼Œä½†å—åˆ°é’ˆå¯¹æ€§æ”»å‡»çš„å¨èƒã€‚è¿™äº›æ”»å‡»é€šè¿‡ manipulate è¾“å…¥æ•°æ®ä¸­çš„å¾®scopic å˜åŒ–ï¼Œä½¿æ¨¡å‹é”™åˆ†æˆ–ç”Ÿæˆé”™è¯¯çš„è¾“å‡ºã€‚è¿™é¡¹å·¥ä½œæ˜¯åŸºäºå¢å¼ºç›®æ ‡åˆ†ç±»å™¨æ¨¡å‹å¯¹é’ˆå¯¹æ€§æ”»å‡»çš„Robustnessã€‚ä¸ºè¾¾åˆ°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºå·ç§¯ autoencoder çš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå¯¹è¾“å…¥å›¾åƒä¸­çš„é’ˆå¯¹æ€§æ”»å‡»è¿›è¡Œåº”å¯¹ã€‚é€šè¿‡ç”Ÿæˆä¸è¾“å…¥å›¾åƒå‡ ä¹ç›¸åŒçš„å›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¸Œæœ›å¯ä»¥æ¢å¤æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Active-Wildfires-Detection-and-Dynamic-Escape-Routes-Planning-for-Humans-through-Information-Fusion-between-Drones-and-Satellites"><a href="#Active-Wildfires-Detection-and-Dynamic-Escape-Routes-Planning-for-Humans-through-Information-Fusion-between-Drones-and-Satellites" class="headerlink" title="Active Wildfires Detection and Dynamic Escape Routes Planning for Humans through Information Fusion between Drones and Satellites"></a>Active Wildfires Detection and Dynamic Escape Routes Planning for Humans through Information Fusion between Drones and Satellites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03519">http://arxiv.org/abs/2312.03519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Liu, Tamas Sziranyi</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§åŸºäºUAVè§†è§‰æŠ€æœ¯å’Œå«æ˜Ÿå›¾åƒåˆ†ææŠ€æœ¯çš„åŠ¨æ€äººå‘˜æ•‘æ´è·¯å¾„è§„åˆ’æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å’Œè¯†åˆ«é‡å¤–ç«ç¾çš„ç«æºä½ç½®å’Œç‡ƒçƒ§åŒºåŸŸï¼Œå¹¶ä¸ºäººä»¬æä¾›å®æ—¶çš„é€ƒç”Ÿè·¯å¾„è§„åˆ’ã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬Sentinel 2å«æ˜Ÿå›¾åƒåˆ†æã€D-linkNetå’ŒNDVIå€¼çš„ä¸­å¿ƒåŒºåŸŸç‡ƒçƒ§ç«æºåˆ†å‰²ã€äººå‘˜å®æ—¶åŠ¨æ€æœ€ä½³è·¯å¾„è§„åˆ’ç­‰ã€‚</li>
<li>results: å¯¹äº8æœˆ24æ—¥é‡åº†é‡ç«çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œç»“æœè¡¨æ˜ï¼ŒåŸºäºUAVå’Œå«æ˜Ÿå›¾åƒä¿¡æ¯çš„åŠ¨æ€æœ€ä½³è·¯å¾„è§„åˆ’ç®—æ³•å¯ä»¥åœ¨å®æ—¶ç«ç¾æƒ…å†µä¸‹ä¸ºäººä»¬æä¾›æœ€ä½³é€ƒç”Ÿè·¯å¾„ã€‚<details>
<summary>Abstract</summary>
UAVs are playing an increasingly important role in the field of wilderness rescue by virtue of their flexibility. This paper proposes a fusion of UAV vision technology and satellite image analysis technology for active wildfires detection and road networks extraction of wildfire areas and real-time dynamic escape route planning for people in distress. Firstly, the fire source location and the segmentation of smoke and flames are targeted based on Sentinel 2 satellite imagery. Secondly, the road segmentation and the road condition assessment are performed by D-linkNet and NDVI values in the central area of the fire source by UAV. Finally, the dynamic optimal route planning for humans in real time is performed by the weighted A* algorithm in the road network with the dynamic fire spread model. Taking the Chongqing wildfire on August 24, 2022, as a case study, the results demonstrate that the dynamic escape route planning algorithm can provide an optimal real-time navigation path for humans in the presence of fire through the information fusion of UAVs and satellites.
</details>
<details>
<summary>æ‘˜è¦</summary>
UAVs åœ¨é‡å¤–æœæ•‘ä¸­å‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ï¼Œå°¤å…¶æ˜¯å› ä¸ºå®ƒä»¬çš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºäº†ç»“åˆ UAV è§†è§‰æŠ€æœ¯å’Œå«æ˜Ÿå›¾åƒåˆ†ææŠ€æœ¯ï¼Œå®æ—¶è®¡ç®— wildfires çš„å‘ç”Ÿåœ°ç‚¹å’Œç‡ƒçƒ§åŒºåŸŸçš„é“è·¯ç½‘ç»œæŠ½å–ï¼Œä»¥åŠåœ¨äººå‘˜å—æŸæ—¶çš„å®æ—¶æœ€ä¼˜è·¯å¾„è§„åˆ’ã€‚é¦–å…ˆï¼Œé€šè¿‡ sentinel 2 å«æ˜Ÿå›¾åƒï¼Œå®šä½ç«æºä½ç½®å’ŒçƒŸé›¾é¢—ç²’çš„åˆ† segmentationã€‚å…¶æ¬¡ï¼Œé€šè¿‡ D-linkNet å’Œ NDVI å€¼åœ¨ä¸­å¿ƒåœ°åŸŸçš„ç«æºä½ç½®ï¼Œè¿›è¡Œé“è·¯åˆ† segmentation å’Œé“è·¯çŠ¶å†µè¯„ä¼°ã€‚æœ€åï¼Œåœ¨è·¯ç½‘ä¸­ï¼Œä½¿ç”¨åŠ æƒ A\* ç®—æ³•ï¼Œåœ¨å®æ—¶ç«åŠ¿æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä¸ºäººå‘˜åœ¨ç«ç¾ä¸­æä¾›æœ€ä¼˜çš„å®æ—¶å¯¼èˆªè·¯å¾„ã€‚ä»¥2022å¹´8æœˆ24æ—¥çš„é‡åº†é‡ç«ä¸ºä¾‹ï¼Œç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€é€ƒç”Ÿè·¯å¾„è§„åˆ’ç®—æ³•å¯ä»¥åœ¨ UAV å’Œå«æ˜Ÿä¿¡æ¯èåˆçš„æƒ…å†µä¸‹ï¼Œä¸ºäººå‘˜åœ¨ç«ç¾ä¸­æä¾›æœ€ä¼˜çš„å®æ—¶å¯¼èˆªè·¯å¾„ã€‚
</details></li>
</ul>
<hr>
<h2 id="FRDiff-Feature-Reuse-for-Exquisite-Zero-shot-Acceleration-of-Diffusion-Models"><a href="#FRDiff-Feature-Reuse-for-Exquisite-Zero-shot-Acceleration-of-Diffusion-Models" class="headerlink" title="FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models"></a>FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03517">http://arxiv.org/abs/2312.03517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhyuk So, Jungwon Lee, Eunhyeok Park<br>for: æé«˜Diffusionæ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼Œä½¿å…¶æ›´åŠ å¹¿æ³›åº”ç”¨ã€‚methods: åˆ©ç”¨æ—¶é—´ç›¸ä¼¼æ€§ redundancyï¼Œé‡ç”¨ç‰¹å¾å›¾ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚results: æå‡ºFRDiffæ–¹æ³•ï¼Œå®ç°äº†ç²¾åº¦å’Œå“åº”é€Ÿåº¦ä¹‹é—´çš„å¹³è¡¡ï¼Œåœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­è·å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚<details>
<summary>Abstract</summary>
The substantial computational costs of diffusion models, particularly due to the repeated denoising steps crucial for high-quality image generation, present a major obstacle to their widespread adoption. While several studies have attempted to address this issue by reducing the number of score function evaluations using advanced ODE solvers without fine-tuning, the decreased number of denoising iterations misses the opportunity to update fine details, resulting in noticeable quality degradation. In our work, we introduce an advanced acceleration technique that leverages the temporal redundancy inherent in diffusion models. Reusing feature maps with high temporal similarity opens up a new opportunity to save computation without sacrificing output quality. To realize the practical benefits of this intuition, we conduct an extensive analysis and propose a novel method, FRDiff. FRDiff is designed to harness the advantages of both reduced NFE and feature reuse, achieving a Pareto frontier that balances fidelity and latency trade-offs in various generative tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
Diffusionæ¨¡å‹çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºé«˜è´¨é‡å›¾åƒç”Ÿæˆéœ€è¦å¤šæ¬¡å‡é›‘æ­¥éª¤ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶å·²ç»å°è¯•é€šè¿‡é™ä½å¾—åˆ†å‡½æ•°è¯„ä¼°æ•°é‡ä½¿ç”¨é«˜çº§ODEè§£å†³æ–¹æ¡ˆæ¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†æ˜¯å‡å°‘å‡é›‘è¿­ä»£æ•°ä¼šé”™è¿‡æ›´æ–°ç»†èŠ‚ï¼Œå¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é«˜çº§åŠ é€ŸæŠ€æœ¯ï¼Œåˆ©ç”¨Diffusionæ¨¡å‹å†…ç½®çš„æ—¶é—´é‡å¤æ€§ã€‚é‡ç”¨æ—¶é—´ç›¸ä¼¼çš„ç‰¹å¾å›¾opens up a new opportunity to save computation without sacrificing output qualityã€‚ä¸ºäº†å®ç°è¿™ä¸ªç†å¿µçš„å®ç”¨æ•ˆæœï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„åˆ†æå¹¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼ŒFRDiffã€‚ FRDiffæ—¨åœ¨åˆ©ç”¨å‡å°‘NFEå’Œç‰¹å¾é‡ç”¨çš„ä¼˜ç‚¹ï¼Œå®ç°å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­çš„å¹³è¡¡è´¨é‡å’Œå»¶è¿Ÿäº¤æ˜“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Speculative-Exploration-on-the-Concept-of-Artificial-Agents-Conducting-Autonomous-Research"><a href="#Speculative-Exploration-on-the-Concept-of-Artificial-Agents-Conducting-Autonomous-Research" class="headerlink" title="Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research"></a>Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03497">http://arxiv.org/abs/2312.03497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/t46/research-automation-perspective-paper">https://github.com/t46/research-automation-perspective-paper</a></li>
<li>paper_authors: Shiro Takagi</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§äººå·¥æ™ºèƒ½å¯ä»¥è¿›è¡Œç ”ç©¶çš„æ¦‚å¿µã€‚</li>
<li>methods: è®ºæ–‡é¦–å…ˆæè¿°äº†ç ”ç©¶çš„æ¦‚å¿µï¼Œä»¥æä¾›åˆ›æ–°çš„å¼€å§‹ç‚¹ã€‚ç„¶åï¼Œå®ƒè€ƒè™‘äº†ç ”ç©¶çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼ŒåŒ…æ‹¬é—®é¢˜å®šä¹‰ã€å‡è®¾ç”Ÿæˆå’Œå‡è®¾éªŒè¯ã€‚è¿™äº›è®¨è®ºåŒ…æ‹¬äº†æœºå™¨è‡ªåŠ¨å®Œæˆè¿™äº›ä»»åŠ¡çš„æ½œåœ¨å’ŒæŒ‘æˆ˜ã€‚</li>
<li>results: è®ºæ–‡ç®€è¦è®¨è®ºäº†è¿™äº›ç ”ç©¶èƒ½åŠ›çš„agentçš„ç›¸äº’å…³ç³»å’Œäº¤å ã€‚æœ€åï¼Œå®ƒæå‡ºäº†åˆæ­¥çš„æ€è€ƒï¼Œä»¥ä¾¿æ¢ç´¢è¿™äº›ç ”ç©¶èƒ½åŠ›agentçš„å‘å±•æŒ‘æˆ˜ã€‚<details>
<summary>Abstract</summary>
This paper engages in a speculative exploration of the concept of an artificial agent capable of conducting research. Initially, it examines how the act of research can be conceptually characterized, aiming to provide a starting point for discussions about what it means to create such agents. The focus then shifts to the core components of research: question formulation, hypothesis generation, and hypothesis verification. This discussion includes a consideration of the potential and challenges associated with enabling machines to autonomously perform these tasks. Subsequently, this paper briefly considers the overlapping themes and interconnections that underlie them. Finally, the paper presents preliminary thoughts on prototyping as an initial step towards uncovering the challenges involved in developing these research-capable agents.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡å±•å¼€äº†ä¸€ç§äººå·¥æ™ºèƒ½å¯ä»¥è¿›è¡Œç ”ç©¶çš„æ¦‚å¿µã€‚æœ€åˆï¼Œå®ƒæè¿°äº†ç ”ç©¶çš„æ¦‚å¿µï¼Œä»¥ä¾¿æä¾›è®¨è®ºçš„èµ·ç‚¹ã€‚ç„¶åï¼Œå®ƒshiftåˆ°ç ”ç©¶çš„æ ¸å¿ƒç»„ä»¶ï¼šé—®é¢˜å®šä¹‰ã€å‡è®¾ç”Ÿæˆå’Œå‡è®¾éªŒè¯ã€‚è¿™ä¸ªè®¨è®ºåŒ…æ‹¬æœºå™¨è‡ªåŠ¨æ‰§è¡Œè¿™äº›ä»»åŠ¡çš„æ½œåœ¨å’ŒæŒ‘æˆ˜ã€‚æ¥ç€ï¼Œè¿™ç¯‡è®ºæ–‡ç®€è¦ä»‹ç»äº†è¿™äº›ä¸»é¢˜ä¹‹é—´çš„é‡å ç‚¹å’Œè”ç³»ã€‚æœ€åï¼Œå®ƒæä¾›äº†åˆæ­¥æ€æƒ³ï¼Œä»¥ä¾¿å¼€å§‹è¯„ä¼°åœ¨å¼€å‘è¿™äº›ç ”ç©¶èƒ½åŠ›çš„æœºå™¨äººæ—¶é‡åˆ°çš„æŒ‘æˆ˜ã€‚Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Learning-From-Scenarios-for-Stochastic-Repairable-Scheduling"><a href="#Learning-From-Scenarios-for-Stochastic-Repairable-Scheduling" class="headerlink" title="Learning From Scenarios for Stochastic Repairable Scheduling"></a>Learning From Scenarios for Stochastic Repairable Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03492">http://arxiv.org/abs/2312.03492</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kimvandenhouten/learning-from-scenarios-for-repairable-stochastic-scheduling">https://github.com/kimvandenhouten/learning-from-scenarios-for-repairable-stochastic-scheduling</a></li>
<li>paper_authors: Kim van den Houten, David M. J. Tax, Esteban Freydell, Mathijs de Weerdt</li>
<li>for:  Linear objective optimization with uncertain parameter values in a stochastic scheduling problem.</li>
<li>methods:  Decision-focused learning with stochastic smoothing to adapt existing techniques to the scheduling problem.</li>
<li>results:  Extensive experimental evaluation to compare the performance of decision-focused learning with the state of the art for scenario-based stochastic optimization.Hereâ€™s the text in Simplified Chinese:</li>
<li>for:  Linearç›®æ ‡ä¼˜åŒ– WITH uncertain parameter values in a stochastic scheduling problem.</li>
<li>methods:  Decision-focused learning WITH stochastic smoothing to adapt existing techniques to the scheduling problem.</li>
<li>results:  Extensive experimental evaluation to compare the performance of decision-focused learning WITH the state of the art for scenario-based stochastic optimization.<details>
<summary>Abstract</summary>
When optimizing problems with uncertain parameter values in a linear objective, decision-focused learning enables end-to-end learning of these values. We are interested in a stochastic scheduling problem, in which processing times are uncertain, which brings uncertain values in the constraints, and thus repair of an initial schedule may be needed. Historical realizations of the stochastic processing times are available. We show how existing decision-focused learning techniques based on stochastic smoothing can be adapted to this scheduling problem. We include an extensive experimental evaluation to investigate in which situations decision-focused learning outperforms the state of the art for such situations: scenario-based stochastic optimization.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“ä¼˜åŒ–å…·æœ‰ä¸ç¡®å®šå‚æ•°å€¼çš„çº¿æ€§ç›®æ ‡é—®é¢˜æ—¶ï¼Œå†³ç­–å…³æ³¨å­¦ä¹ å¯ä»¥å®ç°ç«¯åˆ°ç«¯å­¦ä¹ è¿™äº›å€¼ã€‚æˆ‘ä»¬å…³æ³¨ä¸€ä¸ªéšæœºå¤„ç†æ—¶é—´çš„è°ƒåº¦é—®é¢˜ï¼Œå¤„ç†æ—¶é—´å…·æœ‰éšæœºæ€§ï¼Œå› æ­¤å¯èƒ½éœ€è¦ä¿®å¤åˆå§‹è°ƒåº¦ã€‚å†å²å®ç°éšæœºå¤„ç†æ—¶é—´çš„æ•°æ®å¯ç”¨ã€‚æˆ‘ä»¬ä»‹ç»äº†ç°æœ‰çš„å†³ç­–å…³æ³¨å­¦ä¹ æŠ€æœ¯ï¼ŒåŸºäºéšæœºç¼“å’Œï¼Œå¦‚ä½•åº”ç”¨äºè¿™ä¸ªè°ƒåº¦é—®é¢˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼Œä»¥ Investigateåœ¨å“ªäº›æƒ…å†µä¸‹å†³ç­–å…³æ³¨å­¦ä¹ è¶…è¶Šäº†ç°çŠ¶å¤©åœ°éšæœºä¼˜åŒ–ã€‚Here's the breakdown of the translation:* å½“ä¼˜åŒ– (dÄng yÃ²u jÃ¬) - "when optimizing"* å…·æœ‰ä¸ç¡®å®šå‚æ•°å€¼ (yÇ’u yÇ’u bÃ¹ jÃ¬ pin yÃ¨) - "with uncertain parameter values"* çº¿æ€§ç›®æ ‡é—®é¢˜ (xiÃ n xÃ¬ng mÃ¹ tiÃ o wÃ¨n tÃ­) - "linear objective"* å†³ç­–å…³æ³¨å­¦ä¹  (jÃ¬ dÃ o guÄn zhÃ¹ xuÃ© xÃ­) - "decision-focused learning"* ç«¯åˆ°ç«¯å­¦ä¹  (dÃ­an dÃ o diÃ n xuÃ© xÃ­) - "end-to-end learning"* è¿™äº›å€¼ (zhÃ¨ xiÄ“) - "these values"* éšæœºå¤„ç†æ—¶é—´ (suÃ¬ jiÃ ng hÃ²u zhÃ­ shÃ­) - "processing times are uncertain"* éšæœºå€¼ (suÃ¬ jiÃ ng yÃ¨) - "random values"*  constraints (guÄn lÃ¬) - "constraints"* ä¿®å¤ (xiÅ« gÃ²ng) - "repair"* åˆå§‹è°ƒåº¦ (chÅ« shÃ­ tiÃ o dÃ o) - "initial schedule"* å†å²å®ç° (lÃ¬ shÇ shÃ­ jÃ¬) - "historical realizations"* æ•°æ® (shÃ¹ dÃ o) - "data"* å¯ç”¨ (kÄ› yÃ²u) - "available"* ç°æœ‰çš„ (xiÃ n yÇ’u de) - "existing"* å†³ç­–å…³æ³¨å­¦ä¹ æŠ€æœ¯ (jÃ¬ dÃ o guÄn zhÃ¹ xuÃ© xÃ­ jÃ¬ shÃ¹) - "existing decision-focused learning techniques"* åŸºäºéšæœºç¼“ (jÄ« yÃº suÃ¬ jiÃ ng bÃ¬) - "based on stochastic smoothing"* åº”ç”¨äº (fÃ¹ yÃ¹ yÇ”) - "applied to"* è¿™ä¸ªè°ƒåº¦é—®é¢˜ (zhÃ¨ ge tiÃ o dÃ o wÃ¨n tÃ­) - "this scheduling problem"*  Investigate (yÃ n jÃ­) - "investigate"* æƒ…å†µ (qÃ­ng jÃ¬) - "situations"* è¶…è¶Š (chÄo yÃº) - "outperform"* ç°çŠ¶å¤©åœ° (xiÃ n zhÃ¨ng tiÄn dÃ¬) - "current state of the art"* éšæœºä¼˜åŒ– (suÃ¬ jiÃ ng yÃ¬ huÃ ) - "scenario-based stochastic optimization"
</details></li>
</ul>
<hr>
<h2 id="JAMMIN-GPT-Text-based-Improvisation-using-LLMs-in-Ableton-Live"><a href="#JAMMIN-GPT-Text-based-Improvisation-using-LLMs-in-Ableton-Live" class="headerlink" title="JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live"></a>JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03479">http://arxiv.org/abs/2312.03479</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/supersational/jammin-gpt">https://github.com/supersational/jammin-gpt</a></li>
<li>paper_authors: Sven Hollowell, Tashi Namgyal, Paul Marshall</li>
<li>for: è¿™ä¸ªç³»ç»Ÿæ˜¯ä¸ºAbleton Liveç”¨æˆ·åˆ›å»ºMIDI-clipè€Œè®¾è®¡çš„ï¼Œä»¥ä¾¿é€šè¿‡ musical descriptions æ¥å‘½åå®ƒä»¬ã€‚</li>
<li>methods: è¯¥ç³»ç»Ÿä½¿ç”¨ ChatGPT å›ç­”å™¨æ¥ç”Ÿæˆæ–‡æœ¬åŸºäº musical formatsï¼Œå¦‚ ABC notationã€chord symbols æˆ– drum tablatureï¼Œä»¥ä¾¿åœ¨ Ableton çš„clip viewä¸­æ’å…¥ Musical ideasã€‚</li>
<li>results: è¯¥ç³»ç»Ÿå¯ä»¥å¸®åŠ©ç”¨æˆ·å¿«é€Ÿç”Ÿæˆ musical ideasï¼Œå¹¶ä¸”å¯ä»¥è®©ç”¨æˆ·åœ¨åˆ›ä½œè¿‡ç¨‹ä¸­ä¿æŒæµç•…ï¼Œä¸éœ€è¦åœä¸‹æ¥ç¼–è¾‘ codeã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨æ—¢æé«˜äº† musical åˆ›ä½œæ•ˆç‡ï¼Œä¹Ÿé™ä½äº†å­¦ä¹ æˆæœ¬ã€‚<details>
<summary>Abstract</summary>
We introduce a system that allows users of Ableton Live to create MIDI-clips by naming them with musical descriptions. Users can compose by typing the desired musical content directly in Ableton's clip view, which is then inserted by our integrated system. This allows users to stay in the flow of their creative process while quickly generating musical ideas. The system works by prompting ChatGPT to reply using one of several text-based musical formats, such as ABC notation, chord symbols, or drum tablature. This is an important step in integrating generative AI tools into pre-existing musical workflows, and could be valuable for content makers who prefer to express their creative vision through descriptive language. Code is available at https://github.com/supersational/JAMMIN-GPT.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»ä¸€ä¸ªç³»ç»Ÿï¼Œè®©Ableton Liveç”¨æˆ·å¯ä»¥é€šè¿‡ Musical descriptions åç§° MIDI-clipã€‚ç”¨æˆ·å¯ä»¥åœ¨Abletonçš„ clip view ä¸­ç›´æ¥è¾“å…¥ Desired musical contentï¼Œæˆ‘ä»¬çš„æ•´åˆç³»ç»Ÿå°†å…¶æ’å…¥ã€‚è¿™ä½¿ç”¨æˆ·å¯ä»¥ä¿æŒåˆ›ä½œè¿‡ç¨‹ä¸­çš„æµåŠ¨æ€§ï¼Œå¿«é€Ÿç”Ÿæˆ musical ideasã€‚ç³»ç»Ÿå·¥ä½œæ–¹å¼æ˜¯é€šè¿‡è¯·æ±‚ ChatGPT å›ç­”ä½¿ç”¨ä¸€äº›æ–‡æœ¬åŸºäºçš„ Musical formatsï¼Œä¾‹å¦‚ ABC notationã€chord symbols æˆ– drum tablatureã€‚è¿™æ˜¯ç»Ÿåˆç”Ÿæˆ AI å·¥å…·åˆ°ç°æœ‰çš„ Musical workflows çš„é‡è¦ä¸€æ­¥ï¼Œå¯èƒ½å¯¹å†…å®¹åˆ¶ä½œè€…æœ‰ä»·å€¼ï¼Œä»–ä»¬å¯èƒ½ prefer é€šè¿‡æè¿°æ€§è¯­è¨€è¡¨è¾¾åˆ›ä½œæ„ä¹‰ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/supersational/JAMMIN-GPT è·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Molecule-Joint-Auto-Encoding-Trajectory-Pretraining-with-2D-and-3D-Diffusion"><a href="#Molecule-Joint-Auto-Encoding-Trajectory-Pretraining-with-2D-and-3D-Diffusion" class="headerlink" title="Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion"></a>Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03475">http://arxiv.org/abs/2312.03475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weitao Du, Jiujiu Chen, Xuecang Zhang, Zhiming Ma, Shengchao Liu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜äººå·¥æ™ºèƒ½åœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨å­¦ä¹ å’ŒåŒ–å­¦é¢†åŸŸã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ prÃ©-training æ–¹æ³•ï¼Œç§°ä¸ºåˆ†å­è”åˆè‡ªåŠ¨ç¼–ç ï¼ˆMoleculeJAEï¼‰ï¼Œå¯ä»¥å­¦ä¹ åˆ†å­çš„äºŒç»´ç²¾åº¦ï¼ˆé”®ç»“æ„ï¼‰å’Œä¸‰ç»´å½¢æ€ï¼ˆå‡ ä½•ï¼‰ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿå¢å¼ºçš„æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥è‡ªç„¶åœ°å­¦ä¹ åˆ†å­çš„å†…åœ¨ç»“æ„ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒMoleculeJAE èƒ½å¤Ÿè¾¾åˆ°æ¯”è¾ƒå‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ 20 ä¸ªä»»åŠ¡ä¸­çš„ 15 ä¸ªä»»åŠ¡ä¸­æ¯” 12 ä¸ªåŸºçº¿æ¨¡å‹æ›´é«˜ã€‚<details>
<summary>Abstract</summary>
Recently, artificial intelligence for drug discovery has raised increasing interest in both machine learning and chemistry domains. The fundamental building block for drug discovery is molecule geometry and thus, the molecule's geometrical representation is the main bottleneck to better utilize machine learning techniques for drug discovery. In this work, we propose a pretraining method for molecule joint auto-encoding (MoleculeJAE). MoleculeJAE can learn both the 2D bond (topology) and 3D conformation (geometry) information, and a diffusion process model is applied to mimic the augmented trajectories of such two modalities, based on which, MoleculeJAE will learn the inherent chemical structure in a self-supervised manner. Thus, the pretrained geometrical representation in MoleculeJAE is expected to benefit downstream geometry-related tasks. Empirically, MoleculeJAE proves its effectiveness by reaching state-of-the-art performance on 15 out of 20 tasks by comparing it with 12 competitive baselines.
</details>
<details>
<summary>æ‘˜è¦</summary>
(Note: Simplified Chinese is also known as "ç®€åŒ–å­—" or "ç®€åŒ–å­—".)
</details></li>
</ul>
<hr>
<h2 id="Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data"><a href="#Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data" class="headerlink" title="Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data"></a>Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03455">http://arxiv.org/abs/2312.03455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tashi Namgyal, Alexander Hepburn, Raul Santos-Rodriguez, Valero Laparra, Jesus Malo</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ç”¨äºè¯„ä¼°è‡ªç„¶ä¿¡å·è´¨é‡çš„æ–¹æ³•ï¼Œå¦‚å›¾åƒå’ŒéŸ³é¢‘ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æ„ŸçŸ¥æŒ‡æ ‡æ¥è¯„ä¼°è‡ªç„¶ä¿¡å·çš„è´¨é‡ï¼Œæ„ŸçŸ¥æŒ‡æ ‡æ˜¯åŸºäºäººç±»è§‚å¯Ÿè€…çš„æ„ŸçŸ¥è¡Œä¸ºï¼Œé€šå¸¸èƒ½å¤Ÿæ•æ‰è‡ªç„¶ä¿¡å·ä¸­çš„ç»“æ„ã€‚</li>
<li>results: è®ºæ–‡å‘ç°ï¼Œä½¿ç”¨æ„ŸçŸ¥æŒ‡æ ‡ä½œä¸ºæŸå¤±å‡½æ•°å¯ä»¥è®©ç”Ÿæˆæ¨¡å‹æ›´å¥½åœ°æ•æ‰è‡ªç„¶ä¿¡å·ä¸­çš„ç»“æ„ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶é‡å»ºspectrogramså’Œé‡æ–°ç”Ÿæˆçš„éŸ³é¢‘ä¸­å¾—åˆ°æ›´å¥½çš„ç»“æœï¼Œè¿™è¡¨æ˜ä½¿ç”¨æ„ŸçŸ¥æŒ‡æ ‡å¯ä»¥æ›´å¥½åœ°é€‚åº”æœªç»è§è¿‡çš„è‡ªç„¶ä¿¡å·ã€‚<details>
<summary>Abstract</summary>
Perceptual metrics are traditionally used to evaluate the quality of natural signals, such as images and audio. They are designed to mimic the perceptual behaviour of human observers and usually reflect structures found in natural signals. This motivates their use as loss functions for training generative models such that models will learn to capture the structure held in the metric. We take this idea to the extreme in the audio domain by training a compressive autoencoder to reconstruct uniform noise, in lieu of natural data. We show that training with perceptual losses improves the reconstruction of spectrograms and re-synthesized audio at test time over models trained with a standard Euclidean loss. This demonstrates better generalisation to unseen natural signals when using perceptual metrics.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼ ç»Ÿçš„æ„ŸçŸ¥æŒ‡æ ‡é€šå¸¸ç”¨äºè¯„ä¼°è‡ªç„¶ä¿¡å·çš„è´¨é‡ï¼Œå¦‚å›¾åƒå’ŒéŸ³é¢‘ã€‚å®ƒä»¬æ˜¯ä¸ºæ¨¡ä»¿äººç±»è§‚å¯Ÿè€…çš„æ„ŸçŸ¥è¡Œä¸ºè€Œè®¾è®¡çš„ï¼Œé€šå¸¸åæ˜ è‡ªç„¶ä¿¡å·ä¸­çš„ç»“æ„ã€‚è¿™ç§æƒ³æ³•é©±åŠ¨äº†ä½¿ç”¨æ„ŸçŸ¥æŒ‡æ ‡ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„æŸå¤±å‡½æ•°çš„ä½¿ç”¨ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥æ•æ‰æŒ‡æ ‡ä¸­çš„ç»“æ„ã€‚åœ¨éŸ³é¢‘é¢†åŸŸä¸­ï¼Œæˆ‘ä»¬Push this idea to the extreme by training a compressive autoencoder to reconstruct uniform noise, instead of natural data. We show that training with perceptual losses improves the reconstruction of spectrograms and re-synthesized audio at test time over models trained with a standard Euclidean loss. This demonstrates better generalization to unseen natural signals when using perceptual metrics.Here's the translation breakdown:* ä¼ ç»Ÿçš„æ„ŸçŸ¥æŒ‡æ ‡ (traditional perceptual metrics) -> ä¼ ç»Ÿçš„æ„ŸçŸ¥æŒ‡æ ‡ (traditional perceptual metrics)* è‡ªç„¶ä¿¡å· (natural signals) -> è‡ªç„¶ä¿¡å· (natural signals)* è´¨é‡ (quality) -> è´¨é‡ (quality)* æ¨¡ä»¿ (mimic) -> æ¨¡ä»¿ (mimic)* äººç±»è§‚å¯Ÿè€… (human observers) -> äººç±»è§‚å¯Ÿè€… (human observers)* æ„ŸçŸ¥è¡Œä¸º (perceptual behavior) -> æ„ŸçŸ¥è¡Œä¸º (perceptual behavior)* ç»“æ„ (structure) -> ç»“æ„ (structure)* ç”Ÿæˆæ¨¡å‹ (generative models) -> ç”Ÿæˆæ¨¡å‹ (generative models)* æŸå¤±å‡½æ•° (loss functions) -> æŸå¤±å‡½æ•° (loss functions)* æ•æ‰ (capture) -> æ•æ‰ (capture)* æŒ‡æ ‡ä¸­çš„ç»“æ„ (structure held in the metric) -> æŒ‡æ ‡ä¸­çš„ç»“æ„ (structure held in the metric)* éŸ³é¢‘é¢†åŸŸ (audio domain) -> éŸ³é¢‘é¢†åŸŸ (audio domain)* æŠ½è±¡å‹ç¼© autoencoder (compressive autoencoder) -> æŠ½è±¡å‹ç¼© autoencoder (compressive autoencoder)* é‡å»º (reconstruct) -> é‡å»º (reconstruct)* å‹ç¼© (compressive) -> å‹ç¼© (compressive)* è‡ªç„¶æ•°æ® (natural data) -> è‡ªç„¶æ•°æ® (natural data)* æ ‡å‡†çš„æ¬§å‡ ä½•è½å¤± (standard Euclidean loss) -> æ ‡å‡†çš„æ¬§å‡ ä½•è½å¤± (standard Euclidean loss)* æµ‹è¯•æ—¶ (at test time) -> æµ‹è¯•æ—¶ (at test time)* æ€»ä½“ (overall) -> æ€»ä½“ (overall)* æ›´å¥½çš„æ³›åŒ– (better generalization) -> æ›´å¥½çš„æ³›åŒ– (better generalization)
</details></li>
</ul>
<hr>
<h2 id="Quantum-Inspired-Neural-Network-Model-of-Optical-Illusions"><a href="#Quantum-Inspired-Neural-Network-Model-of-Optical-Illusions" class="headerlink" title="Quantum-Inspired Neural Network Model of Optical Illusions"></a>Quantum-Inspired Neural Network Model of Optical Illusions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03447">http://arxiv.org/abs/2312.03447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan S. Maksymov<br>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶äººç±»å¯¹æ¶‚æŠ¹å¼ä¸ç¨³å®šç‰©ä½“ï¼ˆå¦‚å°¼å…‹å°”ç«‹æ–¹ä½“ï¼‰çš„è§‚å¯Ÿå’Œç†è§£è€Œå†™çš„ã€‚methods: ä½œè€…ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹æ¥æ¨¡æ‹Ÿäººç±»å¯¹å°¼å…‹å°”ç«‹æ–¹ä½“çš„è§‚å¯Ÿå’Œç†è§£ï¼Œå¹¶ä½¿ç”¨é‡å­ç”Ÿæˆå™¨æ¥å®šä¹‰ç¥ç»ç½‘ç»œè¿æ¥çš„æƒé‡ã€‚results: ç ”ç©¶å‘ç°ï¼Œå°¼å…‹å°”ç«‹æ–¹ä½“çš„å®é™…è§‚å¯ŸçŠ¶æ€æ˜¯ä¸€ç§åŸºäºé‡å­æœºåˆ¶çš„è¶…positionï¼Œè¿™ä¸ ĞºĞ»Ğ°ÑÑĞ¸icalç†è®ºé¢„æµ‹çš„ä¸¤ç§åŸºæœ¬è§‚å¯ŸçŠ¶æ€ç›¸ç¬¦ã€‚è¿™äº›ç»“æœå°†æœ‰ç”¨äºè§†é¢‘æ¸¸æˆå’Œè™šæ‹Ÿç°å®ç³»ç»Ÿï¼Œä»¥åŠç ”ç©¶æœºå™¨å­¦ä¹ ã€è§†è§‰ã€å¿ƒç†å­¦å’Œé‡å­æœºåˆ¶çš„äººç±»å¿ƒç†å’Œå†³ç­–ã€‚<details>
<summary>Abstract</summary>
Ambiguous optical illusions have been a paradigmatic object of fascination, research and inspiration in arts, psychology and video games. However, accurate computational models of perception of ambiguous figures have been elusive. In this paper, we design and train a deep neural network model to simulate the human's perception of the Necker cube, an ambiguous drawing with several alternating possible interpretations. Defining the weights of the neural network connection using a quantum generator of truly random numbers, in agreement with the emerging concepts of quantum artificial intelligence and quantum cognition we reveal that the actual perceptual state of the Necker cube is a qubit-like superposition of the two fundamental perceptual states predicted by classical theories. Our results will find applications in video games and virtual reality systems employed for training of astronauts and operators of unmanned aerial vehicles. They will also be useful for researchers working in the fields of machine learning and vision, psychology of perception and quantum-mechanical models of human mind and decision-making.
</details>
<details>
<summary>æ‘˜è¦</summary>
å›°æƒ‘çš„è§†è§‰é”™è§‰å·²ç»æˆä¸ºè‰ºæœ¯ã€å¿ƒç†å­¦å’Œç”µå­æ¸¸æˆç­‰é¢†åŸŸçš„ä¸€ç§ç‹¬ç‰¹çš„å¯¹è±¡ï¼Œä½†æ˜¯å‡†ç¡®çš„è®¡ç®—æ¨¡å‹æ¥è§£é‡Šäººç±»çš„è§†è§‰å´æ˜¯å›°éš¾çš„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºæ¨¡æ‹Ÿäººç±»å¯¹å°¼å…‹å°”ç«‹æ–¹ä½“çš„è§†è§‰å«ä¹‰ã€‚ä½¿ç”¨é‡å­ç”Ÿæˆå™¨ç”ŸæˆçœŸå®éšæœºæ•°çš„æƒé‡ï¼Œä¸é‡å­äººå·¥æ™ºèƒ½å’Œé‡å­è®¤çŸ¥ç†è®ºç›¸å»åˆï¼Œæˆ‘ä»¬å‘ç°äº†äººç±»å¯¹å°¼å…‹å°”ç«‹æ–¹ä½“çš„å®é™…è§†è§‰çŠ¶æ€æ˜¯ä¸€ç§åŸºäºä¸¤ä¸ªåŸºæœ¬è§†è§‰çŠ¶æ€çš„QUBIT-likeè¶…positionã€‚æˆ‘ä»¬çš„ç»“æœå°†æ‰¾åˆ°åº”ç”¨äºç”µå­æ¸¸æˆå’Œè™šæ‹Ÿç°å®ç³»ç»Ÿï¼Œç”¨äºè®­ç»ƒå®‡èˆªå‘˜å’Œæ— äººé£è¡Œå™¨æ“ä½œå‘˜ã€‚åŒæ—¶ï¼Œè¿™äº›ç»“æœä¹Ÿå°†å¯¹æœºå™¨å­¦ä¹ ã€è§†è§‰å’Œå¿ƒç†å­¦ç ”ç©¶æœ‰å¾ˆå¤§çš„å¸®åŠ©ï¼Œä»¥åŠé‡å­æœºå™¨äººæ¨¡å‹å’Œå†³ç­–çš„ç ”ç©¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="Sports-Recommender-Systems-Overview-and-Research-Issues"><a href="#Sports-Recommender-Systems-Overview-and-Research-Issues" class="headerlink" title="Sports Recommender Systems: Overview and Research Issues"></a>Sports Recommender Systems: Overview and Research Issues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03785">http://arxiv.org/abs/2312.03785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Felfernig, Manfred Wundara, Thi Ngoc Trang Tran, Viet-Man Le, Sebastian Lubos, Seda Polat-Erdeniz</li>
<li>for: è¿åŠ¨æ¨èç³»ç»Ÿåœ¨å¥åº·ç”Ÿæ´»ã€äººé™…å…³ç³»å’Œè¿åŠ¨è¡¨ç°ç­‰æ–¹é¢å—åˆ°è¶Šæ¥è¶Šå¤šçš„æ³¨æ„ã€‚è¿™äº›ç³»ç»Ÿå¯ä»¥å¸®åŠ©äººä»¬åœ¨è¿åŠ¨ä¸­é€‰æ‹©é€‚åˆè‡ªå·±çš„é¤é£Ÿã€è®­ç»ƒæ–¹æ³•ã€æ‰èƒ½å’Œå›¢é˜Ÿç­‰ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡åŸºäºä¸åŒçš„å®è·µä¾‹è¿›è¡Œäº†è¿åŠ¨æ¨èç³»ç»Ÿçš„åº”ç”¨å’ŒæŠ€æœ¯çš„æ¦‚è¿°ã€‚å®ƒä»¬åŒ…æ‹¬é¤é£Ÿæ¨èã€è®­ç»ƒæ–¹æ³•æ¨èã€æ‰èƒ½å’Œå›¢é˜Ÿæ¨èä»¥åŠç«èµ›ä¸­çš„ç­–ç•¥æ¨èç­‰ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡åˆ†æäº†è¿åŠ¨æ¨èç³»ç»Ÿçš„ç›¸å…³å›½é™…å’Œå¼€å±•ç ”ç©¶é—®é¢˜ã€‚å®ƒè¿˜æå‡ºäº†ä¸€äº›æœªè§£å†³çš„ç ”ç©¶é—®é¢˜ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ¢ç´¢è¿åŠ¨æ¨èç³»ç»Ÿçš„åº”ç”¨å’ŒæŠ€æœ¯å‘å±•ã€‚<details>
<summary>Abstract</summary>
Sports recommender systems receive an increasing attention due to their potential of fostering healthy living, improving personal well-being, and increasing performances in sport. These systems support people in sports, for example, by the recommendation of healthy and performance boosting food items, the recommendation of training practices, talent and team recommendation, and the recommendation of specific tactics in competitions. With applications in the virtual world, for example, the recommendation of maps or opponents in e-sports, these systems already transcend conventional sports scenarios where physical presence is needed. On the basis of different working examples, we present an overview of sports recommender systems applications and techniques. Overall, we analyze the related state-of-the-art and discuss open research issues.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä½“è‚²æ¨èç³»ç»Ÿåœ¨æœ€è¿‘å‡ å¹´æ¥å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œè¿™ä¸»è¦å½’åŠŸäºå®ƒä»¬åœ¨å¥åº·ç”Ÿæ´»ã€ä¸ªäººå¥åº·å’Œè¿åŠ¨è¡¨ç°æ–¹é¢çš„æ½œåœ¨ä½œç”¨ã€‚è¿™äº›ç³»ç»Ÿæ”¯æŒäººä»¬åœ¨è¿åŠ¨æ–¹é¢ï¼Œä¾‹å¦‚ï¼Œæ¨èå¥åº·å’Œè¡¨ç°æå‡çš„é£Ÿå“ã€è®­ç»ƒæ–¹æ³•ã€æ‰èƒ½å’Œå›¢é˜Ÿæ¨èã€ç«èµ›ä¸­ç‰¹å®šæˆ˜æ–—ç­–ç•¥ç­‰ç­‰ã€‚åœ¨è™šæ‹Ÿä¸–ç•Œä¸­ï¼Œä¾‹å¦‚ç”µå­ç«æŠ€ï¼Œè¿™äº›ç³»ç»Ÿå·²ç»è¶…è¶Šäº†ä¼ ç»Ÿçš„ä½“è‚²åœºæ™¯ï¼Œéœ€è¦ç‰©ç†å­˜åœ¨ã€‚åŸºäºä¸åŒçš„å®è·µä¾‹å­ï¼Œæˆ‘ä»¬æä¾›ä½“è‚²æ¨èç³»ç»Ÿåº”ç”¨å’ŒæŠ€æœ¯çš„æ¦‚è¿°ï¼Œå¹¶æ€»ç»“ç›¸å…³çš„ç°çŠ¶å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="Approximating-Solutions-to-the-Knapsack-Problem-using-the-Lagrangian-Dual-Framework"><a href="#Approximating-Solutions-to-the-Knapsack-Problem-using-the-Lagrangian-Dual-Framework" class="headerlink" title="Approximating Solutions to the Knapsack Problem using the Lagrangian Dual Framework"></a>Approximating Solutions to the Knapsack Problem using the Lagrangian Dual Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03413">http://arxiv.org/abs/2312.03413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mitchell Keegan, Mahdi Abolghasemi</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºLagrangian dual frameworkçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºè§£å†³ç®±å­é—®é¢˜ï¼ˆCombinatorial Optimizationï¼‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿæé«˜çº¦æŸæ»¡è¶³åº¦ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹æ¥è¿‘ä¼¼ç®±å­é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”ä½¿ç”¨Lagrangian dual frameworkæ¥åŠ ä»¥çº¦æŸæ»¡è¶³ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå…·æœ‰å¼ºå¤§çš„çº¦æŸæ»¡è¶³åº¦ï¼Œä½†æ˜¯æœ‰ä¸€å®šçš„ä¼˜åŒ–ç‡ä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸å…·æœ‰çº¦æŸæ¨¡å‹çš„åŸºå‡†ç¥ç»ç½‘ç»œæ¨¡å‹ä¼šå…·æœ‰æ›´é«˜çš„ä¼˜åŒ–ç‡ï¼Œä½†æ˜¯çº¦æŸæ»¡è¶³åº¦è¾ƒå·®ã€‚<details>
<summary>Abstract</summary>
The Knapsack Problem is a classic problem in combinatorial optimisation. Solving these problems may be computationally expensive. Recent years have seen a growing interest in the use of deep learning methods to approximate the solutions to such problems. A core problem is how to enforce or encourage constraint satisfaction in predicted solutions. A promising approach for predicting solutions to constrained optimisation problems is the Lagrangian Dual Framework which builds on the method of Lagrangian Relaxation. In this paper we develop neural network models to approximate Knapsack Problem solutions using the Lagrangian Dual Framework while improving constraint satisfaction. We explore the problems of output interpretation and model selection within this context. Experimental results show strong constraint satisfaction with a minor reduction of optimality as compared to a baseline neural network which does not explicitly model the constraints.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šé›¶é’±åŒ…é—®é¢˜ã€‹æ˜¯ä¸€ä¸ªç»å…¸çš„ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚è§£å†³è¿™ç±»é—®é¢˜å¯èƒ½æ˜¯ computationally expensiveã€‚è¿‘å¹´æ¥ï¼Œæœ‰è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ¥è¿‘ä¼¼è§£å†³è¿™ç±»é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚æ ¸å¿ƒé—®é¢˜æ˜¯å¦‚ä½•åœ¨é¢„æµ‹è§£å†³æ–¹æ¡ˆä¸­å¼ºåˆ¶æˆ–ä¿ƒè¿›çº¦æŸæ»¡è¶³ã€‚æˆ‘ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­å¼€å‘äº†åŸºäºLagrangian Dual Frameworkçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä»¥ä¼˜åŒ–é›¶é’±åŒ…é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æé«˜çº¦æŸæ»¡è¶³æ€§ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è¾“å‡ºè§£é‡Šå’Œæ¨¡å‹é€‰æ‹©é—®é¢˜åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ¨¡å‹å¯ä»¥å¼ºåˆ¶æ»¡è¶³çº¦æŸï¼Œä½†æ˜¯æœ‰ä¸€å®šçš„ä¼˜åŒ–ç‡ä¸‹é™ç›¸æ¯”äºåŸºå‡†ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Generalized-Contrastive-Divergence-Joint-Training-of-Energy-Based-Model-and-Diffusion-Model-through-Inverse-Reinforcement-Learning"><a href="#Generalized-Contrastive-Divergence-Joint-Training-of-Energy-Based-Model-and-Diffusion-Model-through-Inverse-Reinforcement-Learning" class="headerlink" title="Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning"></a>Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03397">http://arxiv.org/abs/2312.03397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangwoong Yoon, Dohyun Kwon, Himchan Hwang, Yung-Kyun Noh, Frank C. Park</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„å¯¹è±¡å‡½æ•°ï¼Œç”¨äºåŒæ—¶è®­ç»ƒèƒ½é‡åŸºæ¨¡å‹ï¼ˆEBMï¼‰å’ŒæŠ½å–æ¨¡å‹ï¼ˆ diffusion modelï¼‰ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬å¯¹EBMå’ŒæŠ½å–æ¨¡å‹è¿›è¡ŒåŒæ—¶è®­ç»ƒï¼Œå¹¶å°†å…¶Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²ä¸ºä¸€ä¸ªæœ€å°åŒ–é—®é¢˜ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åŒæ—¶è®­ç»ƒEBMå’ŒæŠ½å–æ¨¡å‹ï¼Œå¯ä»¥æé«˜æ ·æœ¬è´¨é‡å¹¶å‡å°‘MCMCçš„ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œjoint trainingè¿˜èƒ½å¤Ÿæ”¹å–„EBMçš„è®­ç»ƒæ•ˆæœã€‚<details>
<summary>Abstract</summary>
We present Generalized Contrastive Divergence (GCD), a novel objective function for training an energy-based model (EBM) and a sampler simultaneously. GCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm for training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution with a trainable sampler, such as a diffusion model. In GCD, the joint training of EBM and a diffusion model is formulated as a minimax problem, which reaches an equilibrium when both models converge to the data distribution. The minimax learning with GCD bears interesting equivalence to inverse reinforcement learning, where the energy corresponds to a negative reward, the diffusion model is a policy, and the real data is expert demonstrations. We present preliminary yet promising results showing that joint training is beneficial for both EBM and a diffusion model. GCD enables EBM training without MCMC while improving the sample quality of a diffusion model.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘å›¢é˜Ÿç°åœ¨ä»‹ç»ä¸€ç§æ–°çš„ç›®æ ‡å‡½æ•°ï¼Œå³æ³›åŒ–å¯¹ç…§åˆ†æ•£ï¼ˆGCDï¼‰ï¼Œç”¨äºåŒæ—¶è®­ç»ƒèƒ½é‡åŸºå‹æ¨¡å‹ï¼ˆEBMï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚GCDæ‰©å±•äº†2002å¹´å¸ŒĞ½é¡¿æå‡ºçš„å¯¹ç…§åˆ†æ•£ç®—æ³•ï¼ˆHintonï¼‰ï¼Œå°†é©¬å°”å¯å¤«é“¾ Monte Carloï¼ˆMCMCï¼‰åˆ†å¸ƒæ›¿æ¢ä¸ºå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ã€‚åœ¨GCDä¸­ï¼ŒEBMå’Œæ‰©æ•£æ¨¡å‹çš„å…±åŒè®­ç»ƒè¢«Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°ä¸ºä¸€ä¸ªæœ€å°æœ€å¤§é—®é¢˜ï¼Œå½“ä¸¤ä¸ªæ¨¡å‹éƒ½ convergesåˆ°æ•°æ®åˆ†å¸ƒæ—¶ï¼Œå®ƒä»¬è¾¾åˆ°äº†å¹³è¡¡ã€‚è¿™ç§æœ€å°æœ€å¤§å­¦ä¹ ä¸GCDå…·æœ‰æƒŠäººçš„ç­‰ä»·æ€§ï¼Œä¸åå¥–å­¦ä¹ ç›¸å½“ï¼Œå…¶ä¸­èƒ½é‡å¯¹åº”äºè´Ÿåå¥–ï¼Œæ‰©æ•£æ¨¡å‹å¯¹åº”äºç­–ç•¥ï¼Œè€Œå®é™…æ•°æ®åˆ™æ˜¯ä¸“å®¶ç¤ºèŒƒã€‚æˆ‘ä»¬å±•ç¤ºäº†åˆæ­¥å´æœ‰æŠŠæ¡çš„ç»“æœï¼Œè¡¨æ˜åŒæ—¶è®­ç»ƒEBMå’Œæ‰©æ•£æ¨¡å‹æœ‰åˆ©äºä¸¤è€…ã€‚GCDå…è®¸EBMæ— éœ€MCMCè®­ç»ƒï¼Œå¹¶æé«˜æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬è´¨é‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Diffused-Task-Agnostic-Milestone-Planner"><a href="#Diffused-Task-Agnostic-Milestone-Planner" class="headerlink" title="Diffused Task-Agnostic Milestone Planner"></a>Diffused Task-Agnostic Milestone Planner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03395">http://arxiv.org/abs/2312.03395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mineui Hong, Minjae Kang, Songhwai Oh</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºåºåˆ—é¢„æµ‹çš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å¯¹å†³ç­–é—®é¢˜çš„é•¿æœŸè§„åˆ’ã€è§†è§‰æ§åˆ¶å’Œå¤šä»»é—®é¢˜çš„åº”ç”¨ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨æ•£åº¦åŸºæœ¬ç”Ÿæˆåºåˆ—æ¨¡å‹æ¥è§„åˆ’ä¸€ç³»åˆ—çš„é‡Œç¨‹ç¢‘ï¼Œå¹¶è®©Agentéµå¾ªè¿™äº›é‡Œç¨‹ç¢‘æ¥å®Œæˆä¸€ä¸ªä»»åŠ¡ã€‚æå‡ºçš„æ–¹æ³•å¯ä»¥å­¦ä¹ æ§åˆ¶ç›¸å…³çš„ã€ä½ç»´åº¦çš„latentè¡¨ç¤ºï¼Œä»è€Œå®ç°é•¿æœŸè§„åˆ’å’Œè§†è§‰æ§åˆ¶çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åˆ©ç”¨æ•£åº¦æ¨¡å‹çš„ç”Ÿæˆçµæ´»æ€§ï¼Œå®ç°å¤šä»»é—®é¢˜çš„è§„åˆ’ã€‚</li>
<li>results: æœ¬ç ”ç©¶åœ¨å¤šä¸ªofflineå¾ªç¯å­¦ä¹ ï¼ˆRLï¼‰benchmarkå’Œä¸€ä¸ªè§†è§‰æ§åˆ¶ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¶…è¶Šoffline RLæ–¹æ³•åœ¨è§£å†³é•¿æœŸã€ç½•è§å¥–åŠ±ä»»åŠ¡å’Œå¤šä»»é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„è§†è§‰æ§åˆ¶benchmarkä¸Š achievement state-of-the-artè¡¨ç°ã€‚<details>
<summary>Abstract</summary>
Addressing decision-making problems using sequence modeling to predict future trajectories shows promising results in recent years. In this paper, we take a step further to leverage the sequence predictive method in wider areas such as long-term planning, vision-based control, and multi-task decision-making. To this end, we propose a method to utilize a diffusion-based generative sequence model to plan a series of milestones in a latent space and to have an agent to follow the milestones to accomplish a given task. The proposed method can learn control-relevant, low-dimensional latent representations of milestones, which makes it possible to efficiently perform long-term planning and vision-based control. Furthermore, our approach exploits generation flexibility of the diffusion model, which makes it possible to plan diverse trajectories for multi-task decision-making. We demonstrate the proposed method across offline reinforcement learning (RL) benchmarks and an visual manipulation environment. The results show that our approach outperforms offline RL methods in solving long-horizon, sparse-reward tasks and multi-task problems, while also achieving the state-of-the-art performance on the most challenging vision-based manipulation benchmark.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Lite-Mind-Towards-Efficient-and-Versatile-Brain-Representation-Network"><a href="#Lite-Mind-Towards-Efficient-and-Versatile-Brain-Representation-Network" class="headerlink" title="Lite-Mind: Towards Efficient and Versatile Brain Representation Network"></a>Lite-Mind: Towards Efficient and Versatile Brain Representation Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03781">http://arxiv.org/abs/2312.03781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Gong, Qi Zhang, Duoqian Miao, Guangyin Bao, Liang Hu<br>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æé«˜éä¾µå…¥å¼fMRIçš„ä¿¡æ¯è§£ç æ€§èƒ½ã€‚methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æ·±åº¦å¤šå±‚perceptronï¼ˆMLPï¼‰å’ŒCLIPçš„è§†è§‰å˜æ¢å™¨æ¥å¯¹fMRIåµŒå…¥è¿›è¡Œ alignã€‚results: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ã€é«˜æ•ˆã€å¤šç”¨é€”çš„å¤§è„‘è¡¨ç¤ºç½‘ç»œï¼ˆLite-Mindï¼‰ï¼Œå¯ä»¥é«˜æ•ˆåœ°å°†fMRIç£åŒ–åµŒå…¥ä¸CLIPçš„ç»†è…»ä¿¡æ¯è¿›è¡Œå¯¹åº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLite-Mindåœ¨NSDæ•°æ®é›†ä¸Šå–å¾—äº†94.3%çš„fMRI-to-imageæ£€ç´¢ç²¾åº¦ï¼Œä¸MindEyeç›¸æ¯”å‡å°‘äº†98.7%çš„å‚æ•°æ•°é‡ã€‚<details>
<summary>Abstract</summary>
Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind, a lightweight, efficient, and versatile brain representation network based on discrete Fourier transform, that efficiently aligns fMRI voxels to fine-grained information of CLIP. Our experiments demonstrate that Lite-Mind achieves an impressive 94.3% fMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind is also proven to be able to be migrated to smaller brain datasets and establishes a new state-of-the-art for zero-shot classification on the GOD dataset. The code is available at https://github.com/gongzix/Lite-Mind.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç ”ç©¶åœ¨è§£ç å¤§è„‘ä¿¡æ¯ä¸­è¿›å±• rapilyï¼Œç‰¹åˆ«æ˜¯é€šè¿‡éä¾µå…¥å¼fMRIæ–¹æ³•ã€‚æŒ‘æˆ˜æ¥è‡ªæœ‰é™çš„æ•°æ®å¯ç”¨æ€§å’ŒfMRIä¿¡å·å™ªå£°æ¯”ï¼Œå¯¼è‡´fMRI-to-image Retrieval æ˜¯ä¸€ä¸ªä½ç²¾åº¦ä»»åŠ¡ã€‚ç°æœ‰çš„ MindEye æŠ€æœ¯å¤‡å—æ”¹è¿› fMRI-to-image Retrieval æ€§èƒ½ï¼Œé€šè¿‡ä½¿ç”¨æ·±åº¦ MLP å’Œé«˜å‚æ•°è®¡æ•°ï¼Œä¾‹å¦‚æ¯ä¸ªä¸»ä½“996M MLP Backboneï¼Œå°† fMRI åµŒå…¥çº¿æ€§å¯¹ CLIP è§†transformer çš„æœ€ç»ˆéšè—å±‚è¿›è¡Œå¯¹é½ã€‚ç„¶è€Œï¼Œæ¯ä¸ªä¸»ä½“éƒ½å­˜åœ¨å·®å¼‚ï¼Œå³ä½¿åœ¨åŒä¸€ä¸ªå®éªŒè®¾ç½®ä¸‹ï¼Œéœ€è¦è®­ç»ƒç‰¹å®šä¸»ä½“çš„æ¨¡å‹ã€‚é«˜å‚æ•°æ•°é‡å¯¹å®é™…è®¾å¤‡éƒ¨ç½²é€ æˆäº† significiant æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† Lite-Mindï¼Œä¸€ç§è½»é‡çº§ã€é«˜æ•ˆã€å¤šåŠŸèƒ½å¤§è„‘è¡¨ç¤ºç½‘ç»œï¼ŒåŸºäºç¦»æ•£å‚…é‡Œå¶å˜æ¢ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°† fMRI  voxel å¯¹ CLIP çš„ç»†è…»ä¿¡æ¯è¿›è¡Œå¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLite-Mind å¯ä»¥åœ¨ NSD æ•°æ®é›†ä¸Šè¾¾åˆ°94.3%çš„ fMRI-to-image Retrieval ç²¾åº¦ï¼Œæ¯” MindEye ä½98.7% çš„å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼ŒLite-Mind è¿˜å¯ä»¥è½»æ¾è¿ç§»åˆ° smaller brain æ•°æ®é›†ï¼Œå¹¶åœ¨ GOD æ•°æ®é›†ä¸Šå»ºç«‹äº†æ–°çš„çŠ¶æ€æ€-of-the-art  Ğ´Ğ»Ñé›¶å®¹é‡åˆ†ç±»ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/gongzix/Lite-Mind ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Demand-response-for-residential-building-heating-Effective-Monte-Carlo-Tree-Search-control-based-on-physics-informed-neural-networks"><a href="#Demand-response-for-residential-building-heating-Effective-Monte-Carlo-Tree-Search-control-based-on-physics-informed-neural-networks" class="headerlink" title="Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks"></a>Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03365">http://arxiv.org/abs/2312.03365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Pavirani, Gargya Gokhale, Bert Claessens, Chris Develder</li>
<li>for: æ§åˆ¶å»ºç­‘ç‰©çš„èƒ½æºæ¶ˆè€—ä»¥æé«˜global carbon emissionså’Œé™åˆ¶æ°”å€™å˜åŒ–çš„æ§åˆ¶ã€‚</li>
<li>methods: ä½¿ç”¨Monte Carlo Tree Searchï¼ˆMCTSï¼‰å’ŒPhysics-informed Neural Networkï¼ˆPiNNï¼‰æ¨¡å‹æ¥ä¼˜åŒ–å»ºç­‘ç‰©çš„å†·æš–ç³»ç»Ÿï¼Œä»¥æé«˜DRæ§åˆ¶æ€§èƒ½ã€‚</li>
<li>results: MCTSå’ŒPiNNæ¨¡å‹çš„å®ç°èƒ½å¤Ÿæé«˜DRæ§åˆ¶æ€§èƒ½ï¼Œç›¸æ¯”ä¹‹ä¸‹rule-basedæ§åˆ¶å™¨å¯ä»¥æé«˜10%çš„æˆæœ¬å’Œ35%çš„æ¸©åº¦å·®ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ å±‚çš„æ·»åŠ å¯ä»¥æé«˜è®¡ç®—æˆæœ¬æ•ˆç›Šã€‚<details>
<summary>Abstract</summary>
Controlling energy consumption in buildings through demand response (DR) has become increasingly important to reduce global carbon emissions and limit climate change. In this paper, we specifically focus on controlling the heating system of a residential building to optimize its energy consumption while respecting user's thermal comfort. Recent works in this area have mainly focused on either model-based control, e.g., model predictive control (MPC), or model-free reinforcement learning (RL) to implement practical DR algorithms. A specific RL method that recently has achieved impressive success in domains such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for building control it has remained largely unexplored. Thus, we study MCTS specifically for building demand response. Its natural structure allows a flexible optimization that implicitly integrate exogenous constraints (as opposed, for example, to conventional RL solutions), making MCTS a promising candidate for DR control problems. We demonstrate how to improve MCTS control performance by incorporating a Physics-informed Neural Network (PiNN) model for its underlying thermal state prediction, as opposed to traditional purely data-driven Black-Box approaches. Our MCTS implementation aligned with a PiNN model is able to obtain a 3% increment of the obtained reward compared to a rule-based controller; leading to a 10% cost reduction and 35% reduction on temperature difference with the desired one when applied to an artificial price profile. We further implemented a Deep Learning layer into the Monte Carlo Tree Search technique using a neural network that leads the tree search through more optimal nodes. We then compared this addition with its Vanilla version, showing the improvement in computational cost required.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ§åˆ¶å»ºç­‘ç‰©çš„èƒ½æºæ¶ˆè€—å·²æˆä¸ºé™ä½å…¨çƒç¢³æ’æ”¾å’Œæ§åˆ¶æ°”å€™å˜åŒ–çš„é‡è¦æ–¹æ³•ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ§åˆ¶å…¬å¯“å»ºç­‘ç‰©çš„å†·å´ç³»ç»Ÿï¼Œä»¥ä¼˜åŒ–å…¶èƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶ä¿è¯ç”¨æˆ·çš„å®¤å†…æ¸©åº¦èˆ’é€‚æ€§ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æˆ–æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°å®ç”¨çš„DRç®—æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œ Monte Carlo Tree Searchï¼ˆMCTSï¼‰åœ¨æ£‹ç›˜æ¸¸æˆï¼ˆå¦‚å›´æ£‹ã€å›½é™…è±¡æ£‹ï¼‰ä¸­æœ€è¿‘å‡ å¹´è¡¨ç°å‡ºäº†éå¸¸å‡ºè‰²çš„æˆç»©ã€‚ç„¶è€Œï¼Œåœ¨å»ºç­‘ç‰©æ§åˆ¶é¢†åŸŸï¼ŒMCTSçš„åº”ç”¨ä»ç„¶å¾ˆå°‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ç ”ç©¶MCTSï¼Œå¹¶è¯æ˜å…¶åœ¨å»ºç­‘ç‰©DRæ§åˆ¶é—®é¢˜ä¸­çš„æ½œåœ¨ä¼˜åŠ¿ã€‚MCTSçš„è‡ªç„¶ç»“æ„ä½¿å¾—å¯ä»¥flexiblyè¿›è¡Œä¼˜åŒ–ï¼ŒåŒæ—¶è‡ªåŠ¨æ‰¿è½½å¤–éƒ¨çº¦æŸï¼ˆä¸ä¼ ç»ŸRLæ–¹æ³•ä¸åŒï¼‰ï¼Œè¿™ä½¿MCTSåœ¨DRæ§åˆ¶é—®é¢˜ä¸­æˆä¸ºä¸€ä¸ªéå¸¸æœ‰å‰é€”çš„å€™é€‰è€…ã€‚æˆ‘ä»¬é€šè¿‡å°†PiNNæ¨¡å‹ï¼ˆPhysics-informed Neural Networkï¼‰ä¸MCTSç»“åˆä½¿ç”¨ï¼Œæé«˜äº†æ§åˆ¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„MCTSå®ç°ä¸PiNNæ¨¡å‹ç›¸æ¯”ï¼Œä¸è§„åˆ™æ§åˆ¶å™¨ç›¸æ¯”ï¼Œå¯ä»¥è·å¾—3%çš„å¢é‡å¥–åŠ±ï¼Œå¯¼è‡´10%çš„æˆæœ¬å‡å°‘å’Œ35%çš„æ¸©åº¦å·®å¼‚å‡å°‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ·»åŠ äº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ å±‚åˆ°MCTSæŠ€æœ¯ä¸­ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå¯¼å¼•æœç´¢æ›´ä¼˜åŒ–çš„æ ‘ã€‚ä¸æ™®é€šç‰ˆæœ¬ç›¸æ¯”ï¼Œè¿™ç§æ·»åŠ å‡å°‘äº†è®¡ç®—æˆæœ¬çš„éœ€æ±‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="Teaching-Specific-Scientific-Knowledge-into-Large-Language-Models-through-Additional-Training"><a href="#Teaching-Specific-Scientific-Knowledge-into-Large-Language-Models-through-Additional-Training" class="headerlink" title="Teaching Specific Scientific Knowledge into Large Language Models through Additional Training"></a>Teaching Specific Scientific Knowledge into Large Language Models through Additional Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03360">http://arxiv.org/abs/2312.03360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kan Hatakeyama-Sato, Yasuhiko Igarashi, Shun Katakami, Yuta Nabae, Teruaki Hayakawa</li>
<li>for: é€šè¿‡é¢å¤–è®­ç»ƒï¼Œæ¢ç´¢å°†ä¸“ä¸šç§‘å­¦çŸ¥è¯†åµŒå…¥LLMå¤§è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æ‰©å……æ¥è§£å†³ä¸“ä¸šæ–‡çŒ®ç¼ºä¹é—®é¢˜ï¼ŒåŒ…æ‹¬æ ·å¼è½¬æ¢å’Œç¿»è¯‘ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å‚æ•°ä¼˜åŒ–ã€‚</li>
<li>results: æˆ‘ä»¬æˆåŠŸåœ°åœ¨ä¸€å®šç¨‹åº¦ä¸ŠåµŒå…¥äº†çŸ¥è¯†ï¼Œä½†ç ”ç©¶æ˜¾ç¤ºåµŒå…¥ä¸“ä¸šä¿¡æ¯åˆ°LLMä¸­å­˜åœ¨å¤æ‚æ€§å’Œé™åˆ¶ï¼Œæå‡ºäº†è¿›ä¸€æ­¥æ”¹è¿›çš„æ–¹å‘ã€‚<details>
<summary>Abstract</summary>
Through additional training, we explore embedding specialized scientific knowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that effective knowledge integration requires reading texts from multiple perspectives, especially in instructional formats. We utilize text augmentation to tackle the scarcity of specialized texts, including style conversions and translations. Hyperparameter optimization proves crucial, with different size models (7b, 13b, and 70b) reasonably undergoing additional training. Validating our methods, we construct a dataset of 65,000 scientific papers. Although we have succeeded in partially embedding knowledge, the study highlights the complexities and limitations of incorporating specialized information into LLMs, suggesting areas for further improvement.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šè¿‡è¿›ä¸€æ­¥çš„è®­ç»ƒï¼Œæˆ‘ä»¬æ¢ç´¢å°†ä¸“ä¸šç§‘å­¦çŸ¥è¯† embeddingåˆ°å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚å…³é”®å‘ç°æ˜¾ç¤ºï¼Œæœ‰æ•ˆåœ° integrate çŸ¥è¯†éœ€è¦ä»å¤šä¸ªè§’åº¦é˜…è¯»æ–‡æœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•™å­¦æ ¼å¼ä¸‹ã€‚æˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æ‰©å±•æ¥è§£å†³ä¸“ä¸šæ–‡æœ¬ç¨€ç¼ºé—®é¢˜ï¼ŒåŒ…æ‹¬æ ·å¼è½¬æ¢å’Œç¿»è¯‘ã€‚æ¨¡å‹çš„è¶…å‚æ•°ä¼˜åŒ–è¯æ˜æ˜¯å…³é”®çš„ï¼Œä¸åŒå¤§å°çš„æ¨¡å‹ï¼ˆ7bã€13bå’Œ70bï¼‰éƒ½èƒ½å¤Ÿè¿›è¡Œè¿›ä¸€æ­¥çš„è®­ç»ƒã€‚ä¸ºéªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†65,000ç¯‡ç§‘å­¦è®ºæ–‡çš„æ•°æ®é›†ã€‚è™½ç„¶æˆ‘ä»¬åœ¨éƒ¨åˆ† embedding çŸ¥è¯†ä¸ŠæˆåŠŸï¼Œä½†ç ”ç©¶è¡¨æ˜å°†ä¸“ä¸šä¿¡æ¯ embedding åˆ° LLM ä¸­å­˜åœ¨å¤æ‚æ€§å’Œé™åˆ¶ï¼Œæå‡ºäº†è¿›ä¸€æ­¥æ”¹è¿›çš„æ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="Online-Vectorized-HD-Map-Construction-using-Geometry"><a href="#Online-Vectorized-HD-Map-Construction-using-Geometry" class="headerlink" title="Online Vectorized HD Map Construction using Geometry"></a>Online Vectorized HD Map Construction using Geometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03341">http://arxiv.org/abs/2312.03341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cnzzx/gemap">https://github.com/cnzzx/gemap</a></li>
<li>paper_authors: Zhixin Zhang, Yiyuan Zhang, Xiaohan Ding, Fusheng Jin, Xiangyu Yue</li>
<li>for: æå‡ºäº†ä¸€ç§åŸºäºEuclideanå‡ ä½•å­¦çš„æ˜ å°„å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜åœ¨åŸå¸‚é“è·¯ç³»ç»Ÿä¸­çš„é¢„æµ‹å’Œè§„åˆ’ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§å«åšGeMapçš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ•æ‰åˆ°åŸå¸‚é“è·¯ç³»ç»Ÿä¸­çš„å‡ ä½•å½¢æ€å’Œå…³ç³»ï¼Œå¹¶ä¸”å¯ä»¥ç‹¬ç«‹å¤„ç†å‡ ä½•å½¢æ€å’Œå…³ç³»ã€‚</li>
<li>results: åœ¨NuSceneså’ŒArgoverse 2 datasetsä¸Šå®ç°äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œå…¶ä¸­åœ¨Argoverse 2 datasetä¸Šè¾¾åˆ°äº†71.8%çš„mAPï¼Œæ¯”MapTR V2é«˜4.4%ï¼Œå¹¶é¦–æ¬¡çªç ´äº†70%çš„mAPé˜ˆå€¼ã€‚<details>
<summary>Abstract</summary>
The construction of online vectorized High-Definition (HD) maps is critical for downstream prediction and planning. Recent efforts have built strong baselines for this task, however, shapes and relations of instances in urban road systems are still under-explored, such as parallelism, perpendicular, or rectangle-shape. In our work, we propose GeMap ($\textbf{Ge}$ometry $\textbf{Map}$), which end-to-end learns Euclidean shapes and relations of map instances beyond basic perception. Specifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformations. We also decouple self-attention to independently handle Euclidean shapes and relations. Our method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first time. Code is available at https://github.com/cnzzx/GeMap
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œåœ¨çº¿vectoråŒ–é«˜æ¸…åœ°å›¾çš„æ„å»ºæ˜¯ä¸‹æ¸¸é¢„æµ‹å’Œè§„åˆ’çš„å…³é”®ã€‚ recent efforts have built strong baselines for this task, but shapes and relations of instances in urban road systems are still under-explored, such as parallelism, perpendicular, or rectangle-shape. In our work, we propose GeMapï¼ˆåœ°å›¾å‡ ä½•å¯¹æ˜ ï¼‰, which end-to-end learns Euclidean shapes and relations of map instances beyond basic perception. Specifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformations. We also decouple self-attention to independently handle Euclidean shapes and relations. Our method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first time. â€Here's the breakdown of the translation:* â€œåœ¨çº¿vectoråŒ–é«˜æ¸…åœ°å›¾â€(online vectorized high-definition maps) is translated as â€œåœ¨çº¿vectoråŒ–é«˜æ¸…åœ°å›¾â€(åœ¨çº¿vectorizedé«˜æ¸…åœ°å›¾)* â€œæ„å»ºâ€(construction) is translated as â€œæ„å»ºâ€(æ„å»º)* â€œdownstream prediction and planningâ€(downstream prediction and planning) is translated as â€œä¸‹æ¸¸é¢„æµ‹å’Œè§„åˆ’â€(ä¸‹æ¸¸é¢„æµ‹å’Œè§„åˆ’)* â€œRecent efforts have built strong baselines for this taskâ€(recent efforts have built strong baselines for this task) is translated as â€œrecent efforts have built strong baselines for this taskâ€(recent efforts have built strong baselines for this task)* â€œbut shapes and relations of instances in urban road systems are still under-exploredâ€(but shapes and relations of instances in urban road systems are still under-explored) is translated as â€œbut shapes and relations of instances in urban road systems are still under-exploredâ€(but shapes and relations of instances in urban road systems are still under-explored)* â€œsuch as parallelism, perpendicular, or rectangle-shapeâ€(such as parallelism, perpendicular, or rectangle-shape) is translated as â€œsuch as parallelism, perpendicular, or rectangle-shapeâ€(such as parallelism, perpendicular, or rectangle-shape)* â€œIn our work, we propose GeMapâ€(In our work, we propose GeMap) is translated as â€œIn our work, we propose GeMapâ€(åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GeMap)* â€œwhich end-to-end learns Euclidean shapes and relations of map instances beyond basic perceptionâ€(which end-to-end learns Euclidean shapes and relations of map instances beyond basic perception) is translated as â€œwhich end-to-end learns Euclidean shapes and relations of map instances beyond basic perceptionâ€(which end-to-end learnsEuclidean shapes and relations of map instances beyond basic perception)* â€œSpecifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformationsâ€(Specifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformations) is translated as â€œSpecifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformationsâ€(specifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformations)* â€œWe also decouple self-attention to independently handle Euclidean shapes and relationsâ€(We also decouple self-attention to independently handle Euclidean shapes and relations) is translated as â€œWe also decouple self-attention to independently handle Euclidean shapes and relationsâ€(we also decouple self-attention to independently handle Euclidean shapes and relations)* â€œOur method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasetsâ€(Our method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasets) is translated as â€œour method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasetsâ€(æˆ‘ä»¬çš„æ–¹æ³•åœ¨NuSceneså’ŒArgoverse 2 datasetä¸Šè¾¾åˆ°äº†æ–°çš„state-of-the-artæ€§èƒ½)* â€œRemarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first timeâ€(Remarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first time) is translated as â€œremarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first timeâ€(remarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first time)Note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form as well.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Continual-Learning-from-Cognitive-Perspectives"><a href="#Benchmarking-Continual-Learning-from-Cognitive-Perspectives" class="headerlink" title="Benchmarking Continual Learning from Cognitive Perspectives"></a>Benchmarking Continual Learning from Cognitive Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03309">http://arxiv.org/abs/2312.03309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoqian Liu, Junge Zhang, Mingyi Zhang, Peipei Yang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ continual learning é—®é¢˜ï¼Œå³ä¸æ–­å­¦ä¹ å’Œè½¬ç§»çŸ¥è¯†è€Œä¸å¯¼è‡´è€çŸ¥è¯†å¿˜è®°ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å¤šç§æ–¹æ³•æ¥è¯„ä¼° continual learning æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäº cognitive properties çš„ desideratum å’Œå¤šç§è¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„ continual learning æ¨¡å‹å°šæœªæ»¡è¶³æ‰€æœ‰ desideratumï¼Œå¹¶ä¸”å°šæœªå®ç°çœŸæ­£çš„ continual learningã€‚ although some methods å…·æœ‰ä¸€å®šçš„é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œä½†æ˜¯æ— æ³•è¯†åˆ«ä»»åŠ¡å˜åŒ–æ—¶çš„ä»»åŠ¡å…³ç³»ï¼Œæˆ–è€…å¯»æ±‚ä»»åŠ¡ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œä¸åŒæ€§ã€‚<details>
<summary>Abstract</summary>
Continual learning addresses the problem of continuously acquiring and transferring knowledge without catastrophic forgetting of old concepts. While humans achieve continual learning via diverse neurocognitive mechanisms, there is a mismatch between cognitive properties and evaluation methods of continual learning models. First, the measurement of continual learning models mostly relies on evaluation metrics at a micro-level, which cannot characterize cognitive capacities of the model. Second, the measurement is method-specific, emphasizing model strengths in one aspect while obscuring potential weaknesses in other respects. To address these issues, we propose to integrate model cognitive capacities and evaluation metrics into a unified evaluation paradigm. We first characterize model capacities via desiderata derived from cognitive properties supporting human continual learning. The desiderata concern (1) adaptability in varying lengths of task sequence; (2) sensitivity to dynamic task variations; and (3) efficiency in memory usage and training time consumption. Then we design evaluation protocols for each desideratum to assess cognitive capacities of recent continual learning models. Experimental results show that no method we consider has satisfied all the desiderata and is still far away from realizing truly continual learning. Although some methods exhibit some degree of adaptability and efficiency, no method is able to identify task relationships when encountering dynamic task variations, or achieve a trade-off in learning similarities and differences between tasks. Inspired by these results, we discuss possible factors that influence model performance in these desiderata and provide guidance for the improvement of continual learning models.
</details>
<details>
<summary>æ‘˜è¦</summary>
First, the evaluation metrics used are mainly micro-level, which cannot fully capture the cognitive abilities of the model. Second, the evaluation is method-specific, highlighting the strengths of the model in one aspect while hiding its potential weaknesses in other areas. To address these issues, we propose integrating model cognitive abilities and evaluation metrics into a unified evaluation paradigm.We first define the cognitive capabilities of the model based on the cognitive properties that support human continual learning, including the ability to adapt to varying task sequences, sensitivity to dynamic task variations, and efficient use of memory and training time. Then, we design evaluation protocols for each of these desiderata to assess the cognitive abilities of recent continual learning models.The experimental results show that none of the methods we considered have fully met all of the desiderata and are still far from achieving true continual learning. While some methods have shown some degree of adaptability and efficiency, they have failed to identify task relationships when facing dynamic task variations or balance learning similarities and differences between tasks.Inspired by these results, we discuss potential factors that may influence model performance in these desiderata and provide guidance for improving continual learning models.
</details></li>
</ul>
<hr>
<h2 id="Dyport-Dynamic-Importance-based-Hypothesis-Generation-Benchmarking-Technique"><a href="#Dyport-Dynamic-Importance-based-Hypothesis-Generation-Benchmarking-Technique" class="headerlink" title="Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique"></a>Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03303">http://arxiv.org/abs/2312.03303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ilyatyagin/dyport">https://github.com/ilyatyagin/dyport</a></li>
<li>paper_authors: Ilya Tyagin, Ilya Safro</li>
<li>for: è¿™ paper æ˜¯ä¸€ä¸ªæ–°çš„ç”Ÿç‰©åŒ»å­¦å‡è®¾ç”Ÿæˆç³»ç»Ÿè¯„ä¼°æ¡†æ¶ Dyportã€‚</li>
<li>methods: è¯¥approach ä½¿ç”¨äº†å·²ç»ç²¾å¿ƒç¼–è¾‘çš„æ•°æ®é›†ï¼Œä½¿å¾—æˆ‘ä»¬çš„è¯„ä¼°æ›´åŠ çœŸå®ã€‚å®ƒ integrates çŸ¥è¯†åˆ°curated databases ä¸­çš„åŠ¨æ€å›¾è¡¨ï¼Œå¹¶æä¾›äº†ä¸€ç§é‡åŒ–å‘ç°é‡è¦æ€§çš„æ–¹æ³•ï¼Œä¸ä»…è¯„ä¼°å‡è®¾çš„å‡†ç¡®æ€§ï¼Œè¿˜è¯„ä¼°å…¶åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„å¯èƒ½çš„å½±å“ï¼Œè¿™å¤§å¤§è¶…è¶Šäº†ä¼ ç»Ÿçš„é“¾æ¥é¢„æµ‹benchmarkã€‚</li>
<li>results: æˆ‘ä»¬åœ¨åº”ç”¨äº†several link prediction systems åœ¨ç”Ÿç‰©åŒ»å­¦semantic knowledge graphs ä¸Šçš„å®éªŒä¸­ï¼Œdemonstrated äº†æˆ‘ä»¬çš„è¯„ä¼°ç³»ç»Ÿçš„å¯è¡Œæ€§å’Œçµæ´»æ€§ã€‚<details>
<summary>Abstract</summary>
This paper presents a novel benchmarking framework Dyport for evaluating biomedical hypothesis generation systems. Utilizing curated datasets, our approach tests these systems under realistic conditions, enhancing the relevance of our evaluations. We integrate knowledge from the curated databases into a dynamic graph, accompanied by a method to quantify discovery importance. This not only assesses hypothesis accuracy but also their potential impact in biomedical research which significantly extends traditional link prediction benchmarks. Applicability of our benchmarking process is demonstrated on several link prediction systems applied on biomedical semantic knowledge graphs. Being flexible, our benchmarking system is designed for broad application in hypothesis generation quality verification, aiming to expand the scope of scientific discovery within the biomedical research community. Availability and implementation: Dyport framework is fully open-source. All code and datasets are available at: https://github.com/IlyaTyagin/Dyport
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç”Ÿç‰©åŒ»å­¦å‡è®¾ç”Ÿæˆç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œå³Dyportã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†ä»”ç»†ç¼–è¾‘çš„æ•°æ®é›†ï¼Œä½¿æˆ‘ä»¬çš„è¯„ä¼°æ›´åŠ çœŸå®ã€‚æˆ‘ä»¬å°†çŸ¥è¯†ä» curaated æ•°æ®åº“ integrate åˆ°åŠ¨æ€å›¾ä¸­ï¼Œå¹¶æä¾›ä¸€ç§é‡åŒ–å‘ç°é‡è¦æ€§çš„æ–¹æ³•ã€‚è¿™ä¸ä»…è¯„ä¼°å‡è®¾å‡†ç¡®æ€§ï¼Œè¿˜è¯„ä¼°å…¶åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„å¯èƒ½çš„å½±å“ï¼Œè¿™åœ¨ä¼ ç»Ÿçš„é“¾æ¥é¢„æµ‹æµ‹è¯•ä¸­è¿›è¡Œäº†æ˜¾è‘—æ‰©å±•ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¿‡ç¨‹çš„å¯åº”ç”¨æ€§åœ¨å¤šä¸ªé“¾æ¥é¢„æµ‹ç³»ç»Ÿä¸Šè¿›è¡Œäº†åº”ç”¨ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç³»ç»Ÿæ˜¯ flexible çš„ï¼Œå¯ä»¥å¹¿æ³›åº”ç”¨äºå‡è®¾ç”Ÿæˆè´¨é‡éªŒè¯ä¸­ï¼Œä»¥æ‰©å±•ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç¤¾åŒºçš„ç§‘å­¦å‘ç°èŒƒå›´ã€‚å¯ç”¨æ€§å’Œå®ç°ï¼šDyport æ¡†æ¶æ˜¯å®Œå…¨å¼€æºçš„ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼šhttps://github.com/IlyaTyagin/Dyportã€‚
</details></li>
</ul>
<hr>
<h2 id="SoftMAC-Differentiable-Soft-Body-Simulation-with-Forecast-based-Contact-Model-and-Two-way-Coupling-with-Articulated-Rigid-Bodies-and-Clothes"><a href="#SoftMAC-Differentiable-Soft-Body-Simulation-with-Forecast-based-Contact-Model-and-Two-way-Coupling-with-Articulated-Rigid-Bodies-and-Clothes" class="headerlink" title="SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes"></a>SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03297">http://arxiv.org/abs/2312.03297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damianliumin/SoftMAC">https://github.com/damianliumin/SoftMAC</a></li>
<li>paper_authors: Min Liu, Gang Yang, Siyuan Luo, Chen Yu, Lin Shao</li>
<li>for:  This paper aims to provide a unified framework for simulating diverse robotic manipulation scenarios by integrating soft bodies, articulated rigid bodies, and clothes.</li>
<li>methods: The proposed method, called SoftMAC, uses the Material Point Method (MPM) to simulate soft bodies and a forecast-based contact model to reduce artifacts. It also includes a penetration tracing algorithm to couple MPM particles with deformable and non-volumetric clothes meshes.</li>
<li>results: The authors validate the effectiveness and accuracy of the proposed differentiable pipeline through comprehensive experiments in downstream robotic manipulation applications.Hereâ€™s the Chinese version:</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®çš„æ˜¯æä¾›ä¸€ä¸ªç»¼åˆçš„æœºå™¨äººæ“ä½œåœºæ™¯æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ•´åˆè½¯ä½“ã€éª¨éª¼åˆšä½“å’Œè¡£ç‰©ç­‰å¤šç§ææ–™ã€‚</li>
<li>methods: æè®®çš„æ–¹æ³•æ˜¯SoftMACï¼Œä½¿ç”¨ç‰©ç†ç‚¹æ–¹æ³•ï¼ˆMPMï¼‰æ¨¡æ‹Ÿè½¯ä½“ï¼Œå¹¶é‡‡ç”¨é¢„æµ‹åŸºäºçš„æ¥è§¦æ¨¡å‹æ¥å‡å°‘artefactsã€‚å®ƒè¿˜åŒ…æ‹¬ä¸€ç§ç©¿é€è·Ÿè¸ªç®—æ³•ï¼Œå°†MPMç²’å­ä¸å¯å˜å½¢å’Œéæ¶²ä½“è¡£ç‰©ç½‘æ ¼ç›¸äº’å…³è”ã€‚</li>
<li>results: ä½œè€…é€šè¿‡å¯¹ä¸‹æ¸¸æœºå™¨äººæ“ä½œåº”ç”¨çš„å¹¿æ³›å®éªŒ validateäº†æè®®çš„å¯å¯¼å¼ç®¡é“çš„æ•ˆæœå’Œå‡†ç¡®æ€§ã€‚<details>
<summary>Abstract</summary>
Differentiable physics simulation provides an avenue for tackling previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework coupling soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a forecast-based contact model for MPM, which greatly reduces artifacts like penetration and unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Based on simulators for each modality and the contact model, we develop a differentiable coupling mechanism to simulate the interactions between soft bodies and the other two types of materials. Comprehensive experiments are conducted to validate the effectiveness and accuracy of the proposed differentiable pipeline in downstream robotic manipulation applications. Supplementary materials and videos are available on our project website at https://sites.google.com/view/softmac.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šå¯å¾®åˆ†ç‰©ç†æ¨¡æ‹Ÿï¼šä¸€ç§æé«˜æœºå™¨äººé—®é¢˜è§£å†³æ•ˆç‡çš„æ–°é€”å¾„ã€‹å¯å¾®åˆ†ç‰©ç†æ¨¡æ‹Ÿæä¾›äº†è§£å†³å‰æ— æ³•è§£å†³çš„æŒ‘æˆ˜çš„æ–°é€”å¾„ï¼Œé€šè¿‡æ¢¯åº¦åŸºäºä¼˜åŒ–ï¼Œå¤§å¹…æé«˜æœºå™¨äººé—®é¢˜çš„è§£å†³æ•ˆç‡ã€‚ä¸ºåœ¨å¤šæ ·åŒ–æœºå™¨äººæ“ä½œåœºæ™¯ä¸­åº”ç”¨å¯å¾®åˆ†æ¨¡æ‹Ÿï¼Œä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å°†å„ç§ææ–™é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†SoftMACï¼Œä¸€ä¸ªå¯å¾®åˆ†æ¨¡æ‹Ÿæ¡†æ¶ï¼Œå°†è½¯ä½“ä¸æœºæ¢°è‚¢å’Œè¡£ç‰©ç›¸è¿æ¥ã€‚SoftMACä½¿ç”¨ç‰©ç‚¹æ–¹æ³•ï¼ˆMPMï¼‰æ¥æ¨¡æ‹Ÿè½¯ä½“ï¼Œå¹¶æä¾›äº†ä¸€ç§é¢„æµ‹åŸºäºçš„æ¥è§¦æ¨¡å‹ï¼Œå¯ä»¥å‡å°‘ç©¿é€å’Œä¸è‡ªç„¶çš„åå¼¹ç°è±¡ã€‚ä¸ºå°†MPM particelsä¸å¯å˜å½¢å’Œéæ¶²ä½“è¡£ç‰©ç½‘æ ¼ç›¸è¿æ¥ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç©¿é€è·Ÿè¸ªç®—æ³•ï¼Œå¯ä»¥åœ¨æœ¬åœ°åŒºåŸŸé‡å»ºç­¾åè·ç¦»åœºã€‚åŸºäºæ¨¡æ‹Ÿå™¨å’Œæ¥è§¦æ¨¡å‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯å¾®åˆ†è¿æ¥æœºåˆ¶ï¼Œä»¥æ¨¡æ‹Ÿè½¯ä½“ä¸å…¶ä»–ä¸¤ç§ææ–™ä¹‹é—´çš„äº¤äº’ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥éªŒè¯ææ¡£çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§åœ¨ä¸‹æ¸¸æœºå™¨äººæ“ä½œåº”ç”¨ä¸­ã€‚è¡¥å……ææ–™å’Œè§†é¢‘å¯ä»¥åœ¨æˆ‘ä»¬é¡¹ç›®ç½‘ç«™ï¼ˆhttps://sites.google.com/view/softmacï¼‰ä¸Šè·å¾—ã€‚
</details></li>
</ul>
<hr>
<h2 id="OMNIINPUT-A-Model-centric-Evaluation-Framework-through-Output-Distribution"><a href="#OMNIINPUT-A-Model-centric-Evaluation-Framework-through-Output-Distribution" class="headerlink" title="OMNIINPUT: A Model-centric Evaluation Framework through Output Distribution"></a>OMNIINPUT: A Model-centric Evaluation Framework through Output Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03291">http://arxiv.org/abs/2312.03291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weitang Liu, Ying Wai Li, Tianle Wang, Yi-Zhuang You, Jingbo Shang</li>
<li>for: è¯„ä¼°AI&#x2F;MLæ¨¡å‹é¢„æµ‹ç»“æœçš„è´¨é‡ï¼Œå°¤å…¶æ˜¯å¯¹äºäººç±»ä¸å¯è¯†åˆ«çš„è¾“å…¥ã€‚</li>
<li>methods: ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•é›†å’Œæ¨¡å‹è‡ªèº«çš„è¾“å‡ºåˆ†å¸ƒæ¥è¯„ä¼°æ¨¡å‹è´¨é‡ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„æ•°æ®é›†ä¸­å¿ƒçš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>results: èƒ½å¤Ÿæ›´ç»†åŒ–åœ°æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹ç»“æœå‡ ä¹ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œä»è€Œè·å¾—æ–°çš„å‘ç°å’Œå¯ç¤ºï¼Œæœ‰åŠ©äºè®­ç»ƒæ›´åŠ ç¨³å®šå’Œæ³›åŒ–çš„æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
We propose a novel model-centric evaluation framework, OmniInput, to evaluate the quality of an AI/ML model's predictions on all possible inputs (including human-unrecognizable ones), which is crucial for AI safety and reliability. Unlike traditional data-centric evaluation based on pre-defined test sets, the test set in OmniInput is self-constructed by the model itself and the model quality is evaluated by investigating its output distribution. We employ an efficient sampler to obtain representative inputs and the output distribution of the trained model, which, after selective annotation, can be used to estimate the model's precision and recall at different output values and a comprehensive precision-recall curve. Our experiments demonstrate that OmniInput enables a more fine-grained comparison between models, especially when their performance is almost the same on pre-defined datasets, leading to new findings and insights for how to train more robust, generalizable models.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ä¸­å¿ƒè¯„ä¼°æ¡†æ¶ï¼ŒOmniInputï¼Œä»¥è¯„ä¼°äººå·¥æ™ºèƒ½/æœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸­çš„æ‰€æœ‰å¯èƒ½è¾“å…¥ï¼ˆåŒ…æ‹¬äººç±»æ— æ³•è¯†åˆ«çš„ï¼‰ï¼Œè¿™å¯¹äºäººå·¥æ™ºèƒ½å®‰å…¨å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®ä¸­å¿ƒè¯„ä¼°åŸºäºé¢„å…ˆå®šä¹‰çš„æµ‹è¯•é›†ä¸åŒï¼ŒOmniInput çš„æµ‹è¯•é›†ç”±æ¨¡å‹è‡ªå·±æ„å»ºï¼Œå¹¶é€šè¿‡è°ƒæŸ¥è¾“å‡ºåˆ†å¸ƒæ¥è¯„ä¼°æ¨¡å‹è´¨é‡ã€‚æˆ‘ä»¬ä½¿ç”¨é«˜æ•ˆçš„é‡‡æ ·å™¨è·å–ä»£è¡¨æ€§çš„è¾“å…¥ï¼Œå¹¶å¯¹è®­ç»ƒåçš„æ¨¡å‹è¾“å‡ºè¿›è¡Œé€‰æ‹©æ€§æ ‡æ³¨ï¼Œä»¥ä¾¿è®¡ç®—æ¨¡å‹çš„ç²¾åº¦å’Œå‡†ç¡®ç‡åœ¨ä¸åŒçš„è¾“å‡ºå€¼ä¸Šï¼Œå¹¶ç”Ÿæˆäº†å…¨é¢çš„ç²¾åº¦-å‡†ç¡®ç‡æ›²çº¿ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒOmniInput å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œæ›´ç»†è‡´çš„æ¯”è¾ƒï¼Œç‰¹åˆ«æ˜¯å½“æ¨¡å‹åœ¨é¢„å…ˆå®šä¹‰çš„æ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ ä¹ç›¸åŒæ—¶ï¼Œä»è€Œå¯¼è‡´æ–°çš„å‘ç°å’Œæ´å¯Ÿï¼Œå¸®åŠ©trainæ›´åŠ ç¨³å®šå’Œæ³›åŒ–çš„æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Can-language-agents-be-alternatives-to-PPO-A-Preliminary-Empirical-Study-On-OpenAI-Gym"><a href="#Can-language-agents-be-alternatives-to-PPO-A-Preliminary-Empirical-Study-On-OpenAI-Gym" class="headerlink" title="Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym"></a>Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03290">http://arxiv.org/abs/2312.03290</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mail-ecnu/text-gym-agents">https://github.com/mail-ecnu/text-gym-agents</a></li>
<li>paper_authors: Junjie Sheng, Zixiao Huang, Chuyun Shen, Wenhao Li, Yun Hua, Bo Jin, Hongyuan Zha, Xiangfeng Wang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨è¯­è¨€ä»£ç†æ˜¯å¦å¯ä»¥å–ä»£ä¼ ç»Ÿçš„PPOä»£ç†åœ¨é¡ºåºå†³ç­–ä»»åŠ¡ä¸­ã€‚</li>
<li>methods: ç ”ç©¶è€…é¦–å…ˆä½¿ç”¨OpenAI Gymä¸­æ”¶é›†çš„ç¯å¢ƒä½œä¸ºæµ‹è¯•åºŠï¼Œå¹¶å°†è¿™äº›ç¯å¢ƒè½¬åŒ–ä¸ºæ–‡æœ¬ç¯å¢ƒï¼Œä»¥ä¾¿ä¸è¯­è¨€ä»£ç†è¿›è¡Œç›´è§‚å’Œé«˜æ•ˆçš„æ¯”è¾ƒã€‚</li>
<li>results: ç ”ç©¶è€…é€šè¿‡æ•°å€¼å®éªŒå’Œå‰–æç ”ç©¶ï¼Œæå–äº†è¯­è¨€ä»£ç†çš„å†³ç­–èƒ½åŠ›çš„æœ‰ä»·å€¼ä¿¡æ¯ï¼Œå¹¶å¯¹è¯­è¨€ä»£ç†ä½œä¸ºPPOä»£ç†çš„æ½œåœ¨ä»£æ›¿è¿›è¡Œåˆæ­¥è¯„ä¼°ã€‚<details>
<summary>Abstract</summary>
The formidable capacity for zero- or few-shot decision-making in language agents encourages us to pose a compelling question: Can language agents be alternatives to PPO agents in traditional sequential decision-making tasks? To investigate this, we first take environments collected in OpenAI Gym as our testbeds and ground them to textual environments that construct the TextGym simulator. This allows for straightforward and efficient comparisons between PPO agents and language agents, given the widespread adoption of OpenAI Gym. To ensure a fair and effective benchmarking, we introduce $5$ levels of scenario for accurate domain-knowledge controlling and a unified RL-inspired framework for language agents. Additionally, we propose an innovative explore-exploit-guided language (EXE) agent to solve tasks within TextGym. Through numerical experiments and ablation studies, we extract valuable insights into the decision-making capabilities of language agents and make a preliminary evaluation of their potential to be alternatives to PPO in classical sequential decision-making problems. This paper sheds light on the performance of language agents and paves the way for future research in this exciting domain. Our code is publicly available at~\url{https://github.com/mail-ecnu/Text-Gym-Agents}.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡ä¸­æå‡ºäº†ä¸€ä¸ªå¸å¼•äººçš„é—®é¢˜ï¼šå¯ä»¥å¦ä½¿ç”¨è¯­è¨€ä»£ç†äººä»£æ›¿ä¼ ç»Ÿçš„é¡ºåºå†³ç­–ä»»åŠ¡ä¸­çš„PPOä»£ç†äººï¼Ÿä¸ºäº† investigateè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨OpenAI Gymä¸­æ”¶é›†çš„ç¯å¢ƒä½œä¸ºæµ‹è¯•ç¯å¢ƒï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºæ–‡æœ¬ç¯å¢ƒï¼Œè¿™ä½¿å¾—å¯¹æ¯”è¯­è¨€ä»£ç†äººå’ŒPPOä»£ç†äººçš„æ¯”è¾ƒå˜å¾—æ›´åŠ ç›´è§‚å’Œæ•ˆç‡é«˜ã€‚ä¸ºç¡®ä¿å…¬æ­£å’Œæœ‰æ•ˆçš„å¯¹æ¯”ï¼Œæˆ‘ä»¬å¼•å…¥äº†5çº§çš„æƒ…æ™¯æ¥æ§åˆ¶åŸŸçŸ¥è¯†ï¼Œå¹¶æå‡ºäº†ä¸€ç§RL inspiritedæ¡†æ¶æ¥ guidelineè¯­è¨€ä»£ç†äººè§£å†³TextGymä¸­çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å°è¯•-åˆ©ç”¨-å¼•å¯¼è¯­è¨€ä»£ç†äººï¼ˆEXEï¼‰æ¥è§£å†³TextGymä¸­çš„ä»»åŠ¡ã€‚é€šè¿‡æ•°å€¼å®éªŒå’Œå‰¥ç¦»ç ”ç©¶ï¼Œæˆ‘ä»¬ä»è¯­è¨€ä»£ç†äººçš„å†³ç­–èƒ½åŠ›ä¸­è·å¾—äº†æœ‰ä»·å€¼çš„å‘ç°ï¼Œå¹¶å¯¹è¯­è¨€ä»£ç†äººæ˜¯å¦å¯ä»¥æ›¿ä»£PPOè¿›è¡Œäº†åˆæ­¥è¯„ä¼°ã€‚è¿™ç¯‡è®ºæ–‡ç…§äº®äº†è¯­è¨€ä»£ç†äººçš„è¡¨ç°ï¼Œå¹¶ä¸ºè¿™ä¸€æœ‰è¶£çš„é¢†åŸŸå¼€è¾Ÿäº†æœªæ¥ç ”ç©¶çš„é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€å¯ä»¥åœ¨GitHubä¸Šè·å–ï¼Œè¯·å‚è€ƒ~\url{https://github.com/mail-ecnu/Text-Gym-Agents}.
</details></li>
</ul>
<hr>
<h2 id="STEP-CATFormer-Spatial-Temporal-Effective-Body-Part-Cross-Attention-Transformer-for-Skeleton-based-Action-Recognition"><a href="#STEP-CATFormer-Spatial-Temporal-Effective-Body-Part-Cross-Attention-Transformer-for-Skeleton-based-Action-Recognition" class="headerlink" title="STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition"></a>STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03288">http://arxiv.org/abs/2312.03288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maclong01/STEP-CATFormer">https://github.com/maclong01/STEP-CATFormer</a></li>
<li>paper_authors: Nguyen Huu Bao Long</li>
<li>For: æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«ä¸­Graph Convolutional Convolution networksï¼ˆGCNsï¼‰çš„åº”ç”¨å’Œä¼˜åŒ–ã€‚* Methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸‰ç§Channel-wise Topology Graph Convolutionï¼ˆCTR-GCNï¼‰ï¼Œå¹¶å°†å…¶ä¸ä¸¤ç§è·¨ä½“éƒ¨å…³æ³¨æ¨¡å—ç»“åˆï¼Œä»¥æ•æ‰äººä½“éª¨æ¶ä¸Šä¸‹ä½“éƒ¨å’Œæ‰‹è„šå…³ç³»ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†Temporal Attention Transformersæ¥EXTRACTskeletonç‰¹å¾ã€‚* Results: æœ¬ç ”ç©¶åœ¨NTU RGB+Då’ŒNTU RGB+D 120æ•°æ®é›†ä¸Šè¾¾åˆ°äº† notable high-performanceã€‚Translation:* For: This study explores the application and optimization of Graph Convolutional Convolution networks (GCNs) in skeleton-based action recognition.* Methods: The study proposes three Channel-wise Topology Graph Convolution (CTR-GCN) methods, and combines them with two joint cross-attention modules to capture upper-lower body part and hand-foot relationships in skeleton features. Additionally, the study proposes Temporal Attention Transformers to extract skeleton features effectively.* Results: The study achieves notable high-performance on the NTU RGB+D and NTU RGB+D 120 datasets.<details>
<summary>Abstract</summary>
Graph convolutional networks (GCNs) have been widely used and achieved remarkable results in skeleton-based action recognition. We think the key to skeleton-based action recognition is a skeleton hanging in frames, so we focus on how the Graph Convolutional Convolution networks learn different topologies and effectively aggregate joint features in the global temporal and local temporal. In this work, we propose three Channel-wise Tolopogy Graph Convolution based on Channel-wise Topology Refinement Graph Convolution (CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture the upper-lower body part and hand-foot relationship skeleton features. After that, to capture features of human skeletons changing in frames we design the Temporal Attention Transformers to extract skeletons effectively. The Temporal Attention Transformers can learn the temporal features of human skeleton sequences. Finally, we fuse the temporal features output scale with MLP and classification. We develop a powerful graph convolutional network named Spatial Temporal Effective Body-part Cross Attention Transformer which notably high-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models are available at https://github.com/maclong01/STEP-CATFormer
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå‡ ä½•å·ç§¯ç½‘ç»œï¼ˆGCNsï¼‰åœ¨skeletonåŸºæœ¬åŠ¨ä½œè¯†åˆ«ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨å’Œå‡ºè‰²çš„ç»“æœã€‚æˆ‘ä»¬è®¤ä¸ºskeletonä¸­çš„éª¨æ¶å·ç§¯æ˜¯å…³é”®ï¼Œå› æ­¤æˆ‘ä»¬ä¸“æ³¨äºå¦‚ä½•ä½¿Graph Convolutional Convolutionç½‘ç»œå­¦ä¹ ä¸åŒçš„æ‹“æ‰‘å’Œæœ‰æ•ˆåœ°èšåˆå…³èŠ‚ç‰¹å¾åœ¨å…¨çƒæ—¶é—´å’Œå±€éƒ¨æ—¶é—´ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§é€šé“çº§åˆ«æ‹“æ‰‘å·ç§¯åŸºäºé€šé“çº§åˆ«æ‹“æ‰‘ä¿®å‰ªGraph Convolutionï¼ˆCTR-GCNï¼‰ã€‚å°†CTR-GCNä¸ä¸¤ä¸ªäº¤å‰å…³æ³¨æ¨¡å—ç›¸ç»“åˆå¯ä»¥æ•æ‰ä¸Šä¸‹èº¯ä½“å’Œæ‰‹è„šå…³ç³»éª¨æ¶ç‰¹å¾ã€‚ç„¶åï¼Œä¸ºäº†æœ‰æ•ˆåœ°æå–äººä½“éª¨æ¶åœ¨å¸§å†…çš„å˜åŒ–ç‰¹å¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ—¶é—´æ³¨æ„åŠ›å˜æ¢å™¨ã€‚æ—¶é—´æ³¨æ„åŠ›å˜æ¢å™¨å¯ä»¥å­¦ä¹ äººä½“éª¨æ¶åºåˆ—ä¸­çš„æ—¶é—´ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ—¶é—´ç‰¹å¾è¾“å‡ºè§„æ ¼ä¸å¤šå±‚æ„ŸçŸ¥ï¼ˆMLPï¼‰å’Œåˆ†ç±»ç»“åˆï¼Œå¹¶å‘å±•å‡ºä¸€ç§é«˜æ€§èƒ½çš„å‡ ä½•å·ç§¯ç½‘ç»œï¼Œç§°ä¸ºç©ºé—´æ—¶é—´æœ‰æ•ˆä½“éƒ¨ç›¸å…³è½¬æ¢å™¨ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯ä»¥åœ¨https://github.com/maclong01/STEP-CATFormerä¸Šè·å–ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="VLFM-Vision-Language-Frontier-Maps-for-Zero-Shot-Semantic-Navigation"><a href="#VLFM-Vision-Language-Frontier-Maps-for-Zero-Shot-Semantic-Navigation" class="headerlink" title="VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation"></a>VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03275">http://arxiv.org/abs/2312.03275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, Bernadette Bucher</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯æå‡ºä¸€ç§é›¶shot navigation æ–¹æ³•ï¼Œå¸®åŠ©æœºå™¨äººåœ¨æœªç»è®­ç»ƒçš„ç¯å¢ƒä¸­å¯»æ‰¾ç›®æ ‡å¯¹è±¡ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»æ·±åº¦è§‚å¯ŸValue Mapï¼Œå¹¶ä½¿ç”¨RGB Observationsæ¥ç”Ÿæˆè¯­è¨€æƒé‡å›¾ã€‚</li>
<li>results: è¯¥æ–¹æ³•åœ¨ Gibsonã€Habitat-Matterport 3D å’Œ Matterport 3D æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³çš„ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°chsï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸­éƒ¨ç½²åœ¨ Boston Dynamics Spot ç§»åŠ¨ manipulate å¹³å°ä¸Šï¼Œefficiently å¯¼èˆªåˆ°ç›®æ ‡å¯¹è±¡ã€‚<details>
<summary>Abstract</summary>
Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººç±»å¦‚ä½•åˆ©ç”¨ semantic knowledge æ¥æ¢ç´¢æœªçŸ¥ç¯å¢ƒå¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨å¯¹äºå¼€å‘äººç±»æ ·å¼æœç´¢è¡Œä¸ºçš„æœºå™¨äººæ¥è¯´éå¸¸é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é›¶æ‰¹æ³¨å¯¼èˆªæ–¹æ³•ï¼Œå³ Vision-Language Frontier Maps (VLFM)ï¼Œè¿™ç§æ–¹æ³•çµæ„Ÿè‡ªäººç±»çš„æ€ç»´å’Œå†³ç­–ï¼Œç”¨äºåœ¨æ–°ç¯å¢ƒä¸­å¯¼èˆªåˆ°æœªç»è§è¿‡çš„ semantic å¯¹è±¡ã€‚VLFM ä»æ·±åº¦è§‚æµ‹ä¸­ç”Ÿæˆå æ®åœ°å›¾ï¼Œå¹¶ä½¿ç”¨ RGB è§‚æµ‹å’Œé¢„è®­ç»ƒçš„è§†åŠ›è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯­è¨€å›ºå®šå€¼å›¾ã€‚VLFM ç„¶åä½¿ç”¨è¿™ä¸ªå›¾æ¥ç¡®å®šæœç´¢æœ€æœ‰å‰é€”çš„æ–¹å‘ï¼Œä»¥æ‰¾åˆ°ç»™å®šç›®æ ‡å¯¹è±¡ç±»å‹çš„å®ä¾‹ã€‚æˆ‘ä»¬åœ¨ Gibsonã€Habitat-Matterport 3D å’Œ Matterport 3D  datasets ä¸­çš„ Habitat  simulate ç¯å¢ƒè¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ˜¾ç¤º VLFM åœ¨è¿™äº›æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³çš„æˆç»©ï¼Œ measured by success weighted by path length (SPL) å¯¹è±¡ç›®æ ‡å¯¼èˆªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ VLFM çš„é›¶æ‰¹æ³¨ç‰¹æ€§ä½¿å¾—å®ƒå¯ä»¥è½»æ¾åœ°åœ¨çœŸå®ä¸–ç•Œä¸­éƒ¨ç½²ï¼Œå¦‚ Boston Dynamics Spot ç§»åŠ¨ manipulate å¹³å°ã€‚æˆ‘ä»¬åœ¨ Spot ä¸Šéƒ¨ç½² VLFMï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸­ efficiently å¯¼èˆªåˆ°ç›®æ ‡å¯¹è±¡å†…éƒ¨ï¼Œæ— éœ€ç¯å¢ƒçš„å…ˆå‰çŸ¥è¯†ã€‚VLFM çš„æˆå°±æ¨ç¥Ÿäºè§†åŠ›è¯­è¨€æ¨¡å‹åœ¨ semantic å¯¼èˆªé¢†åŸŸçš„æ½œåœ¨æ½œåŠ›ã€‚è§†é¢‘å¯ä»¥åœ¨ naoki.io/vlfm ä¸Šæ¬£èµã€‚
</details></li>
</ul>
<hr>
<h2 id="Weathering-Ongoing-Uncertainty-Learning-and-Planning-in-a-Time-Varying-Partially-Observable-Environment"><a href="#Weathering-Ongoing-Uncertainty-Learning-and-Planning-in-a-Time-Varying-Partially-Observable-Environment" class="headerlink" title="Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment"></a>Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03263">http://arxiv.org/abs/2312.03263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokul Puthumanaillam, Xiangyu Liu, Negar Mehr, Melkior Ornik</li>
<li>for: This paper aims to improve the optimal decision-making of autonomous systems in uncertain, stochastic, and time-varying environments.</li>
<li>methods: The paper combines Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). The proposed approach includes Memory Prioritized State Estimation (MPSE) and an MPSE-integrated planning strategy.</li>
<li>results: The proposed framework and algorithms demonstrate superior performance over standard methods in simulated and real-world experiments, showcasing their effectiveness in stochastic, uncertain, time-varying domains.<details>
<summary>Abstract</summary>
Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environments. Our results demonstrate superior performance over standard methods, highlighting the framework's effectiveness in stochastic, uncertain, time-varying domains.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼˜åŒ–å†³ç­–presentsthan significant challenge for autonomous systems operating inuncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision-making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environments. Our results demonstrate superior performance over standard methods, highlighting the framework's effectiveness in stochastic, uncertain, time-varying domains.Here's the translation in Traditional Chinese:ä¼˜åŒ–å†³ç­–å‘ˆç°äº† autonomous systems operate inuncertain, stochastic and time-varying environmentsä¸­çš„ä¸€ä¸ª significannot challenge. ç¯å¢ƒå˜åŒ–over timeå¯ä»¥å½±å“ç³»ç»Ÿçš„ä¼˜åŒ–å†³ç­–ç­–ç•¥ï¼Œä»¥ completeloss mission. ä¸ºäº†æ¨¡å‹è¿™äº›ç¯å¢ƒï¼Œæˆ‘ä»¬çš„å·¥ä½œcombines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environments. Our results demonstrate superior performance over standard methods, highlighting the framework's effectiveness in stochastic, uncertain, time-varying domains.
</details></li>
</ul>
<hr>
<h2 id="Customizable-Combination-of-Parameter-Efficient-Modules-for-Multi-Task-Learning"><a href="#Customizable-Combination-of-Parameter-Efficient-Modules-for-Multi-Task-Learning" class="headerlink" title="Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning"></a>Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03248">http://arxiv.org/abs/2312.03248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haowen Wang, Tao Sun, Cong Fan, Jinjie Gu</li>
<li>for: æé«˜å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ ·æœ¬æ•ˆç‡</li>
<li>methods: ä½¿ç”¨è‡ªé€‚åº”ç²¾åº¦å­¦ä¹ ç­–ç•¥å’Œä½çº§æ•°æ®ç²¾åº¦å­¦ä¹ </li>
<li>results: æ¯”å¯¹åŸºelineå’Œä»»åŠ¡ç‰¹å®šå’ŒæŠ€èƒ½æ— å…³åŸºelineçš„å®éªŒç»“æœï¼ŒC-Polyæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ€§èƒ½æå‡<details>
<summary>Abstract</summary>
Modular and composable transfer learning is an emerging direction in the field of Parameter Efficient Fine-Tuning, as it enables neural networks to better organize various aspects of knowledge, leading to improved cross-task generalization. In this paper, we introduce a novel approach Customized Polytropon C-Poly that combines task-common skills and task-specific skills, while the skill parameters being highly parameterized using low-rank techniques. Each task is associated with a customizable number of exclusive specialized skills and also benefits from skills shared with peer tasks. A skill assignment matrix is jointly learned. To evaluate our approach, we conducted extensive experiments on the Super-NaturalInstructions and the SuperGLUE benchmarks. Our findings demonstrate that C-Poly outperforms fully-shared, task-specific, and skill-indistinguishable baselines, significantly enhancing the sample efficiency in multi-task learning scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ¨¡å—åŒ–å’Œå¯ compose çš„ä¼ è¾“å­¦ä¹ æ˜¯ç°ä»£ Parameter Efficient Fine-Tuning é¢†åŸŸçš„ä¸€ä¸ªemerging directionï¼Œå› ä¸ºå®ƒä½¿å¾—ç¥ç»ç½‘ç»œæ›´å¥½åœ°ç»„ç»‡äº†ä¸åŒæ–¹é¢çš„çŸ¥è¯†ï¼Œä»è€Œæé«˜äº†äº¤å‰ä»»åŠ¡æ³›åŒ–æ€§ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•Customized Polytropon C-Polyï¼Œå®ƒå°†ä»»åŠ¡å…±åŒæŠ€èƒ½å’Œä»»åŠ¡ç‰¹å®šæŠ€èƒ½ç»“åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä½¿ç”¨ä½ç»´åº¦æŠ€æœ¯æ¥é«˜åº¦å‚æ•°åŒ–æŠ€èƒ½å‚æ•°ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æœ‰å¯å®šåˆ¶çš„ä¸“å±ç‰¹æœ‰æŠ€èƒ½ï¼ŒåŒæ—¶è¿˜å¯ä»¥ä»åŒç±»ä»»åŠ¡ä¸­è·å¾—å…±äº«çš„æŠ€èƒ½ã€‚ä¸€ä¸ªä»»åŠ¡åˆ†é…çŸ©é˜µæ˜¯åŒæ—¶å­¦ä¹ çš„ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨Super-NaturalInstructionså’ŒSuperGLUE bencmarksä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼ŒC-Poly åœ¨å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Framework-to-Enhance-the-Adversarial-Robustness-of-Deep-Learning-based-Intrusion-Detection-System"><a href="#A-Simple-Framework-to-Enhance-the-Adversarial-Robustness-of-Deep-Learning-based-Intrusion-Detection-System" class="headerlink" title="A Simple Framework to Enhance the Adversarial Robustness of Deep Learning-based Intrusion Detection System"></a>A Simple Framework to Enhance the Adversarial Robustness of Deep Learning-based Intrusion Detection System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03245">http://arxiv.org/abs/2312.03245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinwei Yuan, Shu Han, Wei Huang, Hongliang Ye, Xianglong Kong, Fan Zhang</li>
<li>For: The paper proposes a novel intrusion detection system (IDS) architecture that combines conventional machine learning (ML) models and deep learning (DL) models to enhance the robustness of IDS against adversarial attacks.* Methods: The proposed IDS architecture consists of three components: DL-based IDS, adversarial example (AE) detector, and ML-based IDS. The AE detector is based on the local intrinsic dimensionality (LID), and the ML-based IDS is used to determine the maliciousness of AEs. The fusion mechanism leverages the high prediction accuracy of DL models and low attack transferability between DL models and ML models to improve the robustness of the whole system.* Results: The paper shows a significant improvement in the prediction performance of the IDS when subjected to adversarial attack, achieving high accuracy with low resource consumption.Here are the three key points in Simplified Chinese text:* For: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿï¼ˆIDSï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ï¼Œä»¥æé«˜IDSå¯¹äºæ”»å‡»è€…çš„æŠµæŠ—æ€§ã€‚* Methods: æè®®çš„IDSæ¶æ„åŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šDL-based IDSã€æ”»å‡»ä¾‹ç¤ºå™¨ï¼ˆAEï¼‰æ£€æµ‹å™¨å’ŒML-based IDSã€‚AEæ£€æµ‹å™¨åŸºäºæœ¬åœ°ç‰¹å¾ç»´åº¦ï¼ˆLIDï¼‰ï¼Œè€ŒML-based IDSç”¨äºç¡®å®šAEçš„æ¶æ„ç¨‹åº¦ã€‚æ··åˆæœºåˆ¶åˆ©ç”¨DLæ¨¡å‹çš„é«˜é¢„æµ‹ç²¾åº¦å’ŒDLæ¨¡å‹å’ŒMLæ¨¡å‹ä¹‹é—´çš„ä½æ”»å‡»ä¼ é€’æ€§ï¼Œä»¥æé«˜æ•´ä¸ªç³»ç»Ÿçš„Robustnessã€‚* Results: æœ¬æ–‡å®éªŒç»“æœè¡¨æ˜ï¼Œå½“IDSé¢ä¸´æ”»å‡»æ—¶ï¼Œæè®®çš„IDSæ¶æ„å¯ä»¥è·å¾—é«˜ç²¾åº¦ã€ä½èµ„æºå ç”¨çš„é¢„æµ‹æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Deep learning based intrusion detection systems (DL-based IDS) have emerged as one of the best choices for providing security solutions against various network intrusion attacks. However, due to the emergence and development of adversarial deep learning technologies, it becomes challenging for the adoption of DL models into IDS. In this paper, we propose a novel IDS architecture that can enhance the robustness of IDS against adversarial attacks by combining conventional machine learning (ML) models and Deep Learning models. The proposed DLL-IDS consists of three components: DL-based IDS, adversarial example (AE) detector, and ML-based IDS. We first develop a novel AE detector based on the local intrinsic dimensionality (LID). Then, we exploit the low attack transferability between DL models and ML models to find a robust ML model that can assist us in determining the maliciousness of AEs. If the input traffic is detected as an AE, the ML-based IDS will predict the maliciousness of input traffic, otherwise the DL-based IDS will work for the prediction. The fusion mechanism can leverage the high prediction accuracy of DL models and low attack transferability between DL models and ML models to improve the robustness of the whole system. In our experiments, we observe a significant improvement in the prediction performance of the IDS when subjected to adversarial attack, achieving high accuracy with low resource consumption.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ åŸºäºçš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿï¼ˆDL-IDSï¼‰å·²ç»æˆä¸ºæä¾›å®‰å…¨è§£å†³æ–¹æ¡ˆçš„ä¸€ç§ä¼˜é€‰ï¼Œä½†ç”±äºå¯¹æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ï¼ŒDLæ¨¡å‹åœ¨IDSä¸­çš„é‡‡ç”¨å—åˆ°æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„IDSæ¶æ„ï¼Œå¯ä»¥é€šè¿‡ç»“åˆæ·±åº¦å­¦ä¹ æ¨¡å‹å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹æ¥å¢å¼ºIDSå¯¹å‡æ•°æ®æ”»å‡»çš„Robustnessã€‚æˆ‘ä»¬çš„ææ¡ˆåŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šDL-IDSã€å‡æ•°æ®æ£€æµ‹å™¨å’ŒML-IDSã€‚æˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ç§åŸºäºæœ¬åœ°å†…åœ¨ç»´åº¦ï¼ˆLIDï¼‰çš„å‡æ•°æ®æ£€æµ‹å™¨ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨DLæ¨¡å‹å’ŒMLæ¨¡å‹ä¹‹é—´çš„æ”»å‡»ä¼ é€’ç‡ä½ï¼Œæ‰¾åˆ°ä¸€ä¸ªå¯é çš„MLæ¨¡å‹ï¼Œä»¥ç¡®å®šå‡æ•°æ®çš„Maliciousnessã€‚å¦‚æœè¾“å…¥æµé‡è¢«æ£€æµ‹ä¸ºå‡æ•°æ®ï¼Œåˆ™ML-IDSå°†é¢„æµ‹è¾“å…¥æµé‡çš„Maliciousnessï¼Œå¦åˆ™DL-IDSå°†è¿›è¡Œé¢„æµ‹ã€‚æ··åˆæœºåˆ¶å¯ä»¥åˆ©ç”¨DLæ¨¡å‹çš„é«˜é¢„æµ‹ç²¾åº¦å’ŒMLæ¨¡å‹ä¹‹é—´çš„æ”»å‡»ä¼ é€’ç‡ä½ï¼Œæé«˜æ•´ä½“ç³»ç»Ÿçš„Robustnessã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°å½“è¾“å…¥æµé‡é­å—å‡æ•°æ®æ”»å‡»æ—¶ï¼ŒIDSçš„é¢„æµ‹æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œ Ğ´Ğ¾ÑÑ‚Ğ¸ievingé«˜ç²¾åº¦ä½èµ„æºæ¶ˆè€—ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multicoated-and-Folded-Graph-Neural-Networks-with-Strong-Lottery-Tickets"><a href="#Multicoated-and-Folded-Graph-Neural-Networks-with-Strong-Lottery-Tickets" class="headerlink" title="Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets"></a>Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03236">http://arxiv.org/abs/2312.03236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louivalley/slt-gnn">https://github.com/louivalley/slt-gnn</a></li>
<li>paper_authors: Jiale Yan, Hiroaki Ito, Ãngel LÃ³pez GarcÃ­a-Arias, Yasuyuki Okoshi, Hikari Otsuka, Kazushi Kawamura, Thiem Van Chu, Masato Motomura</li>
<li>For: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨SLTHï¼ˆå¼ºå¤§æŠ½ç­‹å‡è®¾ï¼‰åœ¨æ·±åº¦Graph Neural Networksï¼ˆGNNsï¼‰ä¸­çš„åº”ç”¨ï¼Œä»¥æé«˜ç²¾åº¦å’Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚* Methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å¤šå±‚ææ–™æ©æ¨¡ï¼ˆM-Supï¼‰scalar pruning maskæ–¹æ³•ï¼Œå¹¶æå‡ºäº†é€‚åº”æ€§è°ƒæ•´çš„è®¾å®šç­–ç•¥ï¼Œä»¥å®ç°åœ¨æ·±åº¦GNNsä¸­çš„ç²¾åº¦å’Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚* Results: æœ¬ç ”ç©¶åœ¨Open Graph Benchmarkï¼ˆOGBï¼‰ç­‰å¤šä¸ª datasetä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºäº†SLTH-based GNNså¯ä»¥å®ç°é«˜ç²¾åº¦ã€ç«äº‰æ€§å’Œé«˜å†…å­˜æ•ˆç‡ï¼Œå‡å°‘å†…å­˜æ¶ˆè€—è¾¾98.7%ã€‚<details>
<summary>Abstract</summary>
The Strong Lottery Ticket Hypothesis (SLTH) demonstrates the existence of high-performing subnetworks within a randomly initialized model, discoverable through pruning a convolutional neural network (CNN) without any weight training. A recent study, called Untrained GNNs Tickets (UGT), expanded SLTH from CNNs to shallow graph neural networks (GNNs). However, discrepancies persist when comparing baseline models with learned dense weights. Additionally, there remains an unexplored area in applying SLTH to deeper GNNs, which, despite delivering improved accuracy with additional layers, suffer from excessive memory requirements. To address these challenges, this work utilizes Multicoated Supermasks (M-Sup), a scalar pruning mask method, and implements it in GNNs by proposing a strategy for setting its pruning thresholds adaptively. In the context of deep GNNs, this research uncovers the existence of untrained recurrent networks, which exhibit performance on par with their trained feed-forward counterparts. This paper also introduces the Multi-Stage Folding and Unshared Masks methods to expand the search space in terms of both architecture and parameters. Through the evaluation of various datasets, including the Open Graph Benchmark (OGB), this work establishes a triple-win scenario for SLTH-based GNNs: by achieving high sparsity, competitive performance, and high memory efficiency with up to 98.7\% reduction, it demonstrates suitability for energy-efficient graph processing.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå¼ºå¤§çš„æŠ½å¥–ç¥¨å‡è®¾â€ï¼ˆSLTHï¼‰è¡¨æ˜äº†æ·±åº¦å­¦ä¹ ä¸­çš„é«˜æ€§èƒ½å­ç½‘ç»œï¼Œå¯ä»¥é€šè¿‡éšæœºåˆå§‹åŒ–çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ— éœ€ä»»ä½•è®­ç»ƒæ¥å‘ç°ã€‚ä¸€ recent study called Untrained GNNs Ticketsï¼ˆUGTï¼‰æ‰©å±•äº† SLTH åˆ° shallow graph neural networksï¼ˆGNNsï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æ¯”è¾ƒåŸºeline model ä¸ learned dense weights æ—¶ï¼Œ still æœ‰å·®å¼‚å­˜åœ¨ã€‚æ­¤å¤–ï¼Œ deeper GNNs è¿˜å­˜åœ¨ excessive memory requirements çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¿™ä¸ªå·¥ä½œä½¿ç”¨ MultiCoated SuperMasksï¼ˆM-Supï¼‰ï¼Œä¸€ç§æ•°å€¼é®ç‘•æ³•ï¼Œå¹¶å°†å…¶å®ç°åœ¨ GNNs ä¸­ã€‚åœ¨ deep GNNs çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™ä¸ªç ”ç©¶å‘ç°äº†æœªè®­ç»ƒçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå®ƒä»¬åœ¨ä¸è®­ç»ƒ feed-forward å¯¹åº”çš„è¡¨ç°ç›¸ä¼¼ã€‚è¿™ä¸ª paper è¿˜æå‡ºäº† Multi-Stage Folding å’Œ Unshared Masks æ–¹æ³•ï¼Œä»¥æ‰©å±•æœå¯»ç©ºé—´çš„ both architecture å’Œ parameterã€‚é€šè¿‡è¯„ä¼°å¤šä¸ª datasetï¼ŒåŒ…æ‹¬ Open Graph Benchmarkï¼ˆOGBï¼‰ï¼Œè¿™ä¸ªç ”ç©¶å»ºç«‹äº† SLTH-based GNNs çš„ triple-win scenarioï¼šå®ƒå®ç°äº†é«˜ç®€æ´æ€§ã€ç«äº‰æ€§èƒ½å’Œé«˜å†…å­˜æ•ˆç‡ï¼Œå®ç°äº†èƒ½æºæ•ˆç‡çš„graph processingã€‚
</details></li>
</ul>
<hr>
<h2 id="Deep-Multimodal-Fusion-for-Surgical-Feedback-Classification"><a href="#Deep-Multimodal-Fusion-for-Surgical-Feedback-Classification" class="headerlink" title="Deep Multimodal Fusion for Surgical Feedback Classification"></a>Deep Multimodal Fusion for Surgical Feedback Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03231">http://arxiv.org/abs/2312.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafal Kocielnik, Elyssa Y. Wong, Timothy N. Chu, Lydia Lin, De-An Huang, Jiayun Wang, Anima Anandkumar, Andrew J. Hung</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯ automatize the annotation of real-time contextual surgical feedback at scale.</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº†å¤šç§æ¨¡å¼çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ¥ç±»å‹åŒ»å­¦åé¦ˆï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ¨¡å¼ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…ˆå•ç‹¬è®­ç»ƒæ¯ç§æ¨¡å¼ï¼Œç„¶åå°†å®ƒä»¬ JOINTLY è®­ç»ƒã€‚</li>
<li>results: æˆ‘ä»¬çš„è‡ªåŠ¨åˆ†ç±»æ–¹æ³•å¯ä»¥è¾¾åˆ° AUC å€¼ Between 71.5 and 77.6ï¼Œå¹¶ä¸”å°†äº”ä¸ªç±»åˆ«çš„åŒ»å­¦åé¦ˆåˆ†ç±»ä¸º â€œAnatomicâ€, â€œTechnicalâ€, â€œProceduralâ€, â€œPraiseâ€ å’Œ â€œVisual Aidâ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨é«˜è´¨é‡çš„æ‰‹åŠ¨è¯‘æ–‡å›å¿«å¯ä»¥æé«˜ AUC å€¼è‡³ Between 76.5 and 96.2ã€‚<details>
<summary>Abstract</summary>
Quantification of real-time informal feedback delivered by an experienced surgeon to a trainee during surgery is important for skill improvements in surgical training. Such feedback in the live operating room is inherently multimodal, consisting of verbal conversations (e.g., questions and answers) as well as non-verbal elements (e.g., through visual cues like pointing to anatomic elements). In this work, we leverage a clinically-validated five-category classification of surgical feedback: "Anatomic", "Technical", "Procedural", "Praise" and "Visual Aid". We then develop a multi-label machine learning model to classify these five categories of surgical feedback from inputs of text, audio, and video modalities. The ultimate goal of our work is to help automate the annotation of real-time contextual surgical feedback at scale. Our automated classification of surgical feedback achieves AUCs ranging from 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show that high-quality manual transcriptions of feedback audio from experts improve AUCs to between 76.5 and 96.2, which demonstrates a clear path toward future improvements. Empirically, we find that the Staged training strategy, with first pre-training each modality separately and then training them jointly, is more effective than training different modalities altogether. We also present intuitive findings on the importance of modalities for different feedback categories. This work offers an important first look at the feasibility of automated classification of real-world live surgical feedback based on text, audio, and video modalities.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°åœºæ‰‹æœ¯åŒ»ç”Ÿå¯¹å­¦å‘˜çš„å®æ—¶åé¦ˆæ˜¯é‡è¦çš„ï¼Œä»¥ä¾¿æé«˜æ‰‹æœ¯åŸ¹è®­æŠ€èƒ½ã€‚è¿™ç§åé¦ˆåœ¨å®é™…æ“ä½œå®¤ä¸­æ˜¯å¤šæ¨¡å¼çš„ï¼ŒåŒ…æ‹¬è¯­éŸ³å¯¹è¯ï¼ˆå¦‚é—®é¢˜å’Œç­”æ¡ˆï¼‰ä»¥åŠéè¯­è¨€å…ƒç´ ï¼ˆå¦‚è§†è§‰æŒ‡ç¤ºå™¨ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¥æ ¼éªŒè¯çš„äº”ç±»ç±»åˆ«æ³•å¯¹æ‰‹æœ¯åé¦ˆè¿›è¡Œåˆ†ç±»ï¼šâ€œè§£å‰–å­¦â€ã€â€œæŠ€æœ¯â€ã€â€œè¿‡ç¨‹â€ã€â€œèµèµâ€å’Œâ€œè§†è§‰å¼•å¯¼â€ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šæ ‡ç­¾æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºä»æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ¨¡å¼çš„è¾“å…¥ä¸­åˆ†ç±»è¿™äº›äº”ç±»ç±»åˆ«çš„æ‰‹æœ¯åé¦ˆã€‚æˆ‘ä»¬çš„è‡ªåŠ¨åˆ†ç±»æ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„æ¨¡å¼ä¸‹è¾¾åˆ°AUCå€¼åœ¨71.5%åˆ°77.6%ä¹‹é—´ï¼Œè€Œå°†å¤šä¸ªæ¨¡å¼èåˆå¯ä»¥æé«˜æ€§èƒ½çš„3.1%ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä»ä¸“å®¶æ‰‹åŠ¨æŠ„å†™åé¦ˆéŸ³é¢‘çš„é«˜è´¨é‡æ‰‹åŠ¨è¯‘å½•å¯ä»¥æé«˜AUCå€¼åœ¨76.5%åˆ°96.2%ä¹‹é—´ï¼Œè¿™è¡¨æ˜äº†æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›çš„é“è·¯ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé¢„å…ˆåœ¨æ¯ä¸ªæ¨¡å¼ä¸Šå•ç‹¬é¢„è®­ï¼Œç„¶åå°†å…¶ JOINTLYè®­ç»ƒæ˜¯æ›´æœ‰æ•ˆçš„ï¼Œè€Œä¸”æˆ‘ä»¬è¿˜å‘ç°ä¸åŒçš„åé¦ˆç±»åˆ«å¯¹ä¸åŒçš„æ¨¡å¼å…·æœ‰ä¸åŒçš„é‡è¦æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºè‡ªåŠ¨åŒ–å®æ—¶Contextualæ‰‹æœ¯åé¦ˆçš„åˆ†ç±»æä¾›äº†é‡è¦çš„é¦–æ¬¡ Investigationã€‚
</details></li>
</ul>
<hr>
<h2 id="SDSRA-A-Skill-Driven-Skill-Recombination-Algorithm-for-Efficient-Policy-Learning"><a href="#SDSRA-A-Skill-Driven-Skill-Recombination-Algorithm-for-Efficient-Policy-Learning" class="headerlink" title="SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning"></a>SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03216">http://arxiv.org/abs/2312.03216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ericjiang18/sdsra">https://github.com/ericjiang18/sdsra</a></li>
<li>paper_authors: Eric H. Jiang, Andrew Lizarraga</li>
<li>for: æé«˜å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„æœ€å¤§Entropyæ•ˆç‡</li>
<li>methods: ä½¿ç”¨Skill-Driven Skill Recombination Algorithm (SDSRA)ï¼Œä¸€ç§æ–°å‹çš„åè°ƒæœç´¢æ¡†æ¶ï¼Œå®ç°æ›´é«˜æ•ˆçš„æœ€å¤§Entropyæ•ˆç‡</li>
<li>results: SDSRAæ¯”ä¼ ç»Ÿçš„Soft Actor-Critic (SAC)ç®—æ³•æ›´å¿«åœ° convergesï¼Œå¹¶ç”Ÿæˆäº†æ”¹è¿›çš„ç­–ç•¥ï¼Œåœ¨å¤šç§å¤æ‚å’Œå¤šæ ·çš„ benchmark ä¸­å±•ç°å‡ºäº†remarkableçš„é€‚åº”æ€§å’Œæ€§èƒ½<details>
<summary>Abstract</summary>
In this paper, we introduce a novel algorithm - the Skill-Driven Skill Recombination Algorithm (SDSRA) - an innovative framework that significantly enhances the efficiency of achieving maximum entropy in reinforcement learning tasks. We find that SDSRA achieves faster convergence compared to the traditional Soft Actor-Critic (SAC) algorithm and produces improved policies. By integrating skill-based strategies within the robust Actor-Critic framework, SDSRA demonstrates remarkable adaptability and performance across a wide array of complex and diverse benchmarks.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„ç®—æ³•â€”â€”æŠ€èƒ½é©±åŠ¨æŠ€èƒ½ recombinationç®—æ³•ï¼ˆSDSRAï¼‰â€”â€”ä¸€ç§åˆ›æ–°çš„æ¡†æ¶ï¼Œå¯ä»¥åœ¨å›å½’å­¦ä¹ ä»»åŠ¡ä¸­æé«˜æœ€å¤§Entropyçš„æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°SDSRAæ¯”ä¼ ç»Ÿçš„Soft Actor-Criticï¼ˆSACï¼‰ç®—æ³•æ›´å¿«åœ° convergeså’Œç”Ÿæˆæ›´å¥½çš„ç­–ç•¥ã€‚é€šè¿‡åœ¨Robust Actor-Criticæ¡†æ¶ä¸­ Ğ¸Ğ½Ñ‚ĞµGRATEæŠ€èƒ½basedç­–ç•¥ï¼ŒSDSRAåœ¨å¤šç§å¤æ‚å’Œå¤šæ ·çš„æ ‡å‡†åº•ä¸‹è¡¨ç°å‡ºäº†remarkableçš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/06/cs.AI_2023_12_06/" data-id="clq0ru6py008jto880j9w7z85" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/12/06/cs.CV_2023_12_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-12-06
        
      </div>
    </a>
  
  
    <a href="/2023/12/06/cs.CL_2023_12_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-12-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">140</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">98</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a><span class="archive-list-count">49</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">214</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
