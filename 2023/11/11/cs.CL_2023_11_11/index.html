
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-11-11 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Intentional Biases in LLM Responses paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.07611 repo_url: None paper_authors: Nicklaus Badyal, Derek Jacoby, Yvonne Coady for: 这个研究旨在将对话语言模型中的偏见故意引入，以创造特定的人物形象，用于互动媒体目的。">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-11-11">
<meta property="og:url" content="https://nullscc.github.io/2023/11/11/cs.CL_2023_11_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Intentional Biases in LLM Responses paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.07611 repo_url: None paper_authors: Nicklaus Badyal, Derek Jacoby, Yvonne Coady for: 这个研究旨在将对话语言模型中的偏见故意引入，以创造特定的人物形象，用于互动媒体目的。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-11T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-21T10:03:06.278Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_11_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/11/cs.CL_2023_11_11/" class="article-date">
  <time datetime="2023-11-11T11:00:00.000Z" itemprop="datePublished">2023-11-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-11-11
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Intentional-Biases-in-LLM-Responses"><a href="#Intentional-Biases-in-LLM-Responses" class="headerlink" title="Intentional Biases in LLM Responses"></a>Intentional Biases in LLM Responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.07611">http://arxiv.org/abs/2311.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicklaus Badyal, Derek Jacoby, Yvonne Coady</li>
<li>for: 这个研究旨在将对话语言模型中的偏见故意引入，以创造特定的人物形象，用于互动媒体目的。</li>
<li>methods: 这个研究使用了 Falcon-7b 和 Open AI 的 GPT-4 开源模型，并评估了它们对不同角色的回应。</li>
<li>results: 研究发现，GPT-4 的监管器模型可以确保 AI 的调整，但是它们在创造不同角色的偏见时不够有用。<details>
<summary>Abstract</summary>
In this study we intentionally introduce biases into large language model responses in an attempt to create specific personas for interactive media purposes. We explore the differences between open source models such as Falcon-7b and the GPT-4 model from Open AI, and we quantify some differences in responses afforded by the two systems. We find that the guardrails in the GPT-4 mixture of experts models with a supervisor, while useful in assuring AI alignment in general, are detrimental in trying to construct personas with a variety of uncommon viewpoints. This study aims to set the groundwork for future exploration in intentional biases of large language models such that these practices can be applied in the creative field, and new forms of media.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们故意引入大语言模型的偏见，以创造特定的人物形象，用于互动媒体目的。我们比较了开源模型 falcon-7b 和 open AI 的 GPT-4 模型，并量化了两者响应的一些不同。我们发现，GPT-4 的混合专家模型的监督器，虽有用于保证 AI Compatibility，但在构建多种不同观点的人物时，是不利的。本研究的目的是为未来在大语言模型中意外偏见的实践提供基础，以便在艺术领域和新媒体中应用这些技术。
</details></li>
</ul>
<hr>
<h2 id="A-Template-Is-All-You-Meme"><a href="#A-Template-Is-All-You-Meme" class="headerlink" title="A Template Is All You Meme"></a>A Template Is All You Meme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06649">http://arxiv.org/abs/2311.06649</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ukplab/a-template-is-all-you-meme">https://github.com/ukplab/a-template-is-all-you-meme</a></li>
<li>paper_authors: Luke Bates, Peter Ebert Christensen, Preslav Nakov, Iryna Gurevych</li>
<li>For: The paper aims to improve the understanding of memes and their context, and to develop a method to inject context into machine learning models for better meme classification.* Methods: The authors release a large knowledge base of memes and information from <a target="_blank" rel="noopener" href="http://www.knowyourmeme.com/">www.knowyourmeme.com</a>, and create a non-parametric majority-based classifier called Template-Label Counter (TLC) to test their hypothesis that meme templates can provide missing context for machine learning models.* Results: The authors conduct thorough classification experiments and exploratory data analysis to demonstrate the effectiveness of their method and the value of their knowledge base for meme analysis tasks.<details>
<summary>Abstract</summary>
Memes are a modern form of communication and meme templates possess a base semantics that is customizable by whomever posts it on social media. Machine learning systems struggle with memes, which is likely due to such systems having insufficient context to understand memes, as there is more to memes than the obvious image and text. Here, to aid understanding of memes, we release a knowledge base of memes and information found on www.knowyourmeme.com, which we call the Know Your Meme Knowledge Base (KYMKB), composed of more than 54,000 images. The KYMKB includes popular meme templates, examples of each template, and detailed information about the template. We hypothesize that meme templates can be used to inject models with the context missing from previous approaches. To test our hypothesis, we create a non-parametric majority-based classifier, which we call Template-Label Counter (TLC). We find TLC more effective than or competitive with fine-tuned baselines. To demonstrate the power of meme templates and the value of both our knowledge base and method, we conduct thorough classification experiments and exploratory data analysis in the context of five meme analysis tasks.
</details>
<details>
<summary>摘要</summary>
现代通信的形式之一是memes，它们具有可自定义的基本 semantics，可以在社交媒体上分享。机器学习系统对memes表示困难，可能是因为这些系统缺乏memes的Context，因为memes比图像和文本更多。为了帮助理解memes，我们发布了www.knowyourmeme.com上的知识库，称之为知识库（KYMKB），包含超过54,000个图像。KYMKB包括流行的meme模板，每个模板的示例和详细信息。我们提出的假设是，meme模板可以用来补充过去方法缺失的Context。为了测试这个假设，我们创建了一种非 Parametric多数策略，称之为模板标签计数器（TLC）。我们发现TLC比或与精心调整的基线相当有效。为了证明meme模板和我们的知识库以及方法的力量，我们在五种meme分析任务中进行了严格的分类实验和探索数据分析。
</details></li>
</ul>
<hr>
<h2 id="Robust-Text-Classification-Analyzing-Prototype-Based-Networks"><a href="#Robust-Text-Classification-Analyzing-Prototype-Based-Networks" class="headerlink" title="Robust Text Classification: Analyzing Prototype-Based Networks"></a>Robust Text Classification: Analyzing Prototype-Based Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06647">http://arxiv.org/abs/2311.06647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhivar Sourati, Darshan Deshpande, Filip Ilievski, Kiril Gashteovski, Sascha Saralajew</li>
<li>for: 本研究旨在检验 prototype-based 网络（PBN）在文本分类任务中是否具有鲁棒性特性。</li>
<li>methods: 我们采用了一种模块化和全面的研究框架，包括不同的后处理架构、后处理大小和目标函数。我们的评估协议对模型进行了不同级别的拟合干扰测试。</li>
<li>results: 我们的实验结果表明，PBNs在面对现实的拟合干扰时保持了鲁棒性。此外，PBNs的鲁棒性主要归功于保持概念可读性的目标函数，而与普通模型相比，PBNs在数据越复杂时的鲁棒性差异越加鲜明。<details>
<summary>Abstract</summary>
Downstream applications often require text classification models to be accurate, robust, and interpretable. While the accuracy of the stateof-the-art language models approximates human performance, they are not designed to be interpretable and often exhibit a drop in performance on noisy data. The family of PrototypeBased Networks (PBNs) that classify examples based on their similarity to prototypical examples of a class (prototypes) is natively interpretable and shown to be robust to noise, which enabled its wide usage for computer vision tasks. In this paper, we study whether the robustness properties of PBNs transfer to text classification tasks. We design a modular and comprehensive framework for studying PBNs, which includes different backbone architectures, backbone sizes, and objective functions. Our evaluation protocol assesses the robustness of models against character-, word-, and sentence-level perturbations. Our experiments on three benchmarks show that the robustness of PBNs transfers to NLP classification tasks facing realistic perturbations. Moreover, the robustness of PBNs is supported mostly by the objective function that keeps prototypes interpretable, while the robustness superiority of PBNs over vanilla models becomes more salient as datasets get more complex.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PerceptionGPT-Effectively-Fusing-Visual-Perception-into-LLM"><a href="#PerceptionGPT-Effectively-Fusing-Visual-Perception-into-LLM" class="headerlink" title="PerceptionGPT: Effectively Fusing Visual Perception into LLM"></a>PerceptionGPT: Effectively Fusing Visual Perception into LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06612">http://arxiv.org/abs/2311.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, Tong Zhang</li>
<li>for: 这篇论文旨在做什么？	+ 这篇论文目标是具备Visual Large Language Model（VLLM）Visual perception能力，并且能够高效地使用Large Language Model（LLM）的表示能力来实现这一目标。</li>
<li>methods: 这篇论文使用了什么方法？	+ 这篇论文提出了一种新的综合框架，名为PerceptionGPT，它可以快速和高效地使用VLLM来实现视觉感知任务。该方法通过利用LLM的表示能力，将其token嵌入作为视觉信息的载体，然后使用轻量级的视觉任务编码器和解码器来完成视觉感知任务。</li>
<li>results: 这篇论文的研究结果是什么？	+ 对比之前的方法，这篇论文的方法可以更好地处理多个视觉输出，并且可以减少训练时间和数据量，同时减少批处理时间。这种方法可以帮助未来的研究更好地具备VLLM的视觉感知能力。<details>
<summary>Abstract</summary>
The integration of visual inputs with large language models (LLMs) has led to remarkable advancements in multi-modal capabilities, giving rise to visual large language models (VLLMs). However, effectively harnessing VLLMs for intricate visual perception tasks remains a challenge. In this paper, we present a novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding. Our proposed method treats the token embedding of the LLM as the carrier of spatial information, then leverage lightweight visual task encoders and decoders to perform visual perception tasks (e.g., detection, segmentation). Our approach significantly alleviates the training difficulty suffered by previous approaches that formulate the visual outputs as discrete tokens, and enables achieving superior performance with fewer trainable parameters, less training data and shorted training time. Moreover, as only one token embedding is required to decode the visual outputs, the resulting sequence length during inference is significantly reduced. Consequently, our approach enables accurate and flexible representations, seamless integration of visual perception tasks, and efficient handling of a multiple of visual outputs. We validate the effectiveness and efficiency of our approach through extensive experiments. The results demonstrate significant improvements over previous methods with much fewer trainable parameters and GPU hours, which facilitates future research in enabling LLMs with visual perception abilities.
</details>
<details>
<summary>摘要</summary>
摘要：将视觉输入与大语言模型（LLM）结合，已经导致多模态能力的很大进步，产生了视觉大语言模型（VLLM）。然而，使VLLM进行复杂的视觉感知任务仍然是一大挑战。在这篇论文中，我们提出了一种新的端到端框架，名为PerceptionGPT，可以高效地和有效地让VLLM具备视觉感知能力。我们的提议方法是将LLM的 Token embedding作为空间信息的传递者，然后使用轻量级的视觉任务编码器和解码器来完成视觉感知任务（例如检测和分割）。我们的方法可以减少前一些方法的训练困难，只需要 fewer 的可训练参数和训练数据，同时减少训练时间。此外，只需要一个 Token embedding 来解码视觉输出，因此在推理过程中的序列长度减少了。这使得我们的方法可以实现高精度和灵活的表示，同时实现多个视觉输出的有效集成。我们通过广泛的实验 validate 了我们的方法的有效性和效率。结果表明，我们的方法可以与之前的方法相比，减少很多可训练参数和GPU时间，这为未来启用LLM的视觉感知能力提供了可能性。
</details></li>
</ul>
<hr>
<h2 id="BizBench-A-Quantitative-Reasoning-Benchmark-for-Business-and-Finance"><a href="#BizBench-A-Quantitative-Reasoning-Benchmark-for-Business-and-Finance" class="headerlink" title="BizBench: A Quantitative Reasoning Benchmark for Business and Finance"></a>BizBench: A Quantitative Reasoning Benchmark for Business and Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06602">http://arxiv.org/abs/2311.06602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, Chris Tanner</li>
<li>for: 评估商业和金融领域中模型的数理逻辑能力。</li>
<li>methods: 利用程序生成技术来评估模型对金融数据的问答能力，并 isolate 不同的金融逻辑能力。</li>
<li>results: 通过对开源和商业模型进行评估， illustrate 该 benchmark 对数理逻辑能力的评估是一项挑战性的任务。<details>
<summary>Abstract</summary>
As large language models (LLMs) impact a growing number of complex domains, it is becoming increasingly important to have fair, accurate, and rigorous evaluation benchmarks. Evaluating the reasoning skills required for business and financial NLP stands out as a particularly difficult challenge. We introduce BizBench, a new benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises 8 quantitative reasoning tasks. Notably, BizBench targets the complex task of question-answering (QA) for structured and unstructured financial data via program synthesis (i.e., code generation). We introduce three diverse financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate distinct financial reasoning capabilities required to solve these QA tasks: reading comprehension of financial text and tables, which is required to extract correct intermediate values; and understanding domain knowledge (e.g., financial formulas) needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to extract numeric entities from financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, illustrating that BizBench is a challenging benchmark for quantitative reasoning in the finance and business domain.
</details>
<details>
<summary>摘要</summary>
As large language models (LLMs) impact an increasing number of complex domains, it is becoming increasingly important to have fair, accurate, and rigorous evaluation benchmarks. Evaluating the reasoning skills required for business and financial NLP is a particularly difficult challenge. We introduce BizBench, a new benchmark for evaluating models' ability to reason about realistic financial problems. BizBench consists of 8 quantitative reasoning tasks. Notably, BizBench targets the complex task of question-answering (QA) for structured and unstructured financial data via program synthesis (i.e., code generation). We introduce three diverse financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate distinct financial reasoning capabilities required to solve these QA tasks, including reading comprehension of financial text and tables, which is necessary to extract correct intermediate values, and understanding domain knowledge (e.g., financial formulas) needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to extract numeric entities from financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, illustrating that BizBench is a challenging benchmark for quantitative reasoning in the finance and business domain.
</details></li>
</ul>
<hr>
<h2 id="From-Classification-to-Generation-Insights-into-Crosslingual-Retrieval-Augmented-ICL"><a href="#From-Classification-to-Generation-Insights-into-Crosslingual-Retrieval-Augmented-ICL" class="headerlink" title="From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL"></a>From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06595">http://arxiv.org/abs/2311.06595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoqian Li, Ercong Nie, Sheng Liang</li>
<li>for: 提高低资源语言中大型自然语言模型（LLM）的域内学习（ICL）性能。</li>
<li>methods: 提出了一种新的方法，即跨语言检索增强域内学习（CREA-ICL），通过提取高资源语言中相似的提示，提高多语言预训练语言模型（MPLM）在多种任务上的零基eline性能。</li>
<li>results: 在分类任务中，该方法得到了稳定的提升，但在生成任务中遇到了挑战。我们的评估带来了域内学习在分类和生成领域的性能动态。<details>
<summary>Abstract</summary>
The remarkable ability of Large Language Models (LLMs) to understand and follow instructions has sometimes been limited by their in-context learning (ICL) performance in low-resource languages. To address this, we introduce a novel approach that leverages cross-lingual retrieval-augmented in-context learning (CREA-ICL). By extracting semantically similar prompts from high-resource languages, we aim to improve the zero-shot performance of multilingual pre-trained language models (MPLMs) across diverse tasks. Though our approach yields steady improvements in classification tasks, it faces challenges in generation tasks. Our evaluation offers insights into the performance dynamics of retrieval-augmented in-context learning across both classification and generation domains.
</details>
<details>
<summary>摘要</summary>
LLMs的出色能力理解和遵从指令有时会受到低资源语言的ICL性能的限制。为解决这个问题，我们提出了一种新的方法，即跨语言检索增强ICL（CREA-ICL）。通过从高资源语言提取相似的提示，我们希望提高多语言预训练语言模型（MPLM）的零配置性能。虽然我们的方法在分类任务中得到了稳定的改善，但在生成任务中遇到了挑战。我们的评估对于检索增强ICL在分类和生成领域的性能动态进行了评估。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Cross-Lingual-Sentiment-Classification-under-Distribution-Shift-an-Exploratory-Study"><a href="#Zero-Shot-Cross-Lingual-Sentiment-Classification-under-Distribution-Shift-an-Exploratory-Study" class="headerlink" title="Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study"></a>Zero-Shot Cross-Lingual Sentiment Classification under Distribution Shift: an Exploratory Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06549">http://arxiv.org/abs/2311.06549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maarten De Raedt, Semere Kiros Bitew, Fréderic Godin, Thomas Demeester, Chris Develder</li>
<li>for: This paper is focused on studying the generalization of multi-lingual language models to out-of-distribution (OOD) test data in zero-shot cross-lingual transfer settings, and analyzing the impact of both language and domain shifts on performance.</li>
<li>methods: The paper uses counterfactually augmented data (CAD) to improve OOD generalization in the cross-lingual setting, and proposes two new approaches that avoid the costly annotation process associated with CAD.</li>
<li>results: The paper evaluates the performance of three multilingual models (LaBSE, mBERT, and XLM-R) on OOD test sets in 13 languages, and finds that the proposed cost-effective approaches reach similar or up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.<details>
<summary>Abstract</summary>
The brittleness of finetuned language model performance on out-of-distribution (OOD) test samples in unseen domains has been well-studied for English, yet is unexplored for multi-lingual models. Therefore, we study generalization to OOD test data specifically in zero-shot cross-lingual transfer settings, analyzing performance impacts of both language and domain shifts between train and test data. We further assess the effectiveness of counterfactually augmented data (CAD) in improving OOD generalization for the cross-lingual setting, since CAD has been shown to benefit in a monolingual English setting. Finally, we propose two new approaches for OOD generalization that avoid the costly annotation process associated with CAD, by exploiting the power of recent large language models (LLMs). We experiment with 3 multilingual models, LaBSE, mBERT, and XLM-R trained on English IMDb movie reviews, and evaluate on OOD test sets in 13 languages: Amazon product reviews, Tweets, and Restaurant reviews. Results echo the OOD performance decline observed in the monolingual English setting. Further, (i) counterfactuals from the original high-resource language do improve OOD generalization in the low-resource language, and (ii) our newly proposed cost-effective approaches reach similar or up to +3.1% better accuracy than CAD for Amazon and Restaurant reviews.
</details>
<details>
<summary>摘要</summary>
英文语言模型在不同领域的 OUT-OF-DISTRIBUTION（OOD）测试样本上的 brittleness已经得到了广泛的研究，然而对多语言模型的研究尚未得到了探讨。因此，我们研究了在零shot跨语言传输 Setting中的OOD总结能力，分析了语言和领域之间的数据偏移对测试数据的影响。此外，我们还评估了基于counterfactual augmented data（CAD）的方法在跨语言设置中的有效性，因为CAD在英文设置中已经被证明有助于提高OOD总结能力。最后，我们提出了两种新的OOD总结方法，以避免与CAD相关的昂贵的注释过程，通过利用最新的大语言模型（LLMs）。我们在英语 IMDb 电影评论上训练了3个多语言模型：LaBSE、mBERT和XLM-R，并对13种语言的OOD测试集进行评估：Amazon产品评论、推特和餐厅评论。结果表明，OOD性能减降与英文设置中观察到的类似。此外，（i）原始高资源语言中的counterfactuals实际上提高了低资源语言中的OOD总结能力，和（ii）我们新提出的经济性方法达到了类似或更高于CAD的准确率，为Amazon和餐厅评论达到了+3.1%的提升。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Public-Understanding-of-Court-Opinions-with-Automated-Summarizers"><a href="#Enhancing-Public-Understanding-of-Court-Opinions-with-Automated-Summarizers" class="headerlink" title="Enhancing Public Understanding of Court Opinions with Automated Summarizers"></a>Enhancing Public Understanding of Court Opinions with Automated Summarizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06534">http://arxiv.org/abs/2311.06534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elliott Ash, Aniket Kesari, Suresh Naidu, Lena Song, Dominik Stammbach</li>
<li>for: 帮助非专家理解法律案例</li>
<li>methods: 使用人工智能助手生成简化摘要</li>
<li>results: 调查实验表明，简化摘要可以帮助非专家更好地理解法律案例的关键特征。In English, this translates to:</li>
<li>for: To help non-experts understand legal cases</li>
<li>methods: Using an AI assistant to generate simplified summaries</li>
<li>results: A survey experiment shows that simplified summaries can help non-experts understand the key features of a ruling.<details>
<summary>Abstract</summary>
Written judicial opinions are an important tool for building public trust in court decisions, yet they can be difficult for non-experts to understand. We present a pipeline for using an AI assistant to generate simplified summaries of judicial opinions. These are more accessible to the public and more easily understood by non-experts, We show in a survey experiment that the simplified summaries help respondents understand the key features of a ruling. We discuss how to integrate legal domain knowledge into studies using large language models. Our results suggest a role both for AI assistants to inform the public, and for lawyers to guide the process of generating accessible summaries.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:written judicial opinions are an important tool for building public trust in court decisions, yet they can be difficult for non-experts to understand. we present a pipeline for using an AI assistant to generate simplified summaries of judicial opinions. these are more accessible to the public and more easily understood by non-experts. we show in a survey experiment that the simplified summaries help respondents understand the key features of a ruling. we discuss how to integrate legal domain knowledge into studies using large language models. our results suggest a role both for AI assistants to inform the public, and for lawyers to guide the process of generating accessible summaries.
</details></li>
</ul>
<hr>
<h2 id="Added-Toxicity-Mitigation-at-Inference-Time-for-Multimodal-and-Massively-Multilingual-Translation"><a href="#Added-Toxicity-Mitigation-at-Inference-Time-for-Multimodal-and-Massively-Multilingual-Translation" class="headerlink" title="Added Toxicity Mitigation at Inference Time for Multimodal and Massively Multilingual Translation"></a>Added Toxicity Mitigation at Inference Time for Multimodal and Massively Multilingual Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06532">http://arxiv.org/abs/2311.06532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta R. Costa-jussà, David Dale, Maha Elbayad, Bokai Yu</li>
<li>for: 这 paper 的目的是提出一种新的pipeline来识别添加的毒性并mitigate这个问题，该pipeline在推理时间实现。</li>
<li>methods: 这 paper 使用了一种多modal的毒性检测分类器（speech和text），该分类器可以在大规模语言中工作。mitigation方法直接应用于文本输出中。</li>
<li>results: 这 paper 使用 MinTox pipeline在 SEAMLESSM4T 系统上实现了显著的添加毒性 Mitigation， across domains, modalities和语言方向。 MinTox 能够约Filter出25%-95%的添加毒性（根据模式和领域），保持翻译质量。<details>
<summary>Abstract</summary>
Added toxicity in the context of translation refers to the fact of producing a translation output with more toxicity than there exists in the input. In this paper, we present MinTox which is a novel pipeline to identify added toxicity and mitigate this issue which works at inference time. MinTox uses a toxicity detection classifier which is multimodal (speech and text) and works in languages at scale. The mitigation method is applied to languages at scale and directly in text outputs. MinTox is applied to SEAMLESSM4T, which is the latest multimodal and massively multilingual machine translation system. For this system, MinTox achieves significant added toxicity mitigation across domains, modalities and language directions. MinTox manages to approximately filter out from 25% to 95% of added toxicity (depending on the modality and domain) while keeping translation quality.
</details>
<details>
<summary>摘要</summary>
加入毒性在翻译上指的是生成翻译输出中存在更多的毒性 чем输入。在这篇论文中，我们介绍了一种名为MinTox的新的管道，用于识别加入毒性并缓解这个问题，它在推理时间进行应用。MinTox使用一个多Modal（语音和文本）的毒性检测类ifier，可以在多种语言和模式下进行检测。这种缓解方法直接应用于文本输出中。MinTox在SEAMLESSM4T上进行应用，SEAMLESSM4T是最新的多Modal和大量多语言翻译系统。对这个系统来说，MinTox在域、modal和语言方向上都实现了显著的加入毒性缓解，可以将25%-95%的加入毒性（根据模式和领域）约束出去，而不会影响翻译质量。
</details></li>
</ul>
<hr>
<h2 id="Minimum-Description-Length-Hopfield-Networks"><a href="#Minimum-Description-Length-Hopfield-Networks" class="headerlink" title="Minimum Description Length Hopfield Networks"></a>Minimum Description Length Hopfield Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06518">http://arxiv.org/abs/2311.06518</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matanabudy/mdl-hn">https://github.com/matanabudy/mdl-hn</a></li>
<li>paper_authors: Matan Abudy, Nur Lan, Emmanuel Chemla, Roni Katzir</li>
<li>for: 这个论文是为了研究协同记忆架构的Memorization和Generalization之间的质量。</li>
<li>methods: 这个论文使用Modern Hopfield Networks（MHN）来研究协同记忆架构的Memorization和Generalization。</li>
<li>results: 研究发现，大量的Memorization容量会妨碍Generalization的机会。提出一种使用Minimum Description Length（MDL）来在训练过程中决定保留哪些记忆和哪些记忆数量。<details>
<summary>Abstract</summary>
Associative memory architectures are designed for memorization but also offer, through their retrieval method, a form of generalization to unseen inputs: stored memories can be seen as prototypes from this point of view. Focusing on Modern Hopfield Networks (MHN), we show that a large memorization capacity undermines the generalization opportunity. We offer a solution to better optimize this tradeoff. It relies on Minimum Description Length (MDL) to determine during training which memories to store, as well as how many of them.
</details>
<details>
<summary>摘要</summary>
协同记忆架构是设计来储存信息，但同时也提供了一种通过回溯方法对未见输入进行泛化的机会：储存的记忆可以被看作是类型的范例。专注于现代赫珀维尔网络（MHN），我们表明了大量储存容量会对泛化机会造成干扰。我们提出了一个解决方案，它基于最小描述长度（MDL）来决定在训练过程中哪些记忆要储存，以及哪些记忆要保留多少。
</details></li>
</ul>
<hr>
<h2 id="L3-Ensembles-Lifelong-Learning-Approach-for-Ensemble-of-Foundational-Language-Models"><a href="#L3-Ensembles-Lifelong-Learning-Approach-for-Ensemble-of-Foundational-Language-Models" class="headerlink" title="L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models"></a>L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06493">http://arxiv.org/abs/2311.06493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidin Shiri, Kaushik Roy, Amit Sheth, Manas Gaur</li>
<li>for: 这个论文旨在提出一种基于自然语言处理（NLP）任务的生命长学习（L3）框架，以便高效地进行任务特化和知识传递。</li>
<li>methods: 该方法包括提取有意义的表示，建立结构化知识库，以及在不同任务上进行逐步改进。</li>
<li>results: 经验表明，提出的L3 ensemble方法可以提高模型精度，同时保持或超过当前语言模型（T5）的性能。在STSbenchmark中，L3模型的准确率比原始 Fine-tuned FLM 提高15.4%。<details>
<summary>Abstract</summary>
Fine-tuning pre-trained foundational language models (FLM) for specific tasks is often impractical, especially for resource-constrained devices. This necessitates the development of a Lifelong Learning (L3) framework that continuously adapts to a stream of Natural Language Processing (NLP) tasks efficiently. We propose an approach that focuses on extracting meaningful representations from unseen data, constructing a structured knowledge base, and improving task performance incrementally. We conducted experiments on various NLP tasks to validate its effectiveness, including benchmarks like GLUE and SuperGLUE. We measured good performance across the accuracy, training efficiency, and knowledge transfer metrics. Initial experimental results show that the proposed L3 ensemble method increases the model accuracy by 4% ~ 36% compared to the fine-tuned FLM. Furthermore, L3 model outperforms naive fine-tuning approaches while maintaining competitive or superior performance (up to 15.4% increase in accuracy) compared to the state-of-the-art language model (T5) for the given task, STS benchmark.
</details>
<details>
<summary>摘要</summary>
精度调整预训练基础语言模型（FLM） для特定任务是经常不可能，特别是在有限的设备资源下。这种情况需要开发一个生命时间学习（L3）框架，可以高效地适应流行的自然语言处理（NLP）任务。我们提出了一种方法，强调提取未经见过的数据中有意义的表示，建立结构化的知识库，并在不断更新的任务中提高表现。我们在多个 NLP 任务上进行了实验，以验证其效果，包括 GLUE 和 SuperGLUE 的benchmark。我们发现，在精度、训练效率和知识传递指标方面，L3 ensemble方法表现良好。初步实验结果表明，我们提议的 L3 模型比 fine-tuned FLM 提高4%~36%的模型精度。此外，L3 模型还能在与状态艺术语言模型（T5）相同或更高的精度水平上保持竞争性或超越性（最多提高15.4%的精度），对 STS benchmark进行了证明。
</details></li>
</ul>
<hr>
<h2 id="DocGen-Generating-Detailed-Parameter-Docstrings-in-Python"><a href="#DocGen-Generating-Detailed-Parameter-Docstrings-in-Python" class="headerlink" title="DocGen: Generating Detailed Parameter Docstrings in Python"></a>DocGen: Generating Detailed Parameter Docstrings in Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06453">http://arxiv.org/abs/2311.06453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vatsal Venkatkrishna, Durga Shree Nagabushanam, Emmanuel Iko-Ojo Simon, Melina Vidoni</li>
<li>for: 提高开源软件的有效利用，因为文档债让开发者困惑。</li>
<li>methods: 提出了一种多步骤方法，通过结合多个任务特定的模型，每个模型都专门生成不同的段落，以确保生成的文档准确全面。</li>
<li>results: 与现有的生成模型进行比较，通过自动指标和人 centered评估17名开发者，证明了该方法与现有方法之间的超越。<details>
<summary>Abstract</summary>
Documentation debt hinders the effective utilization of open-source software. Although code summarization tools have been helpful for developers, most would prefer a detailed account of each parameter in a function rather than a high-level summary. However, generating such a summary is too intricate for a single generative model to produce reliably due to the lack of high-quality training data. Thus, we propose a multi-step approach that combines multiple task-specific models, each adept at producing a specific section of a docstring. The combination of these models ensures the inclusion of each section in the final docstring. We compared the results from our approach with existing generative models using both automatic metrics and a human-centred evaluation with 17 participating developers, which proves the superiority of our approach over existing methods.
</details>
<details>
<summary>摘要</summary>
文档债务阻碍开源软件的有效利用。虽然代码概要工具有帮助开发者，但大多数开发者更偏好每个函数参数的详细账户而不是高级概要。然而，生成这样的概要是单一生成模型无法可靠地生成的由于缺乏高质量的训练数据。因此，我们提议一种多步骤方法，将多个任务特定的模型相互结合，以确保每个部分在最终的概要中包含。我们与已有的生成模型进行比较，并通过17名参与者进行人中心评估，证明我们的方法在现有方法之上。
</details></li>
</ul>
<hr>
<h2 id="Separating-the-Wheat-from-the-Chaff-with-BREAD-An-open-source-benchmark-and-metrics-to-detect-redundancy-in-text"><a href="#Separating-the-Wheat-from-the-Chaff-with-BREAD-An-open-source-benchmark-and-metrics-to-detect-redundancy-in-text" class="headerlink" title="Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text"></a>Separating the Wheat from the Chaff with BREAD: An open-source benchmark and metrics to detect redundancy in text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.06440">http://arxiv.org/abs/2311.06440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toizzy/bread">https://github.com/toizzy/bread</a></li>
<li>paper_authors: Isaac Caswell, Lisa Wang, Isabel Papadimitriou</li>
<li>for: 这篇论文的目的是提供一个人类标注的数据集，用于测试语言模型训练数据中的重复文本问题，并评估不同语言中的数据质量。</li>
<li>methods: 该论文使用了人类标注的数据集，创建了一个名为BREAD的数据集，并提供了一些基线分析方法（CRED）来评估数据质量。</li>
<li>results: 该论文通过对BREAD数据集进行分析，发现了一些语言模型训练数据中的重复文本问题，并提供了一些参考实现方法来解决这些问题。<details>
<summary>Abstract</summary>
Data quality is a problem that perpetually resurfaces throughout the field of NLP, regardless of task, domain, or architecture, and remains especially severe for lower-resource languages. A typical and insidious issue, affecting both training data and model output, is data that is repetitive and dominated by linguistically uninteresting boilerplate, such as price catalogs or computer-generated log files. Though this problem permeates many web-scraped corpora, there has yet to be a benchmark to test against, or a systematic study to find simple metrics that generalize across languages and agree with human judgements of data quality. In the present work, we create and release BREAD, a human-labeled benchmark on repetitive boilerplate vs. plausible linguistic content, spanning 360 languages. We release several baseline CRED (Character REDundancy) scores along with it, and evaluate their effectiveness on BREAD. We hope that the community will use this resource to develop better filtering methods, and that our reference implementations of CRED scores can become standard corpus evaluation tools, driving the development of cleaner language modeling corpora, especially in low-resource languages.
</details>
<details>
<summary>摘要</summary>
“资料质量是NLP领域中不断重现的问题，不论任务、领域或架构，它尤其严重 для低资源语言。一个常见的问题是训练数据和模型输出中的重复和 linguistically 无趣的� boilerplate，如价格目录或计算机生成的日志档案。这个问题在许多网页抓取数据中广泛存在，但是还没有一个底线来测试，或一个系统性的研究来找到简单的度量标准，以及与人类判断资料质量的一致性。在现在的工作中，我们创建了BREAD，一个人工标注的底线，涵盖360种语言。我们释出了多个基线CRED（Character REDundancy）分数，并评估它们在BREAD上的效果。我们希望社区可以使用这个资源，发展更好的筛选方法，以提高语言模型数据库的质量，特别是低资源语言。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/11/cs.CL_2023_11_11/" data-id="clp869tui00ewk5885gjn7ec7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/11/cs.AI_2023_11_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-11-11
        
      </div>
    </a>
  
  
    <a href="/2023/11/11/cs.LG_2023_11_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-11-11</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
