
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-11-03 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Quantitative Evaluation of a Multi-Modal Camera Setup for Fusing Event Data with RGB Images paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.01881 repo_url: None paper_authors: Julian Moosmann, Jakub Mandula, Phi">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-11-03">
<meta property="og:url" content="https://nullscc.github.io/2023/11/03/eess.IV_2023_11_03/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Quantitative Evaluation of a Multi-Modal Camera Setup for Fusing Event Data with RGB Images paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.01881 repo_url: None paper_authors: Julian Moosmann, Jakub Mandula, Phi">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-03T09:00:00.000Z">
<meta property="article:modified_time" content="2023-11-06T15:43:59.568Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_11_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/03/eess.IV_2023_11_03/" class="article-date">
  <time datetime="2023-11-03T09:00:00.000Z" itemprop="datePublished">2023-11-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-11-03
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Quantitative-Evaluation-of-a-Multi-Modal-Camera-Setup-for-Fusing-Event-Data-with-RGB-Images"><a href="#Quantitative-Evaluation-of-a-Multi-Modal-Camera-Setup-for-Fusing-Event-Data-with-RGB-Images" class="headerlink" title="Quantitative Evaluation of a Multi-Modal Camera Setup for Fusing Event Data with RGB Images"></a>Quantitative Evaluation of a Multi-Modal Camera Setup for Fusing Event Data with RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01881">http://arxiv.org/abs/2311.01881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Moosmann, Jakub Mandula, Philipp Mayer, Luca Benini, Michele Magno</li>
<li>for: 这个论文的目的是提出一种多模式摄像头设置，用于将高分辨率DVS数据与RGB图像数据进行融合，以便使用两种技术 simultaneously。</li>
<li>methods: 这个论文使用了几种时间基于的同步方法来帮助将DVS数据与RGB图像数据进行对应，并进行了相关的Camera alignment和镜头影响的分析。</li>
<li>results: 实验结果表明，提出的系统具有较低的图像校准误差（less than 0.90px）和像素十分之偏差（1.6px），而使用8毫米 focal length镜头可以检测到距离350米的30厘米大小的 объекts against homogeneous background。<details>
<summary>Abstract</summary>
Event-based cameras, also called silicon retinas, potentially revolutionize computer vision by detecting and reporting significant changes in intensity asynchronous events, offering extended dynamic range, low latency, and low power consumption, enabling a wide range of applications from autonomous driving to longtime surveillance. As an emerging technology, there is a notable scarcity of publicly available datasets for event-based systems that also feature frame-based cameras, in order to exploit the benefits of both technologies. This work quantitatively evaluates a multi-modal camera setup for fusing high-resolution DVS data with RGB image data by static camera alignment. The proposed setup, which is intended for semi-automatic DVS data labeling, combines two recently released Prophesee EVK4 DVS cameras and one global shutter XIMEA MQ022CG-CM RGB camera. After alignment, state-of-the-art object detection or segmentation networks label the image data by mapping boundary boxes or labeled pixels directly to the aligned events. To facilitate this process, various time-based synchronization methods for DVS data are analyzed, and calibration accuracy, camera alignment, and lens impact are evaluated. Experimental results demonstrate the benefits of the proposed system: the best synchronization method yields an image calibration error of less than 0.90px and a pixel cross-correlation deviation of1.6px, while a lens with 8mm focal length enables detection of objects with size 30cm at a distance of 350m against homogeneous background.
</details>
<details>
<summary>摘要</summary>
Event-based 摄像头，也称为silicon retina，有 potential 革命化计算机视觉，因为它可以检测和报告快速变化的强度 asynchronous 事件，提供扩展的动态范围，低延迟，和低功耗，因此可以应用于自动驾驶到长期监测等多种应用。作为新兴技术，公共可用的 dataset  для event-based 系统和 frame-based 摄像头的混合还是罕见的。本研究使用多模式摄像头设置，将高分辨率 DVS 数据与 RGB 图像数据混合，并通过静态摄像头对齐来实现。这种设置是为 semi-automatic DVS 数据标注而设计，使用两个最新发布的 Prophesee EVK4 DVS 摄像头和一个全球闭环 XIMEA MQ022CG-CM RGB 摄像头。在对齐后，使用现状的对象检测或分割网络将图像数据标注为对齐事件。为此，我们分析了多种时间基准的同步方法，并评估了相机对齐精度、镜头影响和摄像头对齐精度。实验结果表明，我们的方法具有优秀的效果：最佳同步方法的图像准确性错误低于0.90px，像素十分之偏移低于1.6px，而8mm focal length 镜头可以检测到30cm大小的 объек的到达350m 距离。
</details></li>
</ul>
<hr>
<h2 id="3-Dimensional-residual-neural-architecture-search-for-ultrasonic-defect-detection"><a href="#3-Dimensional-residual-neural-architecture-search-for-ultrasonic-defect-detection" class="headerlink" title="3-Dimensional residual neural architecture search for ultrasonic defect detection"></a>3-Dimensional residual neural architecture search for ultrasonic defect detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01867">http://arxiv.org/abs/2311.01867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaun McKnight, Christopher MacKinnon, S. Gareth Pierce, Ehsan Mohseni, Vedran Tunukovic, Charles N. MacLeod, Randika K. W. Vithanage, Tom OHare</li>
<li>for: 这种研究使用深度学习方法检测碳纤维复合材料中的缺陷，通过用三维卷积神经网络处理三维超声测试数据。</li>
<li>methods: 这种方法使用了一种新的数据生成方法，通过保留完整的三维数据，使得复杂的预处理步骤减少，神经网络可以利用空间和时间信息，提高模型的性能。</li>
<li>results: 研究 comparing三种体制，包括一种自定义的卷积神经网络，一种使用立方体卷积神经网络，以及一种通过神经网络搜索生成的三维差异神经网络。结果显示，使用全连接层进行维度减少，比使用最大池化层更高的性能。此外，在训练时添加域特性增强方法，也有显著提高模型性能的效果。<details>
<summary>Abstract</summary>
This study presents a deep learning methodology using 3-dimensional (3D) convolutional neural networks to detect defects in carbon fiber reinforced polymer composites through volumetric ultrasonic testing data. Acquiring large amounts of ultrasonic training data experimentally is expensive and time-consuming. To address this issue, a synthetic data generation method was extended to incorporate volumetric data. By preserving the complete volumetric data, complex preprocessing is reduced, and the model can utilize spatial and temporal information that is lost during imaging. This enables the model to utilise important features that might be overlooked otherwise. The performance of three architectures were compared. The first two architectures were hand-designed to address the high aspect ratios between the spatial and temporal dimensions. The first architecture reduced dimensionality in the time domain and used cubed kernels for feature extraction. The second architecture used cuboidal kernels to account for the large aspect ratios. The evaluation included comparing the use of max pooling and convolutional layers for dimensionality reduction, with the fully convolutional layers consistently outperforming the models using max pooling. The third architecture was generated through neural architecture search from a modified 3D Residual Neural Network (ResNet) search space. Additionally, domain-specific augmentation methods were incorporated during training, resulting in significant improvements in model performance for all architectures. The mean accuracy improvements ranged from 8.2% to 22.4%. The best performing models achieved mean accuracies of 91.8%, 92.2%, and 100% for the reduction, constant, and discovered architectures, respectively. Whilst maintaining a model size smaller than most 2-dimensional (2D) ResNets.
</details>
<details>
<summary>摘要</summary>
Three architecture designs were compared: the first two were hand-designed to address high aspect ratios between spatial and temporal dimensions. The first architecture reduced dimensionality in the time domain using cubed kernels for feature extraction, while the second architecture used cuboidal kernels to account for large aspect ratios. The third architecture was generated through neural architecture search from a modified 3D Residual Neural Network (ResNet) search space.During training, domain-specific augmentation methods were incorporated, resulting in significant improvements in model performance for all architectures. The mean accuracy improvements ranged from 8.2% to 22.4%. The best-performing models achieved mean accuracies of 91.8%, 92.2%, and 100% for the reduction, constant, and discovered architectures, respectively, while maintaining a model size smaller than most 2D ResNets.
</details></li>
</ul>
<hr>
<h2 id="Neural-SPDE-solver-for-uncertainty-quantification-in-high-dimensional-space-time-dynamics"><a href="#Neural-SPDE-solver-for-uncertainty-quantification-in-high-dimensional-space-time-dynamics" class="headerlink" title="Neural SPDE solver for uncertainty quantification in high-dimensional space-time dynamics"></a>Neural SPDE solver for uncertainty quantification in high-dimensional space-time dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01783">http://arxiv.org/abs/2311.01783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Beauchamp, Ronan Fablet, Hugo Georgenthum</li>
<li>for: 这篇论文目的是对大型地球物理数据进行插值和资料融合。</li>
<li>methods: 这篇论文使用了Stochastic Partial Differential Equations（SPDE）和 Gaussian Markov Random Fields（GMRF）来处理大数据，并使用了简短精度矩阵来实现插值。</li>
<li>results: 这篇论文的解法提高了Optimal Interpolation（OI）的基eline，并能够quantify the associated uncertainties。它还能够与神经网络结合，实现资料融合和线上参数估测。<details>
<summary>Abstract</summary>
Historically, the interpolation of large geophysical datasets has been tackled using methods like Optimal Interpolation (OI) or model-based data assimilation schemes. However, the recent connection between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) introduced a novel approach to handle large datasets making use of sparse precision matrices in OI. Recent advancements in deep learning also addressed this issue by incorporating data assimilation into neural architectures: it treats the reconstruction task as a joint learning problem involving both prior model and solver as neural networks. Though, it requires further developments to quantify the associated uncertainties. In our work, we leverage SPDEbased Gaussian Processes to estimate complex prior models capable of handling nonstationary covariances in space and time. We develop a specific architecture able to learn both state and SPDE parameters as a neural SPDE solver, while providing the precisionbased analytical form of the SPDE sampling. The latter is used as a surrogate model along the data assimilation window. Because the prior is stochastic, we can easily draw samples from it and condition the members by our neural solver, allowing flexible estimation of the posterior distribution based on large ensemble. We demonstrate this framework on realistic Sea Surface Height datasets. Our solution improves the OI baseline, aligns with neural prior while enabling uncertainty quantification and online parameter estimation.
</details>
<details>
<summary>摘要</summary>
En el pasado, la interpolación de grandes conjuntos de datos geofísicos se ha abordado utilizando métodos como Interpolación Óptima (OI) o esquemas de asimilación de datos basados en modelos. Sin embargo, la reciente conexión entre Ecuaciones Parciales Diferenciales Estocásticas (SPDE) y Campos de Markov Gaussianos (GMRF) presentó una nueva aproximación para manejar grandes conjuntos de datos utilizando matrices de precisión esparcas en OI. Los avances recientes en aprendizaje profundo también abordaron este problema al incorporar la asimilación de datos en arquitecturas neurales: se trata la tarea de reconstrucción como un problema de aprendizaje conjunto que involucra tanto el modelo previo como el solver como redes neuronales. Aunque requiere desarrollos adicionales para cuantificar las incertidumbres asociadas. En nuestro trabajo, utilizamos Procesos de Gaussianas Basadas en SPDE para estimar modelos priorizados complejos capaces de manejar covarianzas no estacionarias en el espacio y el tiempo. Desarrollamos una arquitectura específica que aprende tanto los parámetros del estado como los parámetros de SPDE como un solucionador neural SPDE, mientras proporciona la forma analítica de la SPDE sampling. La última se utiliza como un modelo de surrogato a lo largo de la ventana de asimilación de datos. Como el prior es estocástico, podemos fácilmente extraer muestras de él y condicionarlos con nuestro solucionador neural, lo que permite una estimación flexible de la distribución posterior en función de un gran ensamble. Demostramos este marco en conjuntos de datos de Altura de la Surface del Mar realistas. Nuestra solución mejora el umbral de OI, se alinea con el prior neural y permite la cuantificación de incertidumbres y la estimación en línea de parámetros.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/03/eess.IV_2023_11_03/" data-id="clopawo2h018hag888nqbcgoz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/03/cs.LG_2023_11_03/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-11-03
        
      </div>
    </a>
  
  
    <a href="/2023/11/03/eess.SP_2023_11_03/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.SP - 2023-11-03</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
