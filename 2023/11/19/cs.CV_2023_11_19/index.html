
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-19 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Improved Defect Detection and Classification Method for Advanced IC Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11439 repo_url: None pape">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-19">
<meta property="og:url" content="https://nullscc.github.io/2023/11/19/cs.CV_2023_11_19/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Improved Defect Detection and Classification Method for Advanced IC Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11439 repo_url: None pape">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-19T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-21T10:06:15.478Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.CV_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T13:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-19
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improved-Defect-Detection-and-Classification-Method-for-Advanced-IC-Nodes-by-Using-Slicing-Aided-Hyper-Inference-with-Refinement-Strategy"><a href="#Improved-Defect-Detection-and-Classification-Method-for-Advanced-IC-Nodes-by-Using-Slicing-Aided-Hyper-Inference-with-Refinement-Strategy" class="headerlink" title="Improved Defect Detection and Classification Method for Advanced IC Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy"></a>Improved Defect Detection and Classification Method for Advanced IC Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11439">http://arxiv.org/abs/2311.11439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vic De Ridder, Bappaditya Dey, Victor Blanco, Sandip Halder, Bartel Van Waeyenberge</li>
<li>for: 提高高 numerical aperture EUVL 领域中小 defect 的检测精度</li>
<li>methods: 使用 Slicing Aided Hyper Inference (SAHI) 框架，对大小增加的 SEM 图像进行推理，提高检测小 defect 的效果</li>
<li>results: 在 previously investigated 半导体数据集上，SAHI 方法可以提高小 defect 的检测精度约 2x，并在新的测试数据集上实现了缺陷free 检测率，而未经训练的模型失败了。此外，提出了一种不会 significatively 降低 true positive 预测的 SAHI 扩展。<details>
<summary>Abstract</summary>
In semiconductor manufacturing, lithography has often been the manufacturing step defining the smallest possible pattern dimensions. In recent years, progress has been made towards high-NA (Numerical Aperture) EUVL (Extreme-Ultraviolet-Lithography) paradigm, which promises to advance pattern shrinking (2 nm node and beyond). However, a significant increase in stochastic defects and the complexity of defect detection becomes more pronounced with high-NA. Present defect inspection techniques (both non-machine learning and machine learning based), fail to achieve satisfactory performance at high-NA dimensions. In this work, we investigate the use of the Slicing Aided Hyper Inference (SAHI) framework for improving upon current techniques. Using SAHI, inference is performed on size-increased slices of the SEM images. This leads to the object detector's receptive field being more effective in capturing small defect instances. First, the performance on previously investigated semiconductor datasets is benchmarked across various configurations, and the SAHI approach is demonstrated to substantially enhance the detection of small defects, by approx. 2x. Afterwards, we also demonstrated application of SAHI leads to flawless detection rates on a new test dataset, with scenarios not encountered during training, whereas previous trained models failed. Finally, we formulate an extension of SAHI that does not significantly reduce true-positive predictions while eliminating false-positive predictions.
</details>
<details>
<summary>摘要</summary>
在半导体生产中，镭射曾经是制造过程中定义最小的模式维度的关键步骤。在最近的年头，对高高 numerical aperture（NA）极紫外光刻（EUVL）的进步，使得模式缩小（2nm节点和更进）得到了提高。然而，高NA使得精度偏差和检测复杂性显著增加。现有的检测技术（包括机器学习和非机器学习基于的）在高NA环境下并未达到满意的性能。在这种情况下，我们研究使用扩展的扩展Hyper Inference（SAHI）框架，以改进当前技术。使用SAHI，检测器的受器领域被更好地捕捉到小异常实例。首先，我们对已经调查过的半导体数据集进行了多种配置的比较，并证明SAHI方法可以大幅提高小异常检测率，约2倍。然后，我们还证明SAHI可以在新的测试数据集上达到精准检测率，包括训练过程中未遇到的场景。最后，我们提出了SAHI的扩展，可以不会重要地降低真正的正确预测，而是完全消除假正确预测。
</details></li>
</ul>
<hr>
<h2 id="DiffSCI-Zero-Shot-Snapshot-Compressive-Imaging-via-Iterative-Spectral-Diffusion-Model"><a href="#DiffSCI-Zero-Shot-Snapshot-Compressive-Imaging-via-Iterative-Spectral-Diffusion-Model" class="headerlink" title="DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral Diffusion Model"></a>DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11417">http://arxiv.org/abs/2311.11417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghao Pan, Haijin Zeng, Jiezhang Cao, Kai Zhang, Yongyong Chen<br>for:DiffSCI aims to enhance the precision of snapshot compressive imaging (SCI) reconstruction for multispectral images (MSIs).methods:The proposed method integrates advantages from established SCI techniques, image generative models, and deep prior knowledge. It utilizes a pre-trained diffusion model as a generative denoiser, accounts for spectral band correlations, and introduces an accelerated algorithm to expedite the reconstruction process.results:DiffSCI exhibits improved performance over prevailing self-supervised and zero-shot approaches, even surpassing supervised transformer counterparts, as demonstrated through extensive testing on simulated and real datasets. The code will be available.<details>
<summary>Abstract</summary>
This paper endeavors to advance the precision of snapshot compressive imaging (SCI) reconstruction for multispectral image (MSI). To achieve this, we integrate the advantageous attributes of established SCI techniques and an image generative model, propose a novel structured zero-shot diffusion model, dubbed DiffSCI. DiffSCI leverages the structural insights from the deep prior and optimization-based methodologies, complemented by the generative capabilities offered by the contemporary denoising diffusion model. Specifically, firstly, we employ a pre-trained diffusion model, which has been trained on a substantial corpus of RGB images, as the generative denoiser within the Plug-and-Play framework for the first time. This integration allows for the successful completion of SCI reconstruction, especially in the case that current methods struggle to address effectively. Secondly, we systematically account for spectral band correlations and introduce a robust methodology to mitigate wavelength mismatch, thus enabling seamless adaptation of the RGB diffusion model to MSIs. Thirdly, an accelerated algorithm is implemented to expedite the resolution of the data subproblem. This augmentation not only accelerates the convergence rate but also elevates the quality of the reconstruction process. We present extensive testing to show that DiffSCI exhibits discernible performance enhancements over prevailing self-supervised and zero-shot approaches, surpassing even supervised transformer counterparts across both simulated and real datasets. Our code will be available.
</details>
<details>
<summary>摘要</summary>
Firstly, we use a pre-trained diffusion model, which has been trained on a large corpus of RGB images, as the generative denoiser within the Plug-and-Play framework for the first time. This integration enables successful SCI reconstruction, especially in cases where current methods struggle.Secondly, we systematically account for spectral band correlations and introduce a robust methodology to mitigate wavelength mismatch, allowing for seamless adaptation of the RGB diffusion model to MSIs.Thirdly, we implement an accelerated algorithm to expedite the resolution of the data subproblem. This not only accelerates the convergence rate but also improves the quality of the reconstruction process.We present extensive testing to show that DiffSCI exhibits significant performance enhancements over prevailing self-supervised and zero-shot approaches, surpassing even supervised transformer counterparts across both simulated and real datasets. Our code will be available.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Emerging-Applications-of-Diffusion-Probabilistic-Models-in-MRI"><a href="#A-Survey-of-Emerging-Applications-of-Diffusion-Probabilistic-Models-in-MRI" class="headerlink" title="A Survey of Emerging Applications of Diffusion Probabilistic Models in MRI"></a>A Survey of Emerging Applications of Diffusion Probabilistic Models in MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11383">http://arxiv.org/abs/2311.11383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuheng Fan, Hanxi Liao, Shiqi Huang, Yimin Luo, Huazhu Fu, Haikun Qi</li>
<li>for: 本文是一篇审查文章，旨在帮助MRI社区的研究人员了解Diffusion probabilistic models（DPMs）在不同应用中的进步。</li>
<li>methods: 本文 introduce two dominant kinds of DPMs，即分别是离散时间步骤和连续时间步骤的DPMs，并提供了MRI领域emerging DPMs的全面回顾，包括重建、图像生成、图像翻译、 segmentation、异常检测以及进一步研究话题。</li>
<li>results: 本文结果表明，DPMs在MRI应用中具有高质量和多样性的生成能力，但也存在一些总体和特定于MRI任务的限制。<details>
<summary>Abstract</summary>
Diffusion probabilistic models (DPMs) which employ explicit likelihood characterization and a gradual sampling process to synthesize data, have gained increasing research interest. Despite their huge computational burdens due to the large number of steps involved during sampling, DPMs are widely appreciated in various medical imaging tasks for their high-quality and diversity of generation. Magnetic resonance imaging (MRI) is an important medical imaging modality with excellent soft tissue contrast and superb spatial resolution, which possesses unique opportunities for diffusion models. Although there is a recent surge of studies exploring DPMs in MRI, a survey paper of DPMs specifically designed for MRI applications is still lacking. This review article aims to help researchers in the MRI community to grasp the advances of DPMs in different applications. We first introduce the theory of two dominant kinds of DPMs, categorized according to whether the diffusion time step is discrete or continuous, and then provide a comprehensive review of emerging DPMs in MRI, including reconstruction, image generation, image translation, segmentation, anomaly detection, and further research topics. Finally, we discuss the general limitations as well as limitations specific to the MRI tasks of DPMs and point out potential areas that are worth further exploration.
</details>
<details>
<summary>摘要</summary>
diffusion probabilistic models (DPMs) 分布式概率模型，通过显式概率特征化和渐进采样过程，可以生成数据，在医学影像领域得到了广泛的研究兴趣。尽管DPMs在采样过程中需要承受巨大的计算负担，由于生成的质量和多样性而广泛应用于各种医学影像任务。核磁共振成像（MRI）是医学影像modalities中的一种重要方法，具有优秀的软组织强度对比和出色的空间分辨率，这种特点为Diffusion模型提供了独特的机遇。虽然有一些最近的研究探讨了DPMs在MRI应用中的可能性，但是一篇专门关于MRI应用的DPMs的评论文章仍然缺失。这篇文章的目的是帮助MRI社区的研究者了解DPMs在不同应用中的进步，我们首先介绍了DPMs的两种主要类型，分别是逐步分布式概率模型和连续分布式概率模型，然后提供了MRI应用中emerging DPMs的全面评论，包括重建、图像生成、图像翻译、分割、异常检测以及进一步的研究方向。最后，我们讨论了DPMs在MRI任务中的总限制以及特定任务的限制，并指出了可能的进一步探索领域。
</details></li>
</ul>
<hr>
<h2 id="Evidential-Uncertainty-Quantification-A-Variance-Based-Perspective"><a href="#Evidential-Uncertainty-Quantification-A-Variance-Based-Perspective" class="headerlink" title="Evidential Uncertainty Quantification: A Variance-Based Perspective"></a>Evidential Uncertainty Quantification: A Variance-Based Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11367">http://arxiv.org/abs/2311.11367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kerrydrx/evidentialada">https://github.com/kerrydrx/evidentialada</a></li>
<li>paper_authors: Ruxiao Duan, Brian Caffo, Harrison X. Bai, Haris I. Sair, Craig Jones</li>
<li>for: 这个研究是用于 Deep Neural Networks 的 uncertainty quantification，并且专注于 classification  tasks 中的 aleatoric 和 epistemic  uncertainty。</li>
<li>methods: 这个研究使用了 evidential deep learning 的方法，通过单一的 forward pass 来直接量化 uncertainty。它还使用了 variance-based 方法，将其应用到 classification 中，以量化分类 uncertainty。</li>
<li>results: 实验结果显示，variance-based 方法不仅与 entropy-based 方法相似度在 active domain adaptation 中，同时还提供了关于 class-wise uncertainties 和 between-class correlations 的信息。<details>
<summary>Abstract</summary>
Uncertainty quantification of deep neural networks has become an active field of research and plays a crucial role in various downstream tasks such as active learning. Recent advances in evidential deep learning shed light on the direct quantification of aleatoric and epistemic uncertainties with a single forward pass of the model. Most traditional approaches adopt an entropy-based method to derive evidential uncertainty in classification, quantifying uncertainty at the sample level. However, the variance-based method that has been widely applied in regression problems is seldom used in the classification setting. In this work, we adapt the variance-based approach from regression to classification, quantifying classification uncertainty at the class level. The variance decomposition technique in regression is extended to class covariance decomposition in classification based on the law of total covariance, and the class correlation is also derived from the covariance. Experiments on cross-domain datasets are conducted to illustrate that the variance-based approach not only results in similar accuracy as the entropy-based one in active domain adaptation but also brings information about class-wise uncertainties as well as between-class correlations. The code is available at https://github.com/KerryDRX/EvidentialADA. This alternative means of evidential uncertainty quantification will give researchers more options when class uncertainties and correlations are important in their applications.
</details>
<details>
<summary>摘要</summary>
深度神经网络的不确定性评估已成为研究的活跃领域，对下游任务如活动学习具有关键作用。 latest advances in evidential deep learning 推动了直接量化 aleatoric 和 epistemic 不确定性的方法，通过单个模型前进 pass 直接量化不确定性。 traditional methods 通常采用 entropy-based 方法来 derive evidential uncertainty in classification, 量化不确定性在样本水平。 however, the variance-based method that has been widely applied in regression problems is seldom used in the classification setting. in this work, we adapt the variance-based approach from regression to classification, quantifying classification uncertainty at the class level. the variance decomposition technique in regression is extended to class covariance decomposition in classification based on the law of total covariance, and the class correlation is also derived from the covariance. experiments on cross-domain datasets are conducted to illustrate that the variance-based approach not only results in similar accuracy as the entropy-based one in active domain adaptation but also brings information about class-wise uncertainties as well as between-class correlations. the code is available at https://github.com/KerryDRX/EvidentialADA. this alternative means of evidential uncertainty quantification will give researchers more options when class uncertainties and correlations are important in their applications.Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Scale-aware-competition-network-for-palmprint-recognition"><a href="#Scale-aware-competition-network-for-palmprint-recognition" class="headerlink" title="Scale-aware competition network for palmprint recognition"></a>Scale-aware competition network for palmprint recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11354">http://arxiv.org/abs/2311.11354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengrui Gao, Ziyuan Yang, Min Zhu, Andrew Beng Jin Teo</li>
<li>For: This paper aims to improve the accuracy of palmprint biometrics by addressing the limitations of existing methods that prioritize texture orientation but neglect scale.* Methods: The proposed method, called SAC-Net, consists of two modules: Inner-Scale Competition Module (ISCM) and Across-Scale Competition Module (ASCM). ISCM integrates learnable Gabor filters and a self-attention mechanism to extract rich orientation data, while ASCM leverages a competitive strategy across various scales to capture texture scale elements.* Results: The proposed method outperforms state-of-the-art alternatives in recognizing palmprints, as demonstrated by rigorous experiments on three benchmark datasets.<details>
<summary>Abstract</summary>
Palmprint biometrics garner heightened attention in palm-scanning payment and social security due to their distinctive attributes. However, prevailing methodologies singularly prioritize texture orientation, neglecting the significant texture scale dimension. We design an innovative network for concurrently extracting intra-scale and inter-scale features to redress this limitation. This paper proposes a scale-aware competitive network (SAC-Net), which includes the Inner-Scale Competition Module (ISCM) and the Across-Scale Competition Module (ASCM) to capture texture characteristics related to orientation and scale. ISCM efficiently integrates learnable Gabor filters and a self-attention mechanism to extract rich orientation data and discern textures with long-range discriminative properties. Subsequently, ASCM leverages a competitive strategy across various scales to effectively encapsulate the competitive texture scale elements. By synergizing ISCM and ASCM, our method adeptly characterizes palmprint features. Rigorous experimentation across three benchmark datasets unequivocally demonstrates our proposed approach's exceptional recognition performance and resilience relative to state-of-the-art alternatives.
</details>
<details>
<summary>摘要</summary>
手印生物特征在手印支付和社会保障中受到了更多的关注，这是因为手印有着独特的特征。然而，现有的方法ologies仅强调 текстур方向，忽略了重要的纹理尺度维度。我们设计了一种创新的网络，可以同时提取内纹理和间纹理特征。这篇论文提出了一种具有自适应竞争能力的纹理网络（SAC-Net），包括内纹理竞争模块（ISCM）和跨纹理竞争模块（ASCM），以捕捉手印特征。ISCM使用可学习的加博滤波器和自我注意机制，以提取丰富的方向数据和描述距离特征。然后，ASCM利用不同级别的竞争策略，有效地包装着竞争纹理级别的元素。通过融合ISCM和ASCMS，我们的方法能够有效地描述手印特征。严格的实验证明了我们提出的方法在三个 benchmark 数据集上的突出表现和鲜明性，相比之前的状态态 alternative。
</details></li>
</ul>
<hr>
<h2 id="MoVideo-Motion-Aware-Video-Generation-with-Diffusion-Models"><a href="#MoVideo-Motion-Aware-Video-Generation-with-Diffusion-Models" class="headerlink" title="MoVideo: Motion-Aware Video Generation with Diffusion Models"></a>MoVideo: Motion-Aware Video Generation with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11325">http://arxiv.org/abs/2311.11325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc Van Gool, Rakesh Ranjan</li>
<li>for: 这篇论文旨在提出一种基于运动的视频生成模型（MoVideo），以解决现有的视频生成模型忽略了运动的问题。</li>
<li>methods: 该模型从两个方面考虑运动：视频深度和光流。视频深度规则运动，而光流描述了帧之间的相对关系，以保持细节和改善时间一致性。</li>
<li>results: 在实验中，MoVideo模型在文本生成和图像生成等两个任务上均达到了最佳效果，显示了更好的提示一致性、帧一致性和视觉质量。<details>
<summary>Abstract</summary>
While recent years have witnessed great progress on using diffusion models for video generation, most of them are simple extensions of image generation frameworks, which fail to explicitly consider one of the key differences between videos and images, i.e., motion. In this paper, we propose a novel motion-aware video generation (MoVideo) framework that takes motion into consideration from two aspects: video depth and optical flow. The former regulates motion by per-frame object distances and spatial layouts, while the later describes motion by cross-frame correspondences that help in preserving fine details and improving temporal consistency. More specifically, given a key frame that exists or generated from text prompts, we first design a diffusion model with spatio-temporal modules to generate the video depth and the corresponding optical flows. Then, the video is generated in the latent space by another spatio-temporal diffusion model under the guidance of depth, optical flow-based warped latent video and the calculated occlusion mask. Lastly, we use optical flows again to align and refine different frames for better video decoding from the latent space to the pixel space. In experiments, MoVideo achieves state-of-the-art results in both text-to-video and image-to-video generation, showing promising prompt consistency, frame consistency and visual quality.
</details>
<details>
<summary>摘要</summary>
Recent years have seen significant progress in using diffusion models for video generation, but most of these models are simply extensions of image generation frameworks, neglecting a key difference between videos and images: motion. In this paper, we propose a novel motion-aware video generation (MoVideo) framework that takes motion into account from two aspects: video depth and optical flow. The former regulates motion by per-frame object distances and spatial layouts, while the latter describes motion by cross-frame correspondences that help preserve fine details and improve temporal consistency. Specifically, given a key frame that exists or is generated from text prompts, we first design a diffusion model with spatio-temporal modules to generate the video depth and the corresponding optical flows. Then, the video is generated in the latent space by another spatio-temporal diffusion model under the guidance of depth, optical flow-based warped latent video, and the calculated occlusion mask. Finally, we use optical flows again to align and refine different frames for better video decoding from the latent space to the pixel space. In experiments, MoVideo achieves state-of-the-art results in both text-to-video and image-to-video generation, demonstrating excellent prompt consistency, frame consistency, and visual quality.
</details></li>
</ul>
<hr>
<h2 id="Discrete-approximations-of-Gaussian-smoothing-and-Gaussian-derivatives"><a href="#Discrete-approximations-of-Gaussian-smoothing-and-Gaussian-derivatives" class="headerlink" title="Discrete approximations of Gaussian smoothing and Gaussian derivatives"></a>Discrete approximations of Gaussian smoothing and Gaussian derivatives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11317">http://arxiv.org/abs/2311.11317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tony Lindeberg</li>
<li>for: 这篇论文关注于粗化与梯度计算的粗化空间理论中的推广和数值方法。</li>
<li>methods: 论文考虑了三种主要的粗化方法，包括（i）抽象 Gaussian 函数和其 derivative 函数，（ii）在每个像素支持区域内进行当地积分 Gaussian 函数和其 derivative 函数，以及（iii）基于粗化空间理论的粗化数据分析。</li>
<li>results: 论文通过理论和实验研究这三种粗化方法的性能，并通过量值度量指标，包括对于不同应用场景的缩选任务进行评估，并强调在细节粗化下的表现。结果显示，使用抽象 Gaussian 函数和其 derivative 函数，以及在每个像素支持区域内进行当地积分 Gaussian 函数和其 derivative 函数，在细节粗化下表现很差。对比之下，使用粗化空间理论中的粗化数据分析，在细节粗化下表现很好。<details>
<summary>Abstract</summary>
This paper develops an in-depth treatment concerning the problem of approximating the Gaussian smoothing and Gaussian derivative computations in scale-space theory for application on discrete data. With close connections to previous axiomatic treatments of continuous and discrete scale-space theory, we consider three main ways discretizing these scale-space operations in terms of explicit discrete convolutions, based on either (i) sampling the Gaussian kernels and the Gaussian derivative kernels, (ii) locally integrating the Gaussian kernels and the Gaussian derivative kernels over each pixel support region and (iii) basing the scale-space analysis on the discrete analogue of the Gaussian kernel, and then computing derivative approximations by applying small-support central difference operators to the spatially smoothed image data.   We study the properties of these three main discretization methods both theoretically and experimentally, and characterize their performance by quantitative measures, including the results they give rise to with respect to the task of scale selection, investigated for four different use cases, and with emphasis on the behaviour at fine scales. The results show that the sampled Gaussian kernels and derivatives as well as the integrated Gaussian kernels and derivatives perform very poorly at very fine scales. At very fine scales, the discrete analogue of the Gaussian kernel with its corresponding discrete derivative approximations performs substantially better. The sampled Gaussian kernel and the sampled Gaussian derivatives do, on the other hand, lead to numerically very good approximations of the corresponding continuous results, when the scale parameter is sufficiently large, in the experiments presented in the paper, when the scale parameter is greater than a value of about 1, in units of the grid spacing.
</details>
<details>
<summary>摘要</summary>
这篇论文对抽象尺度空间理论中的高斯滤波和高斯导数计算的精度近似问题进行了深入的处理。与前期AXIOmatic准则的连续和精度数据之间的联系非常密切，我们考虑了三种主要的精度抽象方法，包括（i）采样高斯核函数和高斯导数核函数，（ii）在每个像素支持区域内进行局部积分高斯核函数和高斯导数核函数，以及（iii）基于精度数据的空间平滑化后进行高斯核函数的精度抽象，然后通过小支持中心差分算法计算导数近似。我们在理论和实验两个方面研究了这三种精度抽象方法的性能，并通过量化度量（包括在不同应用场景中的缩放选择问题上的性能）来评估其表现。结果显示，采样高斯核函数和导数核函数以及局部积分高斯核函数和导数核函数在非常细 scales perform very poorly。然而，基于精度数据的空间平滑化后的高斯核函数和其导数近似在非常细 scalesperform substantially better。采样高斯核函数和采样导数核函数则在大 scales（在实验中的grid spacing unit上大于1）numerically very good approximations of the corresponding continuous results。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-rgb-d-semantic-segmentation-through-multi-modal-interaction-and-pooling-attention"><a href="#Optimizing-rgb-d-semantic-segmentation-through-multi-modal-interaction-and-pooling-attention" class="headerlink" title="Optimizing rgb-d semantic segmentation through multi-modal interaction and pooling attention"></a>Optimizing rgb-d semantic segmentation through multi-modal interaction and pooling attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11312">http://arxiv.org/abs/2311.11312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Zhang, Minghong Xie</li>
<li>for: 提高RGB-D图像 semantic segmentation的精度</li>
<li>methods: 利用多模态交互和 Pooling Attention Module 提高网络的表达能力</li>
<li>results: 在NYUDv2和SUN-RGBD两个indoor scene数据集上，MIPANet比现有方法表现出较高的Semantic segmentation性能<details>
<summary>Abstract</summary>
Semantic segmentation of RGB-D images involves understanding the appearance and spatial relationships of objects within a scene, which requires careful consideration of various factors. However, in indoor environments, the simple input of RGB and depth images often results in a relatively limited acquisition of semantic and spatial information, leading to suboptimal segmentation outcomes. To address this, we propose the Multi-modal Interaction and Pooling Attention Network (MIPANet), a novel approach designed to harness the interactive synergy between RGB and depth modalities, optimizing the utilization of complementary information. Specifically, we incorporate a Multi-modal Interaction Fusion Module (MIM) into the deepest layers of the network. This module is engineered to facilitate the fusion of RGB and depth information, allowing for mutual enhancement and correction. Additionally, we introduce a Pooling Attention Module (PAM) at various stages of the encoder. This module serves to amplify the features extracted by the network and integrates the module's output into the decoder in a targeted manner, significantly improving semantic segmentation performance. Our experimental results demonstrate that MIPANet outperforms existing methods on two indoor scene datasets, NYUDv2 and SUN-RGBD, underscoring its effectiveness in enhancing RGB-D semantic segmentation.
</details>
<details>
<summary>摘要</summary>
semantic segmentation of RGB-D images 需要理解图像中物体的出现和空间关系，这需要考虑各种因素。然而，在室内环境中，简单地输入 RGB 和深度图像经常导致 semantic 和空间信息的有限获取，从而导致 segmentation 结果不佳。为解决这问题，我们提出了 Multi-modal Interaction and Pooling Attention Network (MIPANet)，一种新的方法，旨在利用 RGB 和深度模式之间的交互同时性，最大化 complementary 信息的利用。具体来说，我们在网络的最深层添加了 Multi-modal Interaction Fusion Module (MIM)。这个模块是用于融合 RGB 和深度信息，以便互补和修正。此外，我们在编码器中引入了 Pooling Attention Module (PAM)，用于增强网络提取的特征，并将其输出集成到解码器中，以提高 semantic segmentation 性能。我们的实验结果表明，MIPANet 在 NYUDv2 和 SUN-RGBD 两个室内场景数据集上比现有方法更高效，证明了它在 RGB-D semantic segmentation 中的效iveness。
</details></li>
</ul>
<hr>
<h2 id="UMAAF-Unveiling-Aesthetics-via-Multifarious-Attributes-of-Images"><a href="#UMAAF-Unveiling-Aesthetics-via-Multifarious-Attributes-of-Images" class="headerlink" title="UMAAF: Unveiling Aesthetics via Multifarious Attributes of Images"></a>UMAAF: Unveiling Aesthetics via Multifarious Attributes of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11306">http://arxiv.org/abs/2311.11306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Li, Yitian Wan, Xingjiao Wu, Junjie Xu, Liang He</li>
<li>for: 这篇论文主要是为了提出一种基于多属性的图像美学评估方法（UMAAF），以更好地利用图像特征来评估图像的美学价值。</li>
<li>methods: 这篇论文使用了多个维度的维度特征提取模块和一个综合属性互动网络，以捕捉图像的绝对属性和相对属性。另外，这篇论文还使用了一种相对关系损失函数来提高模型的Robustness。</li>
<li>results: 根据实验结果，这种基于UMAAF的方法在TAD66K和AVA数据集上达到了现状之最的性能，并且多个实验证明了模型与人类偏好的一致性。<details>
<summary>Abstract</summary>
With the increasing prevalence of smartphones and websites, Image Aesthetic Assessment (IAA) has become increasingly crucial. While the significance of attributes in IAA is widely recognized, many attribute-based methods lack consideration for the selection and utilization of aesthetic attributes. Our initial step involves the acquisition of aesthetic attributes from both intra- and inter-perspectives. Within the intra-perspective, we extract the direct visual attributes of images, constituting the absolute attribute. In the inter-perspective, our focus lies in modeling the relative score relationships between images within the same sequence, forming the relative attribute. Then, to better utilize image attributes in aesthetic assessment, we propose the Unified Multi-attribute Aesthetic Assessment Framework (UMAAF) to model both absolute and relative attributes of images. For absolute attributes, we leverage multiple absolute-attribute perception modules and an absolute-attribute interacting network. The absolute-attribute perception modules are first pre-trained on several absolute-attribute learning tasks and then used to extract corresponding absolute attribute features. The absolute-attribute interacting network adaptively learns the weight of diverse absolute-attribute features, effectively integrating them with generic aesthetic features from various absolute-attribute perspectives and generating the aesthetic prediction. To model the relative attribute of images, we consider the relative ranking and relative distance relationships between images in a Relative-Relation Loss function, which boosts the robustness of the UMAAF. Furthermore, UMAAF achieves state-of-the-art performance on TAD66K and AVA datasets, and multiple experiments demonstrate the effectiveness of each module and the model's alignment with human preference.
</details>
<details>
<summary>摘要</summary>
随着智能手机和网站的普及，图像美学评价（IAA）日益重要。虽然美学属性在IAA中的重要性广泛被认可，但许多基于属性的方法忽视了属性的选择和使用。我们的初步步骤是从内部和外部两个角度获取美学属性。在内部角度，我们提取图像直接视觉属性，组成绝对属性。在外部角度，我们关注图像序列内相互关系的建模，形成相对属性。然后，为了更好地利用图像属性在美学评价中，我们提议一种统一多属性美学评价框架（UMAAF），以模型图像绝对和相对属性。对绝对属性来说，我们利用多个绝对属性感知模块和绝对属性互动网络。绝对属性感知模块首先在多个绝对属性学习任务上预训练，然后用来提取对应的绝对属性特征。绝对属性互动网络可以适应性地学习绝对属性特征的权重，有效地将多种绝对属性视角的普遍美学特征与通用美学特征结合在一起，生成美学预测。为了模型图像相对属性，我们考虑图像之间的相对排名和相对距离关系，通过相对关系损失函数，提高UMAAF的Robustness。此外，UMAAF在TAD66K和AVA数据集上实现了状态机器人的表现，并通过多个实验证明每个模块和模型与人类偏好的吻合。
</details></li>
</ul>
<hr>
<h2 id="Exchanging-Dual-Encoder-Decoder-A-New-Strategy-for-Change-Detection-with-Semantic-Guidance-and-Spatial-Localization"><a href="#Exchanging-Dual-Encoder-Decoder-A-New-Strategy-for-Change-Detection-with-Semantic-Guidance-and-Spatial-Localization" class="headerlink" title="Exchanging Dual Encoder-Decoder: A New Strategy for Change Detection with Semantic Guidance and Spatial Localization"></a>Exchanging Dual Encoder-Decoder: A New Strategy for Change Detection with Semantic Guidance and Spatial Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11302">http://arxiv.org/abs/2311.11302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijie Zhao, Xueliang Zhang, Pengfeng Xiao, Guangjun He</li>
<li>for: 本研究旨在提高地球观测中的变化检测性能，并采用深度学习方法来解决现有的问题。</li>
<li>methods: 我们提出了一种新的交换双Encoder-Decoder结构，以实现Binary变化检测 WITH semantic guidance和空间局部化。</li>
<li>results: 我们的模型在六个 dataset 上进行了比较，并与 18 个现有方法进行了比较。结果表明，我们的模型在三种场景中都达到了最高的性能，并且高效地解决了多视图建筑物变化检测和内类变化检测等问题。<details>
<summary>Abstract</summary>
Change detection is a critical task in earth observation applications. Recently, deep learning-based methods have shown promising performance and are quickly adopted in change detection. However, the widely used multiple encoder and single decoder (MESD) as well as dual encoder-decoder (DED) architectures still struggle to effectively handle change detection well. The former has problems of bitemporal feature interference in the feature-level fusion, while the latter is inapplicable to intraclass change detection and multiview building change detection. To solve these problems, we propose a new strategy with an exchanging dual encoder-decoder structure for binary change detection with semantic guidance and spatial localization. The proposed strategy solves the problems of bitemporal feature inference in MESD by fusing bitemporal features in the decision level and the inapplicability in DED by determining changed areas using bitemporal semantic features. We build a binary change detection model based on this strategy, and then validate and compare it with 18 state-of-the-art change detection methods on six datasets in three scenarios, including intraclass change detection datasets (CDD, SYSU), single-view building change detection datasets (WHU, LEVIR-CD, LEVIR-CD+) and a multiview building change detection dataset (NJDS). The experimental results demonstrate that our model achieves superior performance with high efficiency and outperforms all benchmark methods with F1-scores of 97.77%, 83.07%, 94.86%, 92.33%, 91.39%, 74.35% on CDD, SYSU, WHU, LEVIR-CD, LEVIR- CD+, and NJDS datasets, respectively. The code of this work will be available at https://github.com/NJU-LHRS/official-SGSLN.
</details>
<details>
<summary>摘要</summary>
<<SYS>> Change detection 是 Earth observation 应用中的一个关键任务。最近，深度学习基于的方法已经表现出了扎根的表现，快速被采纳。然而，广泛使用的多Encoder 和单Decoder（MESD）以及 dual Encoder-Decoder （DED）架构仍然不具备有效地处理变化检测的能力。前者受到帧内特征干扰的问题，而后者不适用于同类变化检测和多视图建筑变化检测。为解决这些问题，我们提出了一种新的策略，即交换 dual Encoder-Decoder 结构，用于二进制变化检测。这种策略可以解决 MESD 中的帧内特征干扰问题，并且可以在 DED 中确定变化区域使用帧内 semantic 特征。我们基于这种策略建立了一个二进制变化检测模型，然后验证和比较它与 18 种现有变化检测方法在六个数据集上的性能。结果显示，我们的模型在高效性和精度方面具有优秀的表现，并且在所有参考方法中具有最高的 F1 分数，分别为 97.77%, 83.07%, 94.86%, 92.33%, 91.39%, 74.35%。代码将在 GitHub 上公开，链接为 <https://github.com/NJU-LHRS/official-SGSLN>。
</details></li>
</ul>
<hr>
<h2 id="Pair-wise-Layer-Attention-with-Spatial-Masking-for-Video-Prediction"><a href="#Pair-wise-Layer-Attention-with-Spatial-Masking-for-Video-Prediction" class="headerlink" title="Pair-wise Layer Attention with Spatial Masking for Video Prediction"></a>Pair-wise Layer Attention with Spatial Masking for Video Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11289">http://arxiv.org/abs/2311.11289</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlvccn/pla_sm_videopred">https://github.com/mlvccn/pla_sm_videopred</a></li>
<li>paper_authors: Ping Li, Chenhan Zhang, Zheng Yang, Xianghua Xu, Mingli Song</li>
<li>for: 预测视频帧，提高预测质量。</li>
<li>methods: 使用Pair-wise Layer Attention（PLA）模块强调层次 semanticdependency，并使用Spatial Masking（SM）模块增强空间特征。</li>
<li>results: 在五个 benchmark 上进行了广泛的实验和严格的ablation 研究，并得到了提高预测质量的结果。<details>
<summary>Abstract</summary>
Video prediction yields future frames by employing the historical frames and has exhibited its great potential in many applications, e.g., meteorological prediction, and autonomous driving. Previous works often decode the ultimate high-level semantic features to future frames without texture details, which deteriorates the prediction quality. Motivated by this, we develop a Pair-wise Layer Attention (PLA) module to enhance the layer-wise semantic dependency of the feature maps derived from the U-shape structure in Translator, by coupling low-level visual cues and high-level features. Hence, the texture details of predicted frames are enriched. Moreover, most existing methods capture the spatiotemporal dynamics by Translator, but fail to sufficiently utilize the spatial features of Encoder. This inspires us to design a Spatial Masking (SM) module to mask partial encoding features during pretraining, which adds the visibility of remaining feature pixels by Decoder. To this end, we present a Pair-wise Layer Attention with Spatial Masking (PLA-SM) framework for video prediction to capture the spatiotemporal dynamics, which reflect the motion trend. Extensive experiments and rigorous ablation studies on five benchmarks demonstrate the advantages of the proposed approach. The code is available at GitHub.
</details>
<details>
<summary>摘要</summary>
<<SYS>>视频预测可以生成未来帧，通过利用历史帧来实现这一点，并在多个应用程序中表现出了很大的潜力，例如气象预测和自动驾驶。过去的工作通常是将高级Semantic特征解码到未来帧中，而不是保留Texture细节，这会下降预测质量。我们被这一点所 inspirited，开发了一个Pair-wise层Attention（PLA）模块，用于增强Translator的层次Semantic依赖关系，并将低级Visualcue和高级特征相互关联。因此，预测帧的Texture细节得到了增强。此外，大多数现有方法在Translator中捕捉了Space-Time动力学，但是忽略了Encoder的空间特征。这使我们开发了一个Spatial Masking（SM）模块，用于在预训练时对部分编码特征进行遮盖，以便Decoder中的视觉特征添加可见性。因此，我们提出了一个Pair-wise层Attention with Spatial Masking（PLA-SM）框架，用于视频预测，以捕捉Space-Time动力学，并反映运动趋势。经过广泛的实验和严格的ablation研究，我们在五个benchmark上证明了我们的方法的优势。代码可以在GitHub上找到。
</details></li>
</ul>
<hr>
<h2 id="LucidDreamer-Towards-High-Fidelity-Text-to-3D-Generation-via-Interval-Score-Matching"><a href="#LucidDreamer-Towards-High-Fidelity-Text-to-3D-Generation-via-Interval-Score-Matching" class="headerlink" title="LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching"></a>LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11284">http://arxiv.org/abs/2311.11284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/envision-research/luciddreamer">https://github.com/envision-research/luciddreamer</a></li>
<li>paper_authors: Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen</li>
<li>for: 提高文本到3D生成模型的质量和效率</li>
<li>methods: 提出了一种新的方法Interval Score Matching（ISM），通过幂等扩散轨迹和间隔分别匹配来缓解过滤效应，同时还 integrates 3D Gaussian Splatting into the text-to-3D generation pipeline</li>
<li>results: 实验表明，该模型在质量和训练效率上有显著提高 compared to 状态前的模型<details>
<summary>Abstract</summary>
The recent advancements in text-to-3D generation mark a significant milestone in generative models, unlocking new possibilities for creating imaginative 3D assets across various real-world scenarios. While recent advancements in text-to-3D generation have shown promise, they often fall short in rendering detailed and high-quality 3D models. This problem is especially prevalent as many methods base themselves on Score Distillation Sampling (SDS). This paper identifies a notable deficiency in SDS, that it brings inconsistent and low-quality updating direction for the 3D model, causing the over-smoothing effect. To address this, we propose a novel approach called Interval Score Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes interval-based score matching to counteract over-smoothing. Furthermore, we incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline. Extensive experiments show that our model largely outperforms the state-of-the-art in quality and training efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Transcending-Forgery-Specificity-with-Latent-Space-Augmentation-for-Generalizable-Deepfake-Detection"><a href="#Transcending-Forgery-Specificity-with-Latent-Space-Augmentation-for-Generalizable-Deepfake-Detection" class="headerlink" title="Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection"></a>Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11278">http://arxiv.org/abs/2311.11278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyuan Yan, Yuhao Luo, Siwei Lyu, Qingshan Liu, Baoyuan Wu</li>
<li>for: 提高深 fake检测器的普适性，即使在数据集的分布不符合时仍能保持良好的检测性能。</li>
<li>methods: 提出了一种简单 yet effective的检测器，即Latent Space Data Augmentation（LSDA），基于一个启发式想法：具有更多类型的深 fake可以学习更普适的决策边界，从而减少方法特有的特征过拟合。通过在幂空间内构建和模拟假造特征之间和 across forgery features的变化，扩大了假造空间，从而获得了丰富的领域特定特征和减少了域间 gap。</li>
<li>results: 经过了广泛的实验，发现提出的方法 surprisingly effective，超越了多个常用的基准检测器在多个广泛使用的 benchmark 上。<details>
<summary>Abstract</summary>
Deepfake detection faces a critical generalization hurdle, with performance deteriorating when there is a mismatch between the distributions of training and testing data. A broadly received explanation is the tendency of these detectors to be overfitted to forgery-specific artifacts, rather than learning features that are widely applicable across various forgeries. To address this issue, we propose a simple yet effective detector called LSDA (\underline{L}atent \underline{S}pace \underline{D}ata \underline{A}ugmentation), which is based on a heuristic idea: representations with a wider variety of forgeries should be able to learn a more generalizable decision boundary, thereby mitigating the overfitting of method-specific features (see Figure. 1). Following this idea, we propose to enlarge the forgery space by constructing and simulating variations within and across forgery features in the latent space. This approach encompasses the acquisition of enriched, domain-specific features and the facilitation of smoother transitions between different forgery types, effectively bridging domain gaps. Our approach culminates in refining a binary classifier that leverages the distilled knowledge from the enhanced features, striving for a generalizable deepfake detector. Comprehensive experiments show that our proposed method is surprisingly effective and transcends state-of-the-art detectors across several widely used benchmarks.
</details>
<details>
<summary>摘要</summary>
深层迷伪检测面临一个重要的通用化难题，其性能在训练和测试数据的分布不匹配时受到影响。一个广泛接受的解释是这些检测器过拟合假造特有的特征，而不是学习通用于不同假造的特征。为解决这个问题，我们提出了一种简单 yet 有效的检测器 called LSDA（Latent Space Data Augmentation），基于一个启发性的想法：具有更多种假造的表示应该能够学习更通用的决策边界，从而减轻方法特有的特征过拟合（参见 Figure. 1）。按照这个想法，我们提议扩大假造空间，通过在和 across forgery 特征上构建和模拟变化来增强表示。这种方法涵盖了获得更加丰富的领域特定特征和促进不同假造类型之间的缓抗过渡，实际上跨域连接。我们的方法最终是通过提高了特征的整合来优化一个二分类器，以便实现一个通用的深层迷伪检测器。我们的实验结果显示，我们的提议方法 surprisingly 有效，并超越了多个广泛使用的标准检测器在多个常用的 benchmark 上。
</details></li>
</ul>
<hr>
<h2 id="Generalization-and-Hallucination-of-Large-Vision-Language-Models-through-a-Camouflaged-Lens"><a href="#Generalization-and-Hallucination-of-Large-Vision-Language-Models-through-a-Camouflaged-Lens" class="headerlink" title="Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens"></a>Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11273">http://arxiv.org/abs/2311.11273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lv Tang, Peng-Tao Jiang, Zhihao Shen, Hao Zhang, Jinwei Chen, Bo Li</li>
<li>for: 这个论文旨在探讨大视语言模型（LVLM）是否可以在无需训练的情况下泛化到难以识别的隐身物体检测（COD）场景中。</li>
<li>methods: 我们提出了一种新的框架，即隐身视语言框架（CPVLF），以探索LVLM是否可以在无需训练的情况下泛化到COD场景中。在泛化过程中，我们发现LVLM因为内部的幻觉问题而可能会错误地感知隐身场景中的 объек，生成对实际情况不符的概念。此外，由于LVLM没有专门为精准地位置隐身 объек 进行训练，因此它在准确地位置隐身 objet 方面存在一定的不确定性。因此，我们提出了一条链条视觉感知，以提高LVLM在隐身场景中的视觉感知，从语言和视觉两个角度增强LVLM对隐身场景的感知，降低幻觉问题，提高LVLM在准确地位置隐身 objet 方面的能力。</li>
<li>results: 我们 validate了CPVLF的效果在三个通用的COD数据集上，实验结果表明LVLM在COD任务中具有潜在的能力。<details>
<summary>Abstract</summary>
Large Vision-Language Model (LVLM) has seen burgeoning development and increasing attention recently. In this paper, we propose a novel framework, camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can generalize to the challenging camouflaged object detection (COD) scenario in a training-free manner. During the process of generalization, we find that due to hallucination issues within LVLM, it can erroneously perceive objects in camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not specifically trained for the precise localization of camouflaged objects, it exhibits a degree of uncertainty in accurately pinpointing these objects. Therefore, we propose chain of visual perception, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects. We validate the effectiveness of CPVLF on three widely used COD datasets, and the experiments show the potential of LVLM in the COD task.
</details>
<details>
<summary>摘要</summary>
大型视言语模型（LVLM）在最近几年得到了广泛的发展和关注。在这篇论文中，我们提出了一种新的框架——隐形视言语框架（CPVLF），以探索LVLM是否可以在无需训练的情况下扩展到具有挑战性的隐形物 detection（COD）场景。在扩展过程中，我们发现LVLM因为幻觉问题而在隐形场景中错误地感知物体，生成了假设性的概念。此外，由于LVLM没有特地对隐形物体的精确定位进行训练，它在找到隐形物体时表现出一定的不确定性。因此，我们提出了视觉链，以增强LVLM在隐形场景中的视觉和语言两个方面的感知，从而降低幻觉问题和提高隐形物体的精确定位能力。我们验证了CPVLF在三个常用的COD数据集上的效果，实验结果表明LVLM在COD任务中具有潜在的能力。
</details></li>
</ul>
<hr>
<h2 id="Radarize-Large-Scale-Radar-SLAM-for-Indoor-Environments"><a href="#Radarize-Large-Scale-Radar-SLAM-for-Indoor-Environments" class="headerlink" title="Radarize: Large-Scale Radar SLAM for Indoor Environments"></a>Radarize: Large-Scale Radar SLAM for Indoor Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11260">http://arxiv.org/abs/2311.11260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emerson Sie, Xinyu Wu, Heyu Guo, Deepak Vasisht</li>
<li>for: 这篇论文是为了提出一种基于单 chip MMwave 雷达的低成本 SLAM 管道，用于室内环境。</li>
<li>methods: 该方法使用雷达特有的电磁现象，如多普勒移动效应，来提高性能。</li>
<li>results: 在一个大规模的数据集上评估了该方法，与现有的雷达基本方法比较，显示该方法在雷达偏移和端到端 SLAM 方面比之前的方法高出约5倍和8倍。<details>
<summary>Abstract</summary>
We present Radarize, a self-contained SLAM pipeline for indoor environments that uses only a low-cost commodity single-chip mmWave radar. Our radar-native approach leverages phenomena unique to radio frequencies, such as doppler shift-based odometry, to improve performance. We evaluate our method on a large-scale dataset of 146 trajectories spanning 4 campus buildings, totaling approximately 4680m of travel distance. Our results show that our method outperforms state-of-the-art radar-based approaches by approximately 5x in terms of odometry and 8x in terms of end-to-end SLAM, as measured by absolute trajectory error (ATE), without the need additional sensors such as IMUs or wheel odometry.
</details>
<details>
<summary>摘要</summary>
我们介绍Radarize，一个低成本干radio雷达单芯片SLAM管道，适用于室内环境。我们的雷达本地方法利用雷达频率特有的现象，如Doppler偏移估计，以提高性能。我们对一个大规模的数据集，包括4座大学建筑，总计约4680米的旅程距离进行评估。我们的结果表明，我们的方法比现有的雷达基于方法高出约5倍的偏移和8倍的综合SLAM（绝对轨迹误差），而无需其他感知器如IMU或轮胎速度测量器。
</details></li>
</ul>
<hr>
<h2 id="Submeter-level-Land-Cover-Mapping-of-Japan"><a href="#Submeter-level-Land-Cover-Mapping-of-Japan" class="headerlink" title="Submeter-level Land Cover Mapping of Japan"></a>Submeter-level Land Cover Mapping of Japan</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11252">http://arxiv.org/abs/2311.11252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoto Yokoya, Junshi Xia, Clifford Broni-Bediako</li>
<li>for: 这 paper 的目的是提出一种低成本的 submeter-level 地形分类方法，以提高国家规模的地形分类地图的自动更新。</li>
<li>methods: 该方法基于 OpenEarthMap 数据集，使用 U-Net 模型进行人工循环学习，通过少量的额外标注数据来改进模型的准确率。</li>
<li>results: 该方法可以在日本全国范围内实现高精度的地形分类地图，并且比传统的方法提高了约 16% 的准确率。<details>
<summary>Abstract</summary>
Deep learning has shown promising performance in submeter-level mapping tasks; however, the annotation cost of submeter-level imagery remains a challenge, especially when applied on a large scale. In this paper, we present the first submeter-level land cover mapping of Japan with eight classes, at a relatively low annotation cost. We introduce a human-in-the-loop deep learning framework leveraging OpenEarthMap, a recently introduced benchmark dataset for global submeter-level land cover mapping, with a U-Net model that achieves national-scale mapping with a small amount of additional labeled data. By adding a small amount of labeled data of areas or regions where a U-Net model trained on OpenEarthMap clearly failed and retraining the model, an overall accuracy of 80\% was achieved, which is a nearly 16 percentage point improvement after retraining. Using aerial imagery provided by the Geospatial Information Authority of Japan, we create land cover classification maps of eight classes for the entire country of Japan. Our framework, with its low annotation cost and high-accuracy mapping results, demonstrates the potential to contribute to the automatic updating of national-scale land cover mapping using submeter-level optical remote sensing data. The mapping results will be made publicly available.
</details>
<details>
<summary>摘要</summary>
深度学习在 submeter 级地形映射任务中表现出了扎实的成绩;然而，对 submeter 级遥感数据的标注成本仍然是一大挑战，特别是在大规模应用中。在这篇论文中，我们首次在日本全国范围内实现了八类 submeter 级地形分类，并且在相对较低的标注成本下完成了。我们提出了一种人工智能在Loop深度学习框架，利用 OpenEarthMap 数据集，一个最近引入的全球 submeter 级地形分类 benchmark，并使用 U-Net 模型实现了全国范围内的地形分类。通过对 OpenEarthMap 模型在特定地区或地方添加小量标注数据重新训练，实现了总准确率为 80%，相比之前训练后提高了约 16%。使用日本地图情报管理局提供的飞行图像，我们生成了日本全国的八类地形分类地图。我们的框架，具有低标注成本和高准确率地形分类结果，表明可以在使用 submeter 级遥感数据自动更新国家级地形分类地图。地图结果将公开发布。
</details></li>
</ul>
<hr>
<h2 id="AutoStory-Generating-Diverse-Storytelling-Images-with-Minimal-Human-Effort"><a href="#AutoStory-Generating-Diverse-Storytelling-Images-with-Minimal-Human-Effort" class="headerlink" title="AutoStory: Generating Diverse Storytelling Images with Minimal Human Effort"></a>AutoStory: Generating Diverse Storytelling Images with Minimal Human Effort</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11243">http://arxiv.org/abs/2311.11243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, Chunhua Shen</li>
<li>for:  Story Visualization</li>
<li>methods: 使用大语言模型进行布局规划，然后利用大规模文本到图像模型生成复杂的故事图像，并使用稀疏控制条件（如矩形框）进行布局规划，以及使用密集控制条件（如素描和关键点）进行图像生成，以提高图像质量并允许用户进行INTuitive interactions。</li>
<li>results: 提出了一种自动化的故事视觉系统，可以生成多样化、高质量和一致的故事图像，并且具有最小化人工干预的特点。此外，还提出了一种简单 yet effective的多视角一致人物图像生成方法，解决了人工劳动集成或绘制人物图像的问题。<details>
<summary>Abstract</summary>
Story visualization aims to generate a series of images that match the story described in texts, and it requires the generated images to satisfy high quality, alignment with the text description, and consistency in character identities. Given the complexity of story visualization, existing methods drastically simplify the problem by considering only a few specific characters and scenarios, or requiring the users to provide per-image control conditions such as sketches. However, these simplifications render these methods incompetent for real applications. To this end, we propose an automated story visualization system that can effectively generate diverse, high-quality, and consistent sets of story images, with minimal human interactions. Specifically, we utilize the comprehension and planning capabilities of large language models for layout planning, and then leverage large-scale text-to-image models to generate sophisticated story images based on the layout. We empirically find that sparse control conditions, such as bounding boxes, are suitable for layout planning, while dense control conditions, e.g., sketches and keypoints, are suitable for generating high-quality image content. To obtain the best of both worlds, we devise a dense condition generation module to transform simple bounding box layouts into sketch or keypoint control conditions for final image generation, which not only improves the image quality but also allows easy and intuitive user interactions. In addition, we propose a simple yet effective method to generate multi-view consistent character images, eliminating the reliance on human labor to collect or draw character images.
</details>
<details>
<summary>摘要</summary>
（简化中文）story visualization 是生成文本中的故事场景，需要生成图像的质量高，对文本描述的对齐，人物形象的一致性。由于故事视觉化的复杂性，现有的方法大多是忽视特定的人物和场景，或者需要用户提供每个图像的控制条件，如绘制箱体。但这些简化方法在实际应用中无法满足需求。为此，我们提出一个自动化的故事视觉化系统，可以生成多样化、高质量、一致的故事图像，最小化用户的互动。我们利用大语言模型的理解和规划能力来进行布局规划，然后利用大规模的文本到图像模型来生成基于布局的故事图像。我们发现，简单的控制条件，如矩形盒体，适合布局规划，而密集的控制条件，如绘制和关键点，适合生成高质量的图像内容。为了取得两者的优点，我们设计了一个密集控制条件生成模块，将简单的矩形盒体转换为绘制或关键点控制条件，从而提高图像质量并使用户交互更加简单。此外，我们提出了一种简单 yet effective的方法来生成多视图一致的人物图像，从而消除人工劳动来收集或绘制人物图像。
</details></li>
</ul>
<hr>
<h2 id="Open-Vocabulary-Camouflaged-Object-Segmentation"><a href="#Open-Vocabulary-Camouflaged-Object-Segmentation" class="headerlink" title="Open-Vocabulary Camouflaged Object Segmentation"></a>Open-Vocabulary Camouflaged Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11241">http://arxiv.org/abs/2311.11241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youwei Pang, Xiaoqi Zhao, Jiaming Zuo, Lihe Zhang, Huchuan Lu</li>
<li>for: 这篇论文主要针对开放词汇涂抹物体识别 зада务进行研究，以提高现有的开放词汇涂抹物体 segmentation 技术。</li>
<li>methods: 该论文提出了一种基于 CLIP 的单阶段开放词汇涂抹物体 segmentation 方法，通过 iterative semantic guidance 和 visual structure cues 来提高捕捉隐藏的物体。</li>
<li>results: 该方法在 authors 提出的 OVCamo 数据集上达到了 state-of-the-art 水平，并且在开放词汇涂抹物体 semantic image segmentation 任务上也表现出了优异。<details>
<summary>Abstract</summary>
Recently, the emergence of the large-scale vision-language model (VLM), such as CLIP, has opened the way towards open-world object perception. Many works has explored the utilization of pre-trained VLM for the challenging open-vocabulary dense prediction task that requires perceive diverse objects with novel classes at inference time. Existing methods construct experiments based on the public datasets of related tasks, which are not tailored for open vocabulary and rarely involves imperceptible objects camouflaged in complex scenes due to data collection bias and annotation costs. To fill in the gaps, we introduce a new task, open-vocabulary camouflaged object segmentation (OVCOS) and construct a large-scale complex scene dataset (\textbf{OVCamo}) which containing 11,483 hand-selected images with fine annotations and corresponding object classes. Further, we build a strong single-stage open-vocabulary \underline{c}amouflaged \underline{o}bject \underline{s}egmentation transform\underline{er} baseline \textbf{OVCoser} attached to the parameter-fixed CLIP with iterative semantic guidance and structure enhancement. By integrating the guidance of class semantic knowledge and the supplement of visual structure cues from the edge and depth information, the proposed method can efficiently capture camouflaged objects. Moreover, this effective framework also surpasses previous state-of-the-arts of open-vocabulary semantic image segmentation by a large margin on our OVCamo dataset. With the proposed dataset and baseline, we hope that this new task with more practical value can further expand the research on open-vocabulary dense prediction tasks.
</details>
<details>
<summary>摘要</summary>
近些时间，大规模视力语言模型（VLM）的出现，如CLIP，已经开创了开放世界物体识别的新途径。许多研究已经利用预训练VLM来进行开放词汇稠密预测任务，需要在推理时识别多种新类的物体。现有方法基于相关任务的公共数据集进行实验，这些数据集通常不适用于开放词汇和罕见的隐形物体，因为数据收集偏好和注释成本的限制。为了填补这些缺陷，我们引入了一个新任务：开放词汇隐形物体分割（OVCOS），并构建了一个大规模的复杂场景数据集（OVCamo），包含11483个手动选择的图像，以及相应的物体类别。此外，我们建立了一个强大的单Stage开放词汇隐形物体分割变换器（OVCoser），将CLIP的参数固定，并采用迭代性启发和结构增强来提高效果。通过结合类别 semantic 知识和视觉结构准则，我们的方法可以高效捕捉隐形物体。此外，我们的有效框架还在我们的OVCamo数据集上超过了先前的开放词汇semantic图像分割状态之差。我们希望通过我们提出的任务和数据集，可以进一步推动开放词汇稠密预测任务的研究。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Radiology-Diagnosis-through-Convolutional-Neural-Networks-for-Computer-Vision-in-Healthcare"><a href="#Enhancing-Radiology-Diagnosis-through-Convolutional-Neural-Networks-for-Computer-Vision-in-Healthcare" class="headerlink" title="Enhancing Radiology Diagnosis through Convolutional Neural Networks for Computer Vision in Healthcare"></a>Enhancing Radiology Diagnosis through Convolutional Neural Networks for Computer Vision in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11234">http://arxiv.org/abs/2311.11234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keshav Kumar K., Dr N V S L Narasimham</li>
<li>for: 这项研究探讨了用 convolutional neural networks (CNNs) 在医学诊断中的转化力量，特别是解释性、有效性和伦理问题。</li>
<li>methods: 研究使用了修改后的 DenseNet 架构，并通过比较分析表明其在特征、敏感度和准确率方面表现出色。</li>
<li>results: 研究表明 CNN 在传统方法的比较分析中表现出明显的效率提升，但解释性问题需要进一步改进。 inteoperability 和医生培训问题也需要进行考虑。<details>
<summary>Abstract</summary>
The transformative power of Convolutional Neural Networks (CNNs) in radiology diagnostics is examined in this study, with a focus on interpretability, effectiveness, and ethical issues. With an altered DenseNet architecture, the CNN performs admirably in terms of particularity, sensitivity, as well as accuracy. Its superiority over conventional methods is validated by comparative analyses, which highlight efficiency gains. Nonetheless, interpretability issues highlight the necessity of sophisticated methods in addition to continuous model improvement. Integration issues like interoperability and radiologists' training lead to suggestions for teamwork. Systematic consideration of the ethical implications is carried out, necessitating extensive frameworks. Refinement of architectures, interpretability, alongside ethical considerations need to be prioritized in future work for responsible CNN deployment in radiology diagnostics.
</details>
<details>
<summary>摘要</summary>
这个研究探讨了计算机神经网络（CNN）在医学诊断中的转化力，强调了可读性、有效性和伦理问题。通过修改了DenseNet架构，CNN表现出色地在特征、敏感性和准确性方面。与传统方法比较分析显示，CNN在效率方面有所提升。然而，可读性问题表明需要进一步开发复杂的方法，同时继续改进模型。医生培训和兼容性问题的解决需要团队合作。对伦理问题的系统考虑需要广泛的框架。未来的工作应该在CNN部署中优先级顺序是解释性、伦理考虑和建立框架。
</details></li>
</ul>
<hr>
<h2 id="GaussianDiffusion-3D-Gaussian-Splatting-for-Denoising-Diffusion-Probabilistic-Models-with-Structured-Noise"><a href="#GaussianDiffusion-3D-Gaussian-Splatting-for-Denoising-Diffusion-Probabilistic-Models-with-Structured-Noise" class="headerlink" title="GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise"></a>GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion Probabilistic Models with Structured Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11221">http://arxiv.org/abs/2311.11221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhai Li, Huaibin Wang, Kuo-Kun Tseng</li>
<li>For:  This paper proposes a novel text-to-3D content generation framework based on Gaussian splatting, which enables fine control over image saturation and produces more realistic images.* Methods: The framework uses Gaussian splatting to generate 3D content, and employs multi-view noise distributions to rectify inconsistencies in multi-view geometry. It also uses a variational Gaussian splatting technique to enhance the quality and stability of 3D appearance.* Results: The proposed method produces more realistic images with fine control over image saturation, and mitigates issues like floaters, burrs, and proliferative elements. It also demonstrates the first comprehensive utilization of Gaussian splatting across the entire spectrum of 3D content generation processes.<details>
<summary>Abstract</summary>
Text-to-3D, known for its efficient generation methods and expansive creative potential, has garnered significant attention in the AIGC domain. However, the amalgamation of Nerf and 2D diffusion models frequently yields oversaturated images, posing severe limitations on downstream industrial applications due to the constraints of pixelwise rendering method. Gaussian splatting has recently superseded the traditional pointwise sampling technique prevalent in NeRF-based methodologies, revolutionizing various aspects of 3D reconstruction. This paper introduces a novel text to 3D content generation framework based on Gaussian splatting, enabling fine control over image saturation through individual Gaussian sphere transparencies, thereby producing more realistic images. The challenge of achieving multi-view consistency in 3D generation significantly impedes modeling complexity and accuracy. Taking inspiration from SJC, we explore employing multi-view noise distributions to perturb images generated by 3D Gaussian splatting, aiming to rectify inconsistencies in multi-view geometry. We ingeniously devise an efficient method to generate noise that produces Gaussian noise from diverse viewpoints, all originating from a shared noise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap models in local minima, causing artifacts like floaters, burrs, or proliferative elements. To mitigate these issues, we propose the variational Gaussian splatting technique to enhance the quality and stability of 3D appearance. To our knowledge, our approach represents the first comprehensive utilization of Gaussian splatting across the entire spectrum of 3D content generation processes.
</details>
<details>
<summary>摘要</summary>
文本到3D，知名于其高效生成方法和广阔的创作潜力，在AIGC领域产生了广泛的关注。然而，将NERF和2D扩散模型合并经常会产生过度饱和的图像，这会限制下游工业应用的可能性，因为像素级渲染方法的约束。在传统的点 wise sampling技术被替代后， Gaussian splatting 在3D重建方面引入了一种新的文本到3D内容生成框架，可以在图像饱和度控制中提供细分的控制，从而生成更真实的图像。然而，在多视图匹配中，3D生成模型的复杂性和准确性受到了严重的挑战。我们从 SJC 中得到了灵感，我们尝试使用多视图噪声分布来扰乱由3D Gaussian splatting 生成的图像，以解决多视图匹配中的不一致性。我们创新地设计了一种高效的噪声生成方法，可以生成多视图的噪声，所有的噪声都来自一个共同的噪声源。此外，普通的3D Gaussian-based生成往往会让模型陷入本地极小值，导致残余、尖刺或肿瘤等问题。为了缓解这些问题，我们提出了可变 Gaussian splatting 技术，以提高3D外观的质量和稳定性。据我们所知，我们的方法是首次在3D内容生成过程中广泛应用 Gaussian splatting。
</details></li>
</ul>
<hr>
<h2 id="Infrared-image-identification-method-of-substation-equipment-fault-under-weak-supervision"><a href="#Infrared-image-identification-method-of-substation-equipment-fault-under-weak-supervision" class="headerlink" title="Infrared image identification method of substation equipment fault under weak supervision"></a>Infrared image identification method of substation equipment fault under weak supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11214">http://arxiv.org/abs/2311.11214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjali Sharma, Priya Banerjee, Nikhil Singh</li>
<li>for: 本研究提出了一种弱监督的方法，用于检测 substation 设备上的缺陷。</li>
<li>methods: 该方法使用 Faster RCNN 模型进行设备标识，通过模型网络结构和参数的修改，提高检测精度。</li>
<li>results: 研究通过使用探测机器人captured的红外图像进行分析，证明提posed算法可以显著提高不同设备类型的缺陷标识精度。<details>
<summary>Abstract</summary>
This study presents a weakly supervised method for identifying faults in infrared images of substation equipment. It utilizes the Faster RCNN model for equipment identification, enhancing detection accuracy through modifications to the model's network structure and parameters. The method is exemplified through the analysis of infrared images captured by inspection robots at substations. Performance is validated against manually marked results, demonstrating that the proposed algorithm significantly enhances the accuracy of fault identification across various equipment types.
</details>
<details>
<summary>摘要</summary>
这种研究提出了一种弱监督的方法，用于检测 substation 设备的缺陷。它利用 Faster RCNN 模型进行设备标识，通过 modify 模型的网络结构和参数，提高检测精度。该方法通过使用检测机器人 capture 的 инфра红图像进行示例，并与手动标注结果进行比较，证明提案的算法可以明显提高不同类型设备的缺陷检测精度。Note: "infrared" is translated as "инфра红" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="HiH-A-Multi-modal-Hierarchy-in-Hierarchy-Network-for-Unconstrained-Gait-Recognition"><a href="#HiH-A-Multi-modal-Hierarchy-in-Hierarchy-Network-for-Unconstrained-Gait-Recognition" class="headerlink" title="HiH: A Multi-modal Hierarchy in Hierarchy Network for Unconstrained Gait Recognition"></a>HiH: A Multi-modal Hierarchy in Hierarchy Network for Unconstrained Gait Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11210">http://arxiv.org/abs/2311.11210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Wang, Yinchi Ma, Peng Luan, Wei Yao, Congcong Li, Bo Liu</li>
<li>for: 本研究旨在提高不受控制的环境下的步态识别精度，通过融合多模态信息。</li>
<li>methods: 本研究使用了一种多模态层次网络（HiH），将Silhouette和pose序列融合进行Robust步态识别。HiH网络包括主支路和 auxillary支路两部分，主支路使用层次步态分解（HGD）模块进行深度 wise和内部模块层次检查普通步态模式。auxillary支路使用改进的空间和时间检索模块（DSE和DTA）来丰富步态分析的空间和时间方面。</li>
<li>results: EXTENSIVE evaluations across diverse indoor and outdoor datasets demonstrate HiH’s state-of-the-art performance, affirming a well-balanced trade-off between accuracy and efficiency.<details>
<summary>Abstract</summary>
Gait recognition has achieved promising advances in controlled settings, yet it significantly struggles in unconstrained environments due to challenges such as view changes, occlusions, and varying walking speeds. Additionally, efforts to fuse multiple modalities often face limited improvements because of cross-modality incompatibility, particularly in outdoor scenarios. To address these issues, we present a multi-modal Hierarchy in Hierarchy network (HiH) that integrates silhouette and pose sequences for robust gait recognition. HiH features a main branch that utilizes Hierarchical Gait Decomposer (HGD) modules for depth-wise and intra-module hierarchical examination of general gait patterns from silhouette data. This approach captures motion hierarchies from overall body dynamics to detailed limb movements, facilitating the representation of gait attributes across multiple spatial resolutions. Complementing this, an auxiliary branch, based on 2D joint sequences, enriches the spatial and temporal aspects of gait analysis. It employs a Deformable Spatial Enhancement (DSE) module for pose-guided spatial attention and a Deformable Temporal Alignment (DTA) module for aligning motion dynamics through learned temporal offsets. Extensive evaluations across diverse indoor and outdoor datasets demonstrate HiH's state-of-the-art performance, affirming a well-balanced trade-off between accuracy and efficiency.
</details>
<details>
<summary>摘要</summary>
遥感人体行走识别技术在控制环境下已经取得了有前途的进步，但在无法控制的环境中却面临着视野变化、遮挡以及不同的走速等挑战。此外，融合多Modalities的尝试通常会面临限制的改进，特别是在开放场景下。为解决这些问题，我们提出了一种多Modalities层次网络（HiH），它将拼接Silhouette和pose序列来实现Robust遥感人体行走识别。HiH网络包括主支路，该支路使用层次式遥感走动分解（HGD）模块进行深度和内部模块层次的总走动模式的检查，从而捕捉人体动态的运动层次。此外，一个辅助支路基于2D JOINT序列，它会提高空间和时间方面的步伐分析，并使用学习时间偏移来对步伐动态进行对齐。广泛的实验证明HiH网络在多种室内和室外数据集上达到了当前最佳性能，证明了一个良好的平衡点 между精度和效率。
</details></li>
</ul>
<hr>
<h2 id="3D-Guidewire-Shape-Reconstruction-from-Monoplane-Fluoroscopic-Images"><a href="#3D-Guidewire-Shape-Reconstruction-from-Monoplane-Fluoroscopic-Images" class="headerlink" title="3D Guidewire Shape Reconstruction from Monoplane Fluoroscopic Images"></a>3D Guidewire Shape Reconstruction from Monoplane Fluoroscopic Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11209">http://arxiv.org/abs/2311.11209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tudor Jianu, Baoru Huang, Pierre Berthet-Rayne, Sebastiano Fichera, Anh Nguyen</li>
<li>for: 用于诊断和治疗内分泌疾病的内血管导航，主要基于fluoroscopic图像，受限于感知反馈。</li>
<li>methods: 我们提出了一种新的方法，利用CathSim（state-of-the-art内血管模拟器）和3D fluoroscopy guidewire reconstruction network (3D-FGRN)来重建3D导wire。</li>
<li>results: 我们的3D-FGRN可以在模拟单平面fluoroscopic图像上达到与传统三角形 reconstruction相同的水平，并且在实验中表明了该方法的效率。<details>
<summary>Abstract</summary>
Endovascular navigation, essential for diagnosing and treating endovascular diseases, predominantly hinges on fluoroscopic images due to the constraints in sensory feedback. Current shape reconstruction techniques for endovascular intervention often rely on either a priori information or specialized equipment, potentially subjecting patients to heightened radiation exposure. While deep learning holds potential, it typically demands extensive data. In this paper, we propose a new method to reconstruct the 3D guidewire by utilizing CathSim, a state-of-the-art endovascular simulator, and a 3D Fluoroscopy Guidewire Reconstruction Network (3D-FGRN). Our 3D-FGRN delivers results on par with conventional triangulation from simulated monoplane fluoroscopic images. Our experiments accentuate the efficiency of the proposed network, demonstrating it as a promising alternative to traditional methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>内窥endovascular导航，对于诊断和治疗endovascular疾病至关重要，主要靠射频影像来确定。现有的形态重建技术 oftentimes rely on either prior knowledge or specialized equipment, potentially exposing patients to increased radiation. While deep learning has potential, it typically requires extensive data. In this paper, we propose a new method to reconstruct the 3D guidewire by utilizing CathSim, a state-of-the-art endovascular simulator, and a 3D Fluoroscopy Guidewire Reconstruction Network (3D-FGRN). Our 3D-FGRN delivers results on par with conventional triangulation from simulated monoplane fluoroscopic images. Our experiments emphasize the efficiency of the proposed network, demonstrating it as a promising alternative to traditional methods.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="LogicNet-A-Logical-Consistency-Embedded-Face-Attribute-Learning-Network"><a href="#LogicNet-A-Logical-Consistency-Embedded-Face-Attribute-Learning-Network" class="headerlink" title="LogicNet: A Logical Consistency Embedded Face Attribute Learning Network"></a>LogicNet: A Logical Consistency Embedded Face Attribute Learning Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11208">http://arxiv.org/abs/2311.11208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyu Wu, Sicong Tian, Huayu Li, Kevin W. Bowyer</li>
<li>for: 提高多属性分类中的逻辑一致性，解决数据逻辑不一致性问题。</li>
<li>methods: 提出了两个挑战，一是如何使用逻辑一致性检查的数据进行训练，二是如何使用不进行逻辑一致性检查的数据进行训练。提出了一种名为LogicNet的对抗训练框架，可以学习多属性之间的逻辑关系。</li>
<li>results: LogicNet在FH37K、FH41K和CelebA-logic等三个数据集上的准确率比下一个最佳方法高出23.05%、9.96%和1.71%。在实际应用中，我们的方法可以将失败案件数减少超过50% compared to other methods。<details>
<summary>Abstract</summary>
Ensuring logical consistency in predictions is a crucial yet overlooked aspect in multi-attribute classification. We explore the potential reasons for this oversight and introduce two pressing challenges to the field: 1) How can we ensure that a model, when trained with data checked for logical consistency, yields predictions that are logically consistent? 2) How can we achieve the same with data that hasn't undergone logical consistency checks? Minimizing manual effort is also essential for enhancing automation. To address these challenges, we introduce two datasets, FH41K and CelebA-logic, and propose LogicNet, an adversarial training framework that learns the logical relationships between attributes. Accuracy of LogicNet surpasses that of the next-best approach by 23.05%, 9.96%, and 1.71% on FH37K, FH41K, and CelebA-logic, respectively. In real-world case analysis, our approach can achieve a reduction of more than 50% in the average number of failed cases compared to other methods.
</details>
<details>
<summary>摘要</summary>
保证多属性分类中的逻辑一致性是一项重要但被忽略的方面。我们探讨了可能导致这种忽略的原因，并提出了两个急需解决的挑战：1）如何使用含有逻辑一致性检查的数据训练模型，以便其生成的预测结果具有逻辑一致性？2）如何实现这一点不进行逻辑一致性检查的数据？减少人工努力也是提高自动化的关键。为解决这些挑战，我们介绍了两个数据集（FH37K和CelebA-logic），并提出了逻辑网络（LogicNet），它是一种基于对属性之间的逻辑关系进行反向工程学习的敌方训练框架。在FH37K、FH41K和CelebA-logic三个数据集上，逻辑网络的准确率与最佳方法相比高出23.05%、9.96%和1.71%。在实际案例分析中，我们的方法可以将失败案例的平均数量降低至其他方法的50%以上。
</details></li>
</ul>
<hr>
<h2 id="Shape-Sensitive-Loss-for-Catheter-and-Guidewire-Segmentation"><a href="#Shape-Sensitive-Loss-for-Catheter-and-Guidewire-Segmentation" class="headerlink" title="Shape-Sensitive Loss for Catheter and Guidewire Segmentation"></a>Shape-Sensitive Loss for Catheter and Guidewire Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11205">http://arxiv.org/abs/2311.11205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chayun Kongtongvattana, Baoru Huang, Jingxuan Kang, Hoan Nguyen, Olajide Olufemi, Anh Nguyen</li>
<li>for: 这种Shape-sensitive损失函数用于提高X射线图像中的导管和导线分割精度，并在大规模图像集上实现新的状态测试纪录。</li>
<li>methods: 我们使用视transformer网络，并将网络生成的预测和相关的实际值转换成签名距离图，以便任何网络都可以专注于重要的边界而不仅仅是总体外形。</li>
<li>results: 我们的方法可以提供高维特征向量，这些向量包含图像中关键特征，并通过计算cosine相似性来了解图像之间的相似性，这超出了传统的 overlap-based 度量。<details>
<summary>Abstract</summary>
We introduce a shape-sensitive loss function for catheter and guidewire segmentation and utilize it in a vision transformer network to establish a new state-of-the-art result on a large-scale X-ray images dataset. We transform network-derived predictions and their corresponding ground truths into signed distance maps, thereby enabling any networks to concentrate on the essential boundaries rather than merely the overall contours. These SDMs are subjected to the vision transformer, efficiently producing high-dimensional feature vectors encapsulating critical image attributes. By computing the cosine similarity between these feature vectors, we gain a nuanced understanding of image similarity that goes beyond the limitations of traditional overlap-based measures. The advantages of our approach are manifold, ranging from scale and translation invariance to superior detection of subtle differences, thus ensuring precise localization and delineation of the medical instruments within the images. Comprehensive quantitative and qualitative analyses substantiate the significant enhancement in performance over existing baselines, demonstrating the promise held by our new shape-sensitive loss function for improving catheter and guidewire segmentation.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种形态敏感损失函数，用于针和导wire分割，并在视transformer网络中应用以实现大规模X射线图像集合上的新状态码。我们将网络预测和相应的实际值转换成签名距离图，以便任何网络都可以关注重要的边界而不仅仅是总轮廓。这些SDMs被视transformer进行处理，生成高维特征向量，包含图像重要特征。通过计算这些特征向量之间的 косинуsimilarity，我们可以获得超出传统重叠度基准的图像相似性理解。我们的方法具有许多优点，包括缩放和平移不变性，以及更好地检测细微差异，从而确保针和导wire在图像中的精确定位和分割。我们的全面量化和质量分析证明了我们新的形态敏感损失函数在现有基准上显著提高了性能，证明了我们的方法在针和导wire分割方面的承诺。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Versus-Supervised-Training-for-Segmentation-of-Organoid-Images"><a href="#Self-Supervised-Versus-Supervised-Training-for-Segmentation-of-Organoid-Images" class="headerlink" title="Self-Supervised Versus Supervised Training for Segmentation of Organoid Images"></a>Self-Supervised Versus Supervised Training for Segmentation of Organoid Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11198">http://arxiv.org/abs/2311.11198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Asmaa Haja, Eric Brouwer, Lambert Schomaker</li>
<li>for: 本研究旨在提高数字顺序镜的标注数据集，以便使用深度学习算法进行有效利用。</li>
<li>methods: 本研究使用了自动学习（SSL）技术，通过在类似主任务下学习内在特征，不需要标注数据。首先使用ResNet50 U-Net将图像恢复到原始图像，然后将 weights 传递到另一个 U-Net 模型，用于图像分割。</li>
<li>results: 结果表明，使用 25% 像素截割或图像模糊增强技术可以比其他增强技术更好地进行自我超视投入，并且在使用 IoU 损失函数时，自我超视学习方法可以在只使用 114 个图像时对主任务进行更好的预测，并且在使用更大的数据集（1,000 个图像）时也可以保持更高的稳定性。<details>
<summary>Abstract</summary>
The process of annotating relevant data in the field of digital microscopy can be both time-consuming and especially expensive due to the required technical skills and human-expert knowledge. Consequently, large amounts of microscopic image data sets remain unlabeled, preventing their effective exploitation using deep-learning algorithms. In recent years it has been shown that a lot of relevant information can be drawn from unlabeled data. Self-supervised learning (SSL) is a promising solution based on learning intrinsic features under a pretext task that is similar to the main task without requiring labels. The trained result is transferred to the main task - image segmentation in our case. A ResNet50 U-Net was first trained to restore images of liver progenitor organoids from augmented images using the Structural Similarity Index Metric (SSIM), alone, and using SSIM combined with L1 loss. Both the encoder and decoder were trained in tandem. The weights were transferred to another U-Net model designed for segmentation with frozen encoder weights, using Binary Cross Entropy, Dice, and Intersection over Union (IoU) losses. For comparison, we used the same U-Net architecture to train two supervised models, one utilizing the ResNet50 encoder as well as a simple CNN. Results showed that self-supervised learning models using a 25\% pixel drop or image blurring augmentation performed better than the other augmentation techniques using the IoU loss. When trained on only 114 images for the main task, the self-supervised learning approach outperforms the supervised method achieving an F1-score of 0.85, with higher stability, in contrast to an F1=0.78 scored by the supervised method. Furthermore, when trained with larger data sets (1,000 images), self-supervised learning is still able to perform better, achieving an F1-score of 0.92, contrasting to a score of 0.85 for the supervised method.
</details>
<details>
<summary>摘要</summary>
“在数字微镜像领域中标注相关数据的过程可能会非常耗时和成本高，因为需要特殊的技术技能和人类专业知识。这导致了大量微镜像数据集未被标注，因此无法有效利用深度学习算法。在过去几年中，人们发现了一些有用的信息可以从无标注数据中提取出来。基于自我监督学习（SSL）的方法是一种可能的解决方案，它通过在类似主任务下学习内在特征，而不需要标签。训练结果可以转移到主任务——图像分割。我们使用了一个ResNet50 U-Net模型，首先在使用SSIM指标和L1损失进行训练下来，然后将Encoder层的参数冻结，并使用 Binary Cross Entropy、Dice和Intersection over Union（IoU）损失函数进行训练。为比较，我们使用了同样的U-Net架构来训练两个监督学习模型，一个使用ResNet50 Encoder以及一个简单的CNN。结果表明，使用25%像素截断或图像模糊增强算法的自我监督学习模型在IoU损失函数下表现较好，并且当训练于主任务时，自我监督学习方法可以超过监督学习方法，其中F1分数为0.85，而监督学习方法的F1分数为0.78。此外，当训练于更大的数据集（1,000张图像）时，自我监督学习方法仍然可以表现更好，其F1分数为0.92，而监督学习方法的F1分数为0.85。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.CV_2023_11_19/" data-id="clp89dofw00nei7889htv0np9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/19/eess.AS_2023_11_19/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-11-19
        
      </div>
    </a>
  
  
    <a href="/2023/11/19/cs.AI_2023_11_19/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-19</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
