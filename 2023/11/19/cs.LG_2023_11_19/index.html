
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-11-19 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11452 repo_url: None paper_authors: Talha Siddique, MD Shaad Mahmud for: 本研究旨在开发一种基于物理">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-11-19">
<meta property="og:url" content="https://nullscc.github.io/2023/11/19/cs.LG_2023_11_19/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11452 repo_url: None paper_authors: Talha Siddique, MD Shaad Mahmud for: 本研究旨在开发一种基于物理">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-19T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-21T12:01:18.663Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.LG_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T10:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-11-19
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Physics-Enhanced-TinyML-for-Real-Time-Detection-of-Ground-Magnetic-Anomalies"><a href="#Physics-Enhanced-TinyML-for-Real-Time-Detection-of-Ground-Magnetic-Anomalies" class="headerlink" title="Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies"></a>Physics-Enhanced TinyML for Real-Time Detection of Ground Magnetic Anomalies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11452">http://arxiv.org/abs/2311.11452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Talha Siddique, MD Shaad Mahmud</li>
<li>for: 本研究旨在开发一种基于物理学的小型机器学习（TinyML）框架，用于实时地磁摇动论中的磁场干扰预测。</li>
<li>methods: 该框架结合物理学基于的规范，在模型训练和压缩阶段进行了约束，以提高预测的可靠性。</li>
<li>results: 研究表明，基于物理学的TinyML框架可以提供更高的准确率和可靠性，相比传统的预测方法。<details>
<summary>Abstract</summary>
Space weather phenomena like geomagnetic disturbances (GMDs) and geomagnetically induced currents (GICs) pose significant risks to critical technological infrastructure. While traditional predictive models, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.
</details>
<details>
<summary>摘要</summary>
Space weather phenomena like geomagnetic disturbances (GMDs) and geomagnetically induced currents (GICs) pose significant risks to critical technological infrastructure. While traditional predictive models, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.Here is the translation of the text into Traditional Chinese:Space weather phenomena like geomagnetic disturbances (GMDs) and geomagnetically induced currents (GICs) pose significant risks to critical technological infrastructure. While traditional predictive models, grounded in simulation, hold theoretical robustness, they grapple with challenges, notably the assimilation of imprecise data and extensive computational complexities. In recent years, Tiny Machine Learning (TinyML) has been adopted to develop Machine Learning (ML)-enabled magnetometer systems for predicting real-time terrestrial magnetic perturbations as a proxy measure for GIC. While TinyML offers efficient, real-time data processing, its intrinsic limitations prevent the utilization of robust methods with high computational needs. This paper developed a physics-guided TinyML framework to address the above challenges. This framework integrates physics-based regularization at the stages of model training and compression, thereby augmenting the reliability of predictions. The developed pruning scheme within the framework harnesses the inherent physical characteristics of the domain, striking a balance between model size and robustness. The study presents empirical results, drawing a comprehensive comparison between the accuracy and reliability of the developed framework and its traditional counterpart. Such a comparative analysis underscores the prospective applicability of the developed framework in conceptualizing robust, ML-enabled magnetometer systems for real-time space weather forecasting.
</details></li>
</ul>
<hr>
<h2 id="Weight-Norm-Control"><a href="#Weight-Norm-Control" class="headerlink" title="Weight Norm Control"></a>Weight Norm Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11446">http://arxiv.org/abs/2311.11446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Ilya Loshchilov</li>
<li>for: 本研究探讨了静态weight decay regularization的特殊情况，即weight norm控制，以及其在优化器中的应用。</li>
<li>methods: 本研究使用了多种优化器，包括Adam和AdamW，以及它们的weight norm控制版本AdamWN。</li>
<li>results: 研究发现，设置weight norm为0可能是优化过程中的优化点，但是可以考虑其他target norm值来优化weight norm控制。此外，引入weight norm控制可能会改变优化过程的各种含义。<details>
<summary>Abstract</summary>
We note that decoupled weight decay regularization is a particular case of weight norm control where the target norm of weights is set to 0. Any optimization method (e.g., Adam) which uses decoupled weight decay regularization (respectively, AdamW) can be viewed as a particular case of a more general algorithm with weight norm control (respectively, AdamWN). We argue that setting the target norm of weights to 0 can be suboptimal and other target norm values can be considered. For instance, any training run where AdamW achieves a particular norm of weights can be challenged by AdamWN scheduled to achieve a comparable norm of weights. We discuss various implications of introducing weight norm control instead of weight decay.
</details>
<details>
<summary>摘要</summary>
我们注意到单独对顶点衰落调整是对顶点 Norm 控制的特例，其中顶点 Norm 设置为 0。任何优化方法（例如 Adam）使用单独对顶点衰落调整（即 AdamW）可以被视为更一般的算法中的 weight Norm 控制（即 AdamWN）。我们认为将顶点 Norm 设置为 0 可能是不佳的选择，而其他顶点 Norm 值可以被考虑。例如，任何训练运行 AdamW  achieves 特定顶点 Norm 的情况可以被 AdamWN 调整到相似的顶点 Norm 挑战。我们讨论了将 weight Norm 控制 instead of weight decay 的各种影响。
</details></li>
</ul>
<hr>
<h2 id="Duality-of-Bures-and-Shape-Distances-with-Implications-for-Comparing-Neural-Representations"><a href="#Duality-of-Bures-and-Shape-Distances-with-Implications-for-Comparing-Neural-Representations" class="headerlink" title="Duality of Bures and Shape Distances with Implications for Comparing Neural Representations"></a>Duality of Bures and Shape Distances with Implications for Comparing Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11436">http://arxiv.org/abs/2311.11436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah E. Harvey, Brett W. Larsen, Alex H. Williams</li>
<li>for: 这篇论文旨在团结两类神经网络表示相似度测量方法的研究领域，以便更好地理解这些方法之间的关系。</li>
<li>methods: 这篇论文使用了一种新的方法，即通过观察cosine函数和尼采尔曼布尔相似性之间的关系，来将这两类方法联系起来。</li>
<li>results: 研究发现，cosine函数和尼采尔曼布尔相似性之间存在一定的关系，这种关系可以帮助我们更好地理解这些方法之间的区别和相似之处。<details>
<summary>Abstract</summary>
A multitude of (dis)similarity measures between neural network representations have been proposed, resulting in a fragmented research landscape. Most of these measures fall into one of two categories.   First, measures such as linear regression, canonical correlations analysis (CCA), and shape distances, all learn explicit mappings between neural units to quantify similarity while accounting for expected invariances. Second, measures such as representational similarity analysis (RSA), centered kernel alignment (CKA), and normalized Bures similarity (NBS) all quantify similarity in summary statistics, such as stimulus-by-stimulus kernel matrices, which are already invariant to expected symmetries. Here, we take steps towards unifying these two broad categories of methods by observing that the cosine of the Riemannian shape distance (from category 1) is equal to NBS (from category 2). We explore how this connection leads to new interpretations of shape distances and NBS, and draw contrasts of these measures with CKA, a popular similarity measure in the deep learning literature.
</details>
<details>
<summary>摘要</summary>
很多（不同）相似度量表有被提议，导致研究领域分化。大多数这些度量都属于两个类别。首先，度量如线性回归、 canonical correlation analysis（CCA）和形状距离，都是学习明确映射 между神经网络单元，以量化相似性，同时考虑预期的变换。其次，度量如表现相似性分析（RSA）、中心kernelAlignment（CKA）和归一化射影度（NBS）都是量化相似性的总统计量，如刺激对刺激的kernel矩阵，这些量已经 invariant于预期的对称性。在这里，我们尝试统一这两个类别的方法，观察cosine函数的里曼矩阵距离（从第一类）是等于NBS（从第二类）。我们探索这种连接如何导致新的含义和解释，并与CKA进行比较。
</details></li>
</ul>
<hr>
<h2 id="Fast-Heavy-Inner-Product-Identification-Between-Weights-and-Inputs-in-Neural-Network-Training"><a href="#Fast-Heavy-Inner-Product-Identification-Between-Weights-and-Inputs-in-Neural-Network-Training" class="headerlink" title="Fast Heavy Inner Product Identification Between Weights and Inputs in Neural Network Training"></a>Fast Heavy Inner Product Identification Between Weights and Inputs in Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11429">http://arxiv.org/abs/2311.11429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianke Qin, Saayan Mitra, Zhao Song, Yuanyuan Yang, Tianyi Zhou</li>
<li>For:  solve the heavy inner product identification problem, which generalizes the Light Bulb problem, and speed up the training of neural networks with ReLU activation function.* Methods:  provide an algorithm that runs in $O(n^{2 \omega &#x2F; 3+ o(1)})$ time to find the $k$ inner product pairs that surpass $\rho \cdot d$ threshold with high probability.* Results:  the algorithm can speed up the training of neural networks with ReLU activation function.Here’s the full Chinese text:* 为: 解决内积极大问题，该问题泛化了光泽问题（参考\cite{prr89}），给定两个集合 $A \subset {-1,+1}^d$ 和 $B \subset {-1,+1}^d$， $|A|&#x3D;|B|&#x3D;n$，如果存在 exact $k$ 对的内积超过某个阈值 $\rho \cdot d$，则目标是确定这些 $k$ 对内积。* 方法: 提供一种算法，运行时间为 $O(n^{2 \omega &#x2F; 3+ o(1)})$，可以高可信度地找到超过 $\rho \cdot d$ 阈值的 $k$ 对内积。* 结果: 该算法可以加速使用 ReLU 激活函数训练神经网络的速度。<details>
<summary>Abstract</summary>
In this paper, we consider a heavy inner product identification problem, which generalizes the Light Bulb problem~(\cite{prr89}): Given two sets $A \subset \{-1,+1\}^d$ and $B \subset \{-1,+1\}^d$ with $|A|=|B| = n$, if there are exact $k$ pairs whose inner product passes a certain threshold, i.e., $\{(a_1, b_1), \cdots, (a_k, b_k)\} \subset A \times B$ such that $\forall i \in [k], \langle a_i,b_i \rangle \geq \rho \cdot d$, for a threshold $\rho \in (0,1)$, the goal is to identify those $k$ heavy inner products. We provide an algorithm that runs in $O(n^{2 \omega / 3+ o(1)})$ time to find the $k$ inner product pairs that surpass $\rho \cdot d$ threshold with high probability, where $\omega$ is the current matrix multiplication exponent. By solving this problem, our method speed up the training of neural networks with ReLU activation function.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了一个重要的内积识别问题，该问题泛化了报灯问题（参考 \cite{prr89}）：给定两个集合 $A \subset \{-1,+1\}^d$ 和 $B \subset \{-1,+1\}^d$， $|A| = |B| = n$，如果存在 exact $k$ 对 whose inner product exceeds a certain threshold, i.e., $\{(a_1, b_1), \cdots, (a_k, b_k)\} \subset A \times B$ such that $\forall i \in [k], \langle a_i, b_i \rangle \geq \rho \cdot d$, for a threshold $\rho \in (0,1)$, the goal is to identify those $k$ heavy inner products. We provide an algorithm that runs in $O(n^{2 \omega / 3+ o(1)})$ time to find the $k$ inner product pairs that surpass $\rho \cdot d$ threshold with high probability, where $\omega$ is the current matrix multiplication exponent. By solving this problem, our method can accelerate the training of neural networks with ReLU activation function.
</details></li>
</ul>
<hr>
<h2 id="Tensor-Aware-Energy-Accounting"><a href="#Tensor-Aware-Energy-Accounting" class="headerlink" title="Tensor-Aware Energy Accounting"></a>Tensor-Aware Energy Accounting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11424">http://arxiv.org/abs/2311.11424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/project-smaragdine/smaragdine">https://github.com/project-smaragdine/smaragdine</a></li>
<li>paper_authors: Timur Babakol, Yu David Liu</li>
<li>for: This paper aims to provide a novel energy accounting system for tensor-based deep learning (DL) programs, called Smaragdine, to improve the energy efficiency of DL applications.</li>
<li>methods: Smaragdine uses a novel white-box methodology of energy accounting that is aware of the internal structure of the DL program, allowing for a detailed breakdown of energy consumption by units aligned with the logical hierarchical decomposition structure.</li>
<li>results: Smaragdine was applied to understand the energy behavior of BERT, a widely used language model, and identified the highest energy&#x2F;power-consuming components of BERT layer by layer and tensor by tensor. Additionally, two case studies were conducted to demonstrate the effectiveness of Smaragdine in supporting downstream toolchain building, one comparing the energy impact of hyperparameter tuning of BERT, and the other analyzing the energy behavior evolution of BERT as it evolves to its next generation, ALBERT.<details>
<summary>Abstract</summary>
With the rapid growth of Artificial Intelligence (AI) applications supported by deep learning (DL), the energy efficiency of these applications has an increasingly large impact on sustainability. We introduce Smaragdine, a new energy accounting system for tensor-based DL programs implemented with TensorFlow. At the heart of Smaragdine is a novel white-box methodology of energy accounting: Smaragdine is aware of the internal structure of the DL program, which we call tensor-aware energy accounting. With Smaragdine, the energy consumption of a DL program can be broken down into units aligned with its logical hierarchical decomposition structure. We apply Smaragdine for understanding the energy behavior of BERT, one of the most widely used language models. Layer-by-layer and tensor-by-tensor, Smaragdine is capable of identifying the highest energy/power-consuming components of BERT. Furthermore, we conduct two case studies on how Smaragdine supports downstream toolchain building, one on the comparative energy impact of hyperparameter tuning of BERT, the other on the energy behavior evolution when BERT evolves to its next generation, ALBERT.
</details>
<details>
<summary>摘要</summary>
随着人工智能（AI）应用的快速发展，深度学习（DL）支持的这些应用的能效性越来越重要。我们介绍了一种新的能源会计系统，名为Smaragdine，用于tensor-based DL程序，实现于TensorFlow。Smaragdine的核心是一种新的白盒方法：它知道DL程序的内部结构，我们称之为tensor-aware energy accounting。通过Smaragdine，DL程序的能 consumption可以被分解成与其逻辑层次结构相对应的单元。我们使用Smaragdine来理解BERT语言模型中最广泛使用的语言模型中的能 behaviors。层次和tensor-by-tensor，Smaragdine可以识别BERT中最高能/功率消耗的组件。此外，我们还进行了两个case study，一是对BERT的超参数调整的能效影响，另一是BERT的下游工具链建立支持的能行为演化。
</details></li>
</ul>
<hr>
<h2 id="Offline-Reinforcement-Learning-for-Wireless-Network-Optimization-with-Mixture-Datasets"><a href="#Offline-Reinforcement-Learning-for-Wireless-Network-Optimization-with-Mixture-Datasets" class="headerlink" title="Offline Reinforcement Learning for Wireless Network Optimization with Mixture Datasets"></a>Offline Reinforcement Learning for Wireless Network Optimization with Mixture Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11423">http://arxiv.org/abs/2311.11423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Yang, Cong Shen, Jing Yang, Shu-ping Yeh, Jerry Sydir</li>
<li>for: 本研究旨在 investigate 在 wireless radio resource management (RRM) 问题上使用 offline reinforcement learning (RL) 算法。</li>
<li>methods: 本研究使用了 several state-of-the-art offline RL algorithms, 包括 behavior constrained Q-learning (BCQ), conservative Q-learning (CQL), 和 implicit Q-learning (IQL)，以解决一个 maximizing Linear combination of sum and 5-percentile rates via user scheduling 的 RRM problem。</li>
<li>results: 研究发现，RL 算法的性能对 RRM 问题的行为策略使用方式有critical 关系，并提出了一种 novel offline RL 解决方案，利用不同行为策略收集的 heterogeneous 数据集。研究表明，通过合理混合数据集，offline RL 可以生成一个 near-optimal RL 策略，即使所有 involved 行为策略都是高度不优化的。<details>
<summary>Abstract</summary>
The recent development of reinforcement learning (RL) has boosted the adoption of online RL for wireless radio resource management (RRM). However, online RL algorithms require direct interactions with the environment, which may be undesirable given the potential performance loss due to the unavoidable exploration in RL. In this work, we first investigate the use of \emph{offline} RL algorithms in solving the RRM problem. We evaluate several state-of-the-art offline RL algorithms, including behavior constrained Q-learning (BCQ), conservative Q-learning (CQL), and implicit Q-learning (IQL), for a specific RRM problem that aims at maximizing a linear combination {of sum and} 5-percentile rates via user scheduling. We observe that the performance of offline RL for the RRM problem depends critically on the behavior policy used for data collection, and further propose a novel offline RL solution that leverages heterogeneous datasets collected by different behavior policies. We show that with a proper mixture of the datasets, offline RL can produce a near-optimal RL policy even when all involved behavior policies are highly suboptimal.
</details>
<details>
<summary>摘要</summary>
现在的增强学习（RL）技术的发展，使得在线RL在无线电缺源管理（RRM）中得到了广泛的应用。然而，在线RL算法需要直接与环境交互，这可能是不可避免的探索成本的原因，从而导致性能下降。在这个工作中，我们首先调查使用停机RL算法解决RRM问题。我们评估了一些当今最佳的停机RL算法，包括行为约束Q学习（BCQ）、保守Q学习（CQL）和隐式Q学习（IQL），以解决一个具有最大化线性组合{的总和和} 5%分位率的用户调度问题。我们发现，RRM问题中停机RL的性能取决于用于数据收集的行为策略，并提出了一种新的停机RL解决方案，利用不同行为策略收集的多样化数据集。我们展示了，对于不同行为策略收集的数据集进行杂合，停机RL可以生成一个几乎优质RL策略，即使所有参与行为策略都是高度不优质的。
</details></li>
</ul>
<hr>
<h2 id="Precision-at-the-indistinguishability-threshold-a-method-for-evaluating-classification-algorithms"><a href="#Precision-at-the-indistinguishability-threshold-a-method-for-evaluating-classification-algorithms" class="headerlink" title="Precision at the indistinguishability threshold: a method for evaluating classification algorithms"></a>Precision at the indistinguishability threshold: a method for evaluating classification algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11422">http://arxiv.org/abs/2311.11422</a></li>
<li>repo_url: None</li>
<li>paper_authors: David J. T. Sumpter</li>
<li>for: 本文旨在提出一种新的评估分类算法性能的指标，以解决现有指标中的缺点。</li>
<li>methods: 本文使用两步方法来建立新的指标。首先，设置一个阈值值，使得当算法被示出两个随机选择的图像（一个高于阈值的图像和另一个真正包含猫的图像）时，图像选择的概率是50%。然后，测量算法的性能，问题是随机选择的图像中是否真正包含猫。这个指标可以被称为“锐度在难以区分边界时的准确率”。</li>
<li>results: 本文表明，这个新的指标可以减少一些现有指标（如AUC和F1分数）中的缺点，并且更好地反映算法的性能。<details>
<summary>Abstract</summary>
There exist a wide range of single number metrics for assessing performance of classification algorithms, including AUC and the F1-score (Wikipedia lists 17 such metrics, with 27 different names). In this article, I propose a new metric to answer the following question: when an algorithm is tuned so that it can no longer distinguish labelled cats from real cats, how often does a randomly chosen image that has been labelled as containing a cat actually contain a cat? The steps to construct this metric are as follows. First, we set a threshold score such that when the algorithm is shown two randomly-chosen images -- one that has a score greater than the threshold (i.e. a picture labelled as containing a cat) and another from those pictures that really does contain a cat -- the probability that the image with the highest score is the one chosen from the set of real cat images is 50\%. At this decision threshold, the set of positively labelled images are indistinguishable from the set of images which are positive. Then, as a second step, we measure performance by asking how often a randomly chosen picture from those labelled as containing a cat actually contains a cat. This metric can be thought of as {\it precision at the indistinguishability threshold}. While this new metric doesn't address the tradeoff between precision and recall inherent to all such metrics, I do show why this method avoids pitfalls that can occur when using, for example AUC, and it is better motivated than, for example, the F1-score.
</details>
<details>
<summary>摘要</summary>
exist 一些各种单一数据指标用于评估分类算法的性能，包括AUC和F1-score（Wikipedia列出了17种指标，共27个不同的名称）。在这篇文章中，我提出了一个新的指标，用于回答以下问题：当算法通过调整可以不再区分标注的猫和真正的猫时，总是如何确定一个随机选择的图像是否实际上包含猫？以下是构建这个指标的步骤：1. 设置一个阈值分数，使得当算法被示出两个随机选择的图像——一个分数高于阈值（即标注为猫的图像）和另一个从真正的猫图像中随机选择的图像——图像分数最高的图像是50%的概率来自真正的猫图像集。在这个决策阈值下，标注为猫的图像集和真正的猫图像集变得无法区分。2. 然后，我们测量性能的方式是，从标注为猫的图像集中随机选择一个图像，并问：这个图像是否实际上包含猫？这个指标可以被称为“阈值上的准确率”。尽管这个新的指标不同于AUC和F1-score中的偏好和损失评估，但我展示了这种方法可以避免AUC和F1-score中的坑，并且更加有意义。
</details></li>
</ul>
<hr>
<h2 id="Large-Pre-trained-time-series-models-for-cross-domain-Time-series-analysis-tasks"><a href="#Large-Pre-trained-time-series-models-for-cross-domain-Time-series-analysis-tasks" class="headerlink" title="Large Pre-trained time series models for cross-domain Time series analysis tasks"></a>Large Pre-trained time series models for cross-domain Time series analysis tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11413">http://arxiv.org/abs/2311.11413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshavardhan Kamarthi, B. Aditya Prakash</li>
<li>for: 这篇论文旨在探讨如何将多种不同领域的时间序列数据集合组合成一个通用的时间序列模型，以提高模型的效能和训练效率。</li>
<li>methods: 这篇论文提出了一个新的自适应学习方法，即使用自我监督学习损失来自动决定最佳的dataset特定的分割策略，以提高时间序列模型的性能。</li>
<li>results: 实验结果显示，这个新的模型（LPTM）可以与专业领域的模型相比，并且在训练时间和数据量方面具有优化的效率。 Specifically, LPTM achieves state-of-the-art performance in a wide range of time-series analysis tasks from multiple disparate domains, while taking up to 40% less data and 50% less training time than existing methods.<details>
<summary>Abstract</summary>
Large pre-trained models have been instrumental in significant advancements in domains like language and vision making model training for individual downstream tasks more efficient as well as provide superior performance. However, tackling time-series analysis tasks usually involves designing and training a separate model from scratch leveraging training data and domain expertise specific to the task. We tackle a significant challenge for pre-training a general time-series model from multiple heterogeneous time-series dataset: providing semantically useful inputs to models for modeling time series of different dynamics from different domains. We observe that partitioning time-series into segments as inputs to sequential models produces semantically better inputs and propose a novel model LPTM that automatically identifies optimal dataset-specific segmentation strategy leveraging self-supervised learning loss during pre-training. LPTM provides performance similar to or better than domain-specific state-of-art model and is significantly more data and compute efficient taking up to 40% less data as well as 50% less training time to achieve state-of-art performance in a wide range of time-series analysis tasks from multiple disparate domain.
</details>
<details>
<summary>摘要</summary>
大型预训模型在语言和视觉领域得到了重要的进步，使得模型训练 для下游任务更加效率，并提供了更高的性能。然而，对时间序列分析任务通常需要采用特定任务和领域专业知识来设计和训练独立的模型。我们解决了将多个不同领域时间序列数据集中的时间序列分解为Semantically meaningful inputs for models的挑战。我们发现，将时间序列分割成段为模型输入会产生更好的输入，并提出了一种新的模型LPTM，它可以自动确定数据集特定的分解策略，并在预训练过程中使用无监督学习损失来优化。LPTM在多个不同领域的时间序列分析任务中提供了与领域专业模型相当或更好的性能，并且需要训练时间和数据量相对减少了40%，以及50%。
</details></li>
</ul>
<hr>
<h2 id="Negotiated-Representations-for-Machine-Mearning-Application"><a href="#Negotiated-Representations-for-Machine-Mearning-Application" class="headerlink" title="Negotiated Representations for Machine Mearning Application"></a>Negotiated Representations for Machine Mearning Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11410">http://arxiv.org/abs/2311.11410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nurikorhan/negotiated-representations">https://github.com/nurikorhan/negotiated-representations</a></li>
<li>paper_authors: Nuri Korhan, Samet Bayram</li>
<li>for: 提高机器学习模型的分类精度和避免过拟合</li>
<li>methods: 通过让模型与已知类别标签进行协商来增强模型的解释能力和泛化能力</li>
<li>results: 通过在多个低级机器学习问题上实验，提出了一种可以增强机器学习模型的解释能力和泛化能力的方法，并且在实验中表明了该方法的可行性和可扩展性。<details>
<summary>Abstract</summary>
Overfitting is a phenomenon that occurs when a machine learning model is trained for too long and focused too much on the exact fitness of the training samples to the provided training labels and cannot keep track of the predictive rules that would be useful on the test data. This phenomenon is commonly attributed to memorization of particular samples, memorization of the noise, and forced fitness into a data set of limited samples by using a high number of neurons. While it is true that the model encodes various peculiarities as the training process continues, we argue that most of the overfitting occurs in the process of reconciling sharply defined membership ratios. In this study, we present an approach that increases the classification accuracy of machine learning models by allowing the model to negotiate output representations of the samples with previously determined class labels. By setting up a negotiation between the models interpretation of the inputs and the provided labels, we not only increased average classification accuracy but also decreased the rate of overfitting without applying any other regularization tricks. By implementing our negotiation paradigm approach to several low regime machine learning problems by generating overfitting scenarios from publicly available data sets such as CIFAR 10, CIFAR 100, and MNIST we have demonstrated that the proposed paradigm has more capacity than its intended purpose. We are sharing the experimental results and inviting the machine learning community to explore the limits of the proposed paradigm. We also aim to incentive the community to exploit the negotiation paradigm to overcome the learning related challenges in other research fields such as continual learning. The Python code of the experimental setup is uploaded to GitHub.
</details>
<details>
<summary>摘要</summary>
Overfitting 是一种现象，当机器学习模型被训练了太长时间，对准确地适应训练样本和提供的训练标签进行过于精准的适应，导致模型无法保持预测规则。这种现象通常被归因于特定样本的记忆、噪声记忆和用于限定数据集的高数 neurons。虽然模型在训练过程中记忆了各种特征，但我们 argue 大多数过度适应发生在确定样本的分类比率的过程中。在这种研究中，我们提出了一种方法，可以提高机器学习模型的分类精度，allowing the model to negotiate output representations of the samples with previously determined class labels。通过在模型对输入的解释和提供的标签之间设置谈判，我们不仅提高了平均分类精度，还降低了过度适应的发生，无需应用其他正则化技巧。通过在低度机器学习问题上实现我们的谈判模式，我们在 CIFAR 10、CIFAR 100 和 MNIST 等公共数据集上生成了过度适应场景，并示出了我们的方法具有更大的容量。我们将实验结果分享，邀请机器学习社区探索我们的方法的限制，并努力启发社区利用谈判模式来超越学习相关的挑战。我们还 aspire to encourage the community to explore the limits of the proposed paradigm in other research fields such as continual learning. Python 代码的实验设置已经上传到 GitHub。
</details></li>
</ul>
<hr>
<h2 id="Towards-interpretable-by-design-deep-learning-algorithms"><a href="#Towards-interpretable-by-design-deep-learning-algorithms" class="headerlink" title="Towards interpretable-by-design deep learning algorithms"></a>Towards interpretable-by-design deep learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11396">http://arxiv.org/abs/2311.11396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Plamen Angelov, Dmitry Kangin, Ziyang Zhang</li>
<li>for: 解决深度学习模型的解释性和快速学习问题</li>
<li>methods: 使用Interpretable-by-design DEep learning ALgorithms (IDEAL)把标准超级vised分类问题转化为与训练数据集中的prototype进行相似性计算，并利用现有的大 neural networks嵌入空间（Foundation Models，FM）来解决解释性问题</li>
<li>results: 提出了一种可解释的深度学习模型，并实现了类增量学习、简单的批量学习和快速的转移学习，而无需Finite-tuning的特征空间在目标数据集上。<details>
<summary>Abstract</summary>
The proposed framework named IDEAL (Interpretable-by-design DEep learning ALgorithms) recasts the standard supervised classification problem into a function of similarity to a set of prototypes derived from the training data, while taking advantage of existing latent spaces of large neural networks forming so-called Foundation Models (FM). This addresses the issue of explainability (stage B) while retaining the benefits from the tremendous achievements offered by DL models (e.g., visual transformers, ViT) pre-trained on huge data sets such as IG-3.6B + ImageNet-1K or LVD-142M (stage A). We show that one can turn such DL models into conceptually simpler, explainable-through-prototypes ones.   The key findings can be summarized as follows: (1) the proposed models are interpretable through prototypes, mitigating the issue of confounded interpretations, (2) the proposed IDEAL framework circumvents the issue of catastrophic forgetting allowing efficient class-incremental learning, and (3) the proposed IDEAL approach demonstrates that ViT architectures narrow the gap between finetuned and non-finetuned models allowing for transfer learning in a fraction of time \textbf{without} finetuning of the feature space on a target dataset with iterative supervised methods.
</details>
<details>
<summary>摘要</summary>
提议的框架被称为IDEAL（可解释的设计深度学习算法），将标准的supervised分类问题转化为与训练数据集中的prototype进行相似性计算的函数，同时利用现有的大神经网络的固有的latent空间形成 socalled Foundation Models (FM)。这解决了解释性（stage B）的问题，而不失去深度学习模型（如视觉转换器、ViT）在大量数据集上的成果（如IG-3.6B + ImageNet-1K或LVD-142M stage A）。我们表明可以将这些深度学习模型转化为概念更简单、可解释的prototype-based模型。关键发现包括：1. 提议的模型可以通过prototype进行解释，解决了叙述混乱的问题，2. IDEAL框架绕过了快速忘记问题，可以有效地进行分类增量学习，3. IDEAL方法示出了使用ViT体系进行传输学习，可以在几乎没有时间的情况下将finetuning的模型与非finetuning模型之间的差异减少，使得传输学习变得可行。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-speed-accuracy-simulation-trade-off-for-adaptive-spiking-neurons"><a href="#Addressing-the-speed-accuracy-simulation-trade-off-for-adaptive-spiking-neurons" class="headerlink" title="Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons"></a>Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11390">http://arxiv.org/abs/2311.11390</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webstorms/blocks">https://github.com/webstorms/blocks</a></li>
<li>paper_authors: Luke Taylor, Andrew J King, Nicol S Harper</li>
<li>for: 帮助解决计算神经科学中的速度精度贸易，提高ALIF模型的并行化效率。</li>
<li>methods: 对ALIF模型进行算法重新解释，降低约束 simulate 复杂性，并使其更容易并行化在GPU上。</li>
<li>results: 在 sintetic 测试套件上实现了更 чем 50倍的培训速度增加，并在不同的supervised 分类任务中达到了相似的性能，却在训练时间上减少了一半。 最后，我们展示了如何使用我们的模型快速和准确地适应真实的电physiological 记录，其中非常细微的sub-毫秒DT是关键 для捕捉精确的脉冲时间。<details>
<summary>Abstract</summary>
The adaptive leaky integrate-and-fire (ALIF) model is fundamental within computational neuroscience and has been instrumental in studying our brains $\textit{in silico}$. Due to the sequential nature of simulating these neural models, a commonly faced issue is the speed-accuracy trade-off: either accurately simulate a neuron using a small discretisation time-step (DT), which is slow, or more quickly simulate a neuron using a larger DT and incur a loss in simulation accuracy. Here we provide a solution to this dilemma, by algorithmically reinterpreting the ALIF model, reducing the sequential simulation complexity and permitting a more efficient parallelisation on GPUs. We computationally validate our implementation to obtain over a $50\times$ training speedup using small DTs on synthetic benchmarks. We also obtained a comparable performance to the standard ALIF implementation on different supervised classification tasks - yet in a fraction of the training time. Lastly, we showcase how our model makes it possible to quickly and accurately fit real electrophysiological recordings of cortical neurons, where very fine sub-millisecond DTs are crucial for capturing exact spike timing.
</details>
<details>
<summary>摘要</summary>
《适应式漏斗启动（ALIF）模型是计算神经科学中的基础模型，对于研究大脑的 simulate 有着重要的作用。由于这种神经网络模型的顺序 simulate 的性质，frequently faced 一个速度-准确性贸易：可以使用小时间步长（DT）来准确地模拟神经元，但是这会比较慢；或者使用大时间步长（DT）来快速地模拟神经元，但是会产生模拟不准确。在这篇文章中，我们提供了一种解决这个困扰的方法，通过对 ALIF 模型的算法重新解释，从而降低了Sequential simulation 的复杂性，并且允许更有效的并行化在 GPU 上。我们通过计算验证了我们的实现，在使用小DT 进行训练时，可以获得超过 50 倍的增速。此外，我们还证明了我们的实现在不同的超visisted classification task 上具有相同的性能，但是在训练时间上占了一小 fraction。最后，我们示例了我们的模型可以快速并准确地适应真实的电physiological 记录，其中非常细的毫秒级DT 是关键 для捕捉精确的抽吸时间。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Reinforcement-Learning-with-Mixture-of-Orthogonal-Experts"><a href="#Multi-Task-Reinforcement-Learning-with-Mixture-of-Orthogonal-Experts" class="headerlink" title="Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts"></a>Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11385">http://arxiv.org/abs/2311.11385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Hendawy, Jan Peters, Carlo D’Eramo</li>
<li>for: 解决多任务束缚学习（MTRL）中agent的普适性问题，即使在不同任务之间共享表示。</li>
<li>methods: 我们提出了一种新的表示学习方法，名为 Mixture Of Orthogonal Experts（MOORE），它利用 Gram-Schmidt  процесс在多任务情况下共享表示空间中找到相似性。当给定任务特定信息时，MOORE可以从共享空间中生成相关的表示。</li>
<li>results: 我们在 MiniGrid 和 MetaWorld 两个 MTRL 测试环境中评估了我们的方法，结果显示 MOORE 超过了相关的基线并在 MetaWorld 上达到了新的州OF-THE-ART 记录。<details>
<summary>Abstract</summary>
Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of endowing agents with skills that generalize across a variety of problems. To this end, sharing representations plays a fundamental role in capturing both unique and common characteristics of the tasks. Tasks may exhibit similarities in terms of skills, objects, or physical properties while leveraging their representations eases the achievement of a universal policy. Nevertheless, the pursuit of learning a shared set of diverse representations is still an open challenge. In this paper, we introduce a novel approach for representation learning in MTRL that encapsulates common structures among the tasks using orthogonal representations to promote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared subspace of representations generated by a mixture of experts. When task-specific information is provided, MOORE generates relevant representations from this shared subspace. We assess the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses related baselines and establishes a new state-of-the-art result on MetaWorld.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:多任务强化学习（MTRL）解决了长期存在的问题，即让代理人具备通用于多种问题的技能。为此，共享表示是关键，它可以捕捉任务的独特和共同特征。任务可能在技能、物品或物理性方面具有相似性，而共享表示可以使得实现通用策略变得更加容易。然而，学习共享多个多样化表示的挑战仍未得到解决。在这篇论文中，我们介绍了一种新的MTRL表示学习方法，名为“ mixture of orthogonal experts”（MOORE）。MOORE使用 Gram-Schmidt 过程将多个专家生成的共享表示空间中的共同结构拟合到一起，并在任务特定信息提供时生成相关的表示。我们在 MiniGrid 和 MetaWorld 两个 MTRL 标准测试集上评估了我们的方法，并证明 MOORE 超过相关的基elines并在 MetaWorld 上设置了新的state-of-the-art 记录。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Locally-Private-Nonparametric-Classification-with-Public-Data"><a href="#Optimal-Locally-Private-Nonparametric-Classification-with-Public-Data" class="headerlink" title="Optimal Locally Private Nonparametric Classification with Public Data"></a>Optimal Locally Private Nonparametric Classification with Public Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11369">http://arxiv.org/abs/2311.11369</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karlmyh/lpct">https://github.com/karlmyh/lpct</a></li>
<li>paper_authors: Yuheng Ma, Hanfang Yang</li>
<li>for: 本研究 investigate 非互动式LDP（本地隐私）学习问题，专注于非 Parametric 分类。</li>
<li>methods: 我们首次 derive 隐私限制下的最佳减少速率，并提出一个名为内部私人分类树的新方法，可以实现最佳减少速率。另外，我们设计了一个数据驱动的剪除程序，以避免参数调整和生成快速减少的估计器。</li>
<li>results: 我们在实验中使用 синтети数据和实际数据，证明了我们的提案方法比private数据更有优势，并且可以实现实际中的应用。<details>
<summary>Abstract</summary>
In this work, we investigate the problem of public data-assisted non-interactive LDP (Local Differential Privacy) learning with a focus on non-parametric classification. Under the posterior drift assumption, we for the first time derive the mini-max optimal convergence rate with LDP constraint. Then, we present a novel approach, the locally private classification tree, which attains the mini-max optimal convergence rate. Furthermore, we design a data-driven pruning procedure that avoids parameter tuning and produces a fast converging estimator. Comprehensive experiments conducted on synthetic and real datasets show the superior performance of our proposed method. Both our theoretical and experimental findings demonstrate the effectiveness of public data compared to private data, which leads to practical suggestions for prioritizing non-private data collection.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了公共数据协助非互动式LDP（本地差分隐私学习）问题，特点是非 Parametric 分类。根据 posterior 漂移假设，我们首次 derivated 最佳 convergence 速率准则，并提出了一种新的方法——本地隐私分类树。此外，我们还设计了一种数据驱动的剪裁过程，以避免参数调整并生成快速收敛的估计器。我们在 synthetic 和实际数据上进行了广泛的实验，结果表明我们的提议方法在效果和速度上都有优势。这些理论和实验成果表明公共数据的优越性，从而提出了实际化非私人数据收集的建议。Note that "Synthetic" and "Real" in the text are translated as "合成" and "实际" respectively, which is a common way to refer to synthetic and real data in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Pretraining-for-Heterogeneous-Hypergraph-Neural-Networks"><a href="#Self-Supervised-Pretraining-for-Heterogeneous-Hypergraph-Neural-Networks" class="headerlink" title="Self-Supervised Pretraining for Heterogeneous Hypergraph Neural Networks"></a>Self-Supervised Pretraining for Heterogeneous Hypergraph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11368">http://arxiv.org/abs/2311.11368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdalgader Abubaker, Takanori Maehara, Madhav Nimishakavi, Vassilis Plachouras</li>
<li>for: This paper is written to present a novel self-supervised pretraining framework for heterogeneous HyperGNNs, which can effectively capture higher-order relations among entities in the data in a self-supervised manner.</li>
<li>methods: The proposed SPHH framework consists of two self-supervised pretraining tasks that aim to simultaneously learn both local and global representations of the entities in the hypergraph by using informative representations derived from the hypergraph structure.</li>
<li>results: The experiments on two real-world benchmarks using four different HyperGNN models show that the proposed SPHH framework consistently outperforms state-of-the-art baselines in various downstream tasks, demonstrating the effectiveness of the proposed framework in improving the performance of various HyperGNN models in various downstream tasks.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了介绍一种新的自动超vised预训练框架 для多种多样性的HyperGNN，该框架可以有效地在自动超vised模式下捕捉图数据中实体之间的高阶关系。</li>
<li>methods: 提出的 SPHH 框架包括两个自动超vised 预训练任务，这些任务的目的是同时学习实体在图数据中的本地和全局表示，使用图数据的结构 derive 出的有用表示。</li>
<li>results: 对于两个实际 benchmark 使用四种不同的 HyperGNN 模型，我们的实验结果表明，提出的 SPHH 框架可以在多种下游任务中表现出色，如节点分类和链接预测等，并且可以在不同的模型和复杂性下表现良好，这表明了我们的框架的稳定性和 universality。<details>
<summary>Abstract</summary>
Recently, pretraining methods for the Graph Neural Networks (GNNs) have been successful at learning effective representations from unlabeled graph data. However, most of these methods rely on pairwise relations in the graph and do not capture the underling higher-order relations between entities. Hypergraphs are versatile and expressive structures that can effectively model higher-order relationships among entities in the data. Despite the efforts to adapt GNNs to hypergraphs (HyperGNN), there are currently no fully self-supervised pretraining methods for HyperGNN on heterogeneous hypergraphs. In this paper, we present SPHH, a novel self-supervised pretraining framework for heterogeneous HyperGNNs. Our method is able to effectively capture higher-order relations among entities in the data in a self-supervised manner. SPHH is consist of two self-supervised pretraining tasks that aim to simultaneously learn both local and global representations of the entities in the hypergraph by using informative representations derived from the hypergraph structure. Overall, our work presents a significant advancement in the field of self-supervised pretraining of HyperGNNs, and has the potential to improve the performance of various graph-based downstream tasks such as node classification and link prediction tasks which are mapped to hypergraph configuration. Our experiments on two real-world benchmarks using four different HyperGNN models show that our proposed SPHH framework consistently outperforms state-of-the-art baselines in various downstream tasks. The results demonstrate that SPHH is able to improve the performance of various HyperGNN models in various downstream tasks, regardless of their architecture or complexity, which highlights the robustness of our framework.
</details>
<details>
<summary>摘要</summary>
近期，图神经网络（GNN）的预训练方法在无标签图数据上学习有效表示得到了成功。然而，大多数这些方法仅依靠图中的对比关系，而不capture图中Entity之间的高阶关系。幂体图是一种灵活和表示力强的结构，可以有效地模型图中Entity之间的高阶关系。尽管有尝试将GNN适应幂体图（HyperGNN），但目前还没有完全自动适应的自我超vised预训练方法 для HyperGNN on 不同类型的幂体图。在这篇论文中，我们提出了一种新的自我超vised预训练框架 для幂体图GNN（SPHH）。我们的方法可以在自我超vised manner中有效地捕捉图中Entity之间的高阶关系。SPHH包括两种自我超vised预训练任务，旨在同时学习图中Entity的本地和全局表示。我们使用幂体图结构提供的有用表示来实现这两种任务。总的来说，我们的工作对于自我超vised预训练HyperGNN的Field提供了重要的进步，并且可以提高多种基于图的下游任务的性能，如节点分类和链接预测任务，它们可以与幂体图配置相对应。我们在两个实际 benchmark上使用四种不同的 HyperGNN 模型进行实验，得到的结果表明，我们提出的 SPHH 框架在多种下游任务中 consistently 超过了当前的基eline。这些结果表明，SPHH 可以在不同的 HyperGNN 模型和配置下提高多种下游任务的性能，这种可靠性 highlights 了我们的框架的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Symmetry-invariant-quantum-machine-learning-force-fields"><a href="#Symmetry-invariant-quantum-machine-learning-force-fields" class="headerlink" title="Symmetry-invariant quantum machine learning force fields"></a>Symmetry-invariant quantum machine learning force fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11362">http://arxiv.org/abs/2311.11362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isabel Nha Minh Le, Oriel Kiss, Julian Schuhmacher, Ivano Tavernelli, Francesco Tacchino</li>
<li>for: 这个论文的目的是为了开发一种基于量子机器学习的分子力场模型，以便在原子尺度上进行精度充足的模拟。</li>
<li>methods: 这篇论文使用了量子机器学习模型来预测分子的潜能能面和原子力，并利用了变量量子学习模型来从初始数据中学习。</li>
<li>results: 研究发现，在使用了物理相对论中的 symmetries 的情况下，量子神经网络模型表现更好，并且在较复杂的分子中也能够保持高度的准确性。此外，研究者还通过对水二分子的示例来展示了这种方法在多 компонент系统中的应用可能性。<details>
<summary>Abstract</summary>
Machine learning techniques are essential tools to compute efficient, yet accurate, force fields for atomistic simulations. This approach has recently been extended to incorporate quantum computational methods, making use of variational quantum learning models to predict potential energy surfaces and atomic forces from ab initio training data. However, the trainability and scalability of such models are still limited, due to both theoretical and practical barriers. Inspired by recent developments in geometric classical and quantum machine learning, here we design quantum neural networks that explicitly incorporate, as a data-inspired prior, an extensive set of physically relevant symmetries. We find that our invariant quantum learning models outperform their more generic counterparts on individual molecules of growing complexity. Furthermore, we study a water dimer as a minimal example of a system with multiple components, showcasing the versatility of our proposed approach and opening the way towards larger simulations. Our results suggest that molecular force fields generation can significantly profit from leveraging the framework of geometric quantum machine learning, and that chemical systems represent, in fact, an interesting and rich playground for the development and application of advanced quantum machine learning tools.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Coverage-Validity-Aware-Algorithmic-Recourse"><a href="#Coverage-Validity-Aware-Algorithmic-Recourse" class="headerlink" title="Coverage-Validity-Aware Algorithmic Recourse"></a>Coverage-Validity-Aware Algorithmic Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11349">http://arxiv.org/abs/2311.11349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngoc Bui, Duy Nguyen, Man-Chung Yue, Viet Anh Nguyen</li>
<li>for: 本研究旨在提高机器学习模型的解释性、透明度和伦理性，通过创建一种基于模型的无关的回溯机制。</li>
<li>methods: 本研究使用了一种基于 Linear Surrogate 的扩展方法，以生成一个可靠性和有效性的回溯机制。</li>
<li>results: 研究表明，该方法可以在不同的模型下提供有效的回溯机制，并且可以避免由模型更新所带来的问题。此外，该方法还可以捕捉到模型的泛化性和解释性。<details>
<summary>Abstract</summary>
Algorithmic recourse emerges as a prominent technique to promote the explainability, transparency and hence ethics of machine learning models. Existing algorithmic recourse approaches often assume an invariant predictive model; however, the predictive model is usually updated upon the arrival of new data. Thus, a recourse that is valid respective to the present model may become invalid for the future model. To resolve this issue, we propose a novel framework to generate a model-agnostic recourse that exhibits robustness to model shifts. Our framework first builds a coverage-validity-aware linear surrogate of the nonlinear (black-box) model; then, the recourse is generated with respect to the linear surrogate. We establish a theoretical connection between our coverage-validity-aware linear surrogate and the minimax probability machines (MPM). We then prove that by prescribing different covariance robustness, the proposed framework recovers popular regularizations for MPM, including the $\ell_2$-regularization and class-reweighting. Furthermore, we show that our surrogate pushes the approximate hyperplane intuitively, facilitating not only robust but also interpretable recourses. The numerical results demonstrate the usefulness and robustness of our framework.
</details>
<details>
<summary>摘要</summary>
算法征求 emerges as a prominent technique to promote the explainability, transparency, and hence ethics of machine learning models. Existing algorithmic recourse approaches often assume an invariant predictive model; however, the predictive model is usually updated upon the arrival of new data. Thus, a recourse that is valid respective to the present model may become invalid for the future model. To resolve this issue, we propose a novel framework to generate a model-agnostic recourse that exhibits robustness to model shifts. Our framework first builds a coverage-validity-aware linear surrogate of the nonlinear (black-box) model; then, the recourse is generated with respect to the linear surrogate. We establish a theoretical connection between our coverage-validity-aware linear surrogate and the minimax probability machines (MPM). We then prove that by prescribing different covariance robustness, the proposed framework recovers popular regularizations for MPM, including the $\ell_2$-regularization and class-reweighting. Furthermore, we show that our surrogate pushes the approximate hyperplane intuitively, facilitating not only robust but also interpretable recourses. The numerical results demonstrate the usefulness and robustness of our framework.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="A-Generative-Model-for-Accelerated-Inverse-Modelling-Using-a-Novel-Embedding-for-Continuous-Variables"><a href="#A-Generative-Model-for-Accelerated-Inverse-Modelling-Using-a-Novel-Embedding-for-Continuous-Variables" class="headerlink" title="A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables"></a>A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11343">http://arxiv.org/abs/2311.11343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sébastien Bompas abd Stefan Sandfeld</li>
<li>for:  accelerated materials design</li>
<li>methods: 使用生成机器学习模型，以避免广泛的实验和搜索多种可能的微结构</li>
<li>results: 提出一种基于二进制浮点数表示的嵌入策略，可以conditioning网络以提供精细的控制权限，并且可以应用于任何数字Here’s the breakdown of each point:</li>
<li>for: The paper is written for accelerated materials design, specifically addressing the challenge of finding suitable microstructures for desired properties using rapid prototyping.</li>
<li>methods: The paper uses generative machine learning models to address the challenge, but notes that existing methods have shortcomings. The authors propose a novel embedding strategy based on the binary representation of floating point numbers to overcome these shortcomings.</li>
<li>results: The proposed embedding strategy allows for fine control over generated microstructure images, providing a versatile embedding space for conditioning the generative model. This technique can be applied to any number, enabling accelerated materials design.<details>
<summary>Abstract</summary>
In materials science, the challenge of rapid prototyping materials with desired properties often involves extensive experimentation to find suitable microstructures. Additionally, finding microstructures for given properties is typically an ill-posed problem where multiple solutions may exist. Using generative machine learning models can be a viable solution which also reduces the computational cost. This comes with new challenges because, e.g., a continuous property variable as conditioning input to the model is required. We investigate the shortcomings of an existing method and compare this to a novel embedding strategy for generative models that is based on the binary representation of floating point numbers. This eliminates the need for normalization, preserves information, and creates a versatile embedding space for conditioning the generative model. This technique can be applied to condition a network on any number, to provide fine control over generated microstructure images, thereby contributing to accelerated materials design.
</details>
<details>
<summary>摘要</summary>
在材料科学中，快速原型材料的性能要求经常带来广泛的实验寻找适合的微结构。此外，为某些属性找到适合的微结构是一个不充分定义的问题，可能存在多种解。使用生成机器学习模型可以是一种可行的解决方案，同时降低计算成本。然而，这也会带来新的挑战，例如需要作为conditioning输入的continueProperty变量的整数化。我们 investigate了现有的方法的缺陷，并与基于二进制浮点数表示的新嵌入策略进行比较。这种策略可以用于condition网络任意数字，提供精细的控制权限，以便加速材料设计。
</details></li>
</ul>
<hr>
<h2 id="On-the-Communication-Complexity-of-Decentralized-Bilevel-Optimization"><a href="#On-the-Communication-Complexity-of-Decentralized-Bilevel-Optimization" class="headerlink" title="On the Communication Complexity of Decentralized Bilevel Optimization"></a>On the Communication Complexity of Decentralized Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11342">http://arxiv.org/abs/2311.11342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Zhang, My T. Thai, Jie Wu, Hongchang Gao</li>
<li>for: 提高 decentralized bilevel optimization 的应用性能，特别是在各种机器学习任务中。</li>
<li>methods: 提出了一种基于 Stochastic Bilevel Gradient Descent 算法的 Decentralized Stochastic Bilevel Gradient Descent 算法，在不同机器学习任务中实现了更好的通信复杂度和更少的通信轮数。</li>
<li>results: 实验结果表明，该算法可以在各种机器学习任务中达到更好的性能，并且在不同的环境下都能够保持稳定性。<details>
<summary>Abstract</summary>
Decentralized bilevel optimization has been actively studied in the past few years since it has widespread applications in machine learning. However, existing algorithms suffer from large communication complexity caused by the estimation of stochastic hypergradient, limiting their application to real-world tasks. To address this issue, we develop a novel decentralized stochastic bilevel gradient descent algorithm under the heterogeneous setting, which enjoys a small communication cost in each round and small communication rounds. As such, it can achieve a much better communication complexity than existing algorithms. Moreover, we extend our algorithm to the more challenging decentralized multi-level optimization. To the best of our knowledge, this is the first time achieving these theoretical results under the heterogeneous setting. At last, the experimental results confirm the efficacy of our algorithm.
</details>
<details>
<summary>摘要</summary>
随机二级优化在过去几年内得到了广泛研究，因为它在机器学习中有广泛的应用。然而，现有的算法受到了随机梯度估计所导致的大量通信复杂度的限制，使得它们在实际任务中应用有限。为解决这个问题，我们开发了一种新的分布式随机二级梯度下降算法，该算法在不同设备上具有小型通信成本和小型通信循环数。因此，它可以在实际任务中实现更好的通信复杂度。此外，我们还扩展了我们的算法到更加复杂的分布式多级优化问题。根据我们所知，这是在不同设备上第一次实现这些理论结果。最后，实验结果证明了我们的算法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Self-Distilled-Representation-Learning-for-Time-Series"><a href="#Self-Distilled-Representation-Learning-for-Time-Series" class="headerlink" title="Self-Distilled Representation Learning for Time Series"></a>Self-Distilled Representation Learning for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11335">http://arxiv.org/abs/2311.11335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Pieper, Konstantin Ditschuneit, Martin Genzel, Alexandra Lindt, Johannes Otterbach</li>
<li>for: 这篇论文旨在探讨时间序列资料上的自主学习，并提出了一种非对照式的方法，基于data2vec自体散布框架。</li>
<li>methods: 这篇论文使用了一种学生教师模式，预测对掩盖时间序列的隐藏表示。这种方法避免了对比例特有的强迫特征和偏见。</li>
<li>results: 论文显示了这种方法在分类和预测任务中的竞争力，与现有自主学习方法相比，在UCRC和UEA archive以及ETT和电力dataset上。<details>
<summary>Abstract</summary>
Self-supervised learning for time-series data holds potential similar to that recently unleashed in Natural Language Processing and Computer Vision. While most existing works in this area focus on contrastive learning, we propose a conceptually simple yet powerful non-contrastive approach, based on the data2vec self-distillation framework. The core of our method is a student-teacher scheme that predicts the latent representation of an input time series from masked views of the same time series. This strategy avoids strong modality-specific assumptions and biases typically introduced by the design of contrastive sample pairs. We demonstrate the competitiveness of our approach for classification and forecasting as downstream tasks, comparing with state-of-the-art self-supervised learning methods on the UCR and UEA archives as well as the ETT and Electricity datasets.
</details>
<details>
<summary>摘要</summary>
自我监督学习 для时间序列数据具有与自然语言处理和计算机视觉领域最近爆发的潜力。大多数现有的方法在这个领域都是基于对照学习，我们提出了一种概念简单又强大的非对照学习方法，基于数据2vec自适应框架。我们的方法的核心是一种学生教师模式， predicts the latent representation of an input time series from masked views of the same time series。这种策略避免了强制特定modalitiespecific的假设和偏见，通常由对照样本对的设计引入。我们在下游任务 classification和预测中示出了我们的方法的竞争力，与现有的自我监督学习方法进行比较，在UCRC和UEA文件库以及ETT和电力集成体上。
</details></li>
</ul>
<hr>
<h2 id="LABCAT-Locally-adaptive-Bayesian-optimization-using-principal-component-aligned-trust-regions"><a href="#LABCAT-Locally-adaptive-Bayesian-optimization-using-principal-component-aligned-trust-regions" class="headerlink" title="LABCAT: Locally adaptive Bayesian optimization using principal component-aligned trust regions"></a>LABCAT: Locally adaptive Bayesian optimization using principal component-aligned trust regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11328">http://arxiv.org/abs/2311.11328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aemiliusretiarius/labcat">https://github.com/aemiliusretiarius/labcat</a></li>
<li>paper_authors: E. Visser, C. E. van Daalen, J. C. Schoeman</li>
<li>for: 优化昂贵黑盒函数（black-box function）的开销成本。</li>
<li>methods: 基于信任区域的投机算法（trust-region-based BO），加入本地策略（local strategy），包括主成分对齐旋转和自适应尺度调整策略，以及自动决定相关性的 Gaussian 过程模型。</li>
<li>results: 通过对一组synthetic测试函数和COCO软件进行广泛的数值实验，显示了LABCAT算法在比较多种状态空间中的优异性，超过了一些现有的BO和黑盒优化算法。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is a popular method for optimizing expensive black-box functions. BO has several well-documented shortcomings, including computational slowdown with longer optimization runs, poor suitability for non-stationary or ill-conditioned objective functions, and poor convergence characteristics. Several algorithms have been proposed that incorporate local strategies, such as trust regions, into BO to mitigate these limitations; however, none address all of them satisfactorily. To address these shortcomings, we propose the LABCAT algorithm, which extends trust-region-based BO by adding principal-component-aligned rotation and an adaptive rescaling strategy based on the length-scales of a local Gaussian process surrogate model with automatic relevance determination. Through extensive numerical experiments using a set of synthetic test functions and the well-known COCO benchmarking software, we show that the LABCAT algorithm outperforms several state-of-the-art BO and other black-box optimization algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Learning-Rates-Improve-Generalization-But-How-Large-Are-We-Talking-About"><a href="#Large-Learning-Rates-Improve-Generalization-But-How-Large-Are-We-Talking-About" class="headerlink" title="Large Learning Rates Improve Generalization: But How Large Are We Talking About?"></a>Large Learning Rates Improve Generalization: But How Large Are We Talking About?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11303">http://arxiv.org/abs/2311.11303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekaterina Lobacheva, Eduard Pockonechnyy, Maxim Kodryan, Dmitry Vetrov</li>
<li>for:  investigate the optimal initial learning rate range for training neural networks to achieve best generalization</li>
<li>methods:  experiments in a simplified setup with precise control of the learning rate hyperparameter, and validation in a more practical setting</li>
<li>results:  find that the optimal initial learning rate range is significantly narrower than previously assumed, and provide guidelines for selecting the best learning rate range for subsequent training with a small LR or weight averaging.<details>
<summary>Abstract</summary>
Inspired by recent research that recommends starting neural networks training with large learning rates (LRs) to achieve the best generalization, we explore this hypothesis in detail. Our study clarifies the initial LR ranges that provide optimal results for subsequent training with a small LR or weight averaging. We find that these ranges are in fact significantly narrower than generally assumed. We conduct our main experiments in a simplified setup that allows precise control of the learning rate hyperparameter and validate our key findings in a more practical setting.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="From-Categories-to-Classifier-Name-Only-Continual-Learning-by-Exploring-the-Web"><a href="#From-Categories-to-Classifier-Name-Only-Continual-Learning-by-Exploring-the-Web" class="headerlink" title="From Categories to Classifier: Name-Only Continual Learning by Exploring the Web"></a>From Categories to Classifier: Name-Only Continual Learning by Exploring the Web</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11293">http://arxiv.org/abs/2311.11293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ameya Prabhu, Hasan Abed Al Kader Hammoud, Ser-Nam Lim, Bernard Ghanem, Philip H. S. Torr, Adel Bibi</li>
<li>for:  This paper explores the use of uncurated webly-supervised data for continual learning in the absence of manually annotated datasets.</li>
<li>methods: The proposed solution leverages the internet to query and download web data for image classification, and harnesses the web to create support sets that surpass state-of-the-art name-only classification.</li>
<li>results: The method achieves up to 25% boost in accuracy compared to models trained on manually annotated datasets, and presents EvoTrends, a class-incremental dataset created from the web to capture real-world trends in just minutes.<details>
<summary>Abstract</summary>
Continual Learning (CL) often relies on the availability of extensive annotated datasets, an assumption that is unrealistically time-consuming and costly in practice. We explore a novel paradigm termed name-only continual learning where time and cost constraints prohibit manual annotation. In this scenario, learners adapt to new category shifts using only category names without the luxury of annotated training data. Our proposed solution leverages the expansive and ever-evolving internet to query and download uncurated webly-supervised data for image classification. We investigate the reliability of our web data and find them comparable, and in some cases superior, to manually annotated datasets. Additionally, we show that by harnessing the web, we can create support sets that surpass state-of-the-art name-only classification that create support sets using generative models or image retrieval from LAION-5B, achieving up to 25% boost in accuracy. When applied across varied continual learning contexts, our method consistently exhibits a small performance gap in comparison to models trained on manually annotated datasets. We present EvoTrends, a class-incremental dataset made from the web to capture real-world trends, created in just minutes. Overall, this paper underscores the potential of using uncurated webly-supervised data to mitigate the challenges associated with manual data labeling in continual learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TimeSQL-Improving-Multivariate-Time-Series-Forecasting-with-Multi-Scale-Patching-and-Smooth-Quadratic-Loss"><a href="#TimeSQL-Improving-Multivariate-Time-Series-Forecasting-with-Multi-Scale-Patching-and-Smooth-Quadratic-Loss" class="headerlink" title="TimeSQL: Improving Multivariate Time Series Forecasting with Multi-Scale Patching and Smooth Quadratic Loss"></a>TimeSQL: Improving Multivariate Time Series Forecasting with Multi-Scale Patching and Smooth Quadratic Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11285">http://arxiv.org/abs/2311.11285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Site Mo, Haoxin Wang, Bixiong Li, Songhai Fan, Yuankai Wu, Xianggen Liu</li>
<li>for: 这篇论文旨在提出一个简单有效的时间序列预测框架，以便在实际世界中处理多重时间序列数据。</li>
<li>methods: 这个框架使用多尺度裁剪和平滑二项损失（SQL）来解决时间序列预测中的问题，包括处理噪音和异常值。</li>
<li>results: 根据论文的 тео리学分析和实验结果，这个框架在八个实际世界benchmark数据集上实现了新的顶峰性表现。<details>
<summary>Abstract</summary>
Time series is a special type of sequence data, a sequence of real-valued random variables collected at even intervals of time. The real-world multivariate time series comes with noises and contains complicated local and global temporal dynamics, making it difficult to forecast the future time series given the historical observations. This work proposes a simple and effective framework, coined as TimeSQL, which leverages multi-scale patching and smooth quadratic loss (SQL) to tackle the above challenges. The multi-scale patching transforms the time series into two-dimensional patches with different length scales, facilitating the perception of both locality and long-term correlations in time series. SQL is derived from the rational quadratic kernel and can dynamically adjust the gradients to avoid overfitting to the noises and outliers. Theoretical analysis demonstrates that, under mild conditions, the effect of the noises on the model with SQL is always smaller than that with MSE. Based on the two modules, TimeSQL achieves new state-of-the-art performance on the eight real-world benchmark datasets. Further ablation studies indicate that the key modules in TimeSQL could also enhance the results of other models for multivariate time series forecasting, standing as plug-and-play techniques.
</details>
<details>
<summary>摘要</summary>
时序序列是特殊类型的序列数据，一个序列实数随时间刻取得的实数随机变量序列。实际世界多变量时序序列受到噪声和复杂的地方和全局时间动态影响，这使得预测未来时序序列给历史观察数据非常困难。这项工作提出了一个简单有效的框架，命名为TimeSQL，该框架利用多尺度补做和平滑二次损失（SQL）来解决以上挑战。多尺度补做将时序序列转换为不同长度级别的二维补做，从而使得时序序列中的本地和长期相关性更加容易发现。SQL是基于理智二次kernels的，可以在运动过程中动态调整Gradient，以避免因噪声和异常值而过拟合。理论分析表明，在某些条件下，TimeSQL模型中噪声对模型的影响总是小于MSE模型。基于这两个模块，TimeSQL在八个实际世界标准数据集上实现了新的状态计算机科学中的最佳性能。进一步的减少研究表明，TimeSQL中的关键模块也可以提高其他多变量时序序列预测模型的结果，作为插件技术。
</details></li>
</ul>
<hr>
<h2 id="Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-I-Communication-Aware-Vehicle-Control"><a href="#Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-I-Communication-Aware-Vehicle-Control" class="headerlink" title="Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part I: Communication-Aware Vehicle Control"></a>Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part I: Communication-Aware Vehicle Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11281">http://arxiv.org/abs/2311.11281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Liu, Lei Lei, Kan Zheng, Xuemin, Shen</li>
<li>For: This paper proposes a joint optimization framework of multi-timescale control and communications (MTCC) based on Deep Reinforcement Learning (DRL) for autonomous driving (AD) use case, specifically for platoon control (PC).* Methods: The proposed MTCC-PC algorithm is based on DRL and decomposes the problem into a communication-aware DRL-based PC sub-problem and a control-aware DRL-based RRA sub-problem. The PC sub-problem is solved assuming an RRA policy is given, and the MTCC-PC algorithm is trained in a delayed environment generated by fine-grained embedded simulation of C-V2X communications.* Results: The MTCC-PC algorithm is compared with baseline DRL algorithms, and the results show that it outperforms the baseline algorithms in terms of PC performance under random observation delay.<details>
<summary>Abstract</summary>
An intelligent decision-making system enabled by Vehicle-to-Everything (V2X) communications is essential to achieve safe and efficient autonomous driving (AD), where two types of decisions have to be made at different timescales, i.e., vehicle control and radio resource allocation (RRA) decisions. The interplay between RRA and vehicle control necessitates their collaborative design. In this two-part paper (Part I and Part II), taking platoon control (PC) as an example use case, we propose a joint optimization framework of multi-timescale control and communications (MTCC) based on Deep Reinforcement Learning (DRL). In this paper (Part I), we first decompose the problem into a communication-aware DRL-based PC sub-problem and a control-aware DRL-based RRA sub-problem. Then, we focus on the PC sub-problem assuming an RRA policy is given, and propose the MTCC-PC algorithm to learn an efficient PC policy. To improve the PC performance under random observation delay, the PC state space is augmented with the observation delay and PC action history. Moreover, the reward function with respect to the augmented state is defined to construct an augmented state Markov Decision Process (MDP). It is proved that the optimal policy for the augmented state MDP is optimal for the original PC problem with observation delay. Different from most existing works on communication-aware control, the MTCC-PC algorithm is trained in a delayed environment generated by the fine-grained embedded simulation of C-V2X communications rather than by a simple stochastic delay model. Finally, experiments are performed to compare the performance of MTCC-PC with those of the baseline DRL algorithms.
</details>
<details>
<summary>摘要</summary>
“一个智能做出决策系统，基于车辆到所有东西（V2X）通信，是自动驾驶（AD）所需的，以达到安全和高效的驾驶。在不同时间尺度上，需要做出两种决策：车辆控制和无线资源分配（RRA）决策。它们之间的互动需要协调设计。在这两部分文章（Part I和Part II）中，以排序控制（PC）为例用 caso，我们提出了一个多个时间尺度的控制和通信（MTCC）基于深度学习（DRL）的联合优化框架。在这篇文章（Part I）中，我们首先将问题分解为一个具有通信意识的 DRL 基于 PC 子问题，以及一个具有控制意识的 DRL 基于 RRA 子问题。然后，我们专注在 PC 子问题上，假设 RRA 政策已经给出，并提出了 MTCC-PC 算法来学习一个高效的 PC 策略。为了提高 PC 性能在随机观察延迟下，PC 状态空间被扩展了，并且 PC 动作历史和观察延迟也被加入。此外，我们定义了对于扩展state的 reward 函数，以建立一个扩展state Markov Decision Process (MDP)。证明了对于扩展state MDP 的最佳策略是对于原始 PC 问题的最佳策略。与大多数现有的通信意识控制相比，MTCC-PC 算法在精细的 C-V2X 通信 fine-grained 模拟中进行训练，而不是使用一个简单的测量延迟模型。最后，我们对 MTCC-PC 与基eline DRL 算法进行比较，以评估其表现。”
</details></li>
</ul>
<hr>
<h2 id="Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-II-Control-Aware-Radio-Resource-Allocation"><a href="#Multi-Timescale-Control-and-Communications-with-Deep-Reinforcement-Learning-–-Part-II-Control-Aware-Radio-Resource-Allocation" class="headerlink" title="Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part II: Control-Aware Radio Resource Allocation"></a>Multi-Timescale Control and Communications with Deep Reinforcement Learning – Part II: Control-Aware Radio Resource Allocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11280">http://arxiv.org/abs/2311.11280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Lei, Tong Liu, Kan Zheng, Xuemin, Shen</li>
<li>for: 这篇论文的目的是为了解决 Cellular Vehicle-to-Everything (C-V2X) 系统中的多时间步骤控制和通信 (MTCC) 问题。</li>
<li>methods: 这篇论文使用 Deep Reinforcement Learning (DRL) 技术来解决 MTCC 问题，并将其分为两个子问题：通信适应 Deep Reinforcement Learning (DRL) 基本的均衡控制 (PC) 和控制适应 DRL 基本的射频资源分配 (RRA)。</li>
<li>results: 这篇论文提出了一个名为 MTCC-PC 的算法，用于学习 PC 政策，并且将 RRA 问题视为一个给定 PC 政策的问题，提出了 MTCC-RRA 算法。此外，这篇论文还使用了 reward shaping 和 reward backpropagation prioritized experience replay (RBPER) 技术来有效地解决多代理和罕见的奖励问题。<details>
<summary>Abstract</summary>
In Part I of this two-part paper (Multi-Timescale Control and Communications with Deep Reinforcement Learning -- Part I: Communication-Aware Vehicle Control), we decomposed the multi-timescale control and communications (MTCC) problem in Cellular Vehicle-to-Everything (C-V2X) system into a communication-aware Deep Reinforcement Learning (DRL)-based platoon control (PC) sub-problem and a control-aware DRL-based radio resource allocation (RRA) sub-problem. We focused on the PC sub-problem and proposed the MTCC-PC algorithm to learn an optimal PC policy given an RRA policy. In this paper (Part II), we first focus on the RRA sub-problem in MTCC assuming a PC policy is given, and propose the MTCC-RRA algorithm to learn the RRA policy. Specifically, we incorporate the PC advantage function in the RRA reward function, which quantifies the amount of PC performance degradation caused by observation delay. Moreover, we augment the state space of RRA with PC action history for a more well-informed RRA policy. In addition, we utilize reward shaping and reward backpropagation prioritized experience replay (RBPER) techniques to efficiently tackle the multi-agent and sparse reward problems, respectively. Finally, a sample- and computational-efficient training approach is proposed to jointly learn the PC and RRA policies in an iterative process. In order to verify the effectiveness of the proposed MTCC algorithm, we performed experiments using real driving data for the leading vehicle, where the performance of MTCC is compared with those of the baseline DRL algorithms.
</details>
<details>
<summary>摘要</summary>
在这两部分文章中（多时间尺度控制和通信 WITH Deep Reinforcement Learning -- 部分 I：通信意识排队控制），我们将多时间尺度控制和通信（MTCC）问题分解为一个基于 Deep Reinforcement Learning（DRL）的排队控制（PC）子问题和一个控制意识DRL的广播资源分配（RRA）子问题。我们主要关注PC子问题，并提出了MTCC-PC算法，以学习一个最佳PC策略。在这篇文章中（部分 II），我们首先关注RRA子问题，假设PC策略已知，并提出了MTCC-RRA算法，以学习RRA策略。具体来说，我们将PC优势函数 incorporated into RRA reward function，该函数量化了由观测延迟引起的PC性能下降。此外，我们将RRA状态空间扩展为包括PC动作历史，以更好地优化RRA策略。此外，我们使用 reward shaping和reward backpropagation prioritized experience replay（RBPER）技术，以有效地解决多机器人和罕见奖励问题。最后，我们提出了一种效率的训练方法，以同时学习PC和RRA策略。为验证我们提出的MTCC算法的有效性，我们使用实际驾驶数据进行了实验，并与基eline DRL算法进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-quantification-for-noisy-inputs-outputs-in-physics-informed-neural-networks-and-neural-operators"><a href="#Uncertainty-quantification-for-noisy-inputs-outputs-in-physics-informed-neural-networks-and-neural-operators" class="headerlink" title="Uncertainty quantification for noisy inputs-outputs in physics-informed neural networks and neural operators"></a>Uncertainty quantification for noisy inputs-outputs in physics-informed neural networks and neural operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11262">http://arxiv.org/abs/2311.11262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongren Zou, Xuhui Meng, George Em Karniadakis</li>
<li>for: 本文主要针对的是科学机器学习（SciML）中的不确定性评估（UQ）问题，特别是physics-informed neural networks（PINNs）和neural operators（NOs）中的输入不确定性。</li>
<li>methods: 本文提出了一种 bayesian 方法来评估 PINNs 和 NOs 中的输入不确定性。这种方法可以轻松地结合 PINNs 和 NOs，使其能够处理含有噪声的输入和输出。</li>
<li>results: 本文的结果表明，bayesian 方法可以有效地评估 PINNs 和 NOs 中的输入不确定性，并且可以在含有噪声的输入和输出的情况下提供可靠的结果。<details>
<summary>Abstract</summary>
Uncertainty quantification (UQ) in scientific machine learning (SciML) becomes increasingly critical as neural networks (NNs) are being widely adopted in addressing complex problems across various scientific disciplines. Representative SciML models are physics-informed neural networks (PINNs) and neural operators (NOs). While UQ in SciML has been increasingly investigated in recent years, very few works have focused on addressing the uncertainty caused by the noisy inputs, such as spatial-temporal coordinates in PINNs and input functions in NOs. The presence of noise in the inputs of the models can pose significantly more challenges compared to noise in the outputs of the models, primarily due to the inherent nonlinearity of most SciML algorithms. As a result, UQ for noisy inputs becomes a crucial factor for reliable and trustworthy deployment of these models in applications involving physical knowledge. To this end, we introduce a Bayesian approach to quantify uncertainty arising from noisy inputs-outputs in PINNs and NOs. We show that this approach can be seamlessly integrated into PINNs and NOs, when they are employed to encode the physical information. PINNs incorporate physics by including physics-informed terms via automatic differentiation, either in the loss function or the likelihood, and often take as input the spatial-temporal coordinate. Therefore, the present method equips PINNs with the capability to address problems where the observed coordinate is subject to noise. On the other hand, pretrained NOs are also commonly employed as equation-free surrogates in solving differential equations and Bayesian inverse problems, in which they take functions as inputs. The proposed approach enables them to handle noisy measurements for both input and output functions with UQ.
</details>
<details>
<summary>摘要</summary>
科学机器学习（SciML）中的不确定性评估（UQ）在科学领域中日益成为关键，因为神经网络（NN）在解决复杂问题上广泛应用。代表性的SciML模型包括物理学习神经网络（PINNs）和神经运算（NOs）。虽然UQ在SciML中已有很多研究，但很少研究涉及到由输入噪声引起的不确定性，如PINNs中的空间-时间坐标和NOs中的输入函数。噪声在模型输入中存在的问题比噪声在模型输出中更加复杂，主要是因为大多数SciML算法具有非线性性。因此，对于可靠和可信worthy的应用，UQ для噪声输入成为关键因素。为此，我们提出了一种 bayesian 方法来评估 PINNs 和 NOs 中由噪声输入引起的不确定性。我们示示该方法可以轻松地 интеGRATE 到 PINNs 和 NOs 中，当它们用于编码物理信息时。PINNs 通过自动微分来包含物理约束，通常是在损失函数或概率函数中，以及输入空间-时间坐标。因此，当前的方法允许 PINNs 解决受噪声影响的问题。另一方面，预训练的 NOs 通常也被用作 equation-free 代表物和抽象问题的解决方案，它们通常接受函数作为输入。我们的方法允许它们处理噪声测量数据，包括输入和输出函数的不确定性评估。
</details></li>
</ul>
<hr>
<h2 id="BOIS-Bayesian-Optimization-of-Interconnected-Systems"><a href="#BOIS-Bayesian-Optimization-of-Interconnected-Systems" class="headerlink" title="BOIS: Bayesian Optimization of Interconnected Systems"></a>BOIS: Bayesian Optimization of Interconnected Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11254">http://arxiv.org/abs/2311.11254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo D. González, Victor M. Zavala</li>
<li>for: 用于优化昂贵的样本系统的全面优化</li>
<li>methods: 使用 Gaussian processes（GP）模型来 caracterize 模型不确定性，并将其用于导引学习和搜索过程</li>
<li>results: 使用 composite functions 可以利用结构知识（例如物理和稀谱连接），并且可以高效地使用 GP 模型来描述 composite functions，从而实现更好的优化性能和准确地捕捉 composite functions 的统计特征。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) has proven to be an effective paradigm for the global optimization of expensive-to-sample systems. One of the main advantages of BO is its use of Gaussian processes (GPs) to characterize model uncertainty which can be leveraged to guide the learning and search process. However, BO typically treats systems as black-boxes and this limits the ability to exploit structural knowledge (e.g., physics and sparse interconnections). Composite functions of the form $f(x, y(x))$, wherein GP modeling is shifted from the performance function $f$ to an intermediate function $y$, offer an avenue for exploiting structural knowledge. However, the use of composite functions in a BO framework is complicated by the need to generate a probability density for $f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is nonlinear it is not possible to obtain a closed-form expression). Previous work has handled this issue using sampling techniques; these are easy to implement and flexible but are computationally intensive. In this work, we introduce a new paradigm which allows for the efficient use of composite functions in BO; this uses adaptive linearizations of $f$ to obtain closed-form expressions for the statistical moments of the composite function. We show that this simple approach (which we call BOIS) enables the exploitation of structural knowledge, such as that arising in interconnected systems as well as systems that embed multiple GP models and combinations of physics and GP models. Using a chemical process optimization case study, we benchmark the effectiveness of BOIS against standard BO and sampling approaches. Our results indicate that BOIS achieves performance gains and accurately captures the statistics of composite functions.
</details>
<details>
<summary>摘要</summary>
bayesian 优化（BO）已经证明是一种有效的全面优化方法，特别是在高效尝试系统上。BO的一个主要优势是使用 Gaussian processes（GP）来描述模型不确定性，这可以用来导引学习和搜索过程。然而，BO通常会将系统看作黑盒子，这限制了能够利用结构知识（例如物理和稀疏连接）。使用 $f(x, y(x)) $ 的复合函数，其中 GP 模型从性能函数 $f$ 转移到中间函数 $y$，提供了利用结构知识的途径。然而，在 BO 框架中使用复合函数增加了生成 $f$ 的概率密度的问题，特别是当 $f$ 非线性时，不能得到关闭式表达。先前的工作通过抽样技术解决这个问题，这些技术容易实现但是计算昂贵。在这篇文章中，我们介绍了一种新的方法，即 BOIS，它使得在 BO 框架中高效地使用复合函数。我们利用了 $f$ 的适应线性化来获得复合函数的统计 moments 的关闭式表达。我们的结果表明，BOIS 可以在化学过程优化案例中实现性能提升和准确地捕捉复合函数的统计特征。
</details></li>
</ul>
<hr>
<h2 id="A-Universal-Framework-for-Accurate-and-Efficient-Geometric-Deep-Learning-of-Molecular-Systems"><a href="#A-Universal-Framework-for-Accurate-and-Efficient-Geometric-Deep-Learning-of-Molecular-Systems" class="headerlink" title="A Universal Framework for Accurate and Efficient Geometric Deep Learning of Molecular Systems"></a>A Universal Framework for Accurate and Efficient Geometric Deep Learning of Molecular Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11228">http://arxiv.org/abs/2311.11228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XieResearchGroup/Physics-aware-Multiplex-GNN">https://github.com/XieResearchGroup/Physics-aware-Multiplex-GNN</a></li>
<li>paper_authors: Shuo Zhang, Yang Liu, Lei Xie</li>
<li>for: 本研究旨在提供一个通用的框架，用于精确地和有效地学习不同类型和大小的分子的表现，并在各种分子系统中实现高精度和高效率。</li>
<li>methods: 本研究使用了几何深度学习，特别是几何神经网络，以实现分子表现的学习。它还将分子表现模型化为几何学问中的力学问题，以实现更好地模型分子的本质和相互作用。</li>
<li>results: 本研究的实验结果显示，PAMNet在三种不同的学习任务中都能够实现高精度和高效率。具体来说，PAMNet在小分子性质、RNA 3D结构和蛋白质-药物结合亲和力等三种任务中，都能够超越现有的基elines。<details>
<summary>Abstract</summary>
Molecular sciences address a wide range of problems involving molecules of different types and sizes and their complexes. Recently, geometric deep learning, especially Graph Neural Networks, has shown promising performance in molecular science applications. However, most existing works often impose targeted inductive biases to a specific molecular system, and are inefficient when applied to macromolecules or large-scale tasks, thereby limiting their applications to many real-world problems. To address these challenges, we present PAMNet, a universal framework for accurately and efficiently learning the representations of three-dimensional (3D) molecules of varying sizes and types in any molecular system. Inspired by molecular mechanics, PAMNet induces a physics-informed bias to explicitly model local and non-local interactions and their combined effects. As a result, PAMNet can reduce expensive operations, making it time and memory efficient. In extensive benchmark studies, PAMNet outperforms state-of-the-art baselines regarding both accuracy and efficiency in three diverse learning tasks: small molecule properties, RNA 3D structures, and protein-ligand binding affinities. Our results highlight the potential for PAMNet in a broad range of molecular science applications.
</details>
<details>
<summary>摘要</summary>
分子科学研究着眼于各种不同类型和大小的分子和其复合物的问题。近年来，深度学习，特别是图 Nuevos Networks（GNN）在分子科学应用中表现了承诺的能力。然而，大多数现有工作通常会对特定分子系统强制投入预设目标，因此在巨大分子或大规模任务中不具有广泛应用的能力。为解决这些挑战，我们提出了PAMNet，一个通用的框架，可以高精度地和高效地学习三维分子的表示。 draw inspiration from molecular mechanics，PAMNet采用物理学习来显式地模型本地和非本地交互，并将其相互作用的结果相互作用。这使得PAMNet可以降低成本，提高时间和内存效率。在广泛的审核实验中，PAMNet在三种多样化学习任务中（小分子性质、RNA 3D结构和蛋白质-药物结合稳定性）都与当前基elines相比，在准确性和效率方面具有显著的优势。这些结果表明PAMNet在分子科学应用中具有广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="TextGuard-Provable-Defense-against-Backdoor-Attacks-on-Text-Classification"><a href="#TextGuard-Provable-Defense-against-Backdoor-Attacks-on-Text-Classification" class="headerlink" title="TextGuard: Provable Defense against Backdoor Attacks on Text Classification"></a>TextGuard: Provable Defense against Backdoor Attacks on Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11225">http://arxiv.org/abs/2311.11225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-secure/textguard">https://github.com/ai-secure/textguard</a></li>
<li>paper_authors: Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song</li>
<li>for: 防止机器学习模型中的后门攻击（backdoor attacks），以提高安全应用中的安全性。</li>
<li>methods:  TextGuard 使用分割训练数据（backdoored）为多个子训练集（sub-training sets），然后从每个子训练集训练基本分类器， ensemble 提供最终预测。 我们 theoretically prove 当背门关键（backdoor trigger）的长度在某些阈值以下时，TextGuard 可以保证其预测结果不受训练和测试输入中的背门关键影响。</li>
<li>results: TextGuard 在三个 benchmark 文本分类任务上显示出超过现有认证防护措施的认证率，并且提出了更多的实际性提升策略。 comparisons 显示 TextGuard 在对多个后门攻击的比较中与现有的实际防护措施相比较高。 code 和数据可以在 <a target="_blank" rel="noopener" href="https://github.com/AI-secure/TextGuard">https://github.com/AI-secure/TextGuard</a> 上取得。<details>
<summary>Abstract</summary>
Backdoor attacks have become a major security threat for deploying machine learning models in security-critical applications. Existing research endeavors have proposed many defenses against backdoor attacks. Despite demonstrating certain empirical defense efficacy, none of these techniques could provide a formal and provable security guarantee against arbitrary attacks. As a result, they can be easily broken by strong adaptive attacks, as shown in our evaluation. In this work, we propose TextGuard, the first provable defense against backdoor attacks on text classification. In particular, TextGuard first divides the (backdoored) training data into sub-training sets, achieved by splitting each training sentence into sub-sentences. This partitioning ensures that a majority of the sub-training sets do not contain the backdoor trigger. Subsequently, a base classifier is trained from each sub-training set, and their ensemble provides the final prediction. We theoretically prove that when the length of the backdoor trigger falls within a certain threshold, TextGuard guarantees that its prediction will remain unaffected by the presence of the triggers in training and testing inputs. In our evaluation, we demonstrate the effectiveness of TextGuard on three benchmark text classification tasks, surpassing the certification accuracy of existing certified defenses against backdoor attacks. Furthermore, we propose additional strategies to enhance the empirical performance of TextGuard. Comparisons with state-of-the-art empirical defenses validate the superiority of TextGuard in countering multiple backdoor attacks. Our code and data are available at https://github.com/AI-secure/TextGuard.
</details>
<details>
<summary>摘要</summary>
《文本攻击：一种新的安全问题》现代机器学习模型在安全敏感应用中部署时，针对文本攻击已成为主要的安全问题。现有的研究努力提出了许多防御策略，尽管在实验室中得到了一定的效果，但是无法提供正式和可证明的安全保证。因此，它们可以被强大的适应性攻击打砸，如我们的评估结果所示。在这项工作中，我们提出了 TextGuard，这是第一个可证明的文本攻击防御策略。TextGuard的核心思想是将（被恶意修改的）训练数据分成子训练集，每个子训练集由每个训练句子分成多个子句。这种分割确保了大多数子训练集不包含恶意触发符。然后，从每个子训练集中训练基础分类器，并将它们的 ensemble 提供最终预测。我们理论上证明，当恶意触发符的长度在某个阈值之下时，TextGuard 的预测将不受训练和测试输入中恶意触发符的影响。在我们的评估中，我们证明了 TextGuard 在三个标准文本分类任务上的效果，超过了现有的证书防御策略对恶意攻击的证书率。此外，我们还提出了更多的实际性优化策略，并与现有的实际防御策略进行比较，证明 TextGuard 在对多个恶意攻击的防御方面具有superiority。我们的代码和数据可以在 GitHub 上找到：https://github.com/AI-secure/TextGuard。
</details></li>
</ul>
<hr>
<h2 id="Robust-Network-Slicing-Multi-Agent-Policies-Adversarial-Attacks-and-Defensive-Strategies"><a href="#Robust-Network-Slicing-Multi-Agent-Policies-Adversarial-Attacks-and-Defensive-Strategies" class="headerlink" title="Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies"></a>Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and Defensive Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11206">http://arxiv.org/abs/2311.11206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Wang, M. Cenk Gursoy, Senem Velipasalar</li>
<li>for: 本文提出了一种基于多智能深度学习（深度RL）的网络切片框架，用于在多个基站和多个用户的动态环境中实现网络切片。特别是，我们提出了一种多actor多评估器（MACC）的深度RL框架，在其中，actor是实现为指针网络，以适应输入的变化维度。</li>
<li>methods: 我们提出了一种基于深度RL的网络切片攻击方法，包括设计了一个具有有限制的优化功能和有限制的功率预算的攻击者。我们还解决了攻击者的找寻位置优化和通信频道优化问题 via 深度RL。</li>
<li>results: 我们通过实验表明，提出的攻击者可以在不知情的情况下，对网络切片的性能产生显著的降低。此外，我们还提出了一种基于 Nash 平衡的策略ensemble 混合策略profile，用于网络切片和攻击者之间的对抗。我们通过在实验中应用该策略ensemble algorithm，显示其效果。<details>
<summary>Abstract</summary>
In this paper, we present a multi-agent deep reinforcement learning (deep RL) framework for network slicing in a dynamic environment with multiple base stations and multiple users. In particular, we propose a novel deep RL framework with multiple actors and centralized critic (MACC) in which actors are implemented as pointer networks to fit the varying dimension of input. We evaluate the performance of the proposed deep RL algorithm via simulations to demonstrate its effectiveness. Subsequently, we develop a deep RL based jammer with limited prior information and limited power budget. The goal of the jammer is to minimize the transmission rates achieved with network slicing and thus degrade the network slicing agents' performance. We design a jammer with both listening and jamming phases and address jamming location optimization as well as jamming channel optimization via deep RL. We evaluate the jammer at the optimized location, generating interference attacks in the optimized set of channels by switching between the jamming phase and listening phase. We show that the proposed jammer can significantly reduce the victims' performance without direct feedback or prior knowledge on the network slicing policies. Finally, we devise a Nash-equilibrium-supervised policy ensemble mixed strategy profile for network slicing (as a defensive measure) and jamming. We evaluate the performance of the proposed policy ensemble algorithm by applying on the network slicing agents and the jammer agent in simulations to show its effectiveness.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一个多代理深度学习游戏（深度RL）框架，用于在多个基站和多个用户之间进行网络剖析。特别是，我们提出了一种新的深度RL框架，称为多actor和中央评估器（MACC）。在这种框架中，代理是实现为指针网络，以适应输入的变化维度。我们通过实验评估了提出的深度RL算法的性能，以示其效果。然后，我们开发了一个基于深度RL的干扰器，具有限制的先验知识和能量预算。干扰器的目标是降低网络剖析代理的性能，以达到干扰网络剖析的目的。我们设计了干扰器的 listening和干扰阶段，并处理干扰通道优化和干扰位置优化via深度RL。我们在优化的位置上测试了干扰器，在优化的set of channels上发生了干扰攻击。我们发现，我们提出的干扰器可以在不知道网络剖析策略的情况下，对受害者的性能进行显著降低。最后，我们设计了一种以战略混合为基础的纳什平衡监督策略套件，用于防御网络剖析和干扰。我们通过在网络剖析代理和干扰器代理上应用该策略套件来评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Scale-free-networks-improved-inference"><a href="#Scale-free-networks-improved-inference" class="headerlink" title="Scale-free networks: improved inference"></a>Scale-free networks: improved inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11200">http://arxiv.org/abs/2311.11200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Nixon Jerez-Lillo, Francisco A. Rodrigues, Pedro L. Ramos</li>
<li>for: 这 paper 是 investigating whether the degree distribution of a network follows a power-law distribution, and providing improved methods for estimating the model parameters using Bayesian inference.</li>
<li>methods: 这 paper 使用 Bayesian inference 来 obtain accurate estimates and precise credibility intervals for the parameters of both continuous and discrete distributions.</li>
<li>results: 该 paper 的 results indicate that the proposed method is more suitable in practice, as it yields a frequency of acceptance close to the specified nominal level, and provides a more accurate discernment of whether a network or any other dataset adheres to a power-law distribution.<details>
<summary>Abstract</summary>
The power-law distribution plays a crucial role in complex networks as well as various applied sciences. Investigating whether the degree distribution of a network follows a power-law distribution is an important concern. The commonly used inferential methods for estimating the model parameters often yield biased estimates, which can lead to the rejection of the hypothesis that a model conforms to a power-law. In this paper, we discuss improved methods that utilize Bayesian inference to obtain accurate estimates and precise credibility intervals. The inferential methods are derived for both continuous and discrete distributions. These methods reveal that objective Bayesian approaches return nearly unbiased estimates for the parameters of both models. Notably, in the continuous case, we identify an explicit posterior distribution. This work enhances the power of goodness-of-fit tests, enabling us to accurately discern whether a network or any other dataset adheres to a power-law distribution. We apply the proposed approach to fit degree distributions for more than 5,000 synthetic networks and over 3,000 real networks. The results indicate that our method is more suitable in practice, as it yields a frequency of acceptance close to the specified nominal level.
</details>
<details>
<summary>摘要</summary>
“带法分布在复杂网络以及各种应用科学中发挥重要作用。确定网络度分布是否遵循带法分布是一项重要的问题。常用的推断方法可能会导致偏向估计，这可能会导致拒绝带法分布模型遵循的假设。在这篇论文中，我们讨论了改进的方法，使用 bayesian 推断来获得准确的估计和准确的信任范围。这些方法适用于连续和整数分布。这些方法表明，对 bayesian 方法进行 objective 推断可以获得准确的参数估计。在连续 случа子中，我们确定了一个Explicit posterior distribution。这项工作使得好宜度适应测试更加准确地判断网络或任何其他数据集是否遵循带法分布。我们对5000个Synthetic网络和3000个实际网络进行适用。结果表明，我们的方法在实践中更加适用，其频率接受接近指定的正常水平。”
</details></li>
</ul>
<hr>
<h2 id="Testing-with-Non-identically-Distributed-Samples"><a href="#Testing-with-Non-identically-Distributed-Samples" class="headerlink" title="Testing with Non-identically Distributed Samples"></a>Testing with Non-identically Distributed Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11194">http://arxiv.org/abs/2311.11194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Shivam Garg, Chirag Pabbaraju, Kirankumar Shiragur, Gregory Valiant</li>
<li>for: 本文研究了在独立不同分布下的非线性样本计数和估计问题。特别是，我们考虑了以下分布测试框架：假设有一组分布在整数支持上，大小为$k$，我们从每个分布中采样$c$次，然后从这些采样中学习或测试一个平均分布，即$\textbf{p}_{\text{avg}$。这种设置符合许多实际应用中的各种各样的Entity不同的情况，例如个人、不同时间 periods、不同空间数据源等。</li>
<li>methods: 我们使用了非线性样本计数和估计方法，包括在$c&#x3D;1$个分布下学习$\textbf{p}_{\text{avg}$，以及在$c \ge 2$个分布下进行uniformity测试和非标准测试。</li>
<li>results: 我们得到了以下结果：在$c&#x3D;1$个分布下，需要$\Theta(k&#x2F;\epsilon^2)$样本来准确地学习$\textbf{p}_{\text{avg}$，而在$c \ge 2$个分布下，只需要$O(\sqrt{k}&#x2F;\epsilon^2 + 1&#x2F;\epsilon^4)$样本来进行uniformity测试和非标准测试。此外，我们还证明了在$c&#x3D;2$个分布下，存在一个常数$\rho &gt; 0$，使得即使有$\rho k$样本，也无法进行uniformity测试。<details>
<summary>Abstract</summary>
We examine the extent to which sublinear-sample property testing and estimation applies to settings where samples are independently but not identically distributed. Specifically, we consider the following distributional property testing framework: Suppose there is a set of distributions over a discrete support of size $k$, $\textbf{p}_1, \textbf{p}_2,\ldots,\textbf{p}_T$, and we obtain $c$ independent draws from each distribution. Suppose the goal is to learn or test a property of the average distribution, $\textbf{p}_{\mathrm{avg}$. This setup models a number of important practical settings where the individual distributions correspond to heterogeneous entities -- either individuals, chronologically distinct time periods, spatially separated data sources, etc. From a learning standpoint, even with $c=1$ samples from each distribution, $\Theta(k/\varepsilon^2)$ samples are necessary and sufficient to learn $\textbf{p}_{\mathrm{avg}$ to within error $\varepsilon$ in TV distance. To test uniformity or identity -- distinguishing the case that $\textbf{p}_{\mathrm{avg}$ is equal to some reference distribution, versus has $\ell_1$ distance at least $\varepsilon$ from the reference distribution, we show that a linear number of samples in $k$ is necessary given $c=1$ samples from each distribution. In contrast, for $c \ge 2$, we recover the usual sublinear sample testing of the i.i.d. setting: we show that $O(\sqrt{k}/\varepsilon^2 + 1/\varepsilon^4)$ samples are sufficient, matching the optimal sample complexity in the i.i.d. case in the regime where $\varepsilon \ge k^{-1/4}$. Additionally, we show that in the $c=2$ case, there is a constant $\rho > 0$ such that even in the linear regime with $\rho k$ samples, no tester that considers the multiset of samples (ignoring which samples were drawn from the same $\textbf{p}_i$) can perform uniformity testing.
</details>
<details>
<summary>摘要</summary>
我们研究对于不同分布的独立样本集合中的输入检测和估计的应用。具体来说，我们考虑以下分布测试框架：我们有一集合 $\textbf{p}_1, \textbf{p}_2, \ldots, \textbf{p}_T$ 中的分布，每个分布都是在有限的类别支持 $\left[k\right]$ 上的。我们从每个分布中获得 $c$ 个独立的样本，并想要检测或学习这些分布的平均分布 $\textbf{p}_{\text{avg}$ 的性质。这个设定模拟了许多实际上重要的问题，例如对于各个分布的不同个体、不同时间期、不同空间位置等。从学习角度来看，就算每个分布只有一个样本，我们还需要 $\Theta\left(\frac{k}{\varepsilon^2}\right)$ 个样本来学习 $\textbf{p}_{\text{avg}$ 到误差 $\varepsilon$ 以内。对于检测均匀性或同一性，我们显示了一个线性的样本数量是必要的，即 $\Theta\left(k\right)$ 个样本，只要有 $c=1$ 个样本来自每个分布。在对比之下，如果 $c \geq 2$，我们可以回复到传统的非线性样本测试情况：我们显示了 $O\left(\sqrt{\frac{k}{\varepsilon^2} + \frac{1}{\varepsilon^4}\right)$ 个样本是充足的，与 i.i.d. 情况的优秀样本复杂度成比例。此外，我们还显示了在 $c=2$ 情况下，存在一个常数 $\rho > 0$，使得甚至在 $\rho k$ 个样本的情况下，不可能将多个样本视为同一个分布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.LG_2023_11_19/" data-id="clp8zxrbd00vdn6887pds3at8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/19/cs.CL_2023_11_19/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-11-19
        
      </div>
    </a>
  
  
    <a href="/2023/11/19/eess.IV_2023_11_19/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-11-19</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
