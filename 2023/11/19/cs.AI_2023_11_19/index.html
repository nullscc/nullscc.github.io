
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-11-19 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="LLM aided semi-supervision for Extractive Dialog Summarization paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11462 repo_url: None paper_authors: Nishant Mishra, Gaurav Sahu, Iacer Calixto, Ameen Abu-Hanna, Iss">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-11-19">
<meta property="og:url" content="https://nullscc.github.io/2023/11/19/cs.AI_2023_11_19/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="LLM aided semi-supervision for Extractive Dialog Summarization paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11462 repo_url: None paper_authors: Nishant Mishra, Gaurav Sahu, Iacer Calixto, Ameen Abu-Hanna, Iss">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-19T12:00:00.000Z">
<meta property="article:modified_time" content="2023-11-21T10:06:15.420Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.AI_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T12:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-11-19
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="LLM-aided-semi-supervision-for-Extractive-Dialog-Summarization"><a href="#LLM-aided-semi-supervision-for-Extractive-Dialog-Summarization" class="headerlink" title="LLM aided semi-supervision for Extractive Dialog Summarization"></a>LLM aided semi-supervision for Extractive Dialog Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11462">http://arxiv.org/abs/2311.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Mishra, Gaurav Sahu, Iacer Calixto, Ameen Abu-Hanna, Issam H. Laradji</li>
<li>for: 提高 chat 对话摘要的质量</li>
<li>methods: 使用 state-of-the-art 大语言模型（LLMs）生成对话 pseudo-labels，然后使用这些 pseudo-labels 微调一个 chat 摘要模型，从而将大型 LLM 中的知识传递到一个更小的专业化模型中</li>
<li>results: 在 \tweetsumm 数据集上测试，只使用 10% 原始标注数据集可以达到 65.9&#x2F;57.0&#x2F;61.0 ROUGE-1&#x2F;-2&#x2F;-L，而当前状态之最高得到 65.16&#x2F;55.81&#x2F;64.37 ROUGE-1&#x2F;-2&#x2F;-L，即在最差情况下（即 ROUGE-L）仍能保持 94.7% 的性能，使用只有 10% 的数据。<details>
<summary>Abstract</summary>
Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a smaller specialized model. We demonstrate our method on the \tweetsumm dataset, and show that using 10\% of the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case (i.e., ROUGE-L) we still effectively retain 94.7% of the performance while using only 10% of the data.
</details>
<details>
<summary>摘要</summary>
通常来说，生成高质量的对话摘要需要大量标注数据。我们提出了一种方法，可以高效地使用无标注数据来抽取对话摘要。在我们的方法中，我们将摘要视为问答问题，使用当前最好的大语言模型（LLM）生成对话中的 pseudo-标签。然后，我们使用这些 pseudo-标签来练化一个对话摘要模型，从而将大型LLM中的知识传递到一个更小的专门模型中。我们在 \tweetsumm 数据集上进行了实验，并证明了使用10%的原始标注数据集可以 дости到65.9/57.0/61.0 ROUGE-1/-2/-L，而当前状态的训练集全部数据集上的最佳性能为65.16/55.81/64.37 ROUGE-1/-2/-L。换句话说，在最差情况下（即 ROUGE-L），我们仍然可以有效地保留94.7%的性能，只用10%的数据。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Public-Perceptions-Machine-Learning-Based-Sentiment-Analysis-of-COVID-19-Vaccines-in-India"><a href="#Unveiling-Public-Perceptions-Machine-Learning-Based-Sentiment-Analysis-of-COVID-19-Vaccines-in-India" class="headerlink" title="Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India"></a>Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11435">http://arxiv.org/abs/2311.11435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milind Gupta, Abhishek Kaushik<br>for: 这个研究旨在了解印度人民对COVID-19疫苗的看法，以便在极度狭隘的国家中成功进行疫苗接种。methods: 这个研究使用数据挖掘技术分析Reddit数据，以评估印度网民对COVID-19疫苗的态度。Python的Text Blob库用于注释评估评论的总体情感。results: 研究结果显示，大多数Reddit用户在印度表现出中性的态度，这对印度政府为了接种大量人口而做出的努力提出了挑战。<details>
<summary>Abstract</summary>
In March 2020, the World Health Organisation declared COVID-19 a global pandemic as it spread to nearly every country. By mid-2021, India had introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure successful vaccination in a densely populated country like India, understanding public sentiment was crucial. Social media, particularly Reddit with over 430 million users, played a vital role in disseminating information. This study employs data mining techniques to analyze Reddit data and gauge Indian sentiments towards COVID-19 vaccines. Using Python's Text Blob library, comments are annotated to assess general sentiments. Results show that most Reddit users in India expressed neutrality about vaccination, posing a challenge for the Indian government's efforts to vaccinate a significant portion of the population.
</details>
<details>
<summary>摘要</summary>
在2020年3月，世界卫生组织宣布COVID-19为全球大流行，这种疾病已经蔓延到大多数国家。到2021年中期，印度已经推出了三种疫苗：Covishield、Covaxin和Sputnik。为了在印度的高度紧张人口 circumstance 下成功进行疫苗接种，了解公众情绪非常重要。社交媒体，特别是Reddit，拥有超过430万用户，在传播信息方面发挥了关键性的作用。本研究使用数据挖掘技术来分析Reddit数据，评估印度用户对COVID-19疫苗的情感。使用Python的Text Blob库，评论被标注以评估总体情感。结果显示，大多数Reddit用户在印度表达中性对于接种疫苗的态度，这对印度政府的大规模接种计划 pose 了挑战。
</details></li>
</ul>
<hr>
<h2 id="Appearance-Codes-using-Joint-Embedding-Learning-of-Multiple-Modalities"><a href="#Appearance-Codes-using-Joint-Embedding-Learning-of-Multiple-Modalities" class="headerlink" title="Appearance Codes using Joint Embedding Learning of Multiple Modalities"></a>Appearance Codes using Joint Embedding Learning of Multiple Modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11427">http://arxiv.org/abs/2311.11427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edogariu/alex-zhang">https://github.com/edogariu/alex-zhang</a></li>
<li>paper_authors: Alex Zhang, Evan Dogariu</li>
<li>for: 提高日夜场景变换的效率和质量</li>
<li>methods: 使用对比损失约束来学习场景的结构和外观空间</li>
<li>results: 可以使用日天外观码生成夜晚场景，而不需要再进行优化迭代Here’s a breakdown of each point:</li>
<li>for: The paper aims to improve the efficiency and quality of scene transformation by learning a joint embedding space for the appearance and structure of the scene.</li>
<li>methods: The proposed framework uses a contrastive loss constraint between different modalities to learn the joint embedding space.</li>
<li>results: The proposed method can generate new renders of night-time photos using day-time appearance codes without additional optimization iterations, and achieves generations of similar quality without learning appearance codes for any unseen images on inference.<details>
<summary>Abstract</summary>
The use of appearance codes in recent work on generative modeling has enabled novel view renders with variable appearance and illumination, such as day-time and night-time renders of a scene. A major limitation of this technique is the need to re-train new appearance codes for every scene on inference, so in this work we address this problem proposing a framework that learns a joint embedding space for the appearance and structure of the scene by enforcing a contrastive loss constraint between different modalities. We apply our framework to a simple Variational Auto-Encoder model on the RADIATE dataset \cite{sheeny2021radiate} and qualitatively demonstrate that we can generate new renders of night-time photos using day-time appearance codes without additional optimization iterations. Additionally, we compare our model to a baseline VAE that uses the standard per-image appearance code technique and show that our approach achieves generations of similar quality without learning appearance codes for any unseen images on inference.
</details>
<details>
<summary>摘要</summary>
在最近的生成模型工作中，使用的外观编码已经实现了变量的外观和照明，如场景的日间和夜间视图。然而，这种技术的主要限制是需要在推理时重新训练新的外观编码，因此在这种情况下我们提出了一个框架，该框架通过对不同模式之间的冲突损失进行约束，学习场景的外观和结构的联合嵌入空间。我们在RADIATE数据集\cite{sheeny2021radiate}上应用了我们的框架，并质量地示出了使用日间外观编码生成夜间照片的能力。此外，我们与标准每个图像的外观编码技术相比，我们的方法可以在无需学习未看到的图像的推理过程中生成类似质量的生成。
</details></li>
</ul>
<hr>
<h2 id="LifeLearner-Hardware-Aware-Meta-Continual-Learning-System-for-Embedded-Computing-Platforms"><a href="#LifeLearner-Hardware-Aware-Meta-Continual-Learning-System-for-Embedded-Computing-Platforms" class="headerlink" title="LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms"></a>LifeLearner: Hardware-Aware Meta Continual Learning System for Embedded Computing Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11420">http://arxiv.org/abs/2311.11420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young D. Kwon, Jagmohan Chauhan, Hong Jia, Stylianos I. Venieris, Cecilia Mascolo</li>
<li>for: 这个论文的目的是探讨如何在具有限制的资源的嵌入式系统上实现持续学习（Continual Learning），以便在上下文、动作和用户变化时，应用程序可以学习并适应。</li>
<li>methods: 该论文提出了一种叫做 LifeLearner 的硬件意识的元 kontinual learning 系统，该系统可以有效地减少系统资源（内存、延迟、能耗）的使用，同时保证高准确性。 LifeLearner 使用了元学习和熬悉策略来直接面对数据稀缺问题，并使用了lossless和lossy压缩技术来减少 CL 和熬悉样本的资源需求。</li>
<li>results: 根据论文描述，LifeLearner 可以实现near-optimal CL 性能，与标准比较基准（Oracle）只偏差2.8%。相比之下，与现有的Meta CL方法相比，LifeLearner 可以减少内存占用（178.7倍）、终端延迟（80.8-94.2%）和能耗（80.9-94.2%）。此外，LifeLearner 已经在两个边缘设备和一个微控制器Unit上成功部署，这意味着LifeLearner 可以在具有限制的平台上实现高效的 CL。<details>
<summary>Abstract</summary>
Continual Learning (CL) allows applications such as user personalization and household robots to learn on the fly and adapt to context. This is an important feature when context, actions, and users change. However, enabling CL on resource-constrained embedded systems is challenging due to the limited labeled data, memory, and computing capacity. In this paper, we propose LifeLearner, a hardware-aware meta continual learning system that drastically optimizes system resources (lower memory, latency, energy consumption) while ensuring high accuracy. Specifically, we (1) exploit meta-learning and rehearsal strategies to explicitly cope with data scarcity issues and ensure high accuracy, (2) effectively combine lossless and lossy compression to significantly reduce the resource requirements of CL and rehearsal samples, and (3) developed hardware-aware system on embedded and IoT platforms considering the hardware characteristics. As a result, LifeLearner achieves near-optimal CL performance, falling short by only 2.8% on accuracy compared to an Oracle baseline. With respect to the state-of-the-art (SOTA) Meta CL method, LifeLearner drastically reduces the memory footprint (by 178.7x), end-to-end latency by 80.8-94.2%, and energy consumption by 80.9-94.2%. In addition, we successfully deployed LifeLearner on two edge devices and a microcontroller unit, thereby enabling efficient CL on resource-constrained platforms where it would be impractical to run SOTA methods and the far-reaching deployment of adaptable CL in a ubiquitous manner. Code is available at https://github.com/theyoungkwon/LifeLearner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Security-Risk-Taxonomy-for-Large-Language-Models"><a href="#A-Security-Risk-Taxonomy-for-Large-Language-Models" class="headerlink" title="A Security Risk Taxonomy for Large Language Models"></a>A Security Risk Taxonomy for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11415">http://arxiv.org/abs/2311.11415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Derner, Kristina Batistič, Jan Zahálka, Robert Babuška</li>
<li>for: 本研究旨在评估大型自然语言模型（LLM）的安全风险，包括伪造、数据泄露和声誉损害等。</li>
<li>methods: 本研究使用推议基本攻击分析pipeline中的用户-模型交互，并将攻击分为目标和攻击类型。</li>
<li>results: 研究提出了一个攻击分类法，并提供了具体的攻击示例，以验证这些风险的实际影响。<details>
<summary>Abstract</summary>
As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by focusing on the security risks posed by LLMs, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize the attacks by target and attack type within a prompt-based interaction scheme. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:large language models (LLMs) 在更多应用程序中普及，需要评估这些模型的相关安全风险。恶意利用者可以通过各种方式，包括假信息、数据泄露和声誉损害，对 LLMs 进行攻击。这篇论文填补当前研究中的一个空白，专注于 LLMs 的安全风险，这些风险不仅包括广泛讨论的伦理和社会因素。我们的工作提出了一种用户-模型通信管道上的安全风险分类法，专注于模型上的提示基本攻击。我们将攻击分为目标和攻击类型，并在提示基本交互方案中进行分类。这种分类机制得到了具体的攻击示例，以示这些风险的实际影响。通过这种分类机制，我们希望激励开发robust和安全的 LLM 应用程序，提高它们的安全性和可信度。
</details></li>
</ul>
<hr>
<h2 id="Make-me-an-Offer-Forward-and-Reverse-Auctioning-Problems-in-the-Tourism-Industry"><a href="#Make-me-an-Offer-Forward-and-Reverse-Auctioning-Problems-in-the-Tourism-Industry" class="headerlink" title="Make me an Offer: Forward and Reverse Auctioning Problems in the Tourism Industry"></a>Make me an Offer: Forward and Reverse Auctioning Problems in the Tourism Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11400">http://arxiv.org/abs/2311.11400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis T. Christou, Dimitris Doukas, Konstantina Skouri, Gerasimos Meletiou</li>
<li>for: 这篇论文的目的是为了解决旅游景点常见的季节性问题，尤其在 covid-19 后期，旅游需求增加但不均衡分布在不同地理区域。</li>
<li>methods: 本论文提出了两种拍卖系统，一种是向下拍卖模型，允许低知名度地区或低季节期Hoteliers拍卖房间，另一种是顾客可以 initiaze 逆向拍卖模型，让Hoteliers在区域发出供应项目，类似于priceline.com 的拍卖概念。</li>
<li>results: 论文发展了数学Programming 模型，显示在这两种拍卖模型中，双方都可以获得重要的经济和社会优点。论文 також提出了算法技术来解决这些优化问题，并使用精确优化 solvers 来解决它们。<details>
<summary>Abstract</summary>
Most tourist destinations are facing regular and consistent seasonality with significant economic and social impacts. This phenomenon is more pronounced in the post-covid era, where demand for travel has increased but unevenly among different geographic areas. To counter these problems that both customers and hoteliers are facing, we have developed two auctioning systems that allow hoteliers of lower popularity tier areas or during low season periods to auction their rooms in what we call a forward auction model, and also allows customers to initiate a bidding process whereby hoteliers in an area may make offers to the customer for their rooms, in what constitutes a reverse auction model initiated by the customer, similar to the bidding concept of priceline.com. We develop mathematical programming models that define explicitly both types of auctions, and show that in each type, there are significant benefits to be gained both on the side of the hotelier as well as on the side of the customer. We discuss algorithmic techniques for the approximate solution of these optimization problems, and present results using exact optimization solvers to solve them to guaranteed optimality. These techniques could be beneficial to both customer and hotelier reducing seasonality during middle and low season and providing the customer with attractive offers.
</details>
<details>
<summary>摘要</summary>
多数旅游目的地都面临 régulière 和consistent的季节性问题，这些问题在covid era后更加突出，旅游需求增加，但不均匀分布在不同地理区域。为了解决这些问题，我们开发了两种拍卖系统，allowing hoteliers in lower popularity tier areas or during low season periods to auction their rooms in a forward auction model, and also allowing customers to initiate a bidding process whereby hoteliers in an area may make offers to the customer for their rooms, similar to the bidding concept of priceline.com. We develop mathematical programming models that explicitly define both types of auctions, and show that in each type, there are significant benefits to be gained on both sides of the hotelier and the customer. We discuss algorithmic techniques for the approximate solution of these optimization problems, and present results using exact optimization solvers to solve them to guaranteed optimality. These techniques could be beneficial to both the customer and the hotelier, reducing seasonality during the middle and low seasons and providing the customer with attractive offers.Note: " régulière" is a French word that means "regular" in English. I left it in the original text as it is a technical term used in the field of auctions.
</details></li>
</ul>
<hr>
<h2 id="Inspecting-Explainability-of-Transformer-Models-with-Additional-Statistical-Information"><a href="#Inspecting-Explainability-of-Transformer-Models-with-Additional-Statistical-Information" class="headerlink" title="Inspecting Explainability of Transformer Models with Additional Statistical Information"></a>Inspecting Explainability of Transformer Models with Additional Statistical Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11378">http://arxiv.org/abs/2311.11378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang C. Nguyen, Haeil Lee, Junmo Kim</li>
<li>for: 本研究的目的是解释Transformer模型在视觉和多modal任务上的含义，以及它在不同变体中的表现。</li>
<li>methods: 该研究使用了 combining attention layers和层normalization层来visualize Transformer模型。</li>
<li>results: 该方法可以有效地解释Swin Transformer和ViT模型的含义，并且可以在不同的任务上进行适应。<details>
<summary>Abstract</summary>
Transformer becomes more popular in the vision domain in recent years so there is a need for finding an effective way to interpret the Transformer model by visualizing it. In recent work, Chefer et al. can visualize the Transformer on vision and multi-modal tasks effectively by combining attention layers to show the importance of each image patch. However, when applying to other variants of Transformer such as the Swin Transformer, this method can not focus on the predicted object. Our method, by considering the statistics of tokens in layer normalization layers, shows a great ability to interpret the explainability of Swin Transformer and ViT.
</details>
<details>
<summary>摘要</summary>
对于过去几年内Transformer在视觉领域的普遍使用，有需要找到一种有效的方法来解释Transformer模型。在最近的工作中，Chefer等人通过结合注意层来有效地图解Transformer在视觉和多modal任务上。然而，当应用到其他Transformer的变体，如Swin Transformer和ViT时，这方法无法对预测物体进行专注。我们的方法，通过考虑层normalization层中的数据统计，展示了Swin Transformer和ViT的解释能力。
</details></li>
</ul>
<hr>
<h2 id="SOccDPT-Semi-Supervised-3D-Semantic-Occupancy-from-Dense-Prediction-Transformers-trained-under-memory-constraints"><a href="#SOccDPT-Semi-Supervised-3D-Semantic-Occupancy-from-Dense-Prediction-Transformers-trained-under-memory-constraints" class="headerlink" title="SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction Transformers trained under memory constraints"></a>SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction Transformers trained under memory constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11371">http://arxiv.org/abs/2311.11371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Nalgunda Ganesh</li>
<li>for: 这个论文的目的是提出一种具有内存效率的方法，用于从单视图图像输入获取3D semantic occupancy prediction。</li>
<li>methods: 这个方法使用dense prediction transformers进行预测，并使用 semi-supervised 训练管道，以便在有限labels的数据集上训练。它还使用patch-wise training来减少在训练过程中的内存使用。</li>
<li>results: 这个方法在无结构化交通数据集上进行训练，并在不结构化交通enario中表现出色，其RMSE score为9.1473， semantic segmentation IoU score为46.02%，并在69.47 Hz的频率下运行。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present SOccDPT, a memory-efficient approach for 3D semantic occupancy prediction from monocular image input using dense prediction transformers. To address the limitations of existing methods trained on structured traffic datasets, we train our model on unstructured datasets including the Indian Driving Dataset and Bengaluru Driving Dataset. Our semi-supervised training pipeline allows SOccDPT to learn from datasets with limited labels by reducing the requirement for manual labelling by substituting it with pseudo-ground truth labels to produce our Bengaluru Semantic Occupancy Dataset. This broader training enhances our model's ability to handle unstructured traffic scenarios effectively. To overcome memory limitations during training, we introduce patch-wise training where we select a subset of parameters to train each epoch, reducing memory usage during auto-grad graph construction. In the context of unstructured traffic and memory-constrained training and inference, SOccDPT outperforms existing disparity estimation approaches as shown by the RMSE score of 9.1473, achieves a semantic segmentation IoU score of 46.02% and operates at a competitive frequency of 69.47 Hz. We make our code and semantic occupancy dataset public.
</details>
<details>
<summary>摘要</summary>
我们介绍SOccDPT，一种具有内存效率的方法，用于从单视图图像输入获得3D semantic occupancy预测。为了解决现有方法在结构化交通数据集上的局限性，我们在不结构化数据集上训练我们的模型，包括印度驾驶数据集和孟买苏驾驶数据集。我们的半监督训练管道，使得SOccDPT可以从有限标签的数据集中学习，而不需要手动标注。为了缓解训练过程中的内存限制，我们引入裁剪训练，其中每 epoch 选择一 subset of 参数进行训练，以减少训练过程中的内存使用。在无结构化交通和内存受限的训练和推理环境下，SOccDPT 在对 dispersive 估计方法的比较中，取得了RMSE 分数为9.1473， achieve semantic segmentation IoU 分数为46.02%，并在竞争性的频率69.47 Hz 上运行。我们将我们的代码和semantic occupancy数据集公开。
</details></li>
</ul>
<hr>
<h2 id="Using-Causal-Threads-to-Explain-Changes-in-a-Dynamic-System"><a href="#Using-Causal-Threads-to-Explain-Changes-in-a-Dynamic-System" class="headerlink" title="Using Causal Threads to Explain Changes in a Dynamic System"></a>Using Causal Threads to Explain Changes in a Dynamic System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11334">http://arxiv.org/abs/2311.11334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert B. Allen</li>
<li>for: 研究建立系统具有厚重 semantics的模型，具体是使用结构 causa explanations 描述系统的状态变化。</li>
<li>methods: 使用 проце序基于动态知识图进行建立。</li>
<li>results: 透过 Snowball Earth 理论中的地质变化的假设建立了一个 causa 线索模型，并描述了一个早期的图形界面来展示解释。Note: “厚重 semantics” (hòu zhòng xiàng yì) refers to the richness and depth of meaning in the models, “结构 causa explanations” (jié zhòng causa yì jiàn) refers to the causal explanations that are structured and explicit, and “ proces序基于动态知识图” (proces序 jī bù dòng tài zhī yì) refers to the process-based dynamic knowledge graphs.<details>
<summary>Abstract</summary>
We explore developing rich semantic models of systems. Specifically, we consider structured causal explanations about state changes in those systems. Essentially, we are developing process-based dynamic knowledge graphs. As an example, we construct a model of the causal threads for geological changes proposed by the Snowball Earth theory. Further, we describe an early prototype of a graphical interface to present the explanations. Unlike statistical approaches to summarization and explanation such as Large Language Models (LLMs), our approach of direct representation can be inspected and verified directly.
</details>
<details>
<summary>摘要</summary>
我们探索构建丰富 semantics的系统模型。 Specifically, we consider structured causal explanations about state changes in those systems. Essentially, we are developing process-based dynamic knowledge graphs. As an example, we construct a model of the causal threads for geological changes proposed by the Snowball Earth theory. Further, we describe an early prototype of a graphical interface to present the explanations. Unlike statistical approaches to summarization and explanation such as Large Language Models (LLMs), our approach of direct representation can be inspected and verified directly.Here's the breakdown of the translation:* 我们 (wǒmen) - we* 探索 (tànsuō) - explore* 构建 (gōngjiàn) - construct* 丰富 semantics (yífù xìngxì) - rich semantic* 系统 (xìtǒng) - system* 模型 (móxìng) - model* Specifically (xīngqī) - specifically* we consider (wǒmen jīngxīn) - we consider* structured causal explanations (lùxìng yòu yìxìng) - structured causal explanations* about state changes (zhèngxìng zhèngtiān) - about state changes* in those systems (fēngxiàng zhèngtiān) - in those systems* Essentially (yìxìng) - essentially* we are developing (wǒmen zhìdào) - we are developing* process-based dynamic knowledge graphs (jìxìng qùxìng zhìxìng) - process-based dynamic knowledge graphs* As an example (jìxìng) - as an example* we construct (wǒmen jìxìng) - we construct* a model of the causal threads (lùxìng yòu qiángxìng) - a model of the causal threads* for geological changes (dìxìng qìngxìng) - for geological changes* proposed by the Snowball Earth theory (xuěxìng zhèndài) - proposed by the Snowball Earth theory* Further (fùqì) - further* we describe (wǒmen xiǎngxìng) - we describe* an early prototype (shèngyì) - an early prototype* of a graphical interface (xìngxìng qiǎngxìng) - of a graphical interface* to present the explanations (jièxìng) - to present the explanations* Unlike statistical approaches (fājì) - unlike statistical approaches* to summarization and explanation (jièxìng) - to summarization and explanation* such as Large Language Models (LLMs) (xìngxìng) - such as Large Language Models (LLMs)* our approach (wǒmen fāngyì) - our approach* of direct representation (jìxìng) - of direct representation* can be inspected and verified directly (dìxiǎng yìxìng) - can be inspected and verified directly
</details></li>
</ul>
<hr>
<h2 id="Portuguese-FAQ-for-Financial-Services"><a href="#Portuguese-FAQ-for-Financial-Services" class="headerlink" title="Portuguese FAQ for Financial Services"></a>Portuguese FAQ for Financial Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11331">http://arxiv.org/abs/2311.11331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulo Finardi, Wanderley M. Melo, Edgard D. Medeiros Neto, Alex F. Mansano, Pablo B. Costa, Vinicius F. Caridá</li>
<li>for: 提高Portuguese финан领域自然语言处理（NLP）应用的发展， Addressing the limitation of domain-specific data scarcity in the Portuguese financial domain.</li>
<li>methods: 使用数据增强技术生成synthetic数据， Employing data augmentation techniques to generate synthetic data.</li>
<li>results: 对具有不同semantic similarity的数据进行Supervised和Unsupervised任务， Evaluating the impact of augmented data on both low and high semantic similarity scenarios.<details>
<summary>Abstract</summary>
Scarcity of domain-specific data in the Portuguese financial domain has disfavored the development of Natural Language Processing (NLP) applications. To address this limitation, the present study advocates for the utilization of synthetic data generated through data augmentation techniques. The investigation focuses on the augmentation of a dataset sourced from the Central Bank of Brazil FAQ, employing techniques that vary in semantic similarity. Supervised and unsupervised tasks are conducted to evaluate the impact of augmented data on both low and high semantic similarity scenarios. Additionally, the resultant dataset will be publicly disseminated on the Hugging Face Datasets platform, thereby enhancing accessibility and fostering broader engagement within the NLP research community.
</details>
<details>
<summary>摘要</summary>
缺乏特定领域数据的问题在葡萄牙金融领域内，妨碍了自然语言处理（NLP）应用的发展。为解决这一问题，当前研究提议利用数据扩充技术生成的合成数据。研究将对由巴西中央银行FAQ获取的数据进行扩充，使用不同的semantic similarity的技术。用于超级vised和无supervised任务进行评估，以确定扩充数据对低和高semantic similarity场景的影响。此外，所生成的数据集也将在Hugging Face Datasets平台上公开发布，以便更多的研究人员可以访问和参与NLP研究领域。
</details></li>
</ul>
<hr>
<h2 id="Bounds-on-Representation-Induced-Confounding-Bias-for-Treatment-Effect-Estimation"><a href="#Bounds-on-Representation-Induced-Confounding-Bias-for-Treatment-Effect-Estimation" class="headerlink" title="Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation"></a>Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11321">http://arxiv.org/abs/2311.11321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel</li>
<li>for: 本研究旨在提供一种新的，不受表示学习影响的 bounds  estimation 方法，用于估计受到维度减少（或其他约束）引起的 repression 抖音偏见。</li>
<li>methods: 我们提出一种新的、不受表示学习影响的 bounds  estimation 方法，包括 theoretically 确定 conditional average treatment effect 是不可识别的，以及提出一种用于估计 repression 抖音偏见的 partial identification 方法。</li>
<li>results: 我们在一系列实验中证明了我们的 bounds 可以准确地估计 repression 抖音偏见，并且可以在实际应用中提高 conditional average treatment effect 的可靠性。<details>
<summary>Abstract</summary>
State-of-the-art methods for conditional average treatment effect (CATE) estimation make widespread use of representation learning. Here, the idea is to reduce the variance of the low-sample CATE estimation by a (potentially constrained) low-dimensional representation. However, low-dimensional representations can lose information about the observed confounders and thus lead to bias, because of which the validity of representation learning for CATE estimation is typically violated. In this paper, we propose a new, representation-agnostic framework for estimating bounds on the representation-induced confounding bias that comes from dimensionality reduction (or other constraints on the representations) in CATE estimation. First, we establish theoretically under which conditions CATEs are non-identifiable given low-dimensional (constrained) representations. Second, as our remedy, we propose to perform partial identification of CATEs or, equivalently, aim at estimating of lower and upper bounds of the representation-induced confounding bias. We demonstrate the effectiveness of our bounds in a series of experiments. In sum, our framework is of direct relevance in practice where the validity of CATE estimation is of importance.
</details>
<details>
<summary>摘要</summary>
现代方法 для conditional average treatment effect（CATE）估计广泛使用表示学习。在这里，想法是通过（可能受限的）低维度表示来减少低样本CATE估计的方差。然而，低维度表示可能会失去观察的混杂变量信息，从而导致偏误，因此表示学习在CATE估计中的有效性通常会被违反。在这篇论文中，我们提出了一种新的、表示不依赖的框架，用于估计受到维度减少（或其他表示约束）在CATE估计中的表示偏误的上下限。首先，我们证明在低维度（受限的）表示下，CATE是非可定义的。其次，作为我们的药物，我们提议在CATE估计中进行部分标识或等效地估计表示偏误的下限和上限。我们在一系列实验中证明了bounds的效果。总之，我们的框架对实际中CATE估计的有效性具有直接的重要性。
</details></li>
</ul>
<hr>
<h2 id="GeoSAM-Fine-tuning-SAM-with-Sparse-and-Dense-Visual-Prompting-for-Automated-Segmentation-of-Mobility-Infrastructure"><a href="#GeoSAM-Fine-tuning-SAM-with-Sparse-and-Dense-Visual-Prompting-for-Automated-Segmentation-of-Mobility-Infrastructure" class="headerlink" title="GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure"></a>GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11319">http://arxiv.org/abs/2311.11319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafi Ibn Sultan, Chengyin Li, Hui Zhu, Prashant Khanduri, Marco Brocanelli, Dongxiao Zhu</li>
<li>for: 这篇论文主要针对的是地理图像分类，特别是使用 Segment Anything Model (SAM) 进行道路和人行道等基础设施的分类。</li>
<li>methods: 这篇论文提出了一个名为 Geographical SAM (GeoSAM) 的新的框架，它利用了细密的可视提示和稀有的可视提示，从零shot学习和预训练的 CNN 分类模型来精确地分类地理图像中的基础设施。</li>
<li>results: 根据论文的评估，GeoSAM 比较已有的方法更高效，对于道路基础设施的分类精度提高了20%，对于人行道基础设施的分类精度提高了14.29%，对于平均的分类精度提高了17.65%。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 20%, 14.29%, and 17.65% for road infrastructure, pedestrian infrastructure, and on average, respectively, representing a momentous leap in leveraging foundation models to segment mobility infrastructure including both road and pedestrian infrastructure in geographical images.
</details>
<details>
<summary>摘要</summary>
《Segment Anything Model》（SAM）在自然图像 segmentation 方面表现出色，但对于航空和卫星图像，特别是涉及到交通基础设施如路、人行道和横交道，其性能较差。这是因为这些物体的窄特征，Texture与周围环境杂mix，以及对象如树、建筑、车辆和行人的干扰，导致模型生成不准确的分 segmentation 图像。为解决这些挑战，我们提出了《地理 SAM》（GeoSAM），一种基于 SAM 的新框架，实现了精度调整策略，使用零shot learning  dense visual prompt 和预训练 CNN segmentation 模型 sparse visual prompt。提出的 GeoSAM 在 geographical 图像 segmentation 方面表现出优于现有方法，具体提高了20%、14.29%和17.65%，分别用于路基础设施、人行道基础设施和平均值，表示了基于基础模型 segment  mobility 基础设施，包括路基础设施和人行道基础设施的大幅提高。
</details></li>
</ul>
<hr>
<h2 id="TPTU-v2-Boosting-Task-Planning-and-Tool-Usage-of-Large-Language-Model-based-Agents-in-Real-world-Systems"><a href="#TPTU-v2-Boosting-Task-Planning-and-Tool-Usage-of-Large-Language-Model-based-Agents-in-Real-world-Systems" class="headerlink" title="TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems"></a>TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11315">http://arxiv.org/abs/2311.11315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, Rui Zhao</li>
<li>for: 提高LLM在真实系统中的任务规划和工具使用能力</li>
<li>methods: 提出了一个全面的框架，包括API选择器、LLM调参器和示例选择器，用于解决实际系统中的三大挑战</li>
<li>results: 通过使用实际系统和开源学术数据集进行验证，结果表明每个组件以及整个框架具有高效性<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated proficiency in addressing tasks that necessitate a combination of task planning and the usage of external tools that require a blend of task planning and the utilization of external tools, such as APIs. However, real-world complex systems present three prevalent challenges concerning task planning and tool usage: (1) The real system usually has a vast array of APIs, so it is impossible to feed the descriptions of all APIs to the prompt of LLMs as the token length is limited; (2) the real system is designed for handling complex tasks, and the base LLMs can hardly plan a correct sub-task order and API-calling order for such tasks; (3) Similar semantics and functionalities among APIs in real systems create challenges for both LLMs and even humans in distinguishing between them. In response, this paper introduces a comprehensive framework aimed at enhancing the Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating within real-world systems. Our framework comprises three key components designed to address these challenges: (1) the API Retriever selects the most pertinent APIs for the user task among the extensive array available; (2) LLM Finetuner tunes a base LLM so that the finetuned LLM can be more capable for task planning and API calling; (3) the Demo Selector adaptively retrieves different demonstrations related to hard-to-distinguish APIs, which is further used for in-context learning to boost the final performance. We validate our methods using a real-world commercial system as well as an open-sourced academic dataset, and the outcomes clearly showcase the efficacy of each individual component as well as the integrated framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-Lies-beyond-the-Pareto-Front-A-Survey-on-Decision-Support-Methods-for-Multi-Objective-Optimization"><a href="#What-Lies-beyond-the-Pareto-Front-A-Survey-on-Decision-Support-Methods-for-Multi-Objective-Optimization" class="headerlink" title="What Lies beyond the Pareto Front? A Survey on Decision-Support Methods for Multi-Objective Optimization"></a>What Lies beyond the Pareto Front? A Survey on Decision-Support Methods for Multi-Objective Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11288">http://arxiv.org/abs/2311.11288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuzanna Osika, Jazmin Zatarain Salazar, Diederik M. Roijers, Frans A. Oliehoek, Pradeep K. Murukannaiah</li>
<li>for: 这 paper 的目的是解决多目标优化（MOO）算法生成的解决方案中的贸易offs。</li>
<li>methods: 这 paper 使用了许多不同的领域的研究方法，包括可视化、解决集挖掘、不确定性探索等方法，以探讨 MOO 算法生成的解决方案中的贸易offs。</li>
<li>results: 这 paper 提供了一种综合的方法来探讨 MOO 算法生成的解决方案中的贸易offs，独立于应用领域。这种方法可以减少研究者和实践者使用 MOO 算法的入门难度，同时也提供了新的研究方向，如交互性、解释性和伦理。<details>
<summary>Abstract</summary>
We present a review that unifies decision-support methods for exploring the solutions produced by multi-objective optimization (MOO) algorithms. As MOO is applied to solve diverse problems, approaches for analyzing the trade-offs offered by MOO algorithms are scattered across fields. We provide an overview of the advances on this topic, including methods for visualization, mining the solution set, and uncertainty exploration as well as emerging research directions, including interactivity, explainability, and ethics. We synthesize these methods drawing from different fields of research to build a unified approach, independent of the application. Our goals are to reduce the entry barrier for researchers and practitioners on using MOO algorithms and to provide novel research directions.
</details>
<details>
<summary>摘要</summary>
我们提出了一篇文章，汇集了多目标优化（MOO）算法生成的解决方案的决策支持方法的评估。由于MOO在解决多种问题时使用，关于这些方法的分析方法在不同领域中散布。我们提供了这些进展的概述，包括视觉化、解决集挖掘、不确定性探索以及emerging research direction，如交互、解释性和伦理。我们将这些方法从不同领域的研究中综合归纳，建立一个独立于应用的统一方法，以降低MOO算法的入门难度，并提供新的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Tactile-Active-Inference-Reinforcement-Learning-for-Efficient-Robotic-Manipulation-Skill-Acquisition"><a href="#Tactile-Active-Inference-Reinforcement-Learning-for-Efficient-Robotic-Manipulation-Skill-Acquisition" class="headerlink" title="Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition"></a>Tactile Active Inference Reinforcement Learning for Efficient Robotic Manipulation Skill Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11287">http://arxiv.org/abs/2311.11287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Liu, Xing Liu, Yizhai Zhang, Zhengxiong Liu, Panfeng Huang<br>for:这个研究旨在开发一种新的机器人掌握技术，以提高机器人在实际应用中的效率和适应能力。methods:我们提出了一种名为“感触主动推导学习”（Tactile-AIRL）的新方法，具有高效训练的能力。这个方法结合了活动推测和内在好奇，以提高RL的训练效率和适应率。此外，我们还使用了视觉感知器来提供细节的感知，以便进行掌握任务。results:我们在非掌握物推动任务上进行了 simulations，结果显示了我们的方法能够实现高效训练。它能够在少量互动回合中将代理人培训到极高水平，超过SAC基准。此外，我们还进行了实验，用于螺旋夹缔任务，结果显示了我们的方法具有快速学习能力和实际应用潜力。<details>
<summary>Abstract</summary>
Robotic manipulation holds the potential to replace humans in the execution of tedious or dangerous tasks. However, control-based approaches are not suitable due to the difficulty of formally describing open-world manipulation in reality, and the inefficiency of existing learning methods. Thus, applying manipulation in a wide range of scenarios presents significant challenges. In this study, we propose a novel method for skill learning in robotic manipulation called Tactile Active Inference Reinforcement Learning (Tactile-AIRL), aimed at achieving efficient training. To enhance the performance of reinforcement learning (RL), we introduce active inference, which integrates model-based techniques and intrinsic curiosity into the RL process. This integration improves the algorithm's training efficiency and adaptability to sparse rewards. Additionally, we utilize a vision-based tactile sensor to provide detailed perception for manipulation tasks. Finally, we employ a model-based approach to imagine and plan appropriate actions through free energy minimization. Simulation results demonstrate that our method achieves significantly high training efficiency in non-prehensile objects pushing tasks. It enables agents to excel in both dense and sparse reward tasks with just a few interaction episodes, surpassing the SAC baseline. Furthermore, we conduct physical experiments on a gripper screwing task using our method, which showcases the algorithm's rapid learning capability and its potential for practical applications.
</details>
<details>
<summary>摘要</summary>
роботизированная манипуляция имеет потенциал заменить людей в выполнении утомительных или опасных задач. Однако, подход на основе контроля неприменим из-за трудности формального описания открытого мира манипуляции в реальности и неэффективности существующих методов обучения. Поэтому применение манипуляции в широком диапазоне сценариев представляет значительные вызовы. В этой статье мы предлагаем новый метод для обучения навыков в роботизированной манипуляции, называемый Тактильным активным инфериенсом реинфорcement learning (Tactile-AIRL), направленный на достижение эффективной подготовки.Чтобы улучшить эффективность реинфорcement learning (RL), мы вводим активный инфериент, который интегрирует моделирующие техники и внутреннюю любопытство в процесс RL. Это интеграция улучшает алгоритм подготовки и адаптацию к скудным наградам. Кроме того, мы используем визон-базированный тактильный сенсор для предоставления подробного восприятия для задач манипуляции. Наконец, мы используем модель-ориентированный подход для представления и планирования подходящих действий через минимизацию свободной энергии. Симуляционные результаты показывают, что наш метод достигает значительно высокой эффективности обучения в задачах непреhensible objects pushing. Он позволяет агентам превзойти базовый SAC в обоих задачах с dense и sparse reward с только несколькими эпизодами взаимодействия, а также физические эксперименты на задаче гриппера скрепления, которые демонстрируют способность алгоритма быстро учиться и его потенциал для практических применений.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Prompt-Tuning-for-Vision-Language-Models"><a href="#Adversarial-Prompt-Tuning-for-Vision-Language-Models" class="headerlink" title="Adversarial Prompt Tuning for Vision-Language Models"></a>Adversarial Prompt Tuning for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11261">http://arxiv.org/abs/2311.11261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaming Zhang, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang</li>
<li>for: 提高多modal学习中VLM的攻击鲁棒性，尤其是图像模式下的攻击鲁棒性问题。</li>
<li>methods: 利用可学习的文本提示和对抗图像嵌入的对应关系，以提高VLM的对抗攻击能力。</li>
<li>results: 提高了对白盒和黑盒攻击的抵抗力，并与现有的图像处理基础技术相结合，进一步提高了防御能力。<details>
<summary>Abstract</summary>
With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code will be available upon publication of the paper.
</details>
<details>
<summary>摘要</summary>
随着多模态学习的快速发展，预训练的视觉语言模型（VLM）如CLIP已经显示出了跨模态桥接的remarkable capacities。然而，这些模型仍然容易受到恶意攻击，特别是在图像模式下，存在重大的安全隐患。本文介绍了一种新的技术——挑战性提示调整（AdvPT），用于增强VLM中图像编码器的恶意攻击抵抗力。AdvPT利用可学习的文本提示和恶意图像嵌入的对齐，以解决VLM中的潜在漏洞，不需要大量参数训练或改变模型结构。我们的实验表明，AdvPT可以提高白盒和黑盒恶意攻击的抵抗力，并且与现有的图像处理基于防御技术相乘 synergistic effect，进一步增强防御能力。我们的实验分析也为挑战性提示调整提供了深入的理解，开启了未来robust multimodal learning研究的新可能性。这些发现将为VLM的安全提供新的可能性。我们的代码将在文章发表后提供。
</details></li>
</ul>
<hr>
<h2 id="Tensor-networks-for-interpretable-and-efficient-quantum-inspired-machine-learning"><a href="#Tensor-networks-for-interpretable-and-efficient-quantum-inspired-machine-learning" class="headerlink" title="Tensor networks for interpretable and efficient quantum-inspired machine learning"></a>Tensor networks for interpretable and efficient quantum-inspired machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11258">http://arxiv.org/abs/2311.11258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shi-Ju Ran, Gang Su</li>
<li>for: 本研究旨在 simultaneously gain high interpretability 和 efficiency with the current schemes of deep machine learning (ML).</li>
<li>methods: 本研究使用 tensor network (TN)，which is a well-established mathematical tool originating from quantum mechanics，to develop efficient “white-box” ML schemes.</li>
<li>results: 研究表明，TN ML 可以具有强大的 interpretability 和高效率，并且可能在 quantum computers 上实现新的 schemes。<details>
<summary>Abstract</summary>
It is a critical challenge to simultaneously gain high interpretability and efficiency with the current schemes of deep machine learning (ML). Tensor network (TN), which is a well-established mathematical tool originating from quantum mechanics, has shown its unique advantages on developing efficient ``white-box'' ML schemes. Here, we give a brief review on the inspiring progresses made in TN-based ML. On one hand, interpretability of TN ML is accommodated with the solid theoretical foundation based on quantum information and many-body physics. On the other hand, high efficiency can be rendered from the powerful TN representations and the advanced computational techniques developed in quantum many-body physics. With the fast development on quantum computers, TN is expected to conceive novel schemes runnable on quantum hardware, heading towards the ``quantum artificial intelligence'' in the forthcoming future.
</details>
<details>
<summary>摘要</summary>
现在的深度机器学习（ML）中，同时获得高度可解释性和高效性是一项极其挑战性的任务。量子网络（TN）是一种已经成熔的数学工具，它在量子力学中的起源，在深度学习中表现出独特的优势。以下是对TN在深度学习中的进步的简短回顾。一方面，TN的可解释性在量子信息和多体物理理论的坚实基础上得到保障。另一方面，TN的表示力和量子多体物理的高级计算技术使得高效性得到实现。随着量子计算机的快速发展，TN预计会推出新的可运行在量子硬件上的方案，逐渐实现“量子人工智能”的未来。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-on-Sentiment-Analysis-Tasks-Approaches-and-Applications"><a href="#A-Comprehensive-Review-on-Sentiment-Analysis-Tasks-Approaches-and-Applications" class="headerlink" title="A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications"></a>A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11250">http://arxiv.org/abs/2311.11250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudhanshu Kumar, Partha Pratim Roy, Debi Prosad Dogra, Byung-Gyu Kim</li>
<li>For: The paper is written for researchers and practitioners in the field of sentiment analysis and text mining, as well as those interested in understanding the current state of the field and its applications in various domains.* Methods: The paper uses a lexicon-based approach and deep learning techniques to analyze sentiment in text, images, videos, and voice data. It also discusses the challenges and opportunities of sentiment analysis in different domains.* Results: The paper provides an overview of the recent research and development in sentiment analysis, including its applications in various domains and the challenges and limitations of the field. It also highlights the potential of deep learning techniques in improving the accuracy of sentiment analysis.<details>
<summary>Abstract</summary>
Sentiment analysis (SA) is an emerging field in text mining. It is the process of computationally identifying and categorizing opinions expressed in a piece of text over different social media platforms. Social media plays an essential role in knowing the customer mindset towards a product, services, and the latest market trends. Most organizations depend on the customer's response and feedback to upgrade their offered products and services. SA or opinion mining seems to be a promising research area for various domains. It plays a vital role in analyzing big data generated daily in structured and unstructured formats over the internet. This survey paper defines sentiment and its recent research and development in different domains, including voice, images, videos, and text. The challenges and opportunities of sentiment analysis are also discussed in the paper.   \keywords{Sentiment Analysis, Machine Learning, Lexicon-based approach, Deep Learning, Natural Language Processing}
</details>
<details>
<summary>摘要</summary>
关键字：Sentiment Analysis, Machine Learning, Lexicon-based approach, Deep Learning, Natural Language Processing
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Dandelion-Network-for-IoT-Intrusion-Detection"><a href="#Open-Set-Dandelion-Network-for-IoT-Intrusion-Detection" class="headerlink" title="Open Set Dandelion Network for IoT Intrusion Detection"></a>Open Set Dandelion Network for IoT Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11249">http://arxiv.org/abs/2311.11249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashu Wu, Hao Dai, Kenneth B. Kent, Jerome Yen, Chengzhong Xu, Yang Wang</li>
<li>for: 这篇论文的目的是提出一个基于无监督多元领域适应的开放集合网络（OSDN）模型，以对 IoT 设备的攻击进行更 precisely 的检测。</li>
<li>methods: 这篇论文使用了一个基于无监督多元领域适应的开放集合网络（OSDN）模型，包括了以下几个方法：1) 将知识源网络攻击领域转换到目标 IoT 攻击领域中，以实现更高精度的检测；2) 使用了一个基于牛顿轨道的目标网络，以实现更好的检测效果；3) 使用了一个对应数据的矩阵来实现更好的检测效果。</li>
<li>results: 根据实验结果，这篇论文的 OSDN 模型比三个州际前方法提高了16.9%。<details>
<summary>Abstract</summary>
As IoT devices become widely, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this paper we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactness. The dandelion-based target membership mechanism then forms the target dandelion. Then, the dandelion angular separation mechanism achieves better inter-category separability, and the dandelion embedding alignment mechanism further aligns both dandelions in a finer manner. To promote intra-category compactness, the discriminating sampled dandelion mechanism is used. Assisted by the intrusion classifier trained using both known and generated unknown intrusion knowledge, a semantic dandelion correction mechanism emphasises easily-confused categories and guides better inter-category separability. Holistically, these mechanisms form the OSDN model that effectively performs intrusion knowledge transfer to benefit IoT intrusion detection. Comprehensive experiments on several intrusion datasets verify the effectiveness of the OSDN model, outperforming three state-of-the-art baseline methods by 16.9%.
</details>
<details>
<summary>摘要</summary>
随着互联网物联网（IoT）设备的普及，保护它们从恶意攻击中免受伤害变得非常重要。然而，由于IoT数据的稀缺，传统的攻击检测方法无法应用，这些方法高度依赖数据。为解决这个问题，我们在这篇论文中提出了基于不supervised heterogeneous domain adaptation的开放集合网络（OSDN）模型。OSDN模型通过从知识充足的源网络攻击域传输攻击知识来提高IoT攻击域的检测精度。在开放集合 Setting下，它还可以检测新出现的目标域攻击，这些攻击没有在源域观察到。为实现这一目标，OSDN模型将源域转化为一个类似于芽孢的特征空间，其中每种攻击类型都是紧凑地归类，同时也能够准确地分割不同的攻击类型，即同时强调 между类别之间的分离和内部分类之间的凝固。然后，目标域被转化为一个类似于芽孢的特征空间，并使用芽孢角度分离机制来实现更好的 между类别之间的分离。最后，通过芽孢嵌入对Alignment机制来进一步准确地匹配两个芽孢。为提高内部分类之间的凝固，我们采用了一种吸引样本芽孢机制，该机制通过使用已知和生成的未知攻击知识来训练攻击分类器。此外，我们还采用了一种 semantic dandelion correction mechanism，该机制通过强调易混淆的类别来提高between类别之间的分离。总的来说，这些机制组成了OSDN模型，该模型可以有效地传输攻击知识，以便为IoT攻击检测提供更高的精度。我们在多个攻击数据集上进行了广泛的实验，结果表明OSDN模型比三种state-of-the-art基eline方法高于16.9%。
</details></li>
</ul>
<hr>
<h2 id="AtomXR-Streamlined-XR-Prototyping-with-Natural-Language-and-Immersive-Physical-Interaction"><a href="#AtomXR-Streamlined-XR-Prototyping-with-Natural-Language-and-Immersive-Physical-Interaction" class="headerlink" title="AtomXR: Streamlined XR Prototyping with Natural Language and Immersive Physical Interaction"></a>AtomXR: Streamlined XR Prototyping with Natural Language and Immersive Physical Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11238">http://arxiv.org/abs/2311.11238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alice Cai, Caine Ardayfio, AnhPhu Nguyen, Tica Lin, Elena Glassman</li>
<li>for: 帮助开发者快速创建扩展现实（XR）应用程序，不需要深入学习开发技能。</li>
<li>methods: 使用自然语言、眼动和触摸交互，并采用高级编程语言AtomScript，以及具有多modal输入的自然语言 интер페이ス。</li>
<li>results: 经验证明，AtomXR可以提供快速、简单的开发体验，并且在速度和用户体验方面与传统系统相比有显著提高。<details>
<summary>Abstract</summary>
As technological advancements in extended reality (XR) amplify the demand for more XR content, traditional development processes face several challenges: 1) a steep learning curve for inexperienced developers, 2) a disconnect between 2D development environments and 3D user experiences inside headsets, and 3) slow iteration cycles due to context switching between development and testing environments. To address these challenges, we introduce AtomXR, a streamlined, immersive, no-code XR prototyping tool designed to empower both experienced and inexperienced developers in creating applications using natural language, eye-gaze, and touch interactions. AtomXR consists of: 1) AtomScript, a high-level human-interpretable scripting language for rapid prototyping, 2) a natural language interface that integrates LLMs and multimodal inputs for AtomScript generation, and 3) an immersive in-headset authoring environment. Empirical evaluation through two user studies offers insights into natural language-based and immersive prototyping, and shows AtomXR provides significant improvements in speed and user experience compared to traditional systems.
</details>
<details>
<summary>摘要</summary>
随着扩展现实（XR）技术的发展，需求更多的XR内容的增加，传统的开发过程面临着一些挑战：1）开发人员无经验的陡峭学习曲线，2）在头盔中的用户体验与2D开发环境之间的断开，3）由于开发和测试环境之间的上下文切换而导致的慢速迭代循环。为解决这些挑战，我们介绍了AtomXR，一种简化的、 immerse 的、无代码 XR 原型工具，旨在帮助经验丰富和无经验的开发人员在使用自然语言、眼动和触摸交互的情况下创建应用程序。AtomXR包括：1）AtomScript，一种高级别的人类可解释的脚本语言，2）一种与大数据学习机器 integrate 的自然语言界面，3）一种 immerse 在头盔中的作者环境。经验证明了在两个用户研究中，AtomXR 可以提供传统系统的速度和用户体验方面的显著改进。
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-AI-Deep-Learning-Algorithm-For-Multi-Modal-Sentiment-Analysis"><a href="#Implementation-of-AI-Deep-Learning-Algorithm-For-Multi-Modal-Sentiment-Analysis" class="headerlink" title="Implementation of AI Deep Learning Algorithm For Multi-Modal Sentiment Analysis"></a>Implementation of AI Deep Learning Algorithm For Multi-Modal Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11237">http://arxiv.org/abs/2311.11237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Wang</li>
<li>for: 本研究设计了一种多模式认情方法，结合了两条通道卷积神经网络和环网。这种方法可以有效提取情感信息，提高学习效率。</li>
<li>methods: 词汇vector化使用GloVe，将词汇vector输入卷积神经网络。兼用注意机制和最大池化抽象BiSRU通道，实现本地深度情感和预后序情感 semantics。最后，融合多个特征，将极性情感输入为认情数据集的认情准确率。</li>
<li>results: 实验显示，基于特征融合的情感分析方法可以增强情感认知数据集的认知率和减少学习时间。模型具有一定的通用性。<details>
<summary>Abstract</summary>
A multi-modal emotion recognition method was established by combining two-channel convolutional neural network with ring network. This method can extract emotional information effectively and improve learning efficiency. The words were vectorized with GloVe, and the word vector was input into the convolutional neural network. Combining attention mechanism and maximum pool converter BiSRU channel, the local deep emotion and pre-post sequential emotion semantics are obtained. Finally, multiple features are fused and input as the polarity of emotion, so as to achieve the emotion analysis of the target. Experiments show that the emotion analysis method based on feature fusion can effectively improve the recognition accuracy of emotion data set and reduce the learning time. The model has a certain generalization.
</details>
<details>
<summary>摘要</summary>
一种多Modal Emotion recognition方法由两个通道卷积神经网络与环形网络结合，该方法可以有效提取情感信息并提高学习效率。文本被vector化使用GloVe，word vector输入卷积神经网络。结合注意机制和最大池化converter BiSRU通道，本地深情和预后序情态 semantics 得到。最后，多个特征 fusion 并输入为情感趋势，以实现目标情感分析。实验显示，基于特征合并的情感分析方法可以提高情感数据集的识别率和减少学习时间。模型具有一定的普适性。Here's the word-for-word translation:一种多Modal Emotion recognition方法由两个通道卷积神经网络与环形网络结合，该方法可以有效提取情感信息并提高学习效率。文本被vector化使用GloVe，word vector输入卷积神经网络。结合注意机制和最大池化converter BiSRU通道，本地深情和预后序情态 semantics 得到。最后，多个特征 fusion 并输入为情感趋势，以实现目标情感分析。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Anomaly’-in-Time-Series-Anomaly-Detection-A-Self-supervised-Tri-domain-Solution"><a href="#Unraveling-the-Anomaly’-in-Time-Series-Anomaly-Detection-A-Self-supervised-Tri-domain-Solution" class="headerlink" title="Unraveling the &#96;Anomaly’ in Time Series Anomaly Detection: A Self-supervised Tri-domain Solution"></a>Unraveling the &#96;Anomaly’ in Time Series Anomaly Detection: A Self-supervised Tri-domain Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11235">http://arxiv.org/abs/2311.11235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuting Sun, Guansong Pang, Guanhua Ye, Tong Chen, Xia Hu, Hongzhi Yin</li>
<li>for: 本研究旨在提出一种基于自动学习的时序异常检测方法，以解决现有时序异常检测中的缺乏异常标签和异常形态的问题。</li>
<li>methods: 本研究使用了三个数据频谱域（时间频谱、频率频谱和差异频谱）中的特征模型，不需要异常标签。它还使用了两种对比损失函数：间频域对比损失和自适应对比损失，以学习通用特征和分辨异常数据。</li>
<li>results: 经过实验证明，TriAD方法可以在UCR数据集上达到三倍的SOTA深度学习模型的PA%K基于F1分数，并且与SOTA对比发现算法相比，提高了50%的准确率。<details>
<summary>Abstract</summary>
The ongoing challenges in time series anomaly detection (TSAD), notably the scarcity of anomaly labels and the variability in anomaly lengths and shapes, have led to the need for a more efficient solution. As limited anomaly labels hinder traditional supervised models in TSAD, various SOTA deep learning techniques, such as self-supervised learning, have been introduced to tackle this issue. However, they encounter difficulties handling variations in anomaly lengths and shapes, limiting their adaptability to diverse anomalies. Additionally, many benchmark datasets suffer from the problem of having explicit anomalies that even random functions can detect. This problem is exacerbated by ill-posed evaluation metrics, known as point adjustment (PA), which can result in inflated model performance. In this context, we propose a novel self-supervised learning based Tri-domain Anomaly Detector (TriAD), which addresses these challenges by modeling features across three data domains - temporal, frequency, and residual domains - without relying on anomaly labels. Unlike traditional contrastive learning methods, TriAD employs both inter-domain and intra-domain contrastive loss to learn common attributes among normal data and differentiate them from anomalies. Additionally, our approach can detect anomalies of varying lengths by integrating with a discord discovery algorithm. It is worth noting that this study is the first to reevaluate the deep learning potential in TSAD, utilizing both rigorously designed datasets (i.e., UCR Archive) and evaluation metrics (i.e., PA%K and affiliation). Through experimental results on the UCR dataset, TriAD achieves an impressive three-fold increase in PA%K based F1 scores over SOTA deep learning models, and 50% increase of accuracy as compared to SOTA discord discovery algorithms.
</details>
<details>
<summary>摘要</summary>
Traditional supervised learning models in time series anomaly detection (TSAD) have been limited by the scarcity of anomaly labels and the variability in anomaly lengths and shapes. To address these challenges, various state-of-the-art (SOTA) deep learning techniques have been introduced, such as self-supervised learning. However, these methods have difficulty handling variations in anomaly lengths and shapes, limiting their adaptability to diverse anomalies. Additionally, many benchmark datasets suffer from the problem of explicit anomalies that can be detected by random functions, which is exacerbated by the use of ill-posed evaluation metrics such as point adjustment (PA).To address these challenges, we propose a novel self-supervised learning based Tri-domain Anomaly Detector (TriAD). TriAD models features across three data domains - temporal, frequency, and residual domains - without relying on anomaly labels. Unlike traditional contrastive learning methods, TriAD employs both inter-domain and intra-domain contrastive loss to learn common attributes among normal data and differentiate them from anomalies. Additionally, TriAD can detect anomalies of varying lengths by integrating with a discord discovery algorithm.This study is the first to reevaluate the deep learning potential in TSAD, utilizing both rigorously designed datasets (i.e., UCR Archive) and evaluation metrics (i.e., PA%K and affiliation). Through experimental results on the UCR dataset, TriAD achieves an impressive three-fold increase in PA%K based F1 scores over SOTA deep learning models, and a 50% increase in accuracy as compared to SOTA discord discovery algorithms.
</details></li>
</ul>
<hr>
<h2 id="FedRA-A-Random-Allocation-Strategy-for-Federated-Tuning-to-Unleash-the-Power-of-Heterogeneous-Clients"><a href="#FedRA-A-Random-Allocation-Strategy-for-Federated-Tuning-to-Unleash-the-Power-of-Heterogeneous-Clients" class="headerlink" title="FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients"></a>FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the Power of Heterogeneous Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11227">http://arxiv.org/abs/2311.11227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leondada/fedra">https://github.com/leondada/fedra</a></li>
<li>paper_authors: Shangchao Su, Bin Li, Xiangyang Xue</li>
<li>for: 这个论文是关于 federated learning 中的基础模型调参问题，特别是在多客户端上协同调参基础模型时，如何处理客户端之间的不同计算和通信资源问题。</li>
<li>methods: 该论文提出了一种新的 federated tuning 算法，即 FedRA，它可以在多客户端上协同调参基础模型，不需要修改原始模型。在每次通信轮次中，FedRA 随机生成分配矩阵，对于资源有限的客户端，它将重新组织一小部分层并使用 LoRA 进行微调，然后将更新后的 LoRA 参数回传给服务器。</li>
<li>results: 该论文在两个大规模的图像数据集上进行了实验，包括 DomainNet 和 NICO++，在不同的非同异设置下进行了比较，结果表明，FedRA 与比较方法相比，表现出了显著的优势。<details>
<summary>Abstract</summary>
With the increasing availability of Foundation Models, federated tuning has garnered attention in the field of federated learning, utilizing data and computation resources from multiple clients to collaboratively fine-tune foundation models. However, in real-world federated scenarios, there often exist a multitude of heterogeneous clients with varying computation and communication resources, rendering them incapable of supporting the entire model fine-tuning process. In response to this challenge, we propose a novel federated tuning algorithm, FedRA. The implementation of FedRA is straightforward and can be seamlessly integrated into any transformer-based model without the need for further modification to the original model. Specifically, in each communication round, FedRA randomly generates an allocation matrix. For resource-constrained clients, it reorganizes a small number of layers from the original model based on the allocation matrix and fine-tunes using LoRA. Subsequently, the server aggregates the updated LoRA parameters from the clients according to the current allocation matrix into the corresponding layers of the original model. It is worth noting that FedRA also supports scenarios where none of the clients can support the entire global model, which is an impressive advantage. We conduct experiments on two large-scale image datasets, DomainNet and NICO++, under various non-iid settings. The results demonstrate that FedRA outperforms the compared methods significantly. The source code is available at \url{https://github.com/leondada/FedRA}.
</details>
<details>
<summary>摘要</summary>
随着基础模型的可用性的增加，联邦调参在联邦学习领域受到了关注，利用多个客户端的数据和计算资源进行共同调参基础模型。然而，在实际的联邦场景中，有多种不同的计算和通信资源，使得客户端无法支持整个模型调参过程。为了解决这个挑战，我们提出了一种新的联邦调参算法，即 FedRA。FedRA的实现方式直观，可以透明地与任何基于转换器的模型集成，无需修改原始模型。在每次通信轮次中，FedRA随机生成一个分配矩阵。对于资源受限的客户端，它将原始模型中的一小部分层重新排序，根据分配矩阵进行练习，并使用LoRA进行微调。然后，服务器根据当前分配矩阵将客户端上的LoRA参数更新到原始模型中相应的层中。需要注意的是，FedRA还支持情况下，无法支持整个全球模型的客户端，这是一项非常优势的特点。我们在DomainNet和NICO++两个大规模图像 dataset上进行了多种非同域设置的实验，结果显示FedRA与比较方法相比有显著的优势。源代码可以在 GitHub 上获取，具体请参考 \url{https://github.com/leondada/FedRA}.
</details></li>
</ul>
<hr>
<h2 id="An-Interactive-Query-Generation-Assistant-using-LLM-based-Prompt-Modification-and-User-Feedback"><a href="#An-Interactive-Query-Generation-Assistant-using-LLM-based-Prompt-Modification-and-User-Feedback" class="headerlink" title="An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback"></a>An Interactive Query Generation Assistant using LLM-based Prompt Modification and User Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11226">http://arxiv.org/abs/2311.11226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaustubh D. Dhole, Ramraj Chandradevan, Eugene Agichtein<br>for: 这篇论文的目的是提出一种新的查询生成助手，用于帮助用户在搜索过程中提出更有效的查询。methods: 该论文使用了多种自然语言处理技术，如语言模型（LLM）和人工智能技术，以帮助用户提出更有效的查询。results: 该论文的实验结果表明，使用该查询生成助手可以帮助用户提出更有效的查询，并且可以帮助用户更好地满足他们的搜索需求。<details>
<summary>Abstract</summary>
While search is the predominant method of accessing information, formulating effective queries remains a challenging task, especially for situations where the users are not familiar with a domain, or searching for documents in other languages, or looking for complex information such as events, which are not easily expressible as queries. Providing example documents or passages of interest, might be easier for a user, however, such query-by-example scenarios are prone to concept drift, and are highly sensitive to the query generation method. This demo illustrates complementary approaches of using LLMs interactively, assisting and enabling the user to provide edits and feedback at all stages of the query formulation process. The proposed Query Generation Assistant is a novel search interface which supports automatic and interactive query generation over a mono-linguial or multi-lingual document collection. Specifically, the proposed assistive interface enables the users to refine the queries generated by different LLMs, to provide feedback on the retrieved documents or passages, and is able to incorporate the users' feedback as prompts to generate more effective queries. The proposed interface is a valuable experimental tool for exploring fine-tuning and prompting of LLMs for query generation to qualitatively evaluate the effectiveness of retrieval and ranking models, and for conducting Human-in-the-Loop (HITL) experiments for complex search tasks where users struggle to formulate queries without such assistance.
</details>
<details>
<summary>摘要</summary>
“寻找信息是主要的访问方法，但是寻找有效的查询仍然是一个困难的任务，特别是当用户不熟悉一个领域，搜寻文档written in other languages，或者搜寻复杂的信息，如事件，这些不易表示为查询。提供示例文档或 интерес的段落，可能是更容易的选择，但这些查询例子enario是易受概念变化的，并且高度受到查询生成方法的影响。本 demo 显示了辅助式查询生成器的可能性，这个 novel search interface 可以在单语言或多语言文档集上进行自动和互动查询生成。具体来说，这个辅助式查询生成器可以让用户在不同的 LLMs 生成的查询中进行修改和反馈，并且能够将用户的反馈当作提示，以生成更有效的查询。本 interface 是一个实用的实验工具，可以用于调整和提示 LLMs 的查询生成，以评估搜寻和排名模型的效iveness，以及进行人类在循环（HITL）实验，用于复杂的搜寻任务，当用户无法自动形成查询。”
</details></li>
</ul>
<hr>
<h2 id="SPLAIN-Augmenting-CybersecurityWarnings-with-Reasons-and-Data"><a href="#SPLAIN-Augmenting-CybersecurityWarnings-with-Reasons-and-Data" class="headerlink" title="SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data"></a>SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11215">http://arxiv.org/abs/2311.11215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vera A. Kazakova, Jena D. Hwang, Bonnie J. Dorr, Yorick Wilks, J. Blake Gage, Alex Memory, Mark A. Clark</li>
<li>for: 这篇论文旨在提供一种自然语言生成器，帮助用户更好地理解和预防网络攻击。</li>
<li>methods: 该论文提出了一种名为Simplified Plaintext Language（SPLAIN）的自然语言生成器，可以将警告数据转化为易于理解的网络攻击预测结果。SPLAIN使用模板基本法，确保警告结构和词汇准确性。</li>
<li>results: 该论文的实验结果表明，SPLAIN可以生成清晰、操作性强的网络攻击预测结果，并且可以在需要时扩展每个威胁和其组成部分，以便提供更多的信息。<details>
<summary>Abstract</summary>
Effective cyber threat recognition and prevention demand comprehensible forecasting systems, as prior approaches commonly offer limited and, ultimately, unconvincing information. We introduce Simplified Plaintext Language (SPLAIN), a natural language generator that converts warning data into user-friendly cyber threat explanations. SPLAIN is designed to generate clear, actionable outputs, incorporating hierarchically organized explanatory details about input data and system functionality. Given the inputs of individual sensor-induced forecasting signals and an overall warning from a fusion module, SPLAIN queries each signal for information on contributing sensors and data signals. This collected data is processed into a coherent English explanation, encompassing forecasting, sensing, and data elements for user review. SPLAIN's template-based approach ensures consistent warning structure and vocabulary. SPLAIN's hierarchical output structure allows each threat and its components to be expanded to reveal underlying explanations on demand. Our conclusions emphasize the need for designers to specify the "how" and "why" behind cyber warnings, advocate for simple structured templates in generating consistent explanations, and recognize that direct causal links in Machine Learning approaches may not always be identifiable, requiring some explanations to focus on general methodologies, such as model and training data.
</details>
<details>
<summary>摘要</summary>
有效的网络威胁认识和预防需要可读取的预测系统，因为先前的方法通常只提供有限和不可靠的信息。我们介绍了简化平文语言（SPLAIN），一种自然语言生成器，它将警告数据转换成易于理解的网络威胁说明。SPLAIN是为了生成清晰、可行的输出，并将输入数据和系统功能进行层次结构化解释。对于每个感知器生成的警告信号，SPLAIN会查询每个信号的相关感知器和数据信号。这些收集的数据被处理成一个 coherent English 说明，以便用户审查。SPLAIN 的模板化方法保证了警告结构的一致性和词汇。SPLAIN 的层次输出结构允许每个威胁和其组成部分进行扩展，以显示底层的解释。我们的结论认为设计者们应该指定警告中的 "如何" 和 "为什么"，支持简化的模板，并认可直接 causal links 在机器学习方法中可能无法识别，需要一些解释专注于方法ologies，如模型和训练数据。
</details></li>
</ul>
<hr>
<h2 id="Can-We-Utilize-Pre-trained-Language-Models-within-Causal-Discovery-Algorithms"><a href="#Can-We-Utilize-Pre-trained-Language-Models-within-Causal-Discovery-Algorithms" class="headerlink" title="Can We Utilize Pre-trained Language Models within Causal Discovery Algorithms?"></a>Can We Utilize Pre-trained Language Models within Causal Discovery Algorithms?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11212">http://arxiv.org/abs/2311.11212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chanhui Lee, Juhyeon Kim, Yongjun Jeong, Juhyun Lyu, Junghee Kim, Sangmin Lee, Sangjun Han, Hyeokjun Choe, Soyeon Park, Woohyung Lim, Sungbin Lim, Sanghack Lee</li>
<li>for: 本研究探讨了预训语言模型（PLM）在 causal reasoning 领域的可扩展性。</li>
<li>methods: 研究使用了 specifically designed prompts 来启发 PLM 进行多次 causal reasoning，并将其结果聚合以实现 causal discovery。</li>
<li>results: 研究发现 PLM-based causal reasoning 受到 prompt design 和 overconfidence 等限制，并提出了一种新的框架，通过结合 PLM 提取的 prior knowledge 和 causal discovery 算法来提高性能。<details>
<summary>Abstract</summary>
Scaling laws have allowed Pre-trained Language Models (PLMs) into the field of causal reasoning. Causal reasoning of PLM relies solely on text-based descriptions, in contrast to causal discovery which aims to determine the causal relationships between variables utilizing data. Recently, there has been current research regarding a method that mimics causal discovery by aggregating the outcomes of repetitive causal reasoning, achieved through specifically designed prompts. It highlights the usefulness of PLMs in discovering cause and effect, which is often limited by a lack of data, especially when dealing with multiple variables. Conversely, the characteristics of PLMs which are that PLMs do not analyze data and they are highly dependent on prompt design leads to a crucial limitation for directly using PLMs in causal discovery. Accordingly, PLM-based causal reasoning deeply depends on the prompt design and carries out the risk of overconfidence and false predictions in determining causal relationships. In this paper, we empirically demonstrate the aforementioned limitations of PLM-based causal reasoning through experiments on physics-inspired synthetic data. Then, we propose a new framework that integrates prior knowledge obtained from PLM with a causal discovery algorithm. This is accomplished by initializing an adjacency matrix for causal discovery and incorporating regularization using prior knowledge. Our proposed framework not only demonstrates improved performance through the integration of PLM and causal discovery but also suggests how to leverage PLM-extracted prior knowledge with existing causal discovery algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>语言模型（PLM）在 causal reasoning 中得到了扩展法律。 causal reasoning 通过文本描述来实现，而不是通过数据来确定 causal 关系。 recent research 表明，可以通过 repetitive causal reasoning 来模拟 causal discovery，并通过特定的提示来实现。 这种方法高亮了 PLM 在发现 cause 和 effect 方面的用途，特别是当 dealing 多个变量时。然而，PLM 的特点是它们不分析数据，依赖于提示设计，这导致了直接使用 PLM 在 causal discovery 中的限制。因此，PLM-based causal reasoning 的可靠性和准确性受到提示设计和 false 预测的影响。在本文中，我们通过对 physics-inspired 的 sintetic data 进行实验，证明了 PLM-based causal reasoning 中所存在的上述限制。然后，我们提出了一种新的框架，即将 PLM 提取的 prior knowledge 与 causal discovery 算法集成。通过在 causal discovery 中 initialize 一个相对matrix 和 incorporate 使用 prior knowledge 的 regularization，我们的提出的框架不仅在 integrating PLM 和 causal discovery 中表现出了改进的性能，还建议了如何将 PLM 提取的 prior knowledge 与现有的 causal discovery 算法集成。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Generative-AI-for-Clinical-Evidence-Summarization-Needs-to-Achieve-Trustworthiness"><a href="#Leveraging-Generative-AI-for-Clinical-Evidence-Summarization-Needs-to-Achieve-Trustworthiness" class="headerlink" title="Leveraging Generative AI for Clinical Evidence Summarization Needs to Achieve Trustworthiness"></a>Leveraging Generative AI for Clinical Evidence Summarization Needs to Achieve Trustworthiness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11211">http://arxiv.org/abs/2311.11211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongbo Zhang, Qiao Jin, Denis Jered McInerney, Yong Chen, Fei Wang, Curtis L. Cole, Qian Yang, Yanshan Wang, Bradley A. Malin, Mor Peleg, Byron C. Wallace, Zhiyong Lu, Chunhua Weng, Yifan Peng</li>
<li>for: 提高医疗质量，使医疗决策和实践受到最佳证据支持。</li>
<li>methods: 利用大语言模型等生成AI技术，自动概要医学证据。</li>
<li>results: 提高医学证据概要化的可靠性和公平性。<details>
<summary>Abstract</summary>
Evidence-based medicine aims to improve the quality of healthcare by empowering medical decisions and practices with the best available evidence. The rapid growth of medical evidence, which can be obtained from various sources, poses a challenge in collecting, appraising, and synthesizing the evidential information. Recent advancements in generative AI, exemplified by large language models, hold promise in facilitating the arduous task. However, developing accountable, fair, and inclusive models remains a complicated undertaking. In this perspective, we discuss the trustworthiness of generative AI in the context of automated summarization of medical evidence.
</details>
<details>
<summary>摘要</summary>
证据基础医学目的是提高医疗质量，通过最佳可用证据来决策和实践医疗。医疗证据的快速增长，从多种来源获取，评估和synthesize证据信息具有挑战。现代生成AI技术，如大语言模型，表现出潜在的优势，能够帮助解决这项复杂任务。然而，构建可信、公正、包容的模型仍然是一项复杂的任务。本观点讨论了生成AI在医学证据自动概要中的可靠性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Noise-Scheduling-for-Generating-Plausible-Designs-with-Diffusion-Models"><a href="#On-the-Noise-Scheduling-for-Generating-Plausible-Designs-with-Diffusion-Models" class="headerlink" title="On the Noise Scheduling for Generating Plausible Designs with Diffusion Models"></a>On the Noise Scheduling for Generating Plausible Designs with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11207">http://arxiv.org/abs/2311.11207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajie Fan, Laure Vuaille, Thomas Bäck, Hao Wang</li>
<li>for: 该论文主要针对 Deep Generative Models (DGMs) 在多个行业中创新设计，包括时尚和汽车等。</li>
<li>methods: 该论文使用 diffusion models 来生成图像，并研究了噪声规划对结果的影响。</li>
<li>results: 该论文发现，适当的噪声规划可以提高 DGM 生成的结果的可能性，从83.4% 提高到93.5%，并且降低 FID 值从7.84 降至4.87。<details>
<summary>Abstract</summary>
Deep Generative Models (DGMs) are widely used to create innovative designs across multiple industries, ranging from fashion to the automotive sector. In addition to generating images of high visual quality, the task of structural design generation imposes more stringent constrains on the semantic expression, e.g., no floating material or missing part, which we refer to as plausibility in this work. We delve into the impact of noise schedules of diffusion models on the plausibility of the outcome: there exists a range of noise levels at which the model's performance decides the result plausibility. Also, we propose two techniques to determine such a range for a given image set and devise a novel parametric noise schedule for better plausibility. We apply this noise schedule to the training and sampling of the well-known diffusion model EDM and compare it to its default noise schedule. Compared to EDM, our schedule significantly improves the rate of plausible designs from 83.4% to 93.5% and Fr\'echet Inception Distance (FID) from 7.84 to 4.87. Further applications of advanced image editing tools demonstrate the model's solid understanding of structure.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unmasking-and-Improving-Data-Credibility-A-Study-with-Datasets-for-Training-Harmless-Language-Models"><a href="#Unmasking-and-Improving-Data-Credibility-A-Study-with-Datasets-for-Training-Harmless-Language-Models" class="headerlink" title="Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models"></a>Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11202">http://arxiv.org/abs/2311.11202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/docta-ai/docta">https://github.com/docta-ai/docta</a></li>
<li>paper_authors: Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu</li>
<li>for: The paper focuses on evaluating the credibility of real-world datasets for training harmless language models, specifically addressing label errors in popular benchmarks such as Jigsaw Civil Comments, Anthropic Harmless &amp; Red Team, PKU BeaverTails &amp; SafeRLHF.</li>
<li>methods: The authors introduce a systematic framework for evaluating dataset credibility, identifying label errors, and assessing the impact of noisy labels on language learning. They use a combination of manual annotation and automated methods to identify and fix label errors.</li>
<li>results: The authors find and fix an average of 6.16% label errors in 11 datasets constructed from the benchmarks, which improves the data credibility and downstream learning performance of the models. The results demonstrate the significance of cleaning existing real-world datasets to ensure the quality of language models.Here’s the Simplified Chinese version of the three key points:</li>
<li>for: 这个研究专注于评估现实世界数据集的可靠性，以便使用安全语言模型进行训练，特别是关注标签错误在常用的 benchmar 如 Jigsaw Civil Comments、Anthropic Harmless &amp; Red Team、PKU BeaverTails &amp; SafeRLHF 中的影响。</li>
<li>methods: 作者们提出了一种系统化的数据集可靠性评估框架，包括标签错误的检测和评估不良标签对语言学习的影响。他们使用组合的手动标注和自动方法来检测和修复标签错误。</li>
<li>results: 作者们在11个数据集中平均发现和修复了6.16%的标签错误，这有效提高了数据集的可靠性和下游学习性能。结果表明，清洁现有的现实世界数据集是训练语言模型的重要前提。<details>
<summary>Abstract</summary>
Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Open-source: https://github.com/Docta-ai/docta.
</details>
<details>
<summary>摘要</summary>
Language models have shown promise in various tasks, but they can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Open-source: <https://github.com/Docta-ai/docta>.
</details></li>
</ul>
<hr>
<h2 id="Assessing-AI-Impact-Assessments-A-Classroom-Study"><a href="#Assessing-AI-Impact-Assessments-A-Classroom-Study" class="headerlink" title="Assessing AI Impact Assessments: A Classroom Study"></a>Assessing AI Impact Assessments: A Classroom Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11193">http://arxiv.org/abs/2311.11193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nari Johnson, Hoda Heidari</li>
<li>for: 这个研究是为了评估现有的人工智能影响评估工具（AIIAs）的有用性和有效性。</li>
<li>methods: 这个研究使用了一个类room研究（N &#x3D; 38），其中学生分配到不同的组织角色（如机器学习科学家或产品经理），并询问参与者们使用现有的AI影响评估工具来评估两个想象中的生成AI系统的可能的影响。</li>
<li>results: 研究发现参与者们通过完成这些活动后回答问卷表示出了对生成AI系统的风险的更好的认识，以及AI专家在避免可能的危害方面承担的责任的水平。此外，研究还发现现有的AIIA工具存在一些共同的局限性，包括格式和内容的问题，以及活动的可行性和有效性。<details>
<summary>Abstract</summary>
Artificial Intelligence Impact Assessments ("AIIAs"), a family of tools that provide structured processes to imagine the possible impacts of a proposed AI system, have become an increasingly popular proposal to govern AI systems. Recent efforts from government or private-sector organizations have proposed many diverse instantiations of AIIAs, which take a variety of forms ranging from open-ended questionnaires to graded score-cards. However, to date that has been limited evaluation of existing AIIA instruments. We conduct a classroom study (N = 38) at a large research-intensive university (R1) in an elective course focused on the societal and ethical implications of AI. We assign students to different organizational roles (for example, an ML scientist or product manager) and ask participant teams to complete one of three existing AI impact assessments for one of two imagined generative AI systems. In our thematic analysis of participants' responses to pre- and post-activity questionnaires, we find preliminary evidence that impact assessments can influence participants' perceptions of the potential risks of generative AI systems, and the level of responsibility held by AI experts in addressing potential harm. We also discover a consistent set of limitations shared by several existing AIIA instruments, which we group into concerns about their format and content, as well as the feasibility and effectiveness of the activity in foreseeing and mitigating potential harms. Drawing on the findings of this study, we provide recommendations for future work on developing and validating AIIAs.
</details>
<details>
<summary>摘要</summary>
人工智能影响评估工具（AIIA），一家家工具，提供结构化的过程，用于想象提案的人工智能系统的可能的影响。近些年，政府或私营组织的努力都提出了许多多样化的AIIA实现方案，这些方案从开放的问卷到分数卡等多种形式。然而，到目前为止，对现有AIIA工具的评估尚有限。我们在一所大型研究型大学（R1）的选修课中，对38名学生进行了教室实验。我们将学生分配到不同的组织角色（例如，机器学习科学家或产品经理），并让参与者队伍完成一个现有的AI影响评估工具，用于想象的两种生成AI系统。在我们对参与者们完成前和后活动问卷的分析中，我们发现了AI影响评估工具可以影响参与者对生成AI系统的可能风险的看法，以及AI专家负担应对可能危害的责任水平。我们还发现了许多现有AIIA工具的共同问题，包括格式和内容的问题，以及活动的可行性和效果。根据这些发现，我们提出了未来AIIA工具的开发和验证方向。
</details></li>
</ul>
<hr>
<h2 id="Attention-Based-Real-Time-Defenses-for-Physical-Adversarial-Attacks-in-Vision-Applications"><a href="#Attention-Based-Real-Time-Defenses-for-Physical-Adversarial-Attacks-in-Vision-Applications" class="headerlink" title="Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications"></a>Attention-Based Real-Time Defenses for Physical Adversarial Attacks in Vision Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11191">http://arxiv.org/abs/2311.11191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulio Rossolini, Alessandro Biondi, Giorgio Buttazzo</li>
<li>for: 防止深度神经网络受到真实世界中的敌意攻击，以确保其应用于安全关键领域。</li>
<li>methods: 提出了一种高效的注意力基于防御机制，通过在多帧设置中快速识别和跟踪恶意物体，并在 shallow 网络层中遮盖恶意效应。</li>
<li>results: 提高了现有的过度活动技术的性能，并在多帧设置中实现了高效的防御性能，同时也降低了计算成本。<details>
<summary>Abstract</summary>
Deep neural networks exhibit excellent performance in computer vision tasks, but their vulnerability to real-world adversarial attacks, achieved through physical objects that can corrupt their predictions, raises serious security concerns for their application in safety-critical domains. Existing defense methods focus on single-frame analysis and are characterized by high computational costs that limit their applicability in multi-frame scenarios, where real-time decisions are crucial.   To address this problem, this paper proposes an efficient attention-based defense mechanism that exploits adversarial channel-attention to quickly identify and track malicious objects in shallow network layers and mask their adversarial effects in a multi-frame setting. This work advances the state of the art by enhancing existing over-activation techniques for real-world adversarial attacks to make them usable in real-time applications. It also introduces an efficient multi-frame defense framework, validating its efficacy through extensive experiments aimed at evaluating both defense performance and computational cost.
</details>
<details>
<summary>摘要</summary>
为解决这个问题，这篇论文提出了一种高效的注意力基于防御机制，通过敌意通道注意力快速识别和跟踪恶意对象在浅网层中，并在多帧场景中遮盖其敌意效果。这项工作提高了现有的真实世界敌意攻击技术的可用性，使其在实时应用中使用。它还介绍了一种高效的多帧防御框架，通过广泛的实验证明了其防御性和计算成本的平衡。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.AI_2023_11_19/" data-id="clp88dbsd0080ob88evx837zm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/19/cs.CV_2023_11_19/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-11-19
        
      </div>
    </a>
  
  
    <a href="/2023/11/19/cs.CL_2023_11_19/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-11-19</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
