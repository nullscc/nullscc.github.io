
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-11-19 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11441 repo_url: None paper_authors: Vasilii G">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-11-19">
<meta property="og:url" content="https://nullscc.github.io/2023/11/19/cs.CL_2023_11_19/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11441 repo_url: None paper_authors: Vasilii G">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-19T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-22T00:13:03.096Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_11_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/19/cs.CL_2023_11_19/" class="article-date">
  <time datetime="2023-11-19T11:00:00.000Z" itemprop="datePublished">2023-11-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-11-19
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spot-the-Bot-Distinguishing-Human-Written-and-Bot-Generated-Texts-Using-Clustering-and-Information-Theory-Techniques"><a href="#Spot-the-Bot-Distinguishing-Human-Written-and-Bot-Generated-Texts-Using-Clustering-and-Information-Theory-Techniques" class="headerlink" title="Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques"></a>Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11441">http://arxiv.org/abs/2311.11441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilii Gromov, Quynh Nhu Dang</li>
<li>for: 本研究旨在提出一种基于无监督学习技术的 Bot 识别算法，不需要大量标注数据和&#x2F;或 Bot 模型架构的先验知识。</li>
<li>methods: 本研究使用语义分析（卷积和杂散）和信息技术，构建了一种可靠的 Bot 识别模型，可以识别不同类型的 Bot 生成的文本。</li>
<li>results: 研究发现，生成文本往往更加杂乱，而文学作品往往更加复杂；同时，人类文本 clustering 结果比 Bot 生成文本的 clustering 结果更加杂乱。<details>
<summary>Abstract</summary>
With the development of generative models like GPT-3, it is increasingly more challenging to differentiate generated texts from human-written ones. There is a large number of studies that have demonstrated good results in bot identification. However, the majority of such works depend on supervised learning methods that require labelled data and/or prior knowledge about the bot-model architecture. In this work, we propose a bot identification algorithm that is based on unsupervised learning techniques and does not depend on a large amount of labelled data. By combining findings in semantic analysis by clustering (crisp and fuzzy) and information techniques, we construct a robust model that detects a generated text for different types of bot. We find that the generated texts tend to be more chaotic while literary works are more complex. We also demonstrate that the clustering of human texts results in fuzzier clusters in comparison to the more compact and well-separated clusters of bot-generated texts.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>随着生成模型如GPT-3的发展，分辨生成文本和人类写的文本变得越来越困难。许多研究已经达到了对 bot 的识别的好结果，但大多数这些工作都依赖于supervised learning方法，需要标注数据和/或对 bot-model architecture 的先前知识。在这项工作中，我们提出了基于无监督学习技术的 bot 识别算法。通过结合 semantics 分析（crisp和fuzzy）和信息技术，我们构建了一个强健的模型，可以对不同类型的 bot 进行文本识别。我们发现生成文本往往更加混乱，而文学作品却更加复杂。此外，我们还示出了人类文本的 clustering 结果比 bot-generated 文本的 clustering 结果更加模糊。
</details></li>
</ul>
<hr>
<h2 id="ML-LMCL-Mutual-Learning-and-Large-Margin-Contrastive-Learning-for-Improving-ASR-Robustness-in-Spoken-Language-Understanding"><a href="#ML-LMCL-Mutual-Learning-and-Large-Margin-Contrastive-Learning-for-Improving-ASR-Robustness-in-Spoken-Language-Understanding" class="headerlink" title="ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding"></a>ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11375">http://arxiv.org/abs/2311.11375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuxin Cheng, Bowen Cao, Qichen Ye, Zhihong Zhu, Hongxiang Li, Yuexian Zou</li>
<li>for: 提高自动语音识别（ASR）错误率下的对话系统任务性能</li>
<li>methods: 提出了一种新的框架 called Mutual Learning and Large-Margin Contrastive Learning（ML-LMCL），通过在精度训练中进行互助学习，使两个对话系统模型在手动训练和ASR训练中分别学习，以便相互交换知识。还引入了距离偏好regularizer，以避免尽可能多地避免推迟内部对应的对话。</li>
<li>results: 实验结果表明，ML-LMCL在三个数据集上比现有模型表现更好，并实现了新的状态级表现。<details>
<summary>Abstract</summary>
Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback-Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
听话语理解（SLU）是对话系统中的基本任务。然而，自动语音识别（ASR）的不可避免的错误通常会影响理解性能，导致错误卷积。虽然有一些尝试通过对比学习解决这个问题，但它们（1）在细化学习中对清晰手动笔记和ASR笔记进行等同处理;（2）忽略了semantic相似对在应用对比学习时被推迟的问题;（3）受到Kullback-Leibler（KL）消失问题。在这篇论文中，我们提出了相互学习和大margin对比学习（ML-LMCL）框架，用于提高ASR对SLU的Robustness。具体来说，在细化学习中，我们将两个SLU模型在手动笔记和ASR笔记上进行相互学习，以便相互分享知识。我们还引入了距离浮动规则，以避免push away intra-cluster对话。此外，我们使用循环退化调度来mitigate KL消失问题。实验表明，ML-LMCL在三个数据集上的性能比既有模型更好，达到了新的状态率。
</details></li>
</ul>
<hr>
<h2 id="CHAMP-Efficient-Annotation-and-Consolidation-of-Cluster-Hierarchies"><a href="#CHAMP-Efficient-Annotation-and-Consolidation-of-Cluster-Hierarchies" class="headerlink" title="CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies"></a>CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11301">http://arxiv.org/abs/2311.11301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ariecattan/champ">https://github.com/ariecattan/champ</a></li>
<li>paper_authors: Arie Cattan, Tom Hope, Doug Downey, Roy Bar-Haim, Lilach Eden, Yoav Kantor, Ido Dagan</li>
<li>for: 该论文旨在提供一种用于快速构建文本中层次结构的开源工具，以便有效地进行文本级别的注释。</li>
<li>methods: 该工具使用增量式的方法，同时构建层次结构和集群，以提高注释效率和精度。</li>
<li>results: 该工具可以快速构建层次结构和集群，并且包含一个整合模式，可以方便地比较多个注释并解决不一致。<details>
<summary>Abstract</summary>
Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such hierarchical structures, we release CHAMP, an open source tool allowing to incrementally construct both clusters and hierarchy simultaneously over any type of texts. This incremental approach significantly reduces annotation time compared to the common pairwise annotation approach and also guarantees maintaining transitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a consolidation mode, where an adjudicator can easily compare multiple cluster hierarchy annotations and resolve disagreements.
</details>
<details>
<summary>摘要</summary>
各种自然语言处理任务需要复杂的层次结构，其中每个节点是一组项目。例如，生成涵义图、跨文档涵引Resolution、标注事件和子事件关系等。为了有效地注释这些层次结构，我们发布了 CHAMP 开源工具，它允许在不同类型文本上逐步构建层次结构和群集，并且同时保证层次结构和群集级别的转移性。此外， CHAMP 还包括一个整合模式，可以轻松地比较多个层次结构和群集注释，并解决不一致。
</details></li>
</ul>
<hr>
<h2 id="A-Cross-Attention-Augmented-Model-for-Event-Triggered-Context-Aware-Story-Generation"><a href="#A-Cross-Attention-Augmented-Model-for-Event-Triggered-Context-Aware-Story-Generation" class="headerlink" title="A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation"></a>A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11271">http://arxiv.org/abs/2311.11271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonywenuon/dialog-coherence-metric">https://github.com/tonywenuon/dialog-coherence-metric</a></li>
<li>paper_authors: Chen Tang, Tyler Loakman, Chenghua Lin<br>for:这篇论文旨在提高生成的故事质量，特别是在 incorporating 上下文和事件特征方面。methods:本文提出了一种基于神经网络的生成模型，即 EtriCA，该模型使用了跨注意力机制来将上下文特征映射到事件序列中，从而更好地利用事件之间的逻辑关系。此外，本文还提出了一种基于大规模书籍资料的知识增强框架（KeEtriCA），以便让模型适应更广泛的数据样本。results:实验结果表明，相比于现有的基准模型，EtriCA 模型在自动化指标和人工评估方面均有约5%的提升，而 KeEtriCA 框架在大规模书籍资料上进行了知识增强，可以让模型适应更广泛的数据样本，从而实现约10%的人工评估提升。<details>
<summary>Abstract</summary>
Despite recent advancements, existing story generation systems continue to encounter difficulties in effectively incorporating contextual and event features, which greatly influence the quality of generated narratives. To tackle these challenges, we introduce a novel neural generation model, EtriCA, that enhances the relevance and coherence of generated stories by employing a cross-attention mechanism to map context features onto event sequences through residual mapping. This feature capturing mechanism enables our model to exploit logical relationships between events more effectively during the story generation process. To further enhance our proposed model, we employ a post-training framework for knowledge enhancement (KeEtriCA) on a large-scale book corpus. This allows EtriCA to adapt to a wider range of data samples. This results in approximately 5\% improvement in automatic metrics and over 10\% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art (SOTA) baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results underscore the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives.
</details>
<details>
<summary>摘要</summary>
尽管最近的进步，现有的故事生成系统仍然困难地兼容上下文特征和事件特征，这些特征对生成的故事质量产生重要的影响。为解决这些挑战，我们介绍了一种新的神经网络生成模型，EtriCA，该模型通过杂合注意机制将上下文特征映射到事件序列中，以 residual mapping 的方式进行增强。这种特征捕捉机制使我们的模型更好地利用事件之间的逻辑关系，从而提高生成的故事质量。为进一步提高我们的提议模型，我们采用了一种基于大规模书籍资料的后期培养框架（KeEtriCA），这使得 EtriCA 能够适应更广泛的数据样本。这 resulted in approximately 5% improvement in automatic metrics and over 10% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art (SOTA) baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results underscore the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives.
</details></li>
</ul>
<hr>
<h2 id="Towards-Real-World-Writing-Assistance-A-Chinese-Character-Checking-Benchmark-with-Faked-and-Misspelled-Characters"><a href="#Towards-Real-World-Writing-Assistance-A-Chinese-Character-Checking-Benchmark-with-Faked-and-Misspelled-Characters" class="headerlink" title="Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters"></a>Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11268">http://arxiv.org/abs/2311.11268</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUKElab/Visual-C3">https://github.com/THUKElab/Visual-C3</a></li>
<li>paper_authors: Yinghui Li, Zishan Xu, Shaoshen Chen, Haojing Huang, Yangning Li, Yong Jiang, Zhongli Li, Qingyu Zhou, Hai-Tao Zheng, Ying Shen</li>
<li>for: 本研究的目的是提高中文输入文本的正确率和质量，尤其是在拼写错误的情况下。</li>
<li>methods: 本研究使用了人工标注的视觉中文字符检查数据集Visual-C$^3$,以及一些基线方法的评估。</li>
<li>results: 实验结果表明，Visual-C$^3$ 是一个高质量又具有挑战性的数据集，而基线方法在这个数据集上的性能也得到了证明。<details>
<summary>Abstract</summary>
Writing assistance is an application closely related to human life and is also a fundamental Natural Language Processing (NLP) research field. Its aim is to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. From the perspective of the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters mainly caused by phonological or visual confusion, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present Visual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C$^3$ is the first real-world visual and the largest human-crafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C$^3$. Extensive empirical results and analyses show that Visual-C$^3$ is high-quality yet challenging. The Visual-C$^3$ dataset and the baseline methods will be publicly available to facilitate further research in the community.
</details>
<details>
<summary>摘要</summary>
文字辅助是人类生活中密切相关的应用，同时也是自然语言处理（NLP）研究领域的基础领域。它的目标是提高输入文本的正确性和质量，特别是在检查和 corrections 中的字符检查非常重要。从实际世界的手写角度来看，人类写错的字符包括伪造字符（即由写入错误而创造的不真实字符）和拼写错误的字符（即正确的字符 incorrect 使用）。然而，现有的数据集和相关研究仅对拼写错误的字符进行研究，忽略了伪造字符，这些伪造字符更为常见和困难。为突破这个困境，我们提出了 Visual-C$^3$，一个由人工标注的视觉中文字符检查数据集，包括伪造和拼写错误的中文字符。我们知道，Visual-C$^3$ 是实际世界中第一个真实的视觉数据集，也是最大的人工制作的中文字符检查数据集。此外，我们还提出了和评估了基eline 方法。广泛的实验结果和分析表明，Visual-C$^3$ 具有高质量 yet 挑战性。Visual-C$^3$ 数据集和基eline 方法将在未来的研究中提供可用的资源。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Large-Language-Models-in-Mental-Health-Applications"><a href="#Rethinking-Large-Language-Models-in-Mental-Health-Applications" class="headerlink" title="Rethinking Large Language Models in Mental Health Applications"></a>Rethinking Large Language Models in Mental Health Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11267">http://arxiv.org/abs/2311.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoxiong Ji, Tianlin Zhang, Kailai Yang, Sophia Ananiadou, Erik Cambria</li>
<li>for: 这篇论文探讨了使用自然语言处理技术在心理健康领域的应用。</li>
<li>methods: 论文提出了关于使用生成模型的稳定性和可靠性问题，以及生成模型可能生成幻见的问题。</li>
<li>results: 论文认为，人工智能专家的Empathy、细腻的解释和Contextual awareness在心理健康咨询中是不可取代的。使用生成模型应该是以人类专家为主的，而不是寻求取代人类专家的方式。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have become valuable assets in mental health, showing promise in both classification tasks and counseling applications. This paper offers a perspective on using LLMs in mental health applications. It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability. The paper also distinguishes between the often interchangeable terms ``explainability'' and ``interpretability'', advocating for developing inherently interpretable methods instead of relying on potentially hallucinated self-explanations generated by LLMs. Despite the advancements in LLMs, human counselors' empathetic understanding, nuanced interpretation, and contextual awareness remain irreplaceable in the sensitive and complex realm of mental health counseling. The use of LLMs should be approached with a judicious and considerate mindset, viewing them as tools that complement human expertise rather than seeking to replace it.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:大型语言模型（LLMs）在心理健康方面已成为有价值资产，在分类任务和辅助应用中都表现出了承诺。这篇文章对使用LLMs在心理健康领域进行了观点，并讨论了预测过程中的生成模型不稳定性和潜在的幻觉输出问题，强调需要持续进行审核和评估以保持其可靠性和可预测性。文章还区分了“可解释性”和“可解读性”这两个术语，强调开发内在可解释的方法而不是仰仗可能生成的自我解释。虽然LLMs在心理健康领域的应用已经取得了进步，但人类师长的Empathy、细化解释和Contextual awareness仍然在敏感和复杂的心理健康辅导中不可或缺。使用LLMs应该具有谨慎和考虑的心态，视其为人类专业知识的辅助工具而不是寻求替代。
</details></li>
</ul>
<hr>
<h2 id="Causal-ATE-Mitigates-Unintended-Bias-in-Controlled-Text-Generation"><a href="#Causal-ATE-Mitigates-Unintended-Bias-in-Controlled-Text-Generation" class="headerlink" title="Causal ATE Mitigates Unintended Bias in Controlled Text Generation"></a>Causal ATE Mitigates Unintended Bias in Controlled Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11229">http://arxiv.org/abs/2311.11229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Madhavan, Kahini Wadhawan</li>
<li>for: 这个论文是关于语言模型中的属性控制问题，通过 causal average treatment effect（Causal ATE）方法进行研究的。</li>
<li>methods: exist 的方法是通过 sentence 中的词语与属性相关的方法来进行控制，但这些方法可能会受到训练数据中的偶散关系的影响，导致模型在推理过程中假设存在属性。这个论文提出了一种简单的摈剂基于的方法，可以消除这种不良影响。</li>
<li>results: 该论文提出的方法可以减少 false positive 的数量，从而解决了不良偏见的问题。具体来说，这种方法在减少抑制保护群体的不良偏见方面具有重要的作用。<details>
<summary>Abstract</summary>
We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methods for the attribute control task in Language Models (LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Additionally, we offer a theoretical foundation for investigating Causal ATE in the classification task, and prove that it reduces the number of false positives -- thereby mitigating the issue of unintended bias. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/19/cs.CL_2023_11_19/" data-id="clp9qz82m00fgok884nba62tl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/19/cs.AI_2023_11_19/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-11-19
        
      </div>
    </a>
  
  
    <a href="/2023/11/19/cs.LG_2023_11_19/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-11-19</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
