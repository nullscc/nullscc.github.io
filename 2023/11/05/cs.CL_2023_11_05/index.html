
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-11-05 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.02777 repo_url: None paper_authors: Michael Ginn, Alexis Pal">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-11-05">
<meta property="og:url" content="https://nullscc.github.io/2023/11/05/cs.CL_2023_11_05/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.02777 repo_url: None paper_authors: Michael Ginn, Alexis Pal">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-05T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-07T04:32:35.013Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/cs.CL_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T11:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-11-05
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Robust-Generalization-Strategies-for-Morpheme-Glossing-in-an-Endangered-Language-Documentation-Context"><a href="#Robust-Generalization-Strategies-for-Morpheme-Glossing-in-an-Endangered-Language-Documentation-Context" class="headerlink" title="Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context"></a>Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02777">http://arxiv.org/abs/2311.02777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Ginn, Alexis Palmer</li>
<li>for:  investigate the ability of morpheme labeling models to generalize in resource-constrained settings</li>
<li>methods:  weight decay optimization, output denoising, and iterative pseudo-labeling</li>
<li>results:  achieve a 2% improvement on a test set containing texts from unseen genres<details>
<summary>Abstract</summary>
Generalization is of particular importance in resource-constrained settings, where the available training data may represent only a small fraction of the distribution of possible texts. We investigate the ability of morpheme labeling models to generalize by evaluating their performance on unseen genres of text, and we experiment with strategies for closing the gap between performance on in-distribution and out-of-distribution data. Specifically, we use weight decay optimization, output denoising, and iterative pseudo-labeling, and achieve a 2% improvement on a test set containing texts from unseen genres. All experiments are performed using texts written in the Mayan language Uspanteko.
</details>
<details>
<summary>摘要</summary>
通用化在有限资源的设置中 particualrly important，因为可能只有一小部分的可用训练数据表示整个分布中的文本。我们 investigate morpheme labeling模型的通用性，通过评估它们在未看到的类型的文本中的性能，并试用了将数据分布减少到另一个数据分布的策略。Specifically, we use weight decay optimization, output denoising, and iterative pseudo-labeling, and achieve a 2% improvement on a test set containing texts from unseen genres. All experiments are performed using texts written in the Mayan language Uspanteko.Note that Simplified Chinese uses a different set of characters and grammar compared to Traditional Chinese, so the translation may look slightly different.
</details></li>
</ul>
<hr>
<h2 id="Attention-or-Convolution-Transformer-Encoders-in-Audio-Language-Models-for-Inference-Efficiency"><a href="#Attention-or-Convolution-Transformer-Encoders-in-Audio-Language-Models-for-Inference-Efficiency" class="headerlink" title="Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency"></a>Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02772">http://arxiv.org/abs/2311.02772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungho Jeon, Ching-Feng Yeh, Hakan Inan, Wei-Ning Hsu, Rashi Rungta, Yashar Mehdad, Daniel Bikel</li>
<li>for: 这个论文目的是为了证明一种简单的自动学习音频模型可以达到与更复杂的预训练模型相同的推理效率。</li>
<li>methods: 这个论文使用了混合卷积模块和自注意模块的speech transformer Encoder来实现。</li>
<li>results: 研究发现，使用这种speech transformer Encoder可以大幅提高预训练音频模型的推理效率，并且可以与更复杂的预训练模型相当。此外，研究还发现，使用高精度的自注意模块可以提高模型的推理效率，但是使用低位数量的权重量quantization技术可以提高模型的可行性。<details>
<summary>Abstract</summary>
In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们显示了一个简单的自动学习预训练音频模型可以达到与更复杂的预训练模型（具有语音变换器Encoder）相似的推理效率。这些语音变换器通过混合卷积模块和自我注意模块来实现。它们在ASR中实现了状态的最佳性能，并且我们首先显示了使用这些语音变换器作为Encoder可以显著改善预训练音频模型的效率。然而，我们的研究表明，我们可以通过高级自注意来达到相似的效率。我们示出了这种更简单的方法在使用低位数量权重量化网络来提高效率时是特别有利。我们假设，这种方法可以避免在不同量化模块之间传递错误，相比于最新的语音变换器混合量化卷积和量化自注意模块。
</details></li>
</ul>
<hr>
<h2 id="Pyclipse-a-library-for-deidentification-of-free-text-clinical-notes"><a href="#Pyclipse-a-library-for-deidentification-of-free-text-clinical-notes" class="headerlink" title="Pyclipse, a library for deidentification of free-text clinical notes"></a>Pyclipse, a library for deidentification of free-text clinical notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02748">http://arxiv.org/abs/2311.02748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Callandra Moore, Jonathan Ranisau, Walter Nelson, Jeremy Petch, Alistair Johnson</li>
<li>for: 本研究旨在提供一个可重现性的自动隐私化评估框架，以便实现手动隐私化的高成本降低，促进临床自然语言处理的进步。</li>
<li>methods: 本研究使用的方法包括提出了一个可 configurable 的评估程序，允许用户在本地临床数据上运行开源隐私化算法，并实现了上述算法之间的比较。</li>
<li>results: 本研究发现，使用不同的文本处理方法、评估方法和来源数据会导致隐私化算法的表现不一，即使在同一个 benchmark 数据集上进行评估。这些差异强调了评估隐私化算法的complexity，并 highlighted 需要一个可重现性、可调整和可扩展的框架，如pyclipse。<details>
<summary>Abstract</summary>
Automated deidentification of clinical text data is crucial due to the high cost of manual deidentification, which has been a barrier to sharing clinical text and the advancement of clinical natural language processing. However, creating effective automated deidentification tools faces several challenges, including issues in reproducibility due to differences in text processing, evaluation methods, and a lack of consistency across clinical domains and institutions. To address these challenges, we propose the pyclipse framework, a unified and configurable evaluation procedure to streamline the comparison of deidentification algorithms. Pyclipse serves as a single interface for running open-source deidentification algorithms on local clinical data, allowing for context-specific evaluation. To demonstrate the utility of pyclipse, we compare six deidentification algorithms across four public and two private clinical text datasets. We find that algorithm performance consistently falls short of the results reported in the original papers, even when evaluated on the same benchmark dataset. These discrepancies highlight the complexity of accurately assessing and comparing deidentification algorithms, emphasizing the need for a reproducible, adjustable, and extensible framework like pyclipse. Our framework lays the foundation for a unified approach to evaluate and improve deidentification tools, ultimately enhancing patient protection in clinical natural language processing.
</details>
<details>
<summary>摘要</summary>
自动化医疗文本数据隐藏是非常重要，因为手动隐藏的成本太高，这成为了分享医疗文本和临床自然语言处理的障碍。然而，创建有效的自动化隐藏工具面临着一些挑战，包括复制性问题由于文本处理方法的不同、评价方法的不一致以及临床领域和机构之间的不一致。为解决这些挑战，我们提出了pyclipse框架，一个统一和可配置的评价过程，以便对临床文本数据进行开源隐藏算法的比较。pyclipse提供了一个单一的界面，用于在本地临床数据上运行开源隐藏算法，以便根据特定的上下文进行评价。为了证明pyclipse的实用性，我们比较了六种隐藏算法在四个公共和两个私人临床文本数据集上的性能。我们发现，算法的性能通常比原始论文中报道的结果低，即使在同一个标准数据集上进行评价。这些差异高亮了评价和比较隐藏算法的复杂性，强调了需要一个可重现、可调整和可扩展的框架，如pyclipse。我们的框架为临床自然语言处理中的患者隐藏做出了基础，以增强患者的保护。
</details></li>
</ul>
<hr>
<h2 id="Nepali-Video-Captioning-using-CNN-RNN-Architecture"><a href="#Nepali-Video-Captioning-using-CNN-RNN-Architecture" class="headerlink" title="Nepali Video Captioning using CNN-RNN Architecture"></a>Nepali Video Captioning using CNN-RNN Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02699">http://arxiv.org/abs/2311.02699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bipesh Subedi, Saugat Singh, Bal Krishna Bal<br>for: 本研究旨在开发一个基于深度神经网络的尼泊尔视频标题生成系统，以提供更 precises和相关的尼泊尔视频描述。methods: 该研究使用了预训练的CNN和RNN，并将MSVD数据集扩展为尼泊尔语标题。研究采用了数据预处理、模型实现和评估等步骤。results: 研究发现，使用EfficientNetB0和BiLSTM结构的模型可以 дости得BLEU-4分数17和METEOR分数46。此外，研究还描述了在尼泊尔语视频标题生成中遇到的挑战和未来研究的方向。<details>
<summary>Abstract</summary>
This article presents a study on Nepali video captioning using deep neural networks. Through the integration of pre-trained CNNs and RNNs, the research focuses on generating precise and contextually relevant captions for Nepali videos. The approach involves dataset collection, data preprocessing, model implementation, and evaluation. By enriching the MSVD dataset with Nepali captions via Google Translate, the study trains various CNN-RNN architectures. The research explores the effectiveness of CNNs (e.g., EfficientNetB0, ResNet101, VGG16) paired with different RNN decoders like LSTM, GRU, and BiLSTM. Evaluation involves BLEU and METEOR metrics, with the best model being EfficientNetB0 + BiLSTM with 1024 hidden dimensions, achieving a BLEU-4 score of 17 and METEOR score of 46. The article also outlines challenges and future directions for advancing Nepali video captioning, offering a crucial resource for further research in this area.
</details>
<details>
<summary>摘要</summary>
To enrich the MSVD dataset with Nepali captions, the researchers use Google Translate. They train various CNN-RNN architectures, including EfficientNetB0, ResNet101, and VGG16, paired with different RNN decoders like long short-term memory (LSTM), gated recurrent unit (GRU), and bidirectional LSTM (BiLSTM). The evaluation metrics used are BLEU and METEOR, and the best model is EfficientNetB0 + BiLSTM with 1024 hidden dimensions, achieving a BLEU-4 score of 17 and METEOR score of 46.The article also discusses challenges and future directions for advancing Nepali video captioning, providing a valuable resource for further research in this area.
</details></li>
</ul>
<hr>
<h2 id="LLM-enhanced-Self-training-for-Cross-domain-Constituency-Parsing"><a href="#LLM-enhanced-Self-training-for-Cross-domain-Constituency-Parsing" class="headerlink" title="LLM-enhanced Self-training for Cross-domain Constituency Parsing"></a>LLM-enhanced Self-training for Cross-domain Constituency Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02660">http://arxiv.org/abs/2311.02660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianling Li, Meishan Zhang, Peiming Guo, Min Zhang, Yue Zhang</li>
<li>for: 本研究探讨了自动训练在跨 домен constituency parsing 中的应用。</li>
<li>methods: 我们提议使用大语言模型（LLM）生成域pecific的raw corpora，以便自动训练。我们还引入了grammar rules来导引LLM生成raw corpora，并设置了pseudo数据选择的 criterion。</li>
<li>results: 我们的实验结果显示，自动训练加LLM在constituency parsing中的性能高于传统方法，无论LLM的性能如何。此外，grammar rules和confidence criterion的结合使得cross-domain constituency parsing的性能最高。<details>
<summary>Abstract</summary>
Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low-quality raw corpora. To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the constituency parsing, we introduce grammar rules that guide the LLM in generating raw corpora and establish criteria for selecting pseudo instances. Our experimental results demonstrate that self-training for constituency parsing, equipped with an LLM, outperforms traditional methods regardless of the LLM's performance. Moreover, the combination of grammar rules and confidence criteria for pseudo-data selection yields the highest performance in the cross-domain constituency parsing.
</details>
<details>
<summary>摘要</summary>
自我训练已经证明是跨领域任务中有效的方法，在这项研究中，我们探讨了它在跨领域成分分析中的应用。传统的自我训练方法受到有限的和可能低质量的原始raw corpora的限制。为了突破这一限制，我们提议利用大语言模型（LLM）来生成领域特定的raw corpora，并在每一轮training中进行多次迭代。对于成分分析，我们引入了语法规则，以引导LLM生成raw corpora，并确定pseudo实例的选择标准。我们的实验结果表明，带有LLM的自我训练对成分分析进行跨领域均有superior表现，而且将语法规则和信任度标准结合使用，可以在跨领域成分分析中实现最高的表现。
</details></li>
</ul>
<hr>
<h2 id="Divide-Conquer-for-Entailment-aware-Multi-hop-Evidence-Retrieval"><a href="#Divide-Conquer-for-Entailment-aware-Multi-hop-Evidence-Retrieval" class="headerlink" title="Divide &amp; Conquer for Entailment-aware Multi-hop Evidence Retrieval"></a>Divide &amp; Conquer for Entailment-aware Multi-hop Evidence Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02616">http://arxiv.org/abs/2311.02616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Luo, Mihai Surdeanu</li>
<li>for: Answering multi-hop questions by retrieving relevant evidence</li>
<li>methods: 使用文本相似性和推理相似性两个Sub-任务，并将其结合以提高各种相关性信号的推理效果</li>
<li>results: 在HotpotQA上实验表明，提出的模型不仅与单独的检索模型相比显著提高了表现，还比两个直觉的组合基线模型更有效。<details>
<summary>Abstract</summary>
Lexical and semantic matches are commonly used as relevance measurements for information retrieval. Together they estimate the semantic equivalence between the query and the candidates. However, semantic equivalence is not the only relevance signal that needs to be considered when retrieving evidences for multi-hop questions. In this work, we demonstrate that textual entailment relation is another important relevance dimension that should be considered. To retrieve evidences that are either semantically equivalent to or entailed by the question simultaneously, we divide the task of evidence retrieval for multi-hop question answering (QA) into two sub-tasks, i.e., semantic textual similarity and inference similarity retrieval. We propose two ensemble models, EAR and EARnest, which tackle each of the sub-tasks separately and then jointly re-rank sentences with the consideration of the diverse relevance signals. Experimental results on HotpotQA verify that our models not only significantly outperform all the single retrieval models it is based on, but is also more effective than two intuitive ensemble baseline models.
</details>
<details>
<summary>摘要</summary>
lexical和semantic匹配通常用于信息检索中的相关性评估。它们共同估计查询和候选结果之间的semantic相等性。但semantic相等性并不是多步问题回答中唯一的相关性信号。在这种情况下，我们表明了文本包含关系是另一个重要的相关性维度。为了同时从多个角度检索对于多步问题回答的证据，我们将证据检索任务分为两个互补性任务：semantic textual similarity和inference similarity检索。我们提议了两种ensemble模型：EAR和EARnest，它们分别处理每个子任务，然后将其结果联合重新排序 sentences，考虑多种相关性信号的多样性。实验结果表明，我们的模型不仅在HotpotQA上表现出色，而且也比两个直觉ensemble基线模型更有效。
</details></li>
</ul>
<hr>
<h2 id="mahaNLP-A-Marathi-Natural-Language-Processing-Library"><a href="#mahaNLP-A-Marathi-Natural-Language-Processing-Library" class="headerlink" title="mahaNLP: A Marathi Natural Language Processing Library"></a>mahaNLP: A Marathi Natural Language Processing Library</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02579">http://arxiv.org/abs/2311.02579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidula Magdum, Omkar Dhekane, Sharayu Hiwarkhedkar, Saloni Mittal, Raviraj Joshi</li>
<li>for: 提供了一个开源的自然语言处理（NLP）库，Specifically built for the Marathi language.</li>
<li>methods: 使用了state-of-the-art MahaBERT-based transformer models，Easy to use, extensible, and modular toolkit for Marathi text analysis.</li>
<li>results: 提供了一个包含多个NLP任务的完整array，包括基本的预处理任务和进阶的NLP任务，如情感分析、名实分辨、讨厌言语检测和句子完成。Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 这个论文是为了提供一个开源的自然语言处理（NLP）库，Specifically built for the Marathi language。</li>
<li>methods: 这个库使用了state-of-the-art MahaBERT-based transformer models，Easy to use, extensible, and modular toolkit for Marathi text analysis。</li>
<li>results: 这个库提供了一个包含多个NLP任务的完整array，包括基本的预处理任务和进阶的NLP任务，如情感分析、名实分辨、讨厌言语检测和句子完成。<details>
<summary>Abstract</summary>
We present mahaNLP, an open-source natural language processing (NLP) library specifically built for the Marathi language. It aims to enhance the support for the low-resource Indian language Marathi in the field of NLP. It is an easy-to-use, extensible, and modular toolkit for Marathi text analysis built on state-of-the-art MahaBERT-based transformer models. Our work holds significant importance as other existing Indic NLP libraries provide basic Marathi processing support and rely on older models with restricted performance. Our toolkit stands out by offering a comprehensive array of NLP tasks, encompassing both fundamental preprocessing tasks and advanced NLP tasks like sentiment analysis, NER, hate speech detection, and sentence completion. This paper focuses on an overview of the mahaNLP framework, its features, and its usage. This work is a part of the L3Cube MahaNLP initiative, more information about it can be found at https://github.com/l3cube-pune/MarathiNLP .
</details>
<details>
<summary>摘要</summary>
我们介绍mahaNLP，一个开源的自然语言处理（NLP）库，专门为旁遮普语言（Marathi）设计。它旨在提高低资源印度语言Marathi在NLP领域的支持。它是一个易于使用、可扩展、归结的工具集，基于现代的MahaBERT变换器模型。我们的工作具有重要性，因为其他现有的印度语言NLP库只提供基本的Marathi处理支持，并且基于较老的模型，性能更 restricted。我们的工具集包括了各种NLP任务，包括基本的预处理任务和高级NLP任务，如情感分析、命名实体识别、恶意言语检测和句子完成。本文将介绍mahaNLP框架、其特点和使用方法。这是L3Cube MahaNLP计划的一部分，更多信息可以在https://github.com/l3cube-pune/MarathiNLP 找到。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Sequencing-of-Documents"><a href="#Temporal-Sequencing-of-Documents" class="headerlink" title="Temporal Sequencing of Documents"></a>Temporal Sequencing of Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02578">http://arxiv.org/abs/2311.02578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Gervers, Gelila Tilahun</li>
<li>for: 这个论文是为了解决历史文档的排序问题，具体来说是美国州OF THE UNIONAddresses和DEEDS古英文财产转让文档集。</li>
<li>methods: 这个论文使用了一种无监督的方法，通过计算word使用的慢变化来对文档进行排序。这种方法使用了Generalized Linear Models（Fan、Heckman、Wand，1995）的非 Parametric预测，并使用Simulated Annealing算法来解决 combinatorial优化问题。</li>
<li>results: 这个论文的结果表明，使用这种无监督方法可以有效地排序历史文档，并且比 randomly sequenced baseline要好。这种方法可以应用于无日期的文档集。<details>
<summary>Abstract</summary>
We outline an unsupervised method for temporal rank ordering of sets of historical documents, namely American State of the Union Addresses and DEEDS, a corpus of medieval English property transfer documents. Our method relies upon effectively capturing the gradual change in word usage via a bandwidth estimate for the non-parametric Generalized Linear Models (Fan, Heckman, and Wand, 1995). The number of possible rank orders needed to search through possible cost functions related to the bandwidth can be quite large, even for a small set of documents. We tackle this problem of combinatorial optimization using the Simulated Annealing algorithm, which allows us to obtain the optimal document temporal orders. Our rank ordering method significantly improved the temporal sequencing of both corpora compared to a randomly sequenced baseline. This unsupervised approach should enable the temporal ordering of undated document sets.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无监督的方法来排序历史文档集合，包括美国国情咨文和中世纪英国财产转让文件集。我们的方法基于有效地捕捉文本中慢慢变化的词汇使用幅度，通过非参数化概线性模型（Fan、Heckman和Wand，1995）来估算带宽。寻找可能的排序方案的可能性数量可能会很大，即使是一个小型文档集合也可能需要很长时间。我们使用伪随机扰动算法来解决这个问题，从而获得最佳的文档排序。我们的排序方法在对两个文档集合进行排序时有显著改善，相比Random Baseline。这种无监督的方法应该能够应用于无日期文档集合的排序问题。
</details></li>
</ul>
<hr>
<h2 id="BanMANI-A-Dataset-to-Identify-Manipulated-Social-Media-News-in-Bangla"><a href="#BanMANI-A-Dataset-to-Identify-Manipulated-Social-Media-News-in-Bangla" class="headerlink" title="BanMANI: A Dataset to Identify Manipulated Social Media News in Bangla"></a>BanMANI: A Dataset to Identify Manipulated Social Media News in Bangla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02570">http://arxiv.org/abs/2311.02570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim</li>
<li>for: 本研究旨在探讨社交媒体新闻中假新闻和误导性新闻的识别，特别是指将关键语句与参考文章进行误导性修改的现象。</li>
<li>methods: 我们采用了一种数据收集方法，使用了限制可用的NLP工具在孟加拉语中进行收集，并将收集到的数据标记为各种信息操纵。</li>
<li>results: 我们的分析发现，当用于零参数和微调设置时，现有的LLMs都面临着挑战，表明这是一个具有挑战性的任务。<details>
<summary>Abstract</summary>
Initial work has been done to address fake news detection and misrepresentation of news in the Bengali language. However, no work in Bengali yet addresses the identification of specific claims in social media news that falsely manipulates a related news article. At this point, this problem has been tackled in English and a few other languages, but not in the Bengali language. In this paper, we curate a dataset of social media content labeled with information manipulation relative to reference articles, called BanMANI. The dataset collection method we describe works around the limitations of the available NLP tools in Bangla. We expect these techniques will carry over to building similar datasets in other low-resource languages. BanMANI forms the basis both for evaluating the capabilities of existing NLP systems and for training or fine-tuning new models specifically on this task. In our analysis, we find that this task challenges current LLMs both under zero-shot and fine-tuned settings.
</details>
<details>
<summary>摘要</summary>
先期工作已经进行了假新闻检测和新闻歪曲的处理，但是没有任何工作在孟加拉语中处理特定新闻媒体中的假消息。在这篇论文中，我们精心准备了一个社交媒体内容标注了信息歪曲相关新闻的数据集，called BanMANI。我们的数据集采集方法考虑了当地的NLP工具的限制。我们预计这些技术将在其他低资源语言上进行推广。BanMANI将成为评估现有NLP系统的基础，以及特定任务上训练或细化新模型的基础。在我们的分析中，我们发现这个任务会挑战当前的LLMs，包括零shot和精度调整的情况。
</details></li>
</ul>
<hr>
<h2 id="Topic-model-based-on-co-occurrence-word-networks-for-unbalanced-short-text-datasets"><a href="#Topic-model-based-on-co-occurrence-word-networks-for-unbalanced-short-text-datasets" class="headerlink" title="Topic model based on co-occurrence word networks for unbalanced short text datasets"></a>Topic model based on co-occurrence word networks for unbalanced short text datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02566">http://arxiv.org/abs/2311.02566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjie Ma, Junping Du, Meiyu Liang, Zeli Guan</li>
<li>for: 本研究旨在探讨短文件中罕见话题的检测，以解决短文件中话题的稀缺性和不均匀性问题。</li>
<li>methods: 本研究提出了一种基于合occurrence字网络的话题模型，称为CWUTM，该模型利用字网络来捕捉每个字的话题分布，并通过调整节点活动和话题表示的方法，提高了检测罕见话题的敏感度。</li>
<li>results: 对于不同的短文件数据集，CWUTM模型在检测罕见话题的任务上表现出优于基eline方法，并且可以在早期准确地检测出emerging话题或意外事件。<details>
<summary>Abstract</summary>
We propose a straightforward solution for detecting scarce topics in unbalanced short-text datasets. Our approach, named CWUTM (Topic model based on co-occurrence word networks for unbalanced short text datasets), Our approach addresses the challenge of sparse and unbalanced short text topics by mitigating the effects of incidental word co-occurrence. This allows our model to prioritize the identification of scarce topics (Low-frequency topics). Unlike previous methods, CWUTM leverages co-occurrence word networks to capture the topic distribution of each word, and we enhanced the sensitivity in identifying scarce topics by redefining the calculation of node activity and normalizing the representation of both scarce and abundant topics to some extent. Moreover, CWUTM adopts Gibbs sampling, similar to LDA, making it easily adaptable to various application scenarios. Our extensive experimental validation on unbalanced short-text datasets demonstrates the superiority of CWUTM compared to baseline approaches in discovering scarce topics. According to the experimental results the proposed model is effective in early and accurate detection of emerging topics or unexpected events on social platforms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种直接 Addressing the challenge of sparse and unbalanced short text topics, our approach, named CWUTM (based on co-occurrence word networks for unbalanced short text datasets), mitigates the effects of incidental word co-occurrence, allowing our model to prioritize the identification of scarce topics (low-frequency topics). Unlike previous methods, CWUTM leverages co-occurrence word networks to capture the topic distribution of each word, and we enhance the sensitivity in identifying scarce topics by redefining the calculation of node activity and normalizing the representation of both scarce and abundant topics to some extent. Moreover, CWUTM adopts Gibbs sampling, similar to LDA, making it easily adaptable to various application scenarios. Our extensive experimental validation on unbalanced short-text datasets demonstrates the superiority of CWUTM compared to baseline approaches in discovering scarce topics. According to the experimental results, the proposed model is effective in early and accurate detection of emerging topics or unexpected events on social platforms.Here's the word-for-word translation:我们提出了一种直接解决稀缺和不均衡短文数据集中的挑战的方法，我们称之为CWUTM（基于相互occurrence word networks for unbalanced short text datasets）。我们的方法可以减轻因为意外的word co-occurrence的影响，使我们的模型更加偏好发现稀缺话题（low-frequency topics）。与前一代方法不同，CWUTM利用相互occurrence word networks来捕捉每个word的话题分布，并通过重新定义节点活动计算和normalize话题表示来提高发现稀缺话题的敏感度。此外，CWUTM采用Gibbs sampling，与LDA类似，使其容易适应不同应用场景。我们对稀缺短文数据集进行了广泛的实验验证，结果表明CWUTM在发现稀缺话题方面超过了基eline方法的表现。据实验结果，提议的模型能够在社交平台上早期准确地探测出emerging topics或意外事件。
</details></li>
</ul>
<hr>
<h2 id="Relation-Extraction-Model-Based-on-Semantic-Enhancement-Mechanism"><a href="#Relation-Extraction-Model-Based-on-Semantic-Enhancement-Mechanism" class="headerlink" title="Relation Extraction Model Based on Semantic Enhancement Mechanism"></a>Relation Extraction Model Based on Semantic Enhancement Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02564">http://arxiv.org/abs/2311.02564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyu Liu, Junping Du, Yingxia Shao, Zeli Guan</li>
<li>for: 解决 triple overlap 问题</li>
<li>methods: 基于 CasRel 框架和 semantic enhancement mechanism</li>
<li>results: 提高 relation extraction 效果， луч于处理 triple overlap 问题<details>
<summary>Abstract</summary>
Relational extraction is one of the basic tasks related to information extraction in the field of natural language processing, and is an important link and core task in the fields of information extraction, natural language understanding, and information retrieval. None of the existing relation extraction methods can effectively solve the problem of triple overlap. The CasAug model proposed in this paper based on the CasRel framework combined with the semantic enhancement mechanism can solve this problem to a certain extent. The CasAug model enhances the semantics of the identified possible subjects by adding a semantic enhancement mechanism, First, based on the semantic coding of possible subjects, pre-classify the possible subjects, and then combine the subject lexicon to calculate the semantic similarity to obtain the similar vocabulary of possible subjects. According to the similar vocabulary obtained, each word in different relations is calculated through the attention mechanism. For the contribution of the possible subject, finally combine the relationship pre-classification results to weight the enhanced semantics of each relationship to find the enhanced semantics of the possible subject, and send the enhanced semantics combined with the possible subject to the object and relationship extraction module. Complete the final relation triplet extraction. The experimental results show that, compared with the baseline model, the CasAug model proposed in this paper has improved the effect of relation extraction, and CasAug's ability to deal with overlapping problems and extract multiple relations is also better than the baseline model, indicating that the semantic enhancement mechanism proposed in this paper It can further reduce the judgment of redundant relations and alleviate the problem of triple overlap.
</details>
<details>
<summary>摘要</summary>
关系提取是信息提取领域的基本任务之一，是信息提取、自然语言理解和信息检索领域的重要链接和核心任务。现有的关系提取方法无法有效解决三重重叠问题。这篇论文提出的CasAug模型基于CasRel框架和semantic enhancement机制，可以在一定程度上解决这个问题。CasAug模型对可能的主题进行semantic enhancement，首先根据可能的主题的semantic coding进行预类别，然后将主题词典与计算semantic similarity来获取相似词语的集合。根据这些相似词语集合，使用注意机制计算每个关系中的每个词语的权重。最后，将预类别结果与权重结果组合，以获取增强的主题 semantics。将增强的主题 semantics与可能的主题一起传递给对象和关系提取模块，以完成最终的关系三元组提取。实验结果显示，相比基eline模型，本文提出的CasAug模型在关系提取效果上有所提高，而且CasAug能够更好地处理重叠问题，提取多个关系，表明semantic enhancement机制可以进一步减少纬度重复的判断和三重重叠问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/cs.CL_2023_11_05/" data-id="cloojsme300dxre882mqn9qgz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/05/cs.AI_2023_11_05/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-11-05
        
      </div>
    </a>
  
  
    <a href="/2023/11/05/cs.LG_2023_11_05/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-11-05</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
