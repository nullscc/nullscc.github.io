
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-11-21 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Intrinsic Image Decomposition via Ordinal Shading paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12792 repo_url: https:&#x2F;&#x2F;github.com&#x2F;compphoto&#x2F;Intrinsic paper_authors: Chris Careaga, Yağız Aksoy for: 高级视觉问题：解剖照明">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-11-21">
<meta property="og:url" content="https://nullscc.github.io/2023/11/21/cs.AI_2023_11_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Intrinsic Image Decomposition via Ordinal Shading paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12792 repo_url: https:&#x2F;&#x2F;github.com&#x2F;compphoto&#x2F;Intrinsic paper_authors: Chris Careaga, Yağız Aksoy for: 高级视觉问题：解剖照明">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-21T12:00:00.000Z">
<meta property="article:modified_time" content="2023-12-09T06:43:17.704Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_11_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/21/cs.AI_2023_11_21/" class="article-date">
  <time datetime="2023-11-21T12:00:00.000Z" itemprop="datePublished">2023-11-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-11-21
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Intrinsic-Image-Decomposition-via-Ordinal-Shading"><a href="#Intrinsic-Image-Decomposition-via-Ordinal-Shading" class="headerlink" title="Intrinsic Image Decomposition via Ordinal Shading"></a>Intrinsic Image Decomposition via Ordinal Shading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12792">http://arxiv.org/abs/2311.12792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/compphoto/Intrinsic">https://github.com/compphoto/Intrinsic</a></li>
<li>paper_authors: Chris Careaga, Yağız Aksoy</li>
<li>for: 高级视觉问题：解剖照明照片和计算摄影涂抹</li>
<li>methods: 使用分解问题为两个部分：首先使用稳定的损失函数来估算精度的排序颜色信息，然后将低分辨率和高分辨率的估算结果集成在一起，通过第二个网络来生成具有全局协调和地方细节的颜色估算。</li>
<li>results: 对比州前方法，我们的方法可以更高精度地估算照明照片的内在分解结果，并且可以进行难以实现的编辑任务，如重新彩色和重新照明。<details>
<summary>Abstract</summary>
Intrinsic decomposition is a fundamental mid-level vision problem that plays a crucial role in various inverse rendering and computational photography pipelines. Generating highly accurate intrinsic decompositions is an inherently under-constrained task that requires precisely estimating continuous-valued shading and albedo. In this work, we achieve high-resolution intrinsic decomposition by breaking the problem into two parts. First, we present a dense ordinal shading formulation using a shift- and scale-invariant loss in order to estimate ordinal shading cues without restricting the predictions to obey the intrinsic model. We then combine low- and high-resolution ordinal estimations using a second network to generate a shading estimate with both global coherency and local details. We encourage the model to learn an accurate decomposition by computing losses on the estimated shading as well as the albedo implied by the intrinsic model. We develop a straightforward method for generating dense pseudo ground truth using our model's predictions and multi-illumination data, enabling generalization to in-the-wild imagery. We present an exhaustive qualitative and quantitative analysis of our predicted intrinsic components against state-of-the-art methods. Finally, we demonstrate the real-world applicability of our estimations by performing otherwise difficult editing tasks such as recoloring and relighting.
</details>
<details>
<summary>摘要</summary>
内在分解是视觉领域的基本中等问题，对各种 inverse rendering 和计算摄影管道起着关键作用。生成高度准确的内在分解是一个具有内在约束的不充分定的任务，需要精准地估计连续值的颜色和反射率。在这种工作中，我们通过分解问题为两个部分来实现高分辨率内在分解。首先，我们提出了一种密集的ORDINAL颜色表示方法，使用偏移量和缩放不变的损失函数来估计ORDINAL颜色提示，而不是限制预测符合内在模型。然后，我们将低分辨率和高分辨率的ORDINAL估计结合使用第二个网络来生成具有全局准确性和本地细节的颜色估计。我们将模型学习准确地分解，通过计算预测的颜色和内在模型预测的反射率之间的损失函数。我们开发了一种简单的方法来生成密集的pseudo实际数据，使得模型可以通过多光源数据进行普适化。我们进行了详细的qualitative和量化分析，证明我们的预测结果与现状方法相比具有明显的优势。最后，我们示出了使用我们的估计结果进行其他难以完成的编辑任务，如重新颜色和重新照明。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Impairment-and-Disease-Severity-Using-AI-Models-Trained-on-Healthy-Subjects"><a href="#Quantifying-Impairment-and-Disease-Severity-Using-AI-Models-Trained-on-Healthy-Subjects" class="headerlink" title="Quantifying Impairment and Disease Severity Using AI Models Trained on Healthy Subjects"></a>Quantifying Impairment and Disease Severity Using AI Models Trained on Healthy Subjects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12781">http://arxiv.org/abs/2311.12781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fishneck/cobra">https://github.com/fishneck/cobra</a></li>
<li>paper_authors: Boyang Yu, Aakash Kaku, Kangning Liu, Avinash Parnandi, Emily Fokas, Anita Venkatesan, Natasha Pandit, Rajesh Ranganath, Heidi Schambra, Carlos Fernandez-Granda</li>
<li>for: 这 paper aims to address the challenge of automatic assessment of impairment and disease severity in data-driven medicine.</li>
<li>methods: The proposed framework leverages AI models trained exclusively on healthy individuals to quantify the deviation of impaired or diseased patients from the healthy population.</li>
<li>results: The COBRA score, computed automatically in under one minute, is strongly correlated with the gold-standard Fugl-Meyer Assessment on two different data modalities (wearable sensors and video) and demonstrates generalizability to other conditions such as knee osteoarthritis.<details>
<summary>Abstract</summary>
Automatic assessment of impairment and disease severity is a key challenge in data-driven medicine. We propose a novel framework to address this challenge, which leverages AI models trained exclusively on healthy individuals. The COnfidence-Based chaRacterization of Anomalies (COBRA) score exploits the decrease in confidence of these models when presented with impaired or diseased patients to quantify their deviation from the healthy population. We applied the COBRA score to address a key limitation of current clinical evaluation of upper-body impairment in stroke patients. The gold-standard Fugl-Meyer Assessment (FMA) requires in-person administration by a trained assessor for 30-45 minutes, which restricts monitoring frequency and precludes physicians from adapting rehabilitation protocols to the progress of each patient. The COBRA score, computed automatically in under one minute, is shown to be strongly correlated with the FMA on an independent test cohort for two different data modalities: wearable sensors ($\rho = 0.845$, 95% CI [0.743,0.908]) and video ($\rho = 0.746$, 95% C.I [0.594, 0.847]). To demonstrate the generalizability of the approach to other conditions, the COBRA score was also applied to quantify severity of knee osteoarthritis from magnetic-resonance imaging scans, again achieving significant correlation with an independent clinical assessment ($\rho = 0.644$, 95% C.I [0.585,0.696]).
</details>
<details>
<summary>摘要</summary>
自动评估残疾和疾病严重度是数据驱动医学中的关键挑战。我们提出了一种新的框架，利用专门用于健康人群训练的AI模型来解决这一挑战。我们称之为COBRA分数（Confidence-Based chaRacterization of Anomalies），它利用这些模型对受损或疾病患者的诊断时的信任度下降来衡量其与健康人群的偏差。我们应用了COBRA分数来解决现有的评估障碍肢上的问题，由于评估需要专业评估员在面对面进行30-45分钟的评估，这限制了监测频率和防止医生根据患者的进步而适应复制办法。COBRA分数可以在一分钟内自动计算，与FMA（Fugl-Meyer评估）的黄金标准相比，在独立测试集上显示强相关性（$\rho = 0.845$, 95% CI [0.743,0.908]）和视频数据（$\rho = 0.746$, 95% C.I [0.594, 0.847]）两种数据模式上。为了证明该方法的普适性，我们还应用了COBRA分数来评估关节风扁病的严重程度，并在独立评估中获得了 statistically significant相关性（$\rho = 0.644$, 95% C.I [0.585,0.696]）。
</details></li>
</ul>
<hr>
<h2 id="Digital-Twin-Framework-for-Optimal-and-Autonomous-Decision-Making-in-Cyber-Physical-Systems-Enhancing-Reliability-and-Adaptability-in-the-Oil-and-Gas-Industry"><a href="#Digital-Twin-Framework-for-Optimal-and-Autonomous-Decision-Making-in-Cyber-Physical-Systems-Enhancing-Reliability-and-Adaptability-in-the-Oil-and-Gas-Industry" class="headerlink" title="Digital Twin Framework for Optimal and Autonomous Decision-Making in Cyber-Physical Systems: Enhancing Reliability and Adaptability in the Oil and Gas Industry"></a>Digital Twin Framework for Optimal and Autonomous Decision-Making in Cyber-Physical Systems: Enhancing Reliability and Adaptability in the Oil and Gas Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12755">http://arxiv.org/abs/2311.12755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carine Menezes Rebello, Johannes Jäschkea, Idelfonso B. R. Nogueira</li>
<li>for: 这个研究的目的是为了提出一个能够在生产过程中实现自主决策的数位双（DT）框架，以增强生产过程中的可靠性和适应力。</li>
<li>methods: 这个研究使用了数位双框架，融合了潜在推理、蒙地卡罗 simulate、传播学习、在线学习和新的战略，包括模型高维度减少和认知攻击。</li>
<li>results: 这个研究获得了一个能够高效、可靠、信任worthy的数位双识别框架，并且可以在变化的环境下适应和 incorporating prediction uncertainty，进一步增强了生产过程中的决策过程。<details>
<summary>Abstract</summary>
The concept of creating a virtual copy of a complete Cyber-Physical System opens up numerous possibilities, including real-time assessments of the physical environment and continuous learning from the system to provide reliable and precise information. This process, known as the twinning process or the development of a digital twin (DT), has been widely adopted across various industries. However, challenges arise when considering the computational demands of implementing AI models, such as those employed in digital twins, in real-time information exchange scenarios. This work proposes a digital twin framework for optimal and autonomous decision-making applied to a gas-lift process in the oil and gas industry, focusing on enhancing the robustness and adaptability of the DT. The framework combines Bayesian inference, Monte Carlo simulations, transfer learning, online learning, and novel strategies to confer cognition to the DT, including model hyperdimensional reduction and cognitive tack. Consequently, creating a framework for efficient, reliable, and trustworthy DT identification was possible. The proposed approach addresses the current gap in the literature regarding integrating various learning techniques and uncertainty management in digital twin strategies. This digital twin framework aims to provide a reliable and efficient system capable of adapting to changing environments and incorporating prediction uncertainty, thus enhancing the overall decision-making process in complex, real-world scenarios. Additionally, this work lays the foundation for further developments in digital twins for process systems engineering, potentially fostering new advancements and applications across various industrial sectors.
</details>
<details>
<summary>摘要</summary>
“创建虚拟复制物的概念打开了许多可能性，如实时评估物理环境和持续学习系统以提供可靠和精准的信息。这个过程，称为虚拟孪生（DT）的开发过程，在不同的业务中广泛应用。但是，在实时信息交换场景中考虑AI模型的计算需求时，会遇到挑战。这种工作提出了一个用于油气业中气动压过程的数字孪生框架，以提高DT的可靠性和自适应性。该框架结合 bayesian 推理、蒙地卡罗 simulate、传输学习、在线学习和创新的策略，包括模型高维度减少和认知护卫。因此，创建高效、可靠、信任worthy的DT标识框架成为可能。该提出的方法填补了现有文献中 интеGRating 多种学习技术和不确定性管理在数字孪生策略中的空白。这个数字孪生框架目标是提供可靠和高效的系统，能够适应变化的环境，并将预测不确定性纳入决策过程中，从而提高复杂实际场景中的决策过程。此外，这种工作为数字孪生技术的进一步发展和应用奠定了基础，可能激发新的进步和应用于不同的产业领域。”
</details></li>
</ul>
<hr>
<h2 id="SelfOcc-Self-Supervised-Vision-Based-3D-Occupancy-Prediction"><a href="#SelfOcc-Self-Supervised-Vision-Based-3D-Occupancy-Prediction" class="headerlink" title="SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction"></a>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12754">http://arxiv.org/abs/2311.12754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huang-yh/selfocc">https://github.com/huang-yh/selfocc</a></li>
<li>paper_authors: Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, Jiwen Lu</li>
<li>for: 提高视觉自动驾驶的稳定性，预测周围3D空间中每个点的占用状态。</li>
<li>methods: 使用自我超级vised学习方法，只使用视频序列来学习3D占用状态。将图像转换到3D空间（例如，鸟瞰视角），然后对3D表示进行约束，并使用多个深度提案来直接优化SDF引用的 weights。</li>
<li>results: 与前一个最佳方法SceneRF相比，SelfOcc在SemanticKITTI上提高了58.7%的性能，并在Occ3D上生成了可靠的3D占用状态。在SemanticKITTI、KITTI-2015和nuScenes等 datasets上，SelfOcc实现了高质量的深度和达到了状态监测的最新水平。代码：<a target="_blank" rel="noopener" href="https://github.com/huang-yh/SelfOcc%E3%80%82">https://github.com/huang-yh/SelfOcc。</a><details>
<summary>Abstract</summary>
3D occupancy prediction is an important task for the robustness of vision-centric autonomous driving, which aims to predict whether each point is occupied in the surrounding 3D space. Existing methods usually require 3D occupancy labels to produce meaningful results. However, it is very laborious to annotate the occupancy status of each voxel. In this paper, we propose SelfOcc to explore a self-supervised way to learn 3D occupancy using only video sequences. We first transform the images into the 3D space (e.g., bird's eye view) to obtain 3D representation of the scene. We directly impose constraints on the 3D representations by treating them as signed distance fields. We can then render 2D images of previous and future frames as self-supervision signals to learn the 3D representations. We propose an MVS-embedded strategy to directly optimize the SDF-induced weights with multiple depth proposals. Our SelfOcc outperforms the previous best method SceneRF by 58.7% using a single frame as input on SemanticKITTI and is the first self-supervised work that produces reasonable 3D occupancy for surround cameras on Occ3D. SelfOcc produces high-quality depth and achieves state-of-the-art results on novel depth synthesis, monocular depth estimation, and surround-view depth estimation on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code: https://github.com/huang-yh/SelfOcc.
</details>
<details>
<summary>摘要</summary>
“3D占用预测是自主驾驶视觉系统中的一项重要任务，它目的是预测环境中每个点是否占用。现有方法通常需要3D占用标签以生成有意义的结果。然而， annotating each voxel's occupancy status is very laborious。在这篇论文中，我们提出了一种自动学习的方法，名为SelfOcc，以使用仅仅是视频序列来学习3D占用。我们首先将图像转换到3D空间（例如，鸟瞰视）以获得场景的3D表示。我们直接对3D表示进行约束，并将其视为签名距离场景。我们可以将先前和后续帧的2D图像渲染成自我超级视觉信号，以便学习3D表示。我们提出了一种嵌入MVS的策略，以直接优化SDF引起的权重。我们的SelfOcc在使用单帧输入时与前一个最佳方法SceneRF的比较达到58.7%的提升。SelfOcc是第一个使用自动学习方法生成有理性3D占用的周边摄像头的自主驾驶方法。SelfOcc在SemanticKITTI、KITTI-2015和nuScenes等三个数据集上取得了最高质量的深度和状态图像，并在新的深度Synthesis、单目深度估计和周边视图深度估计方面取得了最高的成绩。代码：https://github.com/huang-yh/SelfOcc。”
</details></li>
</ul>
<hr>
<h2 id="Image-Transformation-for-IoT-Time-Series-Data-A-Review"><a href="#Image-Transformation-for-IoT-Time-Series-Data-A-Review" class="headerlink" title="Image Transformation for IoT Time-Series Data: A Review"></a>Image Transformation for IoT Time-Series Data: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12742">http://arxiv.org/abs/2311.12742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duygu Altunkaya, Feyza Yildirim Okay, Suat Ozdemir</li>
<li>for: 本研究评论使用图像变换&#x2F;编码技术在IoT领域中进行时间序列数据分类。</li>
<li>methods: 本研究使用图像变换&#x2F;编码技术来转换IoT时间序列数据，并对这些图像进行学习模型训练。</li>
<li>results: 本研究发现，使用图像变换&#x2F;编码技术可以提高IoT时间序列数据分类的性能，并且可以捕捉到时间序列数据中隐藏的动态模式和趋势。<details>
<summary>Abstract</summary>
In the era of the Internet of Things (IoT), where smartphones, built-in systems, wireless sensors, and nearly every smart device connect through local networks or the internet, billions of smart things communicate with each other and generate vast amounts of time-series data. As IoT time-series data is high-dimensional and high-frequency, time-series classification or regression has been a challenging issue in IoT. Recently, deep learning algorithms have demonstrated superior performance results in time-series data classification in many smart and intelligent IoT applications. However, it is hard to explore the hidden dynamic patterns and trends in time-series. Recent studies show that transforming IoT data into images improves the performance of the learning model. In this paper, we present a review of these studies which use image transformation/encoding techniques in IoT domain. We examine the studies according to their encoding techniques, data types, and application areas. Lastly, we emphasize the challenges and future dimensions of image transformation.
</details>
<details>
<summary>摘要</summary>
在互联网物联网（IoT）时代，智能手机、内置系统、无线传感器和几乎所有智能设备通过本地网络或互联网相连，数以亿计的智能东西通过无线方式进行通信，生成大量时间序列数据。由于IoT时间序列数据具有高维度和高频率，时间序列分类或回归成为IoT中的挑战。然而，深度学习算法在IoT应用中表现出色，尤其是在时间序列数据分类方面。然而，挖掘时间序列中隐藏的动态模式和趋势却是一大挑战。近年来，一些研究使用图像转换/编码技术来解决这个问题。在本文中，我们对这些研究进行了一个回顾，分析了他们的编码技术、数据类型和应用领域。最后，我们还强调了图像转换的挑战和未来维度。
</details></li>
</ul>
<hr>
<h2 id="Content-Augmented-Graph-Neural-Networks"><a href="#Content-Augmented-Graph-Neural-Networks" class="headerlink" title="Content Augmented Graph Neural Networks"></a>Content Augmented Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12741">http://arxiv.org/abs/2311.12741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fatemehgholamzadeh/augss-gnn">https://github.com/fatemehgholamzadeh/augss-gnn</a></li>
<li>paper_authors: Fatemeh Gholamzadeh Nasrabadi, AmirHossein Kashani, Pegah Zahedi, Mostafa Haghir Chehreghani</li>
<li>for: 本文提出了一种基于图 neural network (GNN) 的方法，以解决图上问题。这些模型通常利用图的链接结构，并在每层更新节点的嵌入。但是，在这些层次上应用的筛子或卷积会导致初始嵌入的影响减弱，无法在最终嵌入中产生重要效果。</li>
<li>methods: 本文提出了一种解决这个问题的方法，即在 GNN 层次上计算节点的结构嵌入和内容嵌入，并将这两个嵌入组合成节点的嵌入。此外，文章还提出了一些实现方法，如使用自动编码器或建立内容图。</li>
<li>results: 通过在多个真实世界数据集上进行实验，文章证明了该方法的高精度和性能。<details>
<summary>Abstract</summary>
In recent years, graph neural networks (GNNs) have become a popular tool for solving various problems over graphs. In these models, the link structure of the graph is typically exploited and nodes' embeddings are iteratively updated based on adjacent nodes. Nodes' contents are used solely in the form of feature vectors, served as nodes' first-layer embeddings. However, the filters or convolutions, applied during iterations/layers to these initial embeddings lead to their impact diminish and contribute insignificantly to the final embeddings. In order to address this issue, in this paper we propose augmenting nodes' embeddings by embeddings generating from their content, at higher GNN layers. More precisely, we propose models wherein a structural embedding using a GNN and a content embedding are computed for each node. These two are combined using a combination layer to form the embedding of a node at a given layer. We suggest methods such as using an auto-encoder or building a content graph, to generate content embeddings. In the end, by conducting experiments over several real-world datasets, we demonstrate the high accuracy and performance of our models.
</details>
<details>
<summary>摘要</summary>
近年来，图 нейрон网络（GNNs）已成为解决各种图问题的流行工具。在这些模型中，图的链接结构通常被利用，并且节点的嵌入是基于邻居节点的更新。节点的内容仅用作节点的第一层嵌入，而在迭代层次中应用的筛选器或卷积操作会导致它们的影响减弱，对最终嵌入的贡献微不足。为了解决这问题，在本文中我们提出了在高层次GNN层次添加节点嵌入的方法。具体来说，我们提出了一种结构嵌入使用GNN和内容嵌入，并将这两个嵌入组合在一起，以形成每个节点的嵌入。我们还提出了使用自动编码器或构建内容图来生成内容嵌入的方法。在实验中，我们通过使用多个真实世界数据集，证明了我们的模型的高精度和性能。
</details></li>
</ul>
<hr>
<h2 id="Development-of-a-Legal-Document-AI-Chatbot"><a href="#Development-of-a-Legal-Document-AI-Chatbot" class="headerlink" title="Development of a Legal Document AI-Chatbot"></a>Development of a Legal Document AI-Chatbot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12719">http://arxiv.org/abs/2311.12719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Nataraj Devaraj, Rakesh Teja P V, Aaryav Gangrade, Manoj Kumar R</li>
<li>for: 该论文目的是开发一个智能的法律文档谈判机器人，以帮助处理大量的数字法律文档。</li>
<li>methods: 该论文使用了许多相关的技术，包括机器学习、自然语言处理和人机交互。具体来说，该论文使用了一个Android应用程序和Langchain查询处理代码，并通过Flask后端和REST API方式进行集成。</li>
<li>results: 该论文通过实现了一个功能强大的法律文档谈判机器人，可以帮助减少法律文档的处理时间和劳动力成本。<details>
<summary>Abstract</summary>
With the exponential growth of digital data and the increasing complexity of legal documentation, there is a pressing need for efficient and intelligent tools to streamline the handling of legal documents.With the recent developments in the AI field, especially in chatbots, it cannot be ignored as a very compelling solution to this problem.An insight into the process of creating a Legal Documentation AI Chatbot with as many relevant features as possible within the given time frame is presented.The development of each component of the chatbot is presented in detail.Each component's workings and functionality has been discussed.Starting from the build of the Android app and the Langchain query processing code till the integration of both through a Flask backend and REST API methods.
</details>
<details>
<summary>摘要</summary>
中文简体版随着数字数据的急速增长和法律文书的日益复杂化，有一项急需快速有效的工具来简化法律文书的处理。尤其是在人工智能领域的最新发展，尝试不可以忽略聊天机器人的潜在解决方案。本文将对创建法律文书AI聊天机器人的过程进行详细介绍，包括最 relevante 的功能。从安卓应用程序的构建到语言链查询处理代码的集成，以及通过Flask后端和REST API方法的集成。Each component of the chatbot and its workings have been discussed in detail, including the build of the Android app and the Langchain query processing code, as well as the integration of both through a Flask backend and REST API methods.
</details></li>
</ul>
<hr>
<h2 id="minimax-Efficient-Baselines-for-Autocurricula-in-JAX"><a href="#minimax-Efficient-Baselines-for-Autocurricula-in-JAX" class="headerlink" title="minimax: Efficient Baselines for Autocurricula in JAX"></a>minimax: Efficient Baselines for Autocurricula in JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12716">http://arxiv.org/abs/2311.12716</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/minimax">https://github.com/facebookresearch/minimax</a></li>
<li>paper_authors: Minqi Jiang, Michael Dennis, Edward Grefenstette, Tim Rocktäschel</li>
<li>for: 本研究旨在提高自动课程学习（Unsupervised Environment Design，UED）的训练效率，以便培养具有零基eline转移能力的决策maker。</li>
<li>methods: 本研究使用JAX实现了全张量环境和自动课程算法，并使用加速硬件进行编译。</li>
<li>results: 与前一代实现相比，本研究在同等批处理大小的情况下实现了更 чем 120倍的速度提升。<details>
<summary>Abstract</summary>
Unsupervised environment design (UED) is a form of automatic curriculum learning for training robust decision-making agents to zero-shot transfer into unseen environments. Such autocurricula have received much interest from the RL community. However, UED experiments, based on CPU rollouts and GPU model updates, have often required several weeks of training. This compute requirement is a major obstacle to rapid innovation for the field. This work introduces the minimax library for UED training on accelerated hardware. Using JAX to implement fully-tensorized environments and autocurriculum algorithms, minimax allows the entire training loop to be compiled for hardware acceleration. To provide a petri dish for rapid experimentation, minimax includes a tensorized grid-world based on MiniGrid, in addition to reusable abstractions for conducting autocurricula in procedurally-generated environments. With these components, minimax provides strong UED baselines, including new parallelized variants, which achieve over 120$\times$ speedups in wall time compared to previous implementations when training with equal batch sizes. The minimax library is available under the Apache 2.0 license at https://github.com/facebookresearch/minimax.
</details>
<details>
<summary>摘要</summary>
自动课程学习环境设计（UED）是一种自动课程学习方法，用于培养具有零shot传输能力的决策agt。这种自动课程学习方法在RL社区中受到了广泛的关注。然而，UED实验通常需要数周的训练时间，这使得快速创新在该领域受到了阻碍。本工作介绍了用于UED训练的加速硬件库minimax。使用JAX实现完全张量化环境和自动课程算法，minimax可以将整个训练循环编译到硬件加速器上。为提供快速实验的环境，minimax包含了张量化网格世界，以及可重用的自动课程抽象，用于在生成的环境中进行自动课程。通过这些组件，minimax提供了强大的UED基线，包括新的并行化变体，其在与批处理大小相同的情况下实现了120倍的增速。minimax库可以在Apache 2.0 许可下从https://github.com/facebookresearch/minimax获取。
</details></li>
</ul>
<hr>
<h2 id="Alpha-Zero-for-Physics-Application-of-Symbolic-Regression-with-Alpha-Zero-to-find-the-analytical-methods-in-physics"><a href="#Alpha-Zero-for-Physics-Application-of-Symbolic-Regression-with-Alpha-Zero-to-find-the-analytical-methods-in-physics" class="headerlink" title="Alpha Zero for Physics: Application of Symbolic Regression with Alpha Zero to find the analytical methods in physics"></a>Alpha Zero for Physics: Application of Symbolic Regression with Alpha Zero to find the analytical methods in physics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12713">http://arxiv.org/abs/2311.12713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshihiro Michishita</li>
<li>for:  Physicists can use machine learning to find analytical methods.</li>
<li>methods:  The paper proposes using the Alpha Zero algorithm (AZfP) to develop analytical methods in physics.</li>
<li>results:  The paper demonstrates that AZfP can derive the high-frequency expansion in Floquet systems.Here’s the full text in Simplified Chinese:</li>
<li>for: Physics中使用机器学习找到分析方法。</li>
<li>methods: 本文提出使用Alpha Zero算法（AZfP）发展物理分析方法。</li>
<li>results: 本文示例展示AZfP可以 derivate Floquet系统的高频扩展。I hope that helps!<details>
<summary>Abstract</summary>
Machine learning with neural networks is now becoming a more and more powerful tool for various tasks, such as natural language processing, image recognition, winning the game, and even for the issues of physics. Although there are many studies on the application of machine learning to numerical calculation and the assistance of experimental detection, the methods of applying machine learning to find the analytical method are poorly studied. In this paper, we propose the frameworks of developing analytical methods in physics by using the symbolic regression with the Alpha Zero algorithm, that is Alpha Zero for physics (AZfP). As a demonstration, we show that AZfP can derive the high-frequency expansion in the Floquet systems. AZfP may have the possibility of developing a new theoretical framework in physics.
</details>
<details>
<summary>摘要</summary>
机器学习与神经网络现在成为了许多任务的更加有力的工具，如自然语言处理、图像识别、游戏赢得和物理问题。虽然有许多关于机器学习的应用于数值计算和实验探测的研究，但使用机器学习找到分析方法的方法尚未得到充分研究。在这篇论文中，我们提出了在物理中使用符号回归和Alpha Zero算法来开发分析方法的框架，即Alpha Zero for Physics（AZfP）。为了展示，我们显示了AZfP可以 derivate Floquet系统中的高频扩展。AZfP可能有可能开发出新的物理理论框架。Note: "Floquet系统" in the original text is translated as "Floquet系统" in Simplified Chinese, which is the pinyin Romanization of the Chinese term. The correct Chinese term for "Floquet system" is "托托列系统" (tàotào lèixìtsū) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Keeping-Users-Engaged-During-Repeated-Administration-of-the-Same-Questionnaire-Using-Large-Language-Models-to-Reliably-Diversify-Questions"><a href="#Keeping-Users-Engaged-During-Repeated-Administration-of-the-Same-Questionnaire-Using-Large-Language-Models-to-Reliably-Diversify-Questions" class="headerlink" title="Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions"></a>Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12707">http://arxiv.org/abs/2311.12707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hye Sun Yun, Mehdi Arjmand, Phillip Raymond Sherlock, Michael Paasche-Orlow, James W. Griffith, Timothy Bickmore</li>
<li>for: 这篇论文旨在提供一种使用大语言模型生成多种问卷版本，同时保持对 psychometric properties 的控制方法，以解决问卷疲劳问题。</li>
<li>methods: 本研究使用了大语言模型 (LLM) 生成问卷版本，并与专业评估问卷进行比较，以评估其灵活性和有效性。</li>
<li>results: 研究结果显示，LLM-生成问卷版本能够保持与外部标准相似的可靠性和有效性，而且 participants 评价这些问卷版本为比较有趣和有吸引力的。<details>
<summary>Abstract</summary>
Standardized, validated questionnaires are vital tools in HCI research and healthcare, offering dependable self-report data. However, their repeated use in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose utilizing large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants engaged with our agent system and responded daily for two weeks to either a standardized depression questionnaire or one of two LLM-generated questionnaire variants, alongside a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and the focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found the repeated administration of the standardized questionnaire significantly more repetitive compared to the variants. Our findings highlight the potential of LLM-generated variants to invigorate questionnaires, fostering engagement and interest without compromising validity.
</details>
<details>
<summary>摘要</summary>
标准化、验证的问卷是人机交互研究和医疗领域的重要工具，提供可靠的自报数据。然而，在长期或预后研究中，重复使用标准问卷可能会导致参与者疲劳，影响数据质量via 响应偏见和回答率下降。我们建议使用大语言模型（LLM）生成多个问卷版本，保持好的心理测量属性。在一个长期研究中，参与者与我们的代理系统交互，每天对于两周都回答了标准化的抑郁问卷或LLM生成的两个问卷变体，并且与验证的抑郁问卷一起进行心理测量。心理测量表明，在三个条件下，外部标准和关键测量之间存在一致的 covariation，证明LLM生成的变体具有可靠性和有效性。参与者认为，通过 repeatedly 使用标准问卷 significantly 更加厌恶，与变体相比。我们的发现指出，LLM生成的变体可以激励问卷，提高参与者的参与度和兴趣，不会COMPROMISE 有效性。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Understand-Content-and-Propagation-for-Misinformation-Detection-An-Empirical-Study"><a href="#Can-Large-Language-Models-Understand-Content-and-Propagation-for-Misinformation-Detection-An-Empirical-Study" class="headerlink" title="Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study"></a>Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12699">http://arxiv.org/abs/2311.12699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyang Chen, Lingwei Wei, Han Cao, Wei Zhou, Songlin Hu</li>
<li>for: 本研究探讨了大型自然语言模型（LLMs）在识别假信息任务中的表现。</li>
<li>methods: 本研究使用多种提示来评估多种LLMs的理解能力，并设计了四种 instrucion-tuned 策略来提高 LLMS 的识别性能。</li>
<li>results: 研究发现，使用多种提示的 LLMs 在文本基于的假信息检测任务中具有相似的表现，但在媒体传播结构基于的假信息检测任务中表现较差。同时，提出的四种 instrucion-tuned 策略可以提高 LLMS 的识别性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have garnered significant attention for their powerful ability in natural language understanding and reasoning. In this paper, we present a comprehensive empirical study to explore the performance of LLMs on misinformation detection tasks. This study stands as the pioneering investigation into the understanding capabilities of multiple LLMs regarding both content and propagation across social media platforms. Our empirical studies on five misinformation detection datasets show that LLMs with diverse prompts achieve comparable performance in text-based misinformation detection but exhibit notably constrained capabilities in comprehending propagation structure compared to existing models in propagation-based misinformation detection. Besides, we further design four instruction-tuned strategies to enhance LLMs for both content and propagation-based misinformation detection. These strategies boost LLMs to actively learn effective features from multiple instances or hard instances, and eliminate irrelevant propagation structures, thereby achieving better detection performance. Extensive experiments further demonstrate LLMs would play a better capacity in content and propagation structure under these proposed strategies and achieve promising detection performance. These findings highlight the potential ability of LLMs to detect misinformation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="From-Concept-to-Manufacturing-Evaluating-Vision-Language-Models-for-Engineering-Design"><a href="#From-Concept-to-Manufacturing-Evaluating-Vision-Language-Models-for-Engineering-Design" class="headerlink" title="From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design"></a>From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12668">http://arxiv.org/abs/2311.12668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cyril Picard, Kristen M. Edwards, Anna C. Doris, Brandon Man, Giorgio Giannone, Md Ferdous Alam, Faez Ahmed<br>for:This paper aims to evaluate the capabilities of a multimodal vision language model, GPT-4V, in a wide range of engineering design tasks, including conceptual design, system-level and detailed design, manufacturing and inspection, and engineering education tasks.methods:The paper uses a comprehensive evaluation of GPT-4V across a variety of engineering design tasks, including sketch similarity analysis, concept selection using Pugh Charts, material selection, engineering drawing analysis, CAD generation, topology optimization, design for additive and subtractive manufacturing, spatial reasoning challenges, and textbook problems.results:The study shows that GPT-4V demonstrates impressive capabilities in handling complex design and manufacturing challenges, but also identifies its limitations in complex engineering design applications. The paper provides a foundation for future assessments of vision language models and contributes a set of benchmark testing datasets for ongoing advancements in this field.<details>
<summary>Abstract</summary>
Engineering Design is undergoing a transformative shift with the advent of AI, marking a new era in how we approach product, system, and service planning. Large language models have demonstrated impressive capabilities in enabling this shift. Yet, with text as their only input modality, they cannot leverage the large body of visual artifacts that engineers have used for centuries and are accustomed to. This gap is addressed with the release of multimodal vision language models, such as GPT-4V, enabling AI to impact many more types of tasks. In light of these advancements, this paper presents a comprehensive evaluation of GPT-4V, a vision language model, across a wide spectrum of engineering design tasks, categorized into four main areas: Conceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, and Engineering Education Tasks. Our study assesses GPT-4V's capabilities in design tasks such as sketch similarity analysis, concept selection using Pugh Charts, material selection, engineering drawing analysis, CAD generation, topology optimization, design for additive and subtractive manufacturing, spatial reasoning challenges, and textbook problems. Through this structured evaluation, we not only explore GPT-4V's proficiency in handling complex design and manufacturing challenges but also identify its limitations in complex engineering design applications. Our research establishes a foundation for future assessments of vision language models, emphasizing their immense potential for innovating and enhancing the engineering design and manufacturing landscape. It also contributes a set of benchmark testing datasets, with more than 1000 queries, for ongoing advancements and applications in this field.
</details>
<details>
<summary>摘要</summary>
工程设计正在通过人工智能的普及，开启一个新的时代，推动产品、系统和服务规划的方式。大型自然语言模型已经表现出了很好的能力，但它们只能通过文本作为输入模式，无法利用工程师 centuries 使用的大量视觉文件。这个问题被解决通过多Modal 视觉语言模型的推出，如 GPT-4V，使得人工智能能够影响更多类型的任务。鉴于这些进步，本文对 GPT-4V 进行了全面的评估，在工程设计任务中分为四个主要领域：概念设计、系统级别和细节设计、制造和检查、工程教育任务。我们的研究评估 GPT-4V 在设计任务中的能力，包括绘制相似性分析、选择 Pugh 图表、材料选择、工程图分析、 CAD 生成、顺序优化、设计增加和减少制造、空间逻辑挑战和文本教材问题。通过这种结构化的评估，我们不仅探索 GPT-4V 在复杂设计和制造挑战中的能力，还提出了它在复杂工程设计应用中的局限性。我们的研究建立了未来评估视觉语言模型的基础，强调它们在工程设计和制造领域的潜在潜力，并提供了一个包含超过 1000 个查询的测试数据集，为未来的进步和应用提供了基础。
</details></li>
</ul>
<hr>
<h2 id="The-DURel-Annotation-Tool-Human-and-Computational-Measurement-of-Semantic-Proximity-Sense-Clusters-and-Semantic-Change"><a href="#The-DURel-Annotation-Tool-Human-and-Computational-Measurement-of-Semantic-Proximity-Sense-Clusters-and-Semantic-Change" class="headerlink" title="The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change"></a>The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12664">http://arxiv.org/abs/2311.12664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Schlechtweg, Shafqat Mumtaz Virk, Pauline Sander, Emma Sköldberg, Lukas Theuer Linke, Tuo Zhang, Nina Tahmasebi, Jonas Kuhn, Sabine Schulte im Walde</li>
<li>for: 这篇论文是为了介绍一种新的word-in-context模型，用于实现语义距离的注释。</li>
<li>methods: 这篇论文使用了标准化的人工注释和计算注释，并使用Word-in-Context模型来建立语义距离的注释。</li>
<li>results: 该工具可以快速和简单地实现语义距离的注释，并提供了对注释审核者之间的一致性和时间变化的整体分析。<details>
<summary>Abstract</summary>
We present the DURel tool that implements the annotation of semantic proximity between uses of words into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics giving insights into sense frequency distributions, semantic variation or changes of senses over time.
</details>
<details>
<summary>摘要</summary>
我团队现在发布了一个名为DURel的工具，它实现了将单词之间语义靠近性的注释转化为在线、开源界面上的工具。该工具支持标准化的人类注释以及计算机注释，基于最近的Word-in-Context模型。注释审核结果使用图像聚合技术集成并可视化分析，以测量单词的感知词语分布。这使得可以通过简单和直观的微任务判断对用语对之间的语义关系，需要最小的准备努力。工具还提供了比较注释者之间的一致性，以及计算概念频率分布、语义变化或时间变化的摘要统计信息。
</details></li>
</ul>
<hr>
<h2 id="PARK-Parkinson’s-Analysis-with-Remote-Kinetic-tasks"><a href="#PARK-Parkinson’s-Analysis-with-Remote-Kinetic-tasks" class="headerlink" title="PARK: Parkinson’s Analysis with Remote Kinetic-tasks"></a>PARK: Parkinson’s Analysis with Remote Kinetic-tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12654">http://arxiv.org/abs/2311.12654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Saiful Islam, Sangwu Lee, Abdelrahman Abdelkader, Sooyong Park, Ehsan Hoque</li>
<li>for: 这个研究是为了开发一个基于网络的诊断渠道，帮助患有parkinson病的人进行家庭中的诊断。</li>
<li>methods: 这个研究使用了三种任务，包括语音、表情和手部运动，并通过分析这些任务的视频来诊断患有parkinson病的人。</li>
<li>results: 这个研究得到了一种简单易于理解的结果，同时还提供了个性化的资源，以便患有parkinson病的人可以更好地访问治疗和护理。<details>
<summary>Abstract</summary>
We present a web-based framework to screen for Parkinson's disease (PD) by allowing users to perform neurological tests in their homes. Our web framework guides the users to complete three tasks involving speech, facial expression, and finger movements. The task videos are analyzed to classify whether the users show signs of PD. We present the results in an easy-to-understand manner, along with personalized resources to further access to treatment and care. Our framework is accessible by any major web browser, improving global access to neurological care.
</details>
<details>
<summary>摘要</summary>
我们提供一套基于网络的框架，以帮助用户在家中进行parkinson病（PD）的检测。我们的网络框架会引导用户完成三项任务，包括语音、面部表达和手指运动。这些任务的视频会被分析，以确定用户是否显示出PD的 symptoms。我们将结果显示在易于理解的方式下，并提供个性化的资源，以便更好地访问治疗和护理。我们的框架可以通过任何主要浏览器访问，从而提高全球神经科学的访问度。
</details></li>
</ul>
<hr>
<h2 id="Mobile-Seed-Joint-Semantic-Segmentation-and-Boundary-Detection-for-Mobile-Robots"><a href="#Mobile-Seed-Joint-Semantic-Segmentation-and-Boundary-Detection-for-Mobile-Robots" class="headerlink" title="Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots"></a>Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12651">http://arxiv.org/abs/2311.12651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WHU-USI3DV/Mobile-Seed">https://github.com/WHU-USI3DV/Mobile-Seed</a></li>
<li>paper_authors: Youqi Liao, Shuhao Kang, Jianping Li, Yang Liu, Yun Liu, Zhen Dong, Bisheng Yang, Xieyuanli Chen</li>
<li>for: 本研究旨在提供一个轻量级的双任务框架，以同时提高 semantic segmentation 和 bounding detection 的性能。</li>
<li>methods: 该框架包括两条流量对应的 encoder，以及一个 active fusion decoder (AFD) 和双任务调整方法。encoder 分为两条路径：一条捕捉类别意识的 semantic information，另一条从多个比例特征中提取 bounding information。AFD 模组在内部学习通道之间的关系，以允许精确地将每个通道的重要性分配。</li>
<li>results: 比较 existing methods，提案的 Mobile-Seed 提供了轻量级的框架，以提高 semantic segmentation 性能和精确地定义物体 bounding。Cityscapes 数据集的实验结果显示，Mobile-Seed 与 SOTA 基eline 相比，提高了 2.2 分点 (pp) 的 mIoU 和 4.2 pp 的 mF-score，并维持在线执行速度为 23.9 帧&#x2F;秒 (FPS) 的 1024x2048 分辨率输入上的 RTX 2080 Ti GPU 上。<details>
<summary>Abstract</summary>
Precise and rapid delineation of sharp boundaries and robust semantics is essential for numerous downstream robotic tasks, such as robot grasping and manipulation, real-time semantic mapping, and online sensor calibration performed on edge computing units. Although boundary detection and semantic segmentation are complementary tasks, most studies focus on lightweight models for semantic segmentation but overlook the critical role of boundary detection. In this work, we introduce Mobile-Seed, a lightweight, dual-task framework tailored for simultaneous semantic segmentation and boundary detection. Our framework features a two-stream encoder, an active fusion decoder (AFD) and a dual-task regularization approach. The encoder is divided into two pathways: one captures category-aware semantic information, while the other discerns boundaries from multi-scale features. The AFD module dynamically adapts the fusion of semantic and boundary information by learning channel-wise relationships, allowing for precise weight assignment of each channel. Furthermore, we introduce a regularization loss to mitigate the conflicts in dual-task learning and deep diversity supervision. Compared to existing methods, the proposed Mobile-Seed offers a lightweight framework to simultaneously improve semantic segmentation performance and accurately locate object boundaries. Experiments on the Cityscapes dataset have shown that Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA) baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while maintaining an online inference speed of 23.9 frames-per-second (FPS) with 1024x2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on CamVid and PASCAL Context datasets confirm our method's generalizability. Code and additional results are publicly available at \url{https://martin-liao.github.io/Mobile-Seed/}.
</details>
<details>
<summary>摘要</summary>
必须准确快速地定义锐利边界和 Robust  semantics，以便 robotic 任务中的许多任务，如机器人抓取和处理、实时 semantic mapping 和在 Edge computing 单元上进行的 online sensor calibration。虽然边界探测和semantic segmentation 是 complementary 任务，但大多数研究强调轻量级模型 для semantic segmentation，忽略了边界探测的重要性。在这种情况下，我们引入 Mobile-Seed，一个轻量级、双任务框架，适用于同时进行 semantic segmentation 和边界探测。我们的框架包括两条通道：一个捕捉类别意识的信息，另一个是从多尺度特征中提取边界信息。我们的 AFD 模块可以动态地适应 semantic 和边界信息的融合，通过学习通道之间的关系，以便精确地分配每个通道的Weight。此外，我们还引入了一种调节 dual-task 学习和深度多样性supervision的损失函数。相比现有方法，我们的 Mobile-Seed 提供了一个轻量级的框架，可以同时提高 semantic segmentation 性能和精确地定义 объек 的边界。我们在 Cityscapes 数据集上进行了实验，并证明了我们的方法可以与状态理论（SOTA）基eline 相比，在 mIoU 和 mF-score 中提高了2.2个百分点和4.2个百分点，同时保持在线推理速度为23.9帧/秒，分辨率为1024x2048，使用 RTX 2080 Ti GPU。其他 CamVid 和 PASCAL Context 数据集的实验证明了我们的方法的普适性。代码和更多的结果可以在 \url{https://martin-liao.github.io/Mobile-Seed/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="KNVQA-A-Benchmark-for-evaluation-knowledge-based-VQA"><a href="#KNVQA-A-Benchmark-for-evaluation-knowledge-based-VQA" class="headerlink" title="KNVQA: A Benchmark for evaluation knowledge-based VQA"></a>KNVQA: A Benchmark for evaluation knowledge-based VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12639">http://arxiv.org/abs/2311.12639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sirui Cheng, Siyu Zhang, Jiayi Wu, Muchen Lan</li>
<li>for: 这项研究的目的是提出一种新的知识基于VQA任务评估方法，以评估multimodal LVLMs的实际性。</li>
<li>methods: 该研究使用了一种新的KNVQA数据集，通过综合评估LVLMs的语言和视觉系统的感知和理解能力，以及相关的知识信息。</li>
<li>results: 研究发现，现有的LVLMs在知识基于VQA任务中存在诸如 объек hallucination 和 factual accuracy 等问题，而且先前的评估方法更关注语言内容的理解和推理能力，缺乏对多modal交互的全面评估。<details>
<summary>Abstract</summary>
Within the multimodal field, large vision-language models (LVLMs) have made significant progress due to their strong perception and reasoning capabilities in the visual and language systems. However, LVLMs are still plagued by the two critical issues of object hallucination and factual accuracy, which limit the practicality of LVLMs in different scenarios. Furthermore, previous evaluation methods focus more on the comprehension and reasoning of language content but lack a comprehensive evaluation of multimodal interactions, thereby resulting in potential limitations. To this end, we propose a novel KNVQA-Eval, which is devoted to knowledge-based VQA task evaluation to reflect the factuality of multimodal LVLMs. To ensure the robustness and scalability of the evaluation, we develop a new KNVQA dataset by incorporating human judgment and perception, aiming to evaluate the accuracy of standard answers relative to AI-generated answers in knowledge-based VQA. This work not only comprehensively evaluates the contextual information of LVLMs using reliable human annotations, but also further analyzes the fine-grained capabilities of current methods to reveal potential avenues for subsequent optimization of LVLMs-based estimators. Our proposed VQA-Eval and corresponding dataset KNVQA will facilitate the development of automatic evaluation tools with the advantages of low cost, privacy protection, and reproducibility. Our code will be released upon publication.
</details>
<details>
<summary>摘要</summary>
在多模态领域，大视语模型（LVLM）已经做出了重要的进步，因为它们在视觉和语言系统中具有强大的感知和理解能力。然而，LVLM仍然面临着两个重要的问题：物体推断和事实准确性，这些问题限制了LVLM在不同场景中的实际应用。此外，前一代评估方法更关注语言内容的理解和推理，而忽略了多模态互动的全面评估，从而可能导致了限制。为此，我们提出了一种新的KNVQA评估方法，专门用于知识基础VQA任务的评估，以反映LVLM的事实准确性。为确保评估的稳定性和可扩展性，我们开发了一个新的KNVQA数据集，通过人类判断和感知来补充标准答案和AI生成的答案之间的差异。这项工作不仅全面评估了LVLM在多模态互动中的上下文信息，还进一步分析了当前方法的细化能力，以揭示可能的优化方向。我们的提出的VQA评估和相应的KNVQA数据集将促进基于LVLM的自动评估工具的开发，具有低成本、隐私保护和可重现性的优点。我们将在发表时释出代码。
</details></li>
</ul>
<hr>
<h2 id="ChessVision-–-A-Dataset-for-Logically-Coherent-Multi-label-Classification"><a href="#ChessVision-–-A-Dataset-for-Logically-Coherent-Multi-label-Classification" class="headerlink" title="ChessVision – A Dataset for Logically Coherent Multi-label Classification"></a>ChessVision – A Dataset for Logically Coherent Multi-label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12610">http://arxiv.org/abs/2311.12610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/espressovi/chessvisionchallenge">https://github.com/espressovi/chessvisionchallenge</a></li>
<li>paper_authors: Soumadeep Saha, Utpal Garain</li>
<li>for: 这种研究旨在检验深度学习技术是否能够捕捉围棋游戏中的语义上下文和逻辑约束。</li>
<li>methods: 这个研究使用了一个新的围棋游戏图像集（ChessVision Dataset），并设计了一组约束来限制预测的游戏状态。</li>
<li>results: 研究发现，虽然现有的视觉模型在标准指标上表现出色，但它们在面对这个 dataset 时却产生了大量的不一致结果，表明这个 dataset 对未来研究呈现了 significiant 挑战。<details>
<summary>Abstract</summary>
Starting with early successes in computer vision tasks, deep learning based techniques have since overtaken state of the art approaches in a multitude of domains. However, it has been demonstrated time and again that these techniques fail to capture semantic context and logical constraints, instead often relying on spurious correlations to arrive at the answer. Since application of deep learning techniques to critical scenarios are dependent on adherence to domain specific constraints, several attempts have been made to address this issue. One limitation holding back a thorough exploration of this area, is a lack of suitable datasets which feature a rich set of rules. In order to address this, we present the ChessVision Dataset, consisting of 200,000+ images of annotated chess games in progress, requiring recreation of the game state from its corresponding image. This is accompanied by a curated set of rules which constrains the set of predictions to "reasonable" game states, and are designed to probe key semantic abilities like localization and enumeration. Alongside standard metrics, additional metrics to measure performance with regards to logical consistency is presented. We analyze several popular and state of the art vision models on this task, and show that, although their performance on standard metrics are laudable, they produce a plethora of incoherent results, indicating that this dataset presents a significant challenge for future works.
</details>
<details>
<summary>摘要</summary>
开始于计算机视觉任务的早期成功，深度学习基于技术已经在多个领域超越了国际一级的方法。然而，它们时常不能捕捉 semantic context 和逻辑约束，而是通过偶合关系来得到答案。由于应用深度学习技术到重要的应用场景需要遵循域pecific的约束，因此有几个尝试来解决这个问题。一个 limiting factor 是缺乏适用的数据集，该集feature了丰富的规则。为了解决这个问题，我们提出了ChessVision Dataset，包含200,000+个 annotated chess game的图像，需要从图像中重建棋盘状态。此外，我们还提供了一组约束，以限制预测的set of game states，并且是为了考验关键的 semantic ability，如localization和枚举。在此任务上，我们分析了一些流行的视觉模型，并发现它们的性能在标准 metric 上是卓越的，但它们在这个 dataset 上产生了大量的不一致结果，这表明这个 dataset 对 future works 呈现了一个 significiant challenge。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-AI-Deciding-What-to-Decide"><a href="#Trustworthy-AI-Deciding-What-to-Decide" class="headerlink" title="Trustworthy AI: Deciding What to Decide"></a>Trustworthy AI: Deciding What to Decide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12604">http://arxiv.org/abs/2311.12604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caesar Wu, Yuan-Fang Li, Jian Li, Jingjing Xu, Bouvry Pascal</li>
<li>for: 本研究旨在 Addressing the challenge of determining which information can be trusted when using Artificial Intelligence (AI) systems for decision-making, known as Trustworthy AI (TAI).</li>
<li>methods: 本研究提出了一种新的 TAI 框架，包括 representation space、loss function 和 optimizer 三个关键组件，每个组件具有四个 TAI 属性。</li>
<li>results: 通过 quantitive 和 qualitative 研究方法，本研究实现了 twelve TAI 属性，并提出了一个可以用于应用策略投资决策的优化预测模型。<details>
<summary>Abstract</summary>
When engaging in strategic decision-making, we are frequently confronted with overwhelming information and data. The situation can be further complicated when certain pieces of evidence contradict each other or become paradoxical. The primary challenge is how to determine which information can be trusted when we adopt Artificial Intelligence (AI) systems for decision-making. This issue is known as deciding what to decide or Trustworthy AI. However, the AI system itself is often considered an opaque black box. We propose a new approach to address this issue by introducing a novel framework of Trustworthy AI (TAI) encompassing three crucial components of AI: representation space, loss function, and optimizer. Each component is loosely coupled with four TAI properties. Altogether, the framework consists of twelve TAI properties. We aim to use this framework to conduct the TAI experiments by quantitive and qualitative research methods to satisfy TAI properties for the decision-making context. The framework allows us to formulate an optimal prediction model trained by the given dataset for applying the strategic investment decision of credit default swaps (CDS) in the technology sector. Finally, we provide our view of the future direction of TAI research
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a new approach that includes a novel framework for Trustworthy AI (TAI) that encompasses three crucial components of AI: the representation space, the loss function, and the optimizer. Each component is loosely coupled with four TAI properties. In total, the framework consists of twelve TAI properties. We plan to use this framework to conduct experiments using quantitative and qualitative research methods to satisfy TAI properties for the decision-making context.Using this framework, we can formulate an optimal prediction model trained by the given dataset for applying strategic investment decisions in the technology sector. Finally, we provide our view of the future direction of TAI research.
</details></li>
</ul>
<hr>
<h2 id="Visual-tracking-brain-computer-interface"><a href="#Visual-tracking-brain-computer-interface" class="headerlink" title="Visual tracking brain computer interface"></a>Visual tracking brain computer interface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12592">http://arxiv.org/abs/2311.12592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changxing Huang, Nanlin Shi, Yining Miao, Xiaogang Chen, Yijun Wang, Xiaorong Gao<br>for: 这项研究的目的是提出一种基于视觉神经 Interface (BCI) 的自然 kontinuierliche 控制方法，以超越传统的精确点击响应。methods: 该研究使用了非侵入性的 electroencephalography (EEG) 技术，并实现了一种新的空间编码刺激方法，以及相应的投影方法，以实现连续控制。results: 实验结果显示，该 BCIs 的 Fitt’s ITR 为 0.55 bps 和 0.37 bps，表明该方法可以实现高效的连续控制。此外，该 BCIs 还被应用于绘画和游戏等两个应用程序中。<details>
<summary>Abstract</summary>
Brain-computer interfaces (BCIs) offer a way to interact with computers without relying on physical movements. Non-invasive electroencephalography (EEG)-based visual BCIs, known for efficient speed and calibration ease, face limitations in continuous tasks due to discrete stimulus design and decoding methods. To achieve continuous control, we implemented a novel spatial encoding stimulus paradigm and devised a corresponding projection method to enable continuous modulation of decoded velocity. Subsequently, we conducted experiments involving 17 participants and achieved Fitt's ITR of 0.55 bps for the fixed tracking task and 0.37 bps for the random tracking task. The proposed BCI with a high Fitt's ITR was then integrated into two applications, including painting and gaming. In conclusion, this study proposed a visual BCI-based control method to go beyond discrete commands, allowing natural continuous control based on neural activity.
</details>
<details>
<summary>摘要</summary>
��ubble-computer interfaces (BCIs) offer a way to interact with computers without relying on physical movements. Non-invasive electroencephalography (EEG)-based visual BCIs, known for efficient speed and calibration ease, face limitations in continuous tasks due to discrete stimulus design and decoding methods. To achieve continuous control, we implemented a novel spatial encoding stimulus paradigm and devised a corresponding projection method to enable continuous modulation of decoded velocity. Subsequently, we conducted experiments involving 17 participants and achieved Fitt's ITR of 0.55 bps for the fixed tracking task and 0.37 bps for the random tracking task. The proposed BCI with a high Fitt's ITR was then integrated into two applications, including painting and gaming. In conclusion, this study proposed a visual BCI-based control method to go beyond discrete commands, allowing natural continuous control based on neural activity.Here's the translation in Traditional Chinese as well:��ubble-computer interfaces (BCIs) offer a way to interact with computers without relying on physical movements. Non-invasive electroencephalography (EEG)-based visual BCIs, known for efficient speed and calibration ease, face limitations in continuous tasks due to discrete stimulus design and decoding methods. To achieve continuous control, we implemented a novel spatial encoding stimulus paradigm and devised a corresponding projection method to enable continuous modulation of decoded velocity. Subsequently, we conducted experiments involving 17 participants and achieved Fitt's ITR of 0.55 bps for the fixed tracking task and 0.37 bps for the random tracking task. The proposed BCI with a high Fitt's ITR was then integrated into two applications, including painting and gaming. In conclusion, this study proposed a visual BCI-based control method to go beyond discrete commands, allowing natural continuous control based on neural activity.
</details></li>
</ul>
<hr>
<h2 id="Improving-Source-Free-Target-Adaptation-with-Vision-Transformers-Leveraging-Domain-Representation-Images"><a href="#Improving-Source-Free-Target-Adaptation-with-Vision-Transformers-Leveraging-Domain-Representation-Images" class="headerlink" title="Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images"></a>Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12589">http://arxiv.org/abs/2311.12589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gauransh Sawhney, Daksh Dave, Adeel Ahmed, Jiechao Gao, Khalid Saleem</li>
<li>for: 提高基于ViT的频率预测性能在无源频率预测任务中</li>
<li>methods: 使用DRIs作为域特异标记，并将域特异标记与键元素结合，以提高ViT在源自由目标适应任务中的性能</li>
<li>results: 对于Cross Instance DRI SO控制任务，含DRIs的方法可以提高平均准确率，表明DRIs在UDA任务中对ViT的性能有较大提高作用<details>
<summary>Abstract</summary>
Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer from a labeled source domain to an unlabeled target domain, navigating the obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for domain generalization. This paper presents an innovative method to bolster ViT performance in source-free target adaptation, beginning with an evaluation of how key, query, and value elements affect ViT outcomes. Experiments indicate that altering the key component has negligible effects on Transformer performance. Leveraging this discovery, we introduce Domain Representation Images (DRIs), feeding embeddings through the key element. DRIs act as domain-specific markers, effortlessly merging with the training regimen. To assess our method, we perform target adaptation tests on the Cross Instance DRI source-only (SO) control. We measure the efficacy of target adaptation with and without DRIs, against existing benchmarks like SHOT-B* and adaptations via CDTrans. Findings demonstrate that excluding DRIs offers limited gains over SHOT-B*, while their inclusion in the key segment boosts average precision promoting superior domain generalization. This research underscores the vital role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent for further domain adaptation explorations.
</details>
<details>
<summary>摘要</summary>
Unsupervised领域适应（UDA）技术可以将来自已知源频谱的知识传递到无标签目标频谱，缓解频谱差异的障碍。而卷积神经网络（CNNs）是UDA的核心，但是受欢迎的视图转换器（ViTs）也为频谱泛化提供了新的可能性。这篇论文提出了一种创新的方法，用于提高ViT在源频谱自由目标适应中的性能，开始于对键、查询和值元素对ViT的影响进行评估。实验表明，对键元素进行修改对转换器性能的影响是微不足道。基于这一发现，我们引入频谱表示图像（DRIs），将表示图像通过键元素进行 feeding。DRIs acts as domain-specific markers， effortlessly merging with the training regimen。为评估我们的方法，我们在 Cross Instance DRI source-only（SO）控制下进行目标适应测试。我们测量了不包括DRIs的target适应和包括DRIs的target适应，与现有的标准如SHOT-B*和CDTrans的适应进行比较。发现，不包括DRIs的target适应具有有限的提升，而包括DRIs的target适应则提高了平均精度，从而提高了频谱泛化性能。这些研究证明了DRIs在UDA场景中对ViT效率的重要作用，设立了频谱泛化探索的先例。
</details></li>
</ul>
<hr>
<h2 id="Echocardiogram-Foundation-Model-–-Application-1-Estimating-Ejection-Fraction"><a href="#Echocardiogram-Foundation-Model-–-Application-1-Estimating-Ejection-Fraction" class="headerlink" title="Echocardiogram Foundation Model – Application 1: Estimating Ejection Fraction"></a>Echocardiogram Foundation Model – Application 1: Estimating Ejection Fraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12582">http://arxiv.org/abs/2311.12582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adil Dahlan, Cyril Zakka, Abhinav Kumar, Laura Tang, Rohan Shad, Robyn Fong, William Hiesinger</li>
<li>for: 这个论文的目的是提出一种基于自动学习的echocardiogram基础模型，以提高cardiac function的评估精度和效率。</li>
<li>methods: 该模型使用了自动学习（SSL）技术，在150万个echocardiogram中进行自动训练，以提高cardiac function的评估精度和效率。</li>
<li>results: 通过对echocardiogram进行 fine-tuning，模型可以达到专家医生的准确率（9.40%），表明模型的性能与专家医生的评估相当。<details>
<summary>Abstract</summary>
Cardiovascular diseases stand as the primary global cause of mortality. Among the various imaging techniques available for visualising the heart and evaluating its function, echocardiograms emerge as the preferred choice due to their safety and low cost. Quantifying cardiac function based on echocardiograms is very laborious, time-consuming and subject to high interoperator variability. In this work, we introduce EchoAI, an echocardiogram foundation model, that is trained using self-supervised learning (SSL) on 1.5 million echocardiograms. We evaluate our approach by fine-tuning EchoAI to estimate the ejection fraction achieving a mean absolute percentage error of 9.40%. This level of accuracy aligns with the performance of expert sonographers.
</details>
<details>
<summary>摘要</summary>
心血管疾病是全球最主要的死亡原因。 Among various imaging技术用于心脏视图和评估其功能，电子心征成为首选，因为它的安全和成本低。 量化心脏功能基于电子心征是很努力，时间consuming和operator变化很大。 In this work, we introduce EchoAI, an echocardiogram foundation model, which is trained using self-supervised learning (SSL) on 1.5 million echocardiograms. We evaluate our approach by fine-tuning EchoAI to estimate the ejection fraction, achieving a mean absolute percentage error of 9.40%. This level of accuracy aligns with the performance of expert sonographers.Note: "心血管疾病" is a combination of "心血管" (cardiovascular) and "疾病" (disease), and "sonographers" is translated as "专业对心脏的评估专家" (expert sonographers) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="IMGTB-A-Framework-for-Machine-Generated-Text-Detection-Benchmarking"><a href="#IMGTB-A-Framework-for-Machine-Generated-Text-Detection-Benchmarking" class="headerlink" title="IMGTB: A Framework for Machine-Generated Text Detection Benchmarking"></a>IMGTB: A Framework for Machine-Generated Text Detection Benchmarking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12574">http://arxiv.org/abs/2311.12574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Spiegel, Dominik Macko<br>for: 本研究旨在提供一个简单易用的测试框架，以便实现机器生成文本检测方法的测试和比较。methods: 本研究使用的方法包括IMGTB框架，可以让研究人员轻松地整合自己的新方法和评估数据，并且提供了一些默认的分析、 метри和可视化工具，以便与现有的国际标准进行比较。results: 本研究的结果显示，IMGTB框架可以轻松地整合新的检测方法和评估数据，并且可以实现简单易用的测试和比较。default set of analyses、metrics和可视化工具，以便与现有的国际标准进行比较。<details>
<summary>Abstract</summary>
In the era of large language models generating high quality texts, it is a necessity to develop methods for detection of machine-generated text to avoid harmful use or simply due to annotation purposes. It is, however, also important to properly evaluate and compare such developed methods. Recently, a few benchmarks have been proposed for this purpose; however, integration of newest detection methods is rather challenging, since new methods appear each month and provide slightly different evaluation pipelines. In this paper, we present the IMGTB framework, which simplifies the benchmarking of machine-generated text detection methods by easy integration of custom (new) methods and evaluation datasets. Its configurability and flexibility makes research and development of new detection methods easier, especially their comparison to the existing state-of-the-art detectors. The default set of analyses, metrics and visualizations offered by the tool follows the established practices of machine-generated text detection benchmarking found in state-of-the-art literature.
</details>
<details>
<summary>摘要</summary>
在大型语言模型生成高质量文本的时代，检测机器生成文本的方法的开发已成为一项必要的任务，以避免恶势力使用或只是因为标注目的。然而，也重要地是正确评估和比较已经开发出来的方法。最近，一些比较标准的测试准则已经被提出来用于这个目的；然而，新方法每月出现一次，它们的评估管道略有不同。本文介绍了IMGTB框架，它使得机器生成文本检测方法的benchmarking更加简单，可以轻松地 интегра部署自定义（新）方法和评估数据集。它的可 configurability和灵活性使得研发新检测方法的研究和开发变得更加容易，特别是与现有的状态对照检测器进行比较。默认的分析、指标和视觉化工具由工具提供，与现有的机器生成文本检测 benchmarking Literature 中的已Established practices相符。
</details></li>
</ul>
<hr>
<h2 id="Moderating-Model-Marketplaces-Platform-Governance-Puzzles-for-AI-Intermediaries"><a href="#Moderating-Model-Marketplaces-Platform-Governance-Puzzles-for-AI-Intermediaries" class="headerlink" title="Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries"></a>Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12573">http://arxiv.org/abs/2311.12573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Gorwa, Michael Veale</li>
<li>for: 本文探讨了AI系统在多个平台上的Platform Governance问题，即如何管理用户上传的模型和数据。</li>
<li>methods: 本文使用了三个例子来分析平台如何对模型进行模eration：Hugging Face、GitHub和Civitai。</li>
<li>results: 本文结论了现有的industry实践，包括licensing、访问和使用限制、自动内容审核和开放政策开发，以应对模eration问题。<details>
<summary>Abstract</summary>
The AI development community is increasingly making use of hosting intermediaries such as Hugging Face provide easy access to user-uploaded models and training data. These model marketplaces lower technical deployment barriers for hundreds of thousands of users, yet can be used in numerous potentially harmful and illegal ways. In this article, we explain ways in which AI systems, which can both `contain' content and be open-ended tools, present one of the trickiest platform governance challenges seen to date. We provide case studies of several incidents across three illustrative platforms -- Hugging Face, GitHub and Civitai -- to examine how model marketplaces moderate models. Building on this analysis, we outline important (and yet nevertheless limited) practices that industry has been developing to respond to moderation demands: licensing, access and use restrictions, automated content moderation, and open policy development. While the policy challenge at hand is a considerable one, we conclude with some ideas as to how platforms could better mobilize resources to act as a careful, fair, and proportionate regulatory access point.
</details>
<details>
<summary>摘要</summary>
美元发展社区正在越来越多地利用托管中间人如Hugging Face提供用户上传模型和训练数据的容易访问。这些模型市场降低了数量达到百万的用户的技术部署障碍，但可以用于多种可能有害和不法的方式。在这篇文章中，我们介绍了AI系统如何成为Platform Governance挑战之一，该系统可以包含内容并且是开放的工具。我们通过对Hugging Face、GitHub和Civitai等三个示例平台的情况进行分析，探讨了模型市场如何管理模型。以此为基础，我们介绍了行业在应对审核需求方面的重要（却又有限的）实践：许可证、访问和使用限制、自动内容审核、开放政策开发。虽然政策挑战很大，但我们在结尾透过一些想法，以便平台可以更好地借鉴资源，成为一个仔细、公平、评估的法规访问点。
</details></li>
</ul>
<hr>
<h2 id="Scheduling-Distributed-Flexible-Assembly-Lines-using-Safe-Reinforcement-Learning-with-Soft-Shielding"><a href="#Scheduling-Distributed-Flexible-Assembly-Lines-using-Safe-Reinforcement-Learning-with-Soft-Shielding" class="headerlink" title="Scheduling Distributed Flexible Assembly Lines using Safe Reinforcement Learning with Soft Shielding"></a>Scheduling Distributed Flexible Assembly Lines using Safe Reinforcement Learning with Soft Shielding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12572">http://arxiv.org/abs/2311.12572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lele Li, Liyong Lin</li>
<li>for: 这篇论文是为了解决分布式灵活生产线上的作业调度问题，以提高生产效率、减少延迟、提高安全性和可靠性。</li>
<li>methods: 该论文提出了一种基于优先级执行器和批处理学习算法的协调调度方法，以实现分布式灵活生产线上的实时调度。此外，它还提出了一种缩写环境表示方法，以优化行为空间，并使用Monte-Carlo搜索算法来帮助解决长序靠前不安全行为和监测延迟调度的风险。</li>
<li>results: 该论文的实验和评估结果表明，提出的调度方法和软防护Component可以有效地解决分布式灵活生产线上的作业调度问题，提高生产效率和安全性。<details>
<summary>Abstract</summary>
Highly automated assembly lines enable significant productivity gains in the manufacturing industry, particularly in mass production condition. Nonetheless, challenges persist in job scheduling for make-to-job and mass customization, necessitating further investigation to improve efficiency, reduce tardiness, promote safety and reliability. In this contribution, an advantage actor-critic based reinforcement learning method is proposed to address scheduling problems of distributed flexible assembly lines in a real-time manner. To enhance the performance, a more condensed environment representation approach is proposed, which is designed to work with the masks made by priority dispatching rules to generate fixed and advantageous action space. Moreover, a Monte-Carlo tree search based soft shielding component is developed to help address long-sequence dependent unsafe behaviors and monitor the risk of overdue scheduling. Finally, the proposed algorithm and its soft shielding component are validated in performance evaluation.
</details>
<details>
<summary>摘要</summary>
高度自动化的生产线可以实现大量生产的产量增加，但是在make-to-job和个性化生产中，仍然存在困难。为了解决这些问题，本贡献提出了基于优点批评学习的推荐方法，可以在实时下解决分布式 flexible assembly line 的调度问题。为了提高性能，我们还提出了一种更加压缩的环境表示方法，可以通过优先调度规则生成固定和有利的动作空间。此外，我们还开发了基于 Monte-Carlo 搜索的软遮盾组件，可以帮助解决长序 dependent 不安全行为和监测晚点调度的风险。最后，我们验证了提议的算法和软遮盾组件的性能评估。
</details></li>
</ul>
<hr>
<h2 id="Multi-Session-Budget-Optimization-for-Forward-Auction-based-Federated-Learning"><a href="#Multi-Session-Budget-Optimization-for-Forward-Auction-based-Federated-Learning" class="headerlink" title="Multi-Session Budget Optimization for Forward Auction-based Federated Learning"></a>Multi-Session Budget Optimization for Forward Auction-based Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12548">http://arxiv.org/abs/2311.12548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoli Tang, Han Yu</li>
<li>for: 这个研究旨在解决在多次训练Session中，联合学习（Federated Learning，FL）模型用户（Multi-User，MU）需要所有数据所有者（Data Owner，DO）集合在一起才能开始训练的问题。</li>
<li>methods: 本研究提出了一个基于层次强化学习的多Session预算优化策略（Multi-Session Budget Optimization Strategy，MultiBOS-AFL），它将在多Session中协同优化进行拍卖的预算和内部拍卖，以最大化总用户（Utility）。</li>
<li>results: 在六个 benchmark 数据集上进行了广泛的实验，该方法与七种现有的方法进行比较，结果显示，MultiBOS-AFL 可以将总用户提高12.28%，透过拍卖获取更多的数据（14.52%），并且提高模型的测试准确度（1.23%）。<details>
<summary>Abstract</summary>
Auction-based Federated Learning (AFL) has emerged as an important research field in recent years. The prevailing strategies for FL model users (MUs) assume that the entire team of the required data owners (DOs) for an FL task must be assembled before training can commence. In practice, an MU can trigger the FL training process multiple times. DOs can thus be gradually recruited over multiple FL model training sessions. Existing bidding strategies for AFL MUs are not designed to handle such scenarios. Therefore, the problem of multi-session AFL remains open. To address this problem, we propose the Multi-session Budget Optimization Strategy for forward Auction-based Federated Learning (MultiBOS-AFL). Based on hierarchical reinforcement learning, MultiBOS-AFL jointly optimizes inter-session budget pacing and intra-session bidding for AFL MUs, with the objective of maximizing the total utility. Extensive experiments on six benchmark datasets show that it significantly outperforms seven state-of-the-art approaches. On average, MultiBOS-AFL achieves 12.28% higher utility, 14.52% more data acquired through auctions for a given budget, and 1.23% higher test accuracy achieved by the resulting FL model compared to the best baseline. To the best of our knowledge, it is the first budget optimization decision support method with budget pacing capability designed for MUs in multi-session forward auction-based federated learning
</details>
<details>
<summary>摘要</summary>
优胜式联合学习（AFL）在最近几年内成为重要的研究领域。现有的策略假设所有联合数据拥有者（DO）必须在联合学习任务开始之前汇集。然而，实际情况是一个联合学习用户（MU）可以触发多次联合学习训练过程。DO可以逐渐加入多个联合学习训练会议。现有的拍卖策略不能处理这些场景。因此，多会议AFL问题仍然存在。为解决这个问题，我们提出了多会议预算优化策略（MultiBOS-AFL）。基于层次强化学习，MultiBOS-AFL同时优化了间会议预算步骤和会议中拍卖策略，以最大化总用得。经验表明，它在6个标准数据集上表现出色，与7种现有方法相比，平均提高了12.28%的用得，14.52%更多的预算通过拍卖获得，1.23%更高的测试精度。到目前为止，这是首个为MU在多会议前向拍卖基于联合学习的预算优化决策支持方法。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-Functions-with-Varying-Number-of-Minima"><a href="#In-Context-Learning-Functions-with-Varying-Number-of-Minima" class="headerlink" title="In-Context Learning Functions with Varying Number of Minima"></a>In-Context Learning Functions with Varying Number of Minima</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12538">http://arxiv.org/abs/2311.12538</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pittnail/icl-minima">https://github.com/pittnail/icl-minima</a></li>
<li>paper_authors: David Oniani, Yanshan Wang</li>
<li>for: 本研究探讨了语言模型在具体上学习（In-Context Learning，ICL）中的表现，以及ICL如何与函数approximation的特性相互作用。</li>
<li>methods: 本研究使用了正式框架来研究ICL，并提出了一种新的函数approximation任务，即将输入作为最小点生成函数。</li>
<li>results: 研究发现，增加最小点数会下降ICL性能，但ICL在所有设置下都能够超越2层神经网络（2NN）模型，并且ICL在所有设置下都能够更快地学习。这些结论得到了一系列几个极少例示的实验 validate。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have proven effective at In-Context Learning (ICL), an ability that allows them to create predictors from labeled examples. Few studies have explored the interplay between ICL and specific properties of functions it attempts to approximate. In our study, we use a formal framework to explore ICL and propose a new task of approximating functions with varying number of minima. We implement a method that allows for producing functions with given inputs as minima. We find that increasing the number of minima degrades ICL performance. At the same time, our evaluation shows that ICL outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster than 2NN in all settings. We validate the findings through a set of few-shot experiments across various hyperparameter configurations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Oasis-Data-Curation-and-Assessment-System-for-Pretraining-of-Large-Language-Models"><a href="#Oasis-Data-Curation-and-Assessment-System-for-Pretraining-of-Large-Language-Models" class="headerlink" title="Oasis: Data Curation and Assessment System for Pretraining of Large Language Models"></a>Oasis: Data Curation and Assessment System for Pretraining of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12537">http://arxiv.org/abs/2311.12537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tongzhou21/oasis">https://github.com/tongzhou21/oasis</a></li>
<li>paper_authors: Tong Zhou, Yubo Chen, Pengfei Cao, Kang Liu, Jun Zhao, Shengping Liu</li>
<li>for: 这篇论文的目的是提出一种针对大语言模型数据预处理 corpora 的自动化筛选和评估平台，以提高数据质量和量化。</li>
<li>methods: 该论文使用了一种交互式模块化规则筛选器，可以根据显式反馈来定制规则，以及一种倾斜分类模型来除掉偏见。还有一个可以执行大规模重复的文档去重模块。</li>
<li>results: 该论文通过使用 Oasis 平台对各种数据进行自动化筛选和评估，可以提高数据质量和量化。此外，该论文还公开发布了一个800GB的双语 corpora，以便进一步研究和应用。<details>
<summary>Abstract</summary>
Data is one of the most critical elements in building a large language model. However, existing systems either fail to customize a corpus curation pipeline or neglect to leverage comprehensive corpus assessment for iterative optimization of the curation. To this end, we present a pretraining corpus curation and assessment platform called Oasis -- a one-stop system for data quality improvement and quantification with user-friendly interactive interfaces. Specifically, the interactive modular rule filter module can devise customized rules according to explicit feedback. The debiased neural filter module builds the quality classification dataset in a negative-centric manner to remove the undesired bias. The adaptive document deduplication module could execute large-scale deduplication with limited memory resources. These three parts constitute the customized data curation module. And in the holistic data assessment module, a corpus can be assessed in local and global views, with three evaluation means including human, GPT-4, and heuristic metrics. We exhibit a complete process to use Oasis for the curation and assessment of pretraining data. In addition, an 800GB bilingual corpus curated by Oasis is publicly released.
</details>
<details>
<summary>摘要</summary>
<SYS>  将文本翻译到简化中文。</SYS>大量语言模型的建立需要一些关键元素，其中数据是其中之一。然而，现有系统可能会忽略或者不会自定义 corpus 筛选管道，从而导致不充分利用了全面的 corpus 评估，以便在 Iterative 优化中进行更好的数据优化。为此，我们提出了一个预训练 corpus 筛选和评估平台，称为 Oasis。 Oasis 是一个一站式系统，可以帮助用户提高数据质量，并且具有易用的交互式界面。 Specifically，Oasis 中的交互式模块可以根据用户的Explicit 反馈来制定自定义规则。 debiased  нейрон过滤模块可以在负面中建立质量分类数据集，以移除不良偏见。 Adaptive 文档重复模块可以在有限内存资源下执行大规模重复。这三部分组成了自定义数据筛选模块。而整体数据评估模块可以在本地和全球视图下评估 corpus，并使用人工、GPT-4 和 Heuristic 度量。我们展示了使用 Oasis 进行预训练数据的筛选和评估的完整过程。此外，我们也公共发布了 Oasis 预训练 800GB 双语词库。
</details></li>
</ul>
<hr>
<h2 id="Neural-Network-Pruning-by-Gradient-Descent"><a href="#Neural-Network-Pruning-by-Gradient-Descent" class="headerlink" title="Neural Network Pruning by Gradient Descent"></a>Neural Network Pruning by Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12526">http://arxiv.org/abs/2311.12526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/3riccc/neural_pruning">https://github.com/3riccc/neural_pruning</a></li>
<li>paper_authors: Zhang Zhang, Ruyi Tao, Jiang Zhang</li>
<li>for: 这篇论文旨在提出一个新的神经网络删除框架，以实现神经网络矩阵和结构同时优化，并且使用泊松-软max技术。</li>
<li>methods: 这个框架使用测量 Gradient Descent 进行统一的优化，可以同时删除神经网络的矩阵和结构，并且可以从删除的网络中提取重要的特征。</li>
<li>results: 实验结果显示，这个框架可以实现高精度的删除，只使用0.15%的原始网络parameters，并且可以增加神经网络的解释性，例如通过直接从删除的网络中提取特征的重要性，以及通过删除的网络中的特征相互关联和结构。<details>
<summary>Abstract</summary>
The rapid increase in the parameters of deep learning models has led to significant costs, challenging computational efficiency and model interpretability. In this paper, we introduce a novel and straightforward neural network pruning framework that incorporates the Gumbel-Softmax technique. This framework enables the simultaneous optimization of a network's weights and topology in an end-to-end process using stochastic gradient descent. Empirical results demonstrate its exceptional compression capability, maintaining high accuracy on the MNIST dataset with only 0.15\% of the original network parameters. Moreover, our framework enhances neural network interpretability, not only by allowing easy extraction of feature importance directly from the pruned network but also by enabling visualization of feature symmetry and the pathways of information propagation from features to outcomes. Although the pruning strategy is learned through deep learning, it is surprisingly intuitive and understandable, focusing on selecting key representative features and exploiting data patterns to achieve extreme sparse pruning. We believe our method opens a promising new avenue for deep learning pruning and the creation of interpretable machine learning systems.
</details>
<details>
<summary>摘要</summary>
“深度学习模型参数的快速增长导致了 significiant 成本、计算效率和模型解释性的挑战。在这篇论文中，我们介绍了一种新的和简单的神经网络剪枝框架，该框架通过使用Gumbel-Softmax技术来同时优化神经网络的权重和结构。我们通过使用渐进式梯度下降来实现这一目标。实验结果表明，我们的框架可以压缩神经网络参数，保持高度准确性，在MNIST数据集上只需0.15%的原始网络参数。此外，我们的框架还提高了神经网络解释性，不仅可以直接从剪枝后的神经网络中提取特征重要性，还可以Visualize特征的对称性和特征征向信息传递的路径。尽管剪枝策略是通过深度学习学习的，但它却感人INTUITIVE和理解，集中于选择关键代表特征并利用数据模式来实现极度稀疏剪枝。我们认为，我们的方法将开启深度学习剪枝的新的可能性，并创造可解释的机器学习系统。”
</details></li>
</ul>
<hr>
<h2 id="ALPHA-AnomaLous-Physiological-Health-Assessment-Using-Large-Language-Models"><a href="#ALPHA-AnomaLous-Physiological-Health-Assessment-Using-Large-Language-Models" class="headerlink" title="ALPHA: AnomaLous Physiological Health Assessment Using Large Language Models"></a>ALPHA: AnomaLous Physiological Health Assessment Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12524">http://arxiv.org/abs/2311.12524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mcjacktang/llm-healthassistant">https://github.com/mcjacktang/llm-healthassistant</a></li>
<li>paper_authors: Jiankai Tang, Kegang Wang, Hongming Hu, Xiyuxing Zhang, Peiyu Wang, Xin Liu, Yuntao Wang</li>
<li>for: 这项研究旨在评估大自然语言模型（LLMs）在医疗领域的有效性，特别是在个人异常健康监测中的应用。我们的研究主要是通过使用获取了FDA批准设备的生物 физиологи数据进行分析和解释来评估LLMs的能力。</li>
<li>methods: 我们使用了一种模拟海拔低压环境中获取的异常生理数据进行了广泛的分析，以评估LLMs在诊断和评估用户健康状况方面的精度和可靠性。</li>
<li>results: 我们发现LLMs在诊断医学指标方面表现出色，包括心率的 Mean Absolute Error（MAE）低于1 beat&#x2F;分钟，和氧气含量（SpO2）的 Mean Absolute Percentage Error（MAPE）低于1%，总的医疗评估准确率超过85%。在图像分析任务中，我们特制的GPT模型在解读光谱波格raph（PPG）数据方面表现出色，MAE低于1 bpm，心率估计 error低于7.28%。这项研究证明LLMs可以作为健康数据分析工具和高级人工智能医疗助手的重要组成部分，提供个性化的健康洞察和建议。<details>
<summary>Abstract</summary>
This study concentrates on evaluating the efficacy of Large Language Models (LLMs) in healthcare, with a specific focus on their application in personal anomalous health monitoring. Our research primarily investigates the capabilities of LLMs in interpreting and analyzing physiological data obtained from FDA-approved devices. We conducted an extensive analysis using anomalous physiological data gathered in a simulated low-air-pressure plateau environment. This allowed us to assess the precision and reliability of LLMs in understanding and evaluating users' health status with notable specificity. Our findings reveal that LLMs exhibit exceptional performance in determining medical indicators, including a Mean Absolute Error (MAE) of less than 1 beat per minute for heart rate and less than 1% for oxygen saturation (SpO2). Furthermore, the Mean Absolute Percentage Error (MAPE) for these evaluations remained below 1%, with the overall accuracy of health assessments surpassing 85%. In image analysis tasks, such as interpreting photoplethysmography (PPG) data, our specially adapted GPT models demonstrated remarkable proficiency, achieving less than 1 bpm error in cycle count and 7.28 MAE for heart rate estimation. This study highlights LLMs' dual role as health data analysis tools and pivotal elements in advanced AI health assistants, offering personalized health insights and recommendations within the future health assistant framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Classification-of-Tabular-Data-by-Text-Processing"><a href="#Classification-of-Tabular-Data-by-Text-Processing" class="headerlink" title="Classification of Tabular Data by Text Processing"></a>Classification of Tabular Data by Text Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12521">http://arxiv.org/abs/2311.12521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keshav Ramani, Daniel Borrajo</li>
<li>for: 本研究提出了一个新的文本基于分类框架(Text Based Classification, TBC)，用于应用文本处理技术来解决条件分类任务。</li>
<li>methods: 本研究使用了现代文本处理技术，包括文本特征提取和分类模型。</li>
<li>results: 实验结果显示，TBC框架可以与多种现代分类模型相比，在精度、特征选择和预测类别的精度方面具有相似的性能。<details>
<summary>Abstract</summary>
Natural Language Processing technology has advanced vastly in the past decade. Text processing has been successfully applied to a wide variety of domains. In this paper, we propose a novel framework, Text Based Classification(TBC), that uses state of the art text processing techniques to solve classification tasks on tabular data. We provide a set of controlled experiments where we present the benefits of using this approach against other classification methods. Experimental results on several data sets also show that this framework achieves comparable performance to that of several state of the art models in accuracy, precision and recall of predicted classes.
</details>
<details>
<summary>摘要</summary>
自过去一代，自然语言处理技术有了很大的进步。文本处理技术已经成功应用于各种领域。在这篇论文中，我们提出了一个新的框架，文本基于类别（TBC），它使用现代文本处理技术来解决标量数据上的分类任务。我们提供了一组控制的实验，其中我们展示了使用这种方法的好处，与其他分类方法进行比较。实验结果也显示，这个框架在几个数据集上达到了与当今state of the art模型的准确率、精度和准确率的比较。
</details></li>
</ul>
<hr>
<h2 id="Fin-QD-A-Computational-Design-Framework-for-Soft-Grippers-Integrating-MAP-Elites-and-High-fidelity-FEM"><a href="#Fin-QD-A-Computational-Design-Framework-for-Soft-Grippers-Integrating-MAP-Elites-and-High-fidelity-FEM" class="headerlink" title="Fin-QD: A Computational Design Framework for Soft Grippers: Integrating MAP-Elites and High-fidelity FEM"></a>Fin-QD: A Computational Design Framework for Soft Grippers: Integrating MAP-Elites and High-fidelity FEM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12477">http://arxiv.org/abs/2311.12477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Xie, Xing Wang, Fumiya Iida, David Howard</li>
<li>for: 这个论文旨在开发一种自动化计算设计优化框架，以实现手动设计多种吸盘器，以便在不同物理环境下成功地抓取多种几何体积物体。</li>
<li>methods: 该论文使用了一种基于质量多样性的计算设计优化策略，以探索吸盘器框架设计空间中的巨大可能性空间。同时，使用了SOFA中的 contacts-based Finite Element Modeling（FEM）来生成高精度抓取数据，并对吸盘器的特征进行评估和测量。</li>
<li>results: 该论文通过自动化计算设计优化框架，成功地生成了多种吸盘器设计，以便在不同物理环境下抓取多种几何体积物体。这些设计具有较高的吸盘器体积和工作空间，同时能够在简单的控制方案下实现高效的抓取行为。<details>
<summary>Abstract</summary>
Computational design can excite the full potential of soft robotics that has the drawbacks of being highly nonlinear from material, structure, and contact. Up to date, enthusiastic research interests have been demonstrated for individual soft fingers, but the frame design space (how each soft finger is assembled) remains largely unexplored. Computationally design remains challenging for the finger-based soft gripper to grip across multiple geometrical-distinct object types successfully. Including the design space for the gripper frame can bring huge difficulties for conventional optimisation algorithms and fitness calculation methods due to the exponential growth of high-dimensional design space. This work proposes an automated computational design optimisation framework that generates gripper diversity to individually grasp geometrically distinct object types based on a quality-diversity approach. This work first discusses a significantly large design space (28 design parameters) for a finger-based soft gripper, including the rarely-explored design space of finger arrangement that is converted to various configurations to arrange individual soft fingers. Then, a contact-based Finite Element Modelling (FEM) is proposed in SOFA to output high-fidelity grasping data for fitness evaluation and feature measurements. Finally, diverse gripper designs are obtained from the framework while considering features such as the volume and workspace of grippers. This work bridges the gap of computationally exploring the vast design space of finger-based soft grippers while grasping large geometrically distinct object types with a simple control scheme.
</details>
<details>
<summary>摘要</summary>
computational design可以激发软 robotics的全部潜力，软 robotics具有材料、结构和接触的高度非线性。目前，对具有各种软指的研究兴趣很高，但是把各个软指组装在一起的框架设计空间（软指把手的设计）仍然未得到充分探索。计算机设计对软指抓取器抓取多种几何体型成功仍然是挑战。由于抓取器框架的设计空间是高维的，使用传统优化算法和评价方法会遇到巨大的困难。本工作提出了一个自动化计算设计优化框架，该框架可以生成软指抓取器的多样性，以便在多种几何体型上成功抓取。本工作首先介绍了软指抓取器的庞大设计空间（28个参数），包括软指的布局设计空间，该空间通过不同的配置转换为多种配置。然后，提出了基于SOFA的contact-based Finite Element Modelling（FEM），以生成高精度的抓取数据，用于评价和特征测量。最后，该框架可以从多种 consid=ering特征，如抓取器的体积和工作空间，获得多种软指抓取器的设计。本工作将计算机设计探索软指抓取器的庞大设计空间和多种几何体型之间的关系，提供了一个简单的控制方案。
</details></li>
</ul>
<hr>
<h2 id="PhayaThaiBERT-Enhancing-a-Pretrained-Thai-Language-Model-with-Unassimilated-Loanwords"><a href="#PhayaThaiBERT-Enhancing-a-Pretrained-Thai-Language-Model-with-Unassimilated-Loanwords" class="headerlink" title="PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords"></a>PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12475">http://arxiv.org/abs/2311.12475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panyut Sriwirote, Jalinee Thapiang, Vasan Timtong, Attapol T. Rutherford</li>
<li>for: 这篇论文是为了提高泰语语言模型的性能，特别是对外语词汇的理解。</li>
<li>methods: 该论文使用了词汇传递法来扩展 WangchanBERTa 的词汇库，并从 XLM-R 的预训练 tokenizer 中获取外语词汇。然后，该论文使用了这个扩展后的 tokenizer 和 WangchanBERTa 的起点Checkpoint来预训练一个新的模型，并在一个更大的数据集上进行训练。</li>
<li>results: 该论文的新预训练模型（即 PhayaThaiBERT）在多个下游任务和数据集上表现出色，比 WangchanBERTa 更高。<details>
<summary>Abstract</summary>
While WangchanBERTa has become the de facto standard in transformer-based Thai language modeling, it still has shortcomings in regard to the understanding of foreign words, most notably English words, which are often borrowed without orthographic assimilation into Thai in many contexts. We identify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the main source of these shortcomings. We then expand WangchanBERTa's vocabulary via vocabulary transfer from XLM-R's pretrained tokenizer and pretrain a new model using the expanded tokenizer, starting from WangchanBERTa's checkpoint, on a new dataset that is larger than the one used to train WangchanBERTa. Our results show that our new pretrained model, PhayaThaiBERT, outperforms WangchanBERTa in many downstream tasks and datasets.
</details>
<details>
<summary>摘要</summary>
While WangchanBERTa has become the de facto standard in transformer-based Thai language modeling, it still has shortcomings in regard to the understanding of foreign words, most notably English words, which are often borrowed without orthographic assimilation into Thai in many contexts. We identify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the main source of these shortcomings. We then expand WangchanBERTa's vocabulary via vocabulary transfer from XLM-R's pretrained tokenizer and pretrain a new model using the expanded tokenizer, starting from WangchanBERTa's checkpoint, on a new dataset that is larger than the one used to train WangchanBERTa. Our results show that our new pretrained model, PhayaThaiBERT, outperforms WangchanBERTa in many downstream tasks and datasets.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other regions.
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Deconfounding-Against-Spatio-Temporal-Shifts-Theory-and-Modeling"><a href="#Self-Supervised-Deconfounding-Against-Spatio-Temporal-Shifts-Theory-and-Modeling" class="headerlink" title="Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and Modeling"></a>Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12472">http://arxiv.org/abs/2311.12472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shotdowndiane/steve">https://github.com/shotdowndiane/steve</a></li>
<li>paper_authors: Jiahao Ji, Wentao Zhang, Jingyuan Wang, Yue He, Chao Huang</li>
<li>for: 这项研究的目的是提高城市交通效率和推动可持续发展，通过对时空数据进行预测。</li>
<li>methods: 该研究使用 causal 图来构造时空数据的 causal 关系，并提出了一种名为 Disentangled Contextual Adjustment (DCA) 的解决方案，以及一种 Spatio-Temporal sElf-superVised dEconfounding (STEVE) 框架。</li>
<li>results: 该研究的实验结果表明，STEVE 在不同的时空数据外测场景下都能够超过州态艺术的基eline。<details>
<summary>Abstract</summary>
As an important application of spatio-temporal (ST) data, ST traffic forecasting plays a crucial role in improving urban travel efficiency and promoting sustainable development. In practice, the dynamics of traffic data frequently undergo distributional shifts attributed to external factors such as time evolution and spatial differences. This entails forecasting models to handle the out-of-distribution (OOD) issue where test data is distributed differently from training data. In this work, we first formalize the problem by constructing a causal graph of past traffic data, future traffic data, and external ST contexts. We reveal that the failure of prior arts in OOD traffic data is due to ST contexts acting as a confounder, i.e., the common cause for past data and future ones. Then, we propose a theoretical solution named Disentangled Contextual Adjustment (DCA) from a causal lens. It differentiates invariant causal correlations against variant spurious ones and deconfounds the effect of ST contexts. On top of that, we devise a Spatio-Temporal sElf-superVised dEconfounding (STEVE) framework. It first encodes traffic data into two disentangled representations for associating invariant and variant ST contexts. Then, we use representative ST contexts from three conceptually different perspectives (i.e., temporal, spatial, and semantic) as self-supervised signals to inject context information into both representations. In this way, we improve the generalization ability of the learned context-oriented representations to OOD ST traffic forecasting. Comprehensive experiments on four large-scale benchmark datasets demonstrate that our STEVE consistently outperforms the state-of-the-art baselines across various ST OOD scenarios.
</details>
<details>
<summary>摘要</summary>
为了提高城市交通效率和推动可持续发展，ST数据预测作为重要应用在ST数据中扮演着关键的角色。然而，交通数据的动态过程中经常出现分布性的变化，这使得预测模型需要处理异常数据（OOD）问题。在这种情况下，我们首先将问题正式化， constructed a causal graph of past traffic data, future traffic data, and external ST contexts。我们发现，先前的方法在OOD交通数据上出现失败的原因是ST上下文作为共同因素，即过去数据和未来数据之间的共同原因。然后，我们提出了一种名为分解上下文调整（DCA）的理论解决方案，它可以从 causal 镜像中分解不变 causal 相关性和变异的干扰因素。此外，我们还提出了一种STEVE框架，它首先将交通数据编码成两个分解表示，其中一个表示不变的ST上下文，另一个表示变化的ST上下文。然后，我们使用不同的三个视角（时间、空间和semantic）中的代表ST上下文来自顾框架。通过这种方式，我们可以在OOD ST交通预测中提高学习的Context-oriented表示的普适性。我们在四个大规模 benchmark 数据集上进行了广泛的实验，结果显示，我们的STEVE consistently 超过了状态机制基elines across various ST OOD scenarios。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Gateway-for-Knowledge-Graph-Schemas-Collection-Analysis-and-Embedding"><a href="#Towards-a-Gateway-for-Knowledge-Graph-Schemas-Collection-Analysis-and-Embedding" class="headerlink" title="Towards a Gateway for Knowledge Graph Schemas Collection, Analysis, and Embedding"></a>Towards a Gateway for Knowledge Graph Schemas Collection, Analysis, and Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12465">http://arxiv.org/abs/2311.12465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattia Fumagalli, Marco Boffo, Daqian Shi, Mayukh Bagchi, Fausto Giunchiglia</li>
<li>for: 该论文的目的是如何使用现有的知识图来训练统计模型。</li>
<li>methods: 该论文使用了一个名为LiveSchema的网关，该网关可以将多种源 catalogs和Repository 的数据集合起来，并提供了一些关键的功能，如查询所有收集的资源、将每个给定的数据集转换成正式概念分析矩阵、生成模型和张量。</li>
<li>results: 该论文提出了一个初版的LiveSchema网关，该网关可以将多种现有的知识图和ontology 集成起来，并提供了一些关键的功能，如查询所有收集的资源、将每个给定的数据集转换成正式概念分析矩阵、生成模型和张量。<details>
<summary>Abstract</summary>
One of the significant barriers to the training of statistical models on knowledge graphs is the difficulty that scientists have in finding the best input data to address their prediction goal. In addition to this, a key challenge is to determine how to manipulate these relational data, which are often in the form of particular triples (i.e., subject, predicate, object), to enable the learning process. Currently, many high-quality catalogs of knowledge graphs, are available. However, their primary goal is the re-usability of these resources, and their interconnection, in the context of the Semantic Web. This paper describes the LiveSchema initiative, namely, a first version of a gateway that has the main scope of leveraging the gold mine of data collected by many existing catalogs collecting relational data like ontologies and knowledge graphs. At the current state, LiveSchema contains - 1000 datasets from 4 main sources and offers some key facilities, which allow to: i) evolving LiveSchema, by aggregating other source catalogs and repositories as input sources; ii) querying all the collected resources; iii) transforming each given dataset into formal concept analysis matrices that enable analysis and visualization services; iv) generating models and tensors from each given dataset.
</details>
<details>
<summary>摘要</summary>
一个 significante barrier to the training of statistical models on knowledge graphs is the difficulty that scientists have in finding the best input data to address their prediction goal. In addition to this, a key challenge is to determine how to manipulate these relational data, which are often in the form of particular triples (i.e., subject, predicate, object), to enable the learning process. Currently, many high-quality catalogs of knowledge graphs, are available. However, their primary goal is the re-usability of these resources, and their interconnection, in the context of the Semantic Web. This paper describes the LiveSchema initiative, namely, a first version of a gateway that has the main scope of leveraging the gold mine of data collected by many existing catalogs collecting relational data like ontologies and knowledge graphs. At the current state, LiveSchema contains - 1000 datasets from 4 main sources and offers some key facilities, which allow to: i) evolving LiveSchema, by aggregating other source catalogs and repositories as input sources; ii) querying all the collected resources; iii) transforming each given dataset into formal concept analysis matrices that enable analysis and visualization services; iv) generating models and tensors from each given dataset.Here's the breakdown of the translation:* 一个 significante barrier (一个 significante barrier) - This phrase is used to emphasize the significance of the barrier.* difficulty (difficulty) - The word "difficulty" is used to describe the challenge of finding the best input data.* 找到最佳输入数据 (finding the best input data) - This phrase is used to describe the goal of finding the best input data for training statistical models.*  relational data (relational data) - This phrase is used to describe the type of data that is in the form of particular triples.* 如ontologies and knowledge graphs (such as ontologies and knowledge graphs) - This phrase is used to provide examples of the type of data that is being referred to.*  LiveSchema (LiveSchema) - This is the name of the initiative being described.*  gateway (gateway) - This word is used to describe the main scope of the LiveSchema initiative.*  leveraging (leveraging) - This word is used to describe the main goal of the LiveSchema initiative, which is to leverage the data collected by many existing catalogs.*  gold mine (gold mine) - This phrase is used to emphasize the richness of the data collected by the catalogs.*  data (data) - This word is used to refer to the data collected by the catalogs.*  collecting (collecting) - This word is used to describe the action of collecting data.*  relational data (relational data) - This phrase is used to describe the type of data being collected.*  like ontologies and knowledge graphs (like ontologies and knowledge graphs) - This phrase is used to provide examples of the type of data being collected.*  Currently (currently) - This word is used to indicate that the data collection is ongoing.*  many high-quality catalogs (many high-quality catalogs) - This phrase is used to describe the quality of the catalogs.*  are available (are available) - This phrase is used to indicate that the catalogs are accessible.*  primary goal (primary goal) - This phrase is used to describe the main goal of the catalogs, which is the re-usability of the resources.*  interconnection (interconnection) - This phrase is used to describe the goal of interconnecting the resources.*  in the context of the Semantic Web (in the context of the Semantic Web) - This phrase is used to provide context for the goal of interconnecting the resources.*  This paper (this paper) - This phrase is used to refer to the paper being described.*  describes (describes) - This word is used to indicate that the paper describes the LiveSchema initiative.*  the LiveSchema initiative (the LiveSchema initiative) - This phrase is used to refer to the initiative being described.*  a first version (a first version) - This phrase is used to indicate that the LiveSchema initiative is a first version of a gateway.*  that has the main scope (that has the main scope) - This phrase is used to describe the main goal of the LiveSchema initiative.*  of leveraging (of leveraging) - This word is used to describe the main goal of the LiveSchema initiative, which is to leverage the data collected by many existing catalogs.*  the gold mine (the gold mine) - This phrase is used to emphasize the richness of the data collected by the catalogs.*  At the current state (At the current state) - This phrase is used to indicate that the LiveSchema initiative is currently in progress.*  contains (contains) - This word is used to describe the contents of the LiveSchema initiative.*  - 1000 datasets (1000 datasets) - This phrase is used to describe the number of datasets contained in the LiveSchema initiative.*  from 4 main sources (from 4 main sources) - This phrase is used to describe the sources of the datasets.*  and offers (and offers) - This phrase is used to describe the facilities offered by the LiveSchema initiative.*  some key facilities (some key facilities) - This phrase is used to emphasize the importance of the facilities offered.*  i) evolving LiveSchema (i) evolving LiveSchema) - This phrase is used to describe the first facility offered by the LiveSchema initiative.*  by aggregating (by aggregating) - This word is used to describe the action of aggregating data.*  other source catalogs and repositories (other source catalogs and repositories) - This phrase is used to describe the data being aggregated.*  as input sources (as input sources) - This phrase is used to describe the purpose of aggregating the data.*  ii) querying all the collected resources (ii) querying all the collected resources) - This phrase is used to describe the second facility offered by the LiveSchema initiative.*  iii) transforming each given dataset (iii) transforming each given dataset) - This phrase is used to describe the third facility offered by the LiveSchema initiative.*  into formal concept analysis matrices (into formal concept analysis matrices) - This phrase is used to describe the action of transforming the datasets.*  that enable analysis and visualization services (that enable analysis and visualization services) - This phrase is used to describe the purpose of transforming the datasets.*  iv) generating models and tensors (iv) generating models and tensors) - This phrase is used to describe the fourth facility offered by the LiveSchema initiative.*  from each given dataset (from each given dataset) - This phrase is used to describe the purpose of generating models and tensors.
</details></li>
</ul>
<hr>
<h2 id="HierSpeech-Bridging-the-Gap-between-Semantic-and-Acoustic-Representation-of-Speech-by-Hierarchical-Variational-Inference-for-Zero-shot-Speech-Synthesis"><a href="#HierSpeech-Bridging-the-Gap-between-Semantic-and-Acoustic-Representation-of-Speech-by-Hierarchical-Variational-Inference-for-Zero-shot-Speech-Synthesis" class="headerlink" title="HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis"></a>HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12454">http://arxiv.org/abs/2311.12454</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sh-lee-prml/hierspeechpp">https://github.com/sh-lee-prml/hierspeechpp</a></li>
<li>paper_authors: Sang-Hoon Lee, Ha-Yeong Choi, Seung-Bin Kim, Seong-Whan Lee</li>
<li>for: 这篇论文旨在提出一种快速和强大的零shot语音合成器，用于文本到语音（TTS）和语音转换（VC）。</li>
<li>methods: 该论文使用了层次语音合成框架，以提高合成语音的稳定性和表达力。具体来说，它采用了文本到 вектор框架，生成一个自我超vised语音表示和F0表示，然后使用这些表示生成语音。此外，它还引入了高效的语音超分辨框架，从16 kHz提升到48 kHz。</li>
<li>results: 实验结果表明，层次变分 autoencoder 可以作为一个强大的零shot语音合成器，并且其性能高于 LLM-based 和扩散基于的模型。此外，它还实现了首次人类水平质量的零shot语音合成。<details>
<summary>Abstract</summary>
Large language models (LLM)-based speech synthesis has been widely adopted in zero-shot speech synthesis. However, they require a large-scale data and possess the same limitations as previous autoregressive speech models, including slow inference speed and lack of robustness. This paper proposes HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech (TTS) and voice conversion (VC). We verified that hierarchical speech synthesis frameworks could significantly improve the robustness and expressiveness of the synthetic speech. Furthermore, we significantly improve the naturalness and speaker similarity of synthetic speech even in zero-shot speech synthesis scenarios. For text-to-speech, we adopt the text-to-vec framework, which generates a self-supervised speech representation and an F0 representation based on text representations and prosody prompts. Then, HierSpeech++ generates speech from the generated vector, F0, and voice prompt. We further introduce a high-efficient speech super-resolution framework from 16 kHz to 48 kHz. The experimental results demonstrated that the hierarchical variational autoencoder could be a strong zero-shot speech synthesizer given that it outperforms LLM-based and diffusion-based models. Moreover, we achieved the first human-level quality zero-shot speech synthesis. Audio samples and source code are available at https://github.com/sh-lee-prml/HierSpeechpp.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）基于的语音合成已广泛应用于零shot语音合成。然而，它们需要大量数据并具有之前autoregressive语音模型的同样局限性，包括慢速推理速度和缺乏可靠性。这篇论文提出了层次语音合成框架HierSpeech++，它是一个快速强大的零shot语音合成器。我们证明了层次语音合成框架可以显著提高合成语音的稳定性和表达力。此外，我们在零shot语音合成场景下还可以提高合成语音的自然性和发音人类化。对于文本到语音（TTS），我们采用文本到vec框架，它生成了一个自我超vised语音表示和F0表示基于文本表示和气质提示。然后，HierSpeech++生成语音从生成的向量、F0和声音提示。我们还提出了一种高效的语音超分辨框架，从16 kHz提升到48 kHz。实验结果表明，层次变分析器可以作为一个强大的零shot语音合成器，并且超过LLM基于和扩散基于模型。此外，我们实现了人类水平的零shot语音合成。音频样本和源代码可以在https://github.com/sh-lee-prml/HierSpeechpp上获取。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Definienda-in-Mathematical-Scholarly-Articles-with-Transformers"><a href="#Extracting-Definienda-in-Mathematical-Scholarly-Articles-with-Transformers" class="headerlink" title="Extracting Definienda in Mathematical Scholarly Articles with Transformers"></a>Extracting Definienda in Mathematical Scholarly Articles with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12448">http://arxiv.org/abs/2311.12448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sufianj/def_extraction">https://github.com/sufianj/def_extraction</a></li>
<li>paper_authors: Shufan Jiang, Pierre Senellart</li>
<li>for: 本研究旨在自动识别学术论文中定义的概念。</li>
<li>methods: 本研究使用了精度适应的 пре-训练 transformer 进行 Token-level 分类任务，以及一个通用的大语言模型 (GPT) 进行问题解答任务。</li>
<li>results: 实验结果显示，可以使用 recent (和昂贵) GPT 4 或更简单的预训练模型，以高精度和回传率进行定义识别。<details>
<summary>Abstract</summary>
We consider automatically identifying the defined term within a mathematical definition from the text of an academic article. Inspired by the development of transformer-based natural language processing applications, we pose the problem as (a) a token-level classification task using fine-tuned pre-trained transformers; and (b) a question-answering task using a generalist large language model (GPT). We also propose a rule-based approach to build a labeled dataset from the LATEX source of papers. Experimental results show that it is possible to reach high levels of precision and recall using either recent (and expensive) GPT 4 or simpler pre-trained models fine-tuned on our task.
</details>
<details>
<summary>摘要</summary>
我们考虑自动从学术论文中提取定义中的特定 термин。受过 transformations 基础模型的发展启发，我们将问题定义为（a）一个token级别分类任务，使用调整后的预训练 transformers；以及（b）一个问答任务，使用通用大语言模型（GPT）。我们还提出了一种基于规则的方法来从 LATEX 文档中生成标注数据集。实验结果显示，可以使用Recent（和昂贵） GPT 4 或更简单的预训练模型，达到高精度和准确率。
</details></li>
</ul>
<hr>
<h2 id="Designing-Long-term-Group-Fair-Policies-in-Dynamical-Systems"><a href="#Designing-Long-term-Group-Fair-Policies-in-Dynamical-Systems" class="headerlink" title="Designing Long-term Group Fair Policies in Dynamical Systems"></a>Designing Long-term Group Fair Policies in Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12447">http://arxiv.org/abs/2311.12447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miriam Rateike, Isabel Valera, Patrick Forré</li>
<li>for: 这篇论文旨在提出一个新的框架，以长期确保系统动态中的群体公平。</li>
<li>methods: 本论文使用时间同调Markov链来模型系统动态，并利用Markov链均匀定理来优化策略，以确保策略在长期下可以对Targeted公平状态 converge。</li>
<li>results: 本论文提出了一个时间独立的策略，可以在系统动态中实现长期的群体公平，并且可以评估不同的长期目标，包括不同的社会和政策实现的公平目标。<details>
<summary>Abstract</summary>
Neglecting the effect that decisions have on individuals (and thus, on the underlying data distribution) when designing algorithmic decision-making policies may increase inequalities and unfairness in the long term - even if fairness considerations were taken in the policy design process. In this paper, we propose a novel framework for achieving long-term group fairness in dynamical systems, in which current decisions may affect an individual's features in the next step, and thus, future decisions. Specifically, our framework allows us to identify a time-independent policy that converges, if deployed, to the targeted fair stationary state of the system in the long term, independently of the initial data distribution. We model the system dynamics with a time-homogeneous Markov chain and optimize the policy leveraging the Markov chain convergence theorem to ensure unique convergence. We provide examples of different targeted fair states of the system, encompassing a range of long-term goals for society and policymakers. Furthermore, we show how our approach facilitates the evaluation of different long-term targets by examining their impact on the group-conditional population distribution in the long term and how it evolves until convergence.
</details>
<details>
<summary>摘要</summary>
忽略算法决策对个人（以及相应的数据分布）的影响可能导致长期不平等和不公正，即使在政策设计过程中考虑了公平考虑因素。在这篇论文中，我们提出了一种新的框架，用于实现长期团体公平的动态系统中的决策。specifically，我们的框架允许我们在下一步的决策中考虑当前决策对个人的影响，从而在长期实现targeted公平状态。我们使用时间几乎同质的Markov链来模型系统动态，并通过Markov链收敛定理来确保唯一收敛。我们还提供了不同的targeted公平状态的示例，涵盖了社会和政策制定者的长期目标。此外，我们还示了我们的方法如何评估不同的长期目标，以及它们在长期发展中对群体公共分布的影响。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Base-Enabled-Semantic-Communication-A-Generative-Perspective"><a href="#Knowledge-Base-Enabled-Semantic-Communication-A-Generative-Perspective" class="headerlink" title="Knowledge Base Enabled Semantic Communication: A Generative Perspective"></a>Knowledge Base Enabled Semantic Communication: A Generative Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12443">http://arxiv.org/abs/2311.12443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinke Ren, Zezhong Zhang, Jie Xu, Guanying Chen, Yaping Sun, Ping Zhang, Shuguang Cui</li>
<li>for: 提高 sixth-generation 无线网络的通信效率</li>
<li>methods: 利用 semantic knowledge base 技术</li>
<li>results: 提高通信效率，超越传统的 sintactic 通信和 classical semantic 通信Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the communication efficiency of sixth-generation wireless networks by exploiting semantic knowledge base technology.</li>
<li>methods: The paper proposes using semantic knowledge base to represent source messages in low-dimensional subspaces while preserving their desired meaning. This is achieved by introducing three sub-KBs: source, task, and channel KBs.</li>
<li>results: The paper demonstrates the superiority of generative semantic communication over conventional syntactic communication and classical semantic communication through a case study. The results show that generative semantic communication can significantly enhance the communication efficiency.<details>
<summary>Abstract</summary>
Semantic communication is widely touted as a key technology for propelling the sixth-generation (6G) wireless networks. However, providing effective semantic representation is quite challenging in practice. To address this issue, this article takes a crack at exploiting semantic knowledge base (KB) to usher in a new era of generative semantic communication. Via semantic KB, source messages can be characterized in low-dimensional subspaces without compromising their desired meaning, thus significantly enhancing the communication efficiency. The fundamental principle of semantic KB is first introduced, and a generative semantic communication architecture is developed by presenting three sub-KBs, namely source, task, and channel KBs. Then, the detailed construction approaches for each sub-KB are described, followed by their utilization in terms of semantic coding and transmission. A case study is also provided to showcase the superiority of generative semantic communication over conventional syntactic communication and classical semantic communication. In a nutshell, this article establishes a scientific foundation for the exciting uncharted frontier of generative semantic communication.
</details>
<details>
<summary>摘要</summary>
semantic communication 被广泛提出为第六代无线网络（6G）的关键技术。然而，在实践中提供有效的 semantic representation 是很困难的。为解决这个问题，本文尝试利用 semantic knowledge base（KB），推动一个新的 era of generative semantic communication。通过 semantic KB，源消息可以在低维度空间中表示，而不会失去其愿意的含义，因此明显提高了通信效率。 semantic KB 的基本原则首先被介绍，然后是基于三个子 KB：源 KB、任务 KB 和通道 KB。每个子 KB 的建构方法被详细描述，以及它们在 semantic coding 和传输中的使用。此外，还提供了一个 case study，以显示 generative semantic communication 的优越性，相比于 conventional syntactic communication 和 classical semantic communication。简单来说，本文建立了 generative semantic communication 的科学基础，开启了无人曾经踏进的未知领域。
</details></li>
</ul>
<hr>
<h2 id="Fair-Enough-A-map-of-the-current-limitations-of-the-requirements-to-have-“fair’’-algorithms"><a href="#Fair-Enough-A-map-of-the-current-limitations-of-the-requirements-to-have-“fair’’-algorithms" class="headerlink" title="Fair Enough? A map of the current limitations of the requirements to have “fair’’ algorithms"></a>Fair Enough? A map of the current limitations of the requirements to have “fair’’ algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12435">http://arxiv.org/abs/2311.12435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Castelnovo, Nicole Inverardi, Gabriele Nanino, Ilaria Giuseppina Penco, Daniele Regoli</li>
<li>for: The paper discusses the issue of bias and unfairness in automated decision-making systems and the need for more societal choices to make the demand for “fair” algorithms actionable.</li>
<li>methods: The paper highlights the current efforts to assess, quantify, and mitigate biases in algorithms, but argues that these efforts are not enough without a clearer understanding of what “fairness” means in real-world scenarios.</li>
<li>results: The paper identifies a list of fundamental ambiguities and attention points that must be addressed in order to give a concrete meaning to the demand for fairness in automated decision-making systems.<details>
<summary>Abstract</summary>
In the recent years, the raise in the usage and efficiency of Artificial Intelligence and, more in general, of Automated Decision-Making systems has brought with it an increasing and welcome awareness of the risks associated with such systems. One of such risks is that of perpetuating or even amplifying bias and unjust disparities present in the data from which many of these systems learn to adjust and optimise their decisions. This awareness has on one side encouraged several scientific communities to come up with more and more appropriate ways and methods to assess, quantify, and possibly mitigate such biases and disparities. On the other hand, it has prompted more and more layers of society, including policy makers, to call for ``fair'' algorithms. We believe that while a lot of excellent and multidisciplinary research is currently being conducted, what is still fundamentally missing is the awareness that having ``fair'' algorithms is per s\'e a nearly meaningless requirement, that needs to be complemented with a lot of additional societal choices to become actionable. Namely, there is a hiatus between what the society is demanding from Automated Decision-Making systems, and what this demand actually means in real-world scenarios. In this work, we outline the key features of such a hiatus, and pinpoint a list of fundamental ambiguities and attention points that we as a society must address in order to give a concrete meaning to the increasing demand of fairness in Automated Decision-Making systems.
</details>
<details>
<summary>摘要</summary>
在过去几年，人工智能和自动决策系统的使用和效率的提高，使人们对这些系统的风险的认识提高，特别是对它们学习和优化决策的数据中存在的偏见和不公平的问题。这种认识导致了一些科学社区开发出更多和更适合的方法来评估、量化和可能地mitigate这些偏见和不公平。同时，更多的社会层次，包括政策制定者，呼吁“公平”的算法。我们认为，虽然目前有很多优秀的多学科研究，但实际上还缺乏基本的社会选择，以使“公平”的算法成为实际行动的准则。即使在社会对自动决策系统的要求中，有一个差距，这些要求在实际情况下并没有具体的含义。在这项工作中，我们描述了这个差距的关键特征，并指出了一些基本的歧义和注意事项，我们作为一个社会必须解决，以赋予“公平”的算法具体的含义。
</details></li>
</ul>
<hr>
<h2 id="A-recurrent-connectionist-model-of-melody-perception-An-exploration-using-TRACX2"><a href="#A-recurrent-connectionist-model-of-melody-perception-An-exploration-using-TRACX2" class="headerlink" title="A recurrent connectionist model of melody perception : An exploration using TRACX2"></a>A recurrent connectionist model of melody perception : An exploration using TRACX2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12431">http://arxiv.org/abs/2311.12431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Defays, Robert French, Barbara Tillmann</li>
<li>for: 这研究探讨了一种基于认知的连接主义自适应模型（TRACX2）是如何应用于基本旋律识别。</li>
<li>methods: 这个模型是一个三层自适应神经网络，通过识别’’chunk’’的短序列间隔而模仿了语音和序列图像处理。</li>
<li>results: 研究发现，TRACX2可以在音乐中识别’’human-recognizable’’的旋律类别，并且感知音频序列的形态和距离信息。这些结果表明，TRACX2可能是一种通用的分割和拼接机制，不仅适用于单词和图像分割，还适用于基本旋律处理。<details>
<summary>Abstract</summary>
Are similar, or even identical, mechanisms used in the computational modeling of speech segmentation, serial image processing and music processing? We address this question by exploring how TRACX2, (French et al., 2011; French \& Cottrell, 2014; Mareschal \& French, 2017), a recognition-based, recursive connectionist autoencoder model of chunking and sequence segmentation, which has successfully simulated speech and serial-image processing, might be applied to elementary melody perception. The model, a three-layer autoencoder that recognizes ''chunks'' of short sequences of intervals that have been frequently encountered on input, is trained on the tone intervals of melodically simple French children's songs. It dynamically incorporates the internal representations of these chunks into new input. Its internal representations cluster in a manner that is consistent with ''human-recognizable'' melodic categories. TRACX2 is sensitive to both contour and proximity information in the musical chunks that it encounters in its input. It shows the ''end-of-word'' superiority effect demonstrated by Saffran et al. (1999) for short musical phrases. The overall findings suggest that the recursive autoassociative chunking mechanism, as implemented in TRACX2, may be a general segmentation and chunking mechanism, underlying not only word-and imagechunking, but also elementary melody processing.
</details>
<details>
<summary>摘要</summary>
是否存在类似或 même identical 的机制在计算模型中模拟speech segmentation、 serial image processing 和 music processing？我们通过探索TRACX2（French et al., 2011; French & Cottrell, 2014; Mareschal & French, 2017）模型，这是一个基于认知的、复合连接式 autoencoder 模型，可以成功模拟speech 和 serial-image processing，以及如何应用这种模型来处理基本旋律听觉。TRACX2 是一个三层 autoencoder 模型，可以识别 ''chunk'' 的短序列间隔，这些序列频繁出现在输入中。它在新输入中动态 incorporate 这些 chunk 的内部表示，并且这些表示会归一化到 ''human-recognizable'' 的旋律类别。TRACX2 感知输入中的音频 chunk 的形状和距离信息。它展现了 Saffran et al. (1999) 所示的 ''end-of-word'' 优势效应，即短乐段中的 ''end-of-word'' 会受到更多的听觉干扰。总的来说，这种 recursive autoassociative chunking 机制可能是一种通用的分 segmentation 和 chunking 机制，不仅存在 word-和 imagechunking，还存在基本旋律处理中。
</details></li>
</ul>
<hr>
<h2 id="How-Far-Have-We-Gone-in-Vulnerability-Detection-Using-Large-Language-Models"><a href="#How-Far-Have-We-Gone-in-Vulnerability-Detection-Using-Large-Language-Models" class="headerlink" title="How Far Have We Gone in Vulnerability Detection Using Large Language Models"></a>How Far Have We Gone in Vulnerability Detection Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12420">http://arxiv.org/abs/2311.12420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, Chao Zhang</li>
<li>for: 评估大自然语言模型（LLMs）在漏洞检测中的潜力</li>
<li>methods: 使用 VulBench  benchmark，融合 CTFT 挑战和实际应用程序数据，并对每个漏洞函数进行精确的分类</li>
<li>results: 发现一些 LLMs 可以超越传统的深度学习模型，提供更高的漏洞检测性能，这些结果为软件安全领域中 LLMS 的应用提供了新的可能性。<details>
<summary>Abstract</summary>
As software becomes increasingly complex and prone to vulnerabilities, automated vulnerability detection is critically important, yet challenging. Given the significant successes of Large Language Models (LLMs) in various tasks, there is growing anticipation of their efficacy in vulnerability detection. However, a quantitative understanding of their potential in vulnerability detection is still missing. To bridge this gap, we introduce a comprehensive vulnerability benchmark VulBench. This benchmark aggregates high-quality data from a wide range of CTF (Capture-the-Flag) challenges and real-world applications, with annotations for each vulnerable function detailing the vulnerability type and its root cause. Through our experiments encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models and static analyzers, we find that several LLMs outperform traditional deep learning approaches in vulnerability detection, revealing an untapped potential in LLMs. This work contributes to the understanding and utilization of LLMs for enhanced software security.
</details>
<details>
<summary>摘要</summary>
《软件在日益复杂和易受攻击的情况下，自动漏洞检测变得非常重要，却非常困难。在大语言模型（LLM）在不同任务中的显著成功的基础上，有关其在漏洞检测中的潜在效果的期待增加。然而，这一点的量化理解仍然缺失。为了填补这一空白，我们介绍了一个完整的漏洞benchmark VulBench。这个benchmark集成了多种CTF挑战和实际应用程序中的高质量数据，并对每个漏洞函数进行了每种漏洞类型和根本原因的注释。经我们的实验，包括16个LLM和6个现有的深度学习基于模型和静态分析器，我们发现了一些LLMs在漏洞检测中超过传统深度学习方法的表现，这 revelas了LLMs在软件安全方面的未经利用的潜力。这项工作对LLMs的理解和利用做出了贡献，以提高软件安全。》Note: Please keep in mind that the translation is done by a machine and may not be perfect. Also, the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="nach0-Multimodal-Natural-and-Chemical-Languages-Foundation-Model"><a href="#nach0-Multimodal-Natural-and-Chemical-Languages-Foundation-Model" class="headerlink" title="nach0: Multimodal Natural and Chemical Languages Foundation Model"></a>nach0: Multimodal Natural and Chemical Languages Foundation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12410">http://arxiv.org/abs/2311.12410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Micha Livne, Zulfat Miftahutdinov, Elena Tutubalina, Maksim Kuznetsov, Daniil Polykovskiy, Annika Brundyn, Aastha Jhunjhunwala, Anthony Costa, Alex Aliper, Alex Zhavoronkov</li>
<li>For: The paper introduces a new foundation model called nach0, which can solve various chemical and biological tasks such as biomedical question answering, named entity recognition, molecular generation, and others.* Methods: The model is a multi-domain and multi-task encoder-decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings. The authors used instruction tuning to fine-tune the model for the final set of tasks, and leveraged the NeMo framework for efficient parallel optimization of both base and large model versions.* Results: The model outperforms state-of-the-art baselines on single-domain and cross-domain tasks, and can generate high-quality outputs in molecular and textual formats, demonstrating its effectiveness in multi-domain setups.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder-decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）已经在不同领域产生了重要的科学进步，许多论文都表明了它们可以解决复杂问题的创新方案。我们的论文介绍了一个新的基础模型 nach0，可以解决各种化学和生物任务：生物医学问答、命名实体识别、分子生成、分子合成、特征预测等等。nach0 是一个多领域多任务的 encoder-decoder LLM，在不标注的科学文献、专利和分子字符串上进行预训练，以包含多种化学和语言知识。我们使用了“指导调教”方法，通过特定任务相关的指导来细化 nach0  для最终的任务。为了训练 nach0 效果，我们利用了 NeMo 框架，以实现效率的并行优化基模型和大模型版本。广泛的实验表明，我们的模型在单领域和交叉领域任务上表现出优于状态之前的基准点。此外，它可以生成高质量的分子和文本格式输出，展现了其在多领域设置中的效果。
</details></li>
</ul>
<hr>
<h2 id="Infinite-forecast-combinations-based-on-Dirichlet-process"><a href="#Infinite-forecast-combinations-based-on-Dirichlet-process" class="headerlink" title="Infinite forecast combinations based on Dirichlet process"></a>Infinite forecast combinations based on Dirichlet process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12379">http://arxiv.org/abs/2311.12379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinuo Ren, Feng Li, Yanfei Kang</li>
<li>for: 本研究旨在提出一种基于 Dirichlet 过程的深度学习 ensemble 预测模型，用于Integrating information from various sources and improving forecasting accuracy.</li>
<li>methods: 该模型使用三个基础分布作为超参数，通过抽样策略将 Dirichlet 过程转化为有限的一个。然后，通过收集所有检查点，建立一个深度学习子模型池，并在组合过程中使用重量调整和多样性策略。</li>
<li>results: 对于M4竞赛的每周数据集，该模型提出的ensemble模型显示了substantial improvement 的预测精度和稳定性，相比单个标准模型。此外，对于不同的模型数量进行敏感分析，结果表明该模型在不同的情况下具有普遍性和竞争力。<details>
<summary>Abstract</summary>
Forecast combination integrates information from various sources by consolidating multiple forecast results from the target time series. Instead of the need to select a single optimal forecasting model, this paper introduces a deep learning ensemble forecasting model based on the Dirichlet process. Initially, the learning rate is sampled with three basis distributions as hyperparameters to convert the infinite mixture into a finite one. All checkpoints are collected to establish a deep learning sub-model pool, and weight adjustment and diversity strategies are developed during the combination process. The main advantage of this method is its ability to generate the required base learners through a single training process, utilizing the decaying strategy to tackle the challenge posed by the stochastic nature of gradient descent in determining the optimal learning rate. To ensure the method's generalizability and competitiveness, this paper conducts an empirical analysis using the weekly dataset from the M4 competition and explores sensitivity to the number of models to be combined. The results demonstrate that the ensemble model proposed offers substantial improvements in prediction accuracy and stability compared to a single benchmark model.
</details>
<details>
<summary>摘要</summary>
预测组合将多种源信息 integrate into one forecast result by consolidating multiple forecast results from the target time series. Instead of selecting a single optimal forecasting model, this paper introduces a deep learning ensemble forecasting model based on the Dirichlet process. Initially, the learning rate is sampled with three basis distributions as hyperparameters to convert the infinite mixture into a finite one. All checkpoints are collected to establish a deep learning sub-model pool, and weight adjustment and diversity strategies are developed during the combination process. The main advantage of this method is its ability to generate the required base learners through a single training process, utilizing the decaying strategy to tackle the challenge posed by the stochastic nature of gradient descent in determining the optimal learning rate. To ensure the method's generalizability and competitiveness, this paper conducts an empirical analysis using the weekly dataset from the M4 competition and explores sensitivity to the number of models to be combined. The results demonstrate that the ensemble model proposed offers substantial improvements in prediction accuracy and stability compared to a single benchmark model.Here is the text with some additional information about the translation:This text is translated from English to Simplified Chinese. The translation is written in a formal and technical style, using appropriate vocabulary and grammar to convey the meaning of the original text. The text includes some specialized terms and concepts, such as "deep learning ensemble forecasting model" and "Dirichlet process," which are translated accurately and consistently to ensure clarity and precision. The translation also includes some cultural references and idioms that are appropriate for the target audience and context. Overall, the translation is accurate, clear, and fluent, and it effectively conveys the meaning and information of the original text to a Simplified Chinese-speaking audience.
</details></li>
</ul>
<hr>
<h2 id="Post-Training-Quantization-with-Low-precision-Minifloats-and-Integers-on-FPGAs"><a href="#Post-Training-Quantization-with-Low-precision-Minifloats-and-Integers-on-FPGAs" class="headerlink" title="Post-Training Quantization with Low-precision Minifloats and Integers on FPGAs"></a>Post-Training Quantization with Low-precision Minifloats and Integers on FPGAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12359">http://arxiv.org/abs/2311.12359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivam Aggarwal, Alessandro Pappalardo, Hans Jakob Damsgaard, Giuseppe Franco, Thomas B. Preußer, Michaela Blott, Tulika Mitra</li>
<li>for: 本研究旨在提出一种新的减小模型精度的技术，以便降低模型的内存占用量、延迟时间和能耗cost。</li>
<li>methods: 本研究使用了多种PTQ技术，包括weight equalization、bias correction、SmoothQuant、gradient-based learned rounding和GPTQ方法。</li>
<li>results: 实验结果表明，使用低精度minifloat可以与integer quantization schemes相比，在减少精度和维持准确性之间进行trade-off。此外，我们还评估了这些技术的硬件成本，发现integer quantization通常是Pareto优化的选择，因为它的硬件资源占用较少。<details>
<summary>Abstract</summary>
Post-Training Quantization (PTQ) is a powerful technique for model compression, reducing the precision of neural networks without additional training overhead. Recent works have investigated adopting 8-bit floating-point quantization (FP8) in the context of PTQ for model inference. However, the exploration of floating-point formats smaller than 8 bits and their comparison with integer quantization remains relatively limited. In this work, we present minifloats, which are reduced-precision floating-point formats capable of further reducing the memory footprint, latency, and energy cost of a model while approaching full-precision model accuracy. Our work presents a novel PTQ design-space exploration, comparing minifloat and integer quantization schemes across a range of 3 to 8 bits for both weights and activations. We examine the applicability of various PTQ techniques to minifloats, including weight equalization, bias correction, SmoothQuant, gradient-based learned rounding, and the GPTQ method. Our experiments validate the effectiveness of low-precision minifloats when compared to their integer counterparts across a spectrum of accuracy-precision trade-offs on a set of reference deep learning vision workloads. Finally, we evaluate our results against an FPGA-based hardware cost model, showing that integer quantization often remains the Pareto-optimal option, given its relatively smaller hardware resource footprint.
</details>
<details>
<summary>摘要</summary>
POST-TRAINING 量化（PTQ）是一种有力的模型压缩技术，可以降低神经网络的精度无需额外训练开销。Recent works have investigated adopting 8-bit floating-point quantization（FP8）in the context of PTQ for model inference. However, the exploration of floating-point formats smaller than 8 bits and their comparison with integer quantization remains relatively limited. In this work, we present minifloats, which are reduced-precision floating-point formats capable of further reducing the memory footprint, latency, and energy cost of a model while approaching full-precision model accuracy. Our work presents a novel PTQ design-space exploration, comparing minifloat and integer quantization schemes across a range of 3 to 8 bits for both weights and activations. We examine the applicability of various PTQ techniques to minifloats, including weight equalization, bias correction, SmoothQuant, gradient-based learned rounding, and the GPTQ method. Our experiments validate the effectiveness of low-precision minifloats when compared to their integer counterparts across a spectrum of accuracy-precision trade-offs on a set of reference deep learning vision workloads. Finally, we evaluate our results against an FPGA-based hardware cost model, showing that integer quantization often remains the Pareto-optimal option, given its relatively smaller hardware resource footprint.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is in the "Simplified Chinese" language setting, which is the most commonly used language setting for Chinese translation.Please note that the translation may not be perfect and may require some adjustments to accurately convey the intended meaning. Additionally, the translation may not capture all the nuances and idiomatic expressions present in the original text.
</details></li>
</ul>
<hr>
<h2 id="Stable-Diffusion-For-Aerial-Object-Detection"><a href="#Stable-Diffusion-For-Aerial-Object-Detection" class="headerlink" title="Stable Diffusion For Aerial Object Detection"></a>Stable Diffusion For Aerial Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12345">http://arxiv.org/abs/2311.12345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanan Jian, Fuxun Yu, Simranjit Singh, Dimitrios Stamoulis</li>
<li>for: 这篇论文的目的是解决航空图像中物体检测的挑战，特别是实现大规模数据收集和长尾分布的问题。</li>
<li>methods: 这篇论文使用了一个对应于航空图像的数据增强方法，即稳定扩散（SD）方法。</li>
<li>results: 这篇论文通过将这个方法与航空图像相结合，实现了更好的航空图像物体检测。<details>
<summary>Abstract</summary>
Aerial object detection is a challenging task, in which one major obstacle lies in the limitations of large-scale data collection and the long-tail distribution of certain classes. Synthetic data offers a promising solution, especially with recent advances in diffusion-based methods like stable diffusion (SD). However, the direct application of diffusion methods to aerial domains poses unique challenges: stable diffusion's optimization for rich ground-level semantics doesn't align with the sparse nature of aerial objects, and the extraction of post-synthesis object coordinates remains problematic. To address these challenges, we introduce a synthetic data augmentation framework tailored for aerial images. It encompasses sparse-to-dense region of interest (ROI) extraction to bridge the semantic gap, fine-tuning the diffusion model with low-rank adaptation (LORA) to circumvent exhaustive retraining, and finally, a Copy-Paste method to compose synthesized objects with backgrounds, providing a nuanced approach to aerial object detection through synthetic data.
</details>
<details>
<summary>摘要</summary>
天空物体检测是一项具有挑战性的任务，其中一个主要障碍是大规模数据收集的限制和某些类型的长尾分布。人工数据提供了一个有前途的解决方案，特别是在 latest advances in diffusion-based methods like stable diffusion (SD) 中。然而，直接将 diffusion methods 应用于天空领域存在独特的挑战：stable diffusion 的优化 для富有地面 semantics 与天空物体的稀疏性不符，并且提取后synthesis object坐标仍然是一个问题。为了解决这些挑战，我们介绍了一个适用于天空图像的人工数据增强框架。它包括将稀疏区域 Interest (ROI) 提取成为 bridge the semantic gap，使用 low-rank adaptation (LORA) 来绕过耗时 retraining，以及最后，使用 Copy-Paste 方法将生成的对象贴合背景，提供了一种细化的方法 для天空物体检测 through synthetic data。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Large-Language-Models-for-Personalized-and-Explainable-Recommendations"><a href="#A-Survey-on-Large-Language-Models-for-Personalized-and-Explainable-Recommendations" class="headerlink" title="A Survey on Large Language Models for Personalized and Explainable Recommendations"></a>A Survey on Large Language Models for Personalized and Explainable Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12338">http://arxiv.org/abs/2311.12338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Chen</li>
<li>for: 本研究旨在探讨大语言模型在推荐系统中的应用，以提高用户体验。</li>
<li>methods: 本研究使用了大语言模型进行文本处理和个性化推荐，以解决冷启动问题、不公平问题和偏见问题。</li>
<li>results: 研究发现，大语言模型可以提供高效的文本处理和个性化推荐，但存在一些挑战，如冷启动问题、不公平问题和偏见问题。<details>
<summary>Abstract</summary>
In recent years, Recommender Systems(RS) have witnessed a transformative shift with the advent of Large Language Models(LLMs) in the field of Natural Language Processing(NLP). These models such as OpenAI's GPT-3.5/4, Llama from Meta, have demonstrated unprecedented capabilities in understanding and generating human-like text. This has led to a paradigm shift in the realm of personalized and explainable recommendations, as LLMs offer a versatile toolset for processing vast amounts of textual data to enhance user experiences. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey aims to analyze how RS can benefit from LLM-based methodologies. Furthermore, we describe major challenges in Personalized Explanation Generating(PEG) tasks, which are cold-start problems, unfairness and bias problems in RS.
</details>
<details>
<summary>摘要</summary>
Recently, Recommender Systems (RS) have undergone a significant transformation with the emergence of Large Language Models (LLMs) in the field of Natural Language Processing (NLP). These models, such as OpenAI's GPT-3.5/4 and Llama from Meta, have shown unprecedented capabilities in understanding and generating human-like text. This has led to a paradigm shift in the realm of personalized and explainable recommendations, as LLMs provide a versatile toolset for processing vast amounts of textual data to enhance user experiences. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey aims to analyze how RS can benefit from LLM-based methodologies. Furthermore, we describe major challenges in Personalized Explanation Generating (PEG) tasks, such as cold-start problems, unfairness, and bias problems in RS.Here's the translation in Traditional Chinese:近年来，推荐系统（RS）已经受到大量语言模型（LLMs）的革命性变革，这些模型包括OpenAI的GPT-3.5/4和Meta的Llama。这些模型已经展示了人类化文本理解和生成的无 precedent能力，这导致了推荐系统的价值领域进行了重大的变革。这篇调查旨在分析RS如何从LLM-based方法ologies中受益，并描述PEG任务中的主要挑战，包括冷启始问题、不公平和偏见问题。
</details></li>
</ul>
<hr>
<h2 id="Do-Smaller-Language-Models-Answer-Contextualised-Questions-Through-Memorisation-Or-Generalisation"><a href="#Do-Smaller-Language-Models-Answer-Contextualised-Questions-Through-Memorisation-Or-Generalisation" class="headerlink" title="Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?"></a>Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12337">http://arxiv.org/abs/2311.12337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Hartill, Joshua Bensemann, Michael Witbrock, Patricia J. Riddle</li>
<li>for: 本研究旨在检验语言模型是否能通过推理来回答问题，而不是仅仅靠记忆。</li>
<li>methods: 本研究使用语义相似性来 identific evaluation samples，并使用两个语言模型进行多任务训练，其中一个模型添加了两个数据集来增强数字逻辑能力。</li>
<li>results: 研究发现，在一些评价数据集上，使用这种方法可以提高语言模型的表现，特别是在不可 memorization 的subset中。在 DROP 和 ROPES 评价数据集上，表现提高了9.0%和25.7% respectively，而其他评价数据集没有显著变化。<details>
<summary>Abstract</summary>
A distinction is often drawn between a model's ability to predict a label for an evaluation sample that is directly memorised from highly similar training samples versus an ability to predict the label via some method of generalisation. In the context of using Language Models for question-answering, discussion continues to occur as to the extent to which questions are answered through memorisation. We consider this issue for questions that would ideally be answered through reasoning over an associated context. We propose a method of identifying evaluation samples for which it is very unlikely our model would have memorised the answers. Our method is based on semantic similarity of input tokens and label tokens between training and evaluation samples. We show that our method offers advantages upon some prior approaches in that it is able to surface evaluation-train pairs that have overlap in either contiguous or discontiguous sequences of tokens. We use this method to identify unmemorisable subsets of our evaluation datasets. We train two Language Models in a multitask fashion whereby the second model differs from the first only in that it has two additional datasets added to the training regime that are designed to impart simple numerical reasoning strategies of a sort known to improve performance on some of our evaluation datasets but not on others. We then show that there is performance improvement between the two models on the unmemorisable subsets of the evaluation datasets that were expected to benefit from the additional training datasets. Specifically, performance on unmemorisable subsets of two of our evaluation datasets, DROP and ROPES significantly improves by 9.0%, and 25.7% respectively while other evaluation datasets have no significant change in performance.
</details>
<details>
<summary>摘要</summary>
很 oft，一个模型的能力被分为两个方面：直接从高度相似的训练样本中记忆标签，以及通过总结来预测标签。在使用语言模型进行问答时，人们仍在讨论 memorization 的问题。我们认为这个问题在需要通过推理来解答问题时 particualrly 重要。我们提出了一种方法来确定评估样本，这些样本很 unlikely 被我们的模型记忆。我们的方法基于输入和标签Token之间的语义相似性。我们展示了这种方法的优点，可以检测到跨样本序列中的 overlap 和不连续序列中的 overlap。我们使用这种方法来确定评估样本中的不可 memorizable 子集。我们在多任务模式下训练了两个语言模型，其中第二个模型与第一个模型只有两个额外的训练集添加到训练过程中，这些训练集是为了帮助模型学习一些简单的数字推理策略，这些策略已知能够提高一些我们的评估样本中的性能。我们Then 示出了这两个模型在不可 memorizable 子集上的性能改善。特别是，在 DROP 和 ROPES 两个评估样本上，性能提高了9.0%和25.7%。而其他评估样本没有显著变化。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Instagram-fake-users-using-supervised-machine-learning-algorithms"><a href="#Classification-of-Instagram-fake-users-using-supervised-machine-learning-algorithms" class="headerlink" title="Classification of Instagram fake users using supervised machine learning algorithms"></a>Classification of Instagram fake users using supervised machine learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12336">http://arxiv.org/abs/2311.12336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vertika Singh, Naman Tolasaria, Patel Meet Alpeshkumar, Shreyash Bartwal</li>
<li>for: 本研究旨在帮助公司保护自己免受社交媒体上的诈骗和人身侵犯。</li>
<li>methods: 该应用程序使用人工智能和机器学习技术来检测和neutralize社交媒体上的假 profiles和在线隐身。</li>
<li>results: 该应用程序可以帮助调查机构（特别是刑事分部）更好地掌握社交媒体的复杂领域，并与现有的调查程序集成。<details>
<summary>Abstract</summary>
In the contemporary era, online social networks have become integral to social life, revolutionizing the way individuals manage their social connections. While enhancing accessibility and immediacy, these networks have concurrently given rise to challenges, notably the proliferation of fraudulent profiles and online impersonation. This paper proposes an application designed to detect and neutralize such dishonest entities, with a focus on safeguarding companies from potential fraud. The user-centric design of the application ensures accessibility for investigative agencies, particularly the criminal branch, facilitating navigation of complex social media landscapes and integration with existing investigative procedures
</details>
<details>
<summary>摘要</summary>
现代时期，在线社交网络已成为社会生活的重要组成部分，推动了人们如何管理社交连接的方式。然而，这些网络同时也产生了一些挑战，主要是假 profiling 和在线人身伪装。这篇论文提出了一种应用程序，用于检测和消除这些不诚实的实体，特别是保护公司免受诈骗。该应用程序具有用户中心的设计，使得审查机关，特别是刑事部门，可以方便地浏览复杂的社交媒体景观，并与现有的审查过程集成。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Enhanced-Support-Vector-Machine-for-Large-Scale-Stellar-Classification-with-GPU-Acceleration"><a href="#Quantum-Enhanced-Support-Vector-Machine-for-Large-Scale-Stellar-Classification-with-GPU-Acceleration" class="headerlink" title="Quantum-Enhanced Support Vector Machine for Large-Scale Stellar Classification with GPU Acceleration"></a>Quantum-Enhanced Support Vector Machine for Large-Scale Stellar Classification with GPU Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12328">http://arxiv.org/abs/2311.12328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan-Cheng Chen, Xiaotian Xu, Henry Makhanov, Hui-Hsuan Chung, Chen-Yu Liu</li>
<li>for: 这个研究旨在开展一种创新的量子机器学习方法（Quantum-enhanced Support Vector Machine，QSVM），用于星系分类，充分利用量子计算和GPU加速。</li>
<li>methods: 这个方法利用量子原理和GPU加速，实现了高精度的星系分类，特别是在复杂的二元和多项分类情况下。</li>
<li>results: 相比传统方法（如K-Nearest Neighbors和Logistic Regression），QSVM方法在处理复杂的星系分类 задачі中表现出更高的准确率和更快的处理速度。<details>
<summary>Abstract</summary>
In this study, we introduce an innovative Quantum-enhanced Support Vector Machine (QSVM) approach for stellar classification, leveraging the power of quantum computing and GPU acceleration. Our QSVM algorithm significantly surpasses traditional methods such as K-Nearest Neighbors (KNN) and Logistic Regression (LR), particularly in handling complex binary and multi-class scenarios within the Harvard stellar classification system. The integration of quantum principles notably enhances classification accuracy, while GPU acceleration using the cuQuantum SDK ensures computational efficiency and scalability for large datasets in quantum simulators. This synergy not only accelerates the processing process but also improves the accuracy of classifying diverse stellar types, setting a new benchmark in astronomical data analysis. Our findings underscore the transformative potential of quantum machine learning in astronomical research, marking a significant leap forward in both precision and processing speed for stellar classification. This advancement has broader implications for astrophysical and related scientific fields
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们介绍了一种创新的量子机器学习（QSVM）方法，用于星系分类，利用量子计算和GPU加速。我们的QSVM算法在处理复杂的 binary 和多类enario 时表现出特别的优势，特别是在使用哈佛星系分类系统时。量子原理的integrating  notable enhances 分类精度，而使用 cuQuantum SDK 的 GPU 加速 guarantees 计算效率和可扩展性。这种synergy 不仅加速处理过程，还提高了分类多样星系类型的精度，创造了新的benchmark 在天体数据分析领域。我们的发现表明量子机器学习在天体研究中具有转变性，在精度和处理速度方面为星系分类带来了 significiant 进步。这种进步对astrophysical 和相关的科学领域都具有广泛的意义。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Multimodal-Large-Language-Models-for-Autonomous-Driving"><a href="#A-Survey-on-Multimodal-Large-Language-Models-for-Autonomous-Driving" class="headerlink" title="A Survey on Multimodal Large Language Models for Autonomous Driving"></a>A Survey on Multimodal Large Language Models for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12320">http://arxiv.org/abs/2311.12320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/irohxu/awesome-multimodal-llm-autonomous-driving">https://github.com/irohxu/awesome-multimodal-llm-autonomous-driving</a></li>
<li>paper_authors: Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, Tianren Gao, Erlong Li, Kun Tang, Zhipeng Cao, Tong Zhou, Ao Liu, Xinrui Yan, Shuqi Mei, Jianguo Cao, Ziran Wang, Chao Zheng</li>
<li>for: This paper aims to provide a comprehensive understanding of the key challenges, opportunities, and future endeavors in applying Large Language Models (LLMs) to autonomous driving systems.</li>
<li>methods: The paper uses a systematic investigation approach, including an overview of existing MLLM tools, datasets, and benchmarks, as well as a summary of the works presented in the 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD).</li>
<li>results: The paper identifies several important problems that need to be solved by both academia and industry in order to further promote the development of LLMs in autonomous driving systems, including the need for better multimodal models, more diverse and high-quality datasets, and more advanced benchmarks.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文的目的是为了为大语言模型（LLM）在自动驾驶系统中的应用提供全面的理解。</li>
<li>methods: 这篇论文使用了系统性的调查方法，包括现有的 MLLM 工具、数据集和比较指标的概述，以及 WACV 工作坊的作品概述。</li>
<li>results: 这篇论文 indentified several important problems需要由学术和产业共同解决，以进一步推动 MLLM 在自动驾驶系统中的发展，包括建立更好的多模态模型、更多和更高质量的数据集、以及更高级的指标。<details>
<summary>Abstract</summary>
With the emergence of Large Language Models (LLMs) and Vision Foundation Models (VFMs), multimodal AI systems benefiting from large models have the potential to equally perceive the real world, make decisions, and control tools as humans. In recent months, LLMs have shown widespread attention in autonomous driving and map systems. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors to apply in LLM driving systems. In this paper, we present a systematic investigation in this field. We first introduce the background of Multimodal Large Language Models (MLLMs), the multimodal models development using LLMs, and the history of autonomous driving. Then, we overview existing MLLM tools for driving, transportation, and map systems together with existing datasets and benchmarks. Moreover, we summarized the works in The 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD), which is the first workshop of its kind regarding LLMs in autonomous driving. To further promote the development of this field, we also discuss several important problems regarding using MLLMs in autonomous driving systems that need to be solved by both academia and industry.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）和视觉基础模型（VFM）的出现，使得多模态人工智能系统可以和人类一样理解现实世界，做出决策，控制工具。最近几个月，LLM在自动驾驶和地图系统中得到了广泛的关注。虽然它拥有巨大的潜力，但是还没有一个全面的理解关键挑战、机遇和未来应用在LLM驾驶系统中的研究。在这篇论文中，我们提供了一个系统性的调查。我们首先介绍了多模态大语言模型（MLLM）的背景、基于LLM的多模态模型开发、自动驾驶的历史。然后，我们综述了现有的MLLM工具、交通、地图系统以及相关的数据集和标准。此外，我们还概述了在WACV工作坊上关于大语言和视觉模型 для自动驾驶（LLVM-AD）的工作，这是自动驾驶领域中第一次关于LLM的工作坊。为了进一步推动这一领域的发展，我们还讨论了在使用MLLM时需要解决的一些重要问题，这些问题需要由学术和产业界共同努力解决。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Pathology-Image-Data-Deficiency-Generating-Images-from-Pathological-Transformation-Process"><a href="#Overcoming-Pathology-Image-Data-Deficiency-Generating-Images-from-Pathological-Transformation-Process" class="headerlink" title="Overcoming Pathology Image Data Deficiency: Generating Images from Pathological Transformation Process"></a>Overcoming Pathology Image Data Deficiency: Generating Images from Pathological Transformation Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12316">http://arxiv.org/abs/2311.12316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rowerliu/adbd">https://github.com/rowerliu/adbd</a></li>
<li>paper_authors: Zeyu Liu, Yufang He, Yu Zhao, Yunlu Feng, Guanglei Zhang</li>
<li>for:  overcome the limitations of histopathology and provide timely clinical analysis</li>
<li>methods:  adaptive depth-controlled bidirectional diffusion (ADBD) network for image data generation, hybrid attention strategy, and adaptive depth-controlled strategy</li>
<li>results:  unlimited cross-domain intermediate images with corresponding soft labels, effective for overcoming pathological image data deficiency and supportable for further pathology-related research.Here’s the full text in Simplified Chinese:</li>
<li>for:  Histopathology 的限制，提供快速的临床分析</li>
<li>methods:  adaptive depth-controlled bidirectional diffusion (ADBD) 网络，混合注意策略和 adaptive depth-controlled 策略</li>
<li>results:  cross-domain 中间图像的无限数量，与相应的软标签相对应，有效地解决了病理图像数据的不足问题，支持进一步的病理相关研究。<details>
<summary>Abstract</summary>
Histopathology serves as the gold standard for medical diagnosis but faces application limitations due to the shortage of medical resources. Leveraging deep learning, computer-aided diagnosis has the potential to alleviate the pathologist scarcity and provide timely clinical analysis. However, developing a reliable model generally necessitates substantial data for training, which is challenging in pathological field. In response, we propose an adaptive depth-controlled bidirectional diffusion (ADBD) network for image data generation. The domain migration approach can work with small trainset and overcome the diffusion overfitting by source information guidance. Specifically, we developed a hybrid attention strategy to blend global and local attention priorities, which guides the bidirectional diffusion and ensures the migration success. In addition, we developed the adaptive depth-controlled strategy to simulate physiological transformations, capable of yielding unlimited cross-domain intermediate images with corresponding soft labels. ADBD is effective for overcoming pathological image data deficiency and supportable for further pathology-related research.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The translation is written in the traditional Chinese characters, rather than the simplified ones used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="IEKM-A-Model-Incorporating-External-Keyword-Matrices"><a href="#IEKM-A-Model-Incorporating-External-Keyword-Matrices" class="headerlink" title="IEKM: A Model Incorporating External Keyword Matrices"></a>IEKM: A Model Incorporating External Keyword Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12310">http://arxiv.org/abs/2311.12310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Luo, Qin Li, Zhao Yan, Mengliang Rao, Yunbo Cao</li>
<li>for: 这个论文是为了解决客户服务平台系统中的核心文本含义相似性任务中的两个紧迫挑战的：一是不同领域的客户 adaptive（DDA），二是模型很难分辨 literal close yet semantically different 的句子对（hard negative samples）。</li>
<li>methods: 我们提出了一种 incorporating external keywords matrices 模型（IEKM）来解决这些挑战。该模型使用外部工具或词典构建外部矩阵，然后通过门控单元融合到 transformer 结构中的自注意层，以实现灵活的 corrections。</li>
<li>results: 我们在多个 datasets 上评估了该方法，结果显示我们的方法在所有 datasets 上都有提高表现。为了证明我们的方法可以有效解决所有挑战，我们进行了 flexible correction 实验，其结果是从 56.61 提高到 73.53 的 F1 值。<details>
<summary>Abstract</summary>
A customer service platform system with a core text semantic similarity (STS) task faces two urgent challenges: Firstly, one platform system needs to adapt to different domains of customers, i.e., different domains adaptation (DDA). Secondly, it is difficult for the model of the platform system to distinguish sentence pairs that are literally close but semantically different, i.e., hard negative samples. In this paper, we propose an incorporation external keywords matrices model (IEKM) to address these challenges. The model uses external tools or dictionaries to construct external matrices and fuses them to the self-attention layers of the Transformer structure through gating units, thus enabling flexible corrections to the model results. We evaluate the method on multiple datasets and the results show that our method has improved performance on all datasets. To demonstrate that our method can effectively solve all the above challenges, we conduct a flexible correction experiment, which results in an increase in the F1 value from 56.61 to 73.53. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
一个客户服务平台系统面临两个紧迫的挑战：首先，平台系统需要适应不同的客户领域（DDA）。其次，模型很难区分具有相似文本但具有不同含义的句子对，即困难的负样本（hard negative samples）。在这篇论文中，我们提出一种外部关键词矩阵模型（IEKM）来解决这些挑战。该模型通过使用外部工具或词典构建外部矩阵，然后将其与Transformer结构中的自我注意力层进行融合，以实现灵活的修正。我们对多个数据集进行评估，结果显示我们的方法在所有数据集上有改善的表现。为了证明我们的方法可以有效解决所有挑战，我们进行了灵活修正实验，其结果显示F1值从56.61提高到73.53。我们的代码将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Causality-is-all-you-need"><a href="#Causality-is-all-you-need" class="headerlink" title="Causality is all you need"></a>Causality is all you need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12307">http://arxiv.org/abs/2311.12307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ning Xu, Yifei Gao, Hongshuo Tian, Yongdong Zhang, An-An Liu</li>
<li>for: 提出一个整合 causal 框架，以便在数据中探索隐藏的 causal 关系</li>
<li>methods: 基于 intervenion 机制，建立一个 stacked 网络框架，包括多个平行的 deconfounding 块，并通过 sufficient cause 概念选择适合的 deconfounding 方法</li>
<li>results: 在 CV 和 NLP 两个 классических任务上，CGR 可以超越当前状态的方法，并且在 Visual Question Answer 和 Long Document Classification 任务上达到了优秀的结果，具有潜在的应用空间在建立 “causal” pre-training 大规模模型<details>
<summary>Abstract</summary>
In the fundamental statistics course, students are taught to remember the well-known saying: "Correlation is not Causation". Till now, statistics (i.e., correlation) have developed various successful frameworks, such as Transformer and Pre-training large-scale models, which have stacked multiple parallel self-attention blocks to imitate a wide range of tasks. However, in the causation community, how to build an integrated causal framework still remains an untouched domain despite its excellent intervention capabilities. In this paper, we propose the Causal Graph Routing (CGR) framework, an integrated causal scheme relying entirely on the intervention mechanisms to reveal the cause-effect forces hidden in data. Specifically, CGR is composed of a stack of causal layers. Each layer includes a set of parallel deconfounding blocks from different causal graphs. We combine these blocks via the concept of the proposed sufficient cause, which allows the model to dynamically select the suitable deconfounding methods in each layer. CGR is implemented as the stacked networks, integrating no confounder, back-door adjustment, front-door adjustment, and probability of sufficient cause. We evaluate this framework on two classical tasks of CV and NLP. Experiments show CGR can surpass the current state-of-the-art methods on both Visual Question Answer and Long Document Classification tasks. In particular, CGR has great potential in building the "causal" pre-training large-scale model that effectively generalizes to diverse tasks. It will improve the machines' comprehension of causal relationships within a broader semantic space.
</details>
<details>
<summary>摘要</summary>
在基础统计课程中，学生们被教育要记住一句著名的话：“相关性不等于 causation”。到目前为止，统计（即相关性）已经发展出了多种成功的框架，如 transformer 和预训练大规模模型，这些模型堆叠了多个并行的自注意力块，以模拟多种任务。然而，在 causation 社区中，如何建立一个完整的 causal 框架仍然是一个未解决的问题，尽管它的 intervención 能力非常出色。在这篇论文中，我们提出了 causal graph routing（CGR）框架，一种完整的 causal 方案，仅仅通过 intervención 机制来揭示数据中隐藏的 causal 力量。具体来说，CGR 由一 stack 的 causal layers 组成，每层包括一组并行的干扰整理方法。我们通过 proposed sufficient cause 概念，将这些层结合在一起，让模型在不同的 causal graphs 中动态选择适当的干扰方法。CGR 实现为堆叠网络，包括无干扰、后门补做、前门补做和潜在 suficient cause 概念。我们在 CV 和 NLP 两个 класси型任务上进行了实验，结果表明，CGR 可以超越当前状态的艺术方法。特别是，CGR 在建立 "causal" 预训练大规模模型方面有很大的潜力，这将有助于机器更好地理解 causal 关系，并在更广泛的 semantic space 中进行泛化。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Effective-Policies-for-Land-Use-Planning"><a href="#Discovering-Effective-Policies-for-Land-Use-Planning" class="headerlink" title="Discovering Effective Policies for Land-Use Planning"></a>Discovering Effective Policies for Land-Use Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12304">http://arxiv.org/abs/2311.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Risto Miikkulainen, Olivier Francon, Daniel Young, Elliot Meyerson, Babak Hodjat</li>
<li>for: 这项研究的目的是为了提供一种可以快速和效率地评估不同地区决策者可能选择的土地利用政策选择工具。</li>
<li>methods: 该研究使用了可用历史数据变化的土地使用和模拟吸收二氧化碳的过程来学习一个代理模型，以便有效地评估不同选项。然后，使用进化搜索过程来找到适合特定地点的有效土地利用策略。</li>
<li>results: 该系统可以生成自适应性的Pareto前面，其中评估碳气影响和变化量在不同地点之间进行了交换，从而为土地规划提供了一个有用的工具。<details>
<summary>Abstract</summary>
How areas of land are allocated for different uses, such as forests, urban, and agriculture, has a large effect on carbon balance, and therefore climate change. Based on available historical data on changes in land use and a simulation of carbon emissions/absorption, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset and the BLUE simulator. It generates Pareto fronts that trade off carbon impact and amount of change customized to different locations, thus providing a potentially useful tool for land-use planning.
</details>
<details>
<summary>摘要</summary>
预设语言：中文（简体）</SYS>不同用途的土地分配对碳平衡有着很大的影响，从而影响气候变化。通过历史数据变化和碳排放/吸收的模拟，可以学习一个代表模型，以便有效地评估不同选择。然后，一种EVOLUTIONARY搜索过程可以用来找到适合特定地点的有效土地使用政策。这种系统在Project Resilience平台上实现，并使用Land-Use Harmonization数据集和BLUE simulator进行评估。它生成了碳影响和变化量的平衡 Front，以便为土地使用规划提供有用工具。
</details></li>
</ul>
<hr>
<h2 id="Detecting-subtle-macroscopic-changes-in-a-finite-temperature-classical-scalar-field-with-machine-learning"><a href="#Detecting-subtle-macroscopic-changes-in-a-finite-temperature-classical-scalar-field-with-machine-learning" class="headerlink" title="Detecting subtle macroscopic changes in a finite temperature classical scalar field with machine learning"></a>Detecting subtle macroscopic changes in a finite temperature classical scalar field with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12303">http://arxiv.org/abs/2311.12303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiming Yang, Yutong Zheng, Jiahong Zhou, Huiyu Li, Jun Yin</li>
<li>for: 本研究的目的是探索用于检测多体系统中巨观变化的方法，以寻找可以超过物理方法和统计方法的敏感度。</li>
<li>methods: 本研究使用了Scalar field samples在不同温度下进行对比，并评估了物理方法、统计方法和AI方法的 differentiations 的效果。</li>
<li>results: 研究发现，使用AI方法可以具有更高的敏感度，并且能够检测到物理方法和统计方法所忽略的巨观变化。这得到的结果提供了一种可能性，即AI可以在多体系统中检测到物理方法所难以捕捉的巨观变化。<details>
<summary>Abstract</summary>
The ability to detect macroscopic changes is important for probing the behaviors of experimental many-body systems from the classical to the quantum realm. Although abrupt changes near phase boundaries can easily be detected, subtle macroscopic changes are much more difficult to detect as the changes can be obscured by noise. In this study, as a toy model for detecting subtle macroscopic changes in many-body systems, we try to differentiate scalar field samples at varying temperatures. We compare different methods for making such differentiations, from physics method, statistics method, to AI method. Our finding suggests that the AI method outperforms both the statistical method and the physics method in its sensitivity. Our result provides a proof-of-concept that AI can potentially detect macroscopic changes in many-body systems that elude physical measures.
</details>
<details>
<summary>摘要</summary>
importance of detecting macroscopic changes在实验几体系中从古典到量子领域的行为探测中，检测大规模变化的能力是关键。虽然近界面的快速变化容易被检测，但极其微妙的变化可能会受到噪声所隐藏。在这种研究中，我们作为几体系中细微变化探测的玩偶模型，尝试将扩散场样本在不同温度下进行差异化。我们比较了不同的方法进行这种差异化，包括物理方法、统计方法以及人工智能方法。我们的发现表明，人工智能方法在敏感性方面胜过物理方法和统计方法。我们的结果提供了一个证明，AI可能在物理测量所逃脱的几体系中探测到大规模变化。
</details></li>
</ul>
<hr>
<h2 id="Noise-in-Relation-Classification-Dataset-TACRED-Characterization-and-Reduction"><a href="#Noise-in-Relation-Classification-Dataset-TACRED-Characterization-and-Reduction" class="headerlink" title="Noise in Relation Classification Dataset TACRED: Characterization and Reduction"></a>Noise in Relation Classification Dataset TACRED: Characterization and Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12298">http://arxiv.org/abs/2311.12298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Parekh, Ashish Anand, Amit Awekar</li>
<li>for: 本研究的目标是两fold，一是探索基于模型的方法来 caracterize TACRED 数据集中的噪音的主要原因，二是自动标识可能噪音的实例。</li>
<li>methods: 我们分析了当前顶峰模型的预测和性能，以便在 TACRED 数据集中 indentify 噪音的根本原因。我们发现大多数噪音来自于被标记为无关的实例。为了实现第二个目标，我们探索了两种基于 nearest-neighbor 策略的方法来自动标识可能噪音的实例。</li>
<li>results: 我们的实验结果表明，使用 Intrinsic Strategy (IS) 可以提高 SOTA 模型在 TACRED-E 上的平均 F1 分数 by 4%，而使用 Extrinsic Strategy (ES) 可以提高 SOTA 模型在 TACRED-E 和 TACRED-RN 上的平均 F1 分数 by 3.8% 和 4.4%  соответpectively。此外，我们还扩展了 ES 以清理正例，这 führt 到了平均性能提高 5.8% 和 5.6% 在 TACRED-ENP 和 TACRED-RNP 上。<details>
<summary>Abstract</summary>
The overarching objective of this paper is two-fold. First, to explore model-based approaches to characterize the primary cause of the noise. in the RE dataset TACRED Second, to identify the potentially noisy instances. Towards the first objective, we analyze predictions and performance of state-of-the-art (SOTA) models to identify the root cause of noise in the dataset. Our analysis of TACRED shows that the majority of the noise in the dataset originates from the instances labeled as no-relation which are negative examples. For the second objective, we explore two nearest-neighbor-based strategies to automatically identify potentially noisy examples for elimination and reannotation. Our first strategy, referred to as Intrinsic Strategy (IS), is based on the assumption that positive examples are clean. Thus, we have used false-negative predictions to identify noisy negative examples. Whereas, our second approach, referred to as Extrinsic Strategy, is based on using a clean subset of the dataset to identify potentially noisy negative examples. Finally, we retrained the SOTA models on the eliminated and reannotated dataset. Our empirical results based on two SOTA models trained on TACRED-E following the IS show an average 4% F1-score improvement, whereas reannotation (TACRED-R) does not improve the original results. However, following ES, SOTA models show the average F1-score improvement of 3.8% and 4.4% when trained on respective eliminated (TACRED-EN) and reannotated (TACRED-RN) datasets respectively. We further extended the ES for cleaning positive examples as well, which resulted in an average performance improvement of 5.8% and 5.6% for the eliminated (TACRED-ENP) and reannotated (TACRED-RNP) datasets respectively.
</details>
<details>
<summary>摘要</summary>
本文的主要目标是两重。第一，通过模型方法来探讨TACRED数据集中噪音的主要原因。第二，并将潜在的噪音实例标记出来。对于第一个目标，我们分析了领先的模型（SOTA）的预测和性能，以确定TACRED数据集中噪音的根本原因。我们的分析表明，TACRED数据集中的噪音主要来自于标记为“无关”的实例，即负例。为了实现第二个目标，我们探讨了两种基于最近邻居的策略来自动标记潜在的噪音实例。我们的第一种策略（IS）假设正例是干净的，因此我们使用了false-negative预测来标识噪音的负例。而我们的第二种策略（ES）则是基于使用干净的子集来标识潜在的噪音负例。最后，我们在TACRED-E中重新训练了SOTA模型，并基于IS进行了预测。我们的实验结果表明，TACRED-E中使用IS后，SOTA模型的平均F1分数提高了4%。然而，TACRED-R不改善原来的结果。但是，基于ES进行了潜在噪音实例的标记和重新标注，SOTA模型在TACRED-EN和TACRED-RN中分别提高了3.8%和4.4%的平均F1分数。此外，我们还将ES扩展到清理正例上，并在TACRED-ENP和TACRED-RNP中分别提高了5.8%和5.6%的平均F1分数。
</details></li>
</ul>
<hr>
<h2 id="ATLANTIC-Structure-Aware-Retrieval-Augmented-Language-Model-for-Interdisciplinary-Science"><a href="#ATLANTIC-Structure-Aware-Retrieval-Augmented-Language-Model-for-Interdisciplinary-Science" class="headerlink" title="ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science"></a>ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12289">http://arxiv.org/abs/2311.12289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Munikoti, Anurag Acharya, Sridevi Wagle, Sameera Horawalavithana</li>
<li>for: 提高语言模型在科学任务中的表现，特别是 faithfulness 和 relevance 的问题。</li>
<li>methods: 提出了一种基于文档结构的搜索增强技术，通过文档关系图建模文档之间的结构关系，并在语言模型预训练过程中使用这些结构关系。</li>
<li>results: 实验结果表明，基于结构的搜索增强可以提取更加相关、准确和 faithful 的段落，同时与总准确率相对保持接近。<details>
<summary>Abstract</summary>
Large language models record impressive performance on many natural language processing tasks. However, their knowledge capacity is limited to the pretraining corpus. Retrieval augmentation offers an effective solution by retrieving context from external knowledge sources to complement the language model. However, existing retrieval augmentation techniques ignore the structural relationships between these documents. Furthermore, retrieval models are not explored much in scientific tasks, especially in regard to the faithfulness of retrieved documents. In this paper, we propose a novel structure-aware retrieval augmented language model that accommodates document structure during retrieval augmentation. We create a heterogeneous document graph capturing multiple types of relationships (e.g., citation, co-authorship, etc.) that connect documents from more than 15 scientific disciplines (e.g., Physics, Medicine, Chemistry, etc.). We train a graph neural network on the curated document graph to act as a structural encoder for the corresponding passages retrieved during the model pretraining. Particularly, along with text embeddings of the retrieved passages, we obtain structural embeddings of the documents (passages) and fuse them together before feeding them to the language model. We evaluate our model extensively on various scientific benchmarks that include science question-answering and scientific document classification tasks. Experimental results demonstrate that structure-aware retrieval improves retrieving more coherent, faithful and contextually relevant passages, while showing a comparable performance in the overall accuracy.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型在许多任务上表现出色，但它们的知识容量受到预训练文本集的限制。检索增强技术可以补充语言模型，但现有技术忽略了文档之间的结构关系。此外，在科学任务中，检索模型尚未得到充分的研究，特别是文档的准确性。在这篇论文中，我们提出了一种新的结构意识检索增强语言模型，可以在检索过程中考虑文档的结构。我们创建了一个多类关系（如引用、共作者等）连接来自多个科学领域（如物理、医学、化学等）的文档 graphs。我们在这些文档 graphs 上训练了一个图 neural network，作为文档（段落）的结构编码器。特别是在 retrieve 过程中获取的文档（段落）的文本嵌入，我们还获取了它们的结构嵌入，并将它们合并在一起，然后将其传递给语言模型。我们在科学问答和科学文档分类任务中进行了广泛的实验评估。实验结果表明，结构意识检索可以更好地检索具有准确性、完整性和文本关联性的段落，同时保持相对的精度。
</details></li>
</ul>
<hr>
<h2 id="Adapting-LLMs-for-Efficient-Personalized-Information-Retrieval-Methods-and-Implications"><a href="#Adapting-LLMs-for-Efficient-Personalized-Information-Retrieval-Methods-and-Implications" class="headerlink" title="Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications"></a>Adapting LLMs for Efficient, Personalized Information Retrieval: Methods and Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12287">http://arxiv.org/abs/2311.12287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samira Ghodratnama, Mehrdad Zakershahrak</li>
<li>for: This paper focuses on the application of Large Language Models (LLMs) in information retrieval (IR) systems, with the goal of enriching the IR experience and improving result accuracy.</li>
<li>methods: The paper explores various methodologies to optimize the retrieval process, select optimal models, and effectively scale and orchestrate LLMs, while addressing challenges such as model hallucination and ensuring cost-efficiency.</li>
<li>results: The paper unveils innovative strategies for integrating LLMs with IR systems, and highlights the need for a balanced approach aligned with user-centric principles, taking into account considerations such as user privacy, data optimization, and system clarity and interpretability.<details>
<summary>Abstract</summary>
The advent of Large Language Models (LLMs) heralds a pivotal shift in online user interactions with information. Traditional Information Retrieval (IR) systems primarily relied on query-document matching, whereas LLMs excel in comprehending and generating human-like text, thereby enriching the IR experience significantly. While LLMs are often associated with chatbot functionalities, this paper extends the discussion to their explicit application in information retrieval. We explore methodologies to optimize the retrieval process, select optimal models, and effectively scale and orchestrate LLMs, aiming for cost-efficiency and enhanced result accuracy. A notable challenge, model hallucination-where the model yields inaccurate or misinterpreted data-is addressed alongside other model-specific hurdles. Our discourse extends to crucial considerations including user privacy, data optimization, and the necessity for system clarity and interpretability. Through a comprehensive examination, we unveil not only innovative strategies for integrating Language Models (LLMs) with Information Retrieval (IR) systems, but also the consequential considerations that underline the need for a balanced approach aligned with user-centric principles.
</details>
<details>
<summary>摘要</summary>
随着大型自然语言模型（LLMs）的出现，在线用户与信息交互方式发生了重要的转变。传统的信息检索（IR）系统主要依赖于查询文档匹配，而 LLMs 则在理解和生成人类语言方面表现出色，从而对 IR 体验进行了显著改善。虽然 LLMs frequently 被关联到 chatbot 功能，但这篇论文推广了它们在 IR 领域的直接应用。我们探讨了优化检索过程、选择优质模型、并有效地执行和协调 LLMs，以实现成本效益和提高结果准确性。一个显著的挑战是模型幻觉-模型生成的数据不准确或被误 интерпретирова为。我们的讨论还涵盖了关键考虑因素，包括用户隐私、数据优化和系统清晰性和可解释性。通过全面的检视，我们不仅描述了将 Language Models（LLMs）与 Information Retrieval（IR）系统集成的创新策略，还探讨了这些策略的下游考虑。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Forecast-Reconciliation-with-Kullback-Leibler-Divergence-Regularization"><a href="#Probabilistic-Forecast-Reconciliation-with-Kullback-Leibler-Divergence-Regularization" class="headerlink" title="Probabilistic Forecast Reconciliation with Kullback-Leibler Divergence Regularization"></a>Probabilistic Forecast Reconciliation with Kullback-Leibler Divergence Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12279">http://arxiv.org/abs/2311.12279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanyu0316/Probabilistic-Forecast-Reconciliation-with-DL">https://github.com/guanyu0316/Probabilistic-Forecast-Reconciliation-with-DL</a></li>
<li>paper_authors: Guanyu Zhang, Feng Li, Yanfei Kang</li>
<li>for: 这个研究旨在提出一种新的机器学习方法来实现可能性预测重复调和。</li>
<li>methods: 这个方法使用了深度学习框架，并将重复调和步骤与预测步骤融合在一起，使得重复调和步骤更加软和 гиб可。</li>
<li>results: 这个方法在三个层次时间系列数据上进行评估，与其他可能性预测重复调和方法相比，展现了更好的性能。<details>
<summary>Abstract</summary>
As the popularity of hierarchical point forecast reconciliation methods increases, there is a growing interest in probabilistic forecast reconciliation. Many studies have utilized machine learning or deep learning techniques to implement probabilistic forecasting reconciliation and have made notable progress. However, these methods treat the reconciliation step as a fixed and hard post-processing step, leading to a trade-off between accuracy and coherency. In this paper, we propose a new approach for probabilistic forecast reconciliation. Unlike existing approaches, our proposed approach fuses the prediction step and reconciliation step into a deep learning framework, making the reconciliation step more flexible and soft by introducing the Kullback-Leibler divergence regularization term into the loss function. The approach is evaluated using three hierarchical time series datasets, which shows the advantages of our approach over other probabilistic forecast reconciliation methods.
</details>
<details>
<summary>摘要</summary>
As the popularity of hierarchical point forecast reconciliation methods increases, there is a growing interest in probabilistic forecast reconciliation. Many studies have utilized machine learning or deep learning techniques to implement probabilistic forecasting reconciliation and have made notable progress. However, these methods treat the reconciliation step as a fixed and hard post-processing step, leading to a trade-off between accuracy and coherency. In this paper, we propose a new approach for probabilistic forecast reconciliation. Unlike existing approaches, our proposed approach fuses the prediction step and reconciliation step into a deep learning framework, making the reconciliation step more flexible and soft by introducing the Kullback-Leibler divergence regularization term into the loss function. The approach is evaluated using three hierarchical time series datasets, which shows the advantages of our approach over other probabilistic forecast reconciliation methods.Here's the translation breakdown:* "As the popularity of hierarchical point forecast reconciliation methods increases" becomes "随着层次点预测重叠方法的流行度提高"* "there is a growing interest in probabilistic forecast reconciliation" becomes "对 probabilistic forecasting 重叠方法产生了越来越大的兴趣"* "Many studies have utilized machine learning or deep learning techniques to implement probabilistic forecasting reconciliation" becomes "许多研究使用机器学习或深度学习技术实现 probabilistic forecasting 重叠方法"* "and have made notable progress" becomes "并取得了显著的进步"* "However, these methods treat the reconciliation step as a fixed and hard post-processing step" becomes "然而，这些方法将重叠步骤视为一个固定和硬的后处理步骤"* "leading to a trade-off between accuracy and coherency" becomes "导致精度和一致性之间的负担"* "In this paper, we propose a new approach for probabilistic forecast reconciliation" becomes "在这篇论文中，我们提出了一种新的 probabilistic forecasting 重叠方法"* "Unlike existing approaches, our proposed approach fuses the prediction step and reconciliation step into a deep learning framework" becomes "与现有方法不同，我们的提议方法将预测步骤和重叠步骤集成到深度学习框架中"* "making the reconciliation step more flexible and soft" becomes "使重叠步骤更加灵活和软"* "by introducing the Kullback-Leibler divergence regularization term into the loss function" becomes "通过在损失函数中引入库拉-莱布勒分子违异项来实现"* "The approach is evaluated using three hierarchical time series datasets" becomes "该方法在三个层次时间序列数据集上进行评估"* "which shows the advantages of our approach over other probabilistic forecast reconciliation methods" becomes "显示了我们的方法与其他 probabilistic forecasting 重叠方法相比有优势"
</details></li>
</ul>
<hr>
<h2 id="Learning-Causal-Representations-from-General-Environments-Identifiability-and-Intrinsic-Ambiguity"><a href="#Learning-Causal-Representations-from-General-Environments-Identifiability-and-Intrinsic-Ambiguity" class="headerlink" title="Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity"></a>Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12267">http://arxiv.org/abs/2311.12267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jikai Jin, Vasilis Syrgkanis</li>
<li>for: 本文研究了 causal representation learning，即从低级数据中恢复高级 latent variables 和其 causal 关系。</li>
<li>methods: 本文使用 linear causal models 和 general non-parametric causal models，并提供了一种名为 LiNGCReL 的算法，可以在不假设硬 intervenction 的情况下提供 identifiability 保证。</li>
<li>results: 本文提供了一种新的 identifiability 推论，可以在 general environments 下对 causal graph 进行全面回归，但 latent variables 只能被确定到 effect-domination ambiguity（EDA）。同时， LiNGCReL 算法可以在 numerical experiments 中证明其效果。<details>
<summary>Abstract</summary>
This paper studies causal representation learning, the task of recovering high-level latent variables and their causal relationships from low-level data that we observe, assuming access to observations generated from multiple environments. While existing works are able to prove full identifiability of the underlying data generating process, they typically assume access to single-node, hard interventions which is rather unrealistic in practice. The main contribution of this paper is characterize a notion of identifiability which is provably the best one can achieve when hard interventions are not available. First, for linear causal models, we provide identifiability guarantee for data observed from general environments without assuming any similarities between them. While the causal graph is shown to be fully recovered, the latent variables are only identified up to an effect-domination ambiguity (EDA). We then propose an algorithm, LiNGCReL which is guaranteed to recover the ground-truth model up to EDA, and we demonstrate its effectiveness via numerical experiments. Moving on to general non-parametric causal models, we prove the same idenfifiability guarantee assuming access to groups of soft interventions. Finally, we provide counterparts of our identifiability results, indicating that EDA is basically inevitable in our setting.
</details>
<details>
<summary>摘要</summary>
For linear causal models, we provide identifiability guarantees for data observed from general environments without assuming any similarities between them. Although the causal graph is fully recovered, the latent variables are only identified up to an effect-domination ambiguity (EDA). We then propose an algorithm, LiNGCReL, which is guaranteed to recover the ground-truth model up to EDA, and we demonstrate its effectiveness through numerical experiments.For general non-parametric causal models, we prove the same identifiability guarantee assuming access to groups of soft interventions. Finally, we provide counterparts of our identifiability results, indicating that EDA is inevitable in our setting.Translated into Simplified Chinese:这篇论文研究了 causal representation learning，即从低级数据中恢复高级潜在变量和其 causal 关系。现有的工作可以证明潜在数据生成过程的全部可 identificability，但它们假设有访问单个硬 intervención，这是在实际中不太实际。本文的主要贡献是 characterize一种可 identificability 的定义，这是在硬 intervención不可用时最好的。对于线性 causal 模型，我们提供了数据从通用环境中观察的可 identificability  garantuee，无需假设任何相似性。虽然 causal 图完全回归，但潜在变量只能被识别到 effect-domination ambiguity（EDA）。我们Then propose了一个算法，LiNGCReL，它可以 garantuee 回归真实模型，并在数值实验中证明其效果。对于普通非参数 causal 模型，我们证明了同样的可 identificability  garantuee，假设有访问一组软 intervención。最后，我们提供了对我们可 identificability 结果的对应，表明 EDA 是我们设定中不可避免的。
</details></li>
</ul>
<hr>
<h2 id="Resilient-Control-of-Networked-Microgrids-using-Vertical-Federated-Reinforcement-Learning-Designs-and-Real-Time-Test-Bed-Validations"><a href="#Resilient-Control-of-Networked-Microgrids-using-Vertical-Federated-Reinforcement-Learning-Designs-and-Real-Time-Test-Bed-Validations" class="headerlink" title="Resilient Control of Networked Microgrids using Vertical Federated Reinforcement Learning: Designs and Real-Time Test-Bed Validations"></a>Resilient Control of Networked Microgrids using Vertical Federated Reinforcement Learning: Designs and Real-Time Test-Bed Validations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12264">http://arxiv.org/abs/2311.12264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayak Mukherjee, Ramij R. Hossain, Sheik M. Mohiuddin, Yuan Liu, Wei Du, Veronica Adetola, Rohit A. Jinsiwale, Qiuhua Huang, Tianzhixi Yin, Ankit Singhal</li>
<li>for: 这篇论文的目的是提高网络化微型发电团队的系统级别坚韧性，面对增加的增verter-based资源（IBR）人口。</li>
<li>methods: 该论文提出了一种受到敌意cyber事件的攻击的控制设计，并提出了一种新的联邦强化学习（Fed-RL）方法来解决模型复杂性和不确定的IBR设备动态行为问题，以及数据分享在多方所有的网络化发电团队中的隐私问题。</li>
<li>results: 该论文通过将在 simulations 中学习的控制策略转移到硬件实验室中，成功bridging the sim-to-real gap，并且实验结果表明，由RL控制器 Mitigate the injected attacks。<details>
<summary>Abstract</summary>
Improving system-level resiliency of networked microgrids is an important aspect with increased population of inverter-based resources (IBRs). This paper (1) presents resilient control design in presence of adversarial cyber-events, and proposes a novel federated reinforcement learning (Fed-RL) approach to tackle (a) model complexities, unknown dynamical behaviors of IBR devices, (b) privacy issues regarding data sharing in multi-party-owned networked grids, and (2) transfers learned controls from simulation to hardware-in-the-loop test-bed, thereby bridging the gap between simulation and real world. With these multi-prong objectives, first, we formulate a reinforcement learning (RL) training setup generating episodic trajectories with adversaries (attack signal) injected at the primary controllers of the grid forming (GFM) inverters where RL agents (or controllers) are being trained to mitigate the injected attacks. For networked microgrids, the horizontal Fed-RL method involving distinct independent environments is not appropriate, leading us to develop vertical variant Federated Soft Actor-Critic (FedSAC) algorithm to grasp the interconnected dynamics of networked microgrid. Next, utilizing OpenAI Gym interface, we built a custom simulation set-up in GridLAB-D/HELICS co-simulation platform, named Resilient RL Co-simulation (ResRLCoSIM), to train the RL agents with IEEE 123-bus benchmark test systems comprising 3 interconnected microgrids. Finally, the learned policies in simulation world are transferred to the real-time hardware-in-the-loop test-bed set-up developed using high-fidelity Hypersim platform. Experiments show that the simulator-trained RL controllers produce convincing results with the real-time test-bed set-up, validating the minimization of sim-to-real gap.
</details>
<details>
<summary>摘要</summary>
提高网络化微型发电团队的系统级别鲁棒性是一项非常重要的任务，随着升高的人口倾向于投入器件基础资源（IBR）。本文（1）提出了在敌意网络事件存在下的鲁棒控制设计，并提出了一种新的联邦强化学习（Fed-RL）方法来解决模型复杂性、不知道IBR设备的动态行为和多方所有网络发电团队数据共享隐私问题。此外，文章还考虑了将学习控制从模拟世界转移到硬件实验室中的问题。为了实现这些多重目标，我们首先设计了一种基于学习控制的训练布局，生成了随机攻击信号插入Grid Forming Matrix（GFM）逻辑控制器的循环训练过程，以让RL代理（或控制器）在抗击攻击方面进行抵抗。由于网络微型发电团队的横向联邦方法不适用，我们开发了垂直变种的联邦软 actor-critic（FedSAC）算法，以捕捉网络微型发电团队的水平联接动态。然后，我们使用OpenAI Gym接口，在GridLAB-D/HELICS协同 simulate平台上建立了一个自定义的模拟设置，名为Resilient RL Co-simulation（ResRLCoSIM），以在IEEE 123-bus标准测试系统上训练RL代理。最后，在实时硬件实验室中使用高精度的Hypersim平台，将模拟世界中学习到的策略转移到实时硬件实验室中。实验结果表明，模拟世界中训练的RL控制器在实时硬件实验室中的表现非常有力，验证了模拟和实际世界之间的减少。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/21/cs.AI_2023_11_21/" data-id="clpxp6bxn007pee88a5075y8f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/21/cs.CV_2023_11_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-11-21
        
      </div>
    </a>
  
  
    <a href="/2023/11/21/cs.CL_2023_11_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-11-21</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
