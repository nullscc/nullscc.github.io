
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-11-21 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12735 repo_url: https:&#x2F;&#x2F;github.com&#x2F;aunabil4602&#x2F;bnlp-wo">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-11-21">
<meta property="og:url" content="https://nullscc.github.io/2023/11/21/cs.CL_2023_11_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12735 repo_url: https:&#x2F;&#x2F;github.com&#x2F;aunabil4602&#x2F;bnlp-wo">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-21T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-22T13:44:34.775Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_11_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/21/cs.CL_2023_11_21/" class="article-date">
  <time datetime="2023-11-21T11:00:00.000Z" itemprop="datePublished">2023-11-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-11-21
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="LowResource-at-BLP-2023-Task-2-Leveraging-BanglaBert-for-Low-Resource-Sentiment-Analysis-of-Bangla-Language"><a href="#LowResource-at-BLP-2023-Task-2-Leveraging-BanglaBert-for-Low-Resource-Sentiment-Analysis-of-Bangla-Language" class="headerlink" title="LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language"></a>LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12735">http://arxiv.org/abs/2311.12735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aunabil4602/bnlp-workshop-task2-2023">https://github.com/aunabil4602/bnlp-workshop-task2-2023</a></li>
<li>paper_authors: Aunabil Chakma, Masum Hasan</li>
<li>for: 本研究的目的是为BLP-2023任务2中进行情感分析，使用不同社交媒体平台上的公开帖子和评论。</li>
<li>methods: 本研究使用了BanglaBert模型，包括微调、随机tokenDrop和多个外部数据集，以及task-adaptive具体和重新措词。</li>
<li>results: 本研究的最终模型是三个最好的BanglaBert变种的弹性结合。在30支队中的测试集上，我们的系统获得了3rd的成绩，分别为0.718。此外，我们还讨论了不太好的系统，包括使用BanglaT5进行任务适应的具体和重新措词。<details>
<summary>Abstract</summary>
This paper describes the system of the LowResource Team for Task 2 of BLP-2023, which involves conducting sentiment analysis on a dataset composed of public posts and comments from diverse social media platforms. Our primary aim is to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus, using various strategies including fine-tuning, dropping random tokens, and using several external datasets. Our final model is an ensemble of the three best BanglaBert variations. Our system has achieved overall 3rd in the Test Set among 30 participating teams with a score of 0.718. Additionally, we discuss the promising systems that didn't perform well namely task-adaptive pertaining and paraphrasing using BanglaT5. Training codes and external datasets which are used for our system are publicly available at https://github.com/Aunabil4602/bnlp-workshop-task2-2023
</details>
<details>
<summary>摘要</summary>
这篇论文描述了我们在BLP-2023任务2中的系统，即对不同社交媒体平台上的公共帖子和评论进行情感分析。我们的主要目标是使用BanglaBert模型，包括微调、随机删除token和使用外部数据集，以实现不同的推理策略。我们的最终模型是 ensemble of 三个最佳 BanglaBert 变体。我们的系统在30个参与队伍中的测试集中得到了总第三名，得分为0.718。此外，我们还讨论了不太好的系统，namely task-adaptive pertaining和paraphrasing使用BanglaT5。我们使用的训练代码和外部数据集在https://github.com/Aunabil4602/bnlp-workshop-task2-2023上公开可用。
</details></li>
</ul>
<hr>
<h2 id="Soft-Random-Sampling-A-Theoretical-and-Empirical-Analysis"><a href="#Soft-Random-Sampling-A-Theoretical-and-Empirical-Analysis" class="headerlink" title="Soft Random Sampling: A Theoretical and Empirical Analysis"></a>Soft Random Sampling: A Theoretical and Empirical Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12727">http://arxiv.org/abs/2311.12727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaodong Cui, Ashish Mittal, Songtao Lu, Wei Zhang, George Saon, Brian Kingsbury</li>
<li>for: 大规模深度神经网络的高效训练</li>
<li>methods: 随机抽样法（SRS），包括每个纪元内随机选择数据集的子集</li>
<li>results:	+  theoretically analyzed sampling dynamics, including data coverage and occupancy	+  demonstrated convergence with non-convex objective functions and given convergence rate	+  shown to have better accuracy-efficiency trade-off compared to existing coreset-based data selection methods	+  significantly reduced training time with competitive performance on real-world industrial scale data sets, with almost no additional computing cost.<details>
<summary>Abstract</summary>
Soft random sampling (SRS) is a simple yet effective approach for efficient training of large-scale deep neural networks when dealing with massive data. SRS selects a subset uniformly at random with replacement from the full data set in each epoch. In this paper, we conduct a theoretical and empirical analysis of SRS. First, we analyze its sampling dynamics including data coverage and occupancy. Next, we investigate its convergence with non-convex objective functions and give the convergence rate. Finally, we provide its generalization performance. We empirically evaluate SRS for image recognition on CIFAR10 and automatic speech recognition on Librispeech and an in-house payload dataset to demonstrate its effectiveness. Compared to existing coreset-based data selection methods, SRS offers a better accuracy-efficiency trade-off. Especially on real-world industrial scale data sets, it is shown to be a powerful training strategy with significant speedup and competitive performance with almost no additional computing cost.
</details>
<details>
<summary>摘要</summary>
Random sampling (SRS) is a simple yet effective approach for efficient training of large-scale deep neural networks when dealing with massive data. SRS selects a subset uniformly at random with replacement from the full data set in each epoch. In this paper, we conduct a theoretical and empirical analysis of SRS. First, we analyze its sampling dynamics including data coverage and occupancy. Next, we investigate its convergence with non-convex objective functions and give the convergence rate. Finally, we provide its generalization performance. We empirically evaluate SRS for image recognition on CIFAR10 and automatic speech recognition on Librispeech and an in-house payload dataset to demonstrate its effectiveness. Compared to existing coreset-based data selection methods, SRS offers a better accuracy-efficiency trade-off. Especially on real-world industrial scale data sets, it is shown to be a powerful training strategy with significant speedup and competitive performance with almost no additional computing cost.Here's the translation in Traditional Chinese:Random sampling (SRS) is a simple yet effective approach for efficient training of large-scale deep neural networks when dealing with massive data. SRS selects a subset uniformly at random with replacement from the full data set in each epoch. In this paper, we conduct a theoretical and empirical analysis of SRS. First, we analyze its sampling dynamics including data coverage and occupancy. Next, we investigate its convergence with non-convex objective functions and give the convergence rate. Finally, we provide its generalization performance. We empirically evaluate SRS for image recognition on CIFAR10 and automatic speech recognition on Librispeech and an in-house payload dataset to demonstrate its effectiveness. Compared to existing coreset-based data selection methods, SRS offers a better accuracy-efficiency trade-off. Especially on real-world industrial scale data sets, it is shown to be a powerful training strategy with significant speedup and competitive performance with almost no additional computing cost.
</details></li>
</ul>
<hr>
<h2 id="Fair-Text-Classification-with-Wasserstein-Independence"><a href="#Fair-Text-Classification-with-Wasserstein-Independence" class="headerlink" title="Fair Text Classification with Wasserstein Independence"></a>Fair Text Classification with Wasserstein Independence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12689">http://arxiv.org/abs/2311.12689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/letenothibaud/wasserstein_fair_classification">https://github.com/letenothibaud/wasserstein_fair_classification</a></li>
<li>paper_authors: Thibaud Leteno, Antoine Gourru, Charlotte Laclau, Rémi Emonet, Christophe Gravier</li>
<li>for: This paper is written for mitigating biases in neural text classification and achieving fair treatment between sensitive groups.</li>
<li>methods: The paper uses adversarial training to induce Wasserstein independence between representations learned to predict the target label and the ones learned to predict sensitive attributes.</li>
<li>results: The approach provides two significant advantages: it does not require annotations of sensitive attributes in both testing and training data, and it exhibits a comparable or better fairness-accuracy trade-off compared to existing methods.<details>
<summary>Abstract</summary>
Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g. women vs. men) remains an open challenge. This paper presents a novel method for mitigating biases in neural text classification, agnostic to the model architecture. Considering the difficulty to distinguish fair from unfair information in a text encoder, we take inspiration from adversarial training to induce Wasserstein independence between representations learned to predict our target label and the ones learned to predict some sensitive attribute. Our approach provides two significant advantages. Firstly, it does not require annotations of sensitive attributes in both testing and training data. This is more suitable for real-life scenarios compared to existing methods that require annotations of sensitive attributes at train time. Second, our approach exhibits a comparable or better fairness-accuracy trade-off compared to existing methods.
</details>
<details>
<summary>摘要</summary>
group fairness是文本分类中的一个中心研究话题，即在敏感群体（例如男女）中实现公正待遇仍然是一个开放的挑战。本文提出了一种新的方法来减少文本分类中的偏见，不受模型体系限制。由于难以在文本编码器中分辨公正与不公正的信息，我们从对抗训练中习惯到抗 Wasserstein 独立性 между预测目标标签和敏感特征的表示。我们的方法具有两个重要优势：首先，它不需要在测试和训练数据中都提供敏感特征标注。这与现有方法相比更适合实际应用中的情况，其中需要在训练数据中提供敏感特征标注。其次，我们的方法在公正精度之间与现有方法进行比较或更好的质量协调。
</details></li>
</ul>
<hr>
<h2 id="MathGloss-Building-mathematical-glossaries-from-text"><a href="#MathGloss-Building-mathematical-glossaries-from-text" class="headerlink" title="MathGloss: Building mathematical glossaries from text"></a>MathGloss: Building mathematical glossaries from text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12649">http://arxiv.org/abs/2311.12649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucy Horowitz, Valeria de Paiva</li>
<li>for: 该论文旨在创建一个自动生成的数学知识图（KG），用于大学 mathematics 教学，使用现有的自然语言处理（NLP）工具和资源。</li>
<li>methods: 该论文使用了 Wikidata、大学 mathematics 课程涵盖的术语、法国高等教育部 mathematics 课程班本、MuLiMa 数学词典和 nLab Category Theory wiki 等五个资源，通过自动化链接和整合，创建了一个有机的数学知识图。</li>
<li>results: 该论文通过组合不同资源，实现了自动生成数学知识图，帮助学生和数学家按照自己的偏好来学习数学，并且可以帮助数学家和形式化工具专家更好地理解和交流。<details>
<summary>Abstract</summary>
MathGloss is a project to create a knowledge graph (KG) for undergraduate mathematics from text, automatically, using modern natural language processing (NLP) tools and resources already available on the web. MathGloss is a linked database of undergraduate concepts in mathematics. So far, it combines five resources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph hosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses at the University of Chicago, (iii) the syllabus of the French undergraduate mathematics curriculum which includes hyperlinks to the automated theorem prover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by mathematicians, and (v) the nLab, a wiki for category theory also curated by mathematicians. MathGloss's goal is to bring together resources for learning mathematics and to allow every mathematician to tailor their learning to their own preferences. Moreover, by organizing different resources for learning undergraduate mathematics alongside those for learning formal mathematics, we hope to make it easier for mathematicians and formal tools (theorem provers, computer algebra systems, etc) experts to "understand" each other and break down some of the barriers to formal math.
</details>
<details>
<summary>摘要</summary>
mathgloss 是一个项目，旨在使用现代自然语言处理（NLP）工具和已有网络资源自动创建高等数学知识图（KG），以便为大学生数学学习提供一个链接数据库。 mathgloss 是一个跨语言的数学概念链接数据库，目前包括五种资源：（一）Wikidata，由wikimedia基金会共同编辑的多语言知识图；（二）大学 Of Chicago 的数学课程覆盖的概念列表；（三）法国大学数学课程班目录，包括自动证明工具 Lean 4 的链接；（四） MuLiMa，由数学家编辑的多语言数学词典；以及（五） nLab，由数学家编辑的类型论wiki。 mathgloss 的目标是将数学学习资源集成起来，让每个数学家可以根据自己的喜好自定义学习。此外，通过将不同的数学学习资源与正式数学（证明工具、计算代数系统等）相结合，我们希望能够让数学家和正式工具专家更好地理解对方，缓解一些正式数学的障碍。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-Metrics-of-Language-Generation-Models-for-Synthetic-Traffic-Generation-Tasks"><a href="#Evaluation-Metrics-of-Language-Generation-Models-for-Synthetic-Traffic-Generation-Tasks" class="headerlink" title="Evaluation Metrics of Language Generation Models for Synthetic Traffic Generation Tasks"></a>Evaluation Metrics of Language Generation Models for Synthetic Traffic Generation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12534">http://arxiv.org/abs/2311.12534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Filice, Jason Ingyu Choi, Giuseppe Castellucci, Eugene Agichtein, Oleg Rokhlenko</li>
<li>for: 本研究旨在探讨自然语言生成（NLG）任务中，如何评估生成的多个文本是否具备真实用户语言的多样性。</li>
<li>methods: 本研究使用了常见的NLG评价指标，如BLEU，以评估生成的交通数据。同时，我们还提出了一些适用于评估生成交通数据的新指标，并对其进行自动化和人工验证。</li>
<li>results: 实验结果显示，我们提出的新指标能够更好地评估生成交通数据的多样性，与人工判断相吻合度提高至20%。这些发现可能会推动更好的代理人生成文本数据的评估方法的发展。<details>
<summary>Abstract</summary>
Many Natural Language Generation (NLG) tasks aim to generate a single output text given an input prompt. Other settings require the generation of multiple texts, e.g., for Synthetic Traffic Generation (STG). This generation task is crucial for training and evaluating QA systems as well as conversational agents, where the goal is to generate multiple questions or utterances resembling the linguistic variability of real users. In this paper, we show that common NLG metrics, like BLEU, are not suitable for evaluating STG. We propose and evaluate several metrics designed to compare the generated traffic to the distribution of real user texts. We validate our metrics with an automatic procedure to verify whether they capture different types of quality issues of generated data; we also run human annotations to verify the correlation with human judgements. Experiments on three tasks, i.e., Shopping Utterance Generation, Product Question Generation and Query Auto Completion, demonstrate that our metrics are effective for evaluating STG tasks, and improve the agreement with human judgement up to 20% with respect to common NLG metrics. We believe these findings can pave the way towards better solutions for estimating the representativeness of synthetic text data.
</details>
<details>
<summary>摘要</summary>
很多自然语言生成（NLG）任务的目标是生成一个输入提示的输出文本。然而，有些设置需要生成多个文本，如Synthetic Traffic Generation（STG）。这种生成任务对于训练和评估问答系统以及对话代理系统非常重要，其目标是生成多个问题或句子，模拟真实用户的语言多样性。在这篇论文中，我们表明，常见的NLG指标，如BLEU，对STG任务不适用。我们提议和评估了一些适合比较生成的交通数据与真实用户文本的分布的指标。我们使用自动程序来验证我们的指标是否捕捉不同类型的质量问题，并进行了人工标注来验证与人类判断的相关性。我们在购物问题生成、产品问题生成和查询自动完成三个任务上进行了实验，并证明了我们的指标对STG任务有效，可以提高与常见NLG指标的协调性达到20%。我们认为这些发现可以为估计生成文本数据的表现性提供新的方向。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Word-Embeddings-for-Low-Resource-Languages-using-Anchors-and-a-Chain-of-Related-Languages"><a href="#Multilingual-Word-Embeddings-for-Low-Resource-Languages-using-Anchors-and-a-Chain-of-Related-Languages" class="headerlink" title="Multilingual Word Embeddings for Low-Resource Languages using Anchors and a Chain of Related Languages"></a>Multilingual Word Embeddings for Low-Resource Languages using Anchors and a Chain of Related Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12489">http://arxiv.org/abs/2311.12489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viktor Hangya, Silvia Severini, Radoslav Ralev, Alexander Fraser, Hinrich Schütze</li>
<li>for: 本研究旨在提高低资源语言（&lt;5M tokens）和中等资源语言（&lt;50M）的多语言NLP表示法性能。</li>
<li>methods: 我们提出了一种语言链基本方法，通过逐渐添加每种语言到链中，从资源充足的源语言开始，以建立多语言词嵌入（MWEs）。我们还扩展了半联合双语方法，以消除前一代工作中的主要弱点，即独立训练的单语言嵌入。</li>
<li>results: 我们在4种语言家族中进行了双语词induction测试，包括4种低资源语言（&lt;5M tokens）和4种中等资源语言（&lt;50M）的目标语言，并显示了对两种类型的目标语言的改进表现。此外，我们的分析表明，中间语言的质量很重要，以及在多语言空间中使用所有语言的固定点也很重要。<details>
<summary>Abstract</summary>
Very low-resource languages, having only a few million tokens worth of data, are not well-supported by multilingual NLP approaches due to poor quality cross-lingual word representations. Recent work showed that good cross-lingual performance can be achieved if a source language is related to the low-resource target language. However, not all language pairs are related. In this paper, we propose to build multilingual word embeddings (MWEs) via a novel language chain-based approach, that incorporates intermediate related languages to bridge the gap between the distant source and target. We build MWEs one language at a time by starting from the resource rich source and sequentially adding each language in the chain till we reach the target. We extend a semi-joint bilingual approach to multiple languages in order to eliminate the main weakness of previous works, i.e., independently trained monolingual embeddings, by anchoring the target language around the multilingual space. We evaluate our method on bilingual lexicon induction for 4 language families, involving 4 very low-resource (<5M tokens) and 4 moderately low-resource (<50M) target languages, showing improved performance in both categories. Additionally, our analysis reveals the importance of good quality embeddings for intermediate languages as well as the importance of leveraging anchor points from all languages in the multilingual space.
</details>
<details>
<summary>摘要</summary>
非常低资源语言，具有只有几百万个Token的数据，由于跨语言Word表示质量不佳，不受现代多语言NLP方法支持。然而，一些语言对不对。在这篇论文中，我们提议使用语言链approach构建多语言词嵌入(MWEs)，通过包含中间相关语言来桥接源语言和目标语言之间的距离。我们从 richex源语言开始，逐一添加每种语言，直到达到目标语言。我们对多种语言进行扩展，以消除先前工作的主要弱点，即独立地训练的单语言嵌入。我们通过将目标语言固定在多语言空间中进行anchor来消除独立嵌入的主要弱点。我们对4种语言家族进行双语词典推导，包括4种非常低资源（<5M tokens）和4种moderately low-resource（<50M）目标语言，并表示提高了两类表现。此外，我们的分析表明中间语言的高质量嵌入的重要性以及在多语言空间中所有语言的泊点的重要性。
</details></li>
</ul>
<hr>
<h2 id="Speaker-Adapted-End-to-End-Visual-Speech-Recognition-for-Continuous-Spanish"><a href="#Speaker-Adapted-End-to-End-Visual-Speech-Recognition-for-Continuous-Spanish" class="headerlink" title="Speaker-Adapted End-to-End Visual Speech Recognition for Continuous Spanish"></a>Speaker-Adapted End-to-End Visual Speech Recognition for Continuous Spanish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12480">http://arxiv.org/abs/2311.12480</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos</li>
<li>for: 这 paper 的目的是研究如何通过特定人员的特征来提高视觉语音识别系统的质量。</li>
<li>methods: 这 paper 使用了基于精度调整技术的不同适应策略，以及一个预训练的 CTC&#x2F;Attention 架构作为基准。</li>
<li>results: 研究发现，在首先将 VSR 系统适应任务领域后，进行了两步精度调整处理，可以获得显著改善，而且只需使用有限的数据量就可以达到与当前状态艺术相当的结果。<details>
<summary>Abstract</summary>
Different studies have shown the importance of visual cues throughout the speech perception process. In fact, the development of audiovisual approaches has led to advances in the field of speech technologies. However, although noticeable results have recently been achieved, visual speech recognition remains an open research problem. It is a task in which, by dispensing with the auditory sense, challenges such as visual ambiguities and the complexity of modeling silence must be faced. Nonetheless, some of these challenges can be alleviated when the problem is approached from a speaker-dependent perspective. Thus, this paper studies, using the Spanish LIP-RTVE database, how the estimation of specialized end-to-end systems for a specific person could affect the quality of speech recognition. First, different adaptation strategies based on the fine-tuning technique were proposed. Then, a pre-trained CTC/Attention architecture was used as a baseline throughout our experiments. Our findings showed that a two-step fine-tuning process, where the VSR system is first adapted to the task domain, provided significant improvements when the speaker adaptation was addressed. Furthermore, results comparable to the current state of the art were reached even when only a limited amount of data was available.
</details>
<details>
<summary>摘要</summary>
不同的研究表明视觉cue在语音识别过程中具有重要性。实际上，audiovisual Approach的发展在语音技术领域取得了进步。然而，虽然最近得到了显著的成果，视觉语音识别仍然是一个开放的研究问题。这是一个不使用听觉感的任务，需要面临视觉歧义和模拟 silence 的挑战。然而，通过将问题看作 speaker-dependent 的角度，一些这些挑战可以得到解决。因此，本文使用西班牙LIP-RTVE数据库，研究了特定人士特定系统的特化end-to-end系统如何影响语音识别质量。首先，不同的适应策略基于细致调整技术被提出。然后，我们使用了预训练的 CTC/Attention 架构作为基线。我们的发现表明，在任务领域中首先适应 VSR 系统，然后进行第二步的细致调整，可以提供显著改善。此外，即使只有有限的数据可用，我们还可以达到与当前状态艺术的比较好的成果。
</details></li>
</ul>
<hr>
<h2 id="CSMeD-Bridging-the-Dataset-Gap-in-Automated-Citation-Screening-for-Systematic-Literature-Reviews"><a href="#CSMeD-Bridging-the-Dataset-Gap-in-Automated-Citation-Screening-for-Systematic-Literature-Reviews" class="headerlink" title="CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews"></a>CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12474">http://arxiv.org/abs/2311.12474</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wojciechkusa/systematic-review-datasets">https://github.com/wojciechkusa/systematic-review-datasets</a></li>
<li>paper_authors: Wojciech Kusa, Oscar E. Mendoza, Matthias Samwald, Petr Knoth, Allan Hanbury</li>
<li>For: The paper aims to address the challenges of evaluating the performance of automated literature screening systems for systematic literature reviews (SLRs) by introducing a meta-dataset called CSMeD, which consolidates nine publicly released collections of SLRs.* Methods: The paper uses a combination of manual and automated methods to create the CSMeD meta-dataset, which provides unified access to 325 SLRs from the fields of medicine and computer science. The authors also introduce a new dataset called CSMeD-FT, designed specifically for evaluating the full text publication screening task.* Results: The authors conduct experiments and establish baselines on new datasets using CSMeD and CSMeD-FT, demonstrating the utility of the meta-dataset for training and evaluating automated citation screening models.<details>
<summary>Abstract</summary>
Systematic literature reviews (SLRs) play an essential role in summarising, synthesising and validating scientific evidence. In recent years, there has been a growing interest in using machine learning techniques to automate the identification of relevant studies for SLRs. However, the lack of standardised evaluation datasets makes comparing the performance of such automated literature screening systems difficult. In this paper, we analyse the citation screening evaluation datasets, revealing that many of the available datasets are either too small, suffer from data leakage or have limited applicability to systems treating automated literature screening as a classification task, as opposed to, for example, a retrieval or question-answering task. To address these challenges, we introduce CSMeD, a meta-dataset consolidating nine publicly released collections, providing unified access to 325 SLRs from the fields of medicine and computer science. CSMeD serves as a comprehensive resource for training and evaluating the performance of automated citation screening models. Additionally, we introduce CSMeD-FT, a new dataset designed explicitly for evaluating the full text publication screening task. To demonstrate the utility of CSMeD, we conduct experiments and establish baselines on new datasets.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)系统性文献评估 (SLR) 在科学证据的总结、Synthesizing 和验证中发挥了关键作用。在最近几年，使用机器学习技术自动化文献creening 的兴趣在提高。然而，由于缺乏标准化评估数据集，对自动化文献creening 系统的性能评估具有困难。在这篇论文中，我们分析了文献creening 评估数据集，发现许多可用数据集都很小，或者受到数据泄露或者应用范围受限。为了解决这些挑战，我们介绍了 CSMeD，一个meta-dataset，集成了9个公共发布的集合，提供了325篇SLR的资源，来训练和评估自动化文献creening 模型。此外，我们还介绍了 CSMeD-FT，一个专门为评估全文公布creening 任务而设计的新数据集。通过使用 CSMeD，我们实验了新的基准值。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-Visual-Features-for-Continuous-Lipreading-in-Spanish"><a href="#Analysis-of-Visual-Features-for-Continuous-Lipreading-in-Spanish" class="headerlink" title="Analysis of Visual Features for Continuous Lipreading in Spanish"></a>Analysis of Visual Features for Continuous Lipreading in Spanish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12468">http://arxiv.org/abs/2311.12468</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos</li>
<li>for: 这个论文的目的是提出一种自动视觉语音识别系统，以便在没有声音时可以更好地理解语音。</li>
<li>methods: 该论文使用了基于隐藏马尔可夫模型的 Gaussian Mixture Models 来进行视觉语音识别，并使用了 eigenlips 作为视觉特征。</li>
<li>results: 研究结果表明，在限制的条件下，使用 eigenlips 和深度特征可以得到视觉语音识别的良好结果，但该任务 remain 具有挑战性。<details>
<summary>Abstract</summary>
During a conversation, our brain is responsible for combining information obtained from multiple senses in order to improve our ability to understand the message we are perceiving. Different studies have shown the importance of presenting visual information in these situations. Nevertheless, lipreading is a complex task whose objective is to interpret speech when audio is not available. By dispensing with a sense as crucial as hearing, it will be necessary to be aware of the challenge that this lack presents. In this paper, we propose an analysis of different speech visual features with the intention of identifying which of them is the best approach to capture the nature of lip movements for natural Spanish and, in this way, dealing with the automatic visual speech recognition task. In order to estimate our system, we present an audiovisual corpus compiled from a subset of the RTVE database, which has been used in the Albayz\'in evaluations. We employ a traditional system based on Hidden Markov Models with Gaussian Mixture Models. Results show that, although the task is difficult, in restricted conditions we obtain recognition results which determine that using eigenlips in combination with deep features is the best visual approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LIP-RTVE-An-Audiovisual-Database-for-Continuous-Spanish-in-the-Wild"><a href="#LIP-RTVE-An-Audiovisual-Database-for-Continuous-Spanish-in-the-Wild" class="headerlink" title="LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild"></a>LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12457">http://arxiv.org/abs/2311.12457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-gimeno/lip-rtve">https://github.com/david-gimeno/lip-rtve</a></li>
<li>paper_authors: David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos</li>
<li>for: 这篇论文主要是为了提高自动语音识别系统的稳定性和准确性，通过结合音频和视觉信息。</li>
<li>methods: 这篇论文使用了Hidden Markov Models（隐马尔可夫模型），一种传统的Speech Technologies中广泛使用的方法。</li>
<li>results: 这篇论文在无法定的自然 español语言中提供了13小时的数据，并报告了基线结果，包括Speaker-dependent和Speaker-independent两种场景。<details>
<summary>Abstract</summary>
Speech is considered as a multi-modal process where hearing and vision are two fundamentals pillars. In fact, several studies have demonstrated that the robustness of Automatic Speech Recognition systems can be improved when audio and visual cues are combined to represent the nature of speech. In addition, Visual Speech Recognition, an open research problem whose purpose is to interpret speech by reading the lips of the speaker, has been a focus of interest in the last decades. Nevertheless, in order to estimate these systems in the currently Deep Learning era, large-scale databases are required. On the other hand, while most of these databases are dedicated to English, other languages lack sufficient resources. Thus, this paper presents a semi-automatically annotated audiovisual database to deal with unconstrained natural Spanish, providing 13 hours of data extracted from Spanish television. Furthermore, baseline results for both speaker-dependent and speaker-independent scenarios are reported using Hidden Markov Models, a traditional paradigm that has been widely used in the field of Speech Technologies.
</details>
<details>
<summary>摘要</summary>
文本被视为多模态过程中的一个核心，听见和视觉是两个基础柱子。实际上，许多研究表明，将音频和视觉提示结合起来可以提高自动语音识别系统的可靠性。此外，视觉语音识别，一个已经吸引了多年研究的问题，目标是通过读取说话人的唇语来理解说话。然而，在当前的深度学习时代，大规模数据库是必需的。然而，大多数这些数据库专注于英语，其他语言缺乏充足的资源。因此，本文介绍了一个半自动地标注的 audiovisual 数据库，用于处理无结构的自然西班牙语，提供了 13 小时的数据，从西班牙电视中提取。此外，基线结果在两个场景下报告了：一个是受束的场景，另一个是无束的场景，使用传统的隐马kov 模型。
</details></li>
</ul>
<hr>
<h2 id="Visual-Analytics-for-Generative-Transformer-Models"><a href="#Visual-Analytics-for-Generative-Transformer-Models" class="headerlink" title="Visual Analytics for Generative Transformer Models"></a>Visual Analytics for Generative Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12418">http://arxiv.org/abs/2311.12418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raymond Li, Ruixin Yang, Wen Xiao, Ahmed AbuRaed, Gabriel Murray, Giuseppe Carenini</li>
<li>for: 本研究旨在支持分析转换器基于模型的可读性。</li>
<li>methods: 我们提出了一种新的可见分析框架，用于支持转换器基于模型的分析。与之前的研究主要关注encoder模型的分析而言，我们的框架是对转换器基于encoder-decoder模型和decoder模型的分析进行了一种首先的分析。因此，我们提供了一个直观的概述，让用户可以通过交互式视觉化来探索不同方面的模型。</li>
<li>results: 我们通过三个实际的NL表示问题进行了详细的案例研究，以证明我们的框架的可行性和实用性。<details>
<summary>Abstract</summary>
While transformer-based models have achieved state-of-the-art results in a variety of classification and generation tasks, their black-box nature makes them challenging for interpretability. In this work, we present a novel visual analytical framework to support the analysis of transformer-based generative networks. In contrast to previous work, which has mainly focused on encoder-based models, our framework is one of the first dedicated to supporting the analysis of transformer-based encoder-decoder models and decoder-only models for generative and classification tasks. Hence, we offer an intuitive overview that allows the user to explore different facets of the model through interactive visualization. To demonstrate the feasibility and usefulness of our framework, we present three detailed case studies based on real-world NLP research problems.
</details>
<details>
<summary>摘要</summary>
While transformer-based models have achieved state-of-the-art results in a variety of classification and generation tasks, their black-box nature makes them challenging for interpretability. In this work, we present a novel visual analytical framework to support the analysis of transformer-based generative networks. In contrast to previous work, which has mainly focused on encoder-based models, our framework is one of the first dedicated to supporting the analysis of transformer-based encoder-decoder models and decoder-only models for generative and classification tasks. Hence, we offer an intuitive overview that allows the user to explore different facets of the model through interactive visualization. To demonstrate the feasibility and usefulness of our framework, we present three detailed case studies based on real-world NLP research problems.Here's the translation in Traditional Chinese:而 tranformer 型模型在多种标签和生成任务中则 achieved state-of-the-art 结果，但它们的黑盒性质使得它们难以 interpretability。在这个工作中，我们提出了一个新的可视分析框架，用于支持 transformer 型生成网络的分析。与前一些工作不同，我们的框架主要针对 encoder-based 模型，并且是第一个专门针对 transformer 型 encoder-decoder 模型和 decoder-only 模型的生成和标签任务。因此，我们提供了一个直观的概述，让用户可以通过互动式可视化来探索不同的模型方面。为了证明我们的框架的可行性和有用性，我们在三个实际的 NLP 研究问题上进行了详细的案例研究。
</details></li>
</ul>
<hr>
<h2 id="IndoRobusta-Towards-Robustness-Against-Diverse-Code-Mixed-Indonesian-Local-Languages"><a href="#IndoRobusta-Towards-Robustness-Against-Diverse-Code-Mixed-Indonesian-Local-Languages" class="headerlink" title="IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian Local Languages"></a>IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian Local Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12405">http://arxiv.org/abs/2311.12405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Genta Indra Winata, Pascale Fung, Ayu Purwarianti</li>
<li>for: 本研究旨在探讨印尼语言混合现象，尤其是英语、SUNDA、JAVA和马来语与印尼语混合的现象。</li>
<li>methods: 本研究使用四种嵌入语言（英语、SUNDA、JAVA和马来语）与印尼语混合，并引入indoRobusta框架来评估和改进代码混合 robustness。</li>
<li>results: 分析结果显示，预训练词汇偏见对印尼英语混合处理能力产生影响，即使其他本地语言的语言多样性较高。<details>
<summary>Abstract</summary>
Significant progress has been made on Indonesian NLP. Nevertheless, exploration of the code-mixing phenomenon in Indonesian is limited, despite many languages being frequently mixed with Indonesian in daily conversation. In this work, we explore code-mixing in Indonesian with four embedded languages, i.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta, a framework to evaluate and improve the code-mixing robustness. Our analysis shows that the pre-training corpus bias affects the model's ability to better handle Indonesian-English code-mixing when compared to other local languages, despite having higher language diversity.
</details>
<details>
<summary>摘要</summary>
“印度尼西亚NLP的进步非常 significativ。然而，对印度尼西亚语言杂mix的研究 ainda是有限的，尤其是在日常对话中frequently杂mix多种语言。在这种工作中，我们对印度尼西亚语言杂mix进行了四种嵌入语言（英语、 Sundanese、 Javanese 和 Malay）的研究，并引入了IndoRobusta框架来评估和提高代码杂mix的可靠性。我们的分析表明，预训练 corpus 偏见影响了模型对印度尼西亚英语杂mix的处理能力，尽管有更高的语言多样性。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="InterPrompt-Interpretable-Prompting-for-Interrelated-Interpersonal-Risk-Factors-in-Reddit-Posts"><a href="#InterPrompt-Interpretable-Prompting-for-Interrelated-Interpersonal-Risk-Factors-in-Reddit-Posts" class="headerlink" title="InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk Factors in Reddit Posts"></a>InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk Factors in Reddit Posts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12404">http://arxiv.org/abs/2311.12404</a></li>
<li>repo_url: None</li>
<li>paper_authors: MSVPJ Sathvik, Surjodeep Sarkar, Chandni Saxena, Sunghwan Sohn, Muskan Garg</li>
<li>for: 这paper的目的是旨在预测心理健康问题的早期发现，通过人工智能模型进行自动识别和推荐。</li>
<li>methods: 这paper使用的方法是使用GPT-3模型进行N-shot学习，并通过微调GPT-3模型来增强语言修改和提高模型的解释能力。</li>
<li>results: 研究结果表明，当使用InterPrompt方法微调GPT-3模型时，可以获得更好的分类和解释生成结果，而且模型的解释能力得到显著提高。<details>
<summary>Abstract</summary>
Mental health professionals and clinicians have observed the upsurge of mental disorders due to Interpersonal Risk Factors (IRFs). To simulate the human-in-the-loop triaging scenario for early detection of mental health disorders, we recognized textual indications to ascertain these IRFs : Thwarted Belongingness (TBe) and Perceived Burdensomeness (PBu) within personal narratives. In light of this, we use N-shot learning with GPT-3 model on the IRF dataset, and underscored the importance of fine-tuning GPT-3 model to incorporate the context-specific sensitivity and the interconnectedness of textual cues that represent both IRFs.   In this paper, we introduce an Interpretable Prompting (InterPrompt)} method to boost the attention mechanism by fine-tuning the GPT-3 model. This allows a more sophisticated level of language modification by adjusting the pre-trained weights. Our model learns to detect usual patterns and underlying connections across both the IRFs, which leads to better system-level explainability and trustworthiness. The results of our research demonstrate that all four variants of GPT-3 model, when fine-tuned with InterPrompt, perform considerably better as compared to the baseline methods, both in terms of classification and explanation generation.
</details>
<details>
<summary>摘要</summary>
精神医生和临床专业人员已经观察到因人际风险因素（IRF）而导致的精神疾病的增加。为了模拟人类在循环中排查精神疾病的情况，我们在个人故事中找到了文本指示，以确定IRFs：被排挤的belongingness（TBe）和感受到的压力（PBu）。在这种情况下，我们使用N-shot学习的GPT-3模型，并强调了GPT-3模型的Context-specific敏感性和文本提示之间的相互关联性。在这篇论文中，我们介绍了一种可解释的提示方法（InterPrompt），用于提高GPT-3模型的注意力机制。这种方法通过修改预训练的权重来实现更加复杂的语言修改，从而使模型能够更好地检测文本中的常见模式和下面的连接。我们的研究结果表明，对GPT-3模型进行InterPrompt的 fine-tuning，可以使其在分类和解释生成方面表现更好，并且提高系统级别的解释可读性和可信度。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Graph-Meets-Large-Language-Model-Progress-and-Future-Directions"><a href="#A-Survey-of-Graph-Meets-Large-Language-Model-Progress-and-Future-Directions" class="headerlink" title="A Survey of Graph Meets Large Language Model: Progress and Future Directions"></a>A Survey of Graph Meets Large Language Model: Progress and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12399">http://arxiv.org/abs/2311.12399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu</li>
<li>for: This paper is written for researchers and practitioners working in the field of graph-related tasks, particularly those interested in leveraging Large Language Models (LLMs) for improving the performance of graph-based methods.</li>
<li>methods: The paper surveys and analyzes existing methods that integrate LLMs with graphs, and proposes a new taxonomy to categorize these methods based on the role played by LLMs. The paper also provides a systematic review of representative methods along the three categories of the taxonomy.</li>
<li>results: The paper discusses the remaining limitations of existing studies and highlights promising avenues for future research in the field of LLMs in graph-related tasks. The paper also provides a comprehensive overview of the state-of-the-art performance of LLMs in various graph-related tasks.Here are the three key information points in Simplified Chinese text:</li>
<li>for: 这篇论文是为研究者和实践者工作在图相关任务领域，特别是感兴趣利用大语言模型（LLMs）来改进图基的方法的人所写的。</li>
<li>methods: 这篇论文对现有的图与LLMs的方法进行了抽象和分析，并提出了一个新的分类方法，将现有的方法分为三个类别，根据LLMs在图相关任务中的角色。</li>
<li>results: 这篇论文讨论了现有研究的限制，并指出了未来研究的可能性，提供了图相关任务中LMMs的状态艺术表现。<details>
<summary>Abstract</summary>
Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.
</details>
<details>
<summary>摘要</summary>
GRAPH 在实际应用中表示和分析复杂关系的重要角色，如引用网络、社交网络和生物数据。最近，大型自然语言模型（LLMs），在不同领域取得了很大成功，也在图关联任务中被利用，以超越传统的图神经网络（GNNs）基于方法并达到状态之最好性。在这项调查中，我们首先提供了一个完整的评论和分析现有的方法，这些方法是根据 LLMS 在图关联任务中扮演的角色（即增强器、预测器和对齐组件）进行分类。然后，我们系统地报道了代表性方法的评论，并将其分类到三个类别中。最后，我们讨论了现有研究的剩下的限制，并 highlighted 未来研究的可能性。相关论文将在：https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks 中进行系统性的汇总和更新。
</details></li>
</ul>
<hr>
<h2 id="Problems-of-Non-equivalent-Words-in-Technical-Translation"><a href="#Problems-of-Non-equivalent-Words-in-Technical-Translation" class="headerlink" title="Problems of Non-equivalent Words in Technical Translation"></a>Problems of Non-equivalent Words in Technical Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12395">http://arxiv.org/abs/2311.12395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Ibrahim Qani</li>
<li>for: 这个研究论文的主要目的是解决英语和俄语之间的非等效词语问题，以便更好地理解这两种语言之间的连接。</li>
<li>methods: 这篇论文使用了不同的方法和规则来将源语言中的非等效词语翻译到目标语言中，以解决这些词语在不同语言中的差异。</li>
<li>results: 这篇论文的结果表明，通过使用不同的方法和规则，可以减少英语和俄语之间的非等效词语问题，并提高这两种语言之间的连接和理解。<details>
<summary>Abstract</summary>
Translating words which do not have equivalent in target language is not easy and finding proper equivalent of those words are very important to render correctly and understandably, the article defines some thoughts and ideas of scientists on the common problems of non-equivalent words from English to Russian language and includes English and Russian examples and ideas of certain scientist. The English language is worldwide spoken and there are 1.35 billion English speakers and over 258 million Russian speakers according to the 2021s statistics. Inevitably, these billions of speakers around the world have connection and they may have deal in different criteria. In order to understand one another they need to have a pure and fully-understood language. These pure languages understanding directly relates to translation knowledge where linguists and translators need to work and research to eradicate misunderstanding. Misunderstandings mostly appear in non-equivalent words because there are different local and internal words like food, garment, cultural and traditional words and others in every notion. Truly, most of these words do not have equivalent in the target language and these words need to be worked and find their equivalent in the target language to fully understand the both languages. However, some of these non-equivalent words are already professionally rendered to the target language but still there many other words to be rendered. Hence, this research paper includes different ways and rules of rendering non-equivalent words from source language to the target language.
</details>
<details>
<summary>摘要</summary>
英语是全球最广泛使用的语言之一，有135亿英语母语者和258万俄语母语者，根据2021年统计。这些亿万speaker在世界各地有联系，可能有不同的标准和规范。为了理解彼此，他们需要有一种纯正的语言理解。这种纯正语言理解直接与翻译知识相关， linguisits和翻译员需要努力工作和研究，以消除不同解释。非等效词是翻译中最大的障碍，因为每种语言都有不同的本地和内部词汇，如食物、服装、文化和传统词汇等。大多数这些词没有等效的目标语言词汇，需要 linguisits和翻译员一直在工作，以找到目标语言中的等效词汇。然而，一些非等效词已经在目标语言中得到了职业的翻译，但还有很多其他的词汇需要翻译。因此，这篇研究论文探讨了不同的翻译方法和规则，以帮助 linguisits和翻译员更好地翻译非等效词。
</details></li>
</ul>
<hr>
<h2 id="The-Obscure-Limitation-of-Modular-Multilingual-Language-Models"><a href="#The-Obscure-Limitation-of-Modular-Multilingual-Language-Models" class="headerlink" title="The Obscure Limitation of Modular Multilingual Language Models"></a>The Obscure Limitation of Modular Multilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12375">http://arxiv.org/abs/2311.12375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Ayu Purwarianti</li>
<li>for: 这篇论文探讨了模块化多语言模型（MLM）在多语言推理场景中的局限性，特别是在未知语言情况下。</li>
<li>methods: 该论文使用了语言标识（LID）模块，以评估模块化MLM在真实多语言场景中的性能。</li>
<li>results: 研究发现，在加入LID模块后，模块化MLM在多语言推理场景中的性能受到了改善。<details>
<summary>Abstract</summary>
We expose the limitation of modular multilingual language models (MLMs) in multilingual inference scenarios with unknown languages. Existing evaluations of modular MLMs exclude the involvement of language identification (LID) modules, which obscures the performance of real-case multilingual scenarios of modular MLMs. In this work, we showcase the effect of adding LID on the multilingual evaluation of modular MLMs and provide discussions for closing the performance gap of caused by the pipelined approach of LID and modular MLMs.
</details>
<details>
<summary>摘要</summary>
我团队揭示了模块化多语言模型（MLM）在多语言推理场景中的局限性。现有的评估方法中排除了语言标识（LID）模块的参与，这使得真实的多语言场景中模块化MLM的性能被遮盖。在这项工作中，我们表明了将LID纳入多语言评估中对模块化MLM的影响，并提供了关闭性能差距的讨论。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Turing-A-Comparative-Analysis-of-Approaches-for-Detecting-Machine-Generated-Text"><a href="#Beyond-Turing-A-Comparative-Analysis-of-Approaches-for-Detecting-Machine-Generated-Text" class="headerlink" title="Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text"></a>Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12373">http://arxiv.org/abs/2311.12373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Farid Adilazuarda, Nikolaos Nektarios Arkoulis, Oleksii Chumakov</li>
<li>for: 本研究旨在评估三种方法用于人工智能文本与人类文本之间的区分，以提高自然语言处理领域中的机器人混淆检测能力。</li>
<li>methods: 本研究使用传统的浅学习、语言模型精度调整和多语言模型调整三种方法来处理机器生成的文本。</li>
<li>results: 研究发现这三种方法在各种机器生成文本上的表现有显著差异，表明这一领域仍需进一步发展。<details>
<summary>Abstract</summary>
Significant progress has been made on text generation by pre-trained language models (PLMs), yet distinguishing between human and machine-generated text poses an escalating challenge. This paper offers an in-depth evaluation of three distinct methods used to address this task: traditional shallow learning, Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These approaches are rigorously tested on a wide range of machine-generated texts, providing a benchmark of their competence in distinguishing between human-authored and machine-authored linguistic constructs. The results reveal considerable differences in performance across methods, thus emphasizing the continued need for advancement in this crucial area of NLP. This study offers valuable insights and paves the way for future research aimed at creating robust and highly discriminative models.
</details>
<details>
<summary>摘要</summary>
“文本生成领域内，已经取得了重要进展，但是分辨人工和机器生成的文本却成为了一个增长的挑战。这篇论文对三种不同的方法进行了深入的评估：传统的浅学习、语言模型（LM）练习和多语言模型练习。这些方法在各种机器生成文本上进行了严格的测试，为分辨人工和机器生成语言结构的能力提供了标准。结果表明这些方法之间存在显著的差异，因此高亮了需要进一步改进的这一重要领域。这篇研究提供了有价值的洞察和指导，为未来的研究提供了路径。”
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Language-Models-for-Tour-Itinerary-Recommendation"><a href="#Utilizing-Language-Models-for-Tour-Itinerary-Recommendation" class="headerlink" title="Utilizing Language Models for Tour Itinerary Recommendation"></a>Utilizing Language Models for Tour Itinerary Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12355">http://arxiv.org/abs/2311.12355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngai Lam Ho, Kwan Hui Lim</li>
<li>for: 这篇论文旨在提出一种基于语言模型的旅游计划和建议方法，用于推荐个性化的旅游点 Interest (POI) 和根据各种约束制定一个合理的旅游计划。</li>
<li>methods: 本文使用了词嵌入技术 Word2Vec 和 GloVe 来学习 POI 嵌入，以及基于 transformer 技术的 BERT 来生成旅游计划。</li>
<li>results: 本文的实验结果表明，使用语言模型可以提高旅游计划的个性化程度和约束满足度，并且可以减少计划时间。<details>
<summary>Abstract</summary>
Tour itinerary recommendation involves planning a sequence of relevant Point-of-Interest (POIs), which combines challenges from the fields of both Operations Research (OR) and Recommendation Systems (RS). As an OR problem, there is the need to maximize a certain utility (e.g., popularity of POIs in the tour) while adhering to some constraints (e.g., maximum time for the tour). As a RS problem, it is heavily related to problem or filtering or ranking a subset of POIs that are relevant to a user and recommending it as part of an itinerary. In this paper, we explore the use of language models for the task of tour itinerary recommendation and planning. This task has the unique requirement of recommending personalized POIs relevant to users and planning these POIs as an itinerary that satisfies various constraints. We discuss some approaches in this area, such as using word embedding techniques like Word2Vec and GloVe for learning POI embeddings and transformer-based techniques like BERT for generating   itineraries.
</details>
<details>
<summary>摘要</summary>
旅行计划建议涉及到规划一系列相关的点索引（POI），这种问题同时具有操作研究（OR）和推荐系统（RS）两个领域的挑战。作为OR问题，需要最大化一定的用户喜好（例如旅游点的受欢迎程度），同时遵循一些约束（例如旅行时间的上限）。作为RS问题，它与过滤或排序POI相关的用户有着密切的关系，并且需要为用户推荐个性化的旅行计划。在这篇论文中，我们探讨使用语言模型来解决旅行计划建议和规划的问题。这种问题具有个性化推荐用户相关的POI，并将其组织成满足多种约束的旅行计划。我们介绍了一些在这个领域的方法，包括使用word embedding技术如Word2Vec和GloVe来学习POI嵌入，以及使用 transformer-based技术如BERT来生成旅行计划。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Transformer-Architecture-in-Long-Context-Large-Language-Models-A-Comprehensive-Survey"><a href="#Advancing-Transformer-Architecture-in-Long-Context-Large-Language-Models-A-Comprehensive-Survey" class="headerlink" title="Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey"></a>Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12351">http://arxiv.org/abs/2311.12351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/strivin0311/long-llms-learning">https://github.com/strivin0311/long-llms-learning</a></li>
<li>paper_authors: Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, Xiaoxing Ma</li>
<li>for: 本研究主要是为了提高Transformer基于模型在长文本上的能力，以便在实际场景中更有效地应用。</li>
<li>methods: 本文提出了一种全面的分类方法，以便浏览Transformer模型在不同阶段的改进，包括预训练和推理阶段。</li>
<li>results: 本文提供了一些评估需求，包括数据集、评价指标和基准模型，以及一些优化工具包，以提高LLMs的效率和可靠性在不同阶段。<details>
<summary>Abstract</summary>
With the bomb ignited by ChatGPT, Transformer-based Large Language Models (LLMs) have paved a revolutionary path toward Artificial General Intelligence (AGI) and have been applied in diverse areas as knowledge bases, human interfaces, and dynamic agents. However, a prevailing limitation exists: many current LLMs, constrained by resources, are primarily pre-trained on shorter texts, rendering them less effective for longer-context prompts, commonly encountered in real-world settings. In this paper, we present a comprehensive survey focusing on the advancement of model architecture in Transformer-based LLMs to optimize long-context capabilities across all stages from pre-training to inference. We firstly delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. Then, we mainly offer a holistic taxonomy to navigate the landscape of Transformer upgrades on architecture to solve these problems. Afterward, we provide the investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as some amazing optimization toolkits like libraries, systems, and compilers to augment LLMs' efficiency and efficacy across different stages. Finally, we further discuss the predominant challenges and potential avenues for future research in this domain. Additionally, we have established a repository where we curate relevant literature with real-time updates at https://github.com/Strivin0311/long-llms-learning.
</details>
<details>
<summary>摘要</summary>
With the bomb ignited by ChatGPT, Transformer-based Large Language Models (LLMs) have paved a revolutionary path toward Artificial General Intelligence (AGI) and have been applied in diverse areas as knowledge bases, human interfaces, and dynamic agents. However, a prevailing limitation exists: many current LLMs, constrained by resources, are primarily pre-trained on shorter texts, rendering them less effective for longer-context prompts, commonly encountered in real-world settings. In this paper, we present a comprehensive survey focusing on the advancement of model architecture in Transformer-based LLMs to optimize long-context capabilities across all stages from pre-training to inference.Firstly, we delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. Then, we mainly offer a holistic taxonomy to navigate the landscape of Transformer upgrades on architecture to solve these problems. Afterward, we provide the investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as some amazing optimization toolkits like libraries, systems, and compilers to augment LLMs' efficiency and efficacy across different stages. Finally, we further discuss the predominant challenges and potential avenues for future research in this domain. Additionally, we have established a repository where we curate relevant literature with real-time updates at <https://github.com/Strivin0311/long-llms-learning>.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Political-Orientation-of-Social-Media-Posts-An-Extended-Analysis"><a href="#Modeling-Political-Orientation-of-Social-Media-Posts-An-Extended-Analysis" class="headerlink" title="Modeling Political Orientation of Social Media Posts: An Extended Analysis"></a>Modeling Political Orientation of Social Media Posts: An Extended Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12323">http://arxiv.org/abs/2311.12323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadia Kamal, Brenner Little, Jade Gullic, Trevor Harms, Kristin Olofsson, Arunkumar Bagavathi</li>
<li>for: 本研究旨在Characterizing political polarization on online social media, specifically in the social media posts themselves.</li>
<li>methods: 本研究使用two heuristic methods to label social media posts, leveraging on news media bias and post content.  comparison with a randomly sampled human-annotated dataset.</li>
<li>results: 研究表明，current machine learning models can exhibit improved performance in predicting political orientation of social media posts, employing both traditional supervised learning and few-shot learning setups.<details>
<summary>Abstract</summary>
Developing machine learning models to characterize political polarization on online social media presents significant challenges. These challenges mainly stem from various factors such as the lack of annotated data, presence of noise in social media datasets, and the sheer volume of data. The common research practice typically examines the biased structure of online user communities for a given topic or qualitatively measuring the impacts of polarized topics on social media. However, there is limited work focusing on analyzing polarization at the ground-level, specifically in the social media posts themselves. Such existing analysis heavily relies on annotated data, which often requires laborious human labeling, offers labels only to specific problems, and lacks the ability to determine the near-future bias state of a social media conversations. Understanding the degree of political orientation conveyed in social media posts is crucial for quantifying the bias of online user communities and investigating the spread of polarized content. In this work, we first introduce two heuristic methods that leverage on news media bias and post content to label social media posts. Next, we compare the efficacy and quality of heuristically labeled dataset with a randomly sampled human-annotated dataset. Additionally, we demonstrate that current machine learning models can exhibit improved performance in predicting political orientation of social media posts, employing both traditional supervised learning and few-shot learning setups. We conduct experiments using the proposed heuristic methods and machine learning approaches to predict the political orientation of posts collected from two social media forums with diverse political ideologies: Gab and Twitter.
</details>
<details>
<summary>摘要</summary>
开发机器学习模型来 caracterize 在社交媒体上的政治偏见存在 significativ challenges。这些挑战主要来自于各种因素，如社交媒体数据集中的噪音、缺乏标注数据以及数据量的庞大。现有的研究通常是通过分析在线用户社区的偏见结构来研究特定主题的偏见，或者通过衡量偏见话题在社交媒体上的影响来衡量偏见的程度。然而，有限的研究将注意力集中在社交媒体帖子本身的偏见分析上，特别是在帖子中的政治偏见。现有的分析方法通常需要人工标注，且只能对特定问题提供标签，而且无法在社交媒体对话中确定未来偏见状态。了解社交媒体帖子中政治方向的程度是Quantifying the bias of online user communities和推广偏见内容的调查中非常重要的一步。在这种情况下，我们首先介绍了两种可靠的方法，即通过新闻媒体偏见和帖子内容来标注社交媒体帖子。然后，我们比较了这些方法与随机抽样的人工标注数据的效果和质量。此外，我们还示出了现有的机器学习模型可以通过使用传统的超vision学习和几 shot learning的方式提高预测社交媒体帖子的政治方向性表现。我们在两个社交媒体平台上（Gab和Twitter）进行了实验，以验证我们的方法和模型。
</details></li>
</ul>
<hr>
<h2 id="AcademicGPT-Empowering-Academic-Research"><a href="#AcademicGPT-Empowering-Academic-Research" class="headerlink" title="AcademicGPT: Empowering Academic Research"></a>AcademicGPT: Empowering Academic Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12315">http://arxiv.org/abs/2311.12315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shufa Wei, Xiaolong Xu, Xianbiao Qi, Xi Yin, Jun Xia, Jingyi Ren, Peijun Tang, Yuxiang Zhong, Yihao Chen, Xiaoqin Ren, Yuxin Liang, Liankai Huang, Kai Xie, Weikang Gui, Wei Tan, Shuanglong Sun, Yongquan Hu, Qinxian Liu, Nanjin Li, Chihao Dai, Lihua Wang, Xiaohui Liu, Lei Zhang, Yutao Xie</li>
<li>for: 该论文旨在探讨大语言模型（LLMs）在学术研究领域的应用，并提出了一种针对学术研究领域的培训模型——AcademicGPT。</li>
<li>methods: 该论文使用了一种基于LLaMA2-70B的 continual training 模型，并在学术研究领域中进行了培训，主要使用了学术论文、论文集、学术域内容等数据。</li>
<li>results: 该论文通过在多个公共评价指标（MMLU、CEval）和专业学术指标（PubMedQA、SCIEval、ComputerScienceQA）上进行了评估，证明了AcademicGPT在普通知识、中文能力和学术能力等方面具有出色的能力。此外，该论文还推出了一些针对学术领域的应用，如通用学术问答、AI助力论文阅读、论文评审和AI助力标题和摘要生成等。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks. Yet, many of these advanced LLMs are tailored for broad, general-purpose applications. In this technical report, we introduce AcademicGPT, designed specifically to empower academic research. AcademicGPT is a continual training model derived from LLaMA2-70B. Our training corpus mainly consists of academic papers, thesis, content from some academic domain, high-quality Chinese data and others. While it may not be extensive in data scale, AcademicGPT marks our initial venture into a domain-specific GPT tailored for research area. We evaluate AcademicGPT on several established public benchmarks such as MMLU and CEval, as well as on some specialized academic benchmarks like PubMedQA, SCIEval, and our newly-created ComputerScienceQA, to demonstrate its ability from general knowledge ability, to Chinese ability, and to academic ability. Building upon AcademicGPT's foundation model, we also developed several applications catered to the academic area, including General Academic Question Answering, AI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract Generation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经表现出色在不同的自然语言处理任务中。然而，许多这些高级LLM都是为广泛的通用应用设计的。在这份技术报告中，我们介绍AcademicGPT，这是为学术研究而设计的域 especific GPT。AcademicGPT是基于LLaMA2-70B的连续训练模型。我们的训练集主要由学术论文、硬件、学术领域内容、高质量的中文数据和其他组成。虽然数据规模不够广泛，但AcademicGPT标志着我们在域特定GPT领域的首次尝试。我们使用了多个公共的benchmark测试AcademicGPT的能力，包括MMLU和CEval，以及一些专门的学术benchmark，如PubMedQA、SCIEval和我们新创的ComputerScienceQA，以示其在通用知识、中文能力和学术能力方面的能力。基于AcademicGPT的基础模型，我们还开发了一些针对学术领域的应用程序，包括通用学术问答、AI助手读硬件、论文评审和AI助手标题和摘要生成。
</details></li>
</ul>
<hr>
<h2 id="Enabling-On-Device-Large-Language-Model-Personalization-with-Self-Supervised-Data-Selection-and-Synthesis"><a href="#Enabling-On-Device-Large-Language-Model-Personalization-with-Self-Supervised-Data-Selection-and-Synthesis" class="headerlink" title="Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis"></a>Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12275">http://arxiv.org/abs/2311.12275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, Yiyu Shi</li>
<li>for: 这个论文的目的是提出一种基于Edge设备的自然语言处理（NLP）模型个性化方法，以便在用户生成的对话数据上进行实时回答生成。</li>
<li>methods: 该方法使用了自然语言生成（NLG）技术，通过在线选择和存储最有代表性的数据，以及使用多个semantically相似的问题文本和预期答案对进行自我监督学习。</li>
<li>results:  experiments show that the proposed framework achieves the best user-specific content-generating capability (accuracy) and fine-tuning speed (performance) compared with vanilla baselines.<details>
<summary>Abstract</summary>
After a large language model (LLM) is deployed on edge devices, it is desirable for these devices to learn from user-generated conversation data to generate user-specific and personalized responses in real-time. However, user-generated data usually contains sensitive and private information, and uploading such data to the cloud for annotation is not preferred if not prohibited. While it is possible to obtain annotation locally by directly asking users to provide preferred responses, such annotations have to be sparse to not affect user experience. In addition, the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data. It remains an open question how to enable on-device LLM personalization, considering sparse annotation and limited on-device storage. In this paper, we propose a novel framework to select and store the most representative data online in a self-supervised way. Such data has a small memory footprint and allows infrequent requests of user annotations for further fine-tuning. To enhance fine-tuning quality, multiple semantically similar pairs of question texts and expected responses are generated using the LLM. Our experiments show that the proposed framework achieves the best user-specific content-generating capability (accuracy) and fine-tuning speed (performance) compared with vanilla baselines. To the best of our knowledge, this is the very first on-device LLM personalization framework.
</details>
<details>
<summary>摘要</summary>
After a large language model (LLM) is deployed on edge devices, it is desirable for these devices to learn from user-generated conversation data to generate user-specific and personalized responses in real-time. However, user-generated data usually contains sensitive and private information, and uploading such data to the cloud for annotation is not preferred if not prohibited. While it is possible to obtain annotation locally by directly asking users to provide preferred responses, such annotations have to be sparse to not affect user experience. In addition, the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data. It remains an open question how to enable on-device LLM personalization, considering sparse annotation and limited on-device storage. In this paper, we propose a novel framework to select and store the most representative data online in a self-supervised way. Such data has a small memory footprint and allows infrequent requests of user annotations for further fine-tuning. To enhance fine-tuning quality, multiple semantically similar pairs of question texts and expected responses are generated using the LLM. Our experiments show that the proposed framework achieves the best user-specific content-generating capability (准确率) and fine-tuning speed (性能) compared with vanilla baselines. To the best of our knowledge, this is the very first on-device LLM personalization framework.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/21/cs.CL_2023_11_21/" data-id="clpahu71u00fd3h88bk7kcy5z" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/21/cs.AI_2023_11_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-11-21
        
      </div>
    </a>
  
  
    <a href="/2023/11/21/cs.LG_2023_11_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-11-21</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
