
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-21 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12796 repo_url: None paper_authors: David Stotko, Nils Wandel, Reinh">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-21">
<meta property="og:url" content="https://nullscc.github.io/2023/11/21/cs.CV_2023_11_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12796 repo_url: None paper_authors: David Stotko, Nils Wandel, Reinh">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-21T13:00:00.000Z">
<meta property="article:modified_time" content="2023-12-09T06:43:28.965Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/21/cs.CV_2023_11_21/" class="article-date">
  <time datetime="2023-11-21T13:00:00.000Z" itemprop="datePublished">2023-11-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-21
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Physics-guided-Shape-from-Template-Monocular-Video-Perception-through-Neural-Surrogate-Models"><a href="#Physics-guided-Shape-from-Template-Monocular-Video-Perception-through-Neural-Surrogate-Models" class="headerlink" title="Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models"></a>Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12796">http://arxiv.org/abs/2311.12796</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Stotko, Nils Wandel, Reinhard Klein</li>
<li>For: 重建三维动态场景* Methods: 使用预训练神经网络代理模型，快速评估，稳定，生成平滑的重建结果，同时通过对模拟的网格进行可微分渲染，实现像素级比较，从而对Physics simulation进行权重补做，以提取形状信息和物理参数。* Results: 比较 $\phi$-SfT方法，runtime reduction为400-500倍，实现了精确、稳定、平滑的重建结果。<details>
<summary>Abstract</summary>
3D reconstruction of dynamic scenes is a long-standing problem in computer graphics and increasingly difficult the less information is available. Shape-from-Template (SfT) methods aim to reconstruct a template-based geometry from RGB images or video sequences, often leveraging just a single monocular camera without depth information, such as regular smartphone recordings. Unfortunately, existing reconstruction methods are either unphysical and noisy or slow in optimization. To solve this problem, we propose a novel SfT reconstruction algorithm for cloth using a pre-trained neural surrogate model that is fast to evaluate, stable, and produces smooth reconstructions due to a regularizing physics simulation. Differentiable rendering of the simulated mesh enables pixel-wise comparisons between the reconstruction and a target video sequence that can be used for a gradient-based optimization procedure to extract not only shape information but also physical parameters such as stretching, shearing, or bending stiffness of the cloth. This allows to retain a precise, stable, and smooth reconstructed geometry while reducing the runtime by a factor of 400-500 compared to $\phi$-SfT, a state-of-the-art physics-based SfT approach.
</details>
<details>
<summary>摘要</summary>
三维重建动态场景是计算机图形领域的长期问题，随着信息的更少，变得越来越困难。形状从模板（SfT）方法 aspire to reconstruct a template-based geometry from RGB images or video sequences, often leveraging just a single monocular camera without depth information, such as regular smartphone recordings. Unfortunately, existing reconstruction methods are either unphysical and noisy or slow in optimization. To solve this problem, we propose a novel SfT reconstruction algorithm for cloth using a pre-trained neural surrogate model that is fast to evaluate, stable, and produces smooth reconstructions due to a regularizing physics simulation. Differentiable rendering of the simulated mesh enables pixel-wise comparisons between the reconstruction and a target video sequence that can be used for a gradient-based optimization procedure to extract not only shape information but also physical parameters such as stretching, shearing, or bending stiffness of the cloth. This allows to retain a precise, stable, and smooth reconstructed geometry while reducing the runtime by a factor of 400-500 compared to $\phi$-SfT, a state-of-the-art physics-based SfT approach.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is in the form of Traditional Chinese, which is the standard writing system used in Taiwan and other parts of the world. The translation is in Simplified Chinese, which is used in mainland China.The translation is written in a formal and professional style, using technical terms and phrases that are commonly used in the field of computer graphics. I tried to preserve the original meaning and context of the text as much as possible, while also making sure that the translation is accurate and natural-sounding.Please note that the translation may not be perfect, and there may be some nuances or cultural references that are lost in translation. If you have any specific questions or concerns, please feel free to ask.
</details></li>
</ul>
<hr>
<h2 id="ShareGPT4V-Improving-Large-Multi-Modal-Models-with-Better-Captions"><a href="#ShareGPT4V-Improving-Large-Multi-Modal-Models-with-Better-Captions" class="headerlink" title="ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"></a>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12793">http://arxiv.org/abs/2311.12793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ShareGPT4V/ShareGPT4V.github.io">https://github.com/ShareGPT4V/ShareGPT4V.github.io</a></li>
<li>paper_authors: Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin<br>for:* The paper is written to address the scarcity of high-quality image-text data for training large multi-modal models (LMMs).methods:* The authors introduce the ShareGPT4V dataset, a large-scale resource featuring 1.2 million highly descriptive captions that cover world knowledge, object properties, spatial relationships, and aesthetic evaluations.* The dataset is expanded from a curated 100K high-quality captions collected from advanced GPT4-Vision, and is further enhanced with a superb caption model trained on this subset.results:* The ShareGPT4V dataset significantly enhances the performance of LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MME and MMBench benchmarks, with respective gains of 222.8&#x2F;22.0&#x2F;22.3 and 2.7&#x2F;1.3&#x2F;1.5.* The authors also incorporate ShareGPT4V data into both the pre-training and SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple architecture that has remarkable performance across a majority of the multi-modal benchmarks.<details>
<summary>Abstract</summary>
In the realm of large multi-modal models (LMMs), efficient modality alignment is crucial yet often constrained by the scarcity of high-quality image-text data. To address this bottleneck, we introduce the ShareGPT4V dataset, a pioneering large-scale resource featuring 1.2 million highly descriptive captions, which surpasses existing datasets in diversity and information content, covering world knowledge, object properties, spatial relationships, and aesthetic evaluations. Specifically, ShareGPT4V originates from a curated 100K high-quality captions collected from advanced GPT4-Vision and has been expanded to 1.2M with a superb caption model trained on this subset. ShareGPT4V first demonstrates its effectiveness for the Supervised Fine-Tuning (SFT) phase, by substituting an equivalent quantity of detailed captions in existing SFT datasets with a subset of our high-quality captions, significantly enhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MME and MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and 2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training and SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple architecture that has remarkable performance across a majority of the multi-modal benchmarks. This project is available at https://ShareGPT4V.github.io to serve as a pivotal resource for advancing the LMMs community.
</details>
<details>
<summary>摘要</summary>
在大型多modal模型（LMMs）领域，有效的modal匹配是关键，然而经常受到高质量图文数据的缺乏所限制。为解决这个瓶颈，我们介绍了ShareGPT4V dataset，这是一个创新的大规模资源，包含120万高度描述性的标签，超过现有数据集的多样性和信息内容，覆盖世界知识、物品属性、空间关系和艺术评价等领域。具体来说，ShareGPT4V来自于我们自动选择的100K高质量标签，并通过这些标签进行扩展至120万。ShareGPT4V首先在Supervised Fine-Tuning（SFT）阶段表现出色，通过将现有SFT数据集中的相应量的详细标签替换为我们高质量标签，使LMMs like LLaVA-7B、LLaVA-1.5-13B和Qwen-VL-Chat-7B在MME和MMBench bencmarks上显著提高，增幅分别为222.8/22.0/22.3和2.7/1.3/1.5。我们进一步将ShareGPT4V数据集 integrate到预训练和SFT阶段，得到了ShareGPT4V-7B，一个基于简单结构的优秀LMM，在多种多Modalbenchmarks上表现出色。这个项目可以在https://ShareGPT4V.github.io中下载，以便为LMMs社区提供积极的资源。
</details></li>
</ul>
<hr>
<h2 id="SuGaR-Surface-Aligned-Gaussian-Splatting-for-Efficient-3D-Mesh-Reconstruction-and-High-Quality-Mesh-Rendering"><a href="#SuGaR-Surface-Aligned-Gaussian-Splatting-for-Efficient-3D-Mesh-Reconstruction-and-High-Quality-Mesh-Rendering" class="headerlink" title="SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering"></a>SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12775">http://arxiv.org/abs/2311.12775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Guédon, Vincent Lepetit</li>
<li>for: This paper proposes a method for extracting a mesh from 3D Gaussian Splatting, which is a recently popular method for realistic rendering that is faster to train than NeRFs.</li>
<li>methods: The method introduced in the paper includes a regularization term that encourages the gaussians to align with the surface of the scene, and a Poisson reconstruction method to extract a mesh from the gaussians.</li>
<li>results: The paper reports that the proposed method can extract a mesh from 3D Gaussian Splatting within minutes, which is faster than the state-of-the-art methods on neural SDFs, and provides a better rendering quality. Additionally, the method allows for easy editing, sculpting, rigging, animating, compositing, and relighting of the gaussians using traditional software by manipulating the mesh.<details>
<summary>Abstract</summary>
We propose a method to allow precise and extremely fast mesh extraction from 3D Gaussian Splatting. Gaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D gaussians as these gaussians tend to be unorganized after optimization and no method has been proposed so far. Our first key contribution is a regularization term that encourages the gaussians to align well with the surface of the scene. We then introduce a method that exploits this alignment to extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs. Finally, we introduce an optional refinement strategy that binds gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, compositing and relighting of the Gaussians using traditional softwares by manipulating the mesh instead of the gaussians themselves. Retrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art methods on neural SDFs, while providing a better rendering quality.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，允许精确并非常快地从3D Gaussian Splatting中提取网格。 Gaussian Splatting在最近很受欢迎，因为它可以提供真实的渲染，而且在训练时间上比NeRF更快。然而，从millions of tiny 3D Gaussian中提取网格是一个挑战，因为这些Gaussian在优化后通常不具有有序结构。我们的首要贡献是一个正则化项，使Gaussian与场景表面align。我们然后介绍了一种利用这种alignment提取网格的方法，使用Poisson重建，这是快速、可扩展、细节保持的。此外，我们还介绍了一种可选的精度增强策略，将Gaussian绑定到网格上，并通过Gaussian splatting rendering进行共同优化。这使得可以使用传统软件进行编辑、雕塑、rigging、动画、复制和重新照明Gaussian，而不需要直接操作Gaussian本身。通过我们的方法，可以在几分钟内获得高质量的编辑网格，而不是以前的多个小时。
</details></li>
</ul>
<hr>
<h2 id="Iris-Presentation-Attack-Assessing-the-Impact-of-Combining-Vanadium-Dioxide-Films-with-Artificial-Eyes"><a href="#Iris-Presentation-Attack-Assessing-the-Impact-of-Combining-Vanadium-Dioxide-Films-with-Artificial-Eyes" class="headerlink" title="Iris Presentation Attack: Assessing the Impact of Combining Vanadium Dioxide Films with Artificial Eyes"></a>Iris Presentation Attack: Assessing the Impact of Combining Vanadium Dioxide Films with Artificial Eyes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12773">http://arxiv.org/abs/2311.12773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darshika Jauhari, Renu Sharma, Cunjian Chen, Nelson Sepulveda, Arun Ross</li>
<li>for: 这个论文旨在检测人工眼睛上的呈现攻击（presentation attack），以及这些攻击的影响于距离辐射识别系统（iris recognition system）的可靠性。</li>
<li>methods: 这篇论文使用了覆盖人工眼睛表面的 Vanadium Dioxide (VO2) Film，以调整距离辐射捕捉到的对象的光学特性。</li>
<li>results: 研究发现，在使用两种现状顶尖眼识别方法时，加上 VO2 Film 的人工眼睛可能会导致这些方法错分为真实的眼睛。这表示需要系统性的分析和有效地解决这种攻击。<details>
<summary>Abstract</summary>
Iris recognition systems, operating in the near infrared spectrum (NIR), have demonstrated vulnerability to presentation attacks, where an adversary uses artifacts such as cosmetic contact lenses, artificial eyes or printed iris images in order to circumvent the system. At the same time, a number of effective presentation attack detection (PAD) methods have been developed. These methods have demonstrated success in detecting artificial eyes (e.g., fake Van Dyke eyes) as presentation attacks. In this work, we seek to alter the optical characteristics of artificial eyes by affixing Vanadium Dioxide (VO2) films on their surface in various spatial configurations. VO2 films can be used to selectively transmit NIR light and can, therefore, be used to regulate the amount of NIR light from the object that is captured by the iris sensor. We study the impact of such images produced by the sensor on two state-of-the-art iris PA detection methods. We observe that the addition of VO2 films on the surface of artificial eyes can cause the PA detection methods to misclassify them as bonafide eyes in some cases. This represents a vulnerability that must be systematically analyzed and effectively addressed.
</details>
<details>
<summary>摘要</summary>
芳麻识别系统，在近红外谱（NIR）spectrum中运行，已经显示出对于投入攻击（presentation attacks）的感itivity。攻击者可以使用cosmetic contact lenses、人工眼或印刷眼膜来绕过系统。同时，一些有效的投入攻击检测（PAD）方法也已经开发出来。这些方法能够检测到人工眼（如假 Van Dyke 眼）作为投入攻击。在这项工作中，我们想要改变人工眼的光学特性，通过在其表面尝试VO2 films的不同空间配置。VO2 films可以选择性地传输NIR光，因此可以用来控制眼镜传感器接收的NIR光量。我们研究了这些由感器产生的图像对两种现代芳麻PA检测方法的影响。我们发现，在人工眼表面添加VO2 films可以导致PA检测方法在某些情况下错分为真实的眼睛。这表示了一个需要系统分析和解决的漏洞。
</details></li>
</ul>
<hr>
<h2 id="Swift-Parameter-free-Attention-Network-for-Efficient-Super-Resolution"><a href="#Swift-Parameter-free-Attention-Network-for-Efficient-Super-Resolution" class="headerlink" title="Swift Parameter-free Attention Network for Efficient Super-Resolution"></a>Swift Parameter-free Attention Network for Efficient Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12770">http://arxiv.org/abs/2311.12770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hongyuanyu/span">https://github.com/hongyuanyu/span</a></li>
<li>paper_authors: Cheng Wan, Hongyuan Yu, Zhiqi Li, Yihang Chen, Yajun Zou, Yuqing Liu, Xuanwu Yin, Kunlong Zuo</li>
<li>for: 提高单张超解像图像（SISR）任务中的计算效率，使用Parameter-free Attention Network（SPAN）来平衡参数数量、执行速度和图像质量。</li>
<li>methods: 使用新的参数自由注意机制，通过对称的活动函数和循环连接来增强高贡献信息并压抑 redundant信息。</li>
<li>results: 在多个benchmark上评估SPAN，显示其在图像质量和执行速度之间做出了显著的质速交换，并在NTIRE 2023高效超解像挑战中获得了最高PSNR值27.09 dB，同时降低了测试时间7.08ms。<details>
<summary>Abstract</summary>
Single Image Super-Resolution (SISR) is a crucial task in low-level computer vision, aiming to reconstruct high-resolution images from low-resolution counterparts. Conventional attention mechanisms have significantly improved SISR performance but often result in complex network structures and large number of parameters, leading to slow inference speed and large model size. To address this issue, we propose the Swift Parameter-free Attention Network (SPAN), a highly efficient SISR model that balances parameter count, inference speed, and image quality. SPAN employs a novel parameter-free attention mechanism, which leverages symmetric activation functions and residual connections to enhance high-contribution information and suppress redundant information. Our theoretical analysis demonstrates the effectiveness of this design in achieving the attention mechanism's purpose. We evaluate SPAN on multiple benchmarks, showing that it outperforms existing efficient super-resolution models in terms of both image quality and inference speed, achieving a significant quality-speed trade-off. This makes SPAN highly suitable for real-world applications, particularly in resource-constrained scenarios. Notably, our model attains the best PSNR of 27.09 dB, and the test runtime of our team is reduced by 7.08ms in the NTIRE 2023 efficient super-resolution challenge. Our code and models are made publicly available at \url{https://github.com/hongyuanyu/SPAN}.
</details>
<details>
<summary>摘要</summary>
Single Image Super-Resolution (SISR) 是计算机视觉领域中的一项重要任务，目的是从低分辨率图像中重construct高分辨率图像。传统的注意力机制有效提高了 SISR 性能，但常常导致复杂的网络结构和大量参数，从而导致慢速 inference 速度和大型模型。为解决这个问题，我们提出了 Swift Parameter-free Attention Network (SPAN)，一种高效的 SISR 模型，能够平衡参数数量、插入速度和图像质量。SPAN 使用了一种新的参数自由注意机制，通过对称的活动函数和径向连接来增强高贡献信息和抑制 redundancy 信息。我们的理论分析表明这种设计可以实现注意机制的目的。我们在多个标准测试上评估了 SPAN，并证明它在图像质量和插入速度之间实现了显著的质量-速度交换。这使得 SPAN 在实际应用中非常适合，特别是在有限资源的场景下。值得一提的是，我们的模型在 NTIRE 2023 年度高效超分辨计划中达到了最高 PSNR 值为 27.09 dB，并在测试时间上减少了 7.08ms。我们的代码和模型在 <https://github.com/hongyuanyu/SPAN> 上公开提供。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Weight-Perturbed-Deep-Neural-Networks-With-Application-in-Iris-Presentation-Attack-Detection"><a href="#Investigating-Weight-Perturbed-Deep-Neural-Networks-With-Application-in-Iris-Presentation-Attack-Detection" class="headerlink" title="Investigating Weight-Perturbed Deep Neural Networks With Application in Iris Presentation Attack Detection"></a>Investigating Weight-Perturbed Deep Neural Networks With Application in Iris Presentation Attack Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12764">http://arxiv.org/abs/2311.12764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/redwankarimsony/weightperturbation-msu">https://github.com/redwankarimsony/weightperturbation-msu</a></li>
<li>paper_authors: Renu Sharma, Redwan Sony, Arun Ross</li>
<li>for: 这篇论文的目的是分析深度神经网络（DNNs）对参数变化的敏感性，以便在实际应用中使用之前进行分析。</li>
<li>methods: 这篇论文使用了三种DNN架构（VGG、ResNet和DenseNet），以及三种参数变化方法（高斯噪声、重量归零和重量缩放）。它们在芳香检测任务中进行了实验，并使用了两个设定（整个网络和层级）。</li>
<li>results: 根据敏感性分析， authors提出了改进的模型，只需在网络参数中进行小 perturbation，而无需进行再训练。此外，他们还将这些perturbed模型 ensemble在分数级和参数级进行了提升，从而实现了模型的性能提升。 LivDet-Iris-2017 数据集上的平均提升率为43.58%，而 LivDet-Iris-2020 数据集上的提升率为9.25%。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) exhibit superior performance in various machine learning tasks, e.g., image classification, speech recognition, biometric recognition, object detection, etc. However, it is essential to analyze their sensitivity to parameter perturbations before deploying them in real-world applications. In this work, we assess the sensitivity of DNNs against perturbations to their weight and bias parameters. The sensitivity analysis involves three DNN architectures (VGG, ResNet, and DenseNet), three types of parameter perturbations (Gaussian noise, weight zeroing, and weight scaling), and two settings (entire network and layer-wise). We perform experiments in the context of iris presentation attack detection and evaluate on two publicly available datasets: LivDet-Iris-2017 and LivDet-Iris-2020. Based on the sensitivity analysis, we propose improved models simply by perturbing parameters of the network without undergoing training. We further combine these perturbed models at the score-level and at the parameter-level to improve the performance over the original model. The ensemble at the parameter-level shows an average improvement of 43.58% on the LivDet-Iris-2017 dataset and 9.25% on the LivDet-Iris-2020 dataset. The source code is available at \href{https://github.com/redwankarimsony/WeightPerturbation-MSU}{https://github.com/redwankarimsony/WeightPerturbation-MSU}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="High-resolution-Image-based-Malware-Classification-using-Multiple-Instance-Learning"><a href="#High-resolution-Image-based-Malware-Classification-using-Multiple-Instance-Learning" class="headerlink" title="High-resolution Image-based Malware Classification using Multiple Instance Learning"></a>High-resolution Image-based Malware Classification using Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12760">http://arxiv.org/abs/2311.12760</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timppeters/mil-malware-images">https://github.com/timppeters/mil-malware-images</a></li>
<li>paper_authors: Tim Peters, Hikmat Farhat</li>
<li>for: 防御 binary 扩大的恶意软件分类</li>
<li>methods: 使用高精度灰度图像和多实例学习来缺陷 binary 扩大</li>
<li>results: 实验表明，提议方法可以在对抗恶意软件分类中提高准确率，最高达 $96.6%$，比基准 $22.8%$ 高。<details>
<summary>Abstract</summary>
This paper proposes a novel method of classifying malware into families using high-resolution greyscale images and multiple instance learning to overcome adversarial binary enlargement. Current methods of visualisation-based malware classification largely rely on lossy transformations of inputs such as resizing to handle the large, variable-sized images. Through empirical analysis and experimentation, it is shown that these approaches cause crucial information loss that can be exploited. The proposed solution divides the images into patches and uses embedding-based multiple instance learning with a convolutional neural network and an attention aggregation function for classification. The implementation is evaluated on the Microsoft Malware Classification dataset and achieves accuracies of up to $96.6\%$ on adversarially enlarged samples compared to the baseline of $22.8\%$. The Python code is available online at https://github.com/timppeters/MIL-Malware-Images .
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种新的恶意软件分类方法，使用高分辨率灰度图像和多实例学习来超越对恶意 binary 扩大的攻击。现有的视觉化基于识别方法主要通过lossy变换输入来处理大小可变的图像。经验表明，这些方法会导致重要信息的丢失，可以被利用。该提案将图像分成块，使用嵌入式多实例学习的 convolutional neural network 和注意聚合函数进行分类。实现在 Microsoft Malware Classification 数据集上进行评估，可以达到 $96.6\%$ 的准确率，比基准 $22.8\%$ 高出许多。Python 代码可以在 https://github.com/timppeters/MIL-Malware-Images 上下载。
</details></li>
</ul>
<hr>
<h2 id="Towards-Natural-Language-Guided-Drones-GeoText-1652-Benchmark-with-Spatially-Relation-Matching"><a href="#Towards-Natural-Language-Guided-Drones-GeoText-1652-Benchmark-with-Spatially-Relation-Matching" class="headerlink" title="Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatially Relation Matching"></a>Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatially Relation Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12751">http://arxiv.org/abs/2311.12751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Chu, Zhedong Zheng, Wei Ji, Tat-Seng Chua</li>
<li>for: 提高飞行器控制和导航的自然语言指令集成</li>
<li>methods: 利用大型自然语言模型生成数据集，并与预训练视觉模型结合使用，以实现精细的图像文本框架对应。</li>
<li>results: 提出了一个新的优化目标，即缓冲空间匹配，以便在区域级别上进行精细的空间关系匹配。实验结果表明，该方法在不同的描述复杂度下保持了Exceptional recall rate。<details>
<summary>Abstract</summary>
Drone navigation through natural language commands remains a significant challenge due to the lack of publicly available multi-modal datasets and the intricate demands of fine-grained visual-text alignment. In response to this pressing need, we present a new human-computer interaction annotation benchmark called GeoText-1652, meticulously curated through a robust Large Language Model (LLM)-based data generation framework and the expertise of pre-trained vision models. This new dataset seamlessly extends the existing image dataset, \ie, University-1652, with spatial-aware text annotations, encompassing intricate image-text-bounding box associations. Besides, we introduce a new optimization objective to leverage fine-grained spatial associations, called blending spatial matching, for region-level spatial relation matching. Extensive experiments reveal that our approach maintains an exceptional recall rate under varying description complexities. This underscores the promising potential of our approach in elevating drone control and navigation through the seamless integration of natural language commands in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>游戏机器人导航透过自然语言命令仍然是一个重要的挑战，主要因为缺乏公开的多modal资料集和细部的视觉文本对铺设。为解决这个紧迫需求，我们提出了一个新的人机交互标准参考集called GeoText-1652，通过一个可靠的大型自然语言模型（LLM）基础的数据生成框架和预训的视觉模型来实现。这个新标准集扩展了现有的图像集，即University-1652， adds spatial-aware text annotations，包括细部的图像-文本 bounding box 协调。此外，我们引入了一个新的优化目标，叫做混合空间匹配，以利用细部的空间关联。实验结果表明，我们的方法可以在不同的描述复杂性下保持高的精度率。这说明了我们的方法在实际应用中对游戏机器人导航和控制的潜在应用非常高。
</details></li>
</ul>
<hr>
<h2 id="Attacking-Motion-Planners-Using-Adversarial-Perception-Errors"><a href="#Attacking-Motion-Planners-Using-Adversarial-Perception-Errors" class="headerlink" title="Attacking Motion Planners Using Adversarial Perception Errors"></a>Attacking Motion Planners Using Adversarial Perception Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12722">http://arxiv.org/abs/2311.12722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Sadeghi, Nicholas A. Lord, John Redford, Romain Mueller</li>
<li>for: 本文旨在探讨自动驾驶（AD）系统的模块化设计和测试方法，以及测试结果的解释。</li>
<li>methods: 本文使用了一种简单的边界攻击算法，可以系统地构造出识别质量高但导致 плани策略失败的输入。</li>
<li>results: 研究人员使用了这种算法在CARLA simulateur上测试了两个黑盒计划器，并在城市和高速公路驾驶场景中发现了许多攻击。这些攻击是隔离在计划器输入空间中的，并且对自动驾驶系统部署和测试有重要的含义。<details>
<summary>Abstract</summary>
Autonomous driving (AD) systems are often built and tested in a modular fashion, where the performance of different modules is measured using task-specific metrics. These metrics should be chosen so as to capture the downstream impact of each module and the performance of the system as a whole. For example, high perception quality should enable prediction and planning to be performed safely. Even though this is true in general, we show here that it is possible to construct planner inputs that score very highly on various perception quality metrics but still lead to planning failures. In an analogy to adversarial attacks on image classifiers, we call such inputs \textbf{adversarial perception errors} and show they can be systematically constructed using a simple boundary-attack algorithm. We demonstrate the effectiveness of this algorithm by finding attacks for two different black-box planners in several urban and highway driving scenarios using the CARLA simulator. Finally, we analyse the properties of these attacks and show that they are isolated in the input space of the planner, and discuss their implications for AD system deployment and testing.
</details>
<details>
<summary>摘要</summary>
自主驾驶（AD）系统通常以模块化的方式设计和测试，其中不同模块的性能通过任务特定的 метри来进行评估。这些 метри应选择以捕捉下游影响和整个系统性能。例如，高识别质量可以确保安全地进行预测和规划。虽然这是通常的情况，但我们在这里示出了可以通过简单的边界攻击算法构造出具有高识别质量метри但导致规划失败的输入。我们称这些输入为“对抗识别错误”，并证明它们可以通过一种简单的边界攻击算法系统地构造。我们使用 CARLA  simulateur 在城市和高速公路上驾驶场景中对两种黑盒规划器进行了多个攻击检测。最后，我们分析了这些攻击的性质并证明它们在输入空间中孤立，并讨论它们对自动驾驶系统部署和测试的影响。
</details></li>
</ul>
<hr>
<h2 id="Cascade-Learning-Localises-Discriminant-Features-in-Visual-Scene-Classification"><a href="#Cascade-Learning-Localises-Discriminant-Features-in-Visual-Scene-Classification" class="headerlink" title="Cascade Learning Localises Discriminant Features in Visual Scene Classification"></a>Cascade Learning Localises Discriminant Features in Visual Scene Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12704">http://arxiv.org/abs/2311.12704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwen Wang, Katayoun Farrahi</li>
<li>for: 本研究旨在解决深度卷积神经网络（DCNN）在医学领域的解释性不足问题，特别是临床医生希望得到可信worthy的自动化决策。</li>
<li>methods: 本研究使用两种不同的学习方法，分别是传统的端到端（E2E）学习和层次学习（CL），以解析DCNN中特征表示的本地化性。</li>
<li>results: 我们的分析表明，E2E学习策略在多层网络层中具有有限的本地化特征表示能力，而CL学习策略则能够更好地本地化特征表示。我们的结果还表明，在YOLO对象检测框架上，CL学习策略比E2E策略高$2%$的准确率。<details>
<summary>Abstract</summary>
Lack of interpretability of deep convolutional neural networks (DCNN) is a well-known problem particularly in the medical domain as clinicians want trustworthy automated decisions. One way to improve trust is to demonstrate the localisation of feature representations with respect to expert labeled regions of interest. In this work, we investigate the localisation of features learned via two varied learning paradigms and demonstrate the superiority of one learning approach with respect to localisation. Our analysis on medical and natural datasets show that the traditional end-to-end (E2E) learning strategy has a limited ability to localise discriminative features across multiple network layers. We show that a layer-wise learning strategy, namely cascade learning (CL), results in more localised features. Considering localisation accuracy, we not only show that CL outperforms E2E but that it is a promising method of predicting regions. On the YOLO object detection framework, our best result shows that CL outperforms the E2E scheme by $2\%$ in mAP.
</details>
<details>
<summary>摘要</summary>
深度卷积神经网络（DCNN）的解释性不足是医疗领域中一个公认的问题，因为临床医生希望得到可靠的自动化决策。为了提高信任度，我们可以证明特征表示的地方化。在这项工作中，我们研究了DCNN学习两种不同的学习模式下特征的地方化，并证明一种学习方法在地方化方面具有优势。我们对医疗和自然数据集进行分析，发现传统的端到端（E2E）学习策略在多层网络中具有局部特征的局限性。我们显示，层wise学习策略（cascade learning，CL）可以更好地地方化特征。基于地方化精度，我们不仅表明CL exceeds E2E，而且可以有效预测区域。在YOLO对象检测框架上，我们最佳结果表明CL比E2E方案高$2\%$的射频精度。
</details></li>
</ul>
<hr>
<h2 id="Transferring-to-Real-World-Layouts-A-Depth-aware-Framework-for-Scene-Adaptation"><a href="#Transferring-to-Real-World-Layouts-A-Depth-aware-Framework-for-Scene-Adaptation" class="headerlink" title="Transferring to Real-World Layouts: A Depth-aware Framework for Scene Adaptation"></a>Transferring to Real-World Layouts: A Depth-aware Framework for Scene Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12682">http://arxiv.org/abs/2311.12682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mu Chen, Zhedong Zheng, Yi Yang<br>for: 本文主要目标是提出一种深度感知的频率预处理方法，以便在无监督领域适应（UDA）中转移知识，从源频率数据中学习到目标频率数据中，以避免手动标注 pixels。methods: 本文提出了一种depth-aware框架，包括一个深度引导的上下文滤波（DCF）和一个交叉任务编码器。DCF通过模拟真实的世界布局来扩大数据集，而交叉任务编码器则进一步适应了两个补做任务之间的相互作用。此外， authors还使用了公共数据集中没有深度标注，因此他们使用了市场上的深度估计网络来生成假深度。results: 对两个广泛使用的标准 bench-mark 进行了广泛的实验，结果表明， authors 的提出的方法，即使使用假深度，仍能在 GTA 到 Cityscapes 和 Synthia 到 Cityscapes 上达到竞争性表现，具体来说是 77.7 mIoU 和 69.3 mIoU。<details>
<summary>Abstract</summary>
Scene segmentation via unsupervised domain adaptation (UDA) enables the transfer of knowledge acquired from source synthetic data to real-world target data, which largely reduces the need for manual pixel-level annotations in the target domain. To facilitate domain-invariant feature learning, existing methods typically mix data from both the source domain and target domain by simply copying and pasting the pixels. Such vanilla methods are usually sub-optimal since they do not take into account how well the mixed layouts correspond to real-world scenarios. Real-world scenarios are with an inherent layout. We observe that semantic categories, such as sidewalks, buildings, and sky, display relatively consistent depth distributions, and could be clearly distinguished in a depth map. Based on such observation, we propose a depth-aware framework to explicitly leverage depth estimation to mix the categories and facilitate the two complementary tasks, i.e., segmentation and depth learning in an end-to-end manner. In particular, the framework contains a Depth-guided Contextual Filter (DCF) forndata augmentation and a cross-task encoder for contextual learning. DCF simulates the real-world layouts, while the cross-task encoder further adaptively fuses the complementing features between two tasks. Besides, it is worth noting that several public datasets do not provide depth annotation. Therefore, we leverage the off-the-shelf depth estimation network to generate the pseudo depth. Extensive experiments show that our proposed methods, even with pseudo depth, achieve competitive performance on two widely-used bench-marks, i.e. 77.7 mIoU on GTA to Cityscapes and 69.3 mIoU on Synthia to Cityscapes.
</details>
<details>
<summary>摘要</summary>
We observe that semantic categories such as sidewalks, buildings, and sky have consistent depth distributions and can be distinguished in a depth map. Based on this observation, we propose a depth-aware framework that leverages depth estimation to mix categories and perform segmentation and depth learning in an end-to-end manner. The framework includes a Depth-guided Contextual Filter (DCF) for data augmentation and a cross-task encoder for contextual learning. DCF simulates real-world layouts, while the cross-task encoder adaptively fuses complementary features between the two tasks.As some public datasets do not provide depth annotations, we use an off-the-shelf depth estimation network to generate pseudo depth. Our proposed method, even with pseudo depth, achieves competitive performance on two widely-used benchmarks, with a mIoU of 77.7 on GTA to Cityscapes and 69.3 on Synthia to Cityscapes.
</details></li>
</ul>
<hr>
<h2 id="BundleMoCap-Efficient-Robust-and-Smooth-Motion-Capture-from-Sparse-Multiview-Videos"><a href="#BundleMoCap-Efficient-Robust-and-Smooth-Motion-Capture-from-Sparse-Multiview-Videos" class="headerlink" title="BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos"></a>BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12679">http://arxiv.org/abs/2311.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgios Albanis, Nikolaos Zioulis, Kostas Kolomvatsos</li>
<li>for: 这篇论文的目的是提出一种高效且简单的 markerless 动作捕捉方法，以解决现有的方法具有较高计算成本和多个阶段的问题。</li>
<li>methods: 该方法基于 manifold interpolation between latent keyframes，不需要时间约束和多个阶段的数据驱动回归和优化，可以高效地解决动作捕捉问题。</li>
<li>results: 对比于现有的状态对技术，BundleMoCap 能够达到高质量的动作捕捉结果，而无需增加复杂度。更多细节可以参考 <a target="_blank" rel="noopener" href="https://moverseai.github.io/bundle/%E3%80%82">https://moverseai.github.io/bundle/。</a><details>
<summary>Abstract</summary>
Capturing smooth motions from videos using markerless techniques typically involves complex processes such as temporal constraints, multiple stages with data-driven regression and optimization, and bundle solving over temporal windows. These processes can be inefficient and require tuning multiple objectives across stages. In contrast, BundleMoCap introduces a novel and efficient approach to this problem. It solves the motion capture task in a single stage, eliminating the need for temporal smoothness objectives while still delivering smooth motions. BundleMoCap outperforms the state-of-the-art without increasing complexity. The key concept behind BundleMoCap is manifold interpolation between latent keyframes. By relying on a local manifold smoothness assumption, we can efficiently solve a bundle of frames using a single code. Additionally, the method can be implemented as a sliding window optimization and requires only the first frame to be properly initialized, reducing the overall computational burden. BundleMoCap's strength lies in its ability to achieve high-quality motion capture results with simplicity and efficiency. More details can be found at https://moverseai.github.io/bundle/.
</details>
<details>
<summary>摘要</summary>
capturing smooth motions from videos using markerless techniques 通常需要复杂的过程，例如时间约束、多个阶段的数据驱动回归和优化、和时间窗口内的集合解决。这些过程可能是不效率的并需要在不同阶段调整多个目标。相比之下，BundleMoCap 引入了一种新的和高效的解决方案。它在单个阶段内解决动作捕捉任务，消除了时间稳定性目标，同时仍能提供平滑的动作。BundleMoCap 超越了当前状态的性能，而不需增加复杂性。BundleMoCap 的关键思想是在 latent keyframes 之间进行抽象 interpolating。通过假设本地满足性，我们可以高效地解决一个包含多帧的缓存。此外，该方法可以作为滑块窗口优化进行实现，只需要初始化第一帧，从而减少总计算负担。BundleMoCap 的优势在于它可以通过简单和高效的方式实现高质量的动作捕捉结果。更多细节可以在 <https://moverseai.github.io/bundle/> 找到。
</details></li>
</ul>
<hr>
<h2 id="Similar-Document-Template-Matching-Algorithm"><a href="#Similar-Document-Template-Matching-Algorithm" class="headerlink" title="Similar Document Template Matching Algorithm"></a>Similar Document Template Matching Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12663">http://arxiv.org/abs/2311.12663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshitha Yenigalla, Bommareddy Revanth Srinivasa Reddy, Batta Venkata Rahul, Nannapuraju Hemanth Raju</li>
<li>for: 这个研究旨在提供一种涵盖模板EXTRACTION、比较和诈骗检测的全面方法，以验证医疗文档。</li>
<li>methods: 该方法包括使用高级技术进行模板EXTRACTION，包括区域特征分析和边缘识别，以确保模板的清晰度。然后是模板比较算法，使用高级特征匹配和描述符，以增强比较的可靠性。诈骗检测包括计算SSIM量和OCR技术，以EXTRACT文本信息。</li>
<li>results: 该方法可以提供可靠的医疗文档验证，Addressing Complexities in template EXTRACTION、比较和诈骗检测，并且可以适应不同的文档结构。<details>
<summary>Abstract</summary>
This study outlines a comprehensive methodology for verifying medical documents, integrating advanced techniques in template extraction, comparison, and fraud detection. It begins with template extraction using sophisticated region-of-interest (ROI) methods, incorporating contour analysis and edge identification. Pre-processing steps ensure template clarity through morphological operations and adaptive thresholding. The template comparison algorithm utilizes advanced feature matching with key points and descriptors, enhancing robustness through histogram-based analysis for accounting variations. Fraud detection involves the SSIM computation and OCR for textual information extraction. The SSIM quantifies structural similarity, aiding in potential match identification. OCR focuses on critical areas like patient details, provider information, and billing amounts. Extracted information is compared with a reference dataset, and confidence thresholding ensures reliable fraud detection. Adaptive parameters enhance system flexibility for dynamic adjustments to varying document layouts. This methodology provides a robust approach to medical document verification, addressing complexities in template extraction, comparison, fraud detection, and adaptability to diverse document structures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Template extraction using sophisticated ROI methods, including contour analysis and edge identification, to ensure template clarity.2. Pre-processing steps such as morphological operations and adaptive thresholding to enhance template quality.3. Template comparison using advanced feature matching with key points and descriptors, followed by histogram-based analysis to account for variations.4. Fraud detection involving the computation of SSIM (Structural Similarity Index Measure) and OCR (Optical Character Recognition) for textual information extraction.5. Comparison of extracted information with a reference dataset, and confidence thresholding to ensure reliable fraud detection.6. Adaptive parameters to enhance system flexibility and allow for dynamic adjustments to varying document layouts.This methodology provides a robust approach to medical document verification, addressing the complexities in template extraction, comparison, fraud detection, and adaptability to diverse document structures.</details></li>
</ol>
<hr>
<h2 id="Visually-Guided-Object-Grasping"><a href="#Visually-Guided-Object-Grasping" class="headerlink" title="Visually Guided Object Grasping"></a>Visually Guided Object Grasping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12660">http://arxiv.org/abs/2311.12660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Radu Horaud, Fadi Dornaika, Bernard Espiau</li>
<li>for: 本研究旨在提出一种视觉控制方法，用于对物体抓取和精确对接两个实体的问题。</li>
<li>methods: 本方法基于Espiau等人提出的方法，并在摄像头不安装在控制机器人上的情况下进行扩展。我们强调实时计算图形Jacobian的重要性。</li>
<li>results: 我们表示了一种用于描述三维对象的抓取或对接的3D对应空间表示方法，使用不准备 camera参数的单镜照相机。这种3D对应空间表示方法是视觉无关的，可以轻松地将图像集成为准确的设定。此外，我们还进行了视觉控制算法的性能分析和抓取精度的预测。<details>
<summary>Abstract</summary>
In this paper we present a visual servoing approach to the problem of object grasping and more generally, to the problem of aligning an end-effector with an object. First we extend the method proposed by Espiau et al. [1] to the case of a camera which is not mounted onto the robot being controlled and we stress the importance of the real-time estimation of the image Jacobian. Second, we show how to represent a grasp or more generally, an alignment between two solids in 3-D projective space using an uncalibrated stereo rig. Such a 3-D projective representation is view-invariant in the sense that it can be easily mapped into an image set-point without any knowledge about the camera parameters. Third, we perform an analysis of the performances of the visual servoing algorithm and of the grasping precision that can be expected from this type of approach.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种视Servoing方法来解决物体抓取和更广泛的对着器与物体的对齐问题。首先，我们对Espiau等人的方法进行了扩展，并在Camera不 mounted onto the robot being controlled的情况下进行了详细的讨论。其次，我们示出了如何使用无加工stereo rig来表示一个抓或更广泛的对两个物体的对齐在3D项目空间中。这种3D项目空间表示是视点不变的，可以无需了解相机参数直接映射到图像集点。最后，我们进行了视Servoing算法的性能分析和抓取精度的预期分析。
</details></li>
</ul>
<hr>
<h2 id="Hand-Eye-Calibration"><a href="#Hand-Eye-Calibration" class="headerlink" title="Hand-Eye Calibration"></a>Hand-Eye Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12655">http://arxiv.org/abs/2311.12655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IFL-CAMP/easy_handeye">https://github.com/IFL-CAMP/easy_handeye</a></li>
<li>paper_authors: Radu Horaud, Fadi Dornaika</li>
<li>for: 这篇论文是关于机器人手上的感器尺度准确性问题的研究。</li>
<li>methods: 该论文提出了两种解决手机器人尺度准确性问题的方法：一种是将摄像头视角矩阵与机器人手的尺度矩阵相multiply，另一种是使用非线性优化方法同时解决摄像头视角和机器人手尺度的问题。</li>
<li>results: 该论文通过稳定性分析，发现非线性优化方法比线性方法更加稳定，可以更好地抵制噪声和测量误差。<details>
<summary>Abstract</summary>
Whenever a sensor is mounted on a robot hand it is important to know the relationship between the sensor and the hand. The problem of determining this relationship is referred to as hand-eye calibration, which is important in at least two types of tasks: (i) map sensor centered measurements into the robot workspace and (ii) allow the robot to precisely move the sensor. In the past some solutions were proposed in the particular case of a camera. With almost no exception, all existing solutions attempt to solve the homogeneous matrix equation AX=XB. First we show that there are two possible formulations of the hand-eye calibration problem. One formulation is the classical one that we just mentioned. A second formulation takes the form of the following homogeneous matrix equation: MY=M'YB. The advantage of the latter is that the extrinsic and intrinsic camera parameters need not be made explicit. Indeed, this formulation directly uses the 3 by 4 perspective matrices (M and M') associated with two positions of the camera. Moreover, this formulation together with the classical one cover a wider range of camera-based sensors to be calibrated with respect to the robot hand. Second, we develop a common mathematical framework to solve for the hand-eye calibration problem using either of the two formulations. We present two methods, (i) a rotation then translation and (ii) a non-linear solver for rotation and translation. Third, we perform a stability analysis both for our two methods and for the classical linear method developed. In the light of this comparison, the non-linear optimization method, that solves for rotation and translation simultaneously, seems to be the most robust one with respect to noise and to measurement errors.
</details>
<details>
<summary>摘要</summary>
当感测器安装在机器人手上时，必须了解感测器和手的关系。这个问题被称为手眼准备，它在至少两种任务中很重要：（i）将感测器中的测量转换到机器人工作空间中，（ii）让机器人准确地移动感测器。在过去，一些解决方案在特定情况下使用摄像头。大多数现有的解决方案都尝试解决homogeneous matrix equation AX=XB。我们首先表明了手眼准备问题的两种可能的形式化。一种形式是我们已经提到的经典形式。另一种形式为以下homogeneous matrix equation：MY=M'YB。这种形式的优点在于感测器和机器人手之间的外部和内部参数不需要显式地指定。实际上，这种形式直接使用机器人手上的3x4 perspective matrices（M和M'），它们与摄像头的两个位置相关。此外，这种形式与经典形式共同覆盖了更多的摄像头基于的感测器需要与机器人手进行准备。二、我们开发了一个共同的数学框架来解决手眼准备问题，使用任一种形式化。我们提出了两种方法：（i）先转换 rotation then translation，（ii）非线性优化方法。三、我们进行了稳定性分析，包括我们两种方法和经典线性方法。在这种比较下，非线性优化方法，同时解决 rotation和 translation，显示对噪声和测量错误更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Polyhedral-Object-Recognition-by-Indexing"><a href="#Polyhedral-Object-Recognition-by-Indexing" class="headerlink" title="Polyhedral Object Recognition by Indexing"></a>Polyhedral Object Recognition by Indexing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12641">http://arxiv.org/abs/2311.12641</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/crowdbotics-dev/abd-231102-s1-dev-126419">https://github.com/crowdbotics-dev/abd-231102-s1-dev-126419</a></li>
<li>paper_authors: Radu Horaud, Humberto Sossa</li>
<li>for: 本文解决了计算机视觉中的图像索引问题，即从2D图像中识别3D多面体对象。</li>
<li>methods: 本文提出了一种基于多项式特征化和哈希函数的图像索引方法，用于识别多面体对象。</li>
<li>results: 实验结果表明，该方法可以快速和准确地识别多面体对象从2D图像中。<details>
<summary>Abstract</summary>
In computer vision, the indexing problem is the problem of recognizing a few objects in a large database of objects while avoiding the help of the classical image-feature-to-object-feature matching paradigm. In this paper we address the problem of recognizing 3-D polyhedral objects from 2-D images by indexing. Both the objects to be recognized and the images are represented by weighted graphs. The indexing problem is therefore the problem of determining whether a graph extracted from the image is present or absent in a database of model graphs. We introduce a novel method for performing this graph indexing process which is based both on polynomial characterization of binary and weighted graphs and on hashing. We describe in detail this polynomial characterization and then we show how it can be used in the context of polyhedral object recognition. Next we describe a practical recognition-by-indexing system that includes the organization of the database, the representation of polyhedral objects in terms of 2-D characteristic views, the representation of this views in terms of weighted graphs, and the associated image processing. Finally, some experimental results allow the evaluation of the system performance.
</details>
<details>
<summary>摘要</summary>
To solve this problem, we propose a novel method that combines polynomial characterization of binary and weighted graphs with hashing. We provide a detailed description of this polynomial characterization and demonstrate how it can be applied in the context of polyhedral object recognition.Our practical recognition-by-indexing system includes the organization of the database, the representation of polyhedral objects in terms of 2D characteristic views, the representation of these views as weighted graphs, and the associated image processing. We also present experimental results to evaluate the performance of our system.
</details></li>
</ul>
<hr>
<h2 id="GPT4Motion-Scripting-Physical-Motions-in-Text-to-Video-Generation-via-Blender-Oriented-GPT-Planning"><a href="#GPT4Motion-Scripting-Physical-Motions-in-Text-to-Video-Generation-via-Blender-Oriented-GPT-Planning" class="headerlink" title="GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning"></a>GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12631">http://arxiv.org/abs/2311.12631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, Shifeng Chen</li>
<li>for: 提高文本至视频生成质量，增强视频的物理运动准确性和实体一致性。</li>
<li>methods: 利用大语言模型GPT-4生成Blender脚本，基于文本提示 commanded Blender的物理引擎生成基本场景元素，然后通过稳定扩散来生成视频。</li>
<li>results: 在三种基本物理运动场景中（块体落下与碰撞、布料摆倒与摆动、液体流动），GPT4Motion可以高效生成高质量视频，保持运动准确性和实体一致性。<details>
<summary>Abstract</summary>
Recent advances in text-to-video generation have harnessed the power of diffusion models to create visually compelling content conditioned on text prompts. However, they usually encounter high computational costs and often struggle to produce videos with coherent physical motions. To tackle these issues, we propose GPT4Motion, a training-free framework that leverages the planning capability of large language models such as GPT, the physical simulation strength of Blender, and the excellent image generation ability of text-to-image diffusion models to enhance the quality of video synthesis. Specifically, GPT4Motion employs GPT-4 to generate a Blender script based on a user textual prompt, which commands Blender's built-in physics engine to craft fundamental scene components that encapsulate coherent physical motions across frames. Then these components are inputted into Stable Diffusion to generate a video aligned with the textual prompt. Experimental results on three basic physical motion scenarios, including rigid object drop and collision, cloth draping and swinging, and liquid flow, demonstrate that GPT4Motion can generate high-quality videos efficiently in maintaining motion coherency and entity consistency. GPT4Motion offers new insights in text-to-video research, enhancing its quality and broadening its horizon for future explorations.
</details>
<details>
<summary>摘要</summary>
最近的文本到视频生成技术发展，利用了扩散模型来创造基于文本提示的视觉吸引人的内容。然而，它们通常会遇到高计算成本，并且往往难以生成具有连贯的物理动作的视频。为了解决这些问题，我们提出了GPT4Motion，一个无需训练的框架，利用大型自然语言模型 such as GPT的规划能力，Blender的物理渲染力和文本到图像扩散模型的优秀图像生成能力，以提高视频合成的质量。具体来说，GPT4Motion使用GPT-4生成基于用户文本提示的Blender脚本，这个脚本命令Blender的内置物理引擎来生成基于文本提示的场景元素，这些元素包括连续的物理动作。然后，这些元素被输入到稳定扩散模型中，以生成与文本提示相对应的视频。实验结果表明，GPT4Motion可以高效地生成高质量的视频，同时保持动作准确性和实体一致性。GPT4Motion为文本到视频研究提供了新的突破，提高了其质量和扩展了未来的探索领域。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Generalization-Gaps-in-High-Content-Imaging-Through-Online-Self-Supervised-Domain-Adaptation"><a href="#Bridging-Generalization-Gaps-in-High-Content-Imaging-Through-Online-Self-Supervised-Domain-Adaptation" class="headerlink" title="Bridging Generalization Gaps in High Content Imaging Through Online Self-Supervised Domain Adaptation"></a>Bridging Generalization Gaps in High Content Imaging Through Online Self-Supervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12623">http://arxiv.org/abs/2311.12623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Fredin Haslum, Christos Matsoukas, Karl-Johan Leuchowius, Kevin Smith</li>
<li>for: This paper aims to address the challenges of applying machine learning models to high content imaging (HCI) datasets in drug discovery and development pipelines, specifically the problem of domain shift due to differences in imaging equipment.</li>
<li>methods: The proposed method, CODA, is an online self-supervised domain adaptation approach that separates the classifier into a generic feature extractor and a task-specific model. CODA adapts the feature extractor’s weights to the new domain using cross-batch self-supervision, while keeping the task-specific model unchanged.</li>
<li>results: The results show that CODA significantly reduces the generalization gap, achieving up to a 300% improvement when applied to data from different labs utilizing different microscopes. CODA can be applied to new, unlabeled out-of-domain data sources of different sizes, from a single plate to multiple experimental batches.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是解决应用机器学习模型到高内容成像（HCI）数据集中的问题，具体是因为不同的探针设备而导致的领域shift。</li>
<li>methods: 提议的方法是CODA，它是一种在线自动适应领域方法，将分类器分为通用特征提取器和任务特定模型。CODA将通用特征提取器的参数适应新领域使用交叉批自监督，保持任务特定模型不变。</li>
<li>results: 结果表明，CODA可以显著减少总体差距，在不同实验室使用不同探针的数据上达到300%的提升。CODA可以应用到新、无标签的外域数据源，不同的大小，从单个板到多个实验批。<details>
<summary>Abstract</summary>
High Content Imaging (HCI) plays a vital role in modern drug discovery and development pipelines, facilitating various stages from hit identification to candidate drug characterization. Applying machine learning models to these datasets can prove challenging as they typically consist of multiple batches, affected by experimental variation, especially if different imaging equipment have been used. Moreover, as new data arrive, it is preferable that they are analyzed in an online fashion. To overcome this, we propose CODA, an online self-supervised domain adaptation approach. CODA divides the classifier's role into a generic feature extractor and a task-specific model. We adapt the feature extractor's weights to the new domain using cross-batch self-supervision while keeping the task-specific model unchanged. Our results demonstrate that this strategy significantly reduces the generalization gap, achieving up to a 300% improvement when applied to data from different labs utilizing different microscopes. CODA can be applied to new, unlabeled out-of-domain data sources of different sizes, from a single plate to multiple experimental batches.
</details>
<details>
<summary>摘要</summary>
高内容成像（HCI）在现代药物发现和开发流水线中扮演着重要的角色，涵盖各个阶段，从发现到候选药物特性化。在应用机器学习模型时，这些数据集通常包含多个批次，受到实验室内外部变量的影响，特别是使用不同的成像设备时。此外，随着新数据的到达，感知是在线模式下进行分析的。为解决这个问题，我们提出了CODA，一种在线无监督领域适应方法。CODA将分类器的角色分为通用特征提取器和任务特定模型。我们使用交叉批次自监督来适应特征提取器的加载，保持任务特定模型不变。我们的结果显示，这种策略可以减少泛化差距，达到300%的提高，当应用于不同实验室使用不同显微镜的数据时。CODA可以应用于新、无监督的外域数据源，不同的大小，从单个板到多个实验批次。
</details></li>
</ul>
<hr>
<h2 id="Crowd-management-crime-detection-work-monitoring-using-aiml"><a href="#Crowd-management-crime-detection-work-monitoring-using-aiml" class="headerlink" title="Crowd management, crime detection, work monitoring using aiml"></a>Crowd management, crime detection, work monitoring using aiml</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12621">http://arxiv.org/abs/2311.12621</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. R. Adithya, Dheepak. S, B. Akash, Harshini. V, Sai Lakshana</li>
<li>for: 这项研究旨在利用现有的关闭电视（CCTV）网络，实现严格的人群管理、犯罪预防和工作场所监测，通过人工智能（AI）和机器学习（ML）技术的集成。</li>
<li>methods: 本研究使用了AI&#x2F;ML技术进行实时视频分析，以识别和评估人群动态、早期发现可能的犯罪活动，并持续监测工作环境。</li>
<li>results: 本研究通过实时视频分析，提高了公共安全措施和组织Productivity，并且可以在现有基础设施上进行改进，无需重大改进系统。<details>
<summary>Abstract</summary>
This research endeavors to harness the potential of existing Closed-Circuit Television (CCTV) networks for a comprehensive approach to crowd management, crime prevention, and workplace monitoring through the integration of Artificial Intelligence (AI) and Machine Learning (ML) technologies. The primary objective is to develop and implement advanced algorithms capable of real-time analysis of video feeds, enabling the identification and assessment of crowd dynamics, early detection of potential criminal activities, and continuous monitoring of workplace environments. By leveraging AI/ML, the project aims to optimize surveillance capabilities, thereby enhancing public safety measures and improving organizational productivity. This initiative underscores the transformative impact that intelligent video analytics can have on existing infrastructure, mitigating the need for extensive system overhauls while significantly advancing security and operational efficiency.
</details>
<details>
<summary>摘要</summary>
这项研究目标是利用现有的关闭回路电视（CCTV）网络来实现全面的人群管理、犯罪预防和工作场所监测，通过人工智能（AI）和机器学习（ML）技术的集成。项目的 PRIMARY 目标是开发并实施实时视频分析算法，以识别和评估人群动态、早期发现可能的犯罪活动和不断监测工作环境。通过利用 AI/ML，项目旨在提高Surveillance  capabilities，以提高公共安全措施和组织效率。该 initative  highlights  the transformative impact that intelligent video analytics can have on existing infrastructure， mitigating the need for extensive system overhauls while significantly advancing security and operational efficiency.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Unlabeled-Data-for-3D-Medical-Image-Segmentation-through-Self-Supervised-Contrastive-Learning"><a href="#Leveraging-Unlabeled-Data-for-3D-Medical-Image-Segmentation-through-Self-Supervised-Contrastive-Learning" class="headerlink" title="Leveraging Unlabeled Data for 3D Medical Image Segmentation through Self-Supervised Contrastive Learning"></a>Leveraging Unlabeled Data for 3D Medical Image Segmentation through Self-Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12617">http://arxiv.org/abs/2311.12617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanaz Karimijafarbigloo, Reza Azad, Yury Velichko, Ulas Bagci, Dorit Merhof<br>for: 这篇研究目的是提出一种基于semi-supervised learning的3D图像分割方法，以解决现有方法忽略Contextual information和生成可靠pseudo-labels的问题。methods: 方法包括两个独立的子网络，用于探索和利用预测结果的不一致。具体来说，我们在预测结果不一致的地方进行了定向验证训练 процесс，以 corrected erroneous prediction results。此外，我们还使用了一种自动调整的contrastive learning方法，以提高网络的表达能力和预测uncertainty的降低。results: 我们通过对脑血管 segmentation task进行实验，使用了临床MRI和CT扫描数据，并证明了我们的方法的效果，比起现有方法更高。codebase可以在 \href{<a target="_blank" rel="noopener" href="https://github.com/xmindflow/SSL-contrastive%7D%7BGitHub%7D%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xmindflow/SSL-contrastive}{GitHub}上找到。</a><details>
<summary>Abstract</summary>
Current 3D semi-supervised segmentation methods face significant challenges such as limited consideration of contextual information and the inability to generate reliable pseudo-labels for effective unsupervised data use. To address these challenges, we introduce two distinct subnetworks designed to explore and exploit the discrepancies between them, ultimately correcting the erroneous prediction results. More specifically, we identify regions of inconsistent predictions and initiate a targeted verification training process. This procedure strategically fine-tunes and harmonizes the predictions of the subnetworks, leading to enhanced utilization of contextual information. Furthermore, to adaptively fine-tune the network's representational capacity and reduce prediction uncertainty, we employ a self-supervised contrastive learning paradigm. For this, we use the network's confidence to distinguish between reliable and unreliable predictions. The model is then trained to effectively minimize unreliable predictions. Our experimental results for organ segmentation, obtained from clinical MRI and CT scans, demonstrate the effectiveness of our approach when compared to state-of-the-art methods. The codebase is accessible on \href{https://github.com/xmindflow/SSL-contrastive}{GitHub}.
</details>
<details>
<summary>摘要</summary>
当前的3D半监督分割方法面临着有限的上下文信息考虑和不可靠的 Pseudo-标签生成，从而影响其效果性的使用。为解决这些挑战，我们提出了两个不同的子网络，用于探索和利用它们之间的差异，最终 corrections 预测结果。更加具体地说，我们将预测结果中的不一致区域标识出来，并进行Targeted Verification Training进程。这个过程可以减少预测结果中的不确定性，并使用上下文信息进行协调。此外，为了适应性地自我调整网络的表达能力，我们使用了一种自动超参的自我超vised contrastive learning模型。在这个模型中，我们使用网络的信任度来 distinguishes 可靠和不可靠的预测结果，然后训练网络以最小化不可靠的预测结果。我们的实验结果，基于临床MRI和CT扫描数据，表明我们的方法在比较之前的方法的效果性。代码库可以在 \href{https://github.com/xmindflow/SSL-contrastive}{GitHub} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Dense-Pseudo-Label-Selection-for-Semi-supervised-Oriented-Object-Detection"><a href="#Adaptive-Dense-Pseudo-Label-Selection-for-Semi-supervised-Oriented-Object-Detection" class="headerlink" title="Adaptive Dense Pseudo Label Selection for Semi-supervised Oriented Object Detection"></a>Adaptive Dense Pseudo Label Selection for Semi-supervised Oriented Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12608">http://arxiv.org/abs/2311.12608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Zhao, Qiang Fang, Shuohao Shi, Xin Xu</li>
<li>for: 这篇论文主要针对 semi-supervised object detection (SSOD) 问题，特别是在航空照片中常见的多向和紧密的物体上。</li>
<li>methods: 我们提出了 Adaptive Dense Pseudo Label Selection (ADPLS) 方法，包括专门设计的弹性机制，帮助选择精密的pseudo labels。我们还提出了对应的 Mean Feature-Richness Score (mFRS)，来估计潜在物体的密度。</li>
<li>results: 在 DOTA-v1.5 标准 benchmark 上，我们的方法比前一代方法更高，尤其是当 annotated data 仅有 5% 时。例如，我们在 5% annotated data 下 achieve 49.78 mAP，比以前的 state-of-the-art method 在 10% annotated data 下的result surpasses by 1.15 mAP。<details>
<summary>Abstract</summary>
Recently, dense pseudo-label, which directly selects pseudo labels from the original output of the teacher model without any complicated post-processing steps, has received considerable attention in semi-supervised object detection (SSOD). However, for the multi-oriented and dense objects that are common in aerial scenes, existing dense pseudo-label selection methods are inefficient and impede the performance in semi-supervised oriented object detection. Therefore, we propose Adaptive Dense Pseudo Label Selection (ADPLS) for semi-supervised oriented object detection. In ADPLS, we design a simple but effective adaptive mechanism to guide the selection of dense pseudo labels. Specifically, we propose the mean Feature-Richness Score (mFRS) to estimate the density of potential objects and use this score to adjust the number of dense pseudo labels. On the DOTA-v1.5 benchmark, the proposed method outperforms previous methods especially when labeled data are scarce. For example, it achieves 49.78 mAP given only 5% of annotated data, which surpasses previous state-of-the-art method given 10% of annotated data by 1.15 mAP. Our codes will be available soon.
</details>
<details>
<summary>摘要</summary>
ADPLS features an adaptive mechanism that guides the selection of dense pseudo labels. Specifically, we introduce the mean Feature-Richness Score (mFRS) to estimate the density of potential objects and adjust the number of dense pseudo labels based on this score. Our approach outperforms previous methods, especially when labeled data are scarce. For example, on the DOTA-v1.5 benchmark, ADPLS achieves 49.78 mAP with only 5% of annotated data, surpassing the previous state-of-the-art method given 10% of annotated data by 1.15 mAP. Our codes will be available soon.
</details></li>
</ul>
<hr>
<h2 id="Surgical-Temporal-Action-aware-Network-with-Sequence-Regularization-for-Phase-Recognition"><a href="#Surgical-Temporal-Action-aware-Network-with-Sequence-Regularization-for-Phase-Recognition" class="headerlink" title="Surgical Temporal Action-aware Network with Sequence Regularization for Phase Recognition"></a>Surgical Temporal Action-aware Network with Sequence Regularization for Phase Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12603">http://arxiv.org/abs/2311.12603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Chen, Yuhao Zhai, Jun Zhang, Jinqiao Wang</li>
<li>For: 适用于助手在运作室中的干预computer-assisted surgical system，需要对医疗过程的详细理解。* Methods: 提出了一个名为STAR-Net的新网络模型，包括一个高效的多尺度医疗动作（MS-STA）模块和一个双检测器序列调整（DSR），可以对医疗动作进行更加精确的识别。* Results: 实验结果显示，STAR-Net在一个大规模的胃切除手术 dataset 和公共 Cholectc80 标准上实现了与之前的state-of-the-art的比较。<details>
<summary>Abstract</summary>
To assist surgeons in the operating theatre, surgical phase recognition is critical for developing computer-assisted surgical systems, which requires comprehensive understanding of surgical videos. Although existing studies made great progress, there are still two significant limitations worthy of improvement. First, due to the compromise of resource consumption, frame-wise visual features are extracted by 2D networks and disregard spatial and temporal knowledge of surgical actions, which hinders subsequent inter-frame modeling for phase prediction. Second, these works simply utilize ordinary classification loss with one-hot phase labels to optimize the phase predictions, and cannot fully explore surgical videos under inadequate supervision. To overcome these two limitations, we propose a Surgical Temporal Action-aware Network with sequence Regularization, named STAR-Net, to recognize surgical phases more accurately from input videos. Specifically, we propose an efficient multi-scale surgical temporal action (MS-STA) module, which integrates visual features with spatial and temporal knowledge of surgical actions at the cost of 2D networks. Moreover, we devise the dual-classifier sequence regularization (DSR) to facilitate the training of STAR-Net by the sequence guidance of an auxiliary classifier with a smaller capacity. Our STAR-Net with MS-STA and DSR can exploit visual features of surgical actions with effective regularization, thereby leading to the superior performance of surgical phase recognition. Extensive experiments on a large-scale gastrectomy surgery dataset and the public Cholec80 benchmark prove that our STAR-Net significantly outperforms state-of-the-arts of surgical phase recognition.
</details>
<details>
<summary>摘要</summary>
为帮助手术医生在操作室中工作，计算机助手手术系统的开发需要全面的了解手术视频。虽然现有的研究已经做出了很大的进步，但还有两个重要的限制值得进一步改进。首先，由于资源消耗的限制，使用2D网络提取帧级视觉特征，而忽略手术动作的空间和时间知识，这会阻碍后续的帧间模型化 для预测阶段。其次，这些工作都是使用一个简单的分类损失函数和一个热一颗分类器来优化阶段预测，无法充分利用手术视频的不足监督。为了解决这两个限制，我们提议一种名为STAR-Net的手术时间动作相关网络，可以更准确地从输入视频中识别手术阶段。具体来说，我们提出了一种高效的多级手术时间动作（MS-STA）模块，可以同时 интеграVisual特征和手术动作的空间和时间知识，而不是2D网络。此外，我们还提出了双类ifier序列化（DSR），以便通过auxiliary classifier的小容量导航，使STAR-Net在训练中更加有效地进行序列化。我们的STAR-Net with MS-STA and DSR可以有效地利用手术动作的视觉特征，并且通过有效的序列化，从而实现更高的手术阶段识别性。我们在一个大规模的胃手术手术数据集和公共的Cholec80数据集上进行了广泛的实验，证明了我们的STAR-Net在手术阶段识别方面具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="TouchSDF-A-DeepSDF-Approach-for-3D-Shape-Reconstruction-using-Vision-Based-Tactile-Sensing"><a href="#TouchSDF-A-DeepSDF-Approach-for-3D-Shape-Reconstruction-using-Vision-Based-Tactile-Sensing" class="headerlink" title="TouchSDF: A DeepSDF Approach for 3D Shape Reconstruction using Vision-Based Tactile Sensing"></a>TouchSDF: A DeepSDF Approach for 3D Shape Reconstruction using Vision-Based Tactile Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12602">http://arxiv.org/abs/2311.12602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauro Comi, Yijiong Lin, Alex Church, Alessio Tonioni, Laurence Aitchison, Nathan F. Lepora</li>
<li>for: 本研究旨在提出一种数据驱动的战略，用于通过视觉感知器来探索和操纵物体，并使用深度学习来重建3D形状。</li>
<li>methods: 本研究使用了一种组合的方法，包括一个卷积神经网络和深度学习函数，以将战略图像转换为本地网格表示 superficies 的形状。</li>
<li>results: 本研究在实验和实际场景中能够成功地重建3D形状，并且可以在不同的材料和环境下进行robust 3D-aware表示和多模态感知。<details>
<summary>Abstract</summary>
Humans rely on their visual and tactile senses to develop a comprehensive 3D understanding of their physical environment. Recently, there has been a growing interest in exploring and manipulating objects using data-driven approaches that utilise high-resolution vision-based tactile sensors. However, 3D shape reconstruction using tactile sensing has lagged behind visual shape reconstruction because of limitations in existing techniques, including the inability to generalise over unseen shapes, the absence of real-world testing, and limited expressive capacity imposed by discrete representations. To address these challenges, we propose TouchSDF, a Deep Learning approach for tactile 3D shape reconstruction that leverages the rich information provided by a vision-based tactile sensor and the expressivity of the implicit neural representation DeepSDF. Our technique consists of two components: (1) a Convolutional Neural Network that maps tactile images into local meshes representing the surface at the touch location, and (2) an implicit neural function that predicts a signed distance function to extract the desired 3D shape. This combination allows TouchSDF to reconstruct smooth and continuous 3D shapes from tactile inputs in simulation and real-world settings, opening up research avenues for robust 3D-aware representations and improved multimodal perception in robotics. Code and supplementary material are available at: https://touchsdf.github.io/
</details>
<details>
<summary>摘要</summary>
人类依靠视觉和感觉来发展一个完整的3D环境认知。现在，利用数据驱动的方法来探索和操作物体已经吸引了越来越多的关注。然而，使用感觉获取3D形状重建仍然落后于视觉形状重建，因为现有技术存在一些限制，如无法泛化到未经见过的形状、缺乏实际测试和精度表示的受限。为解决这些挑战，我们提出了TouchSDF，一种基于深度学习的感觉3D形状重建方法。我们的技术包括两部分：（1）一个 convolutional neural network 将感觉图像映射到触摸位置的表面的本地网格，和（2）一个隐藏层函数预测 signed distance function，以提取所需的3D形状。这种组合使得TouchSDF可以从感觉输入中提取平滑和连续的3D形状，在实际和模拟环境中进行重建，开启了robust 3D-aware表示和多模态感知的研究途径。代码和补充材料可以在以下链接中找到：https://touchsdf.github.io/
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-detection-of-morphological-features-associated-with-hypoxia-in-H-E-breast-cancer-whole-slide-images"><a href="#Deep-learning-based-detection-of-morphological-features-associated-with-hypoxia-in-H-E-breast-cancer-whole-slide-images" class="headerlink" title="Deep learning-based detection of morphological features associated with hypoxia in H&amp;E breast cancer whole slide images"></a>Deep learning-based detection of morphological features associated with hypoxia in H&amp;E breast cancer whole slide images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12601">http://arxiv.org/abs/2311.12601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petru Manescu, Joseph Geradts, Delmiro Fernandez-Reyes</li>
<li>for: 这个研究用于评估乳癌组织中的衰竭水平，以及这种衰竭水平对肿瘤的生物学进程和临床进程的影响。</li>
<li>methods: 这个研究使用深度学习技术来评估乳癌组织中的衰竭水平。特别是，研究使用了弱监督深度学习（WSDL）模型来检测乳癌组织中的衰竭相关特征。</li>
<li>results: 研究结果表明，使用WSDL模型可以准确地检测乳癌组织中的衰竭相关特征。研究在240个乳癌主要站点的干片上训练和评估了深度多例学习模型，并在遗弃测试集上得到了0.87的AUC平均值。此外，研究还发现了衰竭和正常组织区域之间的显著差异。这种DL衰竭H&amp;E WSI检测模型可能可以扩展到其他肿瘤类型，并且可以轻松地在病理工作流程中integrated，无需额外的成本。<details>
<summary>Abstract</summary>
Hypoxia occurs when tumour cells outgrow their blood supply, leading to regions of low oxygen levels within the tumour. Calculating hypoxia levels can be an important step in understanding the biology of tumours, their clinical progression and response to treatment. This study demonstrates a novel application of deep learning to evaluate hypoxia in the context of breast cancer histomorphology. More precisely, we show that Weakly Supervised Deep Learning (WSDL) models can accurately detect hypoxia associated features in routine Hematoxylin and Eosin (H&E) whole slide images (WSI). We trained and evaluated a deep Multiple Instance Learning model on tiles from WSI H&E tissue from breast cancer primary sites (n=240) obtaining on average an AUC of 0.87 on a left-out test set. We also showed significant differences between features of hypoxic and normoxic tissue regions as distinguished by the WSDL models. Such DL hypoxia H&E WSI detection models could potentially be extended to other tumour types and easily integrated into the pathology workflow without requiring additional costly assays.
</details>
<details>
<summary>摘要</summary>
肿瘤缺氧（hypoxia）发生在肿瘤细胞超出血液供应，导致肿瘤中的氧气水平降低。计算肿瘤缺氧水平是理解肿瘤生物学、临床进程和治疗效果的重要步骤。这项研究展示了深度学习在肿瘤 Histomorphology 中评估缺氧的新应用。具体来说，我们表明了弱监督深度学习（WSDL）模型可以准确地检测肿瘤缺氧相关特征在普通的 Hematoxylin and Eosin（H&E）整个扫描片像（WSI）中。我们使用了240个Breast cancer主病site的H&E扫描片图像进行训练和评估，得到了平均0.87的AUC在留下测试集中。我们还发现了肿瘤缺氧和正常细胞区域之间的差异，这些差异被WSDL模型所 отличи出。这些深度学习肿瘤H&E WSI检测模型可能可以扩展到其他肿瘤类型，并且可以轻松地integrated into Pathology workflow，不需要额外的昂贵的检测。
</details></li>
</ul>
<hr>
<h2 id="HiPose-Hierarchical-Binary-Surface-Encoding-and-Correspondence-Pruning-for-RGB-D-6DoF-Object-Pose-Estimation"><a href="#HiPose-Hierarchical-Binary-Surface-Encoding-and-Correspondence-Pruning-for-RGB-D-6DoF-Object-Pose-Estimation" class="headerlink" title="HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning for RGB-D 6DoF Object Pose Estimation"></a>HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning for RGB-D 6DoF Object Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12588">http://arxiv.org/abs/2311.12588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongliang Lin, Yongzhi Su, Praveen Nathan, Sandeep Inuganti, Yan Di, Martin Sundermeyer, Fabian Manhardt, Didier Stricke, Jason Rambach, Yu Zhang</li>
<li>for: 6DoF object pose estimation from a single RGB-D image</li>
<li>methods: 使用 hierarchical binary surface encoding 和 point-to-surface matching 进行 dense-correspondence</li>
<li>results: 在多个公共benchmark上（LM-O、YCB-V、T-Less）表现出色，超过所有不需要重finement的方法，并且与高效的refinement-based方法匹配。 Code和模型将被释出。<details>
<summary>Abstract</summary>
In this work, we present a novel dense-correspondence method for 6DoF object pose estimation from a single RGB-D image. While many existing data-driven methods achieve impressive performance, they tend to be time-consuming due to their reliance on rendering-based refinement approaches. To circumvent this limitation, we present HiPose, which establishes 3D-3D correspondences in a coarse-to-fine manner with a hierarchical binary surface encoding. Unlike previous dense-correspondence methods, we estimate the correspondence surface by employing point-to-surface matching and iteratively constricting the surface until it becomes a correspondence point while gradually removing outliers. Extensive experiments on public benchmarks LM-O, YCB-V, and T-Less demonstrate that our method surpasses all refinement-free methods and is even on par with expensive refinement-based approaches. Crucially, our approach is computationally efficient and enables real-time critical applications with high accuracy requirements. Code and models will be released.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的粗粒匹配方法，用于基于单个RGB-D图像的6DoF物体pose估计。虽然许多现有的数据驱动方法已经实现了吸引人的性能，但它们往往因为依赖于渲染基于的精度提升方法而变得慢。为了缺乏这些限制，我们提出了HiPose，它在层次 binary 表示中进行粗粒匹配，并通过点到表面匹配和步进抑制表面直到它变成匹配点而逐渐移除异常点。与前一些粗粒匹配方法不同，我们不需要进行渲染基于的精度提升。我们的方法在公共benchmark LM-O、YCB-V 和 T-Less上进行了广泛的实验，并证明了我们的方法超过了所有不包括Refine的方法，并与较昂贵的Refine-based方法相当。这些方法的计算效率高，可以满足实时的应用需求。我们将释放代码和模型。
</details></li>
</ul>
<hr>
<h2 id="A-Region-of-Interest-Focused-Triple-UNet-Architecture-for-Skin-Lesion-Segmentation"><a href="#A-Region-of-Interest-Focused-Triple-UNet-Architecture-for-Skin-Lesion-Segmentation" class="headerlink" title="A Region of Interest Focused Triple UNet Architecture for Skin Lesion Segmentation"></a>A Region of Interest Focused Triple UNet Architecture for Skin Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12581">http://arxiv.org/abs/2311.12581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoqing Liu, Yu Guo, Caiying Wu, Guoqing Chen, Barintag Saheya, Qiyu Jin</li>
<li>for: 这篇论文的目的是提出一个自动标识皮肤病变的方法，以便进行皮肤病变分析和后续的治疗。</li>
<li>methods: 本论文提出了一个名为Triple-UNet的方法，它是三个UNet架构的有机结合。这三个UNet架构分别是第一个UNet、第二个UNet和第三个UNet。为了更好地融合第一个和第二个UNet的output，我们设计了一个区域增强模块（ROIE）。ROIE使用预测得分图来增强目标物体区域的影像，以帮助第二个UNet获得更好的得分图。最后，结果被第三个UNet进行精确化。</li>
<li>results: 我们对一个公开available的皮肤病变标识 dataset进行了实验，结果显示Triple-UNet比前一些方法在皮肤病变标识方面更高效。<details>
<summary>Abstract</summary>
Skin lesion segmentation is of great significance for skin lesion analysis and subsequent treatment. It is still a challenging task due to the irregular and fuzzy lesion borders, and diversity of skin lesions. In this paper, we propose Triple-UNet to automatically segment skin lesions. It is an organic combination of three UNet architectures with suitable modules. In order to concatenate the first and second sub-networks more effectively, we design a region of interest enhancement module (ROIE). The ROIE enhances the target object region of the image by using the predicted score map of the first UNet. The features learned by the first UNet and the enhanced image help the second UNet obtain a better score map. Finally, the results are fine-tuned by the third UNet. We evaluate our algorithm on a publicly available dataset of skin lesion segmentation. Experiments show that Triple-UNet outperforms the state-of-the-art on skin lesion segmentation.
</details>
<details>
<summary>摘要</summary>
皮肤损伤分割是皮肤损伤分析和后续治疗中的关键任务。然而，由于损伤边界的不规则和杂乱，以及皮肤损伤的多样性，这还是一项具有挑战性的任务。在这篇论文中，我们提议了Triple-UNet算法，用于自动分割皮肤损伤。这是三个UNet架构的有机组合，其中每个架构都具有适当的模块。为了更有效地连接第一和第二子网络，我们设计了一个区域增强模块（ROIE）。ROIE使用预测得分图来增强目标对象区域的图像，并使得第二子网络得到更好的分数图。最后，结果被第三子网络进行细化。我们在一个公共可用的皮肤损伤分割数据集上进行了实验，结果显示，Triple-UNet在皮肤损伤分割任务中超过了状态之前的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Multi-Resolution-Planar-Region-Extraction-for-Uneven-Terrains"><a href="#Multi-Resolution-Planar-Region-Extraction-for-Uneven-Terrains" class="headerlink" title="Multi-Resolution Planar Region Extraction for Uneven Terrains"></a>Multi-Resolution Planar Region Extraction for Uneven Terrains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12562">http://arxiv.org/abs/2311.12562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghan Sun, Linfang Zheng, Hua Chen, Wei Zhang</li>
<li>for: 本研究旨在从不规则点云测量中提取平面区域，以解决机器人应用中的感知行进问题。</li>
<li>methods: 我们提出了一种多resolution平面区域提取策略，通过平衡精度和计算效率来解决过去方法的问题。我们的方法包括点 wise分类预处理模块和多resolution分 segmentation模块。</li>
<li>results: 我们通过实验表明，提出的方法可以在不同的不平坦地形上实现高效、稳定的平面区域提取，实现每帧超过35帧的帧率。<details>
<summary>Abstract</summary>
This paper studies the problem of extracting planar regions in uneven terrains from unordered point cloud measurements. Such a problem is critical in various robotic applications such as robotic perceptive locomotion. While existing approaches have shown promising results in effectively extracting planar regions from the environment, they often suffer from issues such as low computational efficiency or loss of resolution. To address these issues, we propose a multi-resolution planar region extraction strategy in this paper that balances the accuracy in boundaries and computational efficiency. Our method begins with a pointwise classification preprocessing module, which categorizes all sampled points according to their local geometric properties to facilitate multi-resolution segmentation. Subsequently, we arrange the categorized points using an octree, followed by an in-depth analysis of nodes to finish multi-resolution plane segmentation. The efficiency and robustness of the proposed approach are verified via synthetic and real-world experiments, demonstrating our method's ability to generalize effectively across various uneven terrains while maintaining real-time performance, achieving frame rates exceeding 35 FPS.
</details>
<details>
<summary>摘要</summary>
Our method begins with a pointwise classification preprocessing module, which categorizes all sampled points based on their local geometric properties to facilitate multi-resolution segmentation. We then arrange the categorized points using an octree, followed by an in-depth analysis of nodes to finish multi-resolution plane segmentation.We verify the efficiency and robustness of our approach through synthetic and real-world experiments, demonstrating its ability to generalize effectively across various uneven terrains while maintaining real-time performance, with frame rates exceeding 35 FPS.
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Neural-Networks-for-Neuroimaging-in-Parkinson’s-Disease-Is-Preprocessing-Needed"><a href="#Convolutional-Neural-Networks-for-Neuroimaging-in-Parkinson’s-Disease-Is-Preprocessing-Needed" class="headerlink" title="Convolutional Neural Networks for Neuroimaging in Parkinson’s Disease: Is Preprocessing Needed?"></a>Convolutional Neural Networks for Neuroimaging in Parkinson’s Disease: Is Preprocessing Needed?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12561">http://arxiv.org/abs/2311.12561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francisco J. Martinez-Murcia, Juan M. Górriz, Javier Ramírez, Andrés Ortiz</li>
<li>for: 本研究旨在检验 convolutional neural networks (CNNs) 是否可以考虑空间和INTENSITY差异，并不需要进行常见的空间和INTENSITYNormalization处理。</li>
<li>methods: 本研究使用了四种不同的 CNN 模型，基于已知的架构，并或不使用不同的空间和INTENSITYNormalization处理。</li>
<li>results: 结果显示，一个足够复杂的模型，如我们的三维版本的 ALEXNET，可以有效地考虑空间差异，达到诊断精度为 94.1%，ROC曲线下的面积为 0.984。仔细分析结果，可以看出模型正确地找到与文献中的模式匹配，无需应用复杂的空间Normalization处理。然而，INTENSITYNormalization的选择和处理方法对结果和准确率产生了重要的影响。<details>
<summary>Abstract</summary>
Spatial and intensity normalization are nowadays a prerequisite for neuroimaging analysis. Influenced by voxel-wise and other univariate comparisons, where these corrections are key, they are commonly applied to any type of analysis and imaging modalities. Nuclear imaging modalities such as PET-FDG or FP-CIT SPECT, a common modality used in Parkinson's Disease diagnosis, are especially dependent on intensity normalization. However, these steps are computationally expensive and furthermore, they may introduce deformations in the images, altering the information contained in them. Convolutional Neural Networks (CNNs), for their part, introduce position invariance to pattern recognition, and have been proven to classify objects regardless of their orientation, size, angle, etc. Therefore, a question arises: how well can CNNs account for spatial and intensity differences when analysing nuclear brain imaging? Are spatial and intensity normalization still needed? To answer this question, we have trained four different CNN models based on well-established architectures, using or not different spatial and intensity normalization preprocessing. The results show that a sufficiently complex model such as our three-dimensional version of the ALEXNET can effectively account for spatial differences, achieving a diagnosis accuracy of 94.1% with an area under the ROC curve of 0.984. The visualization of the differences via saliency maps shows that these models are correctly finding patterns that match those found in the literature, without the need of applying any complex spatial normalization procedure. However, the intensity normalization -- and its type -- is revealed as very influential in the results and accuracy of the trained model, and therefore must be well accounted.
</details>
<details>
<summary>摘要</summary>
现代神经成像分析中，空间和强度 норmalization 已成为必备的前提。这些 corrected 通常适用于所有类型的分析和成像模式。核lear imaging模式，如 PET-FDG 或 FP-CIT SPECT，特别dependent on intensity normalization，是 Parkinson's disease 诊断中常用的一种常见模式。然而，这些步骤可能会 computationally expensive 并且可能会将图像中的信息扭曲。 Convolutional Neural Networks (CNNs) 引入了位置不变性，以便 Pattern recognition 中减少了图像的影响。因此，一个问题出现：CNNs 能够考虑到空间和强度差异吗？是否还需要进行空间和强度 normalization ？为了回答这个问题，我们训练了四个不同的 CNN 模型，使用或不使用不同的空间和强度 normalization 预处理。结果表明，一个足够复杂的模型，如我们的三维版本的 ALEXNET，可以有效地考虑空间差异，达到诊断精度为 94.1%，ROC 曲线下的面积为 0.984。在 Visualization 中，这些模型可以正确地找到与文献中所描述的模式匹配的pattern，无需应用任何复杂的空间normalization 过程。然而，强度normalization 的类型和影响结果的准确性。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-bias-Expanding-clinical-AI-model-card-to-incorporate-bias-reporting-of-social-and-non-social-factors"><a href="#Benchmarking-bias-Expanding-clinical-AI-model-card-to-incorporate-bias-reporting-of-social-and-non-social-factors" class="headerlink" title="Benchmarking bias: Expanding clinical AI model card to incorporate bias reporting of social and non-social factors"></a>Benchmarking bias: Expanding clinical AI model card to incorporate bias reporting of social and non-social factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12560">http://arxiv.org/abs/2311.12560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carolina A. M. Heming, Mohamed Abdalla, Monish Ahluwalia, Linglin Zhang, Hari Trivedi, MinJae Woo, Benjamin Fine, Judy Wawira Gichoya, Leo Anthony Celi, Laleh Seyyed-Kalantari</li>
<li>for: 这篇论文主要是关于扩展临床AI模型报告卡，以包括广泛的偏见报告，包括社交和非社交因素。</li>
<li>methods: 论文使用了AI模型，并使用了不同的方法来检测和报告偏见。</li>
<li>results: 论文发现，扩展报告卡可以帮助检测和解决AI模型中的偏见，并且可以提高AI模型的安全性和可靠性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Clinical AI model reporting cards should be expanded to incorporate a broad bias reporting of both social and non-social factors. Non-social factors consider the role of other factors, such as disease dependent, anatomic, or instrument factors on AI model bias, which are essential to ensure safe deployment.
</details>
<details>
<summary>摘要</summary>
临床AI模型报告卡应该扩展到包括广泛的偏见报告，包括社会和非社会因素。非社会因素包括疾病依赖、解剖学因素和实验室因素等，这些因素对AI模型偏见的影响非常重要，以确保安全的部署。
</details></li>
</ul>
<hr>
<h2 id="“HoVer-UNet”-Accelerating-HoVerNet-with-UNet-based-multi-class-nuclei-segmentation-via-knowledge-distillation"><a href="#“HoVer-UNet”-Accelerating-HoVerNet-with-UNet-based-multi-class-nuclei-segmentation-via-knowledge-distillation" class="headerlink" title="“HoVer-UNet”: Accelerating HoVerNet with UNet-based multi-class nuclei segmentation via knowledge distillation"></a>“HoVer-UNet”: Accelerating HoVerNet with UNet-based multi-class nuclei segmentation via knowledge distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12553">http://arxiv.org/abs/2311.12553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diagnijmegen/hover-unet">https://github.com/diagnijmegen/hover-unet</a></li>
<li>paper_authors: Cristian Tommasino, Cristiano Russo, Antonio Maria Rinaldi, Francesco Ciompi</li>
<li>for: 这个论文是为了提取多支分支 HoVerNet 框架中关于细胞实例分类和识别的知识，以提高 histopathology 领域中 nuclei 实例分类和识别的效率和准确率。</li>
<li>methods: 该论文提出了一种压缩式单个 UNet 网络，并在其中使用了 Mix Vision Transformer 核心，以优化编码 HoVerNet 中的知识。此外，该论文还提出了一种自定义损失函数，以优化模型的训练。</li>
<li>results: 该论文表明，使用 HoVer-UNet 模型可以在 PanNuke 和 Consep 公共数据集上实现与 HoVerNet 相当的性能，但是具有三倍减少的计算时间。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/DIAGNijmegen/HoVer-UNet">https://github.com/DIAGNijmegen/HoVer-UNet</a> 上获取。<details>
<summary>Abstract</summary>
We present "HoVer-UNet", an approach to distill the knowledge of the multi-branch HoVerNet framework for nuclei instance segmentation and classification in histopathology. We propose a compact, streamlined single UNet network with a Mix Vision Transformer backbone, and equip it with a custom loss function to optimally encode the distilled knowledge of HoVerNet, reducing computational requirements without compromising performances. We show that our model achieved results comparable to HoVerNet on the public PanNuke and Consep datasets with a three-fold reduction in inference time. We make the code of our model publicly available at https://github.com/DIAGNijmegen/HoVer-UNet.
</details>
<details>
<summary>摘要</summary>
我们介绍了“HoVer-UNet”方法，用于在 Histopathology 中分类和实例化核体。我们提出了一个嵌入式、流lined的单个 UNet 网络，并将其与 Mix Vision Transformer 背bone结合使用。我们还提出了一个定制的损失函数，以优化压缩了 HoVerNet 的知识，降低计算成本而不妨碍性能。我们的模型在公共 PanNuke 和 Consep 数据集上达到了与 HoVerNet 相同的Result，但具有三倍减少的推理时间。我们将我们的代码公开于 GitHub 上，具体信息请参考 <https://github.com/DIAGNijmegen/HoVer-UNet>。
</details></li>
</ul>
<hr>
<h2 id="GMISeg-General-Medical-Image-Segmentation-without-Re-Training"><a href="#GMISeg-General-Medical-Image-Segmentation-without-Re-Training" class="headerlink" title="GMISeg: General Medical Image Segmentation without Re-Training"></a>GMISeg: General Medical Image Segmentation without Re-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12539">http://arxiv.org/abs/2311.12539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Xu</li>
<li>for: 解决医疗图像分割领域中 unknown 任务，即新的组织结构、图像形态或标签等。</li>
<li>methods: 提出了一种基于 SAM（Segment Anything Model）图像编码器的 novel low-rank 精度调整策略，与提示编码器和面减解编码器结合使用，以 Fine-tune 已经标注的数据集而无需进一步训练。</li>
<li>results: GMISeg 在 unknown 任务中表现出优于最新方法，并进行了对方法的全面分析和总结。<details>
<summary>Abstract</summary>
Although deep learning models have become the main method for medical image segmentation, they often cannot be extended to unknown segmentation tasks involving new anatomical structures, image shapes, or labels. For new segmentation tasks, researchers often have to retrain or fine-tune the model, which is time-consuming and poses a significant obstacle to clinical researchers, who often lack the resources and professional knowledge to train neural networks. Therefore, we proposed a general method that can solve unknown medical image segmentation tasks without requiring additional training. Given an example set of images and prompts for defining new segmentation tasks, GMISeg applies a novel low-rank fine-tuning strategy based on the proposed approach to the SAM (Segment Anything Model) image encoder, and works with the prompt encoder and mask decoder to fine-tune the labeled dataset without the need for additional training. To achieve generalization of new tasks, we used medical image datasets with different imaging modes for different parts. We trained and generalized GMISeg on a different set of anatomical and imaging modes using cardiac images on other site datasets. We have demonstrated that GMISeg outperforms the latest methods on unknown tasks and have conducted a comprehensive analysis and summary of the important performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)尽管深度学习模型已成为医学图像分割的主要方法，但它们经常无法适应未知的分割任务，包括新的解剖结构、图像形态和标签。为新的分割任务，研究人员经常需要重新训练或微调模型，这是时间consuming的并且对临床研究人员来说是一个重大障碍，他们通常缺乏训练神经网络的资源和专业知识。因此，我们提出了一种可以解决未知医学图像分割任务的通用方法。给出了示例集合和用于定义新分割任务的提示，GMISeg应用了一种新的低级别微调策略，基于我们提出的方法，与提示编码器和掩码解码器一起微调已标注数据集，而不需要额外的训练。为实现新任务的总体化，我们使用了不同的医学图像数据集，包括不同的解剖和扫描模式。我们在不同的部位和扫描模式上训练和总结GMISeg，并示出了GMISeg在未知任务上的性能比latest方法更高，并进行了完整的分析和总结。
</details></li>
</ul>
<hr>
<h2 id="Hyb-NeRF-A-Multiresolution-Hybrid-Encoding-for-Neural-Radiance-Fields"><a href="#Hyb-NeRF-A-Multiresolution-Hybrid-Encoding-for-Neural-Radiance-Fields" class="headerlink" title="Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields"></a>Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12490">http://arxiv.org/abs/2311.12490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Wang, Yi Gong, Yuan Zeng</li>
<li>for: 高精度场景重建和新视图合成</li>
<li>methods: 使用多分辨率混合编码，并使用快速优化和本地细节快速学习 Positional 特征，以及使用 cone tracing 特征来消除编码混乱和减少抖音artefacts</li>
<li>results: 在 synthetic 和实际数据集上实现更快的渲染速度，同时维护高质量渲染效果和更低的内存占用率，与之前的状态态method 相比<details>
<summary>Abstract</summary>
Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity scene reconstruction for novel view synthesis. However, NeRF requires hundreds of network evaluations per pixel to approximate a volume rendering integral, making it slow to train. Caching NeRFs into explicit data structures can effectively enhance rendering speed but at the cost of higher memory usage. To address these issues, we present Hyb-NeRF, a novel neural radiance field with a multi-resolution hybrid encoding that achieves efficient neural modeling and fast rendering, which also allows for high-quality novel view synthesis. The key idea of Hyb-NeRF is to represent the scene using different encoding strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits memory-efficiency learnable positional features at coarse resolutions and the fast optimization speed and local details of hash-based feature grids at fine resolutions. In addition, to further boost performance, we embed cone tracing-based features in our learnable positional encoding that eliminates encoding ambiguity and reduces aliasing artifacts. Extensive experiments on both synthetic and real-world datasets show that Hyb-NeRF achieves faster rendering speed with better rending quality and even a lower memory footprint in comparison to previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Hyb-NeRF represents the scene using different encoding strategies from coarse-to-fine resolution levels. It exploits memory-efficiency learnable positional features at coarse resolutions and the fast optimization speed and local details of hash-based feature grids at fine resolutions. Additionally, to further boost performance, we embed cone tracing-based features in our learnable positional encoding, which eliminates encoding ambiguity and reduces aliasing artifacts.Extensive experiments on both synthetic and real-world datasets show that Hyb-NeRF achieves faster rendering speed with better rendering quality and even a lower memory footprint compared to previous state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="HCA-Net-Hierarchical-Context-Attention-Network-for-Intervertebral-Disc-Semantic-Labeling"><a href="#HCA-Net-Hierarchical-Context-Attention-Network-for-Intervertebral-Disc-Semantic-Labeling" class="headerlink" title="HCA-Net: Hierarchical Context Attention Network for Intervertebral Disc Semantic Labeling"></a>HCA-Net: Hierarchical Context Attention Network for Intervertebral Disc Semantic Labeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12486">http://arxiv.org/abs/2311.12486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afshin Bozorgpour, Bobby Azad, Reza Azad, Yury Velichko, Ulas Bagci, Dorit Merhof</li>
<li>for: 这篇论文的目的是提出一个新的内容关注网络架构（HCA-Net），用于医疗影像中的脊椎突出物（IVD）的semantic标签。</li>
<li>methods: HCA-Net使用了对各个IVD的估计，将IVD标签视为一个位置估计问题，并使用了骨骼损失函数来强制模型对骨架的几何依赖。</li>
<li>results: 在多中心脊椎数据集上进行了广泛的实验评估，HCA-Net的方法与之前的州OF-the-art方法相比，在MRI T1w和T2wmodalities上均有优秀的表现。<details>
<summary>Abstract</summary>
Accurate and automated segmentation of intervertebral discs (IVDs) in medical images is crucial for assessing spine-related disorders, such as osteoporosis, vertebral fractures, or IVD herniation. We present HCA-Net, a novel contextual attention network architecture for semantic labeling of IVDs, with a special focus on exploiting prior geometric information. Our approach excels at processing features across different scales and effectively consolidating them to capture the intricate spatial relationships within the spinal cord. To achieve this, HCA-Net models IVD labeling as a pose estimation problem, aiming to minimize the discrepancy between each predicted IVD location and its corresponding actual joint location. In addition, we introduce a skeletal loss term to reinforce the model's geometric dependence on the spine. This loss function is designed to constrain the model's predictions to a range that matches the general structure of the human vertebral skeleton. As a result, the network learns to reduce the occurrence of false predictions and adaptively improves the accuracy of IVD location estimation. Through extensive experimental evaluation on multi-center spine datasets, our approach consistently outperforms previous state-of-the-art methods on both MRI T1w and T2w modalities. The codebase is accessible to the public on \href{https://github.com/xmindflow/HCA-Net}{GitHub}.
</details>
<details>
<summary>摘要</summary>
医学影像中间 vertebral discs（IVD）的准确和自动化分割是评估脊梗疾病的关键，如骨质疏松、vertebral fractures或IVD herniation。我们提出了HCA-Net，一种新的含义 Labeling 网络架构，强调利用先前的几何信息。我们的方法可以处理不同的比例和缩放因子，并将其集成起来，以捕捉脊梗中的复杂的空间关系。为此，HCA-Net 将 IVD 标注作为一个pose estimation问题，以最小化每个预测的 IVD 位置与其对应的实际联合位置之间的差异。此外，我们引入了一个骨架损失函数，以加强模型的几何依赖于脊梗。这个损失函数是设计来限制模型的预测在一个匹配人类脊梗骨架的范围内。因此，网络将学习减少false预测的发生，并适应IVD位置估计的准确性。经过对多中心脊梗数据集的广泛实验评估，我们的方法在 T1w 和 T2w 模态上都能够稳定地超越前一代的状态态势。代码库可以在 \href{https://github.com/xmindflow/HCA-Net}{GitHub} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MaskFlow-Object-Aware-Motion-Estimation"><a href="#MaskFlow-Object-Aware-Motion-Estimation" class="headerlink" title="MaskFlow: Object-Aware Motion Estimation"></a>MaskFlow: Object-Aware Motion Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12476">http://arxiv.org/abs/2311.12476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aria Ahmadi, David R. Walton, Tim Atherton, Cagatay Dikici</li>
<li>for: 估计高精度动态场景中的物体运动场景，尤其是小物体、大移动和快速变化的情况下的准确运动场景估计。</li>
<li>methods: MaskFlow方法使用了对象级别特征和分割，并将其用于估计物体的翻译运动场景。该方法还提出了一种新的把部分翻译运动场景引入后续运动估计网络进行精度调整的方法。</li>
<li>results: MaskFlow方法在我们新的挑战性 synthetic 数据集上测试，与状态 Künstler 的方法相比，在准确性和稳定性方面均有较好的表现，而且在流行的 FlyingThings3D 数据集上也能够达到相似的结果。<details>
<summary>Abstract</summary>
We introduce a novel motion estimation method, MaskFlow, that is capable of estimating accurate motion fields, even in very challenging cases with small objects, large displacements and drastic appearance changes. In addition to lower-level features, that are used in other Deep Neural Network (DNN)-based motion estimation methods, MaskFlow draws from object-level features and segmentations. These features and segmentations are used to approximate the objects' translation motion field. We propose a novel and effective way of incorporating the incomplete translation motion field into a subsequent motion estimation network for refinement and completion. We also produced a new challenging synthetic dataset with motion field ground truth, and also provide extra ground truth for the object-instance matchings and corresponding segmentation masks. We demonstrate that MaskFlow outperforms state of the art methods when evaluated on our new challenging dataset, whilst still producing comparable results on the popular FlyingThings3D benchmark dataset.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的动作估计方法，MaskFlow，可以准确地估计动作场景，即使是非常困难的情况下，包括小物体、大偏移和悬峰变化。除了低级特征，其他深度神经网络（DNN）基于动作估计方法使用的特征之外，MaskFlow还利用了物体级别特征和分割。这些特征和分割用于估计物体的翻译动作场景。我们提出了一种新的和有效的方法，将部分动作场景纳入后续动作估计网络进行修正和完善。我们还生成了一个新的挑战性的 sintetic 数据集，并提供了对象匹配和相应的分割Masks的附加真实的地面实据。我们示示了MaskFlow在我们新的挑战性数据集上的表现优于当前状态的方法，而仍然在Popular FlyingThings3D 数据集上Produce相似的结果。
</details></li>
</ul>
<hr>
<h2 id="GLAD-Global-Local-View-Alignment-and-Background-Debiasing-for-Unsupervised-Video-Domain-Adaptation-with-Large-Domain-Gap"><a href="#GLAD-Global-Local-View-Alignment-and-Background-Debiasing-for-Unsupervised-Video-Domain-Adaptation-with-Large-Domain-Gap" class="headerlink" title="GLAD: Global-Local View Alignment and Background Debiasing for Unsupervised Video Domain Adaptation with Large Domain Gap"></a>GLAD: Global-Local View Alignment and Background Debiasing for Unsupervised Video Domain Adaptation with Large Domain Gap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12467">http://arxiv.org/abs/2311.12467</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khuvll/glad">https://github.com/khuvll/glad</a></li>
<li>paper_authors: Hyogun Lee, Kyungho Bae, Seongjong Ha, Yumin Ko, Gyeongmoon Park, Jinwoo Choi</li>
<li>for: 这个研究是针对无监督类别影片领域对应（UVDA）进行探索，专注于具有较大领域差距的情况，而非现有的工作主要是处理小领域差距。</li>
<li>methods: 我们提出了一种全新的UVDA情况，称为Kinetics-&gt;BABEL，具有更大的领域差距，包括时间动态和背景的类别变化。我们解决时间差距的问题，我们提出了全球-本地视野对齐方法，并通过时间顺序学习和背景增强来减少背景差距。</li>
<li>results: 我们验证了我们的提案在Kinetics-&gt;BABEL数据集上表现出色，与现有方法相比，具有较大的改善。<details>
<summary>Abstract</summary>
In this work, we tackle the challenging problem of unsupervised video domain adaptation (UVDA) for action recognition. We specifically focus on scenarios with a substantial domain gap, in contrast to existing works primarily deal with small domain gaps between labeled source domains and unlabeled target domains. To establish a more realistic setting, we introduce a novel UVDA scenario, denoted as Kinetics->BABEL, with a more considerable domain gap in terms of both temporal dynamics and background shifts. To tackle the temporal shift, i.e., action duration difference between the source and target domains, we propose a global-local view alignment approach. To mitigate the background shift, we propose to learn temporal order sensitive representations by temporal order learning and background invariant representations by background augmentation. We empirically validate that the proposed method shows significant improvement over the existing methods on the Kinetics->BABEL dataset with a large domain gap. The code is available at https://github.com/KHUVLL/GLAD.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们面临着无监督视频领域适应（UVDA）问题的挑战。我们专门关注具有较大领域差异的场景，而不是现有工作主要面临小领域差异 между标注源领域和无标注目标领域。为建立更真实的设定，我们介绍了一个新的 UVDA 场景，称为 Kinetics->BABEL，它具有更大的领域差异以及时间动态和背景偏移。为解决时间偏移问题，即源频率和目标频率之间的动作持续时间差异，我们提议一种全局本地视图匹配方法。为减少背景偏移，我们提议通过时间顺序学习和背景扩展来学习时间顺序敏感的表示和背景不变的表示。我们经验 validate 表示我们提出的方法在 Kinetics->BABEL 数据集上显示出了明显的改进。代码可以在 <https://github.com/KHUVLL/GLAD> 上获取。
</details></li>
</ul>
<hr>
<h2 id="HiFi-Syn-Hierarchical-Granularity-Discrimination-for-High-Fidelity-Synthesis-of-MR-Images-with-Structure-Preservation"><a href="#HiFi-Syn-Hierarchical-Granularity-Discrimination-for-High-Fidelity-Synthesis-of-MR-Images-with-Structure-Preservation" class="headerlink" title="HiFi-Syn: Hierarchical Granularity Discrimination for High-Fidelity Synthesis of MR Images with Structure Preservation"></a>HiFi-Syn: Hierarchical Granularity Discrimination for High-Fidelity Synthesis of MR Images with Structure Preservation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12461">http://arxiv.org/abs/2311.12461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Yu, Botao Zhao, Shengjie Zhang, Xiang Chen, Jianfeng Feng, Tingying Peng, Xiao-Yong Zhang</li>
<li>for: 本研究旨在提高医疗图像的同构翻译，以保持医疗图像中的结构信息。</li>
<li>methods: 我们介绍了层次划分精度，利用不同层次的semantic信息，以及一种重要权重策略来保持医疗图像的结构准确性。</li>
<li>results: 我们的方法在三个独立的数据集（UK Biobank、IXI和BraTS 2018）上进行了测试，并表现出优于当前最佳方法。特别是，我们的模型不仅能够同构正常结构，还能够处理异常（疾病）结构，如脑肿瘤，并且在不同的医疗成像Modalities上具有较高的医学价值。<details>
<summary>Abstract</summary>
Synthesizing medical images while preserving their structural information is crucial in medical research. In such scenarios, the preservation of anatomical content becomes especially important. Although recent advances have been made by incorporating instance-level information to guide translation, these methods overlook the spatial coherence of structural-level representation and the anatomical invariance of content during translation. To address these issues, we introduce hierarchical granularity discrimination, which exploits various levels of semantic information present in medical images. Our strategy utilizes three levels of discrimination granularity: pixel-level discrimination using a Brain Memory Bank, structure-level discrimination on each brain structure with a re-weighting strategy to focus on hard samples, and global-level discrimination to ensure anatomical consistency during translation. The image translation performance of our strategy has been evaluated on three independent datasets (UK Biobank, IXI, and BraTS 2018), and it has outperformed state-of-the-art algorithms. Particularly, our model excels not only in synthesizing normal structures but also in handling abnormal (pathological) structures, such as brain tumors, despite the variations in contrast observed across different imaging modalities due to their pathological characteristics. The diagnostic value of synthesized MR images containing brain tumors has been evaluated by radiologists. This indicates that our model may offer an alternative solution in scenarios where specific MR modalities of patients are unavailable. Extensive experiments further demonstrate the versatility of our method, providing unique insights into medical image translation.
</details>
<details>
<summary>摘要</summary>
医学图像合成是医学研究中非常重要的一环，特别是保持图像结构信息的重要性。虽然最新的进展已经通过使用实例级信息来引导翻译，但这些方法忽视了图像结构水平的准确性和医学特征的不变性。为解决这些问题，我们介绍了层次粒度识别法，利用医学图像中不同级别的Semantic信息。我们的策略使用三级层次识别粒度：像素级识别使用Brain Memory Bank，结构级识别使用重量调整策略，以及全局级识别确保图像结构一致。我们的策略在三个独立的数据集（UK Biobank、IXI和BraTS 2018）上进行了图像翻译性能评估，并在比较当前的算法时表现出色。特别是，我们的模型不仅能够合成正常结构，还能够处理异常（病理）结构，如脑肿瘤，尽管各种成像模式的病理特征导致图像的对比度变化。医生评估的合成MR图像中的脑肿瘤的诊断价值得到了证明，这表明我们的模型可能在某些特定的MR模式不可用时提供一个备用解决方案。广泛的实验还证明了我们的方法的多样性，为医学图像翻译提供了新的视角。
</details></li>
</ul>
<hr>
<h2 id="Learning-Site-specific-Styles-for-Multi-institutional-Unsupervised-Cross-modality-Domain-Adaptation"><a href="#Learning-Site-specific-Styles-for-Multi-institutional-Unsupervised-Cross-modality-Domain-Adaptation" class="headerlink" title="Learning Site-specific Styles for Multi-institutional Unsupervised Cross-modality Domain Adaptation"></a>Learning Site-specific Styles for Multi-institutional Unsupervised Cross-modality Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12437">http://arxiv.org/abs/2311.12437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Liu, Yubo Fan, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz</li>
<li>for: 这个研究是为了解决医疗影像分析中的无监督跨频道领域适应问题，特别是当源频道和目标频道资料来自多个机构时。</li>
<li>methods: 我们使用无对照图像转换来转换源频道图像到目标频道，并设计了动态网络来生成自适应目标频道图像，并且进行自我训练来降低领域差。</li>
<li>results: 我们的解决方案在验证和测试阶段都获得了第一名。<details>
<summary>Abstract</summary>
Unsupervised cross-modality domain adaptation is a challenging task in medical image analysis, and it becomes more challenging when source and target domain data are collected from multiple institutions. In this paper, we present our solution to tackle the multi-institutional unsupervised domain adaptation for the crossMoDA 2023 challenge. First, we perform unpaired image translation to translate the source domain images to the target domain, where we design a dynamic network to generate synthetic target domain images with controllable, site-specific styles. Afterwards, we train a segmentation model using the synthetic images and further reduce the domain gap by self-training. Our solution achieved the 1st place during both the validation and testing phases of the challenge.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>医疗图像分析中无监督交叉模式领域适应是一项挑战，而当源频率和目标频率数据来自多家机构时，这项任务变得更加挑战。在这篇论文中，我们提出了解决多家机构Unsupervised Cross-Modality Domain Adaptation（crossMoDA 2023）挑战的解决方案。首先，我们通过无对应图像翻译将源频率图像翻译到目标频率，并设计了一个动态网络生成可控、站点特定的样本风格的synthetic目标频率图像。然后，我们使用这些synthetic图像进行分类器训练，并通过自我训练来减少频率差距。我们的解决方案在验证和测试阶段的挑战中均获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="AR-Visualization-System-for-Ship-Detection-and-Recognition-Based-on-AI"><a href="#AR-Visualization-System-for-Ship-Detection-and-Recognition-Based-on-AI" class="headerlink" title="AR Visualization System for Ship Detection and Recognition Based on AI"></a>AR Visualization System for Ship Detection and Recognition Based on AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12430">http://arxiv.org/abs/2311.12430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Ye, Limin Huang, Yongji Wu, Min Hu</li>
<li>for: 这个项目是一个基于人工智能和扩展现实技术的船 detection和识别系统，主要包括人工智能模块、Unity开发模块和Hololens2AR模块。</li>
<li>methods: 该项目使用了R3Det算法来完成远程感知图像中船的探测和识别，并在RTX 2080Ti上训练了模型，其检测精度可达96%。然后，通过船类划分和信息获得3D船模型，并在虚拟场景中生成。最后，添加了语音模块和UI交互模块，并通过MRTK部署到Hololens2上。</li>
<li>results: 该系统实现了计算机视觉和扩展现实技术的融合，将物体检测结果映射到AR场景中，是未来技术趋势和智能应用的勇敢一步。<details>
<summary>Abstract</summary>
Augmented reality technology has been widely used in industrial design interaction, exhibition guide, information retrieval and other fields. The combination of artificial intelligence and augmented reality technology has also become a future development trend. This project is an AR visualization system for ship detection and recognition based on AI, which mainly includes three parts: artificial intelligence module, Unity development module and Hololens2AR module. This project is based on R3Det algorithm to complete the detection and recognition of ships in remote sensing images. The recognition rate of model detection trained on RTX 2080Ti can reach 96%. Then, the 3D model of the ship is obtained by ship categories and information and generated in the virtual scene. At the same time, voice module and UI interaction module are added. Finally, we completed the deployment of the project on Hololens2 through MRTK. The system realizes the fusion of computer vision and augmented reality technology, which maps the results of object detection to the AR field, and makes a brave step toward the future technological trend and intelligent application.
</details>
<details>
<summary>摘要</summary>
“增强现实技术在工业设计互动、展览指南、信息检索等领域得广泛应用。融合人工智能和增强现实技术的趋势也在未来发展。这个项目是一个基于AI的船检测和识别的AR视觉系统，主要包括三部分：人工智能模块、Unity开发模块和Hololens2AR模块。这个项目基于R3Det算法来完成远程感知图像中船的检测和识别。训练使用RTX 2080Ti的模型可达96%的识别率。然后，通过船类划分和信息获取，生成船的3D模型在虚拟场景中。同时，添加了语音模块和UI互动模块。最后，通过MRTK进行了Hololens2的部署。系统实现了计算机视觉和增强现实技术的融合，将对象检测结果映射到AR场景中，向未来技术趋势和智能应用踏出了一步。”
</details></li>
</ul>
<hr>
<h2 id="Two-Views-Are-Better-than-One-Monocular-3D-Pose-Estimation-with-Multiview-Consistency"><a href="#Two-Views-Are-Better-than-One-Monocular-3D-Pose-Estimation-with-Multiview-Consistency" class="headerlink" title="Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency"></a>Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12421">http://arxiv.org/abs/2311.12421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Keilstrup Ingwersen, Anders Bjorholm Dahl, Janus Nørtoft Jensen, Morten Rieger Hannemose</li>
<li>for: 提高单视人体 pose 估算模型性能，使其可以在不需要价格的情况下进行定制化。</li>
<li>methods: 提出了一种新的损失函数——多视图一致性损失，使得在没有3D数据时可以通过多视图数据进行精度的定制。</li>
<li>results: 实验表明，只需要使用两个不同视角的数据，可以获得良好的性能，并且随着更多视角的添加，性能只有小幅提高。这些结果开启了新的可行性 для领域适应性的3D人体 pose 估算。<details>
<summary>Abstract</summary>
Deducing a 3D human pose from a single 2D image or 2D keypoints is inherently challenging, given the fundamental ambiguity wherein multiple 3D poses can correspond to the same 2D representation. The acquisition of 3D data, while invaluable for resolving pose ambiguity, is expensive and requires an intricate setup, often restricting its applicability to controlled lab environments. We improve performance of monocular human pose estimation models using multiview data for fine-tuning. We propose a novel loss function, multiview consistency, to enable adding additional training data with only 2D supervision. This loss enforces that the inferred 3D pose from one view aligns with the inferred 3D pose from another view under similarity transformations. Our consistency loss substantially improves performance for fine-tuning with no available 3D data. Our experiments demonstrate that two views offset by 90 degrees are enough to obtain good performance, with only marginal improvements by adding more views. Thus, we enable the acquisition of domain-specific data by capturing activities with off-the-shelf cameras, eliminating the need for elaborate calibration procedures. This research introduces new possibilities for domain adaptation in 3D pose estimation, providing a practical and cost-effective solution to customize models for specific applications. The used dataset, featuring additional views, will be made publicly available.
</details>
<details>
<summary>摘要</summary>
基于单个2D图像或2D关键点的3D人姿推断是内在困难的，因为多个3D姿势可以对应同一个2D表示。获取3D数据可以解决姿势歧义问题，但是costly和需要复杂的设置，通常只能在控制的室内环境中进行应用。我们提高了单视图人姿推断模型的性能，使用多视图数据进行练习。我们提出了一个新的损失函数，多视图一致性，以便在没有3D数据时进行训练。这个损失函数要求一个视图中推断出的3D姿势与另一个视图中推断出的3D姿势相似。我们的一致性损失substantially提高了没有3D数据的情况下的性能。我们的实验表明，两个视图夹角90度的差够以获得良好的性能，只有在加入更多视图时才有些许改善。因此，我们可以通过捕捉活动中的几个视图，无需进行复杂的 calibration 过程，获得域specific的数据。这项研究开创了3D姿势推断领域的新可能性，提供了可靠且cost-effective的解决方案，可以适应特定应用场景。我们使用的数据集，包含多个视图，将在未来公开。
</details></li>
</ul>
<hr>
<h2 id="Board-to-Board-Evaluating-Moonboard-Grade-Prediction-Generalization"><a href="#Board-to-Board-Evaluating-Moonboard-Grade-Prediction-Generalization" class="headerlink" title="Board-to-Board: Evaluating Moonboard Grade Prediction Generalization"></a>Board-to-Board: Evaluating Moonboard Grade Prediction Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12419">http://arxiv.org/abs/2311.12419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a1773620/moonboard-grade-prediction">https://github.com/a1773620/moonboard-grade-prediction</a></li>
<li>paper_authors: Daniel Petashvili, Matthew Rodda</li>
<li>for:  This paper aims to develop a grade prediction model for bouldering routes, using classical and deep-learning techniques to improve the accuracy and reduce bias in grading.</li>
<li>methods:  The paper uses the 2016, 2017, and 2019 Moonboard datasets and features a novel vision-based method of grade prediction. The model does not require decomposing routes into individual moves, which is a common method in literature and introduces bias.</li>
<li>results:  The paper achieves state-of-the-art grade prediction performance with 0.87 MAE and 1.12 RMSE on the Moonboard datasets. The generalization performance of these techniques is below human level performance currently, but the authors propose these methods as a basis for future work.<details>
<summary>Abstract</summary>
Bouldering is a sport where athletes aim to climb up an obstacle using a set of defined holds called a route. Typically routes are assigned a grade to inform climbers of its difficulty and allow them to more easily track their progression. However, the variation in individual climbers technical and physical attributes and many nuances of an individual route make grading a difficult and often biased task. In this work, we apply classical and deep-learning modelling techniques to the 2016, 2017 and 2019 Moonboard datasets, achieving state of the art grade prediction performance with 0.87 MAE and 1.12 RMSE. We achieve this performance on a feature-set that does not require decomposing routes into individual moves, which is a method common in literature and introduces bias. We also demonstrate the generalization capability of this model between editions and introduce a novel vision-based method of grade prediction. While the generalization performance of these techniques is below human level performance currently, we propose these methods as a basis for future work. Such a tool could be implemented in pre-existing mobile applications and would allow climbers to better track their progress and assess new routes with reduced bias.
</details>
<details>
<summary>摘要</summary>
布尔дер运动是一种 Athletes 通过一系列定义的持有（route）进行攀登的运动。通常，路由会被分配一个等级，以便 climbers 了解其困难程度并更容易跟踪其进步。然而，个体 climbers 的技术和物理属性的变化以及路由的多种细节使得评分变得困难且偏向。在这项工作中，我们使用古典和深度学习模型技术来对2016、2017和2019年的月坛数据集进行评分，实现了状态的评分性能，MAE 0.87 和 RMSE 1.12。我们实现了这一性能在一个不需要分解路由为个move的特征集上，这是文献中常见的方法，并且引入偏见。我们还示出了这些模型在不同版本之间的泛化性能，并提出了一种新的视觉评分方法。虽然这些技术的总体性能目前还不及人类水平，但我们提出这些方法作为未来工作的基础。这种工具可以在现有的移动应用程序中实现，并允许 climbers 更好地跟踪其进步和评估新路由，减少偏见。
</details></li>
</ul>
<hr>
<h2 id="Learning-Part-Motion-of-Articulated-Objects-Using-Spatially-Continuous-Neural-Implicit-Representations"><a href="#Learning-Part-Motion-of-Articulated-Objects-Using-Spatially-Continuous-Neural-Implicit-Representations" class="headerlink" title="Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations"></a>Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12407">http://arxiv.org/abs/2311.12407</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Yushi-Du/PartMotion">https://github.com/Yushi-Du/PartMotion</a></li>
<li>paper_authors: Yushi Du, Ruihai Wu, Yan Shen, Hao Dong</li>
<li>for:  This paper aims to improve the understanding and manipulation of articulated objects, such as doors and drawers, by introducing a novel framework for modeling their part motions using neural implicit representations.</li>
<li>methods:  The proposed framework explicitly disentangles the part motion of articulated objects by predicting the transformation matrix of points on the part surface, allowing for smooth modeling of part motion in the space. The method is generic to different kinds of joint motions, meaning it can model diverse types of joint motions in the space.</li>
<li>results:  Quantitative and qualitative results of experiments over diverse categories of articulated objects demonstrate the effectiveness of the proposed framework, showing that it can accurately model part motions and handle diverse types of joint motions.<details>
<summary>Abstract</summary>
Articulated objects (e.g., doors and drawers) exist everywhere in our life. Different from rigid objects, articulated objects have higher degrees of freedom and are rich in geometries, semantics, and part functions. Modeling different kinds of parts and articulations with nerual networks plays an essential role in articulated object understanding and manipulation, and will further benefit 3D vision and robotics communities. To model articulated objects, most previous works directly encode articulated objects into feature representations, without specific designs for parts, articulations and part motions. In this paper, we introduce a novel framework that explicitly disentangles the part motion of articulated objects by predicting the transformation matrix of points on the part surface, using spatially continuous neural implicit representations to model the part motion smoothly in the space. More importantly, while many methods could only model a certain kind of joint motion (such as the revolution in the clockwise order), our proposed framework is generic to different kinds of joint motions in that transformation matrix can model diverse kinds of joint motions in the space. Quantitative and qualitative results of experiments over diverse categories of articulated objects demonstrate the effectiveness of our proposed framework.
</details>
<details>
<summary>摘要</summary>
everywhere in our lives 有各种各样的叉体对象（例如门和柜）。与固定对象不同，叉体对象具有更高的自由度和更丰富的几何、 semantics 和部件功能。使用神经网络模型不同种类的部件和叉动可以在叉体对象理解和操作方面发挥重要作用，并对3D视觉和机器人领域产生积极影响。在模型叉体对象方面，大多数前一些工作直接编码叉体对象到特征表示中，没有特定的部件、叉动和部件运动设计。在这篇论文中，我们提出了一种新的框架，其中明确分离叉体对象的部件运动，通过采用空间连续神经凝重表示来平滑地在空间中模型部件运动。此外，我们的提posed框架可以涵盖不同类型的 JOINT 运动，而不是只能模型某种特定的 JOINT 运动（如顺时静转）。经过对不同类型叉体对象的实验，我们获得了证明我们的提posed框架的有效性的数量和质量结果。
</details></li>
</ul>
<hr>
<h2 id="CASR-Refining-Action-Segmentation-via-Magrinalizing-Frame-levle-Causal-Relationships"><a href="#CASR-Refining-Action-Segmentation-via-Magrinalizing-Frame-levle-Causal-Relationships" class="headerlink" title="CASR: Refining Action Segmentation via Magrinalizing Frame-levle Causal Relationships"></a>CASR: Refining Action Segmentation via Magrinalizing Frame-levle Causal Relationships</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12401">http://arxiv.org/abs/2311.12401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keqing Du, Xinyu Yang, Hang Chen</li>
<li>for: 提高 Temporal Action Segmentation (TAS) 任务的解释性，通过深度学习和 causal discovery  integrate。</li>
<li>methods: 提出了 Causal Abstraction Segmentation Refiner (CASR)，可以通过增强视频 causality 来提高 TAS 结果，并且定义了等效帧级 causal model 和段级 causal model，以便在 marginalized 帧级 causal 关系中表示段级 causal 关系。</li>
<li>results: 对主流数据集进行了广泛的实验，结果显示，CASR 可以significantly 超越现有的多种方法在动作分 segmentation 性能、 causal 解释性和泛化性等方面。<details>
<summary>Abstract</summary>
Integrating deep learning and causal discovery has increased the interpretability of Temporal Action Segmentation (TAS) tasks. However, frame-level causal relationships exist many complicated noises outside the segment-level, making it infeasible to directly express macro action semantics. Thus, we propose \textit{\textbf{Causal Abstraction Segmentation Refiner (CASR)}, which can refine TAS results from various models by enhancing video causality in marginalizing frame-level casual relationships. Specifically, we define the equivalent frame-level casual model and segment-level causal model, so that the causal adjacency matrix constructed from marginalized frame-level causal relationships has the ability to represent the segmnet-level causal relationships. CASR works out by reducing the difference in the causal adjacency matrix between we constructed and pre-segmentation results of backbone models. In addition, we propose a novel evaluation metric Causal Edit Distance (CED) to evaluate the causal interpretability. Extensive experimental results on mainstream datasets indicate that CASR significantly surpasses existing various methods in action segmentation performance, as well as in causal explainability and generalization. Our code will be available soon.
</details>
<details>
<summary>摘要</summary>
integrate deep learning and causal discovery 增加了 Temporal Action Segmentation (TAS) 任务的解释性。然而，框架级别的 causal 关系存在多种复杂的噪声，使得直接表达 macro action semantics 变得不可能。因此，我们提议 \textit{\textbf{Causal Abstraction Segmentation Refiner (CASR)}, 可以修复来自不同模型的 TAS 结果，通过增强视频 causality 来增强 marginalized 框架级别 causal 关系。 Specifically, we define the equivalent frame-level casual model and segment-level causal model, so that the causal adjacency matrix constructed from marginalized frame-level causal relationships has the ability to represent the segment-level causal relationships。CASR works by reducing the difference in the causal adjacency matrix between the constructed and pre-segmentation results of backbone models。 In addition, we propose a novel evaluation metric Causal Edit Distance (CED) to evaluate the causal interpretability。Extensive experimental results on mainstream datasets indicate that CASR significantly surpasses existing various methods in action segmentation performance, as well as in causal explainability and generalization。 Our code will be available soon。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="RFTrans-Leveraging-Refractive-Flow-of-Transparent-Objects-for-Surface-Normal-Estimation-and-Manipulation"><a href="#RFTrans-Leveraging-Refractive-Flow-of-Transparent-Objects-for-Surface-Normal-Estimation-and-Manipulation" class="headerlink" title="RFTrans: Leveraging Refractive Flow of Transparent Objects for Surface Normal Estimation and Manipulation"></a>RFTrans: Leveraging Refractive Flow of Transparent Objects for Surface Normal Estimation and Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12398">http://arxiv.org/abs/2311.12398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tutian Tang, Jiyu Liu, Jieyi Zhang, Haoyuan Fu, Wenqiang Xu, Cewu Lu</li>
<li>for: 本研究的目的是教 robots 如何与透明物体互动，因为直接从 RGB 图像中提取 geometry 信息可能会受到反射和折射的影响，导致 RGB-D 摄像头的准确性受到影响。</li>
<li>methods: 本研究提出了 RFTrans，一种基于 RGB-D 的方法，用于透明物体表面法向估计和操作。该方法利用了射流作为中间表示，从而缺乏直接从 RGB 图像中提取 geometry 信息的缺陷，并帮助bridge sim-to-real  gap。RFTrans 组合了 RFNet 和 F2Net，其中 RFNet 预测射流、物体mask 和边界，然后 F2Net 利用射流来估计表面法。为了实现操作，global optimization module 将预测结果与原始深度进行精细调整，并将点云与法向相结合。接下来，使用 ISF 分析抓取算法来生成抓取姿势。</li>
<li>results: 在 synthetic 数据集上训练 RFTrans 后，它可以在 synthetic 和实际世界 benchmark 中 consistently 超越基线 ClearGrasp 的表现，差异较大。此外，在实际世界中，使用 RFTrans 实现了83%的成功率，证明了射流可以帮助实现直接 sim-to-real 转移。<details>
<summary>Abstract</summary>
Transparent objects are widely used in our daily lives, making it important to teach robots to interact with them. However, it's not easy because the reflective and refractive effects can make RGB-D cameras fail to give accurate geometry measurements. To solve this problem, this paper introduces RFTrans, an RGB-D-based method for surface normal estimation and manipulation of transparent objects. By leveraging refractive flow as an intermediate representation, RFTrans circumvents the drawbacks of directly predicting the geometry (e.g. surface normal) from RGB images and helps bridge the sim-to-real gap. RFTrans integrates the RFNet, which predicts refractive flow, object mask, and boundaries, followed by the F2Net, which estimates surface normal from the refractive flow. To make manipulation possible, a global optimization module will take in the predictions, refine the raw depth, and construct the point cloud with normal. An analytical grasp planning algorithm, ISF, is followed to generate the grasp poses. We build a synthetic dataset with physically plausible ray-tracing rendering techniques to train the networks. Results show that the RFTrans trained on the synthetic dataset can consistently outperform the baseline ClearGrasp in both synthetic and real-world benchmarks by a large margin. Finally, a real-world robot grasping task witnesses an 83% success rate, proving that refractive flow can help enable direct sim-to-real transfer. The code, data, and supplementary materials are available at https://rftrans.robotflow.ai.
</details>
<details>
<summary>摘要</summary>
透明物体在我们日常生活中广泛使用，因此教 robot 如何与它们交互变得非常重要。然而，由于镜面和折射效果，RGB-D 摄像头可能无法提供准确的几何测量。为解决这个问题，这篇论文引入了 RFTrans，一种基于 RGB-D 的表面法向量估计和透明物体把握方法。通过利用折射流作为中间表示，RFTrans 可以缺失直接从 RGB 图像中提取几何信息的缺陷，帮助bridges 实验室到实际的差距。RFTrans 结合了 RFNet，这个网络预测了折射流、物体mask 和边界，然后结合 F2Net，它根据折射流来估计表面法向量。为了实现把握，一个全球优化模块会接受预测结果，进行原始深度的精炼，并将点云与表面法向量结合起来。一种基于分析的抓取算法，ISF，用于生成抓取姿势。我们使用physically plausible的投射渲染技术来训练网络。结果显示，使用 synthetic 数据集训练的 RFTrans 可以在 synthetic 和实际 benchmark 中一直高于基准 ClearGrasp 的表现，差异很大。最后，一个实际 robot 抓取任务中观察到 83% 的成功率，证明了折射流可以帮助实际到 simulate 的直接传输。代码、数据和补充材料可以在 <https://rftrans.robotflow.ai> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Rich-and-Poor-Texture-Contrast-A-Simple-yet-Effective-Approach-for-AI-generated-Image-Detection"><a href="#Rich-and-Poor-Texture-Contrast-A-Simple-yet-Effective-Approach-for-AI-generated-Image-Detection" class="headerlink" title="Rich and Poor Texture Contrast: A Simple yet Effective Approach for AI-generated Image Detection"></a>Rich and Poor Texture Contrast: A Simple yet Effective Approach for AI-generated Image Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12397">http://arxiv.org/abs/2311.12397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Zhong, Yiran Xu, Zhenxing Qian, Xinpeng Zhang</li>
<li>for: 本研究旨在开发一种能够识别由各种生成模型生成的假图像的检测器。</li>
<li>methods: 本研究提出了一种基于图像内部纹理区域的对比特征的方法，通过分解图像为多个小块，然后重建这些块，从而提取各个纹理区域之间的对比特征，以用于识别假图像。</li>
<li>results: 对于16种常见的生成模型，本研究的方法能够准确地识别假图像，并且与现有基elines相比，表现出了显著的提高。<details>
<summary>Abstract</summary>
Recent generative models show impressive performance in generating photographic images. Humans can hardly distinguish such incredibly realistic-looking AI-generated images from real ones. AI-generated images may lead to ubiquitous disinformation dissemination. Therefore, it is of utmost urgency to develop a detector to identify AI-generated images. Most existing detectors suffer from sharp performance drops over unseen generative models. In this paper, we propose a novel AI-generated image detector capable of identifying fake images created by a wide range of generative models. Our approach leverages the inter-pixel correlation contrast between rich and poor texture regions within an image. Pixels in rich texture regions exhibit more significant fluctuations than those in poor texture regions. This discrepancy reflects that the entropy of rich texture regions is larger than that of poor ones. Consequently, synthesizing realistic rich texture regions proves to be more challenging for existing generative models. Based on this principle, we divide an image into multiple patches and reconstruct them into two images, comprising rich-texture and poor-texture patches respectively. Subsequently, we extract the inter-pixel correlation discrepancy feature between rich and poor texture regions. This feature serves as a universal fingerprint used for AI-generated image forensics across different generative models. In addition, we build a comprehensive AI-generated image detection benchmark, which includes 16 kinds of prevalent generative models, to evaluate the effectiveness of existing baselines and our approach. Our benchmark provides a leaderboard for follow-up studies. Extensive experimental results show that our approach outperforms state-of-the-art baselines by a significant margin. Our project: https://fdmas.github.io/AIGCDetect/
</details>
<details>
<summary>摘要</summary>
近期的生成模型显示出了惊人的表现，可以生成出非常真实的图像。人们几乎无法分辨这些由AI生成的图像和真实的图像之间的差异。然而，这些AI生成的图像可能会导致广泛的假信息散布。因此，发展一个可以识别AI生成图像的检测器非常有优先性。现有的检测器往往会在未看过的生成模型上表现维度下降。在这篇论文中，我们提出了一种新的AI生成图像检测器，可以识别由各种生成模型生成的假图像。我们的方法利用图像内部像素之间的干扰对比，rich texture region中像素的波动较大，而poor texture region中像素的波动较小。这种差异反映了生成模型Synthesizing realistic rich texture regions是更加困难的。基于这个原理，我们将图像分解成多个patch，然后将它们重construct成两个图像，包括rich texture和poor texture patches。接着，我们提取rich texture和poor texture patches之间的干扰对比特征。这个特征作为一种通用的人工智能生成图像掌略，可以在不同的生成模型上进行检测。此外，我们建立了一个全面的AI生成图像检测 benchmark，包括16种常见的生成模型。我们的 benchmark 提供了一个领导者榜，用于后续的研究。我们的实验结果表明，我们的方法在现有基elines之上表现出了显著的优势。我们的项目：https://fdmas.github.io/AIGCDetect/
</details></li>
</ul>
<hr>
<h2 id="From-Wrong-To-Right-A-Recursive-Approach-Towards-Vision-Language-Explanation"><a href="#From-Wrong-To-Right-A-Recursive-Approach-Towards-Vision-Language-Explanation" class="headerlink" title="From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation"></a>From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12391">http://arxiv.org/abs/2311.12391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Ge, Sanjay Subramanian, Trevor Darrell, Boyi Li</li>
<li>for: 提高视觉理解任务中的解释质量，尤其是在有限的标注数据下。</li>
<li>methods: 提出了一种循环计算视觉特征（基于文本输入）、答案和解释，以逐步改进解释质量。</li>
<li>results: 与先前的方法相比，我们的方法可以更好地 corrrect its own answers，并且在10个纪录中取得了5%的人工标注数据，并且在VCR和VQA-X数据集上取得了4.2和1.3的BLEU-1分数提高。<details>
<summary>Abstract</summary>
Addressing the challenge of adapting pre-trained vision-language models for generating insightful explanations for visual reasoning tasks with limited annotations, we present ReVisE: a $\textbf{Re}$cursive $\textbf{Vis}$ual $\textbf{E}$xplanation algorithm. Our method iteratively computes visual features (conditioned on the text input), an answer, and an explanation, to improve the explanation quality step by step until the answer converges. We find that this multi-step approach guides the model to correct its own answers and outperforms single-step explanation generation. Furthermore, explanations generated by ReVisE also serve as valuable annotations for few-shot self-training. Our approach outperforms previous methods while utilizing merely 5% of the human-annotated explanations across 10 metrics, demonstrating up to a 4.2 and 1.3 increase in BLEU-1 score on the VCR and VQA-X datasets, underscoring the efficacy and data-efficiency of our method.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:面对预训练视语模型适应视理解任务的限制注释挑战，我们提出了ReVisE：一种循环的视释解释算法。我们的方法在每一步计算视特征（基于文本输入）、答案和解释，以改进解释质量，直到答案 converges。我们发现这种多步 Approach 使模型自 corrections 其答案，并且超过单步解释生成。此外，ReVisE 生成的解释还可以作为有限少量自我训练的有价值注释。我们的方法在10个指标中胜过前一代方法，只用5%的人工注释，在 VCR 和 VQA-X 数据集上表现出4.2和1.3的BLEU-1 分数提高，这表明我们的方法的有效性和数据效果。
</details></li>
</ul>
<hr>
<h2 id="Point-Segment-and-Count-A-Generalized-Framework-for-Object-Counting"><a href="#Point-Segment-and-Count-A-Generalized-Framework-for-Object-Counting" class="headerlink" title="Point, Segment and Count: A Generalized Framework for Object Counting"></a>Point, Segment and Count: A Generalized Framework for Object Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12386">http://arxiv.org/abs/2311.12386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huang Zhizhong, Dai Mingliang, Zhang Yi, Zhang Junping, Shan Hongming<br>for:The paper is written for object counting and detection in images, specifically for few-shot and zero-shot scenarios.methods:The paper proposes a generalized framework for object counting and detection based on detection, which combines the advantages of two foundation models (SAM and CLIP) without compromising their zero-shot capability. The framework includes three steps: point, segment, and count.results:The proposed method, PseCo, achieves state-of-the-art performance in both few-shot&#x2F;zero-shot object counting&#x2F;detection on the FSC-147 dataset, with additional results on large-scale COCO and LVIS datasets. The source code is available at GitHub.<details>
<summary>Abstract</summary>
Class-agnostic object counting aims to count all objects in an image with respect to example boxes or class names, \emph{a.k.a} few-shot and zero-shot counting. Current state-of-the-art methods highly rely on density maps to predict object counts, which lacks model interpretability. In this paper, we propose a generalized framework for both few-shot and zero-shot object counting based on detection. Our framework combines the superior advantages of two foundation models without compromising their zero-shot capability: (\textbf{i}) SAM to segment all possible objects as mask proposals, and (\textbf{ii}) CLIP to classify proposals to obtain accurate object counts. However, this strategy meets the obstacles of efficiency overhead and the small crowded objects that cannot be localized and distinguished. To address these issues, our framework, termed PseCo, follows three steps: point, segment, and count. Specifically, we first propose a class-agnostic object localization to provide accurate but least point prompts for SAM, which consequently not only reduces computation costs but also avoids missing small objects. Furthermore, we propose a generalized object classification that leverages CLIP image/text embeddings as the classifier, following a hierarchical knowledge distillation to obtain discriminative classifications among hierarchical mask proposals. Extensive experimental results on FSC-147 dataset demonstrate that PseCo achieves state-of-the-art performance in both few-shot/zero-shot object counting/detection, with additional results on large-scale COCO and LVIS datasets. The source code is available at \url{https://github.com/Hzzone/PseCo}.
</details>
<details>
<summary>摘要</summary>
现代状态的方法都高度依赖密度地图来预测对象数量，这lacks model interpretability。在这篇论文中，我们提出一种通用的基本框架，用于几拟零shot和零shot对象数量计算。我们的框架结合了两种基础模型的优点，不会削弱它们的零shot能力：（i）SAM用于分 segment所有可能的对象，（ii）CLIP用于分类提案以获取准确的对象数量。然而，这种策略遇到了效率 overhead和小型拥挤的对象无法被本地化和分辨的问题。为解决这些问题，我们提出了一种名为PseCo的框架，包括以下三步：（1）提出一种类型不受限制的对象本地化，以提供准确但是最少的点提示 дляSAM，从而降低计算成本并避免小对象被漏掉。（2）提出一种通用对象分类方法，使用CLIP图像/文本嵌入来分类提案，并通过层次知识储存来获得特征分类。（3）对于拥挤的对象，我们提出了一种Point-Segment-Count的方法，通过对点提示进行分割，以提高对象的本地化和分辨。我们的PseCo框架在FSC-147数据集上实现了当前最佳的几拟零shot/零shot对象数量计算/检测性能，并在COCO和LVIS数据集上获得了优秀的结果。代码可以在GitHub上找到：<https://github.com/Hzzone/PseCo>。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-Medical-Image-Segmentation-via-Query-Distribution-Consistency"><a href="#Semi-supervised-Medical-Image-Segmentation-via-Query-Distribution-Consistency" class="headerlink" title="Semi-supervised Medical Image Segmentation via Query Distribution Consistency"></a>Semi-supervised Medical Image Segmentation via Query Distribution Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12364">http://arxiv.org/abs/2311.12364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rong Wu, Dehua Li, Cong Zhang</li>
<li>for: 这个论文是为了提出一个基于两个模组的DUAL KMAX UX-Net框架，用于实现半指导式医疗影像分类。</li>
<li>methods: 这个方法使用了3D UX-Net作为我们的背景元架，并且使用KMax解oder来强化分类性能。我们还采用了一种相互学习策略，将两个模组相互连接，以优化分类性能。</li>
<li>results: 实验结果显示，我们的方法可以将未abeled Data与labeled Data融合，以提高分类性能。同时，我们的框架在10%和20%具有labeled数据的设定下，与现有的半指导式学习方法相比，表现更好。<details>
<summary>Abstract</summary>
Semi-supervised learning is increasingly popular in medical image segmentation due to its ability to leverage large amounts of unlabeled data to extract additional information. However, most existing semi-supervised segmentation methods focus only on extracting information from unlabeled data. In this paper, we propose a novel Dual KMax UX-Net framework that leverages labeled data to guide the extraction of information from unlabeled data. Our approach is based on a mutual learning strategy that incorporates two modules: 3D UX-Net as our backbone meta-architecture and KMax decoder to enhance the segmentation performance. Extensive experiments on the Atrial Segmentation Challenge dataset have shown that our method can significantly improve performance by merging unlabeled data. Meanwhile, our framework outperforms state-of-the-art semi-supervised learning methods on 10\% and 20\% labeled settings. Code located at: https://github.com/Rows21/DK-UXNet.
</details>
<details>
<summary>摘要</summary>
semi-supervised learning在医疗图像分割中日益受欢迎，因为它可以利用大量无标签数据来提取更多的信息。然而，大多数现有的半指导学习分割方法仅仅是利用无标签数据提取信息。在这篇论文中，我们提出了一种新的 dual KMax UX-Net框架，该框架利用标注数据来引导无标签数据中的信息提取。我们的方法基于一种互助学习策略，该策略包括3D UX-Net作为我们的基础元体系和KMax解码器来增强分割性能。我们的实验在Atrial Segmentation Challenge数据集上展现了，我们的方法可以显著提高性能，并且在10%和20%标注设置下超过了当前最佳的半指导学习方法。代码位于：https://github.com/Rows21/DK-UXNet。
</details></li>
</ul>
<hr>
<h2 id="Modality-Mixer-Exploiting-Complementary-Information-for-Multi-modal-Action-Recognition"><a href="#Modality-Mixer-Exploiting-Complementary-Information-for-Multi-modal-Action-Recognition" class="headerlink" title="Modality Mixer Exploiting Complementary Information for Multi-modal Action Recognition"></a>Modality Mixer Exploiting Complementary Information for Multi-modal Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12344">http://arxiv.org/abs/2311.12344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumin Lee, Sangmin Woo, Muhammad Adi Nugroho, Changick Kim</li>
<li>for: 本研究旨在提高多modal action recognition的精度，通过efficacious incorporation of complementary information across modalities and temporal context of actions。</li>
<li>methods: 我们提出了一种新网络 architecture，名为Modality Mixer（M-Mixer）网络，它可以有效地利用不同modalities之间的协同信息，同时考虑action的时间上下文。MCU是我们的关键组件，它可以在一个模式（例如RGB）中编码动作内容特征，并且在其他模式（例如深度和红外模式）中提取动作内容特征。此外，我们还提出了一个新的模块，名为Complementary Feature Extraction Module（CFEM），它可以提取不同模式之间的相互补充信息。</li>
<li>results: 我们的提出方法在NTU RGB+D 60、NTU RGB+D 120和NW-UCLA数据集上达到了现有方法的最高水平。此外，我们还进行了详细的ablation study，以验证我们的提出方法的有效性。<details>
<summary>Abstract</summary>
Due to the distinctive characteristics of sensors, each modality exhibits unique physical properties. For this reason, in the context of multi-modal action recognition, it is important to consider not only the overall action content but also the complementary nature of different modalities. In this paper, we propose a novel network, named Modality Mixer (M-Mixer) network, which effectively leverages and incorporates the complementary information across modalities with the temporal context of actions for action recognition. A key component of our proposed M-Mixer is the Multi-modal Contextualization Unit (MCU), a simple yet effective recurrent unit. Our MCU is responsible for temporally encoding a sequence of one modality (e.g., RGB) with action content features of other modalities (e.g., depth and infrared modalities). This process encourages M-Mixer network to exploit global action content and also to supplement complementary information of other modalities. Furthermore, to extract appropriate complementary information regarding to the given modality settings, we introduce a new module, named Complementary Feature Extraction Module (CFEM). CFEM incorporates sepearte learnable query embeddings for each modality, which guide CFEM to extract complementary information and global action content from the other modalities. As a result, our proposed method outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA datasets. Moreover, through comprehensive ablation studies, we further validate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
due to the distinctive characteristics of sensors, each modality exhibits unique physical properties. For this reason, in the context of multi-modal action recognition, it is important to consider not only the overall action content but also the complementary nature of different modalities. In this paper, we propose a novel network, named Modality Mixer (M-Mixer) network, which effectively leverages and incorporates the complementary information across modalities with the temporal context of actions for action recognition. A key component of our proposed M-Mixer is the Multi-modal Contextualization Unit (MCU), a simple yet effective recurrent unit. Our MCU is responsible for temporally encoding a sequence of one modality (e.g., RGB) with action content features of other modalities (e.g., depth and infrared modalities). This process encourages M-Mixer network to exploit global action content and also to supplement complementary information of other modalities. Furthermore, to extract appropriate complementary information regarding to the given modality settings, we introduce a new module, named Complementary Feature Extraction Module (CFEM). CFEM incorporates separate learnable query embeddings for each modality, which guide CFEM to extract complementary information and global action content from the other modalities. As a result, our proposed method outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA datasets. Moreover, through comprehensive ablation studies, we further validate the effectiveness of our proposed method.Here's the breakdown of the translation:* due to the distinctive characteristics of sensors: 由于各种感测器的特殊特点* each modality exhibits unique physical properties: 每种模式都表现出独特的物理特性* For this reason, in the context of multi-modal action recognition: 因此，在多modal动作识别中，需要考虑不同模式之间的相互补做* it is important to consider not only the overall action content: 需要考虑不同模式之间的相互补做，以及动作内容的总体特征* but also the complementary nature of different modalities: 并且需要考虑不同模式之间的补做关系* In this paper, we propose a novel network, named Modality Mixer (M-Mixer) network: 在本文中，我们提出了一种新的网络 architecture，名为Modal Mixer（M-Mixer）网络* which effectively leverages and incorporates the complementary information across modalities: 可以有效地利用和集成不同模式之间的补做信息* with the temporal context of actions for action recognition: 以动作识别的时间上下文为基础* A key component of our proposed M-Mixer is the Multi-modal Contextualization Unit (MCU): 我们的提议的M-Mixer中的关键组件是多modal上下文化单元（MCU）* a simple yet effective recurrent unit: 一种简单 yet 有效的循环单元* Our MCU is responsible for temporally encoding a sequence of one modality (e.g., RGB) with action content features of other modalities (e.g., depth and infrared modalities): 我们的MCU负责在不同模式之间进行时间编码，以便从其他模式中提取动作内容特征* This process encourages M-Mixer network to exploit global action content and also to supplement complementary information of other modalities: 这个过程鼓励M-Mixer网络利用全局动作内容，并且补做其他模式的补做信息* Furthermore, to extract appropriate complementary information regarding to the given modality settings, we introduce a new module, named Complementary Feature Extraction Module (CFEM): 此外，为了在给定的模式设置下提取适当的补做信息，我们提出了一个新的模块，名为补做特征提取模块（CFEM）* CFEM incorporates separate learnable query embeddings for each modality, which guide CFEM to extract complementary information and global action content from the other modalities: CFEM使用每个模式的分离学习的查询嵌入，以便从其他模式中提取补做信息和全局动作内容* As a result, our proposed method outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA datasets: 因此，我们的提议方法在NTU RGB+D 60、NTU RGB+D 120和NW-UCLA数据集上表现出了更高的性能* Moreover, through comprehensive ablation studies, we further validate the effectiveness of our proposed method: 此外，我们还通过了全面的抽象研究，以验证我们的提议方法的有效性
</details></li>
</ul>
<hr>
<h2 id="LoCo-Locally-Constrained-Training-Free-Layout-to-Image-Synthesis"><a href="#LoCo-Locally-Constrained-Training-Free-Layout-to-Image-Synthesis" class="headerlink" title="LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis"></a>LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12342">http://arxiv.org/abs/2311.12342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiang Zhao, Han Li, Ruiyang Jin, S. Kevin Zhou</li>
<li>for: Layout-to-image synthesis with high-quality images aligned with textual prompts and spatial layouts.</li>
<li>methods: Localized Attention Constraint and Padding Token Constraint to refine cross-attention and leverage semantic information from padding tokens.</li>
<li>results: Superior performance compared to existing training-free layout-to-image methods, with qualitative and quantitative improvements across multiple benchmarks.Here’s the summary in Traditional Chinese:</li>
<li>for: 实现文本描述和空间配置 aligned 的高品质图像生成。</li>
<li>methods: 地域化注意力限制和padding token限制，从padding tokens中获取semantic information，避免实体融合问题。</li>
<li>results: 与现有training-free layout-to-image方法相比，具有较高的表现和多个benchmark上的量化改进。<details>
<summary>Abstract</summary>
Recent text-to-image diffusion models have reached an unprecedented level in generating high-quality images. However, their exclusive reliance on textual prompts often falls short in accurately conveying fine-grained spatial compositions. In this paper, we propose LoCo, a training-free approach for layout-to-image synthesis that excels in producing high-quality images aligned with both textual prompts and spatial layouts. Our method introduces a Localized Attention Constraint to refine cross-attention for individual objects, ensuring their precise placement in designated regions. We further propose a Padding Token Constraint to leverage the semantic information embedded in previously neglected padding tokens, thereby preventing the undesired fusion of synthesized objects. LoCo seamlessly integrates into existing text-to-image and layout-to-image models, significantly amplifying their performance and effectively addressing semantic failures observed in prior methods. Through extensive experiments, we showcase the superiority of our approach, surpassing existing state-of-the-art training-free layout-to-image methods both qualitatively and quantitatively across multiple benchmarks.
</details>
<details>
<summary>摘要</summary>
最近的文本到图像扩散模型已达到历史上无 precedent 的水平，但它们偏重文本提示语 часто不够精确地传达细腻的空间组合。在这篇论文中，我们提议LoCo，一种不需要训练的方法 для文本到图像合成，可以生成高质量的图像，同时兼顾文本提示语和空间布局。我们的方法引入了局部注意力约束，以提高对个体物体的横跨注意力，使其准确地放置在指定的区域中。此外，我们还提出了补充符约束，以利用padding token中嵌入的semantic信息，避免synthesized对象的不良融合。LoCo可以顺利地与现有的文本到图像和布局到图像模型结合使用，明显提高其性能，并有效地解决先前方法中的语义失败。通过广泛的实验，我们展示了我们的方法的超越性，在多个 benchmark 上胜过现有的状态态的training-free布局到图像方法， both qualitatively and quantitatively。
</details></li>
</ul>
<hr>
<h2 id="ViLaM-A-Vision-Language-Model-with-Enhanced-Visual-Grounding-and-Generalization-Capability"><a href="#ViLaM-A-Vision-Language-Model-with-Enhanced-Visual-Grounding-and-Generalization-Capability" class="headerlink" title="ViLaM: A Vision-Language Model with Enhanced Visual Grounding and Generalization Capability"></a>ViLaM: A Vision-Language Model with Enhanced Visual Grounding and Generalization Capability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12327">http://arxiv.org/abs/2311.12327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymgiant/vilam">https://github.com/anonymgiant/vilam</a></li>
<li>paper_authors: Xiaoyu Yang, Lijian Xu, Hongsheng Li, Shaoting Zhang</li>
<li>for: 这个研究旨在提出一个统一的视力语言模型（ViLaM），以便将大型预训语言模型的知识和推理能力最佳化 для跨modal任务。</li>
<li>methods: 这个模型使用预先冻结的预训encoder来对图像和文本特征进行编码和对alignment，以便处理多种基于文本指令的视觉任务。此外，我们还提出了循环训练的方法来处理参考表达的问题。</li>
<li>results: 我们在公共通用dataset上评估了ViLaM的表现，并确认了它在医学dataset上的一般化性。此外，我们还观察到了模型的优异零shot学习能力，这表明了未来可能将ViLaM应用在医学领域。<details>
<summary>Abstract</summary>
Vision-language models have revolutionized human-computer interaction and shown significant progress in multi-modal tasks. However, applying these models to complex visual tasks like medical image analysis remains challenging. In this study, we propose ViLaM, a unified Vision-Language transformer model that integrates instruction tuning predicated on a large language model. This approach enables us to optimally utilize the knowledge and reasoning capacities of large pre-trained language models for an array of tasks encompassing both language and vision. We employ frozen pre-trained encoders to encode and align both image and text features, enabling ViLaM to handle a variety of visual tasks following textual instructions. Besides, we've designed cycle training for referring expressions to address the need for high-quality, paired referring expression datasets for training large models in terms of both quantity and quality. We evaluated ViLaM's exceptional performance on public general datasets and further confirmed its generalizability on medical datasets. Importantly, we've observed the model's impressive zero-shot learning ability, indicating the potential future application of ViLaM in the medical field.
</details>
<details>
<summary>摘要</summary>
视力语言模型已经革命化人机交互和多模态任务，但对复杂视觉任务如医疗图像分析仍然是挑战。在这项研究中，我们提出了ViLaM，一种 integrate 视力语言变换模型，它通过基于大型预训练语言模型的 instruction 预测来优化视觉任务的执行。我们使用冻结预训练encoder来编码和对应图像和文本特征，使ViLaM可以处理多种视觉任务， seguido de textual  instrucciones。此外，我们还设计了 cycle 训练来改进 Referring 表达，以满足大型模型的训练所需的高质量、量化 paired  Referring 表达数据集。我们在公共普遍数据集上评估了ViLaM的突出表现，并证明其普遍性在医疗数据集上。进一步，我们发现了模型的强大零shot 学习能力，表明 ViLaM 在未来可能在医疗领域应用。
</details></li>
</ul>
<hr>
<h2 id="ABFL-Angular-Boundary-Discontinuity-Free-Loss-for-Arbitrary-Oriented-Object-Detection-in-Aerial-Images"><a href="#ABFL-Angular-Boundary-Discontinuity-Free-Loss-for-Arbitrary-Oriented-Object-Detection-in-Aerial-Images" class="headerlink" title="ABFL: Angular Boundary Discontinuity Free Loss for Arbitrary Oriented Object Detection in Aerial Images"></a>ABFL: Angular Boundary Discontinuity Free Loss for Arbitrary Oriented Object Detection in Aerial Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12311">http://arxiv.org/abs/2311.12311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zifei Zhao, Shengyang Li</li>
<li>for: 这篇论文主要关注的是空中图像中的自由方向物体检测（AOOD） task，并且提出了一个基于 von Mises 分布的 Angular Boundary Free Loss（ABFL）来解决了这个问题。</li>
<li>methods: 本文使用了一个简单且有效的方法，即将角度视为圆形数据，从而引入角度周期性来减少ABD问题，并且不需要额外的编码-解码结构。</li>
<li>results: 实验结果显示，提出的ABFL损失函数可以比一些州前方法更好地解决ABD问题，并且在DOTA和HRSC2016数据集上表现出色。<details>
<summary>Abstract</summary>
Arbitrary oriented object detection (AOOD) in aerial images is a widely concerned and highly challenging task, and plays an important role in many scenarios. The core of AOOD involves the representation, encoding, and feature augmentation of oriented bounding-boxes (Bboxes). Existing methods lack intuitive modeling of angle difference measurement in oriented Bbox representations. Oriented Bboxes under different representations exhibit rotational symmetry with varying periods due to angle periodicity. The angular boundary discontinuity (ABD) problem at periodic boundary positions is caused by rotational symmetry in measuring angular differences. In addition, existing methods also use additional encoding-decoding structures for oriented Bboxes. In this paper, we design an angular boundary free loss (ABFL) based on the von Mises distribution. The ABFL aims to solve the ABD problem when detecting oriented objects. Specifically, ABFL proposes to treat angles as circular data rather than linear data when measuring angle differences, aiming to introduce angle periodicity to alleviate the ABD problem and improve the accuracy of angle difference measurement. In addition, ABFL provides a simple and effective solution for various periodic boundary discontinuities caused by rotational symmetry in AOOD tasks, as it does not require additional encoding-decoding structures for oriented Bboxes. Extensive experiments on the DOTA and HRSC2016 datasets show that the proposed ABFL loss outperforms some state-of-the-art methods focused on addressing the ABD problem.
</details>
<details>
<summary>摘要</summary>
aerial 图像中的随机方向对象检测（AOOD）是一项广泛关注且高度挑战的任务，对多种场景都具有重要作用。 AOOD 的核心在于对 oriented bounding-boxes（Bboxes）的表示、编码和特征增强。现有方法缺乏对夹角差的直观表示。oriented Bboxes 在不同的表示下展现出固定期的旋转对称，导致测量夹角差的问题。此外，现有方法还使用了额外的编码-解码结构 для oriented Bboxes。在这篇论文中，我们设计了一种角度边界缺失损失（ABFL）基于麦斯维度分布。ABFL 目的是解决在检测oriented对象时出现的夹角边界缺失问题。特别是，ABFL 将 treat angles 为圆形数据而不是直线数据，以便在测量夹角差时引入圆形对称，以解决夹角边界缺失问题并提高对角度差的准确度。此外，ABFL 不需要额外的编码-解码结构，可以简单地和有效地解决由旋转对称引起的 periodic boundary discontinuities 在 AOOD 任务中。广泛的实验表明，我们提出的 ABFL 损失可以超越一些专注于解决 ABD 问题的state-of-the-art方法。
</details></li>
</ul>
<hr>
<h2 id="Challenges-in-Video-Based-Infant-Action-Recognition-A-Critical-Examination-of-the-State-of-the-Art"><a href="#Challenges-in-Video-Based-Infant-Action-Recognition-A-Critical-Examination-of-the-State-of-the-Art" class="headerlink" title="Challenges in Video-Based Infant Action Recognition: A Critical Examination of the State of the Art"></a>Challenges in Video-Based Infant Action Recognition: A Critical Examination of the State of the Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12300">http://arxiv.org/abs/2311.12300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ostadabbas/video-based-infant-action-recognition">https://github.com/ostadabbas/video-based-infant-action-recognition</a></li>
<li>paper_authors: Elaheh Hatamimajoumerd, Pooria Daneshvar Kakhaki, Xiaofei Huang, Lingfei Luan, Somaieh Amraee, Sarah Ostadabbas</li>
<li>for: 这个论文的目的是研究婴儿动作识别，这是计算机视觉领域的一个快速发展领域，有很多应用，例如监控、安全、人computer交互、医疗诊断等。</li>
<li>methods: 这篇论文使用了一个新的数据集“InfActPrimitive”，这个数据集包含5种重要的婴儿发展阶段动作类别，并使用了专门的预处理技术来处理婴儿数据。</li>
<li>results: 研究发现，使用 poseC3D 模型可以达到约71%的准确率，但其他模型很难正确地捕捉婴儿动作的动态特征。这显示出婴儿动作识别领域和成年人动作识别领域之间的知识差距非常大，需要开发数据效率的管道模型。<details>
<summary>Abstract</summary>
Automated human action recognition, a burgeoning field within computer vision, boasts diverse applications spanning surveillance, security, human-computer interaction, tele-health, and sports analysis. Precise action recognition in infants serves a multitude of pivotal purposes, encompassing safety monitoring, developmental milestone tracking, early intervention for developmental delays, fostering parent-infant bonds, advancing computer-aided diagnostics, and contributing to the scientific comprehension of child development. This paper delves into the intricacies of infant action recognition, a domain that has remained relatively uncharted despite the accomplishments in adult action recognition. In this study, we introduce a groundbreaking dataset called ``InfActPrimitive'', encompassing five significant infant milestone action categories, and we incorporate specialized preprocessing for infant data. We conducted an extensive comparative analysis employing cutting-edge skeleton-based action recognition models using this dataset. Our findings reveal that, although the PoseC3D model achieves the highest accuracy at approximately 71%, the remaining models struggle to accurately capture the dynamics of infant actions. This highlights a substantial knowledge gap between infant and adult action recognition domains and the urgent need for data-efficient pipeline models.
</details>
<details>
<summary>摘要</summary>
自动化人类动作识别，计算机视觉领域的一个快速发展领域，具有广泛的应用领域，包括监控、安全、人机交互、 теле医疗和体育分析。精准的 infant 动作识别服务多种重要目的，包括安全监控、发展阶段跟踪、早期发现发展延迟、促进父母婴儿缔造、计算机助け的诊断和加强科学child development的认知。本文对 infant 动作识别领域进行了深入的分析，这个领域尚未得到了成人动作识别领域的同等关注，尽管在成人动作识别方面已经取得了 significante accomplishments。在这项研究中，我们提出了一个名为“InfActPrimitive”的新的数据集，包括5个重要的 infant 针对动作类别，并对婴儿数据进行了特殊的预处理。我们通过使用 cutting-edge skeleton-based action recognition models 对这个数据集进行了广泛的比较分析。我们的发现表明，虽然 PoseC3D 模型的准确率达到了大约 71%，但其他模型很难准确地捕捉婴儿动作的动态特征。这种情况 highlights 成人动作识别领域和 infant 动作识别领域之间的知识差距，以及需要数据效率的ipeline models。
</details></li>
</ul>
<hr>
<h2 id="Instance-aware-3D-Semantic-Segmentation-powered-by-Shape-Generators-and-Classifiers"><a href="#Instance-aware-3D-Semantic-Segmentation-powered-by-Shape-Generators-and-Classifiers" class="headerlink" title="Instance-aware 3D Semantic Segmentation powered by Shape Generators and Classifiers"></a>Instance-aware 3D Semantic Segmentation powered by Shape Generators and Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12291">http://arxiv.org/abs/2311.12291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Sun, Qixing Huang, Xiangru Huang</li>
<li>for: 本研究旨在提出一种新的3Dsemantic segmentation方法，能够在实例 уров层提取形状信息。</li>
<li>methods: 该方法结合了多个geometry处理任务，以实例水平进行监督，以提高学习的特征表示。具体来说，该方法使用形状生成器和形状分类器来进行形状重建和分类任务，以便在不同实例之间共享相似的形状特征。</li>
<li>results: 在多个公共benchmark上，该方法与现有方法进行比较，得到了显著的性能提升。<details>
<summary>Abstract</summary>
Existing 3D semantic segmentation methods rely on point-wise or voxel-wise feature descriptors to output segmentation predictions. However, these descriptors are often supervised at point or voxel level, leading to segmentation models that can behave poorly at instance-level. In this paper, we proposed a novel instance-aware approach for 3D semantic segmentation. Our method combines several geometry processing tasks supervised at instance-level to promote the consistency of the learned feature representation. Specifically, our methods use shape generators and shape classifiers to perform shape reconstruction and classification tasks for each shape instance. This enforces the feature representation to faithfully encode both structural and local shape information, with an awareness of shape instances. In the experiments, our method significantly outperform existing approaches in 3D semantic segmentation on several public benchmarks, such as Waymo Open Dataset, SemanticKITTI and ScanNetV2.
</details>
<details>
<summary>摘要</summary>
现有的3Dsemantic segmentation方法通常采用点级或封顶级特征描述器来输出 segmentation 预测。然而，这些描述器经常是以点或封顶为基础，导致 segmentation 模型在实例级别上表现不佳。在这篇论文中，我们提出了一种新的实例意识的方法 для 3Dsemantic segmentation。我们的方法结合了多个geometry处理任务，以实例级supervise 来提高学习的特征表示的一致性。Specifically，我们使用形态生成器和形态分类器来实现形态重建和分类任务，以确保特征表示能够准确地编码 strucutral 和局部形态信息，同时具备实例意识。在实验中，我们的方法在多个公共benchmark上显著超过了现有方法，如 Waymo Open Dataset、SemanticKITTI 和 ScanNetV2。
</details></li>
</ul>
<hr>
<h2 id="Procedural-Generation-of-Grain-Orientations-using-the-Wave-Function-Collapse-Algorithm"><a href="#Procedural-Generation-of-Grain-Orientations-using-the-Wave-Function-Collapse-Algorithm" class="headerlink" title="Procedural Generation of Grain Orientations using the Wave Function Collapse Algorithm"></a>Procedural Generation of Grain Orientations using the Wave Function Collapse Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12272">http://arxiv.org/abs/2311.12272</a></li>
<li>repo_url: None</li>
<li>paper_authors: G. Magny-Fokam, D. Madisetti, J. El-Awady</li>
<li>for: 研究生成代表性的颗粒微结构，以便进一步分析附加的塑性和失效行为。</li>
<li>methods: 使用Wave Function Collapse（WFC）算法和Markov Junior（MJ）软件进行生成代表性颗粒微结构。</li>
<li>results: Markov Junior算法能够生成具有相似orientation和体积分数的 Voronoi划分，可以用于生成EBSD图像，并且与参考图像的orientation和体积分数相似。<details>
<summary>Abstract</summary>
Statistics of grain sizes and orientations in metals correlate to the material's mechanical properties. Reproducing representative volume elements for further analysis of deformation and failure in metals, like 316L stainless steel, is particularly important due to their wide use in manufacturing goods today. Two approaches, initially created for video games, were considered for the procedural generation of representative grain microstructures. The first is the Wave Function Collapse (WFC) algorithm, and the second is constraint propagation and probabilistic inference through Markov Junior, a free and open-source software. This study aimed to investigate these two algorithms' effectiveness in using reference electron backscatter diffraction (EBSD) maps and recreating a statistically similar one that could be used in further research. It utilized two stainless steel EBSD maps as references to test both algorithms. First, the WFC algorithm was too constricting and, thus, incapable of producing images that resembled EBSDs. The second, MarkovJunior, was much more effective in creating a Voronoi tessellation that could be used to create an EBSD map in Python. When comparing the results between the reference and the generated EBSD, we discovered that the orientation and volume fractions were extremely similar. With the study, it was concluded that MarkovJunior is an effective machine learning tool that can reproduce representative grain microstructures.
</details>
<details>
<summary>摘要</summary>
统计性粉体大小和方向在金属中与材料的机械性能相关。为了进一步分析金属的变形和失败，如316L不锈钢，复制代表性粉体微结构非常重要。这种问题的两种方法， initially created for video games， were considered for the procedural generation of representative grain microstructures。一个是波函数崩溃（WFC）算法，另一个是Markov Junior，一个免费和开源的软件。本研究的目的是调查这两种算法在使用参考电子后退干涉diffraction（EBSD）地图来生成一个统计相似的粉体微结构，并在Python中使用Voronoi划分来生成EBSD地图。使用了两个钢铁EBSD地图作为参考，我们发现WFC算法无法生成与EBSD地图相似的图像。然而，Markov Junior算法非常有效，可以生成一个Voronoi划分，并在Python中使用这个划分来生成一个EBSD地图。对比refs和生成的EBSD地图，我们发现它们的方向和体积分布非常相似。结论是Markov Junior是一个有效的机器学习工具，可以复制代表性粉体微结构。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Audio-visual-Zero-shot-Learning-with-Large-Language-Models"><a href="#Boosting-Audio-visual-Zero-shot-Learning-with-Large-Language-Models" class="headerlink" title="Boosting Audio-visual Zero-shot Learning with Large Language Models"></a>Boosting Audio-visual Zero-shot Learning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12268">http://arxiv.org/abs/2311.12268</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenhaoxing/KDA">https://github.com/chenhaoxing/KDA</a></li>
<li>paper_authors: Haoxing Chen, Yaohui Li, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Huijia Zhu, Weiqiang Wang</li>
<li>for: Recognize unseen categories based on paired audio-visual sequences.</li>
<li>methods: Propose a simple yet effective framework named Knowledge-aware Distribution Adaptation (KDA) to better grasp novel action contents with an external knowledge base.</li>
<li>results: Outperform state-of-the-art methods on three popular audio-visual zero-shot learning datasets.<details>
<summary>Abstract</summary>
Audio-visual zero-shot learning aims to recognize unseen categories based on paired audio-visual sequences. Recent methods mainly focus on learning aligned and discriminative multi-modal features to boost generalization towards unseen categories. However, these approaches ignore the obscure action concepts in category names and may inevitably introduce complex network structures with difficult training objectives. In this paper, we propose a simple yet effective framework named Knowledge-aware Distribution Adaptation (KDA) to help the model better grasp the novel action contents with an external knowledge base. Specifically, we first propose using large language models to generate rich descriptions from category names, which leads to a better understanding of unseen categories. Additionally, we propose a distribution alignment loss as well as a knowledge-aware adaptive margin loss to further improve the generalization ability towards unseen categories. Extensive experimental results demonstrate that our proposed KDA can outperform state-of-the-art methods on three popular audio-visual zero-shot learning datasets. Our code will be avaliable at \url{https://github.com/chenhaoxing/KDA}.
</details>
<details>
<summary>摘要</summary>
audio-visual零shot学习目标是通过对应的音频视频序列认识未经见过的类别。现有方法主要关注学习对应的多Modal特征，以提高对未经见过的类别的泛化能力。然而，这些方法可能忽略类别名称中的抽象动作概念，并且可能会采用复杂的网络结构和困难的训练目标。在这篇论文中，我们提出了一个简单 yet effective的框架，即知识aware Distribution Adaptation（KDA），以帮助模型更好地理解未经见过的动作内容。 Specifically,我们首先提出使用大型自然语言模型生成类别名称中的丰富描述，以便更好地理解未经见过的类别。其次，我们提出了分布对齐损失以及知识aware adaptive Margin损失，以进一步提高对未经见过的类别的泛化能力。我们的实验结果表明，我们的提出的KDA可以在三个 популяр的 audio-visual零shot学习数据集上表现出色，超过当前state-of-the-art方法。我们的代码将在 \url{https://github.com/chenhaoxing/KDA} 上提供。
</details></li>
</ul>
<hr>
<h2 id="Virtual-Home-Staging-Inverse-Rendering-and-Editing-an-Indoor-Panorama-under-Natural-Illumination"><a href="#Virtual-Home-Staging-Inverse-Rendering-and-Editing-an-Indoor-Panorama-under-Natural-Illumination" class="headerlink" title="Virtual Home Staging: Inverse Rendering and Editing an Indoor Panorama under Natural Illumination"></a>Virtual Home Staging: Inverse Rendering and Editing an Indoor Panorama under Natural Illumination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12265">http://arxiv.org/abs/2311.12265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gzhji/cali-hdr-dataset">https://github.com/gzhji/cali-hdr-dataset</a></li>
<li>paper_authors: Guanzhou Ji, Azadeh O. Sawyer, Srinivasa G. Narasimhan</li>
<li>for: 这篇论文是为了解决现有indoor panoramic图像下新家具布局的问题，使用自然照明。</li>
<li>methods: 该方法包括三个关键组成部分：（1）扫描房间内的家具检测和移除，（2）自动生成房间地板设计，（3）基于场景几何、新家具对象和实时户外照片的全景渲染。</li>
<li>results: 该方法可以在不同的户外照明条件下实现indoor场景的重新渲染，并且提供了一个新的标准化HDR数据集（Cali-HDR），包括137个标准化indoor panoramic图像和其关联的户外照片。<details>
<summary>Abstract</summary>
We propose a novel inverse rendering method that enables the transformation of existing indoor panoramas with new indoor furniture layouts under natural illumination. To achieve this, we captured indoor HDR panoramas along with real-time outdoor hemispherical HDR photographs. Indoor and outdoor HDR images were linearly calibrated with measured absolute luminance values for accurate scene relighting. Our method consists of three key components: (1) panoramic furniture detection and removal, (2) automatic floor layout design, and (3) global rendering with scene geometry, new furniture objects, and a real-time outdoor photograph. We demonstrate the effectiveness of our workflow in rendering indoor scenes under different outdoor illumination conditions. Additionally, we contribute a new calibrated HDR (Cali-HDR) dataset that consists of 137 calibrated indoor panoramas and their associated outdoor photographs. The source code and dataset are available: https://github.com/Gzhji/Cali-HDR-Dataset.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的反向渲染方法，可以将现有的室内投影改变为新的室内家具布局，同时在自然照明下进行变换。为实现这一点，我们捕捉了室内高Dynamic Range（HDR）投影，以及实时的外部半球形HDR照片。室内和外部HDR图像在灯光照明下进行线性投影，并与测量的绝对亮度值进行准确的场景重新照明。我们的方法包括三个关键组成部分：（1）投影式家具检测和移除，（2）自动化地板设计，（3）基于场景几何、新的家具对象和实时外部照片的全球渲染。我们 demonstarte了我们的工作流程可以在不同的外部照明条件下进行渲染室内场景。此外，我们还提供了一个新的标准化HDR（Cali-HDR）数据集，该数据集包括137个标准化的室内投影和其相关的外部照片。源代码和数据集可以在以下链接中下载：https://github.com/Gzhji/Cali-HDR-Dataset。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/21/cs.CV_2023_11_21/" data-id="clpxp040l00njfm8825f5gjbe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/21/eess.AS_2023_11_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-11-21
        
      </div>
    </a>
  
  
    <a href="/2023/11/21/cs.AI_2023_11_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-21</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
