
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-01 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="What User Behaviors Make the Differences During the Process of Visual Analytics? paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00690 repo_url: None paper_authors: Shahin Doroudian, Zekun Wu, Aidong Lu for: 这项研">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-01">
<meta property="og:url" content="https://nullscc.github.io/2023/11/01/cs.CV_2023_11_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="What User Behaviors Make the Differences During the Process of Visual Analytics? paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00690 repo_url: None paper_authors: Shahin Doroudian, Zekun Wu, Aidong Lu for: 这项研">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-01T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-03T00:52:12.898Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/01/cs.CV_2023_11_01/" class="article-date">
  <time datetime="2023-11-01T13:00:00.000Z" itemprop="datePublished">2023-11-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-01
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="What-User-Behaviors-Make-the-Differences-During-the-Process-of-Visual-Analytics"><a href="#What-User-Behaviors-Make-the-Differences-During-the-Process-of-Visual-Analytics" class="headerlink" title="What User Behaviors Make the Differences During the Process of Visual Analytics?"></a>What User Behaviors Make the Differences During the Process of Visual Analytics?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00690">http://arxiv.org/abs/2311.00690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahin Doroudian, Zekun Wu, Aidong Lu</li>
<li>for: 这项研究的目的是为了提高视觉分析过程中的设计和互动功能，以帮助视觉研究人员从多个方面获得启发。</li>
<li>methods: 这项研究使用了时间序列分类方法来分析用户行为记录，以解决用户行为的复杂性和我们对用户行为的不了解。</li>
<li>results: 研究发现用户在视觉分析过程中的行为可以分 distinguish，并且存在用户physical behaviors和视觉任务之间的强相关性。此外，我们还展示了如何使用我们的模型来自动地研究感知过程，不需要繁琐的手动标注。<details>
<summary>Abstract</summary>
The understanding of visual analytics process can benefit visualization researchers from multiple aspects, including improving visual designs and developing advanced interaction functions. However, the log files of user behaviors are still hard to analyze due to the complexity of sensemaking and our lack of knowledge on the related user behaviors. This work presents a study on a comprehensive data collection of user behaviors, and our analysis approach with time-series classification methods. We have chosen a classical visualization application, Covid-19 data analysis, with common analysis tasks covering geo-spatial, time-series and multi-attributes. Our user study collects user behaviors on a diverse set of visualization tasks with two comparable systems, desktop and immersive visualizations. We summarize the classification results with three time-series machine learning algorithms at two scales, and explore the influences of behavior features. Our results reveal that user behaviors can be distinguished during the process of visual analytics and there is a potentially strong association between the physical behaviors of users and the visualization tasks they perform. We also demonstrate the usage of our models by interpreting open sessions of visual analytics, which provides an automatic way to study sensemaking without tedious manual annotations.
</details>
<details>
<summary>摘要</summary>
理解视觉分析过程可以对视觉研究人员提供多方面的利益，包括改进视觉设计和开发高级交互功能。然而，用户行为的日志仍然困难分析，因为感知的复杂性和我们对相关用户行为的不了知。本研究报告一项全面收集用户行为数据的研究，并使用时间序列分类方法进行分析。我们选择了一个经典的视觉应用程序，即COVID-19数据分析，其中包括地图、时间序列和多属性的常见分析任务。我们的用户研究收集了用户在多种视觉任务上的行为记录，并将其分为两个相似的系统：桌面和 immerse 视觉。我们结合三种时间序列机器学习算法在两个不同的缩放尺度进行分类结果的总结，并探索行为特征的影响。我们的结果表明，用户行为在视觉分析过程中可以被区分开来，而且用户的物理行为和他们执行的视觉任务之间可能存在强相关性。此外，我们还示例了使用我们的模型来自动地研究感知过程，不需要繁琐的手动标注。
</details></li>
</ul>
<hr>
<h2 id="Collaboration-in-Immersive-Environments-Challenges-and-Solutions"><a href="#Collaboration-in-Immersive-Environments-Challenges-and-Solutions" class="headerlink" title="Collaboration in Immersive Environments: Challenges and Solutions"></a>Collaboration in Immersive Environments: Challenges and Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00689">http://arxiv.org/abs/2311.00689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Shahin Doroudian, Zachary Wartell</li>
<li>for: 这篇论文主要是为了探讨虚拟现实（VR）和增强现实（AR）工具在所有工程领域中的应用，以避免使用物理原型，训练在高风险的情况下，并解释真实或模拟结果。</li>
<li>methods: 这篇论文使用了现有的研究方法，包括文献综述、观察分析和实践报告，以探讨现有的协作在虚拟和增强现实环境中的应用。</li>
<li>results: 这篇论文结果显示，协作在虚拟和增强现实环境中是一个复杂的过程，涉及到不同的因素，如交流、协调和社交存在。论文还指出了在这些环境中协作的挑战和限制，例如缺乏物理提示、成本和可用性，以及需要进一步的研究。<details>
<summary>Abstract</summary>
Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in all engineering fields in order to avoid the use of physical prototypes, to train in high-risk situations, and to interpret real or simulated results. In order to complete a shared task or assign tasks to the agents in such immersive environments, collaboration or Shared Cooperative Activities are a necessity. Collaboration in immersive environments is an emerging field of research that aims to study and enhance the ways in which people interact and work together in Virtual and Augmented Reality settings. Collaboration in immersive environments is a complex process that involves different factors such as communication, coordination, and social presence. This paper provides an overview of the current state of research on collaboration in immersive environments. It discusses the different types of immersive environments, including VR and AR, and the different forms of collaboration that can occur in these environments. The paper also highlights the challenges and limitations of collaboration in immersive environments, such as the lack of physical cues, cost and usability and the need for further research in this area. Overall, collaboration in immersive environments is a promising field with a wide range of potential applications, from education to industry, and it can benefit both individuals and groups by enhancing their ability to work together effectively.
</details>
<details>
<summary>摘要</summary>
虚拟现实（VR）和增强现实（AR）工具在所有工程领域中应用，以避免使用物理原型，进行高风险的训练，并解释实际或模拟结果。为完成共同任务或分配任务给代理人，在如此的沉浸环境中进行协作或共同活动是必要。协作在沉浸环境中是一个emerging的研究领域，旨在研究和提高人们在虚拟和增强现实Setting中之间的交互方式和协作方式。协作在沉浸环境中是一个复杂的过程，涉及到不同的因素，如交流、协调和社会存在感。本文提供了关于协作在沉浸环境中的当前研究状况的概述，包括VR和AR等不同类型的沉浸环境，以及在这些环境中的不同合作形式。文章还 highlights了协作在沉浸环境中的挑战和局限性，如物理提示的缺乏、成本和可用性问题，以及需要进一步的研究。总之，协作在沉浸环境中是一个有前途的领域，它可以推动人们之间的协作效果，从教育到工业，并对个人和团队产生积极的影响。
</details></li>
</ul>
<hr>
<h2 id="ProcSim-Proxy-based-Confidence-for-Robust-Similarity-Learning"><a href="#ProcSim-Proxy-based-Confidence-for-Robust-Similarity-Learning" class="headerlink" title="ProcSim: Proxy-based Confidence for Robust Similarity Learning"></a>ProcSim: Proxy-based Confidence for Robust Similarity Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00668">http://arxiv.org/abs/2311.00668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oriol Barbany, Xiaofan Lin, Muhammet Bastan, Arnab Dhua</li>
<li>for: 学习一个嵌入空间，使得输入的距离与其内在 semantics 之间尽量相关。</li>
<li>methods: 使用 ProcSim 框架，对每个样本计算它的 confidence 分数，根据它的normalized 距离与类表示进行评估。</li>
<li>results: 在具有 uniform 和提议的semantic coherent 噪声的 DML benchmark 数据集上实现了状态�ayer的性能。<details>
<summary>Abstract</summary>
Deep Metric Learning (DML) methods aim at learning an embedding space in which distances are closely related to the inherent semantic similarity of the inputs. Previous studies have shown that popular benchmark datasets often contain numerous wrong labels, and DML methods are susceptible to them. Intending to study the effect of realistic noise, we create an ontology of the classes in a dataset and use it to simulate semantically coherent labeling mistakes. To train robust DML models, we propose ProcSim, a simple framework that assigns a confidence score to each sample using the normalized distance to its class representative. The experimental results show that the proposed method achieves state-of-the-art performance on the DML benchmark datasets injected with uniform and the proposed semantically coherent noise.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TPSeNCE-Towards-Artifact-Free-Realistic-Rain-Generation-for-Deraining-and-Object-Detection-in-Rain"><a href="#TPSeNCE-Towards-Artifact-Free-Realistic-Rain-Generation-for-Deraining-and-Object-Detection-in-Rain" class="headerlink" title="TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain"></a>TPSeNCE: Towards Artifact-Free Realistic Rain Generation for Deraining and Object Detection in Rain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00660">http://arxiv.org/abs/2311.00660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenzheng2000/tpsence">https://github.com/shenzheng2000/tpsence</a></li>
<li>paper_authors: Shen Zheng, Changjie Lu, Srinivasa G. Narasimhan</li>
<li>for: 生成潮湿图像，提高降雨方法和场景理解的泛化能力。</li>
<li>methods: 提出了一种基于无对应图像翻译框架的图像生成方法，通过引入TPS约束来减少生成图像中的噪声和扭曲，并通过SeNCE策略重新评估负样本的推动力来提高生成图像的准确性。</li>
<li>results: 实验表明该方法可以生成高质量的潮湿图像，具有最小的噪声和扭曲，可以帮助图像降雨和物体检测在雨中。此外，该方法还可以生成高质量的雪天和夜景图像，表明其可以扩展到更广泛的应用场景。<details>
<summary>Abstract</summary>
Rain generation algorithms have the potential to improve the generalization of deraining methods and scene understanding in rainy conditions. However, in practice, they produce artifacts and distortions and struggle to control the amount of rain generated due to a lack of proper constraints. In this paper, we propose an unpaired image-to-image translation framework for generating realistic rainy images. We first introduce a Triangular Probability Similarity (TPS) constraint to guide the generated images toward clear and rainy images in the discriminator manifold, thereby minimizing artifacts and distortions during rain generation. Unlike conventional contrastive learning approaches, which indiscriminately push negative samples away from the anchors, we propose a Semantic Noise Contrastive Estimation (SeNCE) strategy and reassess the pushing force of negative samples based on the semantic similarity between the clear and the rainy images and the feature similarity between the anchor and the negative samples. Experiments demonstrate realistic rain generation with minimal artifacts and distortions, which benefits image deraining and object detection in rain. Furthermore, the method can be used to generate realistic snowy and night images, underscoring its potential for broader applicability. Code is available at https://github.com/ShenZheng2000/TPSeNCE.
</details>
<details>
<summary>摘要</summary>
雨水生成算法有可能提高雨水方法的总体化和场景理解，但在实践中它们产生artefacts和扭曲，并且控制雨水生成的量因为缺乏适当的约束而困难。在这篇论文中，我们提议一种不带对应图像的图像到图像翻译框架，用于生成真实的雨水图像。我们首先引入一种三角形概率相似（TPS）约束，以引导生成的图像向clear和雨水图像在推理器映射中趋近，从而减少artefacts和扭曲。与传统的对比学习方法不同，我们提议一种语义噪声对照估计（SeNCE）策略，并重新评估负样本的推动力度基于雨水和clear图像的语义相似性和特征相似性。实验表明可以生成真实的雨水图像，并且减少artefacts和扭曲，这对雨水去雨水和物体检测有益。此外，该方法还可以用于生成真实的雪天和夜间图像，从而推断其普遍性。代码可以在https://github.com/ShenZheng2000/TPSeNCE上获取。
</details></li>
</ul>
<hr>
<h2 id="De-Diffusion-Makes-Text-a-Strong-Cross-Modal-Interface"><a href="#De-Diffusion-Makes-Text-a-Strong-Cross-Modal-Interface" class="headerlink" title="De-Diffusion Makes Text a Strong Cross-Modal Interface"></a>De-Diffusion Makes Text a Strong Cross-Modal Interface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00618">http://arxiv.org/abs/2311.00618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Wei, Chenxi Liu, Siyuan Qiao, Zhishuai Zhang, Alan Yuille, Jiahui Yu</li>
<li>for: 这篇论文主要是为了提出一种新的文本 Representation 方法，可以将图像转换为文本，并且可以让文本与图像之间进行协同作业。</li>
<li>methods: 这篇论文使用了一种自适应的文本扩充模型，将输入图像转换为文本，然后使用固定的文本扩充模型来重建原始输入图像。这个过程被称为 De-Diffusion。</li>
<li>results: 实验表明，De-Diffusion 方法可以准确地将图像转换为文本，并且可以在多种多样的模态任务中进行通用应用。例如，一个 De-Diffusion 模型可以生成可转换的提示，用于不同的文本-图像工具，同时也达到了开放式视觉语言任务中的新州OF-the-art。<details>
<summary>Abstract</summary>
We demonstrate text as a strong cross-modal interface. Rather than relying on deep embeddings to connect image and language as the interface representation, our approach represents an image as text, from which we enjoy the interpretability and flexibility inherent to natural language. We employ an autoencoder that uses a pre-trained text-to-image diffusion model for decoding. The encoder is trained to transform an input image into text, which is then fed into the fixed text-to-image diffusion decoder to reconstruct the original input -- a process we term De-Diffusion. Experiments validate both the precision and comprehensiveness of De-Diffusion text representing images, such that it can be readily ingested by off-the-shelf text-to-image tools and LLMs for diverse multi-modal tasks. For example, a single De-Diffusion model can generalize to provide transferable prompts for different text-to-image tools, and also achieves a new state of the art on open-ended vision-language tasks by simply prompting large language models with few-shot examples.
</details>
<details>
<summary>摘要</summary>
我们展示了文本作为强型跨模态界面。而不是依靠深入嵌入来连接图像和语言作为界面表示，我们的方法将图像转换为文本，从而获得了自然语言中的可读性和灵活性。我们使用一个自适应oder，使用预训练的文本到图像扩散模型进行解码。解oder是将输入图像转换为文本，然后将其feed到固定的文本到图像扩散解码器中重construct原始输入——一个我们称为“De-Diffusion”的过程。实验证明了De-Diffusion文本表示图像的精度和全面性，因此可以轻松地被off-the-shelf文本到图像工具和大语言模型接受，用于多样化的跨模态任务。例如，一个单一的De-Diffusion模型可以泛化到提供不同文本到图像工具的可重用提示，并同时实现了开放式视力语言任务的新状态之一，只需提供少量示例来训练大语言模型。
</details></li>
</ul>
<hr>
<h2 id="Occluded-Person-Re-Identification-with-Deep-Learning-A-Survey-and-Perspectives"><a href="#Occluded-Person-Re-Identification-with-Deep-Learning-A-Survey-and-Perspectives" class="headerlink" title="Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives"></a>Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00603">http://arxiv.org/abs/2311.00603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enhao Ning, Changshuo Wang, Huang Zhangc, Xin Ning, Prayag Tiwari</li>
<li>for: 本文主要针对智能监测系统中人员重识别技术（Re-ID）的问题，具体来说是在受到干扰的情况下进行人员重识别。</li>
<li>methods: 本文主要分析了现有的深度学习基于的 occluded person Re-ID 方法，从多种角度进行科学分类和分析，并对这些方法进行系统性比较。</li>
<li>results: 本文对 occluded person Re-ID 方法进行了系统性比较，并提出了未来发展的看法。<details>
<summary>Abstract</summary>
Person re-identification (Re-ID) technology plays an increasingly crucial role in intelligent surveillance systems. Widespread occlusion significantly impacts the performance of person Re-ID. Occluded person Re-ID refers to a pedestrian matching method that deals with challenges such as pedestrian information loss, noise interference, and perspective misalignment. It has garnered extensive attention from researchers. Over the past few years, several occlusion-solving person Re-ID methods have been proposed, tackling various sub-problems arising from occlusion. However, there is a lack of comprehensive studies that compare, summarize, and evaluate the potential of occluded person Re-ID methods in detail. In this review, we start by providing a detailed overview of the datasets and evaluation scheme used for occluded person Re-ID. Next, we scientifically classify and analyze existing deep learning-based occluded person Re-ID methods from various perspectives, summarizing them concisely. Furthermore, we conduct a systematic comparison among these methods, identify the state-of-the-art approaches, and present an outlook on the future development of occluded person Re-ID.
</details>
<details>
<summary>摘要</summary>
人脸重复识别（Re-ID）技术在智能监测系统中扮演着越来越重要的角色。广泛的遮挡对人Re-ID的性能产生了很大的影响。遮挡人Re-ID指的是一种在遮挡情况下进行人脸匹配的技术，它面临着人员信息损失、干扰和视角偏移等挑战。这一领域在过去几年内吸引了广泛的研究者的关注。在这篇评论中，我们首先提供了遮挡人Re-ID的数据集和评价方法的详细介绍。然后，我们科学地将现有的深度学习基于的遮挡人Re-ID方法分类和分析，并将其 concisely 总结。此外，我们进行了系统性的比较和评价这些方法，找到了当前领域的状态机器和未来发展的前景。
</details></li>
</ul>
<hr>
<h2 id="PAUMER-Patch-Pausing-Transformer-for-Semantic-Segmentation"><a href="#PAUMER-Patch-Pausing-Transformer-for-Semantic-Segmentation" class="headerlink" title="PAUMER: Patch Pausing Transformer for Semantic Segmentation"></a>PAUMER: Patch Pausing Transformer for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00586">http://arxiv.org/abs/2311.00586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evann Courdier, Prabhu Teja Sivaprasad, François Fleuret</li>
<li>for: 提高图像 segmentation transformer 的效率，通过不同的计算量来适应不同的图像部分。</li>
<li>methods: 使用预测值的Entropy作为挽 paused 计算 criterion，并在最终decoder中停止计算。</li>
<li>results: 在 Cityscapes 和 ADE20K 两个标准的 segmentation 数据集上，我们的方法可以达到约 $50%$ 更高的 Throughput，与 mIoU 下降约 $0.65%$ 和 $4.6%$ 分别。<details>
<summary>Abstract</summary>
We study the problem of improving the efficiency of segmentation transformers by using disparate amounts of computation for different parts of the image. Our method, PAUMER, accomplishes this by pausing computation for patches that are deemed to not need any more computation before the final decoder. We use the entropy of predictions computed from intermediate activations as the pausing criterion, and find this aligns well with semantics of the image. Our method has a unique advantage that a single network trained with the proposed strategy can be effortlessly adapted at inference to various run-time requirements by modulating its pausing parameters. On two standard segmentation datasets, Cityscapes and ADE20K, we show that our method operates with about a $50\%$ higher throughput with an mIoU drop of about $0.65\%$ and $4.6\%$ respectively.
</details>
<details>
<summary>摘要</summary>
我团队研究如何通过不同的计算量来提高分割变换器的效率。我们的方法PAUMER通过在最终解码器之前暂停计算来实现这一目标。我们使用介词预测的Entropy作为暂停标准，并发现它与图像 semantics吻合得非常好。我们的方法具有单一网络在执行时可以根据运行时需求modulate暂停参数的优点，在Cityscapes和ADE20K两个标准分割dataset上，我们表明我们的方法可以达到约50%更高的throughput，与mIoU下降约0.65%和4.6%相对应。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese scripts used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Deep-Learning-Method-with-Uncertainty-Estimation-for-the-Pathological-Classification-of-Renal-Cell-Carcinoma-based-on-CT-Images"><a href="#A-Robust-Deep-Learning-Method-with-Uncertainty-Estimation-for-the-Pathological-Classification-of-Renal-Cell-Carcinoma-based-on-CT-Images" class="headerlink" title="A Robust Deep Learning Method with Uncertainty Estimation for the Pathological Classification of Renal Cell Carcinoma based on CT Images"></a>A Robust Deep Learning Method with Uncertainty Estimation for the Pathological Classification of Renal Cell Carcinoma based on CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00567">http://arxiv.org/abs/2311.00567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Yao, Hang Hu, Kaicong Chen, Chen Zhao, Yuan Guo, Boya Li, Jiaofen Nan, Yanting Li, Chuang Han, Fubao Zhu, Weihua Zhou, Li Tian<br>for: The paper aims to develop and validate a deep learning-based diagnostic model for the preoperative differentiation of renal cell carcinoma (RCC) subtypes based on CT images.methods: The model incorporates uncertainty estimation and is developed using five-fold cross-validation with an external validation set.results: The model demonstrates robust performance in predicting RCC subtypes, with an area under the receiver operating characteristic curve (AUC) of 0.868, 0.846, and 0.839 for clear cell RCC, papillary RCC, and chromophobe RCC, respectively. The incorporated uncertainty emphasizes the importance of understanding model confidence, which is crucial for assisting clinical decision-making for patients with renal tumors.Here’s the format you requested:for: 助けることは、肾芽癌の疾患分类のためのディープラーニングベースの诊断モデルの开発および适合性の検证です。methods: モデルは不确実性评価を含む五つのクロスバレーションを使用して开発され、外部适合セットを使用して评価されました。results: モデルは肾芽癌の疾患分类に対して强固な性能を示しました。 five-fold クロスバレーションでのAUC（受动的特徴特徴曲线）は、clear cell RCC（ccRCC）、papillary RCC（pRCC）、およびchromophobe RCC（chRCC）の疾患分类に対して0.868（95% CI：0.826-0.923）、0.846（95% CI：0.812-0.886）、および0.839（95% CI：0.802-0.88）を示しました。外部适合セットでは、AUCは0.856（95% CI：0.838-0.882）、0.787（95% CI：0.757-0.818）、および0.793（95% CI：0.758-0.831）を示しました。<details>
<summary>Abstract</summary>
Objectives To develop and validate a deep learning-based diagnostic model incorporating uncertainty estimation so as to facilitate radiologists in the preoperative differentiation of the pathological subtypes of renal cell carcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients, pathologically proven RCC, were retrospectively collected from Center 1. By using five-fold cross-validation, a deep learning model incorporating uncertainty estimation was developed to classify RCC subtypes into clear cell RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external validation set of 78 patients from Center 2 further evaluated the model's performance. Results In the five-fold cross-validation, the model's area under the receiver operating characteristic curve (AUC) for the classification of ccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI: 0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external validation set, the AUCs were 0.856 (95% CI: 0.838-0.882), 0.787 (95% CI: 0.757-0.818), and 0.793 (95% CI: 0.758-0.831) for ccRCC, pRCC, and chRCC, respectively. Conclusions The developed deep learning model demonstrated robust performance in predicting the pathological subtypes of RCC, while the incorporated uncertainty emphasized the importance of understanding model confidence, which is crucial for assisting clinical decision-making for patients with renal tumors. Clinical relevance statement Our deep learning approach, integrated with uncertainty estimation, offers clinicians a dual advantage: accurate RCC subtype predictions complemented by diagnostic confidence references, promoting informed decision-making for patients with RCC.
</details>
<details>
<summary>摘要</summary>
Methods:* Retrospectively collected data from 668 consecutive patients with pathologically proven RCC from Center 1.* Developed a deep learning model with five-fold cross-validation to classify RCC subtypes into clear cell RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC).* Evaluated the model's performance using an external validation set of 78 patients from Center 2.Results:* In the five-fold cross-validation, the model's area under the receiver operating characteristic curve (AUC) for the classification of ccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI: 0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively.* In the external validation set, the AUCs were 0.856 (95% CI: 0.838-0.882), 0.787 (95% CI: 0.757-0.818), and 0.793 (95% CI: 0.758-0.831) for ccRCC, pRCC, and chRCC, respectively.Conclusions:* The developed deep learning model demonstrated robust performance in predicting the pathological subtypes of RCC.* The incorporated uncertainty emphasized the importance of understanding model confidence, which is crucial for assisting clinical decision-making for patients with renal tumors.Clinical relevance statement:* Our deep learning approach, integrated with uncertainty estimation, offers clinicians a dual advantage: accurate RCC subtype predictions complemented by diagnostic confidence references, promoting informed decision-making for patients with RCC.
</details></li>
</ul>
<hr>
<h2 id="CROMA-Remote-Sensing-Representations-with-Contrastive-Radar-Optical-Masked-Autoencoders"><a href="#CROMA-Remote-Sensing-Representations-with-Contrastive-Radar-Optical-Masked-Autoencoders" class="headerlink" title="CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders"></a>CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00566">http://arxiv.org/abs/2311.00566</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antofuller/croma">https://github.com/antofuller/croma</a></li>
<li>paper_authors: Anthony Fuller, Koreen Millard, James R. Green</li>
<li>for: 这种论文主要针对的应用是遥感技术，具体来说是用于学习rich的多modal表示。</li>
<li>methods: 该方法 combinestwo个自我超vised学习目标：对比学习和重构学习，通过将多spectral和Synthetic Aperture Radar（SAR）样本遮盖后，使用cross-modal协同学习，并使用一个轻量级的解码器来预测遮盖的质量。</li>
<li>results: 这种方法可以在具有相同空间和时间协调的多modal数据上取得优秀的表示，并且可以在不同的探测任务上进行扩展。在四个分类标准benchmark上，CROMA比现有的多spectral模型更高效，并且在三个分割标准benchmark上也取得了优秀的成绩。<details>
<summary>Abstract</summary>
A vital and rapidly growing application, remote sensing offers vast yet sparsely labeled, spatially aligned multimodal data; this makes self-supervised learning algorithms invaluable. We present CROMA: a framework that combines contrastive and reconstruction self-supervised objectives to learn rich unimodal and multimodal representations. Our method separately encodes masked-out multispectral optical and synthetic aperture radar samples -- aligned in space and time -- and performs cross-modal contrastive learning. Another encoder fuses these sensors, producing joint multimodal encodings that are used to predict the masked patches via a lightweight decoder. We show that these objectives are complementary when leveraged on spatially aligned multimodal data. We also introduce X- and 2D-ALiBi, which spatially biases our cross- and self-attention matrices. These strategies improve representations and allow our models to effectively extrapolate to images up to 17.6x larger at test-time. CROMA outperforms the current SoTA multispectral model, evaluated on: four classification benchmarks -- finetuning (avg. 1.8%), linear (avg. 2.4%) and nonlinear (avg. 1.4%) probing, kNN classification (avg. 3.5%), and K-means clustering (avg. 8.4%); and three segmentation benchmarks (avg. 6.4%). CROMA's rich, optionally multimodal representations can be widely leveraged across remote sensing applications.
</details>
<details>
<summary>摘要</summary>
“远程感知应用在速速发展中，提供了庞大但缺乏标注的多模态数据，这使得无监督学习算法成为了非常重要的。我们提出了 CROMA 框架，它将对比学习和重建自我监督目标进行混合，以学习丰富的单模态和多模态表示。我们在空间和时间启用了多普通频谱光学和 Synthetic Aperture Radar 样本，并在多模态数据上进行交叉模式对比学习。另一个编码器将这些感知器联合起来，生成共同的多模态编码，并用轻量级解码器预测屏蔽的覆盖物。我们发现这些目标是相互补做的，当在空间启用的多模态数据上运行时，它们具有更高的效果。我们还引入了 X-和2D-ALiBi，它们在交叉和自我注意力矩阵上进行空间偏置，从而改善表示并使我们的模型能够有效地推测大小至多17.6倍的图像。CROMA 在四个分类标准 benchmarck 上表现出色，比如：四个分类 benchmarck 的 finetuning 平均值为1.8%，Linear 和非线性 probing 平均值为2.4%和1.4%，kNN 分类平均值为3.5%，K-means 分群平均值为8.4%。此外，CROMA 还在三个分 segmentation 标准 benchmarck 上表现出色，平均值为6.4%。CROMA 的丰富、可选多模态表示可以广泛应用于远程感知领域。”
</details></li>
</ul>
<hr>
<h2 id="MNN-Mixed-Nearest-Neighbors-for-Self-Supervised-Learning"><a href="#MNN-Mixed-Nearest-Neighbors-for-Self-Supervised-Learning" class="headerlink" title="MNN: Mixed Nearest-Neighbors for Self-Supervised Learning"></a>MNN: Mixed Nearest-Neighbors for Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00562">http://arxiv.org/abs/2311.00562</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pc-cp/mnn">https://github.com/pc-cp/mnn</a></li>
<li>paper_authors: Chen Peng, Xianzhong Long, Yun Li</li>
<li>for: 这个论文是为了解决自动матиче自监学习中的对比样本受限问题，通过包含样本之间的关系来提高模型的泛化能力。</li>
<li>methods: 这个论文提出了一种简单的自动матиче自监学习框架called Mixed Nearest-Neighbors for Self-Supervised Learning (MNN)，通过INTUITIVE weighting Approach和图像混合操作来调整邻近样本对正样本的影响。</li>
<li>results: 研究发现，MNN在四个标准数据集上展现出了优秀的泛化性和训练效率。<details>
<summary>Abstract</summary>
In contrastive self-supervised learning, positive samples are typically drawn from the same image but in different augmented views, resulting in a relatively limited source of positive samples. An effective way to alleviate this problem is to incorporate the relationship between samples, which involves including the top-k nearest neighbors of positive samples in the framework. However, the problem of false neighbors (i.e., neighbors that do not belong to the same category as the positive sample) is an objective but often overlooked challenge due to the query of neighbor samples without human supervision. In this paper, we present a simple Self-supervised learning framework called Mixed Nearest-Neighbors for Self-Supervised Learning (MNN). MNN optimizes the influence of neighbor samples on the semantics of positive samples through an intuitive weighting approach and image mixture operations. The results of our study demonstrate that MNN exhibits exceptional generalization performance and training efficiency on four benchmark datasets.
</details>
<details>
<summary>摘要</summary>
对照自动学习中，正面样本通常是从同一幅图像中不同的扩展视图中提取的，这导致正面样本的数量相对受限。为解决这个问题，可以利用样本之间的关系，例如包括正面样本的top-k最近邻居在内。但是，遇到假邻居（即不属于正面样本类别的邻居）是一个 объекive  yet  oft-overlooked  challenge，因为 Querying neighbor samples without human supervision。在这篇论文中，我们提出了一种简单的自动学习框架called Mixed Nearest-Neighbors for Self-Supervised Learning (MNN)。MNN通过对正面样本的 semantics 进行权重调整和图像混合操作来优化邻居样本对正面样本的影响。我们的研究结果表明，MNN在四个 benchmark 数据集上表现出色的泛化性和训练效率。
</details></li>
</ul>
<hr>
<h2 id="ProBio-A-Protocol-guided-Multimodal-Dataset-for-Molecular-Biology-Lab"><a href="#ProBio-A-Protocol-guided-Multimodal-Dataset-for-Molecular-Biology-Lab" class="headerlink" title="ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab"></a>ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00556">http://arxiv.org/abs/2311.00556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieming Cui, Ziren Gong, Baoxiong Jia, Siyuan Huang, Zilong Zheng, Jianzhu Ma, Yixin Zhu</li>
<li>for: 这 paper 的目的是解决分子生物领域中复现研究结果的问题，通过利用现代智能系统来进行研究。</li>
<li>methods: 这 paper 使用了一系列的现代智能系统技术，包括 Multimodal 数据集 ProBio，以及 two 个困难的标准曲线跟踪和多modal 动作识别 benchmark。</li>
<li>results: 这 paper 通过对当代视频理解模型的实验评估，发现了这些模型在 BioLab 设置下的限制和挑战，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
The challenge of replicating research results has posed a significant impediment to the field of molecular biology. The advent of modern intelligent systems has led to notable progress in various domains. Consequently, we embarked on an investigation of intelligent monitoring systems as a means of tackling the issue of the reproducibility crisis. Specifically, we first curate a comprehensive multimodal dataset, named ProBio, as an initial step towards this objective. This dataset comprises fine-grained hierarchical annotations intended for the purpose of studying activity understanding in BioLab. Next, we devise two challenging benchmarks, transparent solution tracking and multimodal action recognition, to emphasize the unique characteristics and difficulties associated with activity understanding in BioLab settings. Finally, we provide a thorough experimental evaluation of contemporary video understanding models and highlight their limitations in this specialized domain to identify potential avenues for future research. We hope ProBio with associated benchmarks may garner increased focus on modern AI techniques in the realm of molecular biology.
</details>
<details>
<summary>摘要</summary>
研究复制结果的挑战对分子生物学领域带来了重要的阻碍。现代智能系统的出现导致了各个领域的 notable progress。因此，我们开始了一项研究，用智能监测系统来解决复制危机。我们首先筹建了一个完整的多modal数据集，名为ProBio，作为这一目标的第一步。这个数据集包括细化的 hierarchical 注释，用于 BioLab 中的活动理解研究。然后，我们设计了两个具有挑战性的标准，透明解决方案跟踪和多modal 动作识别，以强调 BioLab 中活动理解的特殊特点和挑战。最后，我们进行了 Contemporary video 理解模型的系统性评估，并指出了这些模型在这个特殊领域的局限性，以便未来研究的可能性。我们希望 ProBio 和相关的标准可以吸引更多的现代 AI 技术在分子生物学领域中受到关注。
</details></li>
</ul>
<hr>
<h2 id="Continual-atlas-based-segmentation-of-prostate-MRI"><a href="#Continual-atlas-based-segmentation-of-prostate-MRI" class="headerlink" title="Continual atlas-based segmentation of prostate MRI"></a>Continual atlas-based segmentation of prostate MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00548">http://arxiv.org/abs/2311.00548</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meclabtuda/atlas-replay">https://github.com/meclabtuda/atlas-replay</a></li>
<li>paper_authors: Amin Ranem, Camila Gonázlez, Daniel Pinto dos Santos, Andreas Michael Bucher, Ahmed Ezzat Othman, Anirban Mukhopadhyay</li>
<li>for: 这个研究旨在提高 kontinual learning (CL) 方法的适用范围，使其能够应用于医疗影像分类中，特别是针对肾脏分类。</li>
<li>methods: 这个研究使用的方法是 Atlas-based segmentation，这是一种基于预设的医疗影像分类方法，它利用预设的医疗影像知识，实现了semantically coherent的预测。</li>
<li>results: 研究发现，Atlas Replay 可以在不同的训练分布中保持知识，并且具有优秀的适应能力和稳定性。它比于竞合方法更具有robustness和可靠性，并且能够在未见过的领域中获得高质量的预测结果。<details>
<summary>Abstract</summary>
Continual learning (CL) methods designed for natural image classification often fail to reach basic quality standards for medical image segmentation. Atlas-based segmentation, a well-established approach in medical imaging, incorporates domain knowledge on the region of interest, leading to semantically coherent predictions. This is especially promising for CL, as it allows us to leverage structural information and strike an optimal balance between model rigidity and plasticity over time. When combined with privacy-preserving prototypes, this process offers the advantages of rehearsal-based CL without compromising patient privacy. We propose Atlas Replay, an atlas-based segmentation approach that uses prototypes to generate high-quality segmentation masks through image registration that maintain consistency even as the training distribution changes. We explore how our proposed method performs compared to state-of-the-art CL methods in terms of knowledge transferability across seven publicly available prostate segmentation datasets. Prostate segmentation plays a vital role in diagnosing prostate cancer, however, it poses challenges due to substantial anatomical variations, benign structural differences in older age groups, and fluctuating acquisition parameters. Our results show that Atlas Replay is both robust and generalizes well to yet-unseen domains while being able to maintain knowledge, unlike end-to-end segmentation methods. Our code base is available under https://github.com/MECLabTUDA/Atlas-Replay.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Cardiovascular-Disease-Prediction-Through-Comparative-Analysis-of-Machine-Learning-Models-A-Case-Study-on-Myocardial-Infarction"><a href="#Improving-Cardiovascular-Disease-Prediction-Through-Comparative-Analysis-of-Machine-Learning-Models-A-Case-Study-on-Myocardial-Infarction" class="headerlink" title="Improving Cardiovascular Disease Prediction Through Comparative Analysis of Machine Learning Models: A Case Study on Myocardial Infarction"></a>Improving Cardiovascular Disease Prediction Through Comparative Analysis of Machine Learning Models: A Case Study on Myocardial Infarction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00517">http://arxiv.org/abs/2311.00517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonayet Miah, Duc M Ca, Md Abu Sayed, Ehsanur Rashid Lipu, Fuad Mahmud, S M Yasir Arafat</li>
<li>for: 预测心肺疾病的准确性</li>
<li>methods: 比较六种机器学习模型的表现：梯度下降、支持向量机、决策树、袋包、XGBoost和LightGBM</li>
<li>results: 结果显示XGBoost模型的表现最佳（准确率为92.72%），其他模型的准确率分别为Logistic Regression（81.00%）、Support Vector Machine（75.01%）、Decision Tree（82.30%）、Bagging（83.01%）。<details>
<summary>Abstract</summary>
Cardiovascular disease remains a leading cause of mortality in the contemporary world. Its association with smoking, elevated blood pressure, and cholesterol levels underscores the significance of these risk factors. This study addresses the challenge of predicting myocardial illness, a formidable task in medical research. Accurate predictions are pivotal for refining healthcare strategies. This investigation conducts a comparative analysis of six distinct machine learning models: Logistic Regression, Support Vector Machine, Decision Tree, Bagging, XGBoost, and LightGBM. The attained outcomes exhibit promise, with accuracy rates as follows: Logistic Regression (81.00%), Support Vector Machine (75.01%), XGBoost (92.72%), LightGBM (90.60%), Decision Tree (82.30%), and Bagging (83.01%). Notably, XGBoost emerges as the top-performing model. These findings underscore its potential to enhance predictive precision for coronary infarction. As the prevalence of cardiovascular risk factors persists, incorporating advanced machine learning techniques holds the potential to refine proactive medical interventions.
</details>
<details>
<summary>摘要</summary>
心血管疾病仍然是当代世界中最主要的死亡原因。它与吸烟、高血压和凝血水平的关系，强调了这些风险因素的重要性。这项研究面临着预测心肌疾病的挑战，这是医学研究中的一项复杂任务。准确的预测是健康策略的重要组成部分。这个研究使用六种不同的机器学习模型进行比较分析：Logistic Regression、Support Vector Machine、决策树、Bagging、XGBoost和LightGBM。实际结果展示了承诺，准确率如下：Logistic Regression（81.00%）、Support Vector Machine（75.01%）、XGBoost（92.72%）、LightGBM（90.60%）、决策树（82.30%）和Bagging（83.01%）。特别是，XGBoost emerges as the top-performing model。这些结果强调了它的潜在性能，用于提高心肌梗死预测的精度。随着心血管风险因素的流行，投入进更高级别的机器学习技术可能会提高护理措施的灵活性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Networks-for-Automatic-Speaker-Recognition-Do-Not-Learn-Supra-Segmental-Temporal-Features"><a href="#Deep-Neural-Networks-for-Automatic-Speaker-Recognition-Do-Not-Learn-Supra-Segmental-Temporal-Features" class="headerlink" title="Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features"></a>Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00489">http://arxiv.org/abs/2311.00489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Neururer, Volker Dellow, Thilo Stadelmann</li>
<li>for: 这篇论文旨在探讨深度神经网络在自动人员识别任务中的成功原因，以及这些成功的关键因素是什么。</li>
<li>methods: 本文使用了一种新的测试方法来衡量现代神经网络是否能够很好地模型语音的 supra-segmental 时间信息（SST），并提出了一些使神经网络更加关注 SST 的方法来评估其效果。</li>
<li>results: 结果表明，现有的 CNN 和 RNN 神经网络架构在模型 SST 方面并没有充分的表现，即使使用强制方法也不如。这些结果提供了深入的研究基础，有助于更好地利用语音信号和提高深度学习对语音技术的解释性。<details>
<summary>Abstract</summary>
While deep neural networks have shown impressive results in automatic speaker recognition and related tasks, it is dissatisfactory how little is understood about what exactly is responsible for these results. Part of the success has been attributed in prior work to their capability to model supra-segmental temporal information (SST), i.e., learn rhythmic-prosodic characteristics of speech in addition to spectral features. In this paper, we (i) present and apply a novel test to quantify to what extent the performance of state-of-the-art neural networks for speaker recognition can be explained by modeling SST; and (ii) present several means to force respective nets to focus more on SST and evaluate their merits. We find that a variety of CNN- and RNN-based neural network architectures for speaker recognition do not model SST to any sufficient degree, even when forced. The results provide a highly relevant basis for impactful future research into better exploitation of the full speech signal and give insights into the inner workings of such networks, enhancing explainability of deep learning for speech technologies.
</details>
<details>
<summary>摘要</summary>
In this paper, we:1. Present a new test to measure the extent to which state-of-the-art neural networks for speaker recognition rely on modeling SST.2. Propose several methods to force these networks to focus more on SST and evaluate their effectiveness.Our findings show that a variety of convolutional neural network (CNN) and recurrent neural network (RNN) architectures for speaker recognition do not rely heavily on SST, even when forced to do so. These results provide a valuable foundation for future research into fully exploiting the speech signal and offer insights into the inner workings of these networks, enhancing the explainability of deep learning for speech technologies.
</details></li>
</ul>
<hr>
<h2 id="DEFN-Dual-Encoder-Fourier-Group-Harmonics-Network-for-Three-Dimensional-Macular-Hole-Reconstruction-with-Stochastic-Retinal-Defect-Augmentation-and-Dynamic-Weight-Composition"><a href="#DEFN-Dual-Encoder-Fourier-Group-Harmonics-Network-for-Three-Dimensional-Macular-Hole-Reconstruction-with-Stochastic-Retinal-Defect-Augmentation-and-Dynamic-Weight-Composition" class="headerlink" title="DEFN: Dual-Encoder Fourier Group Harmonics Network for Three-Dimensional Macular Hole Reconstruction with Stochastic Retinal Defect Augmentation and Dynamic Weight Composition"></a>DEFN: Dual-Encoder Fourier Group Harmonics Network for Three-Dimensional Macular Hole Reconstruction with Stochastic Retinal Defect Augmentation and Dynamic Weight Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00483">http://arxiv.org/abs/2311.00483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iipl-hangzhoudianziuniversity/defn-pytorch">https://github.com/iipl-hangzhoudianziuniversity/defn-pytorch</a></li>
<li>paper_authors: Xingru Huang, Yihao Guo, Jian Huang, Zhi Li, Tianyun Zhang, Kunyan Cai, Gaopeng Huang, Wenhao Chen, Zhaoyang Xu, Liangqiong Qu, Ji Hu, Tinyu Wang, Shaowei Jiang, Chenggang Yan, Yaoqi Sun, Xin Ye, Yaqi Wang<br>for:* 这种研究旨在提供高精度的三维Retinal OCT图像分割和实时三维重建，以便诊断和治疗macular hole疾病。methods:* 该研究使用了一个新型的三维分割网络：双解码FuGH网络（DEFN），以及一种新的数据增强方法：随机retinal defect注射（SRDI）和一种网络优化策略：动态weightCompose（DWC）。results:* 相比13个基准，DEFN显示了最好的性能。此外，该研究还提供了高精度的三维 retinal 重建和量化指标，这将为眼科医生提供革命性的诊断和治疗决策工具，并预计将完全改变difficult-to-treat macular degeneration的诊断和治疗模式。<details>
<summary>Abstract</summary>
The spatial and quantitative parameters of macular holes are vital for diagnosis, surgical choices, and post-op monitoring. Macular hole diagnosis and treatment rely heavily on spatial and quantitative data, yet the scarcity of such data has impeded the progress of deep learning techniques for effective segmentation and real-time 3D reconstruction. To address this challenge, we assembled the world's largest macular hole dataset, Retinal OCTfor Macular Hole Enhancement (ROME-3914), and a Comprehensive Archive for Retinal Segmentation (CARS-30k), both expertly annotated. In addition, we developed an innovative 3D segmentation network, the Dual-Encoder FuGH Network (DEFN), which integrates three innovative modules: Fourier Group Harmonics (FuGH), Simplified 3D Spatial Attention (S3DSA) and Harmonic Squeeze-and-Excitation Module (HSE). These three modules synergistically filter noise, reduce computational complexity, emphasize detailed features, and enhance the network's representation ability. We also proposed a novel data augmentation method, Stochastic Retinal Defect Injection (SRDI), and a network optimization strategy DynamicWeightCompose (DWC), to further improve the performance of DEFN. Compared with 13 baselines, our DEFN shows the best performance. We also offer precise 3D retinal reconstruction and quantitative metrics, bringing revolutionary diagnostic and therapeutic decision-making tools for ophthalmologists, and is expected to completely reshape the diagnosis and treatment patterns of difficult-to-treat macular degeneration. The source code is publicly available at: https://github.com/IIPL-HangzhouDianUniversity/DEFN-Pytorch.
</details>
<details>
<summary>摘要</summary>
“macular hole的空间和量值参数是诊断、手术选择和后期监测的关键因素。但由于缺乏这些数据，使得深度学习技术的有效分割和实时3D重建受到了阻碍。为解决这个挑战，我们组织了全球最大的macular hole数据集，Retinal OCTfor Macular Hole Enhancement (ROME-3914)，以及Comprehensive Archive for Retinal Segmentation (CARS-30k)，均为专家标注。此外，我们开发了一种创新的3D分割网络， dual-encoder FuGH网络 (DEFN)，该网络包括三个创新模块：Fourier Group Harmonics (FuGH)、Simplified 3D Spatial Attention (S3DSA)和Harmonic Squeeze-and-Excitation Module (HSE)。这三个模块相互协同筛除噪声、减少计算复杂性、强调细节特征和提高网络表达能力。我们还提出了一种新的数据增强方法，Stochastic Retinal Defect Injection (SRDI)，以及一种网络优化策略，DynamicWeightCompose (DWC)，以进一步提高DEFN的性能。与13个基线相比，OUR DEFN表现最佳。此外，我们还提供了高精度的3D retinal重建和量值度量，为视科医生提供革命性的诊断和治疗决策工具，并预计将完全重新定义脊梁疾病的诊断和治疗方式。数据集和代码可以在 GitHub上获取：https://github.com/IIPL-HangzhouDianUniversity/DEFN-Pytorch。”
</details></li>
</ul>
<hr>
<h2 id="Group-Distributionally-Robust-Knowledge-Distillation"><a href="#Group-Distributionally-Robust-Knowledge-Distillation" class="headerlink" title="Group Distributionally Robust Knowledge Distillation"></a>Group Distributionally Robust Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00476">http://arxiv.org/abs/2311.00476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos Vilouras, Xiao Liu, Pedro Sanchez, Alison Q. O’Neil, Sotirios A. Tsaftaris</li>
<li>for: 提高模型在医学影像分析中的性能，特别是对少数群体的表现。</li>
<li>methods: 基于分布robust优化(DRO)技术，提出一种分组知识填充损失函数，使模型在优化过程中能够动态地关注表现不佳的群体。</li>
<li>results: 在两个标准数据集（自然图像和心脏MR）上进行了实验，并证明了在最差群体准确率上具有稳定的改进。<details>
<summary>Abstract</summary>
Knowledge distillation enables fast and effective transfer of features learned from a bigger model to a smaller one. However, distillation objectives are susceptible to sub-population shifts, a common scenario in medical imaging analysis which refers to groups/domains of data that are underrepresented in the training set. For instance, training models on health data acquired from multiple scanners or hospitals can yield subpar performance for minority groups. In this paper, inspired by distributionally robust optimization (DRO) techniques, we address this shortcoming by proposing a group-aware distillation loss. During optimization, a set of weights is updated based on the per-group losses at a given iteration. This way, our method can dynamically focus on groups that have low performance during training. We empirically validate our method, GroupDistil on two benchmark datasets (natural images and cardiac MRIs) and show consistent improvement in terms of worst-group accuracy.
</details>
<details>
<summary>摘要</summary>
知识储备可以快速和有效地传递学习于大型模型中学习的特征到小型模型中。然而，浸泡目标容易受到少数群体风险的情况，即训练数据集中的少数组团或组织。例如，通过多种扫描仪或医院收集的健康数据训练模型可能会导致少数组团的性能下降。本文，采用分布robust优化（DRO）技术为 inspirations，我们提出了一种组aware的浸泡损失函数。在优化过程中，一组权重会根据每个组的损失值进行更新。这样，我们的方法可以在训练过程中动态地关注性能较低的组。我们在两个 referenze dataset（自然图像和心血扫描）上进行了empirical验证，并证明了我们的方法可以在最差组织精度方面提供了一致性的改进。
</details></li>
</ul>
<hr>
<h2 id="Single-view-3D-Scene-Reconstruction-with-High-fidelity-Shape-and-Texture"><a href="#Single-view-3D-Scene-Reconstruction-with-High-fidelity-Shape-and-Texture" class="headerlink" title="Single-view 3D Scene Reconstruction with High-fidelity Shape and Texture"></a>Single-view 3D Scene Reconstruction with High-fidelity Shape and Texture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00457">http://arxiv.org/abs/2311.00457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DaLi-Jack/SSR-code">https://github.com/DaLi-Jack/SSR-code</a></li>
<li>paper_authors: Yixin Chen, Junfeng Ni, Nan Jiang, Yaowei Zhang, Yixin Zhu, Siyuan Huang</li>
<li>for: 高精度的单视图图像场景重建</li>
<li>methods: 提议使用单视图图像中的形状和Texture场景重建</li>
<li>results: 比前方法提高27.7%和11.6%的高精度文本化3D场景重建和可视化图像生成<details>
<summary>Abstract</summary>
Reconstructing detailed 3D scenes from single-view images remains a challenging task due to limitations in existing approaches, which primarily focus on geometric shape recovery, overlooking object appearances and fine shape details. To address these challenges, we propose a novel framework for simultaneous high-fidelity recovery of object shapes and textures from single-view images. Our approach utilizes the proposed Single-view neural implicit Shape and Radiance field (SSR) representations to leverage both explicit 3D shape supervision and volume rendering of color, depth, and surface normal images. To overcome shape-appearance ambiguity under partial observations, we introduce a two-stage learning curriculum incorporating both 3D and 2D supervisions. A distinctive feature of our framework is its ability to generate fine-grained textured meshes while seamlessly integrating rendering capabilities into the single-view 3D reconstruction model. This integration enables not only improved textured 3D object reconstruction by 27.7% and 11.6% on the 3D-FRONT and Pix3D datasets, respectively, but also supports the rendering of images from novel viewpoints. Beyond individual objects, our approach facilitates composing object-level representations into flexible scene representations, thereby enabling applications such as holistic scene understanding and 3D scene editing. We conduct extensive experiments to demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传送给定文本到简化中文。<</SYS>>现有方法存在重大限制，它们主要关注几何形状恢复，忽略物体外观和细节形状。为解决这些挑战，我们提出一种新的框架，同时实现高精度物体形状和Texture的恢复从单视图图像中。我们的方法利用我们提出的单视图神经隐式形状场（SSR）表示，同时利用Explicit 3D形状supervision和VolumeRendering的颜色、深度和表面法向图像。为了解决形状-外观不确定性，我们提出了一个两stage学习课程，包括3D和2Dsupervision。我们的框架可以生成细节rich的Texture矩阵，同时将Rendering集成到单视图3D重建模型中。这种集成不仅提高了Texture矩阵的3D物体重建精度（3D-FRONT和Pix3D数据集上提高了27.7%和11.6%），而且还支持从新视点渲染图像。此外，我们的方法可以将对象级别的表示集成到 flexible scene表示中，使得应用场景包括整体场景理解和3D场景编辑。我们进行了广泛的实验，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Progressive-Recurrent-Network-for-Shadow-Removal"><a href="#Progressive-Recurrent-Network-for-Shadow-Removal" class="headerlink" title="Progressive Recurrent Network for Shadow Removal"></a>Progressive Recurrent Network for Shadow Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00455">http://arxiv.org/abs/2311.00455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghui Wang, Wengang Zhou, Hao Feng, Li Li, Houqiang Li</li>
<li>for: 本研究旨在提出一种简单 yet effective的 Progressive Recurrent Network (PRNet)，用于逐渐除去单张图像中的阴影。</li>
<li>methods: 我们提出了一种分层执行的推进方法，包括阴影特征提取和进行逐渐阴影除去。特别地，我们使用了一个新的重新集成模块和更新模块，以便更好地利用上一次迭代的输出，从而提高阴影除去的性能。</li>
<li>results: 我们在ISTD、ISTD+和SRD三个benchmark上进行了广泛的实验，结果表明，我们的方法可以有效地除去阴影，并且性能较为出色。<details>
<summary>Abstract</summary>
Single-image shadow removal is a significant task that is still unresolved. Most existing deep learning-based approaches attempt to remove the shadow directly, which can not deal with the shadow well. To handle this issue, we consider removing the shadow in a coarse-to-fine fashion and propose a simple but effective Progressive Recurrent Network (PRNet). The network aims to remove the shadow progressively, enabing us to flexibly adjust the number of iterations to strike a balance between performance and time. Our network comprises two parts: shadow feature extraction and progressive shadow removal. Specifically, the first part is a shallow ResNet which constructs the representations of the input shadow image on its original size, preventing the loss of high-frequency details caused by the downsampling operation. The second part has two critical components: the re-integration module and the update module. The proposed re-integration module can fully use the outputs of the previous iteration, providing input for the update module for further shadow removal. In this way, the proposed PRNet makes the whole process more concise and only uses 29% network parameters than the best published method. Extensive experiments on the three benchmarks, ISTD, ISTD+, and SRD, demonstrate that our method can effectively remove shadows and achieve superior performance.
</details>
<details>
<summary>摘要</summary>
单一图像阴影除除是一个 ainda 未解决的重要任务。大多数现有的深度学习基于的方法都尝试直接 removing 阴影，可以不好地处理阴影。为了解决这个问题，我们考虑使用一个进行逐步除阴影的方法，并提出一个简单 yet 有效的进步Recurrent Network (PRNet)。这个网络的目的是逐步 removing 阴影，让我们可以调整访问次数，寻求在性能和时间之间做出平衡。我们的网络包括两个部分：阴影特征提取和进行逐步阴影除除。具体来说，第一个部分是一个浅层ResNet，它在输入阴影图像的原始大小上建立了阴影的表现，避免因下推过程而失去高频率细节的损失。第二个部分有两个重要的组件：重新组合模组和更新模组。我们提出的重新组合模组可以充分利用上一次迭代的输出，作为更新模组的输入，从而使得整个过程更为简洁，仅需29%的网络参数数量比最佳已公开方法少。广泛的实验表明，我们的方法可以有效地除阴影，并在性能方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="CLIP-AD-A-Language-Guided-Staged-Dual-Path-Model-for-Zero-shot-Anomaly-Detection"><a href="#CLIP-AD-A-Language-Guided-Staged-Dual-Path-Model-for-Zero-shot-Anomaly-Detection" class="headerlink" title="CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection"></a>CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00453">http://arxiv.org/abs/2311.00453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuhai Chen, Jiangning Zhang, Guanzhong Tian, Haoyang He, Wuhao Zhang, Yabiao Wang, Chengjie Wang, Yunsheng Wu, Yong Liu</li>
<li>for: 本文研究零例异常检测（AD），一种有价值但未得到充分研究的任务，即在测试对象没有任何参考图像的情况下进行AD。</li>
<li>methods: 我们采用了语言引导策略，并提出了一种简单 yet effective 的架构 CLIP-AD，利用大型视觉语言模型 CLIP 的出色零例分类能力。</li>
<li>results: 我们的方法可以准确地检测异常，例如在 VisA 上，SDP 模型比 SOTA 高 +1.0&#x2F;+1.2 分在分类&#x2F;分割 F1 分数上。而 SDP+ 模型在 VisA 上还实现了 +1.9&#x2F;+11.7 的提升。<details>
<summary>Abstract</summary>
This paper considers zero-shot Anomaly Detection (AD), a valuable yet under-studied task, which performs AD without any reference images of the test objects. Specifically, we employ a language-guided strategy and propose a simple-yet-effective architecture CLIP-AD, leveraging the superior zero-shot classification capabilities of the large vision-language model CLIP. A natural idea for anomaly segmentation is to directly calculate the similarity between text/image features, but we observe opposite predictions and irrelevant highlights in the results. Inspired by the phenomena, we introduce a Staged Dual-Path model (SDP) that effectively uses features from various levels and applies architecture and feature surgery to address these issues. Furthermore, delving beyond surface phenomena, we identify the problem arising from misalignment of text/image features in the joint embedding space. Thus, we introduce a fine-tuning strategy by adding linear layers and construct an extended model SDP+, further enhancing the performance. Abundant experiments demonstrate the effectiveness of our approach, e.g., on VisA, SDP outperforms SOTA by +1.0/+1.2 in classification/segmentation F1 scores, while SDP+ achieves +1.9/+11.7 improvements.
</details>
<details>
<summary>摘要</summary>
However, we observe that directly calculating the similarity between text/image features can lead to inaccurate predictions and irrelevant highlights. To address this issue, we introduce a Staged Dual-Path (SDP) model that effectively uses features from various levels and applies architecture and feature surgery.Furthermore, we identify the problem of misaligned text/image features in the joint embedding space, which can cause the model to perform poorly. To address this issue, we propose a fine-tuning strategy that involves adding linear layers and constructing an extended model called SDP+. This approach significantly enhances the performance of the model, as demonstrated by abundant experiments.For example, on the VisA dataset, SDP outperforms state-of-the-art (SOTA) models by +1.0/+1.2 in classification/segmentation F1 scores, while SDP+ achieves +1.9/+11.7 improvements. Our approach offers a valuable solution for zero-shot AD, and the SDP+ model represents a new SOTA in this area.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Traffic-Object-Detection-in-Variable-Illumination-with-RGB-Event-Fusion"><a href="#Enhancing-Traffic-Object-Detection-in-Variable-Illumination-with-RGB-Event-Fusion" class="headerlink" title="Enhancing Traffic Object Detection in Variable Illumination with RGB-Event Fusion"></a>Enhancing Traffic Object Detection in Variable Illumination with RGB-Event Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00436">http://arxiv.org/abs/2311.00436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanwen Liu, Nan Yang, Yang Wang, Yuke Li, Xiangmo Zhao, Fei-Yue Wang<br>for: 这个论文的目的是提高对变化照明条件下的交通物体检测精度。methods: 该论文使用了生物体会论所启发的事件摄像头和一种新的结构意识协调网络（SFNet），通过跨Modalities协调来补做图像中失去的信息，以获得适应照明条件的对象结构表示。results: 该论文的实验结果显示，SFNet可以超越传统摄像头的感知boundary，并在MAP50和MAP50:95上比框架方法高出8.0%和5.9%。<details>
<summary>Abstract</summary>
Traffic object detection under variable illumination is challenging due to the information loss caused by the limited dynamic range of conventional frame-based cameras. To address this issue, we introduce bio-inspired event cameras and propose a novel Structure-aware Fusion Network (SFNet) that extracts sharp and complete object structures from the event stream to compensate for the lost information in images through cross-modality fusion, enabling the network to obtain illumination-robust representations for traffic object detection. Specifically, to mitigate the sparsity or blurriness issues arising from diverse motion states of traffic objects in fixed-interval event sampling methods, we propose the Reliable Structure Generation Network (RSGNet) to generate Speed Invariant Frames (SIF), ensuring the integrity and sharpness of object structures. Next, we design a novel Adaptive Feature Complement Module (AFCM) which guides the adaptive fusion of two modality features to compensate for the information loss in the images by perceiving the global lightness distribution of the images, thereby generating illumination-robust representations. Finally, considering the lack of large-scale and high-quality annotations in the existing event-based object detection datasets, we build a DSEC-Det dataset, which consists of 53 sequences with 63,931 images and more than 208,000 labels for 8 classes. Extensive experimental results demonstrate that our proposed SFNet can overcome the perceptual boundaries of conventional cameras and outperform the frame-based method by 8.0% in mAP50 and 5.9% in mAP50:95. Our code and dataset will be available at https://github.com/YN-Yang/SFNet.
</details>
<details>
<summary>摘要</summary>
受变化照明条件影响的交通物体检测是一个挑战，因为传统的帧基式摄像机的动态范围有限，会导致信息损失。为解决这问题，我们介绍了生物体iben inspired事件摄像机和一种新的结构意识混合网络（SFNet），该网络可以从事件流中提取完整和锐化的物体结构，并通过cross-modality混合来补偿图像中丢失的信息，使网络能够获得适应照明的表示。 Specifically，我们提出了可靠结构生成网络（RSGNet），以减少在固定间隔事件采样方法中的缺乏或模糊问题，并确保物体结构的完整性和锐度。然后，我们设计了一种适应特征补偿模块（AFCM），该模块可以根据图像的全局亮度分布进行适应混合，以补偿图像中的信息损失。最后，由于现有的事件基于物体检测数据集lacks large-scale和高质量的标注，我们建立了DSEC-Det数据集，该数据集包括53个序列，63,931个图像和208,000个标注，用于8类物体检测。我们的实验结果表明，我们提出的SFNet可以超越传统摄像机的感知边界，并在MAP50和MAP50:95中比 Frame-based方法提高8.0%和5.9%。我们的代码和数据集将在https://github.com/YN-Yang/SFNet上提供。
</details></li>
</ul>
<hr>
<h2 id="Event-based-Background-Oriented-Schlieren"><a href="#Event-based-Background-Oriented-Schlieren" class="headerlink" title="Event-based Background-Oriented Schlieren"></a>Event-based Background-Oriented Schlieren</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00434">http://arxiv.org/abs/2311.00434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tub-rip/event_based_bos">https://github.com/tub-rip/event_based_bos</a></li>
<li>paper_authors: Shintaro Shiba, Friedhelm Hamann, Yoshimitsu Aoki, Guillermo Gallego</li>
<li>for: 这篇论文的目的是探讨使用事件摄像机进行液体流动的观察，以减少高速摄像机和计算成本的限制。</li>
<li>methods: 该论文提出了一种基于事件的液体流动观察技术，使用事件摄像机捕捉液体流动的变化，并提出了一种基于变形优化的方法来连接事件数据和液体流动。</li>
<li>results: 实验结果表明，该方法可以使用事件摄像机获得与传统帧基于摄像机相同的质量结果，并且在黑暗环境下工作和在慢动作情况下进行分析。此外，该方法还可以减少计算成本和增加数据效率。<details>
<summary>Abstract</summary>
Schlieren imaging is an optical technique to observe the flow of transparent media, such as air or water, without any particle seeding. However, conventional frame-based techniques require both high spatial and temporal resolution cameras, which impose bright illumination and expensive computation limitations. Event cameras offer potential advantages (high dynamic range, high temporal resolution, and data efficiency) to overcome such limitations due to their bio-inspired sensing principle. This paper presents a novel technique for perceiving air convection using events and frames by providing the first theoretical analysis that connects event data and schlieren. We formulate the problem as a variational optimization one combining the linearized event generation model with a physically-motivated parameterization that estimates the temporal derivative of the air density. The experiments with accurately aligned frame- and event camera data reveal that the proposed method enables event cameras to obtain on par results with existing frame-based optical flow techniques. Moreover, the proposed method works under dark conditions where frame-based schlieren fails, and also enables slow-motion analysis by leveraging the event camera's advantages. Our work pioneers and opens a new stack of event camera applications, as we publish the source code as well as the first schlieren dataset with high-quality frame and event data. https://github.com/tub-rip/event_based_bos
</details>
<details>
<summary>摘要</summary>
射频成像是一种光学技术，可以无需粒子种子观测透明媒体的流动，如空气或水。然而，传统的帧基技术需要高分辨率的摄像机和贵重的计算限制。事件相机具有优势（高动态范围、高时间分辨率和数据效率），可以超越这些限制。这篇论文提出了一种使用事件和帧来捕捉空气循环的新方法。我们将问题形式为一种变分优化问题，将线性化事件生成模型与物理激活的参数化相结合，以估计空气密度的时间导数。实验结果表明，我们的方法可以让事件相机与传统的帧基Optical flow技术具有相同的性能。此外，我们的方法在黑暗条件下工作，并且可以利用事件相机的优势进行慢动作分析。我们的工作开创了新的事件相机应用领域，并将源代码以及高质量帧和事件数据的第一个Schlieren数据集发布在GitHub上。https://github.com/tub-rip/event_based_bos
</details></li>
</ul>
<hr>
<h2 id="Feature-oriented-Deep-Learning-Framework-for-Pulmonary-Cone-beam-CT-CBCT-Enhancement-with-Multi-task-Customized-Perceptual-Loss"><a href="#Feature-oriented-Deep-Learning-Framework-for-Pulmonary-Cone-beam-CT-CBCT-Enhancement-with-Multi-task-Customized-Perceptual-Loss" class="headerlink" title="Feature-oriented Deep Learning Framework for Pulmonary Cone-beam CT (CBCT) Enhancement with Multi-task Customized Perceptual Loss"></a>Feature-oriented Deep Learning Framework for Pulmonary Cone-beam CT (CBCT) Enhancement with Multi-task Customized Perceptual Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00412">http://arxiv.org/abs/2311.00412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhujiarui42/cfp-loss">https://github.com/zhujiarui42/cfp-loss</a></li>
<li>paper_authors: Jiarui Zhu, Werxing Chen, Hongfei Sun, Shaohua Zhi, Jing Qin, Jing Cai, Ge Ren<br>for:The paper aims to improve the quality of cone-beam computed tomography (CBCT) images for cancer treatment planning by proposing a novel feature-oriented deep learning framework.methods:The proposed framework consists of two main components: a multi-task learning feature-selection network (MTFS-Net) and a CBCT-to-CT translation network guided by feature-to-feature perceptual loss. The framework uses advanced generative models such as U-Net, GAN, and CycleGAN to generate high-quality synthesized CT (sCT) images.results:The proposed framework outperforms the state-of-the-art models for pulmonary CBCT enhancement, with an average SSIM index of 0.9869 and an average PSNR index of 39.9621. The generated sCT images have visually pleasing performance, with effective artifacts suppression, noise reduction, and distinctive anatomical details preservation.<details>
<summary>Abstract</summary>
Cone-beam computed tomography (CBCT) is routinely collected during image-guided radiation therapy (IGRT) to provide updated patient anatomy information for cancer treatments. However, CBCT images often suffer from streaking artifacts and noise caused by under-rate sampling projections and low-dose exposure, resulting in low clarity and information loss. While recent deep learning-based CBCT enhancement methods have shown promising results in suppressing artifacts, they have limited performance on preserving anatomical details since conventional pixel-to-pixel loss functions are incapable of describing detailed anatomy. To address this issue, we propose a novel feature-oriented deep learning framework that translates low-quality CBCT images into high-quality CT-like imaging via a multi-task customized feature-to-feature perceptual loss function. The framework comprises two main components: a multi-task learning feature-selection network(MTFS-Net) for customizing the perceptual loss function; and a CBCT-to-CT translation network guided by feature-to-feature perceptual loss, which uses advanced generative models such as U-Net, GAN and CycleGAN. Our experiments showed that the proposed framework can generate synthesized CT (sCT) images for the lung that achieved a high similarity to CT images, with an average SSIM index of 0.9869 and an average PSNR index of 39.9621. The sCT images also achieved visually pleasing performance with effective artifacts suppression, noise reduction, and distinctive anatomical details preservation. Our experiment results indicate that the proposed framework outperforms the state-of-the-art models for pulmonary CBCT enhancement. This framework holds great promise for generating high-quality anatomical imaging from CBCT that is suitable for various clinical applications.
</details>
<details>
<summary>摘要</summary>
computed tomography cone-beam (CBCT) routinely collected during image-guided radiation therapy (IGRT) to provide updated patient anatomy information for cancer treatments. However, CBCT images often suffer from streaking artifacts and noise caused by under-rate sampling projections and low-dose exposure, resulting in low clarity and information loss. While recent deep learning-based CBCT enhancement methods have shown promising results in suppressing artifacts, they have limited performance on preserving anatomical details since conventional pixel-to-pixel loss functions are incapable of describing detailed anatomy. To address this issue, we propose a novel feature-oriented deep learning framework that translates low-quality CBCT images into high-quality CT-like imaging via a multi-task customized feature-to-feature perceptual loss function. The framework comprises two main components: a multi-task learning feature-selection network (MTFS-Net) for customizing the perceptual loss function; and a CBCT-to-CT translation network guided by feature-to-feature perceptual loss, which uses advanced generative models such as U-Net, GAN, and CycleGAN. Our experiments showed that the proposed framework can generate synthesized CT (sCT) images for the lung that achieved a high similarity to CT images, with an average SSIM index of 0.9869 and an average PSNR index of 39.9621. The sCT images also achieved visually pleasing performance with effective artifacts suppression, noise reduction, and distinctive anatomical details preservation. Our experiment results indicate that the proposed framework outperforms the state-of-the-art models for pulmonary CBCT enhancement. This framework holds great promise for generating high-quality anatomical imaging from CBCT that is suitable for various clinical applications.
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Face-Recognition-with-Maximal-Entropy-and-Objectosphere-Loss"><a href="#Open-Set-Face-Recognition-with-Maximal-Entropy-and-Objectosphere-Loss" class="headerlink" title="Open-Set Face Recognition with Maximal Entropy and Objectosphere Loss"></a>Open-Set Face Recognition with Maximal Entropy and Objectosphere Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00400">http://arxiv.org/abs/2311.00400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Henrique Vareto, Yu Linghu, Terrance E. Boult, William Robson Schwartz, Manuel Günther</li>
<li>for: 这个研究探讨了面部识别task中的开集领域，特别是在低False Positive Identification Rate下进行识别。</li>
<li>methods: 本研究使用了类别损失(OS)和提案的差异Entropy损失(MEL)，MEL对于负样本进行增加熵和知识目标类别添加罚则，以提高特征提取和类别特有化。</li>
<li>results: 研究获得了适用于三个不同数据集(LFW、IJB-C和UCCS)的开集领域面部识别优异的结果，并在对额外负样本进行微调时达到了顶尖性能。<details>
<summary>Abstract</summary>
Open-set face recognition characterizes a scenario where unknown individuals, unseen during the training and enrollment stages, appear on operation time. This work concentrates on watchlists, an open-set task that is expected to operate at a low False Positive Identification Rate and generally includes only a few enrollment samples per identity. We introduce a compact adapter network that benefits from additional negative face images when combined with distinct cost functions, such as Objectosphere Loss (OS) and the proposed Maximal Entropy Loss (MEL). MEL modifies the traditional Cross-Entropy loss in favor of increasing the entropy for negative samples and attaches a penalty to known target classes in pursuance of gallery specialization. The proposed approach adopts pre-trained deep neural networks (DNNs) for face recognition as feature extractors. Then, the adapter network takes deep feature representations and acts as a substitute for the output layer of the pre-trained DNN in exchange for an agile domain adaptation. Promising results have been achieved following open-set protocols for three different datasets: LFW, IJB-C, and UCCS as well as state-of-the-art performance when supplementary negative data is properly selected to fine-tune the adapter network.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Omni-supervised-Referring-Expression-Segmentation"><a href="#Towards-Omni-supervised-Referring-Expression-Segmentation" class="headerlink" title="Towards Omni-supervised Referring Expression Segmentation"></a>Towards Omni-supervised Referring Expression Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00397">http://arxiv.org/abs/2311.00397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minglang Huang, Yiyi Zhou, Gen Luo, Guannan Jiang, Weilin Zhuang, Xiaoshuai Sun</li>
<li>for: 提高 Referring Expression Segmentation (RES) 训练效率，使用无标注数据、半标注数据和弱标注数据进行高效的 RES 训练。</li>
<li>methods: 提出 Omni-supervised Referring Expression Segmentation (Omni-RES) 任务，利用无标注数据、半标注数据和弱标注数据进行 teacher-student 学习，选择和改进高质量 pseudo-masks，以提高 RES 性能。</li>
<li>results: 对 state-of-the-art RES 模型进行了广泛的实验，并证明了 Omni-RES 方法可以在各种 RES 数据集上具有明显的优势，比如使用仅 10% 完全标注数据可以 дости得 100% 完全超vised 性能，并且比 semi-supervised 方法具有大幅度的提升（+14.93% on RefCOCO 和 +14.95% on RefCOCO+）。 Omni-RES 还可以利用大规模的视力语言如 Visual Genome 来促进低成本 RES 训练，并实现新的 SOTA 性能（80.66 on RefCOCO）。<details>
<summary>Abstract</summary>
Referring Expression Segmentation (RES) is an emerging task in computer vision, which segments the target instances in images based on text descriptions. However, its development is plagued by the expensive segmentation labels. To address this issue, we propose a new learning task for RES called Omni-supervised Referring Expression Segmentation (Omni-RES), which aims to make full use of unlabeled, fully labeled and weakly labeled data, e.g., referring points or grounding boxes, for efficient RES training. To accomplish this task, we also propose a novel yet strong baseline method for Omni-RES based on the recently popular teacher-student learning, where where the weak labels are not directly transformed into supervision signals but used as a yardstick to select and refine high-quality pseudo-masks for teacher-student learning. To validate the proposed Omni-RES method, we apply it to a set of state-of-the-art RES models and conduct extensive experiments on a bunch of RES datasets. The experimental results yield the obvious merits of Omni-RES than the fully-supervised and semi-supervised training schemes. For instance, with only 10% fully labeled data, Omni-RES can help the base model achieve 100% fully supervised performance, and it also outperform the semi-supervised alternative by a large margin, e.g., +14.93% on RefCOCO and +14.95% on RefCOCO+, respectively. More importantly, Omni-RES also enable the use of large-scale vision-langauges like Visual Genome to facilitate low-cost RES training, and achieve new SOTA performance of RES, e.g., 80.66 on RefCOCO.
</details>
<details>
<summary>摘要</summary>
新的 Referring Expression Segmentation (RES) 任务在计算机视觉领域崛起，它根据图像中的文本描述进行目标实例分割。然而，其发展受到严重的分割标签成本的限制。为解决这个问题，我们提出了一个新的学习任务，即 Omni-supervised Referring Expression Segmentation (Omni-RES)，该任务旨在利用不 labels、完全 labels 和弱 labels，如引用点或基础框架，进行高效的 RES 训练。为完成这个任务，我们还提出了一种新的基线方法，基于最近受欢迎的教师学生学习，其中弱标签不直接转化为监督信号，而是用作选择和改进高质量的 pseudo-masks 的依据。为验证提出的 Omni-RES 方法，我们对一些 state-of-the-art RES 模型进行了广泛的实验，并在一些 RES 数据集上进行了extensive的测试。实验结果表明，Omni-RES 方法在资源受限的情况下具有明显的优势，比如只使用 10% 完全标签数据，Omni-RES 可以帮助基本模型达到100% 完全监督性能。此外，Omni-RES 还可以在很大规模的视觉语言，如视觉语言，进行低成本的 RES 训练，并实现新的 SOTA 性能，如 RefCOCO 上的 80.66%。
</details></li>
</ul>
<hr>
<h2 id="Fixation-based-Self-calibration-for-Eye-Tracking-in-VR-Headsets"><a href="#Fixation-based-Self-calibration-for-Eye-Tracking-in-VR-Headsets" class="headerlink" title="Fixation-based Self-calibration for Eye Tracking in VR Headsets"></a>Fixation-based Self-calibration for Eye Tracking in VR Headsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00391">http://arxiv.org/abs/2311.00391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryusei Uramune, Sei Ikeda, Hiroki Ishizuka, Osamu Oshiro<br>for: This paper proposes a novel self-calibration method for eye tracking in virtual reality (VR) headsets.methods: The proposed method uses an extension of the I-VDT algorithm to detect fixations from time-series gaze data, and optimizes calibration parameters by minimizing a dispersion metric of points of regard.results: The proposed method achieved an accuracy of 2.1° in a study with 18 participants walking in two VR environments with many occlusions, which is significantly lower than the average offset. The method also has the potential to improve accuracy by up to 1.2° by refining the fixation detection or optimization algorithm.<details>
<summary>Abstract</summary>
This study proposes a novel self-calibration method for eye tracking in a virtual reality (VR) headset. The proposed method is based on the assumptions that the user's viewpoint can freely move and that the points of regard (PoRs) from different viewpoints are distributed within a small area on an object surface during visual fixation. In the method, fixations are first detected from the time-series data of uncalibrated gaze directions using an extension of the I-VDT (velocity and dispersion threshold identification) algorithm to a three-dimensional (3D) scene. Then, the calibration parameters are optimized by minimizing the sum of a dispersion metrics of the PoRs. The proposed method can potentially identify the optimal calibration parameters representing the user-dependent offset from the optical axis to the visual axis without explicit user calibration, image processing, or marker-substitute objects. For the gaze data of 18 participants walking in two VR environments with many occlusions, the proposed method achieved an accuracy of 2.1$^\circ$, which was significantly lower than the average offset. Our method is the first self-calibration method with an average error lower than 3$^\circ$ in 3D environments. Further, the accuracy of the proposed method can be improved by up to 1.2$^\circ$ by refining the fixation detection or optimization algorithm.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这个研究提出了一种新的自动准确方法 для眼动跟踪在虚拟现实（VR）头盔中。该方法基于用户视点可以自由移动以及不同视点下的点注视（PoR）在物体表面上的分布的假设。在方法中，首先从未加工的眼动资料中检测出了时间序列中的fixation，使用了三维场景中的I-VDT（速度和分散度阈值识别）算法的扩展。然后，使用PoR的分布来优化准确参数。该方法可能可以无需用户准确参数、图像处理或marker substitute对象来确定用户依赖的偏移量从光学轴到视觉轴。为18名参与者在两个VR环境中走动的眼动数据，该方法实现了2.1°的精度，与平均偏移量相比较低。我们的方法是3D环境中第一个自动准确方法，其平均错误低于3°。此外，通过改进fixation检测或优化算法，该方法的精度可以提高到1.2°。
</details></li>
</ul>
<hr>
<h2 id="NeuralGF-Unsupervised-Point-Normal-Estimation-by-Learning-Neural-Gradient-Function"><a href="#NeuralGF-Unsupervised-Point-Normal-Estimation-by-Learning-Neural-Gradient-Function" class="headerlink" title="NeuralGF: Unsupervised Point Normal Estimation by Learning Neural Gradient Function"></a>NeuralGF: Unsupervised Point Normal Estimation by Learning Neural Gradient Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00389">http://arxiv.org/abs/2311.00389</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leoqli/neuralgf">https://github.com/leoqli/neuralgf</a></li>
<li>paper_authors: Qing Li, Huifang Feng, Kanle Shi, Yue Gao, Yi Fang, Yu-Shen Liu, Zhizhong Han</li>
<li>for: 这种paper的目的是提出一种深度学习方法来从点云数据中直接估计方向法向量，不需要使用地面真实的正常指导。</li>
<li>methods: 该方法基于引入新的神经网络梯度函数学习新方法，使神经网络能够更好地适应输入点云数据，并且在点云数据上逐渐建立一个全局表示。</li>
<li>results: 该方法可以在各种常用的点云数据集上取得更高的准确率，比之前的方法更加稳定和Robust。Here is the same information in English:</li>
<li>for: The purpose of this paper is to propose a deep learning method to directly estimate oriented normals from point cloud data without using ground truth normals as supervision.</li>
<li>methods: The method is based on introducing a new paradigm for learning neural gradient functions, which encourages the neural network to fit the input point clouds and yield unit-norm gradients at the points.</li>
<li>results: The method can achieve higher accuracy on various widely used point cloud datasets, and is more robust and stable than previous methods.<details>
<summary>Abstract</summary>
Normal estimation for 3D point clouds is a fundamental task in 3D geometry processing. The state-of-the-art methods rely on priors of fitting local surfaces learned from normal supervision. However, normal supervision in benchmarks comes from synthetic shapes and is usually not available from real scans, thereby limiting the learned priors of these methods. In addition, normal orientation consistency across shapes remains difficult to achieve without a separate post-processing procedure. To resolve these issues, we propose a novel method for estimating oriented normals directly from point clouds without using ground truth normals as supervision. We achieve this by introducing a new paradigm for learning neural gradient functions, which encourages the neural network to fit the input point clouds and yield unit-norm gradients at the points. Specifically, we introduce loss functions to facilitate query points to iteratively reach the moving targets and aggregate onto the approximated surface, thereby learning a global surface representation of the data. Meanwhile, we incorporate gradients into the surface approximation to measure the minimum signed deviation of queries, resulting in a consistent gradient field associated with the surface. These techniques lead to our deep unsupervised oriented normal estimator that is robust to noise, outliers and density variations. Our excellent results on widely used benchmarks demonstrate that our method can learn more accurate normals for both unoriented and oriented normal estimation tasks than the latest methods. The source code and pre-trained model are publicly available at https://github.com/LeoQLi/NeuralGF.
</details>
<details>
<summary>摘要</summary>
通常估算三维点云是三维几何处理的基本任务。现状的方法均依赖于当地表面的尝试学习，但是这些方法的学习环境通常来自于Synthetic shapes的标准测试数据，因此无法适用于实际扫描数据。另外，保持方向orientation的一致性 across shapes remain difficult to achieve without a separate post-processing procedure.为解决这些问题，我们提出了一种新的方法，可以直接从点云中估算方向 orientation，不需要使用真实的标准 норals作为监督。我们通过引入一种新的神经网络梯度函数学习方法，让神经网络能够适应输入点云，并且在点上得到单位范围的梯度。具体来说，我们引入了一种新的损失函数，使得查询点能够逐步到达移动目标，并将查询点积累到 aproximated surface上，从而学习全局表面 Representation of the data。同时，我们将梯度 integrate into surface approximation，以计算最小签名偏差的查询点，从而获得一个协调的梯度场，与表面相关的。这些技术导致我们的深度无监督方向 норals估算器，能够更高效地处理噪声、异常值和密度变化。我们的出色的结果在广泛使用的 benchmark 上表明，我们的方法可以学习更加准确的 normals，并且在无监督 oriented normal estimation 和 oriented normal estimation 两个任务中均有优异表现。我们的代码和预训练模型可以在 <https://github.com/LeoQLi/NeuralGF> 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Cooperative-Trajectory-Representations-for-Motion-Forecasting"><a href="#Learning-Cooperative-Trajectory-Representations-for-Motion-Forecasting" class="headerlink" title="Learning Cooperative Trajectory Representations for Motion Forecasting"></a>Learning Cooperative Trajectory Representations for Motion Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00371">http://arxiv.org/abs/2311.00371</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/air-thu/dair-v2x-seq">https://github.com/air-thu/dair-v2x-seq</a></li>
<li>paper_authors: Hongzhi Ruan, Haibao Yu, Wenxian Yang, Siqi Fan, Yingjuan Tang, Zaiqing Nie</li>
<li>For: This paper focuses on motion forecasting for autonomous driving, specifically using cooperative information from infrastructure and other vehicles to enhance the ego vehicle’s perception capability.* Methods: The proposed method, V2X-Graph, is an interpretable and end-to-end learning framework that leverages cooperative motion and interaction contexts using an interpretable graph.* Results: Experimental results on the V2I motion forecasting dataset V2X-Seq demonstrate the effectiveness of V2X-Graph, and the first real-world V2X motion forecasting dataset V2X-Traj is constructed to further evaluate the method.<details>
<summary>Abstract</summary>
Motion forecasting is an essential task for autonomous driving, and the effective information utilization from infrastructure and other vehicles can enhance motion forecasting capabilities. Existing research have primarily focused on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while underutilizing the motion and interaction information of traffic participants observed from cooperative devices. In this paper, we first propose the cooperative trajectory representations learning paradigm. Specifically, we present V2X-Graph, the first interpretable and end-to-end learning framework for cooperative motion forecasting. V2X-Graph employs an interpretable graph to fully leverage the cooperative motion and interaction contexts. Experimental results on the vehicle-to-infrastructure (V2I) motion forecasting dataset, V2X-Seq, demonstrate the effectiveness of V2X-Graph. To further evaluate on V2X scenario, we construct the first real-world vehicle-to-everything (V2X) motion forecasting dataset V2X-Traj, and the performance shows the advantage of our method. We hope both V2X-Graph and V2X-Traj can facilitate the further development of cooperative motion forecasting. Find project at https://github.com/AIR-THU/V2X-Graph, find data at https://github.com/AIR-THU/DAIR-V2X-Seq.
</details>
<details>
<summary>摘要</summary>
行为预测是自动驾驶中的关键任务，可以通过基础设施和其他车辆的有效信息利用来增强行为预测能力。现有研究主要是利用单一帧合作信息来增强自驾车的有限感知能力，而忽略了交通参与者的运动和互动信息，这些信息可以从合作设备上获得。在这篇论文中，我们首先提出了合作轨迹表示学习 парадигмы。 Specifically, we present V2X-Graph, the first interpretable and end-to-end learning framework for cooperative motion forecasting. V2X-Graph employs an interpretable graph to fully leverage the cooperative motion and interaction contexts. Experimental results on the vehicle-to-infrastructure (V2I) motion forecasting dataset, V2X-Seq, demonstrate the effectiveness of V2X-Graph. To further evaluate on V2X scenario, we construct the first real-world vehicle-to-everything (V2X) motion forecasting dataset V2X-Traj, and the performance shows the advantage of our method. We hope both V2X-Graph and V2X-Traj can facilitate the further development of cooperative motion forecasting. 找到项目在 GitHub上：https://github.com/AIR-THU/V2X-Graph，找到数据在 GitHub上：https://github.com/AIR-THU/DAIR-V2X-Seq。
</details></li>
</ul>
<hr>
<h2 id="LatentWarp-Consistent-Diffusion-Latents-for-Zero-Shot-Video-to-Video-Translation"><a href="#LatentWarp-Consistent-Diffusion-Latents-for-Zero-Shot-Video-to-Video-Translation" class="headerlink" title="LatentWarp: Consistent Diffusion Latents for Zero-Shot Video-to-Video Translation"></a>LatentWarp: Consistent Diffusion Latents for Zero-Shot Video-to-Video Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00353">http://arxiv.org/abs/2311.00353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Bao, Di Qiu, Guoliang Kang, Baochang Zhang, Bo Jin, Kaiye Wang, Pengfei Yan</li>
<li>for: 本研究旨在提出一种新的零shot视频到视频翻译方法，以解决现有方法中的时间一致问题。</li>
<li>methods: 该方法基于图像扩散模型，并通过在潜在空间中添加折叠操作来约束查询符。</li>
<li>results: 实验结果表明，该方法可以有效地提高生成视频的视觉一致性。<details>
<summary>Abstract</summary>
Leveraging the generative ability of image diffusion models offers great potential for zero-shot video-to-video translation. The key lies in how to maintain temporal consistency across generated video frames by image diffusion models. Previous methods typically adopt cross-frame attention, \emph{i.e.,} sharing the \textit{key} and \textit{value} tokens across attentions of different frames, to encourage the temporal consistency. However, in those works, temporal inconsistency issue may not be thoroughly solved, rendering the fidelity of generated videos limited.%The current state of the art cross-frame attention method aims at maintaining fine-grained visual details across frames, but it is still challenged by the temporal coherence problem. In this paper, we find the bottleneck lies in the unconstrained query tokens and propose a new zero-shot video-to-video translation framework, named \textit{LatentWarp}. Our approach is simple: to constrain the query tokens to be temporally consistent, we further incorporate a warping operation in the latent space to constrain the query tokens. Specifically, based on the optical flow obtained from the original video, we warp the generated latent features of last frame to align with the current frame during the denoising process. As a result, the corresponding regions across the adjacent frames can share closely-related query tokens and attention outputs, which can further improve latent-level consistency to enhance visual temporal coherence of generated videos. Extensive experiment results demonstrate the superiority of \textit{LatentWarp} in achieving video-to-video translation with temporal coherence.
</details>
<details>
<summary>摘要</summary>
利用生成能力的图像扩散模型可以实现零码视频到视频翻译，但是保持视频帧之间的时间一致性是关键。先前的方法通常采用跨帧注意力，即在不同帧之间共享键和值符号，以便强制实现时间一致性。然而，这些方法可能并未完全解决时间不一致问题，因此生成的视频质量有限。目前领先的跨帧注意力方法旨在保持细致的视觉细节 across 帧，但它们仍然面临着时间一致性问题。在这篇文章中，我们发现瓶颈在无制限的查询符号上，我们提出了一个新的零码视频到视频翻译框架，名为LatentWarp。我们的方法简单：在 latent 空间中使用扩散操作来约束查询符号，以确保查询符号在时间上是一致的。具体来说，根据原始视频中的Optical Flow，我们在生成最后一帧的缺lazier特征上进行扩散操作，以使得相邻帧中的相关区域可以共享相似的查询符号和注意力输出，从而进一步提高latent 级别的一致性，以提高生成的视频的视觉时间一致性。我们的实验结果表明，LatentWarp 可以很好地实现视频到视频翻译的时间一致性。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Head-Orientation-of-Neurotypical-and-Autistic-Individuals-in-Triadic-Conversations"><a href="#Analyzing-Head-Orientation-of-Neurotypical-and-Autistic-Individuals-in-Triadic-Conversations" class="headerlink" title="Analyzing Head Orientation of Neurotypical and Autistic Individuals in Triadic Conversations"></a>Analyzing Head Orientation of Neurotypical and Autistic Individuals in Triadic Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00343">http://arxiv.org/abs/2311.00343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Onur N. Tepencelik, Wenchuan Wei, Pamela C. Cosman, Sujit Dey<br>for:这个论文是为了提出一种使用低分辨率点云数据来估算人体和头部orientation的系统。methods:这个系统使用椭圆适应和人头部特征提取以及 ensemble of neural network regressors来估算人体和头部orientation。results:相比使用RGB摄像头的其他人体和头部orientation估算系统，这个系统使用LiDAR感知器保护用户隐私，同时达到相似的准确率。这个系统的mean absolute estimation error为5.2度和13.7度。这个系统可以量化对话中参与者之间的行为差异，包括Autism Spectrum Disorder（ASD）人群在内。<details>
<summary>Abstract</summary>
We propose a system that estimates people's body and head orientations using low-resolution point cloud data from two LiDAR sensors. Our models make accurate estimations in real-world conversation settings where the subject moves naturally with varying head and body poses. The body orientation estimation model uses ellipse fitting while the head orientation estimation model is a pipeline of geometric feature extraction and an ensemble of neural network regressors. Compared with other body and head orientation estimation systems using RGB cameras, our proposed system uses LiDAR sensors to preserve user privacy, while achieving comparable accuracy. Unlike other body/head orientation estimation systems, our sensors do not require a specified placement in front of the subject. Our models achieve a mean absolute estimation error of 5.2 degrees for body orientation and 13.7 degrees for head orientation. We use our models to quantify behavioral differences between neurotypical and autistic individuals in triadic conversations. Tests of significance show that people with autism spectrum disorder display significantly different behavior compared to neurotypical individuals in terms of distributing attention between participants in a conversation, suggesting that the approach could be a component of a behavioral analysis or coaching system.
</details>
<details>
<summary>摘要</summary>
我们提出了一个系统，该系统使用低分辨率点云数据从两个LiDAR感知器来估算人体和头部方向。我们的模型在实际世界交流场景中具有高准确性，并且可以处理人体和头部不同姿态的自然运动。身体方向估算模型使用椭球适应，而头部方向估算模型则是一个包括几何特征提取和神经网络回归器的管道。与使用RGB摄像头的其他身体和头部方向估算系统相比，我们的提出的系统使用LiDAR感知器保护用户隐私，同时具有相似的准确性。与其他身体/头部方向估算系统不同的是，我们的感知器不需要在前方的主体上进行特定的安装。我们的模型的平均绝对估算误差为5.2度 для身体方向和13.7度 для头部方向。我们使用我们的模型来评估不同于 nevropatypical 个体和自闭症人群在三人交流中的行为差异。测试显示，自闭症人群在对话中分配注意力的方式与非 nevropatypical 个体有 statistically significant 的差异， suggesting that the approach could be a component of a behavioral analysis or coaching system。
</details></li>
</ul>
<hr>
<h2 id="fMRI-PTE-A-Large-scale-fMRI-Pretrained-Transformer-Encoder-for-Multi-Subject-Brain-Activity-Decoding"><a href="#fMRI-PTE-A-Large-scale-fMRI-Pretrained-Transformer-Encoder-for-Multi-Subject-Brain-Activity-Decoding" class="headerlink" title="fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for Multi-Subject Brain Activity Decoding"></a>fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for Multi-Subject Brain Activity Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00342">http://arxiv.org/abs/2311.00342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuelin Qian, Yun Wang, Jingyang Huo, Jianfeng Feng, Yanwei Fu</li>
<li>For: This paper aims to develop an innovative approach for pre-training fMRI data, addressing the challenges of individual brain differences and improving the quality of brain activity decoding.* Methods: The proposed approach, called fMRI-PTE, uses an auto-encoder to transform fMRI signals into 2D representations, ensuring consistency in dimensions and preserving distinct brain activity patterns. A novel learning strategy is introduced for pre-training 2D fMRI images, which enhances the quality of reconstruction and facilitates various downstream tasks.* Results: Extensive experiments demonstrate the effectiveness of fMRI-PTE in addressing the challenges of fMRI data dimensions and improving brain activity decoding. The proposed approach outperforms existing methods in terms of reconstruction quality and adaptability with image generators, offering a promising foundation for further research in this domain.<details>
<summary>Abstract</summary>
The exploration of brain activity and its decoding from fMRI data has been a longstanding pursuit, driven by its potential applications in brain-computer interfaces, medical diagnostics, and virtual reality. Previous approaches have primarily focused on individual subject analysis, highlighting the need for a more universal and adaptable framework, which is the core motivation behind our work. In this work, we propose fMRI-PTE, an innovative auto-encoder approach for fMRI pre-training, with a focus on addressing the challenges of varying fMRI data dimensions due to individual brain differences. Our approach involves transforming fMRI signals into unified 2D representations, ensuring consistency in dimensions and preserving distinct brain activity patterns. We introduce a novel learning strategy tailored for pre-training 2D fMRI images, enhancing the quality of reconstruction. fMRI-PTE's adaptability with image generators enables the generation of well-represented fMRI features, facilitating various downstream tasks, including within-subject and cross-subject brain activity decoding. Our contributions encompass introducing fMRI-PTE, innovative data transformation, efficient training, a novel learning strategy, and the universal applicability of our approach. Extensive experiments validate and support our claims, offering a promising foundation for further research in this domain.
</details>
<details>
<summary>摘要</summary>
traditional Chinese:探索大脑活动和从fMRI数据中解读的探索，有 been a longstanding pursuit, driven by its potential applications in brain-computer interfaces, medical diagnostics, and virtual reality. Previous approaches have primarily focused on individual subject analysis, highlighting the need for a more universal and adaptable framework, which is the core motivation behind our work. In this work, we propose fMRI-PTE, an innovative auto-encoder approach for fMRI pre-training, with a focus on addressing the challenges of varying fMRI data dimensions due to individual brain differences. Our approach involves transforming fMRI signals into unified 2D representations, ensuring consistency in dimensions and preserving distinct brain activity patterns. We introduce a novel learning strategy tailored for pre-training 2D fMRI images, enhancing the quality of reconstruction. fMRI-PTE's adaptability with image generators enables the generation of well-represented fMRI features, facilitating various downstream tasks, including within-subject and cross-subject brain activity decoding. Our contributions encompass introducing fMRI-PTE, innovative data transformation, efficient training, a novel learning strategy, and the universal applicability of our approach. Extensive experiments validate and support our claims, offering a promising foundation for further research in this domain.Simplified Chinese:探索大脑活动和从fMRI数据中解译的探索，有 been a longstanding pursuit, driven by its potential applications in brain-computer interfaces, medical diagnosis, and virtual reality. Previous approaches have primarily focused on individual subject analysis, highlighting the need for a more universal and adaptable framework, which is the core motivation behind our work. In this work, we propose fMRI-PTE, an innovative auto-encoder approach for fMRI pre-training, with a focus on addressing the challenges of varying fMRI data dimensions due to individual brain differences. Our approach involves transforming fMRI signals into unified 2D representations, ensuring consistency in dimensions and preserving distinct brain activity patterns. We introduce a novel learning strategy tailored for pre-training 2D fMRI images, enhancing the quality of reconstruction. fMRI-PTE's adaptability with image generators enables the generation of well-represented fMRI features, facilitating various downstream tasks, including within-subject and cross-subject brain activity decoding. Our contributions encompass introducing fMRI-PTE, innovative data transformation, efficient training, a novel learning strategy, and the universal applicability of our approach. Extensive experiments validate and support our claims, offering a promising foundation for further research in this domain.
</details></li>
</ul>
<hr>
<h2 id="Space-Narrative-Generating-Images-and-3D-Scenes-of-Chinese-Garden-from-Text-using-Deep-Learning"><a href="#Space-Narrative-Generating-Images-and-3D-Scenes-of-Chinese-Garden-from-Text-using-Deep-Learning" class="headerlink" title="Space Narrative: Generating Images and 3D Scenes of Chinese Garden from Text using Deep Learning"></a>Space Narrative: Generating Images and 3D Scenes of Chinese Garden from Text using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00339">http://arxiv.org/abs/2311.00339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Shi1, Hao Hua1</li>
<li>for: 这个论文的目的是提出一种基于深度学习方法的园林画生成方法，以便在研究和修复传统中国园林时，缺乏直接资料的问题。</li>
<li>methods: 该论文使用了一种文本描述与园林画的对应关系，使用深度学习方法进行学习。文本描述和园林画之间的映射是通过一个文本-图像扩散模型来实现的。</li>
<li>results: 该论文的实验结果表明，使用这种方法可以生成基于文本描述的园林画，并且可以在Unity 3D 环境中进行三维显示。生成的园林画具有传统明朝园林的风格和特点。<details>
<summary>Abstract</summary>
The consistent mapping from poems to paintings is essential for the research and restoration of traditional Chinese gardens. But the lack of firsthand ma-terial is a great challenge to the reconstruction work. In this paper, we pro-pose a method to generate garden paintings based on text descriptions using deep learning method. Our image-text pair dataset consists of more than one thousand Ming Dynasty Garden paintings and their inscriptions and post-scripts. A latent text-to-image diffusion model learns the mapping from de-scriptive texts to garden paintings of the Ming Dynasty, and then the text description of Jichang Garden guides the model to generate new garden paintings. The cosine similarity between the guide text and the generated image is the evaluation criterion for the generated images. Our dataset is used to fine-tune the pre-trained diffusion model using Low-Rank Adapta-tion of Large Language Models (LoRA). We also transformed the generated images into a panorama and created a free-roam scene in Unity 3D. Our post-trained model is capable of generating garden images in the style of Ming Dynasty landscape paintings based on textual descriptions. The gener-ated images are compatible with three-dimensional presentation in Unity 3D.
</details>
<details>
<summary>摘要</summary>
traditional Chinese gardens的研究和修复中，映射 FROM POEMS TO PAINTINGS的稳定性至关重要。但lack of firsthand materials是修复工作的大allenge。在这篇论文中，我们提议一种基于深度学习方法的方法，可以将文本描述转换成园林画作。我们的图像文本对象集包括明朝园林画作和其附注和后cript。一个潜在的文本到图像扩散模型学习了明朝园林画作的描述文本和图像之间的映射。然后，用Jichang园的文本作为指南，使模型生成新的园林画作。cosine similarity between the guide text and the generated image是评价 criterion for the generated images。我们使用LoRA来精度地适应大语言模型，并将生成的图像转换成扩展场景。我们的post-trained模型可以基于文本描述生成明朝园林画作风格的园林图像，并且可以与Unity 3D中的三维场景兼容。
</details></li>
</ul>
<hr>
<h2 id="SDF4CHD-Generative-Modeling-of-Cardiac-Anatomies-with-Congenital-Heart-Defects"><a href="#SDF4CHD-Generative-Modeling-of-Cardiac-Anatomies-with-Congenital-Heart-Defects" class="headerlink" title="SDF4CHD: Generative Modeling of Cardiac Anatomies with Congenital Heart Defects"></a>SDF4CHD: Generative Modeling of Cardiac Anatomies with Congenital Heart Defects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00332">http://arxiv.org/abs/2311.00332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanwei Kong, Sascha Stocker, Perry S. Choi, Michael Ma, Daniel B. Ennis, Alison Marsden</li>
<li>for: 这个研究的目的是创建一种可以自动构建心脏疾病患者的特有心脏形态模型，以提高诊断和治疗规划。</li>
<li>methods: 这个研究使用了深度学习（DL）方法，包括生成模型和 signed distance fields（SDF），来自动构建心脏疾病患者的特有心脏形态。</li>
<li>results: 这个研究的结果显示，这种生成模型可以创建不同的心脏疾病型别的虚拟实验室，并且可以将这些虚拟实验室与实际患者的心脏形态进行比较，以提高诊断和治疗规划的准确性。<details>
<summary>Abstract</summary>
Congenital heart disease (CHD) encompasses a spectrum of cardiovascular structural abnormalities, often requiring customized treatment plans for individual patients. Computational modeling and analysis of these unique cardiac anatomies can improve diagnosis and treatment planning and may ultimately lead to improved outcomes. Deep learning (DL) methods have demonstrated the potential to enable efficient treatment planning by automating cardiac segmentation and mesh construction for patients with normal cardiac anatomies. However, CHDs are often rare, making it challenging to acquire sufficiently large patient cohorts for training such DL models. Generative modeling of cardiac anatomies has the potential to fill this gap via the generation of virtual cohorts; however, prior approaches were largely designed for normal anatomies and cannot readily capture the significant topological variations seen in CHD patients. Therefore, we propose a type- and shape-disentangled generative approach suitable to capture the wide spectrum of cardiac anatomies observed in different CHD types and synthesize differently shaped cardiac anatomies that preserve the unique topology for specific CHD types. Our DL approach represents generic whole heart anatomies with CHD type-specific abnormalities implicitly using signed distance fields (SDF) based on CHD type diagnosis, which conveniently captures divergent anatomical variations across different types and represents meaningful intermediate CHD states. To capture the shape-specific variations, we then learn invertible deformations to morph the learned CHD type-specific anatomies and reconstruct patient-specific shapes. Our approach has the potential to augment the image-segmentation pairs for rarer CHD types for cardiac segmentation and generate cohorts of CHD cardiac meshes for computational simulation.
</details>
<details>
<summary>摘要</summary>
心脏胞茂病（CHD）是指心脏结构变异的谱系，需要根据具体的病人实施个性化的治疗方案。计算机模拟和分析这些特殊的心脏结构可以提高诊断和治疗规划，并可能导致更好的结果。深度学习（DL）方法已经表现出可以通过自动化心脏分 segmentation和心脏穹顶的建立来提高治疗规划的效率。但是，CHD 是罕见的，因此难以收集足够多的患者群来训练这些 DL 模型。生成模型可以填补这个差距，通过生成虚拟患者群来提高模型的鲁棒性和泛化能力。但是，先前的方法主要是为正常心脏结构设计的，无法轻松地捕捉 CHD 患者中的重要 topological variations。因此，我们提出了一种类型和形态分离的生成方法，可以Capture the wide spectrum of cardiac anatomies observed in different CHD types and synthesize differently shaped cardiac anatomies that preserve the unique topology for specific CHD types。我们的 DL 方法使用 signed distance fields (SDF) 来表示具体的心脏类型和形态，并使用这些 SDF 来生成具体的患者心脏模型。然后，我们学习了可逆的扭曲来修改学习到的 CHD 类型特有的心脏形态，以生成 patient-specific 心脏模型。我们的方法有可能增加更多的图像分割对 для更少的 CHD 类型，并生成这些类型中的 cardiac 肋骨模型，以便计算机 simulate 和诊断。
</details></li>
</ul>
<hr>
<h2 id="Flooding-Regularization-for-Stable-Training-of-Generative-Adversarial-Networks"><a href="#Flooding-Regularization-for-Stable-Training-of-Generative-Adversarial-Networks" class="headerlink" title="Flooding Regularization for Stable Training of Generative Adversarial Networks"></a>Flooding Regularization for Stable Training of Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00318">http://arxiv.org/abs/2311.00318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iu Yahiro, Takashi Ishida, Naoto Yokoya</li>
<li>for: 这个论文主要针对 Gan 训练中的稳定性问题。</li>
<li>methods: 该论文提出了一种直接对 adversarial 损失函数进行规范的方法，即通过浸泡法来防止分类器损失函数变得过低。</li>
<li>results: 实验表明，浸泡法可以稳定 Gan 训练，并且可以与其他稳定化技术相结合。此外，研究还发现，限制分类器损失函数不超过浸泡水平， THEN 训练可以稳定进行。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also reveal that by restricting the discriminator's loss to be no greater than flood level, the training proceeds stably even when the flood level is somewhat high.
</details>
<details>
<summary>摘要</summary>
TRANSLATION NOTE:* "flooding" 被翻译成 "淹没"* "overfitting suppression" 被翻译成 "避免过拟合"* "adversarial loss function" 被翻译成 "对抗损失函数"* "binary cross entropy loss" 被翻译成 "二进制极 entropy损失"
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Frame-Selection-for-Text-to-Video-Retrieval"><a href="#An-Empirical-Study-of-Frame-Selection-for-Text-to-Video-Retrieval" class="headerlink" title="An Empirical Study of Frame Selection for Text-to-Video Retrieval"></a>An Empirical Study of Frame Selection for Text-to-Video Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00298">http://arxiv.org/abs/2311.00298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengxia Wu, Min Cao, Yang Bai, Ziyin Zeng, Chen Chen, Liqiang Nie, Min Zhang</li>
<li>for: 提高文本视频对应的效率和准确率</li>
<li>methods: 研究了多种框架选择方法，包括文本无关和文本引导的方法，并对这些方法进行了详细的分析和比较</li>
<li>results: 经过全面的分析和比较，得出了适合文本视频对应的框架选择方法，可以提高对应的效率和准确率<details>
<summary>Abstract</summary>
Text-to-video retrieval (TVR) aims to find the most relevant video in a large video gallery given a query text. The intricate and abundant context of the video challenges the performance and efficiency of TVR. To handle the serialized video contexts, existing methods typically select a subset of frames within a video to represent the video content for TVR. How to select the most representative frames is a crucial issue, whereby the selected frames are required to not only retain the semantic information of the video but also promote retrieval efficiency by excluding temporally redundant frames. In this paper, we make the first empirical study of frame selection for TVR. We systemically classify existing frame selection methods into text-free and text-guided ones, under which we detailedly analyze six different frame selections in terms of effectiveness and efficiency. Among them, two frame selections are first developed in this paper. According to the comprehensive analysis on multiple TVR benchmarks, we empirically conclude that the TVR with proper frame selections can significantly improve the retrieval efficiency without sacrificing the retrieval performance.
</details>
<details>
<summary>摘要</summary>
In this paper, we make the first empirical study of frame selection for TVR. We categorize existing frame selection methods into text-free and text-guided ones, and analyze six different frame selections in terms of effectiveness and efficiency. Two of these frame selections are newly developed in this paper. Our comprehensive analysis on multiple TVR benchmarks shows that proper frame selections can significantly improve retrieval efficiency without sacrificing retrieval performance.
</details></li>
</ul>
<hr>
<h2 id="Graph-Representation-Learning-for-Infrared-and-Visible-Image-Fusion"><a href="#Graph-Representation-Learning-for-Infrared-and-Visible-Image-Fusion" class="headerlink" title="Graph Representation Learning for Infrared and Visible Image Fusion"></a>Graph Representation Learning for Infrared and Visible Image Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00291">http://arxiv.org/abs/2311.00291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Li, Lu Bai, Bin Yang, Chang Li, Lingfei Ma, Edwin R. Hancock</li>
<li>for: 这种论文的目的是怎样？</li>
<li>methods: 这种论文使用了哪些方法？</li>
<li>results: 这种论文得到了什么结果？Here are the answers in Simplified Chinese:</li>
<li>for: 这种论文的目的是提出一种基于图表 Representation 的图像抽取方法，以提高图像抽取的精度和效率。</li>
<li>methods: 这种论文使用了图 convolutional neural networks (GCNs) 来捕捉图像的非本地自similarity (NLss)，并通过图structured  Representation 来提高图像抽取的精度和效率。</li>
<li>results: 实验结果表明，提出的方法可以准确地捕捉图像的NLss，并且在三个数据集上进行了比较，并证明了该方法的有效性和超越性。<details>
<summary>Abstract</summary>
Infrared and visible image fusion aims to extract complementary features to synthesize a single fused image. Many methods employ convolutional neural networks (CNNs) to extract local features due to its translation invariance and locality. However, CNNs fail to consider the image's non-local self-similarity (NLss), though it can expand the receptive field by pooling operations, it still inevitably leads to information loss. In addition, the transformer structure extracts long-range dependence by considering the correlativity among all image patches, leading to information redundancy of such transformer-based methods. However, graph representation is more flexible than grid (CNN) or sequence (transformer structure) representation to address irregular objects, and graph can also construct the relationships among the spatially repeatable details or texture with far-space distance. Therefore, to address the above issues, it is significant to convert images into the graph space and thus adopt graph convolutional networks (GCNs) to extract NLss. This is because the graph can provide a fine structure to aggregate features and propagate information across the nearest vertices without introducing redundant information. Concretely, we implement a cascaded NLss extraction pattern to extract NLss of intra- and inter-modal by exploring interactions of different image pixels in intra- and inter-image positional distance. We commence by preforming GCNs on each intra-modal to aggregate features and propagate information to extract independent intra-modal NLss. Then, GCNs are performed on the concatenate intra-modal NLss features of infrared and visible images, which can explore the cross-domain NLss of inter-modal to reconstruct the fused image. Ablation studies and extensive experiments illustrates the effectiveness and superiority of the proposed method on three datasets.
</details>
<details>
<summary>摘要</summary>
infrared和可见图像融合的目标是提取各自的特征来合成单一融合图像。许多方法使用卷积神经网络（CNN）提取本地特征，因为它们具有翻译不变性和本地性。然而，CNN忽略图像的非本地自相似性（NLss），尽管它可以通过聚合操作扩大感知范围，但仍然不可避免信息损失。此外，基于转换结构的方法可以捕捉图像的长距离相关性，导致信息繁殖。然而，图像表示更为灵活于格子（CNN）或序列（转换结构）表示，可以处理不规则对象，并且图像可以构建相对论的距离的关系。因此，为了解决以上问题，需要将图像转换为格子空间，并采用图像卷积神经网络（GCNs）提取NLss。这是因为格子可以提供细致的结构，以便聚合特征和在最近邻居之间传递信息，而不是引入 redundancy。具体来说，我们实施了级联NLss提取模式，通过探索不同图像像素之间的交互，提取NLss的Intra-和Inter-模态。我们开始通过GCNs处理每个Intra-模态，以聚合特征并传递信息，提取独立的Intra-模态NLss。然后，我们在 concatenate INTRA-模态NLss特征上进行GCNs处理，可以探索交叉领域NLss的Inter-模态，以重construct融合图像。我们的方法的有效性和优越性通过简洁的方法和广泛的实验证明。
</details></li>
</ul>
<hr>
<h2 id="Mixture-of-Experts-for-Open-Set-Domain-Adaptation-A-Dual-Space-Detection-Approach"><a href="#Mixture-of-Experts-for-Open-Set-Domain-Adaptation-A-Dual-Space-Detection-Approach" class="headerlink" title="Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach"></a>Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00285">http://arxiv.org/abs/2311.00285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenbang Du, Jiayu An, Jiahao Hong, Dongrui Wu</li>
<li>for: 这篇论文的目的是解决开放集领域适束（OSDA）中的分布和标签类shift问题，同时确保精准地类别已知类型的标签，并识别目标领域中的未知类型标签。</li>
<li>methods: 这篇论文提出了一种名为“双空间检测”的方法，利用图像特征空间和路由特征空间之间的不一致来检测未知类型标签，不需要手动调整阈值。它还使用了Graph Router来更好地利用图像组件之间的空间信息。</li>
<li>results: 三个不同的数据集上的实验显示了这篇论文的方法的有效性和超越性，并且不需要手动调整阈值。代码将会很 soon。<details>
<summary>Abstract</summary>
Open Set Domain Adaptation (OSDA) aims to cope with the distribution and label shifts between the source and target domains simultaneously, performing accurate classification for known classes while identifying unknown class samples in the target domain. Most existing OSDA approaches, depending on the final image feature space of deep models, require manually-tuned thresholds, and may easily misclassify unknown samples as known classes. Mixture-of-Expert (MoE) could be a remedy. Within an MoE, different experts address different input features, producing unique expert routing patterns for different classes in a routing feature space. As a result, unknown class samples may also display different expert routing patterns to known classes. This paper proposes Dual-Space Detection, which exploits the inconsistencies between the image feature space and the routing feature space to detect unknown class samples without any threshold. Graph Router is further introduced to better make use of the spatial information among image patches. Experiments on three different datasets validated the effectiveness and superiority of our approach. The code will come soon.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TLMCM-Network-for-Medical-Image-Hierarchical-Multi-Label-Classification"><a href="#TLMCM-Network-for-Medical-Image-Hierarchical-Multi-Label-Classification" class="headerlink" title="TLMCM Network for Medical Image Hierarchical Multi-Label Classification"></a>TLMCM Network for Medical Image Hierarchical Multi-Label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00282">http://arxiv.org/abs/2311.00282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Wu, Siyan Luo, Qiyu Wu, Wenbin Ouyang</li>
<li>for: 这篇研究的目的是解决现代医疗中的医疗影像层次多标签分类任务（MI-HMC）中的两个主要挑战：数据不对称和层次约束。</li>
<li>methods: 这篇研究提出了将传播学习与最大约束模组（TLMCM）网络应用到MI-HMC任务中，以解决现有方法所具有的复杂模型架构设计和专业知识要求。</li>
<li>results: 实验结果显示，TLMCM网络在MI-HMC任务中可以达到高多标签预测精度（80%-90%），使其成为医疗领域应用中的有价贡献。<details>
<summary>Abstract</summary>
Medical Image Hierarchical Multi-Label Classification (MI-HMC) is of paramount importance in modern healthcare, presenting two significant challenges: data imbalance and \textit{hierarchy constraint}. Existing solutions involve complex model architecture design or domain-specific preprocessing, demanding considerable expertise or effort in implementation. To address these limitations, this paper proposes Transfer Learning with Maximum Constraint Module (TLMCM) network for the MI-HMC task. The TLMCM network offers a novel approach to overcome the aforementioned challenges, outperforming existing methods based on the Area Under the Average Precision and Recall Curve($AU\overline{(PRC)}$) metric. In addition, this research proposes two novel accuracy metrics, $EMR$ and $HammingAccuracy$, which have not been extensively explored in the context of the MI-HMC task. Experimental results demonstrate that the TLMCM network achieves high multi-label prediction accuracy($80\%$-$90\%$) for MI-HMC tasks, making it a valuable contribution to healthcare domain applications.
</details>
<details>
<summary>摘要</summary>
医疗图像层次多标签分类（MI-HMC）在现代医疗中具有重要性，存在两大挑战：数据不均衡和层次约束。现有的解决方案包括复杂的模型建构设计或域pecific的预处理，需要较大的专业知识或努力进行实现。为了解决这些限制，这篇论文提出了传输学习Maximum Constraint Module（TLMCM）网络来解决MI-HMC任务。TLMCM网络提供了一种新的方法来超越现有的方法，在AU（总平均准确率和准确率曲线）指标上表现出色。此外，本研究还提出了两个新的准确度指标：$EMR$和$HammingAccuracy$,这些指标在MI-HMC任务中尚未得到广泛的探讨。实验结果表明，TLMCM网络在MI-HMC任务中可以达到80%-90%的多标签预测精度，这使得它在医疗领域应用中成为一项有价值的贡献。
</details></li>
</ul>
<hr>
<h2 id="OpenForest-A-data-catalogue-for-machine-learning-in-forest-monitoring"><a href="#OpenForest-A-data-catalogue-for-machine-learning-in-forest-monitoring" class="headerlink" title="OpenForest: A data catalogue for machine learning in forest monitoring"></a>OpenForest: A data catalogue for machine learning in forest monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00277">http://arxiv.org/abs/2311.00277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rolnicklab/openforest">https://github.com/rolnicklab/openforest</a></li>
<li>paper_authors: Arthur Ouaknine, Teja Kattenborn, Etienne Laliberté, David Rolnick</li>
<li>for: The paper aims to provide a comprehensive overview of open access forest datasets across spatial scales, encompassing inventories, ground-based, aerial-based, satellite-based recordings, and country or world maps.</li>
<li>methods: The paper uses a dynamic catalogue called OpenForest to reference all available open access forest datasets, and it also explores the application of machine learning methods for large-scale forest monitoring.</li>
<li>results: The paper provides an extensive overview of 86 open access forest datasets and inspires research in machine learning applied to forest biology by establishing connections between contemporary topics, perspectives, and challenges inherent in both domains.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究的目的是提供一个全面的开放访问森林数据集过iew，覆盖不同的空间尺度，包括 forest inventories，地面、空中、卫星记录等，以及国家或世界地图。</li>
<li>methods: 本研究使用的是一个名为 OpenForest的动态目录，用于参考所有可用的开放访问森林数据集。此外，它还探讨了大规模森林监测中机器学习方法的应用。</li>
<li>results: 本研究提供了86个开放访问森林数据集的广泛概述，并鼓励了机器学习在森林生物学中的研究，并在两个领域之间建立了连接。<details>
<summary>Abstract</summary>
Forests play a crucial role in Earth's system processes and provide a suite of social and economic ecosystem services, but are significantly impacted by human activities, leading to a pronounced disruption of the equilibrium within ecosystems. Advancing forest monitoring worldwide offers advantages in mitigating human impacts and enhancing our comprehension of forest composition, alongside the effects of climate change. While statistical modeling has traditionally found applications in forest biology, recent strides in machine learning and computer vision have reached important milestones using remote sensing data, such as tree species identification, tree crown segmentation and forest biomass assessments. For this, the significance of open access data remains essential in enhancing such data-driven algorithms and methodologies. Here, we provide a comprehensive and extensive overview of 86 open access forest datasets across spatial scales, encompassing inventories, ground-based, aerial-based, satellite-based recordings, and country or world maps. These datasets are grouped in OpenForest, a dynamic catalogue open to contributions that strives to reference all available open access forest datasets. Moreover, in the context of these datasets, we aim to inspire research in machine learning applied to forest biology by establishing connections between contemporary topics, perspectives and challenges inherent in both domains. We hope to encourage collaborations among scientists, fostering the sharing and exploration of diverse datasets through the application of machine learning methods for large-scale forest monitoring. OpenForest is available at this url: https://github.com/RolnickLab/OpenForest
</details>
<details>
<summary>摘要</summary>
森林 играют重要的角色在地球系统中，提供一系列社会和经济生态系统服务，但是受到人类活动的干扰，导致生态系统内的平衡受到明显的干扰。推进全球森林监测可以提供利益，减轻人类的影响，并提高我们对森林结构的理解，以及气候变化的影响。在Machine learning和计算机视觉等领域做出了重要进步，使用遥感数据进行树种识别、树冠分割和森林质量评估等。在这个过程中，开放数据的重要性仍然存在，以增强这些数据驱动的算法和方法。我们提供了86个开放 forest 数据集，覆盖不同的空间尺度，包括 forest инвенタризация、地面、空中、卫星记录等，并分类在 OpenForest 中，这是一个动态目录，欢迎投稿。此外，在这些数据集的背景下，我们想要鼓励研究者通过机器学习应用于森林生物，探讨两个领域之间的联系和挑战。我们希望通过共同分享和探索多样化的数据集，通过机器学习方法实现大规模森林监测。OpenForest 的Url为：https://github.com/RolnickLab/OpenForest
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Latent-Diffusion-Model-for-3D-Medical-Image-to-Image-Translation-Multi-modal-Magnetic-Resonance-Imaging-Study"><a href="#Adaptive-Latent-Diffusion-Model-for-3D-Medical-Image-to-Image-Translation-Multi-modal-Magnetic-Resonance-Imaging-Study" class="headerlink" title="Adaptive Latent Diffusion Model for 3D Medical Image to Image Translation: Multi-modal Magnetic Resonance Imaging Study"></a>Adaptive Latent Diffusion Model for 3D Medical Image to Image Translation: Multi-modal Magnetic Resonance Imaging Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00265">http://arxiv.org/abs/2311.00265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jongdory/aldm">https://github.com/jongdory/aldm</a></li>
<li>paper_authors: Jonghun Kim, Hyunjin Park</li>
<li>for: 这个研究旨在提供一个基于潜在扩散模型（LDM）的图像转换模型，并使用可调整块（MS-SPADE）来实现图像转换。</li>
<li>methods: 这个模型使用了3D LDM和conditioning技术，并使用了MS-SPADE块来实现图像转换。</li>
<li>results: 这个模型在多个源模式转换到多个目标模式时 exhibited 成功的图像合成，并在量值评估中超过了其他模型。<details>
<summary>Abstract</summary>
Multi-modal images play a crucial role in comprehensive evaluations in medical image analysis providing complementary information for identifying clinically important biomarkers. However, in clinical practice, acquiring multiple modalities can be challenging due to reasons such as scan cost, limited scan time, and safety considerations. In this paper, we propose a model based on the latent diffusion model (LDM) that leverages switchable blocks for image-to-image translation in 3D medical images without patch cropping. The 3D LDM combined with conditioning using the target modality allows generating high-quality target modality in 3D overcoming the shortcoming of the missing out-of-slice information in 2D generation methods. The switchable block, noted as multiple switchable spatially adaptive normalization (MS-SPADE), dynamically transforms source latents to the desired style of the target latents to help with the diffusion process. The MS-SPADE block allows us to have one single model to tackle many translation tasks of one source modality to various targets removing the need for many translation models for different scenarios. Our model exhibited successful image synthesis across different source-target modality scenarios and surpassed other models in quantitative evaluations tested on multi-modal brain magnetic resonance imaging datasets of four different modalities and an independent IXI dataset. Our model demonstrated successful image synthesis across various modalities even allowing for one-to-many modality translations. Furthermore, it outperformed other one-to-one translation models in quantitative evaluations.
</details>
<details>
<summary>摘要</summary>
多modal图像在医学影像分析中发挥关键作用，提供补充信息，用于标识临床重要的生物标志物。然而，在临床实践中，获取多modal的挑战性很大，主要包括成本高、时间短、安全因素等。在这篇论文中，我们提出了基于潜在扩散模型（LDM）的模型，利用可 switchable 块来实现图像到图像翻译。3D LDM 结合了目标模式的conditioning，可以生成高质量的目标模式3D图像，超越2D生成方法中缺失的剪辑信息。MS-SPADE块可以将源境文件转换成目标境文件的样式，帮助扩散过程。我们的模型可以处理多种不同的源模式到目标模式的翻译任务，不需要多个翻译模型。我们的模型在多modal脑磁共振成像数据集上表现出色，超过了其他模型。我们的模型可以在不同的模式之间进行一对多翻译，并且在量化评价中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Solutions-to-Elliptic-and-Parabolic-Problems-via-Finite-Difference-Based-Unsupervised-Small-Linear-Convolutional-Neural-Networks"><a href="#Solutions-to-Elliptic-and-Parabolic-Problems-via-Finite-Difference-Based-Unsupervised-Small-Linear-Convolutional-Neural-Networks" class="headerlink" title="Solutions to Elliptic and Parabolic Problems via Finite Difference Based Unsupervised Small Linear Convolutional Neural Networks"></a>Solutions to Elliptic and Parabolic Problems via Finite Difference Based Unsupervised Small Linear Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00259">http://arxiv.org/abs/2311.00259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Celaya, Keegan Kirk, David Fuentes, Beatrice Riviere</li>
<li>for: 解决部分演算式（PDE）问题</li>
<li>methods: 使用小型卷积神经网络</li>
<li>results: 与真实解相比，有较高的准确率<details>
<summary>Abstract</summary>
In recent years, there has been a growing interest in leveraging deep learning and neural networks to address scientific problems, particularly in solving partial differential equations (PDEs). However, current neural network-based PDE solvers often rely on extensive training data or labeled input-output pairs, making them prone to challenges in generalizing to out-of-distribution examples. To mitigate the generalization gap encountered by conventional neural network-based methods in estimating PDE solutions, we formulate a fully unsupervised approach, requiring no training data, to estimate finite difference solutions for PDEs directly via small convolutional neural networks. Our proposed algorithms demonstrate a comparable accuracy to the true solution for several selected elliptic and parabolic problems compared to the finite difference method.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RAUNE-Net-A-Residual-and-Attention-Driven-Underwater-Image-Enhancement-Method"><a href="#RAUNE-Net-A-Residual-and-Attention-Driven-Underwater-Image-Enhancement-Method" class="headerlink" title="RAUNE-Net: A Residual and Attention-Driven Underwater Image Enhancement Method"></a>RAUNE-Net: A Residual and Attention-Driven Underwater Image Enhancement Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00246">http://arxiv.org/abs/2311.00246</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fansuregrin/raune-net">https://github.com/fansuregrin/raune-net</a></li>
<li>paper_authors: Wangzhen Peng, Chenghao Zhou, Runze Hu, Jingchao Cao, Yutao Liu</li>
<li>for: 提高水下图像质量</li>
<li>methods: 使用异常学习和注意力机制，构建一个更可靠和合理的UIE网络</li>
<li>results: 对实际水下图像进行评估，比较其对水下图像增强效果和视觉效果，与其他8种UIE方法进行比较，并取得了优秀的 объекivo表现和可见效果。<details>
<summary>Abstract</summary>
Underwater image enhancement (UIE) poses challenges due to distinctive properties of the underwater environment, including low contrast, high turbidity, visual blurriness, and color distortion. In recent years, the application of deep learning has quietly revolutionized various areas of scientific research, including UIE. However, existing deep learning-based UIE methods generally suffer from issues of weak robustness and limited adaptability. In this paper, inspired by residual and attention mechanisms, we propose a more reliable and reasonable UIE network called RAUNE-Net by employing residual learning of high-level features at the network's bottle-neck and two aspects of attention manipulations in the down-sampling procedure. Furthermore, we collect and create two datasets specifically designed for evaluating UIE methods, which contains different types of underwater distortions and degradations. The experimental validation demonstrates that our method obtains promising objective performance and consistent visual results across various real-world underwater images compared to other eight UIE methods. Our example code and datasets are publicly available at https://github.com/fansuregrin/RAUNE-Net.
</details>
<details>
<summary>摘要</summary>
水下图像增强（UIE）在水下环境中存在一些独特的特性，包括低对比度、高浓度、视觉模糊和颜色扭曲。在最近几年，深度学习的应用在科学研究中已经革命化了许多领域，包括 UIE。然而，现有的深度学习基于的 UIE 方法通常受到软性和限制的问题。在这篇论文中，我们提出了一种更可靠和合理的 UIE 网络，称为 RAUNE-Net，通过在网络的瓶颈部分使用剩余学习高级特征，以及在下采样过程中使用两种注意力操作。此外，我们收集和创建了两个特定用于评估 UIE 方法的数据集，这些数据集包含不同类型的水下扭曲和降低。实验验证表明，我们的方法在真实世界水下图像中获得了优秀的目标性能和一致的视觉结果，相比其他八种 UIE 方法。我们的例子代码和数据集在 <https://github.com/fansuregrin/RAUNE-Net> 上公开可用。
</details></li>
</ul>
<hr>
<h2 id="1DFormer-Learning-1D-Landmark-Representations-via-Transformer-for-Facial-Landmark-Tracking"><a href="#1DFormer-Learning-1D-Landmark-Representations-via-Transformer-for-Facial-Landmark-Tracking" class="headerlink" title="1DFormer: Learning 1D Landmark Representations via Transformer for Facial Landmark Tracking"></a>1DFormer: Learning 1D Landmark Representations via Transformer for Facial Landmark Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00241">http://arxiv.org/abs/2311.00241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shi Yin, Shijie Huan, Defu Lian, Shangfei Wang, Jinshui Hu, Tao Guo, Bing Yin, Baocai Yin, Cong Liu</li>
<li>for: 本研究旨在提高人脸特征点跟踪的性能，通过基于1D地标表示的热图回归方法。</li>
<li>methods: 我们提出了一种基于Transformer架构的1DFormer模型，通过在时间和空间维度进行Token通信来学习有用的1D地标表示。为了模型长期顺序模式，我们提出了循环token混合机制、轴地标位嵌入机制以及自信度提高多头注意机制。为了 структура化模型，我们设计了内组和外组结构模型机制，以编码面部结构特征。</li>
<li>results: 我们在300VW和TF数据库上进行实验，结果表明1DFormer可以很好地模型面部特征点的长期顺序模式以及内在结构特征，从而学习有用的1D地标表示。并且在人脸特征点跟踪任务中，1DFormer达到了现有最佳性能。<details>
<summary>Abstract</summary>
Recently, heatmap regression methods based on 1D landmark representations have shown prominent performance on locating facial landmarks. However, previous methods ignored to make deep explorations on the good potentials of 1D landmark representations for sequential and structural modeling of multiple landmarks to track facial landmarks. To address this limitation, we propose a Transformer architecture, namely 1DFormer, which learns informative 1D landmark representations by capturing the dynamic and the geometric patterns of landmarks via token communications in both temporal and spatial dimensions for facial landmark tracking. For temporal modeling, we propose a recurrent token mixing mechanism, an axis-landmark-positional embedding mechanism, as well as a confidence-enhanced multi-head attention mechanism to adaptively and robustly embed long-term landmark dynamics into their 1D representations; for structure modeling, we design intra-group and inter-group structure modeling mechanisms to encode the component-level as well as global-level facial structure patterns as a refinement for the 1D representations of landmarks through token communications in the spatial dimension via 1D convolutional layers. Experimental results on the 300VW and the TF databases show that 1DFormer successfully models the long-range sequential patterns as well as the inherent facial structures to learn informative 1D representations of landmark sequences, and achieves state-of-the-art performance on facial landmark tracking.
</details>
<details>
<summary>摘要</summary>
最近，基于1D特征表示的热图回归方法已经显示出了明显的表达能力在脸部特征点的定位上。然而，前一些方法忽略了对1D特征表示的深入探索，以挖掘其在序列和结构模型化多个特征点的跟踪过程中的潜在优势。为此，我们提议一种名为1DFormer的Transformer架构，该架构通过在时间和空间维度进行токен交流来学习有用的1D特征表示。为 temporal模型ing，我们提出了循环token混合机制、轴点附加 embedding机制以及信任度加 weights multi-head注意机制，以适应性地并Robustly embedding long-term特征点动态到其1D表示中;为结构模型ing，我们设计了内部组和外部组结构模型机制，以编码Component-level以及全局水平的脸部结构模式，并通过1D卷积层进行空间维度的token交流，以便在1D表示中更好地编码多个特征点的结构模式。实验结果表明，1DFormer成功模型了脸部特征点的长距离序列模式以及内在的脸部结构模式，并在脸部特征点跟踪任务中达到了状态之 искусственный智能的表现。
</details></li>
</ul>
<hr>
<h2 id="DINO-Mix-Enhancing-Visual-Place-Recognition-with-Foundational-Vision-Model-and-Feature-Mixing"><a href="#DINO-Mix-Enhancing-Visual-Place-Recognition-with-Foundational-Vision-Model-and-Feature-Mixing" class="headerlink" title="DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing"></a>DINO-Mix: Enhancing Visual Place Recognition with Foundational Vision Model and Feature Mixing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00230">http://arxiv.org/abs/2311.00230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaoshuang Huang, Yang Zhou, Xiaofei Hu, Chenglong Zhang, Luying Zhao, Wenjian Gan, Mingbo Hou</li>
<li>for: 这研究旨在提高实际应用中的视觉位置识别（VPR）技术的精度，以便在复杂环境下实现高精度的位置检测。</li>
<li>methods: 该研究使用DINOv2模型作为基础网络，并对其进行修剪和精度调整，以提取robust的图像特征。提出了一种新的VPR建筑called DINO-Mix，它将基础视觉模型和特征聚合 together，以实现高精度的位置识别。</li>
<li>results: 实验表明，提出的DINO-Mix建筑在有光变、季节变化和遮挡的测试集（Tokyo24&#x2F;7、Nordland、SF-XL-Testv1）上达到了Top-1准确率为91.75%、80.18%和82%，分别比SOTA方法提高了5.14%的精度。<details>
<summary>Abstract</summary>
Utilizing visual place recognition (VPR) technology to ascertain the geographical location of publicly available images is a pressing issue for real-world VPR applications. Although most current VPR methods achieve favorable results under ideal conditions, their performance in complex environments, characterized by lighting variations, seasonal changes, and occlusions caused by moving objects, is generally unsatisfactory. In this study, we utilize the DINOv2 model as the backbone network for trimming and fine-tuning to extract robust image features. We propose a novel VPR architecture called DINO-Mix, which combines a foundational vision model with feature aggregation. This architecture relies on the powerful image feature extraction capabilities of foundational vision models. We employ an MLP-Mixer-based mix module to aggregate image features, resulting in globally robust and generalizable descriptors that enable high-precision VPR. We experimentally demonstrate that the proposed DINO-Mix architecture significantly outperforms current state-of-the-art (SOTA) methods. In test sets having lighting variations, seasonal changes, and occlusions (Tokyo24/7, Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1 accuracy rates of 91.75%, 80.18%, and 82%, respectively. Compared with SOTA methods, our architecture exhibited an average accuracy improvement of 5.14%.
</details>
<details>
<summary>摘要</summary>
utilizing visual place recognition (VPR) technology to determine the geographical location of publicly available images is a pressing issue for real-world VPR applications. although most current VPR methods achieve favorable results under ideal conditions, their performance in complex environments, characterized by lighting variations, seasonal changes, and occlusions caused by moving objects, is generally unsatisfactory. in this study, we utilize the DINOv2 model as the backbone network for trimming and fine-tuning to extract robust image features. we propose a novel VPR architecture called DINO-Mix, which combines a foundational vision model with feature aggregation. this architecture relies on the powerful image feature extraction capabilities of foundational vision models. we employ an MLP-Mixer-based mix module to aggregate image features, resulting in globally robust and generalizable descriptors that enable high-precision VPR. we experimentally demonstrate that the proposed DINO-Mix architecture significantly outperforms current state-of-the-art (SOTA) methods. in test sets having lighting variations, seasonal changes, and occlusions (Tokyo24/7, Nordland, SF-XL-Testv1), our proposed DINO-Mix architecture achieved Top-1 accuracy rates of 91.75%, 80.18%, and 82%, respectively. compared with SOTA methods, our architecture exhibited an average accuracy improvement of 5.14%.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/01/cs.CV_2023_11_01/" data-id="cloimip9x00k4s488gkvp90di" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/01/eess.AS_2023_11_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-11-01
        
      </div>
    </a>
  
  
    <a href="/2023/11/01/cs.AI_2023_11_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-01</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
