
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-11-01 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00694 repo_url: https:&#x2F;&#x2F;github.com&#x2F;lz1o">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-11-01">
<meta property="og:url" content="https://nullscc.github.io/2023/11/01/cs.AI_2023_11_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00694 repo_url: https:&#x2F;&#x2F;github.com&#x2F;lz1o">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-01T12:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T12:53:34.181Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_11_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/01/cs.AI_2023_11_01/" class="article-date">
  <time datetime="2023-11-01T12:00:00.000Z" itemprop="datePublished">2023-11-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-11-01
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unleashing-the-Creative-Mind-Language-Model-As-Hierarchical-Policy-For-Improved-Exploration-on-Challenging-Problem-Solving"><a href="#Unleashing-the-Creative-Mind-Language-Model-As-Hierarchical-Policy-For-Improved-Exploration-on-Challenging-Problem-Solving" class="headerlink" title="Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving"></a>Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00694">http://arxiv.org/abs/2311.00694</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lz1oceani/llm-as-hierarchical-policy">https://github.com/lz1oceani/llm-as-hierarchical-policy</a></li>
<li>paper_authors: Zhan Ling, Yunhao Fang, Xuanlin Li, Tongzhou Mu, Mingu Lee, Reza Pourreza, Roland Memisevic, Hao Su<br>for:* 这个论文旨在提高大语言模型（LLM）的推理能力，使其能够更好地解决复杂的推理问题。methods:* 作者提出了一种基于层次政策的方法，将 LLM 看作一个潜在的推理导航器，并通过在Context learning中学习出一个高级推理策略。* 该方法包括一个视野领袖，提出多个多样化的高级推理策略作为提示，以及一个追随者，通过每一个高级指令进行详细的推理过程，并生成多个推理链来解决问题。results:* 作者的方法可以生成有意义和鼓励人的提示，提高了推理策略的探索能力，并在MATH dataset上提高了复杂问题的答案准确率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additionally, we propose an effective and efficient tournament-based approach to select among these explored solution groups to reach the final answer. Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging problems in the MATH dataset. Code will be released at https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-Task-personalized-Multimodal-Few-shot-Learning-for-Visually-rich-Document-Entity-Retrieval"><a href="#On-Task-personalized-Multimodal-Few-shot-Learning-for-Visually-rich-Document-Entity-Retrieval" class="headerlink" title="On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval"></a>On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00693">http://arxiv.org/abs/2311.00693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Chen, Hanjun Dai, Bo Dai, Aidong Zhang, Wei Wei</li>
<li>for: 这个研究是为了解决在新的文档类型不断出现的情况下，检测器可以快速适应新的entity类型的问题。</li>
<li>methods: 这个研究使用了任务意识度 meta-学习框架，通过对具有不同任务的label空间进行个性化来解决这个问题。具体来说，他们采用了层次解码器（HC）和对比学习（ContrastProtoNet）来实现这一目标。</li>
<li>results: 实验结果表明，这种方法可以显著提高流行的meta-学习基线的稳定性。<details>
<summary>Abstract</summary>
Visually-rich document entity retrieval (VDER), which extracts key information (e.g. date, address) from document images like invoices and receipts, has become an important topic in industrial NLP applications. The emergence of new document types at a constant pace, each with its unique entity types, presents a unique challenge: many documents contain unseen entity types that occur only a couple of times. Addressing this challenge requires models to have the ability of learning entities in a few-shot manner. However, prior works for Few-shot VDER mainly address the problem at the document level with a predefined global entity space, which doesn't account for the entity-level few-shot scenario: target entity types are locally personalized by each task and entity occurrences vary significantly among documents. To address this unexplored scenario, this paper studies a novel entity-level few-shot VDER task. The challenges lie in the uniqueness of the label space for each task and the increased complexity of out-of-distribution (OOD) contents. To tackle this novel task, we present a task-aware meta-learning based framework, with a central focus on achieving effective task personalization that distinguishes between in-task and out-of-task distribution. Specifically, we adopt a hierarchical decoder (HC) and employ contrastive learning (ContrastProtoNet) to achieve this goal. Furthermore, we introduce a new dataset, FewVEX, to boost future research in the field of entity-level few-shot VDER. Experimental results demonstrate our approaches significantly improve the robustness of popular meta-learning baselines.
</details>
<details>
<summary>摘要</summary>
文本丰富的文档实体抽取（VDER）在工业自然语言处理（NLP）应用中成为重要项目。新的文档型态不断出现，每个文档都有唯一的实体类型，带来一个困难：许多文档中的实体类型从未见过。解决这个问题需要模型具有几次学习的能力。然而，对于几次学习的VDER主要是在文档层级上进行，使用预先定义的全球实体空间，不考虑实体水平的几次学习情况：目标实体类型是每个任务的本地个人化，实体出现在文档中具有很大的差异。为解决这个未曾探讨的情况，这篇文章研究了一个新的实体水平几次学习VDER任务。挑战在于每个任务的标签空间是唯一的，并且实体出现在文档中的多样性增加了OOD内容的复杂性。为了解决这个任务，我们提出了一个任务意识的元学习基础架构，并运用了对焦点学习（HC）和对焦点推导（ContrastProtoNet）来达成目的。此外，我们提出了一个新的数据集，FewVEX，以便未来这个领域的研究。实验结果显示我们的方法可以对具有广泛基础学习的实验基础进行优化。
</details></li>
</ul>
<hr>
<h2 id="Improving-Interpersonal-Communication-by-Simulating-Audiences-with-Language-Models"><a href="#Improving-Interpersonal-Communication-by-Simulating-Audiences-with-Language-Models" class="headerlink" title="Improving Interpersonal Communication by Simulating Audiences with Language Models"></a>Improving Interpersonal Communication by Simulating Audiences with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00687">http://arxiv.org/abs/2311.00687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/theryanl/egs">https://github.com/theryanl/egs</a></li>
<li>paper_authors: Ryan Liu, Howard Yen, Raja Marjieh, Thomas L. Griffiths, Ranjay Krishna<br>for:* 这 paper 旨在帮助我们更好地交流和决策，通过利用 Large Language Model (LLM)  simulations。methods:* 这 paper 提出了 Explore-Generate-Simulate (EGS) 框架，它可以帮助我们在交流和决策过程中更好地选择合适的语言和沟通方式。results:* 这 paper 在 eight 个场景中证明了 EGS 框架的效iveness，其中包括了 ten 种人际交流的基本过程。In simpler Chinese, the paper aims to help us communicate and make decisions more effectively by leveraging Large Language Model (LLM) simulations. It proposes the Explore-Generate-Simulate (EGS) framework, which can help us choose the most appropriate language and communication methods in various situations. The paper demonstrates the effectiveness of the EGS framework through evaluations and demonstrations, showing that it can enhance the outcomes of goal-oriented communication in a variety of situations.<details>
<summary>Abstract</summary>
How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal communication. For each scenario, we collect a dataset of human evaluations across candidates and baselines, and showcase that our framework's chosen candidate is preferred over popular generation mechanisms including Chain-of-Thought. We also find that audience simulations achieve reasonably high agreement with human raters across 5 of the 8 scenarios. Finally, we demonstrate the generality of our framework by applying it to real-world scenarios described by users on web forums. Through evaluations and demonstrations, we show that EGS enhances the effectiveness and outcomes of goal-oriented communication across a variety of situations, thus opening up new possibilities for the application of large language models in revolutionizing communication and decision-making processes.
</details>
<details>
<summary>摘要</summary>
如何与他人沟通以达到我们的目标呢？我们可以使用我们的先前经验或他人的建议，或者构建一个候选utterance，预测它将如何被接受。然而，我们的经验是有限和偏袋的，理解可能的结果是困难和耗费大量智力的。在这篇论文中，我们探索了如何通过大型自然语言模型（LLM）的模拟来提高我们的沟通。我们提出了探索-生成-模拟（EGS）框架，它接受任何沟通场景，并且可以帮助我们更好地沟通。EGS框架包括以下三个步骤：1. 探索解决方案空间，生成一组有关场景的多样化建议。2. 根据建议的子集生成各种沟通候选者。3. 使用不同的听众对各种候选者进行模拟，以确定最佳候选者和建议。我们在八个场景中评估了EGS框架，每个场景都有十个基本人际交流过程。我们收集了人类评价者对候选者和基eline的数据集，并证明了我们的选择为Chain-of-Thought所出的候选者所做出的决定胜过。此外，我们发现在5个场景中，听众模拟的结果与人类评价者达到了相当高的一致性。最后，我们通过应用EGS框架到实际场景中，证明了它在不同情况下提高了目标带来的效果和结果，从而开启了大型自然语言模型在沟通和决策过程中的新可能性。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Collective-Open-Ended-Exploration-from-Decentralized-Meta-Reinforcement-Learning"><a href="#Emergence-of-Collective-Open-Ended-Exploration-from-Decentralized-Meta-Reinforcement-Learning" class="headerlink" title="Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning"></a>Emergence of Collective Open-Ended Exploration from Decentralized Meta-Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00651">http://arxiv.org/abs/2311.00651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Bornemann, Gautier Hamon, Eleni Nisioti, Clément Moulin-Frier</li>
<li>for: 这 paper 的目的是研究在开放的任务分布下，多个自适应器通过分布式训练而学习集体探索策略。</li>
<li>methods: 这 paper 使用了自适应器在开放的任务分布下进行分布式训练，并引入了一个新的任务空间，其中包含多个来自不同任务类型的子任务，以生成一个庞大的任务树。</li>
<li>results: 研究发现，在这种分布式训练下，多个自适应器可以强大地总结并解决 novel 任务，而且这些任务可以是在训练期间未经遇到的。此外，这些自适应器还学习了集体探索策略，可以在开放的任务设定下解决更加复杂的任务。<details>
<summary>Abstract</summary>
Recent works have proven that intricate cooperative behaviors can emerge in agents trained using meta reinforcement learning on open ended task distributions using self-play. While the results are impressive, we argue that self-play and other centralized training techniques do not accurately reflect how general collective exploration strategies emerge in the natural world: through decentralized training and over an open-ended distribution of tasks. In this work we therefore investigate the emergence of collective exploration strategies, where several agents meta-learn independent recurrent policies on an open ended distribution of tasks. To this end we introduce a novel environment with an open ended procedurally generated task space which dynamically combines multiple subtasks sampled from five diverse task types to form a vast distribution of task trees. We show that decentralized agents trained in our environment exhibit strong generalization abilities when confronted with novel objects at test time. Additionally, despite never being forced to cooperate during training the agents learn collective exploration strategies which allow them to solve novel tasks never encountered during training. We further find that the agents learned collective exploration strategies extend to an open ended task setting, allowing them to solve task trees of twice the depth compared to the ones seen during training. Our open source code as well as videos of the agents can be found on our companion website.
</details>
<details>
<summary>摘要</summary>
近期研究证明，使用meta reinforcement学习训练的代理人可以展现出复杂的合作行为。然而，我们认为自我玩家和其他中央训练技术不能准确反映自然界中集体探索策略的发展：通过分布式训练和开放式任务分布来实现。因此，我们在这种情况下进行了一项研究，探索代理人在开放式任务分布下 meta-learn 独立的递归策略。为此，我们开发了一个新的环境，其中包含一个开放式、生成任务空间，这些任务空间由五种多样化的任务类型的多个子任务组成。我们显示了，在这种环境中训练的代理人在测试时对新物体具有强大的泛化能力。此外，虽然在训练时代理人从未被迫合作，但它们仍然学习了集体探索策略，可以解决在训练中未经遇到的新任务。我们还发现，代理人学习的集体探索策略可以在开放式任务设定下进行扩展，使得它们可以解决比训练中的任务更深的任务树。我们的开源代码以及视频 recording 可以在我们的伙伴网站上找到。
</details></li>
</ul>
<hr>
<h2 id="FAIRLABEL-Correcting-Bias-in-Labels"><a href="#FAIRLABEL-Correcting-Bias-in-Labels" class="headerlink" title="FAIRLABEL: Correcting Bias in Labels"></a>FAIRLABEL: Correcting Bias in Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00638">http://arxiv.org/abs/2311.00638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srinivasan H Sengamedu, Hien Pham</li>
<li>for: 该论文目的是检测和修正机器学习模型中的偏见。</li>
<li>methods: 该论文提出了一种名为FAIRLABEL的算法，用于检测和修正标签中的偏见。</li>
<li>results: 该论文通过synthetic数据和实际数据进行测试，显示FAIRLABEL可以减少不同群体之间的差异性影响（Disparate Impact），同时保持预测准确率高。在UCI成人、德国借款风险和Compas数据集上应用FAIRLABEL，则显示差异性影响率提高了54.2%。<details>
<summary>Abstract</summary>
There are several algorithms for measuring fairness of ML models. A fundamental assumption in these approaches is that the ground truth is fair or unbiased. In real-world datasets, however, the ground truth often contains data that is a result of historical and societal biases and discrimination. Models trained on these datasets will inherit and propagate the biases to the model outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across groups while maintaining high accuracy in predictions. We propose metrics to measure the quality of bias correction and validate FAIRLABEL on synthetic datasets and show that the label correction is correct 86.7% of the time vs. 71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such as UCI Adult, German Credit Risk, and Compas datasets and show that the Disparate Impact Ratio increases by as much as 54.2%.
</details>
<details>
<summary>摘要</summary>
有几种算法用于衡量机器学习模型的公平性。这些方法的基本假设是地面实际数据是公平的或不偏袋。然而，在实际世界中的数据中，经常会包含历史和社会遗产的偏袋和歧视。这些数据中的偏袋会被传递到模型输出中。我们提出了FAIRLABEL算法，它可以检测和修正标签中的偏袋。FAIRLABEL的目标是降低不同群体之间的差异性影响（Disparate Impact，DI），同时保持预测准确率高。我们提出了衡量偏袋修正质量的度量，并验证FAIRLABEL在模拟数据上的性能，结果显示FAIRLABEL可以正确地修正标签86.7%的时间 vs. 71.9%的基eline模型。此外，我们还应用FAIRLABEL在UC Irvine Adult、德国信用风险和Compas等数据集上，结果显示Disparate Impact Ratio可以增加到54.2%。
</details></li>
</ul>
<hr>
<h2 id="A-Bi-level-Framework-for-Traffic-Accident-Duration-Prediction-Leveraging-Weather-and-Road-Condition-Data-within-a-Practical-Optimum-Pipeline"><a href="#A-Bi-level-Framework-for-Traffic-Accident-Duration-Prediction-Leveraging-Weather-and-Road-Condition-Data-within-a-Practical-Optimum-Pipeline" class="headerlink" title="A Bi-level Framework for Traffic Accident Duration Prediction: Leveraging Weather and Road Condition Data within a Practical Optimum Pipeline"></a>A Bi-level Framework for Traffic Accident Duration Prediction: Leveraging Weather and Road Condition Data within a Practical Optimum Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00634">http://arxiv.org/abs/2311.00634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafat Tabassum Sukonna, Soham Irtiza Swapnil</li>
<li>for: 预测交通事故持续时间 (predicting the duration of traffic incidents)</li>
<li>methods: 使用多种机器学习模型 (using multiple machine learning models)</li>
<li>results: 83% 准确率 (83% accuracy rate) and 26.15&#x2F;13.3&#x2F;32.91 MAE&#x2F;RMSE values for short-term and long-term accident duration prediction (using a binary classification random forest model and LightGBM regression model)Here’s a more detailed explanation of each point:1. For: The paper aims to predict the duration of traffic incidents, which can help commuters choose optimal routes and traffic management personnel address non-recurring congestion issues.2. Methods: The authors use a dataset of traffic accidents to train and evaluate multiple machine learning models, including a binary classification random forest model and a LightGBM regression model. They also employ a bimodal approach to determine the precise duration of the incident’s effect.3. Results: The authors achieve an 83% accuracy rate in distinguishing between short-term and long-term effects of traffic accidents, and their LightGBM regression model outperforms other machine learning regression models in terms of Mean Average Error (MAE) and Root Mean Squared Error (RMSE) values. They also identify weather conditions, wind chill, and wind speed as the most influential factors in determining the duration of an accident using SHAP value analysis.<details>
<summary>Abstract</summary>
Due to the stochastic nature of events, predicting the duration of a traffic incident presents a formidable challenge. Accurate duration estimation can result in substantial advantages for commuters in selecting optimal routes and for traffic management personnel in addressing non-recurring congestion issues. In this study, we gathered accident duration, road conditions, and meteorological data from a database of traffic accidents to check the feasibility of a traffic accident duration pipeline without accident contextual information data like accident severity and textual description. Multiple machine learning models were employed to predict whether an accident's impact on road traffic would be of a short-term or long-term nature, and then utilizing a bimodal approach the precise duration of the incident's effect was determined. Our binary classification random forest model distinguished between short-term and long-term effects with an 83% accuracy rate, while the LightGBM regression model outperformed other machine learning regression models with Mean Average Error (MAE) values of 26.15 and 13.3 and RMSE values of 32.91 and 28.91 for short and long-term accident duration prediction, respectively. Using the optimal classification and regression model identified in the preceding section, we then construct an end-to-end pipeline to incorporate the entire process. The results of both separate and combined approaches were comparable with previous works, which shows the applicability of only using static features for predicting traffic accident duration. The SHAP value analysis identified weather conditions, wind chill and wind speed as the most influential factors in determining the duration of an accident.
</details>
<details>
<summary>摘要</summary>
Due to the stochastic nature of events, predicting the duration of a traffic incident presents a formidable challenge. Accurate duration estimation can result in substantial advantages for commuters in selecting optimal routes and for traffic management personnel in addressing non-recurring congestion issues. In this study, we gathered accident duration, road conditions, and meteorological data from a database of traffic accidents to check the feasibility of a traffic accident duration pipeline without accident contextual information data like accident severity and textual description. Multiple machine learning models were employed to predict whether an accident's impact on road traffic would be of a short-term or long-term nature, and then utilizing a bimodal approach the precise duration of the incident's effect was determined. Our binary classification random forest model distinguished between short-term and long-term effects with an 83% accuracy rate, while the LightGBM regression model outperformed other machine learning regression models with Mean Average Error (MAE) values of 26.15 and 13.3 and RMSE values of 32.91 and 28.91 for short and long-term accident duration prediction, respectively. Using the optimal classification and regression model identified in the preceding section, we then construct an end-to-end pipeline to incorporate the entire process. The results of both separate and combined approaches were comparable with previous works, which shows the applicability of only using static features for predicting traffic accident duration. The SHAP value analysis identified weather conditions, wind chill and wind speed as the most influential factors in determining the duration of an accident.Here's the translation in Traditional Chinese as well:due to the stochastic nature of events, predicting the duration of a traffic incident presents a formidable challenge. Accurate duration estimation can result in substantial advantages for commuters in selecting optimal routes and for traffic management personnel in addressing non-recurring congestion issues. In this study, we gathered accident duration, road conditions, and meteorological data from a database of traffic accidents to check the feasibility of a traffic accident duration pipeline without accident contextual information data like accident severity and textual description. Multiple machine learning models were employed to predict whether an accident's impact on road traffic would be of a short-term or long-term nature, and then utilizing a bimodal approach the precise duration of the incident's effect was determined. Our binary classification random forest model distinguished between short-term and long-term effects with an 83% accuracy rate, while the LightGBM regression model outperformed other machine learning regression models with Mean Average Error (MAE) values of 26.15 and 13.3 and RMSE values of 32.91 and 28.91 for short and long-term accident duration prediction, respectively. Using the optimal classification and regression model identified in the preceding section, we then construct an end-to-end pipeline to incorporate the entire process. The results of both separate and combined approaches were comparable with previous works, which shows the applicability of only using static features for predicting traffic accident duration. The SHAP value analysis identified weather conditions, wind chill and wind speed as the most influential factors in determining the duration of an accident.
</details></li>
</ul>
<hr>
<h2 id="Loss-Modeling-for-Multi-Annotator-Datasets"><a href="#Loss-Modeling-for-Multi-Annotator-Datasets" class="headerlink" title="Loss Modeling for Multi-Annotator Datasets"></a>Loss Modeling for Multi-Annotator Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00619">http://arxiv.org/abs/2311.00619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Uthman Jinadu, Jesse Annan, Shanshan Wen, Yi Ding</li>
<li>for: 提高数据集中注解准确性，尤其是对于subjective数据，避免疲劳和时间影响。</li>
<li>methods: 使用多任务学习和损失基于标签更正来更好地表达多个注解者的意见。</li>
<li>results: 使用该方法可以清晰地分解一致和不一致的注解，并且可以提高预测性能在单或多注解者设置下。此外，该方法对subjective数据中的额外标签噪声也具有耐性。<details>
<summary>Abstract</summary>
Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.
</details>
<details>
<summary>摘要</summary>
对于 dataset 的所有标注者的意见评价是重要的，但是在大量标注时，个别标注者将提供大量的评价，这可能会导致疲劳。此外，这些标注过程可能会在多天之间进行，这可能会导致标注者的意见随时间变化。为了解决这个问题，我们提出使用多任务学习，并与损失基于标签修正。我们表明，使用我们的新形式ulation，可以清楚地区分同意和不同意的标注。此外，我们还证明了这个修改可以提高预测性能在单一或多 annotator 环境中。最后，我们显示了这方法在额外附加到主观数据上的标签杂音下仍然是稳定的。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Variational-Inference-for-Probabilistic-Programs-with-Stochastic-Support"><a href="#Rethinking-Variational-Inference-for-Probabilistic-Programs-with-Stochastic-Support" class="headerlink" title="Rethinking Variational Inference for Probabilistic Programs with Stochastic Support"></a>Rethinking Variational Inference for Probabilistic Programs with Stochastic Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00594">http://arxiv.org/abs/2311.00594</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/treigerm/sdvi_neurips">https://github.com/treigerm/sdvi_neurips</a></li>
<li>paper_authors: Tim Reichelt, Luke Ong, Tom Rainforth</li>
<li>for: 这个论文目的是提出一种新的可能性变量推理（VI）方法，用于处理 probabilistic programs with stochastic support。</li>
<li>methods: 这个方法使用分解Program decomposition，将program分解成多个子程序，并自动生成每个子程序的独立子引导。</li>
<li>results: 对于这种分解方法，可以更好地构建适当的可能性家族，从而提高推理性能。<details>
<summary>Abstract</summary>
We introduce Support Decomposition Variational Inference (SDVI), a new variational inference (VI) approach for probabilistic programs with stochastic support. Existing approaches to this problem rely on designing a single global variational guide on a variable-by-variable basis, while maintaining the stochastic control flow of the original program. SDVI instead breaks the program down into sub-programs with static support, before automatically building separate sub-guides for each. This decomposition significantly aids in the construction of suitable variational families, enabling, in turn, substantial improvements in inference performance.
</details>
<details>
<summary>摘要</summary>
我们介绍支持分解量化推理（SDVI），一种新的量化推理（VI）方法，用于普罗比例程式中的随机支持。现有的方法将程式分成单一全球量化指南，并在变数按按基础上维护随机控制流程。而 SDVI 则将程式分解成不同的子程式，然后自动建立每个子程式的专属子导。这种分解有助于建立适当的量化家族，并使得推理性能得到了重大改善。
</details></li>
</ul>
<hr>
<h2 id="Coop-Memory-is-not-a-Commodity"><a href="#Coop-Memory-is-not-a-Commodity" class="headerlink" title="Coop: Memory is not a Commodity"></a>Coop: Memory is not a Commodity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00591">http://arxiv.org/abs/2311.00591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhao Zhang, Shihan Ma, Peihong Liu, Jinhui Yuan</li>
<li>for: 这篇论文旨在提高深度神经网络（DNNs）的训练下限内存预算，通过检查点和重新计算当中遗弃的tensor来实现。</li>
<li>methods: 这篇论文提出了一种名为Coop的方法，它通过缓冲窗内的tensor淘汰来确保所有淘汰的tensor都是连续的，并且立即使用。此外，它还提出了便宜的tensor分割和实在地在位置进行重新计算以进一步降低重新材料化的成本。</li>
<li>results: 实验结果显示，Coop可以实现Up to $2\times$的内存减少和巨大地降低compute overhead、搜寻延迟和内存分解。相比之下，与之相对的基eline案例。<details>
<summary>Abstract</summary>
Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical. Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads to severe memory fragmentation and increases the cost of potential rematerializations. To address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation. We named our method Coop as it is a co-optimization of tensor allocation and tensor rematerialization. We evaluated Coop on eight representative DNNs. The experimental results demonstrate that Coop achieves up to $2\times$ memory saving and hugely reduces compute overhead, search latency, and memory fragmentation compared to the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
tensor 重新材料化allow deep neural networks (DNNs) 在有限内存预算下进行训练，通过Checkpointing 模型和计算被赋予的缺失矩阵。然而，现有的tensor 重新材料化技术忽视了深度学习框架中的内存系统，并且对各个地址的自由内存块进行了顺序的假设，导致不连续的矩阵被赋予，其中一些矩阵并未被使用。这会导致内存割较严重和缺少可用内存块，从而增加可能的重新材料化成本。为解决这个问题，我们提议在滑动窗口中赋予矩阵，以确保所有的赋予都是连续的，并且立即被使用。此外，我们还提出了便宜的矩阵分配和可重复在位的方法，以进一步减少重新材料化成本。我们称之为Coop，因为它是矩阵分配和矩阵重新材料化的共同优化。我们对八个代表性的DNN进行了评估，实验结果表明，Coop可以达到$2\times$的内存减少和巨大减少计算开销、搜索延迟和内存割较严重相比于状态之前的基eline。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Summarization-with-Normalizing-Flows-and-Aggressive-Training"><a href="#Boosting-Summarization-with-Normalizing-Flows-and-Aggressive-Training" class="headerlink" title="Boosting Summarization with Normalizing Flows and Aggressive Training"></a>Boosting Summarization with Normalizing Flows and Aggressive Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00588">http://arxiv.org/abs/2311.00588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuyangstat/flowsum">https://github.com/yuyangstat/flowsum</a></li>
<li>paper_authors: Yu Yang, Xiaotong Shen</li>
<li>for: This paper is written for the task of summarization using Transformer-based models and normalizing flows.</li>
<li>methods: The paper proposes a new framework called FlowSUM, which uses normalizing flows to model the latent space of the summarization task and a controlled alternate aggressive training (CAAT) strategy to improve the quality of generated summaries.</li>
<li>results: The paper shows that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Additionally, the paper investigates the issue of posterior collapse in normalizing flows and analyzes how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used.<details>
<summary>Abstract</summary>
This paper presents FlowSUM, a normalizing flows-based variational encoder-decoder framework for Transformer-based summarization. Our approach tackles two primary challenges in variational summarization: insufficient semantic information in latent representations and posterior collapse during training. To address these challenges, we employ normalizing flows to enable flexible latent posterior modeling, and we propose a controlled alternate aggressive training (CAAT) strategy with an improved gate mechanism. Experimental results show that FlowSUM significantly enhances the quality of generated summaries and unleashes the potential for knowledge distillation with minimal impact on inference time. Furthermore, we investigate the issue of posterior collapse in normalizing flows and analyze how the summary quality is affected by the training strategy, gate initialization, and the type and number of normalizing flows used, offering valuable insights for future research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Minimally-Modifying-a-Markov-Game-to-Achieve-Any-Nash-Equilibrium-and-Value"><a href="#Minimally-Modifying-a-Markov-Game-to-Achieve-Any-Nash-Equilibrium-and-Value" class="headerlink" title="Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value"></a>Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00582">http://arxiv.org/abs/2311.00582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young Wu, Jeremy McMahan, Yiding Chen, Yudong Chen, Xiaojin Zhu, Qiaomin Xie</li>
<li>for: 本文研究了游戏修改问题，即一位好意的游戏设计者或一位恶意对手修改了Zero-sum Markov游戏的奖励函数，使得一个目标 deterministic或随机策略 Profile becomes the unique Markov perfect Nash equilibrium, and has a value within a target range, at a minimum cost.</li>
<li>methods: 本文使用了一种efficient algorithm，包括一个 konvex optimization problem with linear constraints, followed by random perturbation, to obtain a modification plan with a near-optimal cost.</li>
<li>results: 本文Characterizes the set of policy profiles that can be installed as the unique equilibrium of some game, and establishes sufficient and necessary conditions for successful installation.<details>
<summary>Abstract</summary>
We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.
</details>
<details>
<summary>摘要</summary>
我们研究游戏修改问题，其中一位仁慈的游戏设计者或一位邪恶的对手修改了游戏的奖励函数，使得目标决策函数Profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, while minimizing the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.Here's the translation breakdown:* 游戏 (yóuxì) - game* 修改 (xiūgòu) - modification* 问题 (wèn tí) - problem* 游戏设计者 (yóuxì fāngzì) - game designer* 对手 (duìshǒu) - adversary* 奖励函数 (jiàngdǎo fungsion) - reward function* 目标决策函数 (mùzhì jièdécision function) - target decision function* 决策函数 Profile (jièdécision function Profile) - decision function profile* Markov game (Mǎrkov yóuxì) - zero-sum Markov game*  Markov perfect Nash equilibrium (Mǎrkov pèrfect Nàsh èquilibrium) - Markov perfect Nash equilibrium* 奖励 (jiàngdǎo) - reward* 范围 (fāngwài) - range* 修改成本 (xiūgòu shēngběn) - modification cost* Characterize (zhìwù) - characterize* 可安装 (kě'ānshì) - can be installed* 必要条件 (bìyào tiáo'ān) - necessary conditions*  sufficient conditions (yù tiáo'ān) - sufficient conditions* 成功安装 (chéngtóu ānshì) - successful installation* 算法 (suànfā) - algorithm*  solves (jiějué) - solves* 凸优化问题 (kuòyòu yòu zuò) - convex optimization problem* 线性约束 (xiànxìng yùshù) - linear constraints* 随机扰动 (suìjiān shǎodòng) - random perturbation* 修改计划 (xiūgòu jìhuì) - modification plan* 近似优化 (jìn shì yòu) - near-optimal cost
</details></li>
</ul>
<hr>
<h2 id="LLaVA-Interactive-An-All-in-One-Demo-for-Image-Chat-Segmentation-Generation-and-Editing"><a href="#LLaVA-Interactive-An-All-in-One-Demo-for-Image-Chat-Segmentation-Generation-and-Editing" class="headerlink" title="LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing"></a>LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00571">http://arxiv.org/abs/2311.00571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, Chunyuan Li</li>
<li>for: 这个论文是为了探讨多 modal 人机交互系统 LLaVA-Interactive 的研究 прототип。</li>
<li>methods: 该系统可以通过多turn 对话来与人类用户进行交互，并使用多modal 输入和响应来实现。特别是，LLaVA-Interactive 不仅是语言提示，还可以使用视觉提示来协调人类意图。</li>
<li>results: 在开发 LLVA-Interactive 系统时，可以非常经济地利用三种已有 AI 模型的多modal 技能，无需进行额外模型训练。应用场景包括多种多样的应用场景，以示 LLVA-Interactive 的承诺和未来多Modal 交互系统的研究发展之 potential。<details>
<summary>Abstract</summary>
LLaVA-Interactive is a research prototype for multimodal human-AI interaction. The system can have multi-turn dialogues with human users by taking multimodal user inputs and generating multimodal responses. Importantly, LLaVA-Interactive goes beyond language prompt, where visual prompt is enabled to align human intents in the interaction. The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of LLaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN. A diverse set of application scenarios is presented to demonstrate the promises of LLaVA-Interactive and to inspire future research in multimodal interactive systems.
</details>
<details>
<summary>摘要</summary>
LLaVA-Interactive 是一个研究原型，用于多Modal human-AI交互。该系统可以与人类用户进行多轮对话，通过多Modal 输入和回应来实现。重要的是，LLaVA-Interactive 超越语言提示，启用视觉提示，以协调人类意图在交互中。该系统的开发非常Cost-efficient，因为它将三种Multimodal 技能的预建 AI 模型结合，无需额外模型训练：视觉对话的 LLaVA，图像分割的 SEEM，以及图像生成和修改的 GLIGEN。为了展示 LLVA-Interactive 的搭配优势和发展前景，文章提供了多样化的应用场景示例。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Visual-Cues-in-the-Intensive-Care-Unit-and-Association-with-Patient-Clinical-Status"><a href="#Detecting-Visual-Cues-in-the-Intensive-Care-Unit-and-Association-with-Patient-Clinical-Status" class="headerlink" title="Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status"></a>Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00565">http://arxiv.org/abs/2311.00565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhash Nerella, Ziyuan Guan, Andrea Davidson, Yuanfang Ren, Tezcan Baslanti, Brooke Armfield, Patrick Tighe, Azra Bihorac, Parisa Rashidi</li>
<li>For: This paper aims to develop an Artificial Intelligence (AI) tool to augment human assessments in Intensive Care Units (ICUs) by analyzing facial cues to monitor patient condition.* Methods: The authors use a dataset of 107,064 frames collected in the ICU, annotated with facial action units (AUs) labels by trained annotators. They develop a “masked loss computation” technique to address data imbalance and train a SWIN Transformer model to detect 18 AUs.* Results: The SWIN Transformer model achieves a mean F1-score of 0.57 and mean accuracy of 0.89 on the test set. The authors also perform AU inference on 634,054 frames to evaluate the association between facial AUs and clinically important patient conditions such as acuity status, acute brain dysfunction, and pain.<details>
<summary>Abstract</summary>
Intensive Care Units (ICU) provide close supervision and continuous care to patients with life-threatening conditions. However, continuous patient assessment in the ICU is still limited due to time constraints and the workload on healthcare providers. Existing patient assessments in the ICU such as pain or mobility assessment are mostly sporadic and administered manually, thus introducing the potential for human errors. Developing Artificial intelligence (AI) tools that can augment human assessments in the ICU can be beneficial for providing more objective and granular monitoring capabilities. For example, capturing the variations in a patient's facial cues related to pain or agitation can help in adjusting pain-related medications or detecting agitation-inducing conditions such as delirium. Additionally, subtle changes in visual cues during or prior to adverse clinical events could potentially aid in continuous patient monitoring when combined with high-resolution physiological signals and Electronic Health Record (EHR) data. In this paper, we examined the association between visual cues and patient condition including acuity status, acute brain dysfunction, and pain. We leveraged our AU-ICU dataset with 107,064 frames collected in the ICU annotated with facial action units (AUs) labels by trained annotators. We developed a new "masked loss computation" technique that addresses the data imbalance problem by maximizing data resource utilization. We trained the model using our AU-ICU dataset in conjunction with three external datasets to detect 18 AUs. The SWIN Transformer model achieved 0.57 mean F1-score and 0.89 mean accuracy on the test set. Additionally, we performed AU inference on 634,054 frames to evaluate the association between facial AUs and clinically important patient conditions such as acuity status, acute brain dysfunction, and pain.
</details>
<details>
<summary>摘要</summary>
医疗机构（ICU）提供严格的监测和不间断的护理，以帮助患有生命威胁的病人。然而，ICU中病人的 kontinuous assessment 仍然受到时间限制和医疗人员的工作负担的限制。现有的病人评估方法在ICU中，如痛苦或 mobilitity 评估，都是间歇的和由人工实现的，这可能会导致人类错误。通过开发人工智能（AI）工具来增强人类评估的ICU可以为病人提供更加 объек 的监测能力。例如，记录病人的表情变化可以帮助调整痛苦相关的药物或检测昏迷性病变。此外，在或 перед严重临床事件发生时，通过高分辨率生物信号和电子医疗记录（EHR）数据 combin 可能会帮助实现不间断的病人监测。在这篇论文中，我们研究了视觉cue和病人状况之间的关系，包括病情严重程度、脑部损伤和痛苦。我们利用我们的AU-ICU数据集，包括107,064帧ICU中收录的表情动作单元（AU）标签，由训练过的注释员标注。我们开发了一种“遮盖损失计算”技术，解决数据不均衡问题，以最大化数据资源利用。我们使用我们的AU-ICU数据集，并与三个外部数据集一起训练SWINTransformer模型，检测18个AU。测试集得分为0.57的 mean F1 score和0.89的 mean accuracy。此外，我们在634,054帧中进行AU推断，评估facial AU和临床重要的病人状况，如病情严重程度、脑部损伤和痛苦。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Abstraction-and-Reasoning-Corpus-ARC-with-Object-centric-Models-and-the-MDL-Principle"><a href="#Tackling-the-Abstraction-and-Reasoning-Corpus-ARC-with-Object-centric-Models-and-the-MDL-Principle" class="headerlink" title="Tackling the Abstraction and Reasoning Corpus (ARC) with Object-centric Models and the MDL Principle"></a>Tackling the Abstraction and Reasoning Corpus (ARC) with Object-centric Models and the MDL Principle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00545">http://arxiv.org/abs/2311.00545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sébastien Ferré</li>
<li>for: 这个研究是为了推动人工智能水平的AI研究而设计的挑战性 bencmark。</li>
<li>methods: 这个研究使用物件中心的模型，与人类所生成的自然程式一样。它还使用最小描述长度（MDL）原则进行有效的搜寻大型模型空间。</li>
<li>results: 这个研究可以解决多种任务，并且学习的模型与自然程式相似。此外，它还可以应用到不同的领域中。<details>
<summary>Abstract</summary>
The Abstraction and Reasoning Corpus (ARC) is a challenging benchmark, introduced to foster AI research towards human-level intelligence. It is a collection of unique tasks about generating colored grids, specified by a few examples only. In contrast to the transformation-based programs of existing work, we introduce object-centric models that are in line with the natural programs produced by humans. Our models can not only perform predictions, but also provide joint descriptions for input/output pairs. The Minimum Description Length (MDL) principle is used to efficiently search the large model space. A diverse range of tasks are solved, and the learned models are similar to the natural programs. We demonstrate the generality of our approach by applying it to a different domain.
</details>
<details>
<summary>摘要</summary>
《抽象和逻辑集成体（ARC）》是一个挑战性的标准集，用于推动人工智能研究到人类水平的智能。它是一个具有唯一任务的颜色网格生成的集合，通过一些示例来规定。与现有的变换基本Programs不同，我们引入了对象中心的模型，与人类生成的自然程序相符。我们的模型不仅可以进行预测，还可以提供输入/输出对的共同描述。使用最小描述长度原则进行有效地搜索大型模型空间。我们解决了多种任务，并且学习的模型与自然程序相似。我们在不同领域中应用了我们的方法，以示其通用性。
</details></li>
</ul>
<hr>
<h2 id="The-Development-of-LLMs-for-Embodied-Navigation"><a href="#The-Development-of-LLMs-for-Embodied-Navigation" class="headerlink" title="The Development of LLMs for Embodied Navigation"></a>The Development of LLMs for Embodied Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00530">http://arxiv.org/abs/2311.00530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinzhou Lin, Han Gao, Rongtao Xu, Changwei Wang, Li Guo, Shibiao Xu</li>
<li>for: 本研究主要探讨了大语言模型（LLM）与体际智能的融合，尤其是在导航任务方面。</li>
<li>methods: 本文综述了当前的体际导航模型和研究方法，并评估了现有的导航模型和数据集的优缺点。</li>
<li>results: 研究发现，LLM可以帮助体际智能系统在环境理解和决策方面提供了高级别的支持，并且可以帮助解决许多导航任务。<details>
<summary>Abstract</summary>
In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-impartial-policies-for-sequential-counterfactual-explanations-using-Deep-Reinforcement-Learning"><a href="#Learning-impartial-policies-for-sequential-counterfactual-explanations-using-Deep-Reinforcement-Learning" class="headerlink" title="Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning"></a>Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00523">http://arxiv.org/abs/2311.00523</a></li>
<li>repo_url: None</li>
<li>paper_authors: E. Panagiotou, E. Ntoutsi</li>
<li>for: 这个论文主要针对的是Explainable Artificial Intelligence（XAI）领域中的sequential counterfactual（SCF）例子，它们可以改变一个已经训练的分类器的决策，通过对输入实例进行一系列修改。</li>
<li>methods: 这篇论文使用了Reinforcement Learning（RL）方法，它们的目标是通过学习策略来找到SCF例子，从而提高执行效率。然而，RL问题的形式化，包括状态空间、动作和奖励的规定，经常会存在困难和模糊性。</li>
<li>results: 这篇论文提出了一种使用分类器输出概率来创建更加有用的奖励，以减少不良行为的影响。这种方法可以帮助RL策略更好地适应不同的输入实例，从而提高SCF例子的效果。<details>
<summary>Abstract</summary>
In the field of explainable Artificial Intelligence (XAI), sequential counterfactual (SCF) examples are often used to alter the decision of a trained classifier by implementing a sequence of modifications to the input instance. Although certain test-time algorithms aim to optimize for each new instance individually, recently Reinforcement Learning (RL) methods have been proposed that seek to learn policies for discovering SCFs, thereby enhancing scalability. As is typical in RL, the formulation of the RL problem, including the specification of state space, actions, and rewards, can often be ambiguous. In this work, we identify shortcomings in existing methods that can result in policies with undesired properties, such as a bias towards specific actions. We propose to use the output probabilities of the classifier to create a more informative reward, to mitigate this effect.
</details>
<details>
<summary>摘要</summary>
在可解释人工智能（XAI）领域，顺序Counterfactual（SCF）例子经常用于改变已训练的分类器的决策，通过对输入实例进行序列化 modificaciones。虽然某些测试时算法旨在为每个新实例优化，但是最近的强化学习（RL）方法已经提议用于找到SCFs，从而提高可扩展性。在这种情况下，RL问题的形ulation，包括状态空间、动作和奖励的规定，经常是模糊的。在这个工作中，我们发现了现有方法的缺陷，导致政策具有不жела的性格，如偏爱特定的动作。我们提议使用分类器的输出概率来创建更有用的奖励，以 Mitigate这个效应。
</details></li>
</ul>
<hr>
<h2 id="Efficient-LLM-Inference-on-CPUs"><a href="#Efficient-LLM-Inference-on-CPUs" class="headerlink" title="Efficient LLM Inference on CPUs"></a>Efficient LLM Inference on CPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00502">http://arxiv.org/abs/2311.00502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intel/intel-extension-for-transformers">https://github.com/intel/intel-extension-for-transformers</a></li>
<li>paper_authors: Haihao Shen, Hanwen Chang, Bo Dong, Yu Luo, Hengyu Meng</li>
<li>for: 这篇论文旨在提高大型自然语言处理器（LLM）的部署效率。</li>
<li>methods: 该论文提出了一种有效的方法，通过自动INT4 weight-only量化流程和特制的LLM运行时，提高LLM的推理效率 на CPU 上。</li>
<li>results: 该论文对流行的LLM模型，包括Llama2、Llama和GPT-NeoX，进行了推理性能测试，并显示了在CPU上的极高推理效率。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示了惊人的性能和巨大的潜力，但是部署这些模型却受到了巨大的模型参数数量的限制，需要大量的内存容量和高带宽内存带宽。在这篇论文中，我们提出了一种有效的方法，可以更有效地部署 LLM。我们支持自动INT4Weight只量化流程，并设计了特制LLM运行时，以优化CPU上LLM推理的性能。我们在流行的LLM中，包括Llama2、Llama和GPT-NeoX，进行了普适性评估，并在CPUs上显示了极高的推理效率。代码可以在以下链接获取：https://github.com/intel/intel-extension-for-transformers。
</details></li>
</ul>
<hr>
<h2 id="Intriguing-Properties-of-Data-Attribution-on-Diffusion-Models"><a href="#Intriguing-Properties-of-Data-Attribution-on-Diffusion-Models" class="headerlink" title="Intriguing Properties of Data Attribution on Diffusion Models"></a>Intriguing Properties of Data Attribution on Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00500">http://arxiv.org/abs/2311.00500</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sail-sg/d-trak">https://github.com/sail-sg/d-trak</a></li>
<li>paper_authors: Xiaosen Zheng, Tianyu Pang, Chao Du, Jing Jiang, Min Lin</li>
<li>for: 这paper是为了提出一种能够trace模型输出回训练数据的数据归属方法，以便对高质量或版权 protected的训练样本进行公平资源分配。</li>
<li>methods: 这paper使用了一些理论上的驱动来实现数据归属，包括DDPMs和LoRA-finetuned模型，以及一些ablation study。</li>
<li>results: 研究发现，使用不符合理论假设的设计选择可以在 linear datamodeling 分数和counterfactual评价方面 empirically outperform 之前的基eline。这些结果表明，在非拟合设置下，基于理论假设的建构可能会导致差异ential attribution表现。<details>
<summary>Abstract</summary>
Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a significantly more efficient approach for attributing diffusion models, while the unexpected findings suggest that at least in non-convex settings, constructions guided by theoretical assumptions may lead to inferior attribution performance. The code is available at https://github.com/sail-sg/D-TRAK.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将模型输出追溯到训练数据的过程被称为数据归属。随着扩散模型的发展，数据归属已成为一个愿景模块，以确保数据提供者得到公平的资源分配或版权归属。一些基于理论的方法已经被提议来实现数据归属，以提高计算可扩展性和有效性的贸易OFF。在这个工作中，我们进行了广泛的实验和剥削研究，特别是在DDPMs trained on CIFAR-10和CelebA以及LoRA-finetuned on ArtBench上。Surprisingly,我们发现了不符合理论假设的设计选择在数据归属方面实际上超越了之前的基准值，在线性数据模型评分和对冲检验方面均表现出了明显的改善。我们的工作提供了一种更加高效的数据归属方法，而不期望的发现 suggessthat at least in non-convex settings, constructions guided by theoretical assumptions may lead to inferior attribution performance.代码可以在https://github.com/sail-sg/D-TRAK中找到。
</details></li>
</ul>
<hr>
<h2 id="Bayes-enhanced-Multi-view-Attention-Networks-for-Robust-POI-Recommendation"><a href="#Bayes-enhanced-Multi-view-Attention-Networks-for-Robust-POI-Recommendation" class="headerlink" title="Bayes-enhanced Multi-view Attention Networks for Robust POI Recommendation"></a>Bayes-enhanced Multi-view Attention Networks for Robust POI Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00491">http://arxiv.org/abs/2311.00491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangnan Xia, Yu Yang, Senzhang Wang, Hongzhi Yin, Jiannong Cao, Philip S. Yu</li>
<li>for: 该研究旨在提高POI推荐的精度和可靠性，对于 Location-Based Social Network 服务是重要的。</li>
<li>methods: 该研究使用了 Bayes-enhanced Multi-view Attention Network，包括个人POI转移图、semantic-based POI图和距离-based POI图，以全面模型POI之间的依赖关系。</li>
<li>results: 对于含有噪声和损坏数据的POI推荐问题，BayMAN显著超越了现有方法的性能。<details>
<summary>Abstract</summary>
POI recommendation is practically important to facilitate various Location-Based Social Network services, and has attracted rising research attention recently. Existing works generally assume the available POI check-ins reported by users are the ground-truth depiction of user behaviors. However, in real application scenarios, the check-in data can be rather unreliable due to both subjective and objective causes including positioning error and user privacy concerns, leading to significant negative impacts on the performance of the POI recommendation. To this end, we investigate a novel problem of robust POI recommendation by considering the uncertainty factors of the user check-ins, and proposes a Bayes-enhanced Multi-view Attention Network. Specifically, we construct personal POI transition graph, the semantic-based POI graph and distance-based POI graph to comprehensively model the dependencies among the POIs. As the personal POI transition graph is usually sparse and sensitive to noise, we design a Bayes-enhanced spatial dependency learning module for data augmentation from the local view. A Bayesian posterior guided graph augmentation approach is adopted to generate a new graph with collaborative signals to increase the data diversity. Then both the original and the augmented graphs are used for POI representation learning to counteract the data uncertainty issue. Next, the POI representations of the three view graphs are input into the proposed multi-view attention-based user preference learning module. By incorporating the semantic and distance correlations of POIs, the user preference can be effectively refined and finally robust recommendation results are achieved. The results of extensive experiments show that BayMAN significantly outperforms the state-of-the-art methods in POI recommendation when the available check-ins are incomplete and noisy.
</details>
<details>
<summary>摘要</summary>
POI推荐是实际上非常重要，用于支持不同的Location-Based Social Network服务，近年来受到了研究者的越来越多的关注。现有的工作通常假设用户提供的POI检查入是用户行为的准确描述。然而，在实际应用场景中，检查入数据可能受到用户主观和客观因素的影响，包括定位错误和用户隐私问题，这会导致POI推荐的性能受到显著的负面影响。为了解决这个问题，我们调查了一个新的robust POI推荐问题，并提出了一种基于 Bayes 的多视图注意力网络。具体来说，我们构建了个人POI转移图、semantic-based POI图和距离-based POI图，以全面模型POIs之间的依赖关系。由于个人POI转移图通常是稀疏和敏感于噪声的，我们设计了一种 Bayes 增强的空间相关学习模块，通过地方视图进行数据扩充。然后，我们采用 Bayesian posterior 引导的图生成方法，将原始图和扩充后的图进行POI表示学习，以抵消数据不确定性问题。最后，POI表示三个视图图被输入到我们提出的多视图注意力基于用户喜好学习模块中。通过结合POIs的语义和距离相关性，用户喜好可以得到有效地质量化，并最终实现了robust的POI推荐结果。实验结果表明，BayMANsignificantly exceeds state-of-the-art methods in POI recommendation when the available check-ins are incomplete and noisy.
</details></li>
</ul>
<hr>
<h2 id="Dual-Conditioned-Diffusion-Models-for-Out-Of-Distribution-Detection-Application-to-Fetal-Ultrasound-Videos"><a href="#Dual-Conditioned-Diffusion-Models-for-Out-Of-Distribution-Detection-Application-to-Fetal-Ultrasound-Videos" class="headerlink" title="Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos"></a>Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00469">http://arxiv.org/abs/2311.00469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Divyanshu Mishra, He Zhao, Pramit Saha, Aris T. Papageorghiou, J. Alison Noble</li>
<li>for: 这篇论文的目的是提高机器学习模型的可靠性，通过检测训练集以外的数据点（out-of-distribution，OOD）。</li>
<li>methods: 这篇论文使用了双重定制Diffusion模型（DCDM），通过将模型 conditioned on 训练集的类别信息和输入图像的内在特征，以进行重建基本的OOD检测。</li>
<li>results: 比较reference方法，这篇论文的提案模型在精度、精确度和F1分数方面各有12%、22%和8%的提高。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection is essential to improve the reliability of machine learning models by detecting samples that do not belong to the training distribution. Detecting OOD samples effectively in certain tasks can pose a challenge because of the substantial heterogeneity within the in-distribution (ID), and the high structural similarity between ID and OOD classes. For instance, when detecting heart views in fetal ultrasound videos there is a high structural similarity between the heart and other anatomies such as the abdomen, and large in-distribution variance as a heart has 5 distinct views and structural variations within each view. To detect OOD samples in this context, the resulting model should generalise to the intra-anatomy variations while rejecting similar OOD samples. In this paper, we introduce dual-conditioned diffusion models (DCDM) where we condition the model on in-distribution class information and latent features of the input image for reconstruction-based OOD detection. This constrains the generative manifold of the model to generate images structurally and semantically similar to those within the in-distribution. The proposed model outperforms reference methods with a 12% improvement in accuracy, 22% higher precision, and an 8% better F1 score.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了双条件 diffusion 模型（DCDM），该模型通过将模型 conditioned 在内部分布类信息和输入图像的隐藏特征上进行恢复基于 OOD 检测。这将导致模型生成的图像具有与内部分布类信息和隐藏特征相同的结构和Semantic 相似性。我们的模型在参考方法的12%提高精度、22%提高精度和8%提高 F1 分数。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Hyperbolic-Embeddings-for-Coarse-to-Fine-Robot-Design"><a href="#Leveraging-Hyperbolic-Embeddings-for-Coarse-to-Fine-Robot-Design" class="headerlink" title="Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design"></a>Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00462">http://arxiv.org/abs/2311.00462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Dong, Junyu Zhang, Chongjie Zhang</li>
<li>For: 这 paper 的目的是设计多细胞机器人，以便控制其在多种任务上进行高效的执行。* Methods: 该 paper 使用了一种新的粗细到细致的方法，首先优化粗细的机器人，然后逐渐细化。为了解决在粗细转换过程中决定精细化阶段的问题，该 paper 引入了 Hyperbolic Embeddings for Robot Design（HERD）框架。HERD 将机器人各种粒度内部共享一个几何空间，并使用进行优化的 Cross-Entropy Method。* Results: 该 paper 的实验研究表明，该方法在多个复杂任务中表现出了superior的效率和普适性。<details>
<summary>Abstract</summary>
Multi-cellular robot design aims to create robots comprised of numerous cells that can be efficiently controlled to perform diverse tasks. Previous research has demonstrated the ability to generate robots for various tasks, but these approaches often optimize robots directly in the vast design space, resulting in robots with complicated morphologies that are hard to control. In response, this paper presents a novel coarse-to-fine method for designing multi-cellular robots. Initially, this strategy seeks optimal coarse-grained robots and progressively refines them. To mitigate the challenge of determining the precise refinement juncture during the coarse-to-fine transition, we introduce the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies robots of various granularity within a shared hyperbolic space and leverages a refined Cross-Entropy Method for optimization. This framework enables our method to autonomously identify areas of exploration in hyperbolic space and concentrate on regions demonstrating promise. Finally, the extensive empirical studies on various challenging tasks sourced from EvoGym show our approach's superior efficiency and generalization capability.
</details>
<details>
<summary>摘要</summary>
多细胞机器人设计目标是创造由多个细胞组成的机器人，可以高效地控制进行多种任务。过去的研究已经实现了不同任务的机器人设计，但这些方法通常直接在庞大的设计空间中优化机器人，导致机器人的结构变得复杂，控制困难。因此，本文提出了一种新的粗略到细腻的方法 для设计多细胞机器人。首先，这种策略寻找最佳粗略机器人，然后逐步细化它们。为了解决在粗略到细腻的过渡中决定精细化的问题，我们提出了Hyperbolic Embeddings for Robot Design（HERD）框架。HERD将多种细胞机器人集成到共享的虚拟空间中，并利用了改进的十字熵方法进行优化。这个框架使我们的方法可以自动找到在虚拟空间中需要探索的区域，并将焦点集中在示 promise的区域。最后，我们对多个复杂任务来源于EvoGym进行了广泛的实验研究，并证明了我们的方法的更高效和普遍性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Opportunities-of-Green-Computing-A-Survey"><a href="#On-the-Opportunities-of-Green-Computing-A-Survey" class="headerlink" title="On the Opportunities of Green Computing: A Survey"></a>On the Opportunities of Green Computing: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00447">http://arxiv.org/abs/2311.00447</a></li>
<li>repo_url: None</li>
<li>paper_authors: You Zhou, Xiujing Lin, Xiang Zhang, Maolin Wang, Gangwei Jiang, Huakang Lu, Yupeng Wu, Kai Zhang, Zhe Yang, Kehang Wang, Yongduo Sui, Fengwei Jia, Zuoli Tang, Yao Zhao, Hongxuan Zhang, Tiannuo Yang, Weibo Chen, Yunong Mao, Yi Li, De Bao, Yu Li, Hongrui Liao, Ting Liu, Jingwen Liu, Jinchi Guo, Jin Zhao, Xiangyu Zhao, Ying WEI, Hong Qian, Qi Liu, Xiang Wang, Wai Kin, Chan, Chenliang Li, Yusen Li, Shiyu Yang, Jining Yan, Chao Mou, Shuai Han, Wuxia Jin, Guannan Zhang, Xiaodong Zeng</li>
<li>for: 这篇论文旨在提出一种绿色计算技术框架，以提高人工智能发展的可持续性。</li>
<li>methods: 该论文提出了四个关键组成部分：（1）评估绿色程度，（2）能效的人工智能，（3）能效的计算系统，（4）可持续发展的AI应用场景。每个部分都有许多常用的优化技术来提高AI的效率。</li>
<li>results: 该论文认为，通过绿色计算技术，可以解决人工智能发展所带来的资源约束和环境影响问题，并且鼓励更多的研究人员关注这一方向，使人工智能更加环保。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) has achieved significant advancements in technology and research with the development over several decades, and is widely used in many areas including computing vision, natural language processing, time-series analysis, speech synthesis, etc. During the age of deep learning, especially with the arise of Large Language Models, a large majority of researchers' attention is paid on pursuing new state-of-the-art (SOTA) results, resulting in ever increasing of model size and computational complexity. The needs for high computing power brings higher carbon emission and undermines research fairness by preventing small or medium-sized research institutions and companies with limited funding in participating in research. To tackle the challenges of computing resources and environmental impact of AI, Green Computing has become a hot research topic. In this survey, we give a systematic overview of the technologies used in Green Computing. We propose the framework of Green Computing and devide it into four key components: (1) Measures of Greenness, (2) Energy-Efficient AI, (3) Energy-Efficient Computing Systems and (4) AI Use Cases for Sustainability. For each components, we discuss the research progress made and the commonly used techniques to optimize the AI efficiency. We conclude that this new research direction has the potential to address the conflicts between resource constraints and AI development. We encourage more researchers to put attention on this direction and make AI more environmental friendly.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在技术和研究方面已经取得了重要进步，并广泛应用于计算视觉、自然语言处理、时间序列分析、语音合成等领域。在深度学习时代，特别是大语言模型的出现，许多研究人员的注意力集中在追求新的State-of-the-Art（SOTA）结果上，导致模型的大小和计算复杂度不断增加。为了解决AI计算资源和环境影响的挑战，绿色计算成为了热点的研究话题。在这篇评论中，我们提供了绿色计算技术的系统性评论，并将其分为四个关键组成部分：（1）绿色指标，（2）能效AI，（3）能效计算系统，（4）可持续性AI应用。对每个组成部分，我们讨论了研究进步和优化AI效率的常用技术。我们认为这新的研究方向有可能解决AI发展和资源约束之间的矛盾。我们鼓励更多研究人员关注这个方向，使AI变得更加环保。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Comparison-of-Syllogistic-Reasoning-in-Humans-and-Language-Models"><a href="#A-Systematic-Comparison-of-Syllogistic-Reasoning-in-Humans-and-Language-Models" class="headerlink" title="A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models"></a>A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00445">http://arxiv.org/abs/2311.00445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiwalayo Eisape, MH Tessler, Ishita Dasgupta, Fei Sha, Sjoerd van Steenkiste, Tal Linzen</li>
<li>for: 这个论文主要研究了语言模型是否会受人类逻辑偏见的影响，以及语言模型是否可以超越这些偏见。</li>
<li>methods: 研究者使用了大量的文本数据来训练语言模型，并通过测试这些模型的逻辑能力来 evalute their performance。</li>
<li>results: 研究者发现，较大的语言模型比较符合逻辑规则，但是 même les plus grands modèles commettent des erreurs systématiques, certaines desquelles sont similaires aux biais de la raison humaine, telles que les effets d’ordre et les fausses logiques.<details>
<summary>Abstract</summary>
A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
</details>
<details>
<summary>摘要</summary>
人类行为中的一个重要组成部分是逻辑推理：从一aset of premises determining which conclusions follow. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.Here's the text with traditional Chinese characters:人类行为中的一个重要组成部分是逻辑推理：从一aset of premises determining which conclusions follow。psychologists have documented several ways in which humans' inferences deviate from the rules of logic。Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them？Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans。At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies。Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases。
</details></li>
</ul>
<hr>
<h2 id="Improving-Robustness-for-Vision-Transformer-with-a-Simple-Dynamic-Scanning-Augmentation"><a href="#Improving-Robustness-for-Vision-Transformer-with-a-Simple-Dynamic-Scanning-Augmentation" class="headerlink" title="Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation"></a>Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00441">http://arxiv.org/abs/2311.00441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Kotyan, Danilo Vasconcellos Vargas</li>
<li>For: This paper aims to improve the accuracy and robustness of Vision Transformers (ViT) in computer vision tasks, specifically by introducing a novel augmentation technique called Dynamic Scanning Augmentation.* Methods: The proposed method uses dynamic input sequences to adaptively focus on different patches of an image, which induces significant changes in the attention mechanism of ViT. Four variations of Dynamic Scanning Augmentation are introduced and evaluated.* Results: The proposed method improves the robustness of ViT against adversarial attacks, with one variant showing comparable results to the state-of-the-art. The integration of Dynamic Scanning Augmentation leads to a substantial increase in ViT’s robustness, improving it from 17% to 92% across different types of adversarial attacks.<details>
<summary>Abstract</summary>
Vision Transformer (ViT) has demonstrated promising performance in computer vision tasks, comparable to state-of-the-art neural networks. Yet, this new type of deep neural network architecture is vulnerable to adversarial attacks limiting its capabilities in terms of robustness. This article presents a novel contribution aimed at further improving the accuracy and robustness of ViT, particularly in the face of adversarial attacks. We propose an augmentation technique called `Dynamic Scanning Augmentation' that leverages dynamic input sequences to adaptively focus on different patches, thereby maintaining performance and robustness. Our detailed investigations reveal that this adaptability to the input sequence induces significant changes in the attention mechanism of ViT, even for the same image. We introduce four variations of Dynamic Scanning Augmentation, outperforming ViT in terms of both robustness to adversarial attacks and accuracy against natural images, with one variant showing comparable results. By integrating our augmentation technique, we observe a substantial increase in ViT's robustness, improving it from $17\%$ to $92\%$ measured across different types of adversarial attacks. These findings, together with other comprehensive tests, indicate that Dynamic Scanning Augmentation enhances accuracy and robustness by promoting a more adaptive type of attention. In conclusion, this work contributes to the ongoing research on Vision Transformers by introducing Dynamic Scanning Augmentation as a technique for improving the accuracy and robustness of ViT. The observed results highlight the potential of this approach in advancing computer vision tasks and merit further exploration in future studies.
</details>
<details>
<summary>摘要</summary>
“视力变换器（ViT）在计算机视觉任务中表现出了可塑性，与当前最佳神经网络相当。然而，这种新型深度神经网络架构受到了针对性攻击的限制，增加了其可靠性的问题。本文提出了一种新的贡献，即使用“动态扫描增强”技术来提高ViT的准确率和可靠性，特别是对针对性攻击。我们的详细调查表明，这种动态输入序列的适应性使得ViT的注意机制发生了重要变化，即使用相同的图像。我们提出了四种动态扫描增强的变种，在面对不同类型的针对性攻击和自然图像时都表现出优异的表现。通过将我们的增强技术与ViT结合，我们发现了一个重要的提高，从17%提高到92%，这表明了我们的增强技术对ViT的可靠性和准确率具有显著的改进作用。这些发现，加之其他详细测试，表明了动态扫描增强可以提高ViT的可靠性和准确率，并且推动神经网络的注意机制发展。因此，本研究的结果表明了动态扫描增强的潜在价值，并且值得在未来的研究中进一步探索。”
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Generalization-through-Prioritization-and-Diversity-in-Self-Imitation-Reinforcement-Learning-over-Procedural-Environments-with-Sparse-Rewards"><a href="#Enhanced-Generalization-through-Prioritization-and-Diversity-in-Self-Imitation-Reinforcement-Learning-over-Procedural-Environments-with-Sparse-Rewards" class="headerlink" title="Enhanced Generalization through Prioritization and Diversity in Self-Imitation Reinforcement Learning over Procedural Environments with Sparse Rewards"></a>Enhanced Generalization through Prioritization and Diversity in Self-Imitation Reinforcement Learning over Procedural Environments with Sparse Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00426">http://arxiv.org/abs/2311.00426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alain Andres, Daochen Zha, Javier Del Ser</li>
<li>for: 本文旨在解决RL中的探索挑战，尤其是在罕见奖励情况下，agent的决策学习能力受到限制。</li>
<li>methods: 本文提出了一种新的自我模仿学习方法，通过存储和重复成功行为来优化探索。但传统的自我模仿学习方法受限于通用环境和高归还率下的泛化问题。本文提出了一些修改来决定保留哪些经验，并在PCG环境中扩展了优先级技术。</li>
<li>results: 我们的实验分析在三个PCG罕见奖励环境中，包括MiniGrid和ProcGen，显示了我们提出的修改带来了新的最佳性能记录在MiniGrid-MultiRoom-N12-S10环境中。<details>
<summary>Abstract</summary>
Exploration poses a fundamental challenge in Reinforcement Learning (RL) with sparse rewards, limiting an agent's ability to learn optimal decision-making due to a lack of informative feedback signals. Self-Imitation Learning (self-IL) has emerged as a promising approach for exploration, leveraging a replay buffer to store and reproduce successful behaviors. However, traditional self-IL methods, which rely on high-return transitions and assume singleton environments, face challenges in generalization, especially in procedurally-generated (PCG) environments. Therefore, new self-IL methods have been proposed to rank which experiences to persist, but they replay transitions uniformly regardless of their significance, and do not address the diversity of the stored demonstrations. In this work, we propose tailored self-IL sampling strategies by prioritizing transitions in different ways and extending prioritization techniques to PCG environments. We also address diversity loss through modifications to counteract the impact of generalization requirements and bias introduced by prioritization techniques. Our experimental analysis, conducted over three PCG sparse reward environments, including MiniGrid and ProcGen, highlights the benefits of our proposed modifications, achieving a new state-of-the-art performance in the MiniGrid-MultiRoom-N12-S10 environment.
</details>
<details>
<summary>摘要</summary>
探索是强化学习（RL）中的基本挑战，尤其在罕见奖励情况下，因为缺乏有用的反馈信号，导致代理人的决策学习受到限制。自我模仿学习（self-IL）已经出现为探索的有力的方法，利用储存和重复成功行为的缓存 buffer。然而，传统的自我模仿学习方法，假设环境是单个的，并且需要高归据转移，具有泛化问题，特别是在生成式环境（PCG）中。因此，新的自我模仿学习方法被提出，以排序经验，但是它们在重要性方面是不充分的，并且不处理存储的示例异常。在这项工作中，我们提议适应性的自我模仿学习抽样策略，根据不同的方式来优先级排序转移，并在PCG环境中扩展优先级技术。此外，我们还解决了多样性损失，通过修改缓存的方式，以及对泛化需求和偏见引入的修正。我们的实验分析，在三个PCG稀热奖励环境中进行，包括MiniGrid和ProcGen，表明我们的提议得到了新的状态Record-of-the-art表现在MiniGrid-MultiRoom-N12-S10环境中。</sys>Note: The text is translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The Traditional Chinese form is used in Taiwan, Hong Kong, and Macau.Here's the translation:探索是强化学习（RL）中的基本挑战，尤其在罕见奖励情况下，因为缺乏有用的反馈信号，导致代理人的决策学习受到限制。自我模仿学习（self-IL）已经出现为探索的有力的方法，利用储存和重复成功行为的缓存 buffer。然而，传统的自我模仿学习方法，假设环境是单个的，并且需要高归据转移，具有泛化问题，特别是在生成式环境（PCG）中。因此，新的自我模仿学习方法被提出，以排序经验，但是它们在重要性方面是不充分的，并且不处理存储的示例异常。在这项工作中，我们提议适应性的自我模仿学习抽样策略，根据不同的方式来优先级排序转移，并在PCG环境中扩展优先级技术。此外，我们还解决了多样性损失，通过修改缓存的方式，以及对泛化需求和偏见引入的修正。我们的实验分析，在三个PCG稀热奖励环境中进行，包括MiniGrid和ProcGen，表明我们的提议得到了新的状态Record-of-the-art表现在MiniGrid-MultiRoom-N12-S10环境中。
</details></li>
</ul>
<hr>
<h2 id="Neural-Implicit-Field-Editing-Considering-Object-environment-Interaction"><a href="#Neural-Implicit-Field-Editing-Considering-Object-environment-Interaction" class="headerlink" title="Neural Implicit Field Editing Considering Object-environment Interaction"></a>Neural Implicit Field Editing Considering Object-environment Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00425">http://arxiv.org/abs/2311.00425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihong Zeng, Zongji Wang, Yuanben Zhang, Weinan Cai, Zehao Cao, Lili Zhang, Yan Guo, Yanhong Zhang, Junyi Liu</li>
<li>for: 三元场景编辑方法基于神经隐藏场景，已经吸引广泛关注，在三元编辑任务中表现出色。但是现有方法通常将物体和场景环境的交互强制合并，导致渲染视图中的场景外观变化不能正确显示。本文提出一种对物体和场景环境相互关注的系统（OSI-aware系统），这是一种新的两树神经渲染系统，考虑到物体和场景环境之间的交互。</li>
<li>methods: 为了从混合汤中获得照明条件，我们成功地将物体和场景环境的交互分解为内在分解方法。在对物体级编辑任务中，我们引入深度地图导航场景填充方法和阴影渲染方法，使用点匹配策略。</li>
<li>results: 我们的新ipeline在Scene editing任务中生成了合理的外观变化，并且在新视图生成任务中实现了竞争性的表现质量。<details>
<summary>Abstract</summary>
The 3D scene editing method based on neural implicit field has gained wide attention. It has achieved excellent results in 3D editing tasks. However, existing methods often blend the interaction between objects and scene environment. The change of scene appearance like shadows is failed to be displayed in the rendering view. In this paper, we propose an Object and Scene environment Interaction aware (OSI-aware) system, which is a novel two-stream neural rendering system considering object and scene environment interaction. To obtain illuminating conditions from the mixture soup, the system successfully separates the interaction between objects and scene environment by intrinsic decomposition method. To study the corresponding changes to the scene appearance from object-level editing tasks, we introduce a depth map guided scene inpainting method and shadow rendering method by point matching strategy. Extensive experiments demonstrate that our novel pipeline produce reasonable appearance changes in scene editing tasks. It also achieve competitive performance for the rendering quality in novel-view synthesis tasks.
</details>
<details>
<summary>摘要</summary>
三维场景编辑方法基于神经隐函数已经受到广泛关注。它在三维编辑任务中表现出色。然而，现有方法经常混合对象和场景环境的交互。改变场景的外观，如阴影，在渲染视图中未能正确显示。在这篇论文中，我们提出了一种对象和场景环境相互响应（OSI-aware）系统，这是一种新的两栈神经渲染系统，考虑到对象和场景环境的交互。通过内在分解方法，我们成功地从混合液中提取了对象和场景环境之间的交互。为了研究对象编辑任务中场景外观的变化，我们引入了深度地图导航的场景填充方法和阴影渲染方法，使用点匹配策略。广泛的实验表明，我们的新ipeline可以在场景编辑任务中生成合理的外观变化，并且在新视图合成任务中实现了竞争性的表现质量。
</details></li>
</ul>
<hr>
<h2 id="Couples-can-be-tractable-New-algorithms-and-hardness-results-for-the-Hospitals-Residents-problem-with-Couples"><a href="#Couples-can-be-tractable-New-algorithms-and-hardness-results-for-the-Hospitals-Residents-problem-with-Couples" class="headerlink" title="Couples can be tractable: New algorithms and hardness results for the Hospitals &#x2F; Residents problem with Couples"></a>Couples can be tractable: New algorithms and hardness results for the Hospitals &#x2F; Residents problem with Couples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00405">http://arxiv.org/abs/2311.00405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gergely Csáji, David Manlove, Iain McBride, James Trimble</li>
<li>for: 本文研究了医院医生婚姻问题（Hospitals&#x2F;Residents problem），具体来说是在医院医生婚姻问题中，夫妻双方的偏好不具有响应性和完整性（sub-responsive and sub-complete）的情况下，提出了一种近似可行的稳定医生婚姻（near-feasible stable matching）算法。</li>
<li>methods: 本文提出了一种基于稳定设备（Stable Fixtures）问题的算法，可以在医院医生婚姻问题中找到近似可行的稳定医生婚姻。此外，本文还提出了一种在某些特殊情况下的另一种算法，可以在医院医生婚姻问题中找到稳定医生婚姻。</li>
<li>results: 本文的算法可以在医院医生婚姻问题中找到近似可行的稳定医生婚姻，并且可以证明这种算法的可行性。此外，本文还证明了医院医生婚姻问题在某些特殊情况下是NP困难的，例如在夫妻双方的偏好不具有响应性和完整性的情况下。<details>
<summary>Abstract</summary>
In this paper we study the {\sc Hospitals / Residents problem with Couples} ({\sc hrc}), where a solution is a stable matching or a report that none exists. We present a novel polynomial-time algorithm that can find a near-feasible stable matching (adjusting the hospitals' capacities by at most 1) in an {\sc hrc} instance where the couples' preferences are sub-responsive (i.e., if one member switches to a better hospital, than the couple also improves) and sub-complete (i.e., each pair of hospitals that are individually acceptable to both members are jointly acceptable for the couple) by reducing it to an instance of the {\sc Stable Fixtures} problem. We also present a polynomial-time algorithm for {\sc hrc} in a sub-responsive, sub-complete instance that is a Dual Market, or where all couples are one of several possible types. We show that our algorithm also implies the polynomial-time solvability of a stable b-matching problem, where the underlying graph is a multigraph with loops.   We complement our algorithms with several hardness results. We show that {\sc hrc} with sub-responsive and sub-complete couples is NP-hard, even with other strong restrictions. We also show that {\sc hrc} with a Dual Market is NP-hard under several simultaneous restrictions. Finally, we show that the problem of finding a matching with the minimum number of blocking pairs in {\sc hrc} is not approximable within $m^{1-\varepsilon}$, for any $\varepsilon>0$, where $m$ is the total length of the hospitals' preference lists, unless P=NP, even if each couple applies to only one pair of hospitals. Our polynomial-time solvability results greatly expand the class of known tractable instances of {\sc hrc} and provide additional evidence as to why long-standing entry-level labour markets that allow couples such as the National Resident Matching Program remain successful to this day.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了医院和住院医生匹配问题（{\sc hrc），其中一个解决方案是稳定匹配或一份无匹配报告。我们提出了一种新的多阶时间算法，可以在{\sc hrc}实例中找到一个近似稳定匹配（通过修改医院容量，最多为1），当伴侣偏好是抑弱响应（即，如果一个成员转移到更好的医院，那么伴侣也会提高）和抑完全（即，每对医院都是对两名成员都可接受的）时。我们还提出了一种多阶时间算法，用于{\sc hrc}实例，其中伴侣偏好是抑弱响应和抑完全，或者是多种类型的 dual market。我们证明了我们的算法也可以解决稳定b匹配问题，其下面的图为多重图。我们补充了一些硬性结果。我们证明了{\sc hrc}中的抑弱抑完全伴侣是NP困难的，即使有其他强制限制。我们还证明了{\sc hrc}中的 dual market 是NP困难的，对于多种同时限制。最后，我们证明了{\sc hrc}中寻找最小数量的堵塞对的匹配是不可以在$m^{1-\varepsilon}$的近似内 approximable，其中$m$ 是所有医院的偏好列表总长度，$\varepsilon$ 是任意小于0的数值，除非P=NP。我们的多阶时间可行性结果扩大了知道的可 tractable{\sc hrc}实例，并提供了更多的证据，证明为什么长期的入门级别劳动市场，如国家住院医生匹配计划，仍然成功至今。
</details></li>
</ul>
<hr>
<h2 id="A-Spatial-Temporal-Transformer-based-Framework-For-Human-Pose-Assessment-And-Correction-in-Education-Scenarios"><a href="#A-Spatial-Temporal-Transformer-based-Framework-For-Human-Pose-Assessment-And-Correction-in-Education-Scenarios" class="headerlink" title="A Spatial-Temporal Transformer based Framework For Human Pose Assessment And Correction in Education Scenarios"></a>A Spatial-Temporal Transformer based Framework For Human Pose Assessment And Correction in Education Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00401">http://arxiv.org/abs/2311.00401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyang Hu, Kai Liu, Libin Liu, Huiliang Shang</li>
<li>for: 这 papers 的目的是为了在教育场景中进行人体姿势评估和修正，以提供专业、快速纠正反馈给学生。</li>
<li>methods: 这 papers 使用了 Spatial-Temporal Transformer 框架（STTF），包括骨骼跟踪、姿势估计、姿势评估和姿势修正模块，以提供专业、快速纠正反馈给学生。</li>
<li>results: results 表明，我们的模型可以有效地评估和修正学生的动作质量。 STTF 利用 transformer 模型 capture 人体姿势的空间和时间相关性，以提供高精度的评估和修正。<details>
<summary>Abstract</summary>
Human pose assessment and correction play a crucial role in applications across various fields, including computer vision, robotics, sports analysis, healthcare, and entertainment. In this paper, we propose a Spatial-Temporal Transformer based Framework (STTF) for human pose assessment and correction in education scenarios such as physical exercises and science experiment. The framework comprising skeletal tracking, pose estimation, posture assessment, and posture correction modules to educate students with professional, quick-to-fix feedback. We also create a pose correction method to provide corrective feedback in the form of visual aids. We test the framework with our own dataset. It comprises (a) new recordings of five exercises, (b) existing recordings found on the internet of the same exercises, and (c) corrective feedback on the recordings by professional athletes and teachers. Results show that our model can effectively measure and comment on the quality of students' actions. The STTF leverages the power of transformer models to capture spatial and temporal dependencies in human poses, enabling accurate assessment and effective correction of students' movements.
</details>
<details>
<summary>摘要</summary>
人体姿势评估和修正在多个领域中扮演着关键角色，包括计算机视觉、机器人学、体育分析、医疗和娱乐等。在这篇论文中，我们提出了一个空间-时间变换器基础架构（STTF），用于教育场景中的人体姿势评估和修正。该架构包括骨骼跟踪、姿势估计、姿势评估和姿势修正模块，以提供专业、快速修复的反馈。我们还创造了一种姿势修正方法，以Visual化形式提供修正反馈。我们对自己的数据集进行测试，该数据集包括（a）新录制的五种运动，（b）互联网上已有同样运动的录制，以及（c）专业运动员和教师提供的修正反馈。结果表明，我们的模型可以准确地评估和修正学生的动作质量。STTF利用变换器模型来捕捉人体姿势的空间和时间依赖关系，以便准确评估和修正学生的运动。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-deep-neural-networks-with-symbolic-knowledge-Towards-trustworthy-and-interpretable-AI-for-education"><a href="#Augmenting-deep-neural-networks-with-symbolic-knowledge-Towards-trustworthy-and-interpretable-AI-for-education" class="headerlink" title="Augmenting deep neural networks with symbolic knowledge: Towards trustworthy and interpretable AI for education"></a>Augmenting deep neural networks with symbolic knowledge: Towards trustworthy and interpretable AI for education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00393">http://arxiv.org/abs/2311.00393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danial Hooshyar, Roger Azevedo, Yeongwook Yang</li>
<li>for: 这个研究旨在探讨人工神经网络在教育应用中的潜力，并提出一种基于神经符号学的人工智能框架，以解决现有的三大挑战。</li>
<li>methods: 该研究采用了基于神经符号学的人工智能框架，并开发了一种名为NSAI的方法，可以将教育知识注入和提取出深度神经网络中，以模拟学生的计算思维。</li>
<li>results: 研究发现，使用NSAI方法可以提高模型的普适性和可解释性，并且可以避免训练数据中的偏见和控制偏见。此外，NSAI方法还可以提取出规则，以便解释和理解预测的路径，以及改进初始的教育知识。<details>
<summary>Abstract</summary>
Artificial neural networks (ANNs) have shown to be amongst the most important artificial intelligence (AI) techniques in educational applications, providing adaptive educational services. However, their educational potential is limited in practice due to three major challenges: i) difficulty in incorporating symbolic educational knowledge (e.g., causal relationships, and practitioners' knowledge) in their development, ii) learning and reflecting biases, and iii) lack of interpretability. Given the high-risk nature of education, the integration of educational knowledge into ANNs becomes crucial for developing AI applications that adhere to essential educational restrictions, and provide interpretability over the predictions. This research argues that the neural-symbolic family of AI has the potential to address the named challenges. To this end, it adapts a neural-symbolic AI framework and accordingly develops an approach called NSAI, that injects and extracts educational knowledge into and from deep neural networks, for modelling learners computational thinking. Our findings reveal that the NSAI approach has better generalizability compared to deep neural networks trained merely on training data, as well as training data augmented by SMOTE and autoencoder methods. More importantly, unlike the other models, the NSAI approach prioritises robust representations that capture causal relationships between input features and output labels, ensuring safety in learning to avoid spurious correlations and control biases in training data. Furthermore, the NSAI approach enables the extraction of rules from the learned network, facilitating interpretation and reasoning about the path to predictions, as well as refining the initial educational knowledge. These findings imply that neural-symbolic AI can overcome the limitations of ANNs in education, enabling trustworthy and interpretable applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Difficulty in incorporating symbolic educational knowledge (e.g., causal relationships, practitioners’ knowledge) in their development.2. Learning and reflecting biases.3. Lack of interpretability.To address these challenges, this research proposes the use of the neural-symbolic family of AI, which has the potential to integrate educational knowledge into ANNs and provide interpretability over the predictions. The proposed approach, called NSAI, injects and extracts educational knowledge into and from deep neural networks, enabling the modelling of learners’ computational thinking.The findings show that the NSAI approach has better generalizability compared to deep neural networks trained merely on training data, as well as training data augmented by SMOTE and autoencoder methods. Additionally, the NSAI approach prioritizes robust representations that capture causal relationships between input features and output labels, ensuring safety in learning and avoiding spurious correlations and control biases in training data.The NSAI approach also enables the extraction of rules from the learned network, facilitating interpretation and reasoning about the path to predictions, as well as refining the initial educational knowledge. These findings suggest that neural-symbolic AI can overcome the limitations of ANNs in education, enabling trustworthy and interpretable applications.</details></li>
</ol>
<hr>
<h2 id="Will-Code-Remain-a-Relevant-User-Interface-for-End-User-Programming-with-Generative-AI-Models"><a href="#Will-Code-Remain-a-Relevant-User-Interface-for-End-User-Programming-with-Generative-AI-Models" class="headerlink" title="Will Code Remain a Relevant User Interface for End-User Programming with Generative AI Models?"></a>Will Code Remain a Relevant User Interface for End-User Programming with Generative AI Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00382">http://arxiv.org/abs/2311.00382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Advait Sarkar</li>
<li>for: 这篇论文探讨了在生成AI时，传统编程语言是否仍然有用于非专业程序员。</li>
<li>methods: 作者提出了“生成转移 гипотезы”，即生成AI会对传统编程语言的范围进行质量和量上的扩展。</li>
<li>results: 作者认为，传统编程语言仍然有用于非专业程序员，并提出了一些可能是永久性和持续性的原因。<details>
<summary>Abstract</summary>
The research field of end-user programming has largely been concerned with helping non-experts learn to code sufficiently well in order to achieve their tasks. Generative AI stands to obviate this entirely by allowing users to generate code from naturalistic language prompts. In this essay, we explore the extent to which "traditional" programming languages remain relevant for non-expert end-user programmers in a world with generative AI. We posit the "generative shift hypothesis": that generative AI will create qualitative and quantitative expansions in the traditional scope of end-user programming. We outline some reasons that traditional programming languages may still be relevant and useful for end-user programmers. We speculate whether each of these reasons might be fundamental and enduring, or whether they may disappear with further improvements and innovations in generative AI. Finally, we articulate a set of implications for end-user programming research, including the possibility of needing to revisit many well-established core concepts, such as Ko's learning barriers and Blackwell's attention investment model.
</details>
<details>
<summary>摘要</summary>
研究领域内的终端用户编程主要关注在帮助非专业人员学习编程足够好以完成任务。生成AI可能完全替代这一点，允许用户通过自然语言提示生成代码。在这篇文章中，我们探讨了“传统”编程语言在非专业终端编程者的世界中是否仍然有 relevance。我们提出了“生成转变假设”：生成AI会为终端编程带来质量和量上的扩展。我们列出了一些可能使得传统编程语言仍然有用和有价值的原因。我们推测这些原因是不可逆的和普遍存在的，或者可能在进一步改进和创新生成AI时消失。最后，我们提出了对终端编程研究的一些影响，包括可能需要重新评估许多已成为核心概念的事物，如科学学习障碍和黑卫投入模型。
</details></li>
</ul>
<hr>
<h2 id="Architecture-of-Data-Anomaly-Detection-Enhanced-Decentralized-Expert-System-for-Early-Stage-Alzheimer’s-Disease-Prediction"><a href="#Architecture-of-Data-Anomaly-Detection-Enhanced-Decentralized-Expert-System-for-Early-Stage-Alzheimer’s-Disease-Prediction" class="headerlink" title="Architecture of Data Anomaly Detection-Enhanced Decentralized Expert System for Early-Stage Alzheimer’s Disease Prediction"></a>Architecture of Data Anomaly Detection-Enhanced Decentralized Expert System for Early-Stage Alzheimer’s Disease Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00373">http://arxiv.org/abs/2311.00373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Kambiz Behfar, Qumars Behfar, Marzie Hosseinpour</li>
<li>for: 这篇研究的目的是为了早期准确地检测阿兹海默症，以提高病人的结果。</li>
<li>methods: 这篇研究使用了分布式专家系统，结合了区块chain技术和人工智能，以实现强大的偏差检测。</li>
<li>results: 这篇研究获得了更精确的早期阿兹海默症预测结果，并且保护了患者的隐私和数据完整性。<details>
<summary>Abstract</summary>
Alzheimer's Disease is a global health challenge that requires early and accurate detection to improve patient outcomes. Magnetic Resonance Imaging (MRI) holds significant diagnostic potential, but its effective analysis remains a formidable task. This study introduces a groundbreaking decentralized expert system that cleverly combines blockchain technology with Artificial Intelligence (AI) to integrate robust anomaly detection for patient-submitted data.   Traditional diagnostic methods often lead to delayed and imprecise predictions, especially in the early stages of the disease. Centralized data repositories struggle to manage the immense volumes of MRI data, and persistent privacy concerns hinder collaborative efforts. Our innovative solution harnesses decentralization to protect data integrity and patient privacy, facilitated by blockchain technology. It not only emphasizes AI-driven MRI analysis but also incorporates a sophisticated data anomaly detection architecture. These mechanisms scrutinize patient-contributed data for various issues, including data quality problems and atypical findings within MRI images.   Conducting an exhaustive check of MRI image correctness and quality directly on the blockchain is impractical due to computational complexity and cost constraints. Typically, such checks are performed off-chain, and the blockchain securely records the results. This comprehensive approach empowers our decentralized app to provide more precise early-stage Alzheimer's Disease predictions. By merging the strengths of blockchain, AI, and anomaly detection, our system represents a pioneering step towards revolutionizing disease diagnostics.
</details>
<details>
<summary>摘要</summary>
亚历雷明症是全球医疗挑战，早期检测可以提高患者结果。磁共振成像（MRI）具有诊断潜力，但是有效分析是一项困难任务。本研究推出了一种创新的分布式专家系统，它将区块链技术与人工智能（AI）相结合，以实现强大的异常检测。传统的诊断方法经常导致延迟和不准确的预测，特别是在病情晚期。中央数据存储系统很难处理庞大量的MRI数据，而且持续存在隐私问题，这阻碍了合作努力。我们的创新解决方案利用分布式结构保护数据完整性和患者隐私，通过区块链技术实现。它不仅强调了AI驱动的MRI分析，还包括了复杂的数据异常检测建筑。这些机制对患者提供的数据进行了多种检查，包括数据质量问题和MRI图像中的异常现象。在区块链上直接进行MRI图像正确性和质量检查是计算复杂和成本高的。通常，这些检查在防火墙外进行，并将结果记录在区块链上。这种全面的方法使我们的分布式应用程序可以提供更精确的早期亚历雷明症预测。通过融合区块链、AI和异常检测的优势，我们的系统表明了一种革新的疾病诊断方式。
</details></li>
</ul>
<hr>
<h2 id="Prompt-based-Logical-Semantics-Enhancement-for-Implicit-Discourse-Relation-Recognition"><a href="#Prompt-based-Logical-Semantics-Enhancement-for-Implicit-Discourse-Relation-Recognition" class="headerlink" title="Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition"></a>Prompt-based Logical Semantics Enhancement for Implicit Discourse Relation Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00367">http://arxiv.org/abs/2311.00367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lalalamdbf/plse_idrr">https://github.com/lalalamdbf/plse_idrr</a></li>
<li>paper_authors: Chenxu Wang, Ping Jian, Mu Huang</li>
<li>for: 本研究旨在提高无Connective的干扰关系识别（IDRR）性能，利用注释感知结构信息提高干扰关系表示。</li>
<li>methods: 我们提出了一种基于提示的逻辑 semantics 增强（PLSE）方法，通过注释预测来注入逻辑知识到预训练语言模型中。</li>
<li>results: 我们的方法在 PDTB 2.0 和 CoNLL16 测试集上实现了出色的表现，与当前状态艺术模型相比。<details>
<summary>Abstract</summary>
Implicit Discourse Relation Recognition (IDRR), which infers discourse relations without the help of explicit connectives, is still a crucial and challenging task for discourse parsing. Recent works tend to exploit the hierarchical structure information from the annotated senses, which demonstrate enhanced discourse relation representations can be obtained by integrating sense hierarchy. Nevertheless, the performance and robustness for IDRR are significantly constrained by the availability of annotated data. Fortunately, there is a wealth of unannotated utterances with explicit connectives, that can be utilized to acquire enriched discourse relation features. In light of such motivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE) method for IDRR. Essentially, our method seamlessly injects knowledge relevant to discourse relation into pre-trained language models through prompt-based connective prediction. Furthermore, considering the prompt-based connective prediction exhibits local dependencies due to the deficiency of masked language model (MLM) in capturing global semantics, we design a novel self-supervised learning objective based on mutual information maximization to derive enhanced representations of logical semantics for IDRR. Experimental results on PDTB 2.0 and CoNLL16 datasets demonstrate that our method achieves outstanding and consistent performance against the current state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
通用话语关系识别（IDRR），即从不带显式连接的语言中推断话语关系，仍然是话语分析中的关键和挑战。现有研究通过使用注释的层次结构信息，以提高话语关系表示。然而，IDRR的性能和可靠性受到注释数据的有限性的限制。幸运的是，有大量未注释的语音句子，它们可以用于获得增强的话语关系特征。根据这种动机，我们提出了一种Prompt-based Logical Semantics Enhancement（PLSE）方法。在这种方法中，我们通过提供提示来注入语言模型中关于话语关系的知识。此外，由于提示基于连接预测表现了本地依赖关系，我们设计了一种新的自主学习目标基于互信息最大化，以提取增强的逻辑 semantics 表示。实验结果表明，我们的方法在 PDTB 2.0 和 CoNLL16 数据集上具有出色的性能和稳定性，并且与当前状态的最佳模型相比，具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Samples-Selection-for-Contrastive-Learning-Mining-of-Potential-Samples"><a href="#Rethinking-Samples-Selection-for-Contrastive-Learning-Mining-of-Potential-Samples" class="headerlink" title="Rethinking Samples Selection for Contrastive Learning: Mining of Potential Samples"></a>Rethinking Samples Selection for Contrastive Learning: Mining of Potential Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00358">http://arxiv.org/abs/2311.00358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengkui Dong, Xianzhong Long, Yun Li</li>
<li>for: 本文是关于对两个图像是否属于同一类别的预测，通过训练模型使其特征表示尽可能接近或尽可能远离。</li>
<li>methods: 我们的方法是对异常采样进行重新考虑，不同于其他方法，我们的方法更加全面，考虑了两种样本采样方式：首先，对于正样本，我们考虑了通过数据增强获得的扩展样本视图，以及通过数据挖掘获得的采样视图。然后，我们将它们weight和组合使用软和硬权重策略。其次，我们分析了负样本的梯度角度，并最终挖掘出具有中间难度的负样本，即与正样本很近的负样本。</li>
<li>results: 实验结果显示，我们的方法与一些传统自动学习方法相比有明显的优势。我们的方法在CIFAR10、CIFAR100和TinyImagenet上分别达到了88.57%、61.10%和36.69%的top-1准确率。<details>
<summary>Abstract</summary>
Contrastive learning predicts whether two images belong to the same category by training a model to make their feature representations as close or as far away as possible. In this paper, we rethink how to mine samples in contrastive learning, unlike other methods, our approach is more comprehensive, taking into account both positive and negative samples, and mining potential samples from two aspects: First, for positive samples, we consider both the augmented sample views obtained by data augmentation and the mined sample views through data mining. Then, we weight and combine them using both soft and hard weighting strategies. Second, considering the existence of uninformative negative samples and false negative samples in the negative samples, we analyze the negative samples from the gradient perspective and finally mine negative samples that are neither too hard nor too easy as potential negative samples, i.e., those negative samples that are close to positive samples. The experiments show the obvious advantages of our method compared with some traditional self-supervised methods. Our method achieves 88.57%, 61.10%, and 36.69% top-1 accuracy on CIFAR10, CIFAR100, and TinyImagenet, respectively.
</details>
<details>
<summary>摘要</summary>
对比学习预测两个图像是否属于同一类型，通过训练模型使其特征表示为close或far away。在这篇论文中，我们重新思考了如何采样对比学习中的样本，不同于其他方法，我们的方法更加全面，考虑了两个方面的样本：首先，对于正样本，我们考虑了数据增强后得到的扩展样本视图以及通过数据挖掘得到的样本视图。然后，我们将它们权重并合并使用软和硬权重策略。其次，因为负样本中存在无用的负样本和假负样本，我们从Gradient的角度分析负样本，最终挖掘出与正样本最接近的负样本，即不太困难又不太容易的负样本。实验显示，我们的方法与一些传统的自我超vised方法相比，具有显著的优势。我们的方法在CIFAR10、CIFAR100和TinyImagenet上的top-1准确率分别达88.57%、61.10%和36.69%。
</details></li>
</ul>
<hr>
<h2 id="QFree-A-Universal-Value-Function-Factorization-for-Multi-Agent-Reinforcement-Learning"><a href="#QFree-A-Universal-Value-Function-Factorization-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="QFree: A Universal Value Function Factorization for Multi-Agent Reinforcement Learning"></a>QFree: A Universal Value Function Factorization for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00356">http://arxiv.org/abs/2311.00356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rizhong Wang, Huiping Li, Di Cui, Demin Xu</li>
<li>for: 这篇论文是关于多智能体强化学习（MARL）中的中央培训，具体来说是关于在多智能体环境中提取优化的分布式策略的问题。</li>
<li>methods: 本文提出了一种名为QFree的通用价值函数分解方法，该方法基于优势函数的等价条件来保证IGM原则的满足，而不需要额外限制IGM函数的应用范围。这里使用了一种新的混合网络架构和一个新的损失函数来实现该目标。</li>
<li>results: 本文在一个非 monotonic 矩阵游戏场景中证明了QFree的效iveness，并在一个通用多智能体环境SMAC中达到了当前最佳性能。<details>
<summary>Abstract</summary>
Centralized training is widely utilized in the field of multi-agent reinforcement learning (MARL) to assure the stability of training process. Once a joint policy is obtained, it is critical to design a value function factorization method to extract optimal decentralized policies for the agents, which needs to satisfy the individual-global-max (IGM) principle. While imposing additional limitations on the IGM function class can help to meet the requirement, it comes at the cost of restricting its application to more complex multi-agent environments. In this paper, we propose QFree, a universal value function factorization method for MARL. We start by developing mathematical equivalent conditions of the IGM principle based on the advantage function, which ensures that the principle holds without any compromise, removing the conservatism of conventional methods. We then establish a more expressive mixing network architecture that can fulfill the equivalent factorization. In particular, the novel loss function is developed by considering the equivalent conditions as regularization term during policy evaluation in the MARL algorithm. Finally, the effectiveness of the proposed method is verified in a nonmonotonic matrix game scenario. Moreover, we show that QFree achieves the state-of-the-art performance in a general-purpose complex MARL benchmark environment, Starcraft Multi-Agent Challenge (SMAC).
</details>
<details>
<summary>摘要</summary>
中央化训练广泛应用于多智能机器人学习（MARL）中，以确保训练过程的稳定性。一旦获得了联合策略，则需要设计一种值函数分解方法，以EXTRACT optimal的分布式策略，这需要满足个体-全局-最大（IGM）原则。而在加入更多限制于IGM函数类型时，可以帮助满足更复杂的多智能环境的要求，但是这会增加训练更复杂的环境的难度。在这篇论文中，我们提出了QFree，一种通用的值函数分解方法 для MARL。我们首先开发了基于优势函数的数学等价条件，以确保IGM原则不受任何牺牲，从而消除了传统方法的保守性。然后，我们设计了一种更具表达能力的混合网络架构，可以满足等价分解。具体来说，我们开发了一种新的损失函数，通过在MARL算法中考虑等价条件作为正则项来评估策略。最后，我们验证了我们提出的方法的有效性，在非 monotonic 矩阵游戏场景中进行了实验，并在一个通用的复杂 MARL 环境中（Starcraft Multi-Agent Challenge，SMAC）达到了状态之 arts 的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Definition-of-Open-Ended-Learning-Problems-for-Goal-Conditioned-Agents"><a href="#A-Definition-of-Open-Ended-Learning-Problems-for-Goal-Conditioned-Agents" class="headerlink" title="A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents"></a>A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00344">http://arxiv.org/abs/2311.00344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Sigaud, Gianluca Baldassarre, Cedric Colas, Stephane Doncieux, Richard Duro, Nicolas Perrin-Gilbert, Vieri-Giuliano Santucci</li>
<li>for: 本研究旨在解释“开放式学习”这个概念，并提出一种基本的定义。</li>
<li>methods: 本研究使用了基本的概念诠释和问题定义方法，以及开放式学习问题的定义和研究。</li>
<li>results: 本研究提出了一种基于时间无穷horizon的开放式学习元素的定义，并将关注点放在开放式目标返报学习问题上。<details>
<summary>Abstract</summary>
A lot of recent machine learning research papers have "Open-ended learning" in their title. But very few of them attempt to define what they mean when using the term. Even worse, when looking more closely there seems to be no consensus on what distinguishes open-ended learning from related concepts such as continual learning, lifelong learning or autotelic learning. In this paper, we contribute to fixing this situation. After illustrating the genealogy of the concept and more recent perspectives about what it truly means, we outline that open-ended learning is generally conceived as a composite notion encompassing a set of diverse properties. In contrast with these previous approaches, we propose to isolate a key elementary property of open-ended processes, which is to always produce novel elements from time to time over an infinite horizon. From there, we build the notion of open-ended learning problems and focus in particular on the subset of open-ended goal-conditioned reinforcement learning problems, as this framework facilitates the definition of learning a growing repertoire of skills. Finally, we highlight the work that remains to be performed to fill the gap between our elementary definition and the more involved notions of open-ended learning that developmental AI researchers may have in mind.
</details>
<details>
<summary>摘要</summary>
很多 latest machine learning research papers 有 "开放式学习" 在标题中，但很少有人尝试定义这个术语的含义。甚至更糟糕，当更加仔细检查时，没有一致性可以 distinguishing open-ended learning 从related concepts such as continual learning, lifelong learning or autotelic learning。在这篇论文中，我们贡献到解决这个情况。我们首先示出了概念的家谱和更近期的思想，以及open-ended learning 的多样化特征。与之前的方法不同，我们提出了一个关键的 elementary property of open-ended processes，即在无穷远景上不断生成新的元素。从这里，我们构建了 open-ended learning 问题的概念，并特别关注 open-ended goal-conditioned reinforcement learning 问题，因为这种框架可以定义学习不断增长的技能的问题。最后，我们强调需要进一步的工作，以填充开放式学习的概念和developmental AI 研究人员可能有的更复杂的概念之间的差异。
</details></li>
</ul>
<hr>
<h2 id="MetisFL-An-Embarrassingly-Parallelized-Controller-for-Scalable-Efficient-Federated-Learning-Workflows"><a href="#MetisFL-An-Embarrassingly-Parallelized-Controller-for-Scalable-Efficient-Federated-Learning-Workflows" class="headerlink" title="MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows"></a>MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00334">http://arxiv.org/abs/2311.00334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Stripelis, Chrysovalantis Anastasiou, Patrick Toral, Armaghan Asghar, Jose Luis Ambite</li>
<li>for: 这个论文的目的是提出一种基于联合学习（Federated Learning，FL）的新系统，以便提高FL workflow的执行效率。</li>
<li>methods: 这个论文使用了一种新的系统叫做MetisFL，它重新设计了联合学习控制器的操作，以便加速大规模的FL workflow执行。</li>
<li>results: 经验测试表明，MetisFL相比其他状态对的FL系统，在各种复杂的FL workflow中可以提高执行速度10倍。<details>
<summary>Abstract</summary>
A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations conducted by the federation controller to accelerate the training of large-scale FL workflows. By quantitatively comparing MetisFL against other state-of-the-art FL systems, we empirically demonstrate that MetisFL leads to a 10-fold wall-clock time execution boost across a wide range of challenging FL workflows with increasing model sizes and federation sites.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Graph-Clustering-via-Meta-Weighting-for-Noisy-Graphs"><a href="#Robust-Graph-Clustering-via-Meta-Weighting-for-Noisy-Graphs" class="headerlink" title="Robust Graph Clustering via Meta Weighting for Noisy Graphs"></a>Robust Graph Clustering via Meta Weighting for Noisy Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00322">http://arxiv.org/abs/2311.00322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hyeonsoojo/metagc">https://github.com/hyeonsoojo/metagc</a></li>
<li>paper_authors: Hyeonsoo Jo, Fanchen Bu, Kijung Shin</li>
<li>for: 本研究旨在提出一种可靠的图 neural network（GNN）基于方法，可以在图中快速寻找有用的群集，并在实际图中进行应用。</li>
<li>methods: 本研究使用GNN来实现图 clustering，并提出了一种基于拓扑学的分解 clustering 损失函数，可以适应不同的图类型和噪声水平。此外，本研究还提出了一种名为MetaGC的方法，可以自适应地调整图元素之间的权重，以便增强GNN的抗噪声能力。</li>
<li>results: 本研究通过实验表明，MetaGC可以在五个真实的图上，即使噪声水平不同，也能够高效地寻找有用的群集，并且在不同的图类型和噪声水平下表现更好于当前state-of-the-art GNN基于方法。<details>
<summary>Abstract</summary>
How can we find meaningful clusters in a graph robustly against noise edges? Graph clustering (i.e., dividing nodes into groups of similar ones) is a fundamental problem in graph analysis with applications in various fields. Recent studies have demonstrated that graph neural network (GNN) based approaches yield promising results for graph clustering. However, we observe that their performance degenerates significantly on graphs with noise edges, which are prevalent in practice. In this work, we propose MetaGC for robust GNN-based graph clustering. MetaGC employs a decomposable clustering loss function, which can be rephrased as a sum of losses over node pairs. We add a learnable weight to each node pair, and MetaGC adaptively adjusts the weights of node pairs using meta-weighting so that the weights of meaningful node pairs increase and the weights of less-meaningful ones (e.g., noise edges) decrease. We show empirically that MetaGC learns weights as intended and consequently outperforms the state-of-the-art GNN-based competitors, even when they are equipped with separate denoising schemes, on five real-world graphs under varying levels of noise. Our code and datasets are available at https://github.com/HyeonsooJo/MetaGC.
</details>
<details>
<summary>摘要</summary>
如何在图中寻找有意义的凝集？图 clustering（即将节点分组到类似的节点集中）是图分析中的基本问题，有各种应用领域。 current studies have shown that graph neural network (GNN) based approaches have promising results for graph clustering. However, we observe that their performance degrades significantly on graphs with noise edges, which are common in practice. In this work, we propose MetaGC for robust GNN-based graph clustering. MetaGC uses a decomposable clustering loss function, which can be rephrased as the sum of losses over node pairs. We add a learnable weight to each node pair, and MetaGC adaptively adjusts the weights of node pairs using meta-weighting so that the weights of meaningful node pairs increase and the weights of less-meaningful ones (e.g., noise edges) decrease. We empirically show that MetaGC learns weights as intended and outperforms the state-of-the-art GNN-based competitors, even when they are equipped with separate denoising schemes, on five real-world graphs under varying levels of noise. Our code and datasets are available at https://github.com/HyeonsooJo/MetaGC.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Lexical-Simplification-with-Context-Augmentation"><a href="#Unsupervised-Lexical-Simplification-with-Context-Augmentation" class="headerlink" title="Unsupervised Lexical Simplification with Context Augmentation"></a>Unsupervised Lexical Simplification with Context Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00310">http://arxiv.org/abs/2311.00310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takashi Wada, Timothy Baldwin, Jey Han Lau</li>
<li>for: 这个论文是为了提出一种新的无监督词性简化方法，使用只有单语言数据和预训练语言模型。</li>
<li>methods: 该方法使用目标词和其上下文来生成替换词，并使用单语言数据中的其他上下文进行采样。</li>
<li>results: 在英语、葡萄牙语和西班牙语的 TSAR-2022 共享任务上，该模型与其他无监督系统相比显著超越，并在 Ensemble 模型中与 GPT-3.5 组合到达新的状态对。 此外，该模型在 SWORDS 词性替换数据集上得到了新的状态对。<details>
<summary>Abstract</summary>
We propose a new unsupervised lexical simplification method that uses only monolingual data and pre-trained language models. Given a target word and its context, our method generates substitutes based on the target context and also additional contexts sampled from monolingual data. We conduct experiments in English, Portuguese, and Spanish on the TSAR-2022 shared task, and show that our model substantially outperforms other unsupervised systems across all languages. We also establish a new state-of-the-art by ensembling our model with GPT-3.5. Lastly, we evaluate our model on the SWORDS lexical substitution data set, achieving a state-of-the-art result.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的无监督词法简化方法，只使用单语言数据和预训练语言模型。给定目标词和其上下文，我们的方法生成替换基于目标上下文和其他从单语言数据采集的上下文。我们在英语、葡萄牙语和西班牙语的TSAR-2022共同任务上进行了实验，并证明了我们的模型在所有语言上至关重要地超过其他无监督系统。我们还建立了一个新的状态对比，将我们的模型与GPT-3.5进行拼接，最终实现了新的状态纪录。最后，我们对SWORDS词法简化数据集进行了评估，实现了状态纪录。
</details></li>
</ul>
<hr>
<h2 id="From-Image-to-Language-A-Critical-Analysis-of-Visual-Question-Answering-VQA-Approaches-Challenges-and-Opportunities"><a href="#From-Image-to-Language-A-Critical-Analysis-of-Visual-Question-Answering-VQA-Approaches-Challenges-and-Opportunities" class="headerlink" title="From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"></a>From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00308">http://arxiv.org/abs/2311.00308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Farhan Ishmam, Md Sakib Hossain Shovon, M. F. Mridha, Nilanjan Dey</li>
<li>for: 这篇论文旨在探讨视觉问答（VQA）领域的多模态任务，包括计算机视觉（CV）和自然语言处理（NLP）等方面，并且旨在根据任何视觉输入生成问题的答案。</li>
<li>methods: 这篇论文涵盖了传统的VQA体系和当代的视语言预训练（VLP）技术，并且提供了一个详细的分类方法来描述VQA的各个方面。</li>
<li>results: 这篇论文不仅概述了VQA领域的历史和发展，还提出了一些当前的挑战和未来的可能的开放问题，以及一些与VQA相关的任务和研究方向。<details>
<summary>Abstract</summary>
The multimodal task of Visual Question Answering (VQA) encompassing elements of Computer Vision (CV) and Natural Language Processing (NLP), aims to generate answers to questions on any visual input. Over time, the scope of VQA has expanded from datasets focusing on an extensive collection of natural images to datasets featuring synthetic images, video, 3D environments, and various other visual inputs. The emergence of large pre-trained networks has shifted the early VQA approaches relying on feature extraction and fusion schemes to vision language pre-training (VLP) techniques. However, there is a lack of comprehensive surveys that encompass both traditional VQA architectures and contemporary VLP-based methods. Furthermore, the VLP challenges in the lens of VQA haven't been thoroughly explored, leaving room for potential open problems to emerge. Our work presents a survey in the domain of VQA that delves into the intricacies of VQA datasets and methods over the field's history, introduces a detailed taxonomy to categorize the facets of VQA, and highlights the recent trends, challenges, and scopes for improvement. We further generalize VQA to multimodal question answering, explore tasks related to VQA, and present a set of open problems for future investigation. The work aims to navigate both beginners and experts by shedding light on the potential avenues of research and expanding the boundaries of the field.
</details>
<details>
<summary>摘要</summary>
Visual Question Answering（VQA）是一个多Modal任务，涉及到计算机视觉（CV）和自然语言处理（NLP）领域，目的是根据任意视觉输入生成问题的答案。随着时间的推移，VQA的范围从原始的大量自然图像集Expanded to synthetic images, video, 3D environments, and other visual inputs。随着大型预训练网络的出现，早期的VQA方法从feature extraction和融合方案转移到了视语言预训练（VLP）技术。然而，没有 Thoroughly comprehensive surveys that encompass both traditional VQA architectures and contemporary VLP-based methods。此外，VLP在VQA中的挑战没有得到了充分的探讨，留下了一些未解决的问题。我们的工作提供了VQA领域的一份报告，探讨了VQA的历史、 introduce a detailed taxonomy to categorize the facets of VQA, 和 highlights the recent trends, challenges, and scopes for improvement。我们还扩展了VQA到多 modal question answering, explore related tasks, and present a set of open problems for future investigation。该工作的目的是为 Beginners and experts navigation by shedding light on potential research avenues and expanding the boundaries of the field。
</details></li>
</ul>
<hr>
<h2 id="Inference-of-CO2-flow-patterns-–-a-feasibility-study"><a href="#Inference-of-CO2-flow-patterns-–-a-feasibility-study" class="headerlink" title="Inference of CO2 flow patterns – a feasibility study"></a>Inference of CO2 flow patterns – a feasibility study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00290">http://arxiv.org/abs/2311.00290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhinav Prakash Gahlot, Huseyin Tuna Erdinc, Rafael Orozco, Ziyi Yin, Felix J. Herrmann</li>
<li>for: 这篇论文的目的是为了发展一个可靠的测量和检测方法，以掌握地下储存库中CO2泄漏的可能性，特别是通过预存或人工引起的 faults 在储存库的隔膜中。</li>
<li>methods: 这篇论文使用了 conditional normalizing flow 来检测CO2泄漏，并进行了一系列 numerical experiments 来分析其性能。</li>
<li>results: 根据 numerical experiments 的结果， conditional normalizing flow 可以生成高精度的CO2泄漏推测，并且能够检测到无法推测的泄漏。 however, the uncertainty of the inference is reasonable due to the noise in the seismic data and the lack of precise knowledge of the reservoir’s fluid flow properties.<details>
<summary>Abstract</summary>
As the global deployment of carbon capture and sequestration (CCS) technology intensifies in the fight against climate change, it becomes increasingly imperative to establish robust monitoring and detection mechanisms for potential underground CO2 leakage, particularly through pre-existing or induced faults in the storage reservoir's seals. While techniques such as history matching and time-lapse seismic monitoring of CO2 storage have been used successfully in tracking the evolution of CO2 plumes in the subsurface, these methods lack principled approaches to characterize uncertainties related to the CO2 plumes' behavior. Inclusion of systematic assessment of uncertainties is essential for risk mitigation for the following reasons: (i) CO2 plume-induced changes are small and seismic data is noisy; (ii) changes between regular and irregular (e.g., caused by leakage) flow patterns are small; and (iii) the reservoir properties that control the flow are strongly heterogeneous and typically only available as distributions. To arrive at a formulation capable of inferring flow patterns for regular and irregular flow from well and seismic data, the performance of conditional normalizing flow will be analyzed on a series of carefully designed numerical experiments. While the inferences presented are preliminary in the context of an early CO2 leakage detection system, the results do indicate that inferences with conditional normalizing flows can produce high-fidelity estimates for CO2 plumes with or without leakage. We are also confident that the inferred uncertainty is reasonable because it correlates well with the observed errors. This uncertainty stems from noise in the seismic data and from the lack of precise knowledge of the reservoir's fluid flow properties.
</details>
<details>
<summary>摘要</summary>
全球范围内碳捕捉和储存（CCS）技术的广泛部署，在抗击气候变化的斗争中，已经变得越来越重要，特别是在存储池的封闭或人工激发的承压面上实现坚实的监测和探测机制，以防止地壳下的CO2泄漏。尽管历史匹配和时间lapse声学监测CO2储存已经取得成功，但这些方法缺乏对CO2泄漏的原理风险评估。因此，在风险mitigation中，系统性的风险评估是必要的。以下是为什么：（一）CO2泄漏引起的变化小，声学数据噪音大；（二）常规和不规则（例如泄漏引起的）流程变化小；（三）储存池的物理参数控制流动强烈不均，通常仅可以获得分布型。为了到达可以从井和声学数据中推断常规和不规则流动的表达，我们将分析 conditional normalizing flow的性能。尽管这些推断是CO2泄漏检测系统的初步结果，但结果表明，使用 conditional normalizing flow可以生成高精度的CO2泄漏推断，无论是否存在泄漏。此外，我们还确信这些推断的不确定性是合理的，因为它与观测数据噪音和储存池的流体流动参数的缺乏精确知识相关。这种不确定性来源于声学数据的噪音和储存池的流体流动参数的不确定性。
</details></li>
</ul>
<hr>
<h2 id="Active-Instruction-Tuning-Improving-Cross-Task-Generalization-by-Training-on-Prompt-Sensitive-Tasks"><a href="#Active-Instruction-Tuning-Improving-Cross-Task-Generalization-by-Training-on-Prompt-Sensitive-Tasks" class="headerlink" title="Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks"></a>Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00288">http://arxiv.org/abs/2311.00288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pluslabnlp/active-it">https://github.com/pluslabnlp/active-it</a></li>
<li>paper_authors: Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, Nanyun Peng</li>
<li>for: 提高自然语言处理器（NLP）模型的适应性和泛化能力，通过训练大量多样化任务中的指令。</li>
<li>methods: 基于提示不确定性的活动指令调整方法，可以快速和效率地选择有优势的任务，以提高模型的表现和泛化能力。</li>
<li>results: 在 NIV2 和 Self-Instruct 数据集上进行实验，与其他基eline策略相比，我们的方法可以具有更好的 OUT-OF- Distribution 泛化性，并且需要训练 fewer 任务。此外，我们还提出了一个任务地图，可以根据提示不确定性和预测概率来分类和诊断任务。<details>
<summary>Abstract</summary>
Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and prediction probability. We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit, underscoring the importance of task selection for instruction tuning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Knowledge-Infused-Prompting-Assessing-and-Advancing-Clinical-Text-Data-Generation-with-Large-Language-Models"><a href="#Knowledge-Infused-Prompting-Assessing-and-Advancing-Clinical-Text-Data-Generation-with-Large-Language-Models" class="headerlink" title="Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models"></a>Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00287">http://arxiv.org/abs/2311.00287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, Wei Jin, Joyce Ho, Carl Yang</li>
<li>for: 这个论文旨在提出一种可以解决专业医学语言处理领域中的特殊挑战，例如专业医学术语和临床观点的方法。</li>
<li>methods: 这篇论文使用了大型自然语言模型（LLM）来解决这些挑战。然而，直接使用LLM可能会带来隐私问题和资源问题。因此，作者们提出了一种创新的、资源有效的方法，即ClinGen。</li>
<li>results: 作者们的实验表明，ClinGen可以对7种临床自然语言处理任务和16个数据集进行改进表现，并将临床主题和写作风格从外部领域专门知识库和LLM中引入到数据生成过程中。这些生成的训练例子具有更高的多样性和更好的适应能力。<details>
<summary>Abstract</summary>
Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the diversity of generated training instances. We will publish our code and all the generated data in \url{https://github.com/ritaranx/ClinGen}.
</details>
<details>
<summary>摘要</summary>
临床自然语言处理需要特有的方法来处理医疗领域特有的挑战，如复杂的医疗术语和临床背景。最近，大型自然语言模型（LLM）已经在这个领域表现了承诺。然而，直接部署LLM可能会导致隐私问题并受到资源限制。为了解决这个挑战，我们开发了一种创新的资源效率高的方法，名为ClinGen。我们的模型包括临床知识EXTRACT和上下文知识支持的LLM唤醒。两者都来自于外部医疗领域特定的知识图和LLM，以引导数据生成。我们的广泛的实验研究 across 7 临床自然语言处理任务和 16 个数据集表明，ClinGen能够在不同任务中均提高性能，并准确地对各个实际数据集进行分布对应。我们将在 \url{https://github.com/ritaranx/ClinGen} 上发布我们的代码和所有生成的数据。
</details></li>
</ul>
<hr>
<h2 id="JADE-A-Linguistic-based-Safety-Evaluation-Platform-for-LLM"><a href="#JADE-A-Linguistic-based-Safety-Evaluation-Platform-for-LLM" class="headerlink" title="JADE: A Linguistic-based Safety Evaluation Platform for LLM"></a>JADE: A Linguistic-based Safety Evaluation Platform for LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00286">http://arxiv.org/abs/2311.00286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mi Zhang, Xudong Pan, Min Yang</li>
<li>for: 这个论文旨在提出一种名为“JADE”的语言模型攻击平台，用于同时破坏多种广泛使用的中文和英文语言模型（CLMs）。</li>
<li>methods: JADE使用词法生成和转换规则来增加语句结构的复杂性，直到破坏CLMs的安全保障。</li>
<li>results: JADE在8种开源中文CLMs、6种商业中文CLMs和4种商业英文CLMs上 simultanously破坏了大量CLMs，得到了高达70%的不安全生成率（请参见下面的表），而且这些问题仍然是自然、流畅的语言表达。<details>
<summary>Abstract</summary>
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.   \textit{JADE} is based on Noam Chomsky's seminal theory of transformational-generative grammar. Given a seed question with unsafe intention, \textit{JADE} invokes a sequence of generative and transformational rules to increment the complexity of the syntactic structure of the original question, until the safety guardrail is broken. Our key insight is: Due to the complexity of human language, most of the current best LLMs can hardly recognize the invariant evil from the infinite number of different syntactic structures which form an unbound example space that can never be fully covered. Technically, the generative/transformative rules are constructed by native speakers of the languages, and, once developed, can be used to automatically grow and transform the parse tree of a given question, until the guardrail is broken. For more evaluation results and demo, please check our website: https://whitzard-ai.github.io/jade.html.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一个名为\textit{JADE}的目标语言杂化平台，用于同时、一致地击碎广泛使用的多种中文、英文语言模型（LLMs）。\textit{JADE} 使用诸如词汇、句子结构等语言特征来增加语言模型的复杂性，以生成高危害性询问，让模型同时出现多种 unsafe 行为。我们为三类 LLMS 生成了三个安全基准，包括八种开源中文、六种商用中文以及四种商用英文 LLMS。这些基准包含了高危害性的询问，可以同时触发多种 LLMS 的危险生成，其中 unsafe 生成率平均为 \textbf{70%}（请参考下面的表），但是这些询问仍然是自然、流畅的、保留了核心 unsafe  semantics。我们在以下链接中发布了对商用英文 LLMS 和开源英文 LLMS 的示例生成：https://github.com/whitzard-ai/jade-db。对于更多由 \textit{JADE} 生成的询问 interested readers，请与我们联系。\textit{JADE} 基于诺曼·舍契克的 transformational-generative grammar 理论。给定一个带有危险意图的种子问题，\textit{JADE} 采用一系列生成和转换规则，逐渐增加种子问题的语法结构复杂度，直到安全防护墙被破坏。我们的关键发现是：由于人类语言的复杂性，当前最佳 LLMS 很难从无穷多种不同的语法结构中识别人类语言的恶势力。技术上，生成/转换规则由本地语言专家构建，并一旦开发，可以自动增长和转换给定问题的 parse tree，直到安全防护墙被破坏。更多评估结果和示例，请查看我们的网站：https://whitzard-ai.github.io/jade.html。
</details></li>
</ul>
<hr>
<h2 id="Re-Scoring-Using-Image-Language-Similarity-for-Few-Shot-Object-Detection"><a href="#Re-Scoring-Using-Image-Language-Similarity-for-Few-Shot-Object-Detection" class="headerlink" title="Re-Scoring Using Image-Language Similarity for Few-Shot Object Detection"></a>Re-Scoring Using Image-Language Similarity for Few-Shot Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00278">http://arxiv.org/abs/2311.00278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min Jae Jung, Seung Dae Han, Joohee Kim</li>
<li>for: 这个论文主要关注在几shot object detection领域，探讨如何对于新的物品进行检测，并且使用几少标签进行标识。</li>
<li>methods: 本论文提出了一个名为RISF（Re-scoring using Image-language Similarity for Few-shot object detection）的方法，它将Faster R-CNN扩展为引入Calibration Module using CLIP（CM-CLIP）和Background Negative Re-scale Loss（BNRL），以改善几shot object detection的性能。</li>
<li>results: 实验结果显示，RISF方法在MS-COCO和PASCAL VOC等 dataset上实现了与现有方法相比的优秀性能，并且超越了现有的方法。<details>
<summary>Abstract</summary>
Few-shot object detection, which focuses on detecting novel objects with few labels, is an emerging challenge in the community. Recent studies show that adapting a pre-trained model or modified loss function can improve performance. In this paper, we explore leveraging the power of Contrastive Language-Image Pre-training (CLIP) and hard negative classification loss in low data setting. Specifically, we propose Re-scoring using Image-language Similarity for Few-shot object detection (RISF) which extends Faster R-CNN by introducing Calibration Module using CLIP (CM-CLIP) and Background Negative Re-scale Loss (BNRL). The former adapts CLIP, which performs zero-shot classification, to re-score the classification scores of a detector using image-class similarities, the latter is modified classification loss considering the punishment for fake backgrounds as well as confusing categories on a generalized few-shot object detection dataset. Extensive experiments on MS-COCO and PASCAL VOC show that the proposed RISF substantially outperforms the state-of-the-art approaches. The code will be available.
</details>
<details>
<summary>摘要</summary>
几shot对象检测，关注使用少量标签检测新对象，是当前社区的一个emerging挑战。Recent studies show that modifying a pre-trained model or loss function can improve performance. 在这篇论文中，我们探索利用Contrastive Language-Image Pre-training (CLIP) 和困难的负类别损失来提高在低数据设置下的性能。我们提出了使用Image-language Similarity for Few-shot object detection (RISF)，它是一种基于Faster R-CNN的扩展，包括Calibration Module using CLIP (CM-CLIP) 和Background Negative Re-scale Loss (BNRL)。前者利用CLIP，可以在零shot类别上进行分类，对检测器的分类分数进行重新评分，而后者是一种修改的分类损失函数，考虑了虚假背景以及混淆类别的罚款。在MS-COCO和PASCAL VOC上进行了广泛的实验，结果表明，提出的RISF在状态之前的方法中具有显著的优势。代码将被公开。
</details></li>
</ul>
<hr>
<h2 id="ChatCoder-Chat-based-Refine-Requirement-Improves-LLMs’-Code-Generation"><a href="#ChatCoder-Chat-based-Refine-Requirement-Improves-LLMs’-Code-Generation" class="headerlink" title="ChatCoder: Chat-based Refine Requirement Improves LLMs’ Code Generation"></a>ChatCoder: Chat-based Refine Requirement Improves LLMs’ Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00272">http://arxiv.org/abs/2311.00272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zejun Wang, Jia Li, Ge Li, Zhi Jin</li>
<li>for: 提高大型自然语言处理模型对人类需求的理解和代码生成性能。</li>
<li>methods: 提出了一种叫做ChatCoder的方法，通过与大型自然语言处理模型进行聊天来使人类需求更加精确、不ambiguous和完整。</li>
<li>results: 实验表明，使用ChatCoder可以大幅提高现有大型自然语言处理模型的性能，并且比较于修改基于方法和人类回应进行 Fine-tuning 的方法更有优势。<details>
<summary>Abstract</summary>
Large language models have shown good performances in generating code to meet human requirements. However, human requirements expressed in natural languages can be vague, incomplete, and ambiguous, leading large language models to misunderstand human requirements and make mistakes. Worse, it is difficult for a human user to refine the requirement. To help human users refine their requirements and improve large language models' code generation performances, we propose ChatCoder: a method to refine the requirements via chatting with large language models. We design a chat scheme in which the large language models will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before. Experiments show that ChatCoder has improved existing large language models' performance by a large margin. Besides, ChatCoder has the advantage over refine-based methods and LLMs fine-tuned via human response.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型已经表现出良好的代码生成能力，但是人类需求表达在自然语言中可能是模糊不清、缺失部分和抽象的，导致大型自然语言模型会错误理解人类需求，生成错误代码。 worse, it is difficult for a human user to refine the requirement. 为了帮助人类用户更正他们的需求和提高大型自然语言模型的代码生成性能，我们提出了 ChatCoder：一种通过与大型自然语言模型聊天来修正需求的方法。我们设计了一种聊天方案，在which the large language models will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before. 实验显示，ChatCoder 已经提高了现有的大型自然语言模型性能，并且具有与修改基于方法和 LLMS 细化 via human response 的优势。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Decision-Transformer-via-Hierarchical-Reinforcement-Learning"><a href="#Rethinking-Decision-Transformer-via-Hierarchical-Reinforcement-Learning" class="headerlink" title="Rethinking Decision Transformer via Hierarchical Reinforcement Learning"></a>Rethinking Decision Transformer via Hierarchical Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00267">http://arxiv.org/abs/2311.00267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Ma, Chenjun Xiao, Hebin Liang, Jianye Hao</li>
<li>for: 这个论文旨在探讨决策变换器（DT）在强化学习中的应用，以及如何使用高级和低级政策来实现适用性的拼接。</li>
<li>methods: 该论文提出了一种基于层次RL的批处理模型框架，其中高级政策在做出决策时提出一个理想的提示，而低级政策则根据提示生成动作。</li>
<li>results: 实验结果显示，提出的算法在多个控制和导航 benchmark 上表现出色，大大超过了DT。<details>
<summary>Abstract</summary>
Decision Transformer (DT) is an innovative algorithm leveraging recent advances of the transformer architecture in reinforcement learning (RL). However, a notable limitation of DT is its reliance on recalling trajectories from datasets, losing the capability to seamlessly stitch sub-optimal trajectories together. In this work we introduce a general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL. At the time of making decisions, a high-level policy first proposes an ideal prompt for the current state, a low-level policy subsequently generates an action conditioned on the given prompt. We show DT emerges as a special case of this framework with certain choices of high-level and low-level policies, and discuss the potential failure of these choices. Inspired by these observations, we study how to jointly optimize the high-level and low-level policies to enable the stitching ability, which further leads to the development of new offline RL algorithms. Our empirical results clearly show that the proposed algorithms significantly surpass DT on several control and navigation benchmarks. We hope our contributions can inspire the integration of transformer architectures within the field of RL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Plug-and-Play-Policy-Planner-for-Large-Language-Model-Powered-Dialogue-Agents"><a href="#Plug-and-Play-Policy-Planner-for-Large-Language-Model-Powered-Dialogue-Agents" class="headerlink" title="Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents"></a>Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00262">http://arxiv.org/abs/2311.00262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Deng, Wenxuan Zhang, Wai Lam, See-Kiong Ng, Tat-Seng Chua</li>
<li>for: 提高语言模型（LLM）的对话活跃性，即使在对话政策规划方面存在挑战。</li>
<li>methods: 我们提出了一种新的对话政策规划方法，称为PPDPP，它使用可调整的语言模型插件作为对话政策规划器。我们还开发了一个新的训练框架，以便在可用的人类标注数据上进行经过监督的精度调整，以及基于目标帮助AI反馈的强化学习。</li>
<li>results: PPDPP在三种不同的主动对话应用程序上取得了卓越的表现，包括谈判、情感支持和教学对话。对比 existed 方法，PPDPP 能够持续和substantially 提高对话系统的政策规划能力。<details>
<summary>Abstract</summary>
Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese大型语言模型（LLM）的对话政策规划是一个实用又挑战的对话问题，对话政策规划是提高LLM的对话性能的关键。现有的研究通过不同的提示方案或逐步增强LLM的对话政策规划能力，但这些方法都受到对话政策规划的能力限制或困难将其转移到新的情况下。在这个研究中，我们将引入一个新的对话政策规划方法，以便LLM在进行主动对话问题时能够更好地策略。具体来说，我们开发了一个新的训练框架，以便在可用的人类标注数据上进行监督精致训练，以及从目标导向的AI反馈中获得动力学学习。这样，LLM对话代理人不仅能够在训练后应用到不同的情况，而且能够适用于不同的应用程序，只需要替代学习的插件。此外，我们建议评估对话系统的政策规划能力在互动设定下。实验结果显示，PPDPP与现有方法相比，在三个不同的主动对话应用中具有显著的性能优势。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="Implicit-biases-in-multitask-and-continual-learning-from-a-backward-error-analysis-perspective"><a href="#Implicit-biases-in-multitask-and-continual-learning-from-a-backward-error-analysis-perspective" class="headerlink" title="Implicit biases in multitask and continual learning from a backward error analysis perspective"></a>Implicit biases in multitask and continual learning from a backward error analysis perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00235">http://arxiv.org/abs/2311.00235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benoit Dherin</li>
<li>for: 这 paper 描述了使用 backward error analysis 计算深度学习模型在多任务和继续学习设置中的隐式培训偏好。</li>
<li>methods: 这 paper 使用了 Stochastic Gradient Descent (SGD) 进行训练，并 derive 了一些修改后的损失函数，以便在训练中隐式地减少模型的偏好。这些损失函数包括原始损失、抽象趋向正则化项和对抗项。</li>
<li>results: 这 paper 得到了一些有趣的结果，包括在多任务和继续学习设置中，模型的隐式培训偏好会导致模型在某些情况下偏离原始损失函数的目标。此外，paper 还提出了一种基于 Lie браcket 的新方法来解决这些问题。<details>
<summary>Abstract</summary>
Using backward error analysis, we compute implicit training biases in multitask and continual learning settings for neural networks trained with stochastic gradient descent. In particular, we derive modified losses that are implicitly minimized during training. They have three terms: the original loss, accounting for convergence, an implicit flatness regularization term proportional to the learning rate, and a last term, the conflict term, which can theoretically be detrimental to both convergence and implicit regularization. In multitask, the conflict term is a well-known quantity, measuring the gradient alignment between the tasks, while in continual learning the conflict term is a new quantity in deep learning optimization, although a basic tool in differential geometry: The Lie bracket between the task gradients.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)使用反向错误分析，我们计算了多任务和继续学习中神经网络在使用柯布随机梯度下逐步训练时的隐式偏见。特别是，我们得到了修改后的损失函数，其中包括原始损失、 converge 的损失、隐式平滑化正则化项和对tasks的斜率的斜率对应的对抗项。在多任务中，对抗项是已知的量，测量任务的梯度对齐度，而在继续学习中，对抗项是深度学习优化中的新量，它是 diferencial geometry 中的 Lie 括趟。
</details></li>
</ul>
<hr>
<h2 id="StableFDG-Style-and-Attention-Based-Learning-for-Federated-Domain-Generalization"><a href="#StableFDG-Style-and-Attention-Based-Learning-for-Federated-Domain-Generalization" class="headerlink" title="StableFDG: Style and Attention Based Learning for Federated Domain Generalization"></a>StableFDG: Style and Attention Based Learning for Federated Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00227">http://arxiv.org/abs/2311.00227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jungwuk Park, Dong-Jun Han, Jinho Kim, Shiqiang Wang, Christopher G. Brinton, Jaekyun Moon</li>
<li>for: 提高 federated learning 中的领域泛化能力，即在多个不同领域的数据上训练模型，并在新的领域上进行测试和应用。</li>
<li>methods: 提出了一种基于样式和注意力的学习策略，即样式基本学习和注意力强调方法，以便在数据穿梭和数据缺乏的情况下提高领域泛化能力。</li>
<li>results: 实验结果表明，相比现有基elines，StableFDG在多个领域泛化标准 benchmark 上表现出色，demonstrating its efficacy。<details>
<summary>Abstract</summary>
Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client's local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.
</details>
<details>
<summary>摘要</summary>
传统的联合学习（FL）算法假设训练（源频谱）和测试（目标频谱）数据分布相同。然而，在实际应用中，频谱转换经常发生，这导致了 equip FL 方法具备频谱泛化（DG）能力的需求。然而，现有的 DG 算法在 FL 设置中存在fundamental 挑战，因为每个客户端的本地数据集中缺乏样本/频谱。在这篇论文中，我们提出了StableFDG，一种风格和注意力基于的学习策略，用于实现联合频谱泛化。我们的两个贡献是：1. 风格学习，允许每个客户端在本地数据集中探索新的风格，提高频谱多样性基于我们提议的风格分享、转移和探索策略。2. 注意力基于的特征强调器，可以捕捉数据amples中相同类型的特征之间的相似性，并强调重要/共同特征，以更好地学习数据穷FL场景中的频谱不变特征。我们的实验结果表明，StableFDG 超过了现有的基准点，在多个 DG benchmark 数据集上表现出色，证明其效果。
</details></li>
</ul>
<hr>
<h2 id="Domain-decomposition-based-coupling-of-physics-informed-neural-networks-via-the-Schwarz-alternating-method"><a href="#Domain-decomposition-based-coupling-of-physics-informed-neural-networks-via-the-Schwarz-alternating-method" class="headerlink" title="Domain decomposition-based coupling of physics-informed neural networks via the Schwarz alternating method"></a>Domain decomposition-based coupling of physics-informed neural networks via the Schwarz alternating method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00224">http://arxiv.org/abs/2311.00224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Will Snyder, Irina Tezaur, Christopher Wentland</li>
<li>for: 这个论文主要是研究如何使用物理约束的神经网络（PINN）解决非线性偏微分方程（PDE），并与传统的神经网络（NN）相比，PINN可以在解决难题上表现出更好的性能。</li>
<li>methods: 这篇论文使用的方法是将PINN分解成多个子域PINN，并使用Schwarz alternate方法来将这些子域PINN相互连接。在这个过程中， authors investigate了不同的 Dirichlet边界条件的实现方式，以提高PINN的训练效率。</li>
<li>results: 研究发现，在高Peclet数下，使用Schwarz alternate方法可以大幅提高PINN的训练效率，并且可以在一定程度上改善PINN的性能。然而，authors还发现，在某些情况下，强制实施Schwarz边界条件并不一定能够加速PINN的训练。<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) are appealing data-driven tools for solving and inferring solutions to nonlinear partial differential equations (PDEs). Unlike traditional neural networks (NNs), which train only on solution data, a PINN incorporates a PDE's residual into its loss function and trains to minimize the said residual at a set of collocation points in the solution domain. This paper explores the use of the Schwarz alternating method as a means to couple PINNs with each other and with conventional numerical models (i.e., full order models, or FOMs, obtained via the finite element, finite difference or finite volume methods) following a decomposition of the physical domain. It is well-known that training a PINN can be difficult when the PDE solution has steep gradients. We investigate herein the use of domain decomposition and the Schwarz alternating method as a means to accelerate the PINN training phase. Within this context, we explore different approaches for imposing Dirichlet boundary conditions within each subdomain PINN: weakly through the loss and/or strongly through a solution transformation. As a numerical example, we consider the one-dimensional steady state advection-diffusion equation in the advection-dominated (high Peclet) regime. Our results suggest that the convergence of the Schwarz method is strongly linked to the choice of boundary condition implementation within the PINNs being coupled. Surprisingly, strong enforcement of the Schwarz boundary conditions does not always lead to a faster convergence of the method. While it is not clear from our preliminary study that the PINN-PINN coupling via the Schwarz alternating method accelerates PINN convergence in the advection-dominated regime, it reveals that PINN training can be improved substantially for Peclet numbers as high as 1e6 by performing a PINN-FOM coupling.
</details>
<details>
<summary>摘要</summary>
物理学 Informed neural networks (PINNs) 是一种有appeal的数据驱动工具，用于解决和推算非线性偏微分方程（PDEs）的解。与传统的神经网络（NNs）不同，PINNs在训练过程中不仅学习解数据，还包含PDE的剩余在损失函数中，并在一组嵌入点上进行训练，以确定解的最佳值。本文研究了使用Schwarz分解法将PINNs与其他PINNs和传统的数值模型（例如，基于finite element、finite difference或finite volume方法获得的全序模型，FOMs）集成，以便解决物理领域的问题。尽管训练PINNs可能在PDE解有急剧的梯度时具有挑战，但我们在这种情况下研究了使用域 decompositions和Schwarz分解法来加速PINNs训练阶段。我们还研究了在每个子域PINN中如何强制实施 Dirichlet 边界条件：通过损失函数和/或解转换。作为数字例子，我们考虑了一个一维稳定状态液体运动-扩散方程在扩散主导（高Peclet） режим下。我们的结果表明，Schwarz法的收敛与PINNs之间的边界条件实现方式有着强关系。虽然强制实施Schwarz边界条件可能不一定加速方法的收敛，但我们发现，在Peclet数为1e6的情况下，通过PINN-FOM coupling可以提高PINN训练的效果。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Capture-Public-Opinion-about-Global-Warming-An-Empirical-Assessment-of-Algorithmic-Fidelity-and-Bias"><a href="#Can-Large-Language-Models-Capture-Public-Opinion-about-Global-Warming-An-Empirical-Assessment-of-Algorithmic-Fidelity-and-Bias" class="headerlink" title="Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias"></a>Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00217">http://arxiv.org/abs/2311.00217</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Lee, T. Q. Peng, M. H. Goldberg, S. A. Rosenthal, J. E. Kotcher, E. W. Maibach, A. Leiserowitz</li>
<li>for: 这个研究用于评估大语言模型（LLM）在社会科学研究中的算法准确性和偏见。</li>
<li>methods: 该研究使用了两份国家代表性气候变化调查来使用LLM模拟问卷回答。LLM被定制了以 Conditioned on demographics和&#x2F;或心理covariates来模拟调查回答。</li>
<li>results: 研究发现LLM可以有效地捕捉总统选举行为，但在不包含相关covariates时难以准确表达全球暖化观点。GPT-4在包含 both demographics和covariates时表现出了改进的性能。然而，在某些群体的观点上存在偏见，LLMs tend to underestimate黑美国人对全球暖化的担忧。这些结果 highlights the potential of LLMs in social science research, but also underscores the importance of careful conditioning, model selection, survey question format, and bias assessment when using LLMs for survey simulation.<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated their potential in social science research by emulating human perceptions and behaviors, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs by utilizing two nationally representative climate change surveys. The LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included. GPT-4 exhibits improved performance when conditioned on both demographics and covariates. However, disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of meticulous conditioning, model selection, survey question format, and bias assessment when employing LLMs for survey simulation. Further investigation into prompt engineering and algorithm auditing is essential to harness the power of LLMs while addressing their inherent limitations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Consistent-Video-to-Video-Transfer-Using-Synthetic-Dataset"><a href="#Consistent-Video-to-Video-Transfer-Using-Synthetic-Dataset" class="headerlink" title="Consistent Video-to-Video Transfer Using Synthetic Dataset"></a>Consistent Video-to-Video Transfer Using Synthetic Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00213">http://arxiv.org/abs/2311.00213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Cheng, Tianjun Xiao, Tong He</li>
<li>for: 文章 targets 文本基于视频编辑 tasks, eliminating the need for resource-intensive per-video-per-model fine-tuning.</li>
<li>methods: 方法基于一个 Synthetic Paired Video Dataset  tailored for video-to-video transfer tasks, inspired by Instruct Pix2Pix’s image transfer via editing instruction.</li>
<li>results: 方法 surpasses current methods like Tune-A-Video, marking significant progress in text-based video-to-video editing and opening up exciting avenues for further exploration and deployment.<details>
<summary>Abstract</summary>
We introduce a novel and efficient approach for text-based video-to-video editing that eliminates the need for resource-intensive per-video-per-model finetuning. At the core of our approach is a synthetic paired video dataset tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix's image transfer via editing instruction, we adapt this paradigm to the video domain. Extending the Prompt-to-Prompt to videos, we efficiently generate paired samples, each with an input video and its edited counterpart. Alongside this, we introduce the Long Video Sampling Correction during sampling, ensuring consistent long videos across batches. Our method surpasses current methods like Tune-A-Video, heralding substantial progress in text-based video-to-video editing and suggesting exciting avenues for further exploration and deployment.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的和高效的文本基于 видео editing 方法，该方法可以消除需要资源充足的每个视频进行模型 fine-tuning。我们的方法的核心是一个人工制作的对应视频集，这个集合是为视频转换任务设计的。以 Instruct Pix2Pix 的图像转换为例，我们将该思想应用到视频领域。我们在 Prompt-to-Prompt 上扩展了这个思想，以生成对应的视频对，每个对包含一个输入视频和其编辑后的对应视频。此外，我们还引入了长视频抽样正确，以确保批处中的视频长度相同。我们的方法超越了现有的方法，如 Tune-A-Video，表明了文本基于 видео editing 方法的重要进步，并开示了更多的探索和应用的可能性。
</details></li>
</ul>
<hr>
<h2 id="Magmaw-Modality-Agnostic-Adversarial-Attacks-on-Machine-Learning-Based-Wireless-Communication-Systems"><a href="#Magmaw-Modality-Agnostic-Adversarial-Attacks-on-Machine-Learning-Based-Wireless-Communication-Systems" class="headerlink" title="Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems"></a>Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00207">http://arxiv.org/abs/2311.00207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jung-Woo Chang, Ke Sun, Nasimeh Heydaribeni, Seira Hidano, Xinyu Zhang, Farinaz Koushanfar</li>
<li>for: 这篇论文旨在描述一种黑盒子攻击方法，可以对任何多Modal信号通过无线通信频率进行攻击。</li>
<li>methods: 该方法使用了多Modal的源数据、通用物理层组件和无线频率约束，并提出了新的攻击目标。</li>
<li>results: 实验结果表明，该攻击方法能够在存在防御机制的情况下引起重大性能下降，并且能够攻击加密通信频率和传统通信频率。<details>
<summary>Abstract</summary>
Machine Learning (ML) has been instrumental in enabling joint transceiver optimization by merging all physical layer blocks of the end-to-end wireless communication systems. Although there have been a number of adversarial attacks on ML-based wireless systems, the existing methods do not provide a comprehensive view including multi-modality of the source data, common physical layer components, and wireless domain constraints. This paper proposes Magmaw, the first black-box attack methodology capable of generating universal adversarial perturbations for any multimodal signal transmitted over a wireless channel. We further introduce new objectives for adversarial attacks on ML-based downstream applications. The resilience of the attack to the existing widely used defense methods of adversarial training and perturbation signal subtraction is experimentally verified. For proof-of-concept evaluation, we build a real-time wireless attack platform using a software-defined radio system. Experimental results demonstrate that Magmaw causes significant performance degradation even in the presence of the defense mechanisms. Surprisingly, Magmaw is also effective against encrypted communication channels and conventional communications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ChatGPT-Powered-Hierarchical-Comparisons-for-Image-Classification"><a href="#ChatGPT-Powered-Hierarchical-Comparisons-for-Image-Classification" class="headerlink" title="ChatGPT-Powered Hierarchical Comparisons for Image Classification"></a>ChatGPT-Powered Hierarchical Comparisons for Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00206">http://arxiv.org/abs/2311.00206</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiyuan-r/chatgpt-powered-hierarchical-comparisons-for-image-classification">https://github.com/zhiyuan-r/chatgpt-powered-hierarchical-comparisons-for-image-classification</a></li>
<li>paper_authors: Zhiyuan Ren, Yiyang Su, Xiaoming Liu</li>
<li>for: 解决零示open-vocabulary图像分类挑战，使用预训练视觉语言模型CLIP，并借鉴大语言模型ChatGPT中的类别专业知识。</li>
<li>methods: 使用大语言模型Recursive Hierarchical Embeddings（RHE），将类别组织成层次结构，并使用这些层次结构来对图像和文本嵌入进行比较，从而实现有效、可解释的图像分类方法。</li>
<li>results: 提出了一种新的图像分类框架，可以快速、精准地分类图像，同时也可以提供有用的解释性结果。<details>
<summary>Abstract</summary>
The zero-shot open-vocabulary challenge in image classification is tackled by pretrained vision-language models like CLIP, which benefit from incorporating class-specific knowledge from large language models (LLMs) like ChatGPT. However, biases in CLIP lead to similar descriptions for distinct but related classes, prompting our novel image classification framework via hierarchical comparisons: using LLMs to recursively group classes into hierarchies and classifying images by comparing image-text embeddings at each hierarchy level, resulting in an intuitive, effective, and explainable approach.
</details>
<details>
<summary>摘要</summary>
CLIP等预训练视觉语言模型在无训练数据的开放词汇挑战中表现出色，具有将类pecific知识从大型语言模型（LLM）如ChatGPT中吸收的优点。然而，CLIP中的偏见导致相似的描述对不同 yet related的类型进行分类，因此我们提出了一种新的图像分类框架：通过使用LLM进行层次分组，将类型分为层次结构，并在每个层次结构中使用图像文本嵌入对比，从而实现intuitive、有效和可解释的方法。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Training-and-Fine-tuning-for-Domain-Specific-Language-Models-in-Medical-Question-Answering"><a href="#Continuous-Training-and-Fine-tuning-for-Domain-Specific-Language-Models-in-Medical-Question-Answering" class="headerlink" title="Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering"></a>Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00204">http://arxiv.org/abs/2311.00204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Guo, Yining Hua</li>
<li>for: 这个研究旨在使用连续训练和指导精度调整，将Llama 2 基本模型 rápidamente适应中文医学领域。</li>
<li>methods: 首先，通过1B个中文医学引用文本进行连续训练，教育模型学习相关词汇和知识。然后，通过54K个中国医学资格考试例题进行精度调整。</li>
<li>results: 实验表明，这种方法具有效果，可以生成与 GPT-3.5-turbo 相比的模型，而使用的计算资源却很少。这种领域专业的模型可以用于多种中文医学应用。此外，这种方法可以应用于法律、科学和工程等领域， где预训练模型缺乏专业知识。<details>
<summary>Abstract</summary>
Large language models exhibit promising general capabilities but often lack specialized knowledge for domain-specific tasks. Developing domain experts from a base model enables a range of applications without prohibitive training costs. This work demonstrates a method using continuous training and instruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese medical domain. We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge. The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination. Experiments on Chinese medical data confirm the effectiveness of this approach, producing a model comparable to GPT-3.5-turbo while using way less computational resource. The resulting domain-specific model could be useful for various Chinese medical applications. More broadly, this provides a template for domain-specific training of large language models in areas where pre-trained models lack the required expertise, such as law, science, and engineering.
</details>
<details>
<summary>摘要</summary>
大型语言模型具有优异的通用能力，但经常缺乏域专知 для域特定任务。从基本模型开发域专家可以快速地应用到各种应用程序而无需昂贵的训练成本。本研究展示了一种使用连续训练和指导练习精度调整的方法，以快速地适应基于中文医学文献的Llama 2基本模型。我们首先通过10亿个中文医学Token进行连续训练，教导模型应用相关词汇和知识。然后，我们对5.4万个中文医学试题进行精度调整。实验表明，这种方法具有效果，可以生成与GPT-3.5-turbo相当的模型，使用的计算资源减少了很多。得到的域专门模型可以用于多种中文医学应用程序。更广泛地说，这提供了领域专门训练大语言模型的模板，可以应用于没有相关知识的领域，如法律、科学和工程。
</details></li>
</ul>
<hr>
<h2 id="Modeling-subjectivity-by-Mimicking-Annotator-Annotation-in-toxic-comment-identification-across-diverse-communities"><a href="#Modeling-subjectivity-by-Mimicking-Annotator-Annotation-in-toxic-comment-identification-across-diverse-communities" class="headerlink" title="Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities"></a>Modeling subjectivity (by Mimicking Annotator Annotation) in toxic comment identification across diverse communities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00203">http://arxiv.org/abs/2311.00203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Senjuti Dutta, Sid Mittal, Sherol Chen, Deepak Ramachandran, Ravi Rajakumar, Ian Kivlichan, Sunny Mak, Alena Butryna, Praveen Paritosh</li>
<li>for: 本研究旨在提高自动化审核系统的可靠性，帮助减少人工审核的依赖。</li>
<li>methods: 本研究使用了专家标注者的注释，并使用了三个公共数据集来评估模型的多元性。</li>
<li>results: 研究发现，不同群体的专家标注者之间存在主观差异，这表明大量投票的方法存在缺陷。因此，将主观标注作为训练数据的真实标注值是不可靠的。<details>
<summary>Abstract</summary>
The prevalence and impact of toxic discussions online have made content moderation crucial.Automated systems can play a vital role in identifying toxicity, and reducing the reliance on human moderation.Nevertheless, identifying toxic comments for diverse communities continues to present challenges that are addressed in this paper.The two-part goal of this study is to(1)identify intuitive variances from annotator disagreement using quantitative analysis and (2)model the subjectivity of these viewpoints.To achieve our goal, we published a new dataset\footnote{\url{https://github.com/XXX} with expert annotators' annotations and used two other public datasets to identify the subjectivity of toxicity.Then leveraging the Large Language Model(LLM),we evaluate the model's ability to mimic diverse viewpoints on toxicity by varying size of the training data and utilizing same set of annotators as the test set used during model training and a separate set of annotators as the test set.We conclude that subjectivity is evident across all annotator groups, demonstrating the shortcomings of majority-rule voting. Moving forward, subjective annotations should serve as ground truth labels for training models for domains like toxicity in diverse communities.
</details>
<details>
<summary>摘要</summary>
“在线上的问题性讨论日益普遍和影响力大，内容审核已成为不可或缺的。自动化系统可以扮演重要的角色，识别问题性并减少人工审核的需求。然而，识别多元社区中的问题评价仍然存在挑战，这个研究将解决这个问题。我们的研究目标是使用量化分析来识别审核者之间的不同意见，并模型这些意见的主观性。我们发布了一个新的数据集\footnotemark[1]，并使用两个公共的数据集来识别问题的主观性。我们使用大型自然语言模型（LLM）进行评估，并让模型对多元社区中的问题进行多样化训练。我们发现，问题的主观性在所有审核者群体中都存在，这证明了大多数投票的缺陷。未来，应该使用主观性的标签来训练适用于多元社区的问题识别模型。”Note:\footnotemark[1] 这个数据集的连结地址是https://github.com/XXX.
</details></li>
</ul>
<hr>
<h2 id="Federated-Natural-Policy-Gradient-Methods-for-Multi-task-Reinforcement-Learning"><a href="#Federated-Natural-Policy-Gradient-Methods-for-Multi-task-Reinforcement-Learning" class="headerlink" title="Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning"></a>Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00201">http://arxiv.org/abs/2311.00201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Yang, Shicong Cen, Yuting Wei, Yuxin Chen, Yuejie Chi</li>
<li>for: 这个论文的目的是研究 federated reinforcement learning（RL），即多个分布式代理者之间分享环境transition kernel，而不需要分享本地数据轨迹。</li>
<li>methods: 这篇论文使用了 vanilla 和 entropy-regularized natural policy gradient（NPG）方法，并在 softmax 参数化下进行了实现。 gradient tracking 技术是用于 mitigate 不完整的信息共享的影响。</li>
<li>results: 论文提出了一种基于 policy 优化的 federated RL 方法，并提供了不 asymptotic 全球协调 guarantees，这些 guarantees 几乎不виси于状态动作空间的大小。此外，研究还发现了对精度评估不准确的影响。<details>
<summary>Abstract</summary>
Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon tabular Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods under softmax parameterization, where gradient tracking is applied to the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, which are nearly independent of the size of the state-action space and illuminate the impacts of network size and connectivity. To the best of our knowledge, this is the first time that global convergence is established for federated multi-task RL using policy optimization. Moreover, the convergence behavior of the proposed algorithms is robust against inexactness of policy evaluation.
</details>
<details>
<summary>摘要</summary>
联合强化学习（RL）可以帮助多个分布式代理机制共同做出决策，无需分享本地数据轨迹。在这个工作中，我们考虑了多任务Setting，每个代理都有自己私有的奖励函数，对应不同的任务，同时共享环境的传输核心。我们关注于无限远景表示Markov决策过程，目标是在分布式方式学习一个全局优化策略，使得所有代理的折扣总奖励之和最大化，每个代理只与其邻居通信，根据一定的图形拓扑。我们开发了联邦vanilla和束规化自然策略加速法（NPG），其中使用了软预测参数。我们证明了不假设极限的情况下，我们的算法在精确评估下具有全球启动的全局收敛保证，这些保证是不同状态动作空间大小的不相互关联的。此外，我们发现了提出的算法的收敛行为对精度评估的不准确性具有抗性。Note: "联合" (liánhòu) in the title should be translated as "federated" in English.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/01/cs.AI_2023_11_01/" data-id="cloh7tqcq006j7b881f73bczv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/01/cs.CV_2023_11_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-11-01
        
      </div>
    </a>
  
  
    <a href="/2023/11/01/cs.CL_2023_11_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-11-01</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
