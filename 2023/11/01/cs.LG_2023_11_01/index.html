
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-11-01 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00696 repo_url: None paper_authors: Seyed Mohammad Eb">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-11-01">
<meta property="og:url" content="https://nullscc.github.io/2023/11/01/cs.LG_2023_11_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.00696 repo_url: None paper_authors: Seyed Mohammad Eb">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-01T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-02T12:53:34.482Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_11_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/01/cs.LG_2023_11_01/" class="article-date">
  <time datetime="2023-11-01T10:00:00.000Z" itemprop="datePublished">2023-11-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-11-01
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Decision-Support-Framework-for-Home-Health-Caregiver-Allocation-A-Case-Study-of-HHC-Agency-in-Tennessee-USA"><a href="#Decision-Support-Framework-for-Home-Health-Caregiver-Allocation-A-Case-Study-of-HHC-Agency-in-Tennessee-USA" class="headerlink" title="Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA"></a>Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00696">http://arxiv.org/abs/2311.00696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyed Mohammad Ebrahim Sharifnia, Faezeh Bagheri, Rupy Sawhney, John E. Kobza, Enrique Macias De Anda, Mostafa Hajiaghaei-Keshteli, Michael Mirrielees</li>
<li>for: 本研究旨在优化家庭健康服务（HHC）资源配置，以提高护理资源的利用率和患者满意度。</li>
<li>methods: 本研究提出了一种决策支持框架，该框架考虑护理工作者的访问顺序灵活性，以减少旅行里程，增加每个规划周期的访问次数，并保持护理连续性。</li>
<li>results: 使用美国田纳西州一家护理机构的数据，本研究的方法在减少旅行里程方面达到了非常出色的效果，最多可以减少42%（根据专业不同）。此外，提出的框架还可以为护理资源管理提供有价值的反思。<details>
<summary>Abstract</summary>
Population aging is a global challenge, leading to increased demand for healthcare and social services for the elderly. Home Health Care (HHC) emerges as a vital solution, specifically designed to serve this population segment. Given the surging demand for HHC, it's essential to coordinate and regulate caregiver allocation efficiently. This is crucial for both budget-optimized planning and ensuring the delivery of high-quality care. This research addresses a key question faced by home health agencies (HHAs): "How can caregiver allocation be optimized, especially when caregivers prefer flexibility in their visiting sequences?". While earlier studies proposed rigid visiting sequences, our study introduces a decision support framework that allocates caregivers through a hybrid method that considers the flexibility in visiting sequences and aims to reduce travel mileage, increase the number of visits per planning period, and maintain the continuity of care - a critical metric for patient satisfaction. Utilizing data from an HHA in Tennessee, United States, our approach led to an impressive reduction in average travel mileage (up to 42% depending on discipline) without imposing restrictions on caregivers. Furthermore, the proposed framework is used for caregivers' supply analysis to provide valuable insights into caregiver resource management.
</details>
<details>
<summary>摘要</summary>
全球人口老龄化问题带来增加医疗和社会服务的需求，特别是为老年人。家庭健康护理（HHC）成为了一种非常重要的解决方案，而且它的需求在不断增长。为了有效地协调和规范护理人员的分配，是非常重要的。这是因为预算的优化和高质量护理的确保是两个不可或缺的要求。本研究面临的关键问题是：“护理人员的分配如何优化，特别是当护理人员喜欢自由地访问时序？”earlier studies proposed rigid visiting sequences, our study introduces a decision support framework that allocates caregivers through a hybrid method that considers the flexibility in visiting sequences and aims to reduce travel mileage, increase the number of visits per planning period, and maintain the continuity of care - a critical metric for patient satisfaction. Using data from an HHA in Tennessee, United States, our approach led to an impressive reduction in average travel mileage (up to 42% depending on discipline) without imposing restrictions on caregivers. Furthermore, the proposed framework is used for caregivers' supply analysis to provide valuable insights into caregiver resource management.
</details></li>
</ul>
<hr>
<h2 id="Software-Repositories-and-Machine-Learning-Research-in-Cyber-Security"><a href="#Software-Repositories-and-Machine-Learning-Research-in-Cyber-Security" class="headerlink" title="Software Repositories and Machine Learning Research in Cyber Security"></a>Software Repositories and Machine Learning Research in Cyber Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00691">http://arxiv.org/abs/2311.00691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mounika Vanamala, Keith Bryant, Alex Caravella</li>
<li>for: 本研究旨在提高软件开发过程中早期发现潜在的安全漏洞，通过利用cyber安全库（如MITRE的CAPEC和CVE数据库）和自然语言处理技术（如主题模型和机器学习）自动化检测软件需求阶段的安全漏洞。</li>
<li>methods: 本研究使用了一系列的机器学习方法，包括不supervised学习方法（如LDA和主题模型）和supervised学习方法（如支持向量机器、愚见树、随机森林和神经网络），以检测软件需求阶段的安全漏洞。</li>
<li>results: 本研究结果表明，采用机器学习技术可以有效地检测软件需求阶段的安全漏洞，并且可以在不同的软件开发场景中提供有价值的助け手。<details>
<summary>Abstract</summary>
In today's rapidly evolving technological landscape and advanced software development, the rise in cyber security attacks has become a pressing concern. The integration of robust cyber security defenses has become essential across all phases of software development. It holds particular significance in identifying critical cyber security vulnerabilities at the initial stages of the software development life cycle, notably during the requirement phase. Through the utilization of cyber security repositories like The Common Attack Pattern Enumeration and Classification (CAPEC) from MITRE and the Common Vulnerabilities and Exposures (CVE) databases, attempts have been made to leverage topic modeling and machine learning for the detection of these early-stage vulnerabilities in the software requirements process. Past research themes have returned successful outcomes in attempting to automate vulnerability identification for software developers, employing a mixture of unsupervised machine learning methodologies such as LDA and topic modeling. Looking ahead, in our pursuit to improve automation and establish connections between software requirements and vulnerabilities, our strategy entails adopting a variety of supervised machine learning techniques. This array encompasses Support Vector Machines (SVM), Na\"ive Bayes, random forest, neural networking and eventually transitioning into deep learning for our investigation. In the face of the escalating complexity of cyber security, the question of whether machine learning can enhance the identification of vulnerabilities in diverse software development scenarios is a paramount consideration, offering crucial assistance to software developers in developing secure software.
</details>
<details>
<summary>摘要</summary>
今天的技术领域在快速发展，软件开发也在不断进步，但同时，网络安全攻击的数量也在增加。为了应对这些攻击，在软件开发过程中integrating Robust cybersecurity defenses has become essential。特别是在软件开发生命周期的需求阶段，革命化的攻击方式和漏洞的发现成为了一项非常重要的任务。通过利用MITRE提供的Common Attack Pattern Enumeration and Classification（CAPEC）和Common Vulnerabilities and Exposures（CVE）数据库，我们尝试通过主题模型和机器学习自动发现早期阶段的漏洞。过去的研究主题已经取得了成功，通过混合无监控机器学习方法，如Latent Dirichlet Allocation（LDA）和主题模型，自动检测漏洞。在前进的步骤中，我们的策略是采用多种监督式机器学习方法，包括Support Vector Machines（SVM）、Na\"ive Bayes、Random Forest、神经网络和最终转换为深度学习，以提高检测漏洞的自动化水平。随着网络安全的升级，机器学习是否能够提高检测漏洞在多样化软件开发场景中的能力，成为了一个非常重要的考虑因素，为软件开发人员提供了关键的帮助，以开发安全的软件。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Classification-of-Gamma-Photon-Interactions-in-Room-Temperature-Semiconductor-Radiation-Detectors"><a href="#Deep-Learning-Based-Classification-of-Gamma-Photon-Interactions-in-Room-Temperature-Semiconductor-Radiation-Detectors" class="headerlink" title="Deep Learning-Based Classification of Gamma Photon Interactions in Room-Temperature Semiconductor Radiation Detectors"></a>Deep Learning-Based Classification of Gamma Photon Interactions in Room-Temperature Semiconductor Radiation Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00682">http://arxiv.org/abs/2311.00682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandeep K. Chaudhuri, Qinyang Li, Krishna C. Mandal, Jianjun Hu</li>
<li>for: 这个研究旨在开发一个基于深度学习的检测器，以分别识别γ&#x2F;X射线 фото散射和电子�hrer interactions。</li>
<li>methods: 研究人员使用了一个深度学习模型CoPhNet，并以实验和 simulated 数据进行验证。</li>
<li>results: 研究结果显示，CoPhNet 模型可以实现高精度的分类，并且具有耐操作 parameter 的性能。<details>
<summary>Abstract</summary>
Photon counting radiation detectors have become an integral part of medical imaging modalities such as Positron Emission Tomography or Computed Tomography. One of the most promising detectors is the wide bandgap room temperature semiconductor detectors, which depends on the interaction gamma/x-ray photons with the detector material involves Compton scattering which leads to multiple interaction photon events (MIPEs) of a single photon. For semiconductor detectors like CdZnTeSe (CZTS), which have a high overlap of detected energies between Compton and photoelectric events, it is nearly impossible to distinguish between Compton scattered events from photoelectric events using conventional readout electronics or signal processing algorithms. Herein, we report a deep learning classifier CoPhNet that distinguishes between Compton scattering and photoelectric interactions of gamma/x-ray photons with CdZnTeSe (CZTS) semiconductor detectors. Our CoPhNet model was trained using simulated data to resemble actual CZTS detector pulses and validated using both simulated and experimental data. These results demonstrated that our CoPhNet model can achieve high classification accuracy over the simulated test set. It also holds its performance robustness under operating parameter shifts such as Signal-Noise-Ratio (SNR) and incident energy. Our work thus laid solid foundation for developing next-generation high energy gamma-rays detectors for better biomedical imaging.
</details>
<details>
<summary>摘要</summary>
光子计数放射测量仪已成为医学成像modalities中的一个重要组成部分，如 пози特短谱Tomography或计算Tomography。最有前途的探测器是宽阻挡带 semiconductor探测器，它的探测器材料与γ/X射线 фотоны的交互导致了多个交互 фото摄事件（MIPEs）。例如，使用CdZnTeSe（CZTS）半导体探测器，由于它们的探测能量范围和Compton散射事件的探测能量重叠大，因此通过传统的读取电路或信号处理算法来分辨Compton散射事件和光电摄事件是很困难的。在这种情况下，我们报告了一种深度学习分类器CoPhNet，可以在CdZnTeSe半导体探测器上分辨Compton散射和光电摄事件。CoPhNet模型通过使用模拟数据来模拟实际CZTS探测器脉冲，并在实验和模拟数据上进行验证。结果表明，CoPhNet模型可以在模拟测试集上 achieve high classification accuracy。它还保持了对操作参数的Shift，如信号噪声比（SNR）和入射能量的Robustness。我们的工作因此为开发下一代高能γ射线探测器提供了坚实的基础，以提高生物医学成像。
</details></li>
</ul>
<hr>
<h2 id="Complexity-of-Single-Loop-Algorithms-for-Nonlinear-Programming-with-Stochastic-Objective-and-Constraints"><a href="#Complexity-of-Single-Loop-Algorithms-for-Nonlinear-Programming-with-Stochastic-Objective-and-Constraints" class="headerlink" title="Complexity of Single Loop Algorithms for Nonlinear Programming with Stochastic Objective and Constraints"></a>Complexity of Single Loop Algorithms for Nonlinear Programming with Stochastic Objective and Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00678">http://arxiv.org/abs/2311.00678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmet Alacaoglu, Stephen J. Wright</li>
<li>for: 解决非凸优化问题中的函数等式约束</li>
<li>methods: 使用单循环二阶假定和增强拉格朗日矩阵法</li>
<li>results: 实现 $\varepsilon$-近似首项条件的找到需要 $\widetilde{O}(\varepsilon^{-3})$ 复杂度（第一个情况）、 $\widetilde{O}(\varepsilon^{-4})$ 复杂度（第二个情况）和 $\widetilde{O}(\varepsilon^{-5})$ 复杂度（第三个情况）<details>
<summary>Abstract</summary>
We analyze the complexity of single-loop quadratic penalty and augmented Lagrangian algorithms for solving nonconvex optimization problems with functional equality constraints. We consider three cases, in all of which the objective is stochastic and smooth, that is, an expectation over an unknown distribution that is accessed by sampling. The nature of the equality constraints differs among the three cases: deterministic and linear in the first case, deterministic, smooth and nonlinear in the second case, and stochastic, smooth and nonlinear in the third case. Variance reduction techniques are used to improve the complexity. To find a point that satisfies $\varepsilon$-approximate first-order conditions, we require $\widetilde{O}(\varepsilon^{-3})$ complexity in the first case, $\widetilde{O}(\varepsilon^{-4})$ in the second case, and $\widetilde{O}(\varepsilon^{-5})$ in the third case. For the first and third cases, they are the first algorithms of "single loop" type (that also use $O(1)$ samples at each iteration) that still achieve the best-known complexity guarantees.
</details>
<details>
<summary>摘要</summary>
我们分析单循环quadratic penalty和扩展Lagrangian算法的复杂性，用于解决非凸优化问题中的函数等式约束。我们考虑了三个情况，其中目标函数都是随机的和凹的，即采样得到的 unknown 分布的期望值。等式约束的性质不同于各个情况：第一个情况下是 deterministic 和线性的，第二个情况下是 deterministic、凹和非线性的，第三个情况下是随机、凹和非线性的。我们使用减少偏差技术来提高复杂性。为找到满足 $\varepsilon$-approximate 首项条件的点，我们需要 $\widetilde{O}(\varepsilon^{-3})$ 复杂性在第一个情况下，$\widetilde{O}(\varepsilon^{-4})$ 在第二个情况下，和 $\widetilde{O}(\varepsilon^{-5})$ 在第三个情况下。对于第一个和第三个情况，它们是单循环类型的第一个算法（也使用 $O(1)$ 采样每个迭代），可以在最佳复杂性保证下实现。
</details></li>
</ul>
<hr>
<h2 id="Last-Iterate-Convergence-Properties-of-Regret-Matching-Algorithms-in-Games"><a href="#Last-Iterate-Convergence-Properties-of-Regret-Matching-Algorithms-in-Games" class="headerlink" title="Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games"></a>Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00676">http://arxiv.org/abs/2311.00676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Cai, Gabriele Farina, Julien Grand-Clément, Christian Kroer, Chung-Wei Lee, Haipeng Luo, Weiqiang Zheng</li>
<li>for: 这个论文主要研究了大规模两个玩家零点游戏中 regret matching 算法的最后轮数据收敛性。</li>
<li>methods: 这篇论文使用了 regret matching 算法及其变种，包括同时采用 RM$^+$、交替采用 RM$^+$ 和预测 RM$^+$。</li>
<li>results: 研究发现，这些变种算法在一个简单的 $3\times 3$ 游戏中缺乏最后轮数据收敛性保证，但是使用缓和技术后的变种算法（extragradient RM$^+$ 和平滑预测 RM$^+$）具有 asymptotic 最后轮数据收敛性和 $1&#x2F;\sqrt{t}$ 最好轮数据收敛性。此外，研究还介绍了 restarted 变种算法，其具有线性速率最后轮数据收敛性。<details>
<summary>Abstract</summary>
Algorithms based on regret matching, specifically regret matching$^+$ (RM$^+$), and its variants are the most popular approaches for solving large-scale two-player zero-sum games in practice. Unlike algorithms such as optimistic gradient descent ascent, which have strong last-iterate and ergodic convergence properties for zero-sum games, virtually nothing is known about the last-iterate properties of regret-matching algorithms. Given the importance of last-iterate convergence for numerical optimization reasons and relevance as modeling real-word learning in games, in this paper, we study the last-iterate convergence properties of various popular variants of RM$^+$. First, we show numerically that several practical variants such as simultaneous RM$^+$, alternating RM$^+$, and simultaneous predictive RM$^+$, all lack last-iterate convergence guarantees even on a simple $3\times 3$ game. We then prove that recent variants of these algorithms based on a smoothing technique do enjoy last-iterate convergence: we prove that extragradient RM$^{+}$ and smooth Predictive RM$^+$ enjoy asymptotic last-iterate convergence (without a rate) and $1/\sqrt{t}$ best-iterate convergence. Finally, we introduce restarted variants of these algorithms, and show that they enjoy linear-rate last-iterate convergence.
</details>
<details>
<summary>摘要</summary>
“algorithm based on regret matching, specifically regret matching$^+$ (RM$^+$) and its variants, are the most popular approaches for solving large-scale two-player zero-sum games in practice. Unlike algorithms such as optimistic gradient descent ascent, which have strong last-iterate and ergodic convergence properties for zero-sum games, virtually nothing is known about the last-iterate properties of regret-matching algorithms. Given the importance of last-iterate convergence for numerical optimization reasons and relevance as modeling real-world learning in games, in this paper, we study the last-iterate convergence properties of various popular variants of RM$^+$. First, we show numerically that several practical variants such as simultaneous RM$^+$, alternating RM$^+$, and simultaneous predictive RM$^+$, all lack last-iterate convergence guarantees even on a simple $3\times 3$ game. We then prove that recent variants of these algorithms based on a smoothing technique do enjoy last-iterate convergence: we prove that extragradient RM$^{+}$ and smooth Predictive RM$^+$ enjoy asymptotic last-iterate convergence (without a rate) and $1/\sqrt{t}$ best-iterate convergence. Finally, we introduce restarted variants of these algorithms, and show that they enjoy linear-rate last-iterate convergence.”Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Recovering-Linear-Causal-Models-with-Latent-Variables-via-Cholesky-Factorization-of-Covariance-Matrix"><a href="#Recovering-Linear-Causal-Models-with-Latent-Variables-via-Cholesky-Factorization-of-Covariance-Matrix" class="headerlink" title="Recovering Linear Causal Models with Latent Variables via Cholesky Factorization of Covariance Matrix"></a>Recovering Linear Causal Models with Latent Variables via Cholesky Factorization of Covariance Matrix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00674">http://arxiv.org/abs/2311.00674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfeng Cai, Xu Li, Minging Sun, Ping Li</li>
<li>for: 这个论文的目的是恢复对观察数据的导irected acyclic graph（DAG）结构，并且在存在隐变量时，这个问题变得更加困难。</li>
<li>methods: 这个论文提出了一种基于covariance矩阵的Cholesky分解算法来恢复DAG结构，该算法具有快速和容易实现的优点，并且有理论保证对应于正确恢复。</li>
<li>results: 在 sintetic 和实际数据上，该算法比之前的方法更快速，并且在等Error variances假设下，通过在Cholesky分解基础上添加优化过程来处理含隐变量的DAG恢复问题，numerical simulations表明该修改后的”Cholesky+优化”算法能够在大多数情况下恢复真实的图结构，并且表现较好。<details>
<summary>Abstract</summary>
Discovering the causal relationship via recovering the directed acyclic graph (DAG) structure from the observed data is a well-known challenging combinatorial problem. When there are latent variables, the problem becomes even more difficult. In this paper, we first propose a DAG structure recovering algorithm, which is based on the Cholesky factorization of the covariance matrix of the observed data. The algorithm is fast and easy to implement and has theoretical grantees for exact recovery. On synthetic and real-world datasets, the algorithm is significantly faster than previous methods and achieves the state-of-the-art performance. Furthermore, under the equal error variances assumption, we incorporate an optimization procedure into the Cholesky factorization based algorithm to handle the DAG recovering problem with latent variables. Numerical simulations show that the modified "Cholesky + optimization" algorithm is able to recover the ground truth graph in most cases and outperforms existing algorithms.
</details>
<details>
<summary>摘要</summary>
发现 causal 关系via  recuperating  directed acyclic graph (DAG) 结构从观察数据中是一个常见的困难的 combinatorial 问题。当存在隐藏变量时，问题变得更加困难。在这篇论文中，我们首先提出了 DAG 结构还原算法，基于观察数据的协方差矩阵的 Cholesky 分解。该算法快速易于实现，并有理论保证对于精确还原。在 sintetic 和实际数据上，该算法比前一些方法快得多，并在状态的表现方面达到了顶峰。此外，在等错误 variances 假设下，我们将 Cholesky 基于算法与 latent 变量处理 DAG 还原问题的优化程序结合在一起。 numrical  simulations 表明，修改后的 "Cholesky + 优化" 算法能够回归真实的图结构，并在大多数情况下超过现有算法。
</details></li>
</ul>
<hr>
<h2 id="Latent-Space-Translation-via-Semantic-Alignment"><a href="#Latent-Space-Translation-via-Semantic-Alignment" class="headerlink" title="Latent Space Translation via Semantic Alignment"></a>Latent Space Translation via Semantic Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00664">http://arxiv.org/abs/2311.00664</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flegyas/latent-translation">https://github.com/flegyas/latent-translation</a></li>
<li>paper_authors: Valentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero, Francesco Locatello, Emanuele Rodolà</li>
<li>for: 理解神经网络模型之间的 latent space 相似性的机制</li>
<li>methods: 使用标准的代数方法直接估计 latent space 之间的转换，无需额外训练</li>
<li>results: 在不同的训练、领域、架构和下游任务中，可以很好地将 encoder 和 decoder 缝合，并且可以实现零基础 multimodal 分类 task<details>
<summary>Abstract</summary>
While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Notably, we show how it is possible to zero-shot stitch text encoders and vision decoders, or vice-versa, yielding surprisingly good classification performance in this multimodal setting.
</details>
<details>
<summary>摘要</summary>
不同的神经网络模型经常表现出semantically相关的幂空间，但这种内在相似性不总是立即可见。为了更好地理解这种现象，我们的工作表明了如何通过简单的变换来将 representations从这些神经模块翻译到不同的预训练网络中。这种方法的优点是可以使用标准的、良好的解释 algebraic procedures来计算这些变换，并且有closed-form solution。我们直接计算了两个给定的幂空间之间的变换，从而实现了无需额外训练的编码器和解码器的封装。我们在不同的实验设置下广泛验证了这种翻译过程的适用性：跨多个训练、领域、architecture（例如ResNet、CNN、ViT）以及多个下游任务（分类、重建）。特别是，我们表明了可以零式封装文本编码器和视觉解码器，或者vice versa，在多Modal Setting中获得了意外好的分类性能。
</details></li>
</ul>
<hr>
<h2 id="Online-Signal-Estimation-on-the-Graph-Edges-via-Line-Graph-Transformation"><a href="#Online-Signal-Estimation-on-the-Graph-Edges-via-Line-Graph-Transformation" class="headerlink" title="Online Signal Estimation on the Graph Edges via Line Graph Transformation"></a>Online Signal Estimation on the Graph Edges via Line Graph Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00656">http://arxiv.org/abs/2311.00656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Yan, Ercan Engin Kuruoglu</li>
<li>for: 这篇论文提出了一种在线时间变化图边缘信号预测算法，即Line Graph Normalized Least Mean Square（LGNLMS）算法。</li>
<li>methods: 该算法利用了Line Graph来将图边缘信号转换为图节点的边缘到Vertex的对偶。这使得边信号可以使用已有的GSP概念进行处理，而不需要重新定义它们在图边上。</li>
<li>results: 该算法可以在线时间变化图上预测边缘信号，并且可以减少预测误差。<details>
<summary>Abstract</summary>
We propose the Line Graph Normalized Least Mean Square (LGNLMS) algorithm for online time-varying graph edge signals prediction. LGNLMS utilizes the Line Graph to transform graph edge signals into the node of its edge-to-vertex dual. This enables edge signals to be processed using established GSP concepts without redefining them on graph edges.
</details>
<details>
<summary>摘要</summary>
我们提出了线图正规最小二乘（LGNLMS）算法，用于在线时变图边信号预测。LGNLMS使用线图将图边信号转换成图边到顶点的对偶点。这使得边信号可以使用已有的图像处理概念进行处理，无需对图边进行重新定义。
</details></li>
</ul>
<hr>
<h2 id="Kronecker-Factored-Approximate-Curvature-for-Modern-Neural-Network-Architectures"><a href="#Kronecker-Factored-Approximate-Curvature-for-Modern-Neural-Network-Architectures" class="headerlink" title="Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures"></a>Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00636">http://arxiv.org/abs/2311.00636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runa Eschenhagen, Alexander Immer, Richard E. Turner, Frank Schneider, Philipp Hennig</li>
<li>for: 这篇论文旨在探讨如何使用 Kronecker-Factored Approximate Curvature (K-FAC) 加速现代神经网络训练，以降低计算成本。</li>
<li>methods: 该论文提出了两种不同的线性Weight-sharing层设置，分别是 expand 和 reduce，并证明它们在深度线性网络中都是正确的。</li>
<li>results: 论文通过对一种宽度神经网络和一种视觉转换器进行训练，发现 K-FAC 可以减少训练时间，并且可以达到相同的验证指标目标，但需要更少的步骤数。<details>
<summary>Abstract</summary>
The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer. However, both variations are able to reach a fixed validation metric target in $50$-$75\%$ of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. This highlights the potential of applying K-FAC to modern neural network architectures.
</details>
<details>
<summary>摘要</summary>
核心component of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimization method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavors of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimizing the marginal likelihood for a Wide ResNet. Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer. However, both variations are able to reach a fixed validation metric target in $50$-$75\%$ of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. This highlights the potential of applying K-FAC to modern neural network architectures.Here's the translation in Traditional Chinese:核心component of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimization method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavors of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimizing the marginal likelihood for a Wide ResNet. Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer. However, both variations are able to reach a fixed validation metric target in $50$-$75\%$ of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. This highlights the potential of applying K-FAC to modern neural network architectures.
</details></li>
</ul>
<hr>
<h2 id="Controllable-Music-Production-with-Diffusion-Models-and-Guidance-Gradients"><a href="#Controllable-Music-Production-with-Diffusion-Models-and-Guidance-Gradients" class="headerlink" title="Controllable Music Production with Diffusion Models and Guidance Gradients"></a>Controllable Music Production with Diffusion Models and Guidance Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00613">http://arxiv.org/abs/2311.00613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Levy, Bruno Di Giorgi, Floris Weers, Angelos Katharopoulos, Tom Nickson</li>
<li>for: 本研究使用扩散模型进行条件生成，以解决音乐生成 tasks 中的多种实际问题，包括续写、填充和重新生成音乐audio，以及创造细腻的音乐过渡和将欲具有的风格特征传递给现有音频clip。</li>
<li>methods: 本研究使用 sampling-time guidance 方法，通过在简单的框架中应用导航，支持 both reconstruction 和 classification 损失，或任何组合其中的两个。这种方法确保生成的音频可以与周围的上下文匹配，或按照任何适当的预训练分类器或嵌入模型的 latent representation 进行匹配。</li>
<li>results: 研究人员通过实验证明，通过使用本方法，可以生成高质量的44.1kHz静音音频，并且可以具有很好的样本时间导航和分类能力。<details>
<summary>Abstract</summary>
We demonstrate how conditional generation from diffusion models can be used to tackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio with sampling-time guidance. The scenarios we consider include continuation, inpainting and regeneration of musical audio, the creation of smooth transitions between two different music tracks, and the transfer of desired stylistic characteristics to existing audio clips. We achieve this by applying guidance at sampling time in a simple framework that supports both reconstruction and classification losses, or any combination of the two. This approach ensures that generated audio can match its surrounding context, or conform to a class distribution or latent representation specified relative to any suitable pre-trained classifier or embedding model.
</details>
<details>
<summary>摘要</summary>
我们示例了如何使用扩散模型进行 conditional generation，用于处理不同的实际音乐生成任务，包括续写、填充和重新生成音乐声音，创造缓冲过渡 между两首不同的乐曲，以及将欲求的风格特征传承到现有音频clip。我们通过在采样时提供指导，实现了这些任务。我们的简单框架支持重建和分类损失，或任何组合其中的两者。这种方法确保生成的声音能匹配周围的上下文，或按照任何适当的预训练分类器或嵌入模型的类型进行分类。
</details></li>
</ul>
<hr>
<h2 id="A-Collaborative-Filtering-Based-Two-Stage-Model-with-Item-Dependency-for-Course-Recommendation"><a href="#A-Collaborative-Filtering-Based-Two-Stage-Model-with-Item-Dependency-for-Course-Recommendation" class="headerlink" title="A Collaborative Filtering-Based Two Stage Model with Item Dependency for Course Recommendation"></a>A Collaborative Filtering-Based Two Stage Model with Item Dependency for Course Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00612">http://arxiv.org/abs/2311.00612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Eric L. Lee, Tsung-Ting Kuo, Shou-De Lin<br>for: 这篇论文旨在推荐课程，并提出了一些应对现有CF模型在建立课程推荐引擎时遇到的挑战的想法。methods: 本文使用了一种两阶段CF模型，其中第一阶段是通过课程依赖网络建立学生的学习路径，第二阶段是使用CF模型进行学生课程推荐。此外，文章还提出了一些 Addressing Challenges in Applying CF Models to Course Recommendation 的想法。results: 实验结果表明，combined with a two-stage CF model regularized by course dependency and a graph-based recommender based on course-transition network, 可以达到AUC为0.97的高精度。<details>
<summary>Abstract</summary>
Recommender systems have been studied for decades with numerous promising models been proposed. Among them, Collaborative Filtering (CF) models are arguably the most successful one due to its high accuracy in recommendation and elimination of privacy-concerned personal meta-data from training. This paper extends the usage of CF-based model to the task of course recommendation. We point out several challenges in applying the existing CF-models to build a course recommendation engine, including the lack of rating and meta-data, the imbalance of course registration distribution, and the demand of course dependency modeling. We then propose several ideas to address these challenges. Eventually, we combine a two-stage CF model regularized by course dependency with a graph-based recommender based on course-transition network, to achieve AUC as high as 0.97 with a real-world dataset.
</details>
<details>
<summary>摘要</summary>
推荐系统已经在数十年内被研究，有很多有前途的模型被提出。其中，协同推荐（CF）模型被认为是最成功的，这是因为它的推荐精度高并且不需要在训练中提供隐私担忧的个人元数据。本文将CF模型应用到课程推荐任务中，并指出了现有CF模型在实施课程推荐引擎时存在的一些挑战，包括缺乏评分和元数据、课程注册分布不均衡以及课程依赖模型的需求。然后，我们提出了一些解决方案。最后，我们将一种两阶段CF模型 regularized by course dependency 与一种基于课程过渡网络的图形推荐模型相结合，以实现使用真实世界数据的AUC为0.97。</sys>Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".Here's the translation in Traditional Chinese:<sys>推证系统已经在数十年内被研究，有很多有前途的模型被提出。其中，协同推证（CF）模型被认为是最成功的，这是因为它的推证精度高且不需要在训练中提供隐私担忧的个人元数据。本文将CF模型应用到课程推证任务中，并指出了现有CF模型在实施课程推证引擎时存在的一些挑战，包括缺乏评分和元数据、课程注册分布不均衡以及课程依赖模型的需求。然后，我们提出了一些解决方案。最后，我们将一种两阶段CF模型 regularized by course dependency 与一种基于课程过渡网络的图形推证模型相结合，以实现使用真实世界数据的AUC为0.97。</sys>Note: Traditional Chinese is also known as "Traditional Mandarin" or "Formosan Chinese".
</details></li>
</ul>
<hr>
<h2 id="Structure-Learning-with-Adaptive-Random-Neighborhood-Informed-MCMC"><a href="#Structure-Learning-with-Adaptive-Random-Neighborhood-Informed-MCMC" class="headerlink" title="Structure Learning with Adaptive Random Neighborhood Informed MCMC"></a>Structure Learning with Adaptive Random Neighborhood Informed MCMC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00599">http://arxiv.org/abs/2311.00599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Caron, Xitong Liang, Samuel Livingstone, Jim Griffin</li>
<li>For: 本研究提出了一种新的Markov Chain Monte Carlo（MCMC）采样算法，称为PARNI-DAG，用于在观察数据下进行完全 Bayesian 结构学习。* Methods: PARNI-DAG 算法基于 causal sufficiency 假设，可以直接采样 posterior 分布中的 Directed Acyclic Graphs（DAGs）。该算法使用本地 Informed 随机邻域提案，使得采样更加快速并更加稳定。此外，为了更好地扩展到更多节点，我们couple PARNI-DAG 算法与一种预处理措施，使用skeleton graph得到了一些约束或 scoring-based 算法。* Results: PARNI-DAG 算法在高维设定下具有更好的混合性和准确性，可以快速 converges to high-probability regions，并且 less likely to get stuck in local modes。在一系列实验中，我们证明了 PARNI-DAG 算法的混合效率和准确性。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel MCMC sampler, PARNI-DAG, for a fully-Bayesian approach to the problem of structure learning under observational data. Under the assumption of causal sufficiency, the algorithm allows for approximate sampling directly from the posterior distribution on Directed Acyclic Graphs (DAGs). PARNI-DAG performs efficient sampling of DAGs via locally informed, adaptive random neighborhood proposal that results in better mixing properties. In addition, to ensure better scalability with the number of nodes, we couple PARNI-DAG with a pre-tuning procedure of the sampler's parameters that exploits a skeleton graph derived through some constraint-based or scoring-based algorithms. Thanks to these novel features, PARNI-DAG quickly converges to high-probability regions and is less likely to get stuck in local modes in the presence of high correlation between nodes in high-dimensional settings. After introducing the technical novelties in PARNI-DAG, we empirically demonstrate its mixing efficiency and accuracy in learning DAG structures on a variety of experiments.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的MCMC抽样器，PARNI-DAG，用于完全 bayesian 的结构学习问题。假设 causal sufficiency，该算法允许直接从 posterior 分布中进行 Approximate 抽样，并且在 Directed Acyclic Graphs (DAGs) 上进行有效的抽样。PARNI-DAG 使用本地 Informed 随机邻域提案，从而提高了混合性质。此外，为了更好地扩展到节点数的增加，我们对 PARNI-DAG 的参数进行预调整，利用一个基于约束或分数基的树图来加速抽样。这些新特点使得 PARNI-DAG 在高维设置中更加快速地趋向高概率区域，并且更 unlikely 地被高 correlate  между节点所陷入本地模式。在介绍 PARNI-DAG 的技术新特点后，我们在各种实验中证明了其混合效率和准确性。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Tails-for-Normalising-Flows-with-Application-to-the-Modelling-of-Financial-Return-Data"><a href="#Flexible-Tails-for-Normalising-Flows-with-Application-to-the-Modelling-of-Financial-Return-Data" class="headerlink" title="Flexible Tails for Normalising Flows, with Application to the Modelling of Financial Return Data"></a>Flexible Tails for Normalising Flows, with Application to the Modelling of Financial Return Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00580">http://arxiv.org/abs/2311.00580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tennessee Hickling, Dennis Prangle</li>
<li>for: 这个论文是为了研究如何使用极值论断来改变分布的尾部性质，以便更好地模型多变量巨大的尖峰分布。</li>
<li>methods: 这个论文使用了极值论断的方法来改变分布的尾部性质，并在许多实际应用中进行了验证。</li>
<li>results: 这个论文的结果表明，使用极值论断可以准确地模型多变量巨大的尖峰分布，并且可以生成新的尖峰分布样本。<details>
<summary>Abstract</summary>
We propose a transformation capable of altering the tail properties of a distribution, motivated by extreme value theory, which can be used as a layer in a normalizing flow to approximate multivariate heavy tailed distributions. We apply this approach to model financial returns, capturing potentially extreme shocks that arise in such data. The trained models can be used directly to generate new synthetic sets of potentially extreme returns
</details>
<details>
<summary>摘要</summary>
我们提出一种转换，可以改变分布的尾部属性，基于极值理论，可以用作正常化流中的层，以近似多变量重核分布。我们对金融回报数据应用这种方法，捕捉可能出现的极端冲击。训练的模型可以直接生成新的可能极端的返回数据集。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Revealing-CNN-Architectures-via-Side-Channel-Analysis-in-Dataflow-based-Inference-Accelerators"><a href="#Revealing-CNN-Architectures-via-Side-Channel-Analysis-in-Dataflow-based-Inference-Accelerators" class="headerlink" title="Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators"></a>Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00579">http://arxiv.org/abs/2311.00579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansika Weerasena, Prabhat Mishra</li>
<li>for: 这种论文是为了研究如何利用数据流基本的 CNN 加速器来恢复 CNN 模型结构。</li>
<li>methods: 该论文使用了记忆基本的边防攻击方法，利用数据流中的空间和时间数据重复来恢复 CNN 模型结构。</li>
<li>results: 实验结果表明，该攻击方法可以成功恢复 Lenet、Alexnet 和 VGGnet16 等popular CNN 模型的结构。<details>
<summary>Abstract</summary>
Convolution Neural Networks (CNNs) are widely used in various domains. Recent advances in dataflow-based CNN accelerators have enabled CNN inference in resource-constrained edge devices. These dataflow accelerators utilize inherent data reuse of convolution layers to process CNN models efficiently. Concealing the architecture of CNN models is critical for privacy and security. This paper evaluates memory-based side-channel information to recover CNN architectures from dataflow-based CNN inference accelerators. The proposed attack exploits spatial and temporal data reuse of the dataflow mapping on CNN accelerators and architectural hints to recover the structure of CNN models. Experimental results demonstrate that our proposed side-channel attack can recover the structures of popular CNN models, namely Lenet, Alexnet, and VGGnet16.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在多个领域广泛应用。最近的数据流基于CNN加速器技术的进步使得CNN推理可以在具有限制的边缘设备中进行。这些数据流加速器利用卷积层的自然数据重用来处理CNN模型高效。隐藏CNN模型的架构是隐私和安全的关键。这篇论文评估了基于数据流的CNN推理加速器中的内存基于侧annel信息，以 recover CNN模型的结构。我们的提议的攻击利用卷积层的空间和时间数据重用以及架构提示来恢复流行的CNN模型Lenet、Alexnet和VGGnet16的结构。实验结果表明，我们的提议的侧annel攻击可以成功地恢复CNN模型的结构。
</details></li>
</ul>
<hr>
<h2 id="Transfer-learning-for-improved-generalizability-in-causal-physics-informed-neural-networks-for-beam-simulations"><a href="#Transfer-learning-for-improved-generalizability-in-causal-physics-informed-neural-networks-for-beam-simulations" class="headerlink" title="Transfer learning for improved generalizability in causal physics-informed neural networks for beam simulations"></a>Transfer learning for improved generalizability in causal physics-informed neural networks for beam simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00578">http://arxiv.org/abs/2311.00578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taniya Kapoor, Hongrui Wang, Alfredo Nunez, Rolf Dollevoet</li>
<li>for: 这个论文提出了一种新的方法来模拟梁在弹性基础上的动态行为。</li>
<li>methods: 该方法使用了一种基于物理学信息学习网络（PINN）框架的转移学习方法，使用了一种尊重 causality的损失函数来缓解大空间时间域的问题。</li>
<li>results: 实验表明，提议的方法可以快速地达到高度准确的结果，并且在多种初始条件下都有优于现状方法。此外，该方法还可以在扩展的空间和时间域中模拟提梁的动态行为。<details>
<summary>Abstract</summary>
This paper introduces a novel methodology for simulating the dynamics of beams on elastic foundations. Specifically, Euler-Bernoulli and Timoshenko beam models on the Winkler foundation are simulated using a transfer learning approach within a causality-respecting physics-informed neural network (PINN) framework. Conventional PINNs encounter challenges in handling large space-time domains, even for problems with closed-form analytical solutions. A causality-respecting PINN loss function is employed to overcome this limitation, effectively capturing the underlying physics. However, it is observed that the causality-respecting PINN lacks generalizability. We propose using solutions to similar problems instead of training from scratch by employing transfer learning while adhering to causality to accelerate convergence and ensure accurate results across diverse scenarios. Numerical experiments on the Euler-Bernoulli beam highlight the efficacy of the proposed approach for various initial conditions, including those with noise in the initial data. Furthermore, the potential of the proposed method is demonstrated for the Timoshenko beam in an extended spatial and temporal domain. Several comparisons suggest that the proposed method accurately captures the inherent dynamics, outperforming the state-of-the-art physics-informed methods under standard $L^2$-norm metric and accelerating convergence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Personalized-Assignment-to-One-of-Many-Treatment-Arms-via-Regularized-and-Clustered-Joint-Assignment-Forests"><a href="#Personalized-Assignment-to-One-of-Many-Treatment-Arms-via-Regularized-and-Clustered-Joint-Assignment-Forests" class="headerlink" title="Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests"></a>Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00577">http://arxiv.org/abs/2311.00577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Ladhania, Jann Spiess, Lyle Ungar, Wenbo Wu</li>
<li>for: 学习个性化赋予分配。</li>
<li>methods: 使用树式分配算法和 clustering 策略，以减少每个赋予分配的异常差异。</li>
<li>results: 在模拟研究中，通过直接优化赋予分配，可以获得更高的实际效果，而不是单独对每个赋予分配进行预测。在理论模型中，我们表明了在许多赋予分配情况下，直接优化赋予分配可以获得显著的实用效果。<details>
<summary>Abstract</summary>
We consider learning personalized assignments to one of many treatment arms from a randomized controlled trial. Standard methods that estimate heterogeneous treatment effects separately for each arm may perform poorly in this case due to excess variance. We instead propose methods that pool information across treatment arms: First, we consider a regularized forest-based assignment algorithm based on greedy recursive partitioning that shrinks effect estimates across arms. Second, we augment our algorithm by a clustering scheme that combines treatment arms with consistently similar outcomes. In a simulation study, we compare the performance of these approaches to predicting arm-wise outcomes separately, and document gains of directly optimizing the treatment assignment with regularization and clustering. In a theoretical model, we illustrate how a high number of treatment arms makes finding the best arm hard, while we can achieve sizable utility gains from personalization by regularized optimization.
</details>
<details>
<summary>摘要</summary>
我团队考虑了个化个性化任务分配到多个治疗臂中。标准方法可能在这种情况下表现不佳，因为它们可能会增加差异。我们提议使用共享信息的方法：首先，我们考虑了一种基于滥览树的评估算法，使用扩展的抽象分解来减小对每个臂的效果估计。其次，我们将治疗臂与相似的结果结合在一起，使用凝结方法来增强个性化任务分配。在一个 simulations 研究中，我们比较了这些方法与分别预测每个臂的结果的性能，并证明了通过直接优化治疗分配来提高个性化效果。在一个理论模型中，我们表明了多个治疗臂使得找到最佳臂变得困难，但通过REGularization和凝结来实现了较大的利用率提升。
</details></li>
</ul>
<hr>
<h2 id="Online-Student-t-Processes-with-an-Overall-local-Scale-Structure-for-Modelling-Non-stationary-Data"><a href="#Online-Student-t-Processes-with-an-Overall-local-Scale-Structure-for-Modelling-Non-stationary-Data" class="headerlink" title="Online Student-$t$ Processes with an Overall-local Scale Structure for Modelling Non-stationary Data"></a>Online Student-$t$ Processes with an Overall-local Scale Structure for Modelling Non-stationary Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00564">http://arxiv.org/abs/2311.00564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taole Sha, Michael Minyi Zhang</li>
<li>for:  handle time-dependent data with non-stationarity and heavy-tailed errors</li>
<li>methods: Bayesian mixture of student-$t$ processes with overall-local scale structure for covariance, SMC sampler for online inference</li>
<li>results: superiority over typical Gaussian process-based models on real-world data sets<details>
<summary>Abstract</summary>
Time-dependent data often exhibit characteristics, such as non-stationarity and heavy-tailed errors, that would be inappropriate to model with the typical assumptions used in popular models. Thus, more flexible approaches are required to be able to accommodate such issues. To this end, we propose a Bayesian mixture of student-$t$ processes with an overall-local scale structure for the covariance. Moreover, we use a sequential Monte Carlo (SMC) sampler in order to perform online inference as data arrive in real-time. We demonstrate the superiority of our proposed approach compared to typical Gaussian process-based models on real-world data sets in order to prove the necessity of using mixtures of student-$t$ processes.
</details>
<details>
<summary>摘要</summary>
时间相关的数据经常具有非站立性和重 tailed 错误的特点，这些特点不适合使用流行的模型假设。因此，我们需要更灵活的方法来满足这些问题。为此，我们提议使用 bayesian 混合学生-$t$ 过程，并使用线性 Monte Carlo（SMC）探针进行在线推断，以便在实时接收数据时进行推断。我们通过对实际数据集进行比较，证明我们的提议方法比typical Gaussian process-based models更为有利。Note: "student-$t$ process" refers to a type of statistical distribution that is similar to the normal distribution but with heavier tails, and "sequential Monte Carlo" (SMC) is a type of algorithm used for Bayesian inference.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-optimize-by-multi-gradient-for-multi-objective-optimization"><a href="#Learning-to-optimize-by-multi-gradient-for-multi-objective-optimization" class="headerlink" title="Learning to optimize by multi-gradient for multi-objective optimization"></a>Learning to optimize by multi-gradient for multi-objective optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00559">http://arxiv.org/abs/2311.00559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linxi Yang, Xinmin Yang, Liping Tang</li>
<li>for: 本研究旨在提出一种基于自动学习的多目标优化（MOO）方法，以替代手动设计的传统MOO方法。</li>
<li>methods: 本研究提出了一种自动学习 парадиг，并使用多个梯度来更新方向的多梯度学习优化（ML2O）方法。该方法可以从当前步骤中获取当前地形信息，并通过历史迭代轨迹数据来捕捉全局经验。</li>
<li>results: 实验结果表明，我们学习的优化器在训练多任务学习（MTL）神经网络时表现出色，超过了手动设计的竞争对手。<details>
<summary>Abstract</summary>
The development of artificial intelligence (AI) for science has led to the emergence of learning-based research paradigms, necessitating a compelling reevaluation of the design of multi-objective optimization (MOO) methods. The new generation MOO methods should be rooted in automated learning rather than manual design. In this paper, we introduce a new automatic learning paradigm for optimizing MOO problems, and propose a multi-gradient learning to optimize (ML2O) method, which automatically learns a generator (or mappings) from multiple gradients to update directions. As a learning-based method, ML2O acquires knowledge of local landscapes by leveraging information from the current step and incorporates global experience extracted from historical iteration trajectory data. By introducing a new guarding mechanism, we propose a guarded multi-gradient learning to optimize (GML2O) method, and prove that the iterative sequence generated by GML2O converges to a Pareto critical point. The experimental results demonstrate that our learned optimizer outperforms hand-designed competitors on training multi-task learning (MTL) neural network.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在科学领域的发展导致了学习基于研究 парадигмы的出现，这导致了多目标优化（MOO）方法的设计需要重新评估。新一代MOO方法应该基于自动学习而非手动设计。在这篇论文中，我们介绍了一种新的自动学习方法来优化MOO问题，并提出了多Gradient学习来优化（ML2O）方法，该方法可以自动学习多个梯度来更新方向。作为一种学习基于方法，ML2O可以从当前步骤和历史迭代轨迹数据中获取本地景观知识。通过引入新的保护机制，我们提出了一种卫士多Gradient学习来优化（GML2O）方法，并证明其迭代序列会 converges to a Pareto kritical point。实验结果表明，我们学习的优化器比手动设计的竞争对手在训练多任务学习（MTL）神经网络上表现更好。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Without-a-Processor-Emergent-Learning-in-a-Nonlinear-Electronic-Metamaterial"><a href="#Machine-Learning-Without-a-Processor-Emergent-Learning-in-a-Nonlinear-Electronic-Metamaterial" class="headerlink" title="Machine Learning Without a Processor: Emergent Learning in a Nonlinear Electronic Metamaterial"></a>Machine Learning Without a Processor: Emergent Learning in a Nonlinear Electronic Metamaterial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00537">http://arxiv.org/abs/2311.00537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Dillavou, Benjamin D Beyer, Menachem Stern, Marc Z Miskin, Andrea J Liu, Douglas J Durian</li>
<li>for: 这个论文旨在探讨electronic learning metamaterials的非线性学习能力，以及其在各种感知和控制应用中的可能性。</li>
<li>methods: 作者提出了一种基于抵抗元件的非线性学习元件，通过自适应调整其电流-电压特性来实现非线性学习。</li>
<li>results: 研究发现，该非线性学习元件可以学习不可达的任务，如XOR和非线性回归，而不需要计算机。此外，该系统的训练模式与人工神经网络中的spectral bias类似，并且具有高效、低功耗和可重新启动的特点。<details>
<summary>Abstract</summary>
Standard deep learning algorithms require differentiating large nonlinear networks, a process that is slow and power-hungry. Electronic learning metamaterials offer potentially fast, efficient, and fault-tolerant hardware for analog machine learning, but existing implementations are linear, severely limiting their capabilities. These systems differ significantly from artificial neural networks as well as the brain, so the feasibility and utility of incorporating nonlinear elements have not been explored. Here we introduce a nonlinear learning metamaterial -- an analog electronic network made of self-adjusting nonlinear resistive elements based on transistors. We demonstrate that the system learns tasks unachievable in linear systems, including XOR and nonlinear regression, without a computer. We find our nonlinear learning metamaterial reduces modes of training error in order (mean, slope, curvature), similar to spectral bias in artificial neural networks. The circuitry is robust to damage, retrainable in seconds, and performs learned tasks in microseconds while dissipating only picojoules of energy across each transistor. This suggests enormous potential for fast, low-power computing in edge systems like sensors, robotic controllers, and medical devices, as well as manufacturability at scale for performing and studying emergent learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Active-Noise-Control-Portable-Device-Design"><a href="#Active-Noise-Control-Portable-Device-Design" class="headerlink" title="Active Noise Control Portable Device Design"></a>Active Noise Control Portable Device Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00535">http://arxiv.org/abs/2311.00535</a></li>
<li>repo_url: None</li>
<li>paper_authors: kai Wu, Yuanyuan Chen<br>for: 降低噪音的技术解决方案，以提高工作效率和人类健康。methods: 使用感应器探测环境中的噪音，然后将噪音处理于电子控制系统中，产生反相频率信号以抵消干扰。results: 这个智能噪音减少系统可以实时减少噪音，包括低频噪音，并且搭配睡眠追踪、音乐播放应用程序、气温感应和智能家居设备控制等功能。<details>
<summary>Abstract</summary>
While our world is filled with its own natural sounds that we can't resist enjoying, it is also chock-full of other sounds that can be irritating, this is noise. Noise not only influences the working efficiency but also the human's health. The problem of reducing noise is one of great importance and great difficulty. The problem has been addressed in many ways over the years. The current methods for noise reducing mostly rely on the materials and transmission medium, which are only effective to some extent for the high frequency noise. However, the effective reduction noise method especially for low frequency noise is very limited.   Here we come up with a noise reduction system consist of a sensor to detect the noise in the environment. Then the noise will be sent to an electronic control system to process the noise, which will generate a reverse phase frequency signal to counteract the disturbance. Finally, the processed smaller noise will be broadcasted by the speaker. Through this smart noise reduction system, even the noise with low-frequency can be eliminated.   The system is also integrated with sleep tracking and music player applications. It can also remember and store settings for the same environment, sense temperature, and smart control of home furniture, fire alarm, etc. This smart system can transfer data easily by Wi-Fi or Bluetooth and controlled by its APP.   In this project, we will present a model of the above technology which can be used in various environments to prevent noise pollution and provide a solution to the people who have difficulties finding a peaceful and quiet environment for sleep, work or study.
</details>
<details>
<summary>摘要</summary>
我们的世界满是自然的声音，我们喜欢听的声音，但是也有吵吵声，这是噪音。噪音不仅影响工作效率，还影响人类的健康。噪音减少问题是非常重要且非常困难的问题。过去多年来，人们已经有很多方法来解决这个问题，但是现有的噪音减少方法主要仅能减少高频噪音，对低频噪音的减少效果非常有限。为了解决这问题，我们开发了一种噪音减少系统，包括一个检测噪音环境的传感器，将噪音传输到电子控制系统进行处理，然后生成一个逆相频率信号，以抵消干扰。最后，处理后的小于噪音将被广播器播放出来。这个智能噪音减少系统可以减少低频噪音。此外，该系统还 integrate了睡眠跟踪和音乐播放应用程序，还可以记录和存储相同环境的设置，感测温度，智能控制家庭家具、火灾警示等。这个智能系统可以通过Wi-Fi或蓝牙传输数据，并由APP控制。在这个项目中，我们将提出一种使用该技术的模型，可以在不同环境中采用以防止噪音污染和提供辛苦找到安静和平静的环境，供着睡眠、工作或学习等方面的人们。
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Based-Reconstruction-For-Time-series-Contrastive-Learning"><a href="#Retrieval-Based-Reconstruction-For-Time-series-Contrastive-Learning" class="headerlink" title="Retrieval-Based Reconstruction For Time-series Contrastive Learning"></a>Retrieval-Based Reconstruction For Time-series Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00519">http://arxiv.org/abs/2311.00519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxwell A. Xu, Alexander Moreno, Hui Wei, Benjamin M. Marlin, James M. Rehg</li>
<li>for: 本研究旨在开发一种基于 RETRIEVAL-BAsed Reconstruction (REBAR) 的自动матиче监督学习方法，以提高时间序列数据的下游任务性能。</li>
<li>methods: 本方法使用卷积混合注意力架构计算两个不同时间序列之间的 REBAR 误差，并通过验证实验表明 REBAR 误差是约束类别成员关系的预测器，因此可以作为正&#x2F;负标注。最后，本方法被integrated into a contrastive learning framework，可以学习一个 дости得州arameter表现出色的嵌入。</li>
<li>results: 在不同模式下，本方法可以学习一个高性能的嵌入，并且在下游任务上达到了状态的最优性能。<details>
<summary>Abstract</summary>
The success of self-supervised contrastive learning hinges on identifying positive data pairs that, when pushed together in embedding space, encode useful information for subsequent downstream tasks. However, in time-series, this is challenging because creating positive pairs via augmentations may break the original semantic meaning. We hypothesize that if we can retrieve information from one subsequence to successfully reconstruct another subsequence, then they should form a positive pair. Harnessing this intuition, we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR) contrastive learning. First, we utilize a convolutional cross-attention architecture to calculate the REBAR error between two different time-series. Then, through validation experiments, we show that the REBAR error is a predictor of mutual class membership, justifying its usage as a positive/negative labeler. Finally, once integrated into a contrastive learning framework, our REBAR method can learn an embedding that achieves state-of-the-art performance on downstream tasks across various modalities.
</details>
<details>
<summary>摘要</summary>
自我超级学习的成功取决于标注出正确的数据对，使得在嵌入空间中拼接起来的信息有用 для后续的下游任务。然而，在时序数据中，创建正确对via扩展可能会破坏原始 semantics。我们提出的假设是，如果可以从一个时序中检索到另一个时序，并成功重建它，那么它们应该组成一个正确的对。基于这种直觉，我们介绍了我们的新方法：REtrieval-BAsed Reconstruction（REBAR）对比学习。首先，我们利用卷积cross-attention架构计算REBAR错误 между两个不同的时序数据。然后，通过验证实验，我们显示了REBAR错误是两个时序数据之间的共同类别的预测器，这 justify its usage as a positive/negative labeler。最后，我们将REBAR方法集成到对比学习框架中，可以学习一个具有State-of-the-art表现的嵌入。
</details></li>
</ul>
<hr>
<h2 id="Fixed-Budget-Best-Arm-Identification-in-Sparse-Linear-Bandits"><a href="#Fixed-Budget-Best-Arm-Identification-in-Sparse-Linear-Bandits" class="headerlink" title="Fixed-Budget Best-Arm Identification in Sparse Linear Bandits"></a>Fixed-Budget Best-Arm Identification in Sparse Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00481">http://arxiv.org/abs/2311.00481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Recep Can Yavas, Vincent Y. F. Tan</li>
<li>for: 本研究探讨了固定预算下的最佳臂 Identification问题，尤其是在稀疏线性抽象下。</li>
<li>methods: 我们提出了一个两阶段算法，即Lasso和Optimal-Design-（Lasso-OD）基于的线性最佳臂 Identification。第一阶段使用过滤lasso（Zhou, 2009）估计Feature vector $\theta^*$的支持集，而第二阶段则应用OD-LinBAI（Yang 和 Tan, 2022）算法。</li>
<li>results: 我们 derive了一个非对数的Upper bound 在 Lasso-OD 的错误几率上，通过精确地选择对数（例如lasso的正规化参数）并将两阶段的错误几率相应平衡。在固定稀疏度 $s$ 和预算 $T$ 下，Lasso-OD 的错误几率 exponent 随 $s$ 而不随维度 $d$ 的增长，实现了稀疏和高维度线性抽象中的显著性能改善。此外，我们还证明了 Lasso-OD 在对数上是几乎最佳的。最后，我们提供了一些实际的数据，以证明 Lasso-OD 对非稀疏线性抽象的性能改善。<details>
<summary>Abstract</summary>
We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\theta^*$ may be of large dimension $d$, but only a few, say $s \ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which estimates the support of $\theta^*$ correctly with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support. We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed sparsity $s$ and budget $T$, the exponent in the error probability of Lasso-OD depends on $s$ but not on the dimension $d$, yielding a significant performance improvement for sparse and high-dimensional linear bandits. Furthermore, we show that Lasso-OD is almost minimax optimal in the exponent. Finally, we provide numerical examples to demonstrate the significant performance improvement over the existing algorithms for non-sparse linear bandits such as OD-LinBAI, BayesGap, Peace, LinearExploration, and GSE.
</details>
<details>
<summary>摘要</summary>
我们研究最好臂识别问题在稀畴线性投机下的固定预算设定下。在稀畴线性投机中，未知特征向量 $\theta^*$ 可能具有很大的维度 $d$，但只有一些，例如 $s \ll d$ 的特征有非零值。我们设计了两相运算程式，即lasso和Optimal-Design-（lasso-OD）基于的线性最好臂识别。第一相运算程式的lasso-OD 利用特征向量的稀畴性，通过对特征向量进行过滤的lasso 引入的范围内的条件，将 $\theta^*$ 的支持正确地估计出来，使用奖励自选投机和judicious的设计矩阵。第二相运算程式的lasso-OD 应用OD-LinBAI 算法，由 Yang 和 Tan （2022）引入的 Optimal-Design-LinBAI 算法。我们对lasso-OD 的误差概率进行非对数几何分析，并且调整几何元素（如lasso 的正规化参数），以实现适当的误差概率。对于固定的 $s$ 和 $T$，lasso-OD 的误差概率指数随 $s$ 而改变，实现了高维度和稀畴的线性投机中的很大性能改进。此外，我们还证明了lasso-OD 是很接近最佳的几何对数几何。最后，我们提供了一些数字示例，以展示lasso-OD 在非稀畴线性投机中的很大性能改进。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-models-for-probabilistic-programming"><a href="#Diffusion-models-for-probabilistic-programming" class="headerlink" title="Diffusion models for probabilistic programming"></a>Diffusion models for probabilistic programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00474">http://arxiv.org/abs/2311.00474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Dirmeier, Fernando Perez-Cruz</li>
<li>for: automated approximate inference in probabilistic programming languages (PPLs)</li>
<li>methods: 使用扩散模型作为variational approximation来 aproximate true posterior distribution，并 derive a novel bound to the marginal likelihood objective used in Bayesian modelling</li>
<li>results: 对一些常见的Bayesian模型进行评估，其 posterior inferences比同时期的方法更准确，computational cost和手动调整相对 Similar，但要更加简单实现和不受 neural network模型的限制。<details>
<summary>Abstract</summary>
We propose Diffusion Model Variational Inference (DMVI), a novel method for automated approximate inference in probabilistic programming languages (PPLs). DMVI utilizes diffusion models as variational approximations to the true posterior distribution by deriving a novel bound to the marginal likelihood objective used in Bayesian modelling. DMVI is easy to implement, allows hassle-free inference in PPLs without the drawbacks of, e.g., variational inference using normalizing flows, and does not make any constraints on the underlying neural network model. We evaluate DMVI on a set of common Bayesian models and show that its posterior inferences are in general more accurate than those of contemporary methods used in PPLs while having a similar computational cost and requiring less manual tuning.
</details>
<details>
<summary>摘要</summary>
我们提出了Diffusion Model Variational Inference（DMVI），一种新的自动化近似推理方法，用于运行概率编程语言（PPL）中的推理。DMVI利用扩散模型作为实际 posterior distribution 的可变近似，通过计算一个新的缩小边界目标，以便在 Bayesian 模型中进行推理。DMVI易于实现，不需要辛苦的推理，不需要对底层神经网络模型做任何限制，并且可以轻松地在 PPL 中进行推理，而不需要丰富的手动调整。我们在一些常见的 Bayesian 模型中评估了 DMVI，发现其 posterior 推理结果比cotemporary方法在 PPL 中更精确，同时computational cost 和手动调整的需求相似。
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-SGD-on-Graphs-a-Unified-Framework-for-Asynchronous-Decentralized-and-Federated-Optimization"><a href="#Asynchronous-SGD-on-Graphs-a-Unified-Framework-for-Asynchronous-Decentralized-and-Federated-Optimization" class="headerlink" title="Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization"></a>Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00465">http://arxiv.org/abs/2311.00465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathieu Even, Anastasia Koloskova, Laurent Massoulié</li>
<li>for: 这篇论文旨在提高分布式机器学习中的通信复杂性，通过异步通信和分布式计算技术来提高算法的效率。</li>
<li>methods: 该论文提出了异步SGD在图上的框架，其可以涵盖异步版本的多种算法，包括SGD、分布式SGD、本地SGD、FedBuff等。</li>
<li>results: 论文提供了较宽松的通信和计算假设下的收敛率，而且可以回归或者超越之前的异步分布式工作的最佳结果。<details>
<summary>Abstract</summary>
Decentralized and asynchronous communications are two popular techniques to speedup communication complexity of distributed machine learning, by respectively removing the dependency over a central orchestrator and the need for synchronization. Yet, combining these two techniques together still remains a challenge. In this paper, we take a step in this direction and introduce Asynchronous SGD on Graphs (AGRAF SGD) -- a general algorithmic framework that covers asynchronous versions of many popular algorithms including SGD, Decentralized SGD, Local SGD, FedBuff, thanks to its relaxed communication and computation assumptions. We provide rates of convergence under much milder assumptions than previous decentralized asynchronous works, while still recovering or even improving over the best know results for all the algorithms covered.
</details>
<details>
<summary>摘要</summary>
分布式机器学习中的通信复杂性可以使用分布式和异步通信技术来加速。 former 可以消除中央把关者的依赖关系，而异步通信可以避免同步。 yet，将这两种技术结合使用仍然是一个挑战。 在这篇论文中，我们采取了这一步，并对 asynchronous SGD on graphs（AGRAF SGD）进行了总体的算法框架。 AGRAF SGD 涵盖了许多流行的算法，包括 SGD、分布式 SGD、Local SGD 和 FedBuff， thanks to its relaxed communication and computation assumptions。 我们提供了对于更加宽松的假设下的收敛率，而且仍然可以恢复或者even improve over the best known results for all the algorithms covered。
</details></li>
</ul>
<hr>
<h2 id="Robust-and-Conjugate-Gaussian-Process-Regression"><a href="#Robust-and-Conjugate-Gaussian-Process-Regression" class="headerlink" title="Robust and Conjugate Gaussian Process Regression"></a>Robust and Conjugate Gaussian Process Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00463">http://arxiv.org/abs/2311.00463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matias Altamirano, François-Xavier Briol, Jeremias Knoblauch</li>
<li>for: 提高 Gaussian Process（GP） regression 的可靠性和不确定性评估，并且能够在实际应用中使用。</li>
<li>methods: 使用总体 Bayesian 推理来实现可证明 robust 和 conjugate Gaussian Process（RCGP） regression，不需要额外成本。 RCGP 具有可 conjugate 关闭形式更新，适用于所有标准 GP 支持的情况。</li>
<li>results: 通过在 Bayesian 优化和稀VAR Gaussian Process 等问题中应用 RCGP，实际表现强劲。<details>
<summary>Abstract</summary>
To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes.
</details>
<details>
<summary>摘要</summary>
要实现关闭形式条件， Gaussian process（GP）回归通常假设独立和同样分布的 Gaussian 观测噪声。这是一个强大且简单的假设，在实践中经常被违反，导致不可靠的推断和不确定性评估。现有的方法用于强化 GPs 会打砸关闭形式条件，使其变得更加不吸引实践者并显着增加计算成本。在这篇论文中，我们展示了如何在几乎没有额外成本下实现可证明 Robust 和 conjugate Gaussian process（RCGP）回归。RCGP 特别是可以在所有设置下实现批量 conjugate 关闭形式更新，从而使其在实践中更加强大。为证明其强大的实际性表现，我们在 Bayesian 优化到 sparse variational Gaussian processes 中应用 RCGP。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Budgeted-Rejection-Sampling-for-Generative-Models"><a href="#Optimal-Budgeted-Rejection-Sampling-for-Generative-Models" class="headerlink" title="Optimal Budgeted Rejection Sampling for Generative Models"></a>Optimal Budgeted Rejection Sampling for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00460">http://arxiv.org/abs/2311.00460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre Verine, Muni Sreenivas Pydi, Benjamin Negrevergne, Yann Chevaleyre</li>
<li>for: 提高权重生成模型的性能</li>
<li>methods: 使用优化采样方法</li>
<li>results: 提高样本质量和多样性<details>
<summary>Abstract</summary>
Rejection sampling methods have recently been proposed to improve the performance of discriminator-based generative models. However, these methods are only optimal under an unlimited sampling budget, and are usually applied to a generator trained independently of the rejection procedure. We first propose an Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal with respect to \textit{any} $f$-divergence between the true distribution and the post-rejection distribution, for a given sampling budget. Second, we propose an end-to-end method that incorporates the sampling scheme into the training procedure to further enhance the model's overall performance. Through experiments and supporting theory, we show that the proposed methods are effective in significantly improving the quality and diversity of the samples.
</details>
<details>
<summary>摘要</summary>
“拒拒样本方法最近被提出来改善基于判断器的生成模型性能。然而，这些方法只有无限样本预算下才是优化的，通常是独立于拒拒程序进行生成器训练。我们首先提出了一种最优预算拒拒样本方案（OBRS），该方案在任何 $f $- divergence 下是可证优化的，对于给定的预算。其次，我们提出了一种综合方法，将拒拒样本方案 incorporated 到训练过程中，以进一步提高模型的总性能。通过实验和支持理论，我们证明了我们的方法可以有效地提高样本质量和多样性。”Here's the translation breakdown:* "拒拒样本方法" (拒拒样本方法) - This refers to the rejection sampling methods proposed in the text.* "最近被提出来" (最近被提出来) - This phrase indicates that the rejection sampling methods have been recently proposed.* "改善基于判断器的生成模型性能" (改善基于判断器的生成模型性能) - This phrase explains the purpose of the rejection sampling methods, which is to improve the performance of generative models based on discriminators.* "然而" (然而) - This word indicates a contrast or exception, indicating that the rejection sampling methods have limitations.* "这些方法只有无限样本预算下才是优化的" (这些方法只有无限样本预算下才是优化的) - This phrase explains the limitation of the rejection sampling methods, which is that they are only optimal with an unlimited sampling budget.* "通常是独立于拒拒程序进行生成器训练" (通常是独立于拒拒程序进行生成器训练) - This phrase explains that the rejection sampling methods are usually applied to a generator trained independently of the rejection procedure.* "我们首先提出了一种最优预算拒拒样本方案" (我们首先提出了一种最优预算拒拒样本方案) - This phrase introduces the first proposed method, which is the optimal budgeted rejection sampling (OBRS) scheme.* "该方案在任何 $f $- divergence 下是可证优化的" (该方案在任何 $f $- divergence 下是可证优化的) - This phrase explains that the OBRS scheme is provably optimal with respect to any $f$-divergence between the true distribution and the post-rejection distribution, for a given sampling budget.* "其次" (其次) - This word indicates a secondary or additional point.* "我们提出了一种综合方法" (我们提出了一种综合方法) - This phrase introduces the second proposed method, which is an end-to-end method that incorporates the sampling scheme into the training procedure.* "以进一步提高模型的总性能" (以进一步提高模型的总性能) - This phrase explains the purpose of the end-to-end method, which is to further enhance the model's overall performance.* "通过实验和支持理论，我们证明了我们的方法可以有效地提高样本质量和多样性" (通过实验和支持理论，我们证明了我们的方法可以有效地提高样本质量和多样性) - This phrase explains that the proposed methods have been shown to be effective in improving the quality and diversity of the samples through experiments and supporting theory.
</details></li>
</ul>
<hr>
<h2 id="Hessian-Eigenvectors-and-Principal-Component-Analysis-of-Neural-Network-Weight-Matrices"><a href="#Hessian-Eigenvectors-and-Principal-Component-Analysis-of-Neural-Network-Weight-Matrices" class="headerlink" title="Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight Matrices"></a>Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00452">http://arxiv.org/abs/2311.00452</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Haink</li>
<li>for: 本研究探讨深度神经网络训练后的行为和网络参数之间的关系。</li>
<li>methods: 本研究使用了许多方法，包括泛化函数梯度下降、主成分分析和特征值分解等。</li>
<li>results: 研究发现了许多关键结果，包括：弹性函数梯度下降可以解释深度神经网络的演化方向；网络参数与梯度下降的关系；以及可以使用主成分分析来简化梯度矩阵。此外，研究还发现了一种可以避免深度神经网络忘记现象的有效策略。<details>
<summary>Abstract</summary>
This study delves into the intricate dynamics of trained deep neural networks and their relationships with network parameters. Trained networks predominantly continue training in a single direction, known as the drift mode. This drift mode can be explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. We unveil a correlation between Hessian eigenvectors and network weights. This relationship, hinging on the magnitude of eigenvalues, allows us to discern parameter directions within the network. Notably, the significance of these directions relies on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extends to the decomposition of weight matrices through singular value decomposition. This approach proves practical in identifying critical directions within the Hessian, considering both their magnitude and curvature. Furthermore, our examination showcases the applicability of principal component analysis in approximating the Hessian, with update parameters emerging as a superior choice over weights for this purpose. Remarkably, our findings unveil a similarity between the largest Hessian eigenvalues of individual layers and the entire network. Notably, higher eigenvalues are concentrated more in deeper layers. Leveraging these insights, we venture into addressing catastrophic forgetting, a challenge of neural networks when learning new tasks while retaining knowledge from previous ones. By applying our discoveries, we formulate an effective strategy to mitigate catastrophic forgetting, offering a possible solution that can be applied to networks of varying scales, including larger architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Form-follows-Function-Text-to-Text-Conditional-Graph-Generation-based-on-Functional-Requirements"><a href="#Form-follows-Function-Text-to-Text-Conditional-Graph-Generation-based-on-Functional-Requirements" class="headerlink" title="Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements"></a>Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00444">http://arxiv.org/abs/2311.00444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Peter A. Zachares, Vahan Hovhannisyan, Alan Mosca, Yarin Gal</li>
<li>for: 本研究targets the novel problem setting of generating graphs conditioned on a description of the graph’s functional requirements in a downstream task.</li>
<li>methods: 我们提出了一种文本-to-文本生成方法，通过预训练大型自然语言模型（LLM）来生成图形。我们还提出了一种假设，通过将信息报告层 integrate into LLM的架构来增加图形的结构信息。</li>
<li>results: 我们在使用公共可用的分子和知识图数据集进行设计了一系列实验，并得到了结果，显示我们的提议方法可以更好地满足请求的功能要求，与类似任务的基线方法相比，差异为统计学上的显著差异。<details>
<summary>Abstract</summary>
This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task. We pose the problem as a text-to-text generation problem and focus on the approach of fine-tuning a pretrained large language model (LLM) to generate graphs. We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture. To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets. Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Crop-Disease-Classification-using-Support-Vector-Machines-with-Green-Chromatic-Coordinate-GCC-and-Attention-based-feature-extraction-for-IoT-based-Smart-Agricultural-Applications"><a href="#Crop-Disease-Classification-using-Support-Vector-Machines-with-Green-Chromatic-Coordinate-GCC-and-Attention-based-feature-extraction-for-IoT-based-Smart-Agricultural-Applications" class="headerlink" title="Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications"></a>Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00429">http://arxiv.org/abs/2311.00429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashwat Jha, Vishvaditya Luhach, Gauri Shanker Gupta, Beependra Singh<br>for: 这篇论文的目的是提供一种新的植物病诊断方法，以便帮助农民快速和准确地诊断植物病情。methods: 本文使用了注意力基于的特征提取、RGB通道基于的色彩分析、支持向量机（SVM）等机器学习和深度学习算法来实现植物病诊断。results: 相比之前的算法，这种新的诊断方法在精度方面达到了99.69%，而且可以与移动应用程序和物联网设备集成，减少了4倍的大小。这些发现有助于帮助农民快速和准确地诊断植物病情，从而保持农业输出和食品安全。<details>
<summary>Abstract</summary>
Crops hold paramount significance as they serve as the primary provider of energy, nutrition, and medicinal benefits for the human population. Plant diseases, however, can negatively affect leaves during agricultural cultivation, resulting in significant losses in crop output and economic value. Therefore, it is crucial for farmers to identify crop diseases. However, this method frequently necessitates hard work, a lot of planning, and in-depth familiarity with plant pathogens. Given these numerous obstacles, it is essential to provide solutions that can easily interface with mobile and IoT devices so that our farmers can guarantee the best possible crop development. Various machine learning (ML) as well as deep learning (DL) algorithms have been created & studied for the identification of plant disease detection, yielding substantial and promising results. This article presents a novel classification method that builds on prior work by utilising attention-based feature extraction, RGB channel-based chromatic analysis, Support Vector Machines (SVM) for improved performance, and the ability to integrate with mobile applications and IoT devices after quantization of information. Several disease classification algorithms were compared with the suggested model, and it was discovered that, in terms of accuracy, Vision Transformer-based feature extraction and additional Green Chromatic Coordinate feature with SVM classification achieved an accuracy of (GCCViT-SVM) - 99.69%, whereas after quantization for IoT device integration achieved an accuracy of - 97.41% while almost reducing 4x in size. Our findings have profound implications because they have the potential to transform how farmers identify crop illnesses with precise and fast information, thereby preserving agricultural output and ensuring food security.
</details>
<details>
<summary>摘要</summary>
This article presents a novel classification method that leverages attention-based feature extraction, RGB channel-based chromatic analysis, and Support Vector Machines (SVM) to improve disease detection accuracy. Additionally, the proposed method can be integrated with mobile applications and IoT devices, allowing for fast and precise information transmission.Several disease classification algorithms were compared with the suggested model, and the results showed that the Vision Transformer-based feature extraction and additional Green Chromatic Coordinate feature with SVM classification achieved an accuracy of 99.69%. After quantization for IoT device integration, the accuracy was 97.41% while reducing the model size by almost 4x. These findings have significant implications for the agricultural industry, as they have the potential to revolutionize how farmers identify crop illnesses quickly and accurately, ensuring food security and preserving agricultural output.
</details></li>
</ul>
<hr>
<h2 id="NEO-KD-Knowledge-Distillation-Based-Adversarial-Training-for-Robust-Multi-Exit-Neural-Networks"><a href="#NEO-KD-Knowledge-Distillation-Based-Adversarial-Training-for-Robust-Multi-Exit-Neural-Networks" class="headerlink" title="NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks"></a>NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00428">http://arxiv.org/abs/2311.00428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seokil Ham, Jungwuk Park, Dong-Jun Han, Jaekyun Moon</li>
<li>for: 本研究旨在提高多入口神经网络对针对攻击的抗性能力，通过使用知识储存技术进行 adversarial 训练策略。</li>
<li>methods: 本研究提出了两大贡献，首先通过邻居知识储存引导输出对恶例数据的输出偏移向 ensemble 输出的clean数据的邻居 exit 的输出偏移。其次，通过 exit-wise 正交知识储存来减少不同子模型之间的攻击传递性。</li>
<li>results: 对于多个数据集和模型，我们的方法实现了最佳的针对攻击精度，同时减少了计算预算。与基于现有 adversarial 训练或知识储存技术的基eline相比，我们的方法实现了更好的抗性能力。<details>
<summary>Abstract</summary>
While multi-exit neural networks are regarded as a promising solution for making efficient inference via early exits, combating adversarial attacks remains a challenging problem. In multi-exit networks, due to the high dependency among different submodels, an adversarial example targeting a specific exit not only degrades the performance of the target exit but also reduces the performance of all other exits concurrently. This makes multi-exit networks highly vulnerable to simple adversarial attacks. In this paper, we propose NEO-KD, a knowledge-distillation-based adversarial training strategy that tackles this fundamental challenge based on two key contributions. NEO-KD first resorts to neighbor knowledge distillation to guide the output of the adversarial examples to tend to the ensemble outputs of neighbor exits of clean data. NEO-KD also employs exit-wise orthogonal knowledge distillation for reducing adversarial transferability across different submodels. The result is a significantly improved robustness against adversarial attacks. Experimental results on various datasets/models show that our method achieves the best adversarial accuracy with reduced computation budgets, compared to the baselines relying on existing adversarial training or knowledge distillation techniques for multi-exit networks.
</details>
<details>
<summary>摘要</summary>
多出口神经网络被视为可以实现高效的早期退出的有望解决方案，但是对抗攻击性攻击仍然是一个挑战。在多出口网络中，由于不同的子模型之间的高度依赖关系，针对特定出口的攻击性示例不仅会降低该出口的性能，还会同时降低所有其他出口的性能。这使得多出口网络对简单的攻击性攻击非常易受到攻击。在这篇论文中，我们提出了NEO-KD，基于两项重要贡献的知识塑化基本攻击训练策略。NEO-KD首先通过邻居知识塑化引导攻击示例的输出倾向于净数据的邻居出口的ensemble输出。NEO-KD还使用出口WISE orthogonal知识塑化来减少不同子模型之间的攻击传递性。这使得我们的方法在不同的数据集/模型上实现了明显提高的抗击性。实验结果表明，我们的方法可以在计算预算限制下达到最佳的抗击精度，比基于现有的对抗训练或知识塑化技术的基准值更好。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-quantification-and-out-of-distribution-detection-using-surjective-normalizing-flows"><a href="#Uncertainty-quantification-and-out-of-distribution-detection-using-surjective-normalizing-flows" class="headerlink" title="Uncertainty quantification and out-of-distribution detection using surjective normalizing flows"></a>Uncertainty quantification and out-of-distribution detection using surjective normalizing flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00377">http://arxiv.org/abs/2311.00377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/irmlma/uncertainty-quantification-snf">https://github.com/irmlma/uncertainty-quantification-snf</a></li>
<li>paper_authors: Simon Dirmeier, Ye Hong, Yanan Xin, Fernando Perez-Cruz</li>
<li>for: 这个研究是为了提供一个简单的方法来量化深度学习模型中的epistemic和 aleatoricuncertainty，以便在不同环境下实现模型的可靠应用。</li>
<li>methods: 这个方法使用surjective normalizing flows来识别深度学习模型中的out-of-distribution数据集，可以在单一的前进过程中进行计算。这个方法建立在深度 uncertainty quantification和生成模型中的normalizing flows之上。</li>
<li>results: 在一个人工合成的数据集和一些实验室实现的数据集上，我们显示了我们的方法可以可靠地区别出对数据集的in-distribution和out-of-distribution数据。我们与Dirichlet process mixture model和bijective flow进行比较，发现surjective flow模型是关键的Component来可靠地分辨对数据集的in-distribution和out-of-distribution数据。<details>
<summary>Abstract</summary>
Reliable quantification of epistemic and aleatoric uncertainty is of crucial importance in applications where models are trained in one environment but applied to multiple different environments, often seen in real-world applications for example, in climate science or mobility analysis. We propose a simple approach using surjective normalizing flows to identify out-of-distribution data sets in deep neural network models that can be computed in a single forward pass. The method builds on recent developments in deep uncertainty quantification and generative modeling with normalizing flows. We apply our method to a synthetic data set that has been simulated using a mechanistic model from the mobility literature and several data sets simulated from interventional distributions induced by soft and atomic interventions on that model, and demonstrate that our method can reliably discern out-of-distribution data from in-distribution data. We compare the surjective flow model to a Dirichlet process mixture model and a bijective flow and find that the surjections are a crucial component to reliably distinguish in-distribution from out-of-distribution data.
</details>
<details>
<summary>摘要</summary>
可靠地量化模型中的 epistemic 和 aleatoric 不确定性是实际应用中非常重要的，因为模型通常在多个不同环境中训练，然后应用于多个环境中。例如，气候科学和流动分析中都有这种情况。我们提出了一种简单的方法，使用射影正规化流来在深度神经网络模型中标识不符合分布数据集。该方法基于深度不确定量和生成模型中的正规化流的最新发展。我们在一个由机制模型生成的 sintetic 数据集和一些基于软件和原子干扰的数据集上应用了我们的方法，并证明了我们的方法可靠地分辨出不符合分布数据集和符合分布数据集之间的差异。我们与 Dirichlet 过程混合模型和 bijection 流进行比较，发现射影是分辨出在 Distribution 和 out-of-distribution 数据集之间的关键组成部分。
</details></li>
</ul>
<hr>
<h2 id="Performance-Optimization-of-Deep-Learning-Sparse-Matrix-Kernels-on-Intel-Max-Series-GPU"><a href="#Performance-Optimization-of-Deep-Learning-Sparse-Matrix-Kernels-on-Intel-Max-Series-GPU" class="headerlink" title="Performance Optimization of Deep Learning Sparse Matrix Kernels on Intel Max Series GPU"></a>Performance Optimization of Deep Learning Sparse Matrix Kernels on Intel Max Series GPU</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00368">http://arxiv.org/abs/2311.00368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Zubair, Christoph Bauinger</li>
<li>for: 这paper主要关注三种稀疏矩阵操作，即稀疏-积累矩阵乘法（SPMM）、采样积累-积累矩阵乘法（SDDMM），以及这两种操作的复合（FusedMM）。</li>
<li>methods: 我们开发了优化的实现方法 для SPMM、SDDMM 和 FusedMM 操作，使用 Intel oneAPI 的 Explicit SIMD（ESIMD） SYCL 扩展 API。这些 API 允许我们编写明确的 SIMD Veccode。</li>
<li>results: 我们的实现方法在目标 Intel Data Center GPU 上实现了稀疏矩阵操作的性能，与 Intel oneMKL 库的性能相似。我们还与 NVIDIA V100 GPU 上的 CUDA 实现和 Intel GPU 上的 oneMKL 库进行比较，并证明了我们的实现方法在稀疏矩阵操作中表现更好。<details>
<summary>Abstract</summary>
In this paper, we focus on three sparse matrix operations that are relevant for machine learning applications, namely, the sparse-dense matrix multiplication (SPMM), the sampled dense-dense matrix multiplication (SDDMM), and the composition of the SDDMM with SPMM, also termed as FusedMM. We develop optimized implementations for SPMM, SDDMM, and FusedMM operations utilizing Intel oneAPI's Explicit SIMD (ESIMD) SYCL extension API. In contrast to CUDA or SYCL, the ESIMD API enables the writing of explicitly vectorized kernel code. Sparse matrix algorithms implemented with the ESIMD API achieved performance close to the peak of the targeted Intel Data Center GPU. We compare our performance results to Intel's oneMKL library on Intel GPUs and to a recent CUDA implementation for the sparse matrix operations on NVIDIA's V100 GPU and demonstrate that our implementations for sparse matrix operations outperform either.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注了三种稀疏矩阵操作，它们对机器学习应用有重要意义，即稀疏积 multiplication（SPMM）、采样积 multiplication（SDDMM）以及这两者的组合（FusedMM）。我们开发了优化的实现方法 для SPMM、SDDMM 和 FusedMM 操作，使用 Intel oneAPI 的 Explicit SIMD（ESIMD） SYCL 扩展 API。与 CUDA 或 SYCL 不同，ESIMD API 允许直接编写向量化kernel代码。我们使用 ESIMD API 实现的稀疏矩阵算法在目标 Intel 数据中心 GPU 的性能几乎达到了最高水平。我们对 Intel 的 oneMKL 库在 Intel GPU 上的性能进行比较，以及 NVIDIA V100 GPU 上的一个 recent CUDA 实现，并证明了我们的稀疏矩阵操作实现的性能高于其中任一。
</details></li>
</ul>
<hr>
<h2 id="Adversarially-Robust-Distributed-Count-Tracking-via-Partial-Differential-Privacy"><a href="#Adversarially-Robust-Distributed-Count-Tracking-via-Partial-Differential-Privacy" class="headerlink" title="Adversarially Robust Distributed Count Tracking via Partial Differential Privacy"></a>Adversarially Robust Distributed Count Tracking via Partial Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00346">http://arxiv.org/abs/2311.00346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongzheng Xiong, Xiaoyi Zhu, Zengfeng Huang</li>
<li>for: 这篇论文关注分布式跟踪模型，即分布式功能监控。这种模型中，每个站点都接收一批项目，并与中央服务器进行交互。服务器需要不断地跟踪所有项目的函数，以最小化交互成本。</li>
<li>methods: 作者使用了随机化算法，但现有的随机算法假设了”无知恶作用者”会在算法开始前构建整个输入流。在这种情况下，作者考虑了适应性的恶作用者，这些恶作用者可以根据先前的答案来选择新的项目。作者证明了随机算法在适应性恶作用者情况下的优势。</li>
<li>results: 作者提供了一个具有最佳通信成本的可靠算法，并证明了这种算法在分布式环境中存在挑战。此外，作者还引入了”偏微分隐私”概念，并证明了一个新的总体公式。这个公式可能在更广泛的应用场景中有着独立的价值。<details>
<summary>Abstract</summary>
We study the distributed tracking model, also known as distributed functional monitoring. This model involves $k$ sites each receiving a stream of items and communicating with the central server. The server's task is to track a function of all items received thus far continuously, with minimum communication cost. For count tracking, it is known that there is a $\sqrt{k}$ gap in communication between deterministic and randomized algorithms. However, existing randomized algorithms assume an "oblivious adversary" who constructs the entire input streams before the algorithm starts. Here we consider adaptive adversaries who can choose new items based on previous answers from the algorithm. Deterministic algorithms are trivially robust to adaptive adversaries, while randomized ones may not. Therefore, we investigate whether the $\sqrt{k}$ advantage of randomized algorithms is from randomness itself or the oblivious adversary assumption. We provide an affirmative answer to this question by giving a robust algorithm with optimal communication. Existing robustification techniques do not yield optimal bounds due to the inherent challenges of the distributed nature of the problem. To address this, we extend the differential privacy framework by introducing "partial differential privacy" and proving a new generalization theorem. This theorem may have broader applications beyond robust count tracking, making it of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究分布式追踪模型，又称为分布式功能监控。这个模型中有 $k$ 个网站，每个网站获得一条流量，并与中央服务器进行通信。服务器的任务是不断地追踪所有收到的物品，并实现最小的通信成本。在数据追踪中，已知存在 $\sqrt{k}$ 差距在决定性和随机化算法之间。然而，现有的随机化算法假设了“无知敌人”可以在算法开始之前构成整个输入流。在这里，我们考虑到可靠性敌人，他们可以根据先前答案选择新的物品。决定性算法是可以对可靠性敌人进行防护的，而随机化算法可能不是。因此，我们调查这 $\sqrt{k}$ 差距是由随机性本身或“无知敌人”假设所带来的。我们提供了一个具有最佳通信的扩展算法，并证明了一个新的通用定理。这个定理可能在更多的应用中有价值，因此是独立的 interessant。
</details></li>
</ul>
<hr>
<h2 id="The-Open-DAC-2023-Dataset-and-Challenges-for-Sorbent-Discovery-in-Direct-Air-Capture"><a href="#The-Open-DAC-2023-Dataset-and-Challenges-for-Sorbent-Discovery-in-Direct-Air-Capture" class="headerlink" title="The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture"></a>The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00341">http://arxiv.org/abs/2311.00341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anuroop Sriram, Sihoon Choi, Xiaohan Yu, Logan M. Brabson, Abhishek Das, Zachary Ulissi, Matt Uyttendaele, Andrew J. Medford, David S. Sholl</li>
<li>For: The paper is written for the purpose of exploring a computational approach to discovering promising metal-organic frameworks (MOFs) for direct air capture (DAC) using machine learning (ML) and density functional theory (DFT) calculations.* Methods: The paper uses a computational approach that involves more than 38 million DFT calculations on over 8,800 MOF materials containing adsorbed CO2 and&#x2F;or H2O to identify promising MOFs for DAC. The dataset used for the study, named Open DAC 2023 (ODAC23), is the largest dataset of MOF adsorption calculations at the DFT level of accuracy currently available.* Results: The paper identifies a large number of MOFs with promising properties for DAC directly in ODAC23, and trains state-of-the-art ML models on the dataset to approximate calculations at the DFT level. The open-source dataset and ML models provide an important baseline for future efforts to identify MOFs for a wide range of applications, including DAC.Here is the simplified Chinese text for the three information points:* For: 本研究是为了通过计算方法发现 dirett air capture (DAC) 中最佳的金属组合物材料 (MOF)，使用机器学习 (ML) 和密度函数理论 (DFT) 计算。* Methods: 本研究使用了大量的 DFT 计算，计算了超过 8,800 个 MOF 材料中的 CO2 和&#x2F;或 H2O 的吸附性。这个数据集名为 Open DAC 2023 (ODAC23)，现在最大的 MOF 吸附计算数据集之一。* Results: 本研究直接在 ODAC23 中发现了许多有 promise 的 MOF  для DAC，并使用了当前最佳的 ML 模型来近似 DFT 计算。这个开源数据集和 ML 模型将为未来的 MOF 发现提供重要的基线。<details>
<summary>Abstract</summary>
New methods for carbon dioxide removal are urgently needed to combat global climate change. Direct air capture (DAC) is an emerging technology to capture carbon dioxide directly from ambient air. Metal-organic frameworks (MOFs) have been widely studied as potentially customizable adsorbents for DAC. However, discovering promising MOF sorbents for DAC is challenging because of the vast chemical space to explore and the need to understand materials as functions of humidity and temperature. We explore a computational approach benefiting from recent innovations in machine learning (ML) and present a dataset named Open DAC 2023 (ODAC23) consisting of more than 38M density functional theory (DFT) calculations on more than 8,800 MOF materials containing adsorbed CO2 and/or H2O. ODAC23 is by far the largest dataset of MOF adsorption calculations at the DFT level of accuracy currently available. In addition to probing properties of adsorbed molecules, the dataset is a rich source of information on structural relaxation of MOFs, which will be useful in many contexts beyond specific applications for DAC. A large number of MOFs with promising properties for DAC are identified directly in ODAC23. We also trained state-of-the-art ML models on this dataset to approximate calculations at the DFT level. This open-source dataset and our initial ML models will provide an important baseline for future efforts to identify MOFs for a wide range of applications, including DAC.
</details>
<details>
<summary>摘要</summary>
新的碳排放除去方法 urgently needed 以 combat global climate change. Direct air capture (DAC) 是一 emerging technology  captures carbon dioxide directly from ambient air. Metal-organic frameworks (MOFs) 已经 widely studied as potentially customizable adsorbents for DAC. However, discovering promising MOF sorbents for DAC 是 challenging because of the vast chemical space to explore and the need to understand materials as functions of humidity and temperature. We explore a computational approach benefiting from recent innovations in machine learning (ML) and present a dataset named Open DAC 2023 (ODAC23) consisting of more than 38M density functional theory (DFT) calculations on more than 8,800 MOF materials containing adsorbed CO2 and/or H2O. ODAC23 is by far the largest dataset of MOF adsorption calculations at the DFT level of accuracy currently available. In addition to probing properties of adsorbed molecules, the dataset is a rich source of information on structural relaxation of MOFs, which will be useful in many contexts beyond specific applications for DAC. A large number of MOFs with promising properties for DAC are identified directly in ODAC23. We also trained state-of-the-art ML models on this dataset to approximate calculations at the DFT level. This open-source dataset and our initial ML models will provide an important baseline for future efforts to identify MOFs for a wide range of applications, including DAC.
</details></li>
</ul>
<hr>
<h2 id="Latent-Space-Inference-For-Spatial-Transcriptomics"><a href="#Latent-Space-Inference-For-Spatial-Transcriptomics" class="headerlink" title="Latent Space Inference For Spatial Transcriptomics"></a>Latent Space Inference For Spatial Transcriptomics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00330">http://arxiv.org/abs/2311.00330</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Ding, S. N. Zaman, P. Y. Chen, D. Wang</li>
<li>For: The paper aims to obtain full genetic expression information for tissue samples while preserving their spatial coordinates.* Methods: The authors use probabilistic machine learning methods and variational inference to map single-cell RNA sequencing and image-based spatial transcriptomics data to a joint latent space representation.* Results: The method allows for the full genetic expression information and spatial coordinates of cells to be decoded, providing greater insights into cellular processes and pathways.<details>
<summary>Abstract</summary>
In order to understand the complexities of cellular biology, researchers are interested in two important metrics: the genetic expression information of cells and their spatial coordinates within a tissue sample. However, state-of-the art methods, namely single-cell RNA sequencing and image based spatial transcriptomics can only recover a subset of this information, either full genetic expression with loss of spatial information, or spatial information with loss of resolution in sequencing data. In this project, we investigate a probabilistic machine learning method to obtain the full genetic expression information for tissues samples while also preserving their spatial coordinates. This is done through mapping both datasets to a joint latent space representation with the use of variational machine learning methods. From here, the full genetic and spatial information can be decoded and to give us greater insights on the understanding of cellular processes and pathways.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-task-Representation-Learning-for-Pure-Exploration-in-Bilinear-Bandits"><a href="#Multi-task-Representation-Learning-for-Pure-Exploration-in-Bilinear-Bandits" class="headerlink" title="Multi-task Representation Learning for Pure Exploration in Bilinear Bandits"></a>Multi-task Representation Learning for Pure Exploration in Bilinear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00327">http://arxiv.org/abs/2311.00327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhojyoti Mukherjee, Qiaomin Xie, Josiah P. Hanna, Robert Nowak</li>
<li>for: 本文研究了多任务表示学习bilinear bandit问题中的纯探索问题。</li>
<li>methods: 我们提出了名为GOBLIN的算法，使用实验设计方法来优化样本分配以学习全局表示以及尽量降低每个任务中最佳对的搜索时间。</li>
<li>results: 我们的结果表明，通过共享表示来加速tasks的搜索过程，可以大幅提高样本复杂度。<details>
<summary>Abstract</summary>
We study multi-task representation learning for the problem of pure exploration in bilinear bandits. In bilinear bandits, an action takes the form of a pair of arms from two different entity types and the reward is a bilinear function of the known feature vectors of the arms. In the \textit{multi-task bilinear bandit problem}, we aim to find optimal actions for multiple tasks that share a common low-dimensional linear representation. The objective is to leverage this characteristic to expedite the process of identifying the best pair of arms for all tasks. We propose the algorithm GOBLIN that uses an experimental design approach to optimize sample allocations for learning the global representation as well as minimize the number of samples needed to identify the optimal pair of arms in individual tasks. To the best of our knowledge, this is the first study to give sample complexity analysis for pure exploration in bilinear bandits with shared representation. Our results demonstrate that by learning the shared representation across tasks, we achieve significantly improved sample complexity compared to the traditional approach of solving tasks independently.
</details>
<details>
<summary>摘要</summary>
我们研究多任务表示学习bilinear投机问题中的纯探索问题。在bilinear投机问题中，一个动作是两个不同实体类型的两个臂的对，奖励是两个已知特征向量的bilinear函数。在我们称为多任务bilinear投机问题中，我们目标是找到共享低维度线性表示的优化动作，以便为所有任务快速确定最佳对。我们提出了GOBLIN算法，该算法使用实验设计方法优化样本分配，以学习全局表示并最小化各个任务中样本数量。根据我们所知，这是bilinear投机问题中纯探索的首次提供样本复杂性分析。我们的结果表明，通过共享表示来加速确定最佳对的过程，可以获得 significatively改进的样本复杂性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Hearing-Programming-Acoustic-Scenes-with-Binaural-Hearables"><a href="#Semantic-Hearing-Programming-Acoustic-Scenes-with-Binaural-Hearables" class="headerlink" title="Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables"></a>Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00320">http://arxiv.org/abs/2311.00320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bandhav Veluri, Malek Itani, Justin Chan, Takuya Yoshioka, Shyamnath Gollakota</li>
<li>for: 这个研究旨在实现智能耳机可以在实际环境中，在噪音和干扰音的情况下，实时专注或忽略特定的声音，同时保留空间对称信息。</li>
<li>methods: 这个研究使用了两项技术贡献：首先，我们提出了首个能够在噪音和背景噪音的情况下进行两声目标声抽象的神经网络，其中包括一个基于 transformer 的网络。其次，我们开发了一个可以对真实世界使用的训练方法，让我们的系统能够在不同的实际环境中进行扩展。</li>
<li>results: 我们的系统可以处理 20 种声音类型，并且在连接到智能手机的情况下，执行时间为 6.56 ms。在实际使用中，我们的证明系统能够在未见过的室内和外部enario中提取目标声音，并且能够保留空间对称信息。<details>
<summary>Abstract</summary>
Imagine being able to listen to the birds chirping in a park without hearing the chatter from other hikers, or being able to block out traffic noise on a busy street while still being able to hear emergency sirens and car honks. We introduce semantic hearing, a novel capability for hearable devices that enables them to, in real-time, focus on, or ignore, specific sounds from real-world environments, while also preserving the spatial cues. To achieve this, we make two technical contributions: 1) we present the first neural network that can achieve binaural target sound extraction in the presence of interfering sounds and background noise, and 2) we design a training methodology that allows our system to generalize to real-world use. Results show that our system can operate with 20 sound classes and that our transformer-based network has a runtime of 6.56 ms on a connected smartphone. In-the-wild evaluation with participants in previously unseen indoor and outdoor scenarios shows that our proof-of-concept system can extract the target sounds and generalize to preserve the spatial cues in its binaural output. Project page with code: https://semantichearing.cs.washington.edu
</details>
<details>
<summary>摘要</summary>
想象你在公园中听到鸟叫，而不听到其他游客的喊叫或 street 上的交通噪音，同时仍能听到紧急警响和汽车喊叫。我们介绍 semantic hearing，一种新的功能 для智能 Device，允许它们在实时中听到或忽略来自实际环境中的特定声音，而无需产生干扰。为达到这一目标，我们提出了两项技术贡献：1. 我们首次提出了一种可以在干扰声和背景噪音的情况下实现binarus target sound extraction的神经网络。2. 我们开发了一种可以让我们的系统在实际场景中进行泛化的训练方法。我们的系统可以处理20种声音类型，并且在连接到智能手机的情况下，runtime 为6.56ms。在实际场景中进行评估，我们的证明系统可以提取目标声音并保持它们的空间信息。项目页面包括代码：https://semantichearing.cs.washington.eduNote: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China. However, if you prefer Traditional Chinese, I can also provide the translation.
</details></li>
</ul>
<hr>
<h2 id="Federated-Topic-Model-and-Model-Pruning-Based-on-Variational-Autoencoder"><a href="#Federated-Topic-Model-and-Model-Pruning-Based-on-Variational-Autoencoder" class="headerlink" title="Federated Topic Model and Model Pruning Based on Variational Autoencoder"></a>Federated Topic Model and Model Pruning Based on Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00314">http://arxiv.org/abs/2311.00314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjie Ma, Yawen Li, Meiyu Liang, Ang Li</li>
<li>for: This paper proposes a method for establishing a federated topic model while ensuring the privacy of each node, and using neural network model pruning to accelerate the model.</li>
<li>methods: The proposed method uses neural network model pruning, where the client periodically sends the model neuron cumulative gradients and model weights to the server, and the server prunes the model. Two different methods are proposed to determine the model pruning rate.</li>
<li>results: Experimental results show that the federated topic model pruning based on the variational autoencoder proposed in this paper can greatly accelerate the model training speed while ensuring the model’s performance.<details>
<summary>Abstract</summary>
Topic modeling has emerged as a valuable tool for discovering patterns and topics within large collections of documents. However, when cross-analysis involves multiple parties, data privacy becomes a critical concern. Federated topic modeling has been developed to address this issue, allowing multiple parties to jointly train models while protecting pri-vacy. However, there are communication and performance challenges in the federated sce-nario. In order to solve the above problems, this paper proposes a method to establish a federated topic model while ensuring the privacy of each node, and use neural network model pruning to accelerate the model, where the client periodically sends the model neu-ron cumulative gradients and model weights to the server, and the server prunes the model. To address different requirements, two different methods are proposed to determine the model pruning rate. The first method involves slow pruning throughout the entire model training process, which has limited acceleration effect on the model training process, but can ensure that the pruned model achieves higher accuracy. This can significantly reduce the model inference time during the inference process. The second strategy is to quickly reach the target pruning rate in the early stage of model training in order to accelerate the model training speed, and then continue to train the model with a smaller model size after reaching the target pruning rate. This approach may lose more useful information but can complete the model training faster. Experimental results show that the federated topic model pruning based on the variational autoencoder proposed in this paper can greatly accelerate the model training speed while ensuring the model's performance.
</details>
<details>
<summary>摘要</summary>
《主题模型的泛化应用》已经成为大量文档中发现模式和话题的有价值工具。然而，当跨分析 involve多方面时，数据隐私成为关键问题。为解决这问题，联邦主题模型被开发出来，允许多方共同训练模型，保护每个节点的隐私。然而，联邦场景中存在通信和性能问题。为解决这些问题，本文提出了一种方法，可以在多方共同训练模型的同时，保证每个节点的隐私，并使用神经网络模型剪辑加速模型训练。在不同的需求下，本文提出了两种不同的方法来确定模型剪辑率。第一种方法是在整个模型训练过程中慢慢剪辑模型，可以在模型训练过程中减少模型的大小，但是这种方法具有限制模型训练速度的缺点。第二种方法是在模型训练过程的早期 quickly reach the target pruning rate，以加速模型训练速度，然后继续使用较小的模型大小进行模型训练。这种方法可能会产生更多的损失信息，但可以更快地完成模型训练。实验结果表明，基于 variational autoencoder 的联邦主题模型剪辑可以大幅提高模型训练速度，而不 sacrificing 模型性能。
</details></li>
</ul>
<hr>
<h2 id="Stacking-an-autoencoder-for-feature-selection-of-zero-day-threats"><a href="#Stacking-an-autoencoder-for-feature-selection-of-zero-day-threats" class="headerlink" title="Stacking an autoencoder for feature selection of zero-day threats"></a>Stacking an autoencoder for feature selection of zero-day threats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00304">http://arxiv.org/abs/2311.00304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmut Tokmak, Mike Nkongolo<br>for:This paper aims to detect zero-day attacks using a stacked autoencoder (SAE) and a Long Short-Term Memory (LSTM) scheme.methods:The paper uses preprocessing, feature selection, and supervised learning to train the SAE and LSTM models.results:The SAE-LSTM model achieves high precision, recall, and F1 score values in identifying various types of zero-day attacks, and generalizes effectively across different attack categories.Here’s the simplified Chinese text:for:这篇论文目的是使用堆叠自编码器（SAE）和长短期记忆（LSTM）方法探测零天攻击。methods:该论文使用预处理、特征选择和监睹学习来训练SAE和LSTM模型。results:SAE-LSTM模型在不同类型的零天攻击中表现出色，具有高精度、回归率和F1分数值，并能有效泛化到不同的攻击类别。<details>
<summary>Abstract</summary>
Zero-day attack detection plays a critical role in mitigating risks, protecting assets, and staying ahead in the evolving threat landscape. This study explores the application of stacked autoencoder (SAE), a type of artificial neural network, for feature selection and zero-day threat classification using a Long Short-Term Memory (LSTM) scheme. The process involves preprocessing the UGRansome dataset and training an unsupervised SAE for feature extraction. Finetuning with supervised learning is then performed to enhance the discriminative capabilities of this model. The learned weights and activations of the autoencoder are analyzed to identify the most important features for discriminating between zero-day threats and normal system behavior. These selected features form a reduced feature set that enables accurate classification. The results indicate that the SAE-LSTM performs well across all three attack categories by showcasing high precision, recall, and F1 score values, emphasizing the model's strong predictive capabilities in identifying various types of zero-day attacks. Additionally, the balanced average scores of the SAE-LSTM suggest that the model generalizes effectively and consistently across different attack categories.
</details>
<details>
<summary>摘要</summary>
zero-day 攻击检测扮演着关键的角色，帮助降低风险，保护资产，并在恶化的威胁领域保持领先地位。这项研究探讨了使用堆式自编码器（SAE），一种人工神经网络， для特征选择和 zero-day 威胁分类，使用Long Short-Term Memory（LSTM）方案。过程包括对UGRansome数据集进行预处理，并使用无监督SAE进行特征提取。然后，通过监督学习进行训练，以提高这个模型的推理能力。通过分析自编码器的权重和活化值，可以确定最重要的特征，以便准确地分类 zero-day 威胁和正常系统行为。这些选择的特征组成一个减少特征集，可以实现高精度的分类。结果表明，SAE-LSTM在所有三个攻击类别中表现出色，具有高精度、回归率和F1分数值，证明这个模型在不同类别的 zero-day 攻击中具有强的预测能力。此外，SAE-LSTM的平衡平均分数表明，这个模型在不同类别中一致地适应和泛化。
</details></li>
</ul>
<hr>
<h2 id="Model-driven-Engineering-for-Machine-Learning-Components-A-Systematic-Literature-Review"><a href="#Model-driven-Engineering-for-Machine-Learning-Components-A-Systematic-Literature-Review" class="headerlink" title="Model-driven Engineering for Machine Learning Components: A Systematic Literature Review"></a>Model-driven Engineering for Machine Learning Components: A Systematic Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00284">http://arxiv.org/abs/2311.00284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hira Naveed, Chetan Arora, Hourieh Khalajzadeh, John Grundy, Omar Haggag</li>
<li>for: 本研究的目的是通过系统atic literature review (SLR) 探讨现有的研究，了解使用模型驱动工程 (MDE) 技术和机器学习 (ML) 之间的优势交叉。</li>
<li>methods: 本研究分析了选择的研究，包括他们的动机、MDE解决方案、评估技术、关键成果和局限性。</li>
<li>results: 我们分析了选择的研究，并发现了以下几个方面：1）使用 MDE4ML 的主要动机；2）各种 MDE 解决方案，如模型语言、模型转换、工具支持、targeted ML 方面等；3）评估技术和指标；4）局限性和未来研究方向。<details>
<summary>Abstract</summary>
Context: Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber-physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components. Objective: The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations. Results: We analyzed selected studies with respect to several areas of interest and identified the following: 1) the key motivations behind using MDE4ML; 2) a variety of MDE solutions applied, such as modeling languages, model transformations, tool support, targeted ML aspects, contributions and more; 3) the evaluation techniques and metrics used; and 4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research. Conclusion: This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners
</details>
<details>
<summary>摘要</summary>
Machine Learning (ML) 已成为现代软件应用程序的重要组件。由于数据量庞大，组织希望通过数据来提取有用的信息，提高业务收益。 ML 组件提供预测能力、异常检测、建议、高精度图像和文本处理、 Informed Decision-Making 等功能。然而，开发具有 ML 组件的系统并不是易事，需要时间、努力、知识和 ML、数据处理和软件工程的专业知识。在过去，有多个研究用于使用 Model-Driven Engineering (MDE) 技术来解决在开发传统软件和Cyber-Physical Systems (CPS) 时出现的挑战。最近，关于应用 MDE 于 ML 系统的兴趣在增长。目标：本研究的目标是进一步探索 MDE 与 ML（MDE4ML）的优秀交叉点，通过系统性文献综述（SLR）。通过这个 SLR，我们想要分析已有的研究，包括他们的动机、MDE 解决方案、评估技术、关键优势和局限性。结果：我们对选择的研究进行了分析，并identified以下几个领域：1）使用 MDE4ML 的主要动机；2）MDE 解决方案的多样性，包括模型语言、模型转换、工具支持、针对 ML 方面的贡献等；3）评估技术和指标的使用；4）限制和未来研究的方向。我们还讨论了现有文献中的缺陷，并提供了未来研究的建议。结论：本 SLR  highlights 当前的趋势、缺陷和未来研究方向，对研究人员和实践者都有帮助。
</details></li>
</ul>
<hr>
<h2 id="Generalization-Bounds-for-Label-Noise-Stochastic-Gradient-Descent"><a href="#Generalization-Bounds-for-Label-Noise-Stochastic-Gradient-Descent" class="headerlink" title="Generalization Bounds for Label Noise Stochastic Gradient Descent"></a>Generalization Bounds for Label Noise Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00274">http://arxiv.org/abs/2311.00274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jung Eun Huh, Patrick Rebeschini</li>
<li>for: This paper is written for understanding the generalization error bounds of stochastic gradient descent (SGD) with label noise in non-convex settings.</li>
<li>methods: The paper uses the framework of algorithmic stability and the concept of Wasserstein distance to derive generalization error bounds for SGD with label noise.</li>
<li>results: The paper achieves time-independent generalization error bounds for the discretized algorithm with a constant learning rate, and the error bound scales polynomially with the dimension $d$ and with the rate of $n^{-2&#x2F;3}$, which is better than the best-known rate of $n^{-1&#x2F;2}$ for stochastic gradient Langevin dynamics (SGLD) under similar conditions.<details>
<summary>Abstract</summary>
We develop generalization error bounds for stochastic gradient descent (SGD) with label noise in non-convex settings under uniform dissipativity and smoothness conditions. Under a suitable choice of semimetric, we establish a contraction in Wasserstein distance of the label noise stochastic gradient flow that depends polynomially on the parameter dimension $d$. Using the framework of algorithmic stability, we derive time-independent generalisation error bounds for the discretized algorithm with a constant learning rate. The error bound we achieve scales polynomially with $d$ and with the rate of $n^{-2/3}$, where $n$ is the sample size. This rate is better than the best-known rate of $n^{-1/2}$ established for stochastic gradient Langevin dynamics (SGLD) -- which employs parameter-independent Gaussian noise -- under similar conditions. Our analysis offers quantitative insights into the effect of label noise.
</details>
<details>
<summary>摘要</summary>
我们研究了随机梯度 descent（SGD）在非конvex设定下对标签噪声的总化错误界限。在适当的semimetric下，我们证明了标签噪声随机梯度流的 Wasserstein距离减少的contraktion，这种减少速率取决于参数维度$d$。使用算法稳定性框架，我们 derivetime-independent的总化错误界限 для精细化算法，该界限与参数维度$d$和学习率$n$相乘，其中$n$是样本大小。这个速率比best-known的$n^{-1/2}$更好，这个速率是在类似条件下随机梯度勒文动力（SGLD）下获得的。我们的分析带来了标签噪声的量化理解。
</details></li>
</ul>
<hr>
<h2 id="Incentivized-Collaboration-in-Active-Learning"><a href="#Incentivized-Collaboration-in-Active-Learning" class="headerlink" title="Incentivized Collaboration in Active Learning"></a>Incentivized Collaboration in Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00260">http://arxiv.org/abs/2311.00260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lee Cohen, Han Shao</li>
<li>for: 这篇论文是关于多 Agent 协同学习中的奖励协同，即多个代理 Trying to learn labels from a common hypothesis。</li>
<li>methods: 这篇论文使用了一种创新的框架，即奖励协同，以增加代理之间的协作。这种协作的目的是使代理们可以获得最少的标签复杂性。作者们关注的是设计(严格)个人合理（IR）合作协议，以确保代理不能通过单独行动减少其预期标签复杂性。</li>
<li>results: 作者们第一先表明，给定任何优化的活动学习算法，与整个数据集进行协同的协作协议就是IR的。然而，计算优化算法是NP困难的。因此，作者们提供了一些IR的协作协议，可以与最佳可追踪的标签复杂性相比。<details>
<summary>Abstract</summary>
In collaborative active learning, where multiple agents try to learn labels from a common hypothesis, we introduce an innovative framework for incentivized collaboration. Here, rational agents aim to obtain labels for their data sets while keeping label complexity at a minimum. We focus on designing (strict) individually rational (IR) collaboration protocols, ensuring that agents cannot reduce their expected label complexity by acting individually. We first show that given any optimal active learning algorithm, the collaboration protocol that runs the algorithm as is over the entire data is already IR. However, computing the optimal algorithm is NP-hard. We therefore provide collaboration protocols that achieve (strict) IR and are comparable with the best known tractable approximation algorithm in terms of label complexity.
</details>
<details>
<summary>摘要</summary>
在合作主动学习中，多个代理人尝试从共同假设中学习标签。我们提出了一个创新的激励合作框架，在这个框架中，理性的代理人尝试获取标签以减少标签复杂性。我们专注于设计（严格）合作协议，以确保代理人不能通过单独行动将标签复杂性降低。我们首先显示，任何最佳活动学习算法都可以在整个数据集上运行，并且已经是合作协议中的严格合作（IR）。但计算最佳算法是NP困难的。我们因此提供了一些合作协议，它们可以确保代理人不能透过单独行动将标签复杂性降低，并且与已知可追踪的数学方法相比，具有相似的标签复杂性。
</details></li>
</ul>
<hr>
<h2 id="Active-Neural-Topological-Mapping-for-Multi-Agent-Exploration"><a href="#Active-Neural-Topological-Mapping-for-Multi-Agent-Exploration" class="headerlink" title="Active Neural Topological Mapping for Multi-Agent Exploration"></a>Active Neural Topological Mapping for Multi-Agent Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00252">http://arxiv.org/abs/2311.00252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Yang, Yuxiang Yang, Chao Yu, Jiayu Chen, Jingchen Yu, Haibing Ren, Huazhong Yang, Yu Wang<br>for: 本研究目的是提高多体协同探索任务的效率和泛化能力。methods: 本研究使用的方法包括 neural topological mapping 和 hierarchical topological planning。results: 实验结果表明，相比计划基eline和RL基eline，MANTM可以降低步数少于26.40%，并在未看过enario中提高效率。<details>
<summary>Abstract</summary>
This paper investigates the multi-agent cooperative exploration problem, which requires multiple agents to explore an unseen environment via sensory signals in a limited time. A popular approach to exploration tasks is to combine active mapping with planning. Metric maps capture the details of the spatial representation, but are with high communication traffic and may vary significantly between scenarios, resulting in inferior generalization. Topological maps are a promising alternative as they consist only of nodes and edges with abstract but essential information and are less influenced by the scene structures. However, most existing topology-based exploration tasks utilize classical methods for planning, which are time-consuming and sub-optimal due to their handcrafted design. Deep reinforcement learning (DRL) has shown great potential for learning (near) optimal policies through fast end-to-end inference. In this paper, we propose Multi-Agent Neural Topological Mapping (MANTM) to improve exploration efficiency and generalization for multi-agent exploration tasks. MANTM mainly comprises a Topological Mapper and a novel RL-based Hierarchical Topological Planner (HTP). The Topological Mapper employs a visual encoder and distance-based heuristics to construct a graph containing main nodes and their corresponding ghost nodes. The HTP leverages graph neural networks to capture correlations between agents and graph nodes in a coarse-to-fine manner for effective global goal selection. Extensive experiments conducted in a physically-realistic simulator, Habitat, demonstrate that MANTM reduces the steps by at least 26.40% over planning-based baselines and by at least 7.63% over RL-based competitors in unseen scenarios.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文研究了多机合作探索问题，即多个机器人需要在未知环境中通过感知信号进行探索，并且在有限时间内完成。一种常见的方法是将活动地图与计划结合，但这可能会带来高度的通信压力和场景结构的变化，导致性能下降。为了解决这个问题，作者们提出了多机 neural topological mapping（MANTM），它包括一个 topological mapper 和一个基于RL的层次 topological planner（HTP）。topological mapper 使用视觉编码器和距离基于的规则来构建一个图，包括主节点和其对应的幽灵节点，而 HTP 利用图 neural networks 来捕捉机器人和图节点之间的相关性，并在层次结构中进行有效的全局目标选择。作者们在一个物理实际的 simulate 中进行了广泛的实验，并证明了 MANTM 可以在未知场景中减少步骤数量，相比计划基本elines 和 RL 基本elines 的减少量分别为至少 26.40% 和 7.63%。
</details></li>
</ul>
<hr>
<h2 id="DistDNAS-Search-Efficient-Feature-Interactions-within-2-Hours"><a href="#DistDNAS-Search-Efficient-Feature-Interactions-within-2-Hours" class="headerlink" title="DistDNAS: Search Efficient Feature Interactions within 2 Hours"></a>DistDNAS: Search Efficient Feature Interactions within 2 Hours</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00231">http://arxiv.org/abs/2311.00231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tunhou Zhang, Wei Wen, Igor Fedorov, Xi Liu, Buyun Zhang, Fangqiu Han, Wen-Yen Chen, Yiping Han, Feng Yan, Hai Li, Yiran Chen</li>
<li>for: 提高推荐系统的功能交互设计效率和服务效率</li>
<li>methods: 提出DistDNAS算法，通过设计各种类型和顺序的交互模块搜索空间，并通过分布式搜索和数据日期分配优化搜索效率，实现25倍的速度提升和2天减少为2小时的搜索成本。同时，DistDNAS引入可微分的成本意识损失函数，惩罚选择重复的交互模块，提高发现的功能交互效率。</li>
<li>results: 对1TB criterio terabyte dataset进行广泛的实验评估，表明DistDNAS可以提供0.001 AUC提升和60% FLOPs减少的Current state-of-the-art CTR模型。<details>
<summary>Abstract</summary>
Search efficiency and serving efficiency are two major axes in building feature interactions and expediting the model development process in recommender systems. On large-scale benchmarks, searching for the optimal feature interaction design requires extensive cost due to the sequential workflow on the large volume of data. In addition, fusing interactions of various sources, orders, and mathematical operations introduces potential conflicts and additional redundancy toward recommender models, leading to sub-optimal trade-offs in performance and serving cost. In this paper, we present DistDNAS as a neat solution to brew swift and efficient feature interaction design. DistDNAS proposes a supernet to incorporate interaction modules of varying orders and types as a search space. To optimize search efficiency, DistDNAS distributes the search and aggregates the choice of optimal interaction modules on varying data dates, achieving over 25x speed-up and reducing search cost from 2 days to 2 hours. To optimize serving efficiency, DistDNAS introduces a differentiable cost-aware loss to penalize the selection of redundant interaction modules, enhancing the efficiency of discovered feature interactions in serving. We extensively evaluate the best models crafted by DistDNAS on a 1TB Criteo Terabyte dataset. Experimental evaluations demonstrate 0.001 AUC improvement and 60% FLOPs saving over current state-of-the-art CTR models.
</details>
<details>
<summary>摘要</summary>
搜索效率和服务效率是建立功能交互和加速模型开发过程中的两个主要轴。在大规模 benchmark 上，搜索最佳功能交互设计需要巨大的成本，因为执行大量数据的顺序工作流程。此外，将不同来源、顺序和数学运算的交互整合到推荐模型中，会导致性能和服务成本之间的冲突和额外冗余。在这篇论文中，我们提出了 DistDNAS，一种简洁的解决方案，用于快速和高效地设计功能交互。DistDNAS 提出了一个超网，用于包含交互模块的不同顺序和类型作为搜索空间。为了优化搜索效率，DistDNAS 将搜索分布到不同的数据日期上，并对选择优化交互模块的选择进行整合，实现了25倍的速度提升，从2天减少到2小时。为了优化服务效率，DistDNAS 引入了可 diferenciable 成本意识损失，以惩罚选择 redundancy 的交互模块，提高发现的功能交互效率在服务中。我们对一个1TB Criteo Terabyte  dataset进行了广泛的实验评估。实验结果显示，由 DistDNAS 打造的最佳模型在 AUC 方面提高了0.001，并在Current State-of-the-art CTR 模型中减少了60%的计算成本。
</details></li>
</ul>
<hr>
<h2 id="Transformers-are-Efficient-In-Context-Estimators-for-Wireless-Communication"><a href="#Transformers-are-Efficient-In-Context-Estimators-for-Wireless-Communication" class="headerlink" title="Transformers are Efficient In-Context Estimators for Wireless Communication"></a>Transformers are Efficient In-Context Estimators for Wireless Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00226">http://arxiv.org/abs/2311.00226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vicram Rajagopalan, Vishnu Teja Kunde, Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Srinivas Shakkottai, Dileep Kalathil, Jean-Francois Chamberland</li>
<li>for: 这种方法是为了解决通信问题，即估算发送的符号从接收的符号中的损失。</li>
<li>methods: 我们使用了基于 transformer 的受限制推论方法，使其能够在只有少量引言的情况下，通过自动学习，来进行受限制推论。</li>
<li>results: 我们通过了广泛的 simulate，显示这种方法不仅可以大幅提高性能，而且可以在几个上下文示例后达到与完全知情上下文情况下的同样性。<details>
<summary>Abstract</summary>
Pre-trained transformers can perform in-context learning, where they adapt to a new task using only a small number of prompts without any explicit model optimization. Inspired by this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine the latent context from pilot symbols to perform end-to-end in-context estimation of transmitted symbols. Furthermore, the transformer should use information efficiently, i.e., it should utilize any pilots received to attain the best possible symbol estimates. Through extensive simulations, we show that in-context estimation not only significantly outperforms standard approaches, but also achieves the same performance as an estimator with perfect knowledge of the latent context within a few context examples. Thus, we make a strong case that transformers are efficient in-context estimators in the communication setting.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)预训练的转换器可以进行境TEXTlearning，在只有少量提示下自适应新任务，而无需显式模型优化。 Drawing inspiration from this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine the latent context from pilot symbols to perform end-to-end in-context estimation of transmitted symbols. Furthermore, the transformer should use information efficiently, i.e., it should utilize any pilots received to attain the best possible symbol estimates. Through extensive simulations, we show that in-context estimation not only significantly outperforms standard approaches, but also achieves the same performance as an estimator with perfect knowledge of the latent context within a few context examples. Therefore, we make a strong case that transformers are efficient in-context estimators in the communication setting.
</details></li>
</ul>
<hr>
<h2 id="WinNet-time-series-forecasting-with-a-window-enhanced-period-extracting-and-interacting"><a href="#WinNet-time-series-forecasting-with-a-window-enhanced-period-extracting-and-interacting" class="headerlink" title="WinNet:time series forecasting with a window-enhanced period extracting and interacting"></a>WinNet:time series forecasting with a window-enhanced period extracting and interacting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00214">http://arxiv.org/abs/2311.00214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Ou, Dongyue Guo, Zheng Zhang, Zhishuo Zhao, Yi Lin</li>
<li>for: 这个研究旨在提出一个高精度且简单结构的 CNN 模型，用于长期时间序列预测任务。</li>
<li>methods: 本研究使用了 Inter-Intra Period Encoder (I2PE)、Two-Dimensional Period Decomposition (TDPD) 和 Decomposition Correlation Block (DCB) 等技术，将 1D 序列转换为 2D 矩阵，并模型长期和短期周期性。</li>
<li>results: 在九个benchmark datasets上，WinNet 能够 achieve SOTA 性能，并且比 CNN、MLP 和 Transformer 等方法来的computational complexity较低。<details>
<summary>Abstract</summary>
Recently, Transformer-based methods have significantly improved state-of-the-art time series forecasting results, but they suffer from high computational costs and the inability to capture the long and short periodicity of time series. We present a highly accurate and simply structured CNN-based model for long-term time series forecasting tasks, called WinNet, including (i) Inter-Intra Period Encoder (I2PE) to transform 1D sequence into 2D tensor with long and short periodicity according to the predefined periodic window, (ii) Two-Dimensional Period Decomposition (TDPD) to model period-trend and oscillation terms, and (iii) Decomposition Correlation Block (DCB) to leverage the correlations of the period-trend and oscillation terms to support the prediction tasks by CNNs. Results on nine benchmark datasets show that the WinNet can achieve SOTA performance and lower computational complexity over CNN-, MLP-, Transformer-based approaches. The WinNet provides potential for the CNN-based methods in the time series forecasting tasks, with perfect tradeoff between performance and efficiency.
</details>
<details>
<summary>摘要</summary>
最近，基于Transformer的方法在时间序列预测中取得了显著改进，但它们受到高计算成本和不能捕捉时间序列的长短周期性的限制。我们介绍了一种高精度且简单结构的Convolutional Neural Network（CNN）模型，称为WinNet，用于长期时间序列预测任务。该模型包括以下三部分：1. 间隔内 period编码器（I2PE）：将1D序列转化为2D张量，同时捕捉长周期和短周期性。2. 二维 periodic decomposition（TDPD）：模型 periodic-trend和抽象oscillation项。3. 分解相关块（DCB）：利用 periodic-trend和抽象oscillation项之间的相关性，支持预测任务。Results on nine benchmark datasets show that WinNet can achieve state-of-the-art performance and lower computational complexity than CNN-, MLP-, Transformer-based approaches. WinNet provides a potential solution for CNN-based methods in time series forecasting tasks, with a perfect tradeoff between performance and efficiency.
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Framework-to-Enforce-Discover-and-Promote-Symmetry-in-Machine-Learning"><a href="#A-Unified-Framework-to-Enforce-Discover-and-Promote-Symmetry-in-Machine-Learning" class="headerlink" title="A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning"></a>A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00212">http://arxiv.org/abs/2311.00212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel E. Otto, Nicholas Zolman, J. Nathan Kutz, Steven L. Brunton</li>
<li>for: 本 paper 旨在探讨如何在机器学习模型中 incorporate Symmetry，以提高模型的泛化能力和性能。</li>
<li>methods: 本 paper 使用了 Lie  derivative 和 fibre-linear Lie group actions on vector bundles 等 математиче工具，提供了一种统一的理论和方法框架，用于在机器学习模型中 enforcing 和 discovering Symmetry。</li>
<li>results: 本 paper 的结果表明，在 enforcing 和 discovering Symmetry 的过程中，可以通过 linear-algebraic 任务和 dual 性来捕捉 Symmetry 的特性，并且可以通过 convex 规则函数和 nuclear norm relaxation 来杜缺 Symmetry breaking。这些想法可以应用于各种机器学习模型，如基函数回归、动力系统发现、多层感知器和图像空间中的神经网络。<details>
<summary>Abstract</summary>
Symmetry is present throughout nature and continues to play an increasingly central role in physics and machine learning. Fundamental symmetries, such as Poincar\'{e} invariance, allow physical laws discovered in laboratories on Earth to be extrapolated to the farthest reaches of the universe. Symmetry is essential to achieving this extrapolatory power in machine learning applications. For example, translation invariance in image classification allows models with fewer parameters, such as convolutional neural networks, to be trained on smaller data sets and achieve state-of-the-art performance. In this paper, we provide a unifying theoretical and methodological framework for incorporating symmetry into machine learning models in three ways: 1. enforcing known symmetry when training a model; 2. discovering unknown symmetries of a given model or data set; and 3. promoting symmetry during training by learning a model that breaks symmetries within a user-specified group of candidates when there is sufficient evidence in the data. We show that these tasks can be cast within a common mathematical framework whose central object is the Lie derivative associated with fiber-linear Lie group actions on vector bundles. We extend and unify several existing results by showing that enforcing and discovering symmetry are linear-algebraic tasks that are dual with respect to the bilinear structure of the Lie derivative. We also propose a novel way to promote symmetry by introducing a class of convex regularization functions based on the Lie derivative and nuclear norm relaxation to penalize symmetry breaking during training of machine learning models. We explain how these ideas can be applied to a wide range of machine learning models including basis function regression, dynamical systems discovery, multilayer perceptrons, and neural networks acting on spatial fields such as images.
</details>
<details>
<summary>摘要</summary>
自然中的对称性在物理和机器学习中发挥了越来越重要的作用。基本对称性，如波兰几何对称性，使得在地球室内实验室中发现的物理法则可以在宇宙中的最远处适用。对称性是机器学习应用中达到这种推断力的关键。例如，图像分类中的翻译对称性使得使用 fewer parameters的卷积神经网络来训练小型数据集，并达到状态对应的性能。在这篇文章中，我们提供一种统一的理论和方法框架，用于在机器学习模型中包含对称性。我们的方法包括：1. 在训练模型时强制执行已知对称性; 2. 发现数据集或模型未知的对称性; 3. 在用户指定的对称组中，通过学习模型破坏对称性来提高模型性能。我们表明这些任务可以在共同的数学框架中进行，核心对象是在纤维线性 Lie 群行为下的 Lie DERIVATIVE。我们扩展和统一了一些现有结果，并证明了强制和发现对称性是线性代数任务，这些任务与模型数据的双线性结构相关。我们还提出了一种新的对称优化方法，基于 Lie DERIVATIVE 和核心 нор relaxation 来强制模型在训练过程中遵循对称性。我们解释了这些想法如何应用于各种机器学习模型，包括基准函数回归、动力系统发现、多层感知网络和图像空间中的神经网络。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-for-accuracy-in-density-functional-approximations"><a href="#Machine-learning-for-accuracy-in-density-functional-approximations" class="headerlink" title="Machine learning for accuracy in density functional approximations"></a>Machine learning for accuracy in density functional approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.00196">http://arxiv.org/abs/2311.00196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johannes Voss</li>
<li>for: 提高 atomistic  simulations 和材料设计 的速度和精度</li>
<li>methods: 使用机器学习技术</li>
<li>results: 提高 computationally efficient electronic structure methods 的预测力和正确性<details>
<summary>Abstract</summary>
Machine learning techniques have found their way into computational chemistry as indispensable tools to accelerate atomistic simulations and materials design. In addition, machine learning approaches hold the potential to boost the predictive power of computationally efficient electronic structure methods, such as density functional theory, to chemical accuracy and to correct for fundamental errors in density functional approaches. Here, recent progress in applying machine learning to improve the accuracy of density functional and related approximations is reviewed. Promises and challenges in devising machine learning models transferable between different chemistries and materials classes are discussed with the help of examples applying promising models to systems far outside their training sets.
</details>
<details>
<summary>摘要</summary>
机器学习技术已经在计算化学中成为不可或缺的工具，以加速原子尺度模拟和材料设计。此外，机器学习方法还拥有提高计算效率的电子结构方法的预测能力的潜力，以及对density functional方法中的基本错误进行修正的能力。本文将 recensents recent progress in applying machine learning to improve the accuracy of density functional and related approximations. 以及在不同化学和材料类中设计可质量转移的机器学习模型的挑战和推荐。通过例子，应用有前景的模型到远 outside its training set。Translation notes:* 计算化学 (computational chemistry) is translated as 计算化学 (computational chemistry)* 原子尺度模拟 (atomistic simulations) is translated as 原子尺度模拟 (atomistic simulations)* density functional theory (DFT) is translated as density functional theory (DFT)* 机器学习 (machine learning) is translated as 机器学习 (machine learning)* 可质量转移 (transfer learning) is translated as 可质量转移 (transfer learning)* 材料类 (materials class) is translated as 材料类 (materials class)
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/01/cs.LG_2023_11_01/" data-id="cloh7tqkn00rn7b88dhtadkzb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/01/cs.CL_2023_11_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-11-01
        
      </div>
    </a>
  
  
    <a href="/2023/11/01/eess.IV_2023_11_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-11-01</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">112</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">62</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
