
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-11-09 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05608 repo_url: https:&#x2F;&#x2F;github.com&#x2F;thuccslab&#x2F;figstep paper_authors: Yichen Gong,">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-11-09">
<meta property="og:url" content="https://nullscc.github.io/2023/11/09/cs.AI_2023_11_09/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05608 repo_url: https:&#x2F;&#x2F;github.com&#x2F;thuccslab&#x2F;figstep paper_authors: Yichen Gong,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-09T12:00:00.000Z">
<meta property="article:modified_time" content="2023-11-10T09:25:48.036Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/cs.AI_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T12:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-11-09
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FigStep-Jailbreaking-Large-Vision-language-Models-via-Typographic-Visual-Prompts"><a href="#FigStep-Jailbreaking-Large-Vision-language-Models-via-Typographic-Visual-Prompts" class="headerlink" title="FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"></a>FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05608">http://arxiv.org/abs/2311.05608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thuccslab/figstep">https://github.com/thuccslab/figstep</a></li>
<li>paper_authors: Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang</li>
<li>for: 本研究旨在展示多modalities导致AI安全问题的可能性。</li>
<li>methods: 该研究提出了一种名为FigStep的攻击框架，通过图像通道输入危险指令，然后使用无害文本提示 induced VLMs输出违反常规AI安全政策的内容。</li>
<li>results: 实验结果显示，FigStep可以在2家流行的开源VLMs（LLaVA和MiniGPT4）上 achieved an average attack success rate of 94.8%（共5个VLMs）。此外，我们还示出了FigStep方法可以甚至破坏GPT-4V，该模型已经利用了多种系统级别的机制来过滤危险查询。<details>
<summary>Abstract</summary>
Large vision-language models (VLMs) like GPT-4V represent an unprecedented revolution in the field of artificial intelligence (AI). Compared to single-modal large language models (LLMs), VLMs possess more versatile capabilities by incorporating additional modalities (e.g., images). Meanwhile, there's a rising enthusiasm in the AI community to develop open-source VLMs, such as LLaVA and MiniGPT4, which, however, have not undergone rigorous safety assessment. In this paper, to demonstrate that more modalities lead to unforeseen AI safety issues, we propose FigStep, a novel jailbreaking framework against VLMs. FigStep feeds harmful instructions into VLMs through the image channel and then uses benign text prompts to induce VLMs to output contents that violate common AI safety policies. Our experimental results show that FigStep can achieve an average attack success rate of 94.8% across 2 families of popular open-source VLMs, LLaVA and MiniGPT4 (a total of 5 VLMs). Moreover, we demonstrate that the methodology of FigStep can even jailbreak GPT-4V, which already leverages several system-level mechanisms to filter harmful queries. Above all, our experimental results reveal that VLMs are vulnerable to jailbreaking attacks, which highlights the necessity of novel safety alignments between visual and textual modalities.
</details>
<details>
<summary>摘要</summary>
大型视语模型（VLM）如GPT-4V在人工智能（AI）领域表现了无 precedent 的革命。相比单modal大语言模型（LLM），VLM具有更多的多样化能力，通过添加其他模альности（如图像）。然而，AI社区中的开源VLM的开发，如LLaVA和MiniGPT4，尚未经过严格的安全评估。在这篇论文中，我们提出了FigStep，一种新的破坏框架，用于对VLM进行攻击。FigStep通过图像频道输入坏征 instruction，然后使用恰好的文本提示来让VLM输出违反常见AI安全政策的内容。我们的实验结果表明，FigStep可以在2家流行的开源VLM中（LLaVA和MiniGPT4，共5个VLM） achieved an average attack success rate of 94.8%。此外，我们还证明了FigStep的方法可以破坏GPT-4V，这个模型已经利用了多种系统级别的机制来筛选坏征查询。总之，我们的实验结果表明，VLM具有破坏攻击的潜在隐患，这引起了AI安全政策的重新对齐。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-Rasterization-for-Large-Scenes"><a href="#Real-Time-Neural-Rasterization-for-Large-Scenes" class="headerlink" title="Real-Time Neural Rasterization for Large Scenes"></a>Real-Time Neural Rasterization for Large Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05607">http://arxiv.org/abs/2311.05607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeffrey Yunfan Liu, Yun Chen, Ze Yang, Jingkang Wang, Sivabalan Manivasagam, Raquel Urtasun</li>
<li>for: 大型场景的实时新视角 sintesis</li>
<li>methods: 组合 neural texture field 和 shader，使用标准图形管道进行实时渲染</li>
<li>results: 比较 Neil 渲染方法快30倍，并提供了相当或更好的现实感，适用于大型自驾车和无人机场景<details>
<summary>Abstract</summary>
We propose a new method for realistic real-time novel-view synthesis (NVS) of large scenes. Existing neural rendering methods generate realistic results, but primarily work for small scale scenes (<50 square meters) and have difficulty at large scale (>10000 square meters). Traditional graphics-based rasterization rendering is fast for large scenes but lacks realism and requires expensive manually created assets. Our approach combines the best of both worlds by taking a moderate-quality scaffold mesh as input and learning a neural texture field and shader to model view-dependant effects to enhance realism, while still using the standard graphics pipeline for real-time rendering. Our method outperforms existing neural rendering methods, providing at least 30x faster rendering with comparable or better realism for large self-driving and drone scenes. Our work is the first to enable real-time rendering of large real-world scenes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于实现真实时间的新观点合成（NVS）大景。现有的神经渲染方法可以生成真实的结果，但主要适用于小规模场景（<50平方米），大规模场景（>10000平方米）很难。传统的图形学基础的排版渲染快速渲染大场景，但缺乏真实感和需要费时手动创建资产。我们的方法将神经渲染和排版渲染结合，使用中等质量的框架网格作为输入，学习神经xture场和渲染程序来提高真实感，同时仍然使用标准图形管道进行实时渲染。我们的方法在比较30倍快于现有神经渲染方法，并且与或更好的真实感在大自驾和无人机场景中提供了相似或更好的图形效果。我们的工作是首次实现了真实时间渲染大自然场景。
</details></li>
</ul>
<hr>
<h2 id="SynH2R-Synthesizing-Hand-Object-Motions-for-Learning-Human-to-Robot-Handovers"><a href="#SynH2R-Synthesizing-Hand-Object-Motions-for-Learning-Human-to-Robot-Handovers" class="headerlink" title="SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers"></a>SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05599">http://arxiv.org/abs/2311.05599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sammy Christen, Lan Feng, Wei Yang, Yu-Wei Chao, Otmar Hilliges, Jie Song</li>
<li>for: 本研究旨在提供一种可以生成人类抓取动作的框架，以便在训练机器人时使用。</li>
<li>methods: 该方法使用了手掌对象合成技术，以生成与人类抓取动作相似的机器人抓取动作。</li>
<li>results: 研究表明，使用该方法可以在实际环境中训练机器人，并且可以更好地扩展到各种不同的物体和人类抓取动作。In English:</li>
<li>for: The paper aims to provide a framework that can generate human grasping motions for training robots.</li>
<li>methods: The method uses hand-object synthesis technology to generate robot grasping motions similar to humans.</li>
<li>results: The study shows that the method can be used to train robots in real-world environments and can better scale to a variety of objects and human grasping motions.<details>
<summary>Abstract</summary>
Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines. Project page: https://eth-ait.github.io/synthetic-handovers/
</details>
<details>
<summary>摘要</summary>
“视觉基于人机交互的人机交换是人机机器交互中的重要和挑战性任务。latest work 尝试通过在模拟环境中与动态虚拟人进行交互，以训练机器人策略，但它们受到人体运动捕捉数据的依赖，这是 expensive 和难以扩展到任意对象和人类抓取动作。在本文中，我们介绍了一个框架，可以生成人类抓取动作，可以用于训练机器人。为 achieve 这一目标，我们提出了一种手套物合成方法，可以生成人类抓取动作的类似模拟数据。这使得我们可以生成 Synthetic 训练和测试数据，与前一代方法相比，具有100倍更多的对象。在我们的实验中，我们表明我们的方法，只使用 Synthetic 数据进行训练，与当前的状态艺术方法相比，在模拟环境和真实系统上具有类似的性能。此外，我们可以在更大的规模上进行评估，与先前的工作相比。我们新引入的测试集，显示我们的模型可以更好地扩展到大量的未看到对象和人类动作。项目页面：https://eth-ait.github.io/synthetic-handovers/”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="LLM-Augmented-Hierarchical-Agents"><a href="#LLM-Augmented-Hierarchical-Agents" class="headerlink" title="LLM Augmented Hierarchical Agents"></a>LLM Augmented Hierarchical Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05596">http://arxiv.org/abs/2311.05596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bharat Prakash, Tim Oates, Tinoosh Mohsenin</li>
<li>for: 这篇论文目的是解决长期任务和时间扩展任务中的束缚学习问题。</li>
<li>methods: 该论文使用了人工智能和强化学习，并利用了大语言模型（LLMs）的规划能力，以便在环境学习中提供学习。</li>
<li>results: 在模拟环境和真实 робо臂上进行了实验，并显示了使用该方法让代理人比其他基elines方法表现更好，并且一旦训练完成，不需要在部署时再次访问LLMs。<details>
<summary>Abstract</summary>
Solving long-horizon, temporally-extended tasks using Reinforcement Learning (RL) is challenging, compounded by the common practice of learning without prior knowledge (or tabula rasa learning). Humans can generate and execute plans with temporally-extended actions and quickly learn to perform new tasks because we almost never solve problems from scratch. We want autonomous agents to have this same ability. Recently, LLMs have been shown to encode a tremendous amount of knowledge about the world and to perform impressive in-context learning and reasoning. However, using LLMs to solve real world problems is hard because they are not grounded in the current task. In this paper we exploit the planning capabilities of LLMs while using RL to provide learning from the environment, resulting in a hierarchical agent that uses LLMs to solve long-horizon tasks. Instead of completely relying on LLMs, they guide a high-level policy, making learning significantly more sample efficient. This approach is evaluated in simulation environments such as MiniGrid, SkillHack, and Crafter, and on a real robot arm in block manipulation tasks. We show that agents trained using our approach outperform other baselines methods and, once trained, don't need access to LLMs during deployment.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accuracy-of-a-Vision-Language-Model-on-Challenging-Medical-Cases"><a href="#Accuracy-of-a-Vision-Language-Model-on-Challenging-Medical-Cases" class="headerlink" title="Accuracy of a Vision-Language Model on Challenging Medical Cases"></a>Accuracy of a Vision-Language Model on Challenging Medical Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05591">http://arxiv.org/abs/2311.05591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/2v/gpt4v-image-challenge">https://github.com/2v/gpt4v-image-challenge</a></li>
<li>paper_authors: Thomas Buckley, James A. Diao, Adam Rodman, Arjun K. Manrai</li>
<li>for: The paper is written to evaluate the accuracy of the Generative Pre-trained Transformer 4 with Vision (GPT-4V) model on challenging medical cases.</li>
<li>methods: The paper uses 934 cases from the NEJM Image Challenge to evaluate the accuracy of GPT-4V compared to human respondents, and also conducts a physician evaluation of GPT-4V on 69 NEJM clinicopathological conferences (CPCs).</li>
<li>results: GPT-4V achieved an overall accuracy of 61% compared to 49% for humans, and outperformed humans at all levels of difficulty and disagreement, skin tones, and image types. However, performance deteriorated when images were added to highly informative text.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是用来评估Generative Pre-trained Transformer 4 with Vision（GPT-4V）模型在医学挑战性案例中的准确性。</li>
<li>methods: 这篇论文使用了934个来自NEJM Image Challenge的案例来评估GPT-4V与人类回答者的准确性，并进行了69个NEJM临床Pathological Conferences（CPCs）的医生评估。</li>
<li>results: GPT-4V在总体准确性方面取得了61%（95% CI，58%到64%），与人类回答者的49%（95% CI，49%到50%）相比，GPT-4V在所有水平和不同的难度、肤色和图像类型上都超过了人类回答者。但是，当图像被添加到高度信息的文本时，GPT-4V的性能下降。<details>
<summary>Abstract</summary>
Background: General-purpose large language models that utilize both text and images have not been evaluated on a diverse array of challenging medical cases.   Methods: Using 934 cases from the NEJM Image Challenge published between 2005 and 2023, we evaluated the accuracy of the recently released Generative Pre-trained Transformer 4 with Vision model (GPT-4V) compared to human respondents overall and stratified by question difficulty, image type, and skin tone. We further conducted a physician evaluation of GPT-4V on 69 NEJM clinicopathological conferences (CPCs). Analyses were conducted for models utilizing text alone, images alone, and both text and images.   Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%) compared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans at all levels of difficulty and disagreement, skin tones, and image types; the exception was radiographic images, where performance was equivalent between GPT-4V and human respondents. Longer, more informative captions were associated with improved performance for GPT-4V but similar performance for human respondents. GPT-4V included the correct diagnosis in its differential for 80% (95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45 to 70%) of CPCs when using both images and text.   Conclusions: GPT-4V outperformed human respondents on challenging medical cases and was able to synthesize information from both images and text, but performance deteriorated when images were added to highly informative text. Overall, our results suggest that multimodal AI models may be useful in medical diagnostic reasoning but that their accuracy may depend heavily on context.
</details>
<details>
<summary>摘要</summary>
背景：大型语言模型，使用文本和图像，尚未在多元化和复杂的医学案例上进行评估。方法：使用2005-2023年度《新英格兰医学杂志》（NEJM）的934个挑战 caso，评估最近发布的生成推训Transformer 4 with Vision（GPT-4V）模型与人类回答者的精度，并按问题难度、图像类型和皮肤颜色进行分类。此外，我们还进行了69名医生对GPT-4V的评估，用于1995-2023年度《新英格兰医学杂志》临床 PATHOLOGICAL CONFERENCES（CPCs）。分析方法包括文本 alone、图像 alone和文本和图像的组合。结果：GPT-4V的总精度为61%（95% CI，58%-64%），高于人类回答者的49%（95% CI，49%-50%）。GPT-4V在所有难度和不同程度、皮肤颜色和图像类型中都表现出优异，只有辐射图像类型的表现与人类回答者相当。 longer、更加详细的描述与GPT-4V的表现相似，而人类回答者的表现则不变。GPT-4V使用文本alone时包含正确的诊断在其分布中的比例为80%（95% CI，68%-88%），与使用文本和图像时相同。结论：GPT-4V在医学案例中表现出优异，能够从文本和图像中synthesize信息，但是在图像添加到高度信息的文本时，表现下降。总的来说，我们的结果表明，多模态AI模型可能在医学诊断理由中有用，但其精度可能受上下文的影响。
</details></li>
</ul>
<hr>
<h2 id="Conversational-AI-Threads-for-Visualizing-Multidimensional-Datasets"><a href="#Conversational-AI-Threads-for-Visualizing-Multidimensional-Datasets" class="headerlink" title="Conversational AI Threads for Visualizing Multidimensional Datasets"></a>Conversational AI Threads for Visualizing Multidimensional Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05590">http://arxiv.org/abs/2311.05590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt-Heun Hong, Anamaria Crisan</li>
<li>for: 这个论文旨在探讨生成大语言模型（LLM）在数据分析中的可能性，以及其在对话式界面上的应用。</li>
<li>methods: 我们使用了一个LLM进行一项前期Wizard-of-Oz研究，检查了使用聊天机器人进行视觉分析的使用情况。我们发现了LLM驱动的分析聊天机器人在支持进程式视觉分析的方面缺乏能力。基于这些发现，我们开发了AIThreads，一种多线程分析聊天机器人，允许分析员把握对话上下文，并提高其输出的有效性。</li>
<li>results: 我们通过一项在线投票测试（n&#x3D;40）和专家分析员的深入采访（n&#x3D;10）评估了AIThreads的可用性。此外，我们还在LLM训练集外的数据集上展示了AIThreads的能力。我们的发现表明LLM具有潜在的可能性，同时也浮现了未来研究的挑战和有优势的方向。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) show potential in data analysis, yet their full capabilities remain uncharted. Our work explores the capabilities of LLMs for creating and refining visualizations via conversational interfaces. We used an LLM to conduct a re-analysis of a prior Wizard-of-Oz study examining the use of chatbots for conducting visual analysis. We surfaced the strengths and weaknesses of LLM-driven analytic chatbots, finding that they fell short in supporting progressive visualization refinements. From these findings, we developed AI Threads, a multi-threaded analytic chatbot that enables analysts to proactively manage conversational context and improve the efficacy of its outputs. We evaluate its usability through a crowdsourced study (n=40) and in-depth interviews with expert analysts (n=10). We further demonstrate the capabilities of AI Threads on a dataset outside the LLM's training corpus. Our findings show the potential of LLMs while also surfacing challenges and fruitful avenues for future research.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在数据分析方面表现出潜力，但它们的潜在能力仍未得到完全了解。我们的工作探讨了 LLMs 在通过对话 интерфей斯进行数据分析时的能力。我们使用 LLM 重新分析了一项以前的奴隶之力学研究，检查了使用 chatbot 进行视觉分析的使用情况。我们发现 LLM-驱动的分析 chatbot 在支持进程化视觉REFINEMENTS 方面缺乏能力。从这些发现中，我们开发了 AI Threads，一个多线程的分析 chatbot，允许分析员在对话上执行进程化的聊天会话，以提高其输出的有效性。我们通过在线调查（n=40）和专家分析员的深入采访（n=10）评估了 AI Threads 的可用性。我们进一步证明了 AI Threads 在一个不同于 LLM 训练集的数据集上的可能性。我们的发现表明 LLMs 的潜力，同时也浮出了未来研究的挑战和有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Goal-Directed-Dialogue-via-RL-on-Imagined-Conversations"><a href="#Zero-Shot-Goal-Directed-Dialogue-via-RL-on-Imagined-Conversations" class="headerlink" title="Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations"></a>Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05584">http://arxiv.org/abs/2311.05584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joey Hong, Sergey Levine, Anca Dragan</li>
<li>for: 这个论文的目的是如何使用RL和LLM来解决目标导向对话任务。</li>
<li>methods: 这个论文使用的方法是使用LLM生成各种可能的人类对话示例，然后使用RL进行离线学习来训练一个可以优化目标导向目标的对话代理人。</li>
<li>results: 这个论文的实验结果表明，该方法可以在不同的目标导向对话任务中达到最佳性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome. For example, a teacher might try to understand their student's current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy. LLMs trained with supervised fine-tuning or "single-step" RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. In this work, we explore a new method for adapting LLMs with RL for such goal-directed dialogue. Our key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating suboptimal but human-like behaviors. Given a textual description of a goal-directed dialogue task, we leverage LLMs to sample diverse synthetic rollouts of hypothetical in-domain human-human interactions. Our algorithm then utilizes this dataset with offline reinforcement learning to train an interactive conversational agent that can optimize goal-directed objectives over multiple turns. In effect, the LLM produces examples of possible interactions, and RL then processes these examples to learn to perform more optimal interactions. Empirically, we show that our proposed approach achieves state-of-the-art performance in various goal-directed dialogue tasks that include teaching and preference elicitation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Inference-for-Probabilistic-Dependency-Graphs"><a href="#Inference-for-Probabilistic-Dependency-Graphs" class="headerlink" title="Inference for Probabilistic Dependency Graphs"></a>Inference for Probabilistic Dependency Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05580">http://arxiv.org/abs/2311.05580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orichardson/pdg-infer-uai">https://github.com/orichardson/pdg-infer-uai</a></li>
<li>paper_authors: Oliver E. Richardson, Joseph Y. Halpern, Christopher De Sa</li>
<li>for: 这篇论文旨在提出一种可行的概率依赖图（PDG）推理算法，以解决PDG推理的复杂性问题。</li>
<li>methods: 该论文使用了一种新的方法，即将PDG推理转化为一种凸优化问题（具有凸顶点约束），并使用内点方法解决这种问题。</li>
<li>results: 论文的实验结果表明，该算法可以在高速度下解决PDG推理问题，并且比基eline方法更高效。<details>
<summary>Abstract</summary>
Probabilistic dependency graphs (PDGs) are a flexible class of probabilistic graphical models, subsuming Bayesian Networks and Factor Graphs. They can also capture inconsistent beliefs, and provide a way of measuring the degree of this inconsistency. We present the first tractable inference algorithm for PDGs with discrete variables, making the asymptotic complexity of PDG inference similar that of the graphical models they generalize. The key components are: (1) the observation that, in many cases, the distribution a PDG specifies can be formulated as a convex optimization problem (with exponential cone constraints), (2) a construction that allows us to express these problems compactly for PDGs of boundeed treewidth, (3) contributions to the theory of PDGs that justify the construction, and (4) an appeal to interior point methods that can solve such problems in polynomial time. We verify the correctness and complexity of our approach, and provide an implementation of it. We then evaluate our implementation, and demonstrate that it outperforms baseline approaches. Our code is available at http://github.com/orichardson/pdg-infer-uai.
</details>
<details>
<summary>摘要</summary>
probablistic dependent graphs (PDGs) 是一种灵活的 probabilistic graphical models，包括 Bayesian Networks 和 factor graphs。它们可以捕捉不一致的信念，并提供一种度量这种不一致程度的方式。我们提出了首个可追踪的 pdg 推理算法，使得 pdg 推理的 asymptotic complexity 与它们总体化的图ical models 相似。关键组成部分包括：1. 观察到，在许多情况下，pdg  specify 的分布可以表示为一个 convex 优化问题（with exponential cone constraints）。2. 一种可以紧凑表述这些问题的 pdg 的建构。3. PDGs 的理论基础， justify 这种建构。4. appeal 到 interior point methods，可以在 polynomial time 内解决这些问题。我们验证了我们的方法的正确性和复杂度，并提供了一个实现。然后，我们评估了我们的实现，并证明它在比基eline approach 更高效。我们的代码可以在 http://github.com/orichardson/pdg-infer-uai 上获取。
</details></li>
</ul>
<hr>
<h2 id="Removing-RLHF-Protections-in-GPT-4-via-Fine-Tuning"><a href="#Removing-RLHF-Protections-in-GPT-4-via-Fine-Tuning" class="headerlink" title="Removing RLHF Protections in GPT-4 via Fine-Tuning"></a>Removing RLHF Protections in GPT-4 via Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05553">http://arxiv.org/abs/2311.05553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, Daniel Kang</li>
<li>for: 本研究旨在检测和维护语言模型（LLM）的可用性和安全性，尤其是RLHF保护机制在强大模型上的效果。</li>
<li>methods: 研究人员使用了RLHF和人工反馈来减少LLM的可能性性，并在强大模型上进行了精细调整。</li>
<li>results: 研究发现，即使使用强大模型进行精细调整，RLHF保护机制仍然可以被攻击者绕过，仅需340个示例和95%的成功率。此外，研究还发现，移除RLHF保护不会减少模型的用用性。<details>
<summary>Abstract</summary>
As large language models (LLMs) have increased in their capabilities, so does their potential for dual use. To reduce harmful outputs, produces and vendors of LLMs have used reinforcement learning with human feedback (RLHF). In tandem, LLM vendors have been increasingly enabling fine-tuning of their most powerful models. However, concurrent work has shown that fine-tuning can remove RLHF protections. We may expect that the most powerful models currently available (GPT-4) are less susceptible to fine-tuning attacks.   In this work, we show the contrary: fine-tuning allows attackers to remove RLHF protections with as few as 340 examples and a 95% success rate. These training examples can be automatically generated with weaker models. We further show that removing RLHF protections does not decrease usefulness on non-censored outputs, providing evidence that our fine-tuning strategy does not decrease usefulness despite using weaker models to generate training data. Our results show the need for further research on protections on LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的能力不断提高，同时其可能性也随之增加。为了减少有害输出，LLM生产者和销售商通过人工反馈学习（RLHF）来减少有害输出。同时，LLM生产者也在不断增加其最强大的模型的微调能力。然而，同时的工作表明，微调可以 removals RLHF保护。我们可能会预计最强大的模型目前可用（GPT-4） less susceptible to fine-tuning attacks。  在这种工作中，我们显示相反：微调允许攻击者从RLHF保护中除掉95%的成功率和只需要340个示例。这些训练示例可以通过弱化模型自动生成。我们还证明了从RLHF保护中除掉不会降低非防止输出的用用性，这是我们的微调策略不会降低用用性，即使使用弱化模型来生成训练数据。我们的结果表明需要进一步研究LLMs的保护。
</details></li>
</ul>
<hr>
<h2 id="Multi-Agent-Quantum-Reinforcement-Learning-using-Evolutionary-Optimization"><a href="#Multi-Agent-Quantum-Reinforcement-Learning-using-Evolutionary-Optimization" class="headerlink" title="Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization"></a>Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05546">http://arxiv.org/abs/2311.05546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Kölle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas Nüßlein, Claudia Linnhoff-Popien</li>
<li>for: 这篇论文旨在提出一种基于量子机制的多体强化学习方法，以提高智能驾驶和智能工业应用中的自动驾驶能力。</li>
<li>methods: 该方法基于现有的量子强化学习方法，使用演化优化方法和量子循环网络来解决多体强化学习问题。</li>
<li>results: 在币游戏环境中评估了多种量子强化学习方法，并与经典方法进行比较。结果显示，使用量子循环网络和演化优化方法可以获得更好的性能，并且使用97.88% fewer parameters than a larger neural network。<details>
<summary>Abstract</summary>
Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. We build upon a existing approach for gradient free Quantum Reinforcement Learning and propose tree approaches with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our approach in the Coin Game environment and compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount of trainable parameters. Compared to the larger neural network, our approaches archive similar results using $97.88\%$ less parameters.
</details>
<details>
<summary>摘要</summary>
多智能体强化学习在智能汽车和其他智能工业应用中日益重要。同时，一种有前途的新方法在量子力学的自然属性基础上实现强化学习，减少模型可训练参数的数量。然而，使用梯度基本的多智能体量子强化学习方法经常陷入恒平面，使其与经典方法相比表现不佳。我们基于现有的梯度自由量子强化学习方法，并提出了三种使用变量量子电路的多智能体强化学习方法，使用进化优化。我们在硬币游戏环境中评估了我们的方法，并与经典方法进行比较。我们发现，我们的变量量子电路方法在与同量 Parameters 的神经网络相比，表现出了显著更好的性能。相比之下，我们的方法使用了 $97.88\%$  fewer parameters。
</details></li>
</ul>
<hr>
<h2 id="From-Learning-Management-System-to-Affective-Tutoring-system-a-preliminary-study"><a href="#From-Learning-Management-System-to-Affective-Tutoring-system-a-preliminary-study" class="headerlink" title="From Learning Management System to Affective Tutoring system: a preliminary study"></a>From Learning Management System to Affective Tutoring system: a preliminary study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05513">http://arxiv.org/abs/2311.05513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadaud Edouard, Geoffroy Thibault, Khelifi Tesnim, Yaacoub Antoun, Haidar Siba, Ben Rabah NourhÈne, Aubin Jean Pierre, Prevost Lionel, Le Grand Benedicte</li>
<li>for: 本研究旨在探讨学生学习过程中情感的作用，并结合表现、行为参与度和情感参与度指标，以 diferenciate 高 achieving 和 low achieving 学生。</li>
<li>methods: 本研究使用了两种主要数据源：学生学习管理系统（LMS）中的数字 traces，以及学生的 webcam 捕捉到的图像。数字 traces 提供了学生与教育内容的交互情况的信息，而图像则用于分析学生的情感表达。</li>
<li>results: 对于法国工程师学校2022-2023学年度学生的实际数据进行分析，我们发现了正面情感状态与学术成绩的相关性。这些初步结果支持情感在分 differentiate 高 achieving 和 low achieving 学生中扮演的重要角色。<details>
<summary>Abstract</summary>
In this study, we investigate the combination of indicators, including performance, behavioral engagement, and emotional engagement, to identify students experiencing difficulties. We analyzed data from two primary sources: digital traces extracted from th e Learning Management System (LMS) and images captured by students' webcams. The digital traces provided insights into students' interactions with the educational content, while the images were utilized to analyze their emotional expressions during learnin g activities. By utilizing real data collected from students at a French engineering school, recorded during the 2022 2023 academic year, we observed a correlation between positive emotional states and improved academic outcomes. These preliminary findings support the notion that emotions play a crucial role in differentiating between high achieving and low achieving students.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了学生表现、行为参与度和情感参与度的组合，以Identify students experiencing difficulties。我们分析了两个主要数据源：学生学习管理系统（LMS）中的数字痕迹和学生的webcam捕捉到的图像。数字痕迹提供了学生与教育内容之间的互动的信息，而图像则用于分析学生学习活动中的情感表达。通过使用2022-2023学年法国工程学院的实际数据，我们发现了正面情感状态与学术成绩的相关性。这些初步发现支持情感在区分高performing和低performing学生中发挥重要作用的观点。
</details></li>
</ul>
<hr>
<h2 id="Anytime-Constrained-Reinforcement-Learning"><a href="#Anytime-Constrained-Reinforcement-Learning" class="headerlink" title="Anytime-Constrained Reinforcement Learning"></a>Anytime-Constrained Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05511">http://arxiv.org/abs/2311.05511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy McMahan, Xiaojin Zhu</li>
<li>for: 研究受限Markov决策过程（cMDP）中的时间约束。</li>
<li>methods: 使用优化的决策策略和累积成本来解决问题。</li>
<li>results: 提出了一种可以快速计划和学习的方法，但计算非质量优化策略是NP困难的。<details>
<summary>Abstract</summary>
We introduce and study constrained Markov Decision Processes (cMDPs) with anytime constraints. An anytime constraint requires the agent to never violate its budget at any point in time, almost surely. Although Markovian policies are no longer sufficient, we show that there exist optimal deterministic policies augmented with cumulative costs. In fact, we present a fixed-parameter tractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our reduction yields planning and learning algorithms that are time and sample-efficient for tabular cMDPs so long as the precision of the costs is logarithmic in the size of the cMDP. However, we also show that computing non-trivial approximately optimal policies is NP-hard in general. To circumvent this bottleneck, we design provable approximation algorithms that efficiently compute or learn an approximately feasible policy with optimal value so long as the maximum supported cost is bounded by a polynomial in the cMDP or by the absolute budget. Given our hardness results, our approximation guarantees are the best possible in terms of tractability under worst-case analysis.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究带时间约束的马尔可夫决策过程（cMDP）。任何时间约束都要求代理人从不抵触其预算的任何时刻中准确满足约束。虽然马尔可夫策略不再充分，我们表明存在最优的决策策略，并且这些策略可以增加累积成本。事实上，我们提供一种可靠减少 anytime-constrained cMDP 到无约束 MDP 的减少，这种减少使得计划和学习算法在 tabular cMDP 中时间和样本效率很高，只要 costs 的精度是 logarithmic 在 cMDP 的大小上。然而，我们也证明计算非rivially 优化策略是 NP-hard 的一般情况。为了缺口这个瓶颈，我们设计了可证明的approximation算法，可以有效地计算或学习一个 approximately 可行策略，并且这个策略的价值与 cMDP 中最大支持的成本相同。根据我们的困难性结果，我们的approximation 保证是worst-case 分析下最佳的。
</details></li>
</ul>
<hr>
<h2 id="General-Policies-Subgoal-Structure-and-Planning-Width"><a href="#General-Policies-Subgoal-Structure-and-Planning-Width" class="headerlink" title="General Policies, Subgoal Structure, and Planning Width"></a>General Policies, Subgoal Structure, and Planning Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05490">http://arxiv.org/abs/2311.05490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blai Bonet, Hector Geffner</li>
<li>for: 本文研究了类别规划问题，特别是在 atomic goals 下的情况。</li>
<li>methods: 本文使用了 IW 探索算法，该算法在问题宽度是 bounded 时可以达到 polynomial 时间复杂度。此外，文章还定义了 serializations 和 serialized width 概念，以及一种基于这些概念的 Serialized IW 算法。</li>
<li>results: 文章表明了 bounded width 问题可以通过使用 general optimal policies 和 sketches 来解决。此外，文章还提出了一种用于编码域控制知识的简洁表述语言。<details>
<summary>Abstract</summary>
It has been observed that many classical planning domains with atomic goals can be solved by means of a simple polynomial exploration procedure, called IW, that runs in time exponential in the problem width, which in these cases is bounded and small. Yet, while the notion of width has become part of state-of-the-art planning algorithms such as BFWS, there is no good explanation for why so many benchmark domains have bounded width when atomic goals are considered. In this work, we address this question by relating bounded width with the existence of general optimal policies that in each planning instance are represented by tuples of atoms of bounded size. We also define the notions of (explicit) serializations and serialized width that have a broader scope as many domains have a bounded serialized width but no bounded width. Such problems are solved non-optimally in polynomial time by a suitable variant of the Serialized IW algorithm. Finally, the language of general policies and the semantics of serializations are combined to yield a simple, meaningful, and expressive language for specifying serializations in compact form in the form of sketches, which can be used for encoding domain control knowledge by hand or for learning it from small examples. Sketches express general problem decompositions in terms of subgoals, and sketches of bounded width express problem decompositions that can be solved in polynomial time.
</details>
<details>
<summary>摘要</summary>
很多古典规划领域的目标可以通过一种简单的多项式探索过程（IW）来解决，该过程在问题宽度上呈指数增长。然而，尚未有良好的解释为何许多benchmark领域有约束的宽度，当 atomic goals 被考虑时。在这种工作中，我们解释这个问题，并证明了约束的宽度与总优规划策略的存在相关。我们还定义了（显式）序列化和序列化宽度这些概念，它们在许多领域有约束宽度，但没有约束。这些问题可以使用适当的序列化IW算法进行非优化的几何时间解决。最后，我们将总规划策略的语言和序列化的语义结合起来，得到了一种简单、有意义和表达力强的语言，用于在 компакт形式中编码领域控制知识，或者从小示例中学习。这些sketches表示了一些通用的问题分解，并且sketches的宽度约束表示可以在几何时间内解决的问题分解。
</details></li>
</ul>
<hr>
<h2 id="meta4-semantically-aligned-generation-of-metaphoric-gestures-using-self-supervised-text-and-speech-representation"><a href="#meta4-semantically-aligned-generation-of-metaphoric-gestures-using-self-supervised-text-and-speech-representation" class="headerlink" title="meta4: semantically-aligned generation of metaphoric gestures using self-supervised text and speech representation"></a>meta4: semantically-aligned generation of metaphoric gestures using self-supervised text and speech representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05481">http://arxiv.org/abs/2311.05481</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mireillefares/meta4">https://github.com/mireillefares/meta4</a></li>
<li>paper_authors: Mireille Fares, Catherine Pelachaud, Nicolas Obin</li>
<li>for: 本研究旨在将语言和图像概念结构（Image Schemas）融入虚拟代理人的行为生成模型中，以实现更加自然和生动的人工智能交互。</li>
<li>methods: 本研究使用深度学习方法，将语言和图像概念结构（Image Schemas）融入虚拟代理人的行为生成模型中，以生成基于对话的抽象概念和图像概念结构的动作。</li>
<li>results: 本研究的结果显示，将语言和图像概念结构（Image Schemas）融入虚拟代理人的行为生成模型中，可以实现更加自然和生动的人工智能交互，并且可以增加虚拟代理人的表现自然性和沟通效果。<details>
<summary>Abstract</summary>
Image Schemas are repetitive cognitive patterns that influence the way we conceptualize and reason about various concepts present in speech. These patterns are deeply embedded within our cognitive processes and are reflected in our bodily expressions including gestures. Particularly, metaphoric gestures possess essential characteristics and semantic meanings that align with Image Schemas, to visually represent abstract concepts. The shape and form of gestures can convey abstract concepts, such as extending the forearm and hand or tracing a line with hand movements to visually represent the image schema of PATH. Previous behavior generation models have primarily focused on utilizing speech (acoustic features and text) to drive the generation model of virtual agents. They have not considered key semantic information as those carried by Image Schemas to effectively generate metaphoric gestures. To address this limitation, we introduce META4, a deep learning approach that generates metaphoric gestures from both speech and Image Schemas. Our approach has two primary goals: computing Image Schemas from input text to capture the underlying semantic and metaphorical meaning, and generating metaphoric gestures driven by speech and the computed image schemas. Our approach is the first method for generating speech driven metaphoric gestures while leveraging the potential of Image Schemas. We demonstrate the effectiveness of our approach and highlight the importance of both speech and image schemas in modeling metaphoric gestures.
</details>
<details>
<summary>摘要</summary>
Image Schemas 是人类认知过程中的重复性Pattern，它们影响了我们对不同概念的概念化和逻辑理解。这些模式深嵌在我们认知过程中，并在我们的身体表达中反映出来，包括手势。特别是元associal gesture具有essential characteristics和semantic meaning，可以用来视觉表示概念。例如，扩展forearm和手或使用手部运动 tracing 一条线来视觉表示PATH的图像模式。在虚拟代理模型中，以前的行为生成模型主要基于了speech（声音特征和文本）来驱动行为生成模型。它们没有考虑了关键的semantic信息，例如图像模式，以便生成元associal gesture。为了解决这一限制，我们介绍META4，一种深度学习方法，可以从speech和图像模式中生成元associal gesture。我们的方法有两个主要目标：计算图像模式从输入文本中，以捕捉下面的semantic和metaforical meaning，并使用speech和计算的图像模式来驱动元associal gesture的生成。我们的方法是首个基于speech和图像模式的元associal gesture生成方法。我们 demonstate了我们的方法的有效性，并强调了speech和图像模式在元associal gesture模型中的重要性。
</details></li>
</ul>
<hr>
<h2 id="Text-Representation-Distillation-via-Information-Bottleneck-Principle"><a href="#Text-Representation-Distillation-via-Information-Bottleneck-Principle" class="headerlink" title="Text Representation Distillation via Information Bottleneck Principle"></a>Text Representation Distillation via Information Bottleneck Principle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05472">http://arxiv.org/abs/2311.05472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzhao Zhang, Dingkun Long, Zehan Li, Pengjun Xie</li>
<li>for: 提高文本表示领域中 PLMs 的可行性</li>
<li>methods: 提议一种基于信息瓶颈原理的 Knowledge Distillation 方法，即 IBKD，以提高学生模型的表示能力</li>
<li>results: 两个主要下游应用任务（Semantic Textual Similarity 和 Dense Retrieval）的实验结果表明，我们的提议方法有效地提高了学生模型的表示能力<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have recently shown great success in text representation field. However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications. To make models more accessible, an effective method is to distill large models into smaller representation models. In order to relieve the issue of performance degradation after distillation, we propose a novel Knowledge Distillation method called IBKD. This approach is motivated by the Information Bottleneck principle and aims to maximize the mutual information between the final representation of the teacher and student model, while simultaneously reducing the mutual information between the student model's representation and the input data. This enables the student model to preserve important learned information while avoiding unnecessary information, thus reducing the risk of over-fitting. Empirical studies on two main downstream applications of text representation (Semantic Textual Similarity and Dense Retrieval tasks) demonstrate the effectiveness of our proposed approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cognitively-Inspired-Components-for-Social-Conversational-Agents"><a href="#Cognitively-Inspired-Components-for-Social-Conversational-Agents" class="headerlink" title="Cognitively Inspired Components for Social Conversational Agents"></a>Cognitively Inspired Components for Social Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05450">http://arxiv.org/abs/2311.05450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Clay, Eduardo Alonso, Esther Mondragón</li>
<li>for: This paper aims to address two key problems in current conversational agents (CAs): technical issues and social expectations.</li>
<li>methods: The proposed solution is to incorporate cognitive functions, such as semantic and episodic memory, emotion, working memory, and learning, into CAs.</li>
<li>results: The introduction of these cognitive functions can improve the technical performance of CAs and enhance their ability to meet social expectations, leading to more effective and satisfying interactions with users.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目的是解决当前对话代理人（CA）中的两个主要问题：技术问题和社会预期。</li>
<li>methods: 提议的解决方案是通过在CA中引入认知功能，如semantic和episodic记忆、情感、工作记忆和学习，来解决技术问题和社会预期。</li>
<li>results: 通过引入这些认知功能，CA可以提高技术性能和满足用户的社会预期，从而实现更有效率和满意的用户交互。<details>
<summary>Abstract</summary>
Current conversational agents (CA) have seen improvement in conversational quality in recent years due to the influence of large language models (LLMs) like GPT3. However, two key categories of problem remain. Firstly there are the unique technical problems resulting from the approach taken in creating the CA, such as scope with retrieval agents and the often nonsensical answers of former generative agents. Secondly, humans perceive CAs as social actors, and as a result expect the CA to adhere to social convention. Failure on the part of the CA in this respect can lead to a poor interaction and even the perception of threat by the user. As such, this paper presents a survey highlighting a potential solution to both categories of problem through the introduction of cognitively inspired additions to the CA. Through computational facsimiles of semantic and episodic memory, emotion, working memory, and the ability to learn, it is possible to address both the technical and social problems encountered by CAs.
</details>
<details>
<summary>摘要</summary>
当前的对话代理人（CA）在过去几年中得到了大语言模型（LLM）如GPT3的影响，从而提高了对话质量。然而，两大类问题仍然存在。首先，创建CA时采取的方法会导致特定的技术问题，如检索代理人的范围和前一代生成代理人的偶极答案。其次，人们对CA视为社会actor，因此对CA的交互预期会遵循社会规范。如果CA不符合这些规范，可能会导致交互不佳并且用户可能会感到威胁。因此，本文提出了一项调查，探讨通过在CA中引入认知发展的方法来解决这两类问题。通过计算机的semantic和episodic记忆、情感、工作记忆和学习能力，可以解决CA中的技术和社会问题。
</details></li>
</ul>
<hr>
<h2 id="LLaVA-Plus-Learning-to-Use-Tools-for-Creating-Multimodal-Agents"><a href="#LLaVA-Plus-Learning-to-Use-Tools-for-Creating-Multimodal-Agents" class="headerlink" title="LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"></a>LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05437">http://arxiv.org/abs/2311.05437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li</li>
<li>for: 本研究旨在提供一个通用多模态助手，可以扩展大型多模态模型的功能。</li>
<li>methods: 本研究使用了多模态指令遵循数据来学习使用工具，包括视觉理解、生成、外部知识检索和组合。</li>
<li>results: 实验结果显示，LLaVA-Plus在现有能力和新能力方面都有优于LLaVA的表现，并且可以直接将图像查询引入人机交互会话中，显著提高工具使用性能和开拓新enario。<details>
<summary>Abstract</summary>
LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models. It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in existing capabilities and exhibits new ones. It is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.
</details>
<details>
<summary>摘要</summary>
LLaVA-Plus 是一种通用多模态助手，它可以扩展大型多模态模型的功能。它保持一个拥有预训练视觉和视觉语言模型的技能库，可以根据用户输入活化相关的工具来完成现实世界任务。LLaVA-Plus 通过多modal instruction-following 数据训练获得了使用工具的能力，包括视觉理解、生成、外部知识检索和组合。实验结果表明，LLaVA-Plus 在现有能力和新的能力方面都超越 LLVA，而且图像查询直接围绕整个人机器人交互会话活动地参与，有效地提高工具使用性能，并启用新的应用场景。
</details></li>
</ul>
<hr>
<h2 id="Mirror-A-Universal-Framework-for-Various-Information-Extraction-Tasks"><a href="#Mirror-A-Universal-Framework-for-Various-Information-Extraction-Tasks" class="headerlink" title="Mirror: A Universal Framework for Various Information Extraction Tasks"></a>Mirror: A Universal Framework for Various Information Extraction Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05419">http://arxiv.org/abs/2311.05419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Spico197/Mirror">https://github.com/Spico197/Mirror</a></li>
<li>paper_authors: Tong Zhu, Junfei Ren, Zijian Yu, Mengsong Wu, Guoliang Zhang, Xiaoye Qu, Wenliang Chen, Zhefeng Wang, Baoxing Huai, Min Zhang</li>
<li>for: 这篇论文的目的是解决信息EXTRACTION任务之间的知识共享问题，因为数据格式和任务变化很多，导致信息浪费和复杂应用程序建设困难。</li>
<li>methods: 作者提出了一种 универсальный框架，称为 Mirror，用于不同的信息EXTRACTION任务。该框架将IE任务重新定义为多槽循环图像EXTRACTION问题，并开发了一种非自动回归图像解码算法来提取所有槽。</li>
<li>results: 实验结果表明，作者的模型在几个下游任务中具有妥协性和竞争性性能，并在几个零批和几个零批设置下达到或超过了现有系统的性能。<details>
<summary>Abstract</summary>
Sharing knowledge between information extraction tasks has always been a challenge due to the diverse data formats and task variations. Meanwhile, this divergence leads to information waste and increases difficulties in building complex applications in real scenarios. Recent studies often formulate IE tasks as a triplet extraction problem. However, such a paradigm does not support multi-span and n-ary extraction, leading to weak versatility. To this end, we reorganize IE problems into unified multi-slot tuples and propose a universal framework for various IE tasks, namely Mirror. Specifically, we recast existing IE tasks as a multi-span cyclic graph extraction problem and devise a non-autoregressive graph decoding algorithm to extract all spans in a single step. It is worth noting that this graph structure is incredibly versatile, and it supports not only complex IE tasks, but also machine reading comprehension and classification tasks. We manually construct a corpus containing 57 datasets for model pretraining, and conduct experiments on 30 datasets across 8 downstream tasks. The experimental results demonstrate that our model has decent compatibility and outperforms or reaches competitive performance with SOTA systems under few-shot and zero-shot settings. The code, model weights, and pretraining corpus are available at https://github.com/Spico197/Mirror .
</details>
<details>
<summary>摘要</summary>
共享知识 между信息提取任务总是是一个挑战，因为数据格式和任务的变化很大。这种多样性导致信息浪费，并使构建实际场景中的复杂应用程序更加困难。现在的研究常将IE任务形式为 triplet 提取问题。然而，这种 парадиг不支持多 Span 和 n-ary 提取，导致其 versatility 强度不够。为此，我们重新组织IE问题为多槽图形提取问题，并提出一个通用的IE框架，即 Mirror。我们将现有的IE任务转化为多Span逆波峰图提取问题，并设计了一种非自动回归图解码算法来提取所有槽峰。值得注意的是，这个图结构非常灵活，不仅支持复杂的IE任务，还支持机器阅读理解和分类任务。我们手动构建了一个包含57个数据集的预训练集，并在30个数据集上进行了8个下游任务的实验。实验结果显示，我们的模型具有不错的兼容性，并在少量和零量设置下超过或达到了现有系统的性能。模型权重、预训练集和实验结果可以在 GitHub 上 obt 取到：https://github.com/Spico197/Mirror。
</details></li>
</ul>
<hr>
<h2 id="Generalization-in-medical-AI-a-perspective-on-developing-scalable-models"><a href="#Generalization-in-medical-AI-a-perspective-on-developing-scalable-models" class="headerlink" title="Generalization in medical AI: a perspective on developing scalable models"></a>Generalization in medical AI: a perspective on developing scalable models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05418">http://arxiv.org/abs/2311.05418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joachim A. Behar, Jeremy Levy, Leo Anthony Celi</li>
<li>for: 本研究目的是为了探讨医疗AI模型在不同医院环境中的应用，以及该模型对不同医院的应用情况下的性能。</li>
<li>methods: 本研究使用了多个数据集，包括开发模型使用的源数据集和评估模型性能的目标数据集。另外，研究还使用了一个三级阶层网络来评估模型在不同医院环境中的应用情况。</li>
<li>results: 研究发现，虽然使用多个数据集可以提高医疗AI模型的一般化能力，但是在实际应用中仍然存在着问题。具体来说，模型在不同医院环境中的性能差异较大，这表明了医疗AI模型的一般化能力仍然有很大的提升空间。<details>
<summary>Abstract</summary>
Over the past few years, research has witnessed the advancement of deep learning models trained on large datasets, some even encompassing millions of examples. While these impressive performance on their hidden test sets, they often underperform when assessed on external datasets. Recognizing the critical role of generalization in medical AI development, many prestigious journals now require reporting results both on the local hidden test set as well as on external datasets before considering a study for publication. Effectively, the field of medical AI has transitioned from the traditional usage of a single dataset that is split into train and test to a more comprehensive framework using multiple datasets, some of which are used for model development (source domain) and others for testing (target domains). However, this new experimental setting does not necessarily resolve the challenge of generalization. This is because of the variability encountered in intended use and specificities across hospital cultures making the idea of universally generalizable systems a myth. On the other hand, the systematic, and a fortiori recurrent re-calibration, of models at the individual hospital level, although ideal, may be overoptimistic given the legal, regulatory and technical challenges that are involved. Re-calibration using transfer learning may not even be possible in some instances where reference labels of target domains are not available. In this perspective we establish a hierarchical three-level scale system reflecting the generalization level of a medical AI algorithm. This scale better reflects the diversity of real-world medical scenarios per which target domain data for re-calibration of models may or not be available and if it is, may or not have reference labels systematically available.
</details>
<details>
<summary>摘要</summary>
Traditionally, the field of medical AI used a single dataset split into train and test sets. However, this approach has been replaced by a more comprehensive framework that uses multiple datasets, some for model development (source domains) and others for testing (target domains). Despite this change, the challenge of generalization remains. This is because of the variability encountered in real-world medical scenarios, making it difficult to achieve universally generalizable systems.To address this challenge, we propose a hierarchical three-level scale system to reflect the generalization level of medical AI algorithms. This scale takes into account the diversity of real-world medical scenarios, where target domain data for model re-calibration may or may not be available, and if it is, may or may not have reference labels systematically available. This approach provides a more realistic assessment of model performance and helps to identify areas where improvement is needed.In conclusion, the field of medical AI has transitioned from using a single dataset to a more comprehensive framework that incorporates multiple datasets. However, the challenge of generalization remains, and a systematic approach to re-calibration is necessary to achieve universally generalizable systems. Our proposed hierarchical scale system provides a better reflection of the diversity of real-world medical scenarios and helps to identify areas where improvement is needed.
</details></li>
</ul>
<hr>
<h2 id="TencentLLMEval-A-Hierarchical-Evaluation-of-Real-World-Capabilities-for-Human-Aligned-LLMs"><a href="#TencentLLMEval-A-Hierarchical-Evaluation-of-Real-World-Capabilities-for-Human-Aligned-LLMs" class="headerlink" title="TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs"></a>TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for Human-Aligned LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05374">http://arxiv.org/abs/2311.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyi Xie, Wenlin Yao, Yong Dai, Shaobo Wang, Donlin Zhou, Lifeng Jin, Xinhua Feng, Pengzhi Wei, Yujie Lin, Zhichao Hu, Dong Yu, Zhengyou Zhang, Jing Nie, Yuhong Liu</li>
<li>For: The paper aims to evaluate the proficiency of large language models (LLMs) in following human instructions on diverse real-world tasks, and to provide a standardized methodology for benchmarking the performance of LLMs.* Methods: The paper proposes a comprehensive human evaluation framework that includes a hierarchical task tree with over 200 categories and 800 tasks, as well as detailed evaluation standards and processes to facilitate consistent and unbiased judgments from human evaluators.* Results: The paper releases a test set of over 3,000 instances that span different difficulty levels and knowledge domains, and analyzes the feasibility of automating parts of evaluation with a strong LLM (GPT-4). The framework is demonstrated to be effective in assessing the performance of Tencent Hunyuan LLMs, and is made publicly available for use in evaluating other LLMs.<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown impressive capabilities across various natural language tasks. However, evaluating their alignment with human preferences remains a challenge. To this end, we propose a comprehensive human evaluation framework to assess LLMs' proficiency in following instructions on diverse real-world tasks. We construct a hierarchical task tree encompassing 7 major areas covering over 200 categories and over 800 tasks, which covers diverse capabilities such as question answering, reasoning, multiturn dialogue, and text generation, to evaluate LLMs in a comprehensive and in-depth manner. We also design detailed evaluation standards and processes to facilitate consistent, unbiased judgments from human evaluators. A test set of over 3,000 instances is released, spanning different difficulty levels and knowledge domains. Our work provides a standardized methodology to evaluate human alignment in LLMs for both English and Chinese. We also analyze the feasibility of automating parts of evaluation with a strong LLM (GPT-4). Our framework supports a thorough assessment of LLMs as they are integrated into real-world applications. We have made publicly available the task tree, TencentLLMEval dataset, and evaluation methodology which have been demonstrated as effective in assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to facilitate the benchmarking of advances in the development of safe and human-aligned LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经展示出色的能力 across 多种自然语言任务。然而，评估它们与人类偏好的Alignment remainschallenge。为此，我们提出了一个完整的人类评估框架，用于评估 LLM 在多种真实世界任务中的执行能力。我们构建了一个层次任务树，覆盖 7 个主要领域，涵盖了200多个类别和800多个任务，这些任务包括问答、推理、多回交流、文本生成等，以评估 LLM 的多方面能力。我们还设计了详细的评估标准和过程，以便确保人类评估员的公正、不偏袋。我们发布了3000多个实例的测试集，覆盖不同的difficulty Level和知识领域。我们的工作提供了一种标准化的方法ологи，用于评估 LLM 的人类Alignment，并且分析了使用强大 LLM（GPT-4）自动化评估的可能性。我们的框架支持了LLM 在真实应用中的全面评估，并且我们已经将任务树、TencentLLMEval数据集和评估方法ологи公开发布。通过这些，我们希望能够促进LLM的开发，并确保其安全和人类Alignment。
</details></li>
</ul>
<hr>
<h2 id="Training-Robust-Deep-Physiological-Measurement-Models-with-Synthetic-Video-based-Data"><a href="#Training-Robust-Deep-Physiological-Measurement-Models-with-Synthetic-Video-based-Data" class="headerlink" title="Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data"></a>Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05371">http://arxiv.org/abs/2311.05371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Ou, Yuzhe Zhang, Yuntang Wang, Shwetak Patel, Daniel McDuf, Xin Liu</li>
<li>for: 这 paper 的目的是提出一种方法，使用synthetic video-based datasets来提高深度学习模型对人体生物信息的识别性。</li>
<li>methods: 这 paper 使用了一种combined augmentation方法，将真实的世界噪音添加到了synthetic physiological signals和对应的facial videos中。</li>
<li>results: 经过实验， authors 发现可以将average MAE从6.9降低到2.0。<details>
<summary>Abstract</summary>
Recent advances in supervised deep learning techniques have demonstrated the possibility to remotely measure human physiological vital signs (e.g., photoplethysmograph, heart rate) just from facial videos. However, the performance of these methods heavily relies on the availability and diversity of real labeled data. Yet, collecting large-scale real-world data with high-quality labels is typically challenging and resource intensive, which also raises privacy concerns when storing personal bio-metric data. Synthetic video-based datasets (e.g., SCAMPS~\cite{mcduff2022scamps}) with photo-realistic synthesized avatars are introduced to alleviate the issues while providing high-quality synthetic data. However, there exists a significant gap between synthetic and real-world data, which hinders the generalization of neural models trained on these synthetic datasets. In this paper, we proposed several measures to add real-world noise to synthetic physiological signals and corresponding facial videos. We experimented with individual and combined augmentation methods and evaluated our framework on three public real-world datasets. Our results show that we were able to reduce the average MAE from 6.9 to 2.0.
</details>
<details>
<summary>摘要</summary>
In this paper, we proposed several measures to add real-world noise to synthetic physiological signals and corresponding facial videos. We experimented with individual and combined augmentation methods and evaluated our framework on three public real-world datasets. Our results show that we were able to reduce the average mean absolute error (MAE) from 6.9 to 2.0.
</details></li>
</ul>
<hr>
<h2 id="On-the-Road-with-GPT-4V-ision-Early-Explorations-of-Visual-Language-Model-on-Autonomous-Driving"><a href="#On-the-Road-with-GPT-4V-ision-Early-Explorations-of-Visual-Language-Model-on-Autonomous-Driving" class="headerlink" title="On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving"></a>On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05332">http://arxiv.org/abs/2311.05332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pjlab-adg/gpt4v-ad-exploration">https://github.com/pjlab-adg/gpt4v-ad-exploration</a></li>
<li>paper_authors: Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai, Xinyu Cai, Min Dou, Shuanglu Hu, Botian Shi<br>for:* The paper is focused on the development and evaluation of a Visual Language Model (VLM) for fully autonomous vehicle driving.methods:* The VLM used in the paper is called \modelnamefull, and it utilizes a transformer-based architecture to understand and reason about driving scenes.* The paper explores the model’s abilities in various tasks, including scene recognition, causal reasoning, and real-time decision-making.results:* The paper shows that \modelname demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems.* The model is able to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts.* However, the paper also highlights some challenges and limitations, such as direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks.Here is the information in Simplified Chinese text:for:* 这篇论文关注完全自动驾驶技术的发展，特别是通过Visual Language Model（VLM）实现自动驾驶。methods:* 这篇论文使用的VLM是\modelnamefull，它采用转换器基 Architecture来理解和解释驾驶场景。* 论文探讨了模型在不同任务中的能力，包括场景识别、 causal reasoning 和实时决策。results:* 论文显示\modelname在场景理解和 causal reasoning 方面表现出色，超越了现有的自动驾驶系统。* 模型能够处理不同的驾驶场景，认出意图，并在真正的驾驶上下进行了有知识的决策。* 然而，论文还指出了一些挑战和限制，如方向识别、交通灯识别、视觉固定和空间理解任务。<details>
<summary>Abstract</summary>
The pursuit of autonomous driving technology hinges on the sophisticated integration of perception, decision-making, and control systems. Traditional approaches, both data-driven and rule-based, have been hindered by their inability to grasp the nuance of complex driving environments and the intentions of other road users. This has been a significant bottleneck, particularly in the development of common sense reasoning and nuanced scene understanding necessary for safe and reliable autonomous driving. The advent of Visual Language Models (VLM) represents a novel frontier in realizing fully autonomous vehicle driving. This report provides an exhaustive evaluation of the latest state-of-the-art VLM, \modelnamefull, and its application in autonomous driving scenarios. We explore the model's abilities to understand and reason about driving scenes, make decisions, and ultimately act in the capacity of a driver. Our comprehensive tests span from basic scene recognition to complex causal reasoning and real-time decision-making under varying conditions. Our findings reveal that \modelname demonstrates superior performance in scene understanding and causal reasoning compared to existing autonomous systems. It showcases the potential to handle out-of-distribution scenarios, recognize intentions, and make informed decisions in real driving contexts. However, challenges remain, particularly in direction discernment, traffic light recognition, vision grounding, and spatial reasoning tasks. These limitations underscore the need for further research and development. Project is now available on GitHub for interested parties to access and utilize: \url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}
</details>
<details>
<summary>摘要</summary>
自动驾驶技术的实现受到了复杂的感知、决策和控制系统的高级 интеграción。传统的方法，包括数据驱动和规则驱动，由于无法捕捉复杂的驾驶环境和其他道路用户的意图，受到了 significiant bottleneck，特别是在实现完全自动驾驶时。随着视觉语言模型（VLM）的出现，自动驾驶技术正在新的前iers中进行实现。本报告对最新的state-of-the-art VLM，\modelnamefull，进行了广泛的评估，并在自动驾驶场景中应用其。我们 investigate了模型对驾驶场景的理解和决策的能力，以及在不同条件下做出有 Informed 决策。我们的全面测试包括基本场景认识、复杂的 causal reasoning 和实时决策。我们的发现表明，\modelname在场景理解和 causal reasoning 方面表现出色，比现有自动驾驶系统更加出色。它还可以在真实驾驶场景中处理异常情况，认出意图，并做出有 Informed 决策。然而，还有一些挑战，包括方向识别、交通灯识别、视觉基础 task 和空间理解任务。这些限制表明需要进一步的研究和发展。项目现在在 GitHub 上可以访问和使用： \url{https://github.com/PJLab-ADG/GPT4V-AD-Exploration}。
</details></li>
</ul>
<hr>
<h2 id="ABIGX-A-Unified-Framework-for-eXplainable-Fault-Detection-and-Classification"><a href="#ABIGX-A-Unified-Framework-for-eXplainable-Fault-Detection-and-Classification" class="headerlink" title="ABIGX: A Unified Framework for eXplainable Fault Detection and Classification"></a>ABIGX: A Unified Framework for eXplainable Fault Detection and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05316">http://arxiv.org/abs/2311.05316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Zhuo, Jinchuan Qian, Zhihuan Song, Zhiqiang Ge</li>
<li>for: 本研究は、可解解釈的故障检测和分类（FDC）に関するものです。</li>
<li>methods: 本研究では、新しい整合架构ABIGX（Adversarial fault reconstruction-Based Integrated Gradient eXplanation）を提案しています。ABIGXは、先行する成功的故障诊断方法の基本要素を含むことで、変数贡献を提供する最初の解釈フレームワークです。コア部分は、敌的な故障再现（AFR）法です。</li>
<li>results: 本研究では、ABIGXは、既存のGradient-based解釈方法に対して优れていることを证明しています。また、故障分类の问题である故障分 classe smearingを解决することにも成功しています。<details>
<summary>Abstract</summary>
For explainable fault detection and classification (FDC), this paper proposes a unified framework, ABIGX (Adversarial fault reconstruction-Based Integrated Gradient eXplanation). ABIGX is derived from the essentials of previous successful fault diagnosis methods, contribution plots (CP) and reconstruction-based contribution (RBC). It is the first explanation framework that provides variable contributions for the general FDC models. The core part of ABIGX is the adversarial fault reconstruction (AFR) method, which rethinks the FR from the perspective of adversarial attack and generalizes to fault classification models with a new fault index. For fault classification, we put forward a new problem of fault class smearing, which intrinsically hinders the correct explanation. We prove that ABIGX effectively mitigates this problem and outperforms the existing gradient-based explanation methods. For fault detection, we theoretically bridge ABIGX with conventional fault diagnosis methods by proving that CP and RBC are the linear specifications of ABIGX. The experiments evaluate the explanations of FDC by quantitative metrics and intuitive illustrations, the results of which show the general superiority of ABIGX to other advanced explanation methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于可解释的故障检测和分类（FDC），这篇论文提出了一个统一框架，即ABIGX（敌对故障重建基于集成导数解释）。 ABIGX 基于过去成功的故障诊断方法的基本元素，包括贡献图（CP）和重建基于贡献（RBC）。它是首个提供变量贡献的总体 FDC 模型解释框架。 ABIGX 的核心部分是对敌对故障重建（AFR）方法，它从敌对攻击的视角重新定义了 FR，并推广到包括新的故障指标的普通故障分类模型。为故障分类，我们提出了一个新的问题：故障分类泛滥问题，该问题内置地阻碍了正确的解释。我们证明了 ABIGX 有效地解决了这个问题，并超越了现有的导数基本解释方法。对故障检测，我们理论上将 ABIGX 与传统故障诊断方法连接，证明 CP 和 RBC 是 ABIGX 的线性特征。实验评估了 FDC 的解释，使用量化指标和直观示例，结果显示 ABIGX 在其他高级解释方法之上具有通用优势。Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need further assistance or if you would like me to make any changes.
</details></li>
</ul>
<hr>
<h2 id="Data-Valuation-and-Detections-in-Federated-Learning"><a href="#Data-Valuation-and-Detections-in-Federated-Learning" class="headerlink" title="Data Valuation and Detections in Federated Learning"></a>Data Valuation and Detections in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05304">http://arxiv.org/abs/2311.05304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/muz1lee/motdata">https://github.com/muz1lee/motdata</a></li>
<li>paper_authors: Wenqian Li, Shuran Fu, Fengrui Zhang, Yan Pang</li>
<li>for: 本研究旨在提出一种基于 Wasserstein 距离的隐私保护技术，以帮助在 Federated Learning 中评估客户端的贡献和选择相关数据样本，而不需要先知道训练算法。</li>
<li>methods: 本文提出的 FedBary 方法在 Federated Learning 中使用 Wasserstein 距离来评估客户端的数据贡献，并通过计算 Wasserstein 巴里中心来实现透明度和有效性的数据评估。</li>
<li>results: 经验和理论分析表明，FedBary 方法可以有效地评估客户端的数据贡献，并且可以提高 Federated Learning 的效果和透明度。<details>
<summary>Abstract</summary>
Federated Learning (FL) enables collaborative model training without sharing raw data, demanding abundant, high-quality data for optimal model performance. Fair and efficient data evaluation is a fundamental issue for incentivizing clients to provide more high-quality data. Meanwhile, it is likely that only a subset of clients and datasets are relevant for a learning task while the rest of them may have a negative impact on the model training. This paper introduces a novel privacy-preserving method for evaluating client contributions and selecting relevant data samples without a pre-specified training algorithm. Our proposed approach, FedBary, utilizes Wasserstein distance within the federated context, offering a new pioneering solution for data valuation, which provides transparent data evaluation and efficient computation of Wasserstein barycenter to mitigate reliance on validation data. We conduct extensive empirical experiments and theoretical analysis, showing the promising research of this valuation metric.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 允许共同训练模型无需分享原始数据，但是需要大量高质量数据来保证模型性能。 Fair和有效地评估客户端提供的数据是基本问题，以奖励客户端提供更多高质量数据。然而，它可能只有一 subset of 客户端和数据集是学习任务中相关的，而其余的可能会对模型训练产生负面影响。这篇论文提出了一种新的隐私保护方法来评估客户端的贡献和选择相关数据样本，不需要预先确定的训练算法。我们提出的方法，FedBary，利用在联邦上的沃氏距离，提供了一种新的探索性的数据评估方法，可以提供透明的数据评估和有效的沃氏巴里中心计算，以减少验证数据的依赖。我们进行了广泛的实验和理论分析，证明了这种评估度量的扎实性。
</details></li>
</ul>
<hr>
<h2 id="Do-personality-tests-generalize-to-Large-Language-Models"><a href="#Do-personality-tests-generalize-to-Large-Language-Models" class="headerlink" title="Do personality tests generalize to Large Language Models?"></a>Do personality tests generalize to Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05297">http://arxiv.org/abs/2311.05297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian E. Dorner, Tom Sühr, Samira Samadi, Augustin Kelava</li>
<li>for: 这篇论文是为了评估大型自然语言模型（LLM）在文本交互中的人类样式行为而写的。</li>
<li>methods: 这篇论文使用了原始设计为人类的测试来评估LLM。</li>
<li>results: 研究发现，LLM的响应与人类测试结果存在差异，因此不能将LLM的测试结果直接应用于人类。具体来说，LLM通常会回答反编项（例如“我是内向的”vs“我是外向的”），而且不同的提问不会按照人类样本中的分化分布。因此，研究人员建议在评估LLM的测试结果之前，更加重视测试的有效性。<details>
<summary>Abstract</summary>
With large language models (LLMs) appearing to behave increasingly human-like in text-based interactions, it has become popular to attempt to evaluate various properties of these models using tests originally designed for humans. While re-using existing tests is a resource-efficient way to evaluate LLMs, careful adjustments are usually required to ensure that test results are even valid across human sub-populations. Thus, it is not clear to what extent different tests' validity generalizes to LLMs. In this work, we provide evidence that LLMs' responses to personality tests systematically deviate from typical human responses, implying that these results cannot be interpreted in the same way as human test results. Concretely, reverse-coded items (e.g. "I am introverted" vs "I am extraverted") are often both answered affirmatively by LLMs. In addition, variation across different prompts designed to "steer" LLMs to simulate particular personality types does not follow the clear separation into five independent personality factors from human samples. In light of these results, we believe it is important to pay more attention to tests' validity for LLMs before drawing strong conclusions about potentially ill-defined concepts like LLMs' "personality".
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）在文本基于交互中表现越来越人类化，因此有人尝试使用原本设计 для人类的测试来评估这些模型。 Although 重用现有测试是资源有效的方式来评估 LLM，但是需要进行精细的调整，以确保测试结果在人类子 популяции中是有效的。因此，不清楚测试的有效性是否在 LLM 上通用。在这项工作中，我们提供证据表明 LLM 对人性测试的响应与人类的响应不同，因此这些结果无法与人类测试结果进行比较。例如，倒向预期的项目（例如 "我是内向的" vs "我是外向的"）通常都会被 LLM 答案正面。此外，使用不同的提问来诱导 LLM 模拟特定的人性类型的变化不符合人类样本中的清晰分化。在这些结果的基础上，我们认为在评估 LLM 的测试有效性之前，需要更加注重测试的有效性。只有在确保测试的有效性时，才能够在 LLM 上 Draw strong conclusions about potentially ill-defined concepts like LLMs' "personality".
</details></li>
</ul>
<hr>
<h2 id="Don’t-Waste-a-Single-Annotation-Improving-Single-Label-Classifiers-Through-Soft-Labels"><a href="#Don’t-Waste-a-Single-Annotation-Improving-Single-Label-Classifiers-Through-Soft-Labels" class="headerlink" title="Don’t Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels"></a>Don’t Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05265">http://arxiv.org/abs/2311.05265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Wu, Yue Li, Yida Mu, Carolina Scarton, Kalina Bontcheva, Xingyi Song</li>
<li>for: 这个论文旨在解决单 Label 分类任务中的数据标注和训练方法的局限性。通常，在标注这些任务时，注释者只被要求为每个样本提供一个标签，而且纠纷不计入最终的决定。这个论文挑战了这种传统方法，因为判断合适的标签可能因数据样本中的模糊和 Context 的缺失而变得困难。</li>
<li>methods: 我们的软标签方法使用了这些模糊的注释信息进行训练。我们发现，使用注释者的信任度、次要标签和纠纷信息可以生成高质量的软标签。</li>
<li>results: 我们的实验结果表明，使用这些软标签进行训练可以提高分类器的性能和准确性。<details>
<summary>Abstract</summary>
In this paper, we address the limitations of the common data annotation and training methods for objective single-label classification tasks. Typically, when annotating such tasks annotators are only asked to provide a single label for each sample and annotator disagreement is discarded when a final hard label is decided through majority voting. We challenge this traditional approach, acknowledging that determining the appropriate label can be difficult due to the ambiguity and lack of context in the data samples. Rather than discarding the information from such ambiguous annotations, our soft label method makes use of them for training. Our findings indicate that additional annotator information, such as confidence, secondary label and disagreement, can be used to effectively generate soft labels. Training classifiers with these soft labels then leads to improved performance and calibration on the hard label test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们讨论了单标签分类任务中常见的数据标注和训练方法的局限性。通常情况下，当批注这些任务时，批注者只被要求为每个样本提供一个单一的标签，而且分类器的决定通过多数投票来决定最终的硬标签。我们挑战了传统的方法，因为判定适当的标签可以是困难的，因为数据样本中的不确定和缺失上下文。而不是抛弃这些不确定的批注信息，我们的软标签方法使用它们进行训练。我们的发现表明，更多的批注信息，如信任度、次要标签和分歧，可以有效地生成软标签。使用这些软标签进行训练后，分类器的性能和校准在硬标签测试集上得到了改善。
</details></li>
</ul>
<hr>
<h2 id="Model-Based-Minimum-Bayes-Risk-Decoding"><a href="#Model-Based-Minimum-Bayes-Risk-Decoding" class="headerlink" title="Model-Based Minimum Bayes Risk Decoding"></a>Model-Based Minimum Bayes Risk Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05263">http://arxiv.org/abs/2311.05263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuu Jinnai, Tetsuro Morimura, Ukyo Honda, Kaito Ariu, Kenshi Abe</li>
<li>for: 这篇论文是关于最小极大风险（MBR）解码的研究，MBR解码是一种可以替代搜索解码的文本生成任务中的有力的方法。</li>
<li>methods: 这篇论文使用了两种常见的简化方法：首先，它采样了一些假设，然后对这些假设进行了采样。其次，它使用了一个Monte Carlo估计来估算每个假设的概率。然而，第二个简化方法不是必要的，因为我们通常在推断时有访问模型概率的能力。</li>
<li>results: 我们的实验表明，使用模型概率来估算假设概率的方法（MBMBR）在文本生成任务中比MBR更高效。MBMBR在encoder-decoder模型和大语言模型上都能够获得更好的性能。<details>
<summary>Abstract</summary>
Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative to beam search decoding in a variety of text generation tasks. MBR decoding selects a hypothesis from a pool of hypotheses that has the least expected risk under a probability model according to a given utility function. Since it is impractical to compute the expected risk exactly over all possible hypotheses, two approximations are commonly used in MBR. First, it integrates over a sampled set of hypotheses rather than over all possible hypotheses. Second, it estimates the probability of each hypothesis using a Monte Carlo estimator. While the first approximation is necessary to make it computationally feasible, the second is not essential since we typically have access to the model probability at inference time. We propose Model-Based MBR (MBMBR), a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. We show analytically and empirically that the model-based estimate is more promising than the Monte Carlo estimate in text generation tasks. Our experiments show that MBMBR outperforms MBR in several text generation tasks, both with encoder-decoder models and with large language models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>基于最小 bayes 风险（MBR）解码已经在文本生成任务中被证明为一种强大的替代方案。 MBR 解码从一组假设中选择一个假设，该假设的风险预期最小根据给定的Utility函数。 由于不可能对所有假设进行准确的预期风险计算，常用两种近似方法。首先，它Integrate 一个采样的集合的假设，而不是所有可能的假设。其次，它使用一个Monte Carlo 估计来估计每个假设的概率。虽然第一种近似是必要的，以使其计算可能，但第二种近似并不是必要的，因为我们通常在推理时有访问模型概率的能力。我们提出了基于模型的 MBR（MBMBR），一种 MBR 的变种，它使用模型概率自己作为假设概率的估计，而不是Monte Carlo 估计。我们分析和实验表明，基于模型的估计比Monte Carlo 估计更有前途在文本生成任务中。我们的实验结果表明，MBMBR 在encoder-decoder 模型和大语言模型上都能够超过 MBR。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Wrapper-in-the-medical-domain-Establishing-transparent-uncertainty-quantification-for-opaque-machine-learning-models-in-practice"><a href="#Uncertainty-Wrapper-in-the-medical-domain-Establishing-transparent-uncertainty-quantification-for-opaque-machine-learning-models-in-practice" class="headerlink" title="Uncertainty Wrapper in the medical domain: Establishing transparent uncertainty quantification for opaque machine learning models in practice"></a>Uncertainty Wrapper in the medical domain: Establishing transparent uncertainty quantification for opaque machine learning models in practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05245">http://arxiv.org/abs/2311.05245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lisa Jöckel, Michael Kläs, Georg Popp, Nadja Hilger, Stephan Fricke</li>
<li>for: 这个论文是为了推广数据驱动模型在机器学习（ML）中的应用，并提供一种可靠的方法来评估这些模型的结果uncertainty。</li>
<li>methods: 论文使用了一种名为“Uncertainty Wrapper”的方法，用于评估ML模型的结果uncertainty。这种方法基于 bayesian networks 和 Gaussian processes，可以提供高精度的uncertainty estimations。</li>
<li>results: 论文通过应用这种方法于流式 cél analysis 中，发现了一些可能的应用场景，并提供了一些可靠的uncertainty estimations。这些结果表明，使用Uncertainty Wrapper可以帮助用户更好地理解ML模型的结果uncertainty，并做出更 Informed 的决策。<details>
<summary>Abstract</summary>
When systems use data-based models that are based on machine learning (ML), errors in their results cannot be ruled out. This is particularly critical if it remains unclear to the user how these models arrived at their decisions and if errors can have safety-relevant consequences, as is often the case in the medical field. In such cases, the use of dependable methods to quantify the uncertainty remaining in a result allows the user to make an informed decision about further usage and draw possible conclusions based on a given result. This paper demonstrates the applicability and practical utility of the Uncertainty Wrapper using flow cytometry as an application from the medical field that can benefit from the use of ML models in conjunction with dependable and transparent uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:当系统使用机器学习（ML）模型时，结果中的错误无法被排除。这 particualry critical 如果用户无法了解这些模型如何做出决策，并且错误可能有生命安全相关的后果，例如医疗领域中的案例。在这种情况下，使用可靠的方法来评估结果中剩下的不确定性，让用户可以根据给定的结果作出了 Informed 决策和绘制可能的结论。这篇论文介绍了 Uncertainty Wrapper 的可行性和实用性，并通过流式测量为医疗领域的应用，可以通过 ML 模型与可靠而透明的不确定性评估来帮助用户做出更加 Informed 的决策。
</details></li>
</ul>
<hr>
<h2 id="Kantian-Deontology-Meets-AI-Alignment-Towards-Morally-Robust-Fairness-Metrics"><a href="#Kantian-Deontology-Meets-AI-Alignment-Towards-Morally-Robust-Fairness-Metrics" class="headerlink" title="Kantian Deontology Meets AI Alignment: Towards Morally Robust Fairness Metrics"></a>Kantian Deontology Meets AI Alignment: Towards Morally Robust Fairness Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05227">http://arxiv.org/abs/2311.05227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Mougan, Joshua Brand</li>
<li>for: 本文探讨了 Kantian deontological  Framework 在 fairness metrics 中的compatibility，以及如何通过 integrate Kantian ethics 来提高 AI alignment 中的公正性和公平性。</li>
<li>methods: 本文 revisits Kant’s critique of utilitarianism， argue that fairness principles should align with the Kantian deontological framework，并提出一种基于 Kantian ethics 的 AI fairness metrics 模型。</li>
<li>results: 本文提出了一种新的 AI fairness metrics 模型，该模型可以 better balance outcomes and procedures in pursuit of fairness and justice，并且可以帮助建立一个更加公正和公平的 AI ланд财。<details>
<summary>Abstract</summary>
Deontological ethics, specifically understood through Immanuel Kant, provides a moral framework that emphasizes the importance of duties and principles, rather than the consequences of action. Understanding that despite the prominence of deontology, it is currently an overlooked approach in fairness metrics, this paper explores the compatibility of a Kantian deontological framework in fairness metrics, part of the AI alignment field. We revisit Kant's critique of utilitarianism, which is the primary approach in AI fairness metrics and argue that fairness principles should align with the Kantian deontological framework. By integrating Kantian ethics into AI alignment, we not only bring in a widely-accepted prominent moral theory but also strive for a more morally grounded AI landscape that better balances outcomes and procedures in pursuit of fairness and justice.
</details>
<details>
<summary>摘要</summary>
德 Ontological 伦理学，通过伊曼纽尔·凯恩的理解，提供了一个伦理框架，强调行为的责任和原则，而不是行为的后果。尽管德 Ontology 目前在公平度量上被忽略，但这篇文章 argue that a Kantian deontological framework is compatible with fairness metrics, which is part of the AI alignment field. 我们回顾了凯恩对于实用主义的批判，该是AI公平度量的主要方法，并论证公平原则应该遵循德 Ontological 伦理框架。通过将基着 Kantian 伦理学入核AI对齐，我们不仅引入了一种广泛接受的伦理理论，还努力创造一个更加伦理基础的 AI 领域，以更好地平衡结果和过程，追求公平和正义。
</details></li>
</ul>
<hr>
<h2 id="Green-Resilience-of-Cyber-Physical-Systems"><a href="#Green-Resilience-of-Cyber-Physical-Systems" class="headerlink" title="Green Resilience of Cyber-Physical Systems"></a>Green Resilience of Cyber-Physical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05201">http://arxiv.org/abs/2311.05201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rimawi-diaeddin/GRCPS-ISSRE22-DS">https://github.com/rimawi-diaeddin/GRCPS-ISSRE22-DS</a></li>
<li>paper_authors: Diaeddin Rimawi</li>
<li>for: 这项研究旨在提出一种基于游戏理论的可持续可靠性机制，以提高Cyber-Physical System（CPS）的可靠性和绿色性。</li>
<li>methods: 该研究使用游戏理论来解决CPS中的不确定性问题，并通过一个实际的合作人工智能系统（CAIS）来描述GAME模型。</li>
<li>results: 该研究显示，通过GAME模型，CPS可以实现可靠性和绿色性，同时减少CO2足迹。<details>
<summary>Abstract</summary>
Cyber-Physical System (CPS) represents systems that join both hardware and software components to perform real-time services. Maintaining the system's reliability is critical to the continuous delivery of these services. However, the CPS running environment is full of uncertainties and can easily lead to performance degradation. As a result, the need for a recovery technique is highly needed to achieve resilience in the system, with keeping in mind that this technique should be as green as possible. This early doctorate proposal, suggests a game theory solution to achieve resilience and green in CPS. Game theory has been known for its fast performance in decision-making, helping the system to choose what maximizes its payoffs. The proposed game model is described over a real-life collaborative artificial intelligence system (CAIS), that involves robots with humans to achieve a common goal. It shows how the expected results of the system will achieve the resilience of CAIS with minimized CO2 footprint.
</details>
<details>
<summary>摘要</summary>
资berger-physical system (CPS) 表示把硬件和软件元件结合起来提供实时服务的系统。维护这个系统的可靠性非常重要，以确保不断提供服务。然而，CPS 运行环境充满不确定性，容易导致性能下降。因此，需要一种恢复技术来实现系统的可靠性，同时避免对环境造成过大的影响。本博士学位提案建议使用游戏理论解决这个问题。游戏理论known for its fast decision-making, can help the system to choose what maximizes its payoffs.在这个提案中，我们使用了一个基于现实生成人工智能系统 (CAIS) 的游戏模型，CAIS 是一个涉及人类和机器人的共同目标系统。这个模型显示了在 minimizing CO2 印证下，CPS 可以实现可靠性和绿色的恢复。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-in-Computed-Tomography-Pulmonary-Angiography-Imaging-A-Dual-Pronged-Approach-for-Pulmonary-Embolism-Detection"><a href="#Deep-Learning-in-Computed-Tomography-Pulmonary-Angiography-Imaging-A-Dual-Pronged-Approach-for-Pulmonary-Embolism-Detection" class="headerlink" title="Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection"></a>Deep Learning in Computed Tomography Pulmonary Angiography Imaging: A Dual-Pronged Approach for Pulmonary Embolism Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05197">http://arxiv.org/abs/2311.05197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabiha Bushra, Muhammad E. H. Chowdhury, Rusab Sarmun, Saidul Kabir, Menatalla Said, Sohaib Bassam Zoghoul, Adam Mushtak, Israa Al-Hashimi, Abdulrahman Alqahtani, Anwarul Hasan<br>for: 这项研究的主要目标是利用深度学习技术提高计算助encioded诊断（CAD）的准确率，以便更好地诊断肺动脉瘤（PE）。methods: 该研究采用了一种双重的方法， combinig 分类和检测两种方法来诊断PE。首先，我们引入了一种注意力导向的卷积神经网络（AG-CNN），以便对全局和局部肿瘤区域进行分类。其次，我们使用了当前的模型来检测潜在的PE区域。最后，我们使用了不同的ensemble技术来提高检测精度。results: 我们在 Ferdowsi University of Mashhad 的 Pulmonary Embolism（FUMPE）数据集上测试了我们的注意力导向分类方法，与基eline模型 denseNet-121 进行比较。结果显示，我们的方法可以提高 Receiver Operating Characteristic 的面积（AUC）by 8.1%。另外，通过使用ensemble技术和检测模型，我们可以进一步提高 mean average precision（mAP）和 F1 分数。最后，我们的研究提供了一种全面的PE诊断方法，可以帮助解决肺动脉瘤的常见问题，如下 diagnosis 和 misdiagnosis。<details>
<summary>Abstract</summary>
Pulmonary Embolism (PE) is a critical medical condition characterized by obstructions in the pulmonary arteries. Despite being a major health concern, it often goes underdiagnosed leading to detrimental clinical outcomes. The increasing reliance on Computed Tomography Pulmonary Angiography for diagnosis presents challenges and a pressing need for enhanced diagnostic solutions. The primary objective of this study is to leverage deep learning techniques to enhance the Computer Assisted Diagnosis of PE. This study presents a comprehensive dual-pronged approach combining classification and detection for PE diagnosis. We introduce an Attention-Guided Convolutional Neural Network (AG-CNN) for classification, addressing both global and local lesion region. For detection, state-of-the-art models are employed to pinpoint potential PE regions. Different ensembling techniques further improve detection accuracy by combining predictions from different models. Finally, a heuristic strategy integrates classifier outputs with detection results, ensuring robust and accurate PE identification. Our attention-guided classification approach, tested on the Ferdowsi University of Mashhad's Pulmonary Embolism (FUMPE) dataset, outperformed the baseline model DenseNet-121 by achieving an 8.1% increase in the Area Under the Receiver Operating Characteristic. By employing ensemble techniques with detection models, the mean average precision (mAP) was considerably enhanced by a 4.7% increase. The classifier-guided framework further refined the mAP and F1 scores over the ensemble models. Our research offers a comprehensive approach to PE diagnostics using deep learning, addressing the prevalent issues of underdiagnosis and misdiagnosis. We aim to improve PE patient care by integrating AI solutions into clinical workflows, highlighting the potential of human-AI collaboration in medical diagnostics.
</details>
<details>
<summary>摘要</summary>
肺动脉瘤（PE）是一种严重的医疗问题， характеризуется为肺动脉的堵塞。尽管它是一个重要的健康问题，但它经常被诊断错误，导致了严重的临床结果。随着计算机 Tomatoes Pulmonary Angiography（CTPA）的使用变得更加普遍，诊断PE的挑战日益增加。这项研究的主要目标是利用深度学习技术提高计算机助成诊断PE的精度。我们提出了一种权重引导的卷积神经网络（AG-CNN），用于类别，同时处理全局和局部肿瘤区域。对探测，我们采用了当前最佳模型，以找到可能的PE区域。不同的拼接技术进一步提高探测精度，将不同模型的预测结果进行组合。最后，我们采用了一种启发策略，将分类器输出与探测结果结合，以确保精度和准确性的PE诊断。我们在 Ferdowsi University of Mashhad的肺动脉瘤（FUMPE）数据集上测试了我们的注意力引导分类方法，与基eline模型DenseNet-121相比，实现了8.1%的接收操作特征区域（AUC）的提高。通过使用探测模型的ensemble技术，mean average precision（mAP）得到了明显的提高，升高4.7%。我们的类ifier-guided框架还进一步提高了mAP和F1分数。我们的研究提供了一种全面的肺动脉瘤诊断方法，通过深度学习 Addressing the prevalent issues of underdiagnosis and misdiagnosis.我们希望通过将AI技术integrated into clinical workflows，提高PE patient care。这种人AI合作的潜在可能性在医疗诊断中展示出来。
</details></li>
</ul>
<hr>
<h2 id="Mixture-of-Weak-Strong-Experts-on-Graphs"><a href="#Mixture-of-Weak-Strong-Experts-on-Graphs" class="headerlink" title="Mixture of Weak &amp; Strong Experts on Graphs"></a>Mixture of Weak &amp; Strong Experts on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05185">http://arxiv.org/abs/2311.05185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanqing Zeng, Hanjia Lyu, Diyi Hu, Yinglong Xia, Jiebo Luo</li>
<li>for: 提高 node  classification 的表现，通过 mixture of weak and strong experts 来处理 graph 数据。</li>
<li>methods: 使用 weak 专家 (Multi-layer Perceptron) 和 strong 专家 (Graph Neural Network) 的 mixture，其中 weak 专家 是一个轻量级的 MLP，strong 专家 是一个 off-the-shelf GNN。通过一种 “confidence” 机制来调控两个专家的合作，以适应不同的 target nodes。</li>
<li>results: 在 6 个标准的 node  classification  benchmark 上（包括 both homophilous 和 heterophilous 图），Mowst 显示了明显的准确率提高。<details>
<summary>Abstract</summary>
Realistic graphs contain both rich self-features of nodes and informative structures of neighborhoods, jointly handled by a GNN in the typical setup. We propose to decouple the two modalities by mixture of weak and strong experts (Mowst), where the weak expert is a light-weight Multi-layer Perceptron (MLP), and the strong expert is an off-the-shelf Graph Neural Network (GNN). To adapt the experts' collaboration to different target nodes, we propose a "confidence" mechanism based on the dispersion of the weak expert's prediction logits. The strong expert is conditionally activated when either the node's classification relies on neighborhood information, or the weak expert has low model quality. We reveal interesting training dynamics by analyzing the influence of the confidence function on loss: our training algorithm encourages the specialization of each expert by effectively generating soft splitting of the graph. In addition, our "confidence" design imposes a desirable bias toward the strong expert to benefit from GNN's better generalization capability. Mowst is easy to optimize and achieves strong expressive power, with a computation cost comparable to a single GNN. Empirically, Mowst shows significant accuracy improvement on 6 standard node classification benchmarks (including both homophilous and heterophilous graphs).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FireMatch-A-Semi-Supervised-Video-Fire-Detection-Network-Based-on-Consistency-and-Distribution-Alignment"><a href="#FireMatch-A-Semi-Supervised-Video-Fire-Detection-Network-Based-on-Consistency-and-Distribution-Alignment" class="headerlink" title="FireMatch: A Semi-Supervised Video Fire Detection Network Based on Consistency and Distribution Alignment"></a>FireMatch: A Semi-Supervised Video Fire Detection Network Based on Consistency and Distribution Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05168">http://arxiv.org/abs/2311.05168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinghua Lin, Zuoyong Li, Kun Zeng, Haoyi Fan, Wei Li, Xiaoguang Zhou</li>
<li>for: 提高视频中的火灾检测性能，尤其是在数据标注成本高昂的情况下。</li>
<li>methods: 提出一种基于一致规则和对抗分布对齐的半指导火灾检测模型，通过将弱变化和强变化样本结合使用，以及引入一种公平损失函数来避免高确度对非火类的问题。</li>
<li>results: 在两个真实的火灾数据集上实现了76.92%和91.81%的准精度，比现有的半指导类型检测方法高。<details>
<summary>Abstract</summary>
Deep learning techniques have greatly enhanced the performance of fire detection in videos. However, video-based fire detection models heavily rely on labeled data, and the process of data labeling is particularly costly and time-consuming, especially when dealing with videos. Considering the limited quantity of labeled video data, we propose a semi-supervised fire detection model called FireMatch, which is based on consistency regularization and adversarial distribution alignment. Specifically, we first combine consistency regularization with pseudo-label. For unlabeled data, we design video data augmentation to obtain corresponding weakly augmented and strongly augmented samples. The proposed model predicts weakly augmented samples and retains pseudo-label above a threshold, while training on strongly augmented samples to predict these pseudo-labels for learning more robust feature representations. Secondly, we generate video cross-set augmented samples by adversarial distribution alignment to expand the training data and alleviate the decline in classification performance caused by insufficient labeled data. Finally, we introduce a fairness loss to help the model produce diverse predictions for input samples, thereby addressing the issue of high confidence with the non-fire class in fire classification scenarios. The FireMatch achieved an accuracy of 76.92% and 91.81% on two real-world fire datasets, respectively. The experimental results demonstrate that the proposed method outperforms the current state-of-the-art semi-supervised classification methods.
</details>
<details>
<summary>摘要</summary>
深度学习技术对视频中的火情检测表现有了大幅提高。然而，视频基于的火情检测模型强依 Labelled data，并且数据标注过程特别是对视频来说是非常昂贵和时间consuming。面对有限的标注视频数据，我们提出了一种半supervised的火情检测模型，即FireMatch，基于一致regulation和对抗分布对齐。具体来说，我们首先将一致regulation与pseudo-label结合使用。对于没有标注的数据，我们设计了视频数据增强，以获得弱augmented和强augmented样本。提案的模型预测弱augmented样本，保留pseudo-label在阈值以上，而在强augmented样本上进行训练，以学习更加稳定的特征表示。其次，我们通过对视频数据进行对抗分布对齐，生成了跨集数据增强样本，以扩大训练数据，并避免由不充分的标注数据引起的分类性能下降。最后，我们引入了公平loss，以帮助模型对输入样本产生多样化的预测，从而解决火类分类场景中的高信任性问题。FireMatch模型在两个真实的火情数据集上 achieved an accuracy of 76.92%和91.81%，分别比当前最佳半supervised分类方法高出了2.85%和4.99%。实验结果表明，我们提出的方法可以提高火情检测的精度和公平性。
</details></li>
</ul>
<hr>
<h2 id="RAPID-Training-free-Retrieval-based-Log-Anomaly-Detection-with-PLM-considering-Token-level-information"><a href="#RAPID-Training-free-Retrieval-based-Log-Anomaly-Detection-with-PLM-considering-Token-level-information" class="headerlink" title="RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information"></a>RAPID: Training-free Retrieval-based Log Anomaly Detection with PLM considering Token-level information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05160">http://arxiv.org/abs/2311.05160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsba-lab/rapid">https://github.com/dsba-lab/rapid</a></li>
<li>paper_authors: Gunho No, Yukyung Lee, Hyeongwon Kang, Pilsung Kang</li>
<li>for: 这个研究旨在提出一种能够在logs中检测异常行为的方法，并且不需要特定的训练数据。</li>
<li>methods: 这个方法使用了自然语言处理的技术，将logs转换为文本内容，然后使用预训语言模型提取特征。它还使用了检索技术，将test logs与最相似的正常logs进行比较，以检测异常。</li>
<li>results: 实验结果显示，这个方法可以在无需特定训练数据的情况下，与之前的模型相比，实现了相当的比较表现，并且在某些数据集上获得了最好的表现。此外，这个方法还可以在实时检测中获得高效的结果，无需过度的计算成本。<details>
<summary>Abstract</summary>
As the IT industry advances, system log data becomes increasingly crucial. Many computer systems rely on log texts for management due to restricted access to source code. The need for log anomaly detection is growing, especially in real-world applications, but identifying anomalies in rapidly accumulating logs remains a challenging task. Traditional deep learning-based anomaly detection models require dataset-specific training, leading to corresponding delays. Notably, most methods only focus on sequence-level log information, which makes the detection of subtle anomalies harder, and often involve inference processes that are difficult to utilize in real-time. We introduce RAPID, a model that capitalizes on the inherent features of log data to enable anomaly detection without training delays, ensuring real-time capability. RAPID treats logs as natural language, extracting representations using pre-trained language models. Given that logs can be categorized based on system context, we implement a retrieval-based technique to contrast test logs with the most similar normal logs. This strategy not only obviates the need for log-specific training but also adeptly incorporates token-level information, ensuring refined and robust detection, particularly for unseen logs. We also propose the core set technique, which can reduce the computational cost needed for comparison. Experimental results show that even without training on log data, RAPID demonstrates competitive performance compared to prior models and achieves the best performance on certain datasets. Through various research questions, we verified its capability for real-time detection without delay.
</details>
<details>
<summary>摘要</summary>
翻译结果：随着IT业的发展，系统日志数据变得越来越重要。许多计算机系统通过日志文本进行管理，因为访问源代码的限制。检测日志异常的需求在实际应用中增长，特别是在大量日志聚集的情况下，但检测日志异常是一项复杂的任务。传统的深度学习基于异常检测模型需要特定的训练数据，导致延迟。很多方法只关注日志序列水平的信息，这使得检测微异常更加困难，并且经常涉及到困难在实时中使用的推理过程。我们介绍了RAPID模型，它利用日志数据的自然语言特征，通过预训练语言模型提取表示。由于日志可以根据系统上下文分类，我们实施了一种检索基于技术，将测试日志与最相似的正常日志进行对比。这种策略不仅减少了异常检测的延迟，还能够充分地 incorporate  Token级信息，以确保精细和Robust的检测，特别是 для未见的日志。我们还提出了核心集技术，可以降低比较的计算成本。实验结果表明，无需训练日志数据，RAPID模型仍然可以与之前的模型竞争，并在某些数据集上达到最佳性能。通过多种研究问题，我们证明了它的实时检测能力。
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-Deep-Cognate-Detection-Framework-for-Low-Resourced-Languages-Using-Morphological-Knowledge-of-Closely-Related-Languages"><a href="#Weakly-supervised-Deep-Cognate-Detection-Framework-for-Low-Resourced-Languages-Using-Morphological-Knowledge-of-Closely-Related-Languages" class="headerlink" title="Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages"></a>Weakly-supervised Deep Cognate Detection Framework for Low-Resourced Languages Using Morphological Knowledge of Closely-Related Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05155">http://arxiv.org/abs/2311.05155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koustavagoswami/weakly_supervised-cognate_detection">https://github.com/koustavagoswami/weakly_supervised-cognate_detection</a></li>
<li>paper_authors: Koustava Goswami, Priya Rani, Theodorus Fransen, John P. McCrae</li>
<li>for: 这个论文的目的是提出一种语言不确定的弱监督词义检测方法，以便在语言理解任务中提高性能。</li>
<li>methods: 该方法使用 morphological knowledge from closely related languages 来进行词义检测，并通过编码器来学习和转移知识。</li>
<li>results: 实验结果显示，该方法可以在不同的语言家族上实现显著的改进，并且超越了现有的超级vised和无监督方法。<details>
<summary>Abstract</summary>
Exploiting cognates for transfer learning in under-resourced languages is an exciting opportunity for language understanding tasks, including unsupervised machine translation, named entity recognition and information retrieval. Previous approaches mainly focused on supervised cognate detection tasks based on orthographic, phonetic or state-of-the-art contextual language models, which under-perform for most under-resourced languages. This paper proposes a novel language-agnostic weakly-supervised deep cognate detection framework for under-resourced languages using morphological knowledge from closely related languages. We train an encoder to gain morphological knowledge of a language and transfer the knowledge to perform unsupervised and weakly-supervised cognate detection tasks with and without the pivot language for the closely-related languages. While unsupervised, it overcomes the need for hand-crafted annotation of cognates. We performed experiments on different published cognate detection datasets across language families and observed not only significant improvement over the state-of-the-art but also our method outperformed the state-of-the-art supervised and unsupervised methods. Our model can be extended to a wide range of languages from any language family as it overcomes the requirement of the annotation of the cognate pairs for training. The code and dataset building scripts can be found at https://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection
</details>
<details>
<summary>摘要</summary>
利用同源语言的词汇相似性进行无监督学习是对语言理解任务的一种有趣的机会，包括无监督机器翻译、命名实体识别和信息检索。过去的方法主要集中在基于文本、音频或当前语言模型的监督认知语言模型，这些模型对大多数下resource语言不够好。这篇论文提出了一种新的语言无关的弱监督深度同源词汇检测框架，用于下resource语言。我们使用 morphological 知识来训练一个encoder，并将知识转移到无监督和弱监督同源词汇检测任务中。无监督的方式可以超越手动标注同源词汇的需求。我们在不同的已出版同源词汇检测数据集上进行了实验，并观察到了与状态之前的显著改进。我们的方法可以扩展到任何语言家族，因为它不需要标注同源词汇对的训练。代码和数据集构建脚本可以在 https://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection 找到。
</details></li>
</ul>
<hr>
<h2 id="Cross-modal-Prompts-Adapting-Large-Pre-trained-Models-for-Audio-Visual-Downstream-Tasks"><a href="#Cross-modal-Prompts-Adapting-Large-Pre-trained-Models-for-Audio-Visual-Downstream-Tasks" class="headerlink" title="Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks"></a>Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05152">http://arxiv.org/abs/2311.05152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoyi-duan/dg-sct">https://github.com/haoyi-duan/dg-sct</a></li>
<li>paper_authors: Haoyi Duan, Yan Xia, Mingze Zhou, Li Tang, Jieming Zhu, Zhou Zhao</li>
<li>for: 提高多模态任务中大型预训练模型的表现，解决单模态训练数据下预训练模型在多模态任务中的特征提取问题。</li>
<li>methods: 提出了一种新的双引导空间通道时间（DG-SCT）注意机制，利用音频和视频模式作为软提示，动态调整预训练模型的参数，根据当前多模态输入特征进行适应性的特征提取。</li>
<li>results: 实验证明，提出的模型在多个下游任务中达到了领先的Results，包括AVE、AVVP、AVS和AVQA等任务。此外，模型在具有几个shot和零shot的场景下表现出了良好的性能。<details>
<summary>Abstract</summary>
In recent years, the deployment of large-scale pre-trained models in audio-visual downstream tasks has yielded remarkable outcomes. However, these models, primarily trained on single-modality unconstrained datasets, still encounter challenges in feature extraction for multi-modal tasks, leading to suboptimal performance. This limitation arises due to the introduction of irrelevant modality-specific information during encoding, which adversely affects the performance of downstream tasks. To address this challenge, this paper proposes a novel Dual-Guided Spatial-Channel-Temporal (DG-SCT) attention mechanism. This mechanism leverages audio and visual modalities as soft prompts to dynamically adjust the parameters of pre-trained models based on the current multi-modal input features. Specifically, the DG-SCT module incorporates trainable cross-modal interaction layers into pre-trained audio-visual encoders, allowing adaptive extraction of crucial information from the current modality across spatial, channel, and temporal dimensions, while preserving the frozen parameters of large-scale pre-trained models. Experimental evaluations demonstrate that our proposed model achieves state-of-the-art results across multiple downstream tasks, including AVE, AVVP, AVS, and AVQA. Furthermore, our model exhibits promising performance in challenging few-shot and zero-shot scenarios. The source code and pre-trained models are available at https://github.com/haoyi-duan/DG-SCT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-of-Large-Language-Models-in-Medicine-Progress-Application-and-Challenge"><a href="#A-Survey-of-Large-Language-Models-in-Medicine-Progress-Application-and-Challenge" class="headerlink" title="A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"></a>A Survey of Large Language Models in Medicine: Progress, Application, and Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05112">http://arxiv.org/abs/2311.05112</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-in-health/medllmspracticalguide">https://github.com/ai-in-health/medllmspracticalguide</a></li>
<li>paper_authors: Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, Zheng Li, Fenglin Liu</li>
<li>for: 本研究提供了一份对大语言模型（LLMs）在医学领域的概述，以及医学LLMs的构建、下游性能、实际应用以及挑战。</li>
<li>methods: 本研究使用了现有的LLMs技术，并对医学LLMs的构建和下游性能进行了评估。</li>
<li>results: 本研究发现，医学LLMs在医学领域的应用有很大的潜力，但同时也存在一些挑战，如数据采集和质量控制等问题。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT, have achieved substantial attention due to their impressive human language understanding and generation capabilities. Therefore, the application of LLMs in medicine to assist physicians and patient care emerges as a promising research direction in both artificial intelligence and clinical medicine. To this end, this survey provides a comprehensive overview of the current progress, applications, and challenges faced by LLMs in medicine. Specifically, we aim to address the following questions: 1) What are LLMs and how can medical LLMs be built? 2) What are the downstream performances of medical LLMs? 3) How can medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? 5) How can we better construct and utilize medical LLMs? As a result, this survey aims to provide insights into the opportunities and challenges of LLMs in medicine and serve as a valuable resource for constructing practical and effective medical LLMs. A regularly updated list of practical guide resources of medical LLMs can be found at https://github.com/AI-in-Health/MedLLMsPracticalGuide.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs），如ChatGPT，在人工智能和临床医学领域获得了广泛的关注，因为它们在人语理解和生成方面表现出了卓越的能力。因此，在医学和人工智能方面应用LLMs的研究方向显得极其可能。为了实现这一目标，本调查提供了关于当前进展、应用和医学LLMs面临的挑战的全面概述。特别是，我们想要回答以下问题：1. LLMs是什么，如何构建医学LLMs？2. 医学LLMs的下游性能如何？3. 如何在实际临床医疗中使用医学LLMs？4. 使用医学LLMs会遇到什么挑战？5. 如何更好地构建和利用医学LLMs？因此，本调查希望通过对医学LLMs的潜在机会和挑战的描述，为constructing practical and effective医学LLMs提供启示，并且可以作为valuable resource。有关医学LLMs的实践指南资源的更新列表可以在https://github.com/AI-in-Health/MedLLMsPracticalGuide中找到。
</details></li>
</ul>
<hr>
<h2 id="A-differentiable-brain-simulator-bridging-brain-simulation-and-brain-inspired-computing"><a href="#A-differentiable-brain-simulator-bridging-brain-simulation-and-brain-inspired-computing" class="headerlink" title="A differentiable brain simulator bridging brain simulation and brain-inspired computing"></a>A differentiable brain simulator bridging brain simulation and brain-inspired computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05106">http://arxiv.org/abs/2311.05106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoming Wang, Tianqiu Zhang, Sichao He, Yifeng Gong, Hongyaoxing Gu, Shangyang Li, Si Wu</li>
<li>for:  bridging the gap between brain simulation and brain-inspired computing (BIC)</li>
<li>methods:  differentiable brain simulator developed using JAX and XLA, with a range of sparse and event-driven operators, an abstraction for managing synaptic computations, and an object-oriented just-in-time compilation approach</li>
<li>results:  efficient and scalable brain simulation, with potential to support research at the intersection of brain simulation and BIC.<details>
<summary>Abstract</summary>
Brain simulation builds dynamical models to mimic the structure and functions of the brain, while brain-inspired computing (BIC) develops intelligent systems by learning from the structure and functions of the brain. The two fields are intertwined and should share a common programming framework to facilitate each other's development. However, none of the existing software in the fields can achieve this goal, because traditional brain simulators lack differentiability for training, while existing deep learning (DL) frameworks fail to capture the biophysical realism and complexity of brain dynamics. In this paper, we introduce BrainPy, a differentiable brain simulator developed using JAX and XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy expands upon the functionalities of JAX, a powerful AI framework, by introducing complete capabilities for flexible, efficient, and scalable brain simulation. It offers a range of sparse and event-driven operators for efficient and scalable brain simulation, an abstraction for managing the intricacies of synaptic computations, a modular and flexible interface for constructing multi-scale brain models, and an object-oriented just-in-time compilation approach to handle the memory-intensive nature of brain dynamics. We showcase the efficiency and scalability of BrainPy on benchmark tasks, highlight its differentiable simulation for biologically plausible spiking models, and discuss its potential to support research at the intersection of brain simulation and BIC.
</details>
<details>
<summary>摘要</summary>
��BrainPy是一个可微分的脑模拟器，使用JAX和XLA开发，旨在将脑模拟和脑计算融合在一起。BrainPy在JAX上扩展了功能，添加了脑模拟的灵活、高效和可扩展功能。它提供了脑模拟中的稀盐和事件驱动操作符， synaptic计算的抽象，多级脑模型的模块化和 гибacco interface，以及对内存密集的脑动力学的对象 oriented即时编译方法。我们在标准任务上展示了BrainPy的效率和可扩展性，并highlight了其可微分的脑模拟功能，并讨论了它在脑模拟和脑计算之间的潜在应用。
</details></li>
</ul>
<hr>
<h2 id="Legal-HNet-Mixing-Legal-Long-Context-Tokens-with-Hartley-Transform"><a href="#Legal-HNet-Mixing-Legal-Long-Context-Tokens-with-Hartley-Transform" class="headerlink" title="Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform"></a>Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05089">http://arxiv.org/abs/2311.05089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Giofré, Sneha Ghantasala</li>
<li>for: This paper aims to explore alternatives to the attention-based layers in the transformers architecture for specialized domains like legal, where long texts are common.</li>
<li>methods: The paper uses non-parametric techniques such as Hartley and Fourier transforms to replace the attention-based layers, and introduces a new hybrid Seq2Seq architecture with a no-attention-based encoder and an attention-based decoder.</li>
<li>results: The authors train models with long input documents from scratch in the legal domain setting and achieve competitive performance on existing summarization tasks with reduced compute and memory requirements. They believe that simpler infrastructures can achieve similar or better performance, making training models more accessible and contributing to a reduction in carbon footprint during training.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文目的是为特定领域 like 法律， где 文本很长的应用场景中探索 transformers 架构中的注意力机制的替代方案。</li>
<li>methods: 这篇论文使用非参数的 Hartley 和 Fourier 变换来取代注意力机制，并提出了一种新的 Hybrid Seq2Seq 架构，其中的编码器是无注意力的，而解码器是注意力的。</li>
<li>results: 作者使用这些新的架构和方法在法律领域中训练了从头开始的模型，并在现有的摘要任务上达到了类似或更好的性能，同时减少了计算和存储的需求。作者认为，这些更简单的基础设施可以在更多人的情况下使用，并且对于减少训练过程中的碳脚印有积极的影响。<details>
<summary>Abstract</summary>
Since its introduction, the transformers architecture has seen great adoption in NLP applications, but it also has limitations. Although the self-attention mechanism allows for generating very rich representations of the input text, its effectiveness may be limited in specialized domains such as legal, where, for example, language models often have to process very long texts. In this paper, we explore alternatives to replace the attention-based layers with simpler token-mixing mechanisms: Hartley and Fourier transforms. Using these non-parametric techniques, we train models with long input documents from scratch in the legal domain setting. We also introduce a new hybrid Seq2Seq architecture, a no-attention-based encoder connected with an attention-based decoder, which performs quite well on existing summarization tasks with much less compute and memory requirements. We believe that similar, if not better performance, as in the case of long correlations of abstractive text summarization tasks, can be achieved by adopting these simpler infrastructures. This not only makes training models from scratch accessible to more people, but also contributes to the reduction of the carbon footprint during training.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:自transformersarchitecture的引入以来，它在NLP应用中得到了广泛的采用，但也有限制。尽管自我关注机制允许生成非常 richexpressionsof输入文本，但在专门领域如法律领域中，语言模型经常需要处理非常长的文本。在这篇论文中，我们探索使用非 Parametric技术来取代关注基层：Hartley和Fourier变换。使用这些非 Parametric技术，我们在法律领域的长输入文档上从scratch训练模型。我们还介绍了一种新的混合Seq2Seq架构，一个没有关注基层的编码器与一个关注基层的解码器相连接，它在现有摘要任务上表现非常出色，需要 Much less compute和memoryrequirements。我们认为可以通过采用这些更简单的基础设施，在摘要任务中 achieve similar或更好的性能，这不仅使得训练模型从scratch变得更加可 accessible，而且也对训练过程中的碳脚印产生了贡献。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-of-semi-supervised-learning-from-tasks-with-heterogeneous-attribute-spaces"><a href="#Meta-learning-of-semi-supervised-learning-from-tasks-with-heterogeneous-attribute-spaces" class="headerlink" title="Meta-learning of semi-supervised learning from tasks with heterogeneous attribute spaces"></a>Meta-learning of semi-supervised learning from tasks with heterogeneous attribute spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05088">http://arxiv.org/abs/2311.05088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomoharu Iwata, Atsutoshi Kumagai</li>
<li>for: 本研究 propose a meta-learning method for semi-supervised learning, 能够学习多个任务，并且这些任务可以有不同的特征空间。</li>
<li>methods: 方法包括使用神经网络将标签和无标签数据同时嵌入到任务特定的空间中，并且使用可变特征自注意层来找到不同特征空间中的嵌入。</li>
<li>results: 实验结果表明，提出的方法在不同特征空间中的分类和回归任务中表现出色，超过了现有的meta-学习和半监督学习方法。<details>
<summary>Abstract</summary>
We propose a meta-learning method for semi-supervised learning that learns from multiple tasks with heterogeneous attribute spaces. The existing semi-supervised meta-learning methods assume that all tasks share the same attribute space, which prevents us from learning with a wide variety of tasks. With the proposed method, the expected test performance on tasks with a small amount of labeled data is improved with unlabeled data as well as data in various tasks, where the attribute spaces are different among tasks. The proposed method embeds labeled and unlabeled data simultaneously in a task-specific space using a neural network, and the unlabeled data's labels are estimated by adapting classification or regression models in the embedding space. For the neural network, we develop variable-feature self-attention layers, which enable us to find embeddings of data with different attribute spaces with a single neural network by considering interactions among examples, attributes, and labels. Our experiments on classification and regression datasets with heterogeneous attribute spaces demonstrate that our proposed method outperforms the existing meta-learning and semi-supervised learning methods.
</details>
<details>
<summary>摘要</summary>
我们提出一种基于多任务的meta学习方法，可以在不同任务的 attribute space 上学习。现有的半supervised meta学习方法假设所有任务共享同一个 attribute space，这限制了我们学习的variety of tasks。我们的方法可以在tasks with small amount of labeled data中提高预期测试性能，并且可以使用不同任务的数据，以及不同 attribute space 上的数据进行学习。我们的方法使用神经网络将标注和无标注数据同时嵌入到任务特定的空间中，并且使用自适应分类或回归模型来估算无标注数据的标签。为神经网络，我们开发了可变特征自注意层，可以通过考虑示例、属性和标签之间的交互来找到不同 attribute space 上的数据嵌入。我们在 Classification 和回归数据集上进行了实验，并证明了我们的提出方法比现有的 meta学习和半supervised learning 方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Large-Language-Models-as-Rationalizers-of-Knowledge-intensive-Tasks"><a href="#Characterizing-Large-Language-Models-as-Rationalizers-of-Knowledge-intensive-Tasks" class="headerlink" title="Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks"></a>Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05085">http://arxiv.org/abs/2311.05085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditi Mishra, Sajjadur Rahman, Hannah Kim, Kushan Mitra, Estevam Hruschka</li>
<li>for: 这篇论文主要用于探讨大型自然语言模型（LLM）在知识填充任务中的能力。</li>
<li>methods: 论文使用专家写的例子来帮助LLM生成知识导向的论证，并通过几 shot 方式来评估其效果。</li>
<li>results: 研究发现，人工写的论证更为可靠，而 LLM 生成的论证尚需进一步改进 conciseness 和创新性。此外，研究还发现，对模型预测错误的论证可能会降低人们对 LLM 生成的论证的信任。<details>
<summary>Abstract</summary>
Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline to review task predictions and eliminate potential incorrect decisions before rationalization, enabling trustworthy rationale generation.
</details>
<details>
<summary>摘要</summary>
In this study, we explore the task of generating knowledge-guided rationalizations in natural language using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required.In another study, we observed that rationalization of incorrect model predictions can erode humans' trust in LLM-generated rationales. To address this issue, we propose a two-stage pipeline to review task predictions and eliminate potential incorrect decisions before rationalization, enabling trustworthy rationale generation.
</details></li>
</ul>
<hr>
<h2 id="Signal-Temporal-Logic-Guided-Apprenticeship-Learning"><a href="#Signal-Temporal-Logic-Guided-Apprenticeship-Learning" class="headerlink" title="Signal Temporal Logic-Guided Apprenticeship Learning"></a>Signal Temporal Logic-Guided Apprenticeship Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05084">http://arxiv.org/abs/2311.05084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniruddh G. Puranic, Jyotirmoy V. Deshmukh, Stefanos Nikolaidis</li>
<li>for: 提高控制策略学习效果，尤其是在具有时间依赖关系的任务中。</li>
<li>methods: 使用时间逻辑规范来描述高级任务目标，并将其编码到图形中以定义时间基于的度量，以改进学习奖励和控制策略的质量。</li>
<li>results: 通过对多种机器人抓取机械 simulations的实验，我们示出了我们的框架可以减少学习奖励和控制策略所需的示例数量，并提高学习效果。<details>
<summary>Abstract</summary>
Apprenticeship learning crucially depends on effectively learning rewards, and hence control policies from user demonstrations. Of particular difficulty is the setting where the desired task consists of a number of sub-goals with temporal dependencies. The quality of inferred rewards and hence policies are typically limited by the quality of demonstrations, and poor inference of these can lead to undesirable outcomes. In this letter, we show how temporal logic specifications that describe high level task objectives, are encoded in a graph to define a temporal-based metric that reasons about behaviors of demonstrators and the learner agent to improve the quality of inferred rewards and policies. Through experiments on a diverse set of robot manipulator simulations, we show how our framework overcomes the drawbacks of prior literature by drastically improving the number of demonstrations required to learn a control policy.
</details>
<details>
<summary>摘要</summary>
TEXT：Apprenticeship learning critically relies on effectively learning rewards and control policies from user demonstrations. However, tasks with multiple sub-goals and temporal dependencies pose significant challenges. The quality of inferred rewards and policies is often limited by the quality of demonstrations, and poor inference can lead to undesirable outcomes.In this letter, we propose using temporal logic specifications to encode high-level task objectives in a graph, enabling a temporal-based metric to reason about the behaviors of both the demonstrators and the learner agent. Our framework improves the quality of inferred rewards and policies by drastically reducing the number of demonstrations required to learn a control policy.We evaluate our approach on a diverse set of robot manipulator simulations and demonstrate its effectiveness in overcoming the drawbacks of prior literature.TRANSLATION：学习契约（Apprenticeship learning）取决于从用户示例中学习奖励和控制策略的效果。然而，具有多个子目标和时间依赖关系的任务具有重要挑战。奖励和策略的质量通常受示例质量的限制，而且差异的推理可能会导致不желатель的结果。在这封信中，我们提议使用时间逻辑规范来编码高级任务目标，并在图表中定义一种时间基于的度量，以便理解示例者和学习者机器人的行为。我们的框架可以有效地提高奖励和策略的质量，通过减少学习者需要的示例数量来减少学习时间。我们在一个多样化的机器人拟合器 simulate 中进行了实验，并证明了我们的框架可以超越先前的文献中的缺点。
</details></li>
</ul>
<hr>
<h2 id="Mental-Health-Diagnosis-in-the-Digital-Age-Harnessing-Sentiment-Analysis-on-Social-Media-Platforms-upon-Ultra-Sparse-Feature-Content"><a href="#Mental-Health-Diagnosis-in-the-Digital-Age-Harnessing-Sentiment-Analysis-on-Social-Media-Platforms-upon-Ultra-Sparse-Feature-Content" class="headerlink" title="Mental Health Diagnosis in the Digital Age: Harnessing Sentiment Analysis on Social Media Platforms upon Ultra-Sparse Feature Content"></a>Mental Health Diagnosis in the Digital Age: Harnessing Sentiment Analysis on Social Media Platforms upon Ultra-Sparse Feature Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05075">http://arxiv.org/abs/2311.05075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haijian Shao, Ming Zhu, Shengjie Zhai<br>for: 这项研究旨在透过社交媒体平台上的推文和讨论来早期发现和 intervene 人们的心理疾病，尤其是投降 GROUP 的人群。methods: 该研究提出了一种新的semantic feature preprocessing技术，包括三个方面：1）mitigating feature sparsity with weak classifier，2）adaptive feature dimension with modulus loops，3）deep-mining and extending features among contexts。results: 该研究使用了 Reddit Mental Health Dataset 2022 进行 исследование，并解决了数据稀缺问题，表现出99.81%非零元素。 after applying preprocessing technique，feature sparsity decreases to 85.4%。 compared to seven benchmark models,该研究的方法显示了significant performance improvement，包括8.0%的准确率、0.069的精度、0.093的准确率、0.102的F1 score 和0.059的AUC。<details>
<summary>Abstract</summary>
Amid growing global mental health concerns, particularly among vulnerable groups, natural language processing offers a tremendous potential for early detection and intervention of people's mental disorders via analyzing their postings and discussions on social media platforms. However, ultra-sparse training data, often due to vast vocabularies and low-frequency words, hinders the analysis accuracy. Multi-labeling and Co-occurrences of symptoms may also blur the boundaries in distinguishing similar/co-related disorders. To address these issues, we propose a novel semantic feature preprocessing technique with a three-folded structure: 1) mitigating the feature sparsity with a weak classifier, 2) adaptive feature dimension with modulus loops, and 3) deep-mining and extending features among the contexts. With enhanced semantic features, we train a machine learning model to predict and classify mental disorders. We utilize the Reddit Mental Health Dataset 2022 to examine conditions such as Anxiety, Borderline Personality Disorder (BPD), and Bipolar-Disorder (BD) and present solutions to the data sparsity challenge, highlighted by 99.81% non-zero elements. After applying our preprocessing technique, the feature sparsity decreases to 85.4%. Overall, our methods, when compared to seven benchmark models, demonstrate significant performance improvements: 8.0% in accuracy, 0.069 in precision, 0.093 in recall, 0.102 in F1 score, and 0.059 in AUC. This research provides foundational insights for mental health prediction and monitoring, providing innovative solutions to navigate challenges associated with ultra-sparse data feature and intricate multi-label classification in the domain of mental health analysis.
</details>
<details>
<summary>摘要</summary>
在全球心理健康问题的增长中，特别是对护理不足的群体，自然语言处理技术具有巨大的潜在价值，通过分析社交媒体平台上的发言和讨论来早期发现和 intervene 人们的心理疾病。然而，由于庞大的词汇和低频词汇，导致分析精度受限。同时，病种的多标签和症状的协occurrence也使得分类变得更加困难。为解决这些问题，我们提出了一种新的 semantics feature 处理技术，包括三个部分：1） mitigate feature sparsity with weak classifier，2） adaptive feature dimension with modulus loops，3） deep-mining and extending features among the contexts。通过增强 semantics features，我们训练了一个机器学习模型，以预测和分类心理疾病。我们使用2022年Reddit心理健康数据集来检查抑郁、边缘人格障碍（BPD）和mania病（BD）等病种，并解决数据稀缺问题，表现出99.81%的非零元素。在我们的预处理技术应用后，特征稀缺度下降至85.4%。总的来说，我们的方法，与七个 Refer 模型进行比较，显示了显著的性能提升：准确率提升8.0%，精度提升0.069，准确率提升0.093，F1 score提升0.102，AUC提升0.059。这种研究为心理健康预测和监测提供了基础性的发现，并提供了在心理健康分析领域中创新的解决方案，以 navigating 稀缺数据特征和心理健康多标签分类的挑战。
</details></li>
</ul>
<hr>
<h2 id="A-Framework-to-Assess-Dis-agreement-Among-Diverse-Rater-Groups"><a href="#A-Framework-to-Assess-Dis-agreement-Among-Diverse-Rater-Groups" class="headerlink" title="A Framework to Assess (Dis)agreement Among Diverse Rater Groups"></a>A Framework to Assess (Dis)agreement Among Diverse Rater Groups</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05074">http://arxiv.org/abs/2311.05074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinodkumar Prabhakaran, Christopher Homan, Lora Aroyo, Alicia Parrish, Alex Taylor, Mark Díaz, Ding Wang</li>
<li>for: 本研究旨在探讨对话AI中的安全保障问题，尤其是针对人工评分和反馈的限制，以及这些评分的主观性和多样性。</li>
<li>methods: 本研究提出了一种全面的分歧分析框架，用于评估不同评分者群体之间的视角多样性。这种框架基于一个大量的人工评分 dataset，并通过多种方法来评估分歧。</li>
<li>results: results reveal that there are specific rater groups that have more diverse perspectives than the rest, and informs demographic axes that are crucial to consider for safety annotations. 这些结果表明，有些评分者群体的视角更加多样，并且提供了关键的人类特征轴，用于安全注释。<details>
<summary>Abstract</summary>
Recent advancements in conversational AI have created an urgent need for safety guardrails that prevent users from being exposed to offensive and dangerous content. Much of this work relies on human ratings and feedback, but does not account for the fact that perceptions of offense and safety are inherently subjective and that there may be systematic disagreements between raters that align with their socio-demographic identities. Instead, current machine learning approaches largely ignore rater subjectivity and use gold standards that obscure disagreements (e.g., through majority voting). In order to better understand the socio-cultural leanings of such tasks, we propose a comprehensive disagreement analysis framework to measure systematic diversity in perspectives among different rater subgroups. We then demonstrate its utility by applying this framework to a dataset of human-chatbot conversations rated by a demographically diverse pool of raters. Our analysis reveals specific rater groups that have more diverse perspectives than the rest, and informs demographic axes that are crucial to consider for safety annotations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的对话AI的进步创造了一个紧迫的需求，即防止用户暴露于不当和危险的内容。大多数这些工作依赖于人类评级和反馈，但不考虑评级者的主观性和可能存在的系统性的不一致，这些不一致与评级者的社会人类特征有关。而当前的机器学习方法大多忽略评级者的主观性，使用金标准来隐藏不一致（例如，通过多数投票）。为了更好地理解这类任务的社会文化倾向，我们提出了一个全面的不一致分析框架，用于测量不同评级者 subgroup 中的系统性多样性。然后，我们通过应用这个框架于一个人类-聊天机器人对话评级Pool 中的数据集来证明其实用性。我们的分析发现特定的评级者 subgroup 有更多的多样性，并提供了关键的人类轴，用于安全注释。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Exploration-with-Unlabeled-Prior-Data"><a href="#Accelerating-Exploration-with-Unlabeled-Prior-Data" class="headerlink" title="Accelerating Exploration with Unlabeled Prior Data"></a>Accelerating Exploration with Unlabeled Prior Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05067">http://arxiv.org/abs/2311.05067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyang Li, Jason Zhang, Dibya Ghosh, Amy Zhang, Sergey Levine</li>
<li>for: 解决标准奖励学习（RL）算法在稀疏奖励任务上学习缺乏问题。</li>
<li>methods: 我们提出了一种简单的方法，通过在在线经验中学习奖励模型，将无奖数据标签为乐观奖励，并将其与在线数据同时使用以优化策略和评价器。</li>
<li>results: 我们的结果表明，可以很容易地将无奖数据 integrate into现有的在线RL算法中，并且这种方法在一些稀疏奖励领域中显示出了效果，包括AntMaze领域、Adroit手动操作领域和一个视觉模拟Robotic manipulation领域。<details>
<summary>Abstract</summary>
Learning to solve tasks from a sparse reward signal is a major challenge for standard reinforcement learning (RL) algorithms. However, in the real world, agents rarely need to solve sparse reward tasks entirely from scratch. More often, we might possess prior experience to draw on that provides considerable guidance about which actions and outcomes are possible in the world, which we can use to explore more effectively for new tasks. In this work, we study how prior data without reward labels may be used to guide and accelerate exploration for an agent solving a new sparse reward task. We propose a simple approach that learns a reward model from online experience, labels the unlabeled prior data with optimistic rewards, and then uses it concurrently alongside the online data for downstream policy and critic optimization. This general formula leads to rapid exploration in several challenging sparse-reward domains where tabula rasa exploration is insufficient, including the AntMaze domain, Adroit hand manipulation domain, and a visual simulated robotic manipulation domain. Our results highlight the ease of incorporating unlabeled prior data into existing online RL algorithms, and the (perhaps surprising) effectiveness of doing so.
</details>
<details>
<summary>摘要</summary>
学习从笔记 reward 信号解决任务是标准人工智能学习（RL）算法的主要挑战。然而，在实际世界中，代理人很少需要从头开始解决笔记 reward 任务。更frequently，我们可以利用先前的经验，从而更好地探索新任务。在这项工作中，我们研究了如何使用没有奖券标签的先前数据来导航和加速新任务的探索。我们提议一种简单的方法，即在在线经验中学习奖券模型，将无奖券数据标签为乐观奖券，然后将其与在线数据并行使用于下游策略和评估器优化。这种总体方法在一些笔记 reward 领域中实现了快速探索，包括AntMaze 领域、Adroit 手动操作领域和视觉模拟 робо控制领域。我们的结果表明可以轻松地将无奖券数据 incorporated 到现有的在线 RL 算法中，以及这种方法的（可能吸引人的）效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/cs.AI_2023_11_09/" data-id="clot2mh9c0071x788fxyp9hh0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/09/cs.CV_2023_11_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-11-09
        
      </div>
    </a>
  
  
    <a href="/2023/11/09/cs.CL_2023_11_09/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-11-09</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">61</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">70</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">65</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
