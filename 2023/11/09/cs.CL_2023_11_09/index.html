
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-11-09 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="FAMuS: Frames Across Multiple Sources paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05601 repo_url: None paper_authors: Siddharth Vashishtha, Alexander Martin, William Gantt, Benjamin Van Durme, Aaron Steven W">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-11-09">
<meta property="og:url" content="https://nullscc.github.io/2023/11/09/cs.CL_2023_11_09/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="FAMuS: Frames Across Multiple Sources paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05601 repo_url: None paper_authors: Siddharth Vashishtha, Alexander Martin, William Gantt, Benjamin Van Durme, Aaron Steven W">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-09T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-10T09:25:48.042Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/cs.CL_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T11:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-11-09
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FAMuS-Frames-Across-Multiple-Sources"><a href="#FAMuS-Frames-Across-Multiple-Sources" class="headerlink" title="FAMuS: Frames Across Multiple Sources"></a>FAMuS: Frames Across Multiple Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05601">http://arxiv.org/abs/2311.05601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Vashishtha, Alexander Martin, William Gantt, Benjamin Van Durme, Aaron Steven White</li>
<li>for: 提供更深刻的事件理解，通过跨文档的信息汇集</li>
<li>methods: 使用Wikipedia passage和不同类型的源文章（非Wikipedia），并对事件和句子进行框架标注</li>
<li>results: 实现了两个关键的事件理解任务：确认文档是否为目标报道事件的有效来源，以及在报道和正确的源文章中提取全文 Argument EXTRACTION<details>
<summary>Abstract</summary>
Understanding event descriptions is a central aspect of language processing, but current approaches focus overwhelmingly on single sentences or documents. Aggregating information about an event \emph{across documents} can offer a much richer understanding. To this end, we present FAMuS, a new corpus of Wikipedia passages that \emph{report} on some event, paired with underlying, genre-diverse (non-Wikipedia) \emph{source} articles for the same event. Events and (cross-sentence) arguments in both report and source are annotated against FrameNet, providing broad coverage of different event types. We present results on two key event understanding tasks enabled by FAMuS: \emph{source validation} -- determining whether a document is a valid source for a target report event -- and \emph{cross-document argument extraction} -- full-document argument extraction for a target event from both its report and the correct source article. We release both FAMuS and our models to support further research.
</details>
<details>
<summary>摘要</summary>
理解事件描述是语言处理的中心方面，但现有的方法主要集中在单个句子或文档之上。将信息about事件通过文档进行聚合可以提供更深刻的理解。为此，我们提出了FAMuS，一个新的Wikipedia段落和下层、种类多样（非Wikipedia）的源文章对应的事件集。这些事件和跨句子Argument在报道和源文章上都被注解为FrameNet，从而提供了不同事件类型的广泛覆盖。我们对两个关键的事件理解任务进行了实验：一是确定报道事件的目标文档是否为有效的源文档（源验证），二是从报道和正确的源文章中提取全文Argument（跨文档Argument提取）。我们将FAMuS和我们的模型公开发布，以支持进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="The-Iron-ic-Melting-Pot-Reviewing-Human-Evaluation-in-Humour-Irony-and-Sarcasm-Generation"><a href="#The-Iron-ic-Melting-Pot-Reviewing-Human-Evaluation-in-Humour-Irony-and-Sarcasm-Generation" class="headerlink" title="The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation"></a>The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05552">http://arxiv.org/abs/2311.05552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Loakman, Aaron Maladry, Chenghua Lin</li>
<li>for: 本研究评估了自然语言生成系统的评估方法，强调了评估人员的特点对于生成不同类型语言（幽默、讽刺、讲究）的影响。</li>
<li>methods: 本研究使用了对例语言形式的分析，以及对当前NLG研究中评估方法的批判性评估。</li>
<li>results: 研究发现，评估人员的特点对于语言形式的解释有很大影响，而现有的评估方法往往不具有开放报告评估人员特点的特点。<details>
<summary>Abstract</summary>
Human evaluation is often considered to be the gold standard method of evaluating a Natural Language Generation system. However, whilst its importance is accepted by the community at large, the quality of its execution is often brought into question. In this position paper, we argue that the generation of more esoteric forms of language - humour, irony and sarcasm - constitutes a subdomain where the characteristics of selected evaluator panels are of utmost importance, and every effort should be made to report demographic characteristics wherever possible, in the interest of transparency and replicability. We support these claims with an overview of each language form and an analysis of examples in terms of how their interpretation is affected by different participant variables. We additionally perform a critical survey of recent works in NLG to assess how well evaluation procedures are reported in this subdomain, and note a severe lack of open reporting of evaluator demographic information, and a significant reliance on crowdsourcing platforms for recruitment.
</details>
<details>
<summary>摘要</summary>
人类评估通常被认为是自然语言生成系统的黄金标准评估方法。然而，虽然其重要性得到了社区的普遍认可，但其执行质量往往受到质疑。在这篇位点纸中，我们 argue that生成更加罕见的语言形式，如幽默、讽刺和讲究，是评估自然语言生成系统的一个子领域，其中选择的评估人员特征的重要性在最高点。我们支持这些主张通过语言形式的概述和例子的分析来证明，并且对近期NLG研究进行了批判性的检查，发现评估过程的报道不够透明和可重复。此外，我们还发现了评估人员的民族特征不够公开报道，以及依赖于协同平台来吸引参与者的偏好。
</details></li>
</ul>
<hr>
<h2 id="Towards-End-to-End-Spoken-Grammatical-Error-Correction"><a href="#Towards-End-to-End-Spoken-Grammatical-Error-Correction" class="headerlink" title="Towards End-to-End Spoken Grammatical Error Correction"></a>Towards End-to-End Spoken Grammatical Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05550">http://arxiv.org/abs/2311.05550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefano Bannò, Rao Ma, Mengjie Qian, Kate M. Knill, Mark J. F. Gales</li>
<li>for: 这个论文的目的是提出一种新的端到端方法来进行口语 grammatical error correction (GEC)。</li>
<li>methods: 这种端到端方法利用了一个基于 speech recognition 的基础模型 Whisper，可以取代传统的批处理 pipeline。</li>
<li>results: 研究发现，使用端到端方法可以实现口语 GEC，但由于数据的限制，其现在的性能较差于使用大量文本基于 GEC 数据。然而，端到端的不连续检测和消除方法可以超越传统的批处理方法。<details>
<summary>Abstract</summary>
Grammatical feedback is crucial for L2 learners, teachers, and testers. Spoken grammatical error correction (GEC) aims to supply feedback to L2 learners on their use of grammar when speaking. This process usually relies on a cascaded pipeline comprising an ASR system, disfluency removal, and GEC, with the associated concern of propagating errors between these individual modules. In this paper, we introduce an alternative "end-to-end" approach to spoken GEC, exploiting a speech recognition foundation model, Whisper. This foundation model can be used to replace the whole framework or part of it, e.g., ASR and disfluency removal. These end-to-end approaches are compared to more standard cascaded approaches on the data obtained from a free-speaking spoken language assessment test, Linguaskill. Results demonstrate that end-to-end spoken GEC is possible within this architecture, but the lack of available data limits current performance compared to a system using large quantities of text-based GEC data. Conversely, end-to-end disfluency detection and removal, which is easier for the attention-based Whisper to learn, does outperform cascaded approaches. Additionally, the paper discusses the challenges of providing feedback to candidates when using end-to-end systems for spoken GEC.
</details>
<details>
<summary>摘要</summary>
grammatical feedback 是对 L2 学习者、教师和测试人员的关键。口语grammatical error correction (GEC) 的目标是为 L2 学习者提供语法使用时的反馈。这个过程通常包括一个级联ipeline，包括 ASR 系统、缺失除去和 GEC，并且有关联的担忧是在这些个人模块之间传递错误。在这篇论文中，我们介绍了一种代替性的 "end-to-end" 方法，使用 Whisper 基础模型进行口语 GEC。这个基础模型可以用于替换整个框架或一部分，例如 ASR 和缺失除去。这些 end-to-end 方法与更惯用的级联方法进行比较，使用来自自由说话的语言评估测试（Linguaskill）的数据。结果表明，end-to-end 口语 GEC 是可能的，但由于数据的有限性，其现在性相比于使用大量文本基础 datos GEC 数据的系统相对较差。然而，end-to-end 缺失检测和除去，这是对 Attention-based Whisper 更容易学习的任务，实际上超过了级联approaches。论文还讨论了使用 end-to-end 系统进行口语 GEC 时给候补的挑战。
</details></li>
</ul>
<hr>
<h2 id="All-Should-Be-Equal-in-the-Eyes-of-Language-Models-Counterfactually-Aware-Fair-Text-Generation"><a href="#All-Should-Be-Equal-in-the-Eyes-of-Language-Models-Counterfactually-Aware-Fair-Text-Generation" class="headerlink" title="All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation"></a>All Should Be Equal in the Eyes of Language Models: Counterfactually Aware Fair Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05451">http://arxiv.org/abs/2311.05451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pragyan Banerjee, Abhinav Java, Surgan Jandial, Simra Shahid, Shaz Furniturewala, Balaji Krishnamurthy, Sumit Bhatia</li>
<li>for: 提高语音模型的公平性，以避免因训练数据中的偏见而导致的不公平性问题。</li>
<li>methods: 提出了一种Counterfactually Aware Fair InferencE（CAFIE）框架，通过比较不同民族群体下的模型理解来生成更公平的句子。</li>
<li>results: CAFIE在不同大小的基础语音模型和三个多样化的数据集上进行了广泛的实验，并显示了较强的表现，可以生成更公平的文本，同时保持语音模型的能力。<details>
<summary>Abstract</summary>
Fairness in Language Models (LMs) remains a longstanding challenge, given the inherent biases in training data that can be perpetuated by models and affect the downstream tasks. Recent methods employ expensive retraining or attempt debiasing during inference by constraining model outputs to contrast from a reference set of biased templates or exemplars. Regardless, they dont address the primary goal of fairness to maintain equitability across different demographic groups. In this work, we posit that inferencing LMs to generate unbiased output for one demographic under a context ensues from being aware of outputs for other demographics under the same context. To this end, we propose Counterfactually Aware Fair InferencE (CAFIE), a framework that dynamically compares the model understanding of diverse demographics to generate more equitable sentences. We conduct an extensive empirical evaluation using base LMs of varying sizes and across three diverse datasets and found that CAFIE outperforms strong baselines. CAFIE produces fairer text and strikes the best balance between fairness and language modeling capability
</details>
<details>
<summary>摘要</summary>
“对于语言模型（LM）中的公平性仍然是一个长期挑战，因为训练数据中内置的偏见可能会被模型传播到下游任务中，导致不公正的结果。现有的方法通常是耗费高的重训或在推理过程中对模型的输出进行减偏，但这些方法不能实现公平性的主要目标，即在不同的民族群体之间维持平等。在这个工作中，我们提出了Counterfactually Aware Fair InferencE（CAFIE）框架，它在对于一个民族群体的推理过程中，通过比较模型对不同民族群体的理解，生成更公平的句子。我们进行了广泛的实验研究，使用不同的基础LM和三个多样化的数据集，发现CAFIE对于公平性和语言模型能力都能实现更好的平衡”
</details></li>
</ul>
<hr>
<h2 id="Memorisation-Cartography-Mapping-out-the-Memorisation-Generalisation-Continuum-in-Neural-Machine-Translation"><a href="#Memorisation-Cartography-Mapping-out-the-Memorisation-Generalisation-Continuum-in-Neural-Machine-Translation" class="headerlink" title="Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation"></a>Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05379">http://arxiv.org/abs/2311.05379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Verna Dankers, Ivan Titov, Dieuwke Hupkes</li>
<li>for: 这个论文研究了神经网络模型在训练过程中是否能够快速记忆一些源-目标映射，以及这种记忆是否会影响神经网络模型的性能。</li>
<li>methods: 该论文使用了counterfactual memorization metric来构建500万个NMT数据点的记忆-总结特征图，并用这个图来预测NMT模型的表现。</li>
<li>results: 研究发现，NMT模型在训练过程中会快速记忆一些数据点，但是这种记忆并不一定是好的。此外，研究还发现，模型在不同的数据点上的表现会受到数据点的表面特征和模型在每个数据点上的训练信号的影响。<details>
<summary>Abstract</summary>
When training a neural network, it will quickly memorise some source-target mappings from your dataset but never learn some others. Yet, memorisation is not easily expressed as a binary feature that is good or bad: individual datapoints lie on a memorisation-generalisation continuum. What determines a datapoint's position on that spectrum, and how does that spectrum influence neural models' performance? We address these two questions for neural machine translation (NMT) models. We use the counterfactual memorisation metric to (1) build a resource that places 5M NMT datapoints on a memorisation-generalisation map, (2) illustrate how the datapoints' surface-level characteristics and a models' per-datum training signals are predictive of memorisation in NMT, (3) and describe the influence that subsets of that map have on NMT systems' performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Build a resource that places 5M NMT datapoints on a memorization-generalization map2. Illustrate how the datapoints’ surface-level characteristics and a models’ per-datum training signals are predictive of memorization in NMT3. Describe the influence that subsets of that map have on NMT systems’ performance.Translated into Simplified Chinese: WHEN 训练一个神经网络，它很快就会记忆一些源-目标映射从你的数据集中，但从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前从来之前从来之前从来之前从来之前从来之前从来之前从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但从来之前 FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但FROM 一个神经网络，它会快速记忆一些源-目标映射从你的数据集中，但FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些源-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神经网络，它会快速记忆一些 source-目标映射 FROM 一个神</details></li>
</ol>
<hr>
<h2 id="There’s-no-Data-Like-Better-Data-Using-QE-Metrics-for-MT-Data-Filtering"><a href="#There’s-no-Data-Like-Better-Data-Using-QE-Metrics-for-MT-Data-Filtering" class="headerlink" title="There’s no Data Like Better Data: Using QE Metrics for MT Data Filtering"></a>There’s no Data Like Better Data: Using QE Metrics for MT Data Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05350">http://arxiv.org/abs/2311.05350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Thorsten Peter, David Vilar, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Markus Freitag</li>
<li>for: 这篇论文是为了评估机器翻译输出质量而写的。</li>
<li>methods: 这篇论文使用了神经级别评估方法来评估机器翻译输出的质量。</li>
<li>results: 研究发现，通过选择训练数据中最高质量的句子对，可以提高翻译质量，同时减少训练数据的量一半。<details>
<summary>Abstract</summary>
Quality Estimation (QE), the evaluation of machine translation output without the need of explicit references, has seen big improvements in the last years with the use of neural metrics. In this paper we analyze the viability of using QE metrics for filtering out bad quality sentence pairs in the training data of neural machine translation systems~(NMT). While most corpus filtering methods are focused on detecting noisy examples in collections of texts, usually huge amounts of web crawled data, QE models are trained to discriminate more fine-grained quality differences. We show that by selecting the highest quality sentence pairs in the training data, we can improve translation quality while reducing the training size by half. We also provide a detailed analysis of the filtering results, which highlights the differences between both approaches.
</details>
<details>
<summary>摘要</summary>
Quality Estimation (QE)，机器翻译输出评估的方法，在过去几年内有了大幅度的改进，通过使用神经级别的评估指标。在这篇论文中，我们分析了使用QE指标来过滤机器翻译系统（NMT）的训练数据中的坏质句子对的可能性。大多数文库过滤方法通常是对大量的网络抓取数据进行干扰检测，而QE模型则是通过检测更细化的质量差异来训练。我们发现，通过选择训练数据中的高品质句子对，可以提高翻译质量，同时减少训练数据的一半。我们还提供了过滤结果的详细分析，这些分析结果显示了两种方法之间的差异。
</details></li>
</ul>
<hr>
<h2 id="DeeLM-Dependency-enhanced-Large-Language-Model-for-Sentence-Embeddings"><a href="#DeeLM-Dependency-enhanced-Large-Language-Model-for-Sentence-Embeddings" class="headerlink" title="DeeLM: Dependency-enhanced Large Language Model for Sentence Embeddings"></a>DeeLM: Dependency-enhanced Large Language Model for Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05296">http://arxiv.org/abs/2311.05296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianming Li, Jing Li</li>
<li>for: 提高句子嵌入表示性</li>
<li>methods: 利用依赖关系增强大型语言模型（LLM），提高句子嵌入表示性</li>
<li>results: 比基eline和其他方法表现出色，在 semantic textual similarity（STS）任务中达到了状态理论水平<details>
<summary>Abstract</summary>
Recent studies have proposed using large language models (LLMs) for sentence embeddings. However, most existing LLMs are built with an autoregressive architecture that primarily captures forward dependencies while neglecting backward dependencies. Previous work has highlighted the importance of backward dependencies in improving sentence embeddings. To address this issue, in this paper, we first present quantitative evidence demonstrating the limited learning of backward dependencies in LLMs. Then, we propose a novel approach called Dependency-Enhanced Large Language Model (DeeLM) to improve sentence embeddings. Specifically, we found a turning point in LLMs, where surpassing specific LLM layers leads to a significant performance drop in the semantic textual similarity (STS) task. STS is a crucial task for evaluating sentence embeddings. We then extract the layers after the turning point to make them bidirectional, allowing for the learning of backward dependencies. Extensive experiments demonstrate that DeeLM outperforms baselines and achieves state-of-the-art performance across various STS tasks.
</details>
<details>
<summary>摘要</summary>
近期研究提出使用大型语言模型（LLM）进行句子嵌入。然而，现有的大多数LLM都是基于推导式架构，主要捕捉前向依赖关系而忽视后向依赖关系。先前的工作表明了后向依赖关系在改进句子嵌入上的重要性。为解决这个问题，在这篇论文中，我们首先提供了LLM中后向依赖关系的有限学习证据。然后，我们提出了一种名为依赖加强大型语言模型（DeeLM）的新方法，以改进句子嵌入。具体来说，我们在LLM中找到了一个转折点，其中继承specific LLM层会导致STS任务中的性能下降。STS任务是评估句子嵌入的关键任务。我们然后提取这些层，使其变为双向的，以便学习后向依赖关系。广泛的实验证明了DeeLM在不同STS任务中的优秀表现，超过了基eline和状态之前的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-from-Text-Unveiling-Interactions-between-Variables"><a href="#Causal-Inference-from-Text-Unveiling-Interactions-between-Variables" class="headerlink" title="Causal Inference from Text: Unveiling Interactions between Variables"></a>Causal Inference from Text: Unveiling Interactions between Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05286">http://arxiv.org/abs/2311.05286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Zhou, Yulan He</li>
<li>for: 用于估计文本数据中的 causal effect</li>
<li>methods: 使用 adjustment for latent covariates 方法，并且通过解耦变量来减少偏见</li>
<li>results: 在实验中，提出的模型significantly outperforms  latest strong baselines，并且在实际场景中可以有效地减少偏见。<details>
<summary>Abstract</summary>
Adjusting for latent covariates is crucial for estimating causal effects from observational textual data. Most existing methods only account for confounding covariates that affect both treatment and outcome, potentially leading to biased causal effects. This bias arises from insufficient consideration of non-confounding covariates, which are relevant only to either the treatment or the outcome. In this work, we aim to mitigate the bias by unveiling interactions between different variables to disentangle the non-confounding covariates when estimating causal effects from text. The disentangling process ensures covariates only contribute to their respective objectives, enabling independence between variables. Additionally, we impose a constraint to balance representations from the treatment group and control group to alleviate selection bias. We conduct experiments on two different treatment factors under various scenarios, and the proposed model significantly outperforms recent strong baselines. Furthermore, our thorough analysis on earnings call transcripts demonstrates that our model can effectively disentangle the variables, and further investigations into real-world scenarios provide guidance for investors to make informed decisions.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:需要调整隐藏的共分布量是适当估计从观察性数据中的 causal effect 的关键。大多数现有的方法只考虑到对从事和结果都有影响的共分布量，可能导致偏见的 causal effect。这个偏见来自于不足考虑非共分布量，这些共分布量仅仅对从事或结果有影响。在这个工作中，我们想要减轻这个偏见，通过揭露不同变量之间的互动来分离非共分布量，从而确保共分布量仅对其所属的目标做出贡献，并且使变量独立。此外，我们对从事和控制群体的表现进行平衡约束，以减少选择偏见。我们在两个不同的从事因子下进行了多种情况的实验，并且我们的提案模型在最近的强大基准下表现出色。此外，我们对会计呼缩说明中的分析表明，我们的模型可以有效地分离变量，并且进一步的实验在实际情况中提供了投资者做出 Informed 决策的指南。
</details></li>
</ul>
<hr>
<h2 id="Modelling-prospective-memory-and-resilient-situated-communications-via-Wizard-of-Oz"><a href="#Modelling-prospective-memory-and-resilient-situated-communications-via-Wizard-of-Oz" class="headerlink" title="Modelling prospective memory and resilient situated communications via Wizard of Oz"></a>Modelling prospective memory and resilient situated communications via Wizard of Oz</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05268">http://arxiv.org/abs/2311.05268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzhe Li, Frank Broz, Mark Neerincx</li>
<li>for: 这个研究旨在 explore 社交辅助机器人（SAR）的记忆模型，以便与老年人进行日常活动时的人机交流。</li>
<li>methods: 该研究使用了一个家庭 Setting 中的老年人和机器人进行人机交流的enario，以收集失败的语音技术和人机交流中的共享记忆数据。</li>
<li>results: 研究会获得日常活动中的语音技术失败和人机交流中的共享记忆数据，以便更好地理解社交辅助机器人（SAR）的记忆模型。<details>
<summary>Abstract</summary>
This abstract presents a scenario for human-robot action in a home setting involving an older adult and a robot. The scenario is designed to explore the envisioned modelling of memory for communication with a socially assistive robots (SAR). The scenario will enable the gathering of data on failures of speech technology and human-robot communication involving shared memory that may occur during daily activities such as a music-listening activity.
</details>
<details>
<summary>摘要</summary>
这个报告提出了一个家庭设定下的人机合作场景，其中包括老年人和一个社会辅助机器人（SAR）。这个场景的目的是探索对话模型的应用，以便与SAR进行交流。通过日常活动，如音乐听众活动，可以收集失败的语音技术和人机交流中的共享记忆的数据。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Hallucination-in-Large-Language-Models-Principles-Taxonomy-Challenges-and-Open-Questions"><a href="#A-Survey-on-Hallucination-in-Large-Language-Models-Principles-Taxonomy-Challenges-and-Open-Questions" class="headerlink" title="A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"></a>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05232">http://arxiv.org/abs/2311.05232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu</li>
<li>for: 这篇论文旨在提供关于大语言模型幻觉的现代概念和研究进展的全面和深入的综述。</li>
<li>methods: 论文使用了一种创新的幻觉分类法，并对幻觉的因素进行了分析。</li>
<li>results: 论文对幻觉探测和缓解方法进行了全面的介绍，并提出了未来研究方向的问题。<details>
<summary>Abstract</summary>
The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs."中文翻译：大型自然语言处理（NLP）模型（LLM）的出现标志着NLP领域的重要突破，导致文本理解和生成的显著进步。然而，LLMs同时也显示出一种重要的倾向，即生成幻觉，导致与实际世界事实或用户输入不符的内容。这种现象对LLMs的实际应用带来了重大挑战，也引起了对幻觉的检测和 Mitigation 的关注。在本文中，我们提供了LLM幻觉领域的全面和深入的概述，包括幻觉分类、幻觉的原因、幻觉检测方法和标准、以及适应幻觉的方法。最后，我们分析了当前的挑战和未知点，以便顺利地探索未来LLM幻觉领域的研究方向。>>
</details></li>
</ul>
<hr>
<h2 id="PRODIGy-a-PROfile-based-DIalogue-Generation-dataset"><a href="#PRODIGy-a-PROfile-based-DIalogue-Generation-dataset" class="headerlink" title="PRODIGy: a PROfile-based DIalogue Generation dataset"></a>PRODIGy: a PROfile-based DIalogue Generation dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05195">http://arxiv.org/abs/2311.05195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniela Occhipinti, Serra Sinem Tekiroglu, Marco Guerini</li>
<li>for: 提高对话机器人的一致性和 coherence，以便进行更好的对话。</li>
<li>methods: 使用标准和更复杂的对话者表示，并创建一个新的资源，每个对话都与所有可能的对话者表示相对应。</li>
<li>results: 比较 Profile-based 模型和只使用对话的模型， Profile-based 模型在预测和交互中有更好的一致性和泛化能力，并且人工评价也表明，生成与对话和 Profile 相符的内容得到了普遍的偏好。<details>
<summary>Abstract</summary>
Providing dialogue agents with a profile representation can improve their consistency and coherence, leading to better conversations. However, current profile-based dialogue datasets for training such agents contain either explicit profile representations that are simple and dialogue-specific, or implicit representations that are difficult to collect. In this work, we propose a unified framework in which we bring together both standard and more sophisticated profile representations by creating a new resource where each dialogue is aligned with all possible speaker representations such as communication style, biographies, and personality. This framework allows to test several baselines built using generative language models with several profile configurations. The automatic evaluation shows that profile-based models have better generalisation capabilities than models trained on dialogues only, both in-domain and cross-domain settings. These results are consistent for fine-tuned models and instruction-based LLMs. Additionally, human evaluation demonstrates a clear preference for generations consistent with both profile and context. Finally, to account for possible privacy concerns, all experiments are done under two configurations: inter-character and intra-character. In the former, the LM stores the information about the character in its internal representation, while in the latter, the LM does not retain any personal information but uses it only at inference time.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)提供对话代理人 Profile 表示可以提高对话的一致性和 coherence，导致更好的对话。然而，当前的对话 Profile 数据集用于训练这些代理人只包含了简单的对话特定的 Profile 表示或困难收集的含义 Profile 表示。在这项工作中，我们提出了一个统一的框架，在该框架中，每个对话都与所有可能的 Speaker 表示相对应，包括交流风格、生平、人性等。这个框架允许我们测试多种 Profile 配置下的基elines 使用生成语言模型。自动评估结果表明， Profile-based 模型在预测和跨预测设置中具有更好的泛化能力。这些结果是精度模型和指令基本语言模型的情况下均持。此外，人工评估表明，生成与 Profile 和上下文一致的生成具有明显的偏好。最后，为了解决 posible privacy concern，我们在两种配置下进行所有实验：inter-character 和 intra-character。在前者中，LM 将对话人物的信息存储在其内部表示中，而在后者中，LM 不会保留任何个人信息，只在推理时使用它们。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-and-Prompt-Engineering-for-Biomedical-Query-Focused-Multi-Document-Summarisation"><a href="#Large-Language-Models-and-Prompt-Engineering-for-Biomedical-Query-Focused-Multi-Document-Summarisation" class="headerlink" title="Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation"></a>Large Language Models and Prompt Engineering for Biomedical Query Focused Multi-Document Summarisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05169">http://arxiv.org/abs/2311.05169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Mollá</li>
<li>for: 本研究使用提示工程和GPT-3.5进行生物医学问题焦点多文摘要。</li>
<li>methods: 使用GPT-3.5和合适的提示，我们的系统在2023年 BioASQ 挑战（BioASQ 11b）中获得了最高的 ROUGE-F1 成绩。</li>
<li>results: 研究证明，1）含有少量示例的提示通常比零例示例提示更好；2）检索增强生成带来最大的改进。这些提示使我们的最佳运行排名在 BioASQ 11b 中的前两名，这说明使用适当的提示对大语言模型，特别是GPT-3.5，在问题焦点 summarization 中具有强大的能力。<details>
<summary>Abstract</summary>
This paper reports on the use of prompt engineering and GPT-3.5 for biomedical query-focused multi-document summarisation. Using GPT-3.5 and appropriate prompts, our system achieves top ROUGE-F1 results in the task of obtaining short-paragraph-sized answers to biomedical questions in the 2023 BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in other domains: 1) Prompts that incorporated few-shot samples generally improved on their counterpart zero-shot variants; 2) The largest improvement was achieved by retrieval augmented generation. The fact that these prompts allow our top runs to rank within the top two runs of BioASQ 11b demonstrate the power of using adequate prompts for Large Language Models in general, and GPT-3.5 in particular, for query-focused summarisation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Prompts that incorporated few-shot samples generally performed better than their zero-shot counterparts.2. Retrieval-augmented generation led to the largest improvement.The use of effective prompts demonstrates the potential of large language models, specifically GPT-3.5, for query-focused summarization. Our top runs ranked among the top two runs in BioASQ 11b, highlighting the power of prompt engineering for improving summarization performance.</details></li>
</ol>
<hr>
<h2 id="Enhancing-Computation-Efficiency-in-Large-Language-Models-through-Weight-and-Activation-Quantization"><a href="#Enhancing-Computation-Efficiency-in-Large-Language-Models-through-Weight-and-Activation-Quantization" class="headerlink" title="Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization"></a>Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05161">http://arxiv.org/abs/2311.05161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jangwhan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook Choi</li>
<li>for: 提高 LLM 的计算效率，增加Task的准确率</li>
<li>methods: 使用 post-training quantization (PTQ) 技术，并提出了两种新技术： activation-quantization-aware scaling (AQAS) 和 sequence-length-aware calibration (SLAC)，以及一种 hybrid data format combining integer and denormal representations (dINT) 来解决 underflow 问题</li>
<li>results: 对 OPT 和 LLaMA 两种 LLVM 进行了严格的评估，并显示了与标准精度模型相当的任务准确率，同时通过开发兼容 dINT 的数学单元，实现了硬件效率的2倍提高<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency -- a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths to target tasks. Moreover, we introduce dINT, a hybrid data format combining integer and denormal representations, to address the underflow issue in W4A8 quantization, where small values are rounded to zero. Through rigorous evaluations of LLMs, including OPT and LLaMA, we demonstrate that our techniques significantly boost task accuracies to levels comparable with full-precision models. By developing arithmetic units compatible with dINT, we further confirm that our methods yield a 2$\times$ hardware efficiency improvement compared to 8-bit integer MAC unit.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在自然语言处理任务中表现出色，但是它们的部署受到广泛的参数大小和计算需求的限制。这篇论文关注 LLM 中的后期量化（PTQ），特别是4位权值和8位活动（W4A8）量化，以提高计算效率——一个相对较少研究的领域。我们提出了两种创新技术：活动量化相关缩放（AQAS）和目标任务相关准备（SLAC），以增强PTQ，并考虑权值和活动的共同效果。此外，我们介绍了 dINT 混合数据格式，用于解决 W4A8 量化中的下流问题，其中小值会被舍入到零。通过对 LLM 进行严格的评估，包括 OPT 和 LLaMA，我们示出了我们的技术可以提高任务准确率至与权重精度模型相当的水平。再者，我们开发了与 dINT 相容的数学单元，以证明我们的方法可以在硬件效率方面实现2倍的提升。
</details></li>
</ul>
<hr>
<h2 id="Quranic-Conversations-Developing-a-Semantic-Search-tool-for-the-Quran-using-Arabic-NLP-Techniques"><a href="#Quranic-Conversations-Developing-a-Semantic-Search-tool-for-the-Quran-using-Arabic-NLP-Techniques" class="headerlink" title="Quranic Conversations: Developing a Semantic Search tool for the Quran using Arabic NLP Techniques"></a>Quranic Conversations: Developing a Semantic Search tool for the Quran using Arabic NLP Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05120">http://arxiv.org/abs/2311.05120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasser Shohoud, Maged Shoman, Sarah Abdelazim</li>
<li>for: 该论文目的是开发一种基于语义搜索的古兰经搜索工具，以便 Muslims 更容易找到关注的 ayahs。</li>
<li>methods: 该工具使用了多个模型，包括 SNxLM 模型，并使用 cosine similarity 来测试每个模型的相似性。</li>
<li>results: 使用该工具，可以寻找与用户的查询或提示相关的 ayahs，并达到 cosine similarity score 为 0.97，相当于阿布达讲解。<details>
<summary>Abstract</summary>
The Holy Book of Quran is believed to be the literal word of God (Allah) as revealed to the Prophet Muhammad (PBUH) over a period of approximately 23 years. It is the book where God provides guidance on how to live a righteous and just life, emphasizing principles like honesty, compassion, charity and justice, as well as providing rules for personal conduct, family matters, business ethics and much more. However, due to constraints related to the language and the Quran organization, it is challenging for Muslims to get all relevant ayahs (verses) pertaining to a matter or inquiry of interest. Hence, we developed a Quran semantic search tool which finds the verses pertaining to the user inquiry or prompt. To achieve this, we trained several models on a large dataset of over 30 tafsirs, where typically each tafsir corresponds to one verse in the Quran and, using cosine similarity, obtained the tafsir tensor which is most similar to the prompt tensor of interest, which was then used to index for the corresponding ayah in the Quran. Using the SNxLM model, we were able to achieve a cosine similarity score as high as 0.97 which corresponds to the abdu tafsir for a verse relating to financial matters.
</details>
<details>
<summary>摘要</summary>
《古兰经》被认为是神的 Literal 话语（Allah），由先知穆罕默德（PBUH）在约23年间预言。这是一本包含如何过一个正直和公正的生活的指南，强调诚实、慈悲、慈善和正义等原则，同时还提供了个人行为、家庭事务、商业伦理等方面的规则。然而由于语言和《古兰经》的组织方式，使得穆斯林找到相关的 Ayah（节）很困难。因此，我们开发了一个《古兰经》 semantically 搜索工具，可以找到用户的关注点或提示中相关的 Ayah。为了实现这一点，我们训练了多个模型，使用了一个大量的数据集，包括30多本《塔夫斯尔》，每本《塔夫斯尔》通常对应一节《古兰经》。我们使用cosine similarity来获得最相似的《塔夫斯尔》矩阵，然后使用该矩阵来索引相关的 Ayah 在《古兰经》中。使用 SNxLM 模型，我们能够达到cosine similarity score为0.97，对应的是《阿布达》《古兰经》中的一节关于财务问题。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Translation-Quality-Estimation-Exploiting-Synthetic-Data-and-Pre-trained-Multilingual-Encoder"><a href="#Unsupervised-Translation-Quality-Estimation-Exploiting-Synthetic-Data-and-Pre-trained-Multilingual-Encoder" class="headerlink" title="Unsupervised Translation Quality Estimation Exploiting Synthetic Data and Pre-trained Multilingual Encoder"></a>Unsupervised Translation Quality Estimation Exploiting Synthetic Data and Pre-trained Multilingual Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05117">http://arxiv.org/abs/2311.05117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuto Kuroda, Atsushi Fujita, Tomoyuki Kajiwara, Takashi Ninomiya</li>
<li>for: 这篇论文旨在研究无监督翻译质量估计（TQE）方法，以减少翻译质量估计所需的训练数据成本。</li>
<li>methods: 该论文使用 sintethic TQE 数据和预训练多语言encoder来研究无监督 sentence-level TQE 方法，这些方法在监督训练场景中已经证明有效。</li>
<li>results: 实验结果表明，该方法可以在高资源和低资源翻译方向中高度超越其他无监督 TQE 方法，并在某些零资源翻译方向中 Predicting post-editing effort。<details>
<summary>Abstract</summary>
Translation quality estimation (TQE) is the task of predicting translation quality without reference translations. Due to the enormous cost of creating training data for TQE, only a few translation directions can benefit from supervised training. To address this issue, unsupervised TQE methods have been studied. In this paper, we extensively investigate the usefulness of synthetic TQE data and pre-trained multilingual encoders in unsupervised sentence-level TQE, both of which have been proven effective in the supervised training scenarios. Our experiment on WMT20 and WMT21 datasets revealed that this approach can outperform other unsupervised TQE methods on high- and low-resource translation directions in predicting post-editing effort and human evaluation score, and some zero-resource translation directions in predicting post-editing effort.
</details>
<details>
<summary>摘要</summary>
翻译质量估计（TQE）是指无需参考翻译的翻译质量预测。由于创建TQE训练数据的成本巨大，只有一些翻译方向能够得到超级vised训练。为解决这个问题，无监督TQE方法已经研究过。本文对无监督句级TQE的有用性进行了广泛的调查，包括使用 sintheic TQE数据和预训练多语言编码器。我们的实验表明，这种方法可以在高资源和低资源翻译方向中预测后期编辑努力和人类评估分数，以及一些零资源翻译方向中预测后期编辑努力。
</details></li>
</ul>
<hr>
<h2 id="Conic10K-A-Challenging-Math-Problem-Understanding-and-Reasoning-Dataset"><a href="#Conic10K-A-Challenging-Math-Problem-Understanding-and-Reasoning-Dataset" class="headerlink" title="Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset"></a>Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05113">http://arxiv.org/abs/2311.05113</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whynlp/conic10k">https://github.com/whynlp/conic10k</a></li>
<li>paper_authors: Haoyi Wu, Wenyang Hui, Yezeng Chen, Weiqi Wu, Kewei Tu, Yi Zhou</li>
<li>for: 这个论文的目的是为了提供一个有挑战性的数学问题集，以测试人工智能的数学理解和逻辑能力。</li>
<li>methods: 该论文使用了中国高中教育中的圆形sections知识作为数学问题集的基础，并提供了不同的逻辑深度的问题，只需要知道圆形sections即可解决。</li>
<li>results: 实验显示现有的大语言模型，包括GPT-4，在复杂的逻辑任务上表现不佳。我们希望这些结果能够激发更多人开发更高级的自然语言理解和逻辑技术。该数据集和代码可以在GitHub上下载。<details>
<summary>Abstract</summary>
Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI's behaviour with reference to different problems within a specific topic in detail. In this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education. Our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. Since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. For each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. We hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. Our dataset and codes are available at https://github.com/whyNLP/Conic10K.
</details>
<details>
<summary>摘要</summary>
matematik zhishixiang yu jianyan shi yi zhong de zhongdian zhengfu yu jianyan shi zhong de ai (AI) de yi zhong de zhengfu. however, existing benchmarks either require only a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI's behaviour with reference to different problems within a specific topic in detail. in this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education. our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. for each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. we hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. our dataset and codes are available at https://github.com/whyNLP/Conic10K.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/cs.CL_2023_11_09/" data-id="clot2mhbi00ebx788geyudwz2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/09/cs.AI_2023_11_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-11-09
        
      </div>
    </a>
  
  
    <a href="/2023/11/09/cs.LG_2023_11_09/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-11-09</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">61</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">70</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">65</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
