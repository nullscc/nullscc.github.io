
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-11-09 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Efficient Parallelization Layouts for Large-Scale Distributed Model Training paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05610 repo_url: https:&#x2F;&#x2F;github.com&#x2F;aleph-alpha&#x2F;neurips-want-submission-efficient-paral">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-11-09">
<meta property="og:url" content="https://nullscc.github.io/2023/11/09/cs.LG_2023_11_09/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Efficient Parallelization Layouts for Large-Scale Distributed Model Training paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.05610 repo_url: https:&#x2F;&#x2F;github.com&#x2F;aleph-alpha&#x2F;neurips-want-submission-efficient-paral">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-09T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-10T09:25:48.067Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_11_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/09/cs.LG_2023_11_09/" class="article-date">
  <time datetime="2023-11-09T10:00:00.000Z" itemprop="datePublished">2023-11-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-11-09
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Parallelization-Layouts-for-Large-Scale-Distributed-Model-Training"><a href="#Efficient-Parallelization-Layouts-for-Large-Scale-Distributed-Model-Training" class="headerlink" title="Efficient Parallelization Layouts for Large-Scale Distributed Model Training"></a>Efficient Parallelization Layouts for Large-Scale Distributed Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05610">http://arxiv.org/abs/2311.05610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aleph-alpha/neurips-want-submission-efficient-parallelization-layouts">https://github.com/aleph-alpha/neurips-want-submission-efficient-parallelization-layouts</a></li>
<li>paper_authors: Johannes Hagemann, Samuel Weinbach, Konstantin Dobler, Maximilian Schall, Gerard de Melo</li>
<li>For: This paper is written for training large language models efficiently, with a focus on optimizing the use of hardware accelerators and minimizing memory usage.* Methods: The paper uses a combination of compute and memory optimizations, including FlashAttention and sequence parallelism, and conducts an ablation study to determine the most effective training configurations.* Results: The paper achieves state-of-the-art training efficiency results, with a Model FLOPs utilization of 70.5% when training a 13B model, and provides several key recommendations for efficient training of large language models.In Simplified Chinese text, the three information points would be:</li>
<li>for: 这篇论文是为了有效地训练大型自然语言模型而写的，关注硬件加速器的并行和内存使用最优化。</li>
<li>methods: 这篇论文使用了一系列compute和内存优化技术，包括FlashAttention和序列并行，并进行了ablation研究以确定最有效的训练配置。</li>
<li>results: 这篇论文实现了状态静态训练效率结果， Model FLOPs 使用率达到70.5% when training a 13B model，并提供了一些关键的训练大型自然语言模型的建议。<details>
<summary>Abstract</summary>
Efficiently training large language models requires parallelizing across hundreds of hardware accelerators and invoking various compute and memory optimizations. When combined, many of these strategies have complex interactions regarding the final training efficiency. Prior work tackling this problem did not have access to the latest set of optimizations, such as FlashAttention or sequence parallelism. In this work, we conduct a comprehensive ablation study of possible training configurations for large language models. We distill this large study into several key recommendations for the most efficient training. For instance, we find that using a micro-batch size of 1 usually enables the most efficient training layouts. Larger micro-batch sizes necessitate activation checkpointing or higher degrees of model parallelism and also lead to larger pipeline bubbles. Our most efficient configurations enable us to achieve state-of-the-art training efficiency results over a range of model sizes, most notably a Model FLOPs utilization of 70.5% when training a 13B model.
</details>
<details>
<summary>摘要</summary>
培训大型自然语言模型需要并行运行数百个硬件加速器，并 invoke 多种compute和内存优化。当这些策略相互作用时，其最终培训效率会受到复杂的影响。先前的工作没有访问最新的优化技术，如 FlashAttention 或序列并行。在这种工作中，我们进行了大规模的ablation研究，探讨可能的培训配置。我们从这个大型研究中提取了一些关键的建议，以实现最高效的培训。例如，我们发现使用 micro-batch 大小为 1 通常可以实现最高效的培训布局。大于 micro-batch 大小的情况需要进行激活检查点或更高的模型并行和更大的管道弹性，并且会导致更大的管道弹性。我们最高效的配置可以实现一个 Model FLOPs 使用率达到 70.5%，当 trains a 13B 模型。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Generative-Multi-Fidelity-Learning-for-Physical-Simulation"><a href="#Diffusion-Generative-Multi-Fidelity-Learning-for-Physical-Simulation" class="headerlink" title="Diffusion-Generative Multi-Fidelity Learning for Physical Simulation"></a>Diffusion-Generative Multi-Fidelity Learning for Physical Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05606">http://arxiv.org/abs/2311.05606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Wang, Shibo Li, Shikai Fang, Shandian Zhe</li>
<li>for: 这篇论文主要关注于Physical simulation related applications中的多元适性学习，以避免在实际应用中重复运行numerical solver，并将多元适性例子用于训练，以大大减少资料收集成本。</li>
<li>methods: 我们采用了一种基于测度过程的Diffusion-Generative Multi-Fidelity（DGMF）学习方法，使用了随机测度方程（SDE）来实现解析生成。我们还提出了一个条件分布模型来控制解析生成的条件，使用了额外的输入（时间或空间变数）来有效地学习和预测多dimensional的解析阵列。</li>
<li>results: 我们的方法能够实现多元适性学习的统一和有效预测，并且在一些典型应用中显示出了优异的成果，这显示了我们的方法对于Physical simulation related applications的应用具有很大的应用前景。<details>
<summary>Abstract</summary>
Multi-fidelity surrogate learning is important for physical simulation related applications in that it avoids running numerical solvers from scratch, which is known to be costly, and it uses multi-fidelity examples for training and greatly reduces the cost of data collection. Despite the variety of existing methods, they all build a model to map the input parameters outright to the solution output. Inspired by the recent breakthrough in generative models, we take an alternative view and consider the solution output as generated from random noises. We develop a diffusion-generative multi-fidelity (DGMF) learning method based on stochastic differential equations (SDE), where the generation is a continuous denoising process. We propose a conditional score model to control the solution generation by the input parameters and the fidelity. By conditioning on additional inputs (temporal or spacial variables), our model can efficiently learn and predict multi-dimensional solution arrays. Our method naturally unifies discrete and continuous fidelity modeling. The advantage of our method in several typical applications shows a promising new direction for multi-fidelity learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sorting-Out-Quantum-Monte-Carlo"><a href="#Sorting-Out-Quantum-Monte-Carlo" class="headerlink" title="Sorting Out Quantum Monte Carlo"></a>Sorting Out Quantum Monte Carlo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05598">http://arxiv.org/abs/2311.05598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Richter-Powell, Luca Thiede, Alán Asparu-Guzik, David Duvenaud</li>
<li>for: 这个论文目的是为了提出一种可扩展到多个粒子系统的量子水平分子模拟方法，并且能够尊重粒子的 symmetries。</li>
<li>methods: 这个论文使用了一种新的排序层（sortlet）来实现对粒子的排序，这个层的时间复杂度为 $O(N \log N)$，相比于传统的 determinant 方法的 $O(N^3)$。</li>
<li>results:  NUMERICAL 结果表明，通过在 neural-network 后处理器上应用这种排序层，可以实现一种灵活的波函数参数化方法，能够达到化学精度水平，在第一行元素和小分子的基态下预测ground state。<details>
<summary>Abstract</summary>
Molecular modeling at the quantum level requires choosing a parameterization of the wavefunction that both respects the required particle symmetries, and is scalable to systems of many particles. For the simulation of fermions, valid parameterizations must be antisymmetric with respect to the exchange of particles. Typically, antisymmetry is enforced by leveraging the anti-symmetry of determinants with respect to the exchange of matrix rows, but this involves computing a full determinant each time the wavefunction is evaluated. Instead, we introduce a new antisymmetrization layer derived from sorting, the $\textit{sortlet}$, which scales as $O(N \log N)$ with regards to the number of particles -- in contrast to $O(N^3)$ for the determinant. We show numerically that applying this anti-symmeterization layer on top of an attention based neural-network backbone yields a flexible wavefunction parameterization capable of reaching chemical accuracy when approximating the ground state of first-row atoms and small molecules.
</details>
<details>
<summary>摘要</summary>
分子模型在量子层面上需要选择一种Parameterization的浓函数，该函数需要遵循必要的粒子对称性，同时可扩展到多个粒子系统。在蛋白质模拟中，有效的Parameterization必须是对粒子交换的反对称的。通常，对粒子交换的反对称是通过对矩阵行交换的反对称性来实现，但这会导致每次评估浓函数时都需要计算全矩阵。而我们现在引入了一种新的反对称层，称为sortlet，该层的时间复杂度为O(N log N)，与粒子数量相比，是对矩阵的计算时间复杂度（O(N^3））的多ples。我们通过数值方法表明，将sortlet层作为浓函数参数化的 neural network 背景下的折衔函数，可以达到化学精度地approximate初列元素和小分子的ground state。
</details></li>
</ul>
<hr>
<h2 id="A-Coefficient-Makes-SVRG-Effective"><a href="#A-Coefficient-Makes-SVRG-Effective" class="headerlink" title="A Coefficient Makes SVRG Effective"></a>A Coefficient Makes SVRG Effective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05589">http://arxiv.org/abs/2311.05589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidyyd/alpha-svrg">https://github.com/davidyyd/alpha-svrg</a></li>
<li>paper_authors: Yida Yin, Zhiqiu Xu, Zhiyuan Li, Trevor Darrell, Zhuang Liu</li>
<li>for: 这个论文是为了证明Stochastic Variance Reduced Gradient（SVRG）在深度学习中的有效性。</li>
<li>methods: 这篇论文使用了Johnson &amp; Zhang（2013）提出的SVRG优化方法，并通过调整变量减少项的强度来改进其效果。</li>
<li>results: 该论文的分析发现，对于深度网络，SVRG中的变量减少项的强度应该逐渐减小，而且可以通过一个线性减少 schedule来控制。该方法被称为α-SVRG。对于多种网络架构和图像分类 dataset，α-SVRG能够更好地优化 neural networks，并在训练loss中减少training loss。<details>
<summary>Abstract</summary>
Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang (2013), is a theoretically compelling optimization method. However, as Defazio & Bottou (2019) highlights, its effectiveness in deep learning is yet to be proven. In this work, we demonstrate the potential of SVRG in optimizing real-world neural networks. Our analysis finds that, for deeper networks, the strength of the variance reduction term in SVRG should be smaller and decrease as training progresses. Inspired by this, we introduce a multiplicative coefficient $\alpha$ to control the strength and adjust it through a linear decay schedule. We name our method $\alpha$-SVRG. Our results show $\alpha$-SVRG better optimizes neural networks, consistently reducing training loss compared to both baseline and the standard SVRG across various architectures and image classification datasets. We hope our findings encourage further exploration into variance reduction techniques in deep learning. Code is available at https://github.com/davidyyd/alpha-SVRG.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Bayesian-Methods-for-Media-Mix-Modelling-with-shape-and-funnel-effects"><a href="#Bayesian-Methods-for-Media-Mix-Modelling-with-shape-and-funnel-effects" class="headerlink" title="Bayesian Methods for Media Mix Modelling with shape and funnel effects"></a>Bayesian Methods for Media Mix Modelling with shape and funnel effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05587">http://arxiv.org/abs/2311.05587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Marin</li>
<li>for: 这项研究旨在探讨Maxwell-Boltzmann方程和Michaelis-Menten模型在广告推广中的应用 potential.</li>
<li>methods: 该研究提议将这些方程 incorporated into Hierarchical Bayesian模型来分析消费者行为。</li>
<li>results: 这些方程集 excell in accurately describing complex systems like social interactions and consumer-advertising interactions的随机动态。<details>
<summary>Abstract</summary>
In recent years, significant progress in generative AI has highlighted the important role of physics-inspired models that utilize advanced mathematical concepts based on fundamental physics principles to enhance artificial intelligence capabilities. Among these models, those based on diffusion equations have greatly improved image quality. This study aims to explore the potential uses of Maxwell-Boltzmann equation, which forms the basis of the kinetic theory of gases, and the Michaelis-Menten model in Marketing Mix Modelling (MMM) applications. We propose incorporating these equations into Hierarchical Bayesian models to analyse consumer behaviour in the context of advertising. These equation sets excel in accurately describing the random dynamics in complex systems like social interactions and consumer-advertising interactions.
</details>
<details>
<summary>摘要</summary>
近年来，生成AI的进步强调了基于物理原理的模型的重要性，以提高人工智能能力。这些模型中，基于卷积Equation的模型在图像质量方面做出了大量改进。本研究旨在探讨使用Maxwell-Boltzmann方程和Michaelis-Menten模型在市场混合模型（MMM）应用中的潜在用途。我们建议将这些方程集成到层次 bayesian模型中，以分析消费者行为在广告投放的 контексте。这些方程集在描述复杂系统中的随机动态方面具有卓越的表现。
</details></li>
</ul>
<hr>
<h2 id="Outlier-Robust-Wasserstein-DRO"><a href="#Outlier-Robust-Wasserstein-DRO" class="headerlink" title="Outlier-Robust Wasserstein DRO"></a>Outlier-Robust Wasserstein DRO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05573">http://arxiv.org/abs/2311.05573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sbnietert/outlier-robust-wdro">https://github.com/sbnietert/outlier-robust-wdro</a></li>
<li>paper_authors: Sloan Nietert, Ziv Goldfeld, Soroosh Shafiee</li>
<li>for: 该论文是为了解决数据驱动决策中受到不确定性的问题而设计的。</li>
<li>methods: 该论文使用了 Wasserstein DRO（WDRO）方法，WDRO 方法是一种可以在数据分布不确定性下进行决策的方法。</li>
<li>results: 该论文提出了一种robust WDRO框架，该框架可以快速计算并且可以抵御各种不同类型的干扰（包括水平干扰和总变分干扰）。<details>
<summary>Abstract</summary>
Distributionally robust optimization (DRO) is an effective approach for data-driven decision-making in the presence of uncertainty. Geometric uncertainty due to sampling or localized perturbations of data points is captured by Wasserstein DRO (WDRO), which seeks to learn a model that performs uniformly well over a Wasserstein ball centered around the observed data distribution. However, WDRO fails to account for non-geometric perturbations such as adversarial outliers, which can greatly distort the Wasserstein distance measurement and impede the learned model. We address this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (Wasserstein) perturbations and non-geometric (total variation (TV)) contamination that allows an $\varepsilon$-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks.
</details>
<details>
<summary>摘要</summary>
distributionally robust optimization (DRO) 是一种有效的方法 для决策数据驱动下的不确定性。水平不确定性，如抽样或本地扰动数据点，可以通过水stein DRO (WDRO) 捕捉，WDRO 目的是学习一个在水stein圈心 around observed data distribution 上 uniformly well 的模型。然而，WDRO 不能考虑非水平扰动，如反对攻击异常点，这些扰动可以很大地扭曲水stein距离测量，阻碍学习的模型。我们Fill this gap by proposing a novel outlier-robust WDRO framework for decision-making under both geometric (waterstein) perturbations and non-geometric (total variation (TV)) contamination that allows an $\varepsilon$-fraction of data to be arbitrarily corrupted. We design an uncertainty set using a certain robust Wasserstein ball that accounts for both perturbation types and derive minimax optimal excess risk bounds for this procedure that explicitly capture the Wasserstein and TV risks. We prove a strong duality result that enables tractable convex reformulations and efficient computation of our outlier-robust WDRO problem. When the loss function depends only on low-dimensional features of the data, we eliminate certain dimension dependencies from the risk bounds that are unavoidable in the general setting. Finally, we present experiments validating our theory on standard regression and classification tasks.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Neural-Network-Statistics-for-Low-Power-DNN-Inference"><a href="#Exploiting-Neural-Network-Statistics-for-Low-Power-DNN-Inference" class="headerlink" title="Exploiting Neural-Network Statistics for Low-Power DNN Inference"></a>Exploiting Neural-Network Statistics for Low-Power DNN Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05557">http://arxiv.org/abs/2311.05557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Bamberg, Ardalan Najafi, Alberto Garcia-Ortiz</li>
<li>for: 这个研究目的是为了提高边缘人工智能推断引擎的能效性，减少处理器和内存的能力浪费。</li>
<li>methods: 这个研究使用了无预量处理和统计分析方法来减少处理器和内存的能力浪费，并且不会对精度产生影响。</li>
<li>results: 这个研究获得了最多80%的处理器和内存能力浪费减少，并且可以降低compute block的能源消耗量最多39%，而且不会对精度产生影响。<details>
<summary>Abstract</summary>
Specialized compute blocks have been developed for efficient DNN execution. However, due to the vast amount of data and parameter movements, the interconnects and on-chip memories form another bottleneck, impairing power and performance. This work addresses this bottleneck by contributing a low-power technique for edge-AI inference engines that combines overhead-free coding with a statistical analysis of the data and parameters of neural networks. Our approach reduces the interconnect and memory power consumption by up to 80% for state-of-the-art benchmarks while providing additional power savings for the compute blocks by up to 39%. These power improvements are achieved with no loss of accuracy and negligible hardware cost.
</details>
<details>
<summary>摘要</summary>
专用计算块已经为深度学习模型的高效执行而开发。然而，由于巨量数据和参数的移动，总线和镜像内存又成为了另一个瓶颈，影响了能效性。这项工作通过将数据和参数统计分析，并使用无开销编程技术，解决了这个瓶颈，从而实现了减少总线和内存的功率消耗，最高可达80%。此外，我们的方法还可以为计算块提供更多的功率改善，最高可达39%。这些功率改善不会导致精度损失和硬件成本的增加。
</details></li>
</ul>
<hr>
<h2 id="Information-theoretic-generalization-bounds-for-learning-from-quantum-data"><a href="#Information-theoretic-generalization-bounds-for-learning-from-quantum-data" class="headerlink" title="Information-theoretic generalization bounds for learning from quantum data"></a>Information-theoretic generalization bounds for learning from quantum data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05529">http://arxiv.org/abs/2311.05529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Caro, Tom Gur, Cambyse Rouzé, Daniel Stilck França, Sathyawageeswar Subramanian</li>
<li>for: 量子学习理论的普遍性和综合性</li>
<li>methods: 使用量子优化运输和量子凝固不等式来建立非凡量子抽象凝固，以获得量子学习场景中的普遍性和综合性</li>
<li>results: 提出一种通用的量子学习框架，可以描述量子学习器在训练量子-классической数据后，如何在新数据上进行推断，并且提供了一系列基于量子信息论和量子概率论的普遍性和综合性 bounds。<details>
<summary>Abstract</summary>
Learning tasks play an increasingly prominent role in quantum information and computation. They range from fundamental problems such as state discrimination and metrology over the framework of quantum probably approximately correct (PAC) learning, to the recently proposed shadow variants of state tomography. However, the many directions of quantum learning theory have so far evolved separately. We propose a general mathematical formalism for describing quantum learning by training on classical-quantum data and then testing how well the learned hypothesis generalizes to new data. In this framework, we prove bounds on the expected generalization error of a quantum learner in terms of classical and quantum information-theoretic quantities measuring how strongly the learner's hypothesis depends on the specific data seen during training.   To achieve this, we use tools from quantum optimal transport and quantum concentration inequalities to establish non-commutative versions of decoupling lemmas that underlie recent information-theoretic generalization bounds for classical machine learning.   Our framework encompasses and gives intuitively accessible generalization bounds for a variety of quantum learning scenarios such as quantum state discrimination, PAC learning quantum states, quantum parameter estimation, and quantumly PAC learning classical functions. Thereby, our work lays a foundation for a unifying quantum information-theoretic perspective on quantum learning.
</details>
<details>
<summary>摘要</summary>
学习任务在量子信息和计算中发挥越来越重要的作用。它们包括从基本问题如状态识别和测量到量子可能错误学习（PAC）学习框架，以及最近提出的阴影变体状态测量。然而，量子学习理论的多种方向ntil now已经发展独立。我们提出一种通用的数学ormalism для描述量子学习，通过在类型-量子数据上训练量子学习模型，然后测试该模型是否能够在新数据上适应。在这种框架下，我们证明在类型-量子信息学术和量子集中度量的基础上，预期量子学习器的总体适应误差的下界。为 достичь这个目标，我们使用量子优化运输和量子集中度量的工具来建立量子不 commutative 版本的解 coupling 证明，这些证明在类型-量子信息学术和量子集中度量的基础上，提供了类型-量子信息学术的总体适应误差 bounds。我们的框架包括并为各种量子学习场景提供直观可 accessible的总体适应误差 bounds，如量子状态识别、PAC学习量子状态、量子参数估计和量子PAC学习类型函数。因此，我们的工作为量子信息学- computation 领域提供了一个统一的视角， laying a foundation for further research in this area.
</details></li>
</ul>
<hr>
<h2 id="Dirichlet-Active-Learning"><a href="#Dirichlet-Active-Learning" class="headerlink" title="Dirichlet Active Learning"></a>Dirichlet Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05501">http://arxiv.org/abs/2311.05501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Miller, Ryan Murray</li>
<li>for: 这篇论文是为了提出 Dirichlet Active Learning（DiAL），一种基于 bayesian 的活动学习算法设计方法。</li>
<li>methods: 这种方法使用 Dirichlet 随机场模型，模型特征受到相似性的观察力，以便在学习任务中进行准确的分类和活动学习。</li>
<li>results: 该方法在具有低标签率的图像学习中进行了成功应用，并与当前最佳实践相比赢得了竞争力。此外，该方法还提供了一系列正式保证，确保其能够同时实现探索和利用。<details>
<summary>Abstract</summary>
This work introduces Dirichlet Active Learning (DiAL), a Bayesian-inspired approach to the design of active learning algorithms. Our framework models feature-conditional class probabilities as a Dirichlet random field and lends observational strength between similar features in order to calibrate the random field. This random field can then be utilized in learning tasks: in particular, we can use current estimates of mean and variance to conduct classification and active learning in the context where labeled data is scarce. We demonstrate the applicability of this model to low-label rate graph learning by constructing ``propagation operators'' based upon the graph Laplacian, and offer computational studies demonstrating the method's competitiveness with the state of the art. Finally, we provide rigorous guarantees regarding the ability of this approach to ensure both exploration and exploitation, expressed respectively in terms of cluster exploration and increased attention to decision boundaries.
</details>
<details>
<summary>摘要</summary>
（这个研究引入了 Dirichlet Active Learning（DiAL），一种基于 Bayesian 的活动学习算法设计方法。我们的框架将特征决策概率模型为 Dirichlet 随机场，并将相似特征之间的观测力共享，以便准确地调整随机场。这个随机场可以在学习任务中使用，特别是在标注数据稀缺的情况下进行分类和活动学习。我们通过基于图 Laplacian 的“传播算子”来实现这种模型，并提供了 computation 研究，证明这种方法与当前的状态艺术水平竞争。最后，我们提供了一系列具有充分的保证，表明这种方法能够保证 both exploration 和 exploitation，即尝试新的cluster和增强决策边界的注意力。）
</details></li>
</ul>
<hr>
<h2 id="Disease-Gene-Prioritization-With-Quantum-Walks"><a href="#Disease-Gene-Prioritization-With-Quantum-Walks" class="headerlink" title="Disease Gene Prioritization With Quantum Walks"></a>Disease Gene Prioritization With Quantum Walks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05486">http://arxiv.org/abs/2311.05486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harto Saarinen, Mark Goldsmith, Rui-Sheng Wang, Joseph Loscalzo, Sabrina Maniscalco</li>
<li>For: 本研究开发了一种基于连续时间量子漫步的疾病基因优先级化算法，用于预测疾病基因。* Methods: 该算法使用蛋白质-蛋白质交互网络中的邻居矩阵，并可以编码种子节点自回路到下面的哈密顿函数中，提高性能。* Results: 对三种疾病集合和七个蛋白质-蛋白质交互网络进行比较，该算法表现出较高的预测疾病基因性能，并进行了拥抱分析和自Loop的影响研究。<details>
<summary>Abstract</summary>
Disease gene prioritization assigns scores to genes or proteins according to their likely relevance for a given disease based on a provided set of seed genes. Here, we describe a new algorithm for disease gene prioritization based on continuous-time quantum walks using the adjacency matrix of a protein-protein interaction (PPI) network. Our algorithm can be seen as a quantum version of a previous method known as the diffusion kernel, but, importantly, has higher performance in predicting disease genes, and also permits the encoding of seed node self-loops into the underlying Hamiltonian, which offers yet another boost in performance. We demonstrate the success of our proposed method by comparing it to several well-known gene prioritization methods on three disease sets, across seven different PPI networks. In order to compare these methods, we use cross-validation and examine the mean reciprocal ranks and recall values. We further validate our method by performing an enrichment analysis of the predicted genes for coronary artery disease. We also investigate the impact of adding self-loops to the seeds, and argue that they allow the quantum walker to remain more local to low-degree seed nodes.
</details>
<details>
<summary>摘要</summary>
疾病基因优先顺序 assigns 分数到基因或蛋白质根据它们可能对某种疾病的可能性，基于提供的种子基因。在这里，我们描述了一种新的疾病基因优先顺序算法，基于连续时间量子游走使用蛋白质-蛋白质互作（PPI）网络的相互作用矩阵。我们的算法可以看作量子版的既有方法，即扩散核，但具有更高的疾病基因预测性能，同时允许种子节点自闭Loop编码到下面的哈密顿中，又提供了另一个提升性能的优势。我们通过对多个疾病集和七个不同的PPI网络进行跨验证，并通过评估平均反转排名和回归值来比较这些方法。我们还进行了预测基因扩充分析，以验证我们的方法的有效性。此外，我们还研究了将自闭Loop添加到种子中的影响，并论证它们使得量子游走者更加偏爱低度种子节点。
</details></li>
</ul>
<hr>
<h2 id="Do-Ensembling-and-Meta-Learning-Improve-Outlier-Detection-in-Randomized-Controlled-Trials"><a href="#Do-Ensembling-and-Meta-Learning-Improve-Outlier-Detection-in-Randomized-Controlled-Trials" class="headerlink" title="Do Ensembling and Meta-Learning Improve Outlier Detection in Randomized Controlled Trials?"></a>Do Ensembling and Meta-Learning Improve Outlier Detection in Randomized Controlled Trials?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05473">http://arxiv.org/abs/2311.05473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hamilton-health-sciences/ml4h-traq">https://github.com/hamilton-health-sciences/ml4h-traq</a></li>
<li>paper_authors: Walter Nelson, Jonathan Ranisau, Jeremy Petch</li>
<li>for: 这些论文主要目标是为了评估现代多中心随机化控制试验中收集的大量表格数据中的异常点。</li>
<li>methods: 这篇论文使用了6种现代机器学习基于异常检测算法对738个实际数据集和77,001名患者从44个国家的7个真实多中心随机控制试验中的数据进行了实验评估。</li>
<li>results: 这篇论文的结果证明了先前的研究中的一些结论，即现有算法可以无监督地识别数据异常，至少有一种算法在70.6%的时间内表现出正确的结果。然而，数据集之间的性能差异非常大，无一个算法在所有数据集中表现一致，因此需要新的方法来选择无监督模型或其他方式将可能冲突的预测结果集成。这篇论文提出了元学习概率ensemble（MePE）算法，可以将多个无监督模型的预测结果集成，并证明了它在 Meta-learning 方法中表现良好。然而，小集合在 average 上表现更好，这可能是一个负面结果，可能导向当前异常检测方法的应用。<details>
<summary>Abstract</summary>
Modern multi-centre randomized controlled trials (MCRCTs) collect massive amounts of tabular data, and are monitored intensively for irregularities by humans. We began by empirically evaluating 6 modern machine learning-based outlier detection algorithms on the task of identifying irregular data in 838 datasets from 7 real-world MCRCTs with a total of 77,001 patients from over 44 countries. Our results reinforce key findings from prior work in the outlier detection literature on data from other domains. Existing algorithms often succeed at identifying irregularities without any supervision, with at least one algorithm exhibiting positive performance 70.6% of the time. However, performance across datasets varies substantially with no single algorithm performing consistently well, motivating new techniques for unsupervised model selection or other means of aggregating potentially discordant predictions from multiple candidate models. We propose the Meta-learned Probabilistic Ensemble (MePE), a simple algorithm for aggregating the predictions of multiple unsupervised models, and show that it performs favourably compared to recent meta-learning approaches for outlier detection model selection. While meta-learning shows promise, small ensembles outperform all forms of meta-learning on average, a negative result that may guide the application of current outlier detection approaches in healthcare and other real-world domains.
</details>
<details>
<summary>摘要</summary>
现代多中心随机控制试验 (MCRCTs) 收集了大量的表格数据，并且被人类紧张监测以寻找异常。我们开始由 empirically 评估 6 种现代机器学习基于算法在异常检测任务中表现的可行性。我们的结果证实了先前Literature中关于数据 других 领域的发现，现有算法可以无监控下检测异常，至少有一个算法在 70.6% 的时间表现出正确的结果。然而，数据集之间的表现差异很大，没有单一的算法能够在所有数据集中表现良好，因此推动了新的无监控模型选择技术或其他方法来聚合可能存在冲突的预测。我们提出了 Meta-learned Probabilistic Ensemble (MePE)，一种简单的算法来聚合多个无监控模型的预测，并证明它与当前的出生学习方法相比表现良好。虽然 meta-learning 有推动之处，但小集合在 average 上表现较佳，这可能导向当前异常检测方法在医疗和其他实际领域的应用中的限制。
</details></li>
</ul>
<hr>
<h2 id="A-Practical-Approach-to-Novel-Class-Discovery-in-Tabular-Data"><a href="#A-Practical-Approach-to-Novel-Class-Discovery-in-Tabular-Data" class="headerlink" title="A Practical Approach to Novel Class Discovery in Tabular Data"></a>A Practical Approach to Novel Class Discovery in Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05440">http://arxiv.org/abs/2311.05440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colin Troisemaine, Alexandre Reiffers-Masson, Stéphane Gosselin, Vincent Lemaire, Sandrine Vaton</li>
<li>for: solving the novel class discovery (NCD) problem in tabular data without prior knowledge of the novel classes.</li>
<li>methods: proposing a simple deep NCD model with only essential elements, adapting $k$-fold cross-validation with hidden classes, and leveraging knowledge of known classes with unsupervised clustering algorithms ($k$-means and Spectral Clustering).</li>
<li>results: impressive performance under realistic conditions, reliable estimation of the number of novel classes, and effective solution to the NCD problem without relying on knowledge from the novel classes.<details>
<summary>Abstract</summary>
The problem of Novel Class Discovery (NCD) consists in extracting knowledge from a labeled set of known classes to accurately partition an unlabeled set of novel classes. While NCD has recently received a lot of attention from the community, it is often solved on computer vision problems and under unrealistic conditions. In particular, the number of novel classes is usually assumed to be known in advance, and their labels are sometimes used to tune hyperparameters. Methods that rely on these assumptions are not applicable in real-world scenarios. In this work, we focus on solving NCD in tabular data when no prior knowledge of the novel classes is available. To this end, we propose to tune the hyperparameters of NCD methods by adapting the $k$-fold cross-validation process and hiding some of the known classes in each fold. Since we have found that methods with too many hyperparameters are likely to overfit these hidden classes, we define a simple deep NCD model. This method is composed of only the essential elements necessary for the NCD problem and performs impressively well under realistic conditions. Furthermore, we find that the latent space of this method can be used to reliably estimate the number of novel classes. Additionally, we adapt two unsupervised clustering algorithms ($k$-means and Spectral Clustering) to leverage the knowledge of the known classes. Extensive experiments are conducted on 7 tabular datasets and demonstrate the effectiveness of the proposed method and hyperparameter tuning process, and show that the NCD problem can be solved without relying on knowledge from the novel classes.
</details>
<details>
<summary>摘要</summary>
文本问题（Novel Class Discovery，NCD）的问题是从已知类别集中提取知识，以便将未知类别集分割为精确的类别。尽管NCD在计算机视觉问题上已经受到了社区的一些关注，但是它们通常是在不实际的假设下解决的，例如，未知类别的数量通常是先知的，而且有时候用于调整超参数。这些假设不适用于实际场景。在这种情况下，我们将关注解决NCD在表格数据上的问题，而无需先知未知类别的数量。为此，我们提议通过适应$k$-fold横排验证过程和隐藏已知类别来调整NCD方法的超参数。由于我们发现了过多的超参数可能会过拟合这些隐藏的类别，所以我们定义了一个简单的深度NCD模型。这种方法由只有NCD问题所需的基本元素组成，并在实际条件下表现出色。此外，我们发现了这种方法的隐藏空间可以用于可靠地估计未知类别的数量。此外，我们将$k$-means和特征分布 clustering两种无监督归类算法调整，以利用已知类别的知识。我们对7个表格数据集进行了广泛的实验，并证明了我们提出的方法和超参数调整过程的效果，以及NCD问题可以无需已知未知类别的知识解决。
</details></li>
</ul>
<hr>
<h2 id="Fair-Wasserstein-Coresets"><a href="#Fair-Wasserstein-Coresets" class="headerlink" title="Fair Wasserstein Coresets"></a>Fair Wasserstein Coresets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05436">http://arxiv.org/abs/2311.05436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikai Xiong, Niccolò Dalmasso, Vamsi K. Potluru, Tucker Balch, Manuela Veloso</li>
<li>For: The paper aims to generate fair synthetic representative samples for downstream learning tasks, addressing biases in the data concerning subgroups defined by factors like race, gender, or other sensitive attributes.* Methods: The paper proposes Fair Wasserstein Coresets (FWC), a novel coreset approach that generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC minimizes the Wasserstein distance between the original datasets and the weighted synthetic samples while enforcing demographic parity, a prominent criterion for algorithmic fairness.* Results: The paper shows that FWC can be thought of as a constrained version of Lloyd’s algorithm for k-medians or k-means clustering, and demonstrates the scalability of the approach through experiments conducted on both synthetic and real datasets. The results highlight the competitive performance of FWC compared to existing fair clustering approaches, even when attempting to enhance the fairness of the latter through fair pre-processing techniques.Here is the same information in Simplified Chinese text:* For: 本研究旨在生成具有宽泛代表性的公平synthetic representative samples，以便在下游学习任务中应用。* Methods: 本研究提出了公平 Wasserstein coresets（FWC），一种新的 coreset方法，该方法生成了公平synthetic representative samples并将其权重为用于下游学习任务。 FWC寻求将原始数据集和重量化synthetic samples之间的普通 Wasserstein distance降为最低，同时保证demographic parity，一种公平性指标。* Results: 本研究表明，FWC可以视为Lloyd的算法 дляk-medians或k-means clustering的受限版本。我们的实验表明，FWC可扩展到大规模数据集，并且在比较公平的情况下表现更加竞争力。<details>
<summary>Abstract</summary>
Recent technological advancements have given rise to the ability of collecting vast amounts of data, that often exceed the capacity of commonly used machine learning algorithms. Approaches such as coresets and synthetic data distillation have emerged as frameworks to generate a smaller, yet representative, set of samples for downstream training. As machine learning is increasingly applied to decision-making processes, it becomes imperative for modelers to consider and address biases in the data concerning subgroups defined by factors like race, gender, or other sensitive attributes. Current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples. These methods, however, are not guaranteed to positively affect the performance or fairness of downstream learning processes. In this work, we present Fair Wasserstein Coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC aims to minimize the Wasserstein distance between the original datasets and the weighted synthetic samples while enforcing (an empirical version of) demographic parity, a prominent criterion for algorithmic fairness, via a linear constraint. We show that FWC can be thought of as a constrained version of Lloyd's algorithm for k-medians or k-means clustering. Our experiments, conducted on both synthetic and real datasets, demonstrate the scalability of our approach and highlight the competitive performance of FWC compared to existing fair clustering approaches, even when attempting to enhance the fairness of the latter through fair pre-processing techniques.
</details>
<details>
<summary>摘要</summary>
Current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples. However, these methods are not guaranteed to positively affect the performance or fairness of downstream learning processes. In this work, we present Fair Wasserstein Coresets (FWC), a novel coreset approach that generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks.FWC aims to minimize the Wasserstein distance between the original datasets and the weighted synthetic samples while enforcing (an empirical version of) demographic parity, a prominent criterion for algorithmic fairness, via a linear constraint. We show that FWC can be thought of as a constrained version of Lloyd's algorithm for k-medians or k-means clustering. Our experiments, conducted on both synthetic and real datasets, demonstrate the scalability of our approach and highlight the competitive performance of FWC compared to existing fair clustering approaches, even when attempting to enhance the fairness of the latter through fair pre-processing techniques.
</details></li>
</ul>
<hr>
<h2 id="Parkinson’s-Disease-Detection-through-Vocal-Biomarkers-and-Advanced-Machine-Learning-Algorithms-A-Comprehensive-Study"><a href="#Parkinson’s-Disease-Detection-through-Vocal-Biomarkers-and-Advanced-Machine-Learning-Algorithms-A-Comprehensive-Study" class="headerlink" title="Parkinson’s Disease Detection through Vocal Biomarkers and Advanced Machine Learning Algorithms: A Comprehensive Study"></a>Parkinson’s Disease Detection through Vocal Biomarkers and Advanced Machine Learning Algorithms: A Comprehensive Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05435">http://arxiv.org/abs/2311.05435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abu Sayed, Sabbir Ahamed, Duc M Cao, Md Eyasin Ul Islam Pavel, Malay Sarkar, Md Tuhin Mia<br>for: 预测parkinson病的发病概率methods: 使用多种先进机器学习算法，包括XGBoost、LightGBM、Bagging、AdaBoost和支持向量机等，评估这些模型的预测性能，并计算准确率、曲线面积、敏感度和特异度等指标。results: 研究发现LightGBM模型表现最佳，其准确率达96%，圆曲线面积也是96%，敏感度为100%，特异度为94.43%，在准确率和曲线面积方面与其他机器学习算法相比较出色。<details>
<summary>Abstract</summary>
Parkinson's disease (PD) is a prevalent neurodegenerative disorder known for its impact on motor neurons, causing symptoms like tremors, stiffness, and gait difficulties. This study explores the potential of vocal feature alterations in PD patients as a means of early disease prediction. This research aims to predict the onset of Parkinson's disease. Utilizing a variety of advanced machine-learning algorithms, including XGBoost, LightGBM, Bagging, AdaBoost, and Support Vector Machine, among others, the study evaluates the predictive performance of these models using metrics such as accuracy, area under the curve (AUC), sensitivity, and specificity. The findings of this comprehensive analysis highlight LightGBM as the most effective model, achieving an impressive accuracy rate of 96%, alongside a matching AUC of 96%. LightGBM exhibited a remarkable sensitivity of 100% and specificity of 94.43%, surpassing other machine learning algorithms in accuracy and AUC scores. Given the complexities of Parkinson's disease and its challenges in early diagnosis, this study underscores the significance of leveraging vocal biomarkers coupled with advanced machine-learning techniques for precise and timely PD detection.
</details>
<details>
<summary>摘要</summary>
帕金森病 (PD) 是一种常见的神经退化疾病，知名于对运动神经元的影响，导致如颤抖、硬直和步态困难等症状。这项研究探讨了在PD患者中 voz feature 的变化作为早期疾病预测的可能性。这项研究的目的是预测帕金森病的病起。使用多种高级机器学习算法，包括 XGBoost、LightGBM、Bagging、AdaBoost 和 Support Vector Machine 等，这项研究评估了这些模型在精度和准确性方面的预测性能。研究发现 LightGBM 是最有效的模型，其精度率达 96%，并与报数曲线 (AUC) 均达 96%。LightGBM 表现出了惊人的敏感性（100%）和特异性（94.43%），在精度和报数曲线方面超过了其他机器学习算法。由于帕金森病的复杂性和早期诊断的挑战，这项研究highlights the significance of combining vocal biomarkers with advanced machine-learning techniques for precise and timely PD detection.
</details></li>
</ul>
<hr>
<h2 id="Taxonomy-for-Resident-Space-Objects-in-LEO-A-Deep-Learning-Approach"><a href="#Taxonomy-for-Resident-Space-Objects-in-LEO-A-Deep-Learning-Approach" class="headerlink" title="Taxonomy for Resident Space Objects in LEO: A Deep Learning Approach"></a>Taxonomy for Resident Space Objects in LEO: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05430">http://arxiv.org/abs/2311.05430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Guimarães, Cláudia Soares, Chiara Manfletti</li>
<li>For: 本研究提出了一个新的taxonomy来加强空间交通管理，以应对随着数量的增加的空间垃圾物体（RSO）的风险。* Methods: 本研究使用了深度学习模型，包括自适应网络和uniform manifold approximation and projection（UMAP），来分类RSOs并提取其特征。* Results: 本研究提出了一个新的taxonomy，并使用了深度学习模型来分类RSOs，并成功地捕捉了RSOs的复杂和非线性关系。这些成果可以帮助改善空间交通管理和降低风险。<details>
<summary>Abstract</summary>
The increasing number of RSOs has raised concerns about the risk of collisions and catastrophic incidents for all direct and indirect users of space. To mitigate this issue, it is essential to have a good understanding of the various RSOs in orbit and their behaviour. A well-established taxonomy defining several classes of RSOs is a critical step in achieving this understanding. This taxonomy helps assign objects to specific categories based on their main characteristics, leading to better tracking services. Furthermore, a well-established taxonomy can facilitate research and analysis processes by providing a common language and framework for better understanding the factors that influence RSO behaviour in space. These factors, in turn, help design more efficient and effective strategies for space traffic management. Our work proposes a new taxonomy for RSOs focusing on the low Earth orbit regime to enhance space traffic management. In addition, we present a deep learning-based model that uses an autoencoder architecture to reduce the features representing the characteristics of the RSOs. The autoencoder generates a lower-dimensional space representation that is then explored using techniques such as Uniform Manifold Approximation and Projection to identify fundamental clusters of RSOs based on their unique characteristics. This approach captures the complex and non-linear relationships between the features and the RSOs' classes identified. Our proposed taxonomy and model offer a significant contribution to the ongoing efforts to mitigate the overall risks posed by the increasing number of RSOs in orbit.
</details>
<details>
<summary>摘要</summary>
随着各种人造卫星的数量不断增加，这引发了对各种卫星的碰撞和灾难性事件的风险的担忧。为了解决这个问题，需要有一个良好的理解各种在轨道上运行的人造卫星的行为。一个well-established taxonomy可以将 объек们分类到特定类别基于其主要特征，从而实现更好的跟踪服务。此外，一个well-established taxonomy还可以促进研究和分析过程，提供一个共同语言和框架，以更好地理解各种卫星行为的因素，并设计更有效率的空间交通管理策略。我们的工作提出了一个专注于低地球轨道域的新分类法，并提出了一种基于自适应神经网络的模型，使用自适应网络架构减少表示卫星特征的特征。这种方法可以捕捉各种卫星特征之间的复杂和非线性关系，并基于这些特征分类出各种卫星的基本类别。我们的提议的分类法和模型对当前各种卫星的增加数量而言，具有积极的贡献。
</details></li>
</ul>
<hr>
<h2 id="Statistical-Learning-of-Conjunction-Data-Messages-Through-a-Bayesian-Non-Homogeneous-Poisson-Process"><a href="#Statistical-Learning-of-Conjunction-Data-Messages-Through-a-Bayesian-Non-Homogeneous-Poisson-Process" class="headerlink" title="Statistical Learning of Conjunction Data Messages Through a Bayesian Non-Homogeneous Poisson Process"></a>Statistical Learning of Conjunction Data Messages Through a Bayesian Non-Homogeneous Poisson Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05426">http://arxiv.org/abs/2311.05426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Guimarães, Cláudia Soares, Chiara Manfletti</li>
<li>for: 避免卫星碰撞和空间交通管理中的挑战，特别是随着卫星数量的不断增加和自动化解决方案的缺乏。</li>
<li>methods: 使用统计学学习模型，以解决卫星碰撞风险评估和避免碰撞机动的问题。</li>
<li>results: 提出了一种 Bayesian 非Homogeneous Poisson 过程，可以更好地描述卫星碰撞风险评估和避免碰撞机动的问题，并与基准模型进行比较，表明该方法可以更准确地解决这些问题。<details>
<summary>Abstract</summary>
Current approaches for collision avoidance and space traffic management face many challenges, mainly due to the continuous increase in the number of objects in orbit and the lack of scalable and automated solutions. To avoid catastrophic incidents, satellite owners/operators must be aware of their assets' collision risk to decide whether a collision avoidance manoeuvre needs to be performed. This process is typically executed through the use of warnings issued in the form of CDMs which contain information about the event, such as the expected TCA and the probability of collision. Our previous work presented a statistical learning model that allowed us to answer two important questions: (1) Will any new conjunctions be issued in the next specified time interval? (2) When and with what uncertainty will the next CDM arrive? However, the model was based on an empirical Bayes homogeneous Poisson process, which assumes that the arrival rates of CDMs are constant over time. In fact, the rate at which the CDMs are issued depends on the behaviour of the objects as well as on the screening process performed by third parties. Thus, in this work, we extend the previous study and propose a Bayesian non-homogeneous Poisson process implemented with high precision using a Probabilistic Programming Language to fully describe the underlying phenomena. We compare the proposed solution with a baseline model to demonstrate the added value of our approach. The results show that this problem can be successfully modelled by our Bayesian non-homogeneous Poisson Process with greater accuracy, contributing to the development of automated collision avoidance systems and helping operators react timely but sparingly with satellite manoeuvres.
</details>
<details>
<summary>摘要</summary>
当前的协调碰撞避免和空间交通管理技术面临着许多挑战，主要是由于遥感器量在轨道上的不断增加和缺乏可扩展和自动化的解决方案。为了避免慢速碰撞，卫星所有者/运营商必须了解自己的资产碰撞风险，以确定是否需要执行碰撞避免操作。这个过程通常通过使用CDM（Conjunction Data Message）中包含的信息进行执行，如预计的TCA（Time of Closest Approach）和碰撞的可能性。我们的前一项研究提出了一种统计学学习模型，可以回答以下两个重要问题：（1）将来有新的相对应吗？（2）预计下一个CDM会在什么时间到达，以及具有多少不确定性？然而，该模型基于empirical Bayes homogeneous Poisson process，即CDM的到达速率是时间不变的。事实上，CDM的发送速率取决于对象的行为以及第三方creening过程。因此，在这项工作中，我们延续前一项研究，并提出一种 Bayesian non-homogeneous Poisson process，使用高精度的可probabilistic Programming Language来完全描述下面现象。我们与基准模型进行比较，以展示我们的方法的added value。结果表明，我们的方法可以更高度准确地模型这个问题，为自动化碰撞避免系统的发展和操作人员在时间上作出合适的决策做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Based-Causal-Representation-Learning"><a href="#Diffusion-Based-Causal-Representation-Learning" class="headerlink" title="Diffusion Based Causal Representation Learning"></a>Diffusion Based Causal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05421">http://arxiv.org/abs/2311.05421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Mohammad Karimi Mamaghan, Andrea Dittadi, Stefan Bauer, Karl Henrik Johansson, Francesco Quinzan</li>
<li>for:  This paper is written for learning causal representations and discovering causal structures in complex systems.</li>
<li>methods:  The paper proposes a new Diffusion-based Causal Representation Learning (DCRL) algorithm, which uses diffusion-based representations for causal discovery and offers access to infinite dimensional latent codes.</li>
<li>results:  The paper demonstrates the effectiveness of DCRL in identifying the causal structure and causal variables, compared to previous methods such as Variational Auto-Encoders (VAE).<details>
<summary>Abstract</summary>
Causal reasoning can be considered a cornerstone of intelligent systems. Having access to an underlying causal graph comes with the promise of cause-effect estimation and the identification of efficient and safe interventions. However, learning causal representations remains a major challenge, due to the complexity of many real-world systems. Previous works on causal representation learning have mostly focused on Variational Auto-Encoders (VAE). These methods only provide representations from a point estimate, and they are unsuitable to handle high dimensions. To overcome these problems, we proposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm. This algorithm uses diffusion-based representations for causal discovery. DCRL offers access to infinite dimensional latent codes, which encode different levels of information in the latent code. In a first proof of principle, we investigate the use of DCRL for causal representation learning. We further demonstrate experimentally that this approach performs comparably well in identifying the causal structure and causal variables.
</details>
<details>
<summary>摘要</summary>
causal reasoning 可以被视为智能系统的基础之一。  possession of an underlying causal graph 提供了 cause-effect estimation 和 identification of efficient and safe interventions 的机会。 However, learning causal representations remains a major challenge, due to the complexity of many real-world systems. Previous works on causal representation learning have mostly focused on Variational Auto-Encoders (VAE). These methods only provide representations from a point estimate, and they are unsuitable to handle high dimensions. To overcome these problems, we proposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm. This algorithm uses diffusion-based representations for causal discovery. DCRL offers access to infinite dimensional latent codes, which encode different levels of information in the latent code. In a first proof of principle, we investigate the use of DCRL for causal representation learning. We further demonstrate experimentally that this approach performs comparably well in identifying the causal structure and causal variables.Here's the word-for-word translation of the text into Simplified Chinese: causal reasoning 可以被视为智能系统的基础之一。  possession of an underlying causal graph 提供了 cause-effect estimation 和 identification of efficient and safe interventions 的机会。 however, learning causal representations remains a major challenge, due to the complexity of many real-world systems. previous works on causal representation learning have mostly focused on Variational Auto-Encoders (VAE). these methods only provide representations from a point estimate, and they are unsuitable to handle high dimensions. to overcome these problems, we proposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm. this algorithm uses diffusion-based representations for causal discovery. DCRL offers access to infinite dimensional latent codes, which encode different levels of information in the latent code. in a first proof of principle, we investigate the use of DCRL for causal representation learning. we further demonstrate experimentally that this approach performs comparably well in identifying the causal structure and causal variables.
</details></li>
</ul>
<hr>
<h2 id="Counterfactually-Fair-Representation"><a href="#Counterfactually-Fair-Representation" class="headerlink" title="Counterfactually Fair Representation"></a>Counterfactually Fair Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05420">http://arxiv.org/abs/2311.05420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-srml/cf_representation_learning">https://github.com/osu-srml/cf_representation_learning</a></li>
<li>paper_authors: Zhiqun Zuo, Mohammad Mahdi Khalili, Xueru Zhang</li>
<li>for: 本研究旨在探讨机器学习模型在高风险应用（如医疗、贷款、大学推荐）中的可能存在的保护群体偏见问题，以及如何通过不同的公平性观念和方法来缓解这些偏见。</li>
<li>methods: 本研究使用了Counterfactual Fairness（CF）作为公平性观念，CF需要实际世界中个体所经历的结果与假设的counterfactual世界中个体所经历的结果相同。 learns CF models only using non-descendants of sensitive attributes while eliminating all descendants，这是一种可以满足CF的简单方法。  however, this work proposes a new algorithm that trains models using all the available features，这种方法可以 theoretically and empirically show that models trained with this method can satisfy CF。</li>
<li>results: 本研究通过 teorically and empirically show that models trained with the proposed method can satisfy CF，提供了一种可以在高风险应用中使用机器学习模型的公平性方法。<details>
<summary>Abstract</summary>
The use of machine learning models in high-stake applications (e.g., healthcare, lending, college admission) has raised growing concerns due to potential biases against protected social groups. Various fairness notions and methods have been proposed to mitigate such biases. In this work, we focus on Counterfactual Fairness (CF), a fairness notion that is dependent on an underlying causal graph and first proposed by Kusner \textit{et al.}~\cite{kusner2017counterfactual}; it requires that the outcome an individual perceives is the same in the real world as it would be in a "counterfactual" world, in which the individual belongs to another social group. Learning fair models satisfying CF can be challenging. It was shown in \cite{kusner2017counterfactual} that a sufficient condition for satisfying CF is to \textbf{not} use features that are descendants of sensitive attributes in the causal graph. This implies a simple method that learns CF models only using non-descendants of sensitive attributes while eliminating all descendants. Although several subsequent works proposed methods that use all features for training CF models, there is no theoretical guarantee that they can satisfy CF. In contrast, this work proposes a new algorithm that trains models using all the available features. We theoretically and empirically show that models trained with this method can satisfy CF\footnote{The code repository for this work can be found in \url{https://github.com/osu-srml/CF_Representation_Learning}.
</details>
<details>
<summary>摘要</summary>
高度的应用（如医疗、贷款、大学招生）中机器学习模型的使用已经引起了增长的关注，因为它们可能会对保护的社会群体产生偏见。不同的公平性概念和方法已经被提出来 mitigate这些偏见。在这项工作中，我们将关注Counterfactual Fairness（CF），这是一种受到下游隐藏 Variable 的依赖关系的公平性概念，由Kusner等人在 \cite{kusner2017counterfactual} 首次提出。它要求个体在真实世界中所感受到的结果与在一个“假设世界”中所感受到的结果一样。学习满足CF的公平模型可以是困难的。在 \cite{kusner2017counterfactual} 中显示了一种 suficient condition，即不使用敏感属性的后代feature。这意味着可以通过不使用敏感属性的后代feature来学习满足CF的模型。然而，后续的一些工作提出了使用所有特征来训练CF模型的方法，但没有理论保证它们可以满足CF。相反，这项工作提出了一种新的算法，该算法使用所有可用特征来训练模型。我们 theoretically和empirically表明，使用该算法可以满足CF。Note: The code repository for this work can be found in \url{https://github.com/osu-srml/CF_Representation_Learning}.
</details></li>
</ul>
<hr>
<h2 id="Predicting-the-Position-Uncertainty-at-the-Time-of-Closest-Approach-with-Diffusion-Models"><a href="#Predicting-the-Position-Uncertainty-at-the-Time-of-Closest-Approach-with-Diffusion-Models" class="headerlink" title="Predicting the Position Uncertainty at the Time of Closest Approach with Diffusion Models"></a>Predicting the Position Uncertainty at the Time of Closest Approach with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05417">http://arxiv.org/abs/2311.05417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Guimarães, Cláudia Soares, Chiara Manfletti</li>
<li>for: 避免航天器相撞，提高航天活动的安全性和效率。</li>
<li>methods: 使用机器学习模型，基于扩散模型来预测objects involved in close encounter的位置不确定性的进化。</li>
<li>results: 与其他状态艺术解决方案和na&quot;ive baseline方法相比，提议的解决方案具有提高航天器操作的安全性和效率的潜在优势。<details>
<summary>Abstract</summary>
The risk of collision between resident space objects has significantly increased in recent years. As a result, spacecraft collision avoidance procedures have become an essential part of satellite operations. To ensure safe and effective space activities, satellite owners and operators rely on constantly updated estimates of encounters. These estimates include the uncertainty associated with the position of each object at the expected TCA. These estimates are crucial in planning risk mitigation measures, such as collision avoidance manoeuvres. As the TCA approaches, the accuracy of these estimates improves, as both objects' orbit determination and propagation procedures are made for increasingly shorter time intervals. However, this improvement comes at the cost of taking place close to the critical decision moment. This means that safe avoidance manoeuvres might not be possible or could incur significant costs. Therefore, knowing the evolution of this variable in advance can be crucial for operators. This work proposes a machine learning model based on diffusion models to forecast the position uncertainty of objects involved in a close encounter, particularly for the secondary object (usually debris), which tends to be more unpredictable. We compare the performance of our model with other state-of-the-art solutions and a na\"ive baseline approach, showing that the proposed solution has the potential to significantly improve the safety and effectiveness of spacecraft operations.
</details>
<details>
<summary>摘要</summary>
随着近年航天空间物体的飞行轨道数量的增加，航天器碰撞避免程序已成为卫星运营中不可或缺的一部分。为确保安全有效的空间活动，卫星所有者和运营商依靠不断更新的遇对总体评估。这些评估包括对每个 объек的位置 uncertainty 的评估，这些评估是规划避免碰撞措施的关键。随着预计的 TCA 接近，对象的轨道决定和推算过程的精度得到改善，但这也意味着在决策时间点附近进行避免碰撞措施可能无法进行或者需要投入大量资源。因此，预测遇对变量的进化可以对操作人员带来很大的帮助。本文提出了基于扩散模型的机器学习模型，用于预测遇对变量的位置不确定性，特别是次要 объек（通常是废弃物）的位置不确定性。我们对本模型与其他当前状态的解决方案和na\"ive基线方法进行比较，显示了本模型在安全性和效果性方面的潜在提升。
</details></li>
</ul>
<hr>
<h2 id="Data-Distillation-for-Neural-Network-Potentials-toward-Foundational-Dataset"><a href="#Data-Distillation-for-Neural-Network-Potentials-toward-Foundational-Dataset" class="headerlink" title="Data Distillation for Neural Network Potentials toward Foundational Dataset"></a>Data Distillation for Neural Network Potentials toward Foundational Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05407">http://arxiv.org/abs/2311.05407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gang Seob Jung, Sangkeun Lee, Jong Youl Choi</li>
<li>for: 本研究旨在快速提高材料设计和发现的效率，通过生成模型提出有潜力的材料。</li>
<li>methods: 本研究使用机器学习技术和原子尺度模拟，并使用扩展ensemble分子动力学来获得广泛的液相和固相配置。然后通过活动学习减少数据，无需失去准确性。</li>
<li>results: 研究发现，通过使用数据浓缩法，可以从初始数据中提取出能量最小化的闭包晶体结构，而这些结构并未直接出现在初始数据中。此外，这些数据还可以翻译到其他金属系统（铝和锰），无需重复样本和浓缩过程。这种数据获取和浓缩方法可以加速NNP的发展，并提高材料设计和发现的效率。<details>
<summary>Abstract</summary>
Machine learning (ML) techniques and atomistic modeling have rapidly transformed materials design and discovery. Specifically, generative models can swiftly propose promising materials for targeted applications. However, the predicted properties of materials through the generative models often do not match with calculated properties through ab initio calculations. This discrepancy can arise because the generated coordinates are not fully relaxed, whereas the many properties are derived from relaxed structures. Neural network-based potentials (NNPs) can expedite the process by providing relaxed structures from the initially generated ones. Nevertheless, acquiring data to train NNPs for this purpose can be extremely challenging as it needs to encompass previously unknown structures. This study utilized extended ensemble molecular dynamics (MD) to secure a broad range of liquid- and solid-phase configurations in one of the metallic systems, nickel. Then, we could significantly reduce them through active learning without losing much accuracy. We found that the NNP trained from the distilled data could predict different energy-minimized closed-pack crystal structures even though those structures were not explicitly part of the initial data. Furthermore, the data can be translated to other metallic systems (aluminum and niobium), without repeating the sampling and distillation processes. Our approach to data acquisition and distillation has demonstrated the potential to expedite NNP development and enhance materials design and discovery by integrating generative models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Sample-Complexity-Of-ERMs-In-Stochastic-Convex-Optimization"><a href="#The-Sample-Complexity-Of-ERMs-In-Stochastic-Convex-Optimization" class="headerlink" title="The Sample Complexity Of ERMs In Stochastic Convex Optimization"></a>The Sample Complexity Of ERMs In Stochastic Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05398">http://arxiv.org/abs/2311.05398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Carmon, Roi Livni, Amir Yehudayoff</li>
<li>for: 这个论文的目的是解决在现代机器学习中最常研究的随机凸优化问题中的一个基本问题，即如何确定训练数据集中的数据点数，使得任何Empirical Risk Minimizer（ERM）在真实人口中表现良好？</li>
<li>methods: 这篇论文使用了随机凸优化的方法，并提出了一种新的分离结果，即$\tilde{O}(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$数据点数足够使ERM表现良好。</li>
<li>results: 这篇论文的结果表明，$\tilde{O}(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$数据点数是必要的和 suficient condition，以确保ERM在真实人口中表现良好。此外，论文还扩展了结果，并证明这种上限 bound 适用于所有对称凸体上的学习问题。<details>
<summary>Abstract</summary>
Stochastic convex optimization is one of the most well-studied models for learning in modern machine learning. Nevertheless, a central fundamental question in this setup remained unresolved: "How many data points must be observed so that any empirical risk minimizer (ERM) shows good performance on the true population?" This question was proposed by Feldman (2016), who proved that $\Omega(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ data points are necessary (where $d$ is the dimension and $\epsilon>0$ is the accuracy parameter). Proving an $\omega(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ lower bound was left as an open problem. In this work we show that in fact $\tilde{O}(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ data points are also sufficient. This settles the question and yields a new separation between ERMs and uniform convergence. This sample complexity holds for the classical setup of learning bounded convex Lipschitz functions over the Euclidean unit ball. We further generalize the result and show that a similar upper bound holds for all symmetric convex bodies. The general bound is composed of two terms: (i) a term of the form $\tilde{O}(\frac{d}{\epsilon})$ with an inverse-linear dependence on the accuracy parameter, and (ii) a term that depends on the statistical complexity of the class of $\textit{linear}$ functions (captured by the Rademacher complexity). The proof builds a mechanism for controlling the behavior of stochastic convex optimization problems.
</details>
<details>
<summary>摘要</summary>
Stochastic convex optimization 是现代机器学习中最受研究的模型之一。然而，一个中心问题在这种设置下仍然未得到解答：“如何确定训练数据点的数量，以便任何empirical risk minimizer（ERM）在真实人口中表现良好？”这个问题由Feldman（2016）提出，并证明了 $\Omega(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ 数据点是必要的（其中 $d$ 是维度， $\epsilon>0$ 是准确度参数）。但是证明 $\omega(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ 下界的问题仍然未得到解决。在这个工作中，我们证明了事实上，$\tilde{O}(\frac{d}{\epsilon}+\frac{1}{\epsilon^2})$ 数据点也是充分的。这个样本复杂度适用于经典的学习凸函数于欧几何Unit ball中的设置。我们进一步推广结论，并证明对所有对称凸体都成立的相似上界。总的来说，这个 bound 由两个项组成：（i）一个类似于 $\tilde{O}(\frac{d}{\epsilon})$ 的项，具有反 proportion 到准确度参数的倒整数关系；（ii）一个依赖于类型 linear 函数的统计复杂度（捕捉在 Rademacher 复杂度中）。证明基于控制随机凸优化问题的机制。
</details></li>
</ul>
<hr>
<h2 id="Beyond-the-training-set-an-intuitive-method-for-detecting-distribution-shift-in-model-based-optimization"><a href="#Beyond-the-training-set-an-intuitive-method-for-detecting-distribution-shift-in-model-based-optimization" class="headerlink" title="Beyond the training set: an intuitive method for detecting distribution shift in model-based optimization"></a>Beyond the training set: an intuitive method for detecting distribution shift in model-based optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05363">http://arxiv.org/abs/2311.05363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farhan Damani, David H Brookes, Theodore Sternlieb, Cameron Webster, Stephen Malina, Rishi Jajoo, Kathy Lin, Sam Sinai</li>
<li>for: 本文的目的是解决在科学和工程设计问题中广泛存在的分布转移问题，即使用模型优化时，模型的预测结果与设计样本的分布不同。</li>
<li>methods: 本文提出了一种简单的方法，通过在设计样本中搜索适合的区域，以避免分布转移对设计质量的影响。该方法通过训练一个二分类器，使其在知道设计样本分布时，将训练数据与设计数据分开。</li>
<li>results: 本文在实际应用中运行了离线模型优化，并评估了分布转移对设计质量的影响。结果表明，分布转移的严重程度与优化算法步数相关，并且该简单的方法可以识别这些转移。这使得用户可以将搜索局限在模型预测结果可靠的区域内，从而提高设计质量。<details>
<summary>Abstract</summary>
Model-based optimization (MBO) is increasingly applied to design problems in science and engineering. A common scenario involves using a fixed training set to train models, with the goal of designing new samples that outperform those present in the training data. A major challenge in this setting is distribution shift, where the distributions of training and design samples are different. While some shift is expected, as the goal is to create better designs, this change can negatively affect model accuracy and subsequently, design quality. Despite the widespread nature of this problem, addressing it demands deep domain knowledge and artful application. To tackle this issue, we propose a straightforward method for design practitioners that detects distribution shifts. This method trains a binary classifier using knowledge of the unlabeled design distribution to separate the training data from the design data. The classifier's logit scores are then used as a proxy measure of distribution shift. We validate our method in a real-world application by running offline MBO and evaluate the effect of distribution shift on design quality. We find that the intensity of the shift in the design distribution varies based on the number of steps taken by the optimization algorithm, and our simple approach can identify these shifts. This enables users to constrain their search to regions where the model's predictions are reliable, thereby increasing the quality of designs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Basis-functions-nonlinear-data-enabled-predictive-control-Consistent-and-computationally-efficient-formulations"><a href="#Basis-functions-nonlinear-data-enabled-predictive-control-Consistent-and-computationally-efficient-formulations" class="headerlink" title="Basis functions nonlinear data-enabled predictive control: Consistent and computationally efficient formulations"></a>Basis functions nonlinear data-enabled predictive control: Consistent and computationally efficient formulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05360">http://arxiv.org/abs/2311.05360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mircea Lazar</li>
<li>for: 这个论文探讨了数据启发预测控制（DeePC）在非线性系统上的扩展，通过一般基函数。</li>
<li>methods: 论文使用了基函数DeePC行为预测器，并确定了相应的必要和充分条件，以确保基函数DeePC的有效性。</li>
<li>results: 论文通过对一个示例非线性抛钩系统进行测试，证明了基函数DeePC的有效性，并提出了一种更高效的计算方法。<details>
<summary>Abstract</summary>
This paper considers the extension of data-enabled predictive control (DeePC) to nonlinear systems via general basis functions. Firstly, we formulate a basis functions DeePC behavioral predictor and we identify necessary and sufficient conditions for equivalence with a corresponding basis functions multi-step identified predictor. The derived conditions yield a dynamic regularization cost function that enables a well-posed (i.e., consistent) basis functions formulation of nonlinear DeePC. To optimize computational efficiency of basis functions DeePC we further develop two alternative formulations that use a simpler, sparse regularization cost function and ridge regression, respectively. Consistency implications for Koopman DeePC as well as several methods for constructing the basis functions representation are also indicated. The effectiveness of the developed consistent basis functions DeePC formulations is illustrated on a benchmark nonlinear pendulum state-space model, for both noise free and noisy data.
</details>
<details>
<summary>摘要</summary>
这篇论文考虑了基于数据预测控制（DeePC）的扩展到非线性系统，使用通用基函数。我们首先构造了基函数DeePC行为预测器，并确定了相应的必要和 suficient conditions for equivalence with a corresponding basis functions multi-step identified predictor。这些条件导出了一个动态正则化成本函数，使得基函数DeePC的形式ulation well-posed（即一致）。为了提高基函数DeePC的计算效率，我们还开发了两种替代形式，使用简单的稀疏正则化成本函数和ridge regression。我们还探讨了 Koopman DeePC 的一致性含义，以及构造基函数表示的方法。这些方法在一个标准的非线性摆车状态空间模型上进行了效果的证明，包括无噪和噪声数据。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Shapley-Value-Approximation-for-Data-Evaluation"><a href="#Accelerated-Shapley-Value-Approximation-for-Data-Evaluation" class="headerlink" title="Accelerated Shapley Value Approximation for Data Evaluation"></a>Accelerated Shapley Value Approximation for Data Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05346">http://arxiv.org/abs/2311.05346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauren Watson, Zeno Kujawa, Rayna Andreeva, Hao-Tsung Yang, Tariq Elahi, Rik Sarkar</li>
<li>for: 这个论文主要是为了提出更高效的数据估价方法，以便在机器学习中实现数据筛选、有效学习和数据分享的奖励。</li>
<li>methods: 论文使用了 SHAPLEY 值来进行数据估价，但 SHAPLEY 值 computationally expensive，因此提出了一种基于机器学习问题的结构性质的更高效的approximation方法。</li>
<li>results:  experiments 表明，使用 $\delta$-Shapley 策略可以快速地计算出数据的估价值和排名，而且可以保持数据的准确性和有效性。在预训练网络中，该方法可以更好地实现准确的评估使用小 subsets。<details>
<summary>Abstract</summary>
Data valuation has found various applications in machine learning, such as data filtering, efficient learning and incentives for data sharing. The most popular current approach to data valuation is the Shapley value. While popular for its various applications, Shapley value is computationally expensive even to approximate, as it requires repeated iterations of training models on different subsets of data. In this paper we show that the Shapley value of data points can be approximated more efficiently by leveraging the structural properties of machine learning problems. We derive convergence guarantees on the accuracy of the approximate Shapley value for different learning settings including Stochastic Gradient Descent with convex and non-convex loss functions. Our analysis suggests that in fact models trained on small subsets are more important in the context of data valuation. Based on this idea, we describe $\delta$-Shapley -- a strategy of only using small subsets for the approximation. Experiments show that this approach preserves approximate value and rank of data, while achieving speedup of up to 9.9x. In pre-trained networks the approach is found to bring more efficiency in terms of accurate evaluation using small subsets.
</details>
<details>
<summary>摘要</summary>
“数据评估在机器学习中找到了多种应用，如数据筛选、高效学习和数据共享的激励。目前最受欢迎的数据评估方法是希布利值。虽然具有多种应用，但希布利值计算成本较高，因为需要重复训练模型在不同的数据 subsets上。在这篇论文中，我们表明希布利值可以更有效地被 aproximated，通过利用机器学习问题的结构性质。我们 deriv 出不同学习设置下的准确性拟合保证，包括杂函数梯度下降和非杂函数梯度下降。我们的分析表明，在数据评估中，使用小subset更重要。基于这个想法，我们描述了 $\delta$-希布利（delta-Shapley）策略，即只使用小subset进行拟合。实验表明，这种方法可以保持数据的approximate值和排名，同时实现速度增加达9.9倍。在预训练网络中，该方法被发现更有效，可以更准确地评估数据使用小 subsets。”
</details></li>
</ul>
<hr>
<h2 id="Real-time-Addressee-Estimation-Deployment-of-a-Deep-Learning-Model-on-the-iCub-Robot"><a href="#Real-time-Addressee-Estimation-Deployment-of-a-Deep-Learning-Model-on-the-iCub-Robot" class="headerlink" title="Real-time Addressee Estimation: Deployment of a Deep-Learning Model on the iCub Robot"></a>Real-time Addressee Estimation: Deployment of a Deep-Learning Model on the iCub Robot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05334">http://arxiv.org/abs/2311.05334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlo Mazzola, Francesco Rea, Alessandra Sciutti</li>
<li>for: 本研究旨在开发有效的对话代理人，以便人机交互更加畅通。</li>
<li>methods: 该研究使用深度学习模型，利用说话者的非语言行为，如眼光和姿势，进行地址者估计。</li>
<li>results: 实验结果表明，基于非语言行为的地址者估计模型在实时人机交互中表现出色，比前一次使用同一 dataset 进行训练的模型更高效。<details>
<summary>Abstract</summary>
Addressee Estimation is the ability to understand to whom a person is talking, a skill essential for social robots to interact smoothly with humans. In this sense, it is one of the problems that must be tackled to develop effective conversational agents in multi-party and unstructured scenarios. As humans, one of the channels that mainly lead us to such estimation is the non-verbal behavior of speakers: first of all, their gaze and body pose. Inspired by human perceptual skills, in the present work, a deep-learning model for Addressee Estimation relying on these two non-verbal features is designed, trained, and deployed on an iCub robot. The study presents the procedure of such implementation and the performance of the model deployed in real-time human-robot interaction compared to previous tests on the dataset used for the training.
</details>
<details>
<summary>摘要</summary>
接受人 judgement 是一种关键的技能，用于社交机器人与人类交互。在这种情况下，它是开发有效的对话代理人的问题之一。人类中一个主要的渠道带我们到此估计是说话人的非语言行为：首先是他的视线和姿势。受人类感知技能的欣慰，在当前工作中，一种基于这两种非语言特征的深度学习模型 для Addressee Estimation 被设计、训练和部署到iCub机器人上。该研究介绍了该实施的过程和在真实的人机交互中模型的性能与之前在用于训练的数据集进行比较。
</details></li>
</ul>
<hr>
<h2 id="RepQ-Generalizing-Quantization-Aware-Training-for-Re-Parametrized-Architectures"><a href="#RepQ-Generalizing-Quantization-Aware-Training-for-Re-Parametrized-Architectures" class="headerlink" title="RepQ: Generalizing Quantization-Aware Training for Re-Parametrized Architectures"></a>RepQ: Generalizing Quantization-Aware Training for Re-Parametrized Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05317">http://arxiv.org/abs/2311.05317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasiia Prutianova, Alexey Zaytsev, Chung-Kuei Lee, Fengyu Sun, Ivan Koryakovskiy</li>
<li>for: 提高神经网络的效率和可扩展性，使其在资源有限的环境中部署更加容易。</li>
<li>methods: 使用量化和重 parametrization 两种方法进行神经网络的提升。量化可以压缩神经网络的大小，而重 parametrization 可以提高神经网络的性能。</li>
<li>results: 提出了一种名为 RepQ 的新方法，通过对重 parametrized 网络进行量化，实现了神经网络的效率提升。 RepQ 在不同的重 parametrized 模型中都能够达到比基eline方法LSQ量化方案更好的性能。<details>
<summary>Abstract</summary>
Existing neural networks are memory-consuming and computationally intensive, making deploying them challenging in resource-constrained environments. However, there are various methods to improve their efficiency. Two such methods are quantization, a well-known approach for network compression, and re-parametrization, an emerging technique designed to improve model performance. Although both techniques have been studied individually, there has been limited research on their simultaneous application. To address this gap, we propose a novel approach called RepQ, which applies quantization to re-parametrized networks. Our method is based on the insight that the test stage weights of an arbitrary re-parametrized layer can be presented as a differentiable function of trainable parameters. We enable quantization-aware training by applying quantization on top of this function. RepQ generalizes well to various re-parametrized models and outperforms the baseline method LSQ quantization scheme in all experiments.
</details>
<details>
<summary>摘要</summary>
现有的神经网络具有较大的内存占用和计算复杂度，因此在有限资源环境中部署困难。然而，有多种方法可以改善其效率。其中两种方法是量化和重 parametrization。虽然这两种方法已经受到了研究，但它们之前的同时应用却很少。为了解决这个空白，我们提出了一种新的方法 called RepQ，该方法将量化应用于重 parametrized 网络。我们的方法基于对任意重 parametrized 层的测试阶段权重可以表示为可导的函数的假设。我们通过应用量化来实现量化执行。RepQ 可以通过多种重 parametrized 模型，并在所有实验中超过基准方法 LSQ 量化方案。
</details></li>
</ul>
<hr>
<h2 id="Reliable-and-Efficient-Data-Collection-in-UAV-based-IoT-Networks"><a href="#Reliable-and-Efficient-Data-Collection-in-UAV-based-IoT-Networks" class="headerlink" title="Reliable and Efficient Data Collection in UAV-based IoT Networks"></a>Reliable and Efficient Data Collection in UAV-based IoT Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05303">http://arxiv.org/abs/2311.05303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Poorvi Joshi, Alakesh Kalita, Mohan Gurusamy</li>
<li>for: 本研究旨在探讨无人机（UAV）在物联网（IoT）中的数据采集问题，以提高IoT网络的可靠性和效率。</li>
<li>methods: 本研究涉及了多种UAV-based数据采集方法，包括通信和网络方面的研究，以及数据采集方法的优缺点分析。此外，本研究还考虑了数据准确性和一致性、网络连接稳定性和数据安全隐私等因素。</li>
<li>results: 本研究结果表明，UAV-assisted IoT networks可以通过轨迹规划、碰撞避免、传感器网络划分、数据聚合和人工智能优化等方法来提高数据采集的可靠性和效率。此外，本研究还提出了在UAV-assisted IoT networks中使用无人机为服务的方法，以提高数据采集的可靠性和效率。<details>
<summary>Abstract</summary>
Internet of Things (IoT) involves sensors for monitoring and wireless networks for efficient communication. However, resource-constrained IoT devices and limitations in existing wireless technologies hinder its full potential. Integrating Unmanned Aerial Vehicles (UAVs) into IoT networks can address some challenges by expanding its' coverage, providing security, and bringing computing closer to IoT devices. Nevertheless, effective data collection in UAV-assisted IoT networks is hampered by factors, including dynamic UAV behavior, environmental variables, connectivity instability, and security considerations. In this survey, we first explore UAV-based IoT networks, focusing on communication and networking aspects. Next, we cover various UAV-based data collection methods their advantages and disadvantages, followed by a discussion on performance metrics for data collection. As this article primarily emphasizes reliable and efficient data collection in UAV-assisted IoT networks, we briefly discuss existing research on data accuracy and consistency, network connectivity, and data security and privacy to provide insights into reliable data collection. Additionally, we discuss efficient data collection strategies in UAV-based IoT networks, covering trajectory and path planning, collision avoidance, sensor network clustering, data aggregation, UAV swarm formations, and artificial intelligence for optimization. We also present two use cases of UAVs as a service for enhancing data collection reliability and efficiency. Finally, we discuss future challenges in data collection for UAV-assisted IoT networks.
</details>
<details>
<summary>摘要</summary>
互联网物件 (IoT) 包括侦测器和无线网络，但是资源有限的 IoT 设备和现有的无线技术限制了它的全面性。将无人航空车 (UAV)  integrate into IoT 网络可以解决一些挑战，例如扩展覆盖范围、提供安全性和让计算更近运行 IoT 设备。然而，UAV 协助 IoT 网络的数据收集过程受到多重因素影响，包括 UAV 动态行为、环境变量、连接不稳定和安全考虑。在这篇文章中，我们首先探讨 UAV 基本的 IoT 网络，专注于通信和网络方面。接着，我们详细介绍了不同的 UAV 数据收集方法，包括优点和缺点。接着，我们讨论了数据收集表现指标，以提供可靠数据收集的深入了解。此外，我们还讨论了现有的数据准确和一致性、网络连接和安全性等问题，以及如何使用 UAV 实现可靠的数据收集。最后，我们介绍了两个 UAV 作为服务的使用案例，以增强数据收集可靠性和效率。最后，我们讨论了未来数据收集的挑战，以及如何通过 UAV 实现更好的数据收集。
</details></li>
</ul>
<hr>
<h2 id="Latent-Task-Specific-Graph-Network-Simulators"><a href="#Latent-Task-Specific-Graph-Network-Simulators" class="headerlink" title="Latent Task-Specific Graph Network Simulators"></a>Latent Task-Specific Graph Network Simulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05256">http://arxiv.org/abs/2311.05256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/philippdahlinger/ltsgns_ai4science">https://github.com/philippdahlinger/ltsgns_ai4science</a></li>
<li>paper_authors: Philipp Dahlinger, Niklas Freymuth, Michael Volpp, Tai Hoang, Gerhard Neumann</li>
<li>for:  mesh-based simulation 的适应性和可重用性问题</li>
<li>methods: 使用 Bayesian meta-learning 方法，通过非积累任务 posterior 近似来采样 latent 描述未知系统特性，并利用运动 primitives 进行高效的全 trajectory 预测</li>
<li>results: 通过多种实验 validate 了方法的效果，与或超过了现有基eline方法的性能，并且可以处理不同类型的上下文数据，如使用点云进行推理。<details>
<summary>Abstract</summary>
Simulating dynamic physical interactions is a critical challenge across multiple scientific domains, with applications ranging from robotics to material science. For mesh-based simulations, Graph Network Simulators (GNSs) pose an efficient alternative to traditional physics-based simulators. Their inherent differentiability and speed make them particularly well-suited for inverse design problems. Yet, adapting to new tasks from limited available data is an important aspect for real-world applications that current methods struggle with. We frame mesh-based simulation as a meta-learning problem and use a recent Bayesian meta-learning method to improve GNSs adaptability to new scenarios by leveraging context data and handling uncertainties. Our approach, latent task-specific graph network simulator, uses non-amortized task posterior approximations to sample latent descriptions of unknown system properties. Additionally, we leverage movement primitives for efficient full trajectory prediction, effectively addressing the issue of accumulating errors encountered by previous auto-regressive methods. We validate the effectiveness of our approach through various experiments, performing on par with or better than established baseline methods. Movement primitives further allow us to accommodate various types of context data, as demonstrated through the utilization of point clouds during inference. By combining GNSs with meta-learning, we bring them closer to real-world applicability, particularly in scenarios with smaller datasets.
</details>
<details>
<summary>摘要</summary>
模拟动态物理交互是多个科学领域的关键挑战，其应用范围从 робо扮到物理科学。为 mesh-based  simulate，图像网络 simulate（GNS）成为效率备受关注的代替方案。它们的自然差分和速度使其特别适合反向设计问题。然而，适应新任务从有限的数据中学习是现实应用中的重要问题，现有方法困难以处理。我们将 mesh-based  simulate 作为一个元学习问题，使用最近的 Bayesian 元学习方法来提高 GNS 的适应新enario 能力，利用上下文数据和处理不确定性。我们的方法，缺失任务特定图像网络 simulate（LT-GNS），使用非折衔任务 posterior  aproximations 采样缺失系统属性的latent描述。此外，我们利用运动 primitives  для高效地预测全 trajectory，有效地解决过去的 auto-regressive 方法所遇到的积累错误问题。我们通过多个实验 validate 了我们的方法的有效性，与或更好于现有基eline方法。运动 primitives 还允许我们根据不同的上下文数据进行可扩展的应用，如通过使用点云进行推理。通过将 GNS 与元学习结合，我们使其更适合实际应用，特别是在小数据量的情况下。
</details></li>
</ul>
<hr>
<h2 id="When-Meta-Learning-Meets-Online-and-Continual-Learning-A-Survey"><a href="#When-Meta-Learning-Meets-Online-and-Continual-Learning-A-Survey" class="headerlink" title="When Meta-Learning Meets Online and Continual Learning: A Survey"></a>When Meta-Learning Meets Online and Continual Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05241">http://arxiv.org/abs/2311.05241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehyeon Son, Soochan Lee, Gunhee Kim</li>
<li>for: This paper aims to provide a comprehensive survey of various learning frameworks, including meta-learning, continual learning, and online learning, and their applications in deep neural networks.</li>
<li>methods: The paper uses a consistent terminology and formal descriptions to organize the problem settings and learning algorithms, and offers an overview of the current state of research in these areas.</li>
<li>results: The paper aims to facilitate a clear understanding of the differences between the learning frameworks and foster further advancements in this promising area of research.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提供深度神经网络中不同学习框架的总结，包括元学习、继续学习和在线学习，以及它们的应用。</li>
<li>methods: 这篇论文使用一致的术语和正式描述来组织问题设定和学习算法，并提供深度神经网络中这些领域的现状报告。</li>
<li>results: 这篇论文目的是为这些学习框架的研究做出清晰的认知，并促进这个领域的进一步发展。<details>
<summary>Abstract</summary>
Over the past decade, deep neural networks have demonstrated significant success using the training scheme that involves mini-batch stochastic gradient descent on extensive datasets. Expanding upon this accomplishment, there has been a surge in research exploring the application of neural networks in other learning scenarios. One notable framework that has garnered significant attention is meta-learning. Often described as "learning to learn," meta-learning is a data-driven approach to optimize the learning algorithm. Other branches of interest are continual learning and online learning, both of which involve incrementally updating a model with streaming data. While these frameworks were initially developed independently, recent works have started investigating their combinations, proposing novel problem settings and learning algorithms. However, due to the elevated complexity and lack of unified terminology, discerning differences between the learning frameworks can be challenging even for experienced researchers. To facilitate a clear understanding, this paper provides a comprehensive survey that organizes various problem settings using consistent terminology and formal descriptions. By offering an overview of these learning paradigms, our work aims to foster further advancements in this promising area of research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Whisper-in-Focus-Enhancing-Stuttered-Speech-Classification-with-Encoder-Layer-Optimization"><a href="#Whisper-in-Focus-Enhancing-Stuttered-Speech-Classification-with-Encoder-Layer-Optimization" class="headerlink" title="Whisper in Focus: Enhancing Stuttered Speech Classification with Encoder Layer Optimization"></a>Whisper in Focus: Enhancing Stuttered Speech Classification with Encoder Layer Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05203">http://arxiv.org/abs/2311.05203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huma Ameer, Seemab Latif, Rabia Latif, Sana Mukhtar</li>
<li>for: 本研究旨在探讨自动识别吞声的可能性，利用深度学习技术来分类吞声中的不一致类型。</li>
<li>methods: 本研究使用了Whisper模型进行分类，并对SEP28-kbenchmark dataset进行改进，以及引入有效的Encoder层冻结策略。</li>
<li>results: 优化后的Whisper模型在分类中获得了0.81的平均F1分数，表明其能力。研究还发现，深度Encoder层在识别不一致类型中具有更大的贡献，相比于初始层。<details>
<summary>Abstract</summary>
In recent years, advancements in the field of speech processing have led to cutting-edge deep learning algorithms with immense potential for real-world applications. The automated identification of stuttered speech is one of such applications that the researchers are addressing by employing deep learning techniques. Recently, researchers have utilized Wav2vec2.0, a speech recognition model to classify disfluency types in stuttered speech. Although Wav2vec2.0 has shown commendable results, its ability to generalize across all disfluency types is limited. In addition, since its base model uses 12 encoder layers, it is considered a resource-intensive model. Our study unravels the capabilities of Whisper for the classification of disfluency types in stuttered speech. We have made notable contributions in three pivotal areas: enhancing the quality of SEP28-k benchmark dataset, exploration of Whisper for classification, and introducing an efficient encoder layer freezing strategy. The optimized Whisper model has achieved the average F1-score of 0.81, which proffers its abilities. This study also unwinds the significance of deeper encoder layers in the identification of disfluency types, as the results demonstrate their greater contribution compared to initial layers. This research represents substantial contributions, shifting the emphasis towards an efficient solution, thereby thriving towards prospective innovation.
</details>
<details>
<summary>摘要</summary>
Recently, advancements in speech processing technology have led to the development of cutting-edge deep learning algorithms with numerous practical applications. One such application is the automated identification of stuttered speech, which researchers are addressing using deep learning techniques. Previously, researchers have utilized Wav2vec2.0, a speech recognition model, to classify disfluency types in stuttered speech. Although Wav2vec2.0 has shown promising results, its ability to generalize across all disfluency types is limited. Additionally, its base model uses 12 encoder layers, making it a resource-intensive model. Our study explores the capabilities of Whisper for classifying disfluency types in stuttered speech. Our contributions include enhancing the quality of the SEP28-k benchmark dataset, exploring the use of Whisper for classification, and introducing an efficient encoder layer freezing strategy. The optimized Whisper model achieved an average F1-score of 0.81, demonstrating its effectiveness. Our findings also highlight the significance of deeper encoder layers in identifying disfluency types, as the results show that they contribute more than the initial layers. This research represents significant contributions and paves the way for more efficient solutions, driving innovation forward.Here's the word-for-word translation of the text into Simplified Chinese:近年来，语音处理技术的进步导致了深度学习算法的出现，这些算法在实际应用中具有广泛的潜力。一种如此应用是自动识别吞声的语音，研究人员通过深度学习技术来实现这一目标。之前，研究人员已经使用Wav2vec2.0，一种语音识别模型，来分类吞声语音中的不顺拍类型。虽然Wav2vec2.0表现了良好，但它的总体化能力有限。此外，它的基础模型使用12层编码层，因此被视为资源占用的模型。我们的研究探讨了适用于吞声语音中的不顺拍类型分类的Whisper模型。我们的贡献包括提高SEP28-k测试数据集的质量、使用Whisper模型进行分类和引入高效的编码层冻结策略。优化后的Whisper模型实现了0.81的平均F1分数，这表明它的能力。我们的发现还表明了深度编码层在识别不顺拍类型时的重要性，结果显示它们比初始层更大的贡献。这种研究代表了重要的贡献，推动了可能的创新。
</details></li>
</ul>
<hr>
<h2 id="Perfecting-Liquid-State-Theories-with-Machine-Intelligence"><a href="#Perfecting-Liquid-State-Theories-with-Machine-Intelligence" class="headerlink" title="Perfecting Liquid-State Theories with Machine Intelligence"></a>Perfecting Liquid-State Theories with Machine Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05167">http://arxiv.org/abs/2311.05167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzhong Wu, Mengyang Gu</li>
<li>for: 预测电子结构、分子力场和各种固体系统的物理化学性质</li>
<li>methods: Functional machine learning技术，包括代理模型、维度减少和不确定性评估</li>
<li>results: 提高精度、扩展应用范围和计算效率等<details>
<summary>Abstract</summary>
Recent years have seen a significant increase in the use of machine intelligence for predicting electronic structure, molecular force fields, and the physicochemical properties of various condensed systems. However, substantial challenges remain in developing a comprehensive framework capable of handling a wide range of atomic compositions and thermodynamic conditions. This perspective discusses potential future developments in liquid-state theories leveraging on recent advancements of functional machine learning. By harnessing the strengths of theoretical analysis and machine learning techniques including surrogate models, dimension reduction and uncertainty quantification, we envision that liquid-state theories will gain significant improvements in accuracy, scalability and computational efficiency, enabling their broader applications across diverse materials and chemical systems.
</details>
<details>
<summary>摘要</summary>
近年来，机器智能的应用在预测电子结构、分子力场和各种固体系统的物理化学性质方面有了显著增长。然而，构建涵盖各种原子组成和热力学条件的全面框架仍存在巨大的挑战。本着观点认为，利用最新的函数机器学习技术，包括代表模型、维度减少和不确定性评估，将来的液体理论将在准确性、可扩展性和计算效率方面做出显著改进，使其在多种材料和化学系统中得到更广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="Counter-Empirical-Attacking-based-on-Adversarial-Reinforcement-Learning-for-Time-Relevant-Scoring-System"><a href="#Counter-Empirical-Attacking-based-on-Adversarial-Reinforcement-Learning-for-Time-Relevant-Scoring-System" class="headerlink" title="Counter-Empirical Attacking based on Adversarial Reinforcement Learning for Time-Relevant Scoring System"></a>Counter-Empirical Attacking based on Adversarial Reinforcement Learning for Time-Relevant Scoring System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05144">http://arxiv.org/abs/2311.05144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangguo Sun, Hong Cheng, Hang Dong, Bo Qiao, Si Qin, Qingwei Lin</li>
<li>for: This paper aims to establish a novel framework to improve scoring systems in the era of big data, without relying on ground truth or prior experience.</li>
<li>methods: The proposed framework uses a “counter-empirical attacking” mechanism to generate attacking behavior traces and evaluate the scoring system’s robustness. An adversarial “enhancer” is then applied to find improvement strategies and learn a proper scoring function.</li>
<li>results: Extensive experiments conducted on two scoring systems demonstrate the effectiveness of the proposed framework in improving the scoring system’s robustness to attacking activity traces.<details>
<summary>Abstract</summary>
Scoring systems are commonly seen for platforms in the era of big data. From credit scoring systems in financial services to membership scores in E-commerce shopping platforms, platform managers use such systems to guide users towards the encouraged activity pattern, and manage resources more effectively and more efficiently thereby. To establish such scoring systems, several "empirical criteria" are firstly determined, followed by dedicated top-down design for each factor of the score, which usually requires enormous effort to adjust and tune the scoring function in the new application scenario. What's worse, many fresh projects usually have no ground-truth or any experience to evaluate a reasonable scoring system, making the designing even harder. To reduce the effort of manual adjustment of the scoring function in every new scoring system, we innovatively study the scoring system from the preset empirical criteria without any ground truth, and propose a novel framework to improve the system from scratch. In this paper, we propose a "counter-empirical attacking" mechanism that can generate "attacking" behavior traces and try to break the empirical rules of the scoring system. Then an adversarial "enhancer" is applied to evaluate the scoring system and find the improvement strategy. By training the adversarial learning problem, a proper scoring function can be learned to be robust to the attacking activity traces that are trying to violate the empirical criteria. Extensive experiments have been conducted on two scoring systems including a shared computing resource platform and a financial credit system. The experimental results have validated the effectiveness of our proposed framework.
</details>
<details>
<summary>摘要</summary>
大数据时代中，普遍出现了评分系统。从金融服务中的信用评分系统到电商平台上的会员评分系统，平台管理者通过这些系统来引导用户遵循推荐的活动模式，更好地管理资源，提高效率。为建立这些评分系统，需要首先确定一些“实证标准”，然后针对每个因素进行专门的顶部设计，这通常需要巨大的努力来调整和调整评分函数在新应用场景中。尤其是许多新项目缺乏实践经验或者实证数据，使得设计更加困难。为了减少每个新评分系统的手动调整努力，我们创新地研究了评分系统，不基于实证标准，而是基于预设的empirical criterion。在这篇论文中，我们提出了一种“反实证攻击”机制，可以生成“攻击”行为轨迹，并尝试破坏empirical规则。然后，我们应用了一种“增强器”来评估评分系统，并找到改进策略。通过训练对抗学习问题，我们可以学习一个robust的评分函数，抵抗攻击行为轨迹的尝试。我们在两个评分系统上进行了广泛的实验，包括共享计算资源平台和金融信用系统。实验结果证明了我们提出的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="On-neural-and-dimensional-collapse-in-supervised-and-unsupervised-contrastive-learning-with-hard-negative-sampling"><a href="#On-neural-and-dimensional-collapse-in-supervised-and-unsupervised-contrastive-learning-with-hard-negative-sampling" class="headerlink" title="On neural and dimensional collapse in supervised and unsupervised contrastive learning with hard negative sampling"></a>On neural and dimensional collapse in supervised and unsupervised contrastive learning with hard negative sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05139">http://arxiv.org/abs/2311.05139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijie Jiang, Thuan Nguyen, Shuchin Aeron, Prakash Ishwar</li>
<li>for: 证明Supervised Contrastive Learning（SCL）、Hard-SCL（HSCL）和Unsupervised Contrastive Learning（UCL）的风险可以通过表现出神经坍缩（NC）来最小化，即类均匀的紧张框（ETF）和数据相同类别的映射到同一个表示。</li>
<li>methods: 使用了 widely-studied data model和通用的损失函数和坚化函数。</li>
<li>results: 证明了对于任何表示映射，HSCL和Hard-UCL的风险都是SCL和UCL风险的下界，而且这种下界是通过NC来实现的。此外，我们的证明比之前的证明更加简洁、紧凑和透明。<details>
<summary>Abstract</summary>
For a widely-studied data model and general loss and sample-hardening functions we prove that the Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) risks are minimized by representations that exhibit Neural Collapse (NC), i.e., the class means form an Equianglular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) risks are lower bounded by the corresponding SCL and UCL risks. Although the optimality of ETF is known for SCL, albeit only for InfoNCE loss, its optimality for HSCL and UCL under general loss and hardening functions is novel. Moreover, our proofs are much simpler, compact, and transparent. We empirically demonstrate, for the first time, that ADAM optimization of HSCL and HUCL risks with random initialization and suitable hardness levels can indeed converge to the NC geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard negatives or feature normalization, however, the representations learned via ADAM suffer from dimensional collapse (DC) and fail to attain the NC geometry.
</details>
<details>
<summary>摘要</summary>
For a widely-studied data model and general loss and sample-hardening functions, we prove that the Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) risks are minimized by representations that exhibit Neural Collapse (NC), i.e., the class means form an Equiangular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) risks are lower bounded by the corresponding SCL and UCL risks. Although the optimality of ETF is known for SCL, albeit only for InfoNCE loss, its optimality for HSCL and UCL under general loss and hardening functions is novel. Moreover, our proofs are much simpler, compact, and transparent. We empirically demonstrate, for the first time, that ADAM optimization of HSCL and HUCL risks with random initialization and suitable hardness levels can indeed converge to the NC geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard negatives or feature normalization, however, the representations learned via ADAM suffer from dimensional collapse (DC) and fail to attain the NC geometry.
</details></li>
</ul>
<hr>
<h2 id="Improving-Computational-Efficiency-for-Powered-Descent-Guidance-via-Transformer-based-Tight-Constraint-Prediction"><a href="#Improving-Computational-Efficiency-for-Powered-Descent-Guidance-via-Transformer-based-Tight-Constraint-Prediction" class="headerlink" title="Improving Computational Efficiency for Powered Descent Guidance via Transformer-based Tight Constraint Prediction"></a>Improving Computational Efficiency for Powered Descent Guidance via Transformer-based Tight Constraint Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05135">http://arxiv.org/abs/2311.05135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Briden, Trey Gurga, Breanna Johnson, Abhishek Cauligi, Richard Linares</li>
<li>for: 这篇论文的目的是提出一种可扩展的算法，以减少直接优化形式中的计算复杂性。</li>
<li>methods: 该算法使用Transformer神经网络，通过在前一次的轨迹优化算法中提取数据，准确预测问题参数和最优解的关系。</li>
<li>results: 在应用于火星推进 descent 问题中，T-PDG 可以在计算3个自由度燃料优化 trajectory 时，比lossless convexification 快得多，从1-8秒钟减少到了500毫秒以下。并且保证了安全和优化的解。<details>
<summary>Abstract</summary>
In this work, we present Transformer-based Powered Descent Guidance (T-PDG), a scalable algorithm for reducing the computational complexity of the direct optimization formulation of the spacecraft powered descent guidance problem. T-PDG uses data from prior runs of trajectory optimization algorithms to train a transformer neural network, which accurately predicts the relationship between problem parameters and the globally optimal solution for the powered descent guidance problem. The solution is encoded as the set of tight constraints corresponding to the constrained minimum-cost trajectory and the optimal final time of landing. By leveraging the attention mechanism of transformer neural networks, large sequences of time series data can be accurately predicted when given only the spacecraft state and landing site parameters. When applied to the real problem of Mars powered descent guidance, T-PDG reduces the time for computing the 3 degree of freedom fuel-optimal trajectory, when compared to lossless convexification, from an order of 1-8 seconds to less than 500 milliseconds. A safe and optimal solution is guaranteed by including a feasibility check in T-PDG before returning the final trajectory.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了Transformer基于Powered Descent Guidance（T-PDG）算法，用于降低直接优化问题的计算复杂性。T-PDG使用之前的轨迹优化算法的数据来训练Transformer神经网络，以准确预测问题参数和 globally optimal solution的关系。解决方案被编码为最佳化的约束集，包括约束最小成本轨迹和着陆时刻。通过转换器神经网络的注意力机制，可以准确预测大量时间序列数据，只要提供空间craft状态和着陆地点参数。在应用于 Mars 着陆引导问题时，T-PDG 比lossless convexification减少了计算3个自由度燃料优化轨迹的时间，从大约1-8秒钟降低到 fewer than 500毫秒。保证安全且优化解决方案的可靠性，T-PDG 中包括了可靠性检查，以确保返回最终轨迹。
</details></li>
</ul>
<hr>
<h2 id="Exploring-and-Analyzing-Wildland-Fire-Data-Via-Machine-Learning-Techniques"><a href="#Exploring-and-Analyzing-Wildland-Fire-Data-Via-Machine-Learning-Techniques" class="headerlink" title="Exploring and Analyzing Wildland Fire Data Via Machine Learning Techniques"></a>Exploring and Analyzing Wildland Fire Data Via Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05128">http://arxiv.org/abs/2311.05128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipak Dulal, Joseph J. Charney, Michael Gallagher, Carmeliza Navasca, Nicholas Skowronski<br>for: 这项研究旨在探讨10Hz时间序列的热电差温度和风速测得的动力动量能量（TKE）之间的相关性，以及利用热电差温度作为预测TKE的可能性。methods: 这项研究使用了机器学习模型，包括深度神经网络、Random Forest Regressor、Gradient Boosting和Gaussian Process Regressor，来评估热电差温度干扰的可能性来预测TKE值。results: 研究发现，使用不同的机器学习模型可以高度准确地预测TKE，尤其是使用回归模型。数据视觉和相关分析显示了热电差温度和TKE之间的各种模式和关系，为火Behavior和烟雾科学提供了重要的新视角。<details>
<summary>Abstract</summary>
This research project investigated the correlation between a 10 Hz time series of thermocouple temperatures and turbulent kinetic energy (TKE) computed from wind speeds collected from a small experimental prescribed burn at the Silas Little Experimental Forest in New Jersey, USA. The primary objective of this project was to explore the potential for using thermocouple temperatures as predictors for estimating the TKE produced by a wildland fire. Machine learning models, including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and Gaussian Process Regressor, are employed to assess the potential for thermocouple temperature perturbations to predict TKE values. Data visualization and correlation analyses reveal patterns and relationships between thermocouple temperatures and TKE, providing insight into the underlying dynamics. The project achieves high accuracy in predicting TKE by employing various machine learning models despite a weak correlation between the predictors and the target variable. The results demonstrate significant success, particularly from regression models, in accurately estimating the TKE. The research findings contribute to fire behavior and smoke modeling science, emphasizing the importance of incorporating machine learning approaches and identifying complex relationships between fine-scale fire behavior and turbulence. Accurate TKE estimation using thermocouple temperatures allows for the refinement of models that can inform decision-making in fire management strategies, facilitate effective risk mitigation, and optimize fire management efforts. This project highlights the valuable role of machine learning techniques in analyzing wildland fire data, showcasing their potential to advance fire research and management practices.
</details>
<details>
<summary>摘要</summary>
Machine learning models, including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and Gaussian Process Regressor, were applied to assess the potential for thermocouple temperature perturbations to predict TKE values. Data visualization and correlation analyses revealed patterns and relationships between thermocouple temperatures and TKE, providing insights into the underlying dynamics.Despite a weak correlation between the predictors and the target variable, the project achieved high accuracy in predicting TKE using various machine learning models. The results demonstrated significant success, particularly from regression models, in accurately estimating TKE. The findings contribute to fire behavior and smoke modeling science, emphasizing the importance of incorporating machine learning approaches and identifying complex relationships between fine-scale fire behavior and turbulence.Accurate TKE estimation using thermocouple temperatures allows for the refinement of models that can inform decision-making in fire management strategies, facilitate effective risk mitigation, and optimize fire management efforts. This project highlights the valuable role of machine learning techniques in analyzing wildland fire data, showcasing their potential to advance fire research and management practices.
</details></li>
</ul>
<hr>
<h2 id="Covering-Number-of-Real-Algebraic-Varieties-Improved-Bound-and-Applications"><a href="#Covering-Number-of-Real-Algebraic-Varieties-Improved-Bound-and-Applications" class="headerlink" title="Covering Number of Real Algebraic Varieties: Improved Bound and Applications"></a>Covering Number of Real Algebraic Varieties: Improved Bound and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05116">http://arxiv.org/abs/2311.05116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhang, Joe Kileel</li>
<li>for: 提供了一个上界bounds on the covering number of real algebraic varieties, 映射 polynomials, and semialgebraic sets.</li>
<li>methods: 使用了新的方法, which improves the best known bound by Yomdin-Comte and has a much simpler proof.</li>
<li>results: 得到了一个更好的 bound on the volume of the tubular neighborhood of a real variety, 并应用到了三个主要应用领域：low rank CP tensors, (general) polynomial optimization problems, 和 deep neural networks with rational or ReLU activations.<details>
<summary>Abstract</summary>
We prove an upper bound on the covering number of real algebraic varieties, images of polynomial maps and semialgebraic sets. The bound remarkably improves the best known bound by Yomdin-Comte, and its proof is much more straightforward. As a consequence, our result gives a bound on volume of the tubular neighborhood of a real variety, improving the results by Lotz and Basu-Lerario. We apply our theory to three main application domains. Firstly, we derive a near-optimal bound on the covering number of low rank CP tensors. Secondly, we prove a bound on the sketching dimension for (general) polynomial optimization problems. Lastly, we deduce generalization error bounds for deep neural networks with rational or ReLU activations, improving or matching the best known results in the literature.
</details>
<details>
<summary>摘要</summary>
我们证明了实数型变量的覆盖数目的上限，包括映射函数图像和 semi-代数集。这个上限意外地改进了最佳知的 bound by Yomdin-Comte，并且证明的过程非常直观。因此，我们的结果可以提供实体变量的卷积附近体积的上限，超越了 Lotz 和 Basu-Lerario 的结果。我们在三个主要应用领域中运用了我们的理论：第一个，我们从低级CP张量中获得了near-优化的覆盖数目上限。第二个，我们证明了普通多项式优化问题的绘制维度上限。第三个，我们从深度神经网络中获得了 rational 或 ReLU 激活函数的泛化误差上限，超越了文献中最佳的结果。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Online-Federated-Learning-with-Multiple-Kernels"><a href="#Personalized-Online-Federated-Learning-with-Multiple-Kernels" class="headerlink" title="Personalized Online Federated Learning with Multiple Kernels"></a>Personalized Online Federated Learning with Multiple Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05108">http://arxiv.org/abs/2311.05108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pouyamghari/pof-mkl">https://github.com/pouyamghari/pof-mkl</a></li>
<li>paper_authors: Pouya M. Ghari, Yanning Shen</li>
<li>for: 在线非齐次函数掌 approximation 中表现出色的多核学习 (MKL) 技术，以实现在多客户端上进行线上非齐次函数掌 approximation。</li>
<li>methods: 这篇 paper 提出了一个算法框架，让客户端与服务器进行通信，实现在多客户端上进行线上非齐次函数掌 approximation，并且使用随机特征（RF）近似来实现扩展性。</li>
<li>results: paper 证明了，使用提出的线上联边MKL算法，每个客户端在它的最佳核函数方面具有子线性 regret，这表明了提出的算法能够有效地处理客户端上的数据不均匀性。实验结果显示，提出的算法与其他线上联边核学学算法相比，具有明显的优势。<details>
<summary>Abstract</summary>
Multi-kernel learning (MKL) exhibits well-documented performance in online non-linear function approximation. Federated learning enables a group of learners (called clients) to train an MKL model on the data distributed among clients to perform online non-linear function approximation. There are some challenges in online federated MKL that need to be addressed: i) Communication efficiency especially when a large number of kernels are considered ii) Heterogeneous data distribution among clients. The present paper develops an algorithmic framework to enable clients to communicate with the server to send their updates with affordable communication cost while clients employ a large dictionary of kernels. Utilizing random feature (RF) approximation, the present paper proposes scalable online federated MKL algorithm. We prove that using the proposed online federated MKL algorithm, each client enjoys sub-linear regret with respect to the RF approximation of its best kernel in hindsight, which indicates that the proposed algorithm can effectively deal with heterogeneity of the data distributed among clients. Experimental results on real datasets showcase the advantages of the proposed algorithm compared with other online federated kernel learning ones.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GeoFormer-Predicting-Human-Mobility-using-Generative-Pre-trained-Transformer-GPT"><a href="#GeoFormer-Predicting-Human-Mobility-using-Generative-Pre-trained-Transformer-GPT" class="headerlink" title="GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT)"></a>GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05092">http://arxiv.org/abs/2311.05092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aivin V. Solatorio</li>
<li>for: 预测人类移动（human mobility prediction）</li>
<li>methods: 使用了一种基于 transformer 架构的 decoder-only 模型（GeoFormer），并在 HuMob Challenge 2023 竞赛中进行了实验和评测。</li>
<li>results: 在 HuMob Challenge 2023 中，GeoFormer 表现出色，在两个数据集上都达到了高水平的预测性能，并在两个性能指标（GEO-BLEU 和 Dynamic Time Warping 指标）上都表现良好。<details>
<summary>Abstract</summary>
Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility. Our proposed model is rigorously tested in the context of the HuMob Challenge 2023 -- a competition designed to evaluate the performance of prediction models on standardized datasets to predict human mobility. The challenge leverages two datasets encompassing urban-scale data of 25,000 and 100,000 individuals over a longitudinal period of 75 days. GeoFormer stands out as a top performer in the competition, securing a place in the top-3 ranking. Its success is underscored by performing well on both performance metrics chosen for the competition -- the GEO-BLEU and the Dynamic Time Warping (DTW) measures. The performance of the GeoFormer on the HuMob Challenge 2023 underscores its potential to make substantial contributions to the field of human mobility prediction, with far-reaching implications for disaster preparedness, epidemic control, and beyond.
</details>
<details>
<summary>摘要</summary>
预测人员流动具有重要的实用价值，其应用范围从增进灾害风险规划到模拟疫情传播。在这篇论文中，我们提出了GeoFormer模型，这是基于GPT架构的解码器只模型，用于预测人员流动。我们的提议的模型在 HuMob Challenge 2023 中得到了证明，这是一项 Competition ，用于评估预测模型在标准化数据集上的性能。这个挑战使用了两个数据集，每个数据集包含25,000和100,000个人的城市规模数据，并且持续75天。GeoFormer在这个竞赛中表现出色，得到了排名前三名的成绩。它在选择竞赛中使用的两个性能指标中表现出色，即GEO-BLEU和动态时间戳滑块度量。GeoFormer在 HuMob Challenge 2023 中的表现表明它在人员流动预测方面具有很大的潜力，这对于灾害准备、疫情控制和其他领域都有广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="Generalized-test-utilities-for-long-tail-performance-in-extreme-multi-label-classification"><a href="#Generalized-test-utilities-for-long-tail-performance-in-extreme-multi-label-classification" class="headerlink" title="Generalized test utilities for long-tail performance in extreme multi-label classification"></a>Generalized test utilities for long-tail performance in extreme multi-label classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05081">http://arxiv.org/abs/2311.05081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Schultheis, Marek Wydmuch, Wojciech Kotłowski, Rohit Babbar, Krzysztof Dembczyński</li>
<li>for: 这篇论文关注于EXTREME多个标签分类任务中，选择一小subset of relevant标签，并且解决长尾标签问题。</li>
<li>methods: 该论文使用了budgeted “at k” metrics作为代替方案，并在预测性能方面做出了优化。</li>
<li>results: 该论文通过使用块协调赋值法和提供证明的 regret保证和模型误差robustness，实现了对XMLC问题的有效解决。<details>
<summary>Abstract</summary>
Extreme multi-label classification (XMLC) is the task of selecting a small subset of relevant labels from a very large set of possible labels. As such, it is characterized by long-tail labels, i.e., most labels have very few positive instances. With standard performance measures such as precision@k, a classifier can ignore tail labels and still report good performance. However, it is often argued that correct predictions in the tail are more interesting or rewarding, but the community has not yet settled on a metric capturing this intuitive concept. The existing propensity-scored metrics fall short on this goal by confounding the problems of long-tail and missing labels. In this paper, we analyze generalized metrics budgeted "at k" as an alternative solution. To tackle the challenging problem of optimizing these metrics, we formulate it in the expected test utility (ETU) framework, which aims at optimizing the expected performance on a fixed test set. We derive optimal prediction rules and construct computationally efficient approximations with provable regret guarantees and robustness against model misspecification. Our algorithm, based on block coordinate ascent, scales effortlessly to XMLC problems and obtains promising results in terms of long-tail performance.
</details>
<details>
<summary>摘要</summary>
极端多标签分类（XMLC）是选择一小集合的相关标签从一个非常大的可能的标签集中的任务。因此，它具有长尾标签，即大多数标签有很少的正例。使用标准的表现度量如精度@k，一个分类器可以忽略尾标签并仍然报告良好的性能。然而，有时被认为正确预测在尾标签上更有趣或奖励，但社区没有尚未定制一个度量捕捉这个直觉概念。现有的可能性分数度量混乱长尾和缺失标签的问题。在这篇论文中，我们分析通用度量预算"at k"作为一个代替解决方案。为了解决这些度量的优化问题，我们在预测测试用户（ETU）框架中对它们进行优化。我们 deriv出了最佳预测规则，并构建了计算效率高并且对模型误差robust的计算方法。我们的算法基于块协调架，可以轻松扩展到XMLC问题，并取得了良好的长尾性能。
</details></li>
</ul>
<hr>
<h2 id="Social-Media-Bot-Detection-using-Dropout-GAN"><a href="#Social-Media-Bot-Detection-using-Dropout-GAN" class="headerlink" title="Social Media Bot Detection using Dropout-GAN"></a>Social Media Bot Detection using Dropout-GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05079">http://arxiv.org/abs/2311.05079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anant Shukla, Martin Jurecek, Mark Stamp</li>
<li>for: 寻找社交媒体平台上的机器人活动，以维护在线讨论的威信和避免网络犯罪。</li>
<li>methods: 使用生成对抗网络（GAN）进行机器人检测，并解决模式塌陷问题，通过多个检测器对一个生成器进行训练，同时将检测器与生成器分离，以便在社交媒体上进行检测和数据增强。</li>
<li>results: 相比之前的状态艺技术，我们的方法在分类精度方面表现出色，并且示出了使用生成器来逃脱这种分类技术的可能性。<details>
<summary>Abstract</summary>
Bot activity on social media platforms is a pervasive problem, undermining the credibility of online discourse and potentially leading to cybercrime. We propose an approach to bot detection using Generative Adversarial Networks (GAN). We discuss how we overcome the issue of mode collapse by utilizing multiple discriminators to train against one generator, while decoupling the discriminator to perform social media bot detection and utilizing the generator for data augmentation. In terms of classification accuracy, our approach outperforms the state-of-the-art techniques in this field. We also show how the generator in the GAN can be used to evade such a classification technique.
</details>
<details>
<summary>摘要</summary>
社交媒体平台上的机器人活动是一个普遍的问题，对在线讨论的准确性产生负面影响，并可能导致网络犯罪。我们提出了基于生成对抗网络（GAN）的机器人检测方法。我们解决了模式塌陷的问题，通过多个检测器来训练一个生成器，并将检测器与生成器分离，以进行社交媒体机器人检测和数据增强。在分类精度方面，我们的方法超越了当前领域的状态艺术技术。我们还展示了如何使用生成器在GAN中逃脱这种分类技术。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/09/cs.LG_2023_11_09/" data-id="clot2mhg700ssx788gtdecc5j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/09/cs.CL_2023_11_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-11-09
        
      </div>
    </a>
  
  
    <a href="/2023/11/09/eess.SP_2023_11_09/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.SP - 2023-11-09</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">61</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">70</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">65</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
