
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.AS - 2023-11-20 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="How does end-to-end speech recognition training impact speech enhancement artifacts? paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11599 repo_url: None paper_authors: Kazuma Iwamoto, Tsubasa Ochiai, Marc Delcr">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.AS - 2023-11-20">
<meta property="og:url" content="https://nullscc.github.io/2023/11/20/eess.AS_2023_11_20/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="How does end-to-end speech recognition training impact speech enhancement artifacts? paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.11599 repo_url: None paper_authors: Kazuma Iwamoto, Tsubasa Ochiai, Marc Delcr">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-20T14:00:00.000Z">
<meta property="article:modified_time" content="2023-11-21T10:06:16.216Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.AS_2023_11_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/20/eess.AS_2023_11_20/" class="article-date">
  <time datetime="2023-11-20T14:00:00.000Z" itemprop="datePublished">2023-11-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.AS - 2023-11-20
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="How-does-end-to-end-speech-recognition-training-impact-speech-enhancement-artifacts"><a href="#How-does-end-to-end-speech-recognition-training-impact-speech-enhancement-artifacts" class="headerlink" title="How does end-to-end speech recognition training impact speech enhancement artifacts?"></a>How does end-to-end speech recognition training impact speech enhancement artifacts?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11599">http://arxiv.org/abs/2311.11599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuma Iwamoto, Tsubasa Ochiai, Marc Delcroix, Rintaro Ikeshita, Hiroshi Sato, Shoko Araki, Shigeru Katagiri<br>for: 这 paper  investigate 如何使用 joint training 来 mitigate 单通道 speech enhancement (SE) 对 automatic speech recognition (ASR) 的影响。methods: 这 paper 使用 joint training 方法，将 SE 和 ASR 两个模块合作训练。results: 实验结果表明，ASR-level training 可以降低 artifact error，但会增加 noise error。此外，可以通过 interpolating 简单地将增强和观察到的信号进行拼接，以达到降低 artifact 和增加 noise 的效果，而不需要修改 SE 和 ASR 模块。这些发现可以帮助我们更好地理解 joint training 的效果，并提供一种 novel 的设计方法 для ASR agnostic SE 前端。<details>
<summary>Abstract</summary>
Jointly training a speech enhancement (SE) front-end and an automatic speech recognition (ASR) back-end has been investigated as a way to mitigate the influence of \emph{processing distortion} generated by single-channel SE on ASR. In this paper, we investigate the effect of such joint training on the signal-level characteristics of the enhanced signals from the viewpoint of the decomposed noise and artifact errors. The experimental analyses provide two novel findings: 1) ASR-level training of the SE front-end reduces the artifact errors while increasing the noise errors, and 2) simply interpolating the enhanced and observed signals, which achieves a similar effect of reducing artifacts and increasing noise, improves ASR performance without jointly modifying the SE and ASR modules, even for a strong ASR back-end using a WavLM feature extractor. Our findings provide a better understanding of the effect of joint training and a novel insight for designing an ASR agnostic SE front-end.
</details>
<details>
<summary>摘要</summary>
jointly 训练一个 speech enhancement（SE）前端和一个 automatic speech recognition（ASR）后端，以减少单通道 SE 处理损害对 ASR 的影响。在这篇论文中，我们研究了将这种联合训练对减去频率特征的影响。我们的实验分析结果表明，1）ASR 级别训练 SE 前端可以降低artifact误差，但是增加雑音误差。2）简单地 interpolating 减去和观察的信号，可以达到降低artifacts和增加雑音的效果，提高 ASR 性能，即使使用强大的 ASR 后端使用 WavLM 特征提取器。我们的发现可以为我们更好地理解联合训练的影响，并提供一种 ASR 无关的 SE 前端设计的新思路。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Neural-network-based-virtual-microphone-estimation-with-virtual-microphone-and-beamformer-level-multi-task-loss"><a href="#Neural-network-based-virtual-microphone-estimation-with-virtual-microphone-and-beamformer-level-multi-task-loss" class="headerlink" title="Neural network-based virtual microphone estimation with virtual microphone and beamformer-level multi-task loss"></a>Neural network-based virtual microphone estimation with virtual microphone and beamformer-level multi-task loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11595">http://arxiv.org/abs/2311.11595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanako Segawa, Tsubasa Ochiai, Marc Delcroix, Tomohiro Nakatani, Rintaro Ikeshita, Shoko Araki, Takeshi Yamada, Shoji Makino</li>
<li>for: 提高降噪抑制性能</li>
<li>methods: 使用神经网络预测虚拟麦克风信号</li>
<li>results: 实现33.1%的相对噪音抑制性能提高和10.8%的相对预测性能提高<details>
<summary>Abstract</summary>
Array processing performance depends on the number of microphones available. Virtual microphone estimation (VME) has been proposed to increase the number of microphone signals artificially. Neural network-based VME (NN-VME) trains an NN with a VM-level loss to predict a signal at a microphone location that is available during training but not at inference. However, this training objective may not be optimal for a specific array processing back-end, such as beamforming. An alternative approach is to use a training objective considering the array-processing back-end, such as a loss on the beamformer output. This approach may generate signals optimal for beamforming but not physically grounded. To combine the advantages of both approaches, this paper proposes a multi-task loss for NN-VME that combines both VM-level and beamformer-level losses. We evaluate the proposed multi-task NN-VME on multi-talker underdetermined conditions and show that it achieves a 33.1 % relative WER improvement compared to using only real microphones and 10.8 % compared to using a prior NN-VME approach.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本）数组处理性能取决于可用的麦克风数量。虚拟麦克风估计（VME）已经提议以增加虚拟麦克风信号的数量。基于神经网络的VME（NN-VME）将神经网络训练为预测在训练过程中可用的麦克风位置上的信号，但在推理过程中不可用。这种训练目标可能并不适合特定的数组处理后端，如扩排器。一种另一种方法是使用包含数组处理后端的训练目标，如扩排器输出的损失函数。这种方法可能生成适合扩排器的信号，但不是物理上的。为了结合这两种方法的优点，这篇文章提出了一种多任务损失函数 дляNN-VME，将VM级别和扩排器级别的损失函数组合在一起。我们对多话者下的多个麦克风不足Conditions进行评估，并显示了使用提议的多任务NN-VME的33.1%相对WRER提升和10.8%相对于使用真正的麦克风和NN-VME先前方法的提升。
</details></li>
</ul>
<hr>
<h2 id="APNet2-High-quality-and-High-efficiency-Neural-Vocoder-with-Direct-Prediction-of-Amplitude-and-Phase-Spectra"><a href="#APNet2-High-quality-and-High-efficiency-Neural-Vocoder-with-Direct-Prediction-of-Amplitude-and-Phase-Spectra" class="headerlink" title="APNet2: High-quality and High-efficiency Neural Vocoder with Direct Prediction of Amplitude and Phase Spectra"></a>APNet2: High-quality and High-efficiency Neural Vocoder with Direct Prediction of Amplitude and Phase Spectra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11545">http://arxiv.org/abs/2311.11545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui-Peng Du, Ye-Xin Lu, Yang Ai, Zhen-Hua Ling</li>
<li>for: 提高高质量语音合成的实时速度</li>
<li>methods: 使用ConvNeXt v2作为后凝网络，并引入多分辨率评估器（MRD），优化某些损失函数的形式</li>
<li>results: 在常规配置下（采样率22.05 kHz，spectral frame shift256点，约11.6ms），提出的APNet2 vocoder比原APNet和Vocos vocoder在生成的语音质量方面表现出色，同时具有较快的推理速度，与HiFi-GAN和iSTFTNet的语音质量相当，但具有更快的推理速度。<details>
<summary>Abstract</summary>
In our previous work, we proposed a neural vocoder called APNet, which directly predicts speech amplitude and phase spectra with a 5 ms frame shift in parallel from the input acoustic features, and then reconstructs the 16 kHz speech waveform using inverse short-time Fourier transform (ISTFT). APNet demonstrates the capability to generate synthesized speech of comparable quality to the HiFi-GAN vocoder but with a considerably improved inference speed. However, the performance of the APNet vocoder is constrained by the waveform sampling rate and spectral frame shift, limiting its practicality for high-quality speech synthesis. Therefore, this paper proposes an improved iteration of APNet, named APNet2. The proposed APNet2 vocoder adopts ConvNeXt v2 as the backbone network for amplitude and phase predictions, expecting to enhance the modeling capability. Additionally, we introduce a multi-resolution discriminator (MRD) into the GAN-based losses and optimize the form of certain losses. At a common configuration with a waveform sampling rate of 22.05 kHz and spectral frame shift of 256 points (i.e., approximately 11.6ms), our proposed APNet2 vocoder outperformed the original APNet and Vocos vocoders in terms of synthesized speech quality. The synthesized speech quality of APNet2 is also comparable to that of HiFi-GAN and iSTFTNet, while offering a significantly faster inference speed.
</details>
<details>
<summary>摘要</summary>
在我们之前的工作中，我们提出了一种神经网络模型called APNet，它直接预测了语音幅度和相位spectrum的5 ms帧shift并使用反时域 Fourier transform (ISTFT)重建16 kHz语音波形。APNet示出了与HiFi-GAN vocoder相同的质量的生成语音，但是具有显著提高的推理速度。然而，APNet的性能受到波形采样率和spectral frame shift的限制，这限制了其在高质量语音生成中的实用性。因此，这篇文章提出了APNet2 vocoder的改进版本。我们在APNet2 vocoder中采用了ConvNeXt v2作为幅度和相位预测的后IONetwork，以提高模型的能力。此外，我们还引入了多resolution discriminator (MRD)到GAN-based的损失中，并优化了certain的损失形式。在常见的配置下（即22.05 kHz的波形采样率和256个spectral frame shift，约等于11.6ms），我们的提案的APNet2 vocoder在与原始APNet和Vocos vocoders的比较中赢得了生成语音质量的比赛。APNet2的生成语音质量也与HiFi-GAN和iSTFTNet相当，而且提供了显著 faster的推理速度。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/20/eess.AS_2023_11_20/" data-id="clp88dc3z0168ob88574d6c8s" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2023/11/20/cs.CV_2023_11_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CV - 2023-11-20</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
