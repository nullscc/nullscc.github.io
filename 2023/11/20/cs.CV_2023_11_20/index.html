
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-20 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12024 repo_url: None paper_authors: Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-20">
<meta property="og:url" content="https://nullscc.github.io/2023/11/20/cs.CV_2023_11_20/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12024 repo_url: None paper_authors: Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-20T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-21T12:27:52.053Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/20/cs.CV_2023_11_20/" class="article-date">
  <time datetime="2023-11-20T13:00:00.000Z" itemprop="datePublished">2023-11-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-20
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="PF-LRM-Pose-Free-Large-Reconstruction-Model-for-Joint-Pose-and-Shape-Prediction"><a href="#PF-LRM-Pose-Free-Large-Reconstruction-Model-for-Joint-Pose-and-Shape-Prediction" class="headerlink" title="PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction"></a>PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12024">http://arxiv.org/abs/2311.12024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, Kai Zhang</li>
<li>for:  reconstruction of 3D objects from a few unposed images, and estimation of relative camera poses</li>
<li>methods: utilizes self-attention blocks to exchange information between 3D object tokens and 2D image tokens, predicts coarse point cloud for each view, and uses a differentiable Perspective-n-Point (PnP) solver to obtain camera poses</li>
<li>results: outperforms baseline methods by a large margin in terms of pose prediction accuracy and 3D reconstruction quality on various unseen evaluation datasets, and demonstrates applicability in downstream text&#x2F;image-to-3D task with fast feed-forward inference.<details>
<summary>Abstract</summary>
We propose a Pose-Free Large Reconstruction Model (PF-LRM) for reconstructing a 3D object from a few unposed images even with little visual overlap, while simultaneously estimating the relative camera poses in ~1.3 seconds on a single A100 GPU. PF-LRM is a highly scalable method utilizing the self-attention blocks to exchange information between 3D object tokens and 2D image tokens; we predict a coarse point cloud for each view, and then use a differentiable Perspective-n-Point (PnP) solver to obtain camera poses. When trained on a huge amount of multi-view posed data of ~1M objects, PF-LRM shows strong cross-dataset generalization ability, and outperforms baseline methods by a large margin in terms of pose prediction accuracy and 3D reconstruction quality on various unseen evaluation datasets. We also demonstrate our model's applicability in downstream text/image-to-3D task with fast feed-forward inference. Our project website is at: https://totoro97.github.io/pf-lrm .
</details>
<details>
<summary>摘要</summary>
我们提出了一种无pose大型重建模型（PF-LRM），可以从几张不带pose的图像中重建3D объек，即使图像之间视觉重叠少，并同时估算摄像头位置，在单个A100 GPU上大约1.3秒内完成。PF-LRM是一种高度可扩展的方法，利用自注意块来在3D对象卡通和2D图像卡通之间交换信息；我们预测每个视角的粗略点云，然后使用可微分的Perspective-n-Point（PnP）解决方法来获取摄像头位置。当在大量多视图卷积数据中训练时，PF-LRM显示出了广泛的跨数据集通用能力，并在不同评估数据集上大幅超越基eline方法的pose预测精度和3D重建质量。此外，我们还证明了我们的模型在文本/图像到3D任务中的应用性，通过快速的前向推理。我们的项目网站是：https://totoro97.github.io/pf-lrm。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-HMR-3D-Human-Mesh-Recovery-from-LiDAR"><a href="#LiDAR-HMR-3D-Human-Mesh-Recovery-from-LiDAR" class="headerlink" title="LiDAR-HMR: 3D Human Mesh Recovery from LiDAR"></a>LiDAR-HMR: 3D Human Mesh Recovery from LiDAR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11971">http://arxiv.org/abs/2311.11971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soullessrobot/lidar-hmr">https://github.com/soullessrobot/lidar-hmr</a></li>
<li>paper_authors: Bohao Fan, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</li>
<li>for: 这篇论文主要针对的是从稀疏 LiDAR 点云中估算人体三维模型。</li>
<li>methods: 该论文提出了一种有效的稀疏到dense重建方案，通过估算人体3D姿势并慢慢地重建人体体姿。为了更好地利用点云的3D结构信息，该方法使用了顺序图变换器（graphormer）引入点云特征。</li>
<li>results: 实验结果表明，该方法在三个公共可用的数据库上具有有效性。代码：<a target="_blank" rel="noopener" href="https://github.com/soullessrobot/LiDAR-HMR/%E3%80%82">https://github.com/soullessrobot/LiDAR-HMR/。</a><details>
<summary>Abstract</summary>
In recent years, point cloud perception tasks have been garnering increasing attention. This paper presents the first attempt to estimate 3D human body mesh from sparse LiDAR point clouds. We found that the major challenge in estimating human pose and mesh from point clouds lies in the sparsity, noise, and incompletion of LiDAR point clouds. Facing these challenges, we propose an effective sparse-to-dense reconstruction scheme to reconstruct 3D human mesh. This involves estimating a sparse representation of a human (3D human pose) and gradually reconstructing the body mesh. To better leverage the 3D structural information of point clouds, we employ a cascaded graph transformer (graphormer) to introduce point cloud features during sparse-to-dense reconstruction. Experimental results on three publicly available databases demonstrate the effectiveness of the proposed approach. Code: https://github.com/soullessrobot/LiDAR-HMR/
</details>
<details>
<summary>摘要</summary>
Recently, point cloud perception tasks have been gaining increasing attention. This paper presents the first attempt to estimate 3D human body mesh from sparse LiDAR point clouds. We found that the major challenge in estimating human pose and mesh from point clouds lies in the sparsity, noise, and incompleteness of LiDAR point clouds. To overcome these challenges, we propose an effective sparse-to-dense reconstruction scheme to reconstruct 3D human mesh. This involves estimating a sparse representation of a human (3D human pose) and gradually reconstructing the body mesh. To better leverage the 3D structural information of point clouds, we employ a cascaded graph transformer (graphormer) to introduce point cloud features during sparse-to-dense reconstruction. Experimental results on three publicly available databases demonstrate the effectiveness of the proposed approach. Code: https://github.com/soullessrobot/LiDAR-HMR/Here's the translation in Traditional Chinese:近年来，点云识别任务已经获得了增加的注意力。本文提出了从简练的LiDAR点云中估计3D人体组件的第一个尝试。我们发现，从点云中估计人姿和组件的主要挑战是点云的简练、噪声和不完整性。面对这些挑战，我们提出了一个有效的简练到简练重建方案，以重建3D人体组件。这 involves 估计人体的3D姿势（3D人体组件），并逐渐重建人体组件。为了更好地利用点云的3D结构信息，我们使用了堆叠 graphs transformer（graphormer）引入点云特征 during sparse-to-dense重建。实验结果显示，我们的方法具有优秀的效果。代码：https://github.com/soullessrobot/LiDAR-HMR/
</details></li>
</ul>
<hr>
<h2 id="SA-Med2D-20M-Dataset-Segment-Anything-in-2D-Medical-Imaging-with-20-Million-masks"><a href="#SA-Med2D-20M-Dataset-Segment-Anything-in-2D-Medical-Imaging-with-20-Million-masks" class="headerlink" title="SA-Med2D-20M Dataset: Segment Anything in 2D Medical Imaging with 20 Million masks"></a>SA-Med2D-20M Dataset: Segment Anything in 2D Medical Imaging with 20 Million masks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11969">http://arxiv.org/abs/2311.11969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/SAM-Med2D">https://github.com/OpenGVLab/SAM-Med2D</a></li>
<li>paper_authors: Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Min Zhu, Shaoting Zhang, Junjun He, Yu Qiao</li>
<li>for: 这篇论文的目的是为了将自然语言像素化模型（Segment Anything Model，SAM）应用到医疗影像分类中，并且将医疗知识 integrate 到 SAM 中。</li>
<li>methods: 这篇论文使用了大量的医疗影像数据集来训练 SAM，并且将这些数据集分为不同的类别，以便更好地应用到医疗影像分类中。</li>
<li>results: 这篇论文所提出的 SA-Med2D-20M 数据集，包含了 4.6 百万帧医疗影像和 19.7 百万帧相应的面精度数据，覆盖了大部分的身体部位和具有很大的多样性。<details>
<summary>Abstract</summary>
Segment Anything Model (SAM) has achieved impressive results for natural image segmentation with input prompts such as points and bounding boxes. Its success largely owes to massive labeled training data. However, directly applying SAM to medical image segmentation cannot perform well because SAM lacks medical knowledge -- it does not use medical images for training. To incorporate medical knowledge into SAM, we introduce SA-Med2D-20M, a large-scale segmentation dataset of 2D medical images built upon numerous public and private datasets. It consists of 4.6 million 2D medical images and 19.7 million corresponding masks, covering almost the whole body and showing significant diversity. This paper describes all the datasets collected in SA-Med2D-20M and details how to process these datasets. Furthermore, comprehensive statistics of SA-Med2D-20M are presented to facilitate the better use of our dataset, which can help the researchers build medical vision foundation models or apply their models to downstream medical applications. We hope that the large scale and diversity of SA-Med2D-20M can be leveraged to develop medical artificial intelligence for enhancing diagnosis, medical image analysis, knowledge sharing, and education. The data with the redistribution license is publicly available at https://github.com/OpenGVLab/SAM-Med2D.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 已经在自然图像分割领域取得了很好的结果，使用输入提示如点和 bounding box。它的成功主要归功于大规模的标注训练数据。然而，直接将 SAM 应用于医疗图像分割不能取得好成绩，因为 SAM 缺乏医疗知识 -- 它没有使用医疗图像进行训练。为了将医疗知识integrated into SAM，我们介绍了 SA-Med2D-20M，一个基于多个公共和私人数据集的大规模分割数据集。它包含460万个2D医疗图像和1970万对应的mask，覆盖了大多数的身体部位，显示了considerable diversity。本文描述了 SA-Med2D-20M 中收集的所有数据集，以及如何处理这些数据集。此外，我们还提供了 SA-Med2D-20M 的完整统计数据，以便更好地使用我们的数据集，帮助研究人员建立医学视觉基础模型或将其模型应用到下游医疗应用。我们希望通过 SA-Med2D-20M 的大规模和多样性，为医学人工智能的发展做出贡献，包括诊断、医疗图像分析、知识共享和教育。数据集的红色分布授权许可可以在 https://github.com/OpenGVLab/SAM-Med2D 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="What-Can-AutoML-Do-For-Continual-Learning"><a href="#What-Can-AutoML-Do-For-Continual-Learning" class="headerlink" title="What Can AutoML Do For Continual Learning?"></a>What Can AutoML Do For Continual Learning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11963">http://arxiv.org/abs/2311.11963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Kilickaya, Joaquin Vanschoren</li>
<li>for: This paper explores the potential of AutoML for incremental (continual) learning, with the goal of encouraging more research in this area.</li>
<li>methods: The paper does not propose a new method, but instead highlights three key areas of research that can contribute to making incremental learners more dynamic, including applying AutoML methods in novel ways and addressing new challenges for AutoML research.</li>
<li>results: The paper does not present any specific results, but rather poses the question “What can AutoML do for incremental learning?” and outlines the potential benefits of using AutoML for this purpose.<details>
<summary>Abstract</summary>
This position paper outlines the potential of AutoML for incremental (continual) learning to encourage more research in this direction. Incremental learning involves incorporating new data from a stream of tasks and distributions to learn enhanced deep representations and adapt better to new tasks. However, a significant limitation of incremental learners is that most current techniques freeze the backbone architecture, hyperparameters, and the order & structure of the learning tasks throughout the learning and adaptation process. We strongly believe that AutoML offers promising solutions to address these limitations, enabling incremental learning to adapt to more diverse real-world tasks. Therefore, instead of directly proposing a new method, this paper takes a step back by posing the question: "What can AutoML do for incremental learning?" We outline three key areas of research that can contribute to making incremental learners more dynamic, highlighting concrete opportunities to apply AutoML methods in novel ways as well as entirely new challenges for AutoML research.
</details>
<details>
<summary>摘要</summary>
Instead of proposing a new method, this paper takes a step back and asks "What can AutoML do for incremental learning?" We identify three key areas of research that can contribute to making incremental learners more dynamic, highlighting concrete opportunities to apply AutoML methods in novel ways as well as entirely new challenges for AutoML research. These areas include:1. Dynamic architecture and hyperparameter optimization: Developing AutoML methods that can adaptively adjust the backbone architecture and hyperparameters of the model during incremental learning, allowing the model to better adapt to new tasks and data.2. Task-aware learning: Developing AutoML methods that can learn task-aware representations, which can be used to adapt the model to new tasks and data while preserving the knowledge gained from previous tasks.3. Multi-task learning: Developing AutoML methods that can handle multiple tasks simultaneously, allowing the model to learn shared representations across tasks and adapt to new tasks more quickly.By exploring these areas of research, we hope to encourage more research in the field of AutoML and incremental learning, and to ultimately develop more dynamic and adaptive machine learning models that can better handle the challenges of real-world applications.
</details></li>
</ul>
<hr>
<h2 id="An-Image-is-Worth-Multiple-Words-Multi-attribute-Inversion-for-Constrained-Text-to-Image-Synthesis"><a href="#An-Image-is-Worth-Multiple-Words-Multi-attribute-Inversion-for-Constrained-Text-to-Image-Synthesis" class="headerlink" title="An Image is Worth Multiple Words: Multi-attribute Inversion for Constrained Text-to-Image Synthesis"></a>An Image is Worth Multiple Words: Multi-attribute Inversion for Constrained Text-to-Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11919">http://arxiv.org/abs/2311.11919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aishwarya Agarwal, Srikrishna Karanam, Tripti Shukla, Balaji Vasan Srinivasan</li>
<li>for: 本研究的目标是使用一张参考图像提取多个特征（例如颜色、对象、布局、风格），并生成新样本以这些特征为condition。</li>
<li>methods: 本研究使用了一种新的多特征倒拍算法（MATTE），它在时间维度和DDPM模型层维度都学习多个嵌入，以提高特征分离。</li>
<li>results: 研究发现，通过使用MATTE算法和增强分离训练loss，可以生成高质量的多特征样本，并且分离了多个特征。<details>
<summary>Abstract</summary>
We consider the problem of constraining diffusion model outputs with a user-supplied reference image. Our key objective is to extract multiple attributes (e.g., color, object, layout, style) from this single reference image, and then generate new samples with them. One line of existing work proposes to invert the reference images into a single textual conditioning vector, enabling generation of new samples with this learned token. These methods, however, do not learn multiple tokens that are necessary to condition model outputs on the multiple attributes noted above. Another line of techniques expand the inversion space to learn multiple embeddings but they do this only along the layer dimension (e.g., one per layer of the DDPM model) or the timestep dimension (one for a set of timesteps in the denoising process), leading to suboptimal attribute disentanglement. To address the aforementioned gaps, the first contribution of this paper is an extensive analysis to determine which attributes are captured in which dimension of the denoising process. As noted above, we consider both the time-step dimension (in reverse denoising) as well as the DDPM model layer dimension. We observe that often a subset of these attributes are captured in the same set of model layers and/or across same denoising timesteps. For instance, color and style are captured across same U-Net layers, whereas layout and color are captured across same timestep stages. Consequently, an inversion process that is designed only for the time-step dimension or the layer dimension is insufficient to disentangle all attributes. This leads to our second contribution where we design a new multi-attribute inversion algorithm, MATTE, with associated disentanglement-enhancing regularization losses, that operates across both dimensions and explicitly leads to four disentangled tokens (color, style, layout, and object).
</details>
<details>
<summary>摘要</summary>
我们考虑到将散射模型输出与用户提供的参考图像进行约束的问题。我们的主要目标是从这个单一的参考图像中提取多个属性（例如颜色、物体、布局和风格），然后产生新的样本，并且将这些属性分类到不同的数据层次上。现有的方法包括将参考图像反转为单一的文本条件vector，以便产生新的样本。然而，这些方法不会学习多个条件实体，以便对模型输出进行多个属性的约束。另一些方法则是扩展对应空间，以学习多个嵌入，但是这些方法只是在层次（例如 DDPM 模型层次）或时间步（例如降噪过程中的时间步）上进行嵌入，从而导致属性分类不够佳。为了解决这些问题，我们的第一个贡献是对对应过程中哪些属性被捕捉的进行全面的分析。我们考虑了时间步 Dimension（在逆降噪过程中）以及 DDPM 模型层次 Dimension。我们发现，常常一些属性会在同一些层次和/或同一些时间步中被捕捉。例如，颜色和风格会在同一个 U-Net 层中被捕捉，而布局和颜色则会在同一个时间步中被捕捉。因此，对于时间步 Dimension 或 DDPM 模型层次的单一对应过程不足以将所有属性分类开来。这导致我们的第二个贡献，即设计了一个新的多属性对应算法，叫做 MATTE，还有相应的增强分类损失。MATTE 可以在时间步 Dimension 和 DDPM 模型层次上运行，并且可以让样本产生器产生四个分类的样本，即颜色、风格、布局和物体。
</details></li>
</ul>
<hr>
<h2 id="Identifying-the-Defective-Detecting-Damaged-Grains-for-Cereal-Appearance-Inspection"><a href="#Identifying-the-Defective-Detecting-Damaged-Grains-for-Cereal-Appearance-Inspection" class="headerlink" title="Identifying the Defective: Detecting Damaged Grains for Cereal Appearance Inspection"></a>Identifying the Defective: Detecting Damaged Grains for Cereal Appearance Inspection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11901">http://arxiv.org/abs/2311.11901</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hellodfan/ai4graininsp">https://github.com/hellodfan/ai4graininsp</a></li>
<li>paper_authors: Lei Fan, Yiwen Ding, Dongdong Fan, Yong Wu, Maurice Pagnucco, Yang Song</li>
<li>for: 这篇论文旨在开发一个自动化的谷物质量检查系统，以提高智能农业的效率。</li>
<li>methods: 该论文使用了一种异常检测模型（AD-GAI），通过分析谷物ernel的特征来识别异常样本。</li>
<li>results: 实验结果显示，AD-GAI模型在比较高级的异常检测方法的比较中表现出了较好的性能，而自动化的谷物检查系统可以在20倍以上的速度提高 inspect efficiency。<details>
<summary>Abstract</summary>
Cereal grain plays a crucial role in the human diet as a major source of essential nutrients. Grain Appearance Inspection (GAI) serves as an essential process to determine grain quality and facilitate grain circulation and processing. However, GAI is routinely performed manually by inspectors with cumbersome procedures, which poses a significant bottleneck in smart agriculture.   In this paper, we endeavor to develop an automated GAI system:AI4GrainInsp. By analyzing the distinctive characteristics of grain kernels, we formulate GAI as a ubiquitous problem: Anomaly Detection (AD), in which healthy and edible kernels are considered normal samples while damaged grains or unknown objects are regarded as anomalies. We further propose an AD model, called AD-GAI, which is trained using only normal samples yet can identify anomalies during inference. Moreover, we customize a prototype device for data acquisition and create a large-scale dataset including 220K high-quality images of wheat and maize kernels. Through extensive experiments, AD-GAI achieves considerable performance in comparison with advanced AD methods, and AI4GrainInsp has highly consistent performance compared to human experts and excels at inspection efficiency over 20x speedup. The dataset, code and models will be released at https://github.com/hellodfan/AI4GrainInsp.
</details>
<details>
<summary>摘要</summary>
粮食作物在人类饮食中扮演着重要角色，是重要营养素的主要来源。 however， Grain Appearance Inspection（GAI）， which is essential to determine grain quality and facilitate grain circulation and processing, is often performed manually by inspectors with cumbersome procedures, which poses a significant bottleneck in smart agriculture. 在这篇论文中，我们努力开发一个自动化GAI系统：AI4GrainInsp。我们将GAI视为一个通用问题：Anomaly Detection（AD），健康和食用粮粒被视为正常样本，而受损或未知物体则被视为异常。我们还提出了一个AD模型， called AD-GAI， 训练用normal样本，但可以在推导过程中识别异常。此外，我们自订了一个原型设备用于数据收集，并创建了包含220,000张高品质小麦和玉米粒的大规模数据集。通过广泛的实验， AD-GAI在与进步AD方法相比表现出色，而AI4GrainInsp具有人工专家比较高的一致性，并在检查效率方面大幅提高，对20倍以上的速度优化。数据、代码和模型将在https://github.com/hellodfan/AI4GrainInsp中发布。
</details></li>
</ul>
<hr>
<h2 id="SniffyArt-The-Dataset-of-Smelling-Persons"><a href="#SniffyArt-The-Dataset-of-Smelling-Persons" class="headerlink" title="SniffyArt: The Dataset of Smelling Persons"></a>SniffyArt: The Dataset of Smelling Persons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11888">http://arxiv.org/abs/2311.11888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Zinnen, Azhar Hussian, Hang Tran, Prathmesh Madhu, Andreas Maier, Vincent Christlein</li>
<li>for: 这篇论文旨在为探索过去的味觉艺术作品中的味觉姿势提供数据集，以便开发混合类型的识别方法。</li>
<li>methods: 这篇论文使用了一个名为SniffyArt的数据集，该数据集包含1941名个体，分别出现在441幅历史艺术作品中。每个人都有紧靠的盒子 bounding box，17个姿势关键点和一个姿势标签。通过这些注释，数据集可以帮助开发混合类型的识别方法。</li>
<li>results: 这篇论文还提供了一个基eline分析，评估了一些代表性的算法在检测、关键点估计和分类任务中的性能，从而探讨将关键点估计与味觉姿势识别结合使用来提高人姿和味觉维度分析的前景。<details>
<summary>Abstract</summary>
Smell gestures play a crucial role in the investigation of past smells in the visual arts yet their automated recognition poses significant challenges. This paper introduces the SniffyArt dataset, consisting of 1941 individuals represented in 441 historical artworks. Each person is annotated with a tightly fitting bounding box, 17 pose keypoints, and a gesture label. By integrating these annotations, the dataset enables the development of hybrid classification approaches for smell gesture recognition. The datasets high-quality human pose estimation keypoints are achieved through the merging of five separate sets of keypoint annotations per person. The paper also presents a baseline analysis, evaluating the performance of representative algorithms for detection, keypoint estimation, and classification tasks, showcasing the potential of combining keypoint estimation with smell gesture classification. The SniffyArt dataset lays a solid foundation for future research and the exploration of multi-task approaches leveraging pose keypoints and person boxes to advance human gesture and olfactory dimension analysis in historical artworks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统艺术作品中的嗅觉姿势扮演着重要的角色，但自动识别嗅觉姿势具有 significante挑战。这篇论文介绍了SniffyArt数据集，包含1941名人物在441幅历史艺术作品中的表现。每名人物被标注为紧靠的 bounding box、17个姿势关键点和姿势标签。通过这些标注，数据集可以激发 hybrid 分类方法的开发，以提高嗅觉姿势识别的精度。数据集的高质量人 pose 关键点是通过每名人物的五个独立的关键点标注集来实现的。论文还提供了基线分析，评估了代表性算法的检测、关键点估计和分类任务的性能，展示了将关键点估计与嗅觉姿势分类结合使用的潜在优势。SniffyArt数据集为未来研究的固定基础，探索将人体姿势和嗅觉维度分析结合起来的多任务方法，以提高人体姿势和嗅觉分析的精度。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Faces-MTF-Data-Set-A-Legally-and-Ethically-Compliant-Collection-of-Face-Images-for-Various-Classification-Tasks"><a href="#Multi-Task-Faces-MTF-Data-Set-A-Legally-and-Ethically-Compliant-Collection-of-Face-Images-for-Various-Classification-Tasks" class="headerlink" title="Multi-Task Faces (MTF) Data Set: A Legally and Ethically Compliant Collection of Face Images for Various Classification Tasks"></a>Multi-Task Faces (MTF) Data Set: A Legally and Ethically Compliant Collection of Face Images for Various Classification Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11882">http://arxiv.org/abs/2311.11882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ramihaf/mtf_data_set">https://github.com/ramihaf/mtf_data_set</a></li>
<li>paper_authors: Rami Haffar, David Sánchez, Josep Domingo-Ferrer</li>
<li>for: 这个论文是为了提供一个多任务人脸数据集（MTF），用于各种人脸分类任务，包括人脸识别、年龄、性别和种族分类。</li>
<li>methods: 这个论文使用了公共可用的影星图片，并且严格遵循了版权法规，以获取和处理数据集。</li>
<li>results: 这个论文提供了MTF数据集的详细描述，并评估了五种深度学习（DL）模型在MTF数据集上的性能，包括人脸识别、年龄、性别和种族分类等任务。同时，该论文还比较了DL模型在加工后的MTF数据集和网络上抓取的原始数据上的性能。<details>
<summary>Abstract</summary>
Human facial data hold tremendous potential to address a variety of classification problems, including face recognition, age estimation, gender identification, emotion analysis, and race classification. However, recent privacy regulations, such as the EU General Data Protection Regulation and others, have restricted the ways in which human images may be collected and used for research. As a result, several previously published data sets containing human faces have been removed from the internet due to inadequate data collection methods that failed to meet privacy regulations. Data sets consisting of synthetic data have been proposed as an alternative, but they fall short of accurately representing the real data distribution. On the other hand, most available data sets are labeled for just a single task, which limits their applicability. To address these issues, we present the Multi-Task Faces (MTF) image data set, a meticulously curated collection of face images designed for various classification tasks, including face recognition, as well as race, gender, and age classification. The MTF data set has been ethically gathered by leveraging publicly available images of celebrities and strictly adhering to copyright regulations. In this paper, we present this data set and provide detailed descriptions of the followed data collection and processing procedures. Furthermore, we evaluate the performance of five deep learning (DL) models on the MTF data set across the aforementioned classification tasks. Additionally, we compare the performance of DL models over the processed MTF data and over raw data crawled from the internet. The reported results constitute a baseline for further research employing these data. The MTF data set can be accessed through the following link (please cite the present paper if you use the data set): https://github.com/RamiHaf/MTF_data_set
</details>
<details>
<summary>摘要</summary>
人脸数据具有巨大的潜力，可以解决多种分类问题，包括人脸识别、年龄估计、性别确定、情感分析和种族分类。然而，最新的隐私法规，如欧盟一般数据保护条例等，限制了人脸图像的收集和使用方式。因此，一些在互联网上已经被下架的人脸数据集被移除，因为其收集方法不符合隐私法规。 Synthetic数据集被提议作为替代方案，但它们无法准确表现实际数据分布。另一方面，大多数可用的数据集仅针对单一任务进行标注，限制了它们的可 reuse。为解决这些问题，我们提出了多任务人脸（MTF）图像数据集，包含人脸图像，用于多种分类任务，包括人脸识别、种族、性别和年龄分类。MTF数据集通过公开可用的明星照片进行了优质的收集，并且严格遵循了版权法规。在这篇论文中，我们介绍了MTF数据集，并提供了收集和处理数据的详细描述。此外，我们使用五种深度学习（DL）模型对MTF数据集进行了评估，并对raw数据和已经处理的MTF数据进行了比较。报告的结果构成了基eline для后续研究。MTF数据集可以通过以下链接获取：https://github.com/RamiHaf/MTF_data_set。请在使用数据集时参考本文。
</details></li>
</ul>
<hr>
<h2 id="VLM-Eval-A-General-Evaluation-on-Video-Large-Language-Models"><a href="#VLM-Eval-A-General-Evaluation-on-Video-Large-Language-Models" class="headerlink" title="VLM-Eval: A General Evaluation on Video Large Language Models"></a>VLM-Eval: A General Evaluation on Video Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11865">http://arxiv.org/abs/2311.11865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuailin Li, Yuang Zhang, Yucheng Zhao, Qiuyue Wang, Fan Jia, Yingfei Liu, Tiancai Wang</li>
<li>for: 这篇论文主要用于评估视频大语言模型（LLMs）的全面评估，包括captioning、问答、检索和动作识别等多个视频任务。</li>
<li>methods: 该论文提出了一种简单的基准：Video-LLaVA，使用单个线性投影，并超越现有的视频LLMs。此外，论文还使用GPT基于的评估方法，可以评估响应质量在多个方面匹配人类性能。</li>
<li>results: 论文的实验结果表明， Video-LLaVA 可以在多个视频任务上达到或超越现有的视频LLMs，而且可以在驾驶场景中使用只需要数百个视频教程对象进行微调，并且获得了鼓舞人的认知和逻辑能力。<details>
<summary>Abstract</summary>
Despite the rapid development of video Large Language Models (LLMs), a comprehensive evaluation is still absent. In this paper, we introduce a unified evaluation that encompasses multiple video tasks, including captioning, question and answering, retrieval, and action recognition. In addition to conventional metrics, we showcase how GPT-based evaluation can match human-like performance in assessing response quality across multiple aspects. We propose a simple baseline: Video-LLaVA, which uses a single linear projection and outperforms existing video LLMs. Finally, we evaluate video LLMs beyond academic datasets, which show encouraging recognition and reasoning capabilities in driving scenarios with only hundreds of video-instruction pairs for fine-tuning. We hope our work can serve as a unified evaluation for video LLMs, and help expand more practical scenarios. The evaluation code will be available soon.
</details>
<details>
<summary>摘要</summary>
尽管视频大语言模型（LLM）的快速发展，但是一个全面的评估仍然缺失。在这篇论文中，我们介绍了一种涵盖多个视频任务的统一评估，包括标题、问答、检索和动作识别。除了传统的指标外，我们示出了如何基于GPT的评估可以匹配人类类似的评估质量在多个方面。我们提出了一个简单的基线：视频-LLaVA，它使用单一的线性投影，并超越了现有的视频LLM。最后，我们评估了视频LLM在外部数据集上的性能，显示出在驾驶场景中只需要百分之几个视频教程对话来进行微调，可以获得鼓舞人的识别和逻辑能力。我们希望我们的工作可以为视频LLM提供一个统一的评估，并帮助扩展更多实用场景。评估代码将很快地提供。
</details></li>
</ul>
<hr>
<h2 id="GP-NeRF-Generalized-Perception-NeRF-for-Context-Aware-3D-Scene-Understanding"><a href="#GP-NeRF-Generalized-Perception-NeRF-for-Context-Aware-3D-Scene-Understanding" class="headerlink" title="GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding"></a>GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11863">http://arxiv.org/abs/2311.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Li, Dingwen Zhang, Yalun Dai, Nian Liu, Lechao Cheng, Jingfeng Li, Jingdong Wang, Junwei Han</li>
<li>for: 提高3D场景理解的场景识别和表示能力</li>
<li>methods: 使用 transformers 对光照和Semantic embedding字段进行汇集，并提出两种自我训练机制来提高Semantic embedding字段的分辨率和准确性</li>
<li>results: 在 semantic 和实例 segmentation 任务上进行比较，与 SOTA 方法相比，OUR 方法提高了6.94%、11.76% 和8.47% 的性能Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文旨在提高3D场景理解的场景识别和表示能力，通过将广泛使用的 segmentation 模型和NeRF 技术结合起来，建立一个通用的框架。</li>
<li>methods: 我们使用 transformers 对光照和Semantic embedding字段进行汇集，并提出两种自我训练机制来提高Semantic embedding字段的分辨率和准确性。</li>
<li>results: 我们在 semantic 和实例 segmentation 任务上进行比较，与 SOTA 方法相比，OUR 方法提高了6.94%、11.76% 和8.47% 的性能。<details>
<summary>Abstract</summary>
Applying NeRF to downstream perception tasks for scene understanding and representation is becoming increasingly popular. Most existing methods treat semantic prediction as an additional rendering task, \textit{i.e.}, the "label rendering" task, to build semantic NeRFs. However, by rendering semantic/instance labels per pixel without considering the contextual information of the rendered image, these methods usually suffer from unclear boundary segmentation and abnormal segmentation of pixels within an object. To solve this problem, we propose Generalized Perception NeRF (GP-NeRF), a novel pipeline that makes the widely used segmentation model and NeRF work compatibly under a unified framework, for facilitating context-aware 3D scene perception. To accomplish this goal, we introduce transformers to aggregate radiance as well as semantic embedding fields jointly for novel views and facilitate the joint volumetric rendering of both fields. In addition, we propose two self-distillation mechanisms, i.e., the Semantic Distill Loss and the Depth-Guided Semantic Distill Loss, to enhance the discrimination and quality of the semantic field and the maintenance of geometric consistency. In evaluation, we conduct experimental comparisons under two perception tasks (\textit{i.e.} semantic and instance segmentation) using both synthetic and real-world datasets. Notably, our method outperforms SOTA approaches by 6.94\%, 11.76\%, and 8.47\% on generalized semantic segmentation, finetuning semantic segmentation, and instance segmentation, respectively.
</details>
<details>
<summary>摘要</summary>
使用NeRF进行下游ognition任务，如场景理解和表示，越来越受欢迎。现有的方法通常将semantic prediction视为额外的渲染任务，即"标签渲染"任务，以建立semantic NeRFs。然而，由于不考虑渲染图像上的上下文信息，这些方法通常会出现模糊的边界分割和对象内部像素的异常分割问题。为解决这个问题，我们提出了通用认知NeRF（GP-NeRF），一种新的管道，可以让通用segmentation模型和NeRF在一个统一的框架下工作，以便实现上下文意识的3D场景识别。为实现这个目标，我们引入了transformer来聚合辐射和semantic嵌入场景的字段，以便在新视图下进行共同渲染。此外，我们还提出了两种自适应机制，即semantic Distill Loss和depth-guided Semantic Distill Loss，以提高semantic场景的分割和质量，并保持Geometric的一致性。在evaluation中，我们对两个ognition任务（即semantic和instance segmentation）进行了比较，使用了both synthetic和实际数据。可见，我们的方法在Generalized Semantic Segmentation、fine-tuning Semantic Segmentation和Instance Segmentation中都有显著的优势，相比SOTA方法，提高了6.94%、11.76%和8.47%。
</details></li>
</ul>
<hr>
<h2 id="LION-Empowering-Multimodal-Large-Language-Model-with-Dual-Level-Visual-Knowledge"><a href="#LION-Empowering-Multimodal-Large-Language-Model-with-Dual-Level-Visual-Knowledge" class="headerlink" title="LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge"></a>LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11860">http://arxiv.org/abs/2311.11860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, Liqiang Nie<br>for: 这个论文的目的是提高多模态大语言模型（MLLM）的性能，使其能够更好地理解和利用多 modal 信号。methods: 该论文使用了两级层次的视觉知识扩充策略，包括进步的细化空间意识视觉知识的包含，以及软提示高级别semantic 视觉证据。results: 该论文的实验结果表明，在多个多模态benchmark上，提高了5%的准确率（VSRevaluation和TextCaps over InstructBLIP）和3%的CIDEr（RefCOCOg over Kosmos-2）。<details>
<summary>Abstract</summary>
Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability to perceive and understand multi-modal signals. However, most of the existing MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text pairs, leading to insufficient extraction and reasoning of visual knowledge. To address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model (LION), which empowers the MLLM by injecting visual knowledge in two levels. 1) Progressive incorporation of fine-grained spatial-aware visual knowledge. We design a vision aggregator cooperated with region-level vision-language (VL) tasks to incorporate fine-grained spatial-aware visual knowledge into the MLLM. To alleviate the conflict between image-level and region-level VL tasks during incorporation, we devise a dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This progressive incorporation scheme contributes to the mutual promotion between these two kinds of VL tasks. 2) Soft prompting of high-level semantic visual evidence. We facilitate the MLLM with high-level semantic visual evidence by leveraging diverse image tags. To mitigate the potential influence caused by imperfect predicted tags, we propose a soft prompting method by embedding a learnable token into the tailored text instruction. Comprehensive experiments on several multi-modal benchmarks demonstrate the superiority of our model (e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over InstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).
</details>
<details>
<summary>摘要</summary>
多modal大语言模型（MLLMs）已经授予大语言模型（LLMs）能够感知和理解多modal信号。然而，现有的大多数MLLMs主要采用视觉编码器预训练在粗粒aligned的图像文本对中，导致视觉知识的不足提取和分析。为解决这个问题，我们设计了一种双级视知能强化多modal大语言模型（LION），它使得MLLM具有更高水平的视觉知识。1. 进展性的细化空间意识视觉知识整合。我们设计了一种视觉聚合器，与区域级vision-language（VL）任务结合，以便在MLLM中整合细化空间意识视觉知识。为了解决图像级和区域级VL任务之间的冲突，我们提出了一种适应器阶段 wise 准确调整策略。这种进程性整合方案为这两种VL任务互相促进。2. 软引导高级别semantic视觉证据。我们为MLLM提供高级别semantic视觉证据，利用多种图像标签。为了减轻可能由预测错误引起的影响，我们提出了一种软引导方法，通过在修改的文本指令中嵌入学习token来实现。我们在多个多modal bencmarks上进行了广泛的实验，并证明了我们的模型在VSRCIDEr等方面的性能有5%提升和3%提升。
</details></li>
</ul>
<hr>
<h2 id="FATURA-A-Multi-Layout-Invoice-Image-Dataset-for-Document-Analysis-and-Understanding"><a href="#FATURA-A-Multi-Layout-Invoice-Image-Dataset-for-Document-Analysis-and-Understanding" class="headerlink" title="FATURA: A Multi-Layout Invoice Image Dataset for Document Analysis and Understanding"></a>FATURA: A Multi-Layout Invoice Image Dataset for Document Analysis and Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11856">http://arxiv.org/abs/2311.11856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Limam, Marwa Dhiaf, Yousri Kessentini</li>
<li>for: 该论文主要用于提供一个大规模、多格式的文档分析和理解 dataset，帮助研究人员在文档分析和理解方面进行更好的训练和测试。</li>
<li>methods: 该论文使用了多种文档分析和理解技术，包括文本内容分析和精确 bounding-box 笔迹标注，以便在不同的文档中分别标识不同的元素。</li>
<li>results: 该论文提供了一个名为 FATURA 的大型开放数据集，包含 $10,000$ 份多格式的票据文档图像，以及对这些图像的详细的标注和评估结果。这个数据集是研究人员在文档分析和理解领域进行训练和测试的重要资源。<details>
<summary>Abstract</summary>
Document analysis and understanding models often require extensive annotated data to be trained. However, various document-related tasks extend beyond mere text transcription, requiring both textual content and precise bounding-box annotations to identify different document elements. Collecting such data becomes particularly challenging, especially in the context of invoices, where privacy concerns add an additional layer of complexity. In this paper, we introduce FATURA, a pivotal resource for researchers in the field of document analysis and understanding. FATURA is a highly diverse dataset featuring multi-layout, annotated invoice document images. Comprising $10,000$ invoices with $50$ distinct layouts, it represents the largest openly accessible image dataset of invoice documents known to date. We also provide comprehensive benchmarks for various document analysis and understanding tasks and conduct experiments under diverse training and evaluation scenarios. The dataset is freely accessible at https://zenodo.org/record/8261508, empowering researchers to advance the field of document analysis and understanding.
</details>
<details>
<summary>摘要</summary>
文档分析和理解模型经常需要大量已经标注数据进行训练。然而，各种文档相关任务超出了简单的文本转写，需要同时包含文本内容和精确的 bounding-box 注释，以识别不同的文档元素。收集这些数据变得特别困难，尤其在帐单上，隐私问题添加了一个额外的层次。在这篇论文中，我们介绍 FATURA，一个重要的研究资源。FATURA 是一个多样化的数据集，包含 $10,000$ 份帐单，每份帐单有 $50$ 种不同的布局。这个数据集是已知的最大的公开访问图像数据集。我们还提供了各种文档分析和理解任务的完整的benchmark，并在多种训练和评估场景下进行了实验。该数据集可以免费下载于 <https://zenodo.org/record/8261508>，为研究人员提供了进一步推动文档分析和理解领域的机会。
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Bioplausible-Neuron-for-Spiking-Neural-Networks-for-Event-Based-Vision"><a href="#Asynchronous-Bioplausible-Neuron-for-Spiking-Neural-Networks-for-Event-Based-Vision" class="headerlink" title="Asynchronous Bioplausible Neuron for Spiking Neural Networks for Event-Based Vision"></a>Asynchronous Bioplausible Neuron for Spiking Neural Networks for Event-Based Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11853">http://arxiv.org/abs/2311.11853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanket Kachole, Hussain Sajwani, Fariborz Baghaei Naeini, Dimitrios Makris, Yahya Zweiri</li>
<li>for: 提高计算机视觉效率和能效性</li>
<li>methods: 使用自适应神经元发射机制维护神经网络的平衡</li>
<li>results: 提高图像分类和 segmentation 性能，保持神经网络的平衡和能效性<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) offer a biologically inspired approach to computer vision that can lead to more efficient processing of visual data with reduced energy consumption. However, maintaining homeostasis within these networks is challenging, as it requires continuous adjustment of neural responses to preserve equilibrium and optimal processing efficiency amidst diverse and often unpredictable input signals. In response to these challenges, we propose the Asynchronous Bioplausible Neuron (ABN), a dynamic spike firing mechanism to auto-adjust the variations in the input signal. Comprehensive evaluation across various datasets demonstrates ABN's enhanced performance in image classification and segmentation, maintenance of neural equilibrium, and energy efficiency.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）提供了生物学发明的计算机视觉方法，可以导致更有效的视觉数据处理，降低能源消耗。然而，保持神经网络中的HOMEOSTASIS具有挑战，因为它需要不断调整神经响应，以维持平衡和最佳处理效率面对多样化和不可预测的输入信号。为解决这些挑战，我们提出了异步生物可能性神经元（ABN），一种动态脉冲发射机制，自动调整输入信号的变化。对于不同的数据集，ABN的综合评估表明它在图像分类和 segmentation 方面具有提高的表现，保持神经平衡，并具有更高的能效性。
</details></li>
</ul>
<hr>
<h2 id="Entangled-View-Epipolar-Information-Aggregation-for-Generalizable-Neural-Radiance-Fields"><a href="#Entangled-View-Epipolar-Information-Aggregation-for-Generalizable-Neural-Radiance-Fields" class="headerlink" title="Entangled View-Epipolar Information Aggregation for Generalizable Neural Radiance Fields"></a>Entangled View-Epipolar Information Aggregation for Generalizable Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11845">http://arxiv.org/abs/2311.11845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatakai1/evenerf">https://github.com/tatakai1/evenerf</a></li>
<li>paper_authors: Zhiyuan Min, Yawei Luo, Wei Yang, Yuesong Wang, Yi Yang</li>
<li>for: 本研究旨在提高NeRF的可重构性，使其能直接生成新场景中的novel view，无需进行场景特定的重新训练。</li>
<li>methods: 我们提出了一种名为EVE-NeRF的Entangled View-Epipolar Information Aggregation方法，它在拼接多视图特征时进行了束合。与现有方法不同的是，EVE-NeRF在拼接过程中具有场景不变的外观和几何约束，从而更好地保证3D表示的普适性。</li>
<li>results: 我们的方法在多种评估场景中达到了状态机器人的性能，比前一代单一维度拼接更加准确地重建了3D场景的几何和外观特征。<details>
<summary>Abstract</summary>
Generalizable NeRF can directly synthesize novel views across new scenes, eliminating the need for scene-specific retraining in vanilla NeRF. A critical enabling factor in these approaches is the extraction of a generalizable 3D representation by aggregating source-view features. In this paper, we propose an Entangled View-Epipolar Information Aggregation method dubbed EVE-NeRF. Different from existing methods that consider cross-view and along-epipolar information independently, EVE-NeRF conducts the view-epipolar feature aggregation in an entangled manner by injecting the scene-invariant appearance continuity and geometry consistency priors to the aggregation process. Our approach effectively mitigates the potential lack of inherent geometric and appearance constraint resulting from one-dimensional interactions, thus further boosting the 3D representation generalizablity. EVE-NeRF attains state-of-the-art performance across various evaluation scenarios. Extensive experiments demonstate that, compared to prevailing single-dimensional aggregation, the entangled network excels in the accuracy of 3D scene geometry and appearance reconstruction.Our project page is https://github.com/tatakai1/EVENeRF.
</details>
<details>
<summary>摘要</summary>
通用的NeRF可以直接生成新场景中的新视图，从而消除需要在普通的NeRF中重新训练场景的需求。一个关键的实现因素在这些方法中是提取一个通用的3D表示。在这篇论文中，我们提出了一种杂合视角和轴向信息的方法，称为EVE-NeRF。与现有的方法不同，EVE-NeRF在杂合过程中包含场景不变的外观连续和几何稳定约束，从而有效地减轻一dimensional交互中可能存在的自然的几何和外观约束不足的问题，并进一步提高3D表示的通用性。EVE-NeRF在多种评价场景中达到了领先的性能。广泛的实验表明，相比传统的单一维度杂合，杂合网络在3D场景几何和外观重建精度方面表现出色。我们的项目页面是https://github.com/tatakai1/EVENeRF。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Multispectral-Segmentation-with-Representations-Generated-by-Reinforcement-Learning"><a href="#Few-shot-Multispectral-Segmentation-with-Representations-Generated-by-Reinforcement-Learning" class="headerlink" title="Few-shot Multispectral Segmentation with Representations Generated by Reinforcement Learning"></a>Few-shot Multispectral Segmentation with Representations Generated by Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11827">http://arxiv.org/abs/2311.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dilith Jayakody, Thanuja Ambegoda</li>
<li>for: 提高几个样本的多spectral图像分割性能</li>
<li>methods: 使用 reinforcement learning 生成表达式，以便在特定类型分割时提供有用的表示</li>
<li>results: 在多spectral图像 datasets 上证明了提高分割性能的效果<details>
<summary>Abstract</summary>
The task of multispectral image segmentation (segmentation of images with numerous channels/bands, each capturing a specific range of wavelengths of electromagnetic radiation) has been previously explored in contexts with large amounts of labeled data. However, these models tend not to generalize well to datasets of smaller size. In this paper, we propose a novel approach for improving few-shot segmentation performance on multispectral images using reinforcement learning to generate representations. These representations are generated in the form of mathematical expressions between channels and are tailored to the specific class being segmented. Our methodology involves training an agent to identify the most informative expressions, updating the dataset using these expressions, and then using the updated dataset to perform segmentation. Due to the limited length of the expressions, the model receives useful representations without any added risk of overfitting. We evaluate the effectiveness of our approach on several multispectral datasets and demonstrate its effectiveness in boosting the performance of segmentation algorithms.
</details>
<details>
<summary>摘要</summary>
多通道图像分割（分割包含多个通道/波段的图像，每个通道/波段捕捉特定范围的电磁辐射波长）的任务已经在大量标注数据的情况下进行过研究。然而，这些模型通常不会在小型数据集上总是适应良好。在这篇论文中，我们提出了一种新的方法，用于在多通道图像上提高几个shot分割性能使用返馈学习生成表示。这些表示是在通道之间的数学表达形式，特制为具体的分割类。我们的方法包括训练一个代理人标识最有用的表达，更新数据集使用这些表达，然后使用更新后的数据集进行分割。由于表达的长度有限，模型可以得到有用的表示而无风险过拟合。我们对多个多通道数据集进行评估，并证明了我们的方法可以提高分割算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Inverse-Rendering-of-Complex-Facade-via-Aerial-3D-Scanning"><a href="#Holistic-Inverse-Rendering-of-Complex-Facade-via-Aerial-3D-Scanning" class="headerlink" title="Holistic Inverse Rendering of Complex Facade via Aerial 3D Scanning"></a>Holistic Inverse Rendering of Complex Facade via Aerial 3D Scanning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11825">http://arxiv.org/abs/2311.11825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Xie, Rengan Xie, Rong Li, Kai Huang, Pengju Qiao, Jingsen Zhu, Xu Yin, Qi Ye, Wei Hua, Yuchi Huo, Hujun Bao<br>for:这篇论文的目的是利用多视图空中图像来重建建筑立面的几何、照明和材料。methods:这篇论文使用了神经签距场（SDF）来实现物理基于的新视图synthesis和编辑。它使用了简单的RGB图像， captured by a drone，作为输入，不需要复杂的设备，可以实现物理基于的高品质新视图synthesis、重新照明和编辑。results:这篇论文的实验表明， compared to state-of-the-art baselines，其方法在facade holistic inverse rendering、新视图synthesis和Scene editing中具有更高的质量。<details>
<summary>Abstract</summary>
In this work, we use multi-view aerial images to reconstruct the geometry, lighting, and material of facades using neural signed distance fields (SDFs). Without the requirement of complex equipment, our method only takes simple RGB images captured by a drone as inputs to enable physically based and photorealistic novel-view rendering, relighting, and editing. However, a real-world facade usually has complex appearances ranging from diffuse rocks with subtle details to large-area glass windows with specular reflections, making it hard to attend to everything. As a result, previous methods can preserve the geometry details but fail to reconstruct smooth glass windows or verse vise. In order to address this challenge, we introduce three spatial- and semantic-adaptive optimization strategies, including a semantic regularization approach based on zero-shot segmentation techniques to improve material consistency, a frequency-aware geometry regularization to balance surface smoothness and details in different surfaces, and a visibility probe-based scheme to enable efficient modeling of the local lighting in large-scale outdoor environments. In addition, we capture a real-world facade aerial 3D scanning image set and corresponding point clouds for training and benchmarking. The experiment demonstrates the superior quality of our method on facade holistic inverse rendering, novel view synthesis, and scene editing compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们使用多视角空中图像来重建建筑外墙的几何、照明和材质。无需复杂的设备，我们的方法只需要简单的RGB图像， captured by drone，作为输入，以实现物理基于的和 photorealistic 的新视图渲染、重新照明和编辑。然而，实际世界中的外墙通常具有复杂的 appearances，从柔软的岩石到精细的细节，以及大面积的玻璃窗户的镜面反射，这使得前一代方法无法兼顾所有的细节。为解决这个挑战，我们提出了三种空间和semantic-adaptive优化策略，包括基于零shot segmentation技术的semantic regularization来提高材质一致性，频率意识的几何 regularization来平衡不同表面的平滑度和细节，以及可见探针基于的方案来实现大规模的outdoor环境中的本地照明模拟。此外，我们收集了一组真实世界上的外墙空中3D扫描图像和对应的点云数据，用于训练和测试。实验结果表明，我们的方法在facade holistic inverse rendering、新视图合成和Scene editing方面具有较高的质量，比前一代基elines superior。
</details></li>
</ul>
<hr>
<h2 id="Cross-View-Graph-Consistency-Learning-for-Invariant-Graph-Representations"><a href="#Cross-View-Graph-Consistency-Learning-for-Invariant-Graph-Representations" class="headerlink" title="Cross-View Graph Consistency Learning for Invariant Graph Representations"></a>Cross-View Graph Consistency Learning for Invariant Graph Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11821">http://arxiv.org/abs/2311.11821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenjie20/cgcl">https://github.com/chenjie20/cgcl</a></li>
<li>paper_authors: Jie Chen, Zhiming Li, Hua Mao, Wai Lok Woo, Xi Peng</li>
<li>For: 这个论文的目的是学习图形数据的基本表现，尤其是找到对称的图形表现。* Methods: 这个论文提出了一个跨观点图形一致学习（CGCL）方法，通过将两个不同的图形观点融合成一个完整的图形数据，以学习对称的图形表现。* Results: 这个论文的实验结果显示，跨观点图形一致学习（CGCL）方法可以实现比较好的预测性和稳定性，并且在图形数据上比较其他state-of-the-art算法。<details>
<summary>Abstract</summary>
Graph representation learning is fundamental for analyzing graph-structured data. Exploring invariant graph representations remains a challenge for most existing graph representation learning methods. In this paper, we propose a cross-view graph consistency learning (CGCL) method that learns invariant graph representations for link prediction. First, two complementary augmented views are derived from an incomplete graph structure through a bidirectional graph structure augmentation scheme. This augmentation scheme mitigates the potential information loss that is commonly associated with various data augmentation techniques involving raw graph data, such as edge perturbation, node removal, and attribute masking. Second, we propose a CGCL model that can learn invariant graph representations. A cross-view training scheme is proposed to train the proposed CGCL model. This scheme attempts to maximize the consistency information between one augmented view and the graph structure reconstructed from the other augmented view. Furthermore, we offer a comprehensive theoretical CGCL analysis. This paper empirically and experimentally demonstrates the effectiveness of the proposed CGCL method, achieving competitive results on graph datasets in comparisons with several state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary>
“图表示学是对图结构数据进行分析的基本概念。寻找不变图表示仍然是现有图表示学方法中的挑战。在这篇论文中，我们提出了跨视图图统一学习（CGCL）方法，用于预测链接。首先，我们从不完整的图结构中 derivate了两个补充的增强视图，通过双向图结构增强方案来解决 Raw 图数据的不同数据增强技术中的信息损失问题。其次，我们提出了一种 CGCL 模型，可以学习不变图表示。我们提出了一种跨视图培训方案，用于培训提出的 CGCL 模型。这种方案尝试使得一个增强视图与另一个增强视图中的图结构重建的信息保持最大化。此外，我们也提供了详细的 CGCL 分析。这篇论文通过实验和理论分析证明了我们提出的 CGCL 方法的有效性，在对图数据集进行比较时与一些现有的算法进行竞争。”
</details></li>
</ul>
<hr>
<h2 id="CrackCLF-Automatic-Pavement-Crack-Detection-based-on-Closed-Loop-Feedback"><a href="#CrackCLF-Automatic-Pavement-Crack-Detection-based-on-Closed-Loop-Feedback" class="headerlink" title="CrackCLF: Automatic Pavement Crack Detection based on Closed-Loop Feedback"></a>CrackCLF: Automatic Pavement Crack Detection based on Closed-Loop Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11815">http://arxiv.org/abs/2311.11815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Li, Zhun Fan, Ying Chen, Huibiao Lin, Laura Moretti, Giuseppe Loprencipe, Weihua Sheng, Kelvin C. P. Wang</li>
<li>for: Automatic pavement crack detection to ensure pavement functional performances during service life.</li>
<li>methods: Encoder-decoder framework with closed-loop feedback (CLF) and generative adversarial networks (GAN) to correct errors and adapt to environment changes.</li>
<li>results: Outperforms other methods on three public datasets, with the proposed CLF module as a plug and play component to improve model performances.Here’s the full Chinese text:</li>
<li>for: 自动道路裂 crack 探测，以 Ensure 道路在服务寿命中的功能表现。</li>
<li>methods: 使用 encoder-decoder 框架，加入closed-loop feedback (CLF) 和生成对抗网络 (GAN)，以自动更正错误和适应环境变化。</li>
<li>results: 在三个公共数据集上表现出色，并且可以将CLF模块定义为可插入式 ком成分，以提高模型表现。<details>
<summary>Abstract</summary>
Automatic pavement crack detection is an important task to ensure the functional performances of pavements during their service life. Inspired by deep learning (DL), the encoder-decoder framework is a powerful tool for crack detection. However, these models are usually open-loop (OL) systems that tend to treat thin cracks as the background. Meanwhile, these models can not automatically correct errors in the prediction, nor can it adapt to the changes of the environment to automatically extract and detect thin cracks. To tackle this problem, we embed closed-loop feedback (CLF) into the neural network so that the model could learn to correct errors on its own, based on generative adversarial networks (GAN). The resulting model is called CrackCLF and includes the front and back ends, i.e. segmentation and adversarial network. The front end with U-shape framework is employed to generate crack maps, and the back end with a multi-scale loss function is used to correct higher-order inconsistencies between labels and crack maps (generated by the front end) to address open-loop system issues. Empirical results show that the proposed CrackCLF outperforms others methods on three public datasets. Moreover, the proposed CLF can be defined as a plug and play module, which can be embedded into different neural network models to improve their performances.
</details>
<details>
<summary>摘要</summary>
自动路面裂 crack 检测是保证路面服务寿命的重要任务。 Drawing inspiration from deep learning (DL), the encoder-decoder framework is a powerful tool for crack detection. However, these models are usually open-loop (OL) systems that tend to treat thin cracks as the background. Moreover, these models cannot automatically correct errors in the prediction, nor can they adapt to changes in the environment to automatically extract and detect thin cracks. To address this problem, we embed closed-loop feedback (CLF) into the neural network, allowing the model to learn to correct errors on its own based on generative adversarial networks (GAN). The resulting model is called CrackCLF and consists of the front and back ends, i.e., segmentation and adversarial networks. The front end uses a U-shape framework to generate crack maps, and the back end employs a multi-scale loss function to correct higher-order inconsistencies between labels and crack maps (generated by the front end) to address open-loop system issues. Empirical results show that the proposed CrackCLF outperforms other methods on three public datasets. Furthermore, the proposed CLF can be defined as a plug and play module, which can be embedded into different neural network models to improve their performances.
</details></li>
</ul>
<hr>
<h2 id="Robot-Hand-Eye-Calibration-using-Structure-from-Motion"><a href="#Robot-Hand-Eye-Calibration-using-Structure-from-Motion" class="headerlink" title="Robot Hand-Eye Calibration using Structure-from-Motion"></a>Robot Hand-Eye Calibration using Structure-from-Motion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11808">http://arxiv.org/abs/2311.11808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Andreff, Bernard Espiau, Radu Horaud</li>
<li>for: 这篇论文旨在提出一种新的手眼均衡方法，并且不需要使用测量仪器床。</li>
<li>methods: 这篇论文使用结构从运动和已知机械运动组合，并且可以在线性形式下解决问题，解决了手眼参数和结构从运动方法中的未知扩展因子问题。</li>
<li>results: 论文通过大量实验证明了这种方法的品质，并且与现有方法进行比较。<details>
<summary>Abstract</summary>
In this paper we propose a new flexible method for hand-eye calibration. The vast majority of existing hand-eye calibration techniques requires a calibration rig which is used in conjunction with camera pose estimation methods. Instead, we combine structure-from-motion with known robot motions and we show that the solution can be obtained in linear form. The latter solves for both the hand-eye parameters and for the unknown scale factor inherent with structure-from-motion methods. The algebraic analysis that is made possible with such a linear formulation allows to investigate not only the well known case of general screw motions but also such singular motions as pure translations, pure rotations, and planar motions. In essence, the robot-mounted camera looks to an unknown rigid layout, tracks points over an image sequence and estimates the camera-to-robot relationship. Such a self calibration process is relevant for unmanned vehicles, robots working in remote places, and so forth. We conduct a large number of experiments which validate the quality of the method by comparing it with existing ones.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的灵活手眼协调方法。大多数现有的手眼协调技术都需要使用协调机械，并且通常与摄像头位置估算方法结合使用。我们则将结构从动与知道机器人运动结合，并证明该解可以得到线性形式。这种线性形式的分析允许我们不仅研究普通的扳手运动，还可以研究特殊的运动，如纯翻译、纯旋转和平面运动。总之，机器人携带的摄像头会看到未知的固定布局，跟踪图像序列中的点并估算摄像头与机器人之间的关系。这种自动协调过程对于无人机、远程工作的机器人等非常重要。我们进行了大量实验，并证明了该方法的高效性，与现有方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Robust-Tumor-Segmentation-with-Hyperspectral-Imaging-and-Graph-Neural-Networks"><a href="#Robust-Tumor-Segmentation-with-Hyperspectral-Imaging-and-Graph-Neural-Networks" class="headerlink" title="Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural Networks"></a>Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11782">http://arxiv.org/abs/2311.11782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayar Lotfy, Anna Alperovich, Tommaso Giannantonio, Bjorn Barz, Xiaohan Zhang, Felix Holm, Nassir Navab, Felix Boehm, Carolin Schwamborn, Thomas K. Hoffmann, Patrick J. Schuler</li>
<li>for: 针对在手术时分界肿瘤和健康组织的问题，该文提出了一种改进的方法，利用图像频谱的空间上下文来增强分割精度。</li>
<li>methods: 该方法使用图像频谱学（HSI）和机器学习（ML）结合，并采用图像 Graph Neural Networks（GNN）传播上下文信息，以提高分割精度。</li>
<li>results: 该方法在临床外的51个HSI图像中，与无上下文方法进行比较，显示出显著的提高，能够准确地分割健康和肿瘤组织，包括在之前未见的患者图像中。此外，通过考虑本地图像质量指标，进一步提高了训练过程的稳定性。<details>
<summary>Abstract</summary>
Segmenting the boundary between tumor and healthy tissue during surgical cancer resection poses a significant challenge. In recent years, Hyperspectral Imaging (HSI) combined with Machine Learning (ML) has emerged as a promising solution. However, due to the extensive information contained within the spectral domain, most ML approaches primarily classify individual HSI (super-)pixels, or tiles, without taking into account their spatial context. In this paper, we propose an improved methodology that leverages the spatial context of tiles for more robust and smoother segmentation. To address the irregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate context information across neighboring regions. The features for each tile within the graph are extracted using a Convolutional Neural Network (CNN), which is trained simultaneously with the subsequent GNN. Moreover, we incorporate local image quality metrics into the loss function to enhance the training procedure's robustness against low-quality regions in the training images. We demonstrate the superiority of our proposed method using a clinical ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the limited dataset, the GNN-based model significantly outperforms context-agnostic approaches, accurately distinguishing between healthy and tumor tissues, even in images from previously unseen patients. Furthermore, we show that our carefully designed loss function, accounting for local image quality, results in additional improvements. Our findings demonstrate that context-aware GNN algorithms can robustly find tumor demarcations on HSI images, ultimately contributing to better surgery success and patient outcome.
</details>
<details>
<summary>摘要</summary>
Segmenting the boundary between tumor and healthy tissue during cancer surgery is a challenging task. Recently, Hyperspectral Imaging (HSI) combined with Machine Learning (ML) has shown promise. However, most ML approaches only classify individual HSI pixels or tiles without considering their spatial context. In this paper, we propose an improved method that leverages the spatial context of tiles for more robust and smoother segmentation. To address the irregular shapes of tiles, we use Graph Neural Networks (GNNs) to propagate context information across neighboring regions. We extract features for each tile within the graph using a Convolutional Neural Network (CNN), which is trained simultaneously with the subsequent GNN. Additionally, we incorporate local image quality metrics into the loss function to enhance the training procedure's robustness against low-quality regions in the training images. We demonstrate the superiority of our proposed method using a clinical ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the limited dataset, the GNN-based model significantly outperforms context-agnostic approaches, accurately distinguishing between healthy and tumor tissues, even in images from previously unseen patients. Furthermore, we show that our carefully designed loss function, accounting for local image quality, results in additional improvements. Our findings demonstrate that context-aware GNN algorithms can robustly find tumor demarcations on HSI images, ultimately contributing to better surgery success and patient outcome.Here's the translation in Traditional Chinese:鉴别癌细胞和健康组织界限在医疗器官切除术是一个具有挑战性的任务。最近，几何spectral imaging（HSI）与机器学习（ML）的结合已经出现了一定的推动力。然而，大多数ML方法仅将单一HSI像素或块分类为不同的类别，未能考虑这些像素之间的空间Context。在这篇文章中，我们提出了一个改进的方法，将HSI块之间的空间Context leveraged来实现更加稳定和平滑的鉴别。为了解决块的不规则形状，我们使用Graph Neural Networks（GNNs）将邻近区域之间的Context信息传递。我们对各块内的GNN构建特定的Convolutional Neural Network（CNN），并将其训练同时进行。此外，我们将本地图像质量指标入力到损失函数中，以增强训练过程的Robustness。我们使用来自30名病人的干式实验数据，包括51几何spectral imaging（HSI）图像。 despite the limited dataset, our proposed method significantly outperforms context-agnostic approaches, accurately distinguishing between healthy and tumor tissues, even in images from previously unseen patients. In addition, we show that our carefully designed loss function, accounting for local image quality, results in additional improvements. Our findings demonstrate that context-aware GNN algorithms can robustly find tumor demarcations on HSI images, ultimately contributing to better surgery success and patient outcome.
</details></li>
</ul>
<hr>
<h2 id="Multimodal-deep-learning-for-mapping-forest-dominant-height-by-fusing-GEDI-with-earth-observation-data"><a href="#Multimodal-deep-learning-for-mapping-forest-dominant-height-by-fusing-GEDI-with-earth-observation-data" class="headerlink" title="Multimodal deep learning for mapping forest dominant height by fusing GEDI with earth observation data"></a>Multimodal deep learning for mapping forest dominant height by fusing GEDI with earth observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11777">http://arxiv.org/abs/2311.11777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Man Chen, Wenquan Dong, Hao Yu, Iain Woodhouse, Casey M. Ryan, Haoyu Liu, Selena Georgiou, Edward T. A. Mitchard</li>
<li>for: 这个论文的目的是使用多源 remote sensing 数据和深度学习模型精准地测量高分辨率森林高度。</li>
<li>methods: 这个论文使用了一种新的深度学习框架，即多Modal attention remote sensing network (MARSNet)，来估算森林主要高度。 MARSNet 使用了 GEDI 相对高度 metric，以及 Setinel-1 数据、ALOS-2 PALSAR-2 数据、Sentinel-2 光学数据和 auxiliary data，并使用了每个 remote sensing 数据模式的独立编码器来提取多尺度特征，以及共享解码器来融合特征并估算高度。</li>
<li>results: 根据实验结果，MARSNet 可以准确地估算森林主要高度，其 R2 值为 0.62，RMSE 值为 2.82 m，比random forest 方法的 R2 值为 0.55，RMSE 值为 3.05 m 高。此外，通过独立验证使用场景测量数据，MARSNet 在 Jilin, China 地区生成的墙壁到墙壁高程图像中达到了 R2 值为 0.58，RMSE 值为 3.76 m，比random forest 基准值的 R2 值为 0.41，RMSE 值为 4.37 m 高。<details>
<summary>Abstract</summary>
The integration of multisource remote sensing data and deep learning models offers new possibilities for accurately mapping high spatial resolution forest height. We found that GEDI relative heights (RH) metrics exhibited strong correlation with the mean of the top 10 highest trees (dominant height) measured in situ at the corresponding footprint locations. Consequently, we proposed a novel deep learning framework termed the multi-modal attention remote sensing network (MARSNet) to estimate forest dominant height by extrapolating dominant height derived from GEDI, using Setinel-1 data, ALOS-2 PALSAR-2 data, Sentinel-2 optical data and ancillary data. MARSNet comprises separate encoders for each remote sensing data modality to extract multi-scale features, and a shared decoder to fuse the features and estimate height. Using individual encoders for each remote sensing imagery avoids interference across modalities and extracts distinct representations. To focus on the efficacious information from each dataset, we reduced the prevalent spatial and band redundancies in each remote sensing data by incorporating the extended spatial and band reconstruction convolution modules in the encoders. MARSNet achieved commendable performance in estimating dominant height, with an R2 of 0.62 and RMSE of 2.82 m, outperforming the widely used random forest approach which attained an R2 of 0.55 and RMSE of 3.05 m. Finally, we applied the trained MARSNet model to generate wall-to-wall maps at 10 m resolution for Jilin, China. Through independent validation using field measurements, MARSNet demonstrated an R2 of 0.58 and RMSE of 3.76 m, compared to 0.41 and 4.37 m for the random forest baseline. Our research demonstrates the effectiveness of a multimodal deep learning approach fusing GEDI with SAR and passive optical imagery for enhancing the accuracy of high resolution dominant height estimation.
</details>
<details>
<summary>摘要</summary>
“多源Remote数据和深度学习模型的结合，提供了新的高分辨率森林高度映射的可能性。我们发现，GEDI相对高度（RH）指标与场地上最高10棵树的平均高度（主对比高度）存在强正相关。因此，我们提出了一个名为多模式注意深度测网络（MARSNet）的新型深度学习框架，以估算森林主对比高度。MARSNet包括每个遥测数据模式的分类器，以EXTRACT多尺度特征，并使用共同解码器将特征联合估算高度。这种多模式注意框架避免了遥测数据模式之间的干扰，并EXTRACT了不同的表现。为了优化遥测数据模式中的常见空间和频率红UNDERSCORE，我们在遥测数据模式中添加了扩展空间和频率重建对应模组。MARSNet在主对比高度估算中表现出色，R2值为0.62，RMSE值为2.82米，比Random Forest方法（R2值为0.55，RMSE值为3.05米）更高。最后，我们将训练好的MARSNet模型应用到Jilin、中国的壁垒壁垒地图上。通过独立验证过程，MARSNet在10米分辨率上的R2值为0.58，RMSE值为3.76米，较Random Forest基eline（R2值为0.41，RMSE值为4.37米）更高。我们的研究显示，结合GEDI、SAR和透明光学图像的多模式深度学习方法可以提高高分辨率主对比高度估算的精度。”
</details></li>
</ul>
<hr>
<h2 id="Practical-cross-sensor-color-constancy-using-a-dual-mapping-strategy"><a href="#Practical-cross-sensor-color-constancy-using-a-dual-mapping-strategy" class="headerlink" title="Practical cross-sensor color constancy using a dual-mapping strategy"></a>Practical cross-sensor color constancy using a dual-mapping strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11773">http://arxiv.org/abs/2311.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwei Yue, Minchen Wei</li>
<li>for: 用于解决数据收集问题，减少感器差异，提高材料推理性能。</li>
<li>methods: 使用双映射策略，只需要一个简单的白点测试感器，从而 derivation 一个映射矩阵，用于重建图像数据和照明。然后，使用轻量级多层感知网络（MLP）模型，对重构的图像数据进行优化，使用重构的照明为真实值。</li>
<li>results: 可以减少感器差异，实现与主流横批方法相当的性能，仅需要小量内存（约0.003 MB）和训练时间（约1小时），并且具有快速执行速度（约0.3 ms和1 ms），不敏感输入图像分辨率。因此，它可以实现现实中的数据收集问题。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have been widely used for illumination estimation, which is time-consuming and requires sensor-specific data collection. Our proposed method uses a dual-mapping strategy and only requires a simple white point from a test sensor under a D65 condition. This allows us to derive a mapping matrix, enabling the reconstructions of image data and illuminants. In the second mapping phase, we transform the re-constructed image data into sparse features, which are then optimized with a lightweight multi-layer perceptron (MLP) model using the re-constructed illuminants as ground truths. This approach effectively reduces sensor discrepancies and delivers performance on par with leading cross-sensor methods. It only requires a small amount of memory (~0.003 MB), and takes ~1 hour training on an RTX3070Ti GPU. More importantly, the method can be implemented very fast, with ~0.3 ms and ~1 ms on a GPU or CPU respectively, and is not sensitive to the input image resolution. Therefore, it offers a practical solution to the great challenges of data recollection that is faced by the industry.
</details>
<details>
<summary>摘要</summary>
Note:* "DNNs" is translated as "深度神经网络" (shēn dào shén zhī wǎng luò)* "illumination estimation" is translated as "照明估算" (zhao ming gueshan)* "sensor-specific data collection" is translated as "传感器特定数据收集" (chuán jiāo tè qì dàta jīng)* "dual-mapping strategy" is translated as "双映射策略" (shuāng yǐng xiàng cè lüè)* "simple white point" is translated as "简单的白点" (jiǎn dan de bái diǎn)* "re-constructed image data" is translated as "重构图像数据" (zhòng gòu tú xiàng shù dà)* "sparse features" is translated as "稀疏特征" (xī shū tè xíng)* "lightweight multi-layer perceptron" is translated as "轻量级多层感知器" (qīng liáng jí duō cèng qiǎng)* "ground truths" is translated as "真实值" (zhēn shí zhí)* "data recollection" is translated as "数据收集" (dàta jīng jī)
</details></li>
</ul>
<hr>
<h2 id="A-Good-Feature-Extractor-Is-All-You-Need-for-Weakly-Supervised-Learning-in-Histopathology"><a href="#A-Good-Feature-Extractor-Is-All-You-Need-for-Weakly-Supervised-Learning-in-Histopathology" class="headerlink" title="A Good Feature Extractor Is All You Need for Weakly Supervised Learning in Histopathology"></a>A Good Feature Extractor Is All You Need for Weakly Supervised Learning in Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11772">http://arxiv.org/abs/2311.11772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Wölflein, Dyke Ferber, Asier Rabasco Meneghetti, Omar S. M. El Nahhas, Daniel Truhn, Zunamys I. Carrero, David J. Harrison, Ognjen Arandjelović, Jakob N. Kather</li>
<li>for: 这 paper 的目的是检验公共可用的 SSL 特征提取器在 computation pathology 领域的稳定性和可靠性。</li>
<li>methods: 这 paper 使用了多种方法来评估特征提取器的稳定性，包括 omitting stain normalization 和图像扩展，以及使用不同的预处理方法和下游结构。</li>
<li>results: 研究发现，不使用 stain normalization 和图像扩展可以减少内存和计算量，同时不会影响下游性能。此外，研究还发现最高级别的特征提取器在不同的染色和旋转等变量下的稳定性非常高。<details>
<summary>Abstract</summary>
Deep learning is revolutionising pathology, offering novel opportunities in disease prognosis and personalised treatment. Historically, stain normalisation has been a crucial preprocessing step in computational pathology pipelines, and persists into the deep learning era. Yet, with the emergence of feature extractors trained using self-supervised learning (SSL) on diverse pathology datasets, we call this practice into question. In an empirical evaluation of publicly available feature extractors, we find that omitting stain normalisation and image augmentations does not compromise downstream performance, while incurring substantial savings in memory and compute. Further, we show that the top-performing feature extractors are remarkably robust to variations in stain and augmentations like rotation in their latent space. Contrary to previous patch-level benchmarking studies, our approach emphasises clinical relevance by focusing on slide-level prediction tasks in a weakly supervised setting with external validation cohorts. This work represents the most comprehensive robustness evaluation of public pathology SSL feature extractors to date, involving more than 6,000 training runs across nine tasks, five datasets, three downstream architectures, and various preprocessing setups. Our findings stand to streamline digital pathology workflows by minimising preprocessing needs and informing the selection of feature extractors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Non-Contact-NIR-PPG-Sensing-through-Large-Sequence-Signal-Regression"><a href="#Non-Contact-NIR-PPG-Sensing-through-Large-Sequence-Signal-Regression" class="headerlink" title="Non-Contact NIR PPG Sensing through Large Sequence Signal Regression"></a>Non-Contact NIR PPG Sensing through Large Sequence Signal Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11757">http://arxiv.org/abs/2311.11757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy Hanley, Dara Golden, Robyn Maxwell, Ashkan Parsi, Joseph Lemley</li>
<li>for: 这个论文是为了演示一种替代Convolution Attention Network（CAN）架构，用于从近红外（NIR）视频中回归血氧饱和信号。</li>
<li>methods: 该论文使用了两个公共可用的数据集，将其分割成训练和测试集，并对这些数据集进行了增强以避免过度适应。</li>
<li>results: 使用这种CAN架构和NIR视频，可以实现 Mean Average Error（MAE）为0.99 bpm的准确信号输出。<details>
<summary>Abstract</summary>
Non-Contact sensing is an emerging technology with applications across many industries from driver monitoring in vehicles to patient monitoring in healthcare. Current state-of-the-art implementations focus on RGB video, but this struggles in varying/noisy light conditions and is almost completely unfeasible in the dark. Near Infra-Red (NIR) video, however, does not suffer from these constraints. This paper aims to demonstrate the effectiveness of an alternative Convolution Attention Network (CAN) architecture, to regress photoplethysmography (PPG) signal from a sequence of NIR frames. A combination of two publicly available datasets, which is split into train and test sets, is used for training the CAN. This combined dataset is augmented to reduce overfitting to the 'normal' 60 - 80 bpm heart rate range by providing the full range of heart rates along with corresponding videos for each subject. This CAN, when implemented over video cropped to the subject's head, achieved a Mean Average Error (MAE) of just 0.99 bpm, proving its effectiveness on NIR video and the architecture's feasibility to regress an accurate signal output.
</details>
<details>
<summary>摘要</summary>
非接触感测是一种emerging技术，有各种应用于多个行业，从车辆驾驶员监测到医疗保健中的病人监测。当前状态的技术实现都是基于RGB视频，但这在不同/噪音的照明条件下遇到困难，而且在黑暗的情况下基本无法实现。然而，近红外（NIR）视频不受这些限制。这篇论文旨在示出一种替代卷积注意网络（CAN）架构，用于从NIR视频序列中回归血液搅动（PPG）信号。使用两个公共可用的数据集，分别用于训练和测试CAN。这些合并的数据集通过减少适应范围的补做来降低过适应，并提供每个主题对应的视频和心率范围的全范围。这种CAN，当实现在对头部截取的视频上时，实现了 Mean Average Error（MAE）仅0.99 bpm，证明其效果性在NIR视频和架构上。
</details></li>
</ul>
<hr>
<h2 id="AdvGen-Physical-Adversarial-Attack-on-Face-Presentation-Attack-Detection-Systems"><a href="#AdvGen-Physical-Adversarial-Attack-on-Face-Presentation-Attack-Detection-Systems" class="headerlink" title="AdvGen: Physical Adversarial Attack on Face Presentation Attack Detection Systems"></a>AdvGen: Physical Adversarial Attack on Face Presentation Attack Detection Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11753">http://arxiv.org/abs/2311.11753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Amrit Patnaik, Shivali Chansoriya, Anil K. Jain, Anoop M. Namboodiri<br>for: 这个论文的目的是评估面部识别系统在真实世界中的风险水平，以确保在实际应用中安全地部署面部识别模型。methods: 该论文使用了一种自动生成的对抗网络（AdvGen）来模拟印刷和重播攻击，并生成了一些可以识别系统欺骗的图像。results: 该论文的实验结果显示，使用AdvGen生成的攻击图像可以成功欺骗state-of-the-art PADs的92.01%。该论文在四个数据集和十个现状顶尖PADs上进行了广泛的测试。此外，该论文还在真实的物理环境中进行了实际的测试，以证明攻击的有效性。<details>
<summary>Abstract</summary>
Evaluating the risk level of adversarial images is essential for safely deploying face authentication models in the real world. Popular approaches for physical-world attacks, such as print or replay attacks, suffer from some limitations, like including physical and geometrical artifacts. Recently, adversarial attacks have gained attraction, which try to digitally deceive the learning strategy of a recognition system using slight modifications to the captured image. While most previous research assumes that the adversarial image could be digitally fed into the authentication systems, this is not always the case for systems deployed in the real world. This paper demonstrates the vulnerability of face authentication systems to adversarial images in physical world scenarios. We propose AdvGen, an automated Generative Adversarial Network, to simulate print and replay attacks and generate adversarial images that can fool state-of-the-art PADs in a physical domain attack setting. Using this attack strategy, the attack success rate goes up to 82.01%. We test AdvGen extensively on four datasets and ten state-of-the-art PADs. We also demonstrate the effectiveness of our attack by conducting experiments in a realistic, physical environment.
</details>
<details>
<summary>摘要</summary>
evaluating the risk level of adversarial images is essential for safely deploying face authentication models in the real world. popular approaches for physical-world attacks, such as print or replay attacks, suffer from some limitations, like including physical and geometrical artifacts. recently, adversarial attacks have gained attraction, which try to digitally deceive the learning strategy of a recognition system using slight modifications to the captured image. while most previous research assumes that the adversarial image could be digitally fed into the authentication systems, this is not always the case for systems deployed in the real world. this paper demonstrates the vulnerability of face authentication systems to adversarial images in physical world scenarios. we propose advgen, an automated generative adversarial network, to simulate print and replay attacks and generate adversarial images that can fool state-of-the-art pad in a physical domain attack setting. using this attack strategy, the attack success rate goes up to 82.01%. we test advgen extensively on four datasets and ten state-of-the-art pad. we also demonstrate the effectiveness of our attack by conducting experiments in a realistic, physical environment.Note: The translation is done using Google Translate, and may not be perfect or exact.
</details></li>
</ul>
<hr>
<h2 id="Fuzzy-Information-Seeded-Region-Growing-for-Automated-Lesions-After-Stroke-Segmentation-in-MR-Brain-Images"><a href="#Fuzzy-Information-Seeded-Region-Growing-for-Automated-Lesions-After-Stroke-Segmentation-in-MR-Brain-Images" class="headerlink" title="Fuzzy Information Seeded Region Growing for Automated Lesions After Stroke Segmentation in MR Brain Images"></a>Fuzzy Information Seeded Region Growing for Automated Lesions After Stroke Segmentation in MR Brain Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11742">http://arxiv.org/abs/2311.11742</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mawio02/fisrg-for-automated-lesion-after-stroke-segmentation-in-mri">https://github.com/mawio02/fisrg-for-automated-lesion-after-stroke-segmentation-in-mri</a></li>
<li>paper_authors: Mario Pascual González</li>
<li>for: 针对干病变的精准分割</li>
<li>methods: 基于模糊逻辑和种子域生长算法</li>
<li>results: 高达94.2%的 dice分数，表明算法可以准确分割干病变Here’s a breakdown of each item:</li>
<li>for: The paper is written for the purpose of precise segmentation of stroke lesions from brain MRI images.</li>
<li>methods: The paper introduces an innovative approach using a Fuzzy Information Seeded Region Growing (FISRG) algorithm, which combines fuzzy logic with Seeded Region Growing (SRG) techniques to enhance segmentation accuracy.</li>
<li>results: The highest Dice score achieved in the experiments was 94.2%, indicating a high degree of similarity between the algorithm’s output and the expert-validated ground truth. The best average Dice score was recorded in the third experiment, amounting to 88.1%, highlighting the algorithm’s efficacy in consistently segmenting stroke lesions across various slices.<details>
<summary>Abstract</summary>
In the realm of medical imaging, precise segmentation of stroke lesions from brain MRI images stands as a critical challenge with significant implications for patient diagnosis and treatment. Addressing this, our study introduces an innovative approach using a Fuzzy Information Seeded Region Growing (FISRG) algorithm. Designed to effectively delineate the complex and irregular boundaries of stroke lesions, the FISRG algorithm combines fuzzy logic with Seeded Region Growing (SRG) techniques, aiming to enhance segmentation accuracy.   The research involved three experiments to optimize the FISRG algorithm's performance, each focusing on different parameters to improve the accuracy of stroke lesion segmentation. The highest Dice score achieved in these experiments was 94.2\%, indicating a high degree of similarity between the algorithm's output and the expert-validated ground truth. Notably, the best average Dice score, amounting to 88.1\%, was recorded in the third experiment, highlighting the efficacy of the algorithm in consistently segmenting stroke lesions across various slices.   Our findings reveal the FISRG algorithm's strengths in handling the heterogeneity of stroke lesions. However, challenges remain in areas of abrupt lesion topology changes and in distinguishing lesions from similar intensity brain regions. The results underscore the potential of the FISRG algorithm in contributing significantly to advancements in medical imaging analysis for stroke diagnosis and treatment.
</details>
<details>
<summary>摘要</summary>
在医疗影像领域，精准分割脑出血损害从脑MRI图像中的stroke损害是一项重要挑战，对患者诊断和治疗具有重要意义。我们的研究推出了一种创新的方法，利用Fuzzy Information Seeded Region Growing（FISRG）算法。这种算法结合了混淆逻辑和Seeded Region Growing（SRG）技术，以提高分割精度。我们的研究包括三个实验，以便优化FISRG算法的性能。每个实验都关注不同的参数，以提高脑出血损害分割的准确率。最高的Dice分数为94.2%，表明算法的输出和专家验证的真实值之间存在高度的相似性。其中第三个实验的平均Dice分数为88.1%，表明算法在不同的剖面上具有一定的稳定性。我们的发现表明FISRG算法在处理脑出血损害的不同类型和分布方面具有优异的能力。然而，在脑出血损害的突然变化和同INTENSITY的脑区域之间存在挑战。结果表明FISRG算法在医疗影像分析方面具有广泛的应用前景，对脑出血损害诊断和治疗具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="On-the-Importance-of-Large-Objects-in-CNN-Based-Object-Detection-Algorithms"><a href="#On-the-Importance-of-Large-Objects-in-CNN-Based-Object-Detection-Algorithms" class="headerlink" title="On the Importance of Large Objects in CNN Based Object Detection Algorithms"></a>On the Importance of Large Objects in CNN Based Object Detection Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11714">http://arxiv.org/abs/2311.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Ben Saad, Gabriele Facciolo, Axel Davy</li>
<li>for: 提高对象检测器的性能（Object Detectors performances）</li>
<li>methods: 引入一个归一化面积大小（object area size）的补偿项到训练损失中（introduce a weighting term into the training loss）</li>
<li>results: 在所有对象大小上提高检测分数 (+2 p.p. of mAP on small objects, +2 p.p. on medium and +4 p.p. on large on COCO val 2017 with InternImage-T)<details>
<summary>Abstract</summary>
Object detection models, a prominent class of machine learning algorithms, aim to identify and precisely locate objects in images or videos. However, this task might yield uneven performances sometimes caused by the objects sizes and the quality of the images and labels used for training. In this paper, we highlight the importance of large objects in learning features that are critical for all sizes. Given these findings, we propose to introduce a weighting term into the training loss. This term is a function of the object area size. We show that giving more weight to large objects leads to improved detection scores across all object sizes and so an overall improvement in Object Detectors performances (+2 p.p. of mAP on small objects, +2 p.p. on medium and +4 p.p. on large on COCO val 2017 with InternImage-T). Additional experiments and ablation studies with different models and on a different dataset further confirm the robustness of our findings.
</details>
<details>
<summary>摘要</summary>
Note:* "p.p." stands for "percentage points"* "COCO" stands for "Common Objects in Context"* "val 2017" stands for "validation set in 2017"* "InternImage-T" stands for "International Image-Text"
</details></li>
</ul>
<hr>
<h2 id="GS-SLAM-Dense-Visual-SLAM-with-3D-Gaussian-Splatting"><a href="#GS-SLAM-Dense-Visual-SLAM-with-3D-Gaussian-Splatting" class="headerlink" title="GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting"></a>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11700">http://arxiv.org/abs/2311.11700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li</li>
<li>for: 这paper是为了提出一种新的同时地址和地图建模（SLAM）系统，使其更好地平衡效率和准确性。</li>
<li>methods: 该方法使用3D Gaussian表示法，并使用实时可导渲染管线来加速地图优化和RGB-D重新渲染。具体来说，该方法提出了一种适应扩展策略，以有效地重建新观察到的场景几何结构，并改进已经观察到的区域的地图。此外，在pose tracking过程中，还设计了一种有效的粗粒度优化技术，以选择可靠的3D Gaussian表示，从而降低运行时间和提高估计稳定性。</li>
<li>results: 该方法在Replica和TUM-RGBD datasets上实现了与现有实时方法相当的性能。Here’s the translation in English for reference:</li>
<li>for: This paper proposes a new Simultaneous Localization and Mapping (SLAM) system that achieves a better balance between efficiency and accuracy.</li>
<li>methods: The method uses 3D Gaussian representation and employs a real-time differentiable splatting rendering pipeline to accelerate map optimization and RGB-D re-rendering. Specifically, the method proposes an adaptive expansion strategy to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. Moreover, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations for camera pose optimization, resulting in runtime reduction and robust estimation.</li>
<li>results: The method achieves competitive performance with existing state-of-the-art real-time methods on the Replica and TUM-RGBD datasets.<details>
<summary>Abstract</summary>
In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D Gaussian representation in the Simultaneous Localization and Mapping (SLAM) system. It facilitates a better balance between efficiency and accuracy. Compared to recent SLAM methods employing neural implicit representations, our method utilizes a real-time differentiable splatting rendering pipeline that offers significant speedup to map optimization and RGB-D re-rendering. Specifically, we propose an adaptive expansion strategy that adds new or deletes noisy 3D Gaussian in order to efficiently reconstruct new observed scene geometry and improve the mapping of previously observed areas. This strategy is essential to extend 3D Gaussian representation to reconstruct the whole scene rather than synthesize a static object in existing methods. Moreover, in the pose tracking process, an effective coarse-to-fine technique is designed to select reliable 3D Gaussian representations to optimize camera pose, resulting in runtime reduction and robust estimation. Our method achieves competitive performance compared with existing state-of-the-art real-time methods on the Replica, TUM-RGBD datasets. The source code will be released upon acceptance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了$\textbf{GS-SLAM}$，它首先利用3D Gaussian表示法在同时定位和地图（SLAM）系统中使用。这种方法可以更好地平衡效率和准确性。与现有的SLAM方法使用神经网络卷积表示法相比，我们的方法使用了实时可导渲染管线，从而提供了显著的速度提升，以便地图优化和RGB-D重新渲染。具体来说，我们提出了适应扩展策略，可以有效地重建新观察到的场景几何结构，并改进之前观察到的地图。这种策略是在扩展3D Gaussian表示法来重建整个场景，而不是在现有方法中Synthesize一个静止物体。此外，在姿态跟踪过程中，我们设计了一种有效的粗略至细节技术，可以选择可靠的3D Gaussian表示法来优化相机姿态，从而实现运行时间减少和稳定估计。我们的方法在Replica和TUM-RGBD数据集上达到了与现有实时方法的竞争性表现。我们计划在接受后发布源代码。
</details></li>
</ul>
<hr>
<h2 id="Cut-and-Paste-Subject-Driven-Video-Editing-with-Attention-Control"><a href="#Cut-and-Paste-Subject-Driven-Video-Editing-with-Attention-Control" class="headerlink" title="Cut-and-Paste: Subject-Driven Video Editing with Attention Control"></a>Cut-and-Paste: Subject-Driven Video Editing with Attention Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11697">http://arxiv.org/abs/2311.11697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhichao Zuo, Zhao Zhang, Yan Luo, Yang Zhao, Haijun Zhang, Yi Yang, Meng Wang</li>
<li>for: 本研究提出了一种名为“剪贴”的框架，用于实际世界上进行Semantic Video editing，以文本提示和补充图像的指导。</li>
<li>methods: 本研究使用了文本驱动视频编辑，并引入了参考图像作为补充输入，以提高精细Semantic editing的控制和背景保留。我们还使用了 crossed attention control方法来限制编辑区域，以保持视频背景和空间时间一致性。</li>
<li>results: 对于文本提示和补充图像的视频编辑，我们的方法比现有方法表现出色， both quantitative and subjective evaluations 都显示了这一点。<details>
<summary>Abstract</summary>
This paper presents a novel framework termed Cut-and-Paste for real-word semantic video editing under the guidance of text prompt and additional reference image. While the text-driven video editing has demonstrated remarkable ability to generate highly diverse videos following given text prompts, the fine-grained semantic edits are hard to control by plain textual prompt only in terms of object details and edited region, and cumbersome long text descriptions are usually needed for the task. We therefore investigate subject-driven video editing for more precise control of both edited regions and background preservation, and fine-grained semantic generation. We achieve this goal by introducing an reference image as supplementary input to the text-driven video editing, which avoids racking your brain to come up with a cumbersome text prompt describing the detailed appearance of the object. To limit the editing area, we refer to a method of cross attention control in image editing and successfully extend it to video editing by fusing the attention map of adjacent frames, which strikes a balance between maintaining video background and spatio-temporal consistency. Compared with current methods, the whole process of our method is like ``cut" the source object to be edited and then ``paste" the target object provided by reference image. We demonstrate that our method performs favorably over prior arts for video editing under the guidance of text prompt and extra reference image, as measured by both quantitative and subjective evaluations.
</details>
<details>
<summary>摘要</summary>
To limit the editing area, we extend a cross attention control method from image editing to video editing by fusing attention maps of adjacent frames. This approach balances video background and spatio-temporal consistency. Our method is like "cutting" the source object to be edited and "pasting" the target object provided by the reference image.Compared to current methods, our approach performs favorably in both quantitative and subjective evaluations. The whole process of our method is simple and easy to use, making it a valuable tool for real-world video editing tasks.
</details></li>
</ul>
<hr>
<h2 id="Clarity-ChatGPT-An-Interactive-and-Adaptive-Processing-System-for-Image-Restoration-and-Enhancement"><a href="#Clarity-ChatGPT-An-Interactive-and-Adaptive-Processing-System-for-Image-Restoration-and-Enhancement" class="headerlink" title="Clarity ChatGPT: An Interactive and Adaptive Processing System for Image Restoration and Enhancement"></a>Clarity ChatGPT: An Interactive and Adaptive Processing System for Image Restoration and Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11695">http://arxiv.org/abs/2311.11695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanyan Wei, Zhao Zhang, Jiahuan Ren, Xiaogang Xu, Richang Hong, Yi Yang, Shuicheng Yan, Meng Wang</li>
<li>for: 提高图像修复和提高（IRE）方法的通用能力和互动性，解决现有IRE方法的限制，包括有限的预训练数据和不具备用户反馈机制等。</li>
<li>methods: 提出了一种基于对话智能的转换系统，名为Clarity ChatGPT，它结合多种IRE方法，可以自动识别图像异常类型并选择合适的IRE方法进行修复，或者基于用户反馈进行迭代生成满意的结果。</li>
<li>results: 通过实验研究，显示了Clarity ChatGPT可以有效提高IRE方法的通用能力和互动性，并在低级别领域中填补现有视力语言模型的空白。<details>
<summary>Abstract</summary>
The generalization capability of existing image restoration and enhancement (IRE) methods is constrained by the limited pre-trained datasets, making it difficult to handle agnostic inputs such as different degradation levels and scenarios beyond their design scopes. Moreover, they are not equipped with interactive mechanisms to consider user preferences or feedback, and their end-to-end settings cannot provide users with more choices. Faced with the above-mentioned IRE method's limited performance and insufficient interactivity, we try to solve it from the engineering and system framework levels. Specifically, we propose Clarity ChatGPT-a transformative system that combines the conversational intelligence of ChatGPT with multiple IRE methods. Clarity ChatGPT can automatically detect image degradation types and select appropriate IRE methods to restore images, or iteratively generate satisfactory results based on user feedback. Its innovative features include a CLIP-powered detector for accurate degradation classification, no-reference image quality evaluation for performance evaluation, region-specific processing for precise enhancements, and advanced fusion techniques for optimal restoration results. Clarity ChatGPT marks a significant advancement in integrating language and vision, enhancing image-text interactions, and providing a robust, high-performance IRE solution. Our case studies demonstrate that Clarity ChatGPT effectively improves the generalization and interaction capabilities in the IRE, and also fills the gap in the low-level domain of the existing vision-language model.
</details>
<details>
<summary>摘要</summary>
现有的图像修复和提高（IRE）方法的通用能力受到先前训练数据的限制，导致处理不同降低水平和场景外的困难。此外，它们没有交互机制来考虑用户喜好或反馈，其端到端设置也无法提供更多的选择。为了解决以上IRE方法的局限性和不足，我们尝试从工程和系统框架层面进行解决方案。具体来说，我们提出了“清晰聊天GPT”——一种将语言智能和多种IRE方法结合起来的转变系统。清晰聊天GPT可以自动探测图像降低类型，选择合适的IRE方法来修复图像，或者基于用户反馈进行迭代生成满意的结果。它的创新特点包括CLIP权重推导的准确降低类型检测、无参考图像质量评估、地区特定处理和高级融合技术。清晰聊天GPT标志着视语 integreation的重大进步，提高图像-文本交互，并提供了高性能的IRE解决方案。我们的案例研究表明，清晰聊天GPT可以提高IRE的通用和交互能力，同时填补现有视语模型的低级域空白。
</details></li>
</ul>
<hr>
<h2 id="Segment-Together-A-Versatile-Paradigm-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Segment-Together-A-Versatile-Paradigm-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Segment Together: A Versatile Paradigm for Semi-Supervised Medical Image Segmentation"></a>Segment Together: A Versatile Paradigm for Semi-Supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11686">http://arxiv.org/abs/2311.11686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingjie Zeng, Yutong Xie, Zilin Lu, Mengkang Lu, Yicheng Wu, Yong Xia<br>for: 这个研究旨在提高医疗影像分类 tasks 的训练，使用 semi-supervised learning 方法，并且能够应用于诊疗实际中。methods: 这个研究使用了一个名为 VerSemi 的框架，它可以统一多个 задачі，并且使用了一个动态的任务讯息来驱动模型的训练。另外，这个研究还使用了一个 synthetic task 的方法，将多个任务的训练结果融合在一起，以提高模型的精度。results: 这个研究的结果显示，VerSemi 可以在四个公开的 benchmarking 数据集上进行高效的训练，并且能够与第二个最佳方法相比，实现了大幅度的提升（例如，平均提升2.69% Dice 数据）， thereby setting a new SOTA performance for semi-supervised medical image segmentation.<details>
<summary>Abstract</summary>
Annotation scarcity has become a major obstacle for training powerful deep-learning models for medical image segmentation, restricting their deployment in clinical scenarios. To address it, semi-supervised learning by exploiting abundant unlabeled data is highly desirable to boost the model training. However, most existing works still focus on limited medical tasks and underestimate the potential of learning across diverse tasks and multiple datasets. Therefore, in this paper, we introduce a \textbf{Ver}satile \textbf{Semi}-supervised framework (VerSemi) to point out a new perspective that integrates various tasks into a unified model with a broad label space, to exploit more unlabeled data for semi-supervised medical image segmentation. Specifically, we introduce a dynamic task-prompted design to segment various targets from different datasets. Next, this unified model is used to identify the foreground regions from all labeled data, to capture cross-dataset semantics. Particularly, we create a synthetic task with a cutmix strategy to augment foreground targets within the expanded label space. To effectively utilize unlabeled data, we introduce a consistency constraint. This involves aligning aggregated predictions from various tasks with those from the synthetic task, further guiding the model in accurately segmenting foreground regions during training. We evaluated our VerSemi model on four public benchmarking datasets. Extensive experiments demonstrated that VerSemi can consistently outperform the second-best method by a large margin (e.g., an average 2.69\% Dice gain on four datasets), setting new SOTA performance for semi-supervised medical image segmentation. The code will be released.
</details>
<details>
<summary>摘要</summary>
Annotation scarcity has become a major obstacle for training powerful deep-learning models for medical image segmentation, restricting their deployment in clinical scenarios. To address it, semi-supervised learning by exploiting abundant unlabeled data is highly desirable to boost the model training. However, most existing works still focus on limited medical tasks and underestimate the potential of learning across diverse tasks and multiple datasets. Therefore, in this paper, we introduce a верatile semi-supervised framework (VerSemi) to point out a new perspective that integrates various tasks into a unified model with a broad label space, to exploit more unlabeled data for semi-supervised medical image segmentation. Specifically, we introduce a dynamic task-prompted design to segment various targets from different datasets. Next, this unified model is used to identify the foreground regions from all labeled data, to capture cross-dataset semantics. Particularly, we create a synthetic task with a cutmix strategy to augment foreground targets within the expanded label space. To effectively utilize unlabeled data, we introduce a consistency constraint. This involves aligning aggregated predictions from various tasks with those from the synthetic task, further guiding the model in accurately segmenting foreground regions during training. We evaluated our VerSemi model on four public benchmarking datasets. Extensive experiments demonstrated that VerSemi can consistently outperform the second-best method by a large margin (e.g., an average 2.69\% Dice gain on four datasets), setting new SOTA performance for semi-supervised medical image segmentation. The code will be released.
</details></li>
</ul>
<hr>
<h2 id="PMP-Swin-Multi-Scale-Patch-Message-Passing-Swin-Transformer-for-Retinal-Disease-Classification"><a href="#PMP-Swin-Multi-Scale-Patch-Message-Passing-Swin-Transformer-for-Retinal-Disease-Classification" class="headerlink" title="PMP-Swin: Multi-Scale Patch Message Passing Swin Transformer for Retinal Disease Classification"></a>PMP-Swin: Multi-Scale Patch Message Passing Swin Transformer for Retinal Disease Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11669">http://arxiv.org/abs/2311.11669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Yang, Zhiming Cheng, Tengjin Weng, Shucheng He, Yaqi Wang, Xin Ye, Shuai Wang</li>
<li>for: 这个研究旨在提出一个新的架构，以帮助早期识别眼睛疾病，并且能够实现高精度的多类别分类。</li>
<li>methods: 我们提出了一个基于Message Passing机制的Patch Message Passing（PMP）模组，以建立全局的交互和捕捉不同疾病之间的微妙差异。此外，我们还将多个PMP模组组合在一起，以捕捉不同大小的病理特征。</li>
<li>results: 我们在两个 dataset 上进行了严格的实验，结果显示我们的方法具有较高的效果，较前一代方法。<details>
<summary>Abstract</summary>
Retinal disease is one of the primary causes of visual impairment, and early diagnosis is essential for preventing further deterioration. Nowadays, many works have explored Transformers for diagnosing diseases due to their strong visual representation capabilities. However, retinal diseases exhibit milder forms and often present with overlapping signs, which pose great difficulties for accurate multi-class classification. Therefore, we propose a new framework named Multi-Scale Patch Message Passing Swin Transformer for multi-class retinal disease classification. Specifically, we design a Patch Message Passing (PMP) module based on the Message Passing mechanism to establish global interaction for pathological semantic features and to exploit the subtle differences further between different diseases. Moreover, considering the various scale of pathological features we integrate multiple PMP modules for different patch sizes. For evaluation, we have constructed a new dataset, named OPTOS dataset, consisting of 1,033 high-resolution fundus images photographed by Optos camera and conducted comprehensive experiments to validate the efficacy of our proposed method. And the results on both the public dataset and our dataset demonstrate that our method achieves remarkable performance compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
retinal disease 是一种主要的视力障碍原因，早期诊断非常重要，以防止进一步的衰退。现在，许多研究已经利用 Transformers 来诊断疾病，因为它们具有强视觉表示能力。然而，Retinal diseases 表现为柔性的形式，经常出现相互重叠的征标，这会对准确多类分类造成很大的挑战。因此，我们提出了一个新的框架，即 Multi-Scale Patch Message Passing Swin Transformer，用于多类 Retinal disease 分类。具体来说，我们设计了一个 Patch Message Passing（PMP）模块，基于Message Passing机制，以便在不同疾病之间建立全局交互，并利用不同疾病之间的微妙差异。此外，考虑到不同疾病的特征缺失，我们将多个 PMP 模块集成到不同的 patch 大小中。为了评估我们的提议方法，我们创建了一个新的数据集，名为 OPTOS 数据集，包含 1,033 个高分辨率背部图像，通过 Optos 摄像机拍摄，并进行了全面的实验来验证我们的提议方法的效果。结果表明，我们的方法在公共数据集和我们的数据集上都具有惊人的表现，与现有方法相比。
</details></li>
</ul>
<hr>
<h2 id="OmniSeg3D-Omniversal-3D-Segmentation-via-Hierarchical-Contrastive-Learning"><a href="#OmniSeg3D-Omniversal-3D-Segmentation-via-Hierarchical-Contrastive-Learning" class="headerlink" title="OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning"></a>OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11666">http://arxiv.org/abs/2311.11666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyang Ying, Yixuan Yin, Jinzhi Zhang, Fan Wang, Tao Yu, Ruqi Huang, Lu Fang</li>
<li>for: 这 paper 旨在提供一种通用的3D场景分割方法，能够无restriction地分割多种对象，同时反映Scene中的层次结构。</li>
<li>methods: 该方法基于一种层次对比学习框架，通过两步来实现：首先，设计了一种新的层次表示，基于类型不敏感的2D分割来模型像素之间的多层关系。其次，从3D特征场景中抽取出图像特征，并在不同层次进行分组，可以根据层次关系进行聚合或分离。</li>
<li>results: 对于高质量的3D分割和准确的层次结构理解，该方法显示了效果。同时，一个图形用户界面可以帮助用户进行自由交互，以实现通用的3D分割。<details>
<summary>Abstract</summary>
Towards holistic understanding of 3D scenes, a general 3D segmentation method is needed that can segment diverse objects without restrictions on object quantity or categories, while also reflecting the inherent hierarchical structure. To achieve this, we propose OmniSeg3D, an omniversal segmentation method aims for segmenting anything in 3D all at once. The key insight is to lift multi-view inconsistent 2D segmentations into a consistent 3D feature field through a hierarchical contrastive learning framework, which is accomplished by two steps. Firstly, we design a novel hierarchical representation based on category-agnostic 2D segmentations to model the multi-level relationship among pixels. Secondly, image features rendered from the 3D feature field are clustered at different levels, which can be further drawn closer or pushed apart according to the hierarchical relationship between different levels. In tackling the challenges posed by inconsistent 2D segmentations, this framework yields a global consistent 3D feature field, which further enables hierarchical segmentation, multi-object selection, and global discretization. Extensive experiments demonstrate the effectiveness of our method on high-quality 3D segmentation and accurate hierarchical structure understanding. A graphical user interface further facilitates flexible interaction for omniversal 3D segmentation.
</details>
<details>
<summary>摘要</summary>
为了实现全面理解三维场景，我们需要一种通用的三维分割方法，可以不受物体数量或类别的限制，同时还能够反映内在的层次结构。为此，我们提出了OmniSeg3D方法，它是一种能够同时分割所有三维场景中的任何事物的全面分割方法。关键思想是通过层次对比学习框架，将多视图不一致的二维分割提升到一致的三维特征场景中，这是通过两个步骤完成的。首先，我们设计了一种新的层次表示，基于类型不敏感的二维分割来模型像素之间的多级关系。其次，从三维特征场景中渲染出的图像特征被划分到不同层次，并且可以根据层次关系进行聚合或分离。在处理不一致的二维分割时，这个框架实现了一个全局一致的三维特征场景，这使得可以进行层次分割、多对象选择和全局精炸。我们的方法在高质量三维分割和准确的层次结构理解方面进行了广泛的实验，并且提供了一个图形用户界面，以便对全面的三维分割进行满意的互动。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Spatio-Temporal-Context-for-Temporally-Consistent-Robust-3D-Human-Motion-Recovery-from-Monocular-Videos"><a href="#Enhanced-Spatio-Temporal-Context-for-Temporally-Consistent-Robust-3D-Human-Motion-Recovery-from-Monocular-Videos" class="headerlink" title="Enhanced Spatio-Temporal Context for Temporally Consistent Robust 3D Human Motion Recovery from Monocular Videos"></a>Enhanced Spatio-Temporal Context for Temporally Consistent Robust 3D Human Motion Recovery from Monocular Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11662">http://arxiv.org/abs/2311.11662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sushovan Chanda, Amogh Tiwari, Lokender Tiwari, Brojeshwar Bhowmick, Avinash Sharma, Hrishav Barua</li>
<li>for: 本研究旨在提高从单摄影ideo中 recuperate 3D人体姿态、形状和运动的精度和可靠性，解决自身遮挡、低光照、复杂的人体姿态和深度抽象等问题。</li>
<li>methods: 该方法使用人体意识特征表示法和每帧初始化和自similarity自注意力来提高每帧的空间时间特征，并使用LSTM进行pose和形状参数的精度修正。</li>
<li>results: 实验结果表明，该方法在公共数据集上达到了 significatively 低的加速误差，并在所有关键量化评价指标上超越现有状态的方法，包括部分遮挡、复杂的姿态和相对较低的照明条件下。<details>
<summary>Abstract</summary>
Recovering temporally consistent 3D human body pose, shape and motion from a monocular video is a challenging task due to (self-)occlusions, poor lighting conditions, complex articulated body poses, depth ambiguity, and limited availability of annotated data. Further, doing a simple perframe estimation is insufficient as it leads to jittery and implausible results. In this paper, we propose a novel method for temporally consistent motion estimation from a monocular video. Instead of using generic ResNet-like features, our method uses a body-aware feature representation and an independent per-frame pose and camera initialization over a temporal window followed by a novel spatio-temporal feature aggregation by using a combination of self-similarity and self-attention over the body-aware features and the perframe initialization. Together, they yield enhanced spatiotemporal context for every frame by considering remaining past and future frames. These features are used to predict the pose and shape parameters of the human body model, which are further refined using an LSTM. Experimental results on the publicly available benchmark data show that our method attains significantly lower acceleration error and outperforms the existing state-of-the-art methods over all key quantitative evaluation metrics, including complex scenarios like partial occlusion, complex poses and even relatively low illumination.
</details>
<details>
<summary>摘要</summary>
取回时间相关的3D人体姿态、形状和运动从单目视频中回复是一项具有挑战性的任务，原因包括自我遮挡、低光照条件、复杂的人体姿态、深度模糊和有限的标注数据。此外，单个帧估计也不够，会导致缓慢且不合理的结果。在这篇论文中，我们提出了一种新的单目视频中的时间相关运动估计方法。而不是使用通用的ResNet类型特征，我们的方法使用人体意识特征表示和每帧姿态和摄像头初始化独立于时间窗口，然后使用一种组合自我相似和自我注意力来聚合体静态特征。这些特征用于预测人体模型的姿态和形状参数，并进一步使用LSTM进行精度调整。实验结果表明，我们的方法在公共可用的benchmark数据上取得了明显更低的加速错误和与现有状态的方法相比，在所有关键量化评价指标上占优。包括部分遮挡、复杂的姿态和低光照等复杂的场景。
</details></li>
</ul>
<hr>
<h2 id="Double-Condensing-Attention-Condenser-Leveraging-Attention-in-Deep-Learning-to-Detect-Skin-Cancer-from-Skin-Lesion-Images"><a href="#Double-Condensing-Attention-Condenser-Leveraging-Attention-in-Deep-Learning-to-Detect-Skin-Cancer-from-Skin-Lesion-Images" class="headerlink" title="Double-Condensing Attention Condenser: Leveraging Attention in Deep Learning to Detect Skin Cancer from Skin Lesion Images"></a>Double-Condensing Attention Condenser: Leveraging Attention in Deep Learning to Detect Skin Cancer from Skin Lesion Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11656">http://arxiv.org/abs/2311.11656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi-en Amy Tai, Elizabeth Janes, Chris Czarnecki, Alexander Wong</li>
<li>for: 这个研究是为了探讨皮肤癌检测，并提出一个具有高效性和可持续性的深度学习模型。</li>
<li>methods: 本研究使用了Double-Condensing Attention Condensers（DC-AC）来实现更快速和更有效率的computation，并采用了一个自我注意力架构来检测皮肤癌。</li>
<li>results: 研究获得了一个可公开使用的深度学习模型，并在SIIM-ISIC Melanoma Classification Challenge中获得了state-of-the-art的成绩。<details>
<summary>Abstract</summary>
Skin cancer is the most common type of cancer in the United States and is estimated to affect one in five Americans. Recent advances have demonstrated strong performance on skin cancer detection, as exemplified by state of the art performance in the SIIM-ISIC Melanoma Classification Challenge; however these solutions leverage ensembles of complex deep neural architectures requiring immense storage and compute costs, and therefore may not be tractable. A recent movement for TinyML applications is integrating Double-Condensing Attention Condensers (DC-AC) into a self-attention neural network backbone architecture to allow for faster and more efficient computation. This paper explores leveraging an efficient self-attention structure to detect skin cancer in skin lesion images and introduces a deep neural network design with DC-AC customized for skin cancer detection from skin lesion images. The final model is publicly available as a part of a global open-source initiative dedicated to accelerating advancement in machine learning to aid clinicians in the fight against cancer.
</details>
<details>
<summary>摘要</summary>
美国最常见的癌症是皮肤癌，其影响率高达一 из五美国人。最新的进展表明深度学习在皮肤癌检测方面表现出色，例如在SIIM-ISIC瘤癌分类挑战中的状态对抗性表现。然而，这些解决方案具有复杂的深度神经网络结构，需要巨大的存储和计算成本，因此可能不太实用。在小型机器学习（TinyML）应用中，把 Double-Condensing Attention Condensers（DC-AC）integrated into a self-attention neural network backbone architecture可以实现更快和高效的计算。本文探讨利用高效的自注意结构来检测皮肤癌，并介绍了一种特化于皮肤癌检测的深度神经网络设计，使用DC-AC。最终模型将公开提供，并成为全球开源Initiave的一部分，旨在加速机器学习的进步，以 помочь临床医生在抗癌战中。
</details></li>
</ul>
<hr>
<h2 id="Cancer-Net-PCa-Data-An-Open-Source-Benchmark-Dataset-for-Prostate-Cancer-Clinical-Decision-Support-using-Synthetic-Correlated-Diffusion-Imaging-Data"><a href="#Cancer-Net-PCa-Data-An-Open-Source-Benchmark-Dataset-for-Prostate-Cancer-Clinical-Decision-Support-using-Synthetic-Correlated-Diffusion-Imaging-Data" class="headerlink" title="Cancer-Net PCa-Data: An Open-Source Benchmark Dataset for Prostate Cancer Clinical Decision Support using Synthetic Correlated Diffusion Imaging Data"></a>Cancer-Net PCa-Data: An Open-Source Benchmark Dataset for Prostate Cancer Clinical Decision Support using Synthetic Correlated Diffusion Imaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11647">http://arxiv.org/abs/2311.11647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hayden Gunraj, Chi-en Amy Tai, Alexander Wong</li>
<li>For: The paper is written to introduce an open-source benchmark dataset of volumetric correlated diffusion imaging (CDI$^s$) data for prostate cancer (PCa) patients, which can aid clinicians in the global fight against cancer.* Methods: The paper uses CDI$^s$ imaging data from a patient cohort of 200 patient cases, along with full annotations (gland masks, tumor masks, and PCa diagnosis for each tumor) to analyze the demographic and label region diversity of the dataset for potential biases.* Results: The paper introduces Cancer-Net PCa-Data, the first-ever public dataset of CDI$^s$ imaging data for PCa, which can be used to advance research efforts in machine learning and imaging research for PCa diagnosis and treatment.<details>
<summary>Abstract</summary>
The recent introduction of synthetic correlated diffusion (CDI$^s$) imaging has demonstrated significant potential in the realm of clinical decision support for prostate cancer (PCa). CDI$^s$ is a new form of magnetic resonance imaging (MRI) designed to characterize tissue characteristics through the joint correlation of diffusion signal attenuation across different Brownian motion sensitivities. Despite the performance improvement, the CDI$^s$ data for PCa has not been previously made publicly available. In our commitment to advance research efforts for PCa, we introduce Cancer-Net PCa-Data, an open-source benchmark dataset of volumetric CDI$^s$ imaging data of PCa patients. Cancer-Net PCa-Data consists of CDI$^s$ volumetric images from a patient cohort of 200 patient cases, along with full annotations (gland masks, tumor masks, and PCa diagnosis for each tumor). We also analyze the demographic and label region diversity of Cancer-Net PCa-Data for potential biases. Cancer-Net PCa-Data is the first-ever public dataset of CDI$^s$ imaging data for PCa, and is a part of the global open-source initiative dedicated to advancement in machine learning and imaging research to aid clinicians in the global fight against cancer.
</details>
<details>
<summary>摘要</summary>
最近，另一种称为合成相关扩散（CDI$^s$）成像技术的引入，在临床决策支持方面表现出了很大的潜力。CDI$^s$ 是一种基于核磁共振成像（MRI）技术，用于Characterize 组织特征，通过不同的温馈振荡敏感性的共同协同。尽管表现得更好，但CDI$^s$ 数据 дляPCa尚未公开可用。为了推动PCa 的研究，我们介绍了Cancer-Net PCa-Data，一个开源的PCa患者数据集。Cancer-Net PCa-Data包括200个病例的CDI$^s$ 三维成像数据，以及每个肿瘤的患者标签（包括腺体涂抹、肿瘤涂抹和PCa诊断）。我们还分析了Cancer-Net PCa-Data的人口和标注区域多样性，以确定可能的偏见。Cancer-Net PCa-Data是PCa 的第一个公开的CDI$^s$ 成像数据集，是全球开源的机器学习和成像研究的一部分，旨在帮助临床医生在抗癌斗争中获得更好的工具。
</details></li>
</ul>
<hr>
<h2 id="CastDet-Toward-Open-Vocabulary-Aerial-Object-Detection-with-CLIP-Activated-Student-Teacher-Learning"><a href="#CastDet-Toward-Open-Vocabulary-Aerial-Object-Detection-with-CLIP-Activated-Student-Teacher-Learning" class="headerlink" title="CastDet: Toward Open Vocabulary Aerial Object Detection with CLIP-Activated Student-Teacher Learning"></a>CastDet: Toward Open Vocabulary Aerial Object Detection with CLIP-Activated Student-Teacher Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11646">http://arxiv.org/abs/2311.11646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Li, Weiwei Guo, Dunyun He, Jiaqi Zhou, Yuze Gao, Wenxian Yu</li>
<li>for: 这篇论文是关于 aerial image 中的物体探测任务，它的目的是实现开放词汇物体探测（OVD），能够在无需训练标签的情况下检测到新的物体类别。</li>
<li>methods: 这篇论文提出了 CastDet，一个基于 CLIP 模型的学生-老师开放词汇物体探测框架。这个框架使用 CLIP 模型作为额外的全知之师，将其整合到学生-老师自学 проце序中，以提高新物体提案和类别。</li>
<li>results: 根据实验结果，CastDet 可以实现高品质的开放词汇物体探测，例如在 VisDroneZSD dataset 上取得 40.0 HM 的最高分，比前一代方法 Detic&#x2F;ViLD 高出 26.9&#x2F;21.1 分。<details>
<summary>Abstract</summary>
Object detection in aerial images is a pivotal task for various earth observation applications, whereas current algorithms learn to detect only a pre-defined set of object categories demanding sufficient bounding-box annotated training samples and fail to detect novel object categories. In this paper, we consider open-vocabulary object detection (OVD) in aerial images that enables the characterization of new objects beyond training categories on the earth surface without annotating training images for these new categories. The performance of OVD depends on the quality of class-agnostic region proposals and pseudo-labels that can generalize well to novel object categories. To simultaneously generate high-quality proposals and pseudo-labels, we propose CastDet, a CLIP-activated student-teacher open-vocabulary object Detection framework. Our end-to-end framework within the student-teacher mechanism employs the CLIP model as an extra omniscient teacher of rich knowledge into the student-teacher self-learning process. By doing so, our approach boosts novel object proposals and classification. Furthermore, we design a dynamic label queue technique to maintain high-quality pseudo labels during batch training and mitigate label imbalance. We conduct extensive experiments on multiple existing aerial object detection datasets, which are set up for the OVD task. Experimental results demonstrate our CastDet achieving superior open-vocabulary detection performance, e.g., reaching 40.0 HM (Harmonic Mean), which outperforms previous methods Detic/ViLD by 26.9/21.1 on the VisDroneZSD dataset.
</details>
<details>
<summary>摘要</summary>
“ aerial 图像中的物体检测是多种地球观测应用中的关键任务，现有算法只能学习预定的物体类别，需要充足的 bounding-box 注意力标注训练样本，并不能检测新的物体类别。在这篇论文中，我们考虑了开放词汇物体检测（OVD）在 aerial 图像中，允许在地球表面上无需注意力标注训练样本中检测新的物体类别。OVD 的性能取决于高质量的类型不敏感区域提案和 Pseudo-标签，这些可以很好地泛化到新的物体类别。为了同时生成高质量的提案和 Pseudo-标签，我们提出了 CastDet，一个基于 CLIP 的学生-教师开放词汇物体检测框架。我们的端到端框架在学生-教师机制中使用 CLIP 模型作为额外的全知之 teacher，从而提高新物体提案和分类。此外，我们设计了动态标签队列技术来保持批处理训练期间高质量的 Pseudo-标签，并 Mitigate 标签偏斜。我们在多个现有的 aerial 物体检测数据集上进行了广泛的实验，实验结果表明我们的 CastDet 在开放词汇检测任务中表现出色，例如在 VisDroneZSD 数据集上达到 40.0 HM（和律 mean），比前方法 Detic/ViLD 的性能提高 26.9/21.1。”
</details></li>
</ul>
<hr>
<h2 id="Video-Face-Re-Aging-Toward-Temporally-Consistent-Face-Re-Aging"><a href="#Video-Face-Re-Aging-Toward-Temporally-Consistent-Face-Re-Aging" class="headerlink" title="Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging"></a>Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11642">http://arxiv.org/abs/2311.11642</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyugorithm/VFRAN">https://github.com/kyugorithm/VFRAN</a></li>
<li>paper_authors: Abdul Muqeet, Kyuchul Lee, Bumsoo Kim, Yohan Hong, Hyungrae Lee, Woonggon Kim, Kwang Hee Lee</li>
<li>for: 提高视频人脸年龄变换的精度和一致性</li>
<li>methods: 基于新生成的同时维护人脸年龄和个体特征的Synthetic video dataset，以及一种基线建模方案和三种专门设计的时间一致度评价指标</li>
<li>results: 在公共数据集上（如VFHQ和CelebV-HQ）进行了广泛的实验，发现我们的方法在年龄变换和时间一致性方面都有较好的表现，超过现有方法的表现。<details>
<summary>Abstract</summary>
Video face re-aging deals with altering the apparent age of a person to the target age in videos. This problem is challenging due to the lack of paired video datasets maintaining temporal consistency in identity and age. Most re-aging methods process each image individually without considering the temporal consistency of videos. While some existing works address the issue of temporal coherence through video facial attribute manipulation in latent space, they often fail to deliver satisfactory performance in age transformation. To tackle the issues, we propose (1) a novel synthetic video dataset that features subjects across a diverse range of age groups; (2) a baseline architecture designed to validate the effectiveness of our proposed dataset, and (3) the development of three novel metrics tailored explicitly for evaluating the temporal consistency of video re-aging techniques. Our comprehensive experiments on public datasets, such as VFHQ and CelebV-HQ, show that our method outperforms the existing approaches in terms of both age transformation and temporal consistency.
</details>
<details>
<summary>摘要</summary>
【文本】视频面部重新年龄处理是修改视频中人物的显示年龄，这问题具有挑战性，主要由于缺乏匹配的视频数据集，保持时间一致性和人脸特征的唯一性。大多数现有方法是处理每帧图像，不考虑视频中的时间一致性。有些现有方法通过视频人脸特征在幂空间中的修改来解决问题，但它们经常无法实现满意的年龄变换表现。为解决这些问题，我们提议以下方法：1. 一个新的人工生成的视频数据集，包含多个年龄组的主题。2. 一个基eline架构，用于验证我们的提议的效果。3. 三个专门为评估视频重新年龄技术的新指标的开发。我们对公共数据集，如VFHQ和CelebV-HQ进行了全面的实验，结果表明，我们的方法在年龄变换和时间一致性两个方面都高于现有方法。【翻译】视频面部重新年龄处理是修改视频中人物的显示年龄，这问题具有挑战性，主要由于缺乏匹配的视频数据集，保持时间一致性和人脸特征的唯一性。大多数现有方法是处理每帧图像，不考虑视频中的时间一致性。有些现有方法通过视频人脸特征在幂空间中的修改来解决问题，但它们经常无法实现满意的年龄变换表现。为解决这些问题，我们提议以下方法：1. 一个新的人工生成的视频数据集，包含多个年龄组的主题。2. 一个基eline架构，用于验证我们的提议的效果。3. 三个专门为评估视频重新年龄技术的新指标的开发。我们对公共数据集，如VFHQ和CelebV-HQ进行了全面的实验，结果表明，我们的方法在年龄变换和时间一致性两个方面都高于现有方法。
</details></li>
</ul>
<hr>
<h2 id="Reti-Diff-Illumination-Degradation-Image-Restoration-with-Retinex-based-Latent-Diffusion-Model"><a href="#Reti-Diff-Illumination-Degradation-Image-Restoration-with-Retinex-based-Latent-Diffusion-Model" class="headerlink" title="Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model"></a>Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11638">http://arxiv.org/abs/2311.11638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunminghe/reti-diff">https://github.com/chunminghe/reti-diff</a></li>
<li>paper_authors: Chunming He, Chengyu Fang, Yulun Zhang, Kai Li, Longxiang Tang, Chenyu You, Fengyang Xiao, Zhenhua Guo, Xiu Li</li>
<li>for: 这种研究旨在提高降低图像质量的方法，以提高降低图像中的可见性和缓解降低照明的不良影响。</li>
<li>methods: 该方法基于扩散模型（DM），并使用一种新的解决方案called Reti-Diff，包括Retinex基于的秘密DM（RLDM）和Retinex指导变换（RGformer）。RLDM使用Retinex知识提取反射和照明秘密，然后RGformer使用这些秘密来导向图像特征的分解。</li>
<li>results: 对三种IDIR任务进行了广泛的实验，并证明Reti-Diff可以超过现有方法，同时在下游应用中也有优秀表现。<details>
<summary>Abstract</summary>
Illumination degradation image restoration (IDIR) techniques aim to improve the visibility of degraded images and mitigate the adverse effects of deteriorated illumination. Among these algorithms, diffusion model (DM)-based methods have shown promising performance but are often burdened by heavy computational demands and pixel misalignment issues when predicting the image-level distribution. To tackle these problems, we propose to leverage DM within a compact latent space to generate concise guidance priors and introduce a novel solution called Reti-Diff for the IDIR task. Reti-Diff comprises two key components: the Retinex-based latent DM (RLDM) and the Retinex-guided transformer (RGformer). To ensure detailed reconstruction and illumination correction, RLDM is empowered to acquire Retinex knowledge and extract reflectance and illumination priors. These priors are subsequently utilized by RGformer to guide the decomposition of image features into their respective reflectance and illumination components. Following this, RGformer further enhances and consolidates the decomposed features, resulting in the production of refined images with consistent content and robustness to handle complex degradation scenarios. Extensive experiments show that Reti-Diff outperforms existing methods on three IDIR tasks, as well as downstream applications. Code will be available at \url{https://github.com/ChunmingHe/Reti-Diff}.
</details>
<details>
<summary>摘要</summary>
illumination degradation image restoration (IDIR) 技术目的是提高受损图像的可见度，并减轻照明的不良影响。在这些算法中，基于分散模型（DM）的方法已经显示出了有前途的表现，但它们经常受到高计算量和像素不对齐问题的压力，尤其是在预测图像水平分布时。为了解决这些问题，我们提议利用DM在紧凑的尺度空间中生成简洁的指导假设，并 introduce a novel solution called Reti-Diff for the IDIR task. Reti-Diff包括两个关键组件：Retinex基于的秘密DM（RLDM）和Retinex引导的 transformer（RGformer）。为确保细节重建和照明更正，RLDM被授权了Retinex知识，并EXTRACT reflectance和照明假设。这些假设后来被RGformer使用来导引图像特征的分解成它们的准确的reflectance和照明组成部分。接下来，RGformer进一步加强和结合这些分解的特征，从而生成高质量的恢复图像，满足复杂的损坏enario下的需求。广泛的实验表明，Reti-Diff在三个 IDIR 任务上表现出优于现有方法，以及下游应用程序。代码将于 \url{https://github.com/ChunmingHe/Reti-Diff} 上提供。
</details></li>
</ul>
<hr>
<h2 id="Generating-Realistic-Counterfactuals-for-Retinal-Fundus-and-OCT-Images-using-Diffusion-Models"><a href="#Generating-Realistic-Counterfactuals-for-Retinal-Fundus-and-OCT-Images-using-Diffusion-Models" class="headerlink" title="Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using Diffusion Models"></a>Generating Realistic Counterfactuals for Retinal Fundus and OCT Images using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11629">http://arxiv.org/abs/2311.11629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indu Ilanchezian, Valentyn Boreiko, Laura Kühlewein, Ziwei Huang, Murat Seçkin Ayhan, Matthias Hein, Lisa Koch, Philipp Berens</li>
<li>for: 该论文旨在用Counterfactual reasoning来解释医疗决策或评估可能的选择。</li>
<li>methods: 该论文使用了一种扩散模型，并与一个对抗性强的分类器一起使用，以生成高度真实的Counterfactual retinal fundus图像和Optical coherence tomography (OCT) B-scan。</li>
<li>results: 在用户研究中，域专家发现使用我们的方法生成的Counterfactuals更真实，甚至与真实图像难以区分。<details>
<summary>Abstract</summary>
Counterfactual reasoning is often used in a clinical setting to explain decisions or weigh alternatives. Therefore, for imaging based modalities such as ophthalmology, it would be beneficial to be able to create counterfactual images, illustrating the answer to the question: "If the subject had had diabetic retinopathy, how would the fundus image have looked?" Here, we demonstrate that using a diffusion model in combination with an adversarially robust classifier trained on retinal disease classification tasks enables generation of highly realistic counterfactuals of retinal fundus images and optical coherence tomorgraphy (OCT) B-scans. Ideally, these classifiers encode the salient features indicative for each disease class and can steer the diffusion model to show realistic disease signs or remove disease-related lesions in a realistic way. Importantly, in a user study, domain experts found the counterfactuals generated using our method significantly more realistic than counterfactuals generated from a previous method, and even indistiguishable from realistic images.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用反因果思维在临床设置中解释决策或评估选项。因此，对于基于成像技术的modalities，如眼科学，可以创建反因果图像，示出问题“如果subject有糖尿病抑或病变，图像如何看起来？”。我们展示了一种使用扩散模型和对眼病分类任务进行 adversarially robust 训练的类ifikator，可以生成高度真实的反因果眼科图像和optical coherence tomography（OCT）B-scan。这些分类器能够捕捉眼病的重要特征，并使扩散模型显示真实的病变或在真实的方式下除病理学病变。在用户研究中，领域专家发现使用我们的方法生成的反因果图像比之前的方法生成的更真实，甚至与真实图像难以区分。Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Semantic-Preserved-Point-based-Human-Avatar"><a href="#Semantic-Preserved-Point-based-Human-Avatar" class="headerlink" title="Semantic-Preserved Point-based Human Avatar"></a>Semantic-Preserved Point-based Human Avatar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11614">http://arxiv.org/abs/2311.11614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lixiang Lin, Jianke Zhu</li>
<li>for: 实现现实主义AR&#x2F;VR和数字娱乐体验，我们提出了首个点云基于人工智能模型，涵盖了数字人类表达的全范围。</li>
<li>methods: 我们使用两个多层感知（MLP）来模型姿态依赖的形变和线性皮革（LBS） веса。人物外观表示方式采用解码器和每个点附加的特征。</li>
<li>results: 我们的方法不仅减少了训练和推断时间，还可以更好地理解人体运动的 semantic 信息。我们还提出了一种将SMPL-X模型的semantic信息传递到点上的新方法，以便进行虚拟试穿和人物组合。实验结果表明我们的方法的效果。<details>
<summary>Abstract</summary>
To enable realistic experience in AR/VR and digital entertainment, we present the first point-based human avatar model that embodies the entirety expressive range of digital humans. We employ two MLPs to model pose-dependent deformation and linear skinning (LBS) weights. The representation of appearance relies on a decoder and the features that attached to each point. In contrast to alternative implicit approaches, the oriented points representation not only provides a more intuitive way to model human avatar animation but also significantly reduces both training and inference time. Moreover, we propose a novel method to transfer semantic information from the SMPL-X model to the points, which enables to better understand human body movements. By leveraging the semantic information of points, we can facilitate virtual try-on and human avatar composition through exchanging the points of same category across different subjects. Experimental results demonstrate the efficacy of our presented method.
</details>
<details>
<summary>摘要</summary>
为实现现实主义的AR/VR和数字娱乐体验，我们提出了首个点云基于人类模型，涵盖了整个数字人类表达范围。我们使用两个多层感知（MLP）来模型 pose-dependent 扭变和线性皮肤（LBS）质量。人物外表表示靠decoder和每个点附加的特征。相比拥有其他几何方法，点云表示方法不仅提供了更直观的人物动画模型，还能够显著降低训练和推理时间。此外，我们提出了一种将SMPL-X模型中的semantic信息传递到点上的新方法，以便更好地理解人体运动。通过使用点上的semantic信息，我们可以实现虚拟试穿和人物组合，通过交换同类点的交换。实验结果表明我们提出的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="CurriculumLoc-Enhancing-Cross-Domain-Geolocalization-through-Multi-Stage-Refinement"><a href="#CurriculumLoc-Enhancing-Cross-Domain-Geolocalization-through-Multi-Stage-Refinement" class="headerlink" title="CurriculumLoc: Enhancing Cross-Domain Geolocalization through Multi-Stage Refinement"></a>CurriculumLoc: Enhancing Cross-Domain Geolocalization through Multi-Stage Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11604">http://arxiv.org/abs/2311.11604</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/npupilab/curriculumloc">https://github.com/npupilab/curriculumloc</a></li>
<li>paper_authors: Boni Hu, Lin Chen, Runjian Chen, Shuhui Bu, Pengcheng Han, Haowei Li</li>
<li>for: 这篇论文旨在提出一种可靠、可扩展的视觉地理Localization方法，能够在不同光照和视角的情况下提供准确的全球定位估计。</li>
<li>methods: 该方法基于curriculum设计，首先认知 semantic scene，然后测量geometry structure。具有全球semantic意识和本地几何验证的新的键点检测和描述算法，以及多Stage Refinement管道，使得方法具有 robustness to appearance changing和极端视角变化。</li>
<li>results: 在 TerraTrack 和 ALTO 两个数据集上进行了广泛的实验，结果表明，该方法可以实现上述的可靠视觉地理Localization解决方案，并 achieved new high recall@1 scores of 62.6% and 94.5% on ALTO 数据集，使得该方法在视觉地理Localization领域具有优秀的性能。<details>
<summary>Abstract</summary>
Visual geolocalization is a cost-effective and scalable task that involves matching one or more query images, taken at some unknown location, to a set of geo-tagged reference images. Existing methods, devoted to semantic features representation, evolving towards robustness to a wide variety between query and reference, including illumination and viewpoint changes, as well as scale and seasonal variations. However, practical visual geolocalization approaches need to be robust in appearance changing and extreme viewpoint variation conditions, while providing accurate global location estimates. Therefore, inspired by curriculum design, human learn general knowledge first and then delve into professional expertise. We first recognize semantic scene and then measure geometric structure. Our approach, termed CurriculumLoc, involves a delicate design of multi-stage refinement pipeline and a novel keypoint detection and description with global semantic awareness and local geometric verification. We rerank candidates and solve a particular cross-domain perspective-n-point (PnP) problem based on these keypoints and corresponding descriptors, position refinement occurs incrementally. The extensive experimental results on our collected dataset, TerraTrack and a benchmark dataset, ALTO, demonstrate that our approach results in the aforementioned desirable characteristics of a practical visual geolocalization solution. Additionally, we achieve new high recall@1 scores of 62.6% and 94.5% on ALTO, with two different distances metrics, respectively. Dataset, code and trained models are publicly available on https://github.com/npupilab/CurriculumLoc.
</details>
<details>
<summary>摘要</summary>
Visual地理位置定位是一项成本效益和可扩展的任务，即将一个或多个查询图像， captured at unknown location，与一组地理标记的参考图像匹配。现有方法主要关注semantic特征表示，逐渐向robustness against各种查询和参考图像的变化，包括照明和视点变化、Scale和季节变化。然而，实际应用中的视地理位置定位方法需要对应变化和极端视点变化的Robustness，同时提供准确的全球位置估计。因此，我们受到curriculum设计的 inspirations，人类首先学习通用知识，然后深入掌握专业技能。我们首先识别semantic scene，然后测量几何结构。我们的方法，称之为CurriculumLoc，包括细腻的多阶段精度提升管道和一种新型的键点检测和描述，具有全球semantic意识和本地几何验证。我们在这些键点和相应的描述符之间进行重新排名，并根据这些键点进行特定的cross-domain perspective-n-point（PnP）问题的解决。在我们收集的dataset上， TerraTrack 和一个标准dataset， ALTO 上进行了广泛的实验，结果表明，我们的方法具有上述desirable特点，并且实现了新的高recall@1分数，分别为62.6%和94.5%。 dataset、代码和训练模型都可以在https://github.com/npupilab/CurriculumLoc上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Equilibrium-Diffusion-Restoration-with-Parallel-Sampling"><a href="#Deep-Equilibrium-Diffusion-Restoration-with-Parallel-Sampling" class="headerlink" title="Deep Equilibrium Diffusion Restoration with Parallel Sampling"></a>Deep Equilibrium Diffusion Restoration with Parallel Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11600">http://arxiv.org/abs/2311.11600</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caojiezhang/DeqIR">https://github.com/caojiezhang/DeqIR</a></li>
<li>paper_authors: Jiezhang Cao, Yue Shi, Kai Zhang, Yulun Zhang, Radu Timofte, Luc Van Gool</li>
<li>for: 这个论文的目的是重新考虑基于扩散模型的图像恢复方法，通过深度平衡（DEQ）稳定点系统来解决长样本链的问题，并在单个图像 sampling 中实现高质量图像恢复。</li>
<li>methods: 该方法使用了基于扩散模型的 JOINT 多变量稳定点系统来解决整个样本链，并通过分析解得出了单个图像 sampling 的解。此外，该方法还计算了快速的梯度值，并发现初始化优化可以提高性能并控制生成方向。</li>
<li>results:  experiments 表明，该方法在典型的恢复任务和实际应用中得到了有效的结果，并且在单个图像 sampling 中实现了高质量图像恢复。code 和模型将会公开发布。<details>
<summary>Abstract</summary>
Diffusion-based image restoration (IR) methods aim to use diffusion models to recover high-quality (HQ) images from degraded images and achieve promising performance. Due to the inherent property of diffusion models, most of these methods need long serial sampling chains to restore HQ images step-by-step. As a result, it leads to expensive sampling time and high computation costs. Moreover, such long sampling chains hinder understanding the relationship between the restoration results and the inputs since it is hard to compute the gradients in the whole chains. In this work, we aim to rethink the diffusion-based IR models through a different perspective, i.e., a deep equilibrium (DEQ) fixed point system. Specifically, we derive an analytical solution by modeling the entire sampling chain in diffusion-based IR models as a joint multivariate fixed point system. With the help of the analytical solution, we are able to conduct single-image sampling in a parallel way and restore HQ images without training. Furthermore, we compute fast gradients in DEQ and found that initialization optimization can boost performance and control the generation direction. Extensive experiments on benchmarks demonstrate the effectiveness of our proposed method on typical IR tasks and real-world settings. The code and models will be made publicly available.
</details>
<details>
<summary>摘要</summary>
各种扩散基本的图像修复（IR）方法都是使用扩散模型来恢复受损图像并实现出色的性能。由于扩散模型的内置性质，大多数这些方法需要长串行采样链来恢复高质量（HQ）图像步骤通过。这会导致昂贵的采样时间和高计算成本。此外，这些长链难以计算整个链的梯度，使得理解恢复结果和输入之间的关系变得困难。在这种情况下，我们想重新思考扩散基本的IR模型，通过一种新的视角，即深度平衡（DEQ）固定点系统。我们在这种系统中模型了扩散基本的采样链，并 derive了一个分析解。通过这个分析解，我们可以在平行的方式上进行单张图像采样，并在没有训练的情况下恢复HQ图像。此外，我们在DEQ中计算了快速的梯度，并发现初始化优化可以提高性能并控制生成方向。我们在典型的IR任务和实际场景中进行了广泛的实验，并证明了我们的提议的效果。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Predicting-urban-tree-cover-from-incomplete-point-labels-and-limited-background-information"><a href="#Predicting-urban-tree-cover-from-incomplete-point-labels-and-limited-background-information" class="headerlink" title="Predicting urban tree cover from incomplete point labels and limited background information"></a>Predicting urban tree cover from incomplete point labels and limited background information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11592">http://arxiv.org/abs/2311.11592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Zhang, Ankit Kariryaa, Venkanna Babu Guthula, Christian Igel, Stefan Oehmcke</li>
<li>for: 本研究旨在利用有限数据和深度学习方法映射城市内部的城市树。</li>
<li>methods: 本研究使用深度学习方法进行 semantic segmentation 的高分辨率遥感图像，并利用有限数据和开源地理数据库的标注来划分城市树。</li>
<li>results: 实验结果表明，系统可以生成包括非 ули道树的树覆盖图，而不需要提供树划分。系统的性能会随着开源地理数据库的使用而下降。<details>
<summary>Abstract</summary>
Trees inside cities are important for the urban microclimate, contributing positively to the physical and mental health of the urban dwellers. Despite their importance, often only limited information about city trees is available. Therefore in this paper, we propose a method for mapping urban trees in high-resolution aerial imagery using limited datasets and deep learning. Deep learning has become best-practice for this task, however, existing approaches rely on large and accurately labelled training datasets, which can be difficult and expensive to obtain. However, often noisy and incomplete data may be available that can be combined and utilized to solve more difficult tasks than those datasets were intended for. This paper studies how to combine accurate point labels of urban trees along streets with crowd-sourced annotations from an open geographic database to delineate city trees in remote sensing images, a task which is challenging even for humans. To that end, we perform semantic segmentation of very high resolution aerial imagery using a fully convolutional neural network. The main challenge is that our segmentation maps are sparsely annotated and incomplete. Small areas around the point labels of the street trees coming from official and crowd-sourced data are marked as foreground class. Crowd-sourced annotations of streets, buildings, etc. define the background class. Since the tree data is incomplete, we introduce a masking to avoid class confusion. Our experiments in Hamburg, Germany, showed that the system is able to produce tree cover maps, not limited to trees along streets, without providing tree delineations. We evaluated the method on manually labelled trees and show that performance drastically deteriorates if the open geographic database is not used.
</details>
<details>
<summary>摘要</summary>
城市内的树木非常重要对于城市微气候，对于城市居民的身体和心理健康做出积极贡献。尽管其重要性，但有限的城市树木信息通常只有有限的信息。因此，我们在这篇论文中提出了一种使用有限数据和深度学习映射城市树木在高分辨率飞行图像中的方法。深度学习已成为最佳实践，但现有方法通常需要大量和准确标注的训练数据，这可能是expensive和困难的。然而，有时候可以获得噪音和不完整的数据，这些数据可以与其他数据结合使用，以解决更难的任务。本文研究如何将精度点标签的城市树木与开源地理数据库的批注结合，以在遥感图像中标识城市树木，这是人类even challenging的任务。为此，我们使用了全连接神经网络进行semantic segmentation。主要挑战在于我们的分 segmentation maps 是 incomplete和稀疏标注的。小区域附近街道树木的点标签从官方和开源数据库获取，标记为前景类。开源数据库中的街道、建筑等批注定义背景类。由于树木数据不完整，我们引入了masking，以避免类冲突。我们在 Hamburg, Germany 进行了实验，结果表明系统能够生成树 cover maps，不仅限于街道上的树木，而无需提供树木定义。我们对手动标注的树木进行评估，结果表明，如果不使用开源地理数据库，系统性能会很差。
</details></li>
</ul>
<hr>
<h2 id="AKConv-Convolutional-Kernel-with-Arbitrary-Sampled-Shapes-and-Arbitrary-Number-of-Parameters"><a href="#AKConv-Convolutional-Kernel-with-Arbitrary-Sampled-Shapes-and-Arbitrary-Number-of-Parameters" class="headerlink" title="AKConv: Convolutional Kernel with Arbitrary Sampled Shapes and Arbitrary Number of Parameters"></a>AKConv: Convolutional Kernel with Arbitrary Sampled Shapes and Arbitrary Number of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11587">http://arxiv.org/abs/2311.11587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cv-zhangxin/akconv">https://github.com/cv-zhangxin/akconv</a></li>
<li>paper_authors: Xin Zhang, Yingze Song, Tingting Song, Degang Yang, Yichen Ye, Jie Zhou, Liming Zhang<br>for: 这个论文的目的是提出一种新的卷积操作方法，以提高深度学习网络的性能。methods: 这个论文使用的方法是 Alterable Kernel Convolution (AKConv)，它使用可变的卷积kernel来提供更多的选择空间 для网络的性能和资源的负担之间的平衡。具体来说，AKConv使用一种新的坐标生成算法来定义卷积kernel的初始位置，并通过偏移来调整卷积样本的形状。results: 在COCO2017、VOC 7+12和VisDrone-DET2021等标准数据集上进行了对象检测实验，显示了AKConv的优势。AKConv可以视为一种替换标准卷积操作的替换方法，以提高网络的性能。codes可以在<a target="_blank" rel="noopener" href="https://github.com/CV-ZhangXin/AKConv%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CV-ZhangXin/AKConv中找到。</a><details>
<summary>Abstract</summary>
Neural networks based on convolutional operations have achieved remarkable results in the field of deep learning, but there are two inherent flaws in standard convolutional operations. On the one hand, the convolution operation be confined to a local window and cannot capture information from other locations, and its sampled shapes is fixed. On the other hand, the size of the convolutional kernel is fixed to k $\times$ k, which is a fixed square shape, and the number of parameters tends to grow squarely with size. It is obvious that the shape and size of targets are various in different datasets and at different locations. Convolutional kernels with fixed sample shapes and squares do not adapt well to changing targets. In response to the above questions, the Alterable Kernel Convolution (AKConv) is explored in this work, which gives the convolution kernel an arbitrary number of parameters and arbitrary sampled shapes to provide richer options for the trade-off between network overhead and performance. In AKConv, we define initial positions for convolutional kernels of arbitrary size by means of a new coordinate generation algorithm. To adapt to changes for targets, we introduce offsets to adjust the shape of the samples at each position. Moreover, we explore the effect of the neural network by using the AKConv with the same size and different initial sampled shapes. AKConv completes the process of efficient feature extraction by irregular convolutional operations and brings more exploration options for convolutional sampling shapes. Object detection experiments on representative datasets COCO2017, VOC 7+12 and VisDrone-DET2021 fully demonstrate the advantages of AKConv. AKConv can be used as a plug-and-play convolutional operation to replace convolutional operations to improve network performance. The code for the relevant tasks can be found at https://github.com/CV-ZhangXin/AKConv.
</details>
<details>
<summary>摘要</summary>
neural networks based on convolutional operations have achieved remarkable results in deep learning, but there are two inherent flaws in standard convolutional operations. on the one hand, the convolution operation is confined to a local window and cannot capture information from other locations, and its sampled shapes is fixed. on the other hand, the size of the convolutional kernel is fixed to k x k, which is a fixed square shape, and the number of parameters tends to grow squarely with size. it is obvious that the shape and size of targets are various in different datasets and at different locations. convolutional kernels with fixed sample shapes and squares do not adapt well to changing targets. in response to the above questions, the Alterable Kernel Convolution (AKConv) is explored in this work, which gives the convolution kernel an arbitrary number of parameters and arbitrary sampled shapes to provide richer options for the trade-off between network overhead and performance. in AKConv, we define initial positions for convolutional kernels of arbitrary size by means of a new coordinate generation algorithm. to adapt to changes for targets, we introduce offsets to adjust the shape of the samples at each position. Moreover, we explore the effect of the neural network by using the AKConv with the same size and different initial sampled shapes. AKConv completes the process of efficient feature extraction by irregular convolutional operations and brings more exploration options for convolutional sampling shapes. object detection experiments on representative datasets COCO2017, VOC 7+12 and VisDrone-DET2021 fully demonstrate the advantages of AKConv. AKConv can be used as a plug-and-play convolutional operation to replace convolutional operations to improve network performance. the code for the relevant tasks can be found at https://github.com/CV-ZhangXin/AKConv.
</details></li>
</ul>
<hr>
<h2 id="SeaDSC-A-video-based-unsupervised-method-for-dynamic-scene-change-detection-in-unmanned-surface-vehicles"><a href="#SeaDSC-A-video-based-unsupervised-method-for-dynamic-scene-change-detection-in-unmanned-surface-vehicles" class="headerlink" title="SeaDSC: A video-based unsupervised method for dynamic scene change detection in unmanned surface vehicles"></a>SeaDSC: A video-based unsupervised method for dynamic scene change detection in unmanned surface vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11580">http://arxiv.org/abs/2311.11580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linh Trinh, Ali Anwar, Siegfried Mercelis</li>
<li>for: 本研究旨在探讨USV（无人水面船）视频数据中的动态场景变化检测问题。</li>
<li>methods: 我们提出了一种完全无监督学习方法，利用修改后的VQ-VAE-2生成图像模型在多个海上数据集上进行特征提取。我们还提出了一种创新的相似度分数技术，通过网格计算在缓存特征上直接计算连续帧中的相似度。</li>
<li>results: 我们使用RoboWhaler nautical视频数据集进行实验，并证明了我们的技术可以高效地检测动态场景变化。<details>
<summary>Abstract</summary>
Recently, there has been an upsurge in the research on maritime vision, where a lot of works are influenced by the application of computer vision for Unmanned Surface Vehicles (USVs). Various sensor modalities such as camera, radar, and lidar have been used to perform tasks such as object detection, segmentation, object tracking, and motion planning. A large subset of this research is focused on the video analysis, since most of the current vessel fleets contain the camera's onboard for various surveillance tasks. Due to the vast abundance of the video data, video scene change detection is an initial and crucial stage for scene understanding of USVs. This paper outlines our approach to detect dynamic scene changes in USVs. To the best of our understanding, this work represents the first investigation of scene change detection in the maritime vision application. Our objective is to identify significant changes in the dynamic scenes of maritime video data, particularly those scenes that exhibit a high degree of resemblance. In our system for dynamic scene change detection, we propose completely unsupervised learning method. In contrast to earlier studies, we utilize a modified cutting-edge generative picture model called VQ-VAE-2 to train on multiple marine datasets, aiming to enhance the feature extraction. Next, we introduce our innovative similarity scoring technique for directly calculating the level of similarity in a sequence of consecutive frames by utilizing grid calculation on retrieved features. The experiments were conducted using a nautical video dataset called RoboWhaler to showcase the efficient performance of our technique.
</details>
<details>
<summary>摘要</summary>
近来，有很多研究者在海上视觉领域进行了大量的研究，这些研究受到了计算机视觉技术应用于无人水面车（USV）的影响。不同的感知模式，如摄像头、雷达和激光雷达，都被用于实现对象检测、分割、跟踪和动作规划等任务。大多数当前的船舶舰队都装备有摄像头，因此海上视觉应用中的视频分析占据了大量的研究领域。由于海上视频数据的庞大量，视频场景变化检测是海上视觉应用中的初始和重要阶段。本文介绍了我们的场景变化检测方法，以及我们对这些方法的初步调研。在我们的系统中，我们提出了一种完全不监督学习方法，使用修改后的VQ-VAE-2模型来训练多个海洋数据集，以提高特征提取。然后，我们引入了我们的创新的相似度评分技术，通过网格计算在相应特征上 retrieve 的数据序列中直接计算相似度水平。我们在使用 RoboWhaler 海上视频数据集进行实验，以示出我们的方法的高效性。
</details></li>
</ul>
<hr>
<h2 id="A-3D-Multi-Style-Cross-Modality-Segmentation-Framework-for-Segmenting-Vestibular-Schwannoma-and-Cochlea"><a href="#A-3D-Multi-Style-Cross-Modality-Segmentation-Framework-for-Segmenting-Vestibular-Schwannoma-and-Cochlea" class="headerlink" title="A 3D Multi-Style Cross-Modality Segmentation Framework for Segmenting Vestibular Schwannoma and Cochlea"></a>A 3D Multi-Style Cross-Modality Segmentation Framework for Segmenting Vestibular Schwannoma and Cochlea</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11578">http://arxiv.org/abs/2311.11578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhou Zhuang</li>
<li>for: 这个论文目的是为了使用不同的扫描图像来分割耳膜 schwannoma 和内耳听力器官区域。</li>
<li>methods: 该论文提出了一种3D多样式交叉Modal segmentation框架，包括多样式翻译和自学习分割阶段。在这个框架中，首先使用最小最大正规化、VOXEL SIZE resampling和中心剪辑来从ceT1和hrT2扫描图像中获得固定大小的子Volume для训练。然后，通过多样式图像翻译阶段来超越INTENSITY分布差异 между不匹配的多Modal扫描图像。specifically，设计了三种不同的翻译网络，用于生成多样式和实际的目标尺寸的目标Volume。最后，通过自学习分割阶段使用nnU-Net框架和迭代自学习方法使用pseudo-标签来训练准确的分割模型在无标签目标Domain中。</li>
<li>results: 在crossMoDA2023验证集上，我们的方法取得了可喜的结果，其中VS肿瘤和内耳听力器官区域的mean DSC值分别为72.78%和80.64%，ASSD值分别为5.85 mm和0.25 mm。此外，对于内耳和外耳的分割，我们的方法取得了DSC值分别为59.77%和77.14%。<details>
<summary>Abstract</summary>
The crossMoDA2023 challenge aims to segment the vestibular schwannoma (sub-divided into intra- and extra-meatal components) and cochlea regions of unlabeled hrT2 scans by leveraging labeled ceT1 scans. In this work, we proposed a 3D multi-style cross-modality segmentation framework for the crossMoDA2023 challenge, including the multi-style translation and self-training segmentation phases. Considering heterogeneous distributions and various image sizes in multi-institutional scans, we first utilize the min-max normalization, voxel size resampling, and center cropping to obtain fixed-size sub-volumes from ceT1 and hrT2 scans for training. Then, we perform the multi-style image translation phase to overcome the intensity distribution discrepancy between unpaired multi-modal scans. Specifically, we design three different translation networks with 2D or 2.5D inputs to generate multi-style and realistic target-like volumes from labeled ceT1 volumes. Finally, we perform the self-training volumetric segmentation phase in the target domain, which employs the nnU-Net framework and iterative self-training method using pseudo-labels for training accurate segmentation models in the unlabeled target domain. On the crossMoDA2023 validation dataset, our method produces promising results and achieves the mean DSC values of 72.78% and 80.64% and ASSD values of 5.85 mm and 0.25 mm for VS tumor and cochlea regions, respectively. Moreover, for intra- and extra-meatal regions, our method achieves the DSC values of 59.77% and 77.14%, respectively.
</details>
<details>
<summary>摘要</summary>
crossMoDA2023挑战的目标是将vestibular schwannoma（分为内耳和外耳组成部分）和cochlea区域分割成unlabeled hrT2扫描图像中，通过利用标注的ceT1扫描图像。在这项工作中，我们提出了一个3D多样式交叉Modal segmentation框架 дляcrossMoDA2023挑战，包括多样式翻译和自我培训分 segmentation阶段。鉴于多个机构的扫描图像具有不同的分布和大小，我们首先使用最小最大normalization、 voxel size resampling和center cropping来 obtaint fixed-size sub-volumes from ceT1和hrT2扫描图像 для训练。然后，我们实施多样式图像翻译阶段，以解决不同Modal scans的intensity分布差异。我们设计了三种不同的翻译网络，用于生成多样式和真实的目标尺寸的target-like 扫描图像。最后，我们实施了自我培训volumetric segmentation阶段，使用nnU-Net框架和迭代式自我培训方法，使用pseudo-labels进行训练准确的分 segmentation模型在没有标注的目标域中。在crossMoDA2023验证集上，我们的方法实现了可塑性的结果，得到了mean DSC值为72.78%和80.64%，和ASSD值为5.85 mm和0.25 mm дляVS肿瘤和cochlea区域，分别。此外， для内耳和外耳区域，我们的方法实现了DSC值为59.77%和77.14%。
</details></li>
</ul>
<hr>
<h2 id="CORE-MM-Complex-Open-Ended-Reasoning-Evaluation-For-Multi-Modal-Large-Language-Models"><a href="#CORE-MM-Complex-Open-Ended-Reasoning-Evaluation-For-Multi-Modal-Large-Language-Models" class="headerlink" title="CORE-MM: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models"></a>CORE-MM: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11567">http://arxiv.org/abs/2311.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil Mrini, Xudong Lin, Yiqi Wang, Bohan Zhai, Jianbo Yuan, Heng Wang, Hongxia Yang</li>
<li>For: The paper aims to evaluate the reasoning capabilities of Multi-modal Large Language Models (MLLMs) by creating a benchmark dataset with complex reasoning tasks.* Methods: The authors manually curate a benchmark dataset that includes three key reasoning categories: deductive, abductive, and analogical reasoning. They incorporate intermediate reasoning steps into their evaluation criteria to assess the reasoning ability of MLLMs.* Results: The authors evaluate a selection of representative MLLMs using their rigorously developed open-ended multi-step elaborate reasoning benchmark and show that their evaluation scheme is more effective in measuring the reasoning capabilities of MLLMs compared to existing benchmarks.Here’s the same information in Simplified Chinese text:* For: 本文旨在评估多模态大语言模型（MLLMs）的逻辑能力，通过创建一个具有复杂逻辑任务的 benchmark 数据集。* Methods: 作者们手动抽取了一个 benchmark 数据集，包括三种关键的逻辑类别：推理、假设和类比逻辑。他们在评估 criteria 中包含了中间逻辑步骤，以评估 MLLMs 的逻辑能力。* Results: 作者们使用自己开发的严格的开端多步详细逻辑 benchmark，评估了一些代表性的 MLLMs，并显示其评估方法比现有的 benchmark 更有效地评估 MLLMs 的逻辑能力。<details>
<summary>Abstract</summary>
Multi-modal Large Language Models (MLLMs) are increasingly prominent in the field of artificial intelligence. These models not only excel in traditional vision-language tasks but also demonstrate impressive performance in contemporary multi-modal benchmarks. Although many of these benchmarks attempt to holistically evaluate MLLMs, they typically concentrate on basic reasoning tasks, often yielding only simple yes/no or multi-choice responses. These methods naturally lead to confusion and difficulties in conclusively determining the reasoning capabilities of MLLMs. To mitigate this issue, we manually curate a benchmark dataset specifically designed for MLLMs, with a focus on complex reasoning tasks. Our benchmark comprises three key reasoning categories: deductive, abductive, and analogical reasoning. The queries in our dataset are intentionally constructed to engage the reasoning capabilities of MLLMs in the process of generating answers. For a fair comparison across various MLLMs, we incorporate intermediate reasoning steps into our evaluation criteria. In instances where an MLLM is unable to produce a definitive answer, its reasoning ability is evaluated by requesting intermediate reasoning steps. If these steps align with our manual annotations, appropriate scores are assigned. This evaluation scheme resembles methods commonly used in human assessments, such as exams or assignments, and represents what we consider a more effective assessment technique compared with existing benchmarks. We evaluate a selection of representative MLLMs using this rigorously developed open-ended multi-step elaborate reasoning benchmark, designed to challenge and accurately measure their reasoning capabilities. The code and data will be released at https://core-mm.github.io/
</details>
<details>
<summary>摘要</summary>
多Modal大语言模型（MLLMs）在人工智能领域日益突出。这些模型不仅在传统的视力语言任务中表现出色，而且在当今的多Modal测试 benchmark 中也显示出了卓越的表现。虽然许多这些 benchmark 尝试通过涵盖性评估 MLLMs，但它们通常集中在基本的理解任务上，经常产生简单的是或否或多选答案。这些方法自然导致混乱和确定 MLLMs 的理解能力的困难。为解决这个问题，我们手动编辑了一个特有的 benchmark 数据集，专门为 MLLMs 设计。我们的 benchmark 包括三个关键的理解类别：推理、推理和类比理解。我们的查询是故意构建的，以便让 MLLMs 在回答时使用其理解能力。为 Ensure 公平的比较，我们在评估标准中包括中间的理解步骤。在 MLLMs 无法提供明确答案时，我们评估其理解能力，请求中间理解步骤。如果这些步骤与我们的手动标注相符，就会分配相应的分数。这种评估方法与人类评估方法类似，例如考试或作业，并且代表我们认为更有效的评估方法，比较现有的 benchmark。我们使用这些精心开发的开放式多步逻辑 benchmark，评估一 selección 的代表 MLLMs，以挑战和准确测量它们的理解能力。我们的代码和数据将在 <https://core-mm.github.io/> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Does-complimentary-information-from-multispectral-imaging-improve-face-presentation-attack-detection"><a href="#Does-complimentary-information-from-multispectral-imaging-improve-face-presentation-attack-detection" class="headerlink" title="Does complimentary information from multispectral imaging improve face presentation attack detection?"></a>Does complimentary information from multispectral imaging improve face presentation attack detection?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11566">http://arxiv.org/abs/2311.11566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Narayan Vetrekar, Raghavendra Ramachandra, Sushma Venkatesh, Jyoti D. Pawar, R. S. Gad</li>
<li>for: 本研究旨在提高面部识别验证系统的安全性，通过利用多spectral成像技术检测面部展示诈骗。</li>
<li>methods: 本研究使用多spectral成像技术 constructed for eight different presentation artifacts resulted from three different artifact species，并 introduce Face Presentation Attack Multispectral (FPAMS) database。</li>
<li>results: 实验结果表明，基于多spectral成像技术的Face Presentation Attack Detection (PAD)方法可以提高face识别验证系统的安全性，并且可以 combining two different ways (image fusion和score fusion) to improve the face PAD。<details>
<summary>Abstract</summary>
Presentation Attack Detection (PAD) has been extensively studied, particularly in the visible spectrum. With the advancement of sensing technology beyond the visible range, multispectral imaging has gained significant attention in this direction. We present PAD based on multispectral images constructed for eight different presentation artifacts resulted from three different artifact species. In this work, we introduce Face Presentation Attack Multispectral (FPAMS) database to demonstrate the significance of employing multispectral imaging. The goal of this work is to study complementary information that can be combined in two different ways (image fusion and score fusion) from multispectral imaging to improve the face PAD. The experimental evaluation results present an extensive qualitative analysis of 61650 sample multispectral images collected for bonafide and artifacts. The PAD based on the score fusion and image fusion method presents superior performance, demonstrating the significance of employing multispectral imaging to detect presentation artifacts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="NePF-Neural-Photon-Field-for-Single-Stage-Inverse-Rendering"><a href="#NePF-Neural-Photon-Field-for-Single-Stage-Inverse-Rendering" class="headerlink" title="NePF: Neural Photon Field for Single-Stage Inverse Rendering"></a>NePF: Neural Photon Field for Single-Stage Inverse Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11555">http://arxiv.org/abs/2311.11555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuen-Yue Tsui, Qin Zou</li>
<li>for:  Addresses the ill-posed inverse rendering problem from multi-view images, recovering geometry, material, and illumination in a single stage.</li>
<li>methods:  Utilizes physical implications of neural implicit surfaces and view-dependent radiance, introduces coordinate-based illumination model and subsurface scattering for diffuse estimation.</li>
<li>results:  Demonstrates superiority in recovering high-fidelity geometry and visual-plausible material attributes, evaluated on real and synthetic datasets.Here’s the full text in Simplified Chinese:</li>
<li>for: 这种方法是为了解决多视图图像中的缺失 inverse rendering 问题，从多个视角图像中恢复geometry、物理和照明的单个阶段。</li>
<li>methods: 这种方法利用了神经凋散表面的物理含义和视角相关的辐射，并引入了坐标基于的照明模型和渐入散射模型 для减少辐射。</li>
<li>results: 这种方法可以准确地恢复高精度的geometry和可见可靠的物理属性，并在实际和 sintetic 数据集上进行了评估。<details>
<summary>Abstract</summary>
We present a novel single-stage framework, Neural Photon Field (NePF), to address the ill-posed inverse rendering from multi-view images. Contrary to previous methods that recover the geometry, material, and illumination in multiple stages and extract the properties from various multi-layer perceptrons across different neural fields, we question such complexities and introduce our method - a single-stage framework that uniformly recovers all properties. NePF achieves this unification by fully utilizing the physical implication behind the weight function of neural implicit surfaces and the view-dependent radiance. Moreover, we introduce an innovative coordinate-based illumination model for rapid volume physically-based rendering. To regularize this illumination, we implement the subsurface scattering model for diffuse estimation. We evaluate our method on both real and synthetic datasets. The results demonstrate the superiority of our approach in recovering high-fidelity geometry and visual-plausible material attributes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的单阶段框架，神经光场（NePF），用于解决多视图图像中的不充分定义的反推问题。与之前的方法不同，我们的方法不需要在多个层次感知器中分别提取不同的物理属性，而是通过全面利用神经积分表面的物理逻辑和视角相关的辐射来实现单一阶段的全属性恢复。此外，我们还提出了一种创新的坐标基础照明模型，用于快速实现物理基于的立体渲染。为了规范这种照明，我们实现了渐入散射模型来估算散射颗粒。我们对真实和 sintetic 数据集进行评估，结果表明我们的方法可以高效地恢复高质量的几何和视觉可能的物理属性。
</details></li>
</ul>
<hr>
<h2 id="Unearthing-Common-Inconsistency-for-Generalisable-Deepfake-Detection"><a href="#Unearthing-Common-Inconsistency-for-Generalisable-Deepfake-Detection" class="headerlink" title="Unearthing Common Inconsistency for Generalisable Deepfake Detection"></a>Unearthing Common Inconsistency for Generalisable Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11549">http://arxiv.org/abs/2311.11549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beilin Chu, Xuan Xu, Weike You, Linna Zhou</li>
<li>for: 这个研究旨在提出一种能够对不同的 manipulate 方法进行检测的方法，以扩展现有的图像级别检测方法。</li>
<li>methods: 本研究使用了一种基于自我指导的对照学习的方法，以捕捉几个不同的伪造技术中的几个共同特征。</li>
<li>results: 实验结果显示了本方法在不同的伪造领域中的扩展能力，并且比起现有的方法更具有抗压缩性和抗预期性。<details>
<summary>Abstract</summary>
Deepfake has emerged for several years, yet efficient detection techniques could generalize over different manipulation methods require further research. While current image-level detection method fails to generalize to unseen domains, owing to the domain-shift phenomenon brought by CNN's strong inductive bias towards Deepfake texture, video-level one shows its potential to have both generalization across multiple domains and robustness to compression. We argue that although distinct face manipulation tools have different inherent bias, they all disrupt the consistency between frames, which is a natural characteristic shared by authentic videos. Inspired by this, we proposed a detection approach by capturing frame inconsistency that broadly exists in different forgery techniques, termed unearthing-common-inconsistency (UCI). Concretely, the UCI network based on self-supervised contrastive learning can better distinguish temporal consistency between real and fake videos from multiple domains. We introduced a temporally-preserved module method to introduce spatial noise perturbations, directing the model's attention towards temporal information. Subsequently, leveraging a multi-view cross-correlation learning module, we extensively learn the disparities in temporal representations between genuine and fake samples. Extensive experiments demonstrate the generalization ability of our method on unseen Deepfake domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Event-Camera-Data-Dense-Pre-training"><a href="#Event-Camera-Data-Dense-Pre-training" class="headerlink" title="Event Camera Data Dense Pre-training"></a>Event Camera Data Dense Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11533">http://arxiv.org/abs/2311.11533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Yang, Liyuan Pan, Liu Liu</li>
<li>for: 这个论文旨在开发一个用于预训神经网络的自我超vised learning框架，并将其应用于离散预测任务中的事件摄像头数据上。</li>
<li>methods: 我们的方法仅使用事件数据进行训练，并将事件图像转换成事件补丁特征，自动找到事件补丁之间的相似关系，将事件补丁分组成不同的上下文，并强制这些上下文之间的相似性来学习特征。</li>
<li>results: 我们的方法在转移到下游对于离散预测任务的训练中表现出色，特别是在DSEC-Flow标准库中获得了最佳成绩。单一的模型在这个挑战性的benchmark中获得了第一名。<details>
<summary>Abstract</summary>
This paper introduces a self-supervised learning framework designed for pre-training neural networks tailored to dense prediction tasks using event camera data. Our approach utilizes solely event data for training.   Transferring achievements from dense RGB pre-training directly to event camera data yields subpar performance. This is attributed to the spatial sparsity inherent in an event image (converted from event data), where many pixels do not contain information. To mitigate this sparsity issue, we encode an event image into event patch features, automatically mine contextual similarity relationships among patches, group the patch features into distinctive contexts, and enforce context-to-context similarities to learn discriminative event features.   For training our framework, we curate a synthetic event camera dataset featuring diverse scene and motion patterns. Transfer learning performance on downstream dense prediction tasks illustrates the superiority of our method over state-of-the-art approaches. Notably, our single model secured the top position in the challenging DSEC-Flow benchmark.
</details>
<details>
<summary>摘要</summary>
The proposed framework is trained using a synthetic event camera dataset featuring diverse scene and motion patterns. The transfer learning performance on downstream dense prediction tasks demonstrates the superiority of the method over state-of-the-art approaches. Notably, the single model achieved the top position in the challenging DSEC-Flow benchmark.Simplified Chinese translation:这篇论文介绍了一种基于自我超级vised学习的神经网络预训练框架，特化于透明度预测任务使用事件相机数据。我们的方法不依赖于RGB数据，直接将RGB预训练的成果转移到事件相机数据上则表现不佳，这是因为事件图像具有较强的空间稀畴性， многи个像素不含信息。为了解决这个稀畴性问题，我们将事件图像编码成事件 patch特征，自动挖掘事件 patch 之间的相似性关系，将 patch 特征分组到不同的上下文中，并且在不同上下文之间强制实施相似性来学习特征。为了训练我们的框架，我们创建了一个synthetic事件相机数据集，该数据集包含多种场景和运动模式。我们的方法在下游密集预测任务上表现出了优于当前最佳方法。特别是，我们的单个模型在挑战性的 DSEC-Flow 比赛中占据了第一名。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Category-Discovery-in-Semantic-Segmentation"><a href="#Generalized-Category-Discovery-in-Semantic-Segmentation" class="headerlink" title="Generalized Category Discovery in Semantic Segmentation"></a>Generalized Category Discovery in Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11525">http://arxiv.org/abs/2311.11525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jethropeng/gcdss">https://github.com/jethropeng/gcdss</a></li>
<li>paper_authors: Zhengyuan Peng, Qijian Tian, Jianqing Xu, Yizhang Jin, Xuequan Lu, Xin Tan, Yuan Xie, Lizhuang Ma</li>
<li>for: 这则论文探索了一个称为通用类别发现（Generalized Category Discovery，GCD）的新设定，旨在将无标签图像分类为不同的类别。</li>
<li>methods: 这则论文提出了一个简单 yet effective的框架，将GCD问题视为一个mask classification的任务。此外，它还提出了一个基eline方法和一个叫做邻居关系导向的Mask Clustering Algorithm（NeRG-MaskCA），用于实现mask categorization。</li>
<li>results: 这则论文发现了GCD问题的可行性和发现 novel object classes 的可能性。它还提出了一个基于自己的方法所生成的pseudo-labels来对其他模型进行超vised training，从而允许它们 segment novel classes。<details>
<summary>Abstract</summary>
This paper explores a novel setting called Generalized Category Discovery in Semantic Segmentation (GCDSS), aiming to segment unlabeled images given prior knowledge from a labeled set of base classes. The unlabeled images contain pixels of the base class or novel class. In contrast to Novel Category Discovery in Semantic Segmentation (NCDSS), there is no prerequisite for prior knowledge mandating the existence of at least one novel class in each unlabeled image. Besides, we broaden the segmentation scope beyond foreground objects to include the entire image. Existing NCDSS methods rely on the aforementioned priors, making them challenging to truly apply in real-world situations. We propose a straightforward yet effective framework that reinterprets the GCDSS challenge as a task of mask classification. Additionally, we construct a baseline method and introduce the Neighborhood Relations-Guided Mask Clustering Algorithm (NeRG-MaskCA) for mask categorization to address the fragmentation in semantic representation. A benchmark dataset, Cityscapes-GCD, derived from the Cityscapes dataset, is established to evaluate the GCDSS framework. Our method demonstrates the feasibility of the GCDSS problem and the potential for discovering and segmenting novel object classes in unlabeled images. We employ the generated pseudo-labels from our approach as ground truth to supervise the training of other models, thereby enabling them with the ability to segment novel classes. It paves the way for further research in generalized category discovery, broadening the horizons of semantic segmentation and its applications. For details, please visit https://github.com/JethroPeng/GCDSS
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Liver-Tumor-Prediction-with-Advanced-Attention-Mechanisms-Integrated-into-a-Depth-Based-Variant-Search-Algorithm"><a href="#Liver-Tumor-Prediction-with-Advanced-Attention-Mechanisms-Integrated-into-a-Depth-Based-Variant-Search-Algorithm" class="headerlink" title="Liver Tumor Prediction with Advanced Attention Mechanisms Integrated into a Depth-Based Variant Search Algorithm"></a>Liver Tumor Prediction with Advanced Attention Mechanisms Integrated into a Depth-Based Variant Search Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11520">http://arxiv.org/abs/2311.11520</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. Kalaiselvi, S. Anusuya</li>
<li>for: 预测肝肿瘤，提高肝病诊断和治疗规划的精度和可靠性。</li>
<li>methods: 使用卷积神经网络（CNN）和深度基于变体搜索算法（CNN-DS-AM），并实现了高精度的肝肿瘤预测。</li>
<li>results: 在计算tomography（CT）扫描数据集上测试，提出的方法实现了95.5%的肝肿瘤预测精度，超过了其他当前状态的方法。<details>
<summary>Abstract</summary>
In recent days, Deep Learning (DL) techniques have become an emerging transformation in the field of machine learning, artificial intelligence, computer vision, and so on. Subsequently, researchers and industries have been highly endorsed in the medical field, predicting and controlling diverse diseases at specific intervals. Liver tumor prediction is a vital chore in analyzing and treating liver diseases. This paper proposes a novel approach for predicting liver tumors using Convolutional Neural Networks (CNN) and a depth-based variant search algorithm with advanced attention mechanisms (CNN-DS-AM). The proposed work aims to improve accuracy and robustness in diagnosing and treating liver diseases. The anticipated model is assessed on a Computed Tomography (CT) scan dataset containing both benign and malignant liver tumors. The proposed approach achieved high accuracy in predicting liver tumors, outperforming other state-of-the-art methods. Additionally, advanced attention mechanisms were incorporated into the CNN model to enable the identification and highlighting of regions of the CT scans most relevant to predicting liver tumors. The results suggest that incorporating attention mechanisms and a depth-based variant search algorithm into the CNN model is a promising approach for improving the accuracy and robustness of liver tumor prediction. It can assist radiologists in their diagnosis and treatment planning. The proposed system achieved a high accuracy of 95.5% in predicting liver tumors, outperforming other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
近些天，深度学习（DL）技术在机器学习、人工智能、计算机视觉等领域成为了一种emerging变革。因此，研究人员和产业在医疗领域中高度支持DL技术，以预测和控制多种疾病。肝肿瘤预测是分析和治疗肝病的重要任务。这篇论文提出了一种基于卷积神经网络（CNN）和深度基本变换搜索算法（CNN-DS-AM）的新方法，以提高肝肿瘤预测的准确性和Robustness。提案的方法在一个计算Tomography（CT）扫描数据集上进行测试，包括了良性和肿瘤肝肿瘤。提案的方法实现了高度的肝肿瘤预测精度，超过了其他当前的方法。此外，提案中还 incorporated了进一步的注意力机制，以便在CT扫描中特定和高亮肝肿瘤的预测区域。结果表明，在CNN模型中integrating注意力机制和深度基本变换搜索算法是一种promising的方法，可以提高肝肿瘤预测的准确性和Robustness。这可以帮助放射科医生在诊断和治疗规划中使用。提案的系统在评估CT扫描数据集上实现了95.5%的肝肿瘤预测精度，超过了其他当前的方法。
</details></li>
</ul>
<hr>
<h2 id="Seeing-through-the-Mask-Multi-task-Generative-Mask-Decoupling-Face-Recognition"><a href="#Seeing-through-the-Mask-Multi-task-Generative-Mask-Decoupling-Face-Recognition" class="headerlink" title="Seeing through the Mask: Multi-task Generative Mask Decoupling Face Recognition"></a>Seeing through the Mask: Multi-task Generative Mask Decoupling Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11512">http://arxiv.org/abs/2311.11512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaohui Wang, Sufang Zhang, Jianteng Peng, Xinyi Wang, Yandong Guo</li>
<li>for: This paper is written for improving the performance of face recognition systems in occluded scenes, where current systems suffer from serious performance degradation.</li>
<li>methods: The proposed method, called Multi-task gEnerative mask dEcoupling face Recognition (MEER), uses a novel mask decoupling module to disentangle mask and identity information, and a joint-training strategy to restore unmasked faces and refine the recognition network.</li>
<li>results: The MEER method outperforms state-of-the-art methods on masked face recognition under realistic and synthetic occlusions benchmarks.<details>
<summary>Abstract</summary>
The outbreak of COVID-19 pandemic make people wear masks more frequently than ever. Current general face recognition system suffers from serious performance degradation,when encountering occluded scenes. The potential reason is that face features are corrupted by occlusions on key facial regions. To tackle this problem, previous works either extract identity-related embeddings on feature level by additional mask prediction, or restore the occluded facial part by generative models. However, the former lacks visual results for model interpretation, while the latter suffers from artifacts which may affect downstream recognition. Therefore, this paper proposes a Multi-task gEnerative mask dEcoupling face Recognition (MEER) network to jointly handle these two tasks, which can learn occlusionirrelevant and identity-related representation while achieving unmasked face synthesis. We first present a novel mask decoupling module to disentangle mask and identity information, which makes the network obtain purer identity features from visible facial components. Then, an unmasked face is restored by a joint-training strategy, which will be further used to refine the recognition network with an id-preserving loss. Experiments on masked face recognition under realistic and synthetic occlusions benchmarks demonstrate that the MEER can outperform the state-ofthe-art methods.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行使人们更加常 wear 面具，但现有的通用面 recognition 系统在遇到 occluded 场景时会出现严重的性能下降。这可能是因为 occlusion 对 key  facial 区域的 face 特征造成了损害。为解决这个问题，先前的工作可以通过添加 mask 预测来提取 identity-related embedding，或者使用生成模型来恢复遮盖的 facial 部分。然而，前者缺乏视觉效果，后者可能会产生遮盖 artifacts，这些遮盖 artifacts 可能会影响下游认识。因此，本文提出了一种 Multi-task gEnerative mask dEcoupling face Recognition (MEER) 网络，该网络可以同时处理这两个任务，学习 occlusion-irrelevant 和 identity-related 表示，并实现无面具 synthesis。我们首先提出了一种 novel mask decoupling module，用于分离 mask 和 identity 信息，使网络从可见 facial 部分获得纯净的 identity 特征。然后，我们使用 joint-training 策略来恢复无面具，这将被用于提高 recognition 网络的性能。实验表明，MEER 可以在实际和Synthetic occlusions benchmarks 上出perform state-of-the-art 方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/20/cs.CV_2023_11_20/" data-id="clp8zxr8p00nmn688cwqac3ro" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/20/eess.AS_2023_11_20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-11-20
        
      </div>
    </a>
  
  
    <a href="/2023/11/20/cs.AI_2023_11_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-20</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
