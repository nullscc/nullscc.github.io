
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-11-20 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Risk-averse Batch Active Inverse Reward Design paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12004 repo_url: https:&#x2F;&#x2F;github.com&#x2F;pliam1105&#x2F;RBAIRD paper_authors: Panagiotis Liampas for: 这个研究是要设计一个完美的赏金函数，以捕捉所有想要">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-11-20">
<meta property="og:url" content="https://nullscc.github.io/2023/11/20/cs.LG_2023_11_20/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Risk-averse Batch Active Inverse Reward Design paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.12004 repo_url: https:&#x2F;&#x2F;github.com&#x2F;pliam1105&#x2F;RBAIRD paper_authors: Panagiotis Liampas for: 这个研究是要设计一个完美的赏金函数，以捕捉所有想要">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-20T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-21T10:06:16.190Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_11_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/20/cs.LG_2023_11_20/" class="article-date">
  <time datetime="2023-11-20T10:00:00.000Z" itemprop="datePublished">2023-11-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-11-20
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Risk-averse-Batch-Active-Inverse-Reward-Design"><a href="#Risk-averse-Batch-Active-Inverse-Reward-Design" class="headerlink" title="Risk-averse Batch Active Inverse Reward Design"></a>Risk-averse Batch Active Inverse Reward Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.12004">http://arxiv.org/abs/2311.12004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pliam1105/RBAIRD">https://github.com/pliam1105/RBAIRD</a></li>
<li>paper_authors: Panagiotis Liampas</li>
<li>for: 这个研究是要设计一个完美的赏金函数，以捕捉所有想要的行为方面。</li>
<li>methods: 这个方法使用了一系列的询问，比较不同的赏金函数，以在单一训练环境中决定一个probability distribution。然后，透过人类提供的信息，决定最有可能性的赏金函数。</li>
<li>results: 这个研究实现了一个可靠、精确和行为内在的赏金函数设计方法，并且能够适应真实世界中的不明点。它比前一些方法更高效、更精确和更有行为内在性。<details>
<summary>Abstract</summary>
Designing a perfect reward function that depicts all the aspects of the intended behavior is almost impossible, especially generalizing it outside of the training environments. Active Inverse Reward Design (AIRD) proposed the use of a series of queries, comparing possible reward functions in a single training environment. This allows the human to give information to the agent about suboptimal behaviors, in order to compute a probability distribution over the intended reward function. However, it ignores the possibility of unknown features appearing in real-world environments, and the safety measures needed until the agent completely learns the reward function. I improved this method and created Risk-averse Batch Active Inverse Reward Design (RBAIRD), which constructs batches, sets of environments the agent encounters when being used in the real world, processes them sequentially, and, for a predetermined number of iterations, asks queries that the human needs to answer for each environment of the batch. After this process is completed in one batch, the probabilities have been improved and are transferred to the next batch. This makes it capable of adapting to real-world scenarios and learning how to treat unknown features it encounters for the first time. I also integrated a risk-averse planner, similar to that of Inverse Reward Design (IRD), which samples a set of reward functions from the probability distribution and computes a trajectory that takes the most certain rewards possible. This ensures safety while the agent is still learning the reward function, and enables the use of this approach in situations where cautiousness is vital. RBAIRD outperformed the previous approaches in terms of efficiency, accuracy, and action certainty, demonstrated quick adaptability to new, unknown features, and can be more widely used for the alignment of crucial, powerful AI models.
</details>
<details>
<summary>摘要</summary>
设计完美的奖励函数，涵盖所有行为目标方面，实际上是不可能的，尤其是泛化到训练环境之外。活动反奖设计（AIRD）提议使用一系列问题，比较可能的奖励函数在单个训练环境中。这样允许人类给agent提供关于不佳行为的信息，以计算概率分布上的奖励函数。然而，它忽略了实际环境中可能出现的未知特征，以及代理人完全学习奖励函数的安全措施。我提高了这种方法，创造了风险规避批量反奖设计（RBAIRD），它在实际世界中顺序处理批处理环境，并在 predetermined 的数量 Of iterations 后，问题人需要回答每个环境。在这个过程中，概率已经提高，并将其传递到下一个批处理。这使得它能够适应实际场景，学习在未知特征出现的第一次处理方式。我还将风险规避探索者（IRD）的风险规避探索者相同的概率分布，并计算出最可靠的奖励函数。这确保了安全性，使得代理人在学习奖励函数时，能够具有紧急应急措施。RBAIRD 在效率、准确性和行为确定性方面超越了先前的方法，快速适应新的未知特征，并可以更广泛地应用于对重要、强大 AI 模型的对齐。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learned-Atomic-Cluster-Expansion-Potentials-for-Fast-and-Quantum-Accurate-Thermal-Simulations-of-Wurtzite-AlN"><a href="#Machine-Learned-Atomic-Cluster-Expansion-Potentials-for-Fast-and-Quantum-Accurate-Thermal-Simulations-of-Wurtzite-AlN" class="headerlink" title="Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN"></a>Machine-Learned Atomic Cluster Expansion Potentials for Fast and Quantum-Accurate Thermal Simulations of Wurtzite AlN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11990">http://arxiv.org/abs/2311.11990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guang Yang, Yuan-Bin Liu, Lei Yang, Bing-Yang Cao</li>
<li>for: 这个论文的目的是为了快速和准确地模拟硼酸铝氮酸 wurtzite 的声波传输性质。</li>
<li>methods: 这个论文使用了 atomic cluster expansion（ACE）框架，开发了一个机器学习式的interatomic potential，用于模拟硼酸铝氮酸 wurtzite 的声波传输性质。</li>
<li>results: 论文通过对硼酸铝氮酸 wurtzite 的多种性质进行预测，如静态压缩 Parameter, 热Capacity, 热膨胀率, bulk modulus, 和声波普通频谱，证明了ACE potential的预测能力。此外，论文还进行了对硼酸铝氮酸 wurtzite 的声波传输性质的验证，并与DFT计算和实验数据进行比较，确认了ACE potential的总体可靠性。<details>
<summary>Abstract</summary>
Using the atomic cluster expansion (ACE) framework, we develop a machine learning interatomic potential for fast and accurately modelling the phonon transport properties of wurtzite aluminum nitride. The predictive power of the ACE potential against density functional theory (DFT) is demonstrated across a broad range of properties of w-AlN, including ground-state lattice parameters, specific heat capacity, coefficients of thermal expansion, bulk modulus, and harmonic phonon dispersions. Validation of lattice thermal conductivity is further carried out by comparing the ACE-predicted values to the DFT calculations and experiments, exhibiting the overall capability of our ACE potential in sufficiently describing anharmonic phonon interactions. As a practical application, we perform a lattice dynamics analysis using the potential to unravel the effects of biaxial strains on thermal conductivity and phonon properties of w-AlN, which is identified as a significant tuning factor for near-junction thermal design of w-AlN-based electronics.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:使用原子团扩展（ACE）框架，我们开发了一种基于机器学习的偶极材料谱可能预测 wurtzite 锆的热传输性质。我们在 w-AlN 的广泛性质上展示了 ACE  potential 的预测力，包括静态凝固参数、热容积、热膨胀率、压力系数和干扰谱峰。对于 lattice 热导率，我们进一步验证了 ACE 预测值与 DFT 计算和实验结果，manifesting ACE potential 的总体可靠性。作为实用应用，我们使用 potential 进行了固体动力学分析，探讨了由biaxial 压力所引起的 w-AlN 的热导率和凝固特性的变化，并证明了这些变化对于 near-junction 热设计的 w-AlN 基于电子器件的调整是一个重要的因素。
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-CVaR-RL-in-Low-rank-MDPs"><a href="#Provably-Efficient-CVaR-RL-in-Low-rank-MDPs" class="headerlink" title="Provably Efficient CVaR RL in Low-rank MDPs"></a>Provably Efficient CVaR RL in Low-rank MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11965">http://arxiv.org/abs/2311.11965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulai Zhao, Wenhao Zhan, Xiaoyan Hu, Ho-fung Leung, Farzan Farnia, Wen Sun, Jason D. Lee</li>
<li>for: 本文研究具有风险敏感性的 reinforcement learning（RL），目标是在固定风险容错率（CVaR）下 Maximize  conditional value at risk（CVaR）。</li>
<li>methods: 本文使用 low-rank Markov decision processes（MDPs）和 nonlinear function approximation，并提出了一种 novel Upper Confidence Bound（UCB）奖励驱动算法，用于兼顾 explore、exploit 和 representation learning。</li>
<li>results: 本文证明了其算法可以在 low-rank MDPs 中 achieve  $\tilde{O}\left(\frac{H^7 A^2 d^4}{\tau^2 \epsilon^2}\right)$ 样本复杂度，以实现 $\epsilon$-优的 CVaR。<details>
<summary>Abstract</summary>
We study risk-sensitive Reinforcement Learning (RL), where we aim to maximize the Conditional Value at Risk (CVaR) with a fixed risk tolerance $\tau$. Prior theoretical work studying risk-sensitive RL focuses on the tabular Markov Decision Processes (MDPs) setting. To extend CVaR RL to settings where state space is large, function approximation must be deployed. We study CVaR RL in low-rank MDPs with nonlinear function approximation. Low-rank MDPs assume the underlying transition kernel admits a low-rank decomposition, but unlike prior linear models, low-rank MDPs do not assume the feature or state-action representation is known. We propose a novel Upper Confidence Bound (UCB) bonus-driven algorithm to carefully balance the interplay between exploration, exploitation, and representation learning in CVaR RL. We prove that our algorithm achieves a sample complexity of $\tilde{O}\left(\frac{H^7 A^2 d^4}{\tau^2 \epsilon^2}\right)$ to yield an $\epsilon$-optimal CVaR, where $H$ is the length of each episode, $A$ is the capacity of action space, and $d$ is the dimension of representations. Computational-wise, we design a novel discretized Least-Squares Value Iteration (LSVI) algorithm for the CVaR objective as the planning oracle and show that we can find the near-optimal policy in a polynomial running time with a Maximum Likelihood Estimation oracle. To our knowledge, this is the first provably efficient CVaR RL algorithm in low-rank MDPs.
</details>
<details>
<summary>摘要</summary>
我们研究风险敏感的奖励学习（RL），目标是在固定风险容忍度 $\tau$ 下最大化条件值风险（CVaR）。先前的理论研究中只关注了tabular Markov Decision Processes（MDPs）的情况。为了将CVaR RL扩展到大型状态空间，需要使用函数近似。我们研究CVaR RL在low-rank MDPs中，其中transition kernel允许低级别分解。不同于先前的线性模型，low-rank MDPs不假设特征或行动表示是已知。我们提出了一种新的Upper Confidence Bound（UCB）奖励驱动算法，用于在exploration、exploitation和表示学习之间进行精准均衡。我们证明了我们的算法可以在 $\tilde{O}\left(\frac{H^7 A^2 d^4}{\tau^2 \epsilon^2}\right)$ 批量样本中获得 $\epsilon$-优的CVaR，其中 $H$ 是每个 episoden 的长度，$A$ 是动作空间的容量，$d$ 是表示的维度。计算机上，我们设计了一种精简的Least-Squares Value Iteration（LSVI）算法，用于CVaR目标作为规划oracle，并证明了我们可以在多项式运行时间内找到近似优化策略，具有Maximum Likelihood Estimation oracle。到我们知道，这是首次可证明有效的CVaR RL算法在low-rank MDPs中。
</details></li>
</ul>
<hr>
<h2 id="Estimation-of-entropy-regularized-optimal-transport-maps-between-non-compactly-supported-measures"><a href="#Estimation-of-entropy-regularized-optimal-transport-maps-between-non-compactly-supported-measures" class="headerlink" title="Estimation of entropy-regularized optimal transport maps between non-compactly supported measures"></a>Estimation of entropy-regularized optimal transport maps between non-compactly supported measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11934">http://arxiv.org/abs/2311.11934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mattwerenski/entropic-map">https://github.com/mattwerenski/entropic-map</a></li>
<li>paper_authors: Matthew Werenski, James M. Murphy, Shuchin Aeron</li>
<li>for: 这个论文解决了估计基于Entropy-Regularized Optimal Transport（EOT）的易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度易见度�<details>
<summary>Abstract</summary>
This paper addresses the problem of estimating entropy-regularized optimal transport (EOT) maps with squared-Euclidean cost between source and target measures that are subGaussian. In the case that the target measure is compactly supported or strongly log-concave, we show that for a recently proposed in-sample estimator, the expected squared $L^2$-error decays at least as fast as $O(n^{-1/3})$ where $n$ is the sample size. For the general subGaussian case we show that the expected $L^1$-error decays at least as fast as $O(n^{-1/6})$, and in both cases we have polynomial dependence on the regularization parameter. While these results are suboptimal compared to known results in the case of compactness of both the source and target measures (squared $L^2$-error converging at a rate $O(n^{-1})$) and for when the source is subGaussian while the target is compactly supported (squared $L^2$-error converging at a rate $O(n^{-1/2})$), their importance lie in eliminating the compact support requirements. The proof technique makes use of a bias-variance decomposition where the variance is controlled using standard concentration of measure results and the bias is handled by T1-transport inequalities along with sample complexity results in estimation of EOT cost under subGaussian assumptions. Our experimental results point to a looseness in controlling the variance terms and we conclude by posing several open problems.
</details>
<details>
<summary>摘要</summary>
Our proof technique uses a bias-variance decomposition, where the variance is controlled using standard concentration of measure results, and the bias is handled by T1-transport inequalities and sample complexity results in estimation of EOT cost under subGaussian assumptions. Our experimental results suggest that there may be looseness in controlling the variance terms, and we conclude by posing several open problems.Translation notes:* "subGaussian" is translated as "subGaussian" (Simplified Chinese: 子高aussian)* "EOT" is translated as "EOT" (Simplified Chinese: EOT)* "source measure" is translated as "source measure" (Simplified Chinese: 源度量)* "target measure" is translated as "target measure" (Simplified Chinese: 目标度量)* "squared-Euclidean cost" is translated as "squared-Euclidean cost" (Simplified Chinese: 平方euclidian cost)* "in-sample estimator" is translated as "in-sample estimator" (Simplified Chinese: 内样估计器)* "compactly supported" is translated as "compactly supported" (Simplified Chinese: 准确地支持)* "strongly log-concave" is translated as "strongly log-concave" (Simplified Chinese: 强式Log-concave)* "bias-variance decomposition" is translated as "bias-variance decomposition" (Simplified Chinese: 偏差-方差分解)* "T1-transport inequality" is translated as "T1-transport inequality" (Simplified Chinese: T1-运输不等式)* "sample complexity results" is translated as "sample complexity results" (Simplified Chinese: 样本复杂性结果)
</details></li>
</ul>
<hr>
<h2 id="Deep-Calibration-of-Market-Simulations-using-Neural-Density-Estimators-and-Embedding-Networks"><a href="#Deep-Calibration-of-Market-Simulations-using-Neural-Density-Estimators-and-Embedding-Networks" class="headerlink" title="Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks"></a>Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11913">http://arxiv.org/abs/2311.11913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Namid R. Stillman, Rory Baggott, Justin Lyon, Jianfei Zhang, Dingqiu Zhu, Tao Chen, Perukrishnen Vytelingum</li>
<li>for: 该文章的目的是开发一种基于深度学习的市场模拟器，以便更好地预测股票市场的行为。</li>
<li>methods: 该文章使用了神经网络density estimator和嵌入网络来适应市场模拟器的准确匹配。</li>
<li>results: 研究人员通过使用这种新方法，成功地预测了市场的行为，并且不需要人工选择或权重ensemble of stylised facts。<details>
<summary>Abstract</summary>
The ability to construct a realistic simulator of financial exchanges, including reproducing the dynamics of the limit order book, can give insight into many counterfactual scenarios, such as a flash crash, a margin call, or changes in macroeconomic outlook. In recent years, agent-based models have been developed that reproduce many features of an exchange, as summarised by a set of stylised facts and statistics. However, the ability to calibrate simulators to a specific period of trading remains an open challenge. In this work, we develop a novel approach to the calibration of market simulators by leveraging recent advances in deep learning, specifically using neural density estimators and embedding networks. We demonstrate that our approach is able to correctly identify high probability parameter sets, both when applied to synthetic and historical data, and without reliance on manually selected or weighted ensembles of stylised facts.
</details>
<details>
<summary>摘要</summary>
可以构建一个真实的金融交易市场模拟器，包括复制限单书的动态，可以给出许多对于假设情况的理解，如快速崩盘、资金征引、或macro经济景气的变化。在过去几年，代理基模型已经开发出来，可以复制许多交易市场的特征，如一组简化的事实和统计。但是，对市场模拟器进行准确的调整仍然是一个开放的挑战。在这项工作中，我们提出了一种新的方法来调整市场模拟器，利用最新的深度学习技术，具体来说是使用神经概率分布计算器和嵌入网络。我们示出了我们的方法可以正确地标识高概率的参数集，无需人工选择或权重的ensemble的预测结果。
</details></li>
</ul>
<hr>
<h2 id="Certification-of-Distributional-Individual-Fairness"><a href="#Certification-of-Distributional-Individual-Fairness" class="headerlink" title="Certification of Distributional Individual Fairness"></a>Certification of Distributional Individual Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11911">http://arxiv.org/abs/2311.11911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Wicker, Vihari Piratia, Adrian Weller</li>
<li>for: 这个论文的目的是提供 formally guaranteeing algorithmic fairness, 以便负责任地部署机器学习算法。</li>
<li>methods: 该论文使用了一种新的几何approximation来保证个体公平（IF）约束，以及一种基于 quasi-convex 优化的新的证明方法来保证分布性个体公平（DIF）。</li>
<li>results: 该论文的结果表明，该方法可以覆盖大型神经网络，并且可以快速提供有效的公平保证。此外，该论文还研究了实际 Distribution Shift 问题，并证明了该方法的可扩展性和实用性。<details>
<summary>Abstract</summary>
Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify distributional individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.
</details>
<details>
<summary>摘要</summary>
提供形式保证算法公平性的重要性在社会责任投入机器学习算法中非常高。在这项工作中，我们研究形式保证，即证书， для个人公平性（IF）神经网络。我们首先介绍一种新的几何减少IF约束的几何近似，这将把计算提供本地个人公平性的成本减少到一个指数级别。我们指出先前的方法受到全局IF证书的限制，因此只能处理一些几十个隐藏神经元的模型，这有限了其实际影响。我们提议验证分布公平性，即验证神经网络在给定的实际分布和所有在γ-沃氏距离球中的分布之间都有保证的个人公平性预测。通过 quasi-conovex 优化技术，我们提供了新的和高效的分布公平性验证 bounds，并证明我们的方法可以证明和规范比先前作品中的模型更大几个数量级。此外，我们研究了实际分布的偏移和发现我们的约束是一个可扩展、实用和有效的源码公平性保证。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Surface-to-Air-Missile-Engagement-Zone-Prediction-Using-Simulation-and-Machine-Learning"><a href="#Real-Time-Surface-to-Air-Missile-Engagement-Zone-Prediction-Using-Simulation-and-Machine-Learning" class="headerlink" title="Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning"></a>Real-Time Surface-to-Air Missile Engagement Zone Prediction Using Simulation and Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11905">http://arxiv.org/abs/2311.11905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpadantas/sam-ez">https://github.com/jpadantas/sam-ez</a></li>
<li>paper_authors: Joao P. A. Dantas, Diego Geraldo, Felipe L. L. Medeiros, Marcos R. O. A. Maximo, Takashi Yoneyama</li>
<li>for: 这个研究旨在提高现代空中防御系统中的 superficie-to-air missile (SAM) 的效iveness，特别是� Engagement Zone (EZ) 的计算，这是� SAM 可以有效地处理和中断目标的空间区域。</li>
<li>methods: 本研究使用机器学习技术，提出了一种将机器学习与自定义的 simulatio tool 结合的方法，以训练supervised algorithmi。使用了大量预先计算的 SAM EZ  simulations 数据集，使我们的模型能够精确地预测新的input parameters 下的 SAM EZ。</li>
<li>results: 本研究获得了以下结果：(1) 使用机器学习技术可以优化 SAM EZ 的计算，提高空中防御 стратегіic 规划和提供实时反应; (2) 比较不同的机器学习算法，发现其能力和性能指标，并提出了未来研究的方向; (3) 这个方法可以提高 SAM 系统的效能，并为空中防御系统提供更好的战略规划和实时反应。<details>
<summary>Abstract</summary>
Surface-to-Air Missiles (SAMs) are crucial in modern air defense systems. A critical aspect of their effectiveness is the Engagement Zone (EZ), the spatial region within which a SAM can effectively engage and neutralize a target. Notably, the EZ is intrinsically related to the missile's maximum range; it defines the furthest distance at which a missile can intercept a target. The accurate computation of this EZ is essential but challenging due to the dynamic and complex factors involved, which often lead to high computational costs and extended processing times when using conventional simulation methods. In light of these challenges, our study investigates the potential of machine learning techniques, proposing an approach that integrates machine learning with a custom-designed simulation tool to train supervised algorithms. We leverage a comprehensive dataset of pre-computed SAM EZ simulations, enabling our model to accurately predict the SAM EZ for new input parameters. It accelerates SAM EZ simulations, enhances air defense strategic planning, and provides real-time insights, improving SAM system performance. The study also includes a comparative analysis of machine learning algorithms, illuminating their capabilities and performance metrics and suggesting areas for future research, highlighting the transformative potential of machine learning in SAM EZ simulations.
</details>
<details>
<summary>摘要</summary>
现代空中防御系统中，地面对空导弹（SAM）具有重要的作用。SAM的效iveness的一个关键因素是作战区（EZ），即导弹可以有效地攻击和neutralize Target的空间区域。需要注意的是，EZ与导弹的最大射程直接相关，它定义了导弹可以 intercept Target的最远距离。正确计算EZ是必要的，但是它具有复杂的因素和高计算成本，使得使用传统的 simulate方法可能会带来延迟和高计算成本。为了解决这些挑战，我们的研究团队提出了一种 integrate machine learning技术的方法，该方法通过与自定义的 simulate工具集成Machine learning算法来训练supervised模型。我们利用了大量预计算SAM EZ的数据集，使得我们的模型可以准确预测新的输入参数下的SAM EZ。这有助于加速SAM EZ的 simulate，提高空防 страте planning，并提供实时的 Insights，从而提高SAM系统的性能。我们的研究还包括了机器学习算法的比较分析，描述了这些算法的能力和性能指标，并建议了未来研究的方向，从而探讨机器学习在SAM EZ simulate中的Transformative潜力。
</details></li>
</ul>
<hr>
<h2 id="Measuring-and-Mitigating-Biases-in-Motor-Insurance-Pricing"><a href="#Measuring-and-Mitigating-Biases-in-Motor-Insurance-Pricing" class="headerlink" title="Measuring and Mitigating Biases in Motor Insurance Pricing"></a>Measuring and Mitigating Biases in Motor Insurance Pricing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11900">http://arxiv.org/abs/2311.11900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mulah Moriah, Franck Vermet, Arthur Charpentier</li>
<li>For: This paper aims to provide a comprehensive set of tools for insurers to formulate fair and optimal pricing strategies in the non-life insurance sector, while taking into account various ethical and societal factors.* Methods: The paper uses a range of statistical methodologies and available data to construct optimal pricing structures that align with the overarching corporate strategy and accommodate market competition.* Results: The study seeks to assess the effectiveness of these tools through practical application in the context of automobile insurance, with a focus on ensuring transparency, explainability, and ethical considerations in pricing practices.<details>
<summary>Abstract</summary>
The non-life insurance sector operates within a highly competitive and tightly regulated framework, confronting a pivotal juncture in the formulation of pricing strategies. Insurers are compelled to harness a range of statistical methodologies and available data to construct optimal pricing structures that align with the overarching corporate strategy while accommodating the dynamics of market competition. Given the fundamental societal role played by insurance, premium rates are subject to rigorous scrutiny by regulatory authorities. These rates must conform to principles of transparency, explainability, and ethical considerations. Consequently, the act of pricing transcends mere statistical calculations and carries the weight of strategic and societal factors. These multifaceted concerns may drive insurers to establish equitable premiums, taking into account various variables. For instance, regulations mandate the provision of equitable premiums, considering factors such as policyholder gender or mutualist group dynamics in accordance with respective corporate strategies. Age-based premium fairness is also mandated. In certain insurance domains, variables such as the presence of serious illnesses or disabilities are emerging as new dimensions for evaluating fairness. Regardless of the motivating factor prompting an insurer to adopt fairer pricing strategies for a specific variable, the insurer must possess the capability to define, measure, and ultimately mitigate any ethical biases inherent in its pricing practices while upholding standards of consistency and performance. This study seeks to provide a comprehensive set of tools for these endeavors and assess their effectiveness through practical application in the context of automobile insurance.
</details>
<details>
<summary>摘要</summary>
非生命保险领域在高度竞争和严格规范的框架下运作，面临着决定性的价格策略形成之 juncture。保险公司需要利用一系列统计方法和可用数据来构建优化的价格结构，以Alignment with overall corporate strategy and market competition dynamics。由于保险在社会中的基本作用，保险费用受到严格的监管和社会关注。因此，价格计算不仅仅是统计计算，还拥有策略和社会因素的重要性。这些多方面的问题可能会导致保险公司采用更加公平的保费策略，考虑多个变量。例如，法规要求提供公平的保费策略，考虑因素 such as 保险Policyholder gender or mutualist group dynamics according to respective corporate strategies。年龄基本的保费公平也是必须的。在某些保险领域，存在严重疾病或残疾的变量是新的评价公平的因素。无论某保险公司采用什么因素来采取更加公平的价格策略，该保险公司必须具备定义、测量和 ultimately mitigate any ethical biases inherent in its pricing practices while upholding standards of consistency and performance。本研究的目的是提供一套全面的工具，并在汽车保险上进行实践应用，以评估这些工具的有效性。
</details></li>
</ul>
<hr>
<h2 id="AMES-A-Differentiable-Embedding-Space-Selection-Framework-for-Latent-Graph-Inference"><a href="#AMES-A-Differentiable-Embedding-Space-Selection-Framework-for-Latent-Graph-Inference" class="headerlink" title="AMES: A Differentiable Embedding Space Selection Framework for Latent Graph Inference"></a>AMES: A Differentiable Embedding Space Selection Framework for Latent Graph Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11891">http://arxiv.org/abs/2311.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Lu, Haitz Sáez de Ocáriz Borde, Pietro Liò</li>
<li>for: 这篇论文的目的是提出一种可 diferenciable的方法来选择最佳的嵌入空间，以便在Point cloud数据上进行强化图 neural network（GNN）的推理。</li>
<li>methods: 该方法基于Attentional Multi-Embedding Selection（AMES）框架，通过反propagation来选择最佳的嵌入空间，并考虑下游任务。</li>
<li>results: 该方法在五个 benchmark dataset上达到了相当或更高的结果，并且消除了需要进行多个实验来确定最佳嵌入空间的需求。此外，该方法还提供了一种可读性技术，可以跟踪不同嵌入空间的梯度贡献，从而解释如何通过注意力机制来选择合适的嵌入空间。<details>
<summary>Abstract</summary>
In real-world scenarios, although data entities may possess inherent relationships, the specific graph illustrating their connections might not be directly accessible. Latent graph inference addresses this issue by enabling Graph Neural Networks (GNNs) to operate on point cloud data, dynamically learning the necessary graph structure. These graphs are often derived from a latent embedding space, which can be modeled using Euclidean, hyperbolic, spherical, or product spaces. However, currently, there is no principled differentiable method for determining the optimal embedding space. In this work, we introduce the Attentional Multi-Embedding Selection (AMES) framework, a differentiable method for selecting the best embedding space for latent graph inference through backpropagation, considering a downstream task. Our framework consistently achieves comparable or superior results compared to previous methods for latent graph inference across five benchmark datasets. Importantly, our approach eliminates the need for conducting multiple experiments to identify the optimal embedding space. Furthermore, we explore interpretability techniques that track the gradient contributions of different latent graphs, shedding light on how our attention-based, fully differentiable approach learns to choose the appropriate latent space. In line with previous works, our experiments emphasize the advantages of hyperbolic spaces in enhancing performance. More importantly, our interpretability framework provides a general approach for quantitatively comparing embedding spaces across different tasks based on their contributions, a dimension that has been overlooked in previous literature on latent graph inference.
</details>
<details>
<summary>摘要</summary>
在实际场景中，虽然数据实体可能具有内在关系，但具体的关系图可能并不直接可访。虚拟图注意力方法可以使得图神经网络（GNNs）在点云数据上运行，动态学习必要的图结构。这些图通常来自一个隐藏空间，可以使用欧几丁素、射影空间、球形空间或产品空间来模型。然而，目前没有原则性的可 diferenciable方法来确定最佳隐藏空间。在这个工作中，我们介绍了注意力多嵌入选择（AMES）框架，一种可 diferenciable方法，通过反射来选择最佳隐藏空间，考虑到下游任务。我们的框架在五个benchmark dataset上 consistentemente achievest comparable或superior resultscmpared to previous methods for latent graph inference。这importantly eliminates the need for conducting multiple experiments to identify the optimal embedding space。此外，我们还探索了解释技术，以跟踪不同隐藏空间对Gradient的贡献，揭示了我们的注意力基于、完全可 diferenciable的方法如何选择合适的隐藏空间。与前一些工作一样，我们的实验强调了使用射影空间的优势，并且我们的解释框架提供了一个通用的方法来比较不同任务中的隐藏空间，这一维度在前一些latent graph inference的文献中被忽略了。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Neural-Networks-for-Tiny-Machine-Learning-A-Comprehensive-Review"><a href="#Efficient-Neural-Networks-for-Tiny-Machine-Learning-A-Comprehensive-Review" class="headerlink" title="Efficient Neural Networks for Tiny Machine Learning: A Comprehensive Review"></a>Efficient Neural Networks for Tiny Machine Learning: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11883">http://arxiv.org/abs/2311.11883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh Tri Lê, Pierre Wolinski, Julyan Arbel</li>
<li>for: 这篇评论文章探讨了在资源有限的微控制器（MCU）上实现小型机器学习（TinyML）应用的最新进展，包括各种高效神经网络和深度学习模型的部署。</li>
<li>methods: 评论文章涵盖了各种高效神经网络技术，如模型压缩、量化和低级分解，以优化神经网络架构，以最小化MCU上的资源消耗。它还讨论了将深度学习模型部署到MCU上的挑战，以及如何使用模型剪辑、硬件加速和算法-架构协同设计等技术来解决这些挑战。</li>
<li>results: 评论文章提供了一个全面的分析，探讨了目前在TinyML领域的限制和未来研究方向，以帮助读者更好地理解这一领域的发展趋势。<details>
<summary>Abstract</summary>
The field of Tiny Machine Learning (TinyML) has gained significant attention due to its potential to enable intelligent applications on resource-constrained devices. This review provides an in-depth analysis of the advancements in efficient neural networks and the deployment of deep learning models on ultra-low power microcontrollers (MCUs) for TinyML applications. It begins by introducing neural networks and discussing their architectures and resource requirements. It then explores MEMS-based applications on ultra-low power MCUs, highlighting their potential for enabling TinyML on resource-constrained devices. The core of the review centres on efficient neural networks for TinyML. It covers techniques such as model compression, quantization, and low-rank factorization, which optimize neural network architectures for minimal resource utilization on MCUs. The paper then delves into the deployment of deep learning models on ultra-low power MCUs, addressing challenges such as limited computational capabilities and memory resources. Techniques like model pruning, hardware acceleration, and algorithm-architecture co-design are discussed as strategies to enable efficient deployment. Lastly, the review provides an overview of current limitations in the field, including the trade-off between model complexity and resource constraints. Overall, this review paper presents a comprehensive analysis of efficient neural networks and deployment strategies for TinyML on ultra-low-power MCUs. It identifies future research directions for unlocking the full potential of TinyML applications on resource-constrained devices.
</details>
<details>
<summary>摘要</summary>
隐藏Machine Learning（TinyML）领域在最近几年内吸引了广泛关注，因为它可以启用智能应用程序在有限的设备上运行。本文提供了TinyML应用中有效神经网络和深度学习模型的部署在低功耗微控制器（MCU）的全面分析。文章首先介绍神经网络，描述其结构和资源需求。然后，文章探讨了基于MEMS的应用程序在低功耗MCU上的潜力，并指出了它们在启用TinyML的资源限制设备上的潜力。文章的核心部分是有效神经网络的优化，包括模型压缩、量化和低级因子分解等技术，以最小化MCU上神经网络的资源使用。文章then delves into the deployment of deep learning models on ultra-low power MCUs, addressing challenges such as limited computational capabilities and memory resources. Techniques like model pruning, hardware acceleration, and algorithm-architecture co-design are discussed as strategies to enable efficient deployment. Finally, the review provides an overview of current limitations in the field, including the trade-off between model complexity and resource constraints. Overall, this review paper presents a comprehensive analysis of efficient neural networks and deployment strategies for TinyML on ultra-low-power MCUs, and identifies future research directions for unlocking the full potential of TinyML applications on resource-constrained devices.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Forward-Gradients-for-Data-Driven-CFD-Wall-Modeling"><a href="#Forward-Gradients-for-Data-Driven-CFD-Wall-Modeling" class="headerlink" title="Forward Gradients for Data-Driven CFD Wall Modeling"></a>Forward Gradients for Data-Driven CFD Wall Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11876">http://arxiv.org/abs/2311.11876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Hückelheim, Tadbhagya Kumar, Krishnan Raghavan, Pinaki Pal</li>
<li>for: 这 paper 的目的是提出一种基于机器学习的增强 CFD 流体动力学模型，以减少计算成本并保持预测精度。</li>
<li>methods: 这 paper 使用了机器学习和其他数据驱动方法来补充现有的墙模型，以解决 CFD  simulate 中的计算成本问题。</li>
<li>results: 这 paper 的结果表明，使用这种新的机器学习方法可以减少 CFD  simulate 的计算成本，同时保持预测精度。<details>
<summary>Abstract</summary>
Computational Fluid Dynamics (CFD) is used in the design and optimization of gas turbines and many other industrial/ scientific applications. However, the practical use is often limited by the high computational cost, and the accurate resolution of near-wall flow is a significant contributor to this cost. Machine learning (ML) and other data-driven methods can complement existing wall models. Nevertheless, training these models is bottlenecked by the large computational effort and memory footprint demanded by back-propagation. Recent work has presented alternatives for computing gradients of neural networks where a separate forward and backward sweep is not needed and storage of intermediate results between sweeps is not required because an unbiased estimator for the gradient is computed in a single forward sweep. In this paper, we discuss the application of this approach for training a subgrid wall model that could potentially be used as a surrogate in wall-bounded flow CFD simulations to reduce the computational overhead while preserving predictive accuracy.
</details>
<details>
<summary>摘要</summary>
计算流体动力学（CFD）在设计和优化液化机和许多其他工业/科学应用中广泛应用。然而，实际应用受到计算成本的限制，而近墙流动的准确解决是计算成本的重要贡献之一。机器学习（ML）和其他数据驱动方法可以补充现有墙模型。然而，训练这些模型受到计算努力和内存占用的瓶颈，因为回传扩散需要大量的计算努力和内存占用。 latest work has presented alternatives for computing gradients of neural networks where a separate forward and backward sweep is not needed and storage of intermediate results between sweeps is not required because an unbiased estimator for the gradient is computed in a single forward sweep. In this paper, we discuss the application of this approach for training a subgrid wall model that could potentially be used as a surrogate in wall-bounded flow CFD simulations to reduce the computational overhead while preserving predictive accuracy.
</details></li>
</ul>
<hr>
<h2 id="Training-robust-and-generalizable-quantum-models"><a href="#Training-robust-and-generalizable-quantum-models" class="headerlink" title="Training robust and generalizable quantum models"></a>Training robust and generalizable quantum models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11871">http://arxiv.org/abs/2311.11871</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daniel-fink-de/training-robust-and-generalizable-quantum-models">https://github.com/daniel-fink-de/training-robust-and-generalizable-quantum-models</a></li>
<li>paper_authors: Julian Berberich, Daniel Fink, Daniel Pranjić, Christian Tutschku, Christian Holm</li>
<li>for: 这 paper  investigate 量子机器学习模型的可靠性和泛化性，使用 Lipschitz 约束来保证模型的可靠性和泛化性。</li>
<li>methods: 作者使用 Lipschitz 约束来研究量子机器学习模型的可靠性和泛化性，并 derive 参数依赖的 Lipschitz 约束，以及数据编码参数对模型的影响。</li>
<li>results: 作者发现，使用 Lipschitz 约束来训练量子机器学习模型可以提高模型的可靠性和泛化性，并且通过 numerics 结果表明，这种训练策略可以提高模型的泛化性和可靠性。<details>
<summary>Abstract</summary>
Adversarial robustness and generalization are both crucial properties of reliable machine learning models. In this paper, we study these properties in the context of quantum machine learning based on Lipschitz bounds. We derive tailored, parameter-dependent Lipschitz bounds for quantum models with trainable encoding, showing that the norm of the data encoding has a crucial impact on the robustness against perturbations in the input data. Further, we derive a bound on the generalization error which explicitly depends on the parameters of the data encoding. Our theoretical findings give rise to a practical strategy for training robust and generalizable quantum models by regularizing the Lipschitz bound in the cost. Further, we show that, for fixed and non-trainable encodings as frequently employed in quantum machine learning, the Lipschitz bound cannot be influenced by tuning the parameters. Thus, trainable encodings are crucial for systematically adapting robustness and generalization during training. With numerical results, we demonstrate that, indeed, Lipschitz bound regularization leads to substantially more robust and generalizable quantum models.
</details>
<details>
<summary>摘要</summary>
机器学习模型的可靠性和泛化性都是非常重要的性能指标。在这篇论文中，我们在量子机器学习的上下文中研究了这两个性能指标，基于Lipschitz约束。我们计算了可调编码的情况下的Lipschitz约束，发现数据编码的 нор方面对输入数据的偏移而且影响机器学习模型的Robustness。此外，我们还计算出了参数化的泛化误差约束，其直接取决于数据编码的参数。我们的理论发现可以通过规范Lipschitz约束来训练Robust和泛化的量子机器学习模型。而固定和非可调编码，常用于量子机器学习中，Lipschitz约束无法通过调整参数来影响。因此，可调编码是系统地适应Robustness和泛化的关键。我们的实验结果表明，果然通过Lipschitz约束规范来训练量子机器学习模型，可以获得更加Robust和泛化的性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-complete-intersection-Calabi-Yau-manifolds"><a href="#Deep-learning-complete-intersection-Calabi-Yau-manifolds" class="headerlink" title="Deep learning complete intersection Calabi-Yau manifolds"></a>Deep learning complete intersection Calabi-Yau manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11847">http://arxiv.org/abs/2311.11847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harold Erbin, Riccardo Finotello</li>
<li>for: 理解如何使用深度学习技术处理代数 topological 数据</li>
<li>methods: 方法方面和数据分析，然后描述神经网络架构</li>
<li>results: 状态之前精度在预测希德数字上，以及从低到高希德数和反之的推断扩展<details>
<summary>Abstract</summary>
We review advancements in deep learning techniques for complete intersection Calabi-Yau (CICY) 3- and 4-folds, with the aim of understanding better how to handle algebraic topological data with machine learning. We first discuss methodological aspects and data analysis, before describing neural networks architectures. Then, we describe the state-of-the art accuracy in predicting Hodge numbers. We include new results on extrapolating predictions from low to high Hodge numbers, and conversely.
</details>
<details>
<summary>摘要</summary>
我团队正在审查深度学习技术在完全交叉Calabi-Yau（CICY）3-4维空间中的进步，以更好地理解如何使用机器学习处理代数 topologic 数据。我们首先讨论方法学和数据分析，然后描述神经网络架构。接着，我们介绍状态艺术精度预测黎Numbers。我们还新增了在低黎数预测到高黎数预测和相反的推断扩展。
</details></li>
</ul>
<hr>
<h2 id="High-Probability-Guarantees-for-Random-Reshuffling"><a href="#High-Probability-Guarantees-for-Random-Reshuffling" class="headerlink" title="High Probability Guarantees for Random Reshuffling"></a>High Probability Guarantees for Random Reshuffling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11841">http://arxiv.org/abs/2311.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengxu Yu, Xiao Li</li>
<li>for: 这个论文是为了研究Stochastic Gradient Method with Random Reshuffling（SGM-RR）在粗糙非 convex 优化问题上的应用。</li>
<li>methods: 本文使用了SGM-RR方法，并 investigate了其抽象采样过程的集中性性质，并提出了一个新的高可probability样本复杂度保证，可以在无期望下驱动梯度（下标为 $\varepsilon$）下降至 $\varepsilon$ 的高可 probabilities。</li>
<li>results: 本文提出了一种基于SGM-RR方法的简单计算可停止 criterion（denoted as $\mathsf{RR}$-$\mathsf{sc}$），可以在有限多个迭代后返回一个梯度下降至 $\varepsilon$ 的高可 probabilities。此外，本文还提出了一种带有随机扰动过程的 perturbed random reshuffling方法（denoted as $\mathsf{p}$-$\mathsf{RR}$），可以快速离开精度点并返回一个第二阶站点。<details>
<summary>Abstract</summary>
We consider the stochastic gradient method with random reshuffling ($\mathsf{RR}$) for tackling smooth nonconvex optimization problems. $\mathsf{RR}$ finds broad applications in practice, notably in training neural networks. In this work, we first investigate the concentration property of $\mathsf{RR}$'s sampling procedure and establish a new high probability sample complexity guarantee for driving the gradient (without expectation) below $\varepsilon$, which effectively characterizes the efficiency of a single $\mathsf{RR}$ execution. Our derived complexity matches the best existing in-expectation one up to a logarithmic term while imposing no additional assumptions nor changing $\mathsf{RR}$'s updating rule. Furthermore, by leveraging our derived high probability descent property and bound on the stochastic error, we propose a simple and computable stopping criterion for $\mathsf{RR}$ (denoted as $\mathsf{RR}$-$\mathsf{sc}$). This criterion is guaranteed to be triggered after a finite number of iterations, and then $\mathsf{RR}$-$\mathsf{sc}$ returns an iterate with its gradient below $\varepsilon$ with high probability. Moreover, building on the proposed stopping criterion, we design a perturbed random reshuffling method ($\mathsf{p}$-$\mathsf{RR}$) that involves an additional randomized perturbation procedure near stationary points. We derive that $\mathsf{p}$-$\mathsf{RR}$ provably escapes strict saddle points and efficiently returns a second-order stationary point with high probability, without making any sub-Gaussian tail-type assumptions on the stochastic gradient errors. Finally, we conduct numerical experiments on neural network training to support our theoretical findings.
</details>
<details>
<summary>摘要</summary>
我们考虑使用测量gradient方法（$\mathsf{RR}$）来解决缓和非凸优化问题。$\mathsf{RR}$在实践中发掘了广泛的应用，特别是在训练神经网络上。在这个工作中，我们首先调查$\mathsf{RR}$的抽样程序中的集中性性质，然后建立一个新的高概率样本复杂度保证，可以在不期望的情况下使得梯度（ без预期）下降至$\varepsilon$，这有效地描述了一个$\mathsf{RR}$执行的效率。我们的定义的复杂度与现有的均衡概率定理相匹配，但不需要任何额外的假设，也不需要改变$\mathsf{RR}$的更新规则。此外，我们运用我们所得到的高概率下降性和缩推错误的上限，提出了一个简单计算可行的停止条件（denoted as $\mathsf{RR}$-$\mathsf{sc}$）。这个条件会在一定的迭代次数之后被触发，并且返回一个梯度下降至$\varepsilon$的条件下的迭代器，具有高概率。此外，我们基于所提出的停止条件，设计了一个受惹随的随机排序方法（$\mathsf{p}$-$\mathsf{RR}$），具有额外的随机干扰程序，以避免困难的积点。我们证明了$\mathsf{p}$-$\mathsf{RR}$可以有效地逃脱紧系积点，并快速返回一个第二类稳定点，不需要任何假设梯度错误的子加速度类型。最后，我们在神经网络训练中进行了实验支持我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Zero-redundancy-distributed-learning-with-differential-privacy"><a href="#Zero-redundancy-distributed-learning-with-differential-privacy" class="headerlink" title="Zero redundancy distributed learning with differential privacy"></a>Zero redundancy distributed learning with differential privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11822">http://arxiv.org/abs/2311.11822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqi Bu, Justin Chiu, Ruixuan Liu, Sheng Zha, George Karypis</li>
<li>for: 这篇论文的目的是为了开发一个能够在隐私保护下训练大型深度学习模型的系统解决方案。</li>
<li>methods: 这篇论文使用了 Zero Redundancy Optimizer (ZeRO) 来实现隐私保护下的分布式学习，并且开发了一个名为 DP-ZeRO 的新系统解决方案，以扩展可训练的隐私保护模型大小，并且保持了与标准 ZeRO 相同的计算和通信效率。</li>
<li>results: 这篇论文的结果显示了 DP-ZeRO 可以训练世界上最大的隐私保护模型，并且与标准 ZeRO 相比，DP-ZeRO 在计算和通信效率方面获得了相似的表现。<details>
<summary>Abstract</summary>
Deep learning using large models have achieved great success in a wide range of domains. However, training these models on billions of parameters is very challenging in terms of the training speed, memory cost, and communication efficiency, especially under the privacy-preserving regime with differential privacy (DP). On the one hand, DP optimization has comparable efficiency to the standard non-private optimization on a single GPU, but on multiple GPUs, existing DP distributed learning (such as pipeline parallel) has suffered from significantly worse efficiency. On the other hand, the Zero Redundancy Optimizer (ZeRO) is a state-of-the-art solution to the standard distributed learning, exhibiting excellent training efficiency on large models, but to work compatibly with DP is technically complicated. In this work, we develop a new systematic solution, DP-ZeRO, (I) to scale up the trainable DP model size, e.g. to GPT-100B, (II) to obtain the same computation and communication efficiency as the standard ZeRO, and (III) to enable mixed-precision DP training. Our DP-ZeRO, like the standard ZeRO, has the potential to train models with arbitrary size and is evaluated on the world's largest DP models in terms of the number of trainable parameters.
</details>
<details>
<summary>摘要</summary>
深度学习使用大型模型已经在各种领域取得了很大成功。然而，在训练这些模型时， billion个参数的训练速度、内存成本和通信效率都是很大的挑战，尤其是在保持隐私的情况下。一方面，DP优化的效率与标准非私钥的训练效率相当，但在多个GPU上分布式学习（如管道并行）中，现有的DP分布式学习表现出了明显更差的效率。另一方面，零重复优化器（ZeRO）是当前最佳的分布式学习解决方案，在大型模型上显示出了出色的训练效率，但与DP兼容需要技术上的努力。在这项工作中，我们开发了一个新的系统性解决方案——DP-ZeRO，以下是我们的目标：1. 扩展可训练DP模型的大小，例如GPT-100B。2. 与标准ZeRO的计算和通信效率相同。3. 支持混合精度DP训练。我们的DP-ZeRO，如标准ZeRO，可以训练任意大小的模型，并在世界上最大的DP模型上进行评估。
</details></li>
</ul>
<hr>
<h2 id="LogLead-–-Fast-and-Integrated-Log-Loader-Enhancer-and-Anomaly-Detector"><a href="#LogLead-–-Fast-and-Integrated-Log-Loader-Enhancer-and-Anomaly-Detector" class="headerlink" title="LogLead – Fast and Integrated Log Loader, Enhancer, and Anomaly Detector"></a>LogLead – Fast and Integrated Log Loader, Enhancer, and Anomaly Detector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11809">http://arxiv.org/abs/2311.11809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evotestops/loglead">https://github.com/evotestops/loglead</a></li>
<li>paper_authors: Mika Mäntylä, Yuqing Wang, Jesse Nyyssölä</li>
<li>for: 这篇论文是关于高效的日志分析工具LogLead的介绍。</li>
<li>methods: 该工具结合了日志处理的三个重要步骤：加载、增强和异常检测。它利用了Polars高速数据Frame库。</li>
<li>results: 论文表明，使用LogLead可以将日志加载到数据框架中比过去的解决方案多于10倍快，并且可以在Drain解析速度上提高约2倍。同时，论文还进行了一些简要的比较，表明使用LogLead的日志表示法超过了bag-of-words的限制。<details>
<summary>Abstract</summary>
This paper introduces LogLead, a tool designed for efficient log analysis. LogLead combines three essential steps in log processing: loading, enhancing, and anomaly detection. The tool leverages Polars, a high-speed DataFrame library. We currently have 7 Loaders out of which 4 is for public data sets (HDFS, Hadoop, BGL, and Thunderbird). We have multiple enhancers with three parsers (Drain, Spell, LenMa), Bert embedding creation and other log representation techniques like bag-of-words. LogLead integrates to 5 supervised and 4 unsupervised machine learning algorithms for anomaly detection from SKLearn. By integrating diverse datasets, log representation methods and anomaly detectors, LogLead facilitates comprehensive benchmarking in log analysis research. We demonstrate that log loading from raw file to dataframe is over 10x faster with LogLead is compared to past solutions. We demonstrate roughly 2x improvement in Drain parsing speed by off-loading log message normalization to LogLead. We demonstrate a brief benchmarking on HDFS suggesting that log representations beyond bag-of-words provide limited benefits. Screencast demonstrating the tool: https://youtu.be/8stdbtTfJVo
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Operator-Learning-for-Continuous-Spatial-Temporal-Model-with-A-Hybrid-Optimization-Scheme"><a href="#Operator-Learning-for-Continuous-Spatial-Temporal-Model-with-A-Hybrid-Optimization-Scheme" class="headerlink" title="Operator Learning for Continuous Spatial-Temporal Model with A Hybrid Optimization Scheme"></a>Operator Learning for Continuous Spatial-Temporal Model with A Hybrid Optimization Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11798">http://arxiv.org/abs/2311.11798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanqi Chen, Jin-Long Wu</li>
<li>for: 用于数字化的空间-时间模型化复杂动力系统的应用</li>
<li>methods: 使用最近的运算学学习进程，提出一种连续的数据驱动模型化框架，具有空间和时间分辨率的不变性</li>
<li>results: 对三个数学示例进行研究，包括维斯喷噷方程、奈尔-斯托克方程和库拉摩-希瓦希涂方程，结果证明提案的模型化框架具有空间和时间分辨率的不变性，同时也能够稳定地进行长期价值预测，并且使用短期时间系列数据和长期统计数据的组合方式可以更好地预测长期统计。<details>
<summary>Abstract</summary>
Partial differential equations are often used in the spatial-temporal modeling of complex dynamical systems in many engineering applications. In this work, we build on the recent progress of operator learning and present a data-driven modeling framework that is continuous in both space and time. A key feature of the proposed model is the resolution-invariance with respect to both spatial and temporal discretizations. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the spatial-temporal continuous learning framework with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results confirm the resolution-invariance of the proposed modeling framework and also demonstrate stable long-term simulations with only short-term time series data. In addition, we show that the proposed model can better predict long-term statistics via the hybrid optimization scheme with a combined use of short-term and long-term data.
</details>
<details>
<summary>摘要</summary>
“partial differential equations often used in spatial-temporal modeling complex dynamical systems many engineering applications. In this work, we build on recent progress operator learning present data-driven modeling framework continuous both space and time. key feature proposed model resolution-invariance respect spatial and temporal discretizations. further propose hybrid optimization scheme leverages gradient-based derivative-free optimization methods efficiently trains both short-term time series long-term statistics. investigate performance spatial-temporal continuous learning framework three numerical examples, including viscous Burgers' equation, Navier-Stokes equations, Kuramoto-Sivashinsky equation. results confirm resolution-invariance proposed modeling framework also demonstrate stable long-term simulations only short-term time series data. addition, show proposed model better predict long-term statistics hybrid optimization scheme combined use short-term long-term data.”
</details></li>
</ul>
<hr>
<h2 id="Approximate-Linear-Programming-and-Decentralized-Policy-Improvement-in-Cooperative-Multi-agent-Markov-Decision-Processes"><a href="#Approximate-Linear-Programming-and-Decentralized-Policy-Improvement-in-Cooperative-Multi-agent-Markov-Decision-Processes" class="headerlink" title="Approximate Linear Programming and Decentralized Policy Improvement in Cooperative Multi-agent Markov Decision Processes"></a>Approximate Linear Programming and Decentralized Policy Improvement in Cooperative Multi-agent Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11789">http://arxiv.org/abs/2311.11789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lakshmi Mandal, Chandrashekar Lakshminarayanan, Shalabh Bhatnagar</li>
<li>for: 这个论文关注的是一种”合作”多智能体Markov决策过程（MDP），其中有多于1个智能体，所有智能体都知道系统模型。在每个决策环节，所有的m智能体合作选择动作，以实现最大化共同长期目标。由于动作的数量呈指数增长，政策提高成本高。</li>
<li>methods: 我们提出了一种分布式政策提高算法，其中每个智能体假设其他智能体的决策是固定的，并在自己的决策中进行改进。我们使用优化方法来计算优化后的政策。我们的算法可以处理大量的状态和多个智能体。</li>
<li>results: 我们提供了论文中的理论保证，以及一些数值示例的表现。我们的算法可以处理无限期和限期的折扣MDP，并且可以处理多个智能体。<details>
<summary>Abstract</summary>
In this work, we consider a `cooperative' multi-agent Markov decision process (MDP) involving m greater than 1 agents, where all agents are aware of the system model. At each decision epoch, all the m agents cooperatively select actions in order to maximize a common long-term objective. Since the number of actions grows exponentially in the number of agents, policy improvement is computationally expensive. Recent works have proposed using decentralized policy improvement in which each agent assumes that the decisions of the other agents are fixed and it improves its decisions unilaterally. Yet, in these works, exact values are computed. In our work, for cooperative multi-agent finite and infinite horizon discounted MDPs, we propose suitable approximate policy iteration algorithms, wherein we use approximate linear programming to compute the approximate value function and use decentralized policy improvement. Thus our algorithms can handle both large number of states as well as multiple agents. We provide theoretical guarantees for our algorithms and also demonstrate the performance of our algorithms on some numerical examples.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们考虑了一种“合作”多代理Markov决策过程（MDP），其中有m个代理，所有代理都知道系统模型。在每个决策瞬间，所有m代理协力选择动作，以最大化共同长期目标。由于行动数量呈指数增长，策略提高是计算昂贵的。现有的工作提议了分布式策略提高，其中每个代理假设别的代理决策是固定的，并优化自己的决策。然而，这些工作都是使用精确值进行计算。在我们的工作中，我们提出了合作多代理负额度MDP的合理approximate策略迭代算法，其中我们使用approximate线性编程计算approximate值函数，并使用分布式策略提高。因此，我们的算法可以处理大量的状态和多个代理。我们提供了理论保证，并在一些数学示例中证明了我们的算法的性能。
</details></li>
</ul>
<hr>
<h2 id="MUVO-A-Multimodal-Generative-World-Model-for-Autonomous-Driving-with-Geometric-Representations"><a href="#MUVO-A-Multimodal-Generative-World-Model-for-Autonomous-Driving-with-Geometric-Representations" class="headerlink" title="MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations"></a>MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11762">http://arxiv.org/abs/2311.11762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Bogdoll, Yitian Yang, J. Marius Zöllner</li>
<li>for: 提高自动驾驶系统的理解能力，增强下游任务的性能</li>
<li>methods: 使用原始相机和激光数据学习感知无关的多Modal世界模型，具有直接用于下游任务的感知agnostic geometric Representation</li>
<li>results: 实现多Modal未来预测，并证明我们的几何表示提高了相机图像和激光点云预测质量<details>
<summary>Abstract</summary>
Learning unsupervised world models for autonomous driving has the potential to improve the reasoning capabilities of today's systems dramatically. However, most work neglects the physical attributes of the world and focuses on sensor data alone. We propose MUVO, a MUltimodal World Model with Geometric VOxel Representations to address this challenge. We utilize raw camera and lidar data to learn a sensor-agnostic geometric representation of the world, which can directly be used by downstream tasks, such as planning. We demonstrate multimodal future predictions and show that our geometric representation improves the prediction quality of both camera images and lidar point clouds.
</details>
<details>
<summary>摘要</summary>
<LC>zh</LC></SYS>学习无监控世界模型可以帮助自动驾驶系统的理解能力提高，但大多数工作忽视物理世界的特性，只ocus on感知数据。我们提出MUVO，一种多Modal世界模型，使用Raw camera和lidar感知数据来学习不受感知器件限制的几何表示。我们证明了多Modal未来预测和显示了我们的几何表示可以提高相机图像和lidar点云预测质量。
</details></li>
</ul>
<hr>
<h2 id="Revealing-behavioral-impact-on-mobility-prediction-networks-through-causal-interventions"><a href="#Revealing-behavioral-impact-on-mobility-prediction-networks-through-causal-interventions" class="headerlink" title="Revealing behavioral impact on mobility prediction networks through causal interventions"></a>Revealing behavioral impact on mobility prediction networks through causal interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11749">http://arxiv.org/abs/2311.11749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Hong, Yanan Xin, Simon Dirmeier, Fernando Perez-Cruz, Martin Raubal<br>for: 这种研究旨在评估 neuronal network 在下一个位置预测任务中受到移动行为因素的影响。methods: 这种研究使用 causal intervention 框架来评估不同移动行为因素对 neuronal network 的影响。研究使用个人移动模型生成 синтетиче的位置访问序列，并通过 intervening 数据生成过程来控制行为动态。然后，研究使用 mobility 指标评估 intervened 位置序列，并输入到已经训练过的网络中进行分析性能变化。results: 研究发现可以生成具有不同移动行为特征的位置序列，从而模拟不同的空间和时间变化。这些变化导致下一个位置预测网络的性能变化，披露了关键的移动行为因素，包括位置转移序列的顺序性、探索新位置的倾向和个体和人口层次的位置选择偏好。这些发现对实际应用中的移动预测网络有重要意义，而 causal inference 框架也预计会促进 neural network 在移动应用中的解释性和可靠性。<details>
<summary>Abstract</summary>
Deep neural networks are increasingly utilized in mobility prediction tasks, yet their intricate internal workings pose challenges for interpretability, especially in comprehending how various aspects of mobility behavior affect predictions. In this study, we introduce a causal intervention framework to assess the impact of mobility-related factors on neural networks designed for next location prediction -- a task focusing on predicting the immediate next location of an individual. To achieve this, we employ individual mobility models to generate synthetic location visit sequences and control behavior dynamics by intervening in their data generation process. We evaluate the interventional location sequences using mobility metrics and input them into well-trained networks to analyze performance variations. The results demonstrate the effectiveness in producing location sequences with distinct mobility behaviors, thus facilitating the simulation of diverse spatial and temporal changes. These changes result in performance fluctuations in next location prediction networks, revealing impacts of critical mobility behavior factors, including sequential patterns in location transitions, proclivity for exploring new locations, and preferences in location choices at population and individual levels. The gained insights hold significant value for the real-world application of mobility prediction networks, and the framework is expected to promote the use of causal inference for enhancing the interpretability and robustness of neural networks in mobility applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-Uncertainty-Estimates-To-Improve-Classifier-Performance"><a href="#Leveraging-Uncertainty-Estimates-To-Improve-Classifier-Performance" class="headerlink" title="Leveraging Uncertainty Estimates To Improve Classifier Performance"></a>Leveraging Uncertainty Estimates To Improve Classifier Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11723">http://arxiv.org/abs/2311.11723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gundeep Arora, Srujana Merugu, Anoop Saladi, Rajeev Rastogi</li>
<li>for: 这 paper 的目的是分析模型分类结果的准确性，并提出了一种基于模型分数和不确定性的决策边界选择方法。</li>
<li>methods: 这 paper 使用了 teoretic 分析和实验证明，证明模型分数估计偏差与分类结果之间的关系，并提出了一种基于动态计划和iso随 regression的算法来解决这个问题。</li>
<li>results: 对三个实际数据集进行评估，提出的方法可以提高 recall 的水平，在高精度 bound 下达到 25%-40% 的提升，这些结果表明了利用不确定性可以提高模型的准确性。<details>
<summary>Abstract</summary>
Binary classification involves predicting the label of an instance based on whether the model score for the positive class exceeds a threshold chosen based on the application requirements (e.g., maximizing recall for a precision bound). However, model scores are often not aligned with the true positivity rate. This is especially true when the training involves a differential sampling across classes or there is distributional drift between train and test settings. In this paper, we provide theoretical analysis and empirical evidence of the dependence of model score estimation bias on both uncertainty and score itself. Further, we formulate the decision boundary selection in terms of both model score and uncertainty, prove that it is NP-hard, and present algorithms based on dynamic programming and isotonic regression. Evaluation of the proposed algorithms on three real-world datasets yield 25%-40% gain in recall at high precision bounds over the traditional approach of using model score alone, highlighting the benefits of leveraging uncertainty.
</details>
<details>
<summary>摘要</summary>
二分类预测基于实例标签的预测结果，通常基于模型分数是否超过选择的阈值（例如，以最大准确率为约束）。然而，模型分数与实际正确率之间并不总是一一对应。特别是在样本采样过程中存在类别差异或测试环境中存在分布误差时，模型分数的估计偏差会变得更加明显。在这篇论文中，我们提供了对模型分数估计偏差的理论分析和实际证据。我们还表述了基于模型分数和不确定性的决策边界选择，证明其是NP困难的，并提出了基于动态Programming和iso顺回归的算法。我们对三个实际 dataset进行评估，发现使用我们的方法可以提高recall的值在高精度约束下，升高了使用模型分数alone的效果。这些结果表明了使用不确定性可以提高预测的性能。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Power-of-Self-Attention-for-Shipping-Cost-Prediction-The-Rate-Card-Transformer"><a href="#Unveiling-the-Power-of-Self-Attention-for-Shipping-Cost-Prediction-The-Rate-Card-Transformer" class="headerlink" title="Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate Card Transformer"></a>Unveiling the Power of Self-Attention for Shipping Cost Prediction: The Rate Card Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11694">http://arxiv.org/abs/2311.11694</a></li>
<li>repo_url: None</li>
<li>paper_authors: P Aditya Sreekar, Sahil Verma, Varun Madhavan, Abhishek Persad</li>
<li>for: 这个研究是为了提高亚马逊的寄送成本估算精度，以便下游系统可以更好地做出金融决策，例如价格策略和删除损失的产品。</li>
<li>methods: 这个研究使用了一个名为“Rate Card Transformer”的新架构，这个架构使用自我注意来将所有寄送信息，例如包装 attribute、交通公司信息和路径规划，转换为数据表格。</li>
<li>results: 研究结果显示，使用 Rate Card Transformer 估算寄送成本时的误差比使用树型模型（GBDT）低得多，且比州先进的 transformer-based 数据表格模型（FTTransformer）高出6.08%。此外，研究还显示 Rate Card Transformer 可以将统计数据映射到更好的数据表格模型中，以提高其性能。<details>
<summary>Abstract</summary>
Amazon ships billions of packages to its customers annually within the United States. Shipping cost of these packages are used on the day of shipping (day 0) to estimate profitability of sales. Downstream systems utilize these days 0 profitability estimates to make financial decisions, such as pricing strategies and delisting loss-making products. However, obtaining accurate shipping cost estimates on day 0 is complex for reasons like delay in carrier invoicing or fixed cost components getting recorded at monthly cadence. Inaccurate shipping cost estimates can lead to bad decision, such as pricing items too low or high, or promoting the wrong product to the customers. Current solutions for estimating shipping costs on day 0 rely on tree-based models that require extensive manual engineering efforts. In this study, we propose a novel architecture called the Rate Card Transformer (RCT) that uses self-attention to encode all package shipping information such as package attributes, carrier information and route plan. Unlike other transformer-based tabular models, RCT has the ability to encode a variable list of one-to-many relations of a shipment, allowing it to capture more information about a shipment. For example, RCT can encode properties of all products in a package. Our results demonstrate that cost predictions made by the RCT have 28.82% less error compared to tree-based GBDT model. Moreover, the RCT outperforms the state-of-the-art transformer-based tabular model, FTTransformer, by 6.08%. We also illustrate that the RCT learns a generalized manifold of the rate card that can improve the performance of tree-based models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unraveling-the-Control-Engineer’s-Craft-with-Neural-Networks"><a href="#Unraveling-the-Control-Engineer’s-Craft-with-Neural-Networks" class="headerlink" title="Unraveling the Control Engineer’s Craft with Neural Networks"></a>Unraveling the Control Engineer’s Craft with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11644">http://arxiv.org/abs/2311.11644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Braghadeesh Lakshminarayanan, Federico Dettù, Cristian R. Rojas, Simone Formentin</li>
<li>for: 这篇论文是关于如何通过数字孪生模型来调整控制器的最佳化方法。</li>
<li>methods: 该方法使用了数字孪生模型生成输入输出数据，然后使用了人工生成的数据来学习控制器调整规则，使用了 state-of-the-art 的神经网络架构。</li>
<li>results: 该方法可以通过数据直接学习控制器调整规则，从而实际替代控制工程师，并且可以在数字孪生模型中拟合不同的参数变化。<details>
<summary>Abstract</summary>
Many industrial processes require suitable controllers to meet their performance requirements. More often, a sophisticated digital twin is available, which is a highly complex model that is a virtual representation of a given physical process, whose parameters may not be properly tuned to capture the variations in the physical process. In this paper, we present a sim2real, direct data-driven controller tuning approach, where the digital twin is used to generate input-output data and suitable controllers for several perturbations in its parameters. State-of-the art neural-network architectures are then used to learn the controller tuning rule that maps input-output data onto the controller parameters, based on artificially generated data from perturbed versions of the digital twin. In this way, as far as we are aware, we tackle for the first time the problem of re-calibrating the controller by meta-learning the tuning rule directly from data, thus practically replacing the control engineer with a machine learning model. The benefits of this methodology are illustrated via numerical simulations for several choices of neural-network architectures.
</details>
<details>
<summary>摘要</summary>
Many industrial processes require suitable controllers to meet their performance requirements. More often, a sophisticated digital twin is available, which is a highly complex model that is a virtual representation of a given physical process, whose parameters may not be properly tuned to capture the variations in the physical process. In this paper, we present a sim2real, direct data-driven controller tuning approach, where the digital twin is used to generate input-output data and suitable controllers for several perturbations in its parameters. State-of-the-art neural-network architectures are then used to learn the controller tuning rule that maps input-output data onto the controller parameters, based on artificially generated data from perturbed versions of the digital twin. In this way, as far as we are aware, we tackle for the first time the problem of re-calibrating the controller by meta-learning the tuning rule directly from data, thus practically replacing the control engineer with a machine learning model. The benefits of this methodology are illustrated via numerical simulations for several choices of neural-network architectures.Here's the translation in Traditional Chinese:Many industrial processes require suitable controllers to meet their performance requirements. More often, a sophisticated digital twin is available, which is a highly complex model that is a virtual representation of a given physical process, whose parameters may not be properly tuned to capture the variations in the physical process. In this paper, we present a sim2real, direct data-driven controller tuning approach, where the digital twin is used to generate input-output data and suitable controllers for several perturbations in its parameters. State-of-the-art neural-network architectures are then used to learn the controller tuning rule that maps input-output data onto the controller parameters, based on artificially generated data from perturbed versions of the digital twin. In this way, as far as we are aware, we tackle for the first time the problem of re-calibrating the controller by meta-learning the tuning rule directly from data, thus practically replacing the control engineer with a machine learning model. The benefits of this methodology are illustrated via numerical simulations for several choices of neural-network architectures.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-LLM-Priors-into-Tabular-Learners"><a href="#Incorporating-LLM-Priors-into-Tabular-Learners" class="headerlink" title="Incorporating LLM Priors into Tabular Learners"></a>Incorporating LLM Priors into Tabular Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11628">http://arxiv.org/abs/2311.11628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Zhu, Siniša Stanivuk, Andrija Petrovic, Mladen Nikolic, Pietro Lio</li>
<li>for: 这篇论文是为了探讨如何将大型语言模型（LLMs）与传统表格资料分类技术结合，以解决 LLMS 的挑战，例如资料序列化敏感性和偏见。</li>
<li>methods: 这篇论文提出了两种使用 LLMS 进行排名数据类别和生成关系函数的策略，以提高几何shot scenario 的性能。它还引入了一个名为 MonotonicLR 的非线性对数函数，用于将排名变数转换为数据类别，并保持 LLMS 决定的顺序。</li>
<li>results: 这篇论文的结果显示，与基eline模型进行比较，提出的方法在几何shot scenario 中表现出色，尤其是在资料少量的情况下。此外，这篇论文还证明了其方法的可读性。<details>
<summary>Abstract</summary>
We present a method to integrate Large Language Models (LLMs) and traditional tabular data classification techniques, addressing LLMs challenges like data serialization sensitivity and biases. We introduce two strategies utilizing LLMs for ranking categorical variables and generating priors on correlations between continuous variables and targets, enhancing performance in few-shot scenarios. We focus on Logistic Regression, introducing MonotonicLR that employs a non-linear monotonic function for mapping ordinals to cardinals while preserving LLM-determined orders. Validation against baseline models reveals the superior performance of our approach, especially in low-data scenarios, while remaining interpretable.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，将大语言模型（LLMs）与传统的表格数据分类技术集成，解决 LLMS 的挑战，如数据序列化敏感性和偏见。我们提出了两种使用 LLMS 对 categorical 变量进行排名和生成连接 continuous 变量和目标的策略，提高了几个示例下的性能。我们主要关注了Logistic Regression，并提出了MonotonicLR，使用非线性卷积函数将 ordinal 映射到 cardinal，保持 LLM 决定的顺序。对于基线模型的验证表明，我们的方法在低数据场景下表现出优于基线模型，而且保持可解释性。
</details></li>
</ul>
<hr>
<h2 id="Testing-multivariate-normality-by-testing-independence"><a href="#Testing-multivariate-normality-by-testing-independence" class="headerlink" title="Testing multivariate normality by testing independence"></a>Testing multivariate normality by testing independence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11575">http://arxiv.org/abs/2311.11575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Povilas Daniušis</li>
<li>for: 该论文是为了提出一种简单的多变量常数测试方法，该方法基于Kac-Bernstein的特征化。</li>
<li>methods: 该方法使用现有的统计独立测试来进行 sums和差分数据样本的测试。</li>
<li>results: 实验表明，在高维数据中，该方法可能比替代方法更高效。<details>
<summary>Abstract</summary>
We propose a simple multivariate normality test based on Kac-Bernstein's characterization, which can be conducted by utilising existing statistical independence tests for sums and differences of data samples. We also perform its empirical investigation, which reveals that for high-dimensional data, the proposed approach may be more efficient than the alternative ones. The accompanying code repository is provided at \url{https://shorturl.at/rtuy5}.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的多变量正态测试，基于加铬-bernstein的特征化，可以通过利用现有的统计独立测试来进行。我们还进行了其实验研究，发现在高维数据上，我们的方法可能更高效than alternative ones。附加的代码库可以在 \url{https://shorturl.at/rtuy5} 获取。Note:* "加铬-bernstein" is the Simplified Chinese name for Kac-Bernstein, which is a characterization of multivariate normality.* "独立测试" is the Simplified Chinese name for independence tests, which are used to determine whether two or more random variables are independent or not.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Genetic-Algorithm-Deep-GA-Approach-for-High-Dimensional-Nonlinear-Parabolic-Partial-Differential-Equations"><a href="#A-Deep-Genetic-Algorithm-Deep-GA-Approach-for-High-Dimensional-Nonlinear-Parabolic-Partial-Differential-Equations" class="headerlink" title="A Deep-Genetic Algorithm (Deep-GA) Approach for High-Dimensional Nonlinear Parabolic Partial Differential Equations"></a>A Deep-Genetic Algorithm (Deep-GA) Approach for High-Dimensional Nonlinear Parabolic Partial Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11558">http://arxiv.org/abs/2311.11558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Endah Rokhmati Merdika Putri, Muhammad Luthfi Shahab, Mohammad Iqbal, Imam Mukhlash, Amirul Hakam, Lutfi Mardianto, Hadi Susanto</li>
<li>for: 加速 deep-BSDE 方法的性能，解决高维 partial differential equations (PDEs) 通过其相应的 backwards stochastic differential equations (BSDEs)。</li>
<li>methods: 嵌入 genetic algorithm (GA) 优化初始猜测选择，以提高 solver 的稳定性和速度。</li>
<li>results: 应用于 two nonlinear parabolic PDEs， specifically the Black-Scholes (BS) equation with default risk and the Hamilton-Jacobi-Bellman (HJB) equation，并与 deep-BSDE 进行比较，显示了 Our method 提供了相当于精度的比较提高计算效率。<details>
<summary>Abstract</summary>
We propose a new method, called a deep-genetic algorithm (deep-GA), to accelerate the performance of the so-called deep-BSDE method, which is a deep learning algorithm to solve high dimensional partial differential equations through their corresponding backward stochastic differential equations (BSDEs). Recognizing the sensitivity of the solver to the initial guess selection, we embed a genetic algorithm (GA) into the solver to optimize the selection. We aim to achieve faster convergence for the nonlinear PDEs on a broader interval than deep-BSDE. Our proposed method is applied to two nonlinear parabolic PDEs, i.e., the Black-Scholes (BS) equation with default risk and the Hamilton-Jacobi-Bellman (HJB) equation. We compare the results of our method with those of the deep-BSDE and show that our method provides comparable accuracy with significantly improved computational efficiency.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，即深度遗传算法（深度-GA），以加速深度BSDE方法的性能，该方法是一种基于深度学习的方法来解决高维 partial differential equations（PDEs）的相应后逆随机差分方程（BSDEs）。我们认为选择初始猜测对解法的敏感性，因此我们将遗传算法（GA） embedding到解法中以优化选择。我们希望在更广泛的时间范围内实现更快的射线整合，而不是只是在深度BSDE中。我们应用我们的方法到两个非线性带拥有PDEs，即黑-股（BS）方程和汉密尔-雅各布-贝尔（HJB）方程。我们比较了我们的方法与深度BSDE的结果，并发现我们的方法可以保持相同的准确性，而且提高了计算效率。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Variation-in-Subpopulation-Susceptibility-to-Poisoning-Attacks"><a href="#Understanding-Variation-in-Subpopulation-Susceptibility-to-Poisoning-Attacks" class="headerlink" title="Understanding Variation in Subpopulation Susceptibility to Poisoning Attacks"></a>Understanding Variation in Subpopulation Susceptibility to Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11544">http://arxiv.org/abs/2311.11544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Rose, Fnu Suya, David Evans</li>
<li>for: 本研究旨在探讨机器学习模型在受到攻击时的漏斗性，具体来说是研究攻击者通过控制训练数据中一小部分数据点来控制模型的行为的情况。</li>
<li>methods: 本研究使用现有的攻击方法和数据集，并通过实验研究了不同的数据集和攻击方法之间的关系。</li>
<li>results: 研究发现， dataset separability 对于不同的子 популяции的漏斗性具有决定性的影响，而well-separated datasets 则更加виси于个体子 популяции的特性。此外，研究还发现了一个关键的子 популяции特性，即模型在净化数据集上的损失差异，可以用于评估攻击的效果。这种特性还可以在高维度的benchmark数据集上进行探讨。在Adult benchmark数据集上，研究发现可以找到semantically-meaningful的子 популяции特性，这些特性与选择的子 популяции的漏斗性相关。<details>
<summary>Abstract</summary>
Machine learning is susceptible to poisoning attacks, in which an attacker controls a small fraction of the training data and chooses that data with the goal of inducing some behavior unintended by the model developer in the trained model. We consider a realistic setting in which the adversary with the ability to insert a limited number of data points attempts to control the model's behavior on a specific subpopulation. Inspired by previous observations on disparate effectiveness of random label-flipping attacks on different subpopulations, we investigate the properties that can impact the effectiveness of state-of-the-art poisoning attacks against different subpopulations. For a family of 2-dimensional synthetic datasets, we empirically find that dataset separability plays a dominant role in subpopulation vulnerability for less separable datasets. However, well-separated datasets exhibit more dependence on individual subpopulation properties. We further discover that a crucial subpopulation property is captured by the difference in loss on the clean dataset between the clean model and a target model that misclassifies the subpopulation, and a subpopulation is much easier to attack if the loss difference is small. This property also generalizes to high-dimensional benchmark datasets. For the Adult benchmark dataset, we show that we can find semantically-meaningful subpopulation properties that are related to the susceptibilities of a selected group of subpopulations. The results in this paper are accompanied by a fully interactive web-based visualization of subpopulation poisoning attacks found at https://uvasrg.github.io/visualizing-poisoning
</details>
<details>
<summary>摘要</summary>
机器学习容易受到毒素攻击，攻击者控制一小部分训练数据，并选择这些数据以实现模型开发者未意的行为。我们考虑了一个现实情况，在其中敌对者有插入有限数据点的能力，并尝试控制模型对特定子population的行为。基于之前观察到不同子population对随机标签反转攻击的不同效果，我们研究了不同子population的攻击效果如何受到 dataset 的性能影响。对于一家二维synthetic dataset的家族，我们发现了实验ally 发现了 dataset 的分离度在不同子population的攻击效果中扮演了主导角色。然而，well-separated dataset 的攻击效果更多地受到个体子population的特性影响。我们还发现，在净 dataset 上模型和目标模型之间的损失差异对于攻击一个子population是非常重要的。这个特性还普遍适用于高维benchmark dataset。对于Adult benchmark dataset，我们发现可以找到semantically-meaningful的子population特性，这些特性与选择的子population攻击性相关。这些结果被 accompanies by a fully interactive web-based visualization of subpopulation poisoning attacks，可以在https://uvasrg.github.io/visualizing-poisoning 中找到。
</details></li>
</ul>
<hr>
<h2 id="An-NMF-Based-Building-Block-for-Interpretable-Neural-Networks-With-Continual-Learning"><a href="#An-NMF-Based-Building-Block-for-Interpretable-Neural-Networks-With-Continual-Learning" class="headerlink" title="An NMF-Based Building Block for Interpretable Neural Networks With Continual Learning"></a>An NMF-Based Building Block for Interpretable Neural Networks With Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11485">http://arxiv.org/abs/2311.11485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian K. Vogel</li>
<li>for: 本研究目的是寻找一种可以兼顾预测性和解释性的学习方法，以便在模型建构中保持高度的解释性，而不需要弃置预测性。</li>
<li>methods: 本研究使用了一种基于 nearest neighbors 和非正式矩阵分解（NMF）的建构块，称为 Predictive Factorized Coupling（PFC）块，它通过结合超vised neural network 训练方法来实现高度的预测性，同时保持 NMF 的解释性特点。</li>
<li>results: 研究人员通过对小型数据集进行测试，发现 PFC 块可以与多层感知机（MLP）模型相比，在预测性方面具有竞争力，同时也提供了更好的解释性。此外，研究人员还展示了使用 PFC 块构建更加表达强的架构，包括一个完全连接的 residual network 和一个分解的 RNN，这些架构可以在不同的情况下表现出色，例如在不同的数据分布情况下、在不同的训练数据量情况下、以及在知识移除之后。<details>
<summary>Abstract</summary>
Existing learning methods often struggle to balance interpretability and predictive performance. While models like nearest neighbors and non-negative matrix factorization (NMF) offer high interpretability, their predictive performance on supervised learning tasks is often limited. In contrast, neural networks based on the multi-layer perceptron (MLP) support the modular construction of expressive architectures and tend to have better recognition accuracy but are often regarded as black boxes in terms of interpretability. Our approach aims to strike a better balance between these two aspects through the use of a building block based on NMF that incorporates supervised neural network training methods to achieve high predictive performance while retaining the desirable interpretability properties of NMF. We evaluate our Predictive Factorized Coupling (PFC) block on small datasets and show that it achieves competitive predictive performance with MLPs while also offering improved interpretability. We demonstrate the benefits of this approach in various scenarios, such as continual learning, training on non-i.i.d. data, and knowledge removal after training. Additionally, we show examples of using the PFC block to build more expressive architectures, including a fully-connected residual network as well as a factorized recurrent neural network (RNN) that performs competitively with vanilla RNNs while providing improved interpretability. The PFC block uses an iterative inference algorithm that converges to a fixed point, making it possible to trade off accuracy vs computation after training but also currently preventing its use as a general MLP replacement in some scenarios such as training on very large datasets. We provide source code at https://github.com/bkvogel/pfc
</details>
<details>
<summary>摘要</summary>
现有的学习方法经常难以平衡解释性和预测性能。 nearest neighbors 和非负矩阵分解（NMF）可以提供高度的解释性，但其预测性能在编程学习任务中经常有限。 相比之下，基于多层感知器（MLP）的神经网络支持模块化的建构，并且具有更好的识别率，但是它们经常被视为黑盒子，即不可解释性。 我们的方法尝试平衡这两个方面，通过使用基于 NMF 的建筑块，并将神经网络训练方法引入，以实现高度的预测性能，同时保留 NMF 的愉悦解释性质。 我们在小 datasets 上评估了我们的 Predictive Factorized Coupling (PFC) 块，并证明它在编程学习任务中可以与 MLP 竞争，同时提供更好的解释性。 我们在不同的场景中展示了这种方法的优势，包括继续学习、训练非 i.i.d. 数据、知识移除等。 此外，我们还示出了使用 PFC 块建立更加表达性的架构，包括完全连接的差异阶段网络以及因子化 RNN，该架构可以与标准 RNN 竞争，同时提供更好的解释性。 PFC 块使用迭代推理算法，其总是会到达稳定点，因此可以在训练后进行精度与计算的交换，但目前不能作为普通 MLP 的替代品，尤其是在训练非常大的 dataset 时。 我们在 GitHub 上提供了代码，请参考 <https://github.com/bkvogel/pfc>。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Interpolation-Flows"><a href="#Gaussian-Interpolation-Flows" class="headerlink" title="Gaussian Interpolation Flows"></a>Gaussian Interpolation Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11475">http://arxiv.org/abs/2311.11475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Gao, Jian Huang, Yuling Jiao</li>
<li>for: 这种论文主要针对的是 simulation-free continuous normalizing flows for generative modeling，即使这种方法在实践中具有成功，它们的理论性和杜尼诺雷茨效应却得到了少量研究。</li>
<li>methods: 这篇论文使用 Gaussian denoising 建立了一种框架，用于研究 simulation-free continuous normalizing flows 的有效性。这种框架被称为 Gaussian interpolation flow，它可以确保流速场的 lipschitz 连续性、流存在和唯一性、流映射和时间反转映射的 lipschitz 连续性，以及目标分布的几种 riclass 的存在。</li>
<li>results: 这篇论文的结果表明，Gaussian interpolation flow 可以为 generative modeling 提供一种有理论基础的框架，并且可以确保这种框架的学习算法具有证明性。这些结论还探讨了这种流的稳定性和源分布的扰动。<details>
<summary>Abstract</summary>
Gaussian denoising has emerged as a powerful principle for constructing simulation-free continuous normalizing flows for generative modeling. Despite their empirical successes, theoretical properties of these flows and the regularizing effect of Gaussian denoising have remained largely unexplored. In this work, we aim to address this gap by investigating the well-posedness of simulation-free continuous normalizing flows built on Gaussian denoising. Through a unified framework termed Gaussian interpolation flow, we establish the Lipschitz regularity of the flow velocity field, the existence and uniqueness of the flow, and the Lipschitz continuity of the flow map and the time-reversed flow map for several rich classes of target distributions. This analysis also sheds light on the auto-encoding and cycle-consistency properties of Gaussian interpolation flows. Additionally, we delve into the stability of these flows in source distributions and perturbations of the velocity field, using the quadratic Wasserstein distance as a metric. Our findings offer valuable insights into the learning techniques employed in Gaussian interpolation flows for generative modeling, providing a solid theoretical foundation for end-to-end error analyses of learning GIFs with empirical observations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-a-Post-Market-Monitoring-Framework-for-Machine-Learning-based-Medical-Devices-A-case-study"><a href="#Towards-a-Post-Market-Monitoring-Framework-for-Machine-Learning-based-Medical-Devices-A-case-study" class="headerlink" title="Towards a Post-Market Monitoring Framework for Machine Learning-based Medical Devices: A case study"></a>Towards a Post-Market Monitoring Framework for Machine Learning-based Medical Devices: A case study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.11463">http://arxiv.org/abs/2311.11463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean Feng, Adarsh Subbaswamy, Alexej Gossmann, Harvineet Singh, Berkman Sahiner, Mi-Ok Kim, Gene Pennello, Nicholas Petrick, Romain Pirracchio, Fan Xia</li>
<li>for: 这 paper 的目的是提出一个系统性的框架，用于比较不同监测选项，以确保机器学习（ML）系统在临床实践中的性能不断监测。</li>
<li>methods: 这 paper 使用了 causal inference 和统计过程控制的工具，来定义候选监测标准、描述可能的偏见和 causal 模型，以及 specifying 和比较候选监测方法。</li>
<li>results: 这 paper 通过一个case study，探讨了一种 ML-based 风险预测算法，用于预测 POSToperative nausea and vomiting（PONV）。研究发现，使用 causal inference 可以 Address 其他偏见的源头，并且可以为更多的监测选项提供一个系统性的框架。<details>
<summary>Abstract</summary>
After a machine learning (ML)-based system is deployed in clinical practice, performance monitoring is important to ensure the safety and effectiveness of the algorithm over time. The goal of this work is to highlight the complexity of designing a monitoring strategy and the need for a systematic framework that compares the multitude of monitoring options. One of the main decisions is choosing between using real-world (observational) versus interventional data. Although the former is the most convenient source of monitoring data, it exhibits well-known biases, such as confounding, selection, and missingness. In fact, when the ML algorithm interacts with its environment, the algorithm itself may be a primary source of bias. On the other hand, a carefully designed interventional study that randomizes individuals can explicitly eliminate such biases, but the ethics, feasibility, and cost of such an approach must be carefully considered. Beyond the decision of the data source, monitoring strategies vary in the performance criteria they track, the interpretability of the test statistics, the strength of their assumptions, and their speed at detecting performance decay. As a first step towards developing a framework that compares the various monitoring options, we consider a case study of an ML-based risk prediction algorithm for postoperative nausea and vomiting (PONV). Bringing together tools from causal inference and statistical process control, we walk through the basic steps of defining candidate monitoring criteria, describing potential sources of bias and the causal model, and specifying and comparing candidate monitoring procedures. We hypothesize that these steps can be applied more generally, as causal inference can address other sources of biases as well.
</details>
<details>
<summary>摘要</summary>
après deployment d'un système d'apprentissage machine (ML) dans la pratique clinique, il est important de surveiller le performance pour garantir la sécurité et l'efficacité de l'algorithme au fil du temps. L'objectif de cette étude est de mettre en évidence la complexité de la stratégie de surveillance et la nécessité d'un framework systématique pour comparer les nombreuses options de surveillance. L'une des décisions les plus importantes est de choisir entre les données réelles (observationnelles) et les données intervenantes. Bien que les premières soient la source de données de surveillance la plus commode, elles présentent des biais bien connus, tels que la confusion, la sélection et la absence. En fait, lorsque l'algorithme ML interagit avec son environnement, l'algorithme peut être une source primaire de biais. D'un autre côté, un étude intervenant rigoureusement randomisée peut éliminer explicitement tels biais, mais la moralité, la faisabilité et le coût de telle approche doivent être soigneusement examinés. En dehors de la décision de la source de données, les stratégies de surveillance varient selon les critères de performance qu'elles suivent, l'interprétabilité des statistiques de test, les hypothèses sous-jacentes et leur capacité à détecter rapidement la décadence des performances. Comme premier pas vers le développement d'un framework qui compare les différentes options de surveillance, nous considérons un étude de cas d'un algorithme de prédiction de risque basé sur l'apprentissage machine pour la nausée et les vomissements postopératoires (PONV). En combinant des outils de la inference causale et de la contrôle statistique de processus, nous passons en revue les étapes de base pour définir les critères de surveillance candidate, décrire les sources potentielles de biais et le modèle causal, et spécifier et comparer des procédures de surveillance candidate. Nous hypothèsons que ces étapes peuvent être appliquées de manière plus générale, car l'inférence causale peut aborder d'autres sources de biais.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/20/cs.LG_2023_11_20/" data-id="clp869u1j00v9k588hky58u9v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/20/cs.CL_2023_11_20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-11-20
        
      </div>
    </a>
  
  
    <a href="/2023/11/20/eess.IV_2023_11_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-11-20</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
