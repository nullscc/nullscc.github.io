
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-11-15 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09197 repo_url: None paper_authors: Jason Gaitonde, Elchanan Mossel for: 这个论文是为了">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-11-15">
<meta property="og:url" content="https://nullscc.github.io/2023/11/15/cs.LG_2023_11_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09197 repo_url: None paper_authors: Jason Gaitonde, Elchanan Mossel for: 这个论文是为了">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-15T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-19T06:27:35.005Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.LG_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T10:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-11-15
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Unified-Approach-to-Learning-Ising-Models-Beyond-Independence-and-Bounded-Width"><a href="#A-Unified-Approach-to-Learning-Ising-Models-Beyond-Independence-and-Bounded-Width" class="headerlink" title="A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width"></a>A Unified Approach to Learning Ising Models: Beyond Independence and Bounded Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09197">http://arxiv.org/abs/2311.09197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Gaitonde, Elchanan Mossel</li>
<li>for: 这个论文是为了有效地从数据中学习ising模型的下面参数。</li>
<li>methods: 这篇论文使用的方法是node-wise logistic regression。</li>
<li>results: 这篇论文的结果是，node-wise logistic regression可以在一些不满足现有的假设的情况下也能够有效地学习ising模型的参数，包括：（1）从各种本地Markov链生成的数据中 recuperate the parameters with optimal sample complexity up to $\log\log n$ factors。（2）在Sherrington-Kirkpatrick模型中，从$\mathsf{poly}(n)$独立样本中 recuperate the parameters in most of the known高温度 regime。（3）在M-regime中的数据中，logistic regression achieve an exponential improvement in learning from samples。<details>
<summary>Abstract</summary>
We revisit the problem of efficiently learning the underlying parameters of Ising models from data. Current algorithmic approaches achieve essentially optimal sample complexity when given i.i.d. samples from the stationary measure and the underlying model satisfies "width" bounds on the total $\ell_1$ interaction involving each node. We show that a simple existing approach based on node-wise logistic regression provably succeeds at recovering the underlying model in several new settings where these assumptions are violated:   (1) Given dynamically generated data from a wide variety of local Markov chains, like block or round-robin dynamics, logistic regression recovers the parameters with optimal sample complexity up to $\log\log n$ factors. This generalizes the specialized algorithm of Bresler, Gamarnik, and Shah [IEEE Trans. Inf. Theory'18] for structure recovery in bounded degree graphs from Glauber dynamics.   (2) For the Sherrington-Kirkpatrick model of spin glasses, given $\mathsf{poly}(n)$ independent samples, logistic regression recovers the parameters in most of the known high-temperature regime via a simple reduction to weaker structural properties of the measure. This improves on recent work of Anari, Jain, Koehler, Pham, and Vuong [ArXiv'23] which gives distribution learning at higher temperature.   (3) As a simple byproduct of our techniques, logistic regression achieves an exponential improvement in learning from samples in the M-regime of data considered by Dutt, Lokhov, Vuffray, and Misra [ICML'21] as well as novel guarantees for learning from the adversarial Glauber dynamics of Chin, Moitra, Mossel, and Sandon [ArXiv'23].   Our approach thus significantly generalizes the elegant analysis of Wu, Sanghavi, and Dimakis [Neurips'19] without any algorithmic modification.
</details>
<details>
<summary>摘要</summary>
我们回归到对零点模型的效率学习问题。现有的算法方法可以在具有独立同分布的样本下 achievement essentially optimal sample complexity。我们显示了一个简单的现有方法，基于单点逻辑回归，可以在以下新的设定中 recuperate 零点模型：(1) 对具有各种本地Markov过程生成的资料进行动态生成，如封页或者轮询动态，逻辑回归可以从样本中 recuperate 零点模型， sample complexity 只差loglog n个因素。这扩展了 Bresler、Gamarnik 和 Shah 的特殊算法 [IEEE Trans. Inf. Theory'18] 用于结构回传graph from Glauber dynamics。(2) 韦伯-基督徒模型中的磁矩玻璃，对于大多数高温区域，逻辑回归可以从独立样本中 recuperate 零点模型， sample complexity 仅差poly(n)个因素。这超越了 recent work of Anari、Jain、Koehler、Pham 和 Vuong [ArXiv'23] 的分布学习。(3) 作为我们的技术的副产物，逻辑回归可以从M-regime的数据中学习，并提供了 novel guarantees for learning from adversarial Glauber dynamics。我们的方法因此可以从Wu、Sanghavi 和 Dimakis [Neurips'19] 的整体分析中获得 significan improvement。我们的方法不需要任何算法修改，可以从零点模型中 recuperate 零点模型，sample complexity 仅差loglog n个因素。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Curriculum-Generation-for-Autonomous-Reinforcement-Learning-without-Task-Specific-Knowledge"><a href="#Self-Supervised-Curriculum-Generation-for-Autonomous-Reinforcement-Learning-without-Task-Specific-Knowledge" class="headerlink" title="Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge"></a>Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09195">http://arxiv.org/abs/2311.09195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hyun Lee, Seung-Woo Seo</li>
<li>for: 实现现实环境中的强化学习算法，即无需人工干预重新设定环境。</li>
<li>methods: 提出了一个新的自主强化学习（ARL）算法，可以根据学习进度自动生成课程。这个课程可以减少需要的人工重新设定，并且不需要任务特定的知识。</li>
<li>results: 在实验中，我们的ARL算法可以生成一个适应学习进度的课程，并允许Agent自动重新设定到多样和有用的初始状态。我们的课程可以帮助Agent快速获得解释，并且在缺乏优化的情况下表现更好。<details>
<summary>Abstract</summary>
A significant bottleneck in applying current reinforcement learning algorithms to real-world scenarios is the need to reset the environment between every episode. This reset process demands substantial human intervention, making it difficult for the agent to learn continuously and autonomously. Several recent works have introduced autonomous reinforcement learning (ARL) algorithms that generate curricula for jointly training reset and forward policies. While their curricula can reduce the number of required manual resets by taking into account the agent's learning progress, they rely on task-specific knowledge, such as predefined initial states or reset reward functions. In this paper, we propose a novel ARL algorithm that can generate a curriculum adaptive to the agent's learning progress without task-specific knowledge. Our curriculum empowers the agent to autonomously reset to diverse and informative initial states. To achieve this, we introduce a success discriminator that estimates the success probability from each initial state when the agent follows the forward policy. The success discriminator is trained with relabeled transitions in a self-supervised manner. Our experimental results demonstrate that our ARL algorithm can generate an adaptive curriculum and enable the agent to efficiently bootstrap to solve sparse-reward maze navigation tasks, outperforming baselines with significantly fewer manual resets.
</details>
<details>
<summary>摘要</summary>
现有一个重要的瓶颈在应用现有的奖励学习算法到实际场景中，那就是需要在每个话语中重置环境。这个重置过程需要大量的人工干预，使得Agent无法不断学习和自动学习。一些最近的工作已经引入了自主奖励学习（ARL）算法，这些算法可以生成合适的课程，以便同时培养重置和前进政策。而这些课程可以通过考虑Agent的学习进度来减少需要的人工重置数量，但是它们需要任务特定的知识，例如预先定义的初始状态或重置奖励函数。在这篇论文中，我们提出了一种新的ARL算法，可以根据Agent的学习进度自适应生成课程，不需要任务特定的知识。我们的课程使得Agent可以自主地重置到多样化和有用的初始状态。为了实现这一点，我们引入了一个成功预测器，该预测器可以在Agent遵循前进政策时，根据每个初始状态的成功概率进行预测。成功预测器通过对每个过程进行自我supervised的标注训练。我们的实验结果表明，我们的ARL算法可以生成适应性的课程，使得Agent能够高效地 bootstrap到解决稀热奖励迷宫 Navigation task，与基eline相比，需要 significatively fewer manual resets。
</details></li>
</ul>
<hr>
<h2 id="Approaching-adverse-event-detection-utilizing-transformers-on-clinical-time-series"><a href="#Approaching-adverse-event-detection-utilizing-transformers-on-clinical-time-series" class="headerlink" title="Approaching adverse event detection utilizing transformers on clinical time-series"></a>Approaching adverse event detection utilizing transformers on clinical time-series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09165">http://arxiv.org/abs/2311.09165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helge Fredriksen, Per Joel Burman, Ashenafi Woldaregay, Karl Øyvind Mikalsen, Ståle Nymo</li>
<li>for: 这个研究旨在发展一个侦测病人医疗追踪的偏离系统，以预防错误诊断或不适当的治疗，从而降低不良事件的机会。</li>
<li>methods: 这个研究使用了一个自动学习框架，基于STraTS transformer架构，将时间序列资料转换为内在空间表示。这些表示之后运用不同的聚集技术来探索患者的临床进程特征。</li>
<li>results: 这个研究的初步结果具有探索患者的临床进程特征，但发现更多的数据，特别是患者的民生资讯，是关键的 для更好地评估方法的性能。<details>
<summary>Abstract</summary>
Patients being admitted to a hospital will most often be associated with a certain clinical development during their stay. However, there is always a risk of patients being subject to the wrong diagnosis or to a certain treatment not pertaining to the desired effect, potentially leading to adverse events. Our research aims to develop an anomaly detection system for identifying deviations from expected clinical trajectories. To address this goal we analyzed 16 months of vital sign recordings obtained from the Nordland Hospital Trust (NHT). We employed an self-supervised framework based on the STraTS transformer architecture to represent the time series data in a latent space. These representations were then subjected to various clustering techniques to explore potential patient phenotypes based on their clinical progress. While our preliminary results from this ongoing research are promising, they underscore the importance of enhancing the dataset with additional demographic information from patients. This additional data will be crucial for a more comprehensive evaluation of the method's performance.
</details>
<details>
<summary>摘要</summary>
Hospitalized patients often experience clinical developments during their stay, but there is a risk of incorrect diagnosis or inappropriate treatment, which can lead to adverse events. Our research aims to develop an anomaly detection system to identify deviations from expected clinical trajectories. We analyzed 16 months of vital sign recordings from the Nordland Hospital Trust (NHT) using a self-supervised framework based on the STraTS transformer architecture to represent the time series data in a latent space. We then applied various clustering techniques to explore potential patient phenotypes based on their clinical progress. While our preliminary results are promising, we recognize the need to enhance the dataset with additional demographic information from patients to evaluate the method's performance more comprehensively.Here's the word-for-word translation of the text into Simplified Chinese: hospitalized patients 常常会经历医学发展 during their stay, but there is a risk of incorrect diagnosis or inappropriate treatment, which can lead to adverse events. 我们的研究目标是开发一个异常检测系统，用于标识不符预期的临床轨迹。我们使用了 Nordland Hospital Trust (NHT) 的 16 个月的重要指标记录数据，并使用了一种无监督框架，基于 STraTS 转换器架构，将时间序列数据表示在幂值空间中。我们然后应用了不同的聚类技术，以探索可能的患者类型，基于他们的临床进程。虽然我们的初步结果是有前途，但我们认为需要更多的人口数据，以更好地评估方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Model-Agnostic-Explainable-Selective-Regression-via-Uncertainty-Estimation"><a href="#Model-Agnostic-Explainable-Selective-Regression-via-Uncertainty-Estimation" class="headerlink" title="Model Agnostic Explainable Selective Regression via Uncertainty Estimation"></a>Model Agnostic Explainable Selective Regression via Uncertainty Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09145">http://arxiv.org/abs/2311.09145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Pugnana, Carlos Mougan, Dan Saattrup Nielsen</li>
<li>for: 提高机器学习系统的可靠性和信任性</li>
<li>methods: 使用模型无参数不确定性估计来实现选择性回归</li>
<li>results: 与现有选择回归方法相比，提出了一种新的选择回归方法，在69个数据集上进行了广泛的比较，并使用可解释AI技术来理解选择回归的驱动力。<details>
<summary>Abstract</summary>
With the wide adoption of machine learning techniques, requirements have evolved beyond sheer high performance, often requiring models to be trustworthy. A common approach to increase the trustworthiness of such systems is to allow them to refrain from predicting. Such a framework is known as selective prediction. While selective prediction for classification tasks has been widely analyzed, the problem of selective regression is understudied. This paper presents a novel approach to selective regression that utilizes model-agnostic non-parametric uncertainty estimation. Our proposed framework showcases superior performance compared to state-of-the-art selective regressors, as demonstrated through comprehensive benchmarking on 69 datasets. Finally, we use explainable AI techniques to gain an understanding of the drivers behind selective regression. We implement our selective regression method in the open-source Python package doubt and release the code used to reproduce our experiments.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的广泛应用，模型的需求已经超出了纯粹的高性能，常常需要模型具备可靠性。一种常见的方法来增强机器学习系统的可靠性是让它们可以决定不预测。这种框架被称为选择性预测。在分类任务上，选择性预测已经得到了广泛的研究，而选择性回归问题尚未得到充分研究。本文提出了一种新的选择性回归方法，利用模型不 Parametric 不确定性估计。我们的提议的框架在69个数据集上进行了完整的比较，并示出了与当前最佳选择性回归器相比的超过其他的性能。最后，我们使用可解释AI技术来理解选择性回归的驱动力。我们实现了我们的选择性回归方法在Python开源包中，并将用于重现我们的实验的代码发布。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-parameter-tracking-with-partial-state-observation"><a href="#Machine-learning-parameter-tracking-with-partial-state-observation" class="headerlink" title="Machine-learning parameter tracking with partial state observation"></a>Machine-learning parameter tracking with partial state observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09142">http://arxiv.org/abs/2311.09142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng-Meng Zhai, Mohammadamin Moradi, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai</li>
<li>for: 这个论文是为了解决复杂非线性动力系统中时间变化的参数的准确追踪问题。</li>
<li>methods: 这个论文使用了模型自由和完全数据驱动的方法，利用储存器计算来准确地追踪时间变化的参数从部分状态观测数据中。</li>
<li>results: 论文通过使用训练数据来预测参数的变化情况，并在低维和高维、马尔可夫和非马尔可夫非线性动力系统上进行了示例。<details>
<summary>Abstract</summary>
Complex and nonlinear dynamical systems often involve parameters that change with time, accurate tracking of which is essential to tasks such as state estimation, prediction, and control. Existing machine-learning methods require full state observation of the underlying system and tacitly assume adiabatic changes in the parameter. Formulating an inverse problem and exploiting reservoir computing, we develop a model-free and fully data-driven framework to accurately track time-varying parameters from partial state observation in real time. In particular, with training data from a subset of the dynamical variables of the system for a small number of known parameter values, the framework is able to accurately predict the parameter variations in time. Low- and high-dimensional, Markovian and non-Markovian nonlinear dynamical systems are used to demonstrate the power of the machine-learning based parameter-tracking framework. Pertinent issues affecting the tracking performance are addressed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>复杂非线性动力系统经常存在时间变化的参数，正确跟踪这些参数是STATE estimation、预测和控制任务中的关键。现有的机器学习方法需要完整的状态观察，而tacitly assume 非断续变化的参数。通过形式化反问题并利用储存计算，我们开发了一种无模型和完全数据驱动的框架，可以在部分状态观察下，在实时中高精度地跟踪时间变化的参数。特别是，通过训练数据来自系统动态变量的一个子集，框架可以准确预测参数的时间变化。低维度和高维度、Markovian和非Markovian非线性动力系统都用来证明框架的力量。关于跟踪性能的问题也进行了解决。
</details></li>
</ul>
<hr>
<h2 id="Causal-prediction-models-for-medication-safety-monitoring-The-diagnosis-of-vancomycin-induced-acute-kidney-injury"><a href="#Causal-prediction-models-for-medication-safety-monitoring-The-diagnosis-of-vancomycin-induced-acute-kidney-injury" class="headerlink" title="Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury"></a>Causal prediction models for medication safety monitoring: The diagnosis of vancomycin-induced acute kidney injury</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09137">http://arxiv.org/abs/2311.09137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izak Yasrebi-de Kom, Joanna Klopotowska, Dave Dongelmans, Nicolette De Keizer, Kitty Jager, Ameen Abu-Hanna, Giovanni Cinà</li>
<li>for: 这个研究的目的是为了开发一种基于观察数据的 causal modeling 方法，用于估计药物可能对患者具有的危害，以提高医院化学otherapy中的药物安全监测。</li>
<li>methods: 这个研究使用了两个关键的 causal inference 组件：（1）目标试验模拟框架，和（2）基于机器学习的个性化治疗效果估计。研究使用了观察数据来估计药物可能对患者具有的危害，并与医学专家提供的估计相比较。</li>
<li>results: 研究发现，使用 causal modeling 方法可以提供更为precise的危害估计，并且可以减少人类偏见的影响。然而，研究还存在一些重要的限制和改进方向，例如需要更多的观察数据和更好的模型。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
The current best practice approach for the retrospective diagnosis of adverse drug events (ADEs) in hospitalized patients relies on a full patient chart review and a formal causality assessment by multiple medical experts. This evaluation serves to qualitatively estimate the probability of causation (PC); the probability that a drug was a necessary cause of an adverse event. This practice is manual, resource intensive and prone to human biases, and may thus benefit from data-driven decision support. Here, we pioneer a causal modeling approach using observational data to estimate a lower bound of the PC (PC$_{low}$). This method includes two key causal inference components: (1) the target trial emulation framework and (2) estimation of individualized treatment effects using machine learning. We apply our method to the clinically relevant use-case of vancomycin-induced acute kidney injury in intensive care patients, and compare our causal model-based PC$_{low}$ estimates to qualitative estimates of the PC provided by a medical expert. Important limitations and potential improvements are discussed, and we conclude that future improved causal models could provide essential data-driven support for medication safety monitoring in hospitalized patients.
</details>
<details>
<summary>摘要</summary>
现有的最佳实践方法 для逆向诊断医药不良反应（ADE）在医院化症病人中是通过全部病人护理记录和多名医疗专家形式的可能性评估来估计可能性（PC），即药物是否是必要的 causa causans 的医学事件。这种评估方法是手动、资源占用和人类偏见易受影响的，因此可能会从数据驱动支持中受益。在这里，我们开拓了一种使用观察数据来估计PC下界（PC$_{low}$）的 causal 模型方法。这个方法包括两个关键的 causal inference 组件：（1）目标试验模拟框架和（2）使用机器学习来估计个体化治疗效果。我们在医学实践中有优势的使用vancomycin引起的急性肾脏损伤的例子中应用了我们的方法，并与医疗专家提供的PC估计进行比较。我们讨论了重要的局限性和改进方向，并结论未来改进的causal模型可能提供数据驱动支持医学安全监测的 essence。
</details></li>
</ul>
<hr>
<h2 id="Fast-Detection-of-Phase-Transitions-with-Multi-Task-Learning-by-Confusion"><a href="#Fast-Detection-of-Phase-Transitions-with-Multi-Task-Learning-by-Confusion" class="headerlink" title="Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion"></a>Fast Detection of Phase Transitions with Multi-Task Learning-by-Confusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09128">http://arxiv.org/abs/2311.09128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Arnold, Frank Schäfer, Niels Lörch</li>
<li>for: 这个论文的目的是提出一种改进learning-by-confusion scheme的方法，以便更快地identify critical points from data without prior knowledge of the underlying phases.</li>
<li>methods: 这个方法使用了多任务学习，只需要训练一个多类分类器，而不是每个可能的grid point的二分类器，从而避免了与grid point数直接相关的计算成本增加。</li>
<li>results: 在应用于易斯宁模型和一个由Stable Diffusion生成的图像集上，发现了明显的加速，与理想情况closely corresponds，只有小量偏差。<details>
<summary>Abstract</summary>
Machine learning has been successfully used to study phase transitions. One of the most popular approaches to identifying critical points from data without prior knowledge of the underlying phases is the learning-by-confusion scheme. As input, it requires system samples drawn from a grid of the parameter whose change is associated with potential phase transitions. Up to now, the scheme required training a distinct binary classifier for each possible splitting of the grid into two sides, resulting in a computational cost that scales linearly with the number of grid points. In this work, we propose and showcase an alternative implementation that only requires the training of a single multi-class classifier. Ideally, such multi-task learning eliminates the scaling with respect to the number of grid points. In applications to the Ising model and an image dataset generated with Stable Diffusion, we find significant speedups that closely correspond to the ideal case, with only minor deviations.
</details>
<details>
<summary>摘要</summary>
机器学习已成功应用于研究相转换。一种最受欢迎的方法是无知下相转换点的识别，即学习混淆方案。作为输入，它需要系统样本从 Parameters 的变化相关的潜在相转换点采样。现在，该方法需要训练每个可能的网格分割两侧的独立二进制分类器，因此计算成本与网格点数成直线关系。在这种工作中，我们提议并展示了一种替代实现，只需训练一个多类分类器。理想情况下，这种多任务学习可以消除与网格点数相关的扩展。在应用于 Ising 模型和通过 Stable Diffusion 生成的图像集上，我们发现了显著的加速，与理想情况几乎完全相符，只有小差异。
</details></li>
</ul>
<hr>
<h2 id="Damped-Proximal-Augmented-Lagrangian-Method-for-weakly-Convex-Problems-with-Convex-Constraints"><a href="#Damped-Proximal-Augmented-Lagrangian-Method-for-weakly-Convex-Problems-with-Convex-Constraints" class="headerlink" title="Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints"></a>Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09065">http://arxiv.org/abs/2311.09065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hari Dahal, Wei Liu, Yangyang Xu</li>
<li>for:  solves problems with a weakly-convex objective and convex linear&#x2F;nonlinear constraints.</li>
<li>methods:  uses a damped proximal augmented Lagrangian method (DPALM) that adopts a damped dual stepsize to ensure the boundedness of dual iterates.</li>
<li>results:  can produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations, and achieves overall iteration complexity of $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$ or $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$ depending on the structure of the problem.<details>
<summary>Abstract</summary>
We give a damped proximal augmented Lagrangian method (DPALM) for solving problems with a weakly-convex objective and convex linear/nonlinear constraints. Instead of taking a full stepsize, DPALM adopts a damped dual stepsize to ensure the boundedness of dual iterates. We show that DPALM can produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations if each DPALM subproblem is solved to a proper accuracy. In addition, we establish overall iteration complexity of DPALM when the objective is either a regularized smooth function or in a regularized compositional form. For the former case, DPALM achieves the complexity of $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$ to produce an $\varepsilon$-KKT point by applying an accelerated proximal gradient (APG) method to each DPALM subproblem. For the latter case, the complexity of DPALM is $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$ to produce a near $\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothed version of each subproblem. Our outer iteration complexity and the overall complexity either generalize existing best ones from unconstrained or linear-constrained problems to convex-constrained ones, or improve over the best-known results on solving the same-structured problems. Furthermore, numerical experiments on linearly/quadratically constrained non-convex quadratic programs and linear-constrained robust nonlinear least squares are conducted to demonstrate the empirical efficiency of the proposed DPALM over several state-of-the art methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一个受束的减少对应扩展拉格朗日方法（DPALM），用于解决具有弱抽象目标函数和凸线性/非凸约束的问题。而不是使用全部步长，DPALM 使用一个减少的对应步长，以确保对应变数的积分。我们证明了 DPALM 可以在 $O(\vareps^{-2})$ 外部迭代中生成一个（近似） $\vareps$-KKT 点。此外，我们确立了 DPALM 的总迭代复杂度，当 objective 是 Either 是弹簧缓和变数弹簧缓 的情况下，DPALM 的总迭代复杂度为 $\widetilde{\mathcal{O}\left(\varepsilon^{-2.5} \right)$，用于生成一个 $\varepsilon$-KKT 点。另一方面，当 objective 是弹簧缓和变数弹簧缓的情况下，DPALM 的总迭代复杂度为 $\widetilde{\mathcal{O}\left(\varepsilon^{-3} \right)$，用于生成一个近似 $\varepsilon$-KKT 点。我们的外部迭代复杂度和总迭代复杂度都可以与现有最佳的结果对应，或者超过相同类型的问题的最佳结果。此外，我们还进行了一些数据实验，用于证明 DPALM 的实际高效性，并与一些现有的方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="New-Horizons-in-Parameter-Regularization-A-Constraint-Approach"><a href="#New-Horizons-in-Parameter-Regularization-A-Constraint-Approach" class="headerlink" title="New Horizons in Parameter Regularization: A Constraint Approach"></a>New Horizons in Parameter Regularization: A Constraint Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09058">http://arxiv.org/abs/2311.09058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jörg K. H. Franke, Michael Hefenbrock, Gregor Koehler, Frank Hutter</li>
<li>for: 这篇论文是为了探讨受限制的参数调整（Constrained Parameter Regularization，CPR），它是传统量化调整的一种替代方案。</li>
<li>methods: 这篇论文使用了一种受限制的参数调整方法，通过强制个别参数群的统计量（例如L$_2$-norm）的Upper bound，将学习变成一个受限制的优化问题。以解决这个问题，这篇论文使用了一种扩展的拉格朗日方法。</li>
<li>results: 这篇论文的实验结果显示，CPR可以对“grokking”现象产生阻据，并且与传统量化调整相比，CPR能够将性能提升到相同或更高的水平。<details>
<summary>Abstract</summary>
This work presents constrained parameter regularization (CPR), an alternative to traditional weight decay. Instead of applying a constant penalty uniformly to all parameters, we enforce an upper bound on a statistical measure (e.g., the L$_2$-norm) of individual parameter groups. This reformulates learning as a constrained optimization problem. To solve this, we utilize an adaptation of the augmented Lagrangian method. Our approach allows for varying regularization strengths across different parameter groups, removing the need for explicit penalty coefficients in the regularization terms. CPR only requires two hyperparameters and introduces no measurable runtime overhead. We offer empirical evidence of CPR's effectiveness through experiments in the "grokking" phenomenon, image classification, and language modeling. Our findings show that CPR can counteract the effects of grokking, and it consistently matches or surpasses the performance of traditional weight decay.
</details>
<details>
<summary>摘要</summary>
这个研究提出了受限制参数规范化（CPR），它是传统权值衰退的替代方案。而不是对所有参数应用一定的罚款，我们强制某个统计量（例如L$_2$-范数）的参数组别的Upper bound。这将学习转化为一个受限制优化问题。为解决这个问题，我们利用一种改进后的扩展拉格朗日方法。我们的方法允许不同参数组别的强制规范强度不同，从而消除了显式的罚款系数在规范项中。CPR只需要两个 гиперпараметров，并没有可观测的运行时间开销。我们提供了CPR的效iveness的实验证据，包括“grokking”现象、图像分类和自然语言处理等领域的实验结果。我们的发现表明，CPR可以抵消grokking的效果，并在性能方面与传统权值衰退相匹配或甚至超过。
</details></li>
</ul>
<hr>
<h2 id="On-the-Foundation-of-Distributionally-Robust-Reinforcement-Learning"><a href="#On-the-Foundation-of-Distributionally-Robust-Reinforcement-Learning" class="headerlink" title="On the Foundation of Distributionally Robust Reinforcement Learning"></a>On the Foundation of Distributionally Robust Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09018">http://arxiv.org/abs/2311.09018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou</li>
<li>for: 提供了一种 Distributionally Robust Reinforcement Learning (DRRL) 的理论基础，以适应环境变化。</li>
<li>methods: 使用 Distributionally Robust Markov Decision Processes (DRMDPs) 模型，要求决策者选择最优策略在最差情况下。</li>
<li>results: 提供了一种推广和扩展现有形式ulation的 DRMDP 框架，并研究了控制器和敌对者属性的影响。 另外，还提供了一些counterexample，表明在某些情况下，DPP 无法存在。<details>
<summary>Abstract</summary>
Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the existence or absence of the dynamic programming principle (DPP). From an algorithmic standpoint, the existence of DPP holds significant implications, as the vast majority of existing data and computationally efficiency RL algorithms are reliant on the DPP. To study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. We also offer counterexamples for settings in which a DPP with full generality is absent.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Motivated by the need for a robust policy in the face of environment shifts between training and the deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embraces various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the existence or absence of the dynamic programming principle (DPP). From an algorithmic standpoint, the existence of DPP holds significant implications, as the vast majority of existing data and computationally efficiency RL algorithms are reliant on the DPP. To study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. We also offer counterexamples for settings in which a DPP with full generality is absent."中文翻译：<<SYS>>受到训练和部署环境变化的需求启发，我们贡献了分布robust reinforcement learning（DRRL）的理论基础。我们通过一个包容性强的模型化框架，中心是分布robust Markov决策过程（DRMDP），要求决策者在敌对者引起的最差分布变化下选择最优策略。我们通过一元化和扩展现有的表述，彻底构造DRMDP，覆盖决策者和敌对者的各种模型特性，包括适应粒度、历史依赖、Markov和Markov时间同质的决策者和敌对者动态。此外，我们还探讨敌对者引起的变化的灵活性，包括SA和S-正方形性。在DRMDP框架中，我们研究DPP的存在或缺失的条件。从算法角度来看，DPP的存在具有重要意义，因为大多数现有的数据和计算效率RL算法都依赖于DPP。为了研究其存在，我们全面检查控制者和敌对者特性的组合，提供了一致的证明方法。我们还提供了对某些设置中DPP的全面性是缺失的counterexample。
</details></li>
</ul>
<hr>
<h2 id="Semidefinite-programs-simulate-approximate-message-passing-robustly"><a href="#Semidefinite-programs-simulate-approximate-message-passing-robustly" class="headerlink" title="Semidefinite programs simulate approximate message passing robustly"></a>Semidefinite programs simulate approximate message passing robustly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09017">http://arxiv.org/abs/2311.09017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Misha Ivkov, Tselil Schramm</li>
<li>for: 这个论文是为了研究approximate message passing（AMP）算法的稳定性和可靠性。</li>
<li>methods: 论文使用了local statistics hierarchy的semidefinite programs（SDPs）来模拟AMP算法，并提供了对多种average-case优化问题的稳定性和可靠性保证。</li>
<li>results: 论文提出了一些robust guarantees для多种average-case优化问题，并与之前的强下界相比，表明AMP算法在这些问题上的性能较高。<details>
<summary>Abstract</summary>
Approximate message passing (AMP) is a family of iterative algorithms that generalize matrix power iteration. AMP algorithms are known to optimally solve many average-case optimization problems. In this paper, we show that a large class of AMP algorithms can be simulated in polynomial time by \emph{local statistics hierarchy} semidefinite programs (SDPs), even when an unknown principal minor of measure $1/\mathrm{polylog}(\mathrm{dimension})$ is adversarially corrupted. Ours are the first robust guarantees for many of these problems. Further, our results offer an interesting counterpoint to strong lower bounds against less constrained SDP relaxations for average-case max-cut-gain (a.k.a. "optimizing the Sherrington-Kirkpatrick Hamiltonian") and other problems.
</details>
<details>
<summary>摘要</summary>
通用消息传递（Approximate Message Passing，简称AMP）是一族迭代算法，可泛化矩阵力豪迭代。AMP算法能够优化许多平均情况优化问题。在这篇论文中，我们展示了一类AMP算法可以通过本地统计层次 semidefinite programs（SDPs）在几乎NP复杂度下模拟，即使Unknown principal minor的推算精度为1/多项式log（维度）。这些结果是许多问题的首次Robust guarantees。此外，我们的结果还提供了一些对具有较弱约束的SDP relaxation的强下界的反对照。
</details></li>
</ul>
<hr>
<h2 id="sQUlearn-unicode-x2013-A-Python-Library-for-Quantum-Machine-Learning"><a href="#sQUlearn-unicode-x2013-A-Python-Library-for-Quantum-Machine-Learning" class="headerlink" title="sQUlearn $\unicode{x2013}$ A Python Library for Quantum Machine Learning"></a>sQUlearn $\unicode{x2013}$ A Python Library for Quantum Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08990">http://arxiv.org/abs/2311.08990</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/squlearn/squlearn">https://github.com/squlearn/squlearn</a></li>
<li>paper_authors: David A. Kreplin, Moritz Willmann, Jan Schnabel, Frederic Rapp, Marco Roth</li>
<li>for: 这个论文旨在提供一个易用的Python库，用于Quantum Machine Learning（QML），并且可以与传统的机器学习工具Like scikit-learn集成。</li>
<li>methods: 这个库使用了双层架构，供QML研究者和实践者使用，并提供了高效的实验、测试和管道功能。它还包括了量子传播方法和量子神经网络，以及自定义数据编码策略、自动化执行处理和特殊的核心调理技术。</li>
<li>results: 这个库的目标是将现有的量子计算能力与实际的机器学习应用 bridging，并提供一个易用的、NISQ-兼容的Python库，以便实际应用。<details>
<summary>Abstract</summary>
sQUlearn introduces a user-friendly, NISQ-ready Python library for quantum machine learning (QML), designed for seamless integration with classical machine learning tools like scikit-learn. The library's dual-layer architecture serves both QML researchers and practitioners, enabling efficient prototyping, experimentation, and pipelining. sQUlearn provides a comprehensive toolset that includes both quantum kernel methods and quantum neural networks, along with features like customizable data encoding strategies, automated execution handling, and specialized kernel regularization techniques. By focusing on NISQ-compatibility and end-to-end automation, sQUlearn aims to bridge the gap between current quantum computing capabilities and practical machine learning applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Multimodal-Dataset-of-21-412-Recorded-Nights-for-Sleep-and-Respiratory-Research"><a href="#A-Multimodal-Dataset-of-21-412-Recorded-Nights-for-Sleep-and-Respiratory-Research" class="headerlink" title="A Multimodal Dataset of 21,412 Recorded Nights for Sleep and Respiratory Research"></a>A Multimodal Dataset of 21,412 Recorded Nights for Sleep and Respiratory Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08979">http://arxiv.org/abs/2311.08979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Diament, Maria Gorodetski, Adam Jankelow, Ayya Keshet, Tal Shor, Daphna Weissglas-Volkov, Hagai Rossman, Eran Segal</li>
<li>for: 这个研究旨在提供一个全新、丰富的家庭呼吸暂停测试数据集，使用FDA批准的WatchPAT-300设备，从7,077名参与者和21,412个夜晚中收集到的数据。</li>
<li>methods: 这个数据集包括三级别的睡眠数据：原始多通道时间序列数据、注释的睡眠事件和计算的摘要统计，其中包括447个与睡眠建筑、呼吸暂停和心率变化相关的特征。</li>
<li>results: 研究人员提供了不同年龄和性别的AHI、睡眠效率、WASO和HRV样本熵的参考值，并证明了该数据集可以提高对各种健康相关特征的预测能力，包括体重组成、骨骼密度、血糖水平和呼吸系统健康。这些结果表明该数据集具有提高睡眠研究、个性化医疗和生物医学机器学习应用的潜力。<details>
<summary>Abstract</summary>
This study introduces a novel, rich dataset obtained from home sleep apnea tests using the FDA-approved WatchPAT-300 device, collected from 7,077 participants over 21,412 nights. The dataset comprises three levels of sleep data: raw multi-channel time-series from sensors, annotated sleep events, and computed summary statistics, which include 447 features related to sleep architecture, sleep apnea, and heart rate variability (HRV). We present reference values for Apnea/Hypopnea Index (AHI), sleep efficiency, Wake After Sleep Onset (WASO), and HRV sample entropy, stratified by age and sex. Moreover, we demonstrate that the dataset improves the predictive capability for various health related traits, including body composition, bone density, blood sugar levels and cardiovascular health. These results illustrate the dataset's potential to advance sleep research, personalized healthcare, and machine learning applications in biomedicine.
</details>
<details>
<summary>摘要</summary>
Note: "Simplified Chinese" is also known as "Mandarin Chinese" or "Standard Chinese". The translation is written in Traditional Chinese characters, which are used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Probability-of-Collision-of-satellites-and-space-debris-for-short-term-encounters-Rederivation-and-fast-to-compute-upper-and-lower-bounds"><a href="#Probability-of-Collision-of-satellites-and-space-debris-for-short-term-encounters-Rederivation-and-fast-to-compute-upper-and-lower-bounds" class="headerlink" title="Probability of Collision of satellites and space debris for short-term encounters: Rederivation and fast-to-compute upper and lower bounds"></a>Probability of Collision of satellites and space debris for short-term encounters: Rederivation and fast-to-compute upper and lower bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08978">http://arxiv.org/abs/2311.08978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Ferreira, Cláudia Soares, Marta Guimarães</li>
<li>for: 预测卫星和遥感器在低地球轨道（LEO）中的碰撞风险，以满足空间业务的需求。</li>
<li>methods: 基于初始原理的新 derivation，可以自然地提供紧凑且快速的上限和下限 bounds for the probability of collision。</li>
<li>results: 比较traditional方法，本研究的实现可以减少计算概率的时间，从80%减少到几乎实时。<details>
<summary>Abstract</summary>
The proliferation of space debris in LEO has become a major concern for the space industry. With the growing interest in space exploration, the prediction of potential collisions between objects in orbit has become a crucial issue. It is estimated that, in orbit, there are millions of fragments a few millimeters in size and thousands of inoperative satellites and discarded rocket stages. Given the high speeds that these fragments can reach, even fragments a few millimeters in size can cause fractures in a satellite's hull or put a serious crack in the window of a space shuttle. The conventional method proposed by Akella and Alfriend in 2000 remains widely used to estimate the probability of collision in short-term encounters. Given the small period of time, it is assumed that, during the encounter: (1) trajectories are represented by straight lines with constant velocity; (2) there is no velocity uncertainty and the position exhibits a stationary distribution throughout the encounter; and (3) position uncertainties are independent and represented by Gaussian distributions. This study introduces a novel derivation based on first principles that naturally allows for tight and fast upper and lower bounds for the probability of collision. We tested implementations of both probability and bound computations with the original and our formulation on a real CDM dataset used in ESA's Collision Avoidance Challenge. Our approach reduces the calculation of the probability to two one-dimensional integrals and has the potential to significantly reduce the processing time compared to the traditional method, from 80% to nearly real-time.
</details>
<details>
<summary>摘要</summary>
space垃圾在低地球轨道（LEO）的扩散已成为航天业的主要问题。随着航天探索的兴趣增加，预测可能的轨道碰撞的问题变得越来越重要。据估计，在轨道上有数百万个几毫米大小的碎片和数千个不工作的卫星和抛弃的火箭阶段。由于这些碎片的高速度，же�eso even small fragments can cause cracks in a satellite's hull or serious damage to the window of a space shuttle.根据阿克拉和Alfriend在2000年提出的传统方法，仍然广泛使用来估算碰撞的可能性。由于短期内的时间非常短暂，这种方法假设：（1）轨道可以用直线 repre sented by constant velocity;（2） velocity uncertainty is zero, and the position exhibits a stationary distribution throughout the encounter; and（3） position uncertainties are independent and represented by Gaussian distributions.本研究提出了一种基于第一原理的新 derivation，自然地允许 для紧凑和快速的上下限计算。我们对原始和我们的表述进行了实现，并在ESA的Collision Avoidance Challenge中使用了一个真实的CDM数据集进行测试。我们的方法将计算概率减少到两个一dimensional integral，并且有可能减少计算时间相比传统方法，从80%降低到实时。
</details></li>
</ul>
<hr>
<h2 id="A-Single-Loop-Algorithm-for-Decentralized-Bilevel-Optimization"><a href="#A-Single-Loop-Algorithm-for-Decentralized-Bilevel-Optimization" class="headerlink" title="A Single-Loop Algorithm for Decentralized Bilevel Optimization"></a>A Single-Loop Algorithm for Decentralized Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08945">http://arxiv.org/abs/2311.08945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youran Dong, Shiqian Ma, Junfeng Yang, Chao Yin</li>
<li>for: 这篇论文关注了分布式机器学习中的二级优化问题。</li>
<li>methods: 我们提出了一种基于单loop的二级优化算法，用于解决分布式二级优化问题，并不需要大量的矩阵-向量乘法。此外，我们的算法不需要任何 gradient 不同假设。</li>
<li>results: 我们的分析表明，我们的算法可以达到最佳已知的优化率。<details>
<summary>Abstract</summary>
Bilevel optimization has received more and more attention recently due to its wide applications in machine learning. In this paper, we consider bilevel optimization in decentralized networks. In particular, we propose a novel single-loop algorithm for solving decentralized bilevel optimization with strongly convex lower level problem. Our algorithm is fully single-loop and does not require heavy matrix-vector multiplications when approximating the hypergradient. Moreover, unlike existing methods for decentralized bilevel optimization and federated bilevel optimization, our algorithm does not require any gradient heterogeneity assumption. Our analysis shows that the proposed algorithm achieves the best known convergence rate for bilevel optimization algorithms.
</details>
<details>
<summary>摘要</summary>
“双层优化在机器学习领域已经收到了更多的关注，特别是在分布式网络中。在这篇论文中，我们考虑了分布式双层优化。我们提出了一种新的单循环算法，用于解决分布式双层优化中的强某下层问题。我们的算法不需要大量的矩阵-向量乘法，而且不需要任何梯度异质假设。我们的分析表明，我们的算法可以达到最佳known的收敛率。”Here's the translation of each sentence:1. “双层优化” (bilevel optimization)2. “在机器学习领域已经收到了更多的关注” (has received more and more attention in the machine learning field)3. “特别是在分布式网络中” (especially in decentralized networks)4. “在这篇论文中，我们考虑了分布式双层优化” (In this paper, we consider decentralized bilevel optimization)5. “我们提出了一种新的单循环算法” (We propose a new single-loop algorithm)6. “用于解决分布式双层优化中的强某下层问题” (to solve the strongly convex lower-level problem in decentralized bilevel optimization)7. “我们的算法不需要大量的矩阵-向量乘法” (Our algorithm does not require heavy matrix-vector multiplications)8. “而且不需要任何梯度异质假设” (and does not require any gradient heterogeneity assumption)9. “我们的分析表明，我们的算法可以达到最佳known的收敛率” (Our analysis shows that our algorithm achieves the best known convergence rate)
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Escaping-Saddle-Points-for-Non-Convex-Policy-Optimization"><a href="#Efficiently-Escaping-Saddle-Points-for-Non-Convex-Policy-Optimization" class="headerlink" title="Efficiently Escaping Saddle Points for Non-Convex Policy Optimization"></a>Efficiently Escaping Saddle Points for Non-Convex Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08914">http://arxiv.org/abs/2311.08914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadegh Khorasani, Saber Salehkaleybar, Negar Kiyavash, Niao He, Matthias Grossglauser</li>
<li>for: 本研究旨在提出一种基于积分 gradient 的减少偏差的 reinforcement learning 方法，可以在不同的偏差下采取优化策略，并且可以在不同的 Random Seed 下实现更好的性能。</li>
<li>methods: 本研究使用了积分 gradient 方法，并利用了第二阶导数信息以实现更好的减少偏差。具体来说，本研究使用了 Hessian vector products (HVP) 来提高方法的精度和可靠性。</li>
<li>results: 实验结果表明，提出的方法可以在不同的 Random Seed 下实现更好的性能，并且比现有的方法具有更好的减少偏差和更高的可靠性。<details>
<summary>Abstract</summary>
Policy gradient (PG) is widely used in reinforcement learning due to its scalability and good performance. In recent years, several variance-reduced PG methods have been proposed with a theoretical guarantee of converging to an approximate first-order stationary point (FOSP) with the sample complexity of $O(\epsilon^{-3})$. However, FOSPs could be bad local optima or saddle points. Moreover, these algorithms often use importance sampling (IS) weights which could impair the statistical effectiveness of variance reduction. In this paper, we propose a variance-reduced second-order method that uses second-order information in the form of Hessian vector products (HVP) and converges to an approximate second-order stationary point (SOSP) with sample complexity of $\tilde{O}(\epsilon^{-3})$. This rate improves the best-known sample complexity for achieving approximate SOSPs by a factor of $O(\epsilon^{-0.5})$. Moreover, the proposed variance reduction technique bypasses IS weights by using HVP terms. Our experimental results show that the proposed algorithm outperforms the state of the art and is more robust to changes in random seeds.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出一种减少差异第二阶方法，使用第二阶信息的形式为Hessian vector products（HVP），并在$\mathcal{O}(\epsilon^{-3})$批量中达到一个相似的第二阶稳定点（SOSP）。这个率提高了目前最佳样本复杂度以实现相似SOSP的比例，即$O(\epsilon^{-0.5})$。此外，我们的减少差异技术可以 circumvent IS权重，使用HVP项。我们的实验结果表明，我们的算法超过了当前的状态，并在Random seed的变化下更加稳定。
</details></li>
</ul>
<hr>
<h2 id="On-the-Importance-of-Step-wise-Embeddings-for-Heterogeneous-Clinical-Time-Series"><a href="#On-the-Importance-of-Step-wise-Embeddings-for-Heterogeneous-Clinical-Time-Series" class="headerlink" title="On the Importance of Step-wise Embeddings for Heterogeneous Clinical Time-Series"></a>On the Importance of Step-wise Embeddings for Heterogeneous Clinical Time-Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08902">http://arxiv.org/abs/2311.08902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ratschlab/clinical-embeddings">https://github.com/ratschlab/clinical-embeddings</a></li>
<li>paper_authors: Rita Kuznetsova, Alizée Pace, Manuel Burger, Hugo Yèche, Gunnar Rätsch</li>
<li>for: 本研究旨在探讨深度学习Architecture在医疗电子记录时序数据模型中的应用，特别是在医学急诊部(ICU)中的问题上。</li>
<li>methods: 本研究使用了最新的深度学习方法，包括表格数据的深度学习方法，以提高时序数据模型的性能。</li>
<li>results: 研究发现，通过joint使用表格数据的深度学习方法，可以提高临床时序模型的性能，并且发现了step-wise embedding的重要性，以及在医疗数据中feature grouping的重要性。<details>
<summary>Abstract</summary>
Recent advances in deep learning architectures for sequence modeling have not fully transferred to tasks handling time-series from electronic health records. In particular, in problems related to the Intensive Care Unit (ICU), the state-of-the-art remains to tackle sequence classification in a tabular manner with tree-based methods. Recent findings in deep learning for tabular data are now surpassing these classical methods by better handling the severe heterogeneity of data input features. Given the similar level of feature heterogeneity exhibited by ICU time-series and motivated by these findings, we explore these novel methods' impact on clinical sequence modeling tasks. By jointly using such advances in deep learning for tabular data, our primary objective is to underscore the importance of step-wise embeddings in time-series modeling, which remain unexplored in machine learning methods for clinical data. On a variety of clinically relevant tasks from two large-scale ICU datasets, MIMIC-III and HiRID, our work provides an exhaustive analysis of state-of-the-art methods for tabular time-series as time-step embedding models, showing overall performance improvement. In particular, we evidence the importance of feature grouping in clinical time-series, with significant performance gains when considering features within predefined semantic groups in the step-wise embedding module.
</details>
<details>
<summary>摘要</summary>
最近各种深度学习架构在序列模型方面的进步未能完全转移到电子医疗记录处理的时间序列问题上。特别是在医疗关键室（ICU）中的问题上，现状的方法仍然是通过树状方法进行时间序列分类。现有的深度学习方法对表格数据进行了更好的处理，已经超越了传统的方法。受到ICU时间序列数据的相似水平特征随机性的激发，我们探索这些新方法在临床时间序列模型任务中的影响。通过结合深度学习对表格数据的进步，我们的主要目标是强调在时间序列模型中的步骤Embedding的重要性，这在机器学习方法中对医疗数据没有被探讨。从两个大规模的ICU数据集MIMIC-III和HiRID中获得的许多临床相关任务上，我们的工作提供了对现状方法的完整分析，并证明了在不同的时间步骤Embedding模型中，特别是在Semantic group内的特征分组中，可以获得显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Towards-Label-Embedding-–-Measuring-classification-difficulty"><a href="#Towards-Label-Embedding-–-Measuring-classification-difficulty" class="headerlink" title="Towards Label Embedding – Measuring classification difficulty"></a>Towards Label Embedding – Measuring classification difficulty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08874">http://arxiv.org/abs/2311.08874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Hechinger, Christoph Koller, Xiao Xiang Zhu, Göran Kauermann</li>
<li>for: 本研究旨在 Addressing the problem of uncertainty in supervised learning, particularly in the labeling step of satellite image classification.</li>
<li>methods: 本研究使用了 Dirichlet-Multinomial 模型和随机渐近 maximum likelihood estimation 算法，将多个票数据 embedding 到 K-维空间中，其中 K 是可能的类别数量。</li>
<li>results: 研究发现，使用这种方法可以获得高质量的 embedding，并且可以 investigate 这些 embedding 的相关矩阵，它们可以看作是通用的混淆矩阵，具有良好的 semantic 相似性。这些结论可以作为通用的标签嵌入，在单个真实标签不可 garantizado的情况下提供有价值的信息。<details>
<summary>Abstract</summary>
Uncertainty quantification in machine learning is a timely and vast field of research. In supervised learning, uncertainty can already occur in the very first stage of the training process, the labelling step. In particular, this is the case when not every instance can be unambiguously classified. The problem occurs for classifying instances, where classes may overlap or instances can not be clearly categorised. In other words, there is inevitable ambiguity in the annotation step and not necessarily a 'ground truth'. We look exemplary at the classification of satellite images. Each image is annotated independently by multiple labellers and classified into local climate zones (LCZs). For each instance we have multiple votes, leading to a distribution of labels rather than a single value. The main idea of this work is that we do not assume a ground truth label but embed the votes into a K-dimensional space, with K as the number of possible categories. The embedding is derived from the voting distribution in a Bayesian setup, modelled via a Dirichlet-Multinomial model. We estimate the model and posteriors using a stochastic Expectation Maximisation algorithm with Markov Chain Monte Carlo steps. While we focus on the particular example of LCZ classification, the methods developed in this paper readily extend to other situations where multiple annotators independently label texts or images. We also apply our approach to two other benchmark datasets for image classification to demonstrate this. Besides the embeddings themselves, we can investigate the resulting correlation matrices, which can be seen as generalised confusion matrices and reflect the semantic similarities of the original classes very well for all three exemplary datasets. The insights gained are valuable and can serve as general label embedding if a single ground truth per observation cannot be guaranteed.
</details>
<details>
<summary>摘要</summary>
机器学习中的不确定性评估是一个时髦的和广泛的研究领域。在指导学习中，不确定性可以在训练过程的第一个阶段出现，即标注阶段。特别是在分类实例时，当类别 overlap 或实例无法明确分类时，标注过程中会具有不可避免的模糊性。我们通过卫星图像的分类为例，每个图像都由多名标注者独立地标注并分类到本地气候区（LCZ）。对每个实例而言，我们有多个选择，导致一个分布而不是单个值。我们的主要想法是不假设固定的真实标签，而是将投票embedded到K维空间中（K为可能的类别数）。这个空间的定义来自于投票分布，通过 Dirichlet-Multinomial 模型来描述。我们使用Markov Chain Monte Carlo步骤来估算模型和 posterior，并使用随机推估最大化算法。我们在这篇论文中专注于 LCZ 分类的情况，但方法可以轻松扩展到其他独立标注者标注的文本或图像。此外，我们还应用了这些方法到两个其他图像分类 benchmark 数据集，以证明其通用性。除了嵌入本身之外，我们还可以研究得到的相关性矩阵，它们可以看作是通用的混淆矩阵，很好地反映原始类别之间的语义相似性。这些发现可以作为一般的标签嵌入，当单个真实标签无法保证时。
</details></li>
</ul>
<hr>
<h2 id="Statistical-learning-by-sparse-deep-neural-networks"><a href="#Statistical-learning-by-sparse-deep-neural-networks" class="headerlink" title="Statistical learning by sparse deep neural networks"></a>Statistical learning by sparse deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08845">http://arxiv.org/abs/2311.08845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Felix Abramovich</li>
<li>for: 这个论文是为了研究一种基于实际风险最小化的深度神经网络估计器，并使用l_1正则化。</li>
<li>methods: 论文使用了一种通用的误差 bound，来研究这种估计器的过剩风险。同时，它也证明了这种估计器在不同的函数集中是可适应的，并且可以在某些情况下达到可适应的最小风险。</li>
<li>results: 论文的研究表明，这种估计器在回归和分类任务中具有可适应的最小风险性，并且在不同的函数集中具有相似的性能。<details>
<summary>Abstract</summary>
We consider a deep neural network estimator based on empirical risk minimization with l_1-regularization. We derive a general bound for its excess risk in regression and classification (including multiclass), and prove that it is adaptively nearly-minimax (up to log-factors) simultaneously across the entire range of various function classes.
</details>
<details>
<summary>摘要</summary>
我们考虑了一种深度神经网络估计器，基于empirical risk minimization，并添加了L1正则化。我们得出了一个通用的副作用误差上限，并证明了它在不同的函数类中同时具有适应性nearly-minimax性（即log-factors）。Here's the breakdown of the translation:* "deep neural network" is 深度神经网络 (shēn dào xīn nǐo wǎng)* "estimator" is 估计器 (gè jì qì)* "based on empirical risk minimization" is 基于empirical risk minimization (jī yǔ empirical risk minimization)* "with L1 regularization" is 并添加L1正则化 (dàn tiêm L1 zhèng yǎ)* "we derive a general bound" is 我们得出了一个通用的副作用误差上限 (wǒ men dé chū yī yī zhòng yòu zhèng yǎ)* "for its excess risk" is 对其副作用误差 (duì qí fù zuò yì zhèng yǎ)* "in regression and classification" is 包括回归和分类 (bāo xīn huí qù yǔ fāng lèi)* "including multiclass" is 包括多类 (bāo xīn duō lèi)* "and prove that it is adaptively nearly-minimax" is 并证明它同时在不同的函数类中具有适应性nearly-minimax性（即log-factors） (dàn shì yī yī zhòng yòu zhèng yǎ)Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I'll be happy to provide it.
</details></li>
</ul>
<hr>
<h2 id="Environment-independent-mmWave-Fall-Detection-with-Interacting-Multiple-Model"><a href="#Environment-independent-mmWave-Fall-Detection-with-Interacting-Multiple-Model" class="headerlink" title="Environment-independent mmWave Fall Detection with Interacting Multiple Model"></a>Environment-independent mmWave Fall Detection with Interacting Multiple Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08755">http://arxiv.org/abs/2311.08755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuyao Yu, Jiazhao Wang, Wenchao Jiang</li>
<li>for: 这个研究旨在开发一个可靠、抗错误的无接触、无合作的跌倒探测系统，以便在智能家居中实现长者日常照顾。</li>
<li>methods: 本研究使用mmWave激光技术，并采用一个叫做互动多模型（IMM）状态估计器，从环境无关的特征提取出高精度、实时跌倒探测。此外，我们也提出了一个可靠的多用户追踪系统，以应对环境噪音和其他人体影响。</li>
<li>results: 实验结果显示，FADE系统在实际应用中可以实现跌倒探测的精度高达95%。<details>
<summary>Abstract</summary>
The ageing society brings attention to daily elderly care through sensing technologies. The future smart home is expected to enable in-home daily monitoring, such as fall detection, for seniors in a non-invasive, non-cooperative, and non-contact manner. The mmWave radar is a promising candidate technology for its privacy-preserving and non-contact manner. However, existing solutions suffer from low accuracy and robustness due to environment dependent features. In this paper, we present FADE (\underline{FA}ll \underline{DE}tection), a practical fall detection radar system with enhanced accuracy and robustness in real-world scenarios. The key enabler underlying FADE is an interacting multiple model (IMM) state estimator that can extract environment-independent features for highly accurate and instantaneous fall detection. Furthermore, we proposed a robust multiple-user tracking system to deal with noises from the environment and other human bodies. We deployed our algorithm on low computing power and low power consumption system-on-chip (SoC) composed of data front end, DSP, and ARM processor, and tested its performance in real-world. The experiment shows that the accuracy of fall detection is up to 95\%.
</details>
<details>
<summary>摘要</summary>
社会老龄化引起每日老人照顾的注意。未来智能家庭预计会实现在家中每日监测，如坠落检测，为年轻人在不侵略、不合作、无接触的情况下提供。 millimeter 雷达是一种有前途的技术，因为它具有隐私保护和无接触的特点。然而，现有解决方案受到环境依赖的特征的准确性和可靠性问题。在本文中，我们提出了FADE（坠落检测），一个实用的坠落检测雷达系统，具有提高了准确性和可靠性的实际应用能力。FADE的关键启发是一种交互式多模型（IMM）状态估计器，可以提取环境无关的特征，以实现高精度和实时坠落检测。此外，我们还提出了一种可靠的多用户跟踪系统，用于处理环境和其他人体的噪音。我们将算法部署到低计算力和低能耗系统在板（SoC）上，包括数据前端、DSP和ARM处理器，并在实际应用中进行测试。实验结果显示，坠落检测精度可达95%。
</details></li>
</ul>
<hr>
<h2 id="Using-Stochastic-Gradient-Descent-to-Smooth-Nonconvex-Functions-Analysis-of-Implicit-Graduated-Optimization-with-Optimal-Noise-Scheduling"><a href="#Using-Stochastic-Gradient-Descent-to-Smooth-Nonconvex-Functions-Analysis-of-Implicit-Graduated-Optimization-with-Optimal-Noise-Scheduling" class="headerlink" title="Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling"></a>Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08745">http://arxiv.org/abs/2311.08745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Sato, Hideaki Iiduka</li>
<li>for: 这个论文是为了解释graduated optimization方法在非 convex函数中的 globally optimal solution 的找到和分析。</li>
<li>methods: 这篇论文使用了一种新的非 convex函数家族，并对这些函数的充分条件进行了讨论。它还提供了一种 graduated optimization 算法的收敛分析。</li>
<li>results: 这篇论文显示了 stochastic gradient descent (SGD)  WITH mini-batch stochastic gradients 可以使函数变得更加平滑，这种效果取决于学习率和 batch size。这个发现为 graduated optimization 的视角提供了理论性的解释，例如大批处理大小会落入锋利的本地极小值，以及 decaying learning rate 和增大批处理大小是有优势的。此外，这篇论文还提出了一种新的 graduated optimization 框架，并通过实验来支持我们的理论发现。<details>
<summary>Abstract</summary>
The graduated optimization approach is a heuristic method for finding globally optimal solutions for nonconvex functions and has been theoretically analyzed in several studies. This paper defines a new family of nonconvex functions for graduated optimization, discusses their sufficient conditions, and provides a convergence analysis of the graduated optimization algorithm for them. It shows that stochastic gradient descent (SGD) with mini-batch stochastic gradients has the effect of smoothing the function, the degree of which is determined by the learning rate and batch size. This finding provides theoretical insights from a graduated optimization perspective on why large batch sizes fall into sharp local minima, why decaying learning rates and increasing batch sizes are superior to fixed learning rates and batch sizes, and what the optimal learning rate scheduling is. To the best of our knowledge, this is the first paper to provide a theoretical explanation for these aspects. Moreover, a new graduated optimization framework that uses a decaying learning rate and increasing batch size is analyzed and experimental results of image classification that support our theoretical findings are reported.
</details>
<details>
<summary>摘要</summary>
“graduated optimization方法是一种启发法求解非对称函数的全球最优解决方案，在多个研究中得到了理论分析。本文定义了一个新的非对称函数家族，讨论了它们的必要条件，并对 graduate optimization算法的收敛分析。结果显示，使用渐进学习率和增加批处理大小可以使函数平滑化，这种效果取决于学习率和批处理大小。这一发现为 graduated optimization视角下解释了大批处理大小落入锐锐的局部最优点、渐进学习率和增加批处理大小的优势，以及最优学习率调整的问题。据我们所知，这是首个提供了这些方面的理论解释。此外，我们还提出了一种使用渐进学习率和增加批处理大小的新 graduated optimization框架，并在图像分类任务上进行了实验研究，支持我们的理论发现。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Graph-Aware-Diffusion-Modeling-for-Collaborative-Filtering"><a href="#Towards-Graph-Aware-Diffusion-Modeling-for-Collaborative-Filtering" class="headerlink" title="Towards Graph-Aware Diffusion Modeling for Collaborative Filtering"></a>Towards Graph-Aware Diffusion Modeling for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08744">http://arxiv.org/abs/2311.08744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunqin Zhu, Chao Wang, Hui Xiong</li>
<li>for: 这篇论文是为了推荐系统中的隐藏偏好还原问题而写的。</li>
<li>methods: 该论文使用了神经网络模型来解决这个问题，特别是使用了Diffusion模型来实现。</li>
<li>results: 该论文的实验结果表明，Compared to现有方法，该模型在一个数据集上表现出了大量的提高，并在其他数据集上表现得也很竞争力强。<details>
<summary>Abstract</summary>
Recovering masked feedback with neural models is a popular paradigm in recommender systems. Seeing the success of diffusion models in solving ill-posed inverse problems, we introduce a conditional diffusion framework for collaborative filtering that iteratively reconstructs a user's hidden preferences guided by its historical interactions. To better align with the intrinsic characteristics of implicit feedback data, we implement forward diffusion by applying synthetic smoothing filters to interaction signals on an item-item graph. The resulting reverse diffusion can be interpreted as a personalized process that gradually refines preference scores. Through graph Fourier transform, we equivalently characterize this model as an anisotropic Gaussian diffusion in the graph spectral domain, establishing both forward and reverse formulations. Our model outperforms state-of-the-art methods by a large margin on one dataset and yields competitive results on the others.
</details>
<details>
<summary>摘要</summary>
“复原对应式反馈”是现代推荐系统中广泛使用的一种方法。发现传播模型在解决过滤问题方面的成功，我们引入一个基于协同游戏的条件传播框架，iteratively重建用户的隐藏偏好 guid by its历史互动。为了更好地适应隐藏反馈数据的自然特性，我们实现前向传播 by applying synthetic smoothing filters to interaction signals on an item-item graph。 resulting reverse diffusion can be interpreted as a personalized process that gradually refines preference scores。通过几何transform， we equivalently characterize this model as an anisotropic Gaussian diffusion in the graph spectral domain, establishing both forward and reverse formulations。我们的模型在一个数据集上比州-of-the-art方法大幅超越，并在其他数据集上实现竞争性的结果。
</details></li>
</ul>
<hr>
<h2 id="Enabling-CMF-Estimation-in-Data-Constrained-Scenarios-A-Semantic-Encoding-Knowledge-Mining-Model"><a href="#Enabling-CMF-Estimation-in-Data-Constrained-Scenarios-A-Semantic-Encoding-Knowledge-Mining-Model" class="headerlink" title="Enabling CMF Estimation in Data-Constrained Scenarios: A Semantic-Encoding Knowledge Mining Model"></a>Enabling CMF Estimation in Data-Constrained Scenarios: A Semantic-Encoding Knowledge Mining Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08690">http://arxiv.org/abs/2311.08690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanlin Qi, Jia Li, Michael Zhang<br>for: 这种研究的目的是提供一种可靠且可行的方法来估算安全措施 modify 因素（CMF），以评估不同安全措施的效果并决定相应的基础设施投资。methods: 这种研究使用了一种新的知识挖掘框架来预测 CMF 值。这个框架 Draws inspiration from human comprehension processes 和 introduces advanced Natural Language Processing (NLP) techniques to extract intricate variations and patterns from existing CMF knowledge.results: 实验 validate 表明，这种新方法可以减少 CMF 估算的成本和时间，同时提供更好的准确性。这种方法可以补充传统的案例特定的方法，特别是当缺乏灾难数据或时间限制时。<details>
<summary>Abstract</summary>
Precise estimation of Crash Modification Factors (CMFs) is central to evaluating the effectiveness of various road safety treatments and prioritizing infrastructure investment accordingly. While customized study for each countermeasure scenario is desired, the conventional CMF estimation approaches rely heavily on the availability of crash data at given sites. This not only makes the estimation costly, but the results are also less transferable, since the intrinsic similarities between different safety countermeasure scenarios are not fully explored. Aiming to fill this gap, this study introduces a novel knowledge-mining framework for CMF prediction. This framework delves into the connections of existing countermeasures and reduces the reliance of CMF estimation on crash data availability and manual data collection. Specifically, it draws inspiration from human comprehension processes and introduces advanced Natural Language Processing (NLP) techniques to extract intricate variations and patterns from existing CMF knowledge. It effectively encodes unstructured countermeasure scenarios into machine-readable representations and models the complex relationships between scenarios and CMF values. This new data-driven framework provides a cost-effective and adaptable solution that complements the case-specific approaches for CMF estimation, which is particularly beneficial when availability of crash data or time imposes constraints. Experimental validation using real-world CMF Clearinghouse data demonstrates the effectiveness of this new approach, which shows significant accuracy improvements compared to baseline methods. This approach provides insights into new possibilities of harnessing accumulated transportation knowledge in various applications.
</details>
<details>
<summary>摘要</summary>
evaluating 安全防护措施的效iveness 需要精准的评估飞行改变因素（CMF）。 conventional 方法取决于具体的countermeasure scenario 的可用性，这不仅使得评估成本高昂，而且结果还 less transferable，因为不完全考虑不同安全防护措施enario 之间的内在相似性。 为了填补这一空白，本研究提出了一种新的知识挖掘框架，用于预测CMF。 该框架 Draws inspiration from human comprehension processes 和 advanced Natural Language Processing（NLP）技术，以EXTRACT intricate variations and patterns 从现有CMF知识中。 它可以将countermeasure scenarios 转化为可读取的机器表示，并模型了countermeasure scenarios 和 CMF值之间的复杂关系。 这种新的数据驱动的框架可以补做case-specific 方法的限制，特别是当缺乏事故数据或时间约束时。 实验 validate 使用实际的CMF Clearinghouse数据，demonstrates 该新方法的有效性，与基准方法相比显示了显著的准确性提升。 这种新方法提供了新的应用 possiblities ，挖掘了交通运输领域的储存知识。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Sparse-Principal-Component-Analysis"><a href="#Federated-Learning-for-Sparse-Principal-Component-Analysis" class="headerlink" title="Federated Learning for Sparse Principal Component Analysis"></a>Federated Learning for Sparse Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08677">http://arxiv.org/abs/2311.08677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sin Cheng Ciou, Pin Jui Chen, Elvin Y. Tseng, Yuh-Jye Lee</li>
<li>For: The paper is written for the rapidly evolving realm of machine learning, specifically addressing the challenge of algorithm effectiveness facing limitations due to data quality and availability.* Methods: The paper uses the federated learning framework, which is a decentralized approach where model training occurs on client sides, preserving privacy by keeping data localized. The paper also applies the Sparse Principal Component Analysis (SPCA) method, which aims to attain sparse component loadings while maximizing data variance for improved interpretability. Additionally, the paper introduces a least squares approximation to original SPCA, which enables analytic solutions on the optimization processes and leads to substantial computational improvements.* Results: The paper’s extensive experiments involve both IID and non-IID random features across various data owners, and results on synthetic and public datasets affirm the efficacy of the federated SPCA approach.<details>
<summary>Abstract</summary>
In the rapidly evolving realm of machine learning, algorithm effectiveness often faces limitations due to data quality and availability. Traditional approaches grapple with data sharing due to legal and privacy concerns. The federated learning framework addresses this challenge. Federated learning is a decentralized approach where model training occurs on client sides, preserving privacy by keeping data localized. Instead of sending raw data to a central server, only model updates are exchanged, enhancing data security. We apply this framework to Sparse Principal Component Analysis (SPCA) in this work. SPCA aims to attain sparse component loadings while maximizing data variance for improved interpretability. Beside the L1 norm regularization term in conventional SPCA, we add a smoothing function to facilitate gradient-based optimization methods. Moreover, in order to improve computational efficiency, we introduce a least squares approximation to original SPCA. This enables analytic solutions on the optimization processes, leading to substantial computational improvements. Within the federated framework, we formulate SPCA as a consensus optimization problem, which can be solved using the Alternating Direction Method of Multipliers (ADMM). Our extensive experiments involve both IID and non-IID random features across various data owners. Results on synthetic and public datasets affirm the efficacy of our federated SPCA approach.
</details>
<details>
<summary>摘要</summary>
在机器学习领域中，算法效果经常受到数据质量和可用性的限制。传统方法面临数据分享的挑战，由于法律和隐私问题。基于联邦学习框架的我们方法解决这个挑战。联邦学习是一种分布式的方法，在客户端上进行模型训练，保持隐私性，不发送原始数据到中央服务器。而是只发送模型更新，提高数据安全性。在这种框架下，我们应用SPCA（稀疏主成分分析）。SPCA的目标是获得稀疏的组件加载，同时最大化数据变化，以提高解释性。在传统的SPCA中，我们采用L1规范化项来进行正则化，同时添加滑动函数，以便使用梯度下降优化方法。此外，为了提高计算效率，我们引入了原始SPCA的最小二乘近似。这使得优化过程中的数学解可以得到 analytic 解，导致计算上的重要提高。在联邦框架中，我们将SPCA表示为一个consensus优化问题，可以使用alternating Direction Method of Multipliers（ADMM）解决。我们的广泛的实验包括了独立和非独立的随机特征 Across various data owners。结果表明我们的联邦SPCA方法是有效的。
</details></li>
</ul>
<hr>
<h2 id="Coreset-Selection-with-Prioritized-Multiple-Objectives"><a href="#Coreset-Selection-with-Prioritized-Multiple-Objectives" class="headerlink" title="Coreset Selection with Prioritized Multiple Objectives"></a>Coreset Selection with Prioritized Multiple Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08675">http://arxiv.org/abs/2311.08675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Xia, Jiale Liu, Shaokun Zhang, Qingyun Wu, Tongliang Liu</li>
<li>for: 降低深度学习算法的计算成本和加速大规模数据处理</li>
<li>methods: 提出了一种新的多目标优先级 coreset 选择方法，以实现最小化 coreset 大小的同时保证模型性能</li>
<li>results: 理论上提供了优先级优化过程的 converges  garantue，并通过广泛的实验证明了该方法的超越性，可以更好地适应实际场景<details>
<summary>Abstract</summary>
Coreset selection is powerful in reducing computational costs and accelerating data processing for deep learning algorithms. It strives to identify a small subset from large-scale data, so that training only on the subset practically performs on par with full data. When coreset selection is applied in realistic scenes, under the premise that the identified coreset has achieved comparable model performance, practitioners regularly desire the identified coreset can have a size as small as possible for lower costs and greater acceleration. Motivated by this desideratum, for the first time, we pose the problem of "coreset selection with prioritized multiple objectives", in which the smallest coreset size under model performance constraints is explored. Moreover, to address this problem, an innovative method is proposed, which maintains optimization priority order over the model performance and coreset size, and efficiently optimizes them in the coreset selection procedure. Theoretically, we provide the convergence guarantee of the proposed method. Empirically, extensive experiments confirm its superiority compared with previous strategies, often yielding better model performance with smaller coreset sizes.
</details>
<details>
<summary>摘要</summary>
核心集选择是深度学习算法中具有强大的计算成本减少和数据处理加速能力的技术。它目标在大规模数据中标识一小subset，以便只需在subset上进行训练，实际上可以与全部数据进行相同的表现。在实际场景中，当 coreset 被应用时，实际Operator们经常希望可以通过最小化 coreset 大小来降低成本和加速计算。motivated by这个需求，我们首次提出了“coreset 选择 WITH 多目标优先级”的问题，在这个问题中，我们寻找最小化 coreset 大小的方法，同时保证模型性能。为了解决这个问题，我们提出了一种创新的方法，它可以在 coreset 选择过程中维护优先级顺序，同时兼顾模型性能和 coreset 大小。我们也提供了这种方法的理论上的收敛保证。实际实验表明，我们的方法在许多情况下比之前的策略更有优势，经常可以在更小的 coreset 大小下实现更好的模型性能。
</details></li>
</ul>
<hr>
<h2 id="Supervised-low-rank-semi-nonnegative-matrix-factorization-with-frequency-regularization-for-forecasting-spatio-temporal-data"><a href="#Supervised-low-rank-semi-nonnegative-matrix-factorization-with-frequency-regularization-for-forecasting-spatio-temporal-data" class="headerlink" title="Supervised low-rank semi-nonnegative matrix factorization with frequency regularization for forecasting spatio-temporal data"></a>Supervised low-rank semi-nonnegative matrix factorization with frequency regularization for forecasting spatio-temporal data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08636">http://arxiv.org/abs/2311.08636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keunsu Kim, Hanbaek Lyu, Jinsu Kim, Jae-Hun Jung</li>
<li>for: 这篇论文是用于预测空间时间资料的新方法论。</li>
<li>methods: 这篇论文使用了监督式半非正数式因子分解（SSNMF），并将频域变数增加到预测模型中，以提高时间特征的清晰度。</li>
<li>results: 在应用这篇论文的方法到 GRACE 资料上，发现结果与预测领域的前期研究相似，但提供更明确的解释。<details>
<summary>Abstract</summary>
We propose a novel methodology for forecasting spatio-temporal data using supervised semi-nonnegative matrix factorization (SSNMF) with frequency regularization. Matrix factorization is employed to decompose spatio-temporal data into spatial and temporal components. To improve clarity in the temporal patterns, we introduce a nonnegativity constraint on the time domain along with regularization in the frequency domain. Specifically, regularization in the frequency domain involves selecting features in the frequency space, making an interpretation in the frequency domain more convenient. We propose two methods in the frequency domain: soft and hard regularizations, and provide convergence guarantees to first-order stationary points of the corresponding constrained optimization problem. While our primary motivation stems from geophysical data analysis based on GRACE (Gravity Recovery and Climate Experiment) data, our methodology has the potential for wider application. Consequently, when applying our methodology to GRACE data, we find that the results with the proposed methodology are comparable to previous research in the field of geophysical sciences but offer clearer interpretability.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法ology，用于预测空间-时间数据，基于经过监督的半正semi-nonnegative矩阵因子（SSNMF），并在频率频谱中进行常数化。矩阵因子被用来分解空间-时间数据为空间和时间成分。为了提高时间特征的明暗性，我们在时间频谱中加入了非负约束，同时在频率频谱中进行常数化。我们提出了两种在频率频谱中的方法：软和硬的常数化，并提供了首次稳定点的准确性保证。我们的主要动机来自地球物理数据分析，基于GRACE（重力回升和气候实验）数据，但我们的方法可以更广泛地应用。当应用于GRACE数据时，我们发现，与先前在地球物理科学领域的研究相比，我们的方法可以提供更明了的解释。
</details></li>
</ul>
<hr>
<h2 id="Non-Uniform-Smoothness-for-Gradient-Descent"><a href="#Non-Uniform-Smoothness-for-Gradient-Descent" class="headerlink" title="Non-Uniform Smoothness for Gradient Descent"></a>Non-Uniform Smoothness for Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08615">http://arxiv.org/abs/2311.08615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lindonroberts/nonuniform-smoothness">https://github.com/lindonroberts/nonuniform-smoothness</a></li>
<li>paper_authors: Albert S. Berahas, Lindon Roberts, Fred Roosta</li>
<li>for: 这个论文的目的是提出一种基于本地首项稳定 oracle（LFSO）的修改后的梯度下降方法，以提高非强 convex 问题的 globally linear convergence rate。</li>
<li>methods: 这个论文使用了一种基于 LFSO 的修改后的梯度下降方法，该方法可以在任何两次导数函数上实现本地首项稳定性。</li>
<li>results: 论文显示了这种修改后的梯度下降方法可以在非强 convex 问题中实现全球线性减少率，并且可以超越通常的 (加速) 首项方法的下界。<details>
<summary>Abstract</summary>
The analysis of gradient descent-type methods typically relies on the Lipschitz continuity of the objective gradient. This generally requires an expensive hyperparameter tuning process to appropriately calibrate a stepsize for a given problem. In this work we introduce a local first-order smoothness oracle (LFSO) which generalizes the Lipschitz continuous gradients smoothness condition and is applicable to any twice-differentiable function. We show that this oracle can encode all relevant problem information for tuning stepsizes for a suitably modified gradient descent method and give global and local convergence results. We also show that LFSOs in this modified first-order method can yield global linear convergence rates for non-strongly convex problems with extremely flat minima, and thus improve over the lower bound on rates achievable by general (accelerated) first-order methods.
</details>
<details>
<summary>摘要</summary>
通常来说，梯度下降类方法的分析假设对目标函数的梯度 lipschitz连续性。这通常需要一个昂贵的hyperparameter调整过程，以适应给定问题。在这种工作中，我们引入了本地首项积分 oracle（LFSO），这个oracle泛化了 lipschitz连续梯度的稳定性条件，并适用于任何两次导数函数。我们表明，这个oracle可以包含所有相关的问题信息，用于调整梯度下降方法中的步长，并给出全球和本地收敛结果。此外，我们还证明，LFSOs在这种修改后的首项方法中可以实现全球线性收敛率，而且对非强转化问题的极小值处存在极其平坦的情况下，可以超越通用（加速）首项方法的下界。
</details></li>
</ul>
<hr>
<h2 id="Converting-Transformers-to-Polynomial-Form-for-Secure-Inference-Over-Homomorphic-Encryption"><a href="#Converting-Transformers-to-Polynomial-Form-for-Secure-Inference-Over-Homomorphic-Encryption" class="headerlink" title="Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption"></a>Converting Transformers to Polynomial Form for Secure Inference Over Homomorphic Encryption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08610">http://arxiv.org/abs/2311.08610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itamar Zimerman, Moran Baruch, Nir Drucker, Gilad Ezov, Omri Soceanu, Lior Wolf</li>
<li>for: This paper is written for the purpose of exploring the application of Homomorphic Encryption (HE) in transformer models, and demonstrating the feasibility of secure inference over HE with transformers.</li>
<li>methods: The paper introduces a novel method for converting operators in transformer models to their polynomial equivalent, enabling the use of HE in transformer models. This method involves tailoring the transformer architecture for HE and developing a novel conversion technique for operators.</li>
<li>results: The paper reports that the proposed polynomial transformer model achieves results comparable to traditional methods on several benchmark datasets, including LMs with WikiText-103, image classification with CIFAR-100 and Tiny-ImageNet. The results demonstrate the viability of HE for state-of-the-art applications and bridge the performance gap with transformers of similar scale.Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文是为了探索使用Homomorphic Encryption（HE）在 transformer 模型中进行安全推理，并实现 HE 上的 transformer 模型的可行性。</li>
<li>methods: 论文提出了一种新的方法，即将 transformer 模型中的运算符转换为其波动函数等价项。这种方法包括对 transformer 模型进行特定的设计和开发，以及一种新的转换技术。</li>
<li>results: 论文报告了一些比较示例，表明提出的波动 transformer 模型可以与传统方法相比，在多个 benchmark 数据集上实现类似的性能。这些结果证明 HE 可以在 state-of-the-art 应用中实现，并将 transformer 模型的性能减小到类似的规模。<details>
<summary>Abstract</summary>
Designing privacy-preserving deep learning models is a major challenge within the deep learning community. Homomorphic Encryption (HE) has emerged as one of the most promising approaches in this realm, enabling the decoupling of knowledge between the model owner and the data owner. Despite extensive research and application of this technology, primarily in convolutional neural networks, incorporating HE into transformer models has been challenging because of the difficulties in converting these models into a polynomial form. We break new ground by introducing the first polynomial transformer, providing the first demonstration of secure inference over HE with transformers. This includes a transformer architecture tailored for HE, alongside a novel method for converting operators to their polynomial equivalent. This innovation enables us to perform secure inference on LMs with WikiText-103. It also allows us to perform image classification with CIFAR-100 and Tiny-ImageNet. Our models yield results comparable to traditional methods, bridging the performance gap with transformers of similar scale and underscoring the viability of HE for state-of-the-art applications. Finally, we assess the stability of our models and conduct a series of ablations to quantify the contribution of each model component.
</details>
<details>
<summary>摘要</summary>
deep learning community 中的一个主要挑战是设计保持隐私的深度学习模型。归一化加密（HE）已经成为这一领域的一个非常有前途的方法，允许知识的解耦 между模型所有者和数据所有者。尽管有广泛的研究和应用这技术，主要在卷积神经网络中，但是将这些模型转化为多项式形式的困难使得HE在转换器模型中进行安全的推理变得困难。我们在这篇论文中突破了一个新的多项式转换器，提供了首次在HE下进行安全的推理的证明，包括一种适应HE的转换器架构，以及一种将操作转换为多项式形式的新方法。这一创新使得我们可以在LMs上进行安全的推理，并在CIFAR-100和Tiny-ImageNet上进行图像分类。我们的模型实现了与传统方法相当的结果，填补了转换器的性能差距，并证明了HE在现代应用中的可行性。最后，我们评估了我们的模型的稳定性，并进行了一系列的减少来评估每个模型组件的贡献。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.LG_2023_11_15/" data-id="clp53jwte00uayp887hhp6erm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/15/cs.CL_2023_11_15/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-11-15
        
      </div>
    </a>
  
  
    <a href="/2023/11/15/eess.IV_2023_11_15/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-11-15</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">125</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">62</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">76</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
