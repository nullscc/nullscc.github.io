
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-11-15 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09030 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yuanbo2020&#x2F;ai-soundscape paper_authors:">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-11-15">
<meta property="og:url" content="https://nullscc.github.io/2023/11/15/cs.SD_2023_11_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09030 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yuanbo2020&#x2F;ai-soundscape paper_authors:">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-15T15:00:00.000Z">
<meta property="article:modified_time" content="2023-11-19T06:27:35.039Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.SD_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T15:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-11-15
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AI-based-soundscape-analysis-Jointly-identifying-sound-sources-and-predicting-annoyance"><a href="#AI-based-soundscape-analysis-Jointly-identifying-sound-sources-and-predicting-annoyance" class="headerlink" title="AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance"></a>AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09030">http://arxiv.org/abs/2311.09030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/ai-soundscape">https://github.com/yuanbo2020/ai-soundscape</a></li>
<li>paper_authors: Yuanbo Hou, Qiaoqiao Ren, Huizhong Zhang, Andrew Mitchell, Francesco Aletta, Jian Kang, Dick Botteldooren</li>
<li>for: 本研究旨在开发一种基于人工智能的双分支卷积神经网络（DCNN-CaF），用于自动地描述声景环境，包括声音识别和评价。</li>
<li>methods: 本研究使用Delta数据集，其包含人类标注的声音来源标签和听到的不适程度。基于Delta数据集，提出了一种基于DCNN-CaF的声音源分类（SSC）和人类对声音不适程度评分（ARP）预测模型。</li>
<li>results: 研究结果表明，（1）使用听力和Mel特征时，DCNN-CaF模型的性能比使用单一特征时高；（2）DCNN-CaF模型与其他常见的人工智能模型和声景相关的传统机器学习方法相比，在SSC和ARP任务上表现出色；（3）对声音来源和不适程度之间的关系，人类和DCNN-CaF模型之间存在一定的相似性。<details>
<summary>Abstract</summary>
Soundscape studies typically attempt to capture the perception and understanding of sonic environments by surveying users. However, for long-term monitoring or assessing interventions, sound-signal-based approaches are required. To this end, most previous research focused on psycho-acoustic quantities or automatic sound recognition. Few attempts were made to include appraisal (e.g., in circumplex frameworks). This paper proposes an artificial intelligence (AI)-based dual-branch convolutional neural network with cross-attention-based fusion (DCNN-CaF) to analyze automatic soundscape characterization, including sound recognition and appraisal. Using the DeLTA dataset containing human-annotated sound source labels and perceived annoyance, the DCNN-CaF is proposed to perform sound source classification (SSC) and human-perceived annoyance rating prediction (ARP). Experimental findings indicate that (1) the proposed DCNN-CaF using loudness and Mel features outperforms the DCNN-CaF using only one of them. (2) The proposed DCNN-CaF with cross-attention fusion outperforms other typical AI-based models and soundscape-related traditional machine learning methods on the SSC and ARP tasks. (3) Correlation analysis reveals that the relationship between sound sources and annoyance is similar for humans and the proposed AI-based DCNN-CaF model. (4) Generalization tests show that the proposed model's ARP in the presence of model-unknown sound sources is consistent with expert expectations and can explain previous findings from the literature on sound-scape augmentation.
</details>
<details>
<summary>摘要</summary>
声域研究通常尝试通过访问用户来捕捉声音环境的感知和理解。然而，为长期监测或评估 intervención，声音信号基于的方法是必要的。为此，前一些研究主要集中在 psycho-acoustic 量或自动声音识别方面。只有一些尝试了评估（例如，在 circumplex 框架中）。这篇文章提出了一种基于人工智能（AI）的 dual-branch 卷积神经网络（DCNN-CaF），用于自动声capeCharacterization，包括声音识别和评估。使用包含人类注解的声音源标签和感知的压力报告的DeLTA数据集，提议的 DCNN-CaF 可以进行声音源类型分类（SSC）和人类感知压力评估预测（ARP）。实验结果表明：1. 提议的 DCNN-CaF 使用响度和Mel特征表现得更好于只使用一个特征。2. 提议的 DCNN-CaF  WITH cross-attention 融合 OUTPERforms 其他常见的 AI 基于模型和声cape 相关的传统机器学习方法在 SSC 和 ARP 任务上。3. 相关分析表明，人类和提议的 AI 基于 DCNN-CaF 模型之间的声音源和感知之间的关系类似。4. 总结测试表明，提议的模型的 ARP 在未知模型声音源的存在下是与专家预期一致的，并且可以解释过去Literature 中声cape 增强的结果。
</details></li>
</ul>
<hr>
<h2 id="CREPE-Notes-A-new-method-for-segmenting-pitch-contours-into-discrete-notes"><a href="#CREPE-Notes-A-new-method-for-segmenting-pitch-contours-into-discrete-notes" class="headerlink" title="CREPE Notes: A new method for segmenting pitch contours into discrete notes"></a>CREPE Notes: A new method for segmenting pitch contours into discrete notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08884">http://arxiv.org/abs/2311.08884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xavier Riley, Simon Dixon</li>
<li>for:  automatic music transcription and note segmentation</li>
<li>methods:  building on CREPE, a state-of-the-art monophonic pitch tracking solution, and using a simple and effective post-processing method</li>
<li>results:  state-of-the-art results on two challenging datasets of monophonic instrumental music, with a 97% reduction in the total number of parameters used compared to other deep learning-based methods.Here’s the full Chinese text:</li>
<li>for:  automatic music transcription和Note segmentation</li>
<li>methods:  builds on CREPE, a state-of-the-art monophonic pitch tracking solution, and uses a simple and effective post-processing method</li>
<li>results:  state-of-the-art results on two challenging datasets of monophonic instrumental music, with a 97% reduction in the total number of parameters used compared to other deep learning-based methods.<details>
<summary>Abstract</summary>
Tracking the fundamental frequency (f0) of a monophonic instrumental performance is effectively a solved problem with several solutions achieving 99% accuracy. However, the related task of automatic music transcription requires a further processing step to segment an f0 contour into discrete notes. This sub-task of note segmentation is necessary to enable a range of applications including musicological analysis and symbolic music generation. Building on CREPE, a state-of-the-art monophonic pitch tracking solution based on a simple neural network, we propose a simple and effective method for post-processing CREPE's output to achieve monophonic note segmentation. The proposed method demonstrates state-of-the-art results on two challenging datasets of monophonic instrumental music. Our approach also gives a 97% reduction in the total number of parameters used when compared with other deep learning based methods.
</details>
<details>
<summary>摘要</summary>
“追踪单一数位音频（f0）的问题已经是一个解决了的问题，有多种解决方案可以达到99%的准确性。但是相关的音乐转换任务仍需要一个额外的处理步骤，将f0走势分成多个独立的音符。这个子任务是为了启用多种应用，包括音乐学研究和象征音乐生成。基于CREPE的单一数位抽象网络，我们提出了一个简单而有效的方法来处理CREPE的输出，以达到单一音符分配。我们的方法在两个挑战性的单一数位乐器音乐数据集上表现出了国际级的成绩。我们的方法也比其他深度学习基于方法具有97%的减少参数数量。”Note: Please keep in mind that the translation is done by a machine and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-Non-intrusive-Hearing-aid-Speech-Assessment-Model"><a href="#Multi-objective-Non-intrusive-Hearing-aid-Speech-Assessment-Model" class="headerlink" title="Multi-objective Non-intrusive Hearing-aid Speech Assessment Model"></a>Multi-objective Non-intrusive Hearing-aid Speech Assessment Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08878">http://arxiv.org/abs/2311.08878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsin-Tien Chiang, Szu-Wei Fu, Hsin-Min Wang, Yu Tsao, John H. L. Hansen</li>
<li>For: This paper is written for the purpose of developing a non-intrusive speech assessment model for individuals with hearing impairments.* Methods: The paper uses deep learning models, specifically pre-trained self-supervised learning (SSL) models, to predict speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns.* Results: The study shows that the proposed model, called HASA-Net Large, achieves significant improvements in speech quality and intelligibility predictions compared to using spectrograms as input, and that incorporating SSL models results in greater transferability to out-of-distribution (OOD) datasets.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发一种不侵入性的听力评估模型，用于评估听力障碍者。</li>
<li>methods: 该论文使用深度学习模型，具体来说是预训练的自动标注学习（SSL）模型，以输入听力信号和指定的听力损耗模式来预测听力质量和理解度分数。</li>
<li>results: 研究显示，提议的模型——HASA-Net Large——在使用预训练SSL模型的情况下，对听力质量和理解度预测具有显著的改善，并且在OOD数据集上表现出更好的传送性。<details>
<summary>Abstract</summary>
Without the need for a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. While deep learning models have been used to develop non-intrusive speech assessment methods with promising results, there is limited research on hearing-impaired subjects. This study proposes a multi-objective non-intrusive hearing-aid speech assessment model, called HASA-Net Large, which predicts speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns. Our experiments showed the utilization of pre-trained SSL models leads to a significant boost in speech quality and intelligibility predictions compared to using spectrograms as input. Additionally, we examined three distinct fine-tuning approaches that resulted in further performance improvements. Furthermore, we demonstrated that incorporating SSL models resulted in greater transferability to OOD dataset. Finally, this study introduces HASA-Net Large, which is a non-invasive approach for evaluating speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and hearing-loss patterns to accurately predict speech quality and intelligibility levels for individuals with normal and impaired hearing and demonstrates superior prediction performance and transferability.
</details>
<details>
<summary>摘要</summary>
Without the need for a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. While deep learning models have been used to develop non-intrusive speech assessment methods with promising results, there is limited research on hearing-impaired subjects. This study proposes a multi-objective non-intrusive hearing-aid speech assessment model, called HASA-Net Large, which predicts speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns. Our experiments showed the utilization of pre-trained SSL models leads to a significant boost in speech quality and intelligibility predictions compared to using spectrograms as input. Additionally, we examined three distinct fine-tuning approaches that resulted in further performance improvements. Furthermore, we demonstrated that incorporating SSL models resulted in greater transferability to OOD dataset. Finally, this study introduces HASA-Net Large, which is a non-invasive approach for evaluating speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and hearing-loss patterns to accurately predict speech quality and intelligibility levels for individuals with normal and impaired hearing and demonstrates superior prediction performance and transferability.Here's the translation in Traditional Chinese: Without the need for a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. While deep learning models have been used to develop non-intrusive speech assessment methods with promising results, there is limited research on hearing-impaired subjects. This study proposes a multi-objective non-intrusive hearing-aid speech assessment model, called HASA-Net Large, which predicts speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns. Our experiments showed the utilization of pre-trained SSL models leads to a significant boost in speech quality and intelligibility predictions compared to using spectrograms as input. Additionally, we examined three distinct fine-tuning approaches that resulted in further performance improvements. Furthermore, we demonstrated that incorporating SSL models resulted in greater transferability to OOD dataset. Finally, this study introduces HASA-Net Large, which is a non-invasive approach for evaluating speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and hearing-loss patterns to accurately predict speech quality and intelligibility levels for individuals with normal and impaired hearing and demonstrates superior prediction performance and transferability.
</details></li>
</ul>
<hr>
<h2 id="Autoencoder-with-Group-based-Decoder-and-Multi-task-Optimization-for-Anomalous-Sound-Detection"><a href="#Autoencoder-with-Group-based-Decoder-and-Multi-task-Optimization-for-Anomalous-Sound-Detection" class="headerlink" title="Autoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection"></a>Autoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08829">http://arxiv.org/abs/2311.08829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhou, Dongxing Xu, Haoran Wei, Yanhua Long</li>
<li>for: 这个研究旨在提高机器学习方法的潜在问题检测精度。</li>
<li>methods: 我们提出了一个新的Autoencoder（AE）基本方法，包括在AE中插入一个辅助分类器，以增强潜在问题检测的多任务学习方式。我们还设计了一个群体基本解oder结构，以及一个适应损失函数，以允许模型获得领域专门知识。</li>
<li>results: 我们在DCASE 2021 Task 2的开发集上进行了实验，结果显示，我们的方法在七台机器上的试验集上获得了相对提高13.11%和15.20%的平均AUC，比官方AE和MobileNetV2更高。<details>
<summary>Abstract</summary>
In industry, machine anomalous sound detection (ASD) is in great demand. However, collecting enough abnormal samples is difficult due to the high cost, which boosts the rapid development of unsupervised ASD algorithms. Autoencoder (AE) based methods have been widely used for unsupervised ASD, but suffer from problems including 'shortcut', poor anti-noise ability and sub-optimal quality of features. To address these challenges, we propose a new AE-based framework termed AEGM. Specifically, we first insert an auxiliary classifier into AE to enhance ASD in a multi-task learning manner. Then, we design a group-based decoder structure, accompanied by an adaptive loss function, to endow the model with domain-specific knowledge. Results on the DCASE 2021 Task 2 development set show that our methods achieve a relative improvement of 13.11% and 15.20% respectively in average AUC over the official AE and MobileNetV2 across test sets of seven machines.
</details>
<details>
<summary>摘要</summary>
在工业中，机器异常声音检测（ASD）的需求很大。然而，收集异常样本很costly，这使得不supervised ASD算法的快速发展得到推动。基于Autoencoder（AE）的方法在无监督ASD方面广泛使用，但它们受到短cut、anti-noise能力不够和特征质量不佳等问题困扰。为了解决这些挑战，我们提出了一个新的AE基于框架，称为AEGM。具体来说，我们首先在AE中插入一个辅助分类器，以增强ASD在多任务学习方式下。然后，我们设计了一个群体基本解码结构，并附加了一个适应损失函数，以赋予模型域pecific知识。在DCASE 2021任务2的开发集上，我们的方法在七台机器的测试集上实现了相对提升13.11%和15.20%的平均AUC，相比官方AE和MobileNetV2。
</details></li>
</ul>
<hr>
<h2 id="CLN-VC-Text-Free-Voice-Conversion-Based-on-Fine-Grained-Style-Control-and-Contrastive-Learning-with-Negative-Samples-Augmentation"><a href="#CLN-VC-Text-Free-Voice-Conversion-Based-on-Fine-Grained-Style-Control-and-Contrastive-Learning-with-Negative-Samples-Augmentation" class="headerlink" title="CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation"></a>CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08670">http://arxiv.org/abs/2311.08670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Deng, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</li>
<li>for: 提高voice conversion质量的关键是更好地分离演说表示。</li>
<li>methods: 我们提议使用增强的负样本选择和高级 Style Modeling 来解决在相似speaker之间的性能降低问题。</li>
<li>results: 我们的方法在voice conversion任务中比前一个工作更高效。<details>
<summary>Abstract</summary>
Better disentanglement of speech representation is essential to improve the quality of voice conversion. Recently contrastive learning is applied to voice conversion successfully based on speaker labels. However, the performance of model will reduce in conversion between similar speakers. Hence, we propose an augmented negative sample selection to address the issue. Specifically, we create hard negative samples based on the proposed speaker fusion module to improve learning ability of speaker encoder. Furthermore, considering the fine-grain modeling of speaker style, we employ a reference encoder to extract fine-grained style and conduct the augmented contrastive learning on global style. The experimental results show that the proposed method outperforms previous work in voice conversion tasks.
</details>
<details>
<summary>摘要</summary>
“更好地分离语音表现是voice对应质量提升的关键。最近，contrastive learning被应用到voice对应成功，但是在相似的话者之间的对应性会下降。因此，我们提出了增强负类样本选择的方法。具体来说，我们根据提出的话者融合模块创建困难的负类样本，以提高话者Encoder的学习能力。此外，为了考虑细致的话者风格，我们还使用参考Encoder提取细致的风格，并对全局风格进行增强的对应学习。实验结果显示，我们的方法在voice对应任务中具有较高的表现。”
</details></li>
</ul>
<hr>
<h2 id="EDMSound-Spectrogram-Based-Diffusion-Models-for-Efficient-and-High-Quality-Audio-Synthesis"><a href="#EDMSound-Spectrogram-Based-Diffusion-Models-for-Efficient-and-High-Quality-Audio-Synthesis" class="headerlink" title="EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis"></a>EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08667">http://arxiv.org/abs/2311.08667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ge Zhu, Yutong Wen, Marc-André Carbonneau, Zhiyao Duan</li>
<li>for: 这篇论文目的是提出一种基于扩散模型的生成模型，用于生成各种听起来的声音。</li>
<li>methods: 该模型使用扩散过程在spectrogram域中进行生成，并使用高效的杜因抽象器来提高生成效果。</li>
<li>results: 模型在DCASE2023 foley音频生成比赛中达到了顶尖基eline的性能，且只用了10步或50步。此外，研究人员还发现了一种可能的问题，即扩散基于音频生成模型通常会生成与训练数据高度相似的样本。Here is the full text in Traditional Chinese:这篇论文的目的是提出一种基于扩散模型的生成模型，用于生成各种听起来的声音。该模型使用扩散过程在spectrogram域中进行生成，并使用高效的杜因抽象器来提高生成效果。研究人员发现，模型在DCASE2023 foley音频生成比赛中达到了顶尖基eline的性能，且只用了10步或50步。此外，研究人员还发现了一种可能的问题，即扩散基于音频生成模型通常会生成与训练数据高度相似的样本。Please note that the text is in Simplified Chinese, as requested.<details>
<summary>Abstract</summary>
Audio diffusion models can synthesize a wide variety of sounds. Existing models often operate on the latent domain with cascaded phase recovery modules to reconstruct waveform. This poses challenges when generating high-fidelity audio. In this paper, we propose EDMSound, a diffusion-based generative model in spectrogram domain under the framework of elucidated diffusion models (EDM). Combining with efficient deterministic sampler, we achieved similar Fr\'echet audio distance (FAD) score as top-ranked baseline with only 10 steps and reached state-of-the-art performance with 50 steps on the DCASE2023 foley sound generation benchmark. We also revealed a potential concern regarding diffusion based audio generation models that they tend to generate samples with high perceptual similarity to the data from training data. Project page: https://agentcooper2002.github.io/EDMSound/
</details>
<details>
<summary>摘要</summary>
Audio 扩散模型可生成各种声音。现有模型通常在幂征频域内使用级联阶段恢复模块来重construct波形，这会对高精度音频生成带来挑战。在这篇论文中，我们提议EDMSound，一种基于扩散模型的生成模型，在幂征频域内使用明确的扩散模型（EDM）框架。与高效的权值抽象器结合，我们在50步内达到了状态的权值评价（FAD）分数，并在10步内达到了同等级别的基eline。我们还发现了扩散基于音频生成模型的一种潜在问题，即它们往往会生成与训练数据高度相似的样本。项目页面：https://agentcooper2002.github.io/EDMSound/
</details></li>
</ul>
<hr>
<h2 id="Multi-channel-Conversational-Speaker-Separation-via-Neural-Diarization"><a href="#Multi-channel-Conversational-Speaker-Separation-via-Neural-Diarization" class="headerlink" title="Multi-channel Conversational Speaker Separation via Neural Diarization"></a>Multi-channel Conversational Speaker Separation via Neural Diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08630">http://arxiv.org/abs/2311.08630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Taherian, DeLiang Wang</li>
<li>for: 提高自动语音识别（ASR）系统在会议环境中的性能，解决单个说话者的ASR系统表现下降的问题。</li>
<li>methods: 提出一种基于神经网络摘要（SSND）的多渠道框架，利用终端到终端摘要系统来标识每个个体说话者的语音活动。</li>
<li>results: 在开放的 LibriCSS  dataset 上评估了提案的摘要和分离方法，与状态之前的摘要和 ASR 结果相比，提高了性能的大幅度。<details>
<summary>Abstract</summary>
When dealing with overlapped speech, the performance of automatic speech recognition (ASR) systems substantially degrades as they are designed for single-talker speech. To enhance ASR performance in conversational or meeting environments, continuous speaker separation (CSS) is commonly employed. However, CSS requires a short separation window to avoid many speakers inside the window and sequential grouping of discontinuous speech segments. To address these limitations, we introduce a new multi-channel framework called "speaker separation via neural diarization" (SSND) for meeting environments. Our approach utilizes an end-to-end diarization system to identify the speech activity of each individual speaker. By leveraging estimated speaker boundaries, we generate a sequence of embeddings, which in turn facilitate the assignment of speakers to the outputs of a multi-talker separation model. SSND addresses the permutation ambiguity issue of talker-independent speaker separation during the diarization phase through location-based training, rather than during the separation process. This unique approach allows multiple non-overlapped speakers to be assigned to the same output stream, making it possible to efficiently process long segments-a task impossible with CSS. Additionally, SSND is naturally suitable for speaker-attributed ASR. We evaluate our proposed diarization and separation methods on the open LibriCSS dataset, advancing state-of-the-art diarization and ASR results by a large margin.
</details>
<details>
<summary>摘要</summary>
当处理重叠的语音时，自动语音识别（ASR）系统的性能会受到重叠的影响，因为它们是单个说话人的设计。为了提高 ASR 在会议环境中的性能，连续说话分离（CSS）常常被使用。然而，CSS 需要一个短暂的分离窗口，以避免在窗口内有多个说话人和顺序分组不连续的语音段。为了解决这些限制，我们介绍了一种新的多通道框架，即“基于神经分类的说话人分离”（SSND），用于会议环境。我们的方法利用一个端到端的分类系统来识别每个个人的语音活动。通过利用估计的说话人边界，我们生成一个序列的嵌入，从而促进将说话人分配到多个说话人分离模型的输出中。SSND 通过在分类阶段使用位置基于的训练来解决 talker-independent  speaker separation 中的 permutation ambiguity 问题，而不是在分离过程中。这种独特的方法使得可以有效地处理长段语音，这是 CSS 不可能实现的。此外，SSND 自然适合 speaker-attributed ASR。我们对 LibriCSS 开放 dataset 进行了我们的提议的分类和分离方法的评估，并在状态 искусственный智能中提高了 ASR 和分类的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.SD_2023_11_15/" data-id="clp53jwvy011fyp8869xecspg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2023/11/15/cs.CV_2023_11_15/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CV - 2023-11-15</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">125</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">62</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">76</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
