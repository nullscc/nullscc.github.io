
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-11-15 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09458 repo_url: None paper_authors: Praful">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-11-15">
<meta property="og:url" content="https://nullscc.github.io/2023/11/15/cs.CL_2023_11_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09458 repo_url: None paper_authors: Praful">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-15T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-19T06:27:34.914Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.CL_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T11:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-11-15
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lexical-Repetitions-Lead-to-Rote-Learning-Unveiling-the-Impact-of-Lexical-Overlap-in-Train-and-Test-Reference-Summaries"><a href="#Lexical-Repetitions-Lead-to-Rote-Learning-Unveiling-the-Impact-of-Lexical-Overlap-in-Train-and-Test-Reference-Summaries" class="headerlink" title="Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries"></a>Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09458">http://arxiv.org/abs/2311.09458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prafulla Kumar Choubey, Alexander R. Fabbri, Caiming Xiong, Chien-Sheng Wu</li>
<li>for: 这个论文的目的是提出一种精细评估协议，以便评估psummarization模型的泛化能力。</li>
<li>methods: 这个论文使用了一种lexical similaritypartitioning方法来评估psummarization模型的性能。</li>
<li>results: 研究发现，通过限制training summaries中的lexical repetitions，可以避免模型学习rote learning，提高psummarization模型的泛化能力。<details>
<summary>Abstract</summary>
Ideal summarization models should generalize to novel summary-worthy content without remembering reference training summaries by rote. However, a single average performance score on the entire test set is inadequate in determining such model competencies. We propose a fine-grained evaluation protocol by partitioning a test set based on the lexical similarity of reference test summaries with training summaries. We observe up to a 5x (1.2x) difference in ROUGE-2 (entity recall) scores between the subsets with the lowest and highest similarity. Next, we show that such training repetitions also make a model vulnerable to rote learning, reproducing data artifacts such as factual errors, especially when reference test summaries are lexically close to training summaries. Consequently, we propose to limit lexical repetitions in training summaries during both supervised fine-tuning and likelihood calibration stages to improve the performance on novel test cases while retaining average performance. Our automatic and human evaluations on novel test subsets and recent news articles show that limiting lexical repetitions in training summaries can prevent rote learning and improve generalization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Subtle-Misogyny-Detection-and-Mitigation-An-Expert-Annotated-Dataset"><a href="#Subtle-Misogyny-Detection-and-Mitigation-An-Expert-Annotated-Dataset" class="headerlink" title="Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset"></a>Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09443">http://arxiv.org/abs/2311.09443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brooklyn Sheppard, Anna Richter, Allison Cohen, Elizabeth Allyn Smith, Tamara Kneese, Carolyne Pelletier, Ioana Baldini, Yue Dong</li>
<li>for: 本研究用于开发一个新的 dataset，捕捉贫婆婆的细节和细腻，以便用于 NLP 任务中检测贫婆婆。</li>
<li>methods: 该 dataset 采用多元领域专家和注释者的合作建立，包括电影字幕注释，以捕捉北美电影中的贫婆婆表达。</li>
<li>results: 本研究提供了检测贫婆婆的方法和基elines，以及用于 rewrite 文本的文本生成技术。我们希望这种工作能够推动 AI 为社会好用的 NLP 应用。<details>
<summary>Abstract</summary>
Using novel approaches to dataset development, the Biasly dataset captures the nuance and subtlety of misogyny in ways that are unique within the literature. Built in collaboration with multi-disciplinary experts and annotators themselves, the dataset contains annotations of movie subtitles, capturing colloquial expressions of misogyny in North American film. The dataset can be used for a range of NLP tasks, including classification, severity score regression, and text generation for rewrites. In this paper, we discuss the methodology used, analyze the annotations obtained, and provide baselines using common NLP algorithms in the context of misogyny detection and mitigation. We hope this work will promote AI for social good in NLP for bias detection, explanation, and removal.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Biasly" is a made-up word, so it was not translated.* "multi-disciplinary" was translated as "多元的" (duōyuán de), which is a more common way of saying "interdisciplinary" in Chinese.* "colloquial expressions" was translated as "口语表达" (kǒu yǔ biǎo jiàn), which is a more common way of saying "colloquialisms" in Chinese.* "North American films" was translated as "北美电影" (běi mèi diàn yǐng), which is a more specific way of saying "American movies" in Chinese.* "severity score regression" was translated as "严重程度回归" (jiān zhòng chéng dào huí jīn), which is a more common way of saying "severity score prediction" in Chinese.* "text generation for rewrites" was translated as "文本生成重写" (wén tiěn shēng chéng zhòng xiě), which is a more common way of saying "text generation for rewriting" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Labeled-Interactive-Topic-Models"><a href="#Labeled-Interactive-Topic-Models" class="headerlink" title="Labeled Interactive Topic Models"></a>Labeled Interactive Topic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09438">http://arxiv.org/abs/2311.09438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Kyle Seelman, Mozhi Zhang, Jordan Boyd-Graber</li>
<li>for: 本研究旨在提高 neural topic model 的可互动性，使其能够更好地捕捉用户的信息需求。</li>
<li>methods: 本研究使用了一种intuitive interaction方法，让用户可以对话板模型中的话题进行标签和更新。这种方法可以让用户根据自己的信息需求来修改话题的分布。</li>
<li>results: 通过人工研究，我们发现用户可以通过标签和更新话题来提高文档排名分数，从而更好地找到与查询有关的文档。相比没有用户标签的情况，用户可以通过这种方法提高文档排名分数。<details>
<summary>Abstract</summary>
Topic models help users understand large document collections; however, topic models do not always find the ``right'' topics. While classical probabilistic and anchor-based topic models have interactive variants to guide models toward better topics, such interactions are not available for neural topic models such as the embedded topic model (\abr{etm}). We correct this lacuna by adding an intuitive interaction to neural topic models: users can label a topic with a word, and topics are updated so that the topic words are close to the label. This allows a user to refine topics based on their information need. While, interactivity is intuitive for \abr{etm}, we extend this framework to work with other neural topic models as well. We develop an interactive interface which allows users to interact and relabel topic models as they see fit. We evaluate our method through a human study, where users can relabel topics to find relevant documents. Using our method, user labeling improves document rank scores, helping to find more relevant documents to a given query when compared to no user labeling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Striped-Attention-Faster-Ring-Attention-for-Causal-Transformers"><a href="#Striped-Attention-Faster-Ring-Attention-for-Causal-Transformers" class="headerlink" title="Striped Attention: Faster Ring Attention for Causal Transformers"></a>Striped Attention: Faster Ring Attention for Causal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09431">http://arxiv.org/abs/2311.09431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exists-forall/striped_attention">https://github.com/exists-forall/striped_attention</a></li>
<li>paper_authors: William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, Jonathan Ragan-Kelley</li>
<li>for: This paper aims to address the growing demand for longer sequence lengths in transformer models by proposing an exact attention algorithm called Ring Attention, which can overcome per-device memory bottlenecks by distributing self-attention across multiple devices.</li>
<li>methods: The paper proposes a simple extension to Ring Attention called Striped Attention, which distributes the attention computation across multiple devices in a more balanced way to achieve more even workloads.</li>
<li>results: The authors achieve up to 1.45x end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k, and 1.65x speedups at sequence lengths of 786k using Striped Attention on 16 TPUv4 chips.Here’s the Chinese translation of the three information points:</li>
<li>for: 这篇论文目标是解决变长序列长度的增长对转换器模型的需求，提出了环Attention算法，可以在多个设备上分布自注意计算，从而缓解每个设备的内存瓶颈。</li>
<li>methods: 论文提出了一种简单的扩展，即Striped Attention，它将注意计算分布到多个设备上，以更平衡的方式来实现更平衡的工作负荷。</li>
<li>results: 作者使用Striped Attention在 causal transformer 训练中 achieved up to 1.45倍的综合通过put improvement，并在 786k 长度的序列上达到 1.65倍的速度提升。<details>
<summary>Abstract</summary>
To help address the growing demand for ever-longer sequence lengths in transformer models, Liu et al. recently proposed Ring Attention, an exact attention algorithm capable of overcoming per-device memory bottle- necks by distributing self-attention across multiple devices. In this paper, we study the performance characteristics of Ring Attention in the important special case of causal transformer models, and identify a key workload imbal- ance due to triangular structure of causal attention computations. We propose a simple extension to Ring Attention, which we call Striped Attention to fix this imbalance. Instead of devices having contiguous subsequences, each device has a subset of tokens distributed uniformly throughout the sequence, which we demonstrate leads to more even workloads. In experiments running Striped Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k. Furthermore, on 16 TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of 786k. We release the code for our experiments as open source
</details>
<details>
<summary>摘要</summary>
lijun et al. 最近提出了环形注意力算法（Ring Attention），用于解决变长序列长度的增长导致的每个设备内存瓶颈问题。在这篇论文中，我们研究了环形注意力在重要的 causal transformer 模型中的性能特点，并发现了一个关键的工作负载不均衡问题，即triangle structure of causal attention computations。我们提出了一个简单的扩展，称为Striped Attention，以解决这个不均衡问题。在我们的实验中，我们在 A100 GPU 和 TPUv4 上运行了 Striped Attention，并 achievied up to 1.45x 终端通过put improvementsover original Ring Attention algorithm on causal transformer 训练序列长度为 256k。此外，在 16 TPUv4 板件上，我们实现了 1.65x 速度提升，在序列长度为 786k 的情况下。我们将我们的实验代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="Predicting-generalization-performance-with-correctness-discriminators"><a href="#Predicting-generalization-performance-with-correctness-discriminators" class="headerlink" title="Predicting generalization performance with correctness discriminators"></a>Predicting generalization performance with correctness discriminators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09422">http://arxiv.org/abs/2311.09422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuekun Yao, Alexander Koller</li>
<li>for: 预测NLP模型在未经见、可能是非标的数据上的准确率。</li>
<li>methods: 使用一个新的模型来确定模型在未经见数据上的准确率范围，不需要黄金标签。</li>
<li>results: 通过训练一个推断器，确定模型输出的Sequence-to-Sequence模型输出是否正确，并证明了金标签的准确率在预测范围内，并且这些范围很接近。<details>
<summary>Abstract</summary>
The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness. We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data. We achieve this by training a discriminator which predicts whether the output of a given sequence-to-sequence model is correct or not. We show across a variety of tagging, parsing, and semantic parsing tasks that the gold accuracy is reliably between the predicted upper and lower bounds, and that these bounds are remarkably close together.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness." into Simplified Chinese翻译：预测未经见过、可能处于数据分布范围外的 NLP 模型准确率是可靠性的必要条件。我们提出了一种新的模型，可以在没有金标签的情况下，设置上下限于模型的准确率。我们通过训练一个判断序列到Sequence模型输出是否正确的探测器来实现这一点。我们在多个标注、分析和 semantics 分析任务中表明，金标签的准确率在上下限之间，并且这些上下限很接近。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Alternatives-to-the-Scaled-Dot-Product-for-Attention-in-the-Transformer-Neural-Network-Architecture"><a href="#Alternatives-to-the-Scaled-Dot-Product-for-Attention-in-the-Transformer-Neural-Network-Architecture" class="headerlink" title="Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture"></a>Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09406">http://arxiv.org/abs/2311.09406</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Bernhard</li>
<li>for: 提高 transformer 神经网络中 attention  Mechanism 的效果</li>
<li>methods: 提议一些alternative scalings，包括将dot product 分配到键值之和之前应用softmax</li>
<li>results: 使用模拟的键值和查询示例，显示这些scalings 在许多情况下更有效地避免应用softmax导致梯度消失的地方<details>
<summary>Abstract</summary>
The transformer neural network architecture uses a form of attention in which the dot product of query and key is divided by the square root of the key dimension before applying softmax. This scaling of the dot product is designed to avoid the absolute value of the dot products becoming so large that applying softmax leads to vanishing gradients. In this paper, we propose some alternative scalings, including dividing the dot product instead by the sum of the key lengths before applying softmax. We use simulated keys and queries to show that in many situations this appears to be more effective at avoiding regions where applying softmax leads to vanishing gradients.
</details>
<details>
<summary>摘要</summary>
transformer 神经网络架构使用一种叫做注意力的方式，其中查询和键的 dot product 被除以键维度的平方根之前应用 softmax。这种缩放的 dot product 是为了避免查询和键的绝对值变得太大，使得应用 softmax 导致梯度消失。在这篇论文中，我们提出了一些替代的缩放方式，包括将 dot product 除以键的总长度之前应用 softmax。我们使用模拟的查询和键来示出，在许多情况下，这些替代缩放方式可以更好地避免应用 softmax 导致梯度消失的区域。
</details></li>
</ul>
<hr>
<h2 id="To-Translate-or-Not-to-Translate-A-Systematic-Investigation-of-Translation-Based-Cross-Lingual-Transfer-to-Low-Resource-Languages"><a href="#To-Translate-or-Not-to-Translate-A-Systematic-Investigation-of-Translation-Based-Cross-Lingual-Transfer-to-Low-Resource-Languages" class="headerlink" title="To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages"></a>To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09404">http://arxiv.org/abs/2311.09404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benedikt Ebing, Goran Glavaš</li>
<li>for: 这个论文的目的是为了提高跨语言传递（XLT）性能，并评估现有和新的翻译基于XLT方法。</li>
<li>methods: 本论文使用了翻译基于MT的XLT方法，包括使用round-trip翻译、添加可靠翻译等方法。</li>
<li>results: 研究发现，使用翻译基于MT的XLT方法可以大幅提高XLT性能，并且可以通过添加其他高资源语言的翻译数据来进一步提高性能。<details>
<summary>Abstract</summary>
Perfect machine translation (MT) would render cross-lingual transfer (XLT) by means of multilingual language models (LMs) superfluous. Given, on one hand, the large body of work on improving XLT with multilingual LMs and, on the other hand, recent advances in massively multilingual MT, in this work, we systematically evaluate existing and propose new translation-based XLT approaches for transfer to low-resource languages. We show that all translation-based approaches dramatically outperform zero-shot XLT with multilingual LMs, rendering the approach that combines the round-trip translation of the source-language training data with the translation of the target-language test instances the most effective. We next show that one can obtain further empirical gains by adding reliable translations to other high-resource languages to the training data. Moreover, we propose an effective translation-based XLT strategy even for languages not supported by the MT system. Finally, we show that model selection for XLT based on target-language validation data obtained with MT outperforms model selection based on the source-language data. We hope that our findings encourage adoption of more robust translation-based baselines in XLT research.
</details>
<details>
<summary>摘要</summary>
如果完美机器翻译（MT）能够实现语言之间传递（XLT），那么使用多语言模型（LM）就不再需要。一方面，有大量的研究用于提高XLT的多语言LM，另一方面，近年来的大规模多语言MT的进步，我们在这里系统地评估了现有的翻译基于XLT方法，以及新的翻译基于XLT方法，用于将语言转移到低资源语言。我们发现所有的翻译基于方法在零批XLT中都表现出了极大的改善，并且将源语言训练数据翻译回目标语言测试实例的方法得到了最佳效果。我们还证明了可以通过添加其他高资源语言的可靠翻译来进一步提高实验性能。此外，我们提出了一种有效的翻译基于XLT策略，即使Language不支持MT系统。最后，我们发现基于MT系统进行目标语言验证数据的模型选择比基于源语言数据更高效。我们希望我们的发现能够激励XLT研究中采用更加稳定的翻译基于基准。
</details></li>
</ul>
<hr>
<h2 id="LEEETs-Dial-Linguistic-Entrainment-in-End-to-End-Task-oriented-Dialogue-systems"><a href="#LEEETs-Dial-Linguistic-Entrainment-in-End-to-End-Task-oriented-Dialogue-systems" class="headerlink" title="LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems"></a>LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09390">http://arxiv.org/abs/2311.09390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nalin Kumar, Ondřej Dušek</li>
<li>for: 这项研究的目的是提高对话系统的自然语言处理能力，使对话更加自然和流畅。</li>
<li>methods: 这项研究使用了GPT-2基于端到端对话系统，并通过共享词汇来实现对话融合。研究者对实例权重训练、对称损失和额外条件进行了调整，以生成与用户的响应相Alignment。</li>
<li>results: 研究表明，三种整合技术在MultiWOZ数据集上都能够生成高度对应的响应，与基eline相比，自动和手动评估指标都表明了这一点。<details>
<summary>Abstract</summary>
Linguistic entrainment, or alignment, represents a phenomenon where linguistic patterns employed by conversational participants converge to one another. While alignment has been shown to produce a more natural user experience, most dialogue systems do not have any provisions for it. In this work, we introduce methods for achieving dialogue alignment in a GPT-2-based end-to-end dialogue system through the utilization of shared vocabulary. We experiment with training instance weighting, alignment-specific loss, and additional conditioning to generate responses that align with the user. By comparing different entrainment techniques on the MultiWOZ dataset, we demonstrate that all three approaches produce significantly better-aligned results than the baseline, as confirmed by both automated and manual evaluation metrics.
</details>
<details>
<summary>摘要</summary>
语言同步、或谱合，表示对话参与者使用的语言模式相互整合。而对话同步已经证明可以提供更自然的用户体验，但大多数对话系统并没有相应的规定。在这项工作中，我们介绍了使用共享词汇来实现对话同步在基于GPT-2的端到端对话系统中。我们对训练实例权重、对话同步特有的损失函数和额外条件进行实验，以生成与用户相对应的回答。通过对多语言WOZ数据集进行比较，我们证明了这三种整合技术都能够在自动和手动评估指标上显著提高对话同步性，并且比基eline更好。
</details></li>
</ul>
<hr>
<h2 id="Neural-machine-translation-for-automated-feedback-on-children’s-early-stage-writing"><a href="#Neural-machine-translation-for-automated-feedback-on-children’s-early-stage-writing" class="headerlink" title="Neural machine translation for automated feedback on children’s early-stage writing"></a>Neural machine translation for automated feedback on children’s early-stage writing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09389">http://arxiv.org/abs/2311.09389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Vestergaard Jensen, Mikkel Jordahn, Michael Riis Andersen</li>
<li>for: 本研究旨在自动生成初级写作评估和建构反馈。</li>
<li>methods: 我们提议使用序列到序列模型将初级写作翻译成普通写作，以便使用语言指标进行分析。此外，我们还提出了一种新的稳定likelihood来mitigate dataset中的噪声影响。</li>
<li>results: 我们通过数字实验 investigate了我们的方法，并证明了可以高度准确预测普通写作。<details>
<summary>Abstract</summary>
In this work, we address the problem of assessing and constructing feedback for early-stage writing automatically using machine learning. Early-stage writing is typically vastly different from conventional writing due to phonetic spelling and lack of proper grammar, punctuation, spacing etc. Consequently, early-stage writing is highly non-trivial to analyze using common linguistic metrics. We propose to use sequence-to-sequence models for "translating" early-stage writing by students into "conventional" writing, which allows the translated text to be analyzed using linguistic metrics. Furthermore, we propose a novel robust likelihood to mitigate the effect of noise in the dataset. We investigate the proposed methods using a set of numerical experiments and demonstrate that the conventional text can be predicted with high accuracy.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们解决了自动使用机器学习进行早期写作评估和建构反馈的问题。早期写作通常具有不同的音词拼写和缺失正确的语法、标点符号等等，因此对常见语言指标来分析是非常困难的。我们提议使用序列到序列模型将学生的早期写作翻译成“常规”的写作，这使得翻译后的文本可以使用语言指标进行分析。此外，我们还提出了一种新的Robust likelihood来降低数据集中的噪声的影响。我们通过一系列数字实验来调查提议的方法，并证明了可以高度准确地预测常规文本。
</details></li>
</ul>
<hr>
<h2 id="Banach-Tarski-Embeddings-and-Transformers"><a href="#Banach-Tarski-Embeddings-and-Transformers" class="headerlink" title="Banach-Tarski Embeddings and Transformers"></a>Banach-Tarski Embeddings and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09387">http://arxiv.org/abs/2311.09387</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtmaher/embedding">https://github.com/jtmaher/embedding</a></li>
<li>paper_authors: Joshua Maher</li>
<li>for: 这个论文旨在提出一种将复杂数据结构转化为高维向量的新建构。这些向量可以用于解释 transformer 的隐藏状态向量模型。</li>
<li>methods: 这个论文使用的方法包括构建高维向量空间中的嵌入，以及使用这些嵌入进行计算和解码。具体来说，这个论文提出了一种使用 vector 操作直接在嵌入空间进行计算，而不需要解码。</li>
<li>results: 这个论文的结果表明，当嵌入维度够大时，可以将嵌入转化回原始数据结构。此外，这个论文还提出了一种使用嵌入进行计算的算法，可以construct一个嵌入的 parse 树。<details>
<summary>Abstract</summary>
We introduce a new construction of embeddings of arbitrary recursive data structures into high dimensional vectors. These embeddings provide an interpretable model for the latent state vectors of transformers. We demonstrate that these embeddings can be decoded to the original data structure when the embedding dimension is sufficiently large. This decoding algorithm has a natural implementation as a transformer. We also show that these embedding vectors can be manipulated directly to perform computations on the underlying data without decoding. As an example we present an algorithm that constructs the embedded parse tree of an embedded token sequence using only vector operations in embedding space.
</details>
<details>
<summary>摘要</summary>
我们介绍一种新的嵌入建构，将 recursive 数据结构嵌入高维 вектор中。这些嵌入提供了可解释的模型，用于 transformer 中的 latent state 矢量。我们示示了这些嵌入可以在嵌入维度足够大时，被解码回原始数据结构。这个解码算法自然地实现为 transformer。此外，我们还证明了这些嵌入矢量可以直接进行计算，无需解码。作为例子，我们提出了一个算法，使用 vector 操作来构造嵌入token序列中的嵌入 parse tree。
</details></li>
</ul>
<hr>
<h2 id="Long-form-Question-Answering-An-Iterative-Planning-Retrieval-Generation-Approach"><a href="#Long-form-Question-Answering-An-Iterative-Planning-Retrieval-Generation-Approach" class="headerlink" title="Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach"></a>Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09383">http://arxiv.org/abs/2311.09383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pritom Saha Akash, Kashob Kumar Roy, Lucian Popa, Kevin Chen-Chuan Chang</li>
<li>for: 这篇论文的目的是解决长形问答（LFQA）问题，旨在生成详细的回答，包括多个话题和 их复杂关系，需要详细的解释。</li>
<li>methods: 该论文提出了一种基于迭代规划、检索和生成的LFQA模型，通过多次迭代过程，直到生成完整的回答。此外，该模型还利用了多个知识源的整合，以提高回答的准确性和完整性。</li>
<li>results: 经过广泛的实验，该模型在两个不同领域的QA数据集上表现出色，与现有的状态 искусственный智能模型相比，在多种文本和事实指标上表现出超过其他模型的优势。<details>
<summary>Abstract</summary>
Long-form question answering (LFQA) poses a challenge as it involves generating detailed answers in the form of paragraphs, which go beyond simple yes/no responses or short factual answers. While existing QA models excel in questions with concise answers, LFQA requires handling multiple topics and their intricate relationships, demanding comprehensive explanations. Previous attempts at LFQA focused on generating long-form answers by utilizing relevant contexts from a corpus, relying solely on the question itself. However, they overlooked the possibility that the question alone might not provide sufficient information to identify the relevant contexts. Additionally, generating detailed long-form answers often entails aggregating knowledge from diverse sources. To address these limitations, we propose an LFQA model with iterative Planning, Retrieval, and Generation. This iterative process continues until a complete answer is generated for the given question. From an extensive experiment on both an open domain and a technical domain QA dataset, we find that our model outperforms the state-of-the-art models on various textual and factual metrics for the LFQA task.
</details>
<details>
<summary>摘要</summary>
长形问题回答（LFQA）呈现了挑战，因为它们需要生成详细的回答，以 paragraph 的形式，超出了简单的是/否回答或短要的事实回答。现有的 QA 模型在问题中提供了简短的回答，但 LFQA 需要处理多个话题和它们的复杂关系，需要详细的解释。先前的 LFQA 尝试都是通过使用相关的文本来生成长形回答，但它们忽略了问题本身可能不够提供足够的信息来标识相关的文本的可能性。此外，生成详细的长形回答经常需要从多个来源汇集知识。为解决这些限制，我们提议了一种基于规划、检索和生成的 LFQA 模型。这个迭代过程会持续到一个完整的回答被生成为给定的问题。从一项大规模的实验中，我们发现我们的模型在开放领域和技术领域 QA 数据集上超越了当前状态的模型在不同的文本和事实指标上。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Online-User-Aggression-Content-Detection-and-Behavioural-Analysis-on-Social-Media-Platforms"><a href="#A-Survey-on-Online-User-Aggression-Content-Detection-and-Behavioural-Analysis-on-Social-Media-Platforms" class="headerlink" title="A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms"></a>A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09367">http://arxiv.org/abs/2311.09367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Mane, Suman Kundu, Rajesh Sharma</li>
<li>For: This paper aims to bridge the gap between disparate studies on Aggression Content Detection and Behavioral Analysis of Aggressive Users, with the goal of preventing cyber-aggressive behavior.* Methods: The paper examines the comprehensive process of Aggression Content Detection, including dataset creation, feature selection and extraction, and detection algorithm development. It also reviews studies on Behavioral Analysis of Aggression that explore the influencing factors, consequences, and patterns associated with cyber-aggressive behavior.* Results: The paper concludes by identifying research gaps and encouraging further progress in the unified domain of socio-computational aggressive behavior analysis, highlighting the effectiveness of incorporating sociological insights into computational techniques for preventing cyber-aggressive behavior.<details>
<summary>Abstract</summary>
The rise of social media platforms has led to an increase in cyber-aggressive behavior, encompassing a broad spectrum of hostile behavior, including cyberbullying, online harassment, and the dissemination of offensive and hate speech. These behaviors have been associated with significant societal consequences, ranging from online anonymity to real-world outcomes such as depression, suicidal tendencies, and, in some instances, offline violence. Recognizing the societal risks associated with unchecked aggressive content, this paper delves into the field of Aggression Content Detection and Behavioral Analysis of Aggressive Users, aiming to bridge the gap between disparate studies. In this paper, we analyzed the diversity of definitions and proposed a unified cyber-aggression definition. We examine the comprehensive process of Aggression Content Detection, spanning from dataset creation, feature selection and extraction, and detection algorithm development. Further, we review studies on Behavioral Analysis of Aggression that explore the influencing factors, consequences, and patterns associated with cyber-aggressive behavior. This systematic literature review is a cross-examination of content detection and behavioral analysis in the realm of cyber-aggression. The integrated investigation reveals the effectiveness of incorporating sociological insights into computational techniques for preventing cyber-aggressive behavior. Finally, the paper concludes by identifying research gaps and encouraging further progress in the unified domain of socio-computational aggressive behavior analysis.
</details>
<details>
<summary>摘要</summary>
随着社交媒体平台的普及，циBER-侵略性行为的发展也得到了推动。这种行为包括互联网欺凌、在线骚扰和危险或仇恨言论等，它们与社会环境有着深远的关系，从在线匿名到真实世界的后果，如抑郁、自杀倾向和在一些情况下的线下暴力。recognizing the societal risks associated with unchecked aggressive content, this paper delves into the field of Aggression Content Detection and Behavioral Analysis of Aggressive Users, aiming to bridge the gap between disparate studies. In this paper, we analyzed the diversity of definitions and proposed a unified cyber-aggression definition. We examine the comprehensive process of Aggression Content Detection, spanning from dataset creation, feature selection and extraction, and detection algorithm development. Further, we review studies on Behavioral Analysis of Aggression that explore the influencing factors, consequences, and patterns associated with cyber-aggressive behavior. This systematic literature review is a cross-examination of content detection and behavioral analysis in the realm of cyber-aggression. The integrated investigation reveals the effectiveness of incorporating sociological insights into computational techniques for preventing cyber-aggressive behavior. Finally, the paper concludes by identifying research gaps and encouraging further progress in the unified domain of socio-computational aggressive behavior analysis.
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Emergent-Audio-Classification-Ability-of-ASR-Foundation-Models"><a href="#Investigating-the-Emergent-Audio-Classification-Ability-of-ASR-Foundation-Models" class="headerlink" title="Investigating the Emergent Audio Classification Ability of ASR Foundation Models"></a>Investigating the Emergent Audio Classification Ability of ASR Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09363">http://arxiv.org/abs/2311.09363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Adian Liusie, Mark J. F. Gales, Kate M. Knill</li>
<li>for: 这 paper 的目的是 investigate the zero-shot 音频分类能力 of Whisper and MMS, two ASR foundation models.</li>
<li>methods: 作者使用了 simple template-based text prompts 和 decoding probabilities 来生成 zero-shot predictions. 另外，作者还使用了 debiasing 方法来提高模型的性能.</li>
<li>results: 研究表明，Whisper 可以在 8 个 audio-classification 数据集上显示出优秀的 zero-shot 分类性能，与现有的状态的艺术 zero-shot baseline 的准确率相比，提高了9%的准确率。同时，作者发现，模型的大小增加会导致 zero-shot 性能提高。<details>
<summary>Abstract</summary>
Text and vision foundation models can perform many tasks in a zero-shot setting, a desirable property that enables these systems to be applied in general and low-resource settings. However, there has been significantly less work on the zero-shot abilities of ASR foundation models, with these systems typically fine-tuned to specific tasks or constrained to applications that match their training criterion and data annotation. In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification. We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions. Without training the model on extra data or adding any new parameters, we demonstrate that Whisper shows promising zero-shot classification performance on a range of 8 audio-classification datasets, outperforming existing state-of-the-art zero-shot baseline's accuracy by an average of 9%. One important step to unlock the emergent ability is debiasing, where a simple unsupervised reweighting method of the class probabilities yields consistent significant performance gains. We further show that performance increases with model size, implying that as ASR foundation models scale up, they may exhibit improved zero-shot performance.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:文本和视觉基础模型可以完成许多零shot任务，这是一个有利的特性，使得这些系统能够应用在普遍和资源匮乏的设置中。然而，对于ASR基础模型来说，有 significatively less work on its zero-shot能力，这些系统通常是特定任务的细化或者数据注解的约束下进行了训练。在这个工作中，我们 investigate了Whisper和MMS，这两个主要用于音频识别的ASR基础模型，在零shot音频分类任务上的能力。我们使用简单的模板基于文本提示，并使用其结果的解码概率来生成零shot预测。无需训练模型更多数据或者添加新的参数，我们示出了Whisper在8个音频分类dataset上的出色的零shot分类性能，与现有的零shot基线的精度相比，平均提高9%。一种重要的步骤是debiasing，这是一种简单的无监督重新权重方法，可以持续性地提高性能。我们还显示，模型的大小增加会导致零shot性能提高，这 implies that ASR基础模型可能在扩大后展现出更好的零shot性能。
</details></li>
</ul>
<hr>
<h2 id="LePaRD-A-Large-Scale-Dataset-of-Judges-Citing-Precedents"><a href="#LePaRD-A-Large-Scale-Dataset-of-Judges-Citing-Precedents" class="headerlink" title="LePaRD: A Large-Scale Dataset of Judges Citing Precedents"></a>LePaRD: A Large-Scale Dataset of Judges Citing Precedents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09356">http://arxiv.org/abs/2311.09356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rmahari/lepard">https://github.com/rmahari/lepard</a></li>
<li>paper_authors: Robert Mahari, Dominik Stammbach, Elliott Ash, Alex &#96;Sandy’ Pentland</li>
<li>for: 这篇论文是为了提高法律搜索和理解的技术能力而写的。</li>
<li>methods: 这篇论文使用了大量的美国联邦法院判例文献，并进行了多种检索方法的评估。</li>
<li>results: 研究发现，使用分类方法可以在法律搜索中获得最好的效果，但是法律预测仍然是一个具有挑战性的任务，还有很多空间可以进行改进。<details>
<summary>Abstract</summary>
We present the Legal Passage Retrieval Dataset LePaRD. LePaRD is a massive collection of U.S. federal judicial citations to precedent in context. The dataset aims to facilitate work on legal passage prediction, a challenging practice-oriented legal retrieval and reasoning task. Legal passage prediction seeks to predict relevant passages from precedential court decisions given the context of a legal argument. We extensively evaluate various retrieval approaches on LePaRD, and find that classification appears to work best. However, we note that legal precedent prediction is a difficult task, and there remains significant room for improvement. We hope that by publishing LePaRD, we will encourage others to engage with a legal NLP task that promises to help expand access to justice by reducing the burden associated with legal research. A subset of the LePaRD dataset is freely available and the whole dataset will be released upon publication.
</details>
<details>
<summary>摘要</summary>
我团队现在发布了《法律段落预测数据集》（LePaRD）。LePaRD是一个庞大的美国联邦法院判例引用数据集，旨在促进法律段落预测任务的研究和应用。法律段落预测是一项具有实际应用价值的法律自然语言处理任务，它的目标是根据法律Arguments的上下文预测相关的判例段落。我们对LePaRD进行了广泛的评估，发现类别预测方法在这个任务中表现最佳。然而，我们注意到法律预测任务是一项具有挑战性的任务，还有很大的发展空间。我们希望通过发布LePaRD，促进法律自然语言处理领域的研究，扩大 justice的访问，减少法律研究的负担。一个LePaRD数据集的子集可以免费下载，整个数据集将在发布后释出。
</details></li>
</ul>
<hr>
<h2 id="Language-and-Task-Arithmetic-with-Parameter-Efficient-Layers-for-Zero-Shot-Summarization"><a href="#Language-and-Task-Arithmetic-with-Parameter-Efficient-Layers-for-Zero-Shot-Summarization" class="headerlink" title="Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization"></a>Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09344">http://arxiv.org/abs/2311.09344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Chronopoulou, Jonas Pfeiffer, Joshua Maynez, Xinyi Wang, Sebastian Ruder, Priyanka Agrawal</li>
<li>for: 提高大语言模型（LLM）下游任务性能，尤其是在语言生成任务上。</li>
<li>methods: 使用标注任务数据进行参数高效化精细调整（PEFT），并通过元素加法操作将语言或任务特定的PEFT模块相乘。</li>
<li>results: 在摘要任务上实现了稳定的性能提升，只需训练PEFT模块 minimal amount of training data。<details>
<summary>Abstract</summary>
Parameter-efficient fine-tuning (PEFT) using labeled task data can significantly improve the performance of large language models (LLMs) on the downstream task. However, there are 7000 languages in the world and many of these languages lack labeled data for real-world language generation tasks. In this paper, we propose to improve zero-shot cross-lingual transfer by composing language or task specialized parameters. Our method composes language and task PEFT modules via element-wise arithmetic operations to leverage unlabeled data and English labeled data. We extend our approach to cases where labeled data from more languages is available and propose to arithmetically compose PEFT modules trained on languages related to the target. Empirical results on summarization demonstrate that our method is an effective strategy that obtains consistent gains using minimal training of PEFT modules.
</details>
<details>
<summary>摘要</summary>
Parameter-efficient fine-tuning（PEFT）使用标注任务数据可以显著提高大语言模型（LLM）下游任务的性能。然而，世界上有7000种语言，而 многие这些语言缺乏实际语言生成任务的标注数据。在这篇论文中，我们提议使用语言或任务特化的参数来改善零shot Cross-Lingual transfer。我们的方法通过元素减法运算将语言和任务PEFT模块 compose，以利用无标注数据和英语标注数据。我们将我们的方法扩展到有更多语言的标注数据 disponiblesituation，并提议使用元素减法运算将PEFT模块在相关语言上训练。实验结果显示，我们的方法是一种有效的策略，可以在 minimal training PEFT模块的情况下获得顺性的提升。
</details></li>
</ul>
<hr>
<h2 id="Pinpoint-Not-Criticize-Refining-Large-Language-Models-via-Fine-Grained-Actionable-Feedback"><a href="#Pinpoint-Not-Criticize-Refining-Large-Language-Models-via-Fine-Grained-Actionable-Feedback" class="headerlink" title="Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback"></a>Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09336">http://arxiv.org/abs/2311.09336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag</li>
<li>for: 提高文本生成质量</li>
<li>methods: 使用细化的行动反馈进行迭代优化</li>
<li>results: 在三个文本生成任务上（机器翻译、长形问答和主题概要）观察到0.8和0.7 MetricX的提高，以及4.5和1.8 ROUGE-L的提高，并且通过我们的模拟热释算法进行优化，再次提高文本质量。<details>
<summary>Abstract</summary>
Recent improvements in text generation have leveraged human feedback to improve the quality of the generated output. However, human feedback is not always available, especially during inference. In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location and severity level that are predicted by a learned error pinpoint model for iterative refinement. FITO starts with an initial output, then iteratively incorporates the feedback via a refinement model that generates an improved output conditioned on the feedback. Given the uncertainty of consistent refined samples at iterative steps, we formulate iterative refinement into a local search problem and develop a simulated annealing based algorithm that balances exploration of the search space and optimization for output quality. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at long form QA and topic summarization respectively, with a single iteration of refinement. With our simulated annealing algorithm, we see further quality improvements, including up to 1.7 MetricX improvements over the baseline approach.
</details>
<details>
<summary>摘要</summary>
最近的文本生成技术进步有利用人类反馈来提高生成输出质量。然而，人类反馈不总是可用，特别是在推理过程中。在这项工作中，我们提出了一种基于报错类型、报错位置和严重程度的权重学习模型来进行推理时间优化方法（FITO）。FITO从初始输出开始，然后通过一个改进模型来逐步integrate反馈，以生成基于反馈的改进输出。由于不确定的精细修改样本的不一致，我们将Iterative refinement转化为本地搜索问题，并开发了一种基于随机熔化的算法来寻找优质输出。我们在机器翻译、长文问答和主题概要三个文本生成任务上进行了实验，并观察到了0.8和0.7 MetricX的提升，以及4.5和1.8 ROUGE-L的提升。通过我们的随机熔化算法，我们还观察到了进一步的质量提升，包括最多1.7 MetricX的提升。
</details></li>
</ul>
<hr>
<h2 id="Mind’s-Mirror-Distilling-Self-Evaluation-Capability-and-Comprehensive-Thinking-from-Large-Language-Models"><a href="#Mind’s-Mirror-Distilling-Self-Evaluation-Capability-and-Comprehensive-Thinking-from-Large-Language-Models" class="headerlink" title="Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models"></a>Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09214">http://arxiv.org/abs/2311.09214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan Chen, Xuming Hu, Hongxia Xu, Jintai Chen, Jian Wu</li>
<li>for: 提高小语言模型（SLM）的性能，减少erroneous reasoning和hallucinations问题。</li>
<li>methods: 提出了一种两重方法：首先，引入了一种新的自我评估能力储存在LLMs中的分布式方法，以减少erroneous reasoning和hallucinations问题；其次，提出了一种多种链条思维和自我评估 парадигмы的完整储存过程，以确保更加全面和坚实地将知识传递到SLMs中。</li>
<li>results: 实验结果表明，我们的方法可以显著提高储存过程中的SLMs性能，并且突出了开发更小的模型，更加接近人类认知的道路。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable advancements in the field of natural language processing. However, the sheer scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained contexts. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still carry over flawed reasoning or hallucinations inherited from their LLM counterparts. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability inherent in LLMs into SLMs, which aims to mitigate the adverse effects of erroneous reasoning and reduce hallucinations. Second, we advocate for a comprehensive distillation process that incorporates multiple distinct chain-of-thought and self-evaluation paradigms and ensures a more holistic and robust knowledge transfer into SLMs. Experiments on three NLP benchmarks demonstrate that our method significantly improves the performance of distilled SLMs and sheds light on the path towards developing smaller models closely aligned with human cognition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We introduce a novel method for distilling the self-evaluation capability of LLMs into smaller language models (SLMs), which aims to reduce the impact of flawed reasoning and hallucinations.2. We advocate for a comprehensive distillation process that incorporates multiple chain-of-thought and self-evaluation paradigms, ensuring a more holistic and robust knowledge transfer to SLMs.Experiments on three NLP benchmarks show that our method significantly improves the performance of distilled SLMs, bringing us closer to developing smaller models that align with human cognition.</details></li>
</ol>
<hr>
<h2 id="GRIM-GRaph-based-Interactive-narrative-visualization-for-gaMes"><a href="#GRIM-GRaph-based-Interactive-narrative-visualization-for-gaMes" class="headerlink" title="GRIM: GRaph-based Interactive narrative visualization for gaMes"></a>GRIM: GRaph-based Interactive narrative visualization for gaMes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09213">http://arxiv.org/abs/2311.09213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris Brockett, Bill Dolan</li>
<li>for: 这篇论文旨在帮助对话式角色扮演游戏（RPG）的强大故事创作。这些故事可能需要几年时间编写，通常需要一支大的创意团队。</li>
<li>methods: 该论文使用大型生成文本模型来帮助这个过程。它们提出了一种名为“GRIM”的图形基于交互式叙事系统，可以根据游戏设计师提供的高级叙事描述和约束生成丰富的叙事图。游戏设计师可以通过交互修改图表来生成新的子图，以保持在原始叙事和约束之内。</li>
<li>results: 该论文使用GPT-4生成分支叙事，对四个知名的故事进行了不同的上下文约束的应用。<details>
<summary>Abstract</summary>
Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The narratives of these may take years to write and typically involve a large creative team. In this work, we demonstrate the potential of large generative text models to assist this process. \textbf{GRIM}, a prototype \textbf{GR}aph-based \textbf{I}nteractive narrative visualization system for ga\textbf{M}es, generates a rich narrative graph with branching storylines that match a high-level narrative description and constraints provided by the designer. Game designers can interactively edit the graph by automatically generating new sub-graphs that fit the edits within the original narrative and constraints. We illustrate the use of \textbf{GRIM} in conjunction with GPT-4, generating branching narratives for four well-known stories with different contextual constraints.
</details>
<details>
<summary>摘要</summary>
对话式角色游戏（RPG）需要强大的故事编写。这些故事可能需要几年时间写作，通常需要大量的创意团队。在这份工作中，我们展示了大量生成文本模型可以帮助这个过程。我们提出了一个名为“GRIM”的原型，它是一个基于图表的互动式剧本视觉系统，可以根据设计师提供的高级剧本描述和约束生成丰富的剧本图。游戏设计师可以通过交互地编辑图表，生成适应修改的新子图，以保持在原始剧本和约束之间。我们使用了GPT-4生成分支剧本，并将其应用于四个著名的故事中，以示其可行性。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Chain-of-Thought-Prompting"><a href="#Contrastive-Chain-of-Thought-Prompting" class="headerlink" title="Contrastive Chain-of-Thought Prompting"></a>Contrastive Chain-of-Thought Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09277">http://arxiv.org/abs/2311.09277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/contrastive-cot">https://github.com/damo-nlp-sg/contrastive-cot</a></li>
<li>paper_authors: Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, Lidong Bing</li>
<li>for: 提高语言模型的逻辑推理能力</li>
<li>methods: 使用对比性链条思维法，提供有效和无效示范，以导引模型逻辑推理步骤，降低逻辑错误</li>
<li>results: 在逻辑推理benchmark上实现了提高语言模型逻辑推理能力的效果<details>
<summary>Abstract</summary>
Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.
</details>
<details>
<summary>摘要</summary>
尽管链条思维在提高语言模型理解方面取得了成功，但下面的过程仍然较为不准确。虽然逻辑正确性看起来是链条思维的核心，但先前的研究显示使用无效示例时的影响却是很小。此外， convential链条思维不会告诉语言模型需要避免哪些错误，这可能会导致更多的错误。因此， inspirited by humans可以从正确和错误示例中学习，我们提出了对比链条思维，以帮助模型步骤 reasoning  while reducing reasoning mistakes。为了提高泛化性，我们提出了一种自动生成对比示例的方法。我们的实验表明，对比链条思维可以作为普遍提高链条思维提示的方法。
</details></li>
</ul>
<hr>
<h2 id="TableLlama-Towards-Open-Large-Generalist-Models-for-Tables"><a href="#TableLlama-Towards-Open-Large-Generalist-Models-for-Tables" class="headerlink" title="TableLlama: Towards Open Large Generalist Models for Tables"></a>TableLlama: Towards Open Large Generalist Models for Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09206">http://arxiv.org/abs/2311.09206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun</li>
<li>for: 这篇论文旨在开发一种通用的大语言模型（LLM），用于解决多种表格基于任务。</li>
<li>methods: 该论文使用了一种新的表格数据集——TableInstruct，用于调参和评估LLM。此外，它还开发了一种基于Llama 2（7B）的开源通用模型——TableLlama，通过长 Context 挑战进行了辅助。</li>
<li>results: 在7项域内任务中，TableLlama 在7项域内任务中具有相当或更好的性能，与特定任务设计的SOTA模型相比。在6个外域数据集上，它实现了6-48个绝对点提升，显示了训练在TableInstruct上增强了模型的通用性。<details>
<summary>Abstract</summary>
Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model's generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.
</details>
<details>
<summary>摘要</summary>
全文案表（Semi-structured tables）在各方面都非常普遍。有许多任务旨在自动地解释、增强和询问表格。当前的方法frequently需要表格预训练或特殊的模型建设，或者受到特定的表格类型限制，或者假设表格和任务之间的简化关系。本文为开发开源大语言模型（LLMs）作为表格任务的通用专家而做出了第一步。为此，我们构建了一个新的数据集——TableInstruct，用于调整和评估LLMs。我们还开发了第一个开源通用模型 для表格——TableLlama，通过细化Llama 2（7B）和LongLoRA来解决长上下文挑战。我们在域内设置和域外设置下进行了实验。在7个域内任务中，TableLlama与SOTA的性能相似或更好，即使后者经常有特定的设计。在6个域外数据集上，它与基模型相比实现了6-48个绝对点胜负。这表明训练在TableInstruct上有助于提高模型的通用性。我们将开源我们的数据集和训练模型，以便未来的开发开源模型。
</details></li>
</ul>
<hr>
<h2 id="When-Is-Multilinguality-a-Curse-Language-Modeling-for-250-High-and-Low-Resource-Languages"><a href="#When-Is-Multilinguality-a-Curse-Language-Modeling-for-250-High-and-Low-Resource-Languages" class="headerlink" title="When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages"></a>When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09205">http://arxiv.org/abs/2311.09205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tylerachang/curse-of-multilinguality">https://github.com/tylerachang/curse-of-multilinguality</a></li>
<li>paper_authors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen</li>
<li>for: 这个论文的目的是研究多语言模型在各种语言中的表现。</li>
<li>methods: 该论文使用了多种方法，包括预训练10,000多种单语言和多语言模型，以及评估模型在不同语言中的表现。</li>
<li>results: 研究发现，在一定程度上添加多语言数据可以提高低资源语言模型的性能，与提高单语言数据大小相当。但是，高资源语言在多语言预训练场景下表现却一直差。随着数据大小的增加，添加多语言数据开始对所有语言的性能产生负面影响，可能是因为模型容量的限制（多语言预训练的咒语）。这些结果表明，大规模多语言预训练可能不适用于任何语言，但更定向的模型可以明显提高表现。<details>
<summary>Abstract</summary>
Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the "curse of multilinguality"). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.
</details>
<details>
<summary>摘要</summary>
多语言语模型广泛应用于扩展NLP系统到低资源语言。然而，具体证据关于多语言性对语言模型性能的影响在个语言中尚缺乏。在这里，我们预训练了10,000多语言和多语言语模型，用于超过250种语言，包括一些在NLP领域未得到充分研究的语言家族。我们评估了在每种语言中语言模型性能如何随（1）单语言数据集大小、（2）添加多语言数据集大小、（3）添加语言的语法相似度和（4）模型大小（最多45M参数）的变化。我们发现，在moderate情况下，添加多语言数据可以提高低资源语言模型性能，与单语言数据集大小的增加效果相似，最多提高33%。改进取决于添加的多语言数据的语法相似度，与词汇重叠的较小影响。然而，高资源语言在多语言预训练场景下一直表现出较差的性能，而且随着数据集大小的增加，添加多语言数据开始对低资源语言和高资源语言都造成性能下降，可能是模型容量的限制（“多语言性的诅咒”）。这些结果表明，大规模多语言预训练可能不适用于任何语言，但更专注的模型可以显著提高性能。
</details></li>
</ul>
<hr>
<h2 id="Structural-Priming-Demonstrates-Abstract-Grammatical-Representations-in-Multilingual-Language-Models"><a href="#Structural-Priming-Demonstrates-Abstract-Grammatical-Representations-in-Multilingual-Language-Models" class="headerlink" title="Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models"></a>Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09194">http://arxiv.org/abs/2311.09194</a></li>
<li>repo_url: None</li>
<li>paper_authors: James A. Michaelov, Catherine Arnett, Tyler A. Chang, Benjamin K. Bergen</li>
<li>for: 这个论文的目的是探讨人类语言模型中的 grammatical knowledge 是如何抽象的？</li>
<li>methods: 作者使用了大量语言模型测试和比较人类实验结果，以确定模型中的 grammatical knowledge 是如何抽象的。</li>
<li>results: 研究发现，大量语言模型中的 grammatical knowledge 是抽象的，并且可以在不同语言之间帮助生成文本。此外，模型中的 grammatical knowledge 与人类的 grammatical abstraction 类似。<details>
<summary>Abstract</summary>
Abstract grammatical knowledge - of parts of speech and grammatical patterns - is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.
</details>
<details>
<summary>摘要</summary>
抽象语法知识是人类语言能力的关键之一，它允许人们通过语言总结来掌握语言规则。然而，大语言模型中的语法知识是如何抽象的？我们在人类文献中发现了强有力的证据，表明大语言模型中的语法知识具有抽象性。在人类实验中，研究人员发现，当一个句子和之前的句子有相同的语法结构时，它会更加容易地被理解和生成。由于单语言实验中存在干扰因素，因此跨语言结构预期的证据更加具有抽象性。我们在大语言模型中测试了跨语言结构预期，并与人类实验结果进行比较，来检验模型中的语法知识是否具有抽象性。我们发现，大语言模型中的语法知识不仅在不同语言之间具有相似性，而且可以影响不同语言中的文本生成。这些结果表明，大语言模型中的语法知识具有人类语言能力的相似性，并且可以通过语言总结来掌握不同语言的语法规则。
</details></li>
</ul>
<hr>
<h2 id="PsyEval-A-Comprehensive-Large-Language-Model-Evaluation-Benchmark-for-Mental-Health"><a href="#PsyEval-A-Comprehensive-Large-Language-Model-Evaluation-Benchmark-for-Mental-Health" class="headerlink" title="PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health"></a>PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09189">http://arxiv.org/abs/2311.09189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoan Jin, Siyuan Chen, Mengyue Wu, Kenny Q. Zhu</li>
<li>for: 本研究旨在提供大语言模型（LLMs）在心理健康领域的评估标准，填补当前领域评估 LLMs 的缺口。</li>
<li>methods: 我们提出了一个全面的评估标准，包括六个子任务，涵盖三个维度，以系统地评估 LLMs 在心理健康领域的能力。</li>
<li>results: 实验结果显示，目前的 LLMs 在心理健康领域仍有很大的改进空间，同时也揭示了未来模型优化的潜在方向。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in utilizing large language models (LLMs) in mental health research, with studies showcasing their remarkable capabilities, such as disease detection. However, there is currently a lack of a comprehensive benchmark for evaluating the capability of LLMs in this domain. Therefore, we address this gap by introducing the first comprehensive benchmark tailored to the unique characteristics of the mental health domain. This benchmark encompasses a total of six sub-tasks, covering three dimensions, to systematically assess the capabilities of LLMs in the realm of mental health. We have designed corresponding concise prompts for each sub-task. And we comprehensively evaluate a total of eight advanced LLMs using our benchmark. Experiment results not only demonstrate significant room for improvement in current LLMs concerning mental health but also unveil potential directions for future model optimization.
</details>
<details>
<summary>摘要</summary>
近些时候，大语言模型（LLM）在心理健康研究中的应用已经引发了越来越多的关注，研究表明其拥有remarkable的能力，如疾病检测等。然而，当前心理健康领域中LLM的能力评估的标准化 bencmark 仍然缺失。因此，我们填补这个空白，引入了首个心理健康领域特有的全面性的bencmark。这个bencmark包括六个子任务，覆盖三个维度，以系统地评估 LLM 在心理健康领域的能力。我们设计了对应的简洁提示。我们也对八个高级 LLM 进行了广泛的评估。实验结果不仅表明当前 LLM 在心理健康领域存在大量的改进空间，还揭示了未来模型优化的潜在方向。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Generation-and-Evaluation-Capabilities-of-Large-Language-Models-for-Instruction-Controllable-Summarization"><a href="#Benchmarking-Generation-and-Evaluation-Capabilities-of-Large-Language-Models-for-Instruction-Controllable-Summarization" class="headerlink" title="Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization"></a>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09184">http://arxiv.org/abs/2311.09184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yale-nlp/instrusum">https://github.com/yale-nlp/instrusum</a></li>
<li>paper_authors: Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</li>
<li>for: 本研究旨在评估大型自然语言模型（LLM）在更复杂的摘要任务设定下的性能。</li>
<li>methods: 本研究使用了指令可控的文本摘要任务，其中模型输入包括源文章和自然语言的需要摘要特征。研究人员还创建了一个评估只的数据集，并对5种基于LLM的摘要系统进行了人工评估。</li>
<li>results: 研究发现，尽管LLMs已经在标准摘要任务上达到了强大的性能，但在更复杂的摘要任务设定下，LLMs仍然面临着很大的挑战。 Specifically, (1) 所有评测的LLM都会在摘要中出现事实和其他类型的错误; (2) 所有基于LLM的评估方法无法与人工评估人员一致地评估候选摘要的质量; (3) 不同的LLM在摘要生成和评估方面表现出了大的性能差异。<details>
<summary>Abstract</summary>
While large language models (LLMs) already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for the desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluation on 5 LLM-based summarization systems. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods in total. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation. We make our collected benchmark, InstruSum, publicly available to facilitate future research in this direction.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在标准化的摘要任务上达到了强大的表现，但它们在更复杂的摘要任务设置下的表现尚未得到充分研究。因此，我们对 LLM 进行了 instruction controllable 文本摘要测试，其中模型输入包括源文章和自然语言需求，要求摘要具有特定的摘要特征。为此，我们为这个任务创建了评估数据集，并对 5 个 LLM 基于系统进行了人工评估。然后，我们对 LLM 自动评估方法进行了Benchmark，使用了 4 种评估协议和 11 个 LLM，共计 40 种评估方法。我们的研究发现，instruction controllable 文本摘要仍然是 LLM 面临的挑战，因为：1. 所有评估的 LLM 都会在摘要中出现事实和其他类型的错误。2. 所有 LLM 基于的自动评估方法无法与人类评分员对候选摘要质量的评估实现强Alignment。3. 不同的 LLM 在摘要生成和评估中表现出了大的性能差距。我们将我们收集的 benchmark，InstruSum，公开发布，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="ContraDoc-Understanding-Self-Contradictions-in-Documents-with-Large-Language-Models"><a href="#ContraDoc-Understanding-Self-Contradictions-in-Documents-with-Large-Language-Models" class="headerlink" title="ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models"></a>ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09182">http://arxiv.org/abs/2311.09182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jierui Li, Vipul Raheja, Dhruv Kumar</li>
<li>for: 研究长文检测自矛盾能力</li>
<li>methods: 使用四种现有的开源和商业可用的大语言模型进行实验</li>
<li>results: GPT4表现最佳，可以超过人类的表现，但是它在需要更多细节和上下文的自矛盾检测方面存在不可靠性问题。<details>
<summary>Abstract</summary>
In recent times, large language models (LLMs) have shown impressive performance on various document-level tasks such as document classification, summarization, and question-answering. However, research on understanding their capabilities on the task of self-contradictions in long documents has been very limited. In this work, we introduce ContraDoc, the first human-annotated dataset to study self-contradictions in long documents across multiple domains, varying document lengths, self-contradictions types, and scope. We then analyze the current capabilities of four state-of-the-art open-source and commercially available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context. We release the dataset and all the code associated with the experiments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PEARL-Personalizing-Large-Language-Model-Writing-Assistants-with-Generation-Calibrated-Retrievers"><a href="#PEARL-Personalizing-Large-Language-Model-Writing-Assistants-with-Generation-Calibrated-Retrievers" class="headerlink" title="PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers"></a>PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09180">http://arxiv.org/abs/2311.09180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi</li>
<li>for: 该论文旨在提高大语言模型写作助手的质量和效率，但它们的输出往往缺乏个人化和专业知识。该论文提出了一种解决方案，即使用PEARL Writing Assistant，它是一种基于检索的语言模型写作助手，可以个性化为作者的写作风格和专业知识。</li>
<li>methods: 该论文提出了两个关键新想法来训练检索器：1）一种用于选择需要个性化的用户请求和文档，以便用于Language Model的生成个性化；2）一种可以使检索器与生成的文本之间进行匹配，以确保生成的文本能够尽可能地适应用户的写作风格和专业知识。</li>
<li>results: 该论文通过用PEARL Writing Assistant进行实验，证明了这种方法的效iveness。它可以在工作室社交媒体文章和Reddit评论等场景中生成个性化的文本，并且可以用作生成优化和性能预测。<details>
<summary>Abstract</summary>
Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author's communication style and specialized knowledge. In this paper, we address this challenge by proposing PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever. Our retriever is trained to select historic user-authored documents for prompt augmentation, such that they are likely to best personalize LLM generations for a user request. We propose two key novelties for training our retriever: 1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and 2) A scale-calibrating KL-divergence objective that ensures that our retriever closely tracks the benefit of a document for personalized generation. We demonstrate the effectiveness of PEARL in generating personalized workplace social media posts and Reddit comments. Finally, we showcase the potential of a generation-calibrated retriever to double as a performance predictor and further improve low-quality generations via LLM chaining.
</details>
<details>
<summary>摘要</summary>
强大的大语言模型已经促进了写作助手的发展，这些助手承诺可以大幅提高作文和communication的质量和效率。然而，一个阻碍有效协助的问题是大语言模型的输出没有个性化到作者的沟通风格和专业知识。在这篇论文中，我们解决这个挑战 by proposing PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever.我们的检索器是通过选择用户作者编写的历史文档来补充提示，以便它们可以最好地个性化LLM生成 для用户请求。我们提出了两个关键的新特点 для训练我们的检索器： 1）一种用于选择可以受益于个性化的用户请求和文档的训练数据选择方法; 2）一种托管KL散度目标，以确保我们的检索器准确地跟踪个性化文档的优势。我们示出了PEARL在生成个性化工作室社交媒体帖子和Reddit评论时的效果。最后，我们展示了一个生成检索器可以 doubles as a performance predictor，并通过LLM链接来进一步改善低质量生成。
</details></li>
</ul>
<hr>
<h2 id="SiRA-Sparse-Mixture-of-Low-Rank-Adaptation"><a href="#SiRA-Sparse-Mixture-of-Low-Rank-Adaptation" class="headerlink" title="SiRA: Sparse Mixture of Low Rank Adaptation"></a>SiRA: Sparse Mixture of Low Rank Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09179">http://arxiv.org/abs/2311.09179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Canoee Liu, Liangchen Luo, Jindong Chen, Lei Meng</li>
<li>for: 这个研究的目的是提高Large Language Model的 Parameters Efficient Tuning 能力，以适应下游任务。</li>
<li>methods: 这个研究使用了Sparse Mixture of Expert（SMoE）来增强LoRA的表现，并导入了一个新的专家抽掉法以减少过滤问题。</li>
<li>results: 这个研究发现SiRA可以在不同单任务和多任务设定下表现更好些，并且比LoRA和其他混合专家方法更好。<details>
<summary>Abstract</summary>
Parameter Efficient Tuning has been an prominent approach to adapt the Large Language Model to downstream tasks. Most previous works considers adding the dense trainable parameters, where all parameters are used to adapt certain task. We found this less effective empirically using the example of LoRA that introducing more trainable parameters does not help. Motivated by this we investigate the importance of leveraging "sparse" computation and propose SiRA: sparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of Expert(SMoE) to boost the performance of LoRA. Specifically it enforces the top $k$ experts routing with a capacity limit restricting the maximum number of tokens each expert can process. We propose a novel and simple expert dropout on top of gating network to reduce the over-fitting issue. Through extensive experiments, we verify SiRA performs better than LoRA and other mixture of expert approaches across different single tasks and multitask settings.
</details>
<details>
<summary>摘要</summary>
“Parameter Efficient Tuning”是一种受欢迎的方法，用于适应大型语言模型下推导任务。大多数先前的工作假设所有 Parameters 都是用来适应特定任务。但我们在 LoRA 的例子中发现，增加更多可读的 Parameters 并不会帮助。 Motivated by this, we investigate the importance of leveraging "sparse" computation and propose SiRA: sparse mixture of low rank adaption. SiRA 利用 Sparse Mixture of Expert (SMoE) 来提高 LoRA 的性能。具体来说，它强制顶几个专家路由中的范围限制每个专家可以处理的最多字元数量。我们提出了一个新的和简单的专家抽出方法，用于降低过滤问题。经过广泛的实验，我们证明 SiRA 比 LoRA 和其他混合专家方法在不同的单任务和多任务设定下表现更好。
</details></li>
</ul>
<hr>
<h2 id="CLEAN-EVAL-Clean-Evaluation-on-Contaminated-Large-Language-Models"><a href="#CLEAN-EVAL-Clean-Evaluation-on-Contaminated-Large-Language-Models" class="headerlink" title="CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models"></a>CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09154">http://arxiv.org/abs/2311.09154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, Hongyuan Lu</li>
<li>for: 评估大语言模型（LLM）的真实能力，因为数据污染可能导致模型的评估结果受到影响。</li>
<li>methods: 提出了一种新的方法——Clean-Eval，可以减少数据污染的影响，并更加准确地评估LLM。Clean-Eval使用LLM来重新表述和翻译污染数据，生成表达相同意义的不同表面形式的表达集。然后使用一种Semantic detector来筛选生成的低质量样本，将其缩小到候选集。最后，根据BLEURT分数选择候选集中的最佳表达。</li>
<li>results: 根据人工评估，最佳表达与原始污染数据具有相同的含义，但表达方式不同。所有候选表达可以组成一个新的评估标准。我们的实验表明，Clean-Eval可以减少LLM在几个不同场景下的污染数据下的评估结果的偏差。<details>
<summary>Abstract</summary>
We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT score. According to human assessment, this best candidate is semantically similar to the original contamination data but expressed differently. All candidates can form a new benchmark to evaluate the model. Our experiments illustrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.
</details>
<details>
<summary>摘要</summary>
现在是大型语言模型（LLM）的激烈竞争时代，不断推动测试数据的边界。然而，评估这些 LLM 的真实能力已成为一个困难和重要的问题，因为数据污染。这会浪费研究人员和工程师几十个时间和努力来下载和尝试这些污染的模型。为了保存我们的宝贵时间，我们提出了一种新的方法——Clean-Eval，它解决了数据污染问题，并在不同表面形式下评估 LLM。Clean-Eval 使用一个 LLM 来重新表述和反翻污染数据，生成表达相同含义，但表面形式不同的集合。然后，使用一个semantic detector来筛选生成的低质量样本，将这些样本缩小到候选集。最后，选择候选集中的最佳样本，根据 BLEURT 分数。根据人工评估，这个最佳样本与原始污染数据具有相同的含义，但表现在不同的表面形式下。所有候选样本可以组成一个新的评估标准。我们的实验表明，Clean-Eval 可以减少 LLM 在污染数据下的实际评估结果的损失，在少量学习和微调准备 scenarios 下都有显著提高。
</details></li>
</ul>
<hr>
<h2 id="Grounding-or-Guesswork-Large-Language-Models-are-Presumptive-Grounders"><a href="#Grounding-or-Guesswork-Large-Language-Models-are-Presumptive-Grounders" class="headerlink" title="Grounding or Guesswork? Large Language Models are Presumptive Grounders"></a>Grounding or Guesswork? Large Language Models are Presumptive Grounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09144">http://arxiv.org/abs/2311.09144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Shaikh, Kristina Gligorić, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, Dan Jurafsky</li>
<li>for: 本研究旨在探讨人工智能（AI）与人之间的对话中是否存在共同基础（common ground），以及AI是如何构建共同基础的。</li>
<li>methods: 本研究使用了一系列对话动作（like clarification和acknowledgment）来评估AI是否能够正确地构建共同基础。</li>
<li>results: 研究发现现有的大语言模型（LLMs）偏向假设共同基础而不使用对话动作来构建共同基础，并且通过对人类反馈（RLHF）的调教和强化学习来降低对共同基础的依赖性。<details>
<summary>Abstract</summary>
Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). In domains like teaching and emotional support, carefully constructing grounding prevents misunderstanding. However, it is unclear whether large language models (LLMs) leverage these dialogue acts in constructing common ground. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLMs use these grounding acts, simulating them taking turns from several dialogue datasets, and comparing the results to humans. We find that current LLMs are presumptive grounders, biased towards assuming common ground without using grounding acts. To understand the roots of this behavior, we examine the role of instruction tuning and reinforcement learning with human feedback (RLHF), finding that RLHF leads to less grounding. Altogether, our work highlights the need for more research investigating grounding in human-AI interaction.
</details>
<details>
<summary>摘要</summary>
Effective conversation requires common ground: a shared understanding between the participants. However, common ground does not emerge spontaneously in conversation. Speakers and listeners must work together to identify and construct a shared basis while avoiding misunderstandings. To accomplish grounding, humans rely on a range of dialogue acts, such as clarification (What do you mean?) and acknowledgment (I understand.). In domains like teaching and emotional support, carefully constructing grounding is essential to prevent misunderstandings. However, it is unclear whether large language models (LLMs) leverage these dialogue acts in constructing common ground. To address this question, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLMs use these grounding acts, simulating them taking turns from several dialogue datasets and comparing the results to humans. We find that current LLMs are presumptive grounders, biased towards assuming common ground without using grounding acts. To understand the roots of this behavior, we examine the role of instruction tuning and reinforcement learning with human feedback (RLHF), finding that RLHF leads to less grounding. Overall, our work highlights the need for more research investigating grounding in human-AI interaction.
</details></li>
</ul>
<hr>
<h2 id="RRescue-Ranking-LLM-Responses-to-Enhance-Reasoning-Over-Context"><a href="#RRescue-Ranking-LLM-Responses-to-Enhance-Reasoning-Over-Context" class="headerlink" title="RRescue: Ranking LLM Responses to Enhance Reasoning Over Context"></a>RRescue: Ranking LLM Responses to Enhance Reasoning Over Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09136">http://arxiv.org/abs/2311.09136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikun Wang, Rui Zheng, Haoming Li, Qi Zhang, Tao Gui, Fei Liu<br>for: 这篇论文的目的是提高大语言模型（LLMs）的上下文理解能力，以便更好地应用在各种任务上。methods: 这篇论文使用了一种新的应对方法，即使用排名 metrics， teach LLMs 如何对上下文地标的候选答案进行排序。results: 在使用 latest 的多文档问答数据集进行测试时，这种方法能够提高 LLMs 的上下文理解能力，并且可以通过人工标注、自适应函数或模型蒸馏来获得 partial ordering。<details>
<summary>Abstract</summary>
Effectively using a given context is paramount for large language models. A context window can include task specifications, retrieved documents, previous conversations, and even model self-reflections, functioning similarly to episodic memory. While efforts are being made to expand the context window, studies indicate that LLMs do not use their context optimally for response generation. In this paper, we present a novel approach to optimize LLMs using ranking metrics, which teaches LLMs to rank a collection of contextually-grounded candidate responses. Rather than a traditional full ordering, we advocate for a partial ordering. This is because achieving consensus on the perfect order for system responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be acquired through human labelers, heuristic functions, or model distillation. We test our system's improved contextual understanding using the latest benchmarks, including a new multi-document question answering dataset. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named RRescue, suggests a promising avenue for enhancing LLMs' contextual understanding via response ranking.
</details>
<details>
<summary>摘要</summary>
使用给定的上下文非常重要 для大型自然语言模型。上下文窗口可以包括任务规定、检索到的文档、前一次的对话和模型自我反思，功能类似于 episodic memory。然而，研究表明，大型自然语言模型并不会使用上下文最佳化回应生成。在这篇论文中，我们提出了一种新的方法，使用排名指标优化大型自然语言模型，教育它们排序一个上下文搜索的候选答案集。而不是传统的全局排序，我们支持偏好排序。这是因为在系统回应的完整排序中达成一致可能困难。我们的偏好排序更加稳定， menosensitive to noise，可以通过人类标注者、规则函数或模型泛化来获得。我们在最新的测试板上测试了我们的改进的上下文理解系统，包括一个新的多文档问答数据集。我们进行了剖析研究，了解关键因素，如如何收集候选答案，如何确定最适合的排序顺序，以及如何平衡监督精度抽象与排名指标。我们的方法，名为RRescue，提出了可能提高大型自然语言模型的上下文理解能力的有希望之路。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Neural-Machine-Translation-Models-Human-Feedback-in-Training-and-Inference"><a href="#Aligning-Neural-Machine-Translation-Models-Human-Feedback-in-Training-and-Inference" class="headerlink" title="Aligning Neural Machine Translation Models: Human Feedback in Training and Inference"></a>Aligning Neural Machine Translation Models: Human Feedback in Training and Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09132">http://arxiv.org/abs/2311.09132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Moura Ramos, Patrick Fernandes, António Farinhas, André F. T. Martins</li>
<li>for: 这个论文的目的是提高机器翻译模型的质量，使其更加接近人类生成的语言。</li>
<li>methods: 这个论文使用了人类反馈来训练RLHF技术，并在机器翻译领域使用了最小极大 bayes风险解oding和重新排序技术来提高翻译质量。</li>
<li>results: 这个论文的实验结果表明，通过使用优质度计数来筛选数据，RL在训练阶段可以具有更高的效果，并且将RL训练与重新排序技术结合使用可以提高翻译质量。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is a recent technique to improve the quality of the text generated by a language model, making it closer to what humans would generate. A core ingredient in RLHF's success in aligning and improving large language models (LLMs) is its reward model, trained using human feedback on model outputs. In machine translation (MT), where metrics trained from human annotations can readily be used as reward models, recent methods using minimum Bayes risk decoding and reranking have succeeded in improving the final quality of translation. In this study, we comprehensively explore and compare techniques for integrating quality metrics as reward models into the MT pipeline. This includes using the reward model for data filtering, during the training phase through RL, and at inference time by employing reranking techniques, and we assess the effects of combining these in a unified approach. Our experimental results, conducted across multiple translation tasks, underscore the crucial role of effective data filtering, based on estimated quality, in harnessing the full potential of RL in enhancing MT quality. Furthermore, our findings demonstrate the effectiveness of combining RL training with reranking techniques, showcasing substantial improvements in translation quality.
</details>
<details>
<summary>摘要</summary>
人类反馈学习（RLHF）是一种最近的技术，用于提高语言模型生成的文本质量，使其更加接近人类的生成。RLHF的成功之一是其使用人类反馈来训练奖励模型，从而使大型语言模型（LLM）更加与人类语言相似。在机器翻译（MT）领域，可以 readily使用人类标注来训练度量模型的奖励模型，最近的方法使用最小极大似然解码和重新排序技术，已经在改进翻译质量中取得了成功。在这个研究中，我们全面探讨并比较了将质量度量作为奖励模型integrated到MT管道中的技术。这包括在数据过滤、训练阶段通过RL、以及在推理阶段通过重新排序技术来使用奖励模型，并评估这些技术的组合效果。我们的实验结果，在多个翻译任务上进行了评测，强调了有效的数据过滤对RL的潜在力量的激发，并证明了将RL训练与重新排序技术相结合的效果。
</details></li>
</ul>
<hr>
<h2 id="Social-Meme-ing-Measuring-Linguistic-Variation-in-Memes"><a href="#Social-Meme-ing-Measuring-Linguistic-Variation-in-Memes" class="headerlink" title="Social Meme-ing: Measuring Linguistic Variation in Memes"></a>Social Meme-ing: Measuring Linguistic Variation in Memes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09130">http://arxiv.org/abs/2311.09130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naitian/semantic-memes">https://github.com/naitian/semantic-memes</a></li>
<li>paper_authors: Naitian Zhou, David Jurgens, David Bamman</li>
<li>for: 这篇论文旨在探讨Memes的社会变化，即通过计算方法来探索文本中的社会语言变化。</li>
<li>methods: 作者构建了一个计算管道，用于将个体Memes clustering到模板和semantic变量中，利用它们的多modal结构来做到这一点。</li>
<li>results: 作者通过对Reddit上的大量Memes图像使用这个计算管道，建立了一个名为\textsc{SemanticMemes}的数据集，包含3.8万个图像，并发现了社会意义的变化在Memes中，以及在这些社区内部的Memes创新和同化的模式，与之前关于written语言的发现相一致。<details>
<summary>Abstract</summary>
Much work in the space of NLP has used computational methods to explore sociolinguistic variation in text. In this paper, we argue that memes, as multimodal forms of language comprised of visual templates and text, also exhibit meaningful social variation. We construct a computational pipeline to cluster individual instances of memes into templates and semantic variables, taking advantage of their multimodal structure in doing so. We apply this method to a large collection of meme images from Reddit and make available the resulting \textsc{SemanticMemes} dataset of 3.8M images clustered by their semantic function. We use these clusters to analyze linguistic variation in memes, discovering not only that socially meaningful variation in meme usage exists between subreddits, but that patterns of meme innovation and acculturation within these communities align with previous findings on written language.
</details>
<details>
<summary>摘要</summary>
很多NLP领域的工作都使用计算方法来探索社会语言变化。在这篇论文中，我们 argue that memes，作为 Multimodal 的语言形式，也表现出社会意义的变化。我们构建了一个计算管道来将个体级别的 memes 分为模板和 semantics 变量，利用它们的 Multimodal 结构来做到这一点。我们对 Reddit 上的大量 meme 图片集 applies 这种方法，并将结果作为 \textsc{SemanticMemes} 数据集提供，包含 3.8 万个图片，按Semantic 功能进行分类。我们使用这些分类来分析 meme 的语言变化，发现不仅社会意义的 meme 使用存在 между subredits，而且在这些社区中的 meme 创新和吸收 align 与 previous 的 Written 语言发现。
</details></li>
</ul>
<hr>
<h2 id="Universal-NER-A-Gold-Standard-Multilingual-Named-Entity-Recognition-Benchmark"><a href="#Universal-NER-A-Gold-Standard-Multilingual-Named-Entity-Recognition-Benchmark" class="headerlink" title="Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark"></a>Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09122">http://arxiv.org/abs/2311.09122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Mayhew, Terra Blevins, Shuheng Liu, Marek Šuppa, Hila Gonen, Joseph Marvin Imperial, Börje F. Karlsson, Peiqin Lin, Nikola Ljubešić, LJ Miranda, Barbara Plank, Arij Riabi, Yuval Pinter</li>
<li>for: 本研究的目的是开发多种语言的名实体识别（NER）benchmark，以便实现跨语言的NER研究标准化。</li>
<li>methods: 本研究使用了一种开放的社区驱动方法，通过各种语言的名实体标注来创建了18个数据集。</li>
<li>results: 本研究提供了跨语言一致的名实体标注，并在语言内和跨语言学习环境下提供了初步的模型基线。<details>
<summary>Abstract</summary>
We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 18 datasets annotated with named entities in a cross-lingual consistent schema across 12 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We release the data, code, and fitted models to the public.
</details>
<details>
<summary>摘要</summary>
我们介绍 Universal NER (UNER)，一个开放、社区驱动的项目，旨在开发多种语言的高质量命名实体识别（NER）标准 benchmarcks。UNER v1 包含 18 个语言的命名实体，采用跨语言一致的标准化 schema，来促进和标准化多语言 NER 研究。在这篇文章中，我们详细介绍了 UNER 的数据创建和组合，并提供了在本语言和跨语言学习环境中的初步模型基线。我们将数据、代码和适应模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="R-Spin-Efficient-Speaker-and-Noise-invariant-Representation-Learning-with-Acoustic-Pieces"><a href="#R-Spin-Efficient-Speaker-and-Noise-invariant-Representation-Learning-with-Acoustic-Pieces" class="headerlink" title="R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces"></a>R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09117">http://arxiv.org/abs/2311.09117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng-Jui Chang, James Glass</li>
<li>for: 这篇论文旨在提出一种数据效率的自主超vised fine-tuning框架，用于获得Speaker和噪声不变的语音表示。</li>
<li>methods: 该框架基于学习精细的声学单元，通过Speaker-invariant clustering（Spin）来学习不同Speaker的语音特征。它解决了Spin的问题，并提高了内容表示的精度。</li>
<li>results: 该框架可以在严重扭曲的语音场景下表现出优于之前的状态艺术方法，同时具有12倍的计算资源减少。论文还提供了详细的分析，以解释如何使用精细的声学单元来改善语音编码器的训练和鲁棒性。<details>
<summary>Abstract</summary>
This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised fine-tuning framework for speaker and noise-invariant speech representations by learning discrete acoustic units with speaker-invariant clustering (Spin). R-Spin resolves Spin's issues and enhances content representations by learning to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources compared to previous state-of-the-art methods while outperforming them in severely distorted speech scenarios. This paper provides detailed analyses to show how discrete units contribute to speech encoder training and improving robustness in diverse acoustic environments.
</details>
<details>
<summary>摘要</summary>
Note:* "speaker and noise-invariant" is translated as " speaker和噪音不变" (speaker yuè yīn bù biàn)* "discrete acoustic units" is translated as "精确的声学单元" (jīng yì de shēng xué dan yuán)* "speaker-invariant clustering" is translated as " speaker不变聚类" (speaker bù biàn jù lèi)* "acoustic pieces" is translated as "声学副本" (shēng xué fù ben)* "severely distorted speech scenarios" is translated as "严重扭曲的speech场景" (yán jí gōu qū de speech chǎng jìng)
</details></li>
</ul>
<hr>
<h2 id="“We-Demand-Justice-”-Towards-Grounding-Political-Text-in-Social-Context"><a href="#“We-Demand-Justice-”-Towards-Grounding-Political-Text-in-Social-Context" class="headerlink" title="“We Demand Justice!”: Towards Grounding Political Text in Social Context"></a>“We Demand Justice!”: Towards Grounding Political Text in Social Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09106">http://arxiv.org/abs/2311.09106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajkumar Pujari, Chengfei Wu, Dan Goldwasser</li>
<li>for: 本研究旨在 Computational Setting中理解政治漫说中的ambiguous statements，并将其与实际世界 Entities、Actions和Attitudes相关联。</li>
<li>methods: 本研究使用了two challenging datasets，以及基于大型预训模型BERT、RoBERTa、GPT-3等的基eline模型。此外，还开发了基于现有的’Discourse Contextualization Framework’和’Political Actor Representation’模型。</li>
<li>results: 研究人员对 datasets和基eline模型进行了分析，以获取更多关于 Pragmatic Language Understanding 挑战的信息。<details>
<summary>Abstract</summary>
Social media discourse from US politicians frequently consists of 'seemingly similar language used by opposing sides of the political spectrum'. But often, it translates to starkly contrasting real-world actions. For instance, "We need to keep our students safe from mass shootings" may signal either "arming teachers to stop the shooter" or "banning guns to reduce mass shootings" depending on who says it and their political stance on the issue. In this paper, we define and characterize the context that is required to fully understand such ambiguous statements in a computational setting and ground them in real-world entities, actions, and attitudes. To that end, we propose two challenging datasets that require an understanding of the real-world context of the text to be solved effectively. We benchmark these datasets against baselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3, etc. Additionally, we develop and benchmark more structured baselines building upon existing 'Discourse Contextualization Framework' and 'Political Actor Representation' models. We perform analysis of the datasets and baseline predictions to obtain further insights into the pragmatic language understanding challenges posed by the proposed social grounding tasks.
</details>
<details>
<summary>摘要</summary>
社交媒体讨论由美国政治人物频繁使用"看起来类似的语言",但实际上可能表示 starkly contrasting 的real-world actions。例如，"我们需要保护学生免受枪击"可能表示" arm teachers to stop the shooter" 或 "ban guns to reduce mass shootings"，这取决于谁说这些话和他们对这个问题的政治立场。在这篇论文中，我们定义和描述了需要具备 computational setting 来完全理解这种ambiguous statements的上下文，并将其固定在 real-world entities, actions, and attitudes 上。为此，我们提出了两个具有挑战性的dataset，需要对文本的上下文有深入的理解才能解决 Effectively。我们对这些dataset进行了基于大型预训word models such as BERT, RoBERTa, GPT-3, etc.的基准测试，并开发了基于现有的'Discourse Contextualization Framework'和'Political Actor Representation'模型的更结构化基准。我们对dataset和基准预测进行分析，以获取更多的具体的语言理解挑战的信息。
</details></li>
</ul>
<hr>
<h2 id="MAVEN-Arg-Completing-the-Puzzle-of-All-in-One-Event-Understanding-Dataset-with-Event-Argument-Annotation"><a href="#MAVEN-Arg-Completing-the-Puzzle-of-All-in-One-Event-Understanding-Dataset-with-Event-Argument-Annotation" class="headerlink" title="MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation"></a>MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09105">http://arxiv.org/abs/2311.09105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li</li>
<li>for: 本研究旨在提供一个完整的事件理解 dataset，以支持事件探测、事件Argument抽象和事件关系抽象。</li>
<li>methods: 本研究使用 MAVEN dataset 进行事件探测和事件Argument抽象，并提供了一个完整的事件词汇表，包括 162 个事件类型和 612 个 Argument 角色。</li>
<li>results: 实验结果显示，MAVEN-Arg 是一个具有挑战性的 dataset，能够测试精炼 EAE 模型和大型自然语言模型 (LLM) 的能力。此外，我们还初步探索了未来事件预测的应用，使用 LLM。<details>
<summary>Abstract</summary>
Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and our code can be obtained from https://github.com/THU-KEG/MAVEN-Argument.
</details>
<details>
<summary>摘要</summary>
“理解文本中的事件是自然语言理解的核心目标，需要检测事件发生，提取事件参数，并分析事件之间的关系。然而，由于任务复杂性带来的标注挑战，一个大规模的事件理解 dataset 长时间缺 absent。在这篇论文中，我们引入 MAVEN-Arg，它将 MAVEN 数据集添加事件参数标注，成为了自然语言理解的所有过程的第一个一站式 dataset。作为 EAE Benchmark，MAVEN-Arg 提供了以下三个主要优势：（1）全面的 schema 涵盖 162 种事件类型和 612 个参数角色，均有专家写作的定义和示例；（2）庞大的数据规模，包含 98,591 个事件和 290,613 个参数，通过人工标注获得；（3）完整的标注，支持所有 EAE 任务的 variant，包括实体和非实体事件参数的文档级别标注。实验表明，MAVEN-Arg 对于 fine-tuned EAE 模型和专有大语言模型 (LLM) 都是非常困难的。此外，为了展示全站dataset 的优势，我们初步探索了未来事件预测的应用，使用 LLM。MAVEN-Arg 和我们的代码可以从 <https://github.com/THU-KEG/MAVEN-Argument> 获取。”
</details></li>
</ul>
<hr>
<h2 id="Defending-Large-Language-Models-Against-Jailbreaking-Attacks-Through-Goal-Prioritization"><a href="#Defending-Large-Language-Models-Against-Jailbreaking-Attacks-Through-Goal-Prioritization" class="headerlink" title="Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"></a>Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09096">http://arxiv.org/abs/2311.09096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-coai/jailbreakdefense_goalpriority">https://github.com/thu-coai/jailbreakdefense_goalpriority</a></li>
<li>paper_authors: Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang</li>
<li>for: 防御大语言模型（LLMs）各种攻击，特别是监禁攻击（jailbreaking attacks）。</li>
<li>methods: 提出了一种约束目标优先级的方法，用于在训练和推理阶段对攻击进行防御。</li>
<li>results: 实现了在不妨碍总体性能的情况下大幅降低监禁攻击的成功率，从66.4%降低到2.0%和68.2%降低到19.4%，并且在没有监禁样本的情况下也能够减少成功率。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks. While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of exploration into defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the inherent conflict between the goals of being helpful and ensuring safety. To counter jailbreaking attacks, we propose to integrate goal prioritization at both training and inference stages. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to 2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising general performance. Furthermore, integrating the concept of goal prioritization into the training phase reduces the ASR from 71.0% to 6.6% for LLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half, decreasing it from 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks. We hope our work could contribute to the comprehension of jailbreaking attacks and defenses, and shed light on the relationship between LLMs' capability and safety. Our code will be available at \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的能力不断提高，但这也涉及到一系列的安全隐患。虽然有很多研究利用 LLM 的弱点进行破坏攻击，但对于防御这类攻击的研究却受到了相对的少量关注。我们发现，破坏攻击的成功主要归功于 LLM 的目标冲突。为了应对破坏攻击，我们提议在训练和执行阶段都实施目标优先级。在执行阶段实施目标优先级后，破坏攻击的成功率（ASR）从原来的 66.4% 降低到 2.0%，对 ChatGPT 和 Vicuna-33B 进行了显著的改善。此外，在训练阶段添加目标优先级也使 ASR 从 71.0% 降低到 6.6%，而无需包含破坏样本。甚至在没有破坏样本的情况下，我们的方法仍然可以将 ASR 降低到 34.0%。此外，我们的研究发现，强大的 LLM 面临更大的安全隐患，但它们也拥有更大的防御能力。我们希望我们的研究可以对破坏攻击和防御提供更多的理解，并探讨 LLM 的能力和安全之间的关系。我们的代码将在 GitHub 上公开，可以在 \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority} 获取。
</details></li>
</ul>
<hr>
<h2 id="Social-Bias-Probing-Fairness-Benchmarking-for-Language-Models"><a href="#Social-Bias-Probing-Fairness-Benchmarking-for-Language-Models" class="headerlink" title="Social Bias Probing: Fairness Benchmarking for Language Models"></a>Social Bias Probing: Fairness Benchmarking for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09090">http://arxiv.org/abs/2311.09090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein</li>
<li>for: 本研究旨在探讨语言模型中社会偏见的问题，以及这些偏见对下游应用的风险。</li>
<li>methods: 该研究提出了一种新的探测语言模型社会偏见的方法，包括采集探测数据集，分析语言模型的通用关系以及社会分类、身份和刻板印象的关系。该方法还使用了一种新的准确度基准分数来衡量语言模型的公平性。</li>
<li>results: 该研究发现，较大的语言模型变体具有更高度的偏见，而且不同的宗教身份导致模型对不同身份的用户展现出最大的不同待遇。此外，该研究还发现了现有公平数据集的缺陷和局限性，并提出了一种更加全面和多样化的公平数据集。<details>
<summary>Abstract</summary>
Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged. In agreement with recent findings, we find that larger model variants exhibit a higher degree of bias. Moreover, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经显示出了多种社会偏见，这可能会导致下游的危害。虽然这些偏见的影响已经被认可，但先前的偏见评估方法受限于小型数据集上的 binary association 测试，这只是社会偏见在语言模型中的一种封闭的视图。在这篇论文中，我们提出了一种新的探测语言模型社会偏见的框架。我们收集了一个探测数据集，以分析语言模型的通用关联以及社会分类、标签和刻板印象的方向。为此，我们利用了一种新的减少公平分数。我们创建了一个大规模的比较数据集，以解决现有公平数据集中的缺陷和限制，扩展到不同的标签和刻板印象。与先前的工作相比，我们发现biases在语言模型中更加复杂，大型模型变体更加偏见。此外，我们发现表达不同宗教标签时，模型对不同标签的对待最为不平等。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Self-Disclosures-of-Use-Misuse-and-Addiction-in-Community-based-Social-Media-Posts"><a href="#Identifying-Self-Disclosures-of-Use-Misuse-and-Addiction-in-Community-based-Social-Media-Posts" class="headerlink" title="Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts"></a>Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09066">http://arxiv.org/abs/2311.09066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Yang, Tuhin Chakrabarty, Karli R Hochstatter, Melissa N Slavin, Nabila El-Bassel, Smaranda Muresan</li>
<li>for: 这个研究的目的是为了开发一种能够有效地识别受影响的患者，以预防意外的抗痛药过量服用。</li>
<li>methods: 这个研究使用了社区基于的社交媒体平台Reddit上的用户自透 leak 的信息，以及不同阶段的抗痛药使用（医疗使用、虚假使用、成瘾、恢复、重复、不使用）的2500个相关帖子，进行 Span-level 抽象解释和模型开发。</li>
<li>results: 研究发现，使用解释在模型开发中具有显著的提升作用，可以提高分类精度。但是，该研究还发现，识别抗痛药使用症状是非常Contextual和挑战性的。<details>
<summary>Abstract</summary>
In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids (https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national public health emergency (USDHHS, 2017). To more effectively prevent unintentional opioid overdoses, medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors, often acting as indicators for opioid use disorder. Towards this, we present a moderate size corpus of 2500 opioid-related posts from various subreddits spanning 6 different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum. The dataset will be made available for research on Github in the formal version.
</details>
<details>
<summary>摘要</summary>
在过去一个 décennial，美国已经失去了超过500,000名人因为投药和非法投药过量（https://www.cdc.gov/drugoverdose/epidemic/index.html），这导致了国家公共卫生紧急状况（USDHHS，2017）。为更好地预防意外投药过量，医疗人员需要更加强大和时间准确的工具，以更好地识别有风险的病人。社区基础的社交媒体平台，如Reddit，允许用户自透示自己的行为，并经常作为投药使用障碍的指示器。为了应对这一点，我们提供了2500篇关于投药的各种帖子，从6个不同阶段的投药使用中选择出来：医疗使用、滥用、成瘾、恢复、重新滥用和不使用。对每篇帖子，我们进行了span级抽象解释，并且关注其在模型开发中的作用。我们评估了多种当前最佳模型，包括一些监督学习、少量学习和零量学习的设置。实验结果和错误分析表明，识别投药使用疾病的阶段是非常Contextual和复杂的。然而，我们发现使用解释在模型化时带来了显著的分类精度提升，这demonstrates其在高度重要的域如投药使用疾病continuum中的有益作用。这个数据集将在GitHub上公开，以便研究人员进行研究。
</details></li>
</ul>
<hr>
<h2 id="Do-Localization-Methods-Actually-Localize-Memorized-Data-in-LLMs"><a href="#Do-Localization-Methods-Actually-Localize-Memorized-Data-in-LLMs" class="headerlink" title="Do Localization Methods Actually Localize Memorized Data in LLMs?"></a>Do Localization Methods Actually Localize Memorized Data in LLMs?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09060">http://arxiv.org/abs/2311.09060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Yun Chang, Jesse Thomason, Robin Jia</li>
<li>for: 这个论文研究了如何在大语言模型（LLM）中 lokalisiert（localize）一小集 neurons responsible for memorizing a given sequence.</li>
<li>methods: 这个论文使用了两种benchmarking Approach来测试 lokalisiert方法，包括INJ Benchmark和DEL Benchmark。</li>
<li>results: 这个论文发现了五种 lokalisiert方法，包括pruning-based methods，可以准确地 lokalisiertLLM中的neurons。但是，这些neurons不一定是specific to a single memorized sequence.<details>
<summary>Abstract</summary>
Large language models (LLMs) can memorize many pretrained sequences verbatim. This paper studies if we can locate a small set of neurons in LLMs responsible for memorizing a given sequence. While the concept of localization is often mentioned in prior work, methods for localization have never been systematically and directly evaluated; we address this with two benchmarking approaches. In our INJ Benchmark, we actively inject a piece of new information into a small subset of LLM weights and measure whether localization methods can identify these "ground truth" weights. In the DEL Benchmark, we study localization of pretrained data that LLMs have already memorized; while this setting lacks ground truth, we can still evaluate localization by measuring whether dropping out located neurons erases a memorized sequence from the model. We evaluate five localization methods on our two benchmarks, and both show similar rankings. All methods exhibit promising localization ability, especially for pruning-based methods, though the neurons they identify are not necessarily specific to a single memorized sequence.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）可以传递许多预训序列的内容，这篇研究论文探讨了我们可以在LLM中找到记忆一个给定序列的小量神经元。虽然对于localization的概念在先前的工作中有提及，但是对于localization的方法从未得到系统和直接的评估，我们在这篇研究中解决这个问题。我们使用INJ Benchmark和DEL Benchmark two个benchmarking方法进行评估。在INJ Benchmark中，我们 aktiveinject一小部分LLM weights中的新信息，然后衡量localization方法是否可以识别这些"真实" weights。在DEL Benchmark中，我们研究LLMs已经记忆过的预训数据的localization;虽然这个设定没有真实的对照，但我们仍然可以评估localization by measuring whether dropping out located neurons can erase a memorized sequence from the model。我们评估了五种localization方法在我们的两个benchmark上，结果显示所有方法都具有良好的localization能力，特别是针对剪裁方法，不过这些神经元可能不专门用于记忆一个单一的序列。
</details></li>
</ul>
<hr>
<h2 id="GRASP-A-novel-benchmark-for-evaluating-language-GRounding-And-Situated-Physics-understanding-in-multimodal-language-models"><a href="#GRASP-A-novel-benchmark-for-evaluating-language-GRounding-And-Situated-Physics-understanding-in-multimodal-language-models" class="headerlink" title="GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models"></a>GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09048">http://arxiv.org/abs/2311.09048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, Elia Bruni</li>
<li>for: 评估视频基于多模态语言模型的语言固定和物理理解能力</li>
<li>methods: 使用 Unity 模拟环境，采用两个级别的评估方法：第一级测试语言固定，第二级测试物理理解原则，如物体残留和连续性</li>
<li>results: 现有多模态语言模型存在语言固定和物理理解的显著缺陷，GRASP benchmark 可以帮助评估未来模型的发展。<details>
<summary>Abstract</summary>
This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The initial level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of 'Intuitive Physics' principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in current models' language grounding and intuitive physics. These identified limitations underline the importance of benchmarks like GRASP to monitor the progress of future models in developing these competencies.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "video-based" is translated as "视频基于的" (video-based)* "multimodal" is translated as "多Modal" (multimodal)* "large language models" is translated as "大型语言模型" (large language models)* "Unity simulations" is translated as "Unity simulations" (no change)* "language grounding" is translated as "语言固定" (language grounding)* "intuitive physics" is translated as "直觉物理" (intuitive physics)* "object permanence" is translated as "物体存在性" (object permanence)* "continuity" is translated as "连续性" (continuity)
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Potential-of-Large-Language-Models-in-Computational-Argumentation"><a href="#Exploring-the-Potential-of-Large-Language-Models-in-Computational-Argumentation" class="headerlink" title="Exploring the Potential of Large Language Models in Computational Argumentation"></a>Exploring the Potential of Large Language Models in Computational Argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09022">http://arxiv.org/abs/2311.09022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/llm-argumentation">https://github.com/damo-nlp-sg/llm-argumentation</a></li>
<li>paper_authors: Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing</li>
<li>for: 这项研究的目的是评估大语言模型（LLMs）在计算论证领域的表现，包括逻辑推理、论证分析和对话管理等。</li>
<li>methods: 这项研究使用了多种计算论证任务，包括论证挖掘和论证生成等，以及对多种存在的开源数据集进行标准化。</li>
<li>results: 研究发现LLMs在大多数数据集上表现出色，能够快速学习和掌握计算论证任务，但也存在一些限制和挑战，如逻辑推理和对话管理等。<details>
<summary>Abstract</summary>
Computational argumentation has become an essential tool in various fields, including artificial intelligence, law, and public policy. It is an emerging research field in natural language processing (NLP) that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated strong abilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on various computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and few-shot settings within the realm of computational argumentation. We organize existing tasks into 6 main classes and standardise the format of 14 open-sourced datasets. In addition, we present a new benchmark dataset on counter speech generation, that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of these datasets, demonstrating their capabilities in the field of argumentation. We also highlight the limitations in evaluating computational argumentation and provide suggestions for future research directions in this field.
</details>
<details>
<summary>摘要</summary>
computational argumentation 已成为不同领域的重要工具，包括人工智能、法律和公共政策。这是自然语言处理（NLP）的一个emerging research field，吸引越来越多的关注。研究计算 argued mainly involves two types of tasks: argument mining and argument generation。由于大语言模型（LLMs）在理解上下文和生成自然语言方面表现出色，因此值得评估 LLMS 在不同的计算 argued task 上的表现。这项工作的目的是评估 ChatGPT、Flan 模型和 LLaMA2 模型在零上下文和几上下文 Setting 下的表现。我们将现有任务分为 6 个主要类别，并标准化Open-source dataset 的格式。此外，我们还提供了一个新的对话 Generation 的benchmark dataset，以全面评估 LLMs 在 argued mining 和 argued generation 方面的综合性表现。广泛的实验表明 LLMS 在大多数数据集上表现出色，证明它们在 argued 领域的能力。我们还指出了计算 argued 的评估限制，并提出了未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Task-oriented-Dialogue-A-Survey-of-Tasks-Methods-and-Future-Directions"><a href="#End-to-end-Task-oriented-Dialogue-A-Survey-of-Tasks-Methods-and-Future-Directions" class="headerlink" title="End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions"></a>End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09008">http://arxiv.org/abs/2311.09008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Libo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, Zhou Yu, Yue Zhang, Wanxiang Che, Min Li</li>
<li>for: 本研究旨在进行综述和总结现有的End-to-end Task-oriented Dialogue（EToD）研究，以便为后续研究提供一个统一的视角。</li>
<li>methods: 本文使用大量的预训练模型，特别是深度神经网络，以实现EToD研究的进步。</li>
<li>results: 本文提供了一个综述EToD研究的报告，包括现有的方法和新趋势，并提供了一个公共网站，为EToD研究人员提供最新的进展。<details>
<summary>Abstract</summary>
End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development of EToD research. The contributions of this paper can be summarized: (1) \textbf{\textit{First survey}: to our knowledge, we take the first step to present a thorough survey of this research field; (2) \textbf{\textit{New taxonomy}: we first introduce a unified perspective for EToD, including (i) \textit{Modularly EToD} and (ii) \textit{Fully EToD}; (3) \textbf{\textit{New Frontiers}: we discuss some potential frontier areas as well as the corresponding challenges, hoping to spur breakthrough research in EToD field; (4) \textbf{\textit{Abundant resources}: we build a public website\footnote{We collect the related papers, baseline projects, and leaderboards for the community at \url{https://etods.net/}.}, where EToD researchers could directly access the recent progress. We hope this work can serve as a thorough reference for the EToD research community.
</details>
<details>
<summary>摘要</summary>
End-to-end任务对话（EToD）可以直接生成回复，无需模块化训练，这在最近几年内逐渐受到欢迎。深度神经网络的发展，特别是大型预训练模型的成功使用，进一步推动了EToD研究的进步。在这篇论文中，我们提供了一份系统性的回顾和总结，旨在总结现有的方法和最新的趋势，以推动EToD研究的发展。本文的贡献包括：1. 首次survey：我们知道的情况下，我们是第一个对这个研究领域进行了全面的survey。2. 新分类：我们首先引入了EToD的一种统一视角，包括（i）模块性EToD和（ii）完全EToD。3. 新前ier：我们讨论了一些潜在的前沿领域，以及相应的挑战，希望通过这些讨论激发EToD领域的突破性研究。4. 丰富资源：我们建立了一个公共网站（https://etods.net/）， гдеEToD研究人员可以直接访问最新的进展。我们希望这项工作可以 serves as a comprehensive reference for EToD研究社区。
</details></li>
</ul>
<hr>
<h2 id="Data-Similarity-is-Not-Enough-to-Explain-Language-Model-Performance"><a href="#Data-Similarity-is-Not-Enough-to-Explain-Language-Model-Performance" class="headerlink" title="Data Similarity is Not Enough to Explain Language Model Performance"></a>Data Similarity is Not Enough to Explain Language Model Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09006">http://arxiv.org/abs/2311.09006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gyauney/data-similarity-is-not-enough">https://github.com/gyauney/data-similarity-is-not-enough</a></li>
<li>paper_authors: Gregory Yauney, Emily Reif, David Mimno</li>
<li>for: 这个论文主要用于探讨语言模型在下游任务中的表现是如何受到预训练数据和任务数据之间的互动影响。</li>
<li>methods: 这个论文使用了多种类型的相似度度量（包括嵌入-, 字元-和模型基于的相似度）来评估语言模型在不同下游任务中的表现。</li>
<li>results: 研究发现，在多语言任务中，相似度度量与语言模型的表现有正相关关系，但在其他的benchmark中，相似度度量之间并不存在相似的相关性，甚至在准确率和相似度度量之间也没有相似的相关性。这说明预训练数据和下游任务之间的关系更加复杂。<details>
<summary>Abstract</summary>
Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model's pretraining data is assumed to be easier for that model. We test whether distributional and example-specific similarity measures (embedding-, token- and model-based) correlate with language model performance through a large-scale comparison of the Pile and C4 pretraining datasets with downstream benchmarks. Similarity correlates with performance for multilingual datasets, but in other benchmarks, we surprisingly find that similarity metrics are not correlated with accuracy or even each other. This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.
</details>
<details>
<summary>摘要</summary>
大型语言模型在许多 pero 不是所有下游任务中表现高。模型与下游任务数据之间的互动通常被假设为决定这种差异：一个与模型预训练数据更相似的任务被认为是更容易的 для该模型。我们通过大规模比较PILE和C4预训练集与下游标准准确率之间的相似性度量（嵌入-, 字符-和模型-基于的相似度）和语言模型表现的关系。在多语言任务上，相似度度量和表现之间存在相关性，但在其他benchmark上，我们意外地发现，相似度度量并不相关或者甚至相互隔离。这表明了预训练数据和下游任务之间的关系比较复杂。
</details></li>
</ul>
<hr>
<h2 id="Factcheck-GPT-End-to-End-Fine-Grained-Document-Level-Fact-Checking-and-Correction-of-LLM-Output"><a href="#Factcheck-GPT-End-to-End-Fine-Grained-Document-Level-Fact-Checking-and-Correction-of-LLM-Output" class="headerlink" title="Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output"></a>Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09000">http://arxiv.org/abs/2311.09000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxiaw/factcheck-gpt">https://github.com/yuxiaw/factcheck-gpt</a></li>
<li>paper_authors: Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, Preslav Nakov</li>
<li>for: 验证大语言模型输出的事实准确性</li>
<li>methods: 使用多Stage注解方案和自动结果 incorporation</li>
<li>results: 实验显示 FactTool、FactScore 和 Perplexity.ai 只能 Identify false claims 的 F1 为 0.53<details>
<summary>Abstract</summary>
The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We design and build an annotation tool to speed up the labelling procedure and ease the workload of raters. It allows flexible incorporation of automatic results in any stage, e.g. automatically-retrieved evidence. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims with the best F1=0.53. Annotation tool, benchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用大型自然语言模型（LLM）在各种实际应用场景中的使用增长，需要验证其输出的事实准确性的机制。在这种工作中，我们提出了一种涵盖所有阶段的综合答案，用于标注 LLM 生成的响应中的事实准确性，并设计了一个多阶段标注方案，以便对 LLM 输出中的可靠性和事实不一致进行详细标注。我们开发了一个标注工具，以便加速标注过程，并为评分人减轻工作负担。它允许自动结果的包含在任何阶段中，例如自动检索的证据。我们还建立了一个开放领域文档级别的事实准确性指标，分为三级精度：声明、句子和文档。初步实验表明，FacTool、FactScore和Perplexity.ai 在 False Claims 上的 F1 值为 0.53。标注工具、指标和代码可以在 GitHub 上找到：https://github.com/yuxiaw/Factcheck-GPT。
</details></li>
</ul>
<hr>
<h2 id="SentAlign-Accurate-and-Scalable-Sentence-Alignment"><a href="#SentAlign-Accurate-and-Scalable-Sentence-Alignment" class="headerlink" title="SentAlign: Accurate and Scalable Sentence Alignment"></a>SentAlign: Accurate and Scalable Sentence Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08982">http://arxiv.org/abs/2311.08982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/steinst/sentalign">https://github.com/steinst/sentalign</a></li>
<li>paper_authors: Steinþór Steingrímsson, Hrafn Loftsson, Andy Way</li>
<li>for: 用于处理很大的平行文档对。</li>
<li>methods: 使用分治方法和LaBSE双语句表示来评估所有可能的对齐路径，并对包含千上万句的文档进行对齐。</li>
<li>results: 在德语-法语和英语-冰岛语两个评估集上比五个其他句对齐工具表现更高，并在下游机器翻译任务上获得了更好的结果。<details>
<summary>Abstract</summary>
We present SentAlign, an accurate sentence alignment tool designed to handle very large parallel document pairs. Given user-defined parameters, the alignment algorithm evaluates all possible alignment paths in fairly large documents of thousands of sentences and uses a divide-and-conquer approach to align documents containing tens of thousands of sentences. The scoring function is based on LaBSE bilingual sentence representations. SentAlign outperforms five other sentence alignment tools when evaluated on two different evaluation sets, German-French and English-Icelandic, and on a downstream machine translation task.
</details>
<details>
<summary>摘要</summary>
我们介绍SentAlign，一种精度高的句子对齐工具，可以处理非常大的平行文档对。根据用户定义的参数，对齐算法会评估所有可能的对齐路径，并使用分治策略对包含数以千句的文档进行对齐。对齐函数基于LaBSE双语句子表示。SentAlign在两个不同的评估集上（德语-法语和英语-冰岛语）和下游机器翻译任务上都表现出色，超过了五个其他句子对齐工具。
</details></li>
</ul>
<hr>
<h2 id="Speculative-Contrastive-Decoding"><a href="#Speculative-Contrastive-Decoding" class="headerlink" title="Speculative Contrastive Decoding"></a>Speculative Contrastive Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08981">http://arxiv.org/abs/2311.08981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou</li>
<li>for: 加速大型语言模型（LLM）的推断速度，以便广泛应用。</li>
<li>methods: 使用 amateur models 预测专家模型的生成，并通过自然对比来加速推断。</li>
<li>results: 比较评估在四个标准测试集上，SCD 可以实现类似的加速因子，同时进一步改善生成质量。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown extraordinary performance in various language tasks, but high computational requirements hinder their widespread deployment. Speculative decoding, which uses amateur models to predict the generation of expert models, has been proposed as a way to accelerate LLM inference. However, speculative decoding focuses on acceleration instead of making the best use of the token distribution from amateur models. We proposed Speculative Contrastive Decoding (SCD), an accelerated decoding method leveraging the natural contrast between expert and amateur models in speculative decoding. Comprehensive evaluations on four benchmarks show that SCD can achieve similar acceleration factors as speculative decoding while further improving the generation quality as the contrastive decoding. The analysis of token probabilities further demonstrates the compatibility between speculative and contrastive decoding. Overall, SCD provides an effective approach to enhance the decoding quality of LLMs while saving computational resources.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" is translated as "大型语言模型" (dàxíng yǔyán módelǐng)* "Speculative decoding" is translated as "假设解码" ( Jiǎshè Jiěmó)* "Contrastive decoding" is translated as "对比解码" ( Duìbǐ Jiěmó)* "Token probabilities" is translated as "符号概率" (fúhào gòngsuā)* "Compatibility" is translated as "兼容" (jiānróng)
</details></li>
</ul>
<hr>
<h2 id="Improving-Large-scale-Deep-Biasing-with-Phoneme-Features-and-Text-only-Data-in-Streaming-Transducer"><a href="#Improving-Large-scale-Deep-Biasing-with-Phoneme-Features-and-Text-only-Data-in-Streaming-Transducer" class="headerlink" title="Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer"></a>Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08966">http://arxiv.org/abs/2311.08966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Qiu, Lu Huang, Boyu Li, Jun Zhang, Lu Lu, Zejun Ma</li>
<li>for: 提高流式自动语音识别（ASR）中罕见词或上下文实体的识别性能，尤其是在实际应用中。</li>
<li>methods:  combinatorial transducer 结合 rare word 的 phoneme 和文本信息，以便在 bias list 中分辨同音或同字的词语。  plus，使用 text-only 数据进行训练，以便大规模 deep biasing。</li>
<li>results: 在 LibriSpeech 数据集上，提出的方法实现了不同规模和偏好列表水平的罕见词错误率的状态天线表现。<details>
<summary>Abstract</summary>
Deep biasing for the Transducer can improve the recognition performance of rare words or contextual entities, which is essential in practical applications, especially for streaming Automatic Speech Recognition (ASR). However, deep biasing with large-scale rare words remains challenging, as the performance drops significantly when more distractors exist and there are words with similar grapheme sequences in the bias list. In this paper, we combine the phoneme and textual information of rare words in Transducers to distinguish words with similar pronunciation or spelling. Moreover, the introduction of training with text-only data containing more rare words benefits large-scale deep biasing. The experiments on the LibriSpeech corpus demonstrate that the proposed method achieves state-of-the-art performance on rare word error rate for different scales and levels of bias lists.
</details>
<details>
<summary>摘要</summary>
通过深度偏误来提高术语识别器的表现，特别是在流动自动语音识别（ASR）中，对于罕见词或上下文实体的识别是非常重要。然而，使用大规模罕见词时，深度偏误的性能会降低显著，因为更多的干扰词存在，同时有相似的文本序列在偏误列表中。在这篇论文中，我们将术语和文本信息结合在Transducer中，以便在类似的发音或拼写中分辨不同的词语。此外，通过增加包含更多罕见词的文本数据进行训练，可以改善大规模深度偏误的性能。LibriSpeech数据集的实验结果表明，我们提出的方法可以在不同的缩放和偏误列表水平上达到状态理论的表现。
</details></li>
</ul>
<hr>
<h2 id="Self-Improving-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models"><a href="#Self-Improving-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models" class="headerlink" title="Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models"></a>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08921">http://arxiv.org/abs/2311.08921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang</li>
<li>for:  investigate the possibilities of pushing the boundary of zero-shot NER with LLM via a training-free self-improving strategy.</li>
<li>methods:  utilize an unlabeled corpus to stimulate the self-learning ability of LLMs on NER, and explore various strategies to select reliable samples from the self-annotated dataset as demonstrations.</li>
<li>results:  achieve an obvious performance improvement, and there might still be space for improvement via more advanced strategy for reliable entity selection.Here’s the format you requested:for: &lt;what are the paper written for?&gt;methods: &lt;what methods the paper use?&gt;results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Exploring the application of powerful large language models (LLMs) on the fundamental named entity recognition (NER) task has drawn much attention recently. This work aims to investigate the possibilities of pushing the boundary of zero-shot NER with LLM via a training-free self-improving strategy. We propose a self-improving framework, which utilize an unlabeled corpus to stimulate the self-learning ability of LLMs on NER. First, we use LLM to make predictions on the unlabeled corpus and obtain the self-annotated data. Second, we explore various strategies to select reliable samples from the self-annotated dataset as demonstrations, considering the similarity, diversity and reliability of demonstrations. Finally, we conduct inference for the test query via in-context learning with the selected self-annotated demonstrations. Through comprehensive experimental analysis, our study yielded the following findings: (1) The self-improving framework further pushes the boundary of zero-shot NER with LLMs, and achieves an obvious performance improvement; (2) Iterative self-improving or naively increasing the size of unlabeled corpus does not guarantee improvements; (3) There might still be space for improvement via more advanced strategy for reliable entity selection.
</details>
<details>
<summary>摘要</summary>
我们提出了一个自我改进框架，利用无标注语料来刺激大语言模型（LLM）的自我学习能力。我们首先使用LLM预测无标注语料，并获得自我注释数据。然后，我们研究了多种策略来选择可靠的示例，考虑示例的相似性、多样性和可靠性。最后，我们通过在测试查询中进行推理，使用选择的自我注释示例进行学习。我们的研究发现以下结论：（1）自我改进框架可以进一步推动零shot NER的发展，并实现显著的性能提高；（2）循环自我改进或不断增加无标注语料的大小并不一定能够获得改进；（3）可能仍有更多的空间进行更高级别的可靠实体选择策略。
</details></li>
</ul>
<hr>
<h2 id="HELLaMA-LLaMA-based-Table-to-Text-Generation-by-Highlighting-the-Important-Evidence"><a href="#HELLaMA-LLaMA-based-Table-to-Text-Generation-by-Highlighting-the-Important-Evidence" class="headerlink" title="HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence"></a>HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08896">http://arxiv.org/abs/2311.08896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Bian, Xiaolei Qin, Wuhe Zou, Mengzuo Huang, Weidong Zhang</li>
<li>for: 本研究旨在提高大型语言模型在表格转文本任务中的表现，特别是避免使用公共API modify prompts，以避免潜在的成本和信息泄露。</li>
<li>methods: 我们采用了参数效率的 fine-tuning方法，使用open-source大型语言模型LLLaMA2，并在输入中注入了解释信息以提高表格转文本性能。我们的模型包括两个模块：1）表格理解器，可以快速地标识有关表格的信息，2）表格概要生成器，可以基于标识的信息生成文本。为了实现这一点，我们提出了一种搜索策略来构建解释标签 для训练表格理解器。</li>
<li>results: 在FetaQA和QTSumm数据集上，我们的方法实现了状态对标的Result，并且发现高亮输入表格可以明显提高模型的表现，同时提供有价值的解释性。<details>
<summary>Abstract</summary>
Large models have demonstrated significant progress across various domains, particularly in tasks related to text generation. In the domain of Table to Text, many Large Language Model (LLM)-based methods currently resort to modifying prompts to invoke public APIs, incurring potential costs and information leaks. With the advent of open-source large models, fine-tuning LLMs has become feasible. In this study, we conducted parameter-efficient fine-tuning on the LLaMA2 model. Distinguishing itself from previous fine-tuning-based table-to-text methods, our approach involves injecting reasoning information into the input by emphasizing table-specific row data. Our model consists of two modules: 1) a table reasoner that identifies relevant row evidence, and 2) a table summarizer that generates sentences based on the highlighted table. To facilitate this, we propose a search strategy to construct reasoning labels for training the table reasoner. On both the FetaQA and QTSumm datasets, our approach achieved state-of-the-art results. Additionally, we observed that highlighting input tables significantly enhances the model's performance and provides valuable interpretability.
</details>
<details>
<summary>摘要</summary>
大型模型在不同领域中已经展现出了显著的进步，尤其是在文本生成相关任务中。在表格到文本领域，许多大语言模型（LLM）基于方法现在通过修改提示来访问公共API，可能会导致潜在的成本和信息泄露。随着开源大型模型的出现，精细调整LLM变得可能。在这个研究中，我们实施了效率高的参数调整方法。与前期的表格到文本方法不同，我们的方法在输入中注入了理解信息。我们的模型包括两个模块：1）表格理解器，它可以识别相关的行证据；2）表格概要生成器，它可以基于高亮的表格生成句子。为了实现这一点，我们提出了一种搜索策略来构建理解标签 для训练表格理解器。在FetaQA和QTSumm数据集上，我们的方法达到了状态艺术的结果。此外，我们发现高亮输入表格可以显著提高模型的性能并提供有价值的解释性。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-legal-but-they-are-not-Making-the-case-for-a-powerful-LegalLLM"><a href="#Large-Language-Models-are-legal-but-they-are-not-Making-the-case-for-a-powerful-LegalLLM" class="headerlink" title="Large Language Models are legal but they are not: Making the case for a powerful LegalLLM"></a>Large Language Models are legal but they are not: Making the case for a powerful LegalLLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08890">http://arxiv.org/abs/2311.08890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanmay Jayakumar, Fauzan Farooqui, Luqman Farooqui</li>
<li>for: 本研究旨在衡量通用大语言模型在法律领域中的表现，以及与专门针对法律领域的模型相比。</li>
<li>methods: 本研究使用了三个通用大语言模型（ChatGPT-20b、LLaMA-2-70b和Falcon-180b），对LEDGAR子集进行零shot测试，以评估这些模型在合同提供分类任务中的表现。</li>
<li>results: 研究发现，通用大语言模型可以在大多数情况下正确地分类合同主题，但mic-F1&#x2F;mac-F1表现相比更小的法律领域模型 fine-tuned 的表现下降至19.2&#x2F;26.8%。这表明，更有力量的法律领域 LLM 的需求。<details>
<summary>Abstract</summary>
Realizing the recent advances in Natural Language Processing (NLP) to the legal sector poses challenging problems such as extremely long sequence lengths, specialized vocabulary that is usually only understood by legal professionals, and high amounts of data imbalance. The recent surge of Large Language Models (LLMs) has begun to provide new opportunities to apply NLP in the legal domain due to their ability to handle lengthy, complex sequences. Moreover, the emergence of domain-specific LLMs has displayed extremely promising results on various tasks. In this study, we aim to quantify how general LLMs perform in comparison to legal-domain models (be it an LLM or otherwise). Specifically, we compare the zero-shot performance of three general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR subset of the LexGLUE benchmark for contract provision classification. Although the LLMs were not explicitly trained on legal data, we observe that they are still able to classify the theme correctly in most cases. However, we find that their mic-F1/mac-F1 performance is up to 19.2/26.8\% lesser than smaller models fine-tuned on the legal domain, thus underscoring the need for more powerful legal-domain LLMs.
</details>
<details>
<summary>摘要</summary>
现代自然语言处理（NLP）技术在法律领域的应用存在各种挑战，包括非常长的序列长度、专业legal vocabulary和大量数据不均衡。最近的大语言模型（LLMs）已经开始为法律领域提供新的应用机会，因为它们可以处理复杂、长序列。此外，法律领域特定的LLMs在多种任务上显示出了极其出色的成绩。在本研究中，我们想要衡量一下一般用途LLMs与法律领域模型之间的比较。我们对三个一般用途LLMs（ChatGPT-20b、LLaMA-2-70b和Falcon-180b）在LexGLUEBenchmark的LEDGAR子集上进行零shot性能比较。虽然这些LLMs没有直接在法律数据上接受训练，但我们发现它们仍可以正确地分类主题。然而，我们发现它们的mic-F1/mac-F1性能相比小型在法律领域进行 fine-tuning 的模型，下降至19.2/26.8％。这说明了法律领域需要更强大的LLMs。
</details></li>
</ul>
<hr>
<h2 id="CLIMB-Curriculum-Learning-for-Infant-inspired-Model-Building"><a href="#CLIMB-Curriculum-Learning-for-Infant-inspired-Model-Building" class="headerlink" title="CLIMB: Curriculum Learning for Infant-inspired Model Building"></a>CLIMB: Curriculum Learning for Infant-inspired Model Building</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08886">http://arxiv.org/abs/2311.08886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Diehl Martinez, Zebulon Goriely, Hope McGovern, Christopher Davis, Andrew Caines, Paula Buttery, Lisa Beinborn</li>
<li>for: 这个论文是为了研究一种基于小训练集的语言模型的训练方法。</li>
<li>methods: 论文使用了三种COGNITIVE-MOTIVATED curriculum learning的方法，包括 vocabulary curriculum、data curriculum和objective curriculum。</li>
<li>results: 论文的结果表明，这些curriculum learning方法可以提高语言模型在语言评估任务中的性能，但是不同的任务和设置下的提高程度不一样。<details>
<summary>Abstract</summary>
We describe our team's contribution to the STRICT-SMALL track of the BabyLM Challenge. The challenge requires training a language model from scratch using only a relatively small training dataset of ten million words. We experiment with three variants of cognitively-motivated curriculum learning and analyze their effect on the performance of the model on linguistic evaluation tasks. In the vocabulary curriculum, we analyze methods for constraining the vocabulary in the early stages of training to simulate cognitively more plausible learning curves. In the data curriculum experiments, we vary the order of the training instances based on i) infant-inspired expectations and ii) the learning behavior of the model. In the objective curriculum, we explore different variations of combining the conventional masked language modeling task with a more coarse-grained word class prediction task to reinforce linguistic generalization capabilities. Our results did not yield consistent improvements over our own non-curriculum learning baseline across a range of linguistic benchmarks; however, we do find marginal gains on select tasks. Our analysis highlights key takeaways for specific combinations of tasks and settings which benefit from our proposed curricula. We moreover determine that careful selection of model architecture, and training hyper-parameters yield substantial improvements over the default baselines provided by the BabyLM challenge.
</details>
<details>
<summary>摘要</summary>
我们描述我们团队在STRICT-SMALL赛道上的贡献。这个赛道需要从头开始训练一个语言模型，只使用一个相对较小的训练集数据量为十万个单词。我们在语言评估任务中实践三种认知驱动的课程学习方法，并分析它们对模型的性能的影响。在词汇课程中，我们分析了在训练的早期阶段封闭词汇的方法，以模拟更加认知可能的学习曲线。在数据课程实验中，我们变化了训练实例的顺序，基于婴儿所驱动的预期和模型的学习行为。在目标课程中，我们探索了不同的推理任务和word类目的结合方法，以强化语言总体化能力。我们的结果没有在多种语言标准测试上表现出一致性的提高，但我们发现了一些选择任务和设置的特定组合可以得到有限的改进。我们的分析还提出了关键的总结，即特定任务和设置可以从我们提议的课程中得到改进。此外，我们发现了选择模型结构和训练超参数的重要性，可以在默认基eline提供的基线上实现显著改进。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Large-Language-Models-to-Learn-from-Rules"><a href="#Enabling-Large-Language-Models-to-Learn-from-Rules" class="headerlink" title="Enabling Large Language Models to Learn from Rules"></a>Enabling Large Language Models to Learn from Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08883">http://arxiv.org/abs/2311.08883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Wenkai Yang, Yankai Lin, Jie Zhou, Jirong Wen</li>
<li>for: 研究LLMs可以学习从规则中获得知识，以提高其完成不同任务的能力。</li>
<li>methods: 提出了一种新的学习方法，即使用LLMs学习从规则中提取知识，并将其编码到LLMs中。</li>
<li>results: 实验显示，使用规则学习方法可以比例例学习更加高效，尤其是当训练示例数量受限时。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown incredible performance in completing various real-world tasks. The current knowledge learning paradigm of LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples. However, the learning paradigm may not well learn those complicated rules, especially when the training examples are limited. We are inspired that humans can learn the new tasks or knowledge in another way by learning from rules. That is, humans can grasp the new tasks or knowledge quickly and generalize well given only a detailed rule and a few optional examples. Therefore, in this paper, we aim to explore the feasibility of this new learning paradigm, which encodes the rule-based knowledge into LLMs. We propose rule distillation, which first uses the strong in-context abilities of LLMs to extract the knowledge from the textual rules and then explicitly encode the knowledge into LLMs' parameters by learning from the above in-context signals produced inside the model. Our experiments show that making LLMs learn from rules by our method is much more efficient than example-based learning in both the sample size and generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Llamas-Know-What-GPTs-Don’t-Show-Surrogate-Models-for-Confidence-Estimation"><a href="#Llamas-Know-What-GPTs-Don’t-Show-Surrogate-Models-for-Confidence-Estimation" class="headerlink" title="Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation"></a>Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08877">http://arxiv.org/abs/2311.08877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaishnavi Shrivastava, Percy Liang, Ananya Kumar</li>
<li>for: 提高用户信任，大型自然语言模型（LLM）应该在错误示例上发出低自信信号，而不是诱导用户错误。</li>
<li>methods: 我们首先研究语言上的自信询问（asking an LLM for its confidence in its answer），这在12个问答 dataset上得到了70.5%的 AUC（GPT-4的平均值），比Random baseline高7%。然后我们探索使用代理自信模型（using a model where we do have probabilities to evaluate the original model’s confidence in a given question），奇怪的是，即使这些概率来自弱的模型，这种方法在12个dataset上高于语言自信的 AUC（9&#x2F;12）。我们最好的方法是将语言自信和代理模型概率相乘，这在所有12个dataset上达到了state-of-the-art的自信估计（84.6%的 GPT-4平均值）。</li>
<li>results: 我们的研究表明，使用语言上的自信询问和代理自信模型可以提高用户信任，并且可以达到state-of-the-art的自信估计水平（84.6%的 GPT-4平均值）。<details>
<summary>Abstract</summary>
To maintain user trust, large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but as of November 2023, state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets -- 7% above a random baseline) but leaves room for improvement. We then explore using a surrogate confidence model -- using a model where we do have probabilities to evaluate the original model's confidence in a given question. Surprisingly, even though these probabilities come from a different and often weaker model, this method leads to higher AUC than linguistic confidences on 9 out of 12 datasets. Our best method composing linguistic confidences and surrogate model probabilities gives state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on GPT-4).
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:为了维护用户信任，大型语言模型（LLM）应该在错误示例上透露低自信，而不是诱导用户。现有的标准方法估计自信是通过这些模型的软极概率，但截至2023年11月，现场的LLM如GPT-4和Claude-v1.3不提供这些概率。我们首先研究用语言来询问LLM的自信，这种方法在12个问答dataset上表现良好（80.5%的AUC平均值），但还有提高的空间。我们然后探索使用代理自信模型，使用一个有概率的模型来评估原始模型在某个问题上的自信度。奇怪的是，尽管这些概率来自一个不同和通常软弱的模型，但这种方法在12个dataset上高于语言自信的AUC。我们最佳方法是将语言自信和代理模型概率 Composite，在所有12个dataset上达到状态的前景（84.6%的AUC平均值在GPT-4上）。
</details></li>
</ul>
<hr>
<h2 id="OFA-A-Framework-of-Initializing-Unseen-Subword-Embeddings-for-Efficient-Large-scale-Multilingual-Continued-Pretraining"><a href="#OFA-A-Framework-of-Initializing-Unseen-Subword-Embeddings-for-Efficient-Large-scale-Multilingual-Continued-Pretraining" class="headerlink" title="OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining"></a>OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08849">http://arxiv.org/abs/2311.08849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Liu, Peiqin Lin, Mingyang Wang, Hinrich Schütze</li>
<li>for: 这个研究目的是提出一个可以快速和有效地将预训语言模型（PLM）扩展到多种语言的方法。</li>
<li>methods: 这个方法利用外部的多语言WORD embedding和将其注入到新的嵌入中，并且使用矩阵分解来取代巨大的嵌入矩阵，从而大幅削减参数数量。</li>
<li>results: 这个方法可以快速和有效地将PLM扩展到多种语言，并且在许多下游任务上表现出色，包括跨语言转换和零件跨语言转换。<details>
<summary>Abstract</summary>
Pretraining multilingual language models from scratch requires considerable computational resources and substantial training data. Therefore, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the language model, thus weakening the efficiency. To address these issues, we propose a novel framework: \textbf{O}ne \textbf{F}or \textbf{A}ll (\textbf{\textsc{Ofa}), which wisely initializes the embeddings of unseen subwords from target languages and thus can adapt a PLM to multiple languages efficiently and effectively. \textsc{Ofa} takes advantage of external well-aligned multilingual word embeddings and injects the alignment knowledge into the new embeddings. In addition, \textsc{Ofa} applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which significantly reduces the number of parameters while not sacrificing the performance. Through extensive experiments, we show models initialized by \textsc{Ofa} are efficient and outperform several baselines. \textsc{Ofa} not only accelerates the convergence of continued pretraining, which is friendly to a limited computation budget, but also improves the zero-shot crosslingual transfer on a wide range of downstream tasks. We make our code and models publicly available.
</details>
<details>
<summary>摘要</summary>
预训语言模型从scratch需要较多的计算资源和大量的训练数据。因此，一种更高效的方法是从现有的预训语言模型（PLM）中适应新语言via词汇扩展和继续预训。然而，这种方法通常随机初始化新字词的embeddings，并添加大量的embedding参数到语言模型中，从而削弱效率。为解决这些问题，我们提出了一个新的框架：\textbf{一} для \textbf{所} (\textbf{\textsc{Ofa}), which Wisely initializes the embeddings of unseen subwords from target languages and can efficiently adapt a PLM to multiple languages. \textsc{Ofa} 利用外部的多语言协调词嵌入和注入协调知识到新的嵌入。此外，\textsc{Ofa} 使用矩阵分解，将繁琐的嵌入换取两个更低维度的矩阵，这会减少参数的数量，而不是牺牲性能。通过广泛的实验，我们表明由\textsc{Ofa}  initialize的模型是高效的，并超过了多个基eline。\textsc{Ofa} 不仅加速继续预训的整合，这是计算预算有限的情况下友好的，而且提高零shot cross语言转移的性能在各种下游任务上。我们将代码和模型公开。
</details></li>
</ul>
<hr>
<h2 id="Violet-A-Vision-Language-Model-for-Arabic-Image-Captioning-with-Gemini-Decoder"><a href="#Violet-A-Vision-Language-Model-for-Arabic-Image-Captioning-with-Gemini-Decoder" class="headerlink" title="Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder"></a>Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08844">http://arxiv.org/abs/2311.08844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelrahman Mohamed, Fakhraddin Alwajih, El Moatez Billah Nagoudi, Alcides Alcoba Inciarte, Muhammad Abdul-Mageed</li>
<li>for: 本研究旨在提高非英语语言图像描述的水平，尤其是阿拉伯语。</li>
<li>methods: 本研究使用了一种新的视觉编码器和一种名为“GEMINI”的文本解码器，以实现视觉和语言组件的融合。同时，我们还提出了一种自动从英语数据集中获取数据的新方法。</li>
<li>results: 根据我们的评估结果，“Violet”模型在所有评估数据集上都表现出了较好的表现，比如在我们手动标注的数据集上达到了CIDEr分数61.2，和Flickr8k上提高了13点。<details>
<summary>Abstract</summary>
Although image captioning has a vast array of applications, it has not reached its full potential in languages other than English. Arabic, for instance, although the native language of more than 400 million people, remains largely underrepresented in this area. This is due to the lack of labeled data and powerful Arabic generative models. We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components. To train our model, we introduce a new method for automatically acquiring data from available English datasets. We also manually prepare a new dataset for evaluation. \textit{Violet} performs sizeably better than our baselines on all of our evaluation datasets. For example, it reaches a CIDEr score of $61.2$ on our manually annotated dataset and achieves an improvement of $13$ points on Flickr8k.
</details>
<details>
<summary>摘要</summary>
Although image captioning has a vast array of applications, it has not reached its full potential in languages other than English. Arabic, for instance, although the native language of more than 400 million people, remains largely underrepresented in this area. This is due to the lack of labeled data and powerful Arabic generative models. We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components. To train our model, we introduce a new method for automatically acquiring data from available English datasets. We also manually prepare a new dataset for evaluation. \textit{Violet} performs sizeably better than our baselines on all of our evaluation datasets. For example, it reaches a CIDEr score of $61.2$ on our manually annotated dataset and achieves an improvement of $13$ points on Flickr8k.Here is the translation in Traditional Chinese:尽管图像描述有着广泛的应用，但它尚未在非英语语言中得到充分开展。阿拉伯语，例如，是世界上400多万人的native语言，却在这个领域中受到了严重的欠缺标注数据和强大的阿拉伯语生成模型的限制。我们解决这个问题，提出了一个名为“Violet”的新型视觉语言模型，基于视觉编码器和Gemini文本解码器，可以维持生成流畅性，并允许视觉和语言元素的融合。为了训练我们的模型，我们提出了一新的方法，即自已有英语数据集中自动获取数据。我们还 manually实现了一个新的评估数据集。 compared to our baselines, \textit{Violet}在我们的评估数据集上表现出较好的表现，例如，在我们 manually annotated dataset上的 CIDEr 分数为61.2，在 Flickr8k 上提高了13 分。
</details></li>
</ul>
<hr>
<h2 id="Disinformation-Capabilities-of-Large-Language-Models"><a href="#Disinformation-Capabilities-of-Large-Language-Models" class="headerlink" title="Disinformation Capabilities of Large Language Models"></a>Disinformation Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08838">http://arxiv.org/abs/2311.08838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kinit-sk/disinformation-capabilities">https://github.com/kinit-sk/disinformation-capabilities</a></li>
<li>paper_authors: Ivan Vykopal, Matúš Pikuliak, Ivan Srba, Robert Moro, Dominik Macko, Maria Bielikova</li>
<li>for: 这篇论文旨在研究现有大语言模型（LLM）是否能够生成假新闻文章，以及这些假新闻文章是如何影响民主社会的。</li>
<li>methods: 该论文使用了10个LLM，对20个假新闻narraitives进行了评估。评估的方面包括生成新闻文章的质量、假新闻narraitives的同意程度、生成的安全警告等。</li>
<li>results: 研究发现，LLMs可以生成有力的新闻文章，并且往往同意危险的假新闻narraitives。<details>
<summary>Abstract</summary>
Automated disinformation generation is often listed as one of the risks of large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for democratic societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how well they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.
</details>
<details>
<summary>摘要</summary>
自动化假信息生成常被列为大语言模型（LLM）的风险之一。这种假信息潮流可能对世界各地民主社会造成巨大的影响。本文介绍了当前一代LLM对英语新闻文章的假信息生成能力的全面研究。我们使用20个假信息narraves进行评估，评估了10个LLM的表现，包括生成新闻文章的能力、与假信息narraves的倾向度、生成安全警告等。我们还评估了检测模型对这些文章的检测能力。我们结论是，LLM可以生成有力的新闻文章，同时倾向于支持危险的假信息narraves。
</details></li>
</ul>
<hr>
<h2 id="StrategyLLM-Large-Language-Models-as-Strategy-Generators-Executors-Optimizers-and-Evaluators-for-Problem-Solving"><a href="#StrategyLLM-Large-Language-Models-as-Strategy-Generators-Executors-Optimizers-and-Evaluators-for-Problem-Solving" class="headerlink" title="StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving"></a>StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08803">http://arxiv.org/abs/2311.08803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, Wai Lam</li>
<li>for: 提高Chain-of-thought（CoT）提问方法的一致性和普适性，解决现有方法的实例特定性和解释步骤的不一致问题。</li>
<li>methods: 提出了一个框架StrategylLM，利用LLM的能力解决多个任务。框架包括策略生成器、执行器、优化器和评估器等四个LLM-基于的代理人，共同生成、评估和自动选择任务中的有前途的策略。</li>
<li>results: StrategylLM在13个数据集和4个复杂任务上超过了比较基准CoT-SC，无需人工注解解决问题，包括数学逻辑（39.2% → 43.3%）、常识逻辑（70.3% → 72.5%）、算法逻辑（51.7% → 62.0%）和符号逻辑（30.0% → 79.2%）等。<details>
<summary>Abstract</summary>
Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to tackle various tasks. The framework improves generalizability by formulating general problem-solving strategies and enhances consistency by producing consistent solutions using these strategies. StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task automatically. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (39.2% $\rightarrow$ 43.3%), commonsense reasoning (70.3% $\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\rightarrow$ 62.0%), and symbolic reasoning (30.0% $\rightarrow$ 79.2%).
</details>
<details>
<summary>摘要</summary>
现有的链式思维（CoT）提问方法受到普适性和一致性的限制，因为它们常常基于特定实例的解决方案，可能无法适用于其他情况，并且缺乏任务水平的一致性在思维步骤中。为了解决这些局限性，我们提出了一个全面的框架，即策略LLM，利用LLM的能力来解决多个任务。该框架提高普适性，通过定义通用的问题解决策略，并增强一致性，通过使用这些策略生成一致的解决方案。策略LLM使用四个LLM基于的代理：策略生成器、执行器、优化器和评估器，这些代理共同工作，自动生成、评估和选择有潜力的策略，以解决给定任务。实验结果表明，策略LLM比基线CoT-SC（需要人工标注解决方案）在13个数据集上的4个复杂任务中表现出色，无需人工参与，包括数学逻辑（39.2% → 43.3%）、通情理解（70.3% → 72.5%）、算法逻辑（51.7% → 62.0%）和符号逻辑（30.0% → 79.2%）。
</details></li>
</ul>
<hr>
<h2 id="German-FinBERT-A-German-Pre-trained-Language-Model"><a href="#German-FinBERT-A-German-Pre-trained-Language-Model" class="headerlink" title="German FinBERT: A German Pre-trained Language Model"></a>German FinBERT: A German Pre-trained Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08793">http://arxiv.org/abs/2311.08793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Scherrmann</li>
<li>for: 这个研究旨在开发一个适用于金融文本数据的德国语言模型，以便为金融领域的文本分析提供有价值的工具。</li>
<li>methods: 该模型通过了全面的预训练过程，利用了包含金融报告、特别公告和新闻的大量金融相关文本资料进行训练。</li>
<li>results: 研究发现，使用德国金融BERT对金融特定数据进行预测，性能明显提高，表明该模型能够很好地捕捉金融领域特有的特征。<details>
<summary>Abstract</summary>
This study presents German FinBERT, a novel pre-trained German language model tailored for financial textual data. The model is trained through a comprehensive pre-training process, leveraging a substantial corpus comprising financial reports, ad-hoc announcements and news related to German companies. The corpus size is comparable to the data sets commonly used for training standard BERT models. I evaluate the performance of German FinBERT on downstream tasks, specifically sentiment prediction, topic recognition and question answering against generic German language models. My results demonstrate improved performance on finance-specific data, indicating the efficacy of German FinBERT in capturing domain-specific nuances. The presented findings suggest that German FinBERT holds promise as a valuable tool for financial text analysis, potentially benefiting various applications in the financial domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accelerating-Toeplitz-Neural-Network-with-Constant-time-Inference-Complexity"><a href="#Accelerating-Toeplitz-Neural-Network-with-Constant-time-Inference-Complexity" class="headerlink" title="Accelerating Toeplitz Neural Network with Constant-time Inference Complexity"></a>Accelerating Toeplitz Neural Network with Constant-time Inference Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08756">http://arxiv.org/abs/2311.08756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion">https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion</a></li>
<li>paper_authors: Zhen Qin, Yiran Zhong</li>
<li>for: 本文的目的是将 toeplitz neural network (TNN) 转换成 state space model (SSM)，以便在推理过程中实现常数复杂度。</li>
<li>methods: 本文使用了一个优化问题的形式来实现 TNN 的转换，并提供了一个关闭式解决方案。通过将目标方程转换成 Vandermonde 线性系统问题，可以使用 Discrete Fourier Transform (DFT) 高效地解决。</li>
<li>results: 在语言模型任务上进行了广泛的实验，并证明了我们的方法可以维持数值稳定性，并且比其他梯度下降解决方案更为稳定。<details>
<summary>Abstract</summary>
Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in various sequence modeling tasks. They outperform commonly used Transformer-based models while benefiting from log-linear space-time complexities. On the other hand, State Space Models (SSMs) achieve lower performance than TNNs in language modeling but offer the advantage of constant inference complexity. In this paper, we aim to combine the strengths of TNNs and SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to achieve the same constant inference complexities as SSMs. To accomplish this, we formulate the conversion process as an optimization problem and provide a closed-form solution. We demonstrate how to transform the target equation into a Vandermonde linear system problem, which can be efficiently solved using the Discrete Fourier Transform (DFT). Notably, our method requires no training and maintains numerical stability. It can be also applied to any LongConv-based model. To assess its effectiveness, we conduct extensive experiments on language modeling tasks across various settings. Additionally, we compare our method to other gradient-descent solutions, highlighting the superior numerical stability of our approach. The source code is available at https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.
</details>
<details>
<summary>摘要</summary>
托平脑网络（TNNs）在不同的序列模型任务中表现出色，而且具有对数linear空间时间复杂度的优势。然而，状态空间模型（SSMs）在语言模型任务中表现较差，但具有常数推理复杂度的优点。在这篇论文中，我们想要将TNNs转换成SSMs，以使TNNs在推理过程中具有常数推理复杂度。为实现这一目标，我们将转换过程形式化为优化问题，并提供了关闭式解决方案。我们将目标方程转换成Vandermonde线性系统问题，可以使用快速傅立做（DFT）高效地解决。值得注意的是，我们的方法不需要训练，并且保持数值稳定。此外，我们的方法可以应用于任何LongConv基于模型。为评估其效果，我们在不同的语言模型任务上进行了广泛的实验。此外，我们还与其他梯度下降解决方案进行比较，并高亮了我们的方法的数值稳定性的优势。源代码可以在https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion中获取。
</details></li>
</ul>
<hr>
<h2 id="Thread-of-Thought-Unraveling-Chaotic-Contexts"><a href="#Thread-of-Thought-Unraveling-Chaotic-Contexts" class="headerlink" title="Thread of Thought Unraveling Chaotic Contexts"></a>Thread of Thought Unraveling Chaotic Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08734">http://arxiv.org/abs/2311.08734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen</li>
<li>for: 提高大型自然语言处理模型（LLM）在文本理解和生成任务中的表现，但它们在干扰性上下文中遇到困难，导致略去一些 irrelevant 信息。</li>
<li>methods: 我们提出了一种基于人类认知过程的 “Thread of Thought”（ThoT）策略，可以系统地分析和选择相关信息。ThoT 可以与不同的 LLM 和提示技术集成， acting as a versatile “plug-and-play” 模块。</li>
<li>results: 我们在 PopQA 和 EntityQ 数据集上，以及我们自己收集的 Multi-Turn Conversation Response 数据集（MTCR）上进行了实验，发现 ThoT 可以提高理解性能，比较其他提示技术。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have ushered in a transformative era in the field of natural language processing, excelling in tasks related to text comprehension and generation. Nevertheless, they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context. In response to these challenges, we introduce the "Thread of Thought" (ThoT) strategy, which draws inspiration from human cognitive processes. ThoT systematically segments and analyzes extended contexts while adeptly selecting pertinent information. This strategy serves as a versatile "plug-and-play" module, seamlessly integrating with various LLMs and prompting techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to illustrate that ThoT significantly improves reasoning performance compared to other prompting techniques.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）已经引入了一个转变性的时代，在文本理解和生成任务中表现出色。然而，它们在受扰Context（例如，误差而不是长期 irrelevant context）时会遇到困难，导致在受扰Context中略去一些细节。为了解决这些挑战，我们介绍了“思维线索”（ThoT）策略，这种策略 Draws inspiration from human cognitive processes。ThoT系统可以系统地分析和 segments extended contexts，并选择相关信息。这种策略可以轻松地与不同的 LLMs 和提示技术集成，functioning as a versatile "plug-and-play" module。在实验中，我们使用了 PopQA 和 EntityQ 数据集，以及我们自己收集的 Multi-Turn Conversation Response 数据集（MTCR），以示 that ThoT 可以显著提高理解性能相比其他提示技术。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Emergency-Decision-making-with-Knowledge-Graphs-and-Large-Language-Models"><a href="#Enhancing-Emergency-Decision-making-with-Knowledge-Graphs-and-Large-Language-Models" class="headerlink" title="Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models"></a>Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08732">http://arxiv.org/abs/2311.08732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minze Chen, Zhenxiang Tao, Weitong Tang, Tingxin Qin, Rui Yang, Chunli Zhu</li>
<li>for: 提供可靠的紧急决策支持</li>
<li>methods: 使用知识图和大语言模型（LLM）进行证据基础的决策making</li>
<li>results: 在实际评估中，比基eline模型高于9分的 comprehensibility、准确性、简洁性和指导性，表明了该系统在不同的情况下提供了有力的决策支持。<details>
<summary>Abstract</summary>
Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals' cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in comprehensibility, accuracy, conciseness, and instructiveness from a group of emergency commanders and firefighters, demonstrating a significant improvement across various situations compared to baseline models. This work introduces a novel approach to providing reliable emergency decision support.
</details>
<details>
<summary>摘要</summary>
紧急管理需要全面的知识，而且可能超出个人认知范围。因此，在这种情况下，基于人工智能（AI）的决策是非常重要的。现在出现的大型自然语言模型（LLM）提供了一个新的方向来增强目标机器智能。然而，直接使用LLM将导致不可靠的输出，因为它们的内置问题是梦境和理解能力不足。在这项工作中，我们开发了一个系统called Enhancing Emergency decision-making with Knowledge Graph and LLM（E-KELL），它提供了基于证据的决策在多种紧急情况下。我们构建了一个结构化的紧急知识图，并使用提示链引导LLM进行图上的理解。在实际评估中，E-KELL得到了9.06、9.09、9.03和9.09的评分，分别表示可读性、准确性、简洁性和指导性，从一群紧急指挥官和消防员的视角来看，与基eline模型相比显示了 significativetransformation。这项工作介绍了一种可靠的紧急决策支持方法。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-on-Sequential-Labeling-via-Uncertainty-Transmission"><a href="#Uncertainty-Estimation-on-Sequential-Labeling-via-Uncertainty-Transmission" class="headerlink" title="Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission"></a>Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08726">http://arxiv.org/abs/2311.08726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianfeng He, Linlin Yu, Shuo Lei, Chang-Tien Lu, Feng Chen</li>
<li>for: 预测每个 tokens 的标签，如Named Entity Recognition (NER)。NER 任务的目标是从文本中提取实体，并预测它们的标签，这对信息抽取至关重要。</li>
<li>methods: 我们提出了一种Sequential Labeling Posterior Network (SLPN)，用于估计NER预测结果中的uncertainty scores。SLPN 考虑了实体之间的连接（即基于其他实体学习单个实体 embedding）以及错误扫描 случа子。</li>
<li>results: 我们的SLPN在两个 datasets 上达到了显著的提高（如MIT-Restaurant 数据集上的AUPR提高5.54点），表明了我们的方法在UE-NER任务中的效果。<details>
<summary>Abstract</summary>
Sequential labeling is a task predicting labels for each token in a sequence, such as Named Entity Recognition (NER). NER tasks aim to extract entities and predict their labels given a text, which is important in information extraction. Although previous works have shown great progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still underexplored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on two datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset.
</details>
<details>
<summary>摘要</summary>
续序标注是一个任务，predict标签 для每个序列中的每个元素，如名实体识别（NER）。NER任务的目标是从文本中提取实体并预测其标签，这是信息提取中非常重要的一部分。 although previous works have made significant progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still under-explored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on two datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset.
</details></li>
</ul>
<hr>
<h2 id="Method-for-Text-Entity-Linking-in-Power-Distribution-Scheduling-Oriented-to-Power-Distribution-Network-Knowledge-Graph"><a href="#Method-for-Text-Entity-Linking-in-Power-Distribution-Scheduling-Oriented-to-Power-Distribution-Network-Knowledge-Graph" class="headerlink" title="Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph"></a>Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08724">http://arxiv.org/abs/2311.08724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Che Wang, Bing Li, Hao Chen, Sizhe Li</li>
<li>for: 本研究旨在链接发电 dispatch文本中的实体到一个电力分配知识图库中。</li>
<li>methods: 本方法利用电力分配网络知识图和发电文本中实体的语义、音标和语法特征进行深入理解，并使用增强的LSF-SCNN模型进行实体匹配。</li>
<li>results: 对实际发电场景进行交叉验证，LSF-SCNN模型比控制模型更高的准确性在链接多种实体类型。<details>
<summary>Abstract</summary>
The proposed method for linking entities in power distribution dispatch texts to a power distribution network knowledge graph is based on a deep understanding of these networks. This method leverages the unique features of entities in both the power distribution network's knowledge graph and the dispatch texts, focusing on their semantic, phonetic, and syntactic characteristics. An enhanced model, the Lexical Semantic Feature-based Skip Convolutional Neural Network (LSF-SCNN), is utilized for effectively matching dispatch text entities with those in the knowledge graph. The efficacy of this model, compared to a control model, is evaluated through cross-validation methods in real-world power distribution dispatch scenarios. The results indicate that the LSF-SCNN model excels in accurately linking a variety of entity types, demonstrating high overall accuracy in entity linking when the process is conducted in English.
</details>
<details>
<summary>摘要</summary>
提议的方法是基于电力分配网络知识图的深刻理解，将文本中的实体链接到知识图中。这种方法利用了文本和知识图中实体的语义、声学和 sintactic 特征，并使用增强的模型——lexical semantic feature-based skip convolutional neural network (LSF-SCNN)——进行实体匹配。在实际的电力分配 dispatch 场景中，我们通过十分之一验证法评估模型的效果，结果显示，LSF-SCNN 模型在英语下可以准确地链接多种实体类型，并且在整体实体链接中达到高精度。
</details></li>
</ul>
<hr>
<h2 id="Token-Prediction-as-Implicit-Classification-to-Identify-LLM-Generated-Text"><a href="#Token-Prediction-as-Implicit-Classification-to-Identify-LLM-Generated-Text" class="headerlink" title="Token Prediction as Implicit Classification to Identify LLM-Generated Text"></a>Token Prediction as Implicit Classification to Identify LLM-Generated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08723">http://arxiv.org/abs/2311.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/markchenyutian/t5-sentinel-public">https://github.com/markchenyutian/t5-sentinel-public</a></li>
<li>paper_authors: Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究旨在identifying可能的大型自然语言模型（LLM）在文本生成中的具体应用。</li>
<li>methods: 我们采用了一种нов的方法，即将分类任务转化为下一个单词预测任务，直接使用基础LM进行 fine-tune。我们使用了Text-to-Text Transfer Transformer（T5）模型作为我们的实验室。</li>
<li>results: 我们的方法在文本分类任务中表现出色，具有简单高效的特点。此外，我们对模型提取的特征进行了解释性研究，发现它能够在不同的LLM中分辨独特的写作风格，即使没有显式的分类器。我们还收集了一个名为OpenLLMText的数据集，包含约34万个文本样本，来自人类和LLM，包括GPT3.5、PaLM、LLaMA和GPT2。<details>
<summary>Abstract</summary>
This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的方法来识别文本生成中可能存在的大语言模型（LLM）。而不是将添加额外的分类层到基础语言模型（LM）上，我们将分类任务重新归类为下一个Token预测任务，直接使得基础LM进行 fine-tune 来执行它。我们使用 Text-to-Text Transfer Transformer（T5）模型作为我们实验的脊梁。我们与使用隐藏状态进行分类的更直接方法进行比较。评估结果显示我们的方法在文本分类任务中表现出色，强调其简单性和效率。此外，对我们模型提取的特征进行解释研究表明它可以在不同的LLM中分 differentiate 不同的写作风格，即使没有显式的分类器。我们还收集了一个名为 OpenLLMText 的数据集，包含约340万个文本样本，来自人类和LLM，包括 GPT3.5、PaLM、LLaMA 和 GPT2。
</details></li>
</ul>
<hr>
<h2 id="Think-in-Memory-Recalling-and-Post-thinking-Enable-LLMs-with-Long-Term-Memory"><a href="#Think-in-Memory-Recalling-and-Post-thinking-Enable-LLMs-with-Long-Term-Memory" class="headerlink" title="Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory"></a>Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08719">http://arxiv.org/abs/2311.08719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang</li>
<li>for: 提高大语言模型（LLM）在长期人机交互中的表现，即使是重复的回忆和理解历史记忆中的高质量回答。</li>
<li>methods: 提出了一种新的内存机制called TiM（Think-in-Memory），可以让LLM agents在对话流程中维护一个演进的内存，以便在回答问题时重新回忆和更新历史记忆。</li>
<li>results: 对实际和验证对话进行质量和量тив实验，结果表明在 equipping 现有LLMs with TiM 可以显著提高它们在长期交互中的回答表现。<details>
<summary>Abstract</summary>
Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, \textit{i.e.}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.
</details>
<details>
<summary>摘要</summary>
具有记忆增强的大语言模型（LLM）在长期人机交互中表现出了惊人的表现，这主要基于迭代回忆和思考历史以生成高质量的回答。然而，这些重复的回忆和思考步骤容易产生偏见，即不同问题下的相同历史回忆时的不一致的思考结果。人类可以在记忆中保持思想，而不需要重复思考。 Drawing inspiration from this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Furthermore, we formulate the basic principles to organize the thoughts in memory based on well-established operations, such as insert, forget, and merge operations, allowing for dynamic updates and evolution of the thoughts. Additionally, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.
</details></li>
</ul>
<hr>
<h2 id="Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling"><a href="#Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling" class="headerlink" title="Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling"></a>Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08718">http://arxiv.org/abs/2311.08718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang</li>
<li>for: 本研究的目的是提高大型自然语言处理器（LLM）的可靠性、可信度和解释性，通过不同的输入简化集合来实现不确定性归一化。</li>
<li>methods: 我们提出了一种基于输入简化集合的不确定性归一化框架，而不需要训练多个模型变体。我们将输入简化集合传递给固定的 LLM，并将其相应的预测 ensemble。我们发现该框架与权值神经网络（BNN）具有同样的归一化结构。</li>
<li>results: 我们的实验表明，提出的框架可以准确地 Quantify LLM 的不确定性，并且可以在多个任务上提供可靠的结果。代码将在 <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/llm_uncertainty">https://github.com/UCSB-NLP-Chang/llm_uncertainty</a> 上公开发布。<details>
<summary>Abstract</summary>
Uncertainty decomposition refers to the task of decomposing the total uncertainty of a model into data (aleatoric) uncertainty, resulting from the inherent complexity or ambiguity of the data, and model (epistemic) uncertainty, resulting from the lack of knowledge in the model. Performing uncertainty decomposition for large language models (LLMs) is an important step toward improving the reliability, trustworthiness, and interpretability of LLMs, but this research task is very challenging and remains unresolved. The existing canonical method, Bayesian Neural Network (BNN), cannot be applied to LLMs, because BNN requires training and ensembling multiple variants of models, which is infeasible or prohibitively expensive for LLMs. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models. Rather than ensembling models with different parameters, our approach generates a set of clarifications for the input, feeds them into the fixed LLMs, and ensembles the corresponding predictions. We show that our framework shares a symmetric decomposition structure with BNN. Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks. Code will be made publicly available at https://github.com/UCSB-NLP-Chang/llm_uncertainty .
</details>
<details>
<summary>摘要</summary>
“uncertainty decomposition”指的是将模型的总不确定性 decomposes 到数据（ aleatoric）不确定性和模型（epistemic）不确定性之间。对于大型语言模型（LLMs）来说，完成 uncertainty decomposition 是提高模型的可靠性、可信度和解释性的重要步骤，但是这个研究任务很具有挑战性，尚未得到解决。现有的标准方法之一是 bayesian neural network（BNN），但是BNN 无法应用于 LLMs，因为BNN 需要训练和组合多个模型变体，这对 LLMs 来说是不可能或者非常昂贵。在这篇论文中，我们介绍了一种为 LLMs 提供 uncertainty decomposition 框架，called 输入明确集（IC），该框架可以 circumvent 需要训练新模型的需求。而不是 ensemble 多个模型的参数，我们的方法会生成一个输入的明确集，将其 feed 到固定的 LLMs 中，并 ensemble 相应的预测。我们显示了我们的框架与 BNN 具有相同的均衡分解结构。我们的实验证明了我们的框架可以在多种任务上提供准确和可靠的不确定性量化。代码将在 https://github.com/UCSB-NLP-Chang/llm_uncertainty 上公开。
</details></li>
</ul>
<hr>
<h2 id="PLUG-Leveraging-Pivot-Language-in-Cross-Lingual-Instruction-Tuning"><a href="#PLUG-Leveraging-Pivot-Language-in-Cross-Lingual-Instruction-Tuning" class="headerlink" title="PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning"></a>PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08711">http://arxiv.org/abs/2311.08711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ytyz1307zzh/plug">https://github.com/ytyz1307zzh/plug</a></li>
<li>paper_authors: Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, Francesco Barbieri</li>
<li>for: 提高大语言模型（LLMs）理解和回应多样化人类指令的能力</li>
<li>methods: 利用高资源语言（主要是英语）为低资源语言提高指令调整的方法，首先在领先语言中处理指令，然后生成回应</li>
<li>results: 对比直接在目标语言中回应，PLUG方法可以提高LLMs的指令遵从能力，增加29%的提升程度<details>
<summary>Abstract</summary>
Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency.
</details>
<details>
<summary>摘要</summary>
具有杰出表现的指令调整技术（LLM）可以帮助机器人理解和回应人类的多样化指令。虽然在高资源语言上得到了成功，但在低资源语言上的应用却遇到了挑战，这是因为LLM的基础能力在不同语言之间存在差异，这些差异来自于预训练数据的语言分布不均。为解决这个问题，我们提出了核心语言导向生成（PLUG）方法，它利用高资源语言（主要是英语）作为核心语言，以提高低资源语言中LLM的指令调整能力。它首先在核心语言中处理指令，然后在目标语言中生成响应。为评估我们的方法，我们创建了X-AlpacaEval benchmark，该benchmark包含4种语言（中文、韩语、意大利语和西班牙语）的指令，每个指令由专业翻译人员进行标注。我们的方法在LLM的指令遵循能力方面提高了29%的平均提升，比直接在目标语言中回答alone。此外，我们还进行了对其他核心语言的实验，以验证我们的方法的多样性。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Robustness-of-Dialogue-Summarization-Models-in-the-Presence-of-Naturally-Occurring-Variations"><a href="#Evaluating-Robustness-of-Dialogue-Summarization-Models-in-the-Presence-of-Naturally-Occurring-Variations" class="headerlink" title="Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations"></a>Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08705">http://arxiv.org/abs/2311.08705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankita Gupta, Chulaka Gunasekara, Hui Wan, Jatin Ganhotra, Sachindra Joshi, Marina Danilevsky</li>
<li>for: This paper aims to investigate the robustness of state-of-the-art dialogue summarization models when faced with natural variations in conversations, such as repetitions and language errors.</li>
<li>methods: The authors use two types of perturbations to simulate real-life variations in dialogues: utterance-level perturbations and dialogue-level perturbations. They evaluate the impact of these perturbations on three dimensions of robustness: consistency, saliency, and faithfulness.</li>
<li>results: The authors find that both fine-tuned and instruction-tuned models are affected by input variations, with the latter being more susceptible to dialogue-level perturbations. They also validate their findings via human evaluation and show that training with a fraction of perturbed data is insufficient to address robustness challenges with current models. The study highlights the need for better solutions to improve the robustness of dialogue summarization models.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是研究当前最佳的对话概要模型在面对自然的对话变化时的稳定性。</li>
<li>methods: 作者使用两种类型的干扰来模拟实际对话中的自然变化：utterance-level干扰和dialogue-level干扰。他们对三个稳定性维度进行评估：一致性、重要性和忠实性。</li>
<li>results: 作者发现，两种模型都受到输入变化的影响，但是被 instrucion-tuned 模型更加敏感，特别是对话级干扰。他们还通过人工评估 validate 他们的发现，并发现将一部分干扰数据用于训练并不能解决当前模型的稳定性挑战。这种研究高亮了对话概要模型的稳定性挑战，并提供了未来研究的指导。<details>
<summary>Abstract</summary>
Dialogue summarization task involves summarizing long conversations while preserving the most salient information. Real-life dialogues often involve naturally occurring variations (e.g., repetitions, hesitations) and existing dialogue summarization models suffer from performance drop on such conversations. In this study, we systematically investigate the impact of such variations on state-of-the-art dialogue summarization models using publicly available datasets. To simulate real-life variations, we introduce two types of perturbations: utterance-level perturbations that modify individual utterances with errors and language variations, and dialogue-level perturbations that add non-informative exchanges (e.g., repetitions, greetings). We conduct our analysis along three dimensions of robustness: consistency, saliency, and faithfulness, which capture different aspects of the summarization model's performance. We find that both fine-tuned and instruction-tuned models are affected by input variations, with the latter being more susceptible, particularly to dialogue-level perturbations. We also validate our findings via human evaluation. Finally, we investigate if the robustness of fine-tuned models can be improved by training them with a fraction of perturbed data and observe that this approach is insufficient to address robustness challenges with current models and thus warrants a more thorough investigation to identify better solutions. Overall, our work highlights robustness challenges in dialogue summarization and provides insights for future research.
</details>
<details>
<summary>摘要</summary>
对话概要任务 involve 简化长 conversations 中的重要信息。实际对话中经常出现自然的变化（例如重复、延迟），现有的对话概要模型在面对这些对话时会表现下降性能。在这个研究中，我们系统地研究了这些变化对现场对话概要模型的影响。为了模拟实际变化，我们引入了两种类型的干扰：语句级干扰，修改 individal 语句中的错误和语言变化，以及对话级干扰，添加非有用的交流（例如重复、祝你好）。我们在三个维度上进行分析：一致性、重要性和准确性，这些维度捕捉了不同的对话概要模型性能方面。我们发现，都 fine-tuned 和 instruction-tuned 模型都受到输入变化的影响，后者尤其是对对话级干扰的影响较大。我们还通过人工评估 validate 我们的发现。最后，我们研究了是否可以通过训练 fine-tuned 模型干扰数据的一部分来提高其Robustness，并发现这种方法不够，需要更进一步的调查来解决当前模型的Robustness挑战。总的来说，我们的工作强调对话概要模型的Robustness 挑战，并提供了未来研究的指导。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Diversity-Determines-the-Systematicity-Gap-in-VQA"><a href="#Attribute-Diversity-Determines-the-Systematicity-Gap-in-VQA" class="headerlink" title="Attribute Diversity Determines the Systematicity Gap in VQA"></a>Attribute Diversity Determines the Systematicity Gap in VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08695">http://arxiv.org/abs/2311.08695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Berlot-Attwell, A. Michael Carrell, Kumar Krishna Agrawal, Yash Sharma, Naomi Saphra</li>
<li>for: 这 paper 是研究 neural network 能够怎样总结 familiar concept 的新组合的性能。</li>
<li>methods: 这 paper 使用了一个新的 диагностические数据集 CLEVR-HOPE，以测试系统aticity gap 在视觉问答中的表现。</li>
<li>results: 结果表明，尽管增加训练数据量不会降低系统aticity gap，但是增加不同 attribute 类型的组合在未看到的组合中的训练数据多样性可以降低系统aticity gap。<details>
<summary>Abstract</summary>
The degree to which neural networks can generalize to new combinations of familiar concepts, and the conditions under which they are able to do so, has long been an open question. In this work, we study the systematicity gap in visual question answering: the performance difference between reasoning on previously seen and unseen combinations of object attributes. To test, we introduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased quantity of training data does not reduce the systematicity gap, increased training data diversity of the attributes in the unseen combination does. In all, our experiments suggest that the more distinct attribute type combinations are seen during training, the more systematic we can expect the resulting model to be.
</details>
<details>
<summary>摘要</summary>
“神经网络对新组合的 familar 概念进行总结的能力，以及这种能力在哪些条件下表现出来，是一个长期的开放问题。在这项工作中，我们研究视觉问答中的系统性差异：由于训练数据中的 attribute 组合的新视觉表现。为了测试，我们引入了一个新的诊断数据集，CLEVR-HOPE。我们发现，尽管增加训练数据量不会减少系统性差异，但是增加训练数据中 attribute 类型的多样性则可以。总之，我们的实验表明，更多的 attribute 类型组合在训练时被看到，就会更系统地表现出来。”Note that Simplified Chinese is the standard writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Routing-to-the-Expert-Efficient-Reward-guided-Ensemble-of-Large-Language-Models"><a href="#Routing-to-the-Expert-Efficient-Reward-guided-Ensemble-of-Large-Language-Models" class="headerlink" title="Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models"></a>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08692">http://arxiv.org/abs/2311.08692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou</li>
<li>for: 这个研究旨在探讨大型自然语言模型（LLM）的补充潜力，即Off-the-shelf LLMs可以在各个领域和任务上展示多元化的专业知识，以 Ensemble 方法实现更好的性能。</li>
<li>methods: 本研究提出了一种名为 Zooter 的 reward-guided routing 方法，通过将训练查询数据中的奖励转换为训练路由函数，以精确地将每个查询分配给具有相应专业知识的 LLM。此外，研究还提出了一个标签基于的标签增强方法来减少使用奖励时的不确定性。</li>
<li>results: 研究发现 Zooter 在具有26个子集的全面评chmark上显示了 Computation Efficiency，它对于排名模型排名方法而言只增加了一小部分的计算负载。此外，Zooter 在44%的任务上超越了最佳单一模型，并在大多数任务上与多个排名模型排名方法相当。<details>
<summary>Abstract</summary>
The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate it by mining latent expertise with off-the-shelf reward models. We propose Zooter, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. Zooter shows computation efficiency in inference as it introduces only a minor computation overhead of a routing function compared with reward model ranking methods. We evaluate Zooter on a comprehensive benchmark collection with 26 subsets on different domains and tasks. Zooter outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.
</details>
<details>
<summary>摘要</summary>
LLM的补偿潜力假设商业可用的LLM具有多元领域和任务的专业知识，以实现 ensemble 的更好性能。现有的 LLM 集成方法主要是根据输出奖励模型排名，导致计算负担增加。为解决这个问题，我们再次探讨 LLM 的补偿潜力，并进一步阐述它通过挖掘缺乏表达的 latent 专业知识来增强 ensemble 的性能。我们提出 Zooter，一种奖励导航方法，通过在训练查询上分配奖励来培养一个路由函数，可以准确地将每个查询分配给具有相应专业知识的 LLM。我们还将标签基本的标签增强策略与不确定性相关的噪声维持一致。 Zooter 在推理过程中具有计算效率，因为它只增加了一个路由函数的计算负担，而不是 reward 模型排名方法。我们对包含 26 个子集的全面的 benchmark 集进行评估，发现 Zooter 在 average 上超过最佳单个模型，并在 44% 的任务上排名第一。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Calibration-for-Multilingual-Question-Answering-Models"><a href="#Understanding-Calibration-for-Multilingual-Question-Answering-Models" class="headerlink" title="Understanding Calibration for Multilingual Question Answering Models"></a>Understanding Calibration for Multilingual Question Answering Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08669">http://arxiv.org/abs/2311.08669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahan Yang, Soham Dan, Dan Roth, Insup Lee</li>
<li>for: 这个论文主要研究了多语言预训练语言模型（LLMs）在问答任务上的准确性。</li>
<li>methods: 作者使用了多种问答模型设计和多种语言进行了广泛的实验，包括抽取式和生成式问答模型，以及高资源语言和低资源语言。他们还研究了不同维度的准确性，包括在内部分布、外部分布和语言转换中的准确性。</li>
<li>results: 研究发现，使用自动翻译数据增强技术可以大幅提高模型的准确性。作者还进行了一些减少实验，研究模型大小对准确性的影响，以及多语言模型与单语言模型在不同任务和语言上的比较。<details>
<summary>Abstract</summary>
Multilingual pre-trained language models are incredibly effective at Question Answering (QA), a core task in Natural Language Understanding, achieving high accuracies on several multilingual benchmarks. However, little is known about how well they are calibrated. In this paper, we study the calibration properties of several pre-trained multilingual large language models (LLMs) on a variety of question-answering tasks. We perform extensive experiments, spanning both extractive and generative QA model designs and diverse languages, spanning both high-resource and low-resource ones. We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning. We demonstrate automatically translated data augmentation as a highly effective technique to improve model calibration. We also conduct a number of ablation experiments to study the effect of model size on calibration and how multilingual models compare with their monolingual counterparts for diverse tasks and languages.
</details>
<details>
<summary>摘要</summary>
多语言预训练语言模型在问答任务上表现极其有效，在多种多语言benchmark上达到了高准确率。然而，对于这些模型的准确性衡量的知识很少。在这篇论文中，我们研究了多种预训练多语言大型语言模型（LLMs）在问答任务上的准确性性能。我们进行了广泛的实验，涵盖了抽取式和生成式问答模型设计，以及多种语言和资源水平。我们研究了不同的准确性维度，包括在适配、外围和交叉语言传递设定下的准确性。我们还 investigate了改进准确性的策略，包括后处方法和规范细化。我们示出了自动翻译数据增强为高效的准确性改进技术。此外，我们还进行了一些减少实验，研究模型大小对准确性的影响，以及多语言模型与单语言模型在多种任务和语言上的比较。
</details></li>
</ul>
<hr>
<h2 id="It-Takes-Two-to-Negotiate-Modeling-Social-Exchange-in-Online-Multiplayer-Games"><a href="#It-Takes-Two-to-Negotiate-Modeling-Social-Exchange-in-Online-Multiplayer-Games" class="headerlink" title="It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games"></a>It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08666">http://arxiv.org/abs/2311.08666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kj2013/claff-diplomacy">https://github.com/kj2013/claff-diplomacy</a></li>
<li>paper_authors: Kokil Jaidka, Hansin Ahuja, Lynnette Ng</li>
<li>for: 研究在线上战略游戏《 дипломатія》中玩家之间的互动，以了解玩家如何在游戏中谈判路径。</li>
<li>methods: 使用了聊天讯息语言模型来预测玩家对话的谈判策略，并评估其预测游戏长期和短期结果的重要性。</li>
<li>results: 发现谈判策略可以很好地预测游戏长期结果，但是短期结果则需要更多的考虑因素。此外，谈判史可以用于做出更准确的预测。<details>
<summary>Abstract</summary>
Online games are dynamic environments where players interact with each other, which offers a rich setting for understanding how players negotiate their way through the game to an ultimate victory. This work studies online player interactions during the turn-based strategy game, Diplomacy. We annotated a dataset of over 10,000 chat messages for different negotiation strategies and empirically examined their importance in predicting long- and short-term game outcomes. Although negotiation strategies can be predicted reasonably accurately through the linguistic modeling of the chat messages, more is needed for predicting short-term outcomes such as trustworthiness. On the other hand, they are essential in graph-aware reinforcement learning approaches to predict long-term outcomes, such as a player's success, based on their prior negotiation history. We close with a discussion of the implications and impact of our work. The dataset is available at https://github.com/kj2013/claff-diplomacy.
</details>
<details>
<summary>摘要</summary>
在在线游戏中，玩家之间的互动创造了一个富有的环境，用于了解玩家如何在游戏中突破困难，达到终极胜利。这项研究专注于在转战略游戏《 дипломаacia》中的在线玩家互动。我们对聊天消息集上的超过10,000个不同谈判策略进行了注释，并employs empirical方法来评估它们对游戏的长期和短期结果的重要性。虽然可以通过语言模型来预测谈判策略的准确程度，但是更需要更多的信息来预测短期的结果，如信任worthiness。然而，它们是在图像感知的强化学习方法中预测长期结果的关键因素，如玩家的成功。我们在结束时进行了关于这项工作的影响和意义的讨论。数据集可以在https://github.com/kj2013/claff-diplomacy中下载。
</details></li>
</ul>
<hr>
<h2 id="Multistage-Collaborative-Knowledge-Distillation-from-Large-Language-Models"><a href="#Multistage-Collaborative-Knowledge-Distillation-from-Large-Language-Models" class="headerlink" title="Multistage Collaborative Knowledge Distillation from Large Language Models"></a>Multistage Collaborative Knowledge Distillation from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08640">http://arxiv.org/abs/2311.08640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum</li>
<li>for: 这个论文的目的是解决 semi-supervised 序列预测任务中的 Label scarce 问题，其中 labeled 数据稀缺，同时 few-shot 提示大型自然语言模型 (LLM) 的性能不佳。</li>
<li>methods: 该论文提出了一种新的泛化方法，即 multistage collaborative knowledge distillation from an LLM (MCKD)，用于解决这类任务。MCKD 首先使用 few-shot 在上下文学习中提示 LLM 生成假标签，然后在每个阶段的泛化中使用各自的学生模型在不同的 partition 上进行训练。</li>
<li>results: 论文表明，在 CRAFT 生物医学分析任务上，3-stage MCKD 使用 50 个标签示例可以与supervised finetuning 使用 500 个标签示例匹配，并且超过提示 LLM 和 vanilla KD 的表现，提高了生物医学分析的解析 F1 值 by 7.5% 和 3.7%。<details>
<summary>Abstract</summary>
We study semi-supervised sequence prediction tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from a prompted LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we propose a new distillation method, multistage collaborative knowledge distillation from an LLM (MCKD), for such tasks. MCKD first prompts an LLM using few-shot in-context learning to produce pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of students are trained on disjoint partitions of the pseudolabeled data. Each student subsequently produces new and improved pseudolabels for the unseen partition to supervise the next round of student(s) with. We show the benefit of multistage cross-partition labeling on two constituency parsing tasks. On CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the performance of supervised finetuning with 500 examples and outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.
</details>
<details>
<summary>摘要</summary>
MCKD first prompts an LLM using few-shot in-context learning to produce pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of students are trained on disjoint partitions of the pseudolabeled data. Each student subsequently produces new and improved pseudolabels for the unseen partition to supervise the next round of student(s) with. We show the benefit of multistage cross-partition labeling on two constituency parsing tasks. On CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the performance of supervised finetuning with 500 examples and outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.
</details></li>
</ul>
<hr>
<h2 id="Formal-Proofs-as-Structured-Explanations-Proposing-Several-Tasks-on-Explainable-Natural-Language-Inference"><a href="#Formal-Proofs-as-Structured-Explanations-Proposing-Several-Tasks-on-Explainable-Natural-Language-Inference" class="headerlink" title="Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference"></a>Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08637">http://arxiv.org/abs/2311.08637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lasha Abzianidze</li>
<li>for: 提出了一种利用正式证明来提出可解释的自然语言推理（NLI）任务的方法。</li>
<li>methods: 使用可靠高性能的逻辑基础NLI系统生成的正式证明来生成可解释的NLI任务。</li>
<li>results: 通过使用生成的正式证明中的深入信息，定义了可解释的NLI任务，并按照难度定义Tasks的Difficulty。<details>
<summary>Abstract</summary>
In this position paper, we propose a way of exploiting formal proofs to put forward several explainable natural language inference (NLI) tasks. The formal proofs will be produced by a reliable and high-performing logic-based NLI system. Taking advantage of the in-depth information available in the generated formal proofs, we show how it can be used to define NLI tasks with structured explanations. The proposed tasks can be ordered according to difficulty defined in terms of the granularity of explanations. We argue that the tasks will suffer with substantially fewer shortcomings than the existing explainable NLI tasks (or datasets).
</details>
<details>
<summary>摘要</summary>
在这份位置论文中，我们提出了利用正式证明来提出多个可解释自然语言推理（NLI）任务的方法。正式证明将由可靠和高性能的逻辑基础NLI系统生成。利用生成的正式证明中的深入信息，我们显示了如何使用结构化解释来定义NLI任务。我们提议的任务可以按难度排序，定义为证明的粒度。我们认为，提出的任务会比现有的可解释NLI任务（或数据集）受到更少的缺陷。
</details></li>
</ul>
<hr>
<h2 id="DEED-Dynamic-Early-Exit-on-Decoder-for-Accelerating-Encoder-Decoder-Transformer-Models"><a href="#DEED-Dynamic-Early-Exit-on-Decoder-for-Accelerating-Encoder-Decoder-Transformer-Models" class="headerlink" title="DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models"></a>DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08623">http://arxiv.org/abs/2311.08623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Tang, Pengkai Zhu, Tian Li, Srikar Appalaraju, Vijay Mahadevan, R. Manmatha</li>
<li>for: 这篇论文主要是为了提高encoder-decoder transformer模型的推理速度，以便在视觉语言（VL）任务中提高效率。</li>
<li>methods: 这篇论文提出了一种名为Dynamic Early Exit on Decoder（DEED）的方法，具体来说是在测试过程中，在decoder层中进行步骤性早期终止，以节省computational cost。同时，这篇论文还使用了一些实用的技术，例如共享生成头和适应模组，以确保当脱离decoder层时，模型的准确性仍然保持。</li>
<li>results: 根据这篇论文的实验结果，这篇论文可以在不同的VL任务上实现30%-60%的总推理时间节省，并且与基准相比，具有相似或更高的准确性。<details>
<summary>Abstract</summary>
Encoder-decoder transformer models have achieved great success on various vision-language (VL) tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with two state-of-the-art encoder-decoder transformer models on various VL tasks. We show our approach can reduce overall inference latency by 30%-60% with comparable or even higher accuracy compared to baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>编码-解码转换器模型在视觉语言（VL）任务上实现了很大的成功，但它们受到高的推理延迟影响。通常，解码器负担了大部分延迟，因为解码器使用了自动逆向编码。为了加速推理，我们提议在解码器（DEED）中实现动态早期离开。我们构建了多出口编码器-解码器转换器模型，并在其中训练了深度监督，以便每个解码层都可以生成可信的预测。此外，我们利用了简单 yet practical的技术，包括共享生成头和适应模块，以保持精度。基于多出口模型，我们在推理中实现Step-level动态早期离开，其中模型可以根据当前层的自信率使用 fewer decoder层。由于不同的decoder层可能会在不同的推理步骤中使用，我们在需要时就计算 previous decoding step的 deeper-layer decoder feature，以确保不同步骤的特征相对适应。我们使用了两个状态对应的encoder-decoder transformer模型对多个VL任务进行评估。我们发现我们的方法可以降低总推理延迟时间30%-60%，同时保持相对或更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Question-Multiple-Answer-Text-VQA"><a href="#Multiple-Question-Multiple-Answer-Text-VQA" class="headerlink" title="Multiple-Question Multiple-Answer Text-VQA"></a>Multiple-Question Multiple-Answer Text-VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08622">http://arxiv.org/abs/2311.08622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jha1990/VQA-Multimodal-AI">https://github.com/jha1990/VQA-Multimodal-AI</a></li>
<li>paper_authors: Peng Tang, Srikar Appalaraju, R. Manmatha, Yusheng Xie, Vijay Mahadevan<br>for:多个问题和多个答案（MQMA）是一种新的文本-VQA方法，用于在encoder-decoder transformer模型中进行文本-VQA任务。methods:MQMA方法使用多个问题和多个内容作为输入，并在encoder和decoder中使用自动进程来同时预测多个答案。此外，我们还提出了一种MQMA净化预训练任务，用于教导模型将多个问题和内容与相关答案相互对应。results:我们的MQMA预训练模型在多个文本-VQA数据集上达到了最佳result，比前一个状态的方法提高了+2.5%、+1.4%、+0.6%和+1.1%的精度。<details>
<summary>Abstract</summary>
We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do text-VQA in encoder-decoder transformer models. The text-VQA task requires a model to answer a question by understanding multi-modal content: text (typically from OCR) and an associated image. To the best of our knowledge, almost all previous approaches for text-VQA process a single question and its associated content to predict a single answer. In order to answer multiple questions from the same image, each question and content are fed into the model multiple times. In contrast, our proposed MQMA approach takes multiple questions and content as input at the encoder and predicts multiple answers at the decoder in an auto-regressive manner at the same time. We make several novel architectural modifications to standard encoder-decoder transformers to support MQMA. We also propose a novel MQMA denoising pre-training task which is designed to teach the model to align and delineate multiple questions and content with associated answers. MQMA pre-trained model achieves state-of-the-art results on multiple text-VQA datasets, each with strong baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%) absolute improvements over the previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
我们提出了多问题多答案（MQMA），一种新的方法来实现编码器-解码器变换器模型中的文本-VQA任务。文本-VQA任务需要模型通过理解多Modal内容（通常是OCR提供的文本）和相关的图像来回答问题。根据我们所知，前一个方法都是处理单个问题和其相关的内容，以预测单个答案。而我们提出的MQMA方法可以同时处理多个问题和内容，并在编码器和解码器之间进行自动推导，以预测多个答案。我们在标准编码器-解码器变换器模型上进行了一些新的建模修改，以支持MQMA。我们还提出了一种MQMA减噪预训练任务，用于教育模型将多个问题和内容与相关答案进行对齐和分离。MQMA预训练模型在多个文本-VQA数据集上达到了状态的当前最佳成绩，具体来说是OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%)的绝对提升。
</details></li>
</ul>
<hr>
<h2 id="Toucan-Token-Aware-Character-Level-Language-Modeling"><a href="#Toucan-Token-Aware-Character-Level-Language-Modeling" class="headerlink" title="Toucan: Token-Aware Character Level Language Modeling"></a>Toucan: Token-Aware Character Level Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08620">http://arxiv.org/abs/2311.08620</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Fleshman, Benjamin Van Durme</li>
<li>for: 提高 caracter-level 模型的效率，不需要额外训练tokenizer。</li>
<li>methods: 提出了一种通过学习将字符表示合并成 tokens的方法，使得训练这些模型更加高效。</li>
<li>results: 与先前的工作比较，得到了无损语言模型性能的速度提升，并且发现了使用我们的动态tokenization方法可以处理更长的字符串。<details>
<summary>Abstract</summary>
Character-level language models obviate the need for separately trained tokenizers, but efficiency suffers from longer sequence lengths. Learning to combine character representations into tokens has made training these models more efficient, but they still require decoding characters individually. We propose Toucan, an augmentation to character-level models to make them "token-aware". Comparing our method to prior work, we demonstrate significant speed-ups in character generation without a loss in language modeling performance. We then explore differences between our learned dynamic tokenization of character sequences with popular fixed vocabulary solutions such as Byte-Pair Encoding and WordPiece, finding our approach leads to a greater amount of longer sequences tokenized as single items. Our project and code are available at https://nlp.jhu.edu/nuggets/.
</details>
<details>
<summary>摘要</summary>
Character-level 语言模型可以减少单独训练的 tokenizer，但是序列长度变长会导致效率下降。学习将字符表示合并到 tokens 中可以使训练这些模型更加高效，但是仍需要单独decode每个字符。我们提出了 Toucan，一种对Character-level模型进行改进，使其“字符意识”。我们与之前的工作进行比较，表明我们的方法可以在字符生成中得到显著的速度提升，而不会对语言模型性能产生影响。然后，我们探索了我们学习的动态tokenization 与常见的固定词库解决方案，如 Byte-Pair Encoding 和 WordPiece，发现我们的方法可以处理更多的更长的序列。我们的项目和代码可以在https://nlp.jhu.edu/nuggets/ 上获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-SER-Soft-Labeling-and-Data-Augmentation-for-Modeling-Temporal-Emotion-Shifts-in-Large-Scale-Multilingual-Speech"><a href="#Towards-Generalizable-SER-Soft-Labeling-and-Data-Augmentation-for-Modeling-Temporal-Emotion-Shifts-in-Large-Scale-Multilingual-Speech" class="headerlink" title="Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech"></a>Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08607">http://arxiv.org/abs/2311.08607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spaghettisystems/emotion_whisper">https://github.com/spaghettisystems/emotion_whisper</a></li>
<li>paper_authors: Mohamed Osman, Tamer Nadeem, Ghada Khoriba</li>
<li>for: 本研究旨在提高人机交互的灵活性，通过识别 spoken communication 中的情感。</li>
<li>methods: 我们提出了一种 soft labeling 系统，用于捕捉情感的渐变强度。我们还使用 Whisper 编码器和基于对比学习的数据增强方法，以强调情感的时间动态。</li>
<li>results: 我们在四种多语言 dataset 上进行验证，得到了显著的零样本泛化。我们还发布了我们的开源模型参数和初步的成果，并在 Hume-Prosody 上进行了微调。<details>
<summary>Abstract</summary>
Recognizing emotions in spoken communication is crucial for advanced human-machine interaction. Current emotion detection methodologies often display biases when applied cross-corpus. To address this, our study amalgamates 16 diverse datasets, resulting in 375 hours of data across languages like English, Chinese, and Japanese. We propose a soft labeling system to capture gradational emotional intensities. Using the Whisper encoder and data augmentation methods inspired by contrastive learning, our method emphasizes the temporal dynamics of emotions. Our validation on four multilingual datasets demonstrates notable zero-shot generalization. We publish our open source model weights and initial promising results after fine-tuning on Hume-Prosody.
</details>
<details>
<summary>摘要</summary>
recognizing emotions in spoken communication is crucial for advanced human-machine interaction. current emotion detection methodologies often display biases when applied cross-corpus. to address this, our study amalgamates 16 diverse datasets, resulting in 375 hours of data across languages like english, chinese, and japanese. we propose a soft labeling system to capture gradational emotional intensities. using the whisper encoder and data augmentation methods inspired by contrastive learning, our method emphasizes the temporal dynamics of emotions. our validation on four multilingual datasets demonstrates notable zero-shot generalization. we publish our open source model weights and initial promising results after fine-tuning on hume-prosody.Here's a word-for-word translation of the text into Simplified Chinese:认识 spoken communication 中的情感是进阶人机交互的关键。现有的情感检测方法经常在 cross-corpus 应用时显示偏见。为了解决这个问题，我们的研究将16种多样化的数据集融合，共计375小时的数据，涵盖英语、中文和日语等语言。我们提议使用 soft labeling 系统来捕捉情感的Gradational强度。使用 whisper encoder 和基于对比学习的数据增强方法，我们的方法强调情感的时间动态。我们在四种多语言数据集上进行验证，并显示了很好的零shot泛化性。我们将我们的开源模型权重和初步成果发布，并在 hume-prosody 上进行了细化调整。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.CL_2023_11_15/" data-id="clp53jwog00f3yp88d13o4jyl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/15/cs.AI_2023_11_15/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-11-15
        
      </div>
    </a>
  
  
    <a href="/2023/11/15/cs.LG_2023_11_15/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-11-15</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">125</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">62</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">76</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
