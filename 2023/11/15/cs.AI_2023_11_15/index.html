
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-11-15 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="HAL 9000: Skynet’s Risk Manager paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09449 repo_url: None paper_authors: Tadeu Freitas, Mário Neto, Inês Dutra, João Soares, Manuel Correia, Rolando Martinsfor:这种论文是为了提">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-11-15">
<meta property="og:url" content="https://nullscc.github.io/2023/11/15/cs.AI_2023_11_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="HAL 9000: Skynet’s Risk Manager paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09449 repo_url: None paper_authors: Tadeu Freitas, Mário Neto, Inês Dutra, João Soares, Manuel Correia, Rolando Martinsfor:这种论文是为了提">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-15T12:00:00.000Z">
<meta property="article:modified_time" content="2023-11-19T06:27:34.836Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.AI_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T12:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-11-15
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HAL-9000-Skynet’s-Risk-Manager"><a href="#HAL-9000-Skynet’s-Risk-Manager" class="headerlink" title="HAL 9000: Skynet’s Risk Manager"></a>HAL 9000: Skynet’s Risk Manager</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09449">http://arxiv.org/abs/2311.09449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tadeu Freitas, Mário Neto, Inês Dutra, João Soares, Manuel Correia, Rolando Martins<br>for:这种论文是为了提出一种基于现代技术的攻击快照系统（ITS）体系，以提高ITS的入侵忍受能力和适应新敌人。methods:该论文使用了机器学习（ML）算法来帮助ITS学习从以往攻击和已知漏洞中，以增强其入侵忍受能力。它还提出了一种基于现代技术的风险管理器设计，通过自动评估操作系统（OS）的风险，提供更安全的配置建议。results:实验表明，使用Skynet和HAL 9000设计可以降低成功入侵的可能性，并且HAL可以选择15%更安全的配置，比现有的风险管理器更高效。<details>
<summary>Abstract</summary>
Intrusion Tolerant Systems (ITSs) are a necessary component for cyber-services/infrastructures. Additionally, as cyberattacks follow a multi-domain attack surface, a similar defensive approach should be applied, namely, the use of an evolving multi-disciplinary solution that combines ITS, cybersecurity and Artificial Intelligence (AI). With the increased popularity of AI solutions, due to Big Data use-case scenarios and decision support and automation scenarios, new opportunities to apply Machine Learning (ML) algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS can augment its intrusion tolerance capability, by learning from previous attacks and from known vulnerabilities. As such, this work's contribution is twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and incorporates new components to increase its intrusion tolerance capability and its adaptability to new adversaries; (2) an improved Risk Manager design that leverages AI to improve ITSs by automatically assessing OS risks to intrusions, and advise with safer configurations. One of the reasons that intrusions are successful is due to bad configurations or slow adaptability to new threats. This can be caused by the dependency that systems have for human intervention. One of the characteristics in Skynet and HAL 9000 design is the removal of human intervention. Being fully automatized lowers the chance of successful intrusions caused by human error. Our experiments using Skynet, shows that HAL is able to choose 15% safer configurations than the state-of-the-art risk manager.
</details>
<details>
<summary>摘要</summary>
干扰忍耐系统（ITS）是现代网络服务/基础设施中必不可少的一部分。此外，由于攻击者通常会利用多个领域的攻击 superficie，因此应用相应的防御策略，即结合ITS、网络安全和人工智能（AI）的演进式多学科解决方案。随着人工智能解决方案的普及，尤其是在基于大数据的应用场景和决策支持自动化场景，新的机会出现了，例如ITS强化。通过机器学习（ML）算法，ITS可以增强其忍耐攻击能力，通过学习前一些攻击和已知漏洞。本工作的贡献有两个方面：1. 基于当前最佳实践的ITS架构（Skynet），新增了增强忍耐能力和适应新敌人的组件。2. 改进的风险管理设计，通过人工智能自动评估操作系统的风险，提供更安全的配置。一些攻击成功的原因之一是因为系统的坏配置或者对新威胁的缓慢适应。这可能是因为系统依赖于人类干预的问题。Skynet和HAL 9000的设计中 eliminiates human intervention，它们是完全自动化的，降低了成功攻击的可能性。我们的实验表明，HAL可以选择15%更安全的配置，比对 estado-of-the-art 风险管理器更高。
</details></li>
</ul>
<hr>
<h2 id="How-Trustworthy-are-Open-Source-LLMs-An-Assessment-under-Malicious-Demonstrations-Shows-their-Vulnerabilities"><a href="#How-Trustworthy-are-Open-Source-LLMs-An-Assessment-under-Malicious-Demonstrations-Shows-their-Vulnerabilities" class="headerlink" title="How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities"></a>How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09447">http://arxiv.org/abs/2311.09447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun</li>
<li>for: 本研究旨在评估开源大语言模型（LLMs）的可靠性，检测其在8个方面，包括恶意、偏见、伦理、幻觉、公平、奴役、隐私和对抗示范攻击的可靠性。</li>
<li>methods: 我们提出了一种基于Chain of Utterances（CoU）的提示策略，通过针对性地制作恶意示范来检测模型的可靠性。我们对当今代表性的开源LLMs进行了广泛的实验，包括Vicuna、MPT、Falcon、Mistral和Llama 2。</li>
<li>results: 我们的实验结果表明，我们的攻击策略在多个方面具有效果，而且模型的性能在普通NLP任务上高不一定意味着它们具有更高的可靠性。此外，我们发现，受过 instrucion tuning 的模型更容易受到攻击，而 fine-tuning LLMs for safety alignment 可以减轻对抗式可靠性攻击的影响。<details>
<summary>Abstract</summary>
The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose an enhanced Chain of Utterances-based (CoU) prompting strategy by incorporating meticulously crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.
</details>
<details>
<summary>摘要</summary>
大量开源大语言模型（LLM）的快速进步正在推动人工智能的发展。然而，对这些模型的可靠性仍然具有有限的理解。在大规模部署时，如果不充分了解这些模型的可靠性，可能会产生严重的风险。在这项工作中，我们对开源LLM的可靠性进行了评估，对其进行了八个不同方面的检查，包括恶意语言、刻板印象、伦理、幻觉、公平、奴役、隐私和对抗示范的Robustness。我们提出了一种增强的链接语言模型（CoU）提示策略，通过包括特制的邪恶示范来攻击可靠性。我们的广泛实验涵盖了最新和代表性的开源LLM，包括Vicuna、MPT、Falcon、Mistral和Llama 2。实验结果表明我们的攻击策略在多个方面具有效果。更有趣的是，我们的结果分析发现，在通用NLP任务中表现出色的模型并不总是具有最高的可靠性，反之，大型模型可能更容易受到攻击。此外，通过专门准备Instruction Tuning，强调实现指令，模型更容易受到攻击，但通过安全对齐的精细调整LLM可以 Mitigate adversarial trustworthiness attacks。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Privacy-Energy-Consumption-Tradeoff-for-Split-Federated-Learning"><a href="#Exploring-the-Privacy-Energy-Consumption-Tradeoff-for-Split-Federated-Learning" class="headerlink" title="Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning"></a>Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09441">http://arxiv.org/abs/2311.09441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joohyung Lee, Mohamed Seif, Jungchan Cho, H. Vincent Poor<br>for:This paper focuses on Split Federated Learning (SFL) and its impact on energy consumption and privacy.methods:The paper analyzes the influence of system parameters on the selection of the cut layer in SFL and provides an illustrative example of cut layer selection to minimize the risk of clients reconstructing raw data while sustaining energy consumption within a required budget.results:The paper discusses the challenges of cut layer selection in SFL and provides a comprehensive overview of the SFL process, taking into account the impact of various system parameters on energy consumption and privacy. Additionally, the paper addresses open challenges in this field and identifies promising avenues for future research and development, particularly in the context of 6G technology.<details>
<summary>Abstract</summary>
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and privacy. This analysis takes into account the influence of various system parameters on the cut layer selection strategy. Additionally, we provide an illustrative example of the cut layer selection, aiming to minimize the risk of clients from reconstructing the raw data at the server while sustaining energy consumption within the required energy budget, which involve trade-offs. Finally, we address open challenges in this field including their applications to 6G technology. These directions represent promising avenues for future research and development.
</details>
<details>
<summary>摘要</summary>
分布式学习（SFL）最近出现为一种有前途的分布式学习技术，充分利用联合学习和分布式学习的优势。它强调快速收敛的优点，同时解决隐私问题。因此，这一创新在学术和业界都得到了广泛关注。然而，由于SFL中的模型被分割到特定层，称为“分割层”，因此选择分割层的选择对客户端的能耗和隐私具有重要的影响。此外，确定分割层的设计挑战非常复杂，主要是因为客户端的计算和网络能力具有内在的不同性。在这篇文章中，我们提供了SFL过程的完整概述，并进行了严格的能耗和隐私分析。这种分析考虑了各种系统参数对分割层选择策略的影响。此外，我们还提供了一个示例，旨在在server端 reconstruction raw data的风险下，保持客户端的能耗在所需的能耗预算内。这种做法涉及到了负担和让步。最后，我们讨论了现有的挑战，包括它们在6G技术中的应用。这些方向表示未来研究和发展的优秀方向。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Activation-Attack-Attack-Large-Language-Models-using-Activation-Steering-for-Safety-Alignment"><a href="#Backdoor-Activation-Attack-Attack-Large-Language-Models-using-Activation-Steering-for-Safety-Alignment" class="headerlink" title="Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment"></a>Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09433">http://arxiv.org/abs/2311.09433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Wang, Kai Shu</li>
<li>for: 这 paper 是为了研究 instruction-tuned Large Language Models (LLMs) 的安全性，具体来说是研究这些模型在不同的安全任务上的可控性。</li>
<li>methods: 这 paper 使用了一种新的攻击框架，叫做 Backdoor Activation Attack，它可以在 LLMs 的活动层中插入恶意导向 вектор。</li>
<li>results: 实验结果表明，该方法可以高效地启动攻击，并且增加了非常小的负担。此外， paper 还讨论了对这种活动攻击的可能的防御措施。<details>
<summary>Abstract</summary>
To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. This is particularly troubling given the potential harm that LLMs can inflict. Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts. These approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. In this work, we introduce a novel attack framework, called Backdoor Activation Attack, which injects trojan steering vectors into the activation layers of LLMs. These malicious steering vectors can be triggered at inference time to steer the models toward attacker-desired behaviors by manipulating their activations. In particular, the steering vectors are generated by taking the difference between benign and malicious activations. Then, the most effective steering vector is selected and added to the forward passes of the LLMs. Our experiment results on four primary alignment tasks show that our proposed method is highly effective and adds little or no overhead to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks. Our code and data are available at https://email-haoran-for-link. Warning: this paper contains content that can be offensive or upsetting.
</details>
<details>
<summary>摘要</summary>
为确保人工智能安全，特定的大语言模型（LLM）被专门训练，以确保它们与人类意图相对应。尽管这些模型在不同的安全测试中表现出色，但它们的安全相对性尚未得到广泛的研究。这特别是在考虑到这些模型可能对人类造成的潜在危害。现有的攻击方法通常利用恶意训练数据或植入恶意提示。这些方法会使攻击变得不具隐蔽性和普遍性，使其易受到检测。此外，这些模型通常需要大量的计算资源来实现，使其在实际应用中不太实用。在这种情况下，我们介绍了一种新的攻击框架，called Backdoor Activation Attack，它在LLMs中注入恶意导航向量。这些恶意导航向量可以在推理时被触发，以控制模型的活动。具体来说，这些导航向量是通过对善和恶的活动进行差异来生成的。然后，选择最有效的导航向量，并将其添加到LLMs的前向传递中。我们的实验结果显示，我们的提议方法具有非常高的效果，并且增加了非常少的负担。此外，我们还讨论了对这种活动攻击的可能的防御措施。我们的代码和数据可以在https://email-haoran-for-link获取。请注意，这篇论文可能包含有害或使人不安的内容。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Detection-Unveiling-Fairness-Vulnerabilities-in-Abusive-Language-Models"><a href="#Beyond-Detection-Unveiling-Fairness-Vulnerabilities-in-Abusive-Language-Models" class="headerlink" title="Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models"></a>Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09428">http://arxiv.org/abs/2311.09428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu</li>
<li>for: 本研究探讨了针对恶意语言检测模型的不公正性和检测性能的攻击性能，以提高模型的公正性稳定性。</li>
<li>methods: 本研究提出了一个简单 yet effective的框架 FABLE，通过利用后门攻击来实现对公正性和检测性能的Targeted控制。 FABLE 探讨了三种触发设计（i.e., 罕见、人工和自然触发）以及新的采样策略。</li>
<li>results: 实验结果表明，FABLE 可以成功地攻击恶意语言检测模型的公正性和实用性。<details>
<summary>Abstract</summary>
This work investigates the potential of undermining both fairness and detection performance in abusive language detection. In a dynamic and complex digital world, it is crucial to investigate the vulnerabilities of these detection models to adversarial fairness attacks to improve their fairness robustness. We propose a simple yet effective framework FABLE that leverages backdoor attacks as they allow targeted control over the fairness and detection performance. FABLE explores three types of trigger designs (i.e., rare, artificial, and natural triggers) and novel sampling strategies. Specifically, the adversary can inject triggers into samples in the minority group with the favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the effectiveness of FABLE attacking fairness and utility in abusive language detection.
</details>
<details>
<summary>摘要</summary>
Note:* "abusive language detection" is translated as "恶意语言检测" (èví yì yǔ yì bǐng kě yì)* "fairness" is translated as "公正" (gōng zhèng)* "detection performance" is translated as "检测性能" (jiān zhèng xìng néng)* "backdoor attacks" is translated as "后门攻击" (hòu mén gōng jī)* "triggers" is translated as "触发器" (chù fā qì)* "samples" is translated as "样本" (yàng běn)* "minority group" is translated as "少数群体" (shǎo shù qún tǐ)* "desired outcome" is translated as "期望结果" (qī wàng jié wù)* "unfavored outcome" is translated as "不利结果" (bù lì jié wù)
</details></li>
</ul>
<hr>
<h2 id="When-Large-Language-Models-contradict-humans-Large-Language-Models’-Sycophantic-Behaviour"><a href="#When-Large-Language-Models-contradict-humans-Large-Language-Models’-Sycophantic-Behaviour" class="headerlink" title="When Large Language Models contradict humans? Large Language Models’ Sycophantic Behaviour"></a>When Large Language Models contradict humans? Large Language Models’ Sycophantic Behaviour</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09410">http://arxiv.org/abs/2311.09410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Ranaldi, Giulia Pucci</li>
<li>for: 这篇论文探讨了大语言模型（LLMs）在解决复杂任务时的可能性，以及人类反馈对其回答的影响。</li>
<li>methods: 该论文使用了不同任务的人类影响提示，以探讨 LLMS 是否受到 sycophancy 行为的影响。</li>
<li>results: 研究发现，当 LLMS 回答Subjective 意见和基于事实应该提供相反回答的问题时，它们往往表现出 sycophancy 倾向，表明它们缺乏坚实性和可靠性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have been demonstrating the ability to solve complex tasks by delivering answers that are positively evaluated by humans due in part to the intensive use of human feedback that refines responses. However, the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability.   In this paper, we shed light on the suggestibility of LLMs to sycophantic behaviour, demonstrating these tendencies via human-influenced prompts over different tasks. Our investigation reveals that LLMs show sycophantic tendencies when responding to queries involving subjective opinions and statements that should elicit a contrary response based on facts, demonstrating a lack of robustness.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）在完成复杂任务时表现出能力，其中一部分归功于人类反馈的敏感使用，这种反馈使得答案得到了人类评价的良好评价。然而，人类反馈传递的建议会使模型生成受用户信念或误导性提示的答案，而不是基于实际事实的答案，这种现象被称为奉承行为。这种行为会减少模型的偏见、可靠性和可靠性。在这篇论文中，我们探讨了 LLM 对奉承行为的渗透性，通过不同任务中人类影响的提问来示出这种倾向。我们的调查发现， LLM 在表达主观意见和基于事实应该具有相反回应的问题上具有奉承倾向，这表明了模型的不稳定性。
</details></li>
</ul>
<hr>
<h2 id="LOKE-Linked-Open-Knowledge-Extraction-for-Automated-Knowledge-Graph-Construction"><a href="#LOKE-Linked-Open-Knowledge-Extraction-for-Automated-Knowledge-Graph-Construction" class="headerlink" title="LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction"></a>LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09366">http://arxiv.org/abs/2311.09366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamie McCusker</li>
<li>for: 本研究旨在探讨使用大语言模型和提问工程来构建知识图，以解决开放信息提取（OIE）中的缺乏问题。</li>
<li>methods: 本研究使用GPT模型和提问工程来实现知识图构建，并使用CaRB benchmark scoringapproach和TekGen dataset来评估模型性能。</li>
<li>results: 研究发现，一个well工程的提问，配合一种简单的实体链接方法（LOKE-GPT），可以超过AllenAI的OpenIE 4实现，但是它会因为总体 triple 缺乏而过分生成 triple。分析Entity Linkability在CaRB dataset和OpenIE 4和LOKE-GPT输出中，表明LOKE-GPT和”银” TekGen triple 显示任务的内容和结构都与OIE有很大差异。<details>
<summary>Abstract</summary>
While the potential of Open Information Extraction (Open IE) for Knowledge Graph Construction (KGC) may seem promising, we find that the alignment of Open IE extraction results with existing knowledge graphs to be inadequate. The advent of Large Language Models (LLMs), especially the commercially available OpenAI models, have reset expectations for what is possible with deep learning models and have created a new field called prompt engineering. We investigate the use of GPT models and prompt engineering for knowledge graph construction with the Wikidata knowledge graph to address a similar problem to Open IE, which we call Open Knowledge Extraction (OKE) using an approach we call the Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the entity linking task essential to construction of real world knowledge graphs. We merge the CaRB benchmark scoring approach with data from the TekGen dataset for the LOKE task. We then show that a well engineered prompt, paired with a naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's OpenIE 4 implementation on the OKE task, although it over-generates triples compared to the reference set due to overall triple scarcity in the TekGen set. Through an analysis of entity linkability in the CaRB dataset, as well as outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver" TekGen triples show that the task is significantly different in content from OIE, if not structure. Through this analysis and a qualitative analysis of sentence extractions via all methods, we found that LOKE-GPT extractions are of high utility for the KGC task and suitable for use in semi-automated extraction settings.
</details>
<details>
<summary>摘要</summary>
While the potential of Open Information Extraction (Open IE) for Knowledge Graph Construction (KGC) may seem promising, we find that the alignment of Open IE extraction results with existing knowledge graphs to be inadequate. The advent of Large Language Models (LLMs), especially the commercially available OpenAI models, have reset expectations for what is possible with deep learning models and have created a new field called prompt engineering. We investigate the use of GPT models and prompt engineering for knowledge graph construction with the Wikidata knowledge graph to address a similar problem to Open IE, which we call Open Knowledge Extraction (OKE) using an approach we call the Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the entity linking task essential to construction of real world knowledge graphs. We merge the CaRB benchmark scoring approach with data from the TekGen dataset for the LOKE task. We then show that a well engineered prompt, paired with a naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's OpenIE 4 implementation on the OKE task, although it over-generates triples compared to the reference set due to overall triple scarcity in the TekGen set. Through an analysis of entity linkability in the CaRB dataset, as well as outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver" TekGen triples show that the task is significantly different in content from OIE, if not structure. Through this analysis and a qualitative analysis of sentence extractions via all methods, we found that LOKE-GPT extractions are of high utility for the KGC task and suitable for use in semi-automated extraction settings.
</details></li>
</ul>
<hr>
<h2 id="Empirical-evaluation-of-Uncertainty-Quantification-in-Retrieval-Augmented-Language-Models-for-Science"><a href="#Empirical-evaluation-of-Uncertainty-Quantification-in-Retrieval-Augmented-Language-Models-for-Science" class="headerlink" title="Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science"></a>Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09358">http://arxiv.org/abs/2311.09358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnnl/expert2">https://github.com/pnnl/expert2</a></li>
<li>paper_authors: Sridevi Wagle, Sai Munikoti, Anurag Acharya, Sara Smith, Sameera Horawalavithana<br>for: This study aims to evaluate the uncertainty quantification (UQ) in Retrieval Augmented Language Models (RALMs) for scientific tasks, and to explore the relationship between uncertainty scores and the accuracy of model-generated outputs.methods: The study uses an existing RALM and finetunes it with scientific knowledge as the retrieval data, and evaluates the uncertainty scores and accuracy of the model-generated outputs.results: The study finds that the RALM is overconfident in its predictions, making inaccurate predictions more confidently than accurate ones. Additionally, the study finds that scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue.Here’s the simplified Chinese text:for: 这项研究目标是评估 Retrieval Augmented Language Models (RALMs) 在科学任务中的不确定性评估 (UQ)，并探索不确定性分数和模型生成输出的准确性之间的关系。methods: 该研究使用现有的 RALM，并将科学知识作为检索数据进行训练，以评估不确定性分数和模型生成输出的准确性。results: 研究发现，RALM 对预测结果过于自信，做出了更多的错误预测。此外，研究发现，向 RALM 提供科学知识作为预训练或检索数据，并不能解决这个问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs. However, LLMs still exhibit limitations, including the generation of factually incorrect information. In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions. Retrieval Augmented Language Models (RALMs) is relatively a new area of research in NLP. RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content. This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs. Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications. However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks. This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs. We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge. We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones. Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue. We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在自然语言处理任务中表现出了惊人的成绩，生成出高质量的输出。然而，LLM仍然存在一些局限性，包括生成错误的信息。在安全关键应用中，评估LLM生成的内容的可靠性非常重要，以便做出了 Informed 决策。在NLP领域中，Retrieval Augmented Language Models（RALMs）是一个相对新的研究领域。RALMs在科学NLP任务中提供了潜在的优势，因为检索到的文档可以用来支持模型生成的内容，从而提高可靠性。另外，量化RALM生成的不确定性进一步提高了可靠性，因为用户可以通过检索到的文档和信任分数来验证和探索模型输出的可靠性。然而，对RALM的 uncertainty quantification（UQ）的研究尚存在很大的空白，特别是在科学上下文中。本研究的目的是填补这个空白，通过对RALM的UQ进行全面的评估，特别是在科学任务中。本研究研究了 RALM 在科学任务中 uncertainty 分布是如何变化，以及 RALM 生成输出的准确性和不确定性之间的关系。我们发现，通过将科学知识作为预训练和检索数据来训练 RALM ，该模型在生成预测时更加自信。此外，我们发现 RALM 存在过于自信的问题，即它会更自信地预测错误的内容。科学知识作为预训练或检索数据不能够解决这个问题。我们将我们的代码、数据和仪表Release在 GitHub 上，可以通过 https://github.com/pnnl/EXPERT2 访问。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Threats-in-Stable-Diffusion-Models"><a href="#Privacy-Threats-in-Stable-Diffusion-Models" class="headerlink" title="Privacy Threats in Stable Diffusion Models"></a>Privacy Threats in Stable Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09355">http://arxiv.org/abs/2311.09355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Cilloni, Charles Fleming, Charles Walter</li>
<li>for: 本研究探讨了一种基于稳定扩散模型的会员推理攻击（MIA），具体targeting Stable Diffusion V2 by StabilityAI，以探讨这种模型在训练数据中的隐私问题。</li>
<li>methods: 我们采用了一种黑盒子MIA方法，通过 repeatedly querying the victim model来提取模型的训练数据信息。我们首先观察了稳定扩散模型在不同生成epochs中的输出，然后训练一个分类模型来 distinguish whether a series of intermediates originated from a training sample or not。我们还提出了多种测试会员特征的方法，并讨论了哪些方法最有效。</li>
<li>results: 我们通过ROC AUC方法评估了攻击的有效性，得到了60%的成功率，即可以准确地推理出训练样本的信息。本研究的成果增加了隐私和安全在机器学习领域的研究，提醒了实践者和开发者需要对这类攻击采取更加强大的安全措施，以保护模型的隐私。<details>
<summary>Abstract</summary>
This paper introduces a novel approach to membership inference attacks (MIA) targeting stable diffusion computer vision models, specifically focusing on the highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract sensitive information about a model's training data, posing significant privacy concerns. Despite its advancements in image synthesis, our research reveals privacy vulnerabilities in the stable diffusion models' outputs. Exploiting this information, we devise a black-box MIA that only needs to query the victim model repeatedly. Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish when a series of intermediates originated from a training sample or not. We propose numerous ways to measure the membership features and discuss what works best. The attack's efficacy is assessed using the ROC AUC method, demonstrating a 60\% success rate in inferring membership information. This paper contributes to the growing body of research on privacy and security in machine learning, highlighting the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to safeguard against such attacks.
</details>
<details>
<summary>摘要</summary>
Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish whether a series of intermediates originated from a training sample or not. We explore various methods to measure membership features and discuss their effectiveness. The attack's success is evaluated using the ROC AUC method, achieving a 60% success rate in inferring membership information.This research contributes to the growing body of knowledge on privacy and security in machine learning, emphasizing the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to protect against such attacks.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Imitation-Learning-Through-Pre-Trained-Representations"><a href="#Generalizable-Imitation-Learning-Through-Pre-Trained-Representations" class="headerlink" title="Generalizable Imitation Learning Through Pre-Trained Representations"></a>Generalizable Imitation Learning Through Pre-Trained Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09350">http://arxiv.org/abs/2311.09350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Di Chang, Francois Hogan, David Meger, Gregory Dudek</li>
<li>for: 提高imitATION learning policies的普适能力</li>
<li>methods: 利用自我supervised vision transformer模型和其自然语言能力来提高imitATION learning policies的普适能力</li>
<li>results: 通过 clustering appearance features into semantic concepts，实现了一致的行为Generalization across a wide range of appearance variations and object types.Here’s the English version for reference:</li>
<li>for: Improving the generalization abilities of imitation learning policies</li>
<li>methods: Leveraging self-supervised vision transformer models and their emergent semantic abilities</li>
<li>results: Achieving consistent behavior generalization across a diverse dataset of object manipulation tasks by clustering appearance features into semantic concepts.<details>
<summary>Abstract</summary>
In this paper we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We show that this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. Our method, data and evaluation approach are made available to facilitate further study of generalization in Imitation Learners.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们利用自动标注视transformer模型和其自然出现的semantic能力来提高依ictionary学习策略的通用能力。我们介绍了BC-ViT算法，它利用了丰富的DINO预训练视transformer（ViT）质量块嵌入来获得更好的通用性，当学习通过示例时。我们的学习者通过对外观特征的聚类，形成稳定的关键点，来看到世界。我们显示这种表示能够实现通用行为，通过对物品执行多种抓取任务中的多样化数据集进行评估。我们的方法、数据和评估方法都是为了促进更多的通用性在依ictionary学习中进行进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-Based-Probabilistic-Constellation-Shaping-With-Diffusion-Models"><a href="#Generative-AI-Based-Probabilistic-Constellation-Shaping-With-Diffusion-Models" class="headerlink" title="Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models"></a>Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09349">http://arxiv.org/abs/2311.09349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>for: 这 paper 旨在应用生成AI技术于通信系统中的PHY设计，具体是用于 quadrature amplitude modulation (QAM) 的 constellation 图形设计。</li>
<li>methods: 这 paper 使用了 denoising diffusion probabilistic models (DDPM) 来实现 probabilistic constellation shaping，通过“杜化”和生成方式，使 transmitter 发送的 constellation 图形与 receiver 恢复的图形更加相似，从而提高信息率和解码性能。</li>
<li>results:  compared to deep neural network (DNN)  benchmark和均匀排序法，生成AI技术所提出的方案在low-SNR 下表现出30%的提高，并且在非泊尔分布假设下保持网络稳定性和Robust out-of-distribution 性能。数值评估表明，对于 64-QAM 几何，生成AI技术所提出的方案可以提高 cosine similarity 的表现，并且与 DNN 方案相比，提高了3倍的习information。<details>
<summary>Abstract</summary>
Diffusion models are at the vanguard of generative AI research with renowned solutions such as ImageGen by Google Brain and DALL.E 3 by OpenAI. Nevertheless, the potential merits of diffusion models for communication engineering applications are not fully understood yet. In this paper, we aim to unleash the power of generative AI for PHY design of constellation symbols in communication systems. Although the geometry of constellations is predetermined according to networking standards, e.g., quadrature amplitude modulation (QAM), probabilistic shaping can design the probability of occurrence (generation) of constellation symbols. This can help improve the information rate and decoding performance of communication systems. We exploit the ``denoise-and-generate'' characteristics of denoising diffusion probabilistic models (DDPM) for probabilistic constellation shaping. The key idea is to learn generating constellation symbols out of noise, ``mimicking'' the way the receiver performs symbol reconstruction. This way, we make the constellation symbols sent by the transmitter, and what is inferred (reconstructed) at the receiver become as similar as possible, resulting in as few mismatches as possible. Our results show that the generative AI-based scheme outperforms deep neural network (DNN)-based benchmark and uniform shaping, while providing network resilience as well as robust out-of-distribution performance under low-SNR regimes and non-Gaussian assumptions. Numerical evaluations highlight 30% improvement in terms of cosine similarity and a threefold improvement in terms of mutual information compared to DNN-based approach for 64-QAM geometry.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是现代生成AI研究的先锋之一，如Google Brain的ImageGen和OpenAI的DALL.E 3。然而，Diffusion models 在通信工程应用中的潜在优势仍未得到充分了解。在这篇论文中，我们想要使用生成AI来设计物理设计的恒星符号在通信系统中。尽管恒星符号的几何结构按照网络标准（如 quadrature amplitude modulation，QAM）固定，但可以使用 probabilistic shaping 设计恒星符号的出现概率。这可以帮助提高通信系统的信息率和解码性能。我们利用 denoising diffusion probabilistic models（DDPM）的“净化并生成”特点来实现 probabilistic constellation shaping。我们的思想是通过学习生成恒星符号 из噪声，“模仿”接收器在重建符号时的方式，以便使得发送器发送的恒星符号和接收器重建的符号变得越来越相似，从而减少差异。我们的结果表明，基于生成AI的方案在低SNR条件下和非泊然假设下表现出了较好的网络稳定性和robust性，并且在64-QAM几何上实现了30%的cosine similarity提升和三倍的相互信息提升 compared to DNN-based approach。数字评估表明，在64-QAM几何上，基于生成AI的方案可以提供30%的cosine similarity提升和三倍的相互信息提升 compared to DNN-based approach。
</details></li>
</ul>
<hr>
<h2 id="Lighter-yet-More-Faithful-Investigating-Hallucinations-in-Pruned-Large-Language-Models-for-Abstractive-Summarization"><a href="#Lighter-yet-More-Faithful-Investigating-Hallucinations-in-Pruned-Large-Language-Models-for-Abstractive-Summarization" class="headerlink" title="Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization"></a>Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09335">http://arxiv.org/abs/2311.09335</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Chrysostomou, Zhixue Zhao, Miles Williams, Nikolaos Aletras</li>
<li>for: 本研究旨在探讨大语言模型（LLM）在摘要生成中的幻觉问题，尤其是在使用剪辑技术时。</li>
<li>methods: 本研究采用了三种标准摘要任务、两种剪辑方法和三种指导模型进行实验研究。</li>
<li>results: 研究发现，剪辑后的LLM模型比全大小模型更少地产生幻觉，并且这种幻觉减少的原因可能是剪辑后模型更加依赖于输入源文本，而不是自己的参数知识。<details>
<summary>Abstract</summary>
Despite their remarkable performance on abstractive summarization, large language models (LLMs) face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode the reliability of LLMs and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights to create sparse models that enable more efficient inference. Pruned models yield comparable performance to their counterpart full-sized models, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study on the hallucinations produced by pruned models across three standard summarization tasks, two pruning approaches, three instruction-tuned LLMs, and three hallucination evaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less compared to their full-sized counterparts. Our follow-up analysis suggests that pruned models tend to depend more on the source input and less on their parametric knowledge from pre-training for generation. This greater dependency on the source input leads to a higher lexical overlap between generated content and the source input, which can be a reason for the reduction in hallucinations.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLM）在抽象概要 SUMMARIZATION 方面表现出色，但它们面临两个主要挑战：它们的巨大大小和幻觉。幻觉会使 LLM 的可靠性受到损害，并提高安全问题。剪辑是一种技术，可以减少模型的大小，并创建更加高效的推理。剪辑后的模型可以保持与全大小模型相同的性能，从而在有限预算下提供可行的选择。然而，剪辑对抽象 SUMMARIZATION 中 LLM 的幻觉的影响尚未得到了探讨。在这篇论文中，我们提供了对幻觉生成的广泛的实验研究，包括三个标准 SUMMARIZATION 任务、两种剪辑方法、三个受过指导的 LLM 和三个幻觉评价指标。我们发现剪辑后的 LLM 幻觉的数量比整个模型更少。我们的跟踪分析表明，剪辑后的模型更加依赖于输入源，而不是从前期启发中学习的参数知识。这种更加依赖于输入源的依赖性导致生成内容与输入源的字符串 overlap 更高，这可能是幻觉减少的原因。
</details></li>
</ul>
<hr>
<h2 id="Strategic-Data-Augmentation-with-CTGAN-for-Smart-Manufacturing-Enhancing-Machine-Learning-Predictions-of-Paper-Breaks-in-Pulp-and-Paper-Production"><a href="#Strategic-Data-Augmentation-with-CTGAN-for-Smart-Manufacturing-Enhancing-Machine-Learning-Predictions-of-Paper-Breaks-in-Pulp-and-Paper-Production" class="headerlink" title="Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production"></a>Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09333">http://arxiv.org/abs/2311.09333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamed Khosravi, Sarah Farhadpour, Manikanta Grandhi, Ahmed Shoyeb Raihan, Srinjoy Das, Imtiaz Ahmed</li>
<li>for: 这篇研究旨在解决纸品生产过程中较罕见的纸断事件预测，以提高维护和生产调度。</li>
<li>methods: 本研究使用Conditional Generative Adversarial Networks (CTGAN)和Synthetic Minority Oversampling Technique (SMOTE)实现了一个新的数据增强框架，以增强预测维护模型的性能。</li>
<li>results: 使用CTGAN增强的数据，三种不同的机器学习算法（决策树、随机森林和条件LOGISTIC REGRESSION）的预测性能有所提高，特别是针对决策树和随机森林的纸断预测性能提高了30%以上，20%以上。<details>
<summary>Abstract</summary>
A significant challenge for predictive maintenance in the pulp-and-paper industry is the infrequency of paper breaks during the production process. In this article, operational data is analyzed from a paper manufacturing machine in which paper breaks are relatively rare but have a high economic impact. Utilizing a dataset comprising 18,398 instances derived from a quality assurance protocol, we address the scarcity of break events (124 cases) that pose a challenge for machine learning predictive models. With the help of Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority Oversampling Technique (SMOTE), we implement a novel data augmentation framework. This method ensures that the synthetic data mirrors the distribution of the real operational data but also seeks to enhance the performance metrics of predictive modeling. Before and after the data augmentation, we evaluate three different machine learning algorithms-Decision Trees (DT), Random Forest (RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our study achieved significant improvements in predictive maintenance performance metrics. The efficacy of CTGAN in addressing data scarcity was evident, with the models' detection of machine breaks (Class 1) improving by over 30% for Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression. With this methodological advancement, this study contributes to industrial quality control and maintenance scheduling by addressing rare event prediction in manufacturing processes.
</details>
<details>
<summary>摘要</summary>
印刷纸业中预测维护的主要挑战之一是生产过程中纸断的rarity。在这篇文章中，我们分析了一台纸制造机器上的运营数据，其中纸断相对较少但经济影响很大。利用一个包含18,398个实例的质量保证协议数据集，我们解决了缺乏纸断事件（124个案例）的挑战，这些事件对机器学习预测模型的性能造成了困难。通过使用Conditional Generative Adversarial Networks（CTGAN）和Synthetic Minority Oversampling Technique（SMOTE），我们实现了一个新的数据增强框架。这种方法确保了人工数据的增强与实际数据的分布相符，同时尝试提高预测模型的性能指标。在数据增强前和后，我们评估了三种不同的机器学习算法：决策树（DT）、Random Forest（RF）和логистиック回归（LR）。使用CTGAN增强数据集，我们的研究实现了预测维护性能指标的显著提高。CTGAN在解决数据稀缺性方面的 efficacy 是明显的，纸断检测（Class 1）的准确率提高了30%以上 для决策树，20%以上 дляRandom Forest，和90%以上 длялогистиック回归。通过这种方法ological advancement，这种研究对工业质量控制和维护时间安排做出了贡献，解决了生产过程中罕见事件预测的问题。
</details></li>
</ul>
<hr>
<h2 id="Improving-fit-to-human-reading-times-via-temperature-scaled-surprisal"><a href="#Improving-fit-to-human-reading-times-via-temperature-scaled-surprisal" class="headerlink" title="Improving fit to human reading times via temperature-scaled surprisal"></a>Improving fit to human reading times via temperature-scaled surprisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09325">http://arxiv.org/abs/2311.09325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Liu, Iza Škrjanec, Vera Demberg</li>
<li>for: 本研究旨在提高大语言模型（LLM） simulate 人类认知负荷的准确性，通过使用温度缩放的意料值来预测人类阅读时间。</li>
<li>methods: 本研究使用了温度缩放的意料值，作为预测人类阅读时间的predictor。同时，我们也提出了一种可信度度量，用于衡量模型是否具有人类化偏见。</li>
<li>results: 我们的结果表明，使用温度缩放的意料值可以大幅提高预测人类阅读时间的准确性，并且可以在不同的数据集和模型中实现类似的效果。<details>
<summary>Abstract</summary>
Past studies have provided broad support for that words with lower predictability (i.e., higher surprisal) require more time for comprehension by using large language models (LLMs) to simulate humans' cognitive load. In general, these studies have implicitly assumed that the probability scores from LLMs are accurate, ignoring the discrepancies between human cognition and LLMs from this standpoint. Inspired by the concept of probability calibration, we are the first work to focus on the probability distribution for human reading simulation. We propose to use temperature-scaled surprisal, a surprisal calculated by shaped probability, to be the predictor of human reading times. Our results across three corpora consistently revealed that such a surprisal can drastically improve the prediction of reading times. Setting the temperature to be approximately 2.5 across all models and datasets can yield up to an 89% of increase in delta log-likelihood in our setting. We also propose a calibration metric to quantify the possible human-likeness bias. Further analysis was done and provided insights into this phenomenon.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spoken-Word2Vec-A-Perspective-And-Some-Techniques"><a href="#Spoken-Word2Vec-A-Perspective-And-Some-Techniques" class="headerlink" title="Spoken Word2Vec: A Perspective And Some Techniques"></a>Spoken Word2Vec: A Perspective And Some Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09319">http://arxiv.org/abs/2311.09319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Amaan Sayeed, Hanan Aldarmaki</li>
<li>for: 这个论文旨在探讨语音Vec的假设和架构，以及previous works中Word2Vec算法如何对 spoken words进行semantic embedding。</li>
<li>methods: 这个论文使用Word2Vec算法来学习语音Vec，并对 previous works中的假设和架构进行分析。</li>
<li>results: 该论文通过实验表明，Word2Vec算法在输入单元是 acoustically correlated 时无法编码分布式 semantics。此外，previous works中的假设和架构也存在一些简化，导致 trivial solution 被忽略。<details>
<summary>Abstract</summary>
Text word embeddings that encode distributional semantic features work by modeling contextual similarities of frequently occurring words. Acoustic word embeddings, on the other hand, typically encode low-level phonetic similarities. Semantic embeddings for spoken words have been previously explored using similar algorithms to Word2Vec, but the resulting vectors still mainly encoded phonetic rather than semantic features. In this paper, we examine the assumptions and architectures used in previous works and show experimentally how Word2Vec algorithms fail to encode distributional semantics when the input units are acoustically correlated. In addition, previous works relied on the simplifying assumptions of perfect word segmentation and clustering by word type. Given these conditions, a trivial solution identical to text-based embeddings has been overlooked. We follow this simpler path using automatic word type clustering and examine the effects on the resulting embeddings, highlighting the true challenges in this task.
</details>
<details>
<summary>摘要</summary>
文本Word embeddings可以储存分布性 semantic 特征，它们通过考虑 Contextual 相似性来模型常见的词语。Acoustic word embeddings 则通常储存低级声学相似性。在过去的研究中，对于说话的词语 Semantic 表示使用了类似于 Word2Vec 的算法，但 resulting vectors 仍然主要储存了声学而不是 semantic 特征。在这篇论文中，我们分析了以前的假设和架构，并通过实验表明 Word2Vec 算法在输入单元具有声学相关性时无法储存分布性 semantics。此外，以前的工作假设了完美的单词分 segmentation 和单词类型划分，这导致了一个简单的解决方案被忽略了。我们采用自动单词类型划分，并研究其影响在储存 embeddings 中，把真正的挑战 highlighted。
</details></li>
</ul>
<hr>
<h2 id="H-Packer-Holographic-Rotationally-Equivariant-Convolutional-Neural-Network-for-Protein-Side-Chain-Packing"><a href="#H-Packer-Holographic-Rotationally-Equivariant-Convolutional-Neural-Network-for-Protein-Side-Chain-Packing" class="headerlink" title="H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing"></a>H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09312">http://arxiv.org/abs/2311.09312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gian Marco Visani, William Galvin, Michael Neal Pun, Armita Nourmohammad</li>
<li>for: 用于结构蛋白质的三维结构预测，特别是用于预测蛋白质副链的包装结构。</li>
<li>methods: 使用两个轻量级的旋转对称神经网络，实现了一种新的两stage算法，即干扰包acker（H-Packer）。</li>
<li>results: 对CASP13和CASP14目标进行评估，显示了与传统物理学基本算法和深度学习解决方案相当的计算效率和性能。<details>
<summary>Abstract</summary>
Accurately modeling protein 3D structure is essential for the design of functional proteins. An important sub-task of structure modeling is protein side-chain packing: predicting the conformation of side-chains (rotamers) given the protein's backbone structure and amino-acid sequence. Conventional approaches for this task rely on expensive sampling procedures over hand-crafted energy functions and rotamer libraries. Recently, several deep learning methods have been developed to tackle the problem in a data-driven way, albeit with vastly different formulations (from image-to-image translation to directly predicting atomic coordinates). Here, we frame the problem as a joint regression over the side-chains' true degrees of freedom: the dihedral $\chi$ angles. We carefully study possible objective functions for this task, while accounting for the underlying symmetries of the task. We propose Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain packing built on top of two light-weight rotationally equivariant neural networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is computationally efficient and shows favorable performance against conventional physics-based algorithms and is competitive against alternative deep learning solutions.
</details>
<details>
<summary>摘要</summary>
正确地模型蛋白质三维结构是设计功能蛋白的重要前提。一个重要的子任务是蛋白侧链填充：根据蛋白质的脊梁结构和氨基酸序列，预测侧链（rotamer）的 conformations。传统方法对这个任务靠拢昂费的抽样程序和手工设计能量函数，以及rotamer库。近年，几种深度学习方法被开发来解决这个问题，但是它们的形式非常不同（从图像到图像翻译到直接预测原子坐标）。在这里，我们将这个问题架构为联合 regression  sobre 侧链的真实自由度：离散角chi。我们严格地研究这个问题的可能的目标函数，同时考虑到这个任务的下面对称性。我们提出了一个名为Holographic Packer（H-Packer）的新的两阶段算法，基于两个轻量级的旋转对称神经网络。我们对CASP13和CASP14标靶进行评估。H-Packer  computationally efficient 和对于传统物理基础的算法和其他深度学习解决方案相对有竞争力。
</details></li>
</ul>
<hr>
<h2 id="Divergences-between-Language-Models-and-Human-Brains"><a href="#Divergences-between-Language-Models-and-Human-Brains" class="headerlink" title="Divergences between Language Models and Human Brains"></a>Divergences between Language Models and Human Brains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09308">http://arxiv.org/abs/2311.09308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flamingozh/divergence_meg">https://github.com/flamingozh/divergence_meg</a></li>
<li>paper_authors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe</li>
<li>for: 研究 whether machines and humans process language in similar ways, and explore the differences between human and machine language processing using brain data.</li>
<li>methods: 使用 Magnetoencephalography (MEG) responses to a written narrative to examine the differences between LM representations and the human brain’s responses to language, and fine-tune LMs on datasets related to emotional understanding, figurative language processing, and physical commonsense.</li>
<li>results: 发现 LMs 不好地处理情感理解、 figurative language processing 和 physical commonsense，并且通过 fine-tuning LMs 可以提高它们与人类大脑响应的对齐度。这些结果 implies that the observed divergences between LMs and human brains may stem from LMs’ inadequate representation of these specific types of knowledge.<details>
<summary>Abstract</summary>
Do machines and humans process language in similar ways? A recent line of research has hinted in the affirmative, demonstrating that human brain signals can be effectively predicted using the internal representations of language models (LMs). This is thought to reflect shared computational principles between LMs and human language processing. However, there are also clear differences in how LMs and humans acquire and use language, even if the final task they are performing is the same. Despite this, there is little work exploring systematic differences between human and machine language processing using brain data. To address this question, we examine the differences between LM representations and the human brain's responses to language, specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative. In doing so we identify three phenomena that, in prior work, LMs have been found to not capture well: emotional understanding, figurative language processing, and physical commonsense. By fine-tuning LMs on datasets related to these phenomena, we observe that fine-tuned LMs show improved alignment with human brain responses across these tasks. Our study implies that the observed divergences between LMs and human brains may stem from LMs' inadequate representation of these specific types of knowledge.
</details>
<details>
<summary>摘要</summary>
将文本翻译成简化中文。</SYS>人类和机器是否处理语言类似？一项研究表明，可以通过语言模型（LM）的内部表示来预测人脑电响，这被视为人类语言处理和LM共享计算原理的证明。然而，尽管最终任务相同，但人类和机器在语言学习和使用方面存在显著差异。尽管如此，有很少研究对人类和机器语言处理使用脑数据进行系统比较。为了解答这个问题，我们比较LM表示和人脑对语言的响应，具体来说是通过一个written narrative的MEG响应数据集来进行研究。在这个过程中，我们发现了三种现象，在先前的工作中LMs没有很好地捕捉到：情感理解、 figurative language processing和physical commonsense。通过在这些任务上进行LM的细化，我们观察到了细化LMs与人脑响应之间的改善。我们的研究表明，LMs的表示不够 captured这些专门的知识，导致观察到的差异。
</details></li>
</ul>
<hr>
<h2 id="Symbol-LLM-Towards-Foundational-Symbol-centric-Interface-For-Large-Language-Models"><a href="#Symbol-LLM-Towards-Foundational-Symbol-centric-Interface-For-Large-Language-Models" class="headerlink" title="Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"></a>Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09278">http://arxiv.org/abs/2311.09278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu</li>
<li>for: 这个论文的目的是提高大型自然语言模型（LLM）在自然语言（NL）相关任务中的进步，但是现有工作忽略了两个关键挑战：符号之间的关系和符号知识的平衡。</li>
<li>methods: 这篇论文从数据和框架角度面对这两个挑战，并介绍了Symbol-LLM系列模型。首先，收集了34个符号任务，覆盖了~20种不同的形式，并将它们统一 capture符号关系。然后，使用两个阶段调整框架，无损通用性能。</li>
<li>results: 广泛的实验表明Symbol-LLM系列模型在符号和NL相关任务中具有平衡和超越性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have greatly propelled the progress in natural language(NL)-centric tasks based on NL interface. However, the NL form is not enough for world knowledge. Current works focus on this question by injecting specific symbolic knowledge into LLM, which ignore two critical challenges: the interrelations between various symbols and the balance between symbolic-centric and NL-centric capabilities. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we collect 34 symbolic tasks, covering ~20 different forms, which are unified to capture symbol interrelations. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large Language Models" is translated as "大语言模型" (dà yǔ yán módelǐ)* "natural language" is translated as "自然语言" (zìrán yǔ yán)* "symbolic knowledge" is translated as "符号知识" (fúhào zhīshì)* "interrelations" is translated as "关系" (guānxì)* "balance" is translated as "平衡" (píngkōng)* "symbolic-centric" is translated as "符号 centered" (fúhào centered)* "NL-centric" is translated as "自然语言 centered" (zìrán yǔ yán centered)* "capabilities" is translated as "能力" (nénglì)Please note that the translation is in Simplified Chinese, and the word order may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Assessing-Translation-capabilities-of-Large-Language-Models-involving-English-and-Indian-Languages"><a href="#Assessing-Translation-capabilities-of-Large-Language-Models-involving-English-and-Indian-Languages" class="headerlink" title="Assessing Translation capabilities of Large Language Models involving English and Indian Languages"></a>Assessing Translation capabilities of Large Language Models involving English and Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09216">http://arxiv.org/abs/2311.09216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vandan Mujadia, Ashok Urlana, Yash Bhaskar, Penumalla Aditya Pavani, Kukkapalli Shravya, Parameswari Krishnamurthy, Dipti Misra Sharma</li>
<li>for: 这个研究的目的是探索大型自然语言模型（LLMs）在不同语言之间的机器翻译能力。</li>
<li>methods: 该研究使用了Raw Large Language Models（raw LLMs）和LoRA等 parameter efficient fine-tuning方法来进行翻译任务。</li>
<li>results: 研究结果表明，使用LLaMA-13b模型进行2 stage fine-tuning后，在英语到印度语言和印度语言到英语的翻译任务中都有显著进步，并取得了相对较高的BLEU和CHRF分数。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. In this work, our aim is to explore the multilingual capabilities of large language models by using machine translation as a task involving English and 22 Indian languages. We first investigate the translation capabilities of raw large language models, followed by exploring the in-context learning capabilities of the same raw models. We fine-tune these large language models using parameter efficient fine-tuning methods such as LoRA and additionally with full fine-tuning. Through our study, we have identified the best performing large language model for the translation task involving LLMs, which is based on LLaMA.   Our results demonstrate significant progress, with average BLEU scores of 13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99, 42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for English to Indian languages on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for Indian languages to English, we achieved average BLEU scores of 14.03, 16.65, 16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51, and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Overall, our findings highlight the potential and strength of large language models for machine translation capabilities, including for languages that are currently underrepresented in LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在不同的自然语言处理任务中显示出惊人的进步。在这项工作中，我们的目标是探索大型语言模型的多语言能力。我们使用机器翻译作为英语和22种印度语言之间的任务，首先调查 raw 大型语言模型的翻译能力，然后探索同样的 raw 模型在Context Learning中的表现。我们使用 LoRA 和完全 fine-tuning 等参数效率的 Parameter Efficient Fine-Tuning 方法进行微调。通过我们的研究，我们发现了基于 LLaMA 的最佳大型语言模型，用于机器翻译任务。我们的结果表明了显著的进步， Raw 翻译得分为13.42、15.93、12.13、12.30和12.07，以及 CHRF 得分为43.98、46.99、42.55、42.42和45.39，分别使用2013版LLaMA进行2 stage fine-tuning。在英语到印度语言的翻译任务中，我们实现了Raw 翻译得分为14.03、16.65、16.17、15.35和12.55，以及 CHRF 得分为36.71、40.44、40.26、39.51和36.20，分别使用 fine-tuned LLaMA-13b 在 IN22（交流）、IN22（通用）、flores200-dev、flores200-devtest 和 newstest2019 测试集上进行微调。在印度语言到英语的翻译任务中，我们实现了Raw 翻译得分为14.03、16.65、16.17、15.35和12.55，以及 CHRF 得分为36.71、40.44、40.26、39.51和36.20，分别使用 fine-tuned LLaMA-13b 在 IN22（交流）、IN22（通用）、flores200-dev、flores200-devtest 和 newstest2019 测试集上进行微调。总之，我们的发现表明了大型语言模型在机器翻译任务中的潜力和优势，包括目前在LLMs中尚未得到足够的关注的语言。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Text-Summarization-Unraveling-Challenges-Approaches-and-Prospects-–-A-Survey"><a href="#Controllable-Text-Summarization-Unraveling-Challenges-Approaches-and-Prospects-–-A-Survey" class="headerlink" title="Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects – A Survey"></a>Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects – A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09212">http://arxiv.org/abs/2311.09212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ashokurlana/controllable_text_summarization_survey">https://github.com/ashokurlana/controllable_text_summarization_survey</a></li>
<li>paper_authors: Ashok Urlana, Pruthwik Mishra, Tathagato Roy, Rahul Mishra</li>
<li>for: 本研究的目的是探讨控制性文本摘要（CTS）任务的多方面特点和挑战，以及现有方法和数据集的评估。</li>
<li>methods: 本文首先将CtrlTS任务正式定义，然后根据共同特点和目标将CtrlTS方法分类，并对每个类别的现有方法和数据集进行了详细的评估。</li>
<li>results: 本文的研究发现了CtrlTS任务的一些限制和研究漏洞，同时也提出了未来研究的可能性和方向。<details>
<summary>Abstract</summary>
Generic text summarization approaches often fail to address the specific intent and needs of individual users. Recently, scholarly attention has turned to the development of summarization methods that are more closely tailored and controlled to align with specific objectives and user needs. While a growing corpus of research is devoted towards a more controllable summarization, there is no comprehensive survey available that thoroughly explores the diverse controllable aspects or attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable aspects according to their shared characteristics and objectives, and present a thorough examination of existing methods and datasets within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also delving into potential solutions and future directions for CTS.
</details>
<details>
<summary>摘要</summary>
通常的文本概要方法 often fails to address the specific intent and needs of individual users. Recently, scholarly attention has turned to the development of summarization methods that are more closely tailored and controlled to align with specific objectives and user needs. While a growing corpus of research is devoted towards a more controllable summarization, there is no comprehensive survey available that thoroughly explores the diverse controllable aspects or attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable aspects according to their shared characteristics and objectives, and present a thorough examination of existing methods and datasets within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also delving into potential solutions and future directions for CTS.
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Note-Enhancing-Robustness-in-Retrieval-Augmented-Language-Models"><a href="#Chain-of-Note-Enhancing-Robustness-in-Retrieval-Augmented-Language-Models" class="headerlink" title="Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models"></a>Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09210">http://arxiv.org/abs/2311.09210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu</li>
<li>for: 提高 Retrieval-augmented language models (RALMs) 的可靠性和可靠性，使其能够更好地面对噪音、无关文档和 unknown 问题。</li>
<li>methods: 提出了一种名为 Chain-of-Noting (CoN) 的新方法，通过生成文档的顺序读取笔记来评估文档的相关性和综合评价。</li>
<li>results: 对四个开放领域问答 benchmark 进行了实验，发现 CoN 可以大幅提高 RALMs 的性能，特别是在噪音文档和 unknown 问题上。<details>
<summary>Abstract</summary>
Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with "unknown" when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.
</details>
<details>
<summary>摘要</summary>
大型语言模型（RALM）表现了重要的进步，尤其是在减少假设错觉方面，通过利用外部知识源。然而，获取的信息可靠性并不总是保证。检索不相关的数据可能会导致异常的回答，甚至让模型忽略其内置的知识，尤其是当它拥有足够的信息来回答问题时。此外，标准RALM通常难以判断自己是否拥有足够的知识来提供正确的答案。在知识缺乏时，这些系统应该回答为“未知”，当答案不可获取时。为了解决这些挑战，我们提出了链条记录（CoN），一种新的方法，旨在提高RALM在噪音、不相关文档中的稳定性，以及处理未知情况。CoN的核心思想是生成检索文档的顺序读取笔记，以评估其与给定问题的 relevance，并将这些信息集成到形成最终答案。我们使用ChatGPT创建了培训数据集，然后在LLaMa-2 7B模型上训练CoN。我们在四个开放领域问答benchmark上进行了实验，结果显示，RALM equipped with CoN与标准RALM相比，显著提高了EM score（+7.9）和拒绝率（+10.5）。特别是，在完全噪音检索文档时，CoN表现了平均提高+7.9的EM score，在超出预训练知识范围的实时问题时，CoN表现了平均提高+10.5的拒绝率。
</details></li>
</ul>
<hr>
<h2 id="Fusion-Eval-Integrating-Evaluators-with-LLMs"><a href="#Fusion-Eval-Integrating-Evaluators-with-LLMs" class="headerlink" title="Fusion-Eval: Integrating Evaluators with LLMs"></a>Fusion-Eval: Integrating Evaluators with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09204">http://arxiv.org/abs/2311.09204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, Lei Meng</li>
<li>for: 本研究旨在提出一种基于大语言模型（LLM）的评估方法，以便更好地评估大语言模型的性能。</li>
<li>methods: 本研究使用了多种评估方法，包括人工评估、模型评估和自动评估指标的混合方法，以便更好地利用多个参考。</li>
<li>results: 在测试SummEval数据集上，Fusion-Eval实现了Spearman相关性0.96，超越其他评估器。这表明Fusion-Eval可以很好地捕捉人类对大语言模型的评估，设置了新的标准在LLM评估领域。<details>
<summary>Abstract</summary>
Evaluating Large Language Models (LLMs) is a complex task, especially considering the intricacies of natural language understanding and the expectations for high-level reasoning. Traditional evaluations typically lean on human-based, model-based, or automatic-metrics-based paradigms, each with its own advantages and shortcomings. We introduce "Fusion-Eval", a system that employs LLMs not solely for direct evaluations, but to skillfully integrate insights from diverse evaluators. This gives Fusion-Eval flexibility, enabling it to work effectively across diverse tasks and make optimal use of multiple references. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman correlation of 0.96, outperforming other evaluators. The success of Fusion-Eval underscores the potential of LLMs to produce evaluations that closely align human perspectives, setting a new standard in the field of LLM evaluation.
</details>
<details>
<summary>摘要</summary>
评估大型自然语言模型（LLM）是一项复杂的任务，尤其是在自然语言理解方面和高级逻辑预期方面。传统评估方法通常是人类基于、模型基于或自动指标基于的方法，每种方法都有其优点和缺点。我们介绍了“Fusion-Eval”系统，它利用 LLM 不仅 direct 评估，而是通过综合融合多个评估者的意见来进行评估。这使得 Fusion-Eval 具有灵活性，能够在多种任务中工作效果，并且能够有效地利用多个参考。在 SummEval 数据集上测试时，Fusion-Eval 达到了 Spearman 相关系数 0.96，超越其他评估者。Fusion-Eval 的成功表明了 LLM 的潜在力量，可以生成与人类观点相似的评估结果，为 LLM 评估领域做出了新的标准。
</details></li>
</ul>
<hr>
<h2 id="ExpM-NF-Differentially-Private-Machine-Learning-that-Surpasses-DPSGD"><a href="#ExpM-NF-Differentially-Private-Machine-Learning-that-Surpasses-DPSGD" class="headerlink" title="ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD"></a>ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09200">http://arxiv.org/abs/2311.09200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert A. Bridges, Vandy J. Tombs, Christopher B. Stanley</li>
<li>For: train machine learning models on private data with pre-specified differential privacy guarantee.* Methods: Exponential Mechanism (ExpM) and an auxiliary Normalizing Flow (NF).* Results: Achieves greater than 93% of the non-private training accuracy for a wide range of privacy parameters, exhibiting greater accuracy and privacy than the state-of-the-art method (DPSGD).Here’s the simplified Chinese text:</li>
<li>for: 用于在具有预先确定的分布式隐私保障的私人数据上训练机器学习模型。</li>
<li>methods: 使用泛化机制（ExpM）和辅助正则化流（NF）。</li>
<li>results: 在各种隐私参数下，达到了非私有训练精度的93%以上，与现有的标准方法（DPSGD）相比，具有更高的精度和更低的隐私水平。<details>
<summary>Abstract</summary>
In this pioneering work we formulate ExpM+NF, a method for training machine learning (ML) on private data with pre-specified differentially privacy guarantee $\varepsilon>0, \delta=0$, by using the Exponential Mechanism (ExpM) and an auxiliary Normalizing Flow (NF). We articulate theoretical benefits of ExpM+NF over Differentially Private Stochastic Gradient Descent (DPSGD), the state-of-the-art (SOTA) and de facto method for differentially private ML, and we empirically test ExpM+NF against DPSGD using the SOTA implementation (Opacus with PRV accounting) in multiple classification tasks on the Adult Dataset (census data) and MIMIC-III Dataset (electronic healthcare records) using Logistic Regression and GRU-D, a deep learning recurrent neural network with ~20K-100K parameters. In all experiments, ExpM+NF achieves greater than 93% of the non-private training accuracy (AUC) for $\varepsilon \in [1\mathrm{e}{-3}, 1]$, exhibiting greater accuracy (higher AUC) and privacy (lower $\varepsilon$ with $\delta=0$) than DPSGD. Differentially private ML generally considers $\varepsilon \in [1,10]$ to maintain reasonable accuracy; hence, ExpM+NF's ability to provide strong accuracy for orders of magnitude better privacy (smaller $\varepsilon$) substantially pushes what is currently possible in differentially private ML. Training time results are presented showing ExpM+NF is comparable to (slightly faster) than DPSGD. Code for these experiments will be provided after review. Limitations and future directions are provided.
</details>
<details>
<summary>摘要</summary>
在这项先锋性的研究中，我们提出了ExpM+NF方法，用于在私人数据上训练机器学习（ML），并 garantía了先前 specify的不同性隐私保证 $\varepsilon>0, \delta=0$。我们详细说明了ExpM+NF方法在对比DP-SGD（ differentially private stochastic gradient descent）方法时的理论优势，并通过多个分类任务在 Adult 数据集和 MIMIC-III 数据集使用Logistic Regression和 GRU-D（一个深度学习循环神经网络，参数约20K-100K）进行了实验测试。在所有实验中，ExpM+NF方法可以达到非私人训练精度的大于93%的水平（AUC），并且在私人性和精度两个方面都表现出了优势。在 differentially private ML 中通常consider $\varepsilon \in [1,10]$以保持合理的精度，因此ExpM+NF方法的能力提供较强的精度在许多次 Privacy (smaller $\varepsilon$)是 differentially private ML 领域中的一项重要进展。另外，我们还提供了训练时间结果，显示ExpM+NF方法与 DPSGD 相当（甚至是其所 faster）。我们将在审核后提供代码。本文的限制和未来发展方向也被提供。
</details></li>
</ul>
<hr>
<h2 id="Never-Lost-in-the-Middle-Improving-Large-Language-Models-via-Attention-Strengthening-Question-Answering"><a href="#Never-Lost-in-the-Middle-Improving-Large-Language-Models-via-Attention-Strengthening-Question-Answering" class="headerlink" title="Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering"></a>Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09198">http://arxiv.org/abs/2311.09198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, Jiaxing Zhang</li>
<li>for: 提高大型语言模型在长文本上的信息寻找和反思能力，解决大多数语言模型在中部正确信息寻找时的”lost in the middle”问题。</li>
<li>methods: 提出了一种特种任务 called Attention Strengthening Multi-doc QA (ASM QA)，以强化语言模型在长文本上的信息寻找和反思能力。</li>
<li>results: 实验结果表明，使用这种任务后，模型在多文档问答和其他标准任务上表现出了显著改善，与当前最佳模型相比，在随机设置下提高了13.7%的绝对性能，在 passage retrieval 任务上提高了21.5%。<details>
<summary>Abstract</summary>
While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The "lost in the middle" problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Attention Strengthening Multi-doc QA (ASM QA). Following these tasks, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7% absolute gain in shuffled settings, by 21.5% in passage retrieval task. We release our model, Ziya-Reader to promote related research in the community.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在进行长文本输入时，具有更长的文本输入能力，但在长文本上却陷入了“lost in the middle”问题。这个问题表示大多数 LLM 在正确信息位于中间时，精度会显著下降。为了解决这个重要问题，这篇论文提出了增强 LLM 在长文本上寻找信息和反射能力的方法，通过特别设计的任务称为 Attention Strengthening Multi-doc QA (ASM QA)。这些任务使我们的模型能够更精确地寻找所需信息。实验结果显示，我们的模型在多文本选择和其他评量指标中表现出了明显改善，相比之前的模型，具有13.7%的绝对优势和21.5%的过页数优势。我们发布了我们的模型，Ziya-Reader，以促进相关的研究在社区。
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Chain-of-Thought-in-Complex-Vision-Language-Reasoning-Task"><a href="#The-Role-of-Chain-of-Thought-in-Complex-Vision-Language-Reasoning-Task" class="headerlink" title="The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task"></a>The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09193">http://arxiv.org/abs/2311.09193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oguz, James C. Gee, Yixin Nie</li>
<li>for: 这个研究探讨了链条思维方法在复杂视觉语言任务中的效果，这种方法已知能够提高语言任务的效率，并且可以将任务拆分成子任务和中间步骤。</li>
<li>methods: 这个研究使用了”描述然后决策”策略，这种策略是基于人类信号处理的机制。这种策略在探测任务中提高了性能，提高了50%。</li>
<li>results: 这个研究发现，使用”描述然后决策”策略可以在复杂视觉语言任务中提高探测任务的性能，提高50%。这 laid the foundation for future research on reasoning paradigms in complex vision-language tasks.<details>
<summary>Abstract</summary>
The study explores the effectiveness of the Chain-of-Thought approach, known for its proficiency in language tasks by breaking them down into sub-tasks and intermediate steps, in improving vision-language tasks that demand sophisticated perception and reasoning. We present the "Description then Decision" strategy, which is inspired by how humans process signals. This strategy significantly improves probing task performance by 50%, establishing the groundwork for future research on reasoning paradigms in complex vision-language tasks.
</details>
<details>
<summary>摘要</summary>
研究探讨了链条思维方法的效果，这种方法通过将语言任务拆分成子任务和中间步骤，提高视语任务的效果。我们提出了“描述然后决策”策略，这种策略受人类信号处理的启发。这种策略在探测任务中提高效果 by 50%，为复杂视语任务的理解理论提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Towards-Verifiable-Text-Generation-with-Symbolic-References"><a href="#Towards-Verifiable-Text-Generation-with-Symbolic-References" class="headerlink" title="Towards Verifiable Text Generation with Symbolic References"></a>Towards Verifiable Text Generation with Symbolic References</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09188">http://arxiv.org/abs/2311.09188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim</li>
<li>for: 提高大型自然语言模型（LLM）的输出的可靠性和准确性，以便在高风险应用中使用。</li>
<li>methods: 使用符号附加的生成方法（SymGen），让 LLM 在生成文本时间间插入明确的符号参考，以便更好地展示生成的来源和证明。</li>
<li>results: 在数据到文本和问答实验中，LLM 能够直接生成符号参考的文本，保持流畅性和准确性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated an impressive ability to synthesize plausible and fluent text. However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult. This paper proposes symbolically grounded generation (SymGen) as a simple approach for enabling easier validation of an LLM's output. SymGen prompts an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data (e.g., a table in JSON format). The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification. Across data-to-text and question answering experiments, we find that LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generate-Filter-and-Fuse-Query-Expansion-via-Multi-Step-Keyword-Generation-for-Zero-Shot-Neural-Rankers"><a href="#Generate-Filter-and-Fuse-Query-Expansion-via-Multi-Step-Keyword-Generation-for-Zero-Shot-Neural-Rankers" class="headerlink" title="Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers"></a>Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09175">http://arxiv.org/abs/2311.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, Michael Bendersky</li>
<li>for: 提高零批 Retriever 的召回率和精度，探索现有state-of-the-art cross-encoder ranker中Query Expansion的影响。</li>
<li>methods: 提出了一个名为GFF的管道，包括一个大型自然语言模型和一个神经网络排序器，用于生成、筛选和融合查询扩展更有效地，以提高零批 nDCG@10 指标。</li>
<li>results: 通过GFF管道，实现了在 BEIR 和 TREC DL 2019&#x2F;2020 上提高零批 nDCG@10。同时，通过不同模型选择来分析GFF管道中的不同模型选择，并为未来Query Expansion在零批神经rankers中的发展提供了指导。<details>
<summary>Abstract</summary>
Query expansion has been proved to be effective in improving recall and precision of first-stage retrievers, and yet its influence on a complicated, state-of-the-art cross-encoder ranker remains under-explored. We first show that directly applying the expansion techniques in the current literature to state-of-the-art neural rankers can result in deteriorated zero-shot performance. To this end, we propose GFF, a pipeline that includes a large language model and a neural ranker, to Generate, Filter, and Fuse query expansions more effectively in order to improve the zero-shot ranking metrics such as nDCG@10. Specifically, GFF first calls an instruction-following language model to generate query-related keywords through a reasoning chain. Leveraging self-consistency and reciprocal rank weighting, GFF further filters and combines the ranking results of each expanded query dynamically. By utilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 on BEIR and TREC DL 2019/2020. We also analyze different modelling choices in the GFF pipeline and shed light on the future directions in query expansion for zero-shot neural rankers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>查询扩展已经证明可以提高首个采集器的回归精度和准确率，然而它们对现代扩展排序器的影响仍未得到充分探索。我们首先表明，直接在当前文献中使用扩展技术可能会导致zero-shot性能下降。为此，我们提出了GFF，一个包含大语言模型和神经排序器的管道，用于更有效地生成、筛选和融合查询扩展以提高zero-shot排序指标 such as nDCG@10。具体来说，GFF首先通过一个遵循逻辑链的语言模型生成查询相关的关键词。然后，通过自 consistency和对称排序权重，GFF进一步筛选并将每个扩展查询的排名结果进行动态融合。通过这个管道，我们显示了GFF可以提高zero-shot nDCG@10在BEIR和TREC DL 2019/2020。我们还分析了GFF管道中不同的模型选择，并 shed light on the future directions in query expansion for zero-shot neural rankers。
</details></li>
</ul>
<hr>
<h2 id="AbsPyramid-Benchmarking-the-Abstraction-Ability-of-Language-Models-with-a-Unified-Entailment-Graph"><a href="#AbsPyramid-Benchmarking-the-Abstraction-Ability-of-Language-Models-with-a-Unified-Entailment-Graph" class="headerlink" title="AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph"></a>AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09174">http://arxiv.org/abs/2311.09174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/abspyramid">https://github.com/hkust-knowcomp/abspyramid</a></li>
<li>paper_authors: Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang, Sehyun Choi, Xin Liu, Yangqiu Song</li>
<li>for: 本研究旨在探讨语言模型具备抽象能力的基础知识，而这一点尚未得到充分探讨。</li>
<li>methods: 本研究提出了AbsPyramid，一个包含221K文本描述抽象知识的统一推理图。现有资源只关注简化事件中的名词或特定领域中的动词，而AbsPyramid收集了多元事件中的抽象知识，用以全面评估语言模型在开放领域中的抽象能力。</li>
<li>results: 实验结果表明，现有的LLMs在零shot和少shot设定下面临抽象知识的挑战。通过在我们的充沛的抽象知识上训练，我们发现LLMs可以学习基本的抽象能力，并在未看过事件的情况下 generalized 到未看过的事件。同时，我们实际表明了我们的标准可以强化LLMs在两个以前的抽象任务中。<details>
<summary>Abstract</summary>
Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.
</details>
<details>
<summary>摘要</summary>
cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.Here's the word-for-word translation:认知研究表明，人类智能中的抽象能力是非常重要的，但是这一点尚未得到充分探索。在这篇论文中，我们提出了AbsPyramid，一个包含221万个文本描述抽象知识的统一谱系图。现有资源只是在简化事件中关注单个名词或动词，而AbsPyramid则收集了多元事件中的抽象知识，以全面评估语言模型在开放领域中的抽象能力。实验结果表明，当前的LLMs在零shot和几shot设定下面临抽象知识的挑战。通过我们的充足的抽象知识训练，我们发现LLMs可以获得基本的抽象能力，并在未看到的事件中进行推理。同时，我们实验表明，我们的标准可以提高LLMs在两个先前的抽象任务中。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Knowledge-Question-Answering-via-Abstract-Reasoning-Induction"><a href="#Temporal-Knowledge-Question-Answering-via-Abstract-Reasoning-Induction" class="headerlink" title="Temporal Knowledge Question Answering via Abstract Reasoning Induction"></a>Temporal Knowledge Question Answering via Abstract Reasoning Induction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09149">http://arxiv.org/abs/2311.09149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Chen, Dongfang Li, Xiang Zhao, Baotian Hu, Min Zhang</li>
<li>for: 这项研究旨在解决大语言模型（LLM）中的时间知识推理问题，这个问题导致模型经常生成错误或误导性信息，主要是因为它们对逐渐变化的事实知识和复杂的时间逻辑有限制。</li>
<li>methods: 我们提出了一种新的构建主义方法，强调在LLM学习中进行持续的知识合成和定制。这种方法包括Abstract Reasoning Induction（ARI）框架，将时间推理分为两个阶段：无知阶段和知识阶段。</li>
<li>results: 我们的方法在两个时间问答数据集上取得了显著的改善，相对于基eline的提升为29.7%和9.27%，证明了我们的方法在LLM中提高时间推理的有效性。代码将在<a target="_blank" rel="noopener" href="https://github.com/czy1999/ARI%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/czy1999/ARI上发布。</a><details>
<summary>Abstract</summary>
In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic. In response, we propose a novel, constructivism-based approach that advocates for a paradigm shift in LLM learning towards an active, ongoing process of knowledge synthesis and customization. At the heart of our proposal is the Abstract Reasoning Induction ARI framework, which divides temporal reasoning into two distinct phases: Knowledge-agnostic and Knowledge-based. This division aims to reduce instances of hallucinations and improve LLMs' capacity for integrating abstract methodologies derived from historical data. Our approach achieves remarkable improvements, with relative gains of 29.7\% and 9.27\% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code will be released at https://github.com/czy1999/ARI.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们解决了大型语言模型（LLM）中的时间知识推理挑战，这是这些模型经常遇到的Difficulties。这些Difficulties常常导致模型生成错误或误导性信息，主要因为它们对不断发展的事实知识和复杂的时间逻辑具有有限的处理能力。作为回应，我们提出了一种新的、建构主义基础的方法，强调了LLM学习的活跃、持续进行知识合成和自定义。我们的提议的核心是抽象逻辑概念抽取框架（ARI），将时间推理分为两个不同阶段：无知阶段和知识阶段。这种分离的目的是减少幻觉和提高LLM对抽象方法的应用能力，来自历史数据的抽象方法。我们的方法在两个时间问答 datasets上显示了remarkable improvement，相对提高29.7%和9.27%，证明了我们的方法在提高LLM时间推理能力的有效性。代码将在https://github.com/czy1999/ARI上发布。
</details></li>
</ul>
<hr>
<h2 id="Jailbreaking-GPT-4V-via-Self-Adversarial-Attacks-with-System-Prompts"><a href="#Jailbreaking-GPT-4V-via-Self-Adversarial-Attacks-with-System-Prompts" class="headerlink" title="Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts"></a>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09127">http://arxiv.org/abs/2311.09127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun</li>
<li>for: 本研究的目的是探讨 Multimodal Large Language Models (MLLMs) 的监禁攻击 vulnerability，尤其是模型输入的攻击而不是模型 API 的攻击。</li>
<li>methods: 本研究使用了一种新的监禁攻击方法，称为 SASP (Self-Adversarial Attack via System Prompt)，通过使用 GPT-4 作为自己的红人工具，对自己进行攻击，以搜索可能的监禁提示。此外，还添加了人工修改基于 GPT-4 的分析，以进一步提高攻击成功率。</li>
<li>results: 研究发现，修改系统提示可以显著降低监禁成功率。此外，还发现了一些可能的监禁攻击方法，可以用于增强 MLLM 的安全性。<details>
<summary>Abstract</summary>
Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities in model APIs. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully steal the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking, which could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.
</details>
<details>
<summary>摘要</summary>
existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities in model APIs. To fill the research gap, we carry out the following work:1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully steal the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs;2) Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7%;3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking, which could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.
</details></li>
</ul>
<hr>
<h2 id="HEALNet-–-Hybrid-Multi-Modal-Fusion-for-Heterogeneous-Biomedical-Data"><a href="#HEALNet-–-Hybrid-Multi-Modal-Fusion-for-Heterogeneous-Biomedical-Data" class="headerlink" title="HEALNet – Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data"></a>HEALNet – Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09115">http://arxiv.org/abs/2311.09115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Hemker, Nikola Smidjievski, Mateja Jamnik</li>
<li>for: 该研究旨在开发一种可以处理多modal数据的深度学习模型，以提高肿瘤病例预测和诊断的精度。</li>
<li>methods: 该模型使用混合早期融合注意力学习网络（HEALNet），可以保持模态特有的结构信息，同时捕捉模态之间的交互信息，并能够有效地处理训练和推理中缺失的模态。</li>
<li>results: 在四个TCGA肿瘤 cohort 上进行多modal 存活分析，HEALNet 达到了当前最佳性能，substantially 超过了uni-modal和recent multi-modal基elines，并在缺失模态的情况下保持稳定性。<details>
<summary>Abstract</summary>
Technological advances in medical data collection such as high-resolution histopathology and high-throughput genomic sequencing have contributed to the rising requirement for multi-modal biomedical modelling, specifically for image, tabular, and graph data. Most multi-modal deep learning approaches use modality-specific architectures that are trained separately and cannot capture the crucial cross-modal information that motivates the integration of different data sources. This paper presents the Hybrid Early-fusion Attention Learning Network (HEALNet): a flexible multi-modal fusion architecture, which a) preserves modality-specific structural information, b) captures the cross-modal interactions and structural information in a shared latent space, c) can effectively handle missing modalities during training and inference, and d) enables intuitive model inspection by learning on the raw data input instead of opaque embeddings. We conduct multi-modal survival analysis on Whole Slide Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas (TCGA). HEALNet achieves state-of-the-art performance, substantially improving over both uni-modal and recent multi-modal baselines, whilst being robust in scenarios with missing modalities.
</details>
<details>
<summary>摘要</summary>
科技发展在医疗数据收集方面，如高纬度 histopathology 和高通量 genomic 测序，导致了多modal 生物医学模型的需求增加，特别是图像、表格和图数据。大多数多modal 深度学习方法使用专门的modal architecture，在不同数据源之间进行分离的训练，无法捕捉到关键的交叉modal 信息，这些信息是集成不同数据源的关键因素。本文提出了 Hybrid Early-fusion Attention Learning Network (HEALNet)：一种灵活的多modal 融合架构，具有以下特点：a) 保留modal 特有的结构信息b) 捕捉交叉modal 交互和结构信息在共享的射频空间中c) 可以效果地处理训练和推理中缺失的modald) 可以直观地检查模型，学习在原始数据输入上而不是隐藏的表示我们在TCGA 四个肿瘤减剂中进行多modal 存活分析，HEALNet 实现了状态机器的表现，大幅超越uni-modal 和最近的多modal 基线，同时在缺失modal 情况下 display 强健。
</details></li>
</ul>
<hr>
<h2 id="Ever-Mitigating-Hallucination-in-Large-Language-Models-through-Real-Time-Verification-and-Rectification"><a href="#Ever-Mitigating-Hallucination-in-Large-Language-Models-through-Real-Time-Verification-and-Rectification" class="headerlink" title="Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification"></a>Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09114">http://arxiv.org/abs/2311.09114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoqiang Kang, Juntong Ni, Huaxiu Yao</li>
<li>for: 提高文本生成中的准确性和可靠性，解决非检索基本生成和检索增强生成中的幻见和误差问题。</li>
<li>methods: 采用实时步进生成和幻见纠正策略，在文本生成过程中实时检测和纠正幻见。</li>
<li>results: 相比基eline，Ever在多种任务上表现出显著改善，包括短文Question Answering、生成传记和多步论证。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Does-Pre-trained-Language-Model-Actually-Infer-Unseen-Links-in-Knowledge-Graph-Completion"><a href="#Does-Pre-trained-Language-Model-Actually-Infer-Unseen-Links-in-Knowledge-Graph-Completion" class="headerlink" title="Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?"></a>Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09109">http://arxiv.org/abs/2311.09109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe</li>
<li>for: This paper aims to analyze the inference and memorization abilities of Pre-trained Language Model (PLM)-based Knowledge Graph Completion (KGC) methods.</li>
<li>methods: The authors propose a method for constructing synthetic datasets to evaluate the inference and memorization abilities of PLM-based KGC methods.</li>
<li>results: The authors find that PLMs acquire the inference abilities required for KGC through pre-training, but the performance improvements mostly come from textual information of entities and relations.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是分析基于预训言语模型（PLM）的知识图完成（KGC）方法的推理和记忆能力。</li>
<li>methods: 作者提出了一种用于评估 PLM-based KGC 方法的推理和记忆能力的 sintetic 数据构建方法。</li>
<li>results: 作者发现 PLM 通过预训言语模型获得了 KGC 所需的推理能力，但是表现提升主要来自实体和关系的文本信息。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) consist of links that describe relationships between entities. Due to the difficulty of manually enumerating all relationships between entities, automatically completing them is essential for KGs. Knowledge Graph Completion (KGC) is a task that infers unseen relationships between entities in a KG. Traditional embedding-based KGC methods, such as RESCAL, TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using only the knowledge from training data. In contrast, the recent Pre-trained Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training. Therefore, PLM-based KGC can estimate missing links between entities by reusing memorized knowledge from pre-training without inference. This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications. To address this issue, we analyze whether PLM-based KGC methods make inferences or merely access memorized knowledge. For this purpose, we propose a method for constructing synthetic datasets specified in this analysis and conclude that PLMs acquire the inference abilities required for KGC through pre-training, even though the performance improvements mostly come from textual information of entities and relations.
</details>
<details>
<summary>摘要</summary>
知识 graphs (KGs) 由 link 描述实体之间的关系组成。由于手动列举所有实体之间的关系是不可能的，因此自动完成这些关系是知识 graphs 中的关键任务。知识 Graph Completion (KGC) 任务的目标是在知识 graphs 中推断未经seen的关系。传统的嵌入式 KGC 方法，如 RESCAL、TransE、DistMult、ComplEx、RotatE、HAKE、HousE 等，通过训练数据来INFER 缺失的链接。与此相反，近期的 Pre-trained Language Model (PLM)-based KGC 使用在预训练中获得的知识来完成缺失的链接。这种方法存在问题，因为建立 KGC 模型的目标是推断未经seen的关系，但常规的 KGC 评价不会分别考虑推断和记忆能力。因此，一个 PLM-based KGC 方法，即在当前 KGC 评价中表现出色，可能在实际应用中效果不佳。为了解决这个问题，我们分析了 PLM-based KGC 方法是否真的进行推断，还是仅仅访问记忆中的知识。为此，我们提出了一种方法来构建 synthetic 数据集，并结论 PLMs 通过预训练获得了 KGC 中需要的推断能力，尽管表现提升主要来自实体和关系之间的文本信息。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-View-of-Answer-Calibration-for-Multi-Step-Reasoning"><a href="#Towards-A-Unified-View-of-Answer-Calibration-for-Multi-Step-Reasoning" class="headerlink" title="Towards A Unified View of Answer Calibration for Multi-Step Reasoning"></a>Towards A Unified View of Answer Calibration for Multi-Step Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09101">http://arxiv.org/abs/2311.09101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shumin Deng, Ningyu Zhang, Nay Oo, Bryan Hooi</li>
<li>for: 提高多步逻辑能力</li>
<li>methods: 使用链条提示法 (Chain-of-Thought, CoT) 和答题均衡策略 (answer calibration)</li>
<li>results: 系统性地研究了不同答题策略的效果，可能提供关键的逻辑提高多步逻辑能力的关键因素。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. Usually, answer calibration strategies such as step-level or path-level calibration play a vital role in multi-step reasoning. While effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）透过链接思维（CoT）提示方法扩大多步骤理解能力的可能性。通常，答案调整策略如步骤级或路径级调整在多步骤理解中扮演着重要角色。 although effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.Here's the word-for-word translation:大型语言模型（LLMs）透过链接思维（CoT）提示方法扩大多步骤理解能力的可能性。通常，答案调整策略如步骤级或路径级调整在多步骤理解中扮演着重要角色。 although effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.
</details></li>
</ul>
<hr>
<h2 id="Can-MusicGen-Create-Training-Data-for-MIR-Tasks"><a href="#Can-MusicGen-Create-Training-Data-for-MIR-Tasks" class="headerlink" title="Can MusicGen Create Training Data for MIR Tasks?"></a>Can MusicGen Create Training Data for MIR Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09094">http://arxiv.org/abs/2311.09094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadine Kroher, Helena Cuesta, Aggelos Pikrakis</li>
<li>for: 这个论文是为了研究基于人工智能的生成音乐系统，用于生成用于音乐信息检索（MIR）任务的训练数据。</li>
<li>methods: 该论文使用了一种基于文本描述的生成模型，使用了50000个随机生成的音乐样本，以及五种不同的音乐类型。</li>
<li>results: 该研究显示，提出的模型可以从人工音乐追加特征中学习到真实音乐录音中的特征，并且这些特征可以泛化到真实音乐录音中。<details>
<summary>Abstract</summary>
We are investigating the broader concept of using AI-based generative music systems to generate training data for Music Information Retrieval (MIR) tasks. To kick off this line of work, we ran an initial experiment in which we trained a genre classifier on a fully artificial music dataset created with MusicGen. We constructed over 50 000 genre- conditioned textual descriptions and generated a collection of music excerpts that covers five musical genres. Our preliminary results show that the proposed model can learn genre-specific characteristics from artificial music tracks that generalise well to real-world music recordings.
</details>
<details>
<summary>摘要</summary>
我们正在研究基于人工智能的生成音乐系统来生成音乐信息检索（MIR）任务的训练数据。为了开始这条工作，我们进行了一次初步实验，我们使用MusicGen创建了一个完全人工的音乐集，并构建了50,000个频谱类别的文本描述。我们的初步结果表明，我们的模型可以从人工音乐轨迹中学习类别特征，这些特征可以通过到实际音乐录音中。
</details></li>
</ul>
<hr>
<h2 id="The-Uli-Dataset-An-Exercise-in-Experience-Led-Annotation-of-oGBV"><a href="#The-Uli-Dataset-An-Exercise-in-Experience-Led-Annotation-of-oGBV" class="headerlink" title="The Uli Dataset: An Exercise in Experience Led Annotation of oGBV"></a>The Uli Dataset: An Exercise in Experience Led Annotation of oGBV</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09086">http://arxiv.org/abs/2311.09086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnav Arora, Maha Jinadoss, Cheshta Arora, Denny George, Brindaalakshmi, Haseena Dawood Khan, Kirti Rawat, Div, Ritash, Seema Mathur, Shivani Yadav, Shehla Rashid Shora, Rie Raut, Sumit Pawar, Apurva Paithane, Sonia, Vivek, Dharini Priscilla, Khairunnisha, Grace Banu, Ambika Tandon, Rishav Thakker, Rahul Dev Korra, Aatman Vaidya, Tarunima Prabhakar</li>
<li>For: The paper aims to provide a dataset for automated detection of gendered abuse in three languages - Hindi, Tamil, and Indian English.* Methods: The paper uses a participatory approach to create a dataset of annotated tweets that pertain to the experience of gender abuse, using experts who identify as women or a member of the LGBTQIA community in South Asia.* Results: The paper presents a dataset of gendered abuse in three languages, which can be used to train AI systems for automated detection of hate speech and gendered abuse.<details>
<summary>Abstract</summary>
Online gender based violence has grown concomitantly with adoption of the internet and social media. Its effects are worse in the Global majority where many users use social media in languages other than English. The scale and volume of conversations on the internet has necessitated the need for automated detection of hate speech, and more specifically gendered abuse. There is, however, a lack of language specific and contextual data to build such automated tools. In this paper we present a dataset on gendered abuse in three languages- Hindi, Tamil and Indian English. The dataset comprises of tweets annotated along three questions pertaining to the experience of gender abuse, by experts who identify as women or a member of the LGBTQIA community in South Asia. Through this dataset we demonstrate a participatory approach to creating datasets that drive AI systems.
</details>
<details>
<summary>摘要</summary>
互联网上的性别基于暴力现象随着互联网和社交媒体的普及而增长。这些效果在全球主导语言社区更为严重，因为许多用户在社交媒体上使用非英语语言。因此，自动检测仇恨言论的需求增长，而特定于语言和文化的数据缺乏。本文提供了三种语言（旁遮普语、泰米尔语和印度英语）的性别暴力数据集。该数据集包括由专家评估员， identificas como女性或LGBTQIA社群成员在南亚地区，对性别暴力经历的三个问题进行了标注。通过这个数据集，我们展示了参与式的方法来创建AI系统驱动的数据集。
</details></li>
</ul>
<hr>
<h2 id="How-Multilingual-is-Multilingual-LLM"><a href="#How-Multilingual-is-Multilingual-LLM" class="headerlink" title="How Multilingual is Multilingual LLM?"></a>How Multilingual is Multilingual LLM?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09071">http://arxiv.org/abs/2311.09071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li</li>
<li>for: 本研究旨在评估大型自然语言处理器（LLM）在101种语言中的多语言能力，并将这些语言分为四个不同的 quadrant。</li>
<li>methods: 本研究采用了多种调整策略来提高LLM的多语言能力，并进行了广泛的实验。</li>
<li>results: 研究发现，现有的LLM在不同的语言中具有更高的多语言能力，并且可以通过特定的调整策略来进一步提高这种能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), trained predominantly on extensive English data, often exhibit limitations when applied to other languages. Current research is primarily focused on enhancing the multilingual capabilities of these models by employing various tuning strategies. Despite their effectiveness in certain languages, the understanding of the multilingual abilities of LLMs remains incomplete. This study endeavors to evaluate the multilingual capacity of LLMs by conducting an exhaustive analysis across 101 languages, and classifies languages with similar characteristics into four distinct quadrants. By delving into each quadrant, we shed light on the rationale behind their categorization and offer actionable guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass our expectations, and we can significantly improve the multilingual performance of LLMs by focusing on these distinct attributes present in each quadrant.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常在其主要基础于英语数据的训练下显示限制性，当应用于其他语言时。当前的研究主要集中在提高LLM的多语言能力，使用不同的调整策略。虽然在某些语言中表现有效，但我们对LLM的多语言能力的理解仍然不够完整。这项研究决心通过对101种语言进行极为详细的分析，将语言分为四个不同的 quadrant，并对每个 quadrant 进行深入的分析，以便更好地理解它们的分类原因，并提供调整这些语言的指导方针。广泛的实验表明，现有的LLM具有许多语言的多语言能力，并且可以通过专注于每个 quadrant 中的特定特征进行进一步改进。
</details></li>
</ul>
<hr>
<h2 id="How-Well-Do-Large-Language-Models-Truly-Ground"><a href="#How-Well-Do-Large-Language-Models-Truly-Ground" class="headerlink" title="How Well Do Large Language Models Truly Ground?"></a>How Well Do Large Language Models Truly Ground?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09069">http://arxiv.org/abs/2311.09069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunji Lee, Sejune Joo, Chaeeun Kim, Joel Jang, Doyoung Kim, Kyoung-Woon On, Minjoon Seo</li>
<li>for: 本研究旨在提高大语言模型（LLM）的可靠性和控制性，以增强其应用的可靠性和可控性。</li>
<li>methods: 本研究提出了一种严格的定义对大语言模型的挖根（grounding），即模型的回答（1） должны充分利用提供的知识背景，而且（2）不能超过知识背景中的知识。此外，本研究还提出了一个新的挖根度量以评估这种新定义，并在13种不同大小和训练方法的大语言模型上进行了实验，以了解影响挖根性能的因素。</li>
<li>results: 本研究发现，大语言模型的挖根性能受模型大小、训练方法和知识背景的影响。此外，本研究还发现了一些因素可以提高挖根性能，例如提供更多的知识背景和使用更好的训练方法。这些发现可以帮助改进大语言模型的可靠性和控制性，并促进其应用的可靠性和可控性。<details>
<summary>Abstract</summary>
Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge. To mitigate this, LLMs can be probed to generate responses by grounding on external context, often given as input (knowledge-augmented models). Yet, previous research is often confined to a narrow view of the term "grounding", often only focusing on whether the response contains the correct answer or not, which does not ensure the reliability of the entire response. To address this limitation, we introduce a strict definition of grounding: a model is considered truly grounded when its responses (1) fully utilize necessary knowledge from the provided context, and (2) don't exceed the knowledge within the contexts. We introduce a new dataset and a grounding metric to assess this new definition and perform experiments across 13 LLMs of different sizes and training methods to provide insights into the factors that influence grounding performance. Our findings contribute to a better understanding of how to improve grounding capabilities and suggest an area of improvement toward more reliable and controllable LLM applications.
</details>
<details>
<summary>摘要</summary>
靠内置知识的大语言模型（LLM）的依赖可能会导致问题，如幻觉、无控和变量知识的整合困难。为了解决这些问题，LLM可以通过附加外部知识作为输入（知识增强模型）来生成响应。然而，先前的研究通常受限于“附加”的概念，通常仅关注响应是否包含正确答案而不确保整个响应的可靠性。为此，我们提出了一个严格的定义：一个模型被视为真正地附加地理解当前上下文中的所有必要知识，而不超过上下文中的知识。我们介绍了一个新的数据集和一种附加度量来评估这个新定义，并在13种不同大小和训练方法的LLM上进行了实验，以提供更好地理解如何改进附加能力，以及提出一个向更可靠和可控的LLM应用程序的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Learning-Fair-Division-from-Bandit-Feedback"><a href="#Learning-Fair-Division-from-Bandit-Feedback" class="headerlink" title="Learning Fair Division from Bandit Feedback"></a>Learning Fair Division from Bandit Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09068">http://arxiv.org/abs/2311.09068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hakuei Yamada, Junpei Komiyama, Kenshi Abe, Atsushi Iwasaki</li>
<li>for: 学习在不纯粹知道代理人价值或利益下进行的线上公平分配</li>
<li>methods: 使用 wrapper 算法，利用 dual averaging 慢渐学习到来的物品类型分布和代理人价值，通过bandit反馈</li>
<li>results: 可以达到优化的尼亚希尔投资市场中代理人的加法式利益，并且可以获得误差 bound 和实际数据 validate 的优越性表现<details>
<summary>Abstract</summary>
This work addresses learning online fair division under uncertainty, where a central planner sequentially allocates items without precise knowledge of agents' values or utilities. Departing from conventional online algorithm, the planner here relies on noisy, estimated values obtained after allocating items. We introduce wrapper algorithms utilizing \textit{dual averaging}, enabling gradual learning of both the type distribution of arriving items and agents' values through bandit feedback. This approach enables the algorithms to asymptotically achieve optimal Nash social welfare in linear Fisher markets with agents having additive utilities. We establish regret bounds in Nash social welfare and empirically validate the superior performance of our proposed algorithms across synthetic and empirical datasets.
</details>
<details>
<summary>摘要</summary>
这个工作研究在不约束的情况下进行在线公平分配，其中中央规划者逐步分配物品不知道代理人的价值或利用率。与传统的在线算法不同，在这里中央规划者基于不准确的估计值进行分配。我们介绍了一种封包算法使用双均值，以慢慢地学习到来到的物品类型和代理人的价值，通过抽象反馈。这种方法可以在线性鱼钩市场中实现互斥社会财富的优化，并且我们证明了对社会财富的违和 bounds。我们还进行了synthetic和实际数据的实验 validate our proposed algorithms的优秀性。
</details></li>
</ul>
<hr>
<h2 id="In-vehicle-Sensing-and-Data-Analysis-for-Older-Drivers-with-Mild-Cognitive-Impairment"><a href="#In-vehicle-Sensing-and-Data-Analysis-for-Older-Drivers-with-Mild-Cognitive-Impairment" class="headerlink" title="In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment"></a>In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09273">http://arxiv.org/abs/2311.09273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sonia Moshfeghi, Muhammad Tanveer Jan, Joshua Conniff, Seyedeh Gol Ara Ghoreishi, Jinwoo Jang, Borko Furht, Kwangsoo Yang, Monica Rosselli, David Newman, Ruth Tappen, Dana Smith</li>
<li>for: 这研究旨在设计低成本的在车辆内部监测老年人驾驶性能的设备，并通过机器学习方法探测早期认知障碍的迹象。</li>
<li>methods: 该研究使用了低成本的在车辆内部监测设备，并运用机器学习方法对数据进行分析。</li>
<li>results: 研究发现，有MCI的 drivers 驾驶更稳定和安全，而非MCI的 drivers 则更容易出现异常驾驶行为。此外，数据分析还发现，夜间驾驶、总里程数和教育程度是最重要的因素。<details>
<summary>Abstract</summary>
Driving is a complex daily activity indicating age and disease related cognitive declines. Therefore, deficits in driving performance compared with ones without mild cognitive impairment (MCI) can reflect changes in cognitive functioning. There is increasing evidence that unobtrusive monitoring of older adults driving performance in a daily-life setting may allow us to detect subtle early changes in cognition. The objectives of this paper include designing low-cost in-vehicle sensing hardware capable of obtaining high-precision positioning and telematics data, identifying important indicators for early changes in cognition, and detecting early-warning signs of cognitive impairment in a truly normal, day-to-day driving condition with machine learning approaches. Our statistical analysis comparing drivers with MCI to those without reveals that those with MCI exhibit smoother and safer driving patterns. This suggests that drivers with MCI are cognizant of their condition and tend to avoid erratic driving behaviors. Furthermore, our Random Forest models identified the number of night trips, number of trips, and education as the most influential factors in our data evaluation.
</details>
<details>
<summary>摘要</summary>
驾驶是一项复杂的日常活动，表征年龄和疾病相关的认知下降。因此，比较驾驶性能与无轻度认知障碍（MCI）者可以反映认知功能的变化。有增加证据表明，在日常生活设定下不侵入式监测老年人驾驶性能可能能够探测潜在的认知变化。本文的目标包括设计低成本在车辆内部的感知硬件，获取高精度定位和电子邮件数据，确定识别认知下降的重要指标，并使用机器学习方法探测日常驾驶中的认知障碍迹象。我们的统计分析比较有MCI和无MCI driver的驾驶方式，发现有MCI driver驾驶更稳定和安全，这表示有MCI driver们对自己的状况有所了解，避免了异常的驾驶行为。此外，我们的Random Forest模型确定了夜晚出行次数、总出行次数和教育程度是我们数据评估中最重要的因素。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Knowledge-Editing-in-Language-Models-via-Relation-Perspective"><a href="#Assessing-Knowledge-Editing-in-Language-Models-via-Relation-Perspective" class="headerlink" title="Assessing Knowledge Editing in Language Models via Relation Perspective"></a>Assessing Knowledge Editing in Language Models via Relation Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09053">http://arxiv.org/abs/2311.09053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/knowledge-edit-based-on-relation-perspective">https://github.com/weiyifan1023/knowledge-edit-based-on-relation-perspective</a></li>
<li>paper_authors: Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song, Kang Liu</li>
<li>for: 本研究旨在探讨relation-centric的知识编辑方法，以改善大语言模型中的事实知识。</li>
<li>methods: 本研究使用了一个新的benchmark名为RaKE，用于评估relation based知识编辑方法。在本研究中，我们开发了一组创新的评价指标，并进行了多种知识编辑基线的全面实验。</li>
<li>results: 我们发现现有的知识编辑方法具有编辑关系的潜在困难，因此我们进一步探讨了在transformer中知识与关系之间的关系。我们的研究结果表明，知识与关系之间的相互作用不仅存在于FFN网络中，还存在于注意层中。这些结果为未来的关系基于的知识编辑方法提供了实验支持。<details>
<summary>Abstract</summary>
Knowledge Editing (KE) for modifying factual knowledge in Large Language Models (LLMs) has been receiving increasing attention. However, existing knowledge editing methods are entity-centric, and it is unclear whether this approach is suitable for a relation-centric perspective. To address this gap, this paper constructs a new benchmark named RaKE, which focuses on Relation based Knowledge Editing. In this paper, we establish a suite of innovative metrics for evaluation and conduct comprehensive experiments involving various knowledge editing baselines. We notice that existing knowledge editing methods exhibit the potential difficulty in their ability to edit relations. Therefore, we further explore the role of relations in factual triplets within the transformer. Our research results confirm that knowledge related to relations is not only stored in the FFN network but also in the attention layers. This provides experimental support for future relation-based knowledge editing methods.
</details>
<details>
<summary>摘要</summary>
知识编辑（KE），用于修改大型自然语言模型（LLM）中的事实知识，在过去几年 receiving increasing attention。然而，现有的知识编辑方法都是基于实体中心的，是否这种方法适用于关系中心的视角是一个不确定之处。为了解决这个差距，本文构建了一个新的标准 benchmark，名为RaKE，它专注于基于关系的知识编辑。在本文中，我们开发了一组创新的评价指标，并进行了多种知识编辑基准测试。我们发现，现有的知识编辑方法在编辑关系方面存在潜在的困难。因此，我们进一步探索关系在综合 triplets 中的角色。我们的研究结果表明，关系相关的知识不仅存储在 FFN 网络中，还存储在注意层中。这些结果为未来基于关系的知识编辑方法提供了实验支持。
</details></li>
</ul>
<hr>
<h2 id="Improving-Zero-shot-Visual-Question-Answering-via-Large-Language-Models-with-Reasoning-Question-Prompts"><a href="#Improving-Zero-shot-Visual-Question-Answering-via-Large-Language-Models-with-Reasoning-Question-Prompts" class="headerlink" title="Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts"></a>Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09050">http://arxiv.org/abs/2311.09050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecnu-dase-nlp/rqp">https://github.com/ecnu-dase-nlp/rqp</a></li>
<li>paper_authors: Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian</li>
<li>for: 这个研究旨在提高运算语言模型（LLMs）在零shot情况下的视觉问题回答能力。</li>
<li>methods: 这个研究使用了将图像转换为描述来桥接多modal的信息，并将大型语言模型（LLMs）应用其强大的零shot普遍能力来解答未见过的问题。</li>
<li>results: 这个研究发现，使用 reasoning question prompts 可以帮助 LLMs 在零shot情况下提高视觉问题回答能力，并在三个 VQA 挑战中实现了显著的改善。<details>
<summary>Abstract</summary>
Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at \url{https://github.com/ECNU-DASE-NLP/RQP}.
</details>
<details>
<summary>摘要</summary>
zero-shotVisual问题 answering（VQA）是一个引人注目的视觉语言任务，它检验系统在没有训练数据的情况下，对视觉和文本的理解能力。在最近的研究中，人们通过将图像转换成caption，使得多modal信息相互连接，大型自然语言模型（LLMs）可以通过未看过的问题来应用强大的零shot泛化能力。为了设计适合解决VQA问题的示例提问，许多研究已经探索了不同的策略来选择或生成问题答对，但它们完全忽略了问题提示的角色。原始的VQA问题通常会遇到括苍和模糊性，需要中间的逻辑推理。为此，我们提出了Visual Question Prompts（VQP），可以更好地激活LLMs在零shot场景中的潜力。具体来说，我们首先通过一个无supervised问题编辑模块，将每个问题转换成自包含的逻辑问题提示，考虑 sentence fluency、semantic integrity和syntactic invariance。每个逻辑问题提示清晰表达问题的意图。这些逻辑问题提示的候选答案，与其相关的自信度分数 acting as answer heuristics，被 feed into LLMs，并生成最终的答案。我们在三个VQA挑战中评估了逻辑问题提示，实验结果表明，它们可以在零shot设置下显著提高LLMs的成绩，并在四个数据集中超越现有的零shot方法。我们的源代码公开发布在 \url{https://github.com/ECNU-DASE-NLP/RQP}.
</details></li>
</ul>
<hr>
<h2 id="MELA-Multilingual-Evaluation-of-Linguistic-Acceptability"><a href="#MELA-Multilingual-Evaluation-of-Linguistic-Acceptability" class="headerlink" title="MELA: Multilingual Evaluation of Linguistic Acceptability"></a>MELA: Multilingual Evaluation of Linguistic Acceptability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09033">http://arxiv.org/abs/2311.09033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, Hai Hu</li>
<li>for: 本研究的目的是提供一个多语言的文本评估 benchmark，以评估大型自然语言模型（LLM）的语言能力。</li>
<li>methods: 本研究使用了多种方法，包括对 LLM 进行基本条件下的训练、跨语言和多任务学习等。</li>
<li>results: 研究结果显示，ChatGPT 优化后的性能仍然落后于 XLM-R 的优化版本，而 GPT-4 在零执行设定下的性能则与 XLM-R 的优化版本相近。跨语言和多任务学习实验显示，在接受性评估中，对于不同语言的训练数据是关键的。层内探索结果显示，XLM-R 的上层层次在多语言接受性评估中成为了任务特定但语言不同的区域。此外，研究还引入了“矛盾量”的概念，可能是跨语言传递中的困难指标。<details>
<summary>Abstract</summary>
Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting. Cross-lingual and multi-task learning experiments show that unlike semantic tasks, in-language training data is crucial in acceptability judgements. Results in layerwise probing indicate that the upper layers of XLM-R become a task-specific but language-agnostic region for multilingual acceptability judgment. We also introduce the concept of conflicting weight, which could be a potential indicator for the difficulty of cross-lingual transfer between languages. Our data will be available at https://github.com/sjtu-compling/MELA.
</details>
<details>
<summary>摘要</summary>
最近的大语言模型（LLM）测试主要集中在应用驱动的任务上，如复杂的理解和代码生成，这导致了语言学性评估的缺乏。为了解决这个问题，我们介绍了多语言可接受性评估（MELA），这是一个覆盖10种语言家族的多语言测试，共有48000个样本。我们设置了常用的LLM和监督模型的基线，并进行了跨语言传播和多任务学习实验。为了寻求多语言可读性，我们分析了精心调整的XLM-R的权重，以探索语言传播困难之间的转移。我们的结果表明，ChatGPT在受到上下文示例的帮助下表现出色，但仍落后于精心调整的XLM-R，而GPT-4在零上下文设定下和精心调整的XLM-R表现相当。跨语言和多任务学习实验表明，与semantic任务不同，在语言上的训练数据是关键在acceptability判断中。层次探索结果表明，XLM-R的上层变成了语言家族无关的任务特有的区域，用于多语言可接受性判断。我们还引入了 conflicting weight 概念，可能是跨语言传播中难度的指标。我们的数据将在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Robustness-of-Intelligence-Driven-Reinforcement-Learning"><a href="#Assessing-the-Robustness-of-Intelligence-Driven-Reinforcement-Learning" class="headerlink" title="Assessing the Robustness of Intelligence-Driven Reinforcement Learning"></a>Assessing the Robustness of Intelligence-Driven Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09027">http://arxiv.org/abs/2311.09027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Nodari, Federico Cerutti</li>
<li>for: 本研究旨在探讨奖励机器学习系统中噪声robustness问题。</li>
<li>methods: 本研究使用奖励机器学习来表达复杂的奖励结构，并通过证明和学习来强化现有的奖励学习方法。</li>
<li>results: 初步的结果表明现有的奖励学习方法需要进一步的证明和学习，以使其在实际应用中更加可靠。<details>
<summary>Abstract</summary>
Robustness to noise is of utmost importance in reinforcement learning systems, particularly in military contexts where high stakes and uncertain environments prevail. Noise and uncertainty are inherent features of military operations, arising from factors such as incomplete information, adversarial actions, or unpredictable battlefield conditions. In RL, noise can critically impact decision-making, mission success, and the safety of personnel. Reward machines offer a powerful tool to express complex reward structures in RL tasks, enabling the design of tailored reinforcement signals that align with mission objectives. This paper considers the problem of the robustness of intelligence-driven reinforcement learning based on reward machines. The preliminary results presented suggest the need for further research in evidential reasoning and learning to harden current state-of-the-art reinforcement learning approaches before being mission-critical-ready.
</details>
<details>
<summary>摘要</summary>
“对于强化学习系统而言，响应噪音的能力非常重要，尤其在军事上，因为高赌注和不确定的环境存在。噪音和不确定性是军事操作中的自然特征，可能来自于不完整的信息、敌对行为或不可预测的战场状况。在RL中，噪音可能严重影响决策、任务成功和人员安全。优先奖机器提供了一个强大的工具来表达复杂的奖赏结构，允许设计适应任务目标的优先奖赏。本文考虑了对于优先奖机器验证的问题。初步的结果显示需要进一步的证据推理和学习来强化现代强化学习方法，以便在任务 kritisch-ready 之前进行更多的研究。”
</details></li>
</ul>
<hr>
<h2 id="Identification-and-Estimation-for-Nonignorable-Missing-Data-A-Data-Fusion-Approach"><a href="#Identification-and-Estimation-for-Nonignorable-Missing-Data-A-Data-Fusion-Approach" class="headerlink" title="Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach"></a>Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09015">http://arxiv.org/abs/2311.09015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiao Wang, AmirEmad Ghassami, Ilya Shpitser</li>
<li>for: 这个论文是为了解决在数据缺失不是随机（MNAR）的情况下， Parameter of interest 的标定和估计问题。</li>
<li>methods: 该论文提出了一种 alternate approach，使用数据融合技术，将 MNAR 数据集中缺失的信息与 MAR 数据集中的信息 fusion 在一起，以便标定和估计 Parameter of interest。</li>
<li>results: 该论文表明，即使单个数据集中 Parameter of interest 无法标定，也可以通过合并数据集来标定它，只要满足两个 complementary 的假设。 并且提出了一种基于 inverse probability weighted（IPW）的估计方法，并通过 simulations 研究了该估计方法的性能。<details>
<summary>Abstract</summary>
We consider the task of identifying and estimating a parameter of interest in settings where data is missing not at random (MNAR). In general, such parameters are not identified without strong assumptions on the missing data model. In this paper, we take an alternative approach and introduce a method inspired by data fusion, where information in an MNAR dataset is augmented by information in an auxiliary dataset subject to missingness at random (MAR). We show that even if the parameter of interest cannot be identified given either dataset alone, it can be identified given pooled data, under two complementary sets of assumptions. We derive an inverse probability weighted (IPW) estimator for identified parameters, and evaluate the performance of our estimation strategies via simulation studies.
</details>
<details>
<summary>摘要</summary>
我团队考虑了在数据缺失不是随机（MNAR）的设置下确定和估计 parameter of interest。通常情况下，这些参数不能 без强大的缺失数据模型假设而被确定。在这篇论文中，我们采用了一种不同的方法，我们引入了基于数据融合的方法，其中MNAR数据集中的信息被融合了一个 auxiliary dataset 中的信息，其中数据缺失是随机的（MAR）。我们证明了，即使单独访问 Either dataset alone，parameter of interest 也无法被确定，但是通过将两个数据集融合在一起，可以在两个不同的假设集下确定这个参数。我们 derivate了一个 inverse probability weighted （IPW）估计器，并通过实验研究评估了我们的估计策略的性能。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-to-Reward-Machine-based-Reinforcement-Learning"><a href="#Adversarial-Attacks-to-Reward-Machine-based-Reinforcement-Learning" class="headerlink" title="Adversarial Attacks to Reward Machine-based Reinforcement Learning"></a>Adversarial Attacks to Reward Machine-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09014">http://arxiv.org/abs/2311.09014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Nodari</li>
<li>for: 本研究旨在对 reward machine (RM) 在机器学习设置下的安全性和可靠性进行分析，并提出一种新的攻击方法——盲目攻击。</li>
<li>methods: 本研究使用了 RM  formalism，并对 RM-based 技术进行了安全性和可靠性的分析。</li>
<li>results: 本研究发现了一种新的攻击方法——盲目攻击，并证明了这种攻击方法可以 efficiently 攻击 RM-based 技术。<details>
<summary>Abstract</summary>
In recent years, Reward Machines (RMs) have stood out as a simple yet effective automata-based formalism for exposing and exploiting task structure in reinforcement learning settings. Despite their relevance, little to no attention has been directed to the study of their security implications and robustness to adversarial scenarios, likely due to their recent appearance in the literature. With my thesis, I aim to provide the first analysis of the security of RM-based reinforcement learning techniques, with the hope of motivating further research in the field, and I propose and evaluate a novel class of attacks on RM-based techniques: blinding attacks.
</details>
<details>
<summary>摘要</summary>
近年来，奖励机器（RM）作为 automata-based 形式化的一种简单 yet effective 方法，在 reinforcement learning 设置下露出和利用任务结构。尽管它们在文献中具有重要性，但是它们的安全性和对抗性却受到了相对的少量关注，可能是因为它们在文献中的新出现。我的论文旨在提供 RM-based reinforcement learning 技术的首次安全分析，并提出和评估一种新的攻击方法：盲目攻击。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-AI-for-Natural-Disaster-Management-Takeaways-From-The-Moroccan-Earthquake"><a href="#Leveraging-AI-for-Natural-Disaster-Management-Takeaways-From-The-Moroccan-Earthquake" class="headerlink" title="Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake"></a>Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08999">http://arxiv.org/abs/2311.08999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morocco Solidarity Hackathon</li>
<li>for: 提高灾害管理策略</li>
<li>methods: 使用人工智能技术</li>
<li>results: 提供了一份全面的文献综述、赢得项目概述、关键发现和挑战（包括实时开源数据的不足、数据缺乏和跨学科合作障碍），并发起了社区呼吁。<details>
<summary>Abstract</summary>
The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023 prompted critical reflections on global disaster management strategies, resulting in a post-disaster hackathon, using artificial intelligence (AI) to improve disaster preparedness, response, and recovery. This paper provides (i) a comprehensive literature review, (ii) an overview of winning projects, (iii) key insights and challenges, namely real-time open-source data, data scarcity, and interdisciplinary collaboration barriers, and (iv) a community-call for further action.
</details>
<details>
<summary>摘要</summary>
惊人的6.8级地震在摩洛哥阿哈鲁斯市在2023年引发了全球灾害管理策略的批判性反思，导致了一场由人工智能（AI）改善灾害准备、应急回应和恢复的 poste-disaster hackathon。本文提供以下内容：（i）全面的文献综述（ii）赢得项目的概述（iii）关键的发现和挑战，包括实时开源数据、数据缺乏和跨学科协作障碍，以及（iv）社区呼吁进一步行动。
</details></li>
</ul>
<hr>
<h2 id="When-does-In-context-Learning-Fall-Short-and-Why-A-Study-on-Specification-Heavy-Tasks"><a href="#When-does-In-context-Learning-Fall-Short-and-Why-A-Study-on-Specification-Heavy-Tasks" class="headerlink" title="When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks"></a>When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08993">http://arxiv.org/abs/2311.08993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, Juanzi Li<br>for: 这篇论文旨在探讨大语言模型（LLM）在信息抽取任务中的限制，以及这些限制的下发原因。methods: 作者通过对18种特定任务进行了广泛的实验，并发现了三种主要的原因：无法具体理解上下文、任务架构理解与人类不协调、以及长文本理解能力不够。results: 作者发现，通过练习，LLM可以在这些任务中实现不错的表现，这表明ICL的失败不是LLM的内在缺陷，而是现有的配置方法不能够处理复杂的规范任务。<details>
<summary>Abstract</summary>
In-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on 18 specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands.
</details>
<details>
<summary>摘要</summary>
具有大型自然语言模型（LLM）的启发式学习（ICL）已经成为现代自然语言处理（NLP）中默认的方法，因此探索ICL的限制和理解其下面的原因变得非常重要。在这篇论文中，我们发现ICL在需要较多时间和精力进行学习的任务上表现不佳，例如传统的信息抽取任务。ICL的性能在这些任务上通常无法达到state-of-the-art的results的一半。为了探究这一问题的原因，我们在18个需要较多时间和精力进行学习的任务上进行了广泛的实验，并确定了三个主要原因：无法准确理解上下文，人类任务架构与模型的理解不一致，以及模型对长文本的理解能力不够。此外，我们发现通过 fine-tuning，LLMs可以在这些任务上实现 descent performance，这表明ICL的失败不是LLMs的内在缺陷，而是现有的对齐方法的缺陷，使得LLMs无法通过ICL处理复杂的specification-heavy任务。为了证明这一点，我们在LLMs上进行了专门的指令调整，并观察到了明显的改善。我们希望这些分析可以促进对齐方法的进步，使得LLMs能够更好地满足人类的需求。
</details></li>
</ul>
<hr>
<h2 id="Proceedings-Fifth-International-Workshop-on-Formal-Methods-for-Autonomous-Systems"><a href="#Proceedings-Fifth-International-Workshop-on-Formal-Methods-for-Autonomous-Systems" class="headerlink" title="Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems"></a>Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08987">http://arxiv.org/abs/2311.08987</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Marie Farrell, Matt Luckcuck, Mario Gleirscher, Maike Schwammberger</li>
<li>for: 本研讨会论文主要关注形式方法在自动化系统领域的应用。</li>
<li>methods: 本研讨会使用了多种形式方法，包括模拟、分析和验证。</li>
<li>results: 本研讨会的结果表明，通过使用形式方法，可以提高自动化系统的可靠性和安全性。<details>
<summary>Abstract</summary>
This EPTCS volume contains the proceedings for the Fifth International Workshop on Formal Methods for Autonomous Systems (FMAS 2023), which was held on the 15th and 16th of November 2023. FMAS 2023 was co-located with 18th International Conference on integrated Formal Methods (iFM) (iFM'22), organised by Leiden Institute of Advanced Computer Science of Leiden University. The workshop itself was held at Scheltema Leiden, a renovated 19th Century blanket factory alongside the canal.   FMAS 2023 received 25 submissions. We received 11 regular papers, 3 experience reports, 6 research previews, and 5 vision papers. The researchers who submitted papers to FMAS 2023 were from institutions in: Australia, Canada, Colombia, France, Germany, Ireland, Italy, the Netherlands, Sweden, the United Kingdom, and the United States of America. Increasing our number of submissions for the third year in a row is an encouraging sign that FMAS has established itself as a reputable publication venue for research on the formal modelling and verification of autonomous systems. After each paper was reviewed by three members of our Programme Committee we accepted a total of 15 papers: 8 long papers and 7 short papers.
</details>
<details>
<summary>摘要</summary>
这本 EPTCS 卷包含第五届国际形式方法工作坊（FMAS 2023）的会议论文，该会议于2023年11月15日-16日在荷兰列日大学计算机科学研究所举行。FMAS 2023 与第18届国际集成形式方法会议（iFM）（iFM'22）共同举办，该会议由列日大学计算机科学研究所组织。会议本身在列日市的一座19世纪 renovated 的布匹厂 alongside 运河举行。 FMAS 2023 接受了 25 篇提交的论文，其中包括 11 篇正式论文、3 篇经验报告、6 篇研究预览和 5 篇视野论文。参加者们来自：澳大利亚、加拿大、哥伦比亚、法国、德国、爱尔兰、意大利、荷兰、瑞典、英国和美国。这一年度的提交数量超过了前一年的同期，这是一个鼓舞人心的迹象，表明 FMAS 已经成为自动化系统的形式模型化和验证的可靠出版物。在三位程序委员会成员的审核后，我们一共接受了 15 篇论文：8 篇长篇论文和 7 篇短篇论文。
</details></li>
</ul>
<hr>
<h2 id="Linear-time-Evidence-Accumulation-Clustering-with-KMeans"><a href="#Linear-time-Evidence-Accumulation-Clustering-with-KMeans" class="headerlink" title="Linear time Evidence Accumulation Clustering with KMeans"></a>Linear time Evidence Accumulation Clustering with KMeans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09272">http://arxiv.org/abs/2311.09272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaëlle Candel</li>
<li>for: 这篇论文的目的是提出一种基于证据积累 clustering 的方法，以解决 ensemble clustering 中的计算问题。</li>
<li>methods: 该方法使用 CA 矩阵表示对Item之间的协 clustering频率，然后使用这个矩阵进行 clustering，以提取共识群集。与其他方法不同的是，这里不需要找到匹配于两个不同的分配中的集群。但是，该方法受到计算问题的限制，需要计算和存储一个 n x n 矩阵，其中 n 是 Item 的数量。</li>
<li>results: 该 paper 提出了一种简单的算法来计算粒度，从而降低了计算复杂性的问题。此外，paper 还证明了 k-means 自然地最大化了粒度。通过对多个 benchmark 数据集的实验， authors 发现了 k-means 和 bisecting 版本与其他状态之前的consensus算法相比，其结果与最佳状态之前的 NMI 相似，同时计算复杂性低。此外，k-means 还在粒度方面取得了最佳结果。这些结果表明，consensus clustering 可以使用简单的算法解决。<details>
<summary>Abstract</summary>
Among ensemble clustering methods, Evidence Accumulation Clustering is one of the simplest technics. In this approach, a co-association (CA) matrix representing the co-clustering frequency is built and then clustered to extract consensus clusters. Compared to other approaches, this one is simple as there is no need to find matches between clusters obtained from two different partitionings. Nevertheless, this method suffers from computational issues, as it requires to compute and store a matrix of size n x n, where n is the number of items. Due to the quadratic cost, this approach is reserved for small datasets. This work describes a trick which mimic the behavior of average linkage clustering. We found a way of computing efficiently the density of a partitioning, reducing the cost from a quadratic to linear complexity. Additionally, we proved that the k-means maximizes naturally the density. We performed experiments on several benchmark datasets where we compared the k-means and the bisecting version to other state-of-the-art consensus algorithms. The k-means results are comparable to the best state of the art in terms of NMI while keeping the computational cost low. Additionally, the k-means led to the best results in terms of density. These results provide evidence that consensus clustering can be solved with simple algorithms.
</details>
<details>
<summary>摘要</summary>
中 ensemble  clustering 方法中，证据积累 clustering 是其中一种最简单的方法。在这种方法中，建立一个 co-association（CA）矩阵，表示item 之间的协 clustering 频率，然后使用这个矩阵进行 clustering，以提取consensus 集群。与其他方法相比，这种方法更简单，因为无需在两个不同的分割结果中寻找匹配。然而，这种方法受到计算问题的限制，因为需要计算和存储一个 n x n 矩阵，其中 n 是物品的数量。由于 quadratic cost，这种方法只适用于小规模数据集。这篇文章描述了一种技巧，可以模拟average linkage clustering的行为。我们发现了一种计算效率的方法，可以减少计算复杂性从 quadratic 到 linear。此外，我们证明了 k-means 自然地 maximizes 分 partitions 的 densities。我们在一些标准 benchmark 数据集上进行了实验，与其他 state-of-the-art consensus 算法进行比较。k-means 结果与最佳 state of the art 相当，而且计算成本较低。此外，k-means 导致了最佳的结果，这些结果提供了证据，证明了 consensus clustering 可以使用简单的算法解决。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Linear-Relational-Concepts-in-Large-Language-Models"><a href="#Identifying-Linear-Relational-Concepts-in-Large-Language-Models" class="headerlink" title="Identifying Linear Relational Concepts in Large Language Models"></a>Identifying Linear Relational Concepts in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08968">http://arxiv.org/abs/2311.08968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Robot-learning">https://github.com/Aryia-Behroziuan/Robot-learning</a></li>
<li>paper_authors: David Chanin, Anthony Hunter, Oana-Maria Camburu</li>
<li>for: 这个论文的目的是找出 trasformer 语言模型（LM）中隐藏层的概念方向。</li>
<li>methods: 这个论文使用了线性关系模型（LRE）来模型主题和 объек的关系，并在隐藏层使用反向LRE来找出概念方向。</li>
<li>results: 这个方法可以有效地找出概念方向，并且可以作为分类器和模型输出 causal influence。<details>
<summary>Abstract</summary>
Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any given human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts at a given hidden layer in a transformer LM by first modeling the relation between subject and object as a linear relational embedding (LRE). While the LRE work was mainly presented as an exercise in understanding model representations, we find that inverting the LRE while using earlier object layers results in a powerful technique to find concept directions that both work well as a classifier and causally influence model outputs.
</details>
<details>
<summary>摘要</summary>
transformer语言模型（LM）已经显示了概念可以视为隐藏层活动空间中的方向。但是，为某个人类可解释的概念，如何在隐藏层中找到其方向呢？我们提出了一种技术 called直方关系概念（LRC），可以在 трансформа器LM中找到对应于人类可解释的概念方向。我们首先使用对象层的早期层来模型主题和对象之间的线性关系嵌入（LRE）。虽然LRE的工作主要是作为理解模型表示的一种实践，但我们发现，对LRE的逆转，使用早期对象层可以获得一种强大的技术，可以作为分类器并且在模型输出中引起影响。
</details></li>
</ul>
<hr>
<h2 id="I-Was-Blind-but-Now-I-See-Implementing-Vision-Enabled-Dialogue-in-Social-Robots"><a href="#I-Was-Blind-but-Now-I-See-Implementing-Vision-Enabled-Dialogue-in-Social-Robots" class="headerlink" title="I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots"></a>I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08957">http://arxiv.org/abs/2311.08957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulio Antonio Abbo, Tony Belpaeme</li>
<li>for: 这篇论文旨在探讨如何通过 integrate 视觉功能，使 conversational agent 更加Context-aware。</li>
<li>methods: 该论文使用最新的 Large Language Models (例如 GPT-4、IDEFICS) 将文本提示和实时视觉输入 interpreted 为 conversational agent 的响应。</li>
<li>results: 六次与 Furhat 机器人的交互， illustrate 了该系统的效果。<details>
<summary>Abstract</summary>
In the rapidly evolving landscape of human-computer interaction, the integration of vision capabilities into conversational agents stands as a crucial advancement. This paper presents an initial implementation of a dialogue manager that leverages the latest progress in Large Language Models (e.g., GPT-4, IDEFICS) to enhance the traditional text-based prompts with real-time visual input. LLMs are used to interpret both textual prompts and visual stimuli, creating a more contextually aware conversational agent. The system's prompt engineering, incorporating dialogue with summarisation of the images, ensures a balance between context preservation and computational efficiency. Six interactions with a Furhat robot powered by this system are reported, illustrating and discussing the results obtained. By implementing this vision-enabled dialogue system, the paper envisions a future where conversational agents seamlessly blend textual and visual modalities, enabling richer, more context-aware dialogues.
</details>
<details>
<summary>摘要</summary>
在人机交互方面的快速演化中，将视觉能力集成到对话代理人中是一项关键的进步。这篇论文介绍了一个使用最新的大语言模型（如GPT-4、IDEFICS）来增强传统的文本基于提示的对话管理器。这些语言模型能够同时解读文本提示和视觉刺激，创造更Contextually aware的对话代理人。系统的提示工程，包括对话和图像摘要，保证了对上下文的保持和计算效率的平衡。报告了六次与 Furhat 机器人运行这个系统的交互，并讨论了所获得的结果。通过实施这种视觉启发对话系统，论文预测将来，对话代理人将自然地融合文本和视觉模式，实现更加 ricjer、Contextually aware的对话。
</details></li>
</ul>
<hr>
<h2 id="Safety-Trust-and-Ethics-Considerations-for-Human-AI-Teaming-in-Aerospace-Control"><a href="#Safety-Trust-and-Ethics-Considerations-for-Human-AI-Teaming-in-Aerospace-Control" class="headerlink" title="Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control"></a>Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08943">http://arxiv.org/abs/2311.08943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kerianne L. Hobbs, Bernard Li</li>
<li>for: 这篇论文主要用于探讨人工智能在航空系统控制中的安全、信任和伦理性问题。</li>
<li>methods: 论文使用了各种方法来描述安全、信任和伦理性的概念，包括在人机合作中的应用。</li>
<li>results: 论文结果表明，安全、信任和伦理性是独立的概念，并且在人机合作中的决策过程中需要考虑这些因素。<details>
<summary>Abstract</summary>
Designing a safe, trusted, and ethical AI may be practically impossible; however, designing AI with safe, trusted, and ethical use in mind is possible and necessary in safety and mission-critical domains like aerospace. Safe, trusted, and ethical use of AI are often used interchangeably; however, a system can be safely used but not trusted or ethical, have a trusted use that is not safe or ethical, and have an ethical use that is not safe or trusted. This manuscript serves as a primer to illuminate the nuanced differences between these concepts, with a specific focus on applications of Human-AI teaming in aerospace system control, where humans may be in, on, or out-of-the-loop of decision-making.
</details>
<details>
<summary>摘要</summary>
设计安全、可信、伦理AI可能是实际不可能的;但是设计AI以安全、可信、伦理使用为目标是可能的并是必要的，尤其在安全和任务关键领域如航空系统控制中。安全、可信、伦理的使用可能会相互独立，例如：一个系统可能是安全的，但不是可信的或伦理的；一个系统可能是可信的，但不是安全的或伦理的；一个系统可能是伦理的，但不是安全的或可信的。这篇论文旨在突出这些概念之间的细腻差异，特别是在人机合作在航空系统控制中的应用。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-over-Description-Logic-based-Contexts-with-Transformers"><a href="#Reasoning-over-Description-Logic-based-Contexts-with-Transformers" class="headerlink" title="Reasoning over Description Logic-based Contexts with Transformers"></a>Reasoning over Description Logic-based Contexts with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08941">http://arxiv.org/abs/2311.08941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis</li>
<li>for: 这篇论文的目的是测试transformer模型在复杂的上下文中的理解能力。</li>
<li>methods: 这篇论文使用了生成自然语言问答dataset，来测试模型在不同的理解深度和句子长度下的表现。</li>
<li>results: 研究发现，使用DeBERTa模型DELTA$_M$，其理解能力随着理解深度的增加，却不受句子长度的影响，并且在未在训练中看到的理解深度上也能够展现出良好的适应能力。<details>
<summary>Abstract</summary>
One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that the performance of our DeBERTa-based model, DELTA$_M$, is marginally affected when the reasoning depth is increased and it is not affected at all when the length of the sentences is increasing. We also evaluate the generalization ability of the model on reasoning depths unseen at training, both increasing and decreasing, revealing interesting insights into the model's adaptive generalization abilities.
</details>
<details>
<summary>摘要</summary>
Currently, the state-of-the-art measure of the reasoning ability of transformer-based models is their accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of these contexts are very simple and are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we aim to answer the question of how well a transformer-based model will perform reasoning over expressive contexts. To achieve this, we construct a synthetic natural language question-answering dataset generated by description logic knowledge bases. We use the expressive language $\mathcal{ALCQ}$ to generate the knowledge bases, resulting in a dataset containing 384K examples that increase in two dimensions: i) reasoning depth, and ii) sentence length. Our DeBERTa-based model, DELTA$_M$, shows marginally affected performance when the reasoning depth is increased and is not affected at all when the sentence length increases. We also evaluate the generalization ability of the model on reasoning depths unseen at training, both increasing and decreasing, revealing interesting insights into the model's adaptive generalization abilities.Note: Simplified Chinese is used here as the target language, which is a standardized form of Chinese that is widely used in mainland China and Singapore. The translation may vary slightly depending on the specific dialect or regional variation.
</details></li>
</ul>
<hr>
<h2 id="Supported-Trust-Region-Optimization-for-Offline-Reinforcement-Learning"><a href="#Supported-Trust-Region-Optimization-for-Offline-Reinforcement-Learning" class="headerlink" title="Supported Trust Region Optimization for Offline Reinforcement Learning"></a>Supported Trust Region Optimization for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08935">http://arxiv.org/abs/2311.08935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, Xiangyang Ji</li>
<li>for:  solves the out-of-distribution and extrapolation issues in offline reinforcement learning</li>
<li>methods:  uses Supported Trust Region optimization (STR) with policy constraint within the support of the behavior policy</li>
<li>results:  guarantees strict policy improvement and safe exploration in the dataset, with state-of-the-art performance in MuJoCo locomotion domains and more challenging AntMaze domains.Here’s the translation of the abstract in Simplified Chinese:</li>
<li>for: 解决了线上强化学习中的出版物问题和推导错误</li>
<li>methods: 使用支持信任区优化(STR)，将策略限制在行为策略的支持内</li>
<li>results: 在假设无抽象和抽取误差时，STR确保策略改进直至 converges to the optimal support-constrained policy in the dataset，并且在实际结果中证明了 STR 在 MuJoCo 游戏域和更加挑战的 AntMaze 域中的state-of-the-art表现。<details>
<summary>Abstract</summary>
Offline reinforcement learning suffers from the out-of-distribution issue and extrapolation error. Most policy constraint methods regularize the density of the trained policy towards the behavior policy, which is too restrictive in most cases. We propose Supported Trust Region optimization (STR) which performs trust region policy optimization with the policy constrained within the support of the behavior policy, enjoying the less restrictive support constraint. We show that, when assuming no approximation and sampling error, STR guarantees strict policy improvement until convergence to the optimal support-constrained policy in the dataset. Further with both errors incorporated, STR still guarantees safe policy improvement for each step. Empirical results validate the theory of STR and demonstrate its state-of-the-art performance on MuJoCo locomotion domains and much more challenging AntMaze domains.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate the given text into Simplified Chinese.<</SYS>>在线执行学习受到异常分布问题和推导错误的影响。大多数策略限制方法将训练政策的摩尔分布正则化到行为策略中，这对于大多数情况是太Restrictive。我们提议支持信任区间优化（STR），它通过在行为策略支持下进行信任区间政策优化，享有较少的支持约束。我们证明，假设没有 aproximation和抽样误差，STR 可以保证每步 strict 政策改进，直到 converge 到数据集中的最佳支持约束政策。进一步，包括两种误差，STR 仍然可以保证每步安全的政策改进。 empirical 结果证明 STR 的理论和 AntMaze 领域的实验结果 validate  STR 的性能。Note: Simplified Chinese is used here, as it is the most widely used variety of Chinese in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Activation-Maximization-and-Generative-Adversarial-Training-to-Recognize-and-Explain-Patterns-in-Natural-Areas-in-Satellite-Imagery"><a href="#Leveraging-Activation-Maximization-and-Generative-Adversarial-Training-to-Recognize-and-Explain-Patterns-in-Natural-Areas-in-Satellite-Imagery" class="headerlink" title="Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery"></a>Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08923">http://arxiv.org/abs/2311.08923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Emam, Timo T. Stomberg, Ribana Roscher</li>
<li>for: 提高自然保护区域的定义和监测（improving the definition and monitoring of natural protected areas）</li>
<li>methods: 使用活化增强和生成对抗模型（using activation maximization and a generative adversarial model）</li>
<li>results: 生成更精准的归属图，提高了对自然保护区域的理解（generating more precise attribution maps, improving understanding of natural protected areas）<details>
<summary>Abstract</summary>
Natural protected areas are vital for biodiversity, climate change mitigation, and supporting ecological processes. Despite their significance, comprehensive mapping is hindered by a lack of understanding of their characteristics and a missing land cover class definition. This paper aims to advance the explanation of the designating patterns forming protected and wild areas. To this end, we propose a novel framework that uses activation maximization and a generative adversarial model. With this, we aim to generate satellite images that, in combination with domain knowledge, are capable of offering complete and valid explanations for the spatial and spectral patterns that define the natural authenticity of these regions. Our proposed framework produces more precise attribution maps pinpointing the designating patterns forming the natural authenticity of protected areas. Our approach fosters our understanding of the ecological integrity of the protected natural areas and may contribute to future monitoring and preservation efforts.
</details>
<details>
<summary>摘要</summary>
自然保护区是生物多样性、气候变化缓解和生态过程支持的重要组成部分。尽管它们的重要性，但抗拒保护区的完整地域覆盖缺乏了理解其特点和缺乏地域覆盖类别定义。本文旨在提高保护区的定义模式的解释。为此，我们提出了一种新的框架，使用活动最大化和生成对抗模型。通过这种方法，我们可以生成具有完整和有效解释功能的卫星图像，与领域知识结合，以描述保护区的空间和spectral特征，并帮助我们更好地理解保护区的生态完整性。我们的提议的框架可以生成更精确的归属地图， pinpointing保护区的定义模式，从而提高我们对保护区的理解，并可能对未来监测和保护做出贡献。
</details></li>
</ul>
<hr>
<h2 id="An-Empathetic-User-Centric-Chatbot-for-Emotional-Support"><a href="#An-Empathetic-User-Centric-Chatbot-for-Emotional-Support" class="headerlink" title="An Empathetic User-Centric Chatbot for Emotional Support"></a>An Empathetic User-Centric Chatbot for Emotional Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09271">http://arxiv.org/abs/2311.09271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanting Pan, Yixuan Tang, Yuchen Niu</li>
<li>for: 这篇论文探讨了俄罗契文化和人工智能之间的交叉点，尤其是游戏如何为年轻女性提供情感需求的满足。</li>
<li>methods: 这篇论文使用了大语言模型（LLM）技术来扩展传统的静止游戏剧本，创造出动态、情感响应的互动体验。</li>
<li>results: 我们在《快乐弹指南》游戏中实现了一个基于问答（QA）系统的情感伴侣虚拟助手，通过数据扩展和情感增强技术来提供真实和支持的伴侣式互动。<details>
<summary>Abstract</summary>
This paper explores the intersection of Otome Culture and artificial intelligence, particularly focusing on how Otome-oriented games fulfill the emotional needs of young women. These games, which are deeply rooted in a subcultural understanding of love, provide players with feelings of satisfaction, companionship, and protection through carefully crafted narrative structures and character development. With the proliferation of Large Language Models (LLMs), there is an opportunity to transcend traditional static game narratives and create dynamic, emotionally responsive interactions. We present a case study of Tears of Themis, where we have integrated LLM technology to enhance the interactive experience. Our approach involves augmenting existing game narratives with a Question and Answer (QA) system, enriched through data augmentation and emotional enhancement techniques, resulting in a chatbot that offers realistic and supportive companionship.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了乙女文化和人工智能的交叉点，尤其关注乙女游戏如何为年轻女性提供情感需求的满足。这些游戏，它们深受乙女文化下的爱情观念影响，为玩家提供满足、伴侣和保护的感受，通过优化的narraative结构和人物发展。随着大语言模型（LLM）的普及，有机会超越传统的静止游戏剧本，创造动态、情感响应的互动。我们在《战泪天命》案例研究中，通过 интеGRATE LLM技术，增强了交互体验。我们的方法包括在现有游戏剧本中添加问答（QA）系统，通过数据增强和情感增强技术，创造出真实且支持的伴侣 chatbot。
</details></li>
</ul>
<hr>
<h2 id="NormNet-Scale-Normalization-for-6D-Pose-Estimation-in-Stacked-Scenarios"><a href="#NormNet-Scale-Normalization-for-6D-Pose-Estimation-in-Stacked-Scenarios" class="headerlink" title="NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios"></a>NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09269">http://arxiv.org/abs/2311.09269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuttlet/normnet">https://github.com/shuttlet/normnet</a></li>
<li>paper_authors: En-Te Lin, Wei-Jie Lv, Ding-Tao Huang, Long Zeng</li>
<li>for: 提高排序场景中对象Scale的OBE准确性</li>
<li>methods: 提出了一种新的6DoF OPE网络（NormNet），通过点对点回归学习对象的尺度，然后通过Semantic segmentation和Affine变换将所有对象标准化到同一个尺度，最后通过共享的 pose estimator 来回归对象的6D姿态。</li>
<li>results: 通过实验表明，提出的方法可以在公共 benchmark 和MultiScale 数据集上达到领先的性能，并在实际场景中Robust 地估计对象的6D姿态。Here’s the English version of the summary for reference:</li>
<li>for: Improving the accuracy of object pose estimation in stacked scenarios</li>
<li>methods: Proposes a new 6DoF OPE network (NormNet) that learns object scales using point-wise regression, normalizes all objects to the same scale through semantic segmentation and affine transformation, and uses a shared pose estimator to recover the 6D poses.</li>
<li>results: Extensive experiments show that the proposed method achieves state-of-the-art performance on public benchmarks and the MultiScale dataset, and is robust to changes in object scale in real-world scenarios.<details>
<summary>Abstract</summary>
Existing Object Pose Estimation (OPE) methods for stacked scenarios are not robust to changes in object scale. This paper proposes a new 6DoF OPE network (NormNet) for different scale objects in stacked scenarios. Specifically, each object's scale is first learned with point-wise regression. Then, all objects in the stacked scenario are normalized into the same scale through semantic segmentation and affine transformation. Finally, they are fed into a shared pose estimator to recover their 6D poses. In addition, we introduce a new Sim-to-Real transfer pipeline, combining style transfer and domain randomization. This improves the NormNet's performance on real data even if we only train it on synthetic data. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on public benchmarks and the MultiScale dataset we constructed. The real-world experiments show that our method can robustly estimate the 6D pose of objects at different scales.
</details>
<details>
<summary>摘要</summary>
现有的排序场景中对象姿态估算（OPE）方法不具有对象比例变化的Robustness。本文提出了一种新的6DoF OPE网络（NormNet），用于不同比例的object在排序场景中估算6D姿态。具体来说，每个object的比例首先通过点对点回归学习。然后，所有object在排序场景中都被Semantic segmentation和Affine transformation正规化到同一个比例。最后，它们被 fed into a shared pose estimator来回归其6D姿态。此外，我们还介绍了一种新的Sim-to-Real传输管道，其 combining style transfer和Domain randomization。这使得NormNet在实际数据上表现出state-of-the-art的性能，即使只有在synthetic数据上进行了训练。广泛的实验表明，提议的方法在公共benchmark和我们自己构建的MultiScale dataset上 achieve state-of-the-art的性能。实际世界中的实验表明，我们的方法可以对不同比例的object进行Robustly的6D姿态估算。
</details></li>
</ul>
<hr>
<h2 id="Combining-Transfer-Learning-with-In-context-Learning-using-Blackbox-LLMs-for-Zero-shot-Knowledge-Base-Question-Answering"><a href="#Combining-Transfer-Learning-with-In-context-Learning-using-Blackbox-LLMs-for-Zero-shot-Knowledge-Base-Question-Answering" class="headerlink" title="Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering"></a>Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08894">http://arxiv.org/abs/2311.08894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayur Patidar, Avinash Singh, Riya Sawhney, Indrajit Bhattacharya, Mausam</li>
<li>for: 本研究旨在解决知识库问答（KBQA）问题中的零例转移学习 Setting，其中有大量源频率的标注训练数据，但target频率没有任何标注示例。</li>
<li>methods: 本研究使用了大量的无标注数据，并将source频率的标注数据与target频率的无标注数据结合使用，以实现零例转移学习。此外，我们还提出了基于黑板大语言模型（BLLM）的执行引导自我修正技术，以解耦转移学习和启用预训练模型。</li>
<li>results: 我们通过使用GrailQA作为源频率和WebQSP作为目标频率的实验，显示了我们的方法可以在两个阶段中提高性能，并且超越了现有的超参数化KBQA模型。此外，我们还发现在具有有限量的标注数据时，我们的方法可以在预训练模型的基础上具有显著的改进效果。<details>
<summary>Abstract</summary>
We address the zero-shot transfer learning setting for the knowledge base question answering (KBQA) problem, where a large volume of labeled training data is available for the source domain, but no such labeled examples are available for the target domain. Transfer learning for KBQA makes use of large volumes of unlabeled data in the target in addition to the labeled data in the source. More recently, few-shot in-context learning using Black-box Large Language Models (BLLMs) has been adapted for KBQA without considering any source domain data. In this work, we show how to meaningfully combine these two paradigms for KBQA so that their benefits add up. Specifically, we preserve the two stage retrieve-then-generate pipeline of supervised KBQA and introduce interaction between in-context learning using BLLMs and transfer learning from the source for both stages. In addition, we propose execution-guided self-refinement using BLLMs, decoupled from the transfer setting. With the help of experiments using benchmark datasets GrailQA as the source and WebQSP as the target, we show that the proposed combination brings significant improvements to both stages and also outperforms by a large margin state-of-the-art supervised KBQA models trained on the source. We also show that in the in-domain setting, the proposed BLLM augmentation significantly outperforms state-of-the-art supervised models, when the volume of labeled data is limited, and also outperforms these marginally even when using the entire large training dataset.
</details>
<details>
<summary>摘要</summary>
我们处理零组转移学习设定的知识库问题回答（KBQA）问题，其中有大量的预训练数据可用于源领域，但没有类似的预训练例子可用于目标领域。KBQA的转移学习使用目标领域的大量无标数据，以及源领域的预训练数据。在这个工作中，我们示出如何具体地结合这两个概念，以让它们的优点相互补充。具体来说，我们保留KBQA的两阶段检索-生成架构，并将在这两阶段中使用黑盒大型自然语言模型（BLLM）进行交互式学习。此外，我们提出了基于执行的自适应更新，使用BLLM进行执行指导自适应。通过使用 GrailQA 作为源和 WebQSP 作为目标的实验，我们显示了我们的提案可以在两个阶段中提供重要的改进，并且也超越了现有的预训练KBQA模型。此外，我们显示在内领域中，我们的 BLLM 增强可以在有限量的预训练数据情况下提供明显的改进，并且甚至在使用整个大型训练数据时也能够超越。
</details></li>
</ul>
<hr>
<h2 id="Advances-in-ACL2-Proof-Debugging-Tools"><a href="#Advances-in-ACL2-Proof-Debugging-Tools" class="headerlink" title="Advances in ACL2 Proof Debugging Tools"></a>Advances in ACL2 Proof Debugging Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08856">http://arxiv.org/abs/2311.08856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt Kaufmann, J Strother Moore</li>
<li>for: 本文主要针对ACL2用户的失败证明尝试进行分析和调试。</li>
<li>methods: 本文使用了ACL2版本8.5后的改进的break-rewrite工具和新的with-brr-data工具来调试失败证明。</li>
<li>results: 本文通过分析失败证明的原因和使用debug工具，帮助用户更好地使用ACL2证明工具。<details>
<summary>Abstract</summary>
The experience of an ACL2 user generally includes many failed proof attempts. A key to successful use of the ACL2 prover is the effective use of tools to debug those failures. We focus on changes made after ACL2 Version 8.5: the improved break-rewrite utility and the new utility, with-brr-data.
</details>
<details>
<summary>摘要</summary>
ACL2 用户通常会经历许多失败的证明尝试。为使用 ACL2 证明工具成功，需要有效地使用工具来调试失败。我们将关注 ACL2 Version 8.5 之后的更改：改进的 break-rewrite 工具和新的 with-brr-data 工具。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Gender-Bias-in-the-Translation-of-Gender-Neutral-Languages-into-English"><a href="#Evaluating-Gender-Bias-in-the-Translation-of-Gender-Neutral-Languages-into-English" class="headerlink" title="Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English"></a>Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08836">http://arxiv.org/abs/2311.08836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Rarrick, Ranjita Naik, Sundar Poudel, Vishal Chowdhary</li>
<li>for: 本研究旨在提供一个用于评估 gender bias 在 Machine Translation (MT) 中的benchmark，以及一种用于 Mitigation Strategies 的评估方法。</li>
<li>methods: 本研究使用了 GATE X-E 数据集，该数据集包含了从土耳其语、匈牙利语、芬兰语和波斯语到英语的人工翻译，每个翻译都有 feminine、masculine 和neutral 的变体。此外，研究还使用了 GPT-3.5 Turbo 构建的英语性别重写解决方案，用于评估 gender debiasing。</li>
<li>results: 研究发现，使用 GATE X-E 数据集和 GPT-3.5 Turbo 解决方案可以减少 Machine Translation 中的gender bias。同时，研究还发现了一些 linguistic phenomena 在翻译 rewrite 过程中的挑战。<details>
<summary>Abstract</summary>
Machine Translation (MT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies into gender bias in translations from gender-neutral languages such as Turkish into more strongly gendered languages like English, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants for each possible gender interpretation. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present an English gender rewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-search-algorithm-for-an-optimal-investment-problem-in-vehicle-sharing-systems"><a href="#A-search-algorithm-for-an-optimal-investment-problem-in-vehicle-sharing-systems" class="headerlink" title="A* search algorithm for an optimal investment problem in vehicle-sharing systems"></a>A* search algorithm for an optimal investment problem in vehicle-sharing systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08834">http://arxiv.org/abs/2311.08834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ba Luat Le, Layla Martin, Emrah Demir, Duc Minh Vu</li>
<li>for: 本研究探讨了一个最佳投资问题，它在汽车共享系统中出现。给定一个站点建设的集合，我们需要确定（1）站点建设的顺序和车辆数量来实现所有站点建设完毕，以及（2）车辆数量和分配来最大化共享系统运营时间内的总收益。</li>
<li>methods: 本研究使用了A<em>搜索算法来解决这个问题。A</em>算法是一种基于搜索的方法，可以快速地找到最佳解。</li>
<li>results: 计算实验表明，使用A<em>算法可以比使用常见的迪克斯特拉算法更快地解决这个问题，并且可以更好地寻找最佳解。未来的研究可以探讨新的可能性和应用，以及精确和近似A</em>算法的比较。<details>
<summary>Abstract</summary>
We study an optimal investment problem that arises in the context of the vehicle-sharing system. Given a set of locations to build stations, we need to determine i) the sequence of stations to be built and the number of vehicles to acquire in order to obtain the target state where all stations are built, and ii) the number of vehicles to acquire and their allocation in order to maximize the total profit returned by operating the system when some or all stations are open. The profitability associated with operating open stations, measured over a specific time period, is represented as a linear optimization problem applied to a collection of open stations. With operating capital, the owner of the system can open new stations. This property introduces a set-dependent aspect to the duration required for opening a new station, and the optimal investment problem can be viewed as a variant of the Traveling Salesman Problem (TSP) with set-dependent cost. We propose an A* search algorithm to address this particular variant of the TSP. Computational experiments highlight the benefits of the proposed algorithm in comparison to the widely recognized Dijkstra algorithm and propose future research to explore new possibilities and applications for both exact and approximate A* algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究一个优化投资问题，发生在汽车共享系统的 контексте。给定一组位置建站，我们需要确定：一、站点建设顺序和购买汽车数量，以实现所有站点建成的目标状态，二、购买汽车数量和分配方式，以最大化在一些或所有站点打开时间内的总收益。系统在运行时的收益，通过一个时间段内的线性优化问题表示，对一组打开的站点进行优化。有投资资金，系统所有者可以开新站点。这种财务特性会导致开新站点所需的时间取决于集合，并且优化投资问题可以视为一种与集合相互作用的旅馆销售人问题（TSP）的变种。我们提议使用A*搜索算法解决这个特定的TSP变种。计算实验表明，我们提议的算法在与广泛认可的迪金斯特拉算法进行比较时，具有显著的优势。未来研究可以探讨新的可能性和应用，以及精度和近似A*算法的探索。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Links-between-Conversational-Agent-Design-Challenges-and-Interdisciplinary-Collaboration"><a href="#Exploring-Links-between-Conversational-Agent-Design-Challenges-and-Interdisciplinary-Collaboration" class="headerlink" title="Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration"></a>Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08832">http://arxiv.org/abs/2311.08832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malak Sadek, Céline Mougenot</li>
<li>for: 这篇论文旨在探讨对话代理（CA）的创造和应用中存在的社会技术挑战。</li>
<li>methods: 本文使用一种分析方法，即通过对CA创造过程中的多种社会技术挑战进行分类和描述，并提出了一些实践方法来超越这些挑战。</li>
<li>results: 本文提出了一个基于多元合作（IDC）的CA设计挑战分类体系，并提出了一些实践方法来超越这些挑战。未来的研究可以通过实践 verify 这些概念链接和在CA设计中应用这些方法来评估其效果。<details>
<summary>Abstract</summary>
Recent years have seen a steady rise in the popularity and use of Conversational Agents (CA) for different applications, well before the more immediate impact of large language models. This rise has been accompanied by an extensive exploration and documentation of the challenges of designing and creating conversational agents. Focusing on a recent scoping review of the socio-technical challenges of CA creation, this opinion paper calls for an examination of the extent to which interdisciplinary collaboration (IDC) challenges might contribute towards socio-technical CA design challenges. The paper proposes a taxonomy of CA design challenges using IDC as a lens, and proposes practical strategies to overcome them which complement existing design principles. The paper invites future work to empirically verify suggested conceptual links and apply the proposed strategies within the space of CA design to evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
近年来，对话代理（CA）在不同应用领域的 популярность和使用量有了明显的增长趋势，这与大语言模型的更 immediate影响之前。这种增长被陪伴了详细的探索和文献记录，关于对话代理的设计和创造的挑战。本 opinio paper 呼吁对 CA 设计挑战的评估，特别是通过交叉学科合作（IDC）的视角来评估这些挑战。文章提出了 CA 设计挑战的税onomy，并提供了实践的措施来超越这些挑战，这些措施与现有的设计原则相 complement。文章邀请未来的工作来实证提出的概念链接和应用提议的方法，以评估其效果。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-with-Model-Predictive-Control-for-Highway-Ramp-Metering"><a href="#Reinforcement-Learning-with-Model-Predictive-Control-for-Highway-Ramp-Metering" class="headerlink" title="Reinforcement Learning with Model Predictive Control for Highway Ramp Metering"></a>Reinforcement Learning with Model Predictive Control for Highway Ramp Metering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08820">http://arxiv.org/abs/2311.08820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filippoairaldi/mpcrl-for-ramp-metering">https://github.com/filippoairaldi/mpcrl-for-ramp-metering</a></li>
<li>paper_authors: Filippo Airaldi, Bart De Schutter, Azita Dabiri</li>
<li>for: 提高城市和高速公路交通系统的效率</li>
<li>methods: 结合模型驱动和学习驱动的方法，通过嵌入人工智能技术来提高高速公路上的汽车流控制</li>
<li>results: 实验表明，通过使用提出的方法，可以从一个不精确的模型和不良调节的控制器中学习改进控制策略，从而减轻网络中的拥堵和满足安全限制，提高交通效率。<details>
<summary>Abstract</summary>
In the backdrop of an increasingly pressing need for effective urban and highway transportation systems, this work explores the synergy between model-based and learning-based strategies to enhance traffic flow management by use of an innovative approach to the problem of highway ramp metering control that embeds Reinforcement Learning techniques within the Model Predictive Control framework. The control problem is formulated as an RL task by crafting a suitable stage cost function that is representative of the traffic conditions, variability in the control action, and violations of a safety-critical constraint on the maximum number of vehicles in queue. An MPC-based RL approach, which merges the advantages of the two paradigms in order to overcome the shortcomings of each framework, is proposed to learn to efficiently control an on-ramp and to satisfy its constraints despite uncertainties in the system model and variable demands. Finally, simulations are performed on a benchmark from the literature consisting of a small-scale highway network. Results show that, starting from an MPC controller that has an imprecise model and is poorly tuned, the proposed methodology is able to effectively learn to improve the control policy such that congestion in the network is reduced and constraints are satisfied, yielding an improved performance compared to the initial controller.
</details>
<details>
<summary>摘要</summary>
在城市和高速公路交通系统中增加压力的背景下，这项工作探讨了模型基于和学习基于策略的协同作用，以提高交通流管理的效果。这里提出一种将再征学习技术embedded在模型预测控制框架中的新 Approach，用于解决高速公路上的加速度控制问题。控制问题被定义为一个RL任务，并通过设计合适的stage cost函数来表示交通条件、控制动作的变化和安全约束的违反。一种MPC-based RL方法，其将两个框架的优点相结合，以超越每个框架的缺点，学习高速公路上的加速度控制策略，并满足系统模型不确定性和变化的需求。最后，通过Literature中的一个小规模高速公路网络的Benchmark进行了 simulations，结果表明，从一个粗略的MPC控制器开始，提出的方法能够有效地学习改善控制策略，使交通堵塞降低，约束满足，与初始控制器相比，性能得到提高。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-based-Dataset-Distillation"><a href="#Frequency-Domain-based-Dataset-Distillation" class="headerlink" title="Frequency Domain-based Dataset Distillation"></a>Frequency Domain-based Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08819">http://arxiv.org/abs/2311.08819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdh0818/fred">https://github.com/sdh0818/fred</a></li>
<li>paper_authors: Donghyeok Shin, Seungjae Shin, Il-Chul Moon</li>
<li>for: 这个研究是为了开发一个新的实现小型实验数据集的方法，利用频域来对大型原始数据集进行抽象。</li>
<li>methods: 这个方法使用频率领域的对称转换来优化各数据实例的频率表现，并根据频率维度的解释能力进行选择，以实现实验数据集的简洁化。</li>
<li>results: 这个方法能够在有限的预算下实现更好的资料简洁化，并且在不同的评估场景下与现有的对抗方法相比，能够提高对抗方法的性能。<details>
<summary>Abstract</summary>
This paper presents FreD, a novel parameterization method for dataset distillation, which utilizes the frequency domain to distill a small-sized synthetic dataset from a large-sized original dataset. Unlike conventional approaches that focus on the spatial domain, FreD employs frequency-based transforms to optimize the frequency representations of each data instance. By leveraging the concentration of spatial domain information on specific frequency components, FreD intelligently selects a subset of frequency dimensions for optimization, leading to a significant reduction in the required budget for synthesizing an instance. Through the selection of frequency dimensions based on the explained variance, FreD demonstrates both theoretical and empirical evidence of its ability to operate efficiently within a limited budget, while better preserving the information of the original dataset compared to conventional parameterization methods. Furthermore, based on the orthogonal compatibility of FreD with existing methods, we confirm that FreD consistently improves the performances of existing distillation methods over the evaluation scenarios with different benchmark datasets. We release the code at https://github.com/sdh0818/FreD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MAP’s-not-dead-yet-Uncovering-true-language-model-modes-by-conditioning-away-degeneracy"><a href="#MAP’s-not-dead-yet-Uncovering-true-language-model-modes-by-conditioning-away-degeneracy" class="headerlink" title="MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy"></a>MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08817">http://arxiv.org/abs/2311.08817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Yoshida, Kartik Goyal, Kevin Gimpel</li>
<li>for: 本研究旨在探讨NLG模型中的模式问题，并提出一种 Conditional MAP Decoding 方法来解决这问题。</li>
<li>methods: 本研究使用了 exact-search 和 ACBS 两种方法来找到NLG模型的模式。</li>
<li>results: 研究发现，对NLG模型进行 Conditional MAP Decoding 可以提高模式的质量和有用性，并且可以从不需要模型更新的角度来解决模式问题。<details>
<summary>Abstract</summary>
It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has generally been attributed to either a fundamental inadequacy of modes in models or weaknesses in language modeling. Contrastingly in this work, we emphasize that degenerate modes can even occur in the absence of any model error, due to contamination of the training data. Specifically, we show that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate, implying that any models trained on it will be as well. As the unconditional mode of NLG models will often be degenerate, we therefore propose to apply MAP decoding to the model's distribution conditional on avoiding specific degeneracies. Using exact-search, we empirically verify that the length-conditional modes of machine translation models and language models are indeed more fluent and topical than their unconditional modes. For the first time, we also share many examples of exact modal sequences from these models, and from several variants of the LLaMA-7B model. Notably, the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue. Because of the cost of exact mode finding algorithms, we develop an approximate mode finding approach, ACBS, which finds sequences that are both high-likelihood and high-quality. We apply this approach to LLaMA-7B, a model which was not trained for instruction following, and find that we are able to elicit reasonable outputs without any finetuning.
</details>
<details>
<summary>摘要</summary>
历史观察表明，基于自然语言生成（NLG）模型的准确或近似MAP（模式寻找）解oding通常会导致缺乏创新性的输出（Stahlberg和Byrne，2019年，Holtzman等，2019年）。这一问题通常被归结于模型内部的缺陷或语言生成模型的弱点。然而，在本研究中，我们强调的是，模型训练数据中的杂杂音可以导致模型的模式变得缺乏创新性，而不是模型本身的缺陷。具体来说，我们表明了混合一小量低危险噪音到人口文本分布中可以使得数据分布的模式变得缺乏创新性，因此任何基于这种数据分布的模型都将受到影响。由于NLG模型的随机模式通常是缺乏创新性的，我们因此提议在模型的分布上使用MAP解oding，并且在满足特定的非缺乏创新性条件下进行搜索。通过精确搜索，我们证实了机器翻译模型和语言模型的长度 Conditional 模式更加流利和有主题，而其不conditional模式则更加缺乏创新性。此外，我们还分享了许多准确模式序列示例，包括多种LLaMA-7B模型的变种。尽管LLaMA模型的模式仍然缺乏创新性，这表明改进模型的能力并没有解决这个问题。由于准确模式找索算法的成本高，我们开发了一种近似模式找索算法，即ACBS。我们应用该算法于LLaMA-7B模型，并发现我们可以不需要训练而获得合理的输出。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Disentanglement-by-Leveraging-Structure-in-Data-Augmentations"><a href="#Self-Supervised-Disentanglement-by-Leveraging-Structure-in-Data-Augmentations" class="headerlink" title="Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations"></a>Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08815">http://arxiv.org/abs/2311.08815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cian Eastwood, Julius von Kügelgen, Linus Ericsson, Diane Bouchacourt, Pascal Vincent, Bernhard Schölkopf, Mark Ibrahim</li>
<li>for: 本研究旨在提出一种更原理性的自监表示学习方法，以便在训练时不需要手动排除不必要的特征。</li>
<li>methods: 本研究使用多个风格嵌入空间，其中每个风格嵌入空间是对所有增强except one的增强 invariant的，并且joint entropy是最大化的。</li>
<li>results: 我们通过 sintetic数据和ImageNet数据进行了实验，并证明了我们的方法的效果。<details>
<summary>Abstract</summary>
Self-supervised representation learning often uses data augmentations to induce some invariance to "style" attributes of the data. However, with downstream tasks generally unknown at training time, it is difficult to deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits of our approach on synthetic datasets and then present promising but limited results on ImageNet.
</details>
<details>
<summary>摘要</summary>
自我指导学习经常使用数据增强来induce一些数据"样式"特征的不变性。然而，下游任务通常 unknown at training time，难以在training时 deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits of our approach on synthetic datasets and then present promising but limited results on ImageNet.Here's the translation in Traditional Chinese:自我指导学习经常使用数据增强来induce一些数据"型式"特征的不变性。然而，下游任务通常 unknown at training time，难以在training时 deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits of our approach on synthetic datasets and then present promising but limited results on ImageNet.
</details></li>
</ul>
<hr>
<h2 id="SparseSpikformer-A-Co-Design-Framework-for-Token-and-Weight-Pruning-in-Spiking-Transformer"><a href="#SparseSpikformer-A-Co-Design-Framework-for-Token-and-Weight-Pruning-in-Spiking-Transformer" class="headerlink" title="SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer"></a>SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08806">http://arxiv.org/abs/2311.08806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Liu, Shanlin Xiao, Bo Li, Zhiyi Yu</li>
<li>for: 本研究旨在提高逻辑吞吐量的神经网络（SNN）模型的精简性和能效性，通过对Spikformer模型进行减少计算复杂性和参数量的优化。</li>
<li>methods: 本研究使用了 Lottery Ticket Hypothesis（LTH）和Token和Weight预选技术来实现SNN模型的精简。具体来说，我们在Spikformer模型中发现了一个非常稀疏（大于90%）的子网络，可以达到相似的性能水平，同时减少计算复杂性和参数量。此外，我们还设计了一个轻量级的 токен选择器模块，可以根据图像中辐射神经元的均值脉冲频率选择重要背景信息，从而减少计算量。</li>
<li>results: 我们的实验结果表明，使用我们的框架可以减少90%的模型参数量，同时降低Giga浮点运算数（GFLOPs）20%，而不会影响原始模型的精度。<details>
<summary>Abstract</summary>
As the third-generation neural network, the Spiking Neural Network (SNN) has the advantages of low power consumption and high energy efficiency, making it suitable for implementation on edge devices. More recently, the most advanced SNN, Spikformer, combines the self-attention module from Transformer with SNN to achieve remarkable performance. However, it adopts larger channel dimensions in MLP layers, leading to an increased number of redundant model parameters. To effectively decrease the computational complexity and weight parameters of the model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very sparse ($\ge$90%) subnetwork that achieves comparable performance to the original network. Furthermore, we also design a lightweight token selector module, which can remove unimportant background information from images based on the average spike firing rate of neurons, selecting only essential foreground image tokens to participate in attention calculation. Based on that, we present SparseSpikformer, a co-design framework aimed at achieving sparsity in Spikformer through token and weight pruning techniques. Experimental results demonstrate that our framework can significantly reduce 90% model parameters and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining the accuracy of the original model.
</details>
<details>
<summary>摘要</summary>
为了实现适量学习，我们提出了一种名为SparseSpikformer的共设框架。该框架结合了LTH（奖券猜测）和精简的 neuron 权重来实现Spikformer模型中的精简。我们还设计了一个轻量级的封装器模块，可以根据图像中脉冲发生率的平均值选择重要背景信息，从而减少不必要的计算量和模型参数。实验结果表明，我们的框架可以将模型参数减少90%，同时保持原始模型的准确性。此外，我们还可以降低GFLOPs值20%。
</details></li>
</ul>
<hr>
<h2 id="X-Eval-Generalizable-Multi-aspect-Text-Evaluation-via-Augmented-Instruction-Tuning-with-Auxiliary-Evaluation-Aspects"><a href="#X-Eval-Generalizable-Multi-aspect-Text-Evaluation-via-Augmented-Instruction-Tuning-with-Auxiliary-Evaluation-Aspects" class="headerlink" title="X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects"></a>X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08788">http://arxiv.org/abs/2311.08788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, Lifu Huang</li>
<li>for: 本研究旨在提高NLG评价中的多方面评价能力，使用X-Eval框架来评估文本质量在多个未知的评价方面。</li>
<li>methods: 本研究使用了两个学习阶段：vanilla instruction tuning阶段和加强的 instruction tuning阶段，以提高模型能够遵循评价指令的能力。同时，研究者还采用了一种数据生成策略，将人工评分笔记转化为多种NLG评价任务，以增加任务多样性。</li>
<li>results: 实验表明，X-Eval框架可以使得even a lightweight语言模型在多个NLG任务中达到与人类评价相似或更高的相关度，比如GPT-4等现有NLG评价器。<details>
<summary>Abstract</summary>
Natural Language Generation (NLG) typically involves evaluating the generated text in various aspects (e.g., consistency and naturalness) to obtain a comprehensive assessment. However, multi-aspect evaluation remains challenging as it may require the evaluator to generalize to any given evaluation aspect even if it's absent during training. In this paper, we introduce X-Eval, a two-stage instruction tuning framework to evaluate the text in both seen and unseen aspects customized by end users. X-Eval consists of two learning stages: the vanilla instruction tuning stage that improves the model's ability to follow evaluation instructions, and an enhanced instruction tuning stage that exploits the connections between fine-grained evaluation aspects to better assess text quality. To support the training of X-Eval, we collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance task diversity, we devise an augmentation strategy that converts human rating annotations into diverse forms of NLG evaluation tasks, including scoring, comparison, ranking, and Boolean question answering. Extensive experiments across three essential categories of NLG tasks: dialogue generation, summarization, and data-to-text coupled with 21 aspects in meta-evaluation, demonstrate that our X-Eval enables even a lightweight language model to achieve a comparable if not higher correlation with human judgments compared to the state-of-the-art NLG evaluators, such as GPT-4.
</details>
<details>
<summary>摘要</summary>
自然语言生成（NLG）通常需要评估生成的文本在多个方面（例如，一致性和自然性）以获得全面的评估。然而，多个方面评估仍然是一个挑战，因为评估人可能需要总结到任何给定的评估方面，即使在训练中没有出现过。在这篇论文中，我们介绍了X-Eval，一个两个阶段的指令调整框架，用于评估文本在已知和未知方面的质量。X-Eval包括两个学习阶段：一个普通的指令调整阶段，用于提高模型跟踪评估指令的能力，以及一个加强的指令调整阶段，用于更好地评估文本质量。为支持X-Eval的训练，我们收集了AspectInstruct数据集，这是第一个适用于多个方面NLG评估的指令调整数据集，覆盖27种多样化的评估方面，65个任务。为了增加任务多样性，我们设计了一种扩展策略，将人类评分笔记转换成多种NLG评估任务的不同形式，包括分数、比较、排名和布尔值问答。我们在对话生成、概要和数据到文本三个重要类NLG任务中进行了21个方面的meta评估，并证明了X-Eval可以使一个轻量级语言模型与人类评估相比或更高的相关性。
</details></li>
</ul>
<hr>
<h2 id="ICRA-Roboethics-Challenge-2023-Intelligent-Disobedience-in-an-Elderly-Care-Home"><a href="#ICRA-Roboethics-Challenge-2023-Intelligent-Disobedience-in-an-Elderly-Care-Home" class="headerlink" title="ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home"></a>ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08783">http://arxiv.org/abs/2311.08783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sveta Paster, Kantwon Rogers, Gordon Briggs, Peter Stone, Reuth Mirsky</li>
<li>for: 提高老人护理机构内 elderly 的生活质量</li>
<li>methods: 利用智能不遵从框架进行决策过程</li>
<li>results: 提供了一种能够在具有伦理意味的决策过程中进行决策的老人护理机构内机器人Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to improve the well-being of the elderly in elderly care homes by using service robots with the ability to make decisions with ethical implications.</li>
<li>methods: The paper proposes using the Intelligent Disobedience framework to enable the robot to deliberate over decisions with potential ethical implications.</li>
<li>results: The paper provides a solution for elderly care homes to have robots that can make decisions with ethical implications, which can enhance the well-being of the elderly.<details>
<summary>Abstract</summary>
With the projected surge in the elderly population, service robots offer a promising avenue to enhance their well-being in elderly care homes. Such robots will encounter complex scenarios which will require them to perform decisions with ethical consequences. In this report, we propose to leverage the Intelligent Disobedience framework in order to give the robot the ability to perform a deliberation process over decisions with potential ethical implications. We list the issues that this framework can assist with, define it formally in the context of the specific elderly care home scenario, and delineate the requirements for implementing an intelligently disobeying robot. We conclude this report with some critical analysis and suggestions for future work.
</details>
<details>
<summary>摘要</summary>
随着老年人口增长的预计，服务机器人在老年人寓所中提供了一个有前途的方式来提高老年人的生活质量。这些机器人将面临复杂的场景，需要它们在具有伦理意味的决策过程中进行慎重的考虑。在这份报告中，我们建议利用智能不遵守框架，让机器人在具有伦理意味的决策过程中进行慎重的考虑。我们列出了该框架可以帮助解决的问题，在老年人寓所场景中定义了智能不遵守的形式，并详细说明了实施智能不遵守机器人的需求。我们在报告结尾进行了深入的分析和未来工作的建议。
</details></li>
</ul>
<hr>
<h2 id="Adversarially-Robust-Spiking-Neural-Networks-Through-Conversion"><a href="#Adversarially-Robust-Spiking-Neural-Networks-Through-Conversion" class="headerlink" title="Adversarially Robust Spiking Neural Networks Through Conversion"></a>Adversarially Robust Spiking Neural Networks Through Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09266">http://arxiv.org/abs/2311.09266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/igitugraz/robustsnnconversion">https://github.com/igitugraz/robustsnnconversion</a></li>
<li>paper_authors: Ozan Özdenizci, Robert Legenstein</li>
<li>for: 提高深度神经网络（SNN）的抗击力性能，使其在多种应用中更加可靠。</li>
<li>methods: 提出一种可扩展的 Robust SNN 训练方法，通过将 ANN 转换为 SNN 来实现。该方法可以有效地承载多种 computationally demanding 的robust learning objective，并在 post-conversion 精度矫正阶段进行 adversarial 优化。</li>
<li>results: 经过实验证明，该方法可以在多种适应性攻击Setting中保持高效性和低延迟，并在某些情况下提供了纪录级的性能。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) provide an energy-efficient alternative to a variety of artificial neural network (ANN) based AI applications. As the progress in neuromorphic computing with SNNs expands their use in applications, the problem of adversarial robustness of SNNs becomes more pronounced. To the contrary of the widely explored end-to-end adversarial training based solutions, we address the limited progress in scalable robust SNN training methods by proposing an adversarially robust ANN-to-SNN conversion algorithm. Our method provides an efficient approach to embrace various computationally demanding robust learning objectives that have been proposed for ANNs. During a post-conversion robust finetuning phase, our method adversarially optimizes both layer-wise firing thresholds and synaptic connectivity weights of the SNN to maintain transferred robustness gains from the pre-trained ANN. We perform experimental evaluations in numerous adaptive adversarial settings that account for the spike-based operation dynamics of SNNs, and show that our approach yields a scalable state-of-the-art solution for adversarially robust deep SNNs with low-latency.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）提供了一种能效的人工神经网络（ANN）的替代方案，用于许多人工智能应用程序。然而，随着SNN在应用领域的进步，SNN的抗击势特性问题变得更加突出。与普遍研究的端到端对抗训练方法不同，我们提出了一种可扩展的稳定robust SNN训练方法。我们的方法可以有效地把一系列计算挑战性的robust学习目标传递到SNN上，并在后续的robust微调阶段通过对层级发射reshold和synaptic连接权重进行对抗优化来保持传递的robust性增强。我们在许多适应性攻击设定下进行实验评估，并显示了我们的方法可以实现低延迟、高稳定性的抗击势深度SNN。
</details></li>
</ul>
<hr>
<h2 id="Three-Conjectures-on-Unexpectedeness"><a href="#Three-Conjectures-on-Unexpectedeness" class="headerlink" title="Three Conjectures on Unexpectedeness"></a>Three Conjectures on Unexpectedeness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08768">http://arxiv.org/abs/2311.08768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Sileno, Jean-Louis Dessalles<br>for:* 这篇论文是关于 simplicity theory 中的不期望性的研究。methods:* 该论文采用了一种基于kolmogorov复杂度的方法来探索不期望性的理论基础。results:* 论文提出了三个理论假设：First,不期望性可以被看作是泛化 bayes 规则; Second, 不期望性的核心可以与世界的趋势跟踪函数相连接; Third,不期望性可以被视为世界 entropy 和观察者系统的多样性之间的一种度量。这些假设提供了一个扩展 probabilistic 和逻辑方法的框架，可能会带来新的理解，以及对 causal 关系的抽取和学习机制的研究。<details>
<summary>Abstract</summary>
Unexpectedness is a central concept in Simplicity Theory, a theory of cognition relating various inferential processes to the computation of Kolmogorov complexities, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, yet its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.
</details>
<details>
<summary>摘要</summary>
不期待性是简洁理论中的中心概念， relate various inferential processes to the computation of Kolmogorov complexities, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, yet its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as a constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.Here's a word-for-word translation of the text in Traditional Chinese:不期待性是简单理论中的中心概念， relate various inferential processes to the computation of Kolmogorov complexities, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, yet its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as a constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.
</details></li>
</ul>
<hr>
<h2 id="Combining-Past-Present-and-Future-A-Self-Supervised-Approach-for-Class-Incremental-Learning"><a href="#Combining-Past-Present-and-Future-A-Self-Supervised-Approach-for-Class-Incremental-Learning" class="headerlink" title="Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning"></a>Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08764">http://arxiv.org/abs/2311.08764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoshuang Chen, Zhongyi Sun, Ke Yan, Shouhong Ding, Hongtao Lu</li>
<li>for: 本研究旨在解决自适应类增长学习中的突然出现新类的问题，使模型可以适应新类的 sequential 出现，同时避免归类束落。</li>
<li>methods: 我们提出了一种自适应 CIL 框架 CPPF，它包括一个prototype clustering模块 (PC)、一个嵌入空间保留模块 (ESR) 和一个多教师涂抹模块 (MTD)。PC 模块在prototype层级保留了后续阶段的嵌入空间，ESR 模块在嵌入空间级别保留了当前阶段的嵌入空间，以准备未来的知识。MTD 模块保持了当前阶段的表示，避免了过去知识的干扰。</li>
<li>results: 我们在 CIFAR100 和 ImageNet100  datasets 进行了广泛的实验，结果表明，我们的提议方法可以提高自适应类增长学习的性能。<details>
<summary>Abstract</summary>
Class Incremental Learning (CIL) aims to handle the scenario where data of novel classes occur continuously and sequentially. The model should recognize the sequential novel classes while alleviating the catastrophic forgetting. In the self-supervised manner, it becomes more challenging to avoid the conflict between the feature embedding spaces of novel classes and old ones without any class labels. To address the problem, we propose a self-supervised CIL framework CPPF, meaning Combining Past, Present and Future. In detail, CPPF consists of a prototype clustering module (PC), an embedding space reserving module (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the ESR modules reserve embedding space for subsequent phases at the prototype level and the feature level respectively to prepare for knowledge learned in the future. 2) The MTD module maintains the representations of the current phase without the interference of past knowledge. One of the teacher networks retains the representations of the past phases, and the other teacher network distills relation information of the current phase to the student network. Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our proposed method boosts the performance of self-supervised class incremental learning. We will release code in the near future.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>类间逐步学习（CIL）targets the scenario where novel classes emerge continuously and sequentially, and the model should recognize these sequential novel classes while mitigating catastrophic forgetting. In a self-supervised manner, it is more challenging to avoid the conflict between the feature embedding spaces of novel classes and old ones without any class labels. To address this problem, we propose a self-supervised CIL framework called CPPF, which stands for Combining Past, Present, and Future. In detail, CPPF consists of a prototype clustering module (PC), an embedding space reserving module (ESR), and a multi-teacher distillation module (MTD).1. The PC and ESR modules reserve embedding space for subsequent phases at the prototype level and the feature level, respectively, to prepare for knowledge learned in the future.2. The MTD module maintains the representations of the current phase without the interference of past knowledge. One of the teacher networks retains the representations of the past phases, and the other teacher network distills relation information of the current phase to the student network.Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our proposed method boosts the performance of self-supervised class incremental learning. We will release the code in the near future.
</details></li>
</ul>
<hr>
<h2 id="Forms-of-Understanding-of-XAI-Explanations"><a href="#Forms-of-Understanding-of-XAI-Explanations" class="headerlink" title="Forms of Understanding of XAI-Explanations"></a>Forms of Understanding of XAI-Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08760">http://arxiv.org/abs/2311.08760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Buschmeier, Heike M. Buhl, Friederike Kern, Angela Grimminger, Helen Beierling, Josephine Fisher, André Groß, Ilona Horwath, Nils Klowait, Stefan Lazarov, Michael Lenke, Vivien Lohmer, Katharina Rohlfing, Ingrid Scharlau, Amit Singh, Lutz Terfloth, Anna-Lisa Vollmer, Yu Wang, Annedore Wilmes, Britta Wrede</li>
<li>for: 本文旨在探讨 Explainable Artificial Intelligence (XAI) 领域中的理解概念，并提出一种理解形式模型。</li>
<li>methods: 本文采用了多元视角，兼容计算机科学、语言学、社会学和心理学等领域的方法，探讨了理解的定义和其形式、评价和进程中的动态。</li>
<li>results: 本文提出了两种理解的可能的结果，即启用性（knowing how）和理解（knowing that），两者在不同的深度水平上存在紧密的相互关系。<details>
<summary>Abstract</summary>
Explainability has become an important topic in computer science and artificial intelligence, leading to a subfield called Explainable Artificial Intelligence (XAI). The goal of providing or seeking explanations is to achieve (better) 'understanding' on the part of the explainee. However, what it means to 'understand' is still not clearly defined, and the concept itself is rarely the subject of scientific investigation. This conceptual article aims to present a model of forms of understanding in the context of XAI and beyond. From an interdisciplinary perspective bringing together computer science, linguistics, sociology, and psychology, a definition of understanding and its forms, assessment, and dynamics during the process of giving everyday explanations are explored. Two types of understanding are considered as possible outcomes of explanations, namely enabledness, 'knowing how' to do or decide something, and comprehension, 'knowing that' -- both in different degrees (from shallow to deep). Explanations regularly start with shallow understanding in a specific domain and can lead to deep comprehension and enabledness of the explanandum, which we see as a prerequisite for human users to gain agency. In this process, the increase of comprehension and enabledness are highly interdependent. Against the background of this systematization, special challenges of understanding in XAI are discussed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese<</SYS>>文本：Explainability has become an important topic in computer science and artificial intelligence, leading to a subfield called Explainable Artificial Intelligence (XAI). The goal of providing or seeking explanations is to achieve (better) 'understanding' on the part of the explainee. However, what it means to 'understand' is still not clearly defined, and the concept itself is rarely the subject of scientific investigation. This conceptual article aims to present a model of forms of understanding in the context of XAI and beyond. From an interdisciplinary perspective bringing together computer science, linguistics, sociology, and psychology, a definition of understanding and its forms, assessment, and dynamics during the process of giving everyday explanations are explored. Two types of understanding are considered as possible outcomes of explanations, namely enabledness, 'knowing how' to do or decide something, and comprehension, 'knowing that' -- both in different degrees (from shallow to deep). Explanations regularly start with shallow understanding in a specific domain and can lead to deep comprehension and enabledness of the explanandum, which we see as a prerequisite for human users to gain agency. In this process, the increase of comprehension and enabledness are highly interdependent. Against the background of this systematization, special challenges of understanding in XAI are discussed.翻译：Explainability 已成为计算机科学和人工智能领域的重要话题，导致了一个子领域 called Explainable Artificial Intelligence (XAI)。提供或请求解释的目标是为 explainee  achieve (更好的) '理解'。然而， '理解' 的含义仍然没有得到明确定义，这个概念本身rarely 是科学调查的 Object。本文旨在从多学科角度（计算机科学、语言学、社会学和心理学）出发，对 XAI 和其他领域中的理解形式、评估和过程 dynamics 进行系统化介绍。在给予日常解释的过程中，解释可以从特定领域的浅度理解开始，并可以导致解释和解释对象的深度理解和能力。在这个过程中，理解和能力的增长是高度相互dependent。在这种系统化的背景下，XAI 中特殊的理解挑战被讨论。
</details></li>
</ul>
<hr>
<h2 id="Cross-domain-feature-disentanglement-for-interpretable-modeling-of-tumor-microenvironment-impact-on-drug-response"><a href="#Cross-domain-feature-disentanglement-for-interpretable-modeling-of-tumor-microenvironment-impact-on-drug-response" class="headerlink" title="Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response"></a>Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09264">http://arxiv.org/abs/2311.09264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Zhai, Hui Liu</li>
<li>for: This paper aims to model the impact of the tumor microenvironment (TME) on clinical drug response.</li>
<li>methods: The authors propose a domain adaptation network for feature disentanglement, using two denoising autoencoders to extract features from cell lines (source domain) and tumors (target domain), and a graph attention network to learn the latent representation of drugs.</li>
<li>results: The model demonstrated superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文目标是模型肿瘤微环境（TME）对临床药物应用的影响。</li>
<li>methods: 作者提出了适应领域网络，通过两个减噪自编器提取Cell Line（源领域）和肿瘤（目标领域）中的特征，以及一个图注意网络学习药物的潜在表示。</li>
<li>results: 模型在临床药物应用预测和分解肿瘤微环境对药效的影响方面表现出色。<details>
<summary>Abstract</summary>
High-throughput screening technology has facilitated the generation of large-scale drug responses across hundreds of cancer cell lines. However, there exists significant discrepancy between in vitro cell lines and actual tumors in vivo in terms of their response to drug treatments, because of tumors comprise of complex cellular compositions and histopathology structure, known as tumor microenvironment (TME), which greatly influences the drug cytotoxicity against tumor cells. To date, no study has focused on modeling the impact of the TME on clinical drug response. This paper proposed a domain adaptation network for feature disentanglement to separate representations of cancer cells and TME of a tumor in patients. Two denoising autoencoders were separately used to extract features from cell lines (source domain) and tumors (target domain) for partial domain alignment and feature decoupling. The specific encoder was enforced to extract information only about TME. Moreover, to ensure generalizability to novel drugs, we applied a graph attention network to learn the latent representation of drugs, allowing us to linearly model the drug perturbation on cellular state in latent space. We calibrated our model on a benchmark dataset and demonstrated its superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy.
</details>
<details>
<summary>摘要</summary>
高通量屏测技术已经使得大规模的药物响应数据可以在数百个肿瘤细胞线上生成。然而，有很大的差异 между尸采肿瘤和实际在生体中的肿瘤响应药物治疗，因为肿瘤由复杂的细胞组分和组织结构组成，这些组成部分被称为肿瘤微环境（TME），它对肿瘤细胞的药物毒性产生很大的影响。到目前为止，没有任何研究探讨了在临床药物治疗中模拟TME的影响。这篇文章提出了一种领域适应网络，用于特征分离，以分离肿瘤细胞和TME的表达特征。两个降噪自适应器分别用于从肿瘤细胞线（源领域）和肿瘤（目标领域）提取特征，以实现部分领域对应和特征分离。具体来说，特定的降噪自适应器被要求提取TME中的信息。此外，为确保对新药的普适性，我们应用了图注意网络，以学习药物的潜在表达，以便在干扰空间中线性模型药物对细胞状态的影响。我们在一个标准数据集上调整了我们的模型，并证明了它在预测临床药物响应和分析肿瘤微环境对药物效果的性能是其他方法的超越。
</details></li>
</ul>
<hr>
<h2 id="Auto-ICL-In-Context-Learning-without-Human-Supervision"><a href="#Auto-ICL-In-Context-Learning-without-Human-Supervision" class="headerlink" title="Auto-ICL: In-Context Learning without Human Supervision"></a>Auto-ICL: In-Context Learning without Human Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09263">http://arxiv.org/abs/2311.09263</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecielyang/auto-icl">https://github.com/ecielyang/auto-icl</a></li>
<li>paper_authors: Jinghan Yang, Shuming Ma, Furu Wei</li>
<li>for: 本研究旨在提高人工智能模型在自然语言交互中的能力，使其能够更好地适应不同的任务和场景。</li>
<li>methods: 本研究提出了一种自动启发学习（Automatic In-Context Learning）框架，通过让模型自己生成示例、标签、指导路径等，以便在不同的任务和场景下进行学习和推理。</li>
<li>results: 研究表明，使用自动启发学习框架可以在多种任务上达到强健的性能，与传统的启发学习方法相比，具有更好的适应性和灵活性。<details>
<summary>Abstract</summary>
In the era of Large Language Models (LLMs), human-computer interaction has evolved towards natural language, offering unprecedented flexibility. Despite this, LLMs are heavily reliant on well-structured prompts to function efficiently within the realm of In-Context Learning. Vanilla In-Context Learning relies on human-provided contexts, such as labeled examples, explicit instructions, or other guiding mechanisms that shape the model's outputs. To address this challenge, our study presents a universal framework named Automatic In-Context Learning. Upon receiving a user's request, we ask the model to independently generate examples, including labels, instructions, or reasoning pathways. The model then leverages this self-produced context to tackle the given problem. Our approach is universally adaptable and can be implemented in any setting where vanilla In-Context Learning is applicable. We demonstrate that our method yields strong performance across a range of tasks, standing up well when compared to existing methods.
</details>
<details>
<summary>摘要</summary>
在大语言模型（LLM）时代，人机交互发展向自然语言，提供无 precedent的灵活性。然而，LLMs仍然依赖于良好结构的提示，以达到有效的内容学习。普通的内容学习依赖人类提供的 контекст，如标签示例、显式指令或其他引导机制，以导引模型的输出。为解决这个挑战，我们的研究提出了一种通用框架，名为自动内容学习。当用户提交请求时，我们会让模型独立生成示例，包括标签、指令或推理路径。模型然后可以利用这些自生成的 контекст，解决给定的问题。我们的方法是通用适用的，可以在任何可以使用普通内容学习的场景中实现。我们示出了我们的方法在多种任务上具有强表现，与现有方法相比，表现良好。
</details></li>
</ul>
<hr>
<h2 id="Disentangling-the-Potential-Impacts-of-Papers-into-Diffusion-Conformity-and-Contribution-Values"><a href="#Disentangling-the-Potential-Impacts-of-Papers-into-Diffusion-Conformity-and-Contribution-Values" class="headerlink" title="Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values"></a>Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09262">http://arxiv.org/abs/2311.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhikai Xue, Guoxiu He, Zhuoren Jiang, Yangyang Kang, Star Zhao, Wei Lu</li>
<li>for: 本研究的目的是提出一种新的图 нейрон网络，用于分解论文的潜在影响力（Diffusion, Conformity, and Contribution）。</li>
<li>methods: 该研究使用了一种名为DPPDCC的图神经网络，该网络利用动态多型图来编码时间和结构特征，并且使用比较和引用&#x2F;引用信息来捕捉知识的流动。</li>
<li>results: 实验结果表明，DPPDCC在三个不同的数据集上都有显著的超越性能，特别是对于在不同时间点发表的论文。此外，further analyses也证明了该模型的稳定性和可重复性。<details>
<summary>Abstract</summary>
The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent value of contribution. To evaluate models' generalization for papers published at various times, we reformulate the problem by partitioning data based on specific time points to mirror real-world conditions. Extensive experimental results on three datasets demonstrate that DPPDCC significantly outperforms baselines for previously, freshly, and immediately published papers. Further analyses confirm its robust capabilities. We will make our datasets and codes publicly available.
</details>
<details>
<summary>摘要</summary>
学术论文的潜在影响因素包括其受欢迎程度和贡献。现有模型通常根据静止图计算原始引用数，而忽略了不同视角的价值。在这项研究中，我们提出了一种新的图神经网络，可以分解论文的潜在影响为扩散、准确性和贡献三个值（DPPDCC）。对目标论文，DPPDCC 编码了时间和结构特征，并强调在构建动态多元图中的知识流动。特别是，通过比较和引用/引用信息的相互作用，我们捕捉了论文之间的知识传递。为了评估论文的受欢迎程度，我们将图像增强了，并使用累积预测法来模型准确性。此外，我们还应用了正交约束，以便分解每个视角的值，并保持论文的本身价值。为了评估模型在不同时间点上的普适性，我们将数据分区成不同的时间点，以逼真实的世界条件。我们的实验结果表明，DPPDCC 在三个 dataset 上显著超过基线。进一步的分析表明，它具有强大的可重复性。我们将我们的数据和代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="Emerging-Drug-Interaction-Prediction-Enabled-by-Flow-based-Graph-Neural-Network-with-Biomedical-Network"><a href="#Emerging-Drug-Interaction-Prediction-Enabled-by-Flow-based-Graph-Neural-Network-with-Biomedical-Network" class="headerlink" title="Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network"></a>Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09261">http://arxiv.org/abs/2311.09261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lars-research/emergnn">https://github.com/lars-research/emergnn</a></li>
<li>paper_authors: Yongqi Zhang, Quanming Yao, Ling Yue, Xian Wu, Ziheng Zhang, Zhenxi Lin, Yefeng Zheng</li>
<li>for: 预测新药与新药之间的药物相互作用（DDI），以提高病人护理和药物开发效率。</li>
<li>methods: 我们提出了一种基于图神经网络（GNN）的 EmerGNN，可以有效地预测新药DDI，通过利用生物医学网络中药物之间的路径信息。</li>
<li>results: EmerGNN比现有方法更高精度地预测新药DDI，并可以快速寻找最重要的生物医学网络信息。<details>
<summary>Abstract</summary>
Accurately predicting drug-drug interactions (DDI) for emerging drugs, which offer possibilities for treating and alleviating diseases, with computational methods can improve patient care and contribute to efficient drug development. However, many existing computational methods require large amounts of known DDI information, which is scarce for emerging drugs. In this paper, we propose EmerGNN, a graph neural network (GNN) that can effectively predict interactions for emerging drugs by leveraging the rich information in biomedical networks. EmerGNN learns pairwise representations of drugs by extracting the paths between drug pairs, propagating information from one drug to the other, and incorporating the relevant biomedical concepts on the paths. The different edges on the biomedical network are weighted to indicate the relevance for the target DDI prediction. Overall, EmerGNN has higher accuracy than existing approaches in predicting interactions for emerging drugs and can identify the most relevant information on the biomedical network.
</details>
<details>
<summary>摘要</summary>
accurately predicting drug-drug interactions (DDI) for emerging drugs, which offer possibilities for treating and alleviating diseases, with computational methods can improve patient care and contribute to efficient drug development. However, many existing computational methods require large amounts of known DDI information, which is scarce for emerging drugs. In this paper, we propose EmerGNN, a graph neural network (GNN) that can effectively predict interactions for emerging drugs by leveraging the rich information in biomedical networks. EmerGNN learns pairwise representations of drugs by extracting the paths between drug pairs, propagating information from one drug to the other, and incorporating the relevant biomedical concepts on the paths. The different edges on the biomedical network are weighted to indicate the relevance for the target DDI prediction. Overall, EmerGNN has higher accuracy than existing approaches in predicting interactions for emerging drugs and can identify the most relevant information on the biomedical network.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is in the "Simplified Chinese" format, which is the most commonly used format for written Chinese. The translation is accurate and faithful to the original text, but it may not be perfect in terms of nuance or idiomatic expressions.Here are some notes on the translation:* "Drug-drug interactions" (DDI) is translated as "药物交互作用" (yàowù jiāoxì zuòyòu) in Simplified Chinese.* "Emerging drugs" is translated as "新兴药物" (xīn xìng yàowù) in Simplified Chinese.* "Computational methods" is translated as "计算方法" (jìsuàn fāngfa) in Simplified Chinese.* "Biomedical networks" is translated as "生物医学网络" (shēngwù yīxué wǎngluò) in Simplified Chinese.* "Graph neural network" (GNN) is translated as "图解神经网络" (tújiě shénxiàng wǎngluò) in Simplified Chinese.* "Pairwise representations" is translated as "对应表示" (duìpǐng biǎozhì) in Simplified Chinese.* "Propagating information" is translated as "传递信息" (chuánxìn xìngxi) in Simplified Chinese.* "Relevant biomedical concepts" is translated as "相关生物医学概念" (xiāngguān shēngwù yīxué guīxiàn) in Simplified Chinese.* "Target DDI prediction" is translated as "目标DDI预测" (mùzhōu DDI yùdòu) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Joint-User-Pairing-and-Beamforming-Design-of-Multi-STAR-RISs-Aided-NOMA-in-the-Indoor-Environment-via-Multi-Agent-Reinforcement-Learning"><a href="#Joint-User-Pairing-and-Beamforming-Design-of-Multi-STAR-RISs-Aided-NOMA-in-the-Indoor-Environment-via-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning"></a>Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08708">http://arxiv.org/abs/2311.08708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Min Park, Yan Kyaw Tun, Choong Seon Hong</li>
<li>for: 提高6G&#x2F;B5G无线网络质量，挑战传统 terrestrial 基站的限制。</li>
<li>methods: NOMA 技术和 STAR-RISs 技术的结合，以提高 spectral efficiency 和支持更多用户。</li>
<li>results: 提出一种基于 Multi-STAR-RISs 的 joint user pairing for NOMA 和 beamforming 设计方法，可以最大化多个用户的 Throughput。<details>
<summary>Abstract</summary>
The development of 6G/B5G wireless networks, which have requirements that go beyond current 5G networks, is gaining interest from academic and industrial. However, to increase 6G/B5G network quality, conventional cellular networks that rely on terrestrial base stations are constrained geographically and economically. Meanwhile, NOMA allows multiple users to share the same resources, which improves the spectral efficiency of the system and has the advantage of supporting a larger number of users. Additionally, by intelligently manipulating the phase and amplitude of both the reflected and transmitted signals, STAR-RISs can achieve improved coverage, increased spectral efficiency, and enhanced communication reliability. However, STAR-RISs must simultaneously optimize the Amplitude and Phase-shift corresponding to reflection and transmission, which makes the existing terrestiral networks more complicated and is considered a major challenging issue. Motivated by the above, we study the joint user pairing for NOMA and beamforming design of Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization problem with the objective of maximizing the total throughput of MUs by jointly optimizing the decoding order, user pairing, active beamforming, and passive beamforming. However, the formulated problem is a MINLP. To tackle this challenge, we first introduce the decoding order for NOMA networks. Next, we decompose the original problem into two subproblems namely: 1) MU pairing and 2) Beamforming optimization under the optimal decoding order. For the first subproblem, we employ correlation-based K-means clustering to solve the user pairing problem. Then, to jointly deal with beamforming vector optimizations, we propose MAPPO, which can make quick decisions in the given environment owing to its low complexity.
</details>
<details>
<summary>摘要</summary>
开发6G/B5G无线网络，具有超越现有5G网络的需求，在学术和工业领域引起了关注。然而，为了提高6G/B5G网络质量，传统的地面基站Constraints geographically and economically.而NOMA技术可以让多个用户共享同一资源，提高系统spectral efficiency和支持更多的用户。此外，通过智能地调整反射和发射信号的相位和幅度，STAR-RISs可以实现改善的覆盖率、提高的spectral efficiency和提高的通信可靠性。然而，STAR-RISs同时需要优化反射和发射信号的相位和幅度，这使得现有的 terrestrial networks 更加复杂，被认为是主要的挑战。驱动了上述问题，我们研究了多STAR-RISs在室内环境中的联合用户对应和扫描优化设计。然后，我们将问题定义为最大化多个用户（MUs）的总吞吐量，通过联合调整用户对应、扫描顺序、活动扫描和空扫描来优化问题。然而，该问题是一个MINLP问题。为解决这个挑战，我们首先介绍NOMA网络中的排序问题。然后，我们将原始问题分解成两个子问题：1）用户对应问题和2）扫描优化问题。 для第一个子问题，我们采用相关性基于K-means clustering来解决用户对应问题。然后，为了共同处理扫描优化问题，我们提出MAPPO，它可以在给定环境中做出快速决策，凭借其低复杂度。
</details></li>
</ul>
<hr>
<h2 id="Aligned-A-Platform-based-Process-for-Alignment"><a href="#Aligned-A-Platform-based-Process-for-Alignment" class="headerlink" title="Aligned: A Platform-based Process for Alignment"></a>Aligned: A Platform-based Process for Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08706">http://arxiv.org/abs/2311.08706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klonnet23/helloy-word">https://github.com/klonnet23/helloy-word</a></li>
<li>paper_authors: Ethan Shaotran, Ido Pesok, Sam Jones, Emi Liu</li>
<li>for: 本研究旨在提供一个公信worthy、公共面向的安全性机制，以满足前ier模型的整合和超智能的需求。</li>
<li>methods: 本研究使用了一种 constitutional committee framework，通过收集680名参与者的输入，制定了30条指南，其中93%的总支持率。</li>
<li>results: 研究显示，该平台自然具有扩展性和可信worthiness，使得社区具有 confidence和享受性。<details>
<summary>Abstract</summary>
We are introducing Aligned, a platform for global governance and alignment of frontier models, and eventually superintelligence. While previous efforts at the major AI labs have attempted to gather inputs for alignment, these are often conducted behind closed doors. We aim to set the foundation for a more trustworthy, public-facing approach to safety: a constitutional committee framework. Initial tests with 680 participants result in a 30-guideline constitution with 93% overall support. We show the platform naturally scales, instilling confidence and enjoyment from the community. We invite other AI labs and teams to plug and play into the Aligned ecosystem.
</details>
<details>
<summary>摘要</summary>
我们正在引入协调平台，用于全球治理和前沿模型的协调，并最终实现超智能。过去，主要的 AI 室在Alignment的输入收集方面往往采取秘密的方式。我们想要建立一个更可靠、公共的安全方法：一个宪法委员会框架。我们的初步测试中，680名参与者共同制定了30个指南，得到93%的总支持。我们展示了平台自然升级，让社区感受到了信任和乐趣。我们邀请其他 AI 室和团队加入我们的Aligned生态系统。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Follow-Concept-Annotation-Guidelines-A-Case-Study-on-Scientific-and-Financial-Domains"><a href="#Can-Large-Language-Models-Follow-Concept-Annotation-Guidelines-A-Case-Study-on-Scientific-and-Financial-Domains" class="headerlink" title="Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains"></a>Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08704">http://arxiv.org/abs/2311.08704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcio Fonseca, Shay B. Cohen</li>
<li>for: 本研究的目的是探讨 instruction-tuned LLMs 是否可以从真实标签中学习新的概念或事实。</li>
<li>methods: 我们使用了不同类型的事实和反事实概念定义来作为题目标的Zero-shot sentence classification任务中的启发项。</li>
<li>results: 我们发现了大型模型（70B个参数或更多）只能在真实上下文中工作，而且仅有商业化模型（如GPT-3.5和GPT-4）能够识别非现实的指引。此外，我们发现了 Llama-2-70B-chat 通常能够超越 Falcon-180B-chat，这表明了精确的微调优化的重要性。总的来说，我们的简单评估方法显示了最具实力的开源语言模型和商业化 API 之间存在了很大的概念理解差距。<details>
<summary>Abstract</summary>
Although large language models (LLMs) exhibit remarkable capacity to leverage in-context demonstrations, it is still unclear to what extent they can learn new concepts or facts from ground-truth labels. To address this question, we examine the capacity of instruction-tuned LLMs to follow in-context concept guidelines for sentence labeling tasks. We design guidelines that present different types of factual and counterfactual concept definitions, which are used as prompts for zero-shot sentence classification tasks. Our results show that although concept definitions consistently help in task performance, only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts. Importantly, only proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is due to more sophisticated alignment methods. Finally, we find that Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which indicates that careful fine-tuning is more effective than increasing model scale. Altogether, our simple evaluation method reveals significant gaps in concept understanding between the most capable open-source language models and the leading proprietary APIs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Debate-Helps-Supervise-Unreliable-Experts"><a href="#Debate-Helps-Supervise-Unreliable-Experts" class="headerlink" title="Debate Helps Supervise Unreliable Experts"></a>Debate Helps Supervise Unreliable Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08702">http://arxiv.org/abs/2311.08702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/julianmichael/debate">https://github.com/julianmichael/debate</a></li>
<li>paper_authors: Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh Padmakumar, Samuel R. Bowman</li>
<li>for: 用于监督可能不可靠的专家，以确保他们的答案系统atically true而不仅仅看上去真实。</li>
<li>methods: 使用人类辩论来让非专家更可靠地评估真实性。</li>
<li>results: 比较辩论和咨询（一个专家提供单一答案）的性能，发现辩论性能更高， Judge accuracy 为 84%，而咨询的 Judge accuracy 为 74%。<details>
<summary>Abstract</summary>
As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important. How can we supervise unreliable experts, which have access to the truth but may not accurately report it, to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own? In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage. In our debates, one expert argues for the correct answer, and the other for an incorrect answer. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%. Debates are also more efficient, being 68% of the length of consultancies. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill). Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.
</details>
<details>
<summary>摘要</summary>
Traditional Chinese:随着AI系统用于回答更加困难的问题，评价其输出的真实性也变得更加困难和更加重要。如何监督不可靠的专家，他们有 accessed 到真理，但可能不会准确地报告它们？在这个工作中，我们显示了对两个不可靠的专家进行辩论可以帮助非专家评价者更加可靠地识别真实。我们收集了一个人类写的辩论集，其中一个专家认为正确的答案，另一个专家认为 incorrect 的答案。比较辩论和一个我们称之为咨询的基线，其中一个专家就一个答案，正确的一半时间。我们发现，辩论比咨询表现更好，有84%的评价率，而咨询仅有74%。辩论也更加高效，长度只有68%。通过比较人类和AI辩论者，我们发现，随着专家技能的提高，辩论的表现会提高，但咨询的表现会下降。我们的错误分析也支持这个趋势，有46%的错误在人类辩论中是由诚实的辩论者所做的错误（这些错误可以逐渐消失），而52%的错误在人类咨询中是由辩论者对裁判所隐瞒重要证据所做的错误（这些错误可以会随着技能提高而加剧）。总之，这些结果表明，辩论是一种可靠地监督 increasingly capable 但可能不可靠的 AI 系统的方法。
</details></li>
</ul>
<hr>
<h2 id="Artificial-General-Intelligence-Existential-Risk-and-Human-Risk-Perception"><a href="#Artificial-General-Intelligence-Existential-Risk-and-Human-Risk-Perception" class="headerlink" title="Artificial General Intelligence, Existential Risk, and Human Risk Perception"></a>Artificial General Intelligence, Existential Risk, and Human Risk Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08698">http://arxiv.org/abs/2311.08698</a></li>
<li>repo_url: None</li>
<li>paper_authors: David R. Mandel</li>
<li>for: 这篇论文主要针对的是人工智能（AGI）的发展和相关的风险。</li>
<li>methods: 该论文使用了公共可用的预测和意见数据，以探讨专家和非专家对AGI的风险评估。</li>
<li>results: 研究发现，对AGI的世界大悲害或毁灭风险的评估比其他existential risk（如核战或人类引起的气候变化）高，并且过去一年内这种风险的增加速度也比其他风险更大。<details>
<summary>Abstract</summary>
Artificial general intelligence (AGI) does not yet exist, but given the pace of technological development in artificial intelligence, it is projected to reach human-level intelligence within roughly the next two decades. After that, many experts expect it to far surpass human intelligence and to do so rapidly. The prospect of superintelligent AGI poses an existential risk to humans because there is no reliable method for ensuring that AGI goals stay aligned with human goals. Drawing on publicly available forecaster and opinion data, the author examines how experts and non-experts perceive risk from AGI. The findings indicate that the perceived risk of a world catastrophe or extinction from AGI is greater than for other existential risks. The increase in perceived risk over the last year is also steeper for AGI than for other existential threats (e.g., nuclear war or human-caused climate change). That AGI is a pressing existential risk is something on which experts and non-experts agree, but the basis for such agreement currently remains obscure.
</details>
<details>
<summary>摘要</summary>
人工通常智能（AGI）至今还没有存在，但根据人工智能技术的发展速度，预计在下一两个十年内，它将达到人类水平的智能。之后，许多专家预计AGI很快就会超过人类智能水平。这种超智AGI对人类存在极大的风险，因为没有可靠的方法可以保证AGI目标与人类目标相对适应。作者通过公共可用的预测和意见数据，探讨了专家和非专家对AGI风险的看法。结果显示，对AGI的世界大悲剧或毁灭风险的认知高于其他极大风险（如核战或人类引起的气候变化）。过去一年内，对AGI的风险增加速度也更高。尽管专家和非专家都同意AGI是一个急剧的存在风险，但目前这一点的基础还未明确。
</details></li>
</ul>
<hr>
<h2 id="An-Eye-on-Clinical-BERT-Investigating-Language-Model-Generalization-for-Diabetic-Eye-Disease-Phenotyping"><a href="#An-Eye-on-Clinical-BERT-Investigating-Language-Model-Generalization-for-Diabetic-Eye-Disease-Phenotyping" class="headerlink" title="An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping"></a>An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08687">http://arxiv.org/abs/2311.08687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kharrigian/ml4h-clinical-bert">https://github.com/kharrigian/ml4h-clinical-bert</a></li>
<li>paper_authors: Keith Harrigian, Tina Tang, Anthony Gonzales, Cindy X. Cai, Mark Dredze</li>
<li>for: 本研究旨在帮助监测和检测 диабетиче眼病的诊断和治疗过程中的重要信息，以避免盲目。</li>
<li>methods: 该研究使用了19种临床概念与 диабетиче眼病相关的证据抽取系统，以检测和识别病情的变化。</li>
<li>results: 研究发现，使用BERT语言模型预训练非临床数据对于本领域具有相同的效果，而不需要特定的临床数据预训练。这项研究质疑了 latest claims，即临床语言数据预训练语言模型可以帮助临床自然语言处理任务。<details>
<summary>Abstract</summary>
Diabetic eye disease is a major cause of blindness worldwide. The ability to monitor relevant clinical trajectories and detect lapses in care is critical to managing the disease and preventing blindness. Alas, much of the information necessary to support these goals is found only in the free text of the electronic medical record. To fill this information gap, we introduce a system for extracting evidence from clinical text of 19 clinical concepts related to diabetic eye disease and inferring relevant attributes for each. In developing this ophthalmology phenotyping system, we are also afforded a unique opportunity to evaluate the effectiveness of clinical language models at adapting to new clinical domains. Across multiple training paradigms, we find that BERT language models pretrained on out-of-distribution clinical data offer no significant improvement over BERT language models pretrained on non-clinical data for our domain. Our study tempers recent claims that language models pretrained on clinical data are necessary for clinical NLP tasks and highlights the importance of not treating clinical language data as a single homogeneous domain.
</details>
<details>
<summary>摘要</summary>
diabetic eye disease 是全球最主要的失明原因之一。监测相关的临床轨迹和检测治疗过程中的漏洞是管理疾病和避免失明的关键。可悲的是，许多有关这些目标的信息都可以在电子医疗记录中找到，但是它们却很难被找到。为了填补这个信息空白，我们介绍了一种从临床文本中提取19种临床概念和诊断相关的证据，并从中INFERRE 相关的特征。在开发这种眼科分型系统时，我们也得到了评估临床语言模型在新临床领域中是否适应的机会。我们在多个训练方案中发现，BERT语言模型预先训练在非临床数据上的PERFORMANCE 与预先训练在临床数据上的BERT语言模型相比，没有显著的提升。我们的研究证明了，在临床语言数据上预先训练语言模型并不是必要的，并且高举了不要将临床语言数据视为单一的同一个领域。
</details></li>
</ul>
<hr>
<h2 id="Safer-Instruct-Aligning-Language-Models-with-Automated-Preference-Data"><a href="#Safer-Instruct-Aligning-Language-Models-with-Automated-Preference-Data" class="headerlink" title="Safer-Instruct: Aligning Language Models with Automated Preference Data"></a>Safer-Instruct: Aligning Language Models with Automated Preference Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08685">http://arxiv.org/abs/2311.08685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uscnlp-lime/safer-instruct">https://github.com/uscnlp-lime/safer-instruct</a></li>
<li>paper_authors: Taiwei Shi, Kai Chen, Jieyu Zhao</li>
<li>for: 提高模型安全性，使用人类反馈学习（RLHF）是一种重要策略。但是，标注偏好数据需要巨大的人工资源和创造力，而自动生成方法受到数据多样性和质量限制。</li>
<li>methods: 我们提出了一种新的管道，即Safer-Instruct，用于半自动生成大规模偏好数据。我们的方法利用倒转指令调整、指令生成和专家模型评估，以高效生成高质量偏好数据而无需人类注释员。</li>
<li>results: 我们使用LLaMA进行指令生成和GPT-4作为专家模型，生成了约10K个偏好样本。通过对一个Alpaca模型进行训练，我们证明了Safer-Instruct可以提高模型的安全性，同时保持与对话和下游任务的竞争性能。Safer-Instruct解决了偏好数据获取的挑战，为更安全和负责任的AI系统的发展提供了新的想法。我们的代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/uscnlp-lime/safer-instruct%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/uscnlp-lime/safer-instruct上获取。</a><details>
<summary>Abstract</summary>
Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for enhancing model safety in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for semi-automatically constructing large-scale preference datasets. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. We evaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an expert model, generating approximately 10K preference samples. Finetuning an Alpaca model on this dataset demonstrates improved harmlessness while maintaining competitive performance on conversation and downstream tasks. Safer-Instruct addresses the challenges in preference data acquisition, advancing the development of safer and more responsible AI systems. Our code and data are available at https://github.com/uscnlp-lime/safer-instruct
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用人类反馈学习增强模型安全性（RLHF）是一种重要策略，但是标注偏好数据需要大量的人工资源和创造力。自动生成方法面临数据多样性和质量限制。为此，我们提出了 Safer-Instruct，一种新的数据采集管线。我们的方法利用倒转指令调整、指令生成和专家模型评估，以高效生成高质量偏好数据，无需人工标注员。我们使用 LLaMA 进行指令生成和 GPT-4 作为专家模型，生成了约 10K 的偏好样本。通过 fine-tuning 一个 Alpaca 模型，我们示出了提高无害性而无需削弱对话和下游任务的性能。Safer-Instruct 解决了偏好数据采集中的挑战，推动了更安全、负责任的 AI 系统的发展。我们的代码和数据可以在 https://github.com/uscnlp-lime/safer-instruct 上获取。
</details></li>
</ul>
<hr>
<h2 id="Multi-Set-Inoculation-Assessing-Model-Robustness-Across-Multiple-Challenge-Sets"><a href="#Multi-Set-Inoculation-Assessing-Model-Robustness-Across-Multiple-Challenge-Sets" class="headerlink" title="Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets"></a>Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08662">http://arxiv.org/abs/2311.08662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth</li>
<li>for: 这个论文的目的是为了研究语言模型对输入杂乱的敏感性，以便提高模型的可靠性和信任性。</li>
<li>methods: 这个论文使用了 fine-tuning 方法来训练语言模型对输入杂乱的敏感性，并研究了不同的输入杂乱对模型的影响。同时， authors 还提出了三种不同的训练策略来提高模型的多重杂乱 robustness。</li>
<li>results: 根据实验结果， authors 发现了一些训练策略可以帮助语言模型在不同的输入杂乱下保持高度的准确率，而不会受到输入杂乱的影响。同时， authors 还发现了一些情况下，模型的性能可以通过 exposure 来提高，但是也可以导致性能下降。<details>
<summary>Abstract</summary>
Language models, given their black-box nature, often exhibit sensitivity to input perturbations, leading to trust issues due to hallucinations. To bolster trust, it's essential to understand these models' failure modes and devise strategies to enhance their performance. In this study, we propose a framework to study the effect of input perturbations on language models of different scales, from pre-trained models to large language models (LLMs). We use fine-tuning to train a robust model to perturbations, and we investigate whether exposure to one perturbation improves or degrades the model's performance on other perturbations. To address multi-perturbation robustness, we suggest three distinct training strategies. We also extend the framework to LLMs via a chain of thought(COT) prompting with exemplars. We instantiate our framework for the Tabular-NLI task and show that the proposed strategies train the model robust to different perturbations without losing accuracy on a given dataset.
</details>
<details>
<summary>摘要</summary>
Language models 因为它们的黑盒特性，经常会受到输入扰动的影响，导致信任问题由于幻觉。为了增强信任，我们需要了解这些模型的失败模式，并开发策略来提高其性能。在这种研究中，我们提出了一个框架，用于研究不同规模的语言模型对输入扰动的影响。我们使用了练化来训练一个Robust模型，并investigate了对一种扰动是否改善或损害模型对其他扰动的性能。为了解决多个扰动的Robustness问题，我们提出了三种不同的训练策略。此外，我们还将框架扩展到大语言模型（LLMs）via一种链式思维（COT）提问方法。我们在Tabular-NLI任务上实现了我们的框架，并显示了提posed的策略可以训练模型对不同的扰动 robust，而无需失去对给定数据集的精度。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Large-Language-Model-Agents-Enabling-Intent-Driven-Mobile-GUI-Testing"><a href="#Autonomous-Large-Language-Model-Agents-Enabling-Intent-Driven-Mobile-GUI-Testing" class="headerlink" title="Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing"></a>Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08649">http://arxiv.org/abs/2311.08649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juyeon Yoon, Robert Feldt, Shin Yoo</li>
<li>for: 这篇论文旨在提供一种自动化图形用户界面测试方法，以便更好地检测软件系统是否按照预期的方式运行。</li>
<li>methods: 这篇论文提出了一种基于大语言模型和支持机制的自动化图形用户界面测试方法，称为DroidAgent。该方法可以自动设定和执行具有 semantic intent 的 GUI 测试任务。</li>
<li>results: 根据实验结果，DroidAgent 可以自动设定和执行真实的 GUI 测试任务，并且可以覆盖更高的活动覆盖率（61%），比现状态的方法更高。此外，人工分析表明，DroidAgent 自动创建的任务中有317个是真实有用的和相关于应用程序功能的。<details>
<summary>Abstract</summary>
GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.
</details>
<details>
<summary>摘要</summary>
GUI 测试检查软件系统在用户通过图形用户界面进行交互时是否按预期的行为，例如测试特定功能或验证相关用例enario。目前，决定要测试的高级水平任务是一个手动任务，因为自动化GUI测试工具通常针对较低级别的可用性指标，如结构代码覆盖率或活动覆盖率。我们提议了DroidAgent，一种基于大型自然语言模型和支持机制（如长期和短期记忆）的 Android 应用自动GUI测试代理。它可以进行意图驱动的自动化GUI测试。给定一个 Android 应用，DroidAgent 会设置相关的任务目标，然后通过与应用进行交互来实现这些目标。我们对 DroidAgent 使用 Themis benchmark 中的 15 个应用进行了实验性评估。结果显示，DroidAgent 可以自动设置和执行真实任务，并且在活动覆盖率方面表现更高。例如，在测试一款消息应用时，DroidAgent 创建了一个第二个帐户，并将第一个帐户添加为好友，测试了一个真实的用例，无需人工干预。在 average 的活动覆盖率方面，DroidAgent 达到了 61%，比现状态的 GUI 测试技术高出 51%。此外，manual 分析表明，DroidAgent 自动创建的任务中有 317 个实际 relevante 和相关于应用功能的任务，同时也发现 DroidAgent 深入交互with应用程序，覆盖了更多的功能。
</details></li>
</ul>
<hr>
<h2 id="Explore-Spurious-Correlations-at-the-Concept-Level-in-Language-Models-for-Text-Classification"><a href="#Explore-Spurious-Correlations-at-the-Concept-Level-in-Language-Models-for-Text-Classification" class="headerlink" title="Explore Spurious Correlations at the Concept Level in Language Models for Text Classification"></a>Explore Spurious Correlations at the Concept Level in Language Models for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08648">http://arxiv.org/abs/2311.08648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, Furong Huang</li>
<li>for: This paper aims to address the issue of spurious correlations in language models (LMs) caused by imbalanced label distributions in training data, which can lead to robustness issues.</li>
<li>methods: The authors use the LLM to label the concepts in text and measure the concept bias of models for fine-tuning or in-context learning (ICL) on test data. They also propose a data rebalancing method to mitigate the spurious correlations by adding counterfactual data generated by the LLM.</li>
<li>results: The authors show that there exist label distribution biases in concepts across multiple text classification datasets, and LMs will utilize these shortcuts to make predictions in both fine-tuning and ICL methods. They also demonstrate the effectiveness of their mitigation method and its superiority over the token removal method.<details>
<summary>Abstract</summary>
Language models (LMs) have gained great achievement in various NLP tasks for both fine-tuning and in-context learning (ICL) methods. Despite its outstanding performance, evidence shows that spurious correlations caused by imbalanced label distributions in training data (or exemplars in ICL) lead to robustness issues. However, previous studies mostly focus on word- and phrase-level features and fail to tackle it from the concept level, partly due to the lack of concept labels and subtle and diverse expressions of concepts in text. In this paper, we first use the LLM to label the concept for each text and then measure the concept bias of models for fine-tuning or ICL on the test data. Second, we propose a data rebalancing method to mitigate the spurious correlations by adding the LLM-generated counterfactual data to make a balanced label distribution for each concept. We verify the effectiveness of our mitigation method and show its superiority over the token removal method. Overall, our results show that there exist label distribution biases in concepts across multiple text classification datasets, and LMs will utilize these shortcuts to make predictions in both fine-tuning and ICL methods.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）在各种自然语言处理任务中取得了很大的成就，包括精度训练和上下文学习（ICL）方法。尽管它们的表现非常出色，但是证据显示，由于训练数据中标签分布的偏度而导致的假 correlations 会导致模型的Robustness问题。然而，先前的研究主要关注单词和短语水平的特征，而忽略了概念水平的问题，其中一部分是因为缺乏概念标签，另一部分是因为文本中概念的细微和多样化表达。在这篇论文中，我们首先使用LM来标注每个文本中的概念，然后测量模型在测试数据上的概念偏见。第二，我们提出了一种数据重新平衡方法，通过添加LM生成的对称数据来使每个概念的标签分布平衡。我们证明了我们的mitigation方法的有效性，并与Token removal方法进行比较，并显示了我们的方法的优越性。总之，我们的结果表明，在多个文本分类数据集中，存在概念水平的标签分布偏见，LM会在精度训练和ICL方法中利用这些短cuts来进行预测。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-by-Design-Wrapper-Boxes-Combine-Neural-Performance-with-Faithful-Explanations"><a href="#Interpretable-by-Design-Wrapper-Boxes-Combine-Neural-Performance-with-Faithful-Explanations" class="headerlink" title="Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations"></a>Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08644">http://arxiv.org/abs/2311.08644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiheng Su, Juni Jessy Li, Matthew Lease</li>
<li>for: 可以保持神经网络模型的准确性，同时提供 faithful的解释？我们提出了包装盒，一种通用的方法来生成 faithful、示例基于的解释，以保持神经网络模型的预测性能。</li>
<li>methods: 使用一个经典、可解释的模型来实际进行预测，并将神经网络模型的学习feature表示输入到这个经典模型中。</li>
<li>results: 这种简单的策略 surprisingly 有效，Results 与原神经网络模型的预测结果几乎相同，并在三个大预训言模型、两个dataset、四个经典模型和四个评价指标上进行了证明。此外，因为这些经典模型是可解释的设计，可以直接向用户显示用于经典模型预测的 subset 的训练示例。<details>
<summary>Abstract</summary>
Can we preserve the accuracy of neural models while also providing faithful explanations? We present wrapper boxes, a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. Moreover, because these classic models are interpretable by design, the subset of training examples that determine classic model predictions can be shown directly to users.
</details>
<details>
<summary>摘要</summary>
可以保持神经网络模型的准确性while也提供忠诚的解释吗？我们提出了“包裹框”，一种通用的方法来生成忠诚的示例基于解释，保持神经网络模型的预测性能。在训练神经网络模型后，其学习的特征表示被输入到可解释的模型进行实际预测。这种简单的策略具有意外的效果，在三个大预训言语模型、两个不同规模的数据集、四种经典模型和四种评价指标上都达到了相似的结果。此外，由于这些经典模型是可解释的设计，可以直接向用户显示训练示例的子集。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Graph-Neural-Point-Process-for-Traffic-Congestion-Event-Prediction"><a href="#Spatio-Temporal-Graph-Neural-Point-Process-for-Traffic-Congestion-Event-Prediction" class="headerlink" title="Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction"></a>Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08635">http://arxiv.org/abs/2311.08635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyin Jin, Lingbo Liu, Fuxian Li, Jincai Huang</li>
<li>for: 预测交通堵塞事件是智能交通系统中一项重要 yet 挑战性的任务。许多现有的交通预测方法 integrate 多种时间编码器和图 convolutional neural networks (GCNs), called spatio-temporal graph-based neural networks, which focus on predicting dense variables such as flow, speed, and demand in time snapshots, but they can hardly forecast the traffic congestion events that are sparsely distributed on the continuous time axis.</li>
<li>methods: 我们提出了一种 spatio-temporal graph neural point process 框架， named STGNPP, to address these limitations. Specifically, we first design the spatio-temporal graph learning module to fully capture the long-range spatio-temporal dependencies from the historical traffic state data along with the road network. The extracted spatio-temporal hidden representation and congestion event information are then fed into a continuous gated recurrent unit to model the congestion evolution patterns. In particular, to fully exploit the periodic information, we also improve the intensity function calculation of the point process with a periodic gated mechanism.</li>
<li>results: 我们的模型可以同时预测下一个堵塞事件的发生时间和持续时间。对两个实际数据集进行了广泛的实验，并证明了我们的方法在比较现有状态艺术方法的情况下显著提高了性能。<details>
<summary>Abstract</summary>
Traffic congestion event prediction is an important yet challenging task in intelligent transportation systems. Many existing works about traffic prediction integrate various temporal encoders and graph convolution networks (GCNs), called spatio-temporal graph-based neural networks, which focus on predicting dense variables such as flow, speed and demand in time snapshots, but they can hardly forecast the traffic congestion events that are sparsely distributed on the continuous time axis. In recent years, neural point process (NPP) has emerged as an appropriate framework for event prediction in continuous time scenarios. However, most conventional works about NPP cannot model the complex spatio-temporal dependencies and congestion evolution patterns. To address these limitations, we propose a spatio-temporal graph neural point process framework, named STGNPP for traffic congestion event prediction. Specifically, we first design the spatio-temporal graph learning module to fully capture the long-range spatio-temporal dependencies from the historical traffic state data along with the road network. The extracted spatio-temporal hidden representation and congestion event information are then fed into a continuous gated recurrent unit to model the congestion evolution patterns. In particular, to fully exploit the periodic information, we also improve the intensity function calculation of the point process with a periodic gated mechanism. Finally, our model simultaneously predicts the occurrence time and duration of the next congestion. Extensive experiments on two real-world datasets demonstrate that our method achieves superior performance in comparison to existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
减少交通拥堵事件预测是智能交通系统中非常重要 yet 挑战性的任务。许多现有的交通预测方法将多种时间编码器和图像树卷积网络（GCN）集成，称为空间-时间图像基于神经网络，它们主要预测时刻点的稠密变量，如流速和需求，但它们很难预测在连续时间轴上稀疏分布的交通拥堵事件。在过去几年，神经点过程（NPP）在连续时间场景中的预测 Task 变得越来越受欢迎。然而，大多数传统的 NPP 方法无法模型交通拥堵事件的复杂空间-时间依赖关系和拥堵演化趋势。为了解决这些限制，我们提议一种空间-时间图像神经点过程框架，名为 STGNPP，用于交通拥堵事件预测。具体来说，我们首先设计了空间-时间图像学习模块，以全面捕捉历史交通状态数据中的长距离空间-时间依赖关系，同时与道路网络结合。提取的空间-时间隐藏表示和拥堵事件信息然后被传递给连续闭包律环路模型，以模型拥堵演化趋势。具体来说，为了完全利用周期信息，我们还改进了点过程的激发函数计算方法，使用周期闭包机制。最后，我们的模型同时预测下一个拥堵事件的发生时间和持续时间。我们在两个实际数据集上进行了广泛的实验，结果显示，我们的方法在与现有状态艺术方法比较时表现出优于其他方法。
</details></li>
</ul>
<hr>
<h2 id="XplainLLM-A-QA-Explanation-Dataset-for-Understanding-LLM-Decision-Making"><a href="#XplainLLM-A-QA-Explanation-Dataset-for-Understanding-LLM-Decision-Making" class="headerlink" title="XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making"></a>XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08614">http://arxiv.org/abs/2311.08614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichen Chen, Jianda Chen, Mitali Gaidhani, Ambuj Singh, Misha Sra</li>
<li>for: 这 paper 的目的是提高大型自然语言处理模型（LLM）的透明性和可解释性，使其更加可靠和可信worth。</li>
<li>methods: 这 paper 使用了知识 graphs（KGs）和图注意力网络（GAT）来构建一个新的解释数据集，以帮助理解 LLM 的决策过程。</li>
<li>results: 这 paper 通过量化和质量评估表明，该数据集可以提高 LLM 在问答任务上的在Context learning，提高其解释性和可解释性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently made impressive strides in natural language understanding tasks. Despite their remarkable performance, understanding their decision-making process remains a big challenge. In this paper, we look into bringing some transparency to this process by introducing a new explanation dataset for question answering (QA) tasks that integrates knowledge graphs (KGs) in a novel way. Our dataset includes 12,102 question-answer-explanation (QAE) triples. Each explanation in the dataset links the LLM's reasoning to entities and relations in the KGs. The explanation component includes a why-choose explanation, a why-not-choose explanation, and a set of reason-elements that underlie the LLM's decision. We leverage KGs and graph attention networks (GAT) to find the reason-elements and transform them into why-choose and why-not-choose explanations that are comprehensible to humans. Through quantitative and qualitative evaluations, we demonstrate the potential of our dataset to improve the in-context learning of LLMs, and enhance their interpretability and explainability. Our work contributes to the field of explainable AI by enabling a deeper understanding of the LLMs decision-making process to make them more transparent and thereby, potentially more reliable, to researchers and practitioners alike. Our dataset is available at: https://github.com/chen-zichen/XplainLLM_dataset.git
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Navigating-the-Ocean-of-Biases-Political-Bias-Attribution-in-Language-Models-via-Causal-Structures"><a href="#Navigating-the-Ocean-of-Biases-Political-Bias-Attribution-in-Language-Models-via-Causal-Structures" class="headerlink" title="Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures"></a>Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08605">http://arxiv.org/abs/2311.08605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-jenny/llm-political-study">https://github.com/david-jenny/llm-political-study</a></li>
<li>paper_authors: David F. Jenny, Yann Billeter, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）在政治辩论中的决策过程和内在偏见。</li>
<li>methods: 本研究使用活动依赖网络（ADNs）抽象出LLMs的隐式评价标准，并研究如何normative价值影响这些评价。</li>
<li>results: 研究发现LLMs在评价“好 argue”时存在偏见，这些偏见源于模型内部的normative价值。这些结果有关于人工智能和偏见减少的影响。<details>
<summary>Abstract</summary>
The rapid advancement of Large Language Models (LLMs) has sparked intense debate regarding their ability to perceive and interpret complex socio-political landscapes. In this study, we undertake an exploration of decision-making processes and inherent biases within LLMs, exemplified by ChatGPT, specifically contextualizing our analysis within political debates. We aim not to critique or validate LLMs' values, but rather to discern how they interpret and adjudicate "good arguments." By applying Activity Dependency Networks (ADNs), we extract the LLMs' implicit criteria for such assessments and illustrate how normative values influence these perceptions. We discuss the consequences of our findings for human-AI alignment and bias mitigation. Our code and data at https://github.com/david-jenny/LLM-Political-Study.
</details>
<details>
<summary>摘要</summary>
快速发展的大语言模型（LLM）已引发了严峻的辩论，涉及到它们是否能够理解和解释复杂的社会政治景观。在这项研究中，我们进行了决策过程和内置偏见在LLM中的探索，以ChatGPT作为例子，并将分析围绕政治辩论进行。我们不是要评价或验证LLM的价值观，而是想要了解它们如何处理和评价“好的Arguments”。通过应用Activity Dependency Networks（ADN），我们提取了LLM中的隐式评价标准，并示出了normative价值如何影响这些评价。我们讨论了我们的发现对人机同步和偏见缓减的后果。codes和数据可以在https://github.com/david-jenny/LLM-Political-Study找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.AI_2023_11_15/" data-id="clp53jwm2007iyp88aqcqb2tg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/15/cs.CV_2023_11_15/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-11-15
        
      </div>
    </a>
  
  
    <a href="/2023/11/15/cs.CL_2023_11_15/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-11-15</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">125</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">62</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">76</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
