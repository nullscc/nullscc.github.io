
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-15 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Predicting Spine Geometry and Scoliosis from DXA Scans paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09424 repo_url: None paper_authors: Amir Jamaludin, Timor Kadir, Emma Clark, Andrew Zisserman for: 估算 DXA 扫描">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-15">
<meta property="og:url" content="https://nullscc.github.io/2023/11/15/cs.CV_2023_11_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Predicting Spine Geometry and Scoliosis from DXA Scans paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.09424 repo_url: None paper_authors: Amir Jamaludin, Timor Kadir, Emma Clark, Andrew Zisserman for: 估算 DXA 扫描">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-15T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-19T06:27:34.961Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.CV_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T13:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-15
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Predicting-Spine-Geometry-and-Scoliosis-from-DXA-Scans"><a href="#Predicting-Spine-Geometry-and-Scoliosis-from-DXA-Scans" class="headerlink" title="Predicting Spine Geometry and Scoliosis from DXA Scans"></a>Predicting Spine Geometry and Scoliosis from DXA Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09424">http://arxiv.org/abs/2311.09424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Jamaludin, Timor Kadir, Emma Clark, Andrew Zisserman</li>
<li>for: 估算 DXA 扫描中脊梁 curvature</li>
<li>methods: 使用神经网络预测中脊梁曲线，然后使用积分方法确定脊梁曲线上的曲率</li>
<li>results: 比往期工作（Jamaludin et al. 2018）有更好的性能，并可以用最大曲率作为评价脊梁弯曲的分数函数<details>
<summary>Abstract</summary>
Our objective in this paper is to estimate spine curvature in DXA scans. To this end we first train a neural network to predict the middle spine curve in the scan, and then use an integral-based method to determine the curvature along the spine curve. We use the curvature to compare to the standard angle scoliosis measure obtained using the DXA Scoliosis Method (DSM). The performance improves over the prior work of Jamaludin et al. 2018. We show that the maximum curvature can be used as a scoring function for ordering the severity of spinal deformation.
</details>
<details>
<summary>摘要</summary>
我们的目标在这篇论文中是估计DXA扫描中的脊梁弯曲度。为达到这个目标，我们首先使用神经网络预测扫描中的中脊梁弯曲度，然后使用积分方法确定脊梁弯曲度的各个点的弯曲度。我们使用弯曲度与DSM法（DXA扫描风扭度测量方法）所获取的标准风扭度比较。我们显示，使用最大弯曲度作为评分函数可以对脊梁弯曲度的严重程度进行排序。Here's the translation of the text into Traditional Chinese:我们的目标在这篇论文中是估计DXA扫描中的脊梁弯曲度。为达到这个目标，我们首先使用神经网络预测扫描中的中脊梁弯曲度，然后使用积分方法确定脊梁弯曲度的各个点的弯曲度。我们使用弯曲度与DSM法（DXA扫描风扭度测量方法）所获取的标准风扭度比较。我们显示，使用最大弯曲度作为评分函数可以对脊梁弯曲度的严重程度进行排序。
</details></li>
</ul>
<hr>
<h2 id="Synthetically-Enhanced-Unveiling-Synthetic-Data’s-Potential-in-Medical-Imaging-Research"><a href="#Synthetically-Enhanced-Unveiling-Synthetic-Data’s-Potential-in-Medical-Imaging-Research" class="headerlink" title="Synthetically Enhanced: Unveiling Synthetic Data’s Potential in Medical Imaging Research"></a>Synthetically Enhanced: Unveiling Synthetic Data’s Potential in Medical Imaging Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09402">http://arxiv.org/abs/2311.09402</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bardiakh/syntheticallyenhanced">https://github.com/bardiakh/syntheticallyenhanced</a></li>
<li>paper_authors: Bardia Khosravi, Frank Li, Theo Dapamede, Pouria Rouzrokh, Cooper U. Gamble, Hari M. Trivedi, Cody C. Wyles, Andrew B. Sellergren, Saptarshi Purkayastha, Bradley J. Erickson, Judy W. Gichoya</li>
<li>for: 这个研究旨在测试深度学习（DL）分类器在胸部X射像（CXR）分析中的性能，以及使用扩散模型生成的合成数据是否能够提高模型的准确性。</li>
<li>methods: 我们使用了三个数据集：CheXpert、MIMIC-CXR和Emory Chest X-ray，并训练了条件扩散数据模型（DDPMs）来生成合成前面影像。我们确保了合成图像对应原始数据中的人口和疾病特征。</li>
<li>results: 我们发现，将合成数据融入到真实数据中可以提高模型的准确性，特别是检测较少见的疾病。此外，使用仅合成数据进行训练的模型也可以将模型的性能提高到与使用真实数据进行训练的模型相当的水平。这表示合成数据可能可以补偿实际数据的短缺，对于训练具有优秀性的DL模型而言。但是，优秀的实际数据仍然保持优势。<details>
<summary>Abstract</summary>
Chest X-rays (CXR) are the most common medical imaging study and are used to diagnose multiple medical conditions. This study examines the impact of synthetic data supplementation, using diffusion models, on the performance of deep learning (DL) classifiers for CXR analysis. We employed three datasets: CheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising diffusion probabilistic models (DDPMs) to generate synthetic frontal radiographs. Our approach ensured that synthetic images mirrored the demographic and pathological traits of the original data. Evaluating the classifiers' performance on internal and external datasets revealed that synthetic data supplementation enhances model accuracy, particularly in detecting less prevalent pathologies. Furthermore, models trained on synthetic data alone approached the performance of those trained on real data. This suggests that synthetic data can potentially compensate for real data shortages in training robust DL models. However, despite promising outcomes, the superiority of real data persists.
</details>
<details>
<summary>摘要</summary>
颈部X-射影（CXR）是医学影像研究最常用的方法，用于诊断多种医学疾病。本研究检查使用扩散模型生成的合成数据对深度学习（DL）分类器的表现的影响。我们使用了三个数据集：CheXpert、MIMIC-CXR和Emory颈部X-射影，使用条件扩散概率模型（DDPM）来生成合成前视图预制图像。我们的方法确保了合成图像反映了原始数据中的人口和疾病特征。我们对内部和外部数据集进行评估，发现使用合成数据支持DL模型的准确率提高，特别是检测较少发生的疾病。此外，使用合成数据alone训练的模型在实际数据上达到了与实际数据训练的模型相同的性能。这表明合成数据可能可以补做实际数据短缺，培养Robust DL模型。然而，尽管结果很有前途，实际数据仍然占据优势。
</details></li>
</ul>
<hr>
<h2 id="MoCo-Transfer-Investigating-out-of-distribution-contrastive-learning-for-limited-data-domains"><a href="#MoCo-Transfer-Investigating-out-of-distribution-contrastive-learning-for-limited-data-domains" class="headerlink" title="MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains"></a>MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09401">http://arxiv.org/abs/2311.09401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwen Chen, Helen Zhou, Zachary C. Lipton</li>
<li>for: 本研究旨在探讨通过在不同领域的数据上进行自动增强的抽象表示学习，以提高医疗影像数据中的模型开发。</li>
<li>methods: 本研究使用了自适应对比表示学习（MoCo）的准则预训练，并对不同领域的数据进行跨领域传输学习。</li>
<li>results: 研究发现，在有限量的标注和无标注数据下，将自适应对比表示学习的模型训练在更大的对比领域上可以达到与在预训练领域内进行预训练相似或更好的性能，并且将预训练模型传输到相关领域可以提高性能。<details>
<summary>Abstract</summary>
Medical imaging data is often siloed within hospitals, limiting the amount of data available for specialized model development. With limited in-domain data, one might hope to leverage larger datasets from related domains. In this paper, we analyze the benefit of transferring self-supervised contrastive representations from moment contrast (MoCo) pretraining on out-of-distribution data to settings with limited data. We consider two X-ray datasets which image different parts of the body, and compare transferring from each other to transferring from ImageNet. We find that depending on quantity of labeled and unlabeled data, contrastive pretraining on larger out-of-distribution datasets can perform nearly as well or better than MoCo pretraining in-domain, and pretraining on related domains leads to higher performance than if one were to use the ImageNet pretrained weights. Finally, we provide a preliminary way of quantifying similarity between datasets.
</details>
<details>
<summary>摘要</summary>
医疗成像数据经常受到医院内部的限制，因此有限的数据对特殊模型的开发具有有利的作用。在这篇论文中，我们分析了将自动掌握异构表示（MoCo）预训练的异构数据传递到有限数据的设置中的好处。我们考虑了两个X射线数据集，它们分别捕捉不同部位的影像，并比较了将每个数据集中的图像传递给另一个数据集，以及使用ImageNet预训练的模型。我们发现，根据数据集的量和无 labels数据，在大型异构数据集上进行自动掌握异构表示预训练可以与在预训练中的MoCo预训练具有相似或更高的性能，而将预训练模型在相关的领域中进行预训练可以高于使用ImageNet预训练的模型。最后，我们提供了一种初步的方法来衡量数据集之间的相似性。
</details></li>
</ul>
<hr>
<h2 id="RENI-A-Rotation-Equivariant-Scale-Invariant-Natural-Illumination-Prior"><a href="#RENI-A-Rotation-Equivariant-Scale-Invariant-Natural-Illumination-Prior" class="headerlink" title="RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior"></a>RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09361">http://arxiv.org/abs/2311.09361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadgardner/ns_reni">https://github.com/jadgardner/ns_reni</a></li>
<li>paper_authors: James A. D. Gardner, Bernhard Egger, William A. P. Smith</li>
<li>for: 这个论文的目的是提出一种基于神经网络的自然照明模型，以解决 inverse rendering 问题。</li>
<li>methods: 这个论文使用了conditional neural field representation和 transformer decoder，并使用了 Vector Neurons 技术来实现 equivariance 性。</li>
<li>results: 论文通过使用一个 curated  dataset of 1.6K HDR 环境图像，证明了其模型的可靠性和精度，并完成了 inverse rendering 任务和环境图像完成任务。<details>
<summary>Abstract</summary>
Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. This results in limitations for the inverse setting in terms of the expressivity of the illumination conditions, especially when taking specular reflections into account. We propose a conditional neural field representation based on a variational auto-decoder and a transformer decoder. We extend Vector Neurons to build equivariance directly into our architecture, and leveraging insights from depth estimation through a scale-invariant loss function, we enable the accurate representation of High Dynamic Range (HDR) images. The result is a compact, rotation-equivariant HDR neural illumination model capable of capturing complex, high-frequency features in natural environment maps. Training our model on a curated dataset of 1.6K HDR environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations. We share our PyTorch implementation, dataset and trained models at https://github.com/JADGardner/ns_reni
</details>
<details>
<summary>摘要</summary>
“倒排问题是一个不充分定义的问题。先前的工作强调了物体或场景形状或外观的确定，以解决这个问题。在这个工作中，我们则专注在自然照明的确定上。现有的方法通常使用圆柱对称照明或其他通用表示，并仅对参数进行简单的确定。这会导致倒排设定中的表现照明条件的限制，特别是当考虑到镜面反射时。我们提议一个基于征化自适应器和对映器构成的对映领域表示。我们将vector neuron扩展到建立对称性直接到我们的架构中，并参考depth estimation的尺度不敏感损失函数，以获得高动态范围（HDR）图像的精确表示。我们将这个模型训练在1.6K HDR环境地图中，并与传统表示进行比较，证明其适用于倒排 зада项和环境地图完成 task。我们在github上分享我们的PyTorch实现、数据集和训练模型，请参考https://github.com/JADGardner/ns_reni。”
</details></li>
</ul>
<hr>
<h2 id="Nothing-Stands-Still-A-Spatiotemporal-Benchmark-on-3D-Point-Cloud-Registration-Under-Large-Geometric-and-Temporal-Change"><a href="#Nothing-Stands-Still-A-Spatiotemporal-Benchmark-on-3D-Point-Cloud-Registration-Under-Large-Geometric-and-Temporal-Change" class="headerlink" title="Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change"></a>Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09346">http://arxiv.org/abs/2311.09346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Sun, Yan Hao, Shengyu Huang, Silvio Savarese, Konrad Schindler, Marc Pollefeys, Iro Armeni</li>
<li>for: 这个论文旨在探讨如何在建筑环境中处理大规模的空间和时间变化。</li>
<li>methods: 该论文使用了多个方法来处理3D点云数据，包括标准的对应 registration 和多对多 registration。</li>
<li>results: 研究发现现有的方法无法处理大规模的空间和时间变化，新的方法是需要特地设计来处理这类变化。<details>
<summary>Abstract</summary>
Building 3D geometric maps of man-made spaces is a well-established and active field that is fundamental to computer vision and robotics. However, considering the evolving nature of built environments, it is essential to question the capabilities of current mapping efforts in handling temporal changes. In addition, spatiotemporal mapping holds significant potential for achieving sustainability and circularity goals. Existing mapping approaches focus on small changes, such as object relocation or self-driving car operation; in all cases where the main structure of the scene remains fixed. Consequently, these approaches fail to address more radical changes in the structure of the built environment, such as geometry and topology. To this end, we introduce the Nothing Stands Still (NSS) benchmark, which focuses on the spatiotemporal registration of 3D scenes undergoing large spatial and temporal change, ultimately creating one coherent spatiotemporal map. Specifically, the benchmark involves registering two or more partial 3D point clouds (fragments) from the same scene but captured from different spatiotemporal views. In addition to the standard pairwise registration, we assess the multi-way registration of multiple fragments that belong to any temporal stage. As part of NSS, we introduce a dataset of 3D point clouds recurrently captured in large-scale building indoor environments that are under construction or renovation. The NSS benchmark presents three scenarios of increasing difficulty, to quantify the generalization ability of point cloud registration methods over space (within one building and across buildings) and time. We conduct extensive evaluations of state-of-the-art methods on NSS. The results demonstrate the necessity for novel methods specifically designed to handle large spatiotemporal changes. The homepage of our benchmark is at http://nothing-stands-still.com.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:建立3D geometric图的人工空间是一个已有的和活跃的领域，对计算机视觉和机器人学是基础。然而，考虑到建筑环境的发展性，现有的地图努力的能力是否能够处理时间变化？此外，空间时间地图具有可持续性和循环性目标的潜在价值。现有的地图方法主要关注小型变化，如物体重新布置或自动驾驶车辆操作。这些方法无法处理更大的空间时间变化，如建筑结构的改变。为此，我们介绍了Nothing Stands Still（NSS）标准，该标准关注3D场景在不同的空间时间视图中进行大规模的空间时间 региSTRONGistration，最终创建一个一致的空间时间地图。特别是，NSS标准包括将多个不同空间时间视图中的3D点云（碎片）进行对比注册，以及评估多个时间阶段中的多个碎片之间的多方注册。作为NSS的一部分，我们提供了大规模建筑内部环境中逐渐更新的3D点云集。NSS标准提供三个不同难度的enario，用于评估点云注册方法的通用能力，包括空间内部（在一个建筑物内）和空间间（ между多个建筑物）的泛化能力，以及时间方向的泛化能力。我们对现有方法进行了广泛的评估，结果表明，现有的方法无法处理大规模的空间时间变化。NSS标准的主页是http://nothing-stands-still.com。
</details></li>
</ul>
<hr>
<h2 id="Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion"><a href="#Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion" class="headerlink" title="Single-Image 3D Human Digitization with Shape-Guided Diffusion"></a>Single-Image 3D Human Digitization with Shape-Guided Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09221">http://arxiv.org/abs/2311.09221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, Jia-Bin Huang</li>
<li>for: 这篇论文的目的是生成一个人体的全 körper 360度视图，从单个输入图像中生成高分辨率、具有一致性的图像。</li>
<li>methods: 该论文使用了高能范围的2D扩散模型作为人体图像的外观先验，通过填充缺失区域来实现多视图图像的生成，并通过反推 Rendering 将多视图图像合并成一个完全纹理化的高分辨率3D模型。</li>
<li>results: 该论文的实验结果表明，其方法可以超越先前的方法，从单个图像中生成高质量、具有一致性的360度人体图像，并且可以处理复杂的服装和皮肤 texture。<details>
<summary>Abstract</summary>
We present an approach to generate a 360-degree view of a person with a consistent, high-resolution appearance from a single input image. NeRF and its variants typically require videos or images from different viewpoints. Most existing approaches taking monocular input either rely on ground-truth 3D scans for supervision or lack 3D consistency. While recent 3D generative models show promise of 3D consistent human digitization, these approaches do not generalize well to diverse clothing appearances, and the results lack photorealism. Unlike existing work, we utilize high-capacity 2D diffusion models pretrained for general image synthesis tasks as an appearance prior of clothed humans. To achieve better 3D consistency while retaining the input identity, we progressively synthesize multiple views of the human in the input image by inpainting missing regions with shape-guided diffusion conditioned on silhouette and surface normal. We then fuse these synthesized multi-view images via inverse rendering to obtain a fully textured high-resolution 3D mesh of the given person. Experiments show that our approach outperforms prior methods and achieves photorealistic 360-degree synthesis of a wide range of clothed humans with complex textures from a single image.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，可以从单个输入图像中生成一个360度的人体视图，具有一致的高分辨率外观。NeRF和其变种通常需要不同视角的视频或图像。现有的方法，只有使用准确的3D扫描数据进行超vision。而最近的3D生成模型显示了人体数字化的3D一致性，但这些方法不能泛化到多样化的服装外观，并且结果缺乏真实感。与现有的工作不同，我们利用高容量2D扩散模型，已经预训练用于通用图像生成任务，作为衣服人体的外观先验。为了提高3D一致性，同时保持输入人物的身份，我们逐渐合成了输入图像中缺失的区域，使用形态指导的扩散条件，基于 outline和表面法向。然后，我们将这些合成的多视图图像进行 inverse rendering，以获得一个完全纹理化的高分辨率3D mesh。实验表明，我们的方法超过了先前的方法，并实现了从单个图像中渲染出一个具有较复杂的 texture 的360度人体视图，具有真实感。
</details></li>
</ul>
<hr>
<h2 id="DMV3D-Denoising-Multi-View-Diffusion-using-3D-Large-Reconstruction-Model"><a href="#DMV3D-Denoising-Multi-View-Diffusion-using-3D-Large-Reconstruction-Model" class="headerlink" title="DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model"></a>DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09217">http://arxiv.org/abs/2311.09217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, Kai Zhang</li>
<li>for: 本研究的目的是提出一种基于 transformer 的 3D 生成方法，用于去噪多视图填充。</li>
<li>methods: 该方法使用一种基于 NeRF 的三平面表示，通过 NeRF 重建和渲染来去噪多视图图像，实现单stage 3D 生成在 $\sim$30s 内。</li>
<li>results: 我们在大规模多视图图像 dataset 上训练了 \textbf{DMV3D}，并在单个 A100 GPU 上实现了单stage 3D 生成。我们的结果表明，\textbf{DMV3D} 可以在不访问 3D 资产的情况下，实现高质量的单像 reconstruction 和文本到 3D 生成。<details>
<summary>Abstract</summary>
We propose \textbf{DMV3D}, a novel 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. Our reconstruction model incorporates a triplane NeRF representation and can denoise noisy multi-view images via NeRF reconstruction and rendering, achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train \textbf{DMV3D} on large-scale multi-view image datasets of highly diverse objects using only image reconstruction losses, without accessing 3D assets. We demonstrate state-of-the-art results for the single-image reconstruction problem where probabilistic modeling of unseen object parts is required for generating diverse reconstructions with sharp textures. We also show high-quality text-to-3D generation results outperforming previous 3D diffusion models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .
</details>
<details>
<summary>摘要</summary>
我们提出了\textbf{DMV3D}，一种新的3D生成方法，使用变换器基于3D大型重建模型来除噪多视射度。我们的重建模型包括三平面NeRF表示法，可以通过NeRF重建和渲染，将噪声多视图图像进行去噪，实现单stage 3D生成在$\sim$30s内的单个A100 GPU上。我们在大规模多视图图像数据集上培养\textbf{DMV3D}，使用仅图像重建损失进行训练，不需要访问3D资产。我们示出了单个图像重建问题中的状态足够结果，需要采用概率模型来描述未经见到的对象部分，以生成具有锐利тексту化的多种重建。我们还显示了在文本到3D生成中的高质量结果，超过了前一代3D噪声模型。我们的项目网站是：https://justimyhxu.github.io/projects/dmv3d/。
</details></li>
</ul>
<hr>
<h2 id="ConvNet-vs-Transformer-Supervised-vs-CLIP-Beyond-ImageNet-Accuracy"><a href="#ConvNet-vs-Transformer-Supervised-vs-CLIP-Beyond-ImageNet-Accuracy" class="headerlink" title="ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy"></a>ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09215">http://arxiv.org/abs/2311.09215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kirill-vish/beyond-inet">https://github.com/kirill-vish/beyond-inet</a></li>
<li>paper_authors: Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu</li>
<li>for:  This paper aims to provide a comprehensive comparative analysis of different model architectures and training protocols for computer vision tasks, beyond just their ImageNet accuracy.</li>
<li>methods:  The authors use a variety of modern computer vision models, including ConvNets and Vision Transformers, and compare their performance on several tasks beyond ImageNet accuracy, such as mistake types, output calibration, transferability, and feature invariance.</li>
<li>results:  The authors find that while the models have similar ImageNet accuracies and compute requirements, they differ in many other aspects, highlighting the need for more nuanced analysis when choosing among different models.<details>
<summary>Abstract</summary>
Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our code is available at https://github.com/kirill-vish/Beyond-INet.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉提供了多种模型选择，为特定应用场景选择合适的模型可能很吃力。传统上，不同模型架构和训练方法之间的比较通常基于ImageNet分类精度。然而，这一metric不能完全捕捉特定任务中模型表现的细节。在这项工作中，我们进行了深入的比较分析，探讨了不同模型在超参数和CLIP训练方法下的行为特性，包括模型错误类型、输出均衡、传输性和特征不变性等方面。虽然我们选择的模型在ImageNet精度和计算需求上相似，但我们发现它们在很多方面有所不同，包括模型错误类型、输出均衡、传输性和特征不变性等方面。这种多样性在模型特征上，不被传统的指标捕捉，强调了选择模型时需要更加细化的分析。我们的代码可以在https://github.com/kirill-vish/Beyond-INet上找到。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Citizen-Science-for-Flood-Extent-Detection-using-Machine-Learning-Benchmark-Dataset"><a href="#Leveraging-Citizen-Science-for-Flood-Extent-Detection-using-Machine-Learning-Benchmark-Dataset" class="headerlink" title="Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset"></a>Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09276">http://arxiv.org/abs/2311.09276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muthukumaran Ramasubramanian, Iksha Gurung, Shubhankar Gahlot, Ronny Hänsch, Andrew L. Molthan, Manil Maskey</li>
<li>for: 本研究旨在提供高精度的洪水覆盖面积推测方法，以便在紧急应急响应和恢复工作中使用。</li>
<li>methods: 本研究使用了Sentinel-1 C-Band Synthetic Aperture Radar（SAR）图像，并利用机器学习技术来准确地推测洪水覆盖面积。</li>
<li>results: 研究创造了一个标注了水体覆盖面积和洪水覆盖面积的数据集，并在这个数据集基础上进行了一系列的数据处理和模型训练。在 Competition 中，研究者还开放了该数据集，并邀请了社区成员提交自己的模型来测试。研究结果表明，使用机器学习技术可以准确地推测洪水覆盖面积，并且可以在不同的地区和不同的背景下提供高精度的推测结果。<details>
<summary>Abstract</summary>
Accurate detection of inundated water extents during flooding events is crucial in emergency response decisions and aids in recovery efforts. Satellite Remote Sensing data provides a global framework for detecting flooding extents. Specifically, Sentinel-1 C-Band Synthetic Aperture Radar (SAR) imagery has proven to be useful in detecting water bodies due to low backscatter of water features in both co-polarized and cross-polarized SAR imagery. However, increased backscatter can be observed in certain flooded regions such as presence of infrastructure and trees - rendering simple methods such as pixel intensity thresholding and time-series differencing inadequate. Machine Learning techniques has been leveraged to precisely capture flood extents in flooded areas with bumps in backscatter but needs high amounts of labelled data to work desirably. Hence, we created a labeled known water body extent and flooded area extents during known flooding events covering about 36,000 sq. kilometers of regions within mainland U.S and Bangladesh. Further, We also leveraged citizen science by open-sourcing the dataset and hosting an open competition based on the dataset to rapidly prototype flood extent detection using community generated models. In this paper we present the information about the dataset, the data processing pipeline, a baseline model and the details about the competition, along with discussion on winning approaches. We believe the dataset adds to already existing datasets based on Sentinel-1C SAR data and leads to more robust modeling of flood extents. We also hope the results from the competition pushes the research in flood extent detection further.
</details>
<details>
<summary>摘要</summary>
检测洪水覆盖面积是应急应对和恢复努力中非常重要的一环。卫星Remote Sensing数据提供了全球检测洪水覆盖面积的基础。特别是Sentinel-1 C-Band Synthetic Aperture Radar（SAR）成像数据表明可以准确检测水体。然而，在洪水区域中存在特定的障碍物，如基础设施和树木，可能会导致SAR成像数据中的回射强度增加，从而使得简单的像素强度阈值和时间序列差分方法无法准确检测洪水覆盖面积。为了精准检测洪水覆盖面积，我们利用机器学习技术。但是，这些技术需要大量标注数据来工作有效。因此，我们创建了一个标注水体覆盖面积和洪水覆盖面积的数据集，覆盖了美国大陆和孟加拉国的约36,000平方公里地区。此外，我们还利用公民科学，将数据集开源并在基于该数据集的竞赛中公布了社区生成的模型。在本文中，我们介绍了数据集、数据处理管道、基线模型以及竞赛的详细信息，以及赢得竞赛的方法。我们认为该数据集将加强现有基于Sentinel-1C SAR数据的数据集，并促进洪水覆盖面积的模型化。我们也希望竞赛的结果能够推动洪水覆盖面积检测的研究进一步。
</details></li>
</ul>
<hr>
<h2 id="Domain-Aligned-CLIP-for-Few-shot-Classification"><a href="#Domain-Aligned-CLIP-for-Few-shot-Classification" class="headerlink" title="Domain Aligned CLIP for Few-shot Classification"></a>Domain Aligned CLIP for Few-shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09191">http://arxiv.org/abs/2311.09191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Waleed Gondal, Jochen Gast, Inigo Alonso Ruiz, Richard Droste, Tommaso Macri, Suren Kumar, Luitpold Staudigl<br>for:  This paper aims to improve the performance of CLIP, a large vision-language representation learning model, on downstream tasks without requiring full-scale fine-tuning.methods: The proposed method, called Domain Aligned CLIP (DAC), uses a lightweight adapter trained with an intra-modal contrastive objective and a simple framework to modulate the precomputed class text embeddings to improve both intra-modal and inter-modal alignment on target distributions.results: The proposed method achieves consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrates competitive performance on 4 OOD robustness benchmarks, without requiring fine-tuning the main model.<details>
<summary>Abstract</summary>
Large vision-language representation learning models like CLIP have demonstrated impressive performance for zero-shot transfer to downstream tasks while largely benefiting from inter-modal (image-text) alignment via contrastive objectives. This downstream performance can further be enhanced by full-scale fine-tuning which is often compute intensive, requires large labelled data, and can reduce out-of-distribution (OOD) robustness. Furthermore, sole reliance on inter-modal alignment might overlook the rich information embedded within each individual modality. In this work, we introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain Aligned CLIP (DAC), which improves both intra-modal (image-image) and inter-modal alignment on target distributions without fine-tuning the main model. For intra-modal alignment, we introduce a lightweight adapter that is specifically trained with an intra-modal contrastive objective. To improve inter-modal alignment, we introduce a simple framework to modulate the precomputed class text embeddings. The proposed few-shot fine-tuning framework is computationally efficient, robust to distribution shifts, and does not alter CLIP's parameters. We study the effectiveness of DAC by benchmarking on 11 widely used image classification tasks with consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrate competitive performance on 4 OOD robustness benchmarks.
</details>
<details>
<summary>摘要</summary>
大型视力语言表示学习模型如CLIP在零引入下流程任务上表现出色，主要受益于图像文本对对比目标的协调。然而，这些下流程性能可以通过全规模精度调整进一步提高，但这需要大量标注数据、计算密集型和可能导致对于异常情况的不稳定性。此外，凭借solely rely on inter-modal alignment可能会忽略每个个体模式中的丰富信息。在这项工作中，我们提出了一种efficient domain adaptation strategy for CLIP，称为Domain Aligned CLIP（DAC），该策略可以在目标分布上提高图像-图像和图像-文本对对比，无需修改主模型的参数。为了实现图像-图像对对比，我们提出了一个轻量级适配器，该适配器通过图像内模式对对比 objective进行特性化训练。为了提高图像-文本对对比，我们提出了一个简单的框架，用于修改预计算的类文本嵌入。该一些少量微调框架 computationally efficient，对分布变化强度Robust，并不改变CLIP的参数。我们通过对11种广泛使用的图像分类任务进行测试，发现DAC可以在16枚微调时提供2.3%的提升，并在4个OOD robustness benchmark上显示竞争性表现。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computation-of-the-Gaussian-Rate-Distortion-Perception-Function"><a href="#On-the-Computation-of-the-Gaussian-Rate-Distortion-Perception-Function" class="headerlink" title="On the Computation of the Gaussian Rate-Distortion-Perception Function"></a>On the Computation of the Gaussian Rate-Distortion-Perception Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09190">http://arxiv.org/abs/2311.09190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Serra, Photios A. Stavrou, Marios Kountouris</li>
<li>for: 这个论文研究了一种多变量 Gaussian 源的 computation rate-distortion-perception function (RDPF) 的问题，使用 Mean Squared Error (MSE) 损失和不同的抽象度量指标（Kullback-Leibler divergence、geometric Jensen-Shannon divergence、squared Hellinger distance、squared Wasserstein-2 distance）。</li>
<li>methods: 论文首先提供了一个简单的 Gaussian RDPF 的分析 bounds，然后使用 alternating minimization scheme 来解决多变量 RDPF 问题，并提供了一个算法实现。</li>
<li>results: 论文显示了一个关于 tensorizable 损失和感知度量的结论，即依靠 source covariance matrix 的 eigenvector，可以将多变量 RDPF 转化为一个简单的 scalar Gaussian RDPF 问题。此外，论文还提供了一个可行的算法实现，并证明了其收敛性和收敛速率。最后，论文还提供了一个 “perfect realism”  régime 的分析解决方案。<details>
<summary>Abstract</summary>
In this paper, we study the computation of the rate-distortion-perception function (RDPF) for a multivariate Gaussian source under mean squared error (MSE) distortion and, respectively, Kullback-Leibler divergence, geometric Jensen-Shannon divergence, squared Hellinger distance, and squared Wasserstein-2 distance perception metrics. To this end, we first characterize the analytical bounds of the scalar Gaussian RDPF for the aforementioned divergence functions, also providing the RDPF-achieving forward "test-channel" realization. Focusing on the multivariate case, we establish that, for tensorizable distortion and perception metrics, the optimal solution resides on the vector space spanned by the eigenvector of the source covariance matrix. Consequently, the multivariate optimization problem can be expressed as a function of the scalar Gaussian RDPFs of the source marginals, constrained by global distortion and perception levels. Leveraging this characterization, we design an alternating minimization scheme based on the block nonlinear Gauss-Seidel method, which optimally solves the problem while identifying the Gaussian RDPF-achieving realization. Furthermore, the associated algorithmic embodiment is provided, as well as the convergence and the rate of convergence characterization. Lastly, for the "perfect realism" regime, the analytical solution for the multivariate Gaussian RDPF is obtained. We corroborate our results with numerical simulations and draw connections to existing results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了多变量 Gaussian 源的 computation rate-distortion-perception function (RDPF) 下的mean squared error (MSE) 损均和 Kullback-Leibler divergence、geometric Jensen-Shannon divergence、squared Hellinger distance 和 squared Wasserstein-2 distance 的感知metric下的计算。为此，我们首先对scalar Gaussian RDPF 进行了分析，并提供了RDPF-实现的前向"测试渠道"实现。在多变量情况下，我们证明了，对于tensorizable 损均和感知metric，最佳解在源协方差矩阵的 eigenvector 上。因此，多变量优化问题可以表示为scalar Gaussian RDPF 的source marginals的函数，受到全局损均和感知水平的限制。基于这种特征，我们提出了一种alternating minimization scheme，使用非线性 Gauss-Seidel 方法来优化问题，并实现了RDPF-实现。此外，我们还提供了算法的实现、 convergency 和速度分析。最后，在"完美现实" regime 下，我们获得了多变量 Gaussian RDPF 的分析解。我们的结果通过数值仿真得到了证明，并与现有结果进行了比较。
</details></li>
</ul>
<hr>
<h2 id="RBPGAN-Recurrent-Back-Projection-GAN-for-Video-Super-Resolution"><a href="#RBPGAN-Recurrent-Back-Projection-GAN-for-Video-Super-Resolution" class="headerlink" title="RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution"></a>RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09178">http://arxiv.org/abs/2311.09178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dareen Hussein, Hesham Eraqi, Israa Fahmy, Marwah Sulaiman, Mohammed Barakat, Mohammed El-Naggar, Moustafa Youssef, Zahraa Shehabeldin</li>
<li>for: 提高视频超解像（VSR）任务中的时间启示和空间细节准确性。</li>
<li>methods: 提议使用Recurrent Back-Projection Generative Adversarial Network（RBPGAN）模型，结合了两种现有模型的优点，以提高时间一致性和空间细节准确性。</li>
<li>results: 模型在不同数据集上表现出较高的时间一致性和空间细节准确性，与之前的工作相比，模型的性能有所提高。<details>
<summary>Abstract</summary>
Recently, video super resolution (VSR) has become a very impactful task in the area of Computer Vision due to its various applications. In this paper, we propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for VSR in an attempt to generate temporally coherent solutions while preserving spatial details. RBPGAN integrates two state-of-the-art models to get the best in both worlds without compromising the accuracy of produced video. The generator of the model is inspired by RBPN system, while the discriminator is inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal consistency over time. Our contribution together results in a model that outperforms earlier work in terms of temporally consistent details, as we will demonstrate qualitatively and quantitatively using different datasets.
</details>
<details>
<summary>摘要</summary>
近些时候，视频超分辨 (VSR) 已成为计算机视觉领域的非常有影响的任务，因为它在各种应用领域中扮演着重要的角色。在这篇论文中，我们提出了循环回投生成 adversarial网络 (RBPGAN) 模型，用于解决 VSR 问题，并且保持时间准确性和空间细节的平衡。RBPGAN 模型 integrate two state-of-the-art 模型，以获得最佳的效果。生成器部分受到 RBPN 系统的启发，而抑制器部分受到 TecoGAN 的启发。我们还使用了 ping-pong 损失函数，以提高时间一致性。我们的贡献结合起来，导致模型在时间一致性和细节上表现出色，我们将通过不同的数据集来证明这一点。
</details></li>
</ul>
<hr>
<h2 id="WildlifeDatasets-An-open-source-toolkit-for-animal-re-identification"><a href="#WildlifeDatasets-An-open-source-toolkit-for-animal-re-identification" class="headerlink" title="WildlifeDatasets: An open-source toolkit for animal re-identification"></a>WildlifeDatasets: An open-source toolkit for animal re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09118">http://arxiv.org/abs/2311.09118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wildlifedatasets/wildlife-datasets">https://github.com/wildlifedatasets/wildlife-datasets</a></li>
<li>paper_authors: Vojtěch Čermák, Lukas Picek, Lukáš Adam, Kostas Papafitsoros</li>
<li>for:  primarily for ecologists and computer-vision &#x2F; machine-learning researchers</li>
<li>methods:  provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning</li>
<li>results:  provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin.<details>
<summary>Abstract</summary>
In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了 WildlifeDatasets（https://github.com/WildlifeDatasets/wildlife-datasets）-一个开源工具箱，主要针对生态学家和计算机视觉/机器学习研究人员。WildlifeDatasets 是使用 Python 编写的，可以方便地访问公共可用的野生生物数据集，并提供了许多方法 для数据集预处理、性能分析和模型细化。我们在多种场景和基线实验中展示了工具箱，包括我们知道的最完整的 эксперименталь比较 wildlife 重复标识数据集和方法，包括本地描述器和深度学习方法。此外，我们还提供了首先基于个体重复标识的宽范种类基模型 - MegaDescriptor - ，它在动物重复标识数据集中提供了状态机器的性能，并在其他预训练模型 such as CLIP 和 DINOv2 之上出色表现。为使模型对普通公众开放，并让其与任何现有的野生监测应用程序轻松集成，我们提供了多种 MegaDescriptor FLAVORS（i.e., Small, Medium, and Large）通过 HuggingFace 平台（https://huggingface.co/BVRA）。
</details></li>
</ul>
<hr>
<h2 id="Cross-view-and-Cross-pose-Completion-for-3D-Human-Understanding"><a href="#Cross-view-and-Cross-pose-Completion-for-3D-Human-Understanding" class="headerlink" title="Cross-view and Cross-pose Completion for 3D Human Understanding"></a>Cross-view and Cross-pose Completion for 3D Human Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09104">http://arxiv.org/abs/2311.09104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas, Vincent Leroy, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez</li>
<li>for: 这个论文主要针对人类视觉理解领域，旨在利用大量预训练模型和大数据来提高计算机视觉的性能。</li>
<li>methods: 这个论文提出了一种基于自我超vised学习的预训练方法，使用人类数据集来学习人类特征和运动约束。该方法使用了双视相差和时间相差对来学习3D和人体运动的约束。</li>
<li>results: 该论文通过对多种人类中心任务进行预训练，并在多种人类模型回归任务上达到了最佳性能。此外，该方法还可以在不需要大量 Label 的情况下进行预训练。<details>
<summary>Abstract</summary>
Human perception and understanding is a major domain of computer vision which, like many other vision subdomains recently, stands to gain from the use of large models pre-trained on large datasets. We hypothesize that the most common pre-training strategy of relying on general purpose, object-centric image datasets such as ImageNet, is limited by an important domain shift. On the other hand, collecting domain specific ground truth such as 2D or 3D labels does not scale well. Therefore, we propose a pre-training approach based on self-supervised learning that works on human-centric data using only images. Our method uses pairs of images of humans: the first is partially masked and the model is trained to reconstruct the masked parts given the visible ones and a second image. It relies on both stereoscopic (cross-view) pairs, and temporal (cross-pose) pairs taken from videos, in order to learn priors about 3D as well as human motion. We pre-train a model for body-centric tasks and one for hand-centric tasks. With a generic transformer architecture, these models outperform existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and obtain state-of-the-art performance for instance when fine-tuning for model-based and model-free human mesh recovery.
</details>
<details>
<summary>摘要</summary>
人类认知和理解是计算机视觉中的一个主要领域，与其他视觉子领域一样，它也可以从使用大型预训练模型和大量数据中获得更好的表现。我们假设，使用通用的物体中心图像集如ImageNet进行预训练的方法，受到重要的领域转换的限制。而收集特定领域的实际数据，如2D或3D标签，不可靠。因此，我们提出了基于自我超vised学习的预训练方法，该方法使用人类数据图像来进行预训练，并且只使用图像。我们的方法使用人类图像的两个视图（即左右两个图像）和视频中的两个姿态（即左右两个姿态）来学习人体的3D和运动约束。我们预训练了一个身体中心任务的模型和一个手中心任务的模型。使用通用的转换架构，这些模型在许多人类中心下沉天 зада务中表现出色，并在人体碰撞恢复任务中获得了状态之一的表现。
</details></li>
</ul>
<hr>
<h2 id="Guided-Scale-Space-Radon-Transform-for-linear-structures-detection"><a href="#Guided-Scale-Space-Radon-Transform-for-linear-structures-detection" class="headerlink" title="Guided Scale Space Radon Transform for linear structures detection"></a>Guided Scale Space Radon Transform for linear structures detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09103">http://arxiv.org/abs/2311.09103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aicha Baya Goumeidane, Djemel Ziou, Nafaa Nacereddine</li>
<li>for:  automatic detection of thick linear structures in gray scale and binary images</li>
<li>methods:  using the Scale Space Radon Transform (SSRT) with calculated Hessian orientations to emphasize linear structures and reduce unwanted peaks</li>
<li>results:  efficient detection of lines of different thickness in synthetic and real images, with robustness against noise and complex background<details>
<summary>Abstract</summary>
Using integral transforms to the end of lines detection in images with complex background, makes the detection a hard task needing additional processing to manage the detection. As an integral transform, the Scale Space Radon Transform (SSRT) suffers from such drawbacks, even with its great abilities for thick lines detection. In this work, we propose a method to address this issue for automatic detection of thick linear structures in gray scale and binary images using the SSRT, whatever the image background content. This method involves the calculated Hessian orientations of the investigated image while computing its SSRT, in such a way that linear structures are emphasized in the SSRT space. As a consequence, the subsequent maxima detection in the SSRT space is done on a modified transform space freed from unwanted parts and, consequently, from irrelevant peaks that usually drown the peaks representing lines. Besides, highlighting the linear structure in the SSRT space permitting, thus, to efficiently detect lines of different thickness in synthetic and real images, the experiments show also the method robustness against noise and complex background.
</details>
<details>
<summary>摘要</summary>
Our method involves calculating the Hessian orientations of the investigated image while computing its SSRT, which emphasizes linear structures in the SSRT space. As a result, the subsequent maxima detection in the SSRT space is done on a modified transform space that is free from unwanted parts and irrelevant peaks that usually drown the peaks representing lines. This allows us to efficiently detect lines of different thickness in synthetic and real images, and the experiments show that our method is robust against noise and complex backgrounds.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Computer-Vision-in-Autonomous-Vehicles-Methods-Challenges-and-Future-Directions"><a href="#Applications-of-Computer-Vision-in-Autonomous-Vehicles-Methods-Challenges-and-Future-Directions" class="headerlink" title="Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions"></a>Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09093">http://arxiv.org/abs/2311.09093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingshuai Dong, Massimiliano L. Cappuccio</li>
<li>For: 本文review了过去十年内关于计算机视觉和自动驾驶的发展，包括主要汽车制造商在不同国家的自动驾驶系统的开发，感知器和测试数据集的使用，以及计算机视觉在自动驾驶中的应用。* Methods: 本文 investigate了自动驾驶系统的开发，包括感知器和测试数据集的使用，以及计算机视觉在自动驾驶中的应用，包括深度估计、物体检测、车道检测和交通标志识别等。* Results: 本文 analyze了自动驾驶技术的当前挑战和未来研究方向，包括计算机视觉在自动驾驶中的应用和发展趋势。<details>
<summary>Abstract</summary>
Autonomous vehicle refers to a vehicle capable of perceiving its surrounding environment and driving with little or no human driver input. The perception system is a fundamental component which enables the autonomous vehicle to collect data and extract relevant information from the environment to drive safely. Benefit from the recent advances in computer vision, the perception task can be achieved by using sensors, such as camera, LiDAR, radar, and ultrasonic sensor. This paper reviews publications on computer vision and autonomous driving that are published during the last ten years. In particular, we first investigate the development of autonomous driving systems and summarize these systems that are developed by the major automotive manufacturers from different countries. Second, we investigate the sensors and benchmark data sets that are commonly utilized for autonomous driving. Then, a comprehensive overview of computer vision applications for autonomous driving such as depth estimation, object detection, lane detection, and traffic sign recognition are discussed. Additionally, we review public opinions and concerns on autonomous vehicles. Based on the discussion, we analyze the current technological challenges that autonomous vehicles meet with. Finally, we present our insights and point out some promising directions for future research. This paper will help the reader to understand autonomous vehicles from the perspectives of academia and industry.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆指一种可以自动感知周围环境并减少或完全无需人类驾驶员输入的车辆。感知系统是自动驾驶车辆的基本组件，它使得自动驾驶车辆能够收集环境数据并提取有关信息以安全驾驶。利用计算机视觉的最新进展，感知任务可以通过感知器 such as 摄像头、LiDAR、雷达和超声波感知器来实现。本文对计算机视觉和自动驾驶相关的出版物进行了十年的查询和总结。特别是，我们首先调查了各国主要汽车制造商开发的自动驾驶系统，然后探讨了通用的感知器和测试数据集。接着，我们对计算机视觉在自动驾驶中的应用进行了全面的概述，包括深度估计、物体检测、路径检测和交通信号识别。此外，我们还审查了自动驾驶车辆的公众意见和担忧，并分析了自动驾驶车辆面临的当前技术挑战。最后，我们提出了一些有前途的研究方向。本文将帮助读者更好地理解自动驾驶车辆的学术和industry 的视角。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Transformer-Learning-with-Proximity-Data-Generation-for-Text-Based-Person-Search"><a href="#Contrastive-Transformer-Learning-with-Proximity-Data-Generation-for-Text-Based-Person-Search" class="headerlink" title="Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search"></a>Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09084">http://arxiv.org/abs/2311.09084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hcplab-sysu/personsearch-ctlg">https://github.com/hcplab-sysu/personsearch-ctlg</a></li>
<li>paper_authors: Hefeng Wu, Weifeng Chen, Zhibin Liu, Tianshui Chen, Zhiguang Chen, Liang Lin</li>
<li>for: 文章目的是提出一种简单 yet effective的 dual Transformer模型，用于图像库中的文本基于人脸检索任务。</li>
<li>methods: 该模型使用一种具有困难感的对比学习策略，以及一种自动生成数据模块（PDG模块）来提高数据多样性。</li>
<li>results: 实验结果表明，提出的方法可以 evidently 超越当前最佳方法，例如在CUHK-PEDES和ICFG-PEDES两个数据集上提高Top1、Top5、Top10的性能，具体提高3.88%、4.02%和2.92%。<details>
<summary>Abstract</summary>
Given a descriptive text query, text-based person search (TBPS) aims to retrieve the best-matched target person from an image gallery. Such a cross-modal retrieval task is quite challenging due to significant modality gap, fine-grained differences and insufficiency of annotated data. To better align the two modalities, most existing works focus on introducing sophisticated network structures and auxiliary tasks, which are complex and hard to implement. In this paper, we propose a simple yet effective dual Transformer model for text-based person search. By exploiting a hardness-aware contrastive learning strategy, our model achieves state-of-the-art performance without any special design for local feature alignment or side information. Moreover, we propose a proximity data generation (PDG) module to automatically produce more diverse data for cross-modal training. The PDG module first introduces an automatic generation algorithm based on a text-to-image diffusion model, which generates new text-image pair samples in the proximity space of original ones. Then it combines approximate text generation and feature-level mixup during training to further strengthen the data diversity. The PDG module can largely guarantee the reasonability of the generated samples that are directly used for training without any human inspection for noise rejection. It improves the performance of our model significantly, providing a feasible solution to the data insufficiency problem faced by such fine-grained visual-linguistic tasks. Extensive experiments on two popular datasets of the TBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach outperforms state-of-the-art approaches evidently, e.g., improving by 3.88%, 4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be available at https://github.com/HCPLab-SYSU/PersonSearch-CTLG
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>文本基于人搜索（TBPS）目标是从图库中检索最佳匹配的人。这是跨Modalities的检索任务，因为视觉和语言之间存在显著的差异和细化差异，同时缺乏标注数据。大多数现有的方法都是通过引入复杂的网络结构和辅助任务来减少这些差异，但这些方法复杂和困难实现。在这篇论文中，我们提出了一种简单 yet 有效的双Transformer模型，用于文本基于人搜索。我们通过强制硬度感知的对比学习策略，使我们的模型在没有特殊设计的本地特征对齐和侧信息的情况下，达到了当前最佳性能。此外，我们还提出了一种近似数据生成（PDG）模块，用于自动生成更多的跨Modalities训练数据。PDG模块首先引入了基于文本到图像扩散模型的自动生成算法，生成了新的文本-图像对amples在原始对amples的 proximity 空间中。然后，它将approximate text生成和特征级mixup在训练中结合，以进一步增强数据多样性。PDG模块可以确保生成的样本的合理性，不需要人工检查噪音抛弃。这使我们的模型表现得更好，提供了跨Modalities任务中数据不足问题的可行解决方案。我们在CUHK-PEDES和ICFG-PEDES两个流行的TBPS任务上进行了广泛的实验，并证明了我们的方法在Top1、Top5、Top10等指标上明显超过了现有方法，例如在CUHK-PEDES上提高了3.88%, 4.02%, 2.92%。代码将在https://github.com/HCPLab-SYSU/PersonSearch-CTLG 上提供。
</details></li>
</ul>
<hr>
<h2 id="Spiking-NeRF-Representing-the-Real-World-Geometry-by-a-Discontinuous-Representation"><a href="#Spiking-NeRF-Representing-the-Real-World-Geometry-by-a-Discontinuous-Representation" class="headerlink" title="Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation"></a>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09077">http://arxiv.org/abs/2311.09077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan</li>
<li>for: 本研究的目的是提出一种使用脉冲神经网络（Spiking Neural Network，SNN）和混合ANN-SNN框架来构建不连续的温度场，以获得准确的geometry表示。</li>
<li>methods: 本研究使用了多层感知器（Multi-layer Perceptron，MLP）来构建感知场，但是实际的geometry或温度场经常在空气和表面之间存在缺陷，这会导致不准确的geometry表示。为了解决这个问题，本研究提出了使用脉冲神经网络来构建不连续的温度场。</li>
<li>results: 本研究的结果达到了顶尖性能水平。我们的代码和数据将被公开发布。<details>
<summary>Abstract</summary>
A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of spiking neuron and the theoretical accuracy of geometry, Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our results achieve SOTA performance. Our code and data will be released to the public.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate the following text into Simplified Chinese:A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of spiking neuron and the theoretical accuracy of geometry. Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our results achieve SOTA performance. Our code and data will be released to the public.中文简体版：一个重要的原因是现有的NeRF基于方法的成功是通过多层见解器层（MLP）建立神经核密度场来表示几何。然而，实际的几何或密度场经常在空气和表面之间存在缺陷，这会导致不准确的几何表示。为此，这篇论文提出了脉冲NeRF，利用脉冲神经和混合人工神经网络（ANN）-脉冲神经网络（SNN）框架建立不连续密度场以实现准确的几何表示。具体来说，我们首先解释了连续密度场会带来的不准确性。然后，我们提出使用脉冲神经建立不连续密度场。我们对现有脉冲神经模型的问题进行了全面的分析，并提供了密度场参数与理论几何准确性之间的数学关系。基于这，我们提出了固定脉冲神经来建立不连续密度场。我们的结果达到了最佳性能。我们的代码和数据将被公开发布。
</details></li>
</ul>
<hr>
<h2 id="Imagine-the-Unseen-World-A-Benchmark-for-Systematic-Generalization-in-Visual-World-Models"><a href="#Imagine-the-Unseen-World-A-Benchmark-for-Systematic-Generalization-in-Visual-World-Models" class="headerlink" title="Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models"></a>Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09064">http://arxiv.org/abs/2311.09064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeongbin Kim, Gautam Singh, Junyeong Park, Caglar Gulcehre, Sungjin Ahn</li>
<li>for: 这篇论文主要目标是提出一个新的视觉具有系统性的想象能力挑战，即通过创建一个世界模型来适应新的情境。</li>
<li>methods: 论文提出了一个新的测试框架——系统性视觉幻想评价框架（SVIB），用于评价模型在视觉域的系统性想象能力。SVIB包括一个小型世界模型问题，其中模型需要根据一个 latent 世界动力来生成一步图像到图像的转换。</li>
<li>results: 论文对 SVIB 进行了全面的评价，并提供了一些基线模型的性能分析，以帮助进一步推动视觉系统性 compositionality。<details>
<summary>Abstract</summary>
Systematic compositionality, or the ability to adapt to novel situations by creating a mental model of the world using reusable pieces of knowledge, remains a significant challenge in machine learning. While there has been considerable progress in the language domain, efforts towards systematic visual imagination, or envisioning the dynamical implications of a visual observation, are in their infancy. We introduce the Systematic Visual Imagination Benchmark (SVIB), the first benchmark designed to address this problem head-on. SVIB offers a novel framework for a minimal world modeling problem, where models are evaluated based on their ability to generate one-step image-to-image transformations under a latent world dynamics. The framework provides benefits such as the possibility to jointly optimize for systematic perception and imagination, a range of difficulty levels, and the ability to control the fraction of possible factor combinations used during training. We provide a comprehensive evaluation of various baseline models on SVIB, offering insight into the current state-of-the-art in systematic visual imagination. We hope that this benchmark will help advance visual systematic compositionality.
</details>
<details>
<summary>摘要</summary>
系统化组合性，或者在新的情况下适应by创建一个使用可重用知识的世界模型，是机器学习中的一项重要挑战。虽然在语言领域已经取得了很大的进步，但尝试实现系统视觉想象，或者从视觉观察中预测动态效果，还处于初期阶段。我们提出了系统视觉想象标准套件（SVIB），这是第一个直接面对这个问题的标准套件。SVIB提供了一个新的世界模型最小化问题的框架，在这个框架中，模型被评估基于它们在一步图像到图像转换中的能力。这个框架具有多种优点，如同时优化系统见解和想象、多个难度水平和在训练中使用可能的因素组合的控制。我们对SVIB进行了全面的评估，并提供了当前领域的状态态报告，以帮助进一步提高视觉系统组合性。
</details></li>
</ul>
<hr>
<h2 id="Self-Annotated-3D-Geometric-Learning-for-Smeared-Points-Removal"><a href="#Self-Annotated-3D-Geometric-Learning-for-Smeared-Points-Removal" class="headerlink" title="Self-Annotated 3D Geometric Learning for Smeared Points Removal"></a>Self-Annotated 3D Geometric Learning for Smeared Points Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09029">http://arxiv.org/abs/2311.09029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaowei Wang, Daniel Morris</li>
<li>for: 提高consumer级密集深度感知器的准确性和质量</li>
<li>methods: 使用自动检测和标注多视角3D几何证据来自动检测和标注涂抹点</li>
<li>results: 比 tradicional filters和其他自动标注方法高效，可以减少 fictitious surfaces 的影响Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文提出了一种自动检测和标注 consumer级密集深度感知器中的涂抹点（smeared points）的方法，以提高深度图的准确性和质量。</li>
<li>methods: 我们使用多视角3D几何证据自动检测和标注涂抹点，并通过自动生成的 annotated dataset 进行训练。</li>
<li>results: 我们的方法比传统的滤波器和其他自动标注方法高效，可以减少 fictitious surfaces 的影响，提高深度图的准确性和质量。I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
There has been significant progress in improving the accuracy and quality of consumer-level dense depth sensors. Nevertheless, there remains a common depth pixel artifact which we call smeared points. These are points not on any 3D surface and typically occur as interpolations between foreground and background objects. As they cause fictitious surfaces, these points have the potential to harm applications dependent on the depth maps. Statistical outlier removal methods fare poorly in removing these points as they tend also to remove actual surface points. Trained network-based point removal faces difficulty in obtaining sufficient annotated data. To address this, we propose a fully self-annotated method to train a smeared point removal classifier. Our approach relies on gathering 3D geometric evidence from multiple perspectives to automatically detect and annotate smeared points and valid points. To validate the effectiveness of our method, we present a new benchmark dataset: the Real Azure-Kinect dataset. Experimental results and ablation studies show that our method outperforms traditional filters and other self-annotated methods. Our work is publicly available at https://github.com/wangmiaowei/wacv2024_smearedremover.git.
</details>
<details>
<summary>摘要</summary>
在提高consumer级稠密深度感知器的准确性和质量方面，有了 significiant progress。然而，仍然存在一种常见的深度像素artefact，我们称之为“杂Points”。这些点不在任何3D表面上，通常发生在前景和背景对象之间的插值。由于它们引起的虚拟表面，这些点有可能对依赖深度地图的应用程序产生害。统计方法不够 Effective in removing these points, as they tend to remove actual surface points as well. 网络训练方法也难以获得足够的标注数据。为此，我们提出了一种完全自动注释的方法，用于训练深度杂点除去类ifier。我们的方法基于多个视角中收集的3D几何证据自动检测和注释杂点和有效点。为验证我们的方法的有效性，我们提出了一个新的benchmark数据集：Real Azure-Kinect数据集。实验和剥离学研究表明，我们的方法比传统的筛选器和其他自动注释方法更高效。我们的工作可以在https://github.com/wangmiaowei/wacv2024_smearedremover.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Fast-Certification-of-Vision-Language-Models-Using-Incremental-Randomized-Smoothing"><a href="#Fast-Certification-of-Vision-Language-Models-Using-Incremental-Randomized-Smoothing" class="headerlink" title="Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing"></a>Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09024">http://arxiv.org/abs/2311.09024</a></li>
<li>repo_url: None</li>
<li>paper_authors: A K Nirala, A Joshi, C Hegde, S Sarkar</li>
<li>for: 这个研究是为了提高CLIP模型的可靠性和扩展性，使其能够在不同领域中进行灵活的应用。</li>
<li>methods: 这个研究使用了Randomized Smoothing技术来实现Open Vocabulary Certification（OVC），将一个基本的“训练”集合Prompts和其相应的CLIP分类器作为起点，然后通过将这个分类器视为附近的基本训练集合中的一个“偏移”版本，快速认证新的分类器。</li>
<li>results: 这个研究通过实验评估，显示OVC可以实现大约两个数量级的加速，并且可以在不同的视觉语言后门上进行扩展。<details>
<summary>Abstract</summary>
A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild.   In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base "training" set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.
</details>
<details>
<summary>摘要</summary>
“CLIP型深度视语模型具有零shot开 vocabulary分类的能力，即用户可以使用自然语言提示来定义新的类别标签在推理时。然而，CLIP基于模型对攻击性质的敏感性很高，因此确保其可靠部署在野外是关键。在这项工作中，我们介绍了开 vocabulary认证（OVC），一种基于随机平滑技术的快速认证方法，专门针对开 vocabulary模型 like CLIP。给定一个基础“训练”集的提示和其相应的认证CLIP分类器，OVC利用提示的近似性来快速认证novel分类器。通过缓存技术，我们实现了约两个数量级的加速。此外，OVC使用多重正态分布来缓过 embedding 空间的计算，从而快速地 aproximate  embedding 空间。我们通过实验评估了OVC在 CIFAR-10 和 ImageNet 测试集上的效果。”Note that Simplified Chinese is used here, which is a common writing system used in mainland China. If you prefer Traditional Chinese, I can also provide the translation.
</details></li>
</ul>
<hr>
<h2 id="Incremental-Object-Based-Novelty-Detection-with-Feedback-Loop"><a href="#Incremental-Object-Based-Novelty-Detection-with-Feedback-Loop" class="headerlink" title="Incremental Object-Based Novelty Detection with Feedback Loop"></a>Incremental Object-Based Novelty Detection with Feedback Loop</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09004">http://arxiv.org/abs/2311.09004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Caldarella, Elisa Ricci, Rahaf Aljundi</li>
<li>for: 本研究旨在提高对物体检测模型的不确定性检测，以避免在实际应用中可能产生危害的行为，如自动驾驶车或自动机器人中的对象检测模型。</li>
<li>methods: 本研究提出了一种基于人工反馈的对象基本不确定性检测方法，假设可以在预训练后请求人类反馈，并将其 incorporated 到ND模型中，以提高ND模型的Robustness。</li>
<li>results: 研究人员通过设计一种轻量级的ND模块，并在这个模块上进行了增量更新，使得ND模型能够更好地适应不确定性检测任务。同时，研究人员还提出了一个新的评估标准，用于评估ND方法在这种新的设定下的性能。经过广泛的测试和比较，研究人员发现他们的ND方法在robustness和feedback incorporation两个方面都表现出色。<details>
<summary>Abstract</summary>
Object-based Novelty Detection (ND) aims to identify unknown objects that do not belong to classes seen during training by an object detection model. The task is particularly crucial in real-world applications, as it allows to avoid potentially harmful behaviours, e.g. as in the case of object detection models adopted in a self-driving car or in an autonomous robot. Traditional approaches to ND focus on one time offline post processing of the pretrained object detection output, leaving no possibility to improve the model robustness after training and discarding the abundant amount of out-of-distribution data encountered during deployment.   In this work, we propose a novel framework for object-based ND, assuming that human feedback can be requested on the predicted output and later incorporated to refine the ND model without negatively affecting the main object detection performance. This refinement operation is repeated whenever new feedback is available. To tackle this new formulation of the problem for object detection, we propose a lightweight ND module attached on top of a pre-trained object detection model, which is incrementally updated through a feedback loop. We also propose a new benchmark to evaluate methods on this new setting and test extensively our ND approach against baselines, showing increased robustness and a successful incorporation of the received feedback.
</details>
<details>
<summary>摘要</summary>
Traditional approaches to ND focus on one-time offline post-processing of the pre-trained object detection output, leaving no room for improving the model's robustness after training and discarding a large amount of out-of-distribution data encountered during deployment.In this work, we propose a new framework for object-based ND, which assumes that human feedback can be requested on the predicted output and later incorporated to refine the ND model without negatively affecting the main object detection performance. This refinement operation is repeated whenever new feedback is available.To tackle this new formulation of the problem for object detection, we propose a lightweight ND module attached on top of a pre-trained object detection model, which is incrementally updated through a feedback loop. We also propose a new benchmark to evaluate methods on this new setting and test extensively our ND approach against baselines, showing increased robustness and a successful incorporation of the received feedback.
</details></li>
</ul>
<hr>
<h2 id="Simple-but-Effective-Unsupervised-Classification-for-Specified-Domain-Images-A-Case-Study-on-Fungi-Images"><a href="#Simple-but-Effective-Unsupervised-Classification-for-Specified-Domain-Images-A-Case-Study-on-Fungi-Images" class="headerlink" title="Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images"></a>Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08995">http://arxiv.org/abs/2311.08995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaocong liu, Fa Zhang, Lin Cheng, Huanxi Deng, Xiaoyan Yang, Zhenyu Zhang, Chichun Zhou</li>
<li>for: 这篇论文的目的是提出一种无监督分类方法，以减少对特殊领域图像分类器的依赖。</li>
<li>methods: 本文使用了三个关键想法：1）使用预训模型和数据推导学习独立Featuredimensionality reduction，2）多个聚类算法的投票机制，3）后期对于没有预先注释的图像进行调整。</li>
<li>results: 本文的无监督分类方法在 fungal 图像数据集上表现出来的准确率高于超级监督方法，实际上是 94.1% 和 96.7%。这种无监督分类方法可以实现减少对预先注释数据的依赖，并且可以让研究人员在不同的领域中轻松地建立数据集，促进AI应用程序在特殊领域的图像分类。<details>
<summary>Abstract</summary>
High-quality labeled datasets are essential for deep learning. Traditional manual annotation methods are not only costly and inefficient but also pose challenges in specialized domains where expert knowledge is needed. Self-supervised methods, despite leveraging unlabeled data for feature extraction, still require hundreds or thousands of labeled instances to guide the model for effective specialized image classification. Current unsupervised learning methods offer automatic classification without prior annotation but often compromise on accuracy. As a result, efficiently procuring high-quality labeled datasets remains a pressing challenge for specialized domain images devoid of annotated data. Addressing this, an unsupervised classification method with three key ideas is introduced: 1) dual-step feature dimensionality reduction using a pre-trained model and manifold learning, 2) a voting mechanism from multiple clustering algorithms, and 3) post-hoc instead of prior manual annotation. This approach outperforms supervised methods in classification accuracy, as demonstrated with fungal image data, achieving 94.1% and 96.7% on public and private datasets respectively. The proposed unsupervised classification method reduces dependency on pre-annotated datasets, enabling a closed-loop for data classification. The simplicity and ease of use of this method will also bring convenience to researchers in various fields in building datasets, promoting AI applications for images in specialized domains.
</details>
<details>
<summary>摘要</summary>
高品质标注数据是深度学习的关键。传统的手动标注方法不仅成本高昂且不可靠，而且在专业领域中需要专业知识。无监督方法，尽管利用无标注数据进行特征提取，仍需要数百或千个标注实例来导引模型以获得有效的专业图像分类。当前的无监督学习方法可以自动分类无需先前的标注，但通常会妥协准确性。因此，efficiently 获取高品质标注数据仍然是专业领域图像中无标注数据的焦点问题。为解决这个问题，一种无监督分类方法的三个关键想法被提出：1）使用预训练模型和拓扑学来实现双步特征维度减少，2）从多个聚类算法中选择多数票，和3） poste-hoc 而不是先前的手动标注。这种方法在分类准确率方面超过了指导方法，如果用蘑菇图像数据进行示例，分别达到了94.1%和96.7%的公共和私人数据集的分类精度。提议的无监督分类方法减少了对预先标注数据的依赖，使得数据分类成为了关闭的循环。此方法的简单易用性也将为各种领域的研究人员在建立数据集方面带来便利，推动图像特有领域的AI应用。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-approaches-based-on-optimal-transport-and-convex-analysis-for-inverse-problems-in-imaging"><a href="#Unsupervised-approaches-based-on-optimal-transport-and-convex-analysis-for-inverse-problems-in-imaging" class="headerlink" title="Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging"></a>Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08972">http://arxiv.org/abs/2311.08972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcello Carioni, Subhadip Mukherjee, Hong Ye Tan, Junqi Tang</li>
<li>for: 本文主要针对无监督学习方法在图像逆问题中的应用，尤其是基于最优运输和凸分析的方法。</li>
<li>methods: 本文主要介绍了一些有理性的无监督学习方法，包括循环一致性基于模型和学习反抗训练方法，以及一些最近的加速图像逆问题解决的提速学习算法。</li>
<li>results: 本文综述了一些有理性的无监督学习方法，包括某些加速图像逆问题解决的提速学习算法，以及一些相关的无监督学习框架。<details>
<summary>Abstract</summary>
Unsupervised deep learning approaches have recently become one of the crucial research areas in imaging owing to their ability to learn expressive and powerful reconstruction operators even when paired high-quality training data is scarcely available. In this chapter, we review theoretically principled unsupervised learning schemes for solving imaging inverse problems, with a particular focus on methods rooted in optimal transport and convex analysis. We begin by reviewing the optimal transport-based unsupervised approaches such as the cycle-consistency-based models and learned adversarial regularization methods, which have clear probabilistic interpretations. Subsequently, we give an overview of a recent line of works on provably convergent learned optimization algorithms applied to accelerate the solution of imaging inverse problems, alongside their dedicated unsupervised training schemes. We also survey a number of provably convergent plug-and-play algorithms (based on gradient-step deep denoisers), which are among the most important and widely applied unsupervised approaches for imaging problems. At the end of this survey, we provide an overview of a few related unsupervised learning frameworks that complement our focused schemes. Together with a detailed survey, we provide an overview of the key mathematical results that underlie the methods reviewed in the chapter to keep our discussion self-contained.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过深度学习的无监督方法，近期在成像领域中成为一个关键的研究方向，因为它们可以学习表达力强的重建算法，即使精美训练数据 scarce。在这章中，我们将 theoretically principled 无监督学习方案 для解决成像 inverse problems 进行文献综述，尤其是基于最优运输和几何分析的方法。我们首先介绍了基于循环一致性的模型和学习对抗regularization方法，这些方法具有明确的概率解释。然后，我们给出了一个最近的一线工作的概述，包括加速解决成像 inverse problems 的学习优化算法，以及其相应的无监督训练方案。此外，我们还介绍了一些可靠地 convergent 的插件和执行算法（基于梯度步骤深度去噪器），它们在成像问题中广泛应用。在这个综述中，我们还提供了一些相关的无监督学习框架，以及这些方法的关键数学结果，以使我们的讨论自包含。
</details></li>
</ul>
<hr>
<h2 id="A-Spectral-Diffusion-Prior-for-Hyperspectral-Image-Super-Resolution"><a href="#A-Spectral-Diffusion-Prior-for-Hyperspectral-Image-Super-Resolution" class="headerlink" title="A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution"></a>A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08955">http://arxiv.org/abs/2311.08955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjun Liu, Zebin Wu, Liang Xiao</li>
<li>for:  fusion-based hyperspectral image (HSI) super-resolution</li>
<li>methods: spectral diffusion prior, maximum a posteriori, Adam optimization</li>
<li>results: effective in enhancing spatial resolution and preserving spectral information, demonstrated on both synthetic and real datasets.Here’s the full Chinese text:</li>
<li>for: 融合基于干扰图像（HSI）超分辨率，旨在生成高空间分辨率HSI，通过融合低空间分辨率HSI和高空间分辨率多spectral图像。</li>
<li>methods:  spectral diffusion prior，最大 posteriori，Adam优化</li>
<li>results: 能够提高空间分辨率，同时保持spectral信息，在 sintetic和实际数据集上进行了实验。Note: “spectral diffusion prior” is a translation of “spectral diffusion model” in the original text.<details>
<summary>Abstract</summary>
Fusion-based hyperspectral image (HSI) super-resolution aims to produce a high-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a high-spatial-resolution multispectral image. Such a HSI super-resolution process can be modeled as an inverse problem, where the prior knowledge is essential for obtaining the desired solution. Motivated by the success of diffusion models, we propose a novel spectral diffusion prior for fusion-based HSI super-resolution. Specifically, we first investigate the spectrum generation problem and design a spectral diffusion model to model the spectral data distribution. Then, in the framework of maximum a posteriori, we keep the transition information between every two neighboring states during the reverse generative process, and thereby embed the knowledge of trained spectral diffusion model into the fusion problem in the form of a regularization term. At last, we treat each generation step of the final optimization problem as its subproblem, and employ the Adam to solve these subproblems in a reverse sequence. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The code of the proposed approach will be available on https://github.com/liuofficial/SDP.
</details>
<details>
<summary>摘要</summary>
融合基于快照成像（HSI）超分解目的是生成高空间分辨率HSI，通过融合低空间分辨率HSI和高空间分辨率多spectral图像。这种HSI超分解过程可以被视为一个逆 проблеml，其中假设知识是获得所求解的关键。鼓动于扩散模型的成功，我们提出了一种新的 spectral扩散前提（SDP）。 Specifically, we first investigate the spectrum generation problem and design a spectral diffusion model to model the spectral data distribution. Then, in the framework of maximum a posteriori, we keep the transition information between every two neighboring states during the reverse generative process, and thereby embed the knowledge of trained spectral diffusion model into the fusion problem in the form of a regularization term. At last, we treat each generation step of the final optimization problem as its subproblem, and employ the Adam to solve these subproblems in a reverse sequence. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The code of the proposed approach will be available on https://github.com/liuofficial/SDP.
</details></li>
</ul>
<hr>
<h2 id="Automated-Volume-Corrected-Mitotic-Index-Calculation-Through-Annotation-Free-Deep-Learning-using-Immunohistochemistry-as-Reference-Standard"><a href="#Automated-Volume-Corrected-Mitotic-Index-Calculation-Through-Annotation-Free-Deep-Learning-using-Immunohistochemistry-as-Reference-Standard" class="headerlink" title="Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard"></a>Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08949">http://arxiv.org/abs/2311.08949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Ammeling, Moritz Hecker, Jonathan Ganz, Taryn A. Donovan, Christof A. Bertram, Katharina Breininger, Marc Aubreville</li>
<li>for:  This paper is written for assessing the prognostic value of invasive breast carcinomas using a deep learning pipeline.</li>
<li>methods: The paper uses an annotation-free, immunohistochemistry-based approach to estimate epithelial segmentation in canine breast carcinomas.</li>
<li>results: The deep learning-based pipeline shows expert-level performance, with time efficiency and reproducibility, compared to manually annotated M&#x2F;V-Index.<details>
<summary>Abstract</summary>
The volume-corrected mitotic index (M/V-Index) was shown to provide prognostic value in invasive breast carcinomas. However, despite its prognostic significance, it is not established as the standard method for assessing aggressive biological behaviour, due to the high additional workload associated with determining the epithelial proportion. In this work, we show that using a deep learning pipeline solely trained with an annotation-free, immunohistochemistry-based approach, provides accurate estimations of epithelial segmentation in canine breast carcinomas. We compare our automatic framework with the manually annotated M/V-Index in a study with three board-certified pathologists. Our results indicate that the deep learning-based pipeline shows expert-level performance, while providing time efficiency and reproducibility.
</details>
<details>
<summary>摘要</summary>
它们表示了抑制性肿瘤指数（M/V-Index）在侵袭性乳腺癌中提供了预后价值。然而，尽管它具有预后意义，但它并没有被认定为评估攻击性生物行为的标准方法，因为确定细胞性占比带来了高额附加工作负担。在这项工作中，我们表明了使用深度学习框架，只靠基于无注解、免疫抑制技术的方法来提供精准的细胞分割估计。我们将自动框架与三位医学博士评估员 manually annotated M/V-Index进行比较。我们的结果表明，深度学习基于的框架具有专家水平的性能，同时具有时间效率和重复性。
</details></li>
</ul>
<hr>
<h2 id="Confident-Naturalness-Explanation-CNE-A-Framework-to-Explain-and-Assess-Patterns-Forming-Naturalness"><a href="#Confident-Naturalness-Explanation-CNE-A-Framework-to-Explain-and-Assess-Patterns-Forming-Naturalness" class="headerlink" title="Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness"></a>Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08936">http://arxiv.org/abs/2311.08936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Emam, Mohamed Farag, Ribana Roscher</li>
<li>for: 本研究旨在使用机器学习模型对保护区域的自然性进行更好的理解和映射，以便更好地保护这些区域。</li>
<li>methods: 本研究使用可解释的机器学习方法分析卫星图像，并使用不确定度量化来评估模型的可靠性。</li>
<li>results: 研究提出了一种名为“可信度自然性解释”（CNE）框架，该框架结合可解释的机器学习和不确定度量化来评估和解释自然性。研究还提出了一种新的量化度量，用于衡量模型对自然性的可靠性。<details>
<summary>Abstract</summary>
Protected natural areas are regions that have been minimally affected by human activities such as urbanization, agriculture, and other human interventions. To better understand and map the naturalness of these areas, machine learning models can be used to analyze satellite imagery. Specifically, explainable machine learning methods show promise in uncovering patterns that contribute to the concept of naturalness within these protected environments. Additionally, addressing the uncertainty inherent in machine learning models is crucial for a comprehensive understanding of this concept. However, existing approaches have limitations. They either fail to provide explanations that are both valid and objective or struggle to offer a quantitative metric that accurately measures the contribution of specific patterns to naturalness, along with the associated confidence. In this paper, we propose a novel framework called the Confident Naturalness Explanation (CNE) framework. This framework combines explainable machine learning and uncertainty quantification to assess and explain naturalness. We introduce a new quantitative metric that describes the confident contribution of patterns to the concept of naturalness. Furthermore, we generate an uncertainty-aware segmentation mask for each input sample, highlighting areas where the model lacks knowledge. To demonstrate the effectiveness of our framework, we apply it to a study site in Fennoscandia using two open-source satellite datasets.
</details>
<details>
<summary>摘要</summary>
保护自然区域是人类活动的有限影响地区，如城市化、农业和其他人类干预。为了更好地理解和地图这些地区的自然性，机器学习模型可以使用卫星影像进行分析。特别是使用可解释机器学习方法可以揭示保护区域中自然性的特征。然而，现有的方法有限制。它们可能无法提供有效和客观的解释，或者很难提供量化度量自然性的贡献，以及相关的信息。在这篇论文中，我们提出了一种新的框架，即可信度自然性解释（CNE）框架。这个框架结合可解释机器学习和不确定量化来评估和解释自然性。我们还提出了一个新的量化度量，用于描述模型对自然性的有效贡献。此外，我们生成了每个输入样本的不确定性感知分 segmentation掩模，以显示模型在哪些地方缺乏知识。为了证明我们的框架的有效性，我们在芬兰地区使用了两个开源卫星数据集进行应用。
</details></li>
</ul>
<hr>
<h2 id="Structural-Based-Uncertainty-in-Deep-Learning-Across-Anatomical-Scales-Analysis-in-White-Matter-Lesion-Segmentation"><a href="#Structural-Based-Uncertainty-in-Deep-Learning-Across-Anatomical-Scales-Analysis-in-White-Matter-Lesion-Segmentation" class="headerlink" title="Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation"></a>Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08931">http://arxiv.org/abs/2311.08931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medical-image-analysis-laboratory/ms_wml_uncs">https://github.com/medical-image-analysis-laboratory/ms_wml_uncs</a></li>
<li>paper_authors: Nataliia Molchanova, Vatsal Raina, Andrey Malinin, Francesco La Rosa, Adrien Depeursinge, Mark Gales, Cristina Granziera, Henning Muller, Mara Graziani, Meritxell Bach Cuadra<br>for:这个研究旨在评估自动深度学习（DL）工具在多发性脑损（MS）患者的白 matter损害（WML）分 segmentation任务中的可靠性。methods:我们的研究集中在两个主要的不确定性方面：首先，我们提出了一个好的不确定性量应该指示预测可能有误的高不确定性值。其次，我们 investigate了不确定性的不同解剖级别（voxel、lesion、patient）之间的关系。我们假设不确定性在每个级别都与特定类型的错误有关。我们的研究旨在证实这种关系，通过在域内和域外两种设定下进行分离分析。我们的主要方法贡献包括：（i）开发了novel的lesion和patient级别的不确定性量计算方法，基于结构预测差异，以及（ii）将错误保留曲线分析框架扩展到facilitate the evaluation of UQ performance at both lesion and patient scales。results:我们使用了172名患者的多中心MRI数据集，并证实了我们提出的不确定性量更好地捕捉模型错误在lesion和patient级别上。我们提供了UQ协议代码，可以在<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs中找到。</a><details>
<summary>Abstract</summary>
This paper explores uncertainty quantification (UQ) as an indicator of the trustworthiness of automated deep-learning (DL) tools in the context of white matter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of multiple sclerosis (MS) patients. Our study focuses on two principal aspects of uncertainty in structured output segmentation tasks. Firstly, we postulate that a good uncertainty measure should indicate predictions likely to be incorrect with high uncertainty values. Second, we investigate the merit of quantifying uncertainty at different anatomical scales (voxel, lesion, or patient). We hypothesize that uncertainty at each scale is related to specific types of errors. Our study aims to confirm this relationship by conducting separate analyses for in-domain and out-of-domain settings. Our primary methodological contributions are (i) the development of novel measures for quantifying uncertainty at lesion and patient scales, derived from structural prediction discrepancies, and (ii) the extension of an error retention curve analysis framework to facilitate the evaluation of UQ performance at both lesion and patient scales. The results from a multi-centric MRI dataset of 172 patients demonstrate that our proposed measures more effectively capture model errors at the lesion and patient scales compared to measures that average voxel-scale uncertainty values. We provide the UQ protocols code at https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We propose that a good uncertainty measure should indicate predictions that are likely to be incorrect with high uncertainty values.2. We investigate the value of quantifying uncertainty at different anatomical scales (voxel, lesion, or patient) and hypothesize that uncertainty at each scale is related to specific types of errors.Our study aims to confirm this relationship by conducting separate analyses for in-domain and out-of-domain settings. Our main methodological contributions are:1. The development of novel measures for quantifying uncertainty at lesion and patient scales, derived from structural prediction discrepancies.2. The extension of an error retention curve analysis framework to facilitate the evaluation of UQ performance at both lesion and patient scales.The results from a multi-centric MRI dataset of 172 patients demonstrate that our proposed measures more effectively capture model errors at the lesion and patient scales compared to measures that average voxel-scale uncertainty values. The UQ protocols code is available at <a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs">https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs</a>.</details></li>
</ol>
<hr>
<h2 id="Progressive-Feedback-Enhanced-Transformer-for-Image-Forgery-Localization"><a href="#Progressive-Feedback-Enhanced-Transformer-for-Image-Forgery-Localization" class="headerlink" title="Progressive Feedback-Enhanced Transformer for Image Forgery Localization"></a>Progressive Feedback-Enhanced Transformer for Image Forgery Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08910">http://arxiv.org/abs/2311.08910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Zhu, Gang Cao, Xianglin Huang</li>
<li>for: 本研究旨在提高数字图像中的违�检测精度和可靠性，以便应对地方图像修改技术的恶意使用。</li>
<li>methods: 我们提出了一种Progressive FeedbACk-enhanced Transformer（ProFact）网络，以实现粗细到细的图像违�检测。特别是，初始分支网络生成的粗细 lokalisa map 被适应地反馈到早期的 transformer 嵌入层，以增强图像的正面特征表示，同时抑制干扰因素。另外，我们还提出了一种 Contextual Spatial Pyramid Module，用于改进识别性的伪造特征，以提高违�检测的准确性和可靠性。</li>
<li>results: 我们在九个公共的审查数据集上进行了广泛的实验，结果表明，我们的提出的 lokaliser 在泛化能力和图像违�检测的稳定性方面都有显著的优势，超过了现有的状态态技术。<details>
<summary>Abstract</summary>
Blind detection of the forged regions in digital images is an effective authentication means to counter the malicious use of local image editing techniques. Existing encoder-decoder forensic networks overlook the fact that detecting complex and subtle tampered regions typically requires more feedback information. In this paper, we propose a Progressive FeedbACk-enhanced Transformer (ProFact) network to achieve coarse-to-fine image forgery localization. Specifically, the coarse localization map generated by an initial branch network is adaptively fed back to the early transformer encoder layers for enhancing the representation of positive features while suppressing interference factors. The cascaded transformer network, combined with a contextual spatial pyramid module, is designed to refine discriminative forensic features for improving the forgery localization accuracy and reliability. Furthermore, we present an effective strategy to automatically generate large-scale forged image samples close to real-world forensic scenarios, especially in realistic and coherent processing. Leveraging on such samples, a progressive and cost-effective two-stage training protocol is applied to the ProFact network. The extensive experimental results on nine public forensic datasets show that our proposed localizer greatly outperforms the state-of-the-art on the generalization ability and robustness of image forgery localization. Code will be publicly available at https://github.com/multimediaFor/ProFact.
</details>
<details>
<summary>摘要</summary>
《快速检测数字图像中forge的地方是一种有效的身份验证方法，以防止本地图像修改技术的恶意使用。现有的编码器-解码器审查网络忽略了需要更多反馈信息来检测复杂且微妙的forge地方的事实。本文提出了一种 Progressive FeedbACk-enhanced Transformer（ProFact）网络，以实现从粗到细图像伪造地点检测。具体来说，初始分支网络生成的粗略定位图是通过对初期转换器层进行反馈来提高正面特征表示，同时抑制干扰因素。cascade转换网络，配合Contextual Spatial Pyramid模块，用于细化审查特征，提高伪造地点检测精度和可靠性。此外，我们还提出了一种生成大量伪造图像样本的自动化策略，特别是在真实和coherent处理中。基于这些样本，我们采用了一种进步和成本效果的两个阶段训练方案，并在九个公共审查 dataset 上进行了广泛的实验。结果表明，我们提出的定位器在总体能力和对图像伪造地点检测的稳定性方面具有明显的优势。代码将在https://github.com/multimediaFor/ProFact 上公开。
</details></li>
</ul>
<hr>
<h2 id="DLAS-An-Exploration-and-Assessment-of-the-Deep-Learning-Acceleration-Stack"><a href="#DLAS-An-Exploration-and-Assessment-of-the-Deep-Learning-Acceleration-Stack" class="headerlink" title="DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack"></a>DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08909">http://arxiv.org/abs/2311.08909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Perry Gibson, José Cano, Elliot J. Crowley, Amos Storkey, Michael O’Boyle</li>
<li>for: 这个论文主要是为了提高深度学习模型在有限资源的设备上的运行速度。</li>
<li>methods: 这个论文使用了机器学习和系统技术，提出了深度学习加速堆栈（DLAS），并进行了透明度研究以确定不同参数对整个堆栈的影响。</li>
<li>results: 该论文的evaluation表明，不同的DLAS参数的变化可以导致巨大的变化和堆栈之间的互动。最高级别的观察结论是，模型大小、准确率和执行时间之间并不是一定的相关性。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are extremely computationally demanding, which presents a large barrier to their deployment on resource-constrained devices. Since such devices are where many emerging deep learning applications lie (e.g., drones, vision-based medical technology), significant bodies of work from both the machine learning and systems communities have attempted to provide optimizations to accelerate DNNs. To help unify these two perspectives, in this paper we combine machine learning and systems techniques within the Deep Learning Acceleration Stack (DLAS), and demonstrate how these layers can be tightly dependent on each other with an across-stack perturbation study. We evaluate the impact on accuracy and inference time when varying different parameters of DLAS across two datasets, seven popular DNN architectures, four DNN compression techniques, three algorithmic primitives with sparse and dense variants, untuned and auto-scheduled code generation, and four hardware platforms. Our evaluation highlights how perturbations across DLAS parameters can cause significant variation and across-stack interactions. The highest level observation from our evaluation is that the model size, accuracy, and inference time are not guaranteed to be correlated. Overall we make 13 key observations, including that speedups provided by compression techniques are very hardware dependent, and that compiler auto-tuning can significantly alter what the best algorithm to use for a given configuration is. With DLAS, we aim to provide a reference framework to aid machine learning and systems practitioners in reasoning about the context in which their respective DNN acceleration solutions exist in. With our evaluation strongly motivating the need for co-design, we believe that DLAS can be a valuable concept for exploring the next generation of co-designed accelerated deep learning solutions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Brain-MRI-Image-Classification-with-SIBOW-SVM"><a href="#Robust-Brain-MRI-Image-Classification-with-SIBOW-SVM" class="headerlink" title="Robust Brain MRI Image Classification with SIBOW-SVM"></a>Robust Brain MRI Image Classification with SIBOW-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08908">http://arxiv.org/abs/2311.08908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyun Zeng, Hao Helen Zhang</li>
<li>for: 本研究旨在开发一种高效的脑肿瘤图像分类方法，以提高脑肿瘤诊断精度和患者生活质量。</li>
<li>methods: 该方法基于 Bag-of-Features 模型、SIFT特征提取和加权支持向量机（wSVM），能够捕捉脑肿瘤图像中隐藏的特征，并且可以准确地分类不同的肿瘤类型。</li>
<li>results: 对一个公共的脑肿瘤MRI图像集进行了测试，结果显示，新方法可以高效地分类不同的肿瘤类型，并且比之前的方法更高效。<details>
<summary>Abstract</summary>
The majority of primary Central Nervous System (CNS) tumors in the brain are among the most aggressive diseases affecting humans. Early detection of brain tumor types, whether benign or malignant, glial or non-glial, is critical for cancer prevention and treatment, ultimately improving human life expectancy. Magnetic Resonance Imaging (MRI) stands as the most effective technique to detect brain tumors by generating comprehensive brain images through scans. However, human examination can be error-prone and inefficient due to the complexity, size, and location variability of brain tumors. Recently, automated classification techniques using machine learning (ML) methods, such as Convolutional Neural Network (CNN), have demonstrated significantly higher accuracy than manual screening, while maintaining low computational costs. Nonetheless, deep learning-based image classification methods, including CNN, face challenges in estimating class probabilities without proper model calibration. In this paper, we propose a novel brain tumor image classification method, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with SIFT feature extraction and weighted Support Vector Machines (wSVMs). This new approach effectively captures hidden image features, enabling the differentiation of various tumor types and accurate label predictions. Additionally, the SIBOW-SVM is able to estimate the probabilities of images belonging to each class, thereby providing high-confidence classification decisions. We have also developed scalable and parallelable algorithms to facilitate the practical implementation of SIBOW-SVM for massive images. As a benchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI images containing four classes: glioma, meningioma, pituitary, and normal. Our results show that the new method outperforms state-of-the-art methods, including CNN.
</details>
<details>
<summary>摘要</summary>
主要的中枢神经系统（CNS）肿瘤在脑中是人类最严重的一种疾病之一。早期识别脑肿瘤类型，无论是良性或恶性， glial 或非 glial，是致命的，以提高人类生存时间。磁共振成像（MRI）是识别脑肿瘤最有效的技术，可以生成详细的脑图像。然而，人工检查可能会受到脑肿瘤的复杂性、大小和位置的变化带来干扰，导致误差和不效率。最近，使用机器学习（ML）方法，如卷积神经网络（CNN），已经达到了人工检查的高精度。然而，深度学习基于的图像分类方法，包括CNN，面临着估计类别概率的挑战。在本文中，我们提出了一种新的脑肿瘤图像分类方法，称为SIBOW-SVM，它将袋子模型（BoF）和SIFT特征提取结合了支持向量机（wSVM）。这种新的方法能够捕捉隐藏的图像特征，以便 diferenciar 多种肿瘤类型并提供准确的标签预测。此外，SIBOW-SVM还能够估计每个图像所属的类别概率，从而提供高置信的分类决策。我们还开发了可扩展和并行的算法，以便实现SIBOW-SVM的实际应用。作为一个标准，我们将SIBOW-SVM应用于脑肿瘤MRI图像数据集，该数据集包含四个类别： glioma， meningioma， pituitary，和正常。我们的结果表明，新方法在比较之下超越了现有的方法，包括CNN。
</details></li>
</ul>
<hr>
<h2 id="AdapterShadow-Adapting-Segment-Anything-Model-for-Shadow-Detection"><a href="#AdapterShadow-Adapting-Segment-Anything-Model-for-Shadow-Detection" class="headerlink" title="AdapterShadow: Adapting Segment Anything Model for Shadow Detection"></a>AdapterShadow: Adapting Segment Anything Model for Shadow Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08891">http://arxiv.org/abs/2311.08891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiping Jie, Hui Zhang</li>
<li>for: 提高阴影检测精度</li>
<li>methods: 利用可调适器插入into SAM模型，并使用格子采样方法生成密集点提示来自动 segment shadow</li>
<li>results: 在四个广泛使用的 benchmark 数据集上进行了广泛的实验，并达到了提高阴影检测精度的目的<details>
<summary>Abstract</summary>
Segment anything model (SAM) has shown its spectacular performance in segmenting universal objects, especially when elaborate prompts are provided. However, the drawback of SAM is twofold. On the first hand, it fails to segment specific targets, e.g., shadow images or lesions in medical images. On the other hand, manually specifying prompts is extremely time-consuming. To overcome the problems, we propose AdapterShadow, which adapts SAM model for shadow detection. To adapt SAM for shadow images, trainable adapters are inserted into the frozen image encoder of SAM, since the training of the full SAM model is both time and memory consuming. Moreover, we introduce a novel grid sampling method to generate dense point prompts, which helps to automatically segment shadows without any manual interventions. Extensive experiments are conducted on four widely used benchmark datasets to demonstrate the superior performance of our proposed method. Codes will are publicly available at https://github.com/LeipingJie/AdapterShadow.
</details>
<details>
<summary>摘要</summary>
Segment anything model (SAM) 已经显示出了杰出的表现在universal对象上，尤其是当提供了复杂的提示时。然而，SAM的缺点是双重的。首先，它无法正确分割特定目标，例如阴影图像或医学图像中的病变。其次，手动提供提示是非常时间和 памяти消耗的。为了解决这些问题，我们提议了AdapterShadow，它将SAM模型适应到阴影检测中。为了适应SAM模型到阴影图像，我们在SAM模型的冻结图像Encoder中插入可教育的适应器。此外，我们还引入了一种新的格子采样方法，用于生成密集点提示，以帮助自动分割阴影而无需任何人工干预。我们在四个广泛使用的benchmark数据集上进行了extensive的实验，以证明我们的提议方法的优秀性。代码将在https://github.com/LeipingJie/AdapterShadow上公开。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Federated-Learning-with-Classifier-Guided-Diffusion-Models"><a href="#One-Shot-Federated-Learning-with-Classifier-Guided-Diffusion-Models" class="headerlink" title="One-Shot Federated Learning with Classifier-Guided Diffusion Models"></a>One-Shot Federated Learning with Classifier-Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08870">http://arxiv.org/abs/2311.08870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue</li>
<li>for: 本研究旨在提出一种基于协同学习的一批联合学习方法，以便在实际应用中避免额外的数据采集和传输成本。</li>
<li>methods: 我们提出了一种基于扩散模型的一批联合学习方法，使用客户端分类器的指导来生成符合客户端分布的数据，并在服务器端进行模型聚合。我们在两个方面进行了targeted优化：首先，我们通过条件编辑随机生成的初始噪声，使其嵌入特定的语义和分布，从而提高生成的质量和稳定性。其次，我们使用客户端分类器的BN统计来为生成过程提供详细的指导。</li>
<li>results: 我们的方法可以有效地处理不同客户端模型和非相关的特征或标签问题，并且不需要训练任何生成器或传输任何附加信息到客户端，从而避免额外的隐私泄露风险。通过利用扩散模型已经储存的广泛知识，我们可以生成符合客户端分布的 sintetic数据，以帮助我们超越客户端样本的知识限制，实现聚合模型的性能超越中央化训练的情况，这些结果得到了 suficient的量化和视觉化实验的证明。<details>
<summary>Abstract</summary>
One-shot federated learning (OSFL) has gained attention in recent years due to its low communication cost. However, most of the existing methods require auxiliary datasets or training generators, which hinders their practicality in real-world scenarios. In this paper, we explore the novel opportunities that diffusion models bring to OSFL and propose FedCADO, utilizing guidance from client classifiers to generate data that complies with clients' distributions and subsequently training the aggregated model on the server. Specifically, our method involves targeted optimizations in two aspects. On one hand, we conditionally edit the randomly sampled initial noises, embedding them with specified semantics and distributions, resulting in a significant improvement in both the quality and stability of generation. On the other hand, we employ the BN statistics from the classifiers to provide detailed guidance during generation. These tailored optimizations enable us to limitlessly generate datasets, which closely resemble the distribution and quality of the original client dataset. Our method effectively handles the heterogeneous client models and the problems of non-IID features or labels. In terms of privacy protection, our method avoids training any generator or transferring any auxiliary information on clients, eliminating any additional privacy leakage risks. Leveraging the extensive knowledge stored in the pre-trained diffusion model, the synthetic datasets can assist us in surpassing the knowledge limitations of the client samples, resulting in aggregation models that even outperform the performance ceiling of centralized training in some cases, which is convincingly demonstrated in the sufficient quantification and visualization experiments conducted on three large-scale multi-domain image datasets.
</details>
<details>
<summary>摘要</summary>
一种新型的一击联合学习（OSFL）方法，它在最近几年内得到了广泛关注，因为它的通信成本较低。然而，大多数现有的方法需要附加的数据集或训练生成器，这限制了它们在实际场景中的实用性。在这篇论文中，我们探讨了Diffusion模型带来的新机遇，并提出了FedCADO方法，通过客户端分类器的指导，在服务器上训练汇集模型。具体来说，我们的方法包括两个方面的优化。一方面，我们根据客户端的分布和特性进行 Conditional Editing，使得随机生成的初始噪音中嵌入了指定的 semantics和分布，从而大幅提高生成质量和稳定性。另一方面，我们利用客户端的BN统计来提供详细的指导，以便在生成过程中进行精细的调整。这些定制的优化使得我们可以无限制地生成数据集，它们与原始客户端数据的分布和质量具有高度的相似性。我们的方法能够有效地处理不同客户端模型以及非标一致的特征或标签问题。另外，我们的方法不需要训练生成器或传输附加的auxiliary信息，因此不会增加隐私泄露风险。利用批量训练的普遍知识，我们的Synthetic数据可以帮助我们突破客户端样本的知识限制，从而实现汇集模型的性能超越中心化训练的表现峰值，这在三个大规模多domain图像数据集的充分量和视觉化实验中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Toulouse-Hyperspectral-Data-Set-a-benchmark-data-set-to-assess-semi-supervised-spectral-representation-learning-and-pixel-wise-classification-techniques"><a href="#Toulouse-Hyperspectral-Data-Set-a-benchmark-data-set-to-assess-semi-supervised-spectral-representation-learning-and-pixel-wise-classification-techniques" class="headerlink" title="Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques"></a>Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08863">http://arxiv.org/abs/2311.08863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romain3ch216/tlse-experiments">https://github.com/romain3ch216/tlse-experiments</a></li>
<li>paper_authors: Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet</li>
<li>for: 这个论文用于描述一个新的 Toulouse 遥感谱数据集，以便用于 spectral representation learning 和分类大规模遥感谱图像中的少量标注像素。</li>
<li>methods: 这个论文使用了 semi-supervised 和 self-supervised 技术来解决遥感谱数据集中标注的稀缺性问题，并实验了 Masked Autoencoders 自我隐藏任务以及一种基于普通自动编码器和Random Forest分类器的像素级分类方法。</li>
<li>results: 这个论文的实验结果显示，使用 Toulouse 遥感谱数据集和 Masked Autoencoders 自我隐藏任务可以达到 82% 的总准确率和 74% F1 分数。<details>
<summary>Abstract</summary>
Airborne hyperspectral images can be used to map the land cover in large urban areas, thanks to their very high spatial and spectral resolutions on a wide spectral domain. While the spectral dimension of hyperspectral images is highly informative of the chemical composition of the land surface, the use of state-of-the-art machine learning algorithms to map the land cover has been dramatically limited by the availability of training data. To cope with the scarcity of annotations, semi-supervised and self-supervised techniques have lately raised a lot of interest in the community. Yet, the publicly available hyperspectral data sets commonly used to benchmark machine learning models are not totally suited to evaluate their generalization performances due to one or several of the following properties: a limited geographical coverage (which does not reflect the spectral diversity in metropolitan areas), a small number of land cover classes and a lack of appropriate standard train / test splits for semi-supervised and self-supervised learning. Therefore, we release in this paper the Toulouse Hyperspectral Data Set that stands out from other data sets in the above-mentioned respects in order to meet key issues in spectral representation learning and classification over large-scale hyperspectral images with very few labeled pixels. Besides, we discuss and experiment the self-supervised task of Masked Autoencoders and establish a baseline for pixel-wise classification based on a conventional autoencoder combined with a Random Forest classifier achieving 82% overall accuracy and 74% F1 score. The Toulouse Hyperspectral Data Set and our code are publicly available at https://www.toulouse-hyperspectral-data-set.com and https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</details>
<details>
<summary>摘要</summary>
“空中卫星偏振图像可以用于覆盖大都市区域的土地覆盖，因为它们具有非常高的空间和спектраль分辨率，覆盖广泛的 спектраль频谱。而偏振图像的spectral维度对地表化学成分非常有助于，但是使用当今最先进的机器学习算法来地图土地覆盖却受到数据库的有限性的限制。为了应对数据缺乏， semi-supervised和self-supervised技术在社区中引起了很多关注。然而，公共可用的偏振数据集通常用于机器学习模型的benchmarking是不完全适合评估其泛化性能，因为它们具有以下一些属性：地理覆盖率偏低（不反映大都市区域的 спектраль多样性），土地覆盖类型很少（不能反映实际情况）和缺乏适合的标准训练/测试分割。因此，我们在这篇论文中发布了 Toulouse 偏振数据集，它与其他数据集不同，以解决上述问题，以便在大规模偏振图像上进行 spectral representation 学习和分类。此外，我们还讨论了自动化任务的Masked Autoencoders，并在基于普通自动编码器和随机森林分类器的基础上建立了82%的总准确率和74%的F1分数的基eline。 Toulouse 偏振数据集和我们的代码在 https://www.toulouse-hyperspectral-data-set.com 和 https://www.github.com/Romain3Ch216/tlse-experiments 上公开提供。”
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentations-in-Deep-Weight-Spaces"><a href="#Data-Augmentations-in-Deep-Weight-Spaces" class="headerlink" title="Data Augmentations in Deep Weight Spaces"></a>Data Augmentations in Deep Weight Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08851">http://arxiv.org/abs/2311.08851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviv Shamsian, David W. Zhang, Aviv Navon, Yan Zhang, Miltiadis Kofinas, Idan Achituve, Riccardo Valperga, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, Ethan Fetaya, Gal Chechik, Haggai Maron</li>
<li>for: This paper focuses on addressing the challenge of overfitting in learning in weight spaces, a promising research direction with applications in various fields.</li>
<li>methods: The paper investigates data augmentations for weight spaces, a set of techniques that enable generating new data examples on the fly without having to train additional input weight space elements.</li>
<li>results: The paper introduces a novel augmentation scheme based on the Mixup method and evaluates the performance of several recently proposed data augmentation schemes on existing and new benchmarks.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文关注了在权重空间学习中的过拟合问题，这是一个有前途的研究方向，应用于各种领域。</li>
<li>methods: 论文研究了权重空间数据扩充技术，可以在 fly 上生成新的数据示例，无需再训练输入权重空间元素。</li>
<li>results: 论文提出了一种基于 Mixup 方法的新数据扩充方案，并评估了一些最近提出的数据扩充方案的性能在现有和新的标准 bench 上。<details>
<summary>Abstract</summary>
Learning in weight spaces, where neural networks process the weights of other deep neural networks, has emerged as a promising research direction with applications in various fields, from analyzing and editing neural fields and implicit neural representations, to network pruning and quantization. Recent works designed architectures for effective learning in that space, which takes into account its unique, permutation-equivariant, structure. Unfortunately, so far these architectures suffer from severe overfitting and were shown to benefit from large datasets. This poses a significant challenge because generating data for this learning setup is laborious and time-consuming since each data sample is a full set of network weights that has to be trained. In this paper, we address this difficulty by investigating data augmentations for weight spaces, a set of techniques that enable generating new data examples on the fly without having to train additional input weight space elements. We first review several recently proposed data augmentation schemes %that were proposed recently and divide them into categories. We then introduce a novel augmentation scheme based on the Mixup method. We evaluate the performance of these techniques on existing benchmarks as well as new benchmarks we generate, which can be valuable for future studies.
</details>
<details>
<summary>摘要</summary>
学习在权重空间中， где神经网络处理其他深度神经网络的权重，已经成为一个有前途的研究方向，它在各种领域都有应用，从分析和编辑神经场和隐藏神经表示之间，到网络剪枝和量化。现有的设计架构具有考虑其特殊、协变结构的能力，但是目前这些架构受到严重的馈敏化问题困扰。这种问题的主要原因是生成这种学习环境的数据是劳动ious和时间consuming，因为每个数据样本都需要训练全部网络权重。在这篇论文中，我们解决这个困难，通过调查数据增强技术，可以在飞行时生成新的数据示例，无需进行额外的输入权重空间元素训练。我们首先回顾最近提出的数据增强方案，并将它们分为类别。然后，我们介绍了一种基于 Mixup 方法的新的增强方案。我们对这些技术在现有的标准准测试集上进行评估，以及我们自己生成的新测试集，以便对未来的研究提供价值。
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Output-of-a-Generative-Model-by-Latent-Feature-Vector-Shifting"><a href="#Controlling-the-Output-of-a-Generative-Model-by-Latent-Feature-Vector-Shifting" class="headerlink" title="Controlling the Output of a Generative Model by Latent Feature Vector Shifting"></a>Controlling the Output of a Generative Model by Latent Feature Vector Shifting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08850">http://arxiv.org/abs/2311.08850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Róbert Belanec, Peter Lacko, Kristína Malinovská</li>
<li>for: 这个论文的目的是提出一种控制输出图像修改的方法，使用StyleGAN3生成器生成的图像中的Semantic特征进行修改。</li>
<li>methods: 该方法使用了一个预训练的StyleGAN3生成器，并补充了一个基于Convolutional Neural Network（CNN）的分类器，用于将生成的图像分类为CelebA数据集中的二分类特征。另外，该方法还使用了一个叫做”latent feature shifter”的神经网络模型，用于将生成器的含义向量Shift到指定的特征方向。</li>
<li>results: 经过训练， latent feature shifter方法可以成功地控制StyleGAN3生成器中的图像修改，并且比基eline方法更多地生成了满足要求的图像。<details>
<summary>Abstract</summary>
State-of-the-art generative models (e.g. StyleGAN3 \cite{karras2021alias}) often generate photorealistic images based on vectors sampled from their latent space. However, the ability to control the output is limited. Here we present our novel method for latent vector shifting for controlled output image modification utilizing semantic features of the generated images. In our approach we use a pre-trained model of StyleGAN3 that generates images of realistic human faces in relatively high resolution. We complement the generative model with a convolutional neural network classifier, namely ResNet34, trained to classify the generated images with binary facial features from the CelebA dataset. Our latent feature shifter is a neural network model with a task to shift the latent vectors of a generative model into a specified feature direction. We have trained latent feature shifter for multiple facial features, and outperformed our baseline method in the number of generated images with the desired feature. To train our latent feature shifter neural network, we have designed a dataset of pairs of latent vectors with and without a certain feature. Based on the evaluation, we conclude that our latent feature shifter approach was successful in the controlled generation of the StyleGAN3 generator.
</details>
<details>
<summary>摘要</summary>
现代生成模型（如StyleGAN3 \cite{karras2021alias）often生成高品质图像，基于生成器的 latent space 中采样的 вектор。然而，控制输出的能力有限。我们现在介绍一种新的方法，即 latent vector shifting，用于控制生成器输出图像的修改。我们使用 StyleGAN3 生成器，该生成器可生成高分辨率的人脸图像。我们补充了生成器的 pre-trained 模型，并使用 ResNet34  convolutional neural network 分类器，用于从 CelebA 数据集中分类生成器的生成图像的binary facial features。我们的 latent feature shifter 是一种 neural network 模型，其任务是将生成器的 latent vector Shift into 指定的特征方向。我们在多个 facial features 上训练了 latent feature shifter 模型，并超越了我们的基线方法。为了训练我们的 latent feature shifter 模型，我们设计了一个包含 latent vector 与和无特征的对应对的数据集。根据评估结果，我们得出结论，我们的 latent feature shifter 方法成功地控制了 StyleGAN3 生成器的输出。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Video-Relighting-Using-Casual-Light-Stage"><a href="#Personalized-Video-Relighting-Using-Casual-Light-Stage" class="headerlink" title="Personalized Video Relighting Using Casual Light Stage"></a>Personalized Video Relighting Using Casual Light Stage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08843">http://arxiv.org/abs/2311.08843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Myeong Choi, Max Christman, Roni Sengupta</li>
<li>for: 这个论文是为了开发一种个性化视频重新照明算法，能够在任何姿势、表情和照明条件下生成高质量、时间兼容的重新照明视频。</li>
<li>methods: 该算法使用了一种新的神经网络重新照明架构，可以有效地分离出照明源的光照、物体的geometry和反射特征，并将其与目标照明结合以生成重新照明图像。</li>
<li>results: 根据质量和时间稳定性的评估，该算法在使用LSYD和OLAT数据集上提高了肖像图像重新照明质量和时间稳定性，比之前的方法有显著提升。<details>
<summary>Abstract</summary>
In this paper, we develop a personalized video relighting algorithm that produces high-quality and temporally consistent relit video under any pose, expression, and lighting conditions in real-time. Existing relighting algorithms typically rely either on publicly available synthetic data, which yields poor relighting results, or instead on Light Stage data which is inaccessible and is not publicly available. We show that by casually capturing video of a user watching YouTube videos on a monitor we can train a personalized algorithm capable of producing high-quality relighting under any condition. Our key contribution is a novel neural relighting architecture that effectively separates the intrinsic appearance features, geometry and reflectance, from the source lighting and then combines it with the target lighting to generate a relit image. This neural architecture enables smoothing of intrinsic appearance features leading to temporally stable video relighting. Both qualitative and quantitative evaluations show that our relighting architecture improves portrait image relighting quality and temporal consistency over state-of-the-art approaches on both casually captured Light Stage at Your Desk (LSYD) data and Light Stage captured One Light At a Time (OLAT) datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一种个性化视频重新照明算法，可以在实时下生成高质量、时间协调的重新照明视频，无论人姿、表情或照明条件如何。现有的重新照明算法通常基于公共可用的 sintetic 数据，导致重新照明结果较差，或者使用 Light Stage 数据，但该数据不公开可用。我们展示了通过通过抓取用户在 monitor 上观看 YouTube 视频的视频来训练个性化算法，可以在任何条件下生成高质量的重新照明。我们的关键贡献是一种新的神经网络重新照明架构，可以有效分离源照明中的内在特征、几何和反射特征，并将其与目标照明结合以生成一个重新照明的图像。这种神经网络架构使得内在特征的平滑化，导致视频重新照明的时间稳定。我们的重新照明算法在 Light Stage at Your Desk 数据集和 Light Stage 捕捉 One Light At a Time 数据集上的质量和时间稳定性都超过了状态之前的方法。
</details></li>
</ul>
<hr>
<h2 id="Correlation-guided-Query-Dependency-Calibration-in-Video-Representation-Learning-for-Temporal-Grounding"><a href="#Correlation-guided-Query-Dependency-Calibration-in-Video-Representation-Learning-for-Temporal-Grounding" class="headerlink" title="Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding"></a>Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08835">http://arxiv.org/abs/2311.08835</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wjun0830/cgdetr">https://github.com/wjun0830/cgdetr</a></li>
<li>paper_authors: WonJun Moon, Sangeek Hyun, SuBeen Lee, Jae-Pil Heo</li>
<li>for: 本研究的目的是提供关键clip-wise的交叉模态交互，以便更好地利用视频和文本查询之间的关系。</li>
<li>methods: 我们使用了一种适应性权重层和假token来适应文本查询，并通过学习高级概念的共同embedding来推断clip-word关系。</li>
<li>results: 我们的CG-DETR模型在多个benchmark上达到了当前最佳的Result，并且在高级概念 Retrieval和突出部分检测中都表现出色。<details>
<summary>Abstract</summary>
Recent endeavors in video temporal grounding enforce strong cross-modal interactions through attention mechanisms to overcome the modality gap between video and text query. However, previous works treat all video clips equally regardless of their semantic relevance with the text query in attention modules. In this paper, our goal is to provide clues for query-associated video clips within the crossmodal encoding process. With our Correlation-Guided Detection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of cross-modal interactions and how to exploit such degrees for prediction. First, we design an adaptive cross-attention layer with dummy tokens. Dummy tokens conditioned by text query take a portion of the attention weights, preventing irrelevant video clips from being represented by the text query. Yet, not all word tokens equally inherit the text query's correlation to video clips. Thus, we further guide the cross-attention map by inferring the fine-grained correlation between video clips and words. We enable this by learning a joint embedding space for high-level concepts, i.e., moment and sentence level, and inferring the clip-word correlation. Lastly, we use a moment-adaptive saliency detector to exploit each video clip's degrees of text engagement. We validate the superiority of CG-DETR with the state-of-the-art results on various benchmarks for both moment retrieval and highlight detection. Codes are available at https://github.com/wjun0830/CGDETR.
</details>
<details>
<summary>摘要</summary>
近期的视频时间固定工作通过注意力机制来强化视频和文本查询之间的跨模态交互，以抗跨模态差距。然而，前一些工作往往对所有视频片段进行同样的注意力处理，不充分考虑视频片段与文本查询之间的 semantic relevance。在这篇论文中，我们的目标是为文本查询关联的视频片段在多模态编码过程中提供线索。我们提出了一种适应性跨注意力层，使用幂token来约束视频片段与文本查询之间的交互。然而，不同的单词token不一样继承文本查询对视频片段的相关性。因此，我们进一步指导跨注意力地图，通过学习高级概念的共同空间，即时刻和句子水平，推断视频片段和单词之间的相关性。最后，我们使用时刻适应性的焦点探测器来利用每个视频片段的文本参与度。我们 Validate 我们的 CG-DETR 模型的优越性，并在多个 benchmark 上达到了状态之巅的结果，包括时刻检索和突出部分检测。代码可以在 https://github.com/wjun0830/CGDETR 上找到。
</details></li>
</ul>
<hr>
<h2 id="Target-oriented-Domain-Adaptation-for-Infrared-Image-Super-Resolution"><a href="#Target-oriented-Domain-Adaptation-for-Infrared-Image-Super-Resolution" class="headerlink" title="Target-oriented Domain Adaptation for Infrared Image Super-Resolution"></a>Target-oriented Domain Adaptation for Infrared Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08816">http://arxiv.org/abs/2311.08816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongsongh/dasrgan">https://github.com/yongsongh/dasrgan</a></li>
<li>paper_authors: Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Yafei Dong, Shinichiro Omachi<br>for:这篇论文的目的是提高红外超分解的精度，并使用可见光图像来增强红外图像的текстура细节。methods:该方法使用了两个关键组件：1）Texture-Oriented Adaptation（TOA），用于精细地修饰tekstura细节，和2）Noise-Oriented Adaptation（NOA），用于减少噪声传输。特别是，TOA使用了一个专门的检定器，包括一个优先EXTRACTING分支，并使用 Sobel指导的对抗损失来有效地对 tekstura分布进行对齐。同时，NOA使用了一种噪声对抗损失来在对抗训练中分别分配生成和Gaussian噪声模式分布。results:我们的广泛实验表明，DASRGAN在多个标准 benchmarcks 和不同的upsampling factor上表现出色，并在比较分析中超越了现有的方法。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/yongsongH/DASRGAN%7D">https://github.com/yongsongH/DASRGAN}</a> 上获取。<details>
<summary>Abstract</summary>
Recent efforts have explored leveraging visible light images to enrich texture details in infrared (IR) super-resolution. However, this direct adaptation approach often becomes a double-edged sword, as it improves texture at the cost of introducing noise and blurring artifacts. To address these challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN), an innovative framework specifically engineered for robust IR super-resolution model adaptation. DASRGAN operates on the synergy of two key components: 1) Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and 2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer. Specifically, TOA uniquely integrates a specialized discriminator, incorporating a prior extraction branch, and employs a Sobel-guided adversarial loss to align texture distributions effectively. Concurrently, NOA utilizes a noise adversarial loss to distinctly separate the generative and Gaussian noise pattern distributions during adversarial training. Our extensive experiments confirm DASRGAN's superiority. Comparative analyses against leading methods across multiple benchmarks and upsampling factors reveal that DASRGAN sets new state-of-the-art performance standards. Code are available at \url{https://github.com/yongsongH/DASRGAN}.
</details>
<details>
<summary>摘要</summary>
最近的努力已经探索了使用可见光图像来增强探测器的细节Texture details in infrared (IR) super-resolution. 然而，这种直接适应approach  часто变成了一把两面刃，因为它会提高细节，但同时也会引入噪声和抖音 artifacts。为了解决这些挑战，我们提出了Target-oriented Domain Adaptation SRGAN（DASRGAN），一种创新的框架，特地设计用于Robust IR super-resolution model adaptation。DASRGAN 的核心思想是通过两个关键组成部分来实现：1）Texture-Oriented Adaptation（TOA），用于精细地修正细节 detail; 2）Noise-Oriented Adaptation（NOA），用于减少噪声的传输。具体来说，TOA 使用了一个特殊的探测器，包括一个特殊的提取分支，并使用 Sobel 导向的对抗损失来有效地对细节 distribuition 进行对应。同时，NOA 使用了噪声对抗损失来在对抗训练中分别地分离生成的 Pattern distribution 和 Gaussian 噪声 Pattern distribution。我们的广泛的实验证明了 DASRGAN 的优越性。与其他方法进行比较分析，我们在多个 benchmark 和 upsampling factor 上达到了新的州优性标准。代码可以在 \url{https://github.com/yongsongH/DASRGAN} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Correlation-aware-active-learning-for-surgery-video-segmentation"><a href="#Correlation-aware-active-learning-for-surgery-video-segmentation" class="headerlink" title="Correlation-aware active learning for surgery video segmentation"></a>Correlation-aware active learning for surgery video segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08811">http://arxiv.org/abs/2311.08811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Wu, Pablo Marquez-Neila, Mingyi Zheng, Hedyeh Rafii-Tari, Raphael Sznitman</li>
<li>for: 本研究旨在提高针对手术视频分割的semantic segmentation模型性能，采用活动学习（AL）策略，并考虑视频序列中模型uncertainty和时间特征。</li>
<li>methods: 本研究提出了一种新的AL策略，称为COrrelation-aWare Active Learning（\COALSamp{}），它通过在彩色学习中精心调整的特征空间中Project images，然后从视频帧本地集中随机选择一定数量的代表图像进行标注。</li>
<li>results: 在两个手术工具视频数据集和三个真实世界视频数据集上，本研究实现了高效的semantic segmentation，并且在不同的视频序列中保持了高度的一致性。<details>
<summary>Abstract</summary>
Semantic segmentation is a complex task that relies heavily on large amounts of annotated image data. However, annotating such data can be time-consuming and resource-intensive, especially in the medical domain. Active Learning (AL) is a popular approach that can help to reduce this burden by iteratively selecting images for annotation to improve the model performance. In the case of video data, it is important to consider the model uncertainty and the temporal nature of the sequences when selecting images for annotation. This work proposes a novel AL strategy for surgery video segmentation, \COALSamp{}, COrrelation-aWare Active Learning. Our approach involves projecting images into a latent space that has been fine-tuned using contrastive learning and then selecting a fixed number of representative images from local clusters of video frames. We demonstrate the effectiveness of this approach on two video datasets of surgical instruments and three real-world video datasets. The datasets and code will be made publicly available upon receiving necessary approvals.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>semantic segmentation是一项复杂的任务，它依赖于大量的标注图像数据。然而，在医疗领域中标注这些数据可以是时间消耗和资源占用的。活动学习（AL）是一种受欢迎的方法，它可以在每一轮选择图像进行标注，以提高模型性能。在视频数据中，需要考虑模型不确定和时间序列的特性，并在本地帧cluster中选择一定数量的表示性图像。我们提出了一种新的AL策略，称为\COALSamp{}，它是基于对比学习细化的latent space进行 проекции，并从本地帧cluster中选择一定数量的表示性图像。我们在两个手术工具视频数据集和三个实际世界视频数据集上进行了实验，并将数据和代码公开发布。
</details></li>
</ul>
<hr>
<h2 id="EyeLS-Shadow-Guided-Instrument-Landing-System-for-Intraocular-Target-Approaching-in-Robotic-Eye-Surgery"><a href="#EyeLS-Shadow-Guided-Instrument-Landing-System-for-Intraocular-Target-Approaching-in-Robotic-Eye-Surgery" class="headerlink" title="EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery"></a>EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08799">http://arxiv.org/abs/2311.08799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Yang, Zhihao Zhao, Siyuan Shen, Daniel Zapp, Mathias Maier, Kai Huang, Nassir Navab, M. Ali Nasseri</li>
<li>for:  robotic ophthalmic surgery, high-precision interventions, retina penetration, subretinal injection, removal of floating tissues, retinal detachment</li>
<li>methods:  image-based methods, shadow positions, relative depth position, instrument tip insertion trajectory, surgical simulator</li>
<li>results:  average depth error, 0.0127 mm for floating targets, 0.3473 mm for retinal targets, no retinal damage<details>
<summary>Abstract</summary>
Robotic ophthalmic surgery is an emerging technology to facilitate high-precision interventions such as retina penetration in subretinal injection and removal of floating tissues in retinal detachment depending on the input imaging modalities such as microscopy and intraoperative OCT (iOCT). Although iOCT is explored to locate the needle tip within its range-limited ROI, it is still difficult to coordinate iOCT's motion with the needle, especially at the initial target-approaching stage. Meanwhile, due to 2D perspective projection and thus the loss of depth information, current image-based methods cannot effectively estimate the needle tip's trajectory towards both retinal and floating targets. To address this limitation, we propose to use the shadow positions of the target and the instrument tip to estimate their relative depth position and accordingly optimize the instrument tip's insertion trajectory until the tip approaches targets within iOCT's scanning area. Our method succeeds target approaching on a retina model, and achieves an average depth error of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively in the surgical simulator without damaging the retina.
</details>
<details>
<summary>摘要</summary>
robotic ophthalmic surgery 是一种emerging technology，用于实现高精度干预，如retina penetration 和 floating tissues 的 eliminating ，具体取决于输入 imaging modalities 如 microscopy 和 intraoperative OCT (iOCT)。 although iOCT 可以used to locate the needle tip within its range-limited ROI，但是却难以协调 iOCT 的运动和针，特别是在初始目标接近阶段。 Meanwhile，due to 2D perspective projection and thus the loss of depth information，current image-based methods cannot effectively estimate the needle tip's trajectory towards both retinal and floating targets。 To address this limitation，我们提议使用target 和 instrumente tip的阴影位置来估算它们的相对深度位置，并根据此估算instrumente tip的插入轨迹，直到针到达目标在 iOCT 的扫描范围内。 our method succeeds in approaching targets on a retina model, and achieves an average depth error of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively in the surgical simulator without damaging the retina.
</details></li>
</ul>
<hr>
<h2 id="HFORD-High-Fidelity-and-Occlusion-Robust-De-identification-for-Face-Privacy-Protection"><a href="#HFORD-High-Fidelity-and-Occlusion-Robust-De-identification-for-Face-Privacy-Protection" class="headerlink" title="HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection"></a>HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08786">http://arxiv.org/abs/2311.08786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongxin Chen, Mingrui Zhu, Nannan Wang, Xinbo Gao</li>
<li>for: 隐藏人脸信息，解决智能设备和计算机视觉技术的人脸隐私问题。</li>
<li>methods: 提出了一种高品质和遮挡 robust de-identification（HFORD）方法，通过分离个体特征和特征之间的关系，保留人脸特征和背景信息，并在遮挡场景下提供高品质的隐私保护。</li>
<li>results: 对比其他面部隐私方法，HFORD方法具有更高的质量、更好的细节准确性和更强的遮挡鲁棒性。<details>
<summary>Abstract</summary>
With the popularity of smart devices and the development of computer vision technology, concerns about face privacy protection are growing. The face de-identification technique is a practical way to solve the identity protection problem. The existing facial de-identification methods have revealed several problems, including the impact on the realism of anonymized results when faced with occlusions and the inability to maintain identity-irrelevant details in anonymized results. We present a High-Fidelity and Occlusion-Robust De-identification (HFORD) method to deal with these issues. This approach can disentangle identities and attributes while preserving image-specific details such as background, facial features (e.g., wrinkles), and lighting, even in occluded scenes. To disentangle the latent codes in the GAN inversion space, we introduce an Identity Disentanglement Module (IDM). This module selects the latent codes that are closely related to the identity. It further separates the latent codes into identity-related codes and attribute-related codes, enabling the network to preserve attributes while only modifying the identity. To ensure the preservation of image details and enhance the network's robustness to occlusions, we propose an Attribute Retention Module (ARM). This module adaptively preserves identity-irrelevant details and facial occlusions and blends them into the generated results in a modulated manner. Extensive experiments show that our method has higher quality, better detail fidelity, and stronger occlusion robustness than other face de-identification methods.
</details>
<details>
<summary>摘要</summary>
WITH 智能设备的普及和计算机视觉技术的发展，人脸隐私保护的问题日益突出。面部隐私化技术是一种实际的解决方案。现有的面部隐私化方法存在一些问题，包括在 occlusion 时的真实性降低和保留不相关的特征信息。我们提出了高品质和防护环境鲁棒的面部隐私化方法（HFORD）。这种方法可以分离特征和属性，同时保留图像特有的背景、表情特征（如皱纹）和照明信息，即使在 occluded 场景下。为了分离 GAN 倒推空间中的秘密码，我们引入了个性隐私分解模块（IDM）。这个模块选择与identify closely related的秘密码，然后将其分解成属于特征和属性的秘密码，使网络只修改特征，保留属性。为确保图像细节的保留和网络的防护环境鲁棒性，我们提出了特征保持模块（ARM）。这个模块可以自适应保留不相关的特征和人脸干扰，并将其混合到生成结果中，以提高网络的鲁棒性和质量。我们的方法在多种场景下展现出较高的质量、更好的细节准确性和更强的防护环境鲁棒性，与其他面部隐私化方法相比。
</details></li>
</ul>
<hr>
<h2 id="Language-Semantic-Graph-Guided-Data-Efficient-Learning"><a href="#Language-Semantic-Graph-Guided-Data-Efficient-Learning" class="headerlink" title="Language Semantic Graph Guided Data-Efficient Learning"></a>Language Semantic Graph Guided Data-Efficient Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08782">http://arxiv.org/abs/2311.08782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Ma, Shuang Li, Lincan Cai, Jingxuan Kang</li>
<li>for: 提高机器学习模型的数据效率，使其能够从有限数据中学习并减少人工监督的依赖度。</li>
<li>methods: 利用语言semantic图（LSG），通过自然语言描述中的标签信息提取高级别semantic关系，并将其用于导航主模型的训练，以更好地利用标签知识。</li>
<li>results: 在图像、视频和音频模式下，通过LSG方法在传输学习和半supervised学习场景中显著提高性能，并且对训练过程也有加速效果。<details>
<summary>Abstract</summary>
Developing generalizable models that can effectively learn from limited data and with minimal reliance on human supervision is a significant objective within the machine learning community, particularly in the era of deep neural networks. Therefore, to achieve data-efficient learning, researchers typically explore approaches that can leverage more related or unlabeled data without necessitating additional manual labeling efforts, such as Semi-Supervised Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL leverages unlabeled data in the training process, while TL enables the transfer of expertise from related data distributions. DA broadens the dataset by synthesizing new data from existing examples. However, the significance of additional knowledge contained within labels has been largely overlooked in research. In this paper, we propose a novel perspective on data efficiency that involves exploiting the semantic information contained in the labels of the available data. Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过使用已有数据中的标签信息，我们提出了一种新的数据效率角度，即利用标签中的语义信息。我们首先构建了一个语义图（LSG），该图基于标签中的自然语言描述。然后，我们训练了一个辅助图神经网络，以EXTRACT高级语义关系。这些语义关系 затем用于引导主模型的训练，以更好地利用标签知识。在图像、视频和音频模式下，我们在TL和SSL场景中应用了LSG方法，并证明其在其他数据效率学习方法中表现出色。此外，我们的深入分析表明，LSG方法也可以加速训练过程。Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Two-stage-Joint-Transductive-and-Inductive-learning-for-Nuclei-Segmentation"><a href="#Two-stage-Joint-Transductive-and-Inductive-learning-for-Nuclei-Segmentation" class="headerlink" title="Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation"></a>Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08774">http://arxiv.org/abs/2311.08774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hesham Ali, Idriss Tondji, Mennatullah Siam</li>
<li>for: 静脉 histopathological 图像中的核体分 segmentation 是诊断和治疗肿瘤疾病的关键任务，可以降低手动检查微scopic 组织图像所需的时间，并解决了诊断过程中的冲突。深度学习 已经在这种任务中证明了其用用。</li>
<li>methods: 我们提出了一种新的核体分 segmentation 方法，利用可用的标注和无标注数据。我们的方法结合了transductive 和 inductive 学习的优点，以前两者已经分别在单独的方法中实现。 inductive 学习 aspires to approximate the general function and generalize to unseen test data, while transductive learning has the potential of leveraging the unlabelled test data to improve the classification。</li>
<li>results: 我们在 MoNuSeg 测试集上评估了我们的方法，并证明了我们的方法的效果和潜在性。这是我们知道的第一个尝试将 transductive 和 inductive 学习结合在一起的研究。此外，我们还提出了一种新的两stage 推理方案。<details>
<summary>Abstract</summary>
AI-assisted nuclei segmentation in histopathological images is a crucial task in the diagnosis and treatment of cancer diseases. It decreases the time required to manually screen microscopic tissue images and can resolve the conflict between pathologists during diagnosis. Deep Learning has proven useful in such a task. However, lack of labeled data is a significant barrier for deep learning-based approaches. In this study, we propose a novel approach to nuclei segmentation that leverages the available labelled and unlabelled data. The proposed method combines the strengths of both transductive and inductive learning, which have been previously attempted separately, into a single framework. Inductive learning aims at approximating the general function and generalizing to unseen test data, while transductive learning has the potential of leveraging the unlabelled test data to improve the classification. To the best of our knowledge, this is the first study to propose such a hybrid approach for medical image segmentation. Moreover, we propose a novel two-stage transductive inference scheme. We evaluate our approach on MoNuSeg benchmark to demonstrate the efficacy and potential of our method.
</details>
<details>
<summary>摘要</summary>
人工智能助成肿瘤分 segmentation在 histopathological 图像中是诊断和治疗 cancer 疾病的关键任务。它可以减少手动检查 microscopic 组织图像所需的时间，并解决了诊断过程中的冲突。深度学习已经证明是非常有用的。然而，缺乏标注数据是深度学习基本的障碍。在这项研究中，我们提出了一种新的 nuclei 分 segmentation 方法，利用可用的标注和未标注数据。我们的方法结合了泛化学习和推导学习的优点，这两种方法在过去分别进行了独立的尝试。泛化学习旨在近似通用函数，并将其推广到未见过的测试数据中，而推导学习则可以利用测试数据来改进分类。到目前为止，这是首次提出了这种混合方法的医疗图像分 segmentation 研究。此外，我们还提出了一种新的两 stage 混合推理方案。我们在 MoNuSeg 数据集上评估了我们的方法，以示我们的方法的效果和潜力。
</details></li>
</ul>
<hr>
<h2 id="FastBlend-a-Powerful-Model-Free-Toolkit-Making-Video-Stylization-Easier"><a href="#FastBlend-a-Powerful-Model-Free-Toolkit-Making-Video-Stylization-Easier" class="headerlink" title="FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier"></a>FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09265">http://arxiv.org/abs/2311.09265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/artiprocher/sd-webui-fastblend">https://github.com/artiprocher/sd-webui-fastblend</a></li>
<li>paper_authors: Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang, Mingyi Jin</li>
<li>for:  addresses the consistency problem in video processing for tasks such as style transfer and image editing.</li>
<li>methods:  based on a patch matching algorithm, with two inference modes: blending and interpolation.</li>
<li>results:  outperforms existing methods for video deflickering and video synthesis in the blending mode, and surpasses video interpolation and model-based video processing approaches in the interpolation mode.Here’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文是为了解决视频处理中的一致性问题，例如样式传输和图像修改等任务。</li>
<li>methods: 该论文提出了一个强大的模型自由工具kit called FastBlend，基于一种贴图匹配算法，包括拼接和 interpolate两种推理模式。在拼接模式下，FastBlend 消除了视频闪烁，通过拼接视频帧内的滑动窗口进行拼接。此外，我们优化了不同应用场景下的计算效率和视频质量。在 interpolate 模式下，给定一个或多个随机生成的Diffusion模型 Keyframe，FastBlend 可以Render整个视频。由于 FastBlend 不改变Diffusion模型的生成过程，它表现出了优秀的兼容性。</li>
<li>results: 实验证明，FastBlend 在拼接模式下超越现有的视频闪烁消除和视频合成方法，在 interpolate 模式下超越视频 interpolate 和基于模型的视频处理方法。软件代码已经在 GitHub 上发布。<details>
<summary>Abstract</summary>
With the emergence of diffusion models and rapid development in image processing, it has become effortless to generate fancy images in tasks such as style transfer and image editing. However, these impressive image processing approaches face consistency issues in video processing. In this paper, we propose a powerful model-free toolkit called FastBlend to address the consistency problem for video processing. Based on a patch matching algorithm, we design two inference modes, including blending and interpolation. In the blending mode, FastBlend eliminates video flicker by blending the frames within a sliding window. Moreover, we optimize both computational efficiency and video quality according to different application scenarios. In the interpolation mode, given one or more keyframes rendered by diffusion models, FastBlend can render the whole video. Since FastBlend does not modify the generation process of diffusion models, it exhibits excellent compatibility. Extensive experiments have demonstrated the effectiveness of FastBlend. In the blending mode, FastBlend outperforms existing methods for video deflickering and video synthesis. In the interpolation mode, FastBlend surpasses video interpolation and model-based video processing approaches. The source codes have been released on GitHub.
</details>
<details>
<summary>摘要</summary>
“随着扩散模型的出现和图像处理的快速发展，实现了丰富的图像处理任务，如Style Transfer和图像修复。但这些图像处理方法在视频处理中存在一致性问题。在本文中，我们提出了一个强大的无模型工具组合called FastBlend，以解决视频处理中的一致性问题。基于一个贴合算法，我们设计了两种推论方式，包括融合和 interpolate。在融合模式下，FastBlend 删除了视频闪烁，通过视频内的滑块窗口融合几帧帧。此外，我们对不同应用场景进行了优化，以提高计算效率和视频质量。在 interpolate 模式下，给定一帧或更多的静止图像，FastBlend 可以预测整个视频。由于 FastBlend 不会修改扩散模型的生成过程，因此它具有很好的相容性。实验结果显示 FastBlend 的效果。在融合模式下，FastBlend 比现有的视频闪烁和视频合成方法更好。在 interpolate 模式下，FastBlend 比 виде interpolate 和基于模型的视频处理方法更好。源代码已经在 GitHub 上发布。”
</details></li>
</ul>
<hr>
<h2 id="4K-Resolution-Photo-Exposure-Correction-at-125-FPS-with-8K-Parameters"><a href="#4K-Resolution-Photo-Exposure-Correction-at-125-FPS-with-8K-Parameters" class="headerlink" title="4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters"></a>4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08759">http://arxiv.org/abs/2311.08759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhou-yijie/msltnet">https://github.com/zhou-yijie/msltnet</a></li>
<li>paper_authors: Yijie Zhou, Chao Li, Jin Liang, Tianyi Xu, Xin Liu, Jun Xu</li>
<li>for: 这篇论文旨在提出一种高效且轻量级的多层感知网络，用于纠正摄像头暴露不当的问题。</li>
<li>methods: 该方法使用了多层感知网络，首先使用拉普拉斯 pyramid 技术将输入图像分解成高和低频层，然后采用 pixel-adaptive 线性变换，通过有效的双边网格学习或 1x1 卷积实现。</li>
<li>results: 实验结果表明，提出的 MSLT 网络在两个标准数据集上比现有的方法更高效，并且可以在 Titan RTX GPU 上处理 4K 分辨率的 sRGB 图像，每秒 125 帧。<details>
<summary>Abstract</summary>
The illumination of improperly exposed photographs has been widely corrected using deep convolutional neural networks or Transformers. Despite with promising performance, these methods usually suffer from large parameter amounts and heavy computational FLOPs on high-resolution photographs. In this paper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale Linear Transformation (MSLT) networks under the multi-layer perception architecture, which can process 4K-resolution sRGB images at 125 Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT networks first decompose an input image into high and low frequency layers by Laplacian pyramid techniques, and then sequentially correct different layers by pixel-adaptive linear transformation, which is implemented by efficient bilateral grid learning or 1x1 convolutions. Experiments on two benchmark datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts on photo exposure correction. Extensive ablation studies validate the effectiveness of our contributions. The code is available at https://github.com/Zhou-Yijie/MSLTNet.
</details>
<details>
<summary>摘要</summary>
“对于不当露光的照片，使用深度卷积神经网或Transformers进行修复已经得到了广泛的改善。然而，这些方法通常具有较大的参数数量和高 computational FLOPs，尤其是在高分辨率的照片上。在这篇论文中，我们提出了 extremely light-weight（只有约8K参数）的多尺度线性转换网络（MSLT），可以在Titan RTX GPU上处理4K分辨率的sRGB图像，每秒125帧（FPS）。具体来说，我们的MSLT网络首先将输入图像分解成高和低频层使用拉普拉斯 pyramid 技术，然后遍历不同层次进行像素适应的直线转换，这是通过高效的双方格学习或1x1卷积而实现。实验结果显示，我们的MSLTs在照片露光修复方面与现有的state-of-the-arts进行比较，提供了更高效的修复效果。广泛的ablation study validate了我们的贡献。code可以在https://github.com/Zhou-Yijie/MSLTNet中找到。”
</details></li>
</ul>
<hr>
<h2 id="Improved-Dense-Nested-Attention-Network-Based-on-Transformer-for-Infrared-Small-Target-Detection"><a href="#Improved-Dense-Nested-Attention-Network-Based-on-Transformer-for-Infrared-Small-Target-Detection" class="headerlink" title="Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection"></a>Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08747">http://arxiv.org/abs/2311.08747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun Bao, Jie Cao, Yaqian Ning, Tianhua Zhao, Zhijun Li, Zechen Wang, Li Zhang, Qun Hao</li>
<li>For: 这个论文旨在提出一种基于深度学习的红外小目标检测方法，以提高红外小目标在背景干扰下的检测精度。* Methods: 该方法基于 transformer 架构，保留 dense nested attention network (DNANet) 的粗糙结构，并在特征提取阶段引入 Swin-transformer，以增强特征的连续性。 此外，该方法还 integrate ACmix attention structure 到 dense nested structure 中，以提高中间层特征的表示能力。* Results: 该方法在对公共数据集进行测试时，与其他当前领先方法进行比较，在检测概率 (P_d)、假阳性率 (F_a) 和 mean intersection of union ($mIoU$) 上均表现出色，特别是在 NUDT-SIRST 数据集上，$mIoU$ 达到 90.89%。<details>
<summary>Abstract</summary>
Infrared small target detection based on deep learning offers unique advantages in separating small targets from complex and dynamic backgrounds. However, the features of infrared small targets gradually weaken as the depth of convolutional neural network (CNN) increases. To address this issue, we propose a novel method for detecting infrared small targets called improved dense nested attention network (IDNANet), which is based on the transformer architecture. We preserve the dense nested structure of dense nested attention network (DNANet) and introduce the Swin-transformer during feature extraction stage to enhance the continuity of features. Furthermore, we integrate the ACmix attention structure into the dense nested structure to enhance the features of intermediate layers. Additionally, we design a weighted dice binary cross-entropy (WD-BCE) loss function to mitigate the negative impact of foreground-background imbalance in the samples. Moreover, we develop a dataset specifically for infrared small targets, called BIT-SIRST. The dataset comprises a significant amount of real-world targets and manually annotated labels, as well as synthetic data and corresponding labels. We have evaluated the effectiveness of our method through experiments conducted on public datasets. In comparison to other state-of-the-art methods, our approach outperforms in terms of probability of detection (P_d), false-alarm rate (F_a), and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.
</details>
<details>
<summary>摘要</summary>
infrared小目标检测基于深度学习具有独特优势，可以在复杂和动态背景中分离小目标。然而，深度 convolutional neural network (CNN) 中小目标特征逐渐弱化。为解决这个问题，我们提出一种改进的 dense nested attention network (IDNANet)，基于 transformer 架构。我们保留 dense nested structure 的 dense nested attention network (DNANet)，并在特征提取阶段引入 Swin-transformer，以增强特征的连续性。此外，我们将 ACmix attention 结构integrated into dense nested structure，以提高中间层特征的表达能力。此外，我们还提出了一种weighted dice binary cross-entropy (WD-BCE) 损失函数，以减少样本中背景干扰的负面影响。此外，我们还开发了一个专门 для infrared小目标的数据集，名为 BIT-SIRST。该数据集包括大量真实世界目标和手动标注的标签，以及人工生成的数据和相应的标签。我们通过对公共数据集进行实验，证明了我们的方法的效果。与其他当前状态的方法相比，我们的方法在检测概率（P_d）、false-alarm rate（F_a）和 Mean Intersection of Union（$mIoU）方面表现出色，其中 $mIoU$ 达到了 90.89% 和 79.72% 在 NUDT-SIRST 和 NUAA-SIRST 数据集上。
</details></li>
</ul>
<hr>
<h2 id="A-Diffusion-Model-Based-Quality-Enhancement-Method-for-HEVC-Compressed-Video"><a href="#A-Diffusion-Model-Based-Quality-Enhancement-Method-for-HEVC-Compressed-Video" class="headerlink" title="A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video"></a>A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08746">http://arxiv.org/abs/2311.08746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Honggang Qi</li>
<li>for: 提高压缩 видео质量</li>
<li>methods: 使用扩散模型进行 posterior 处理</li>
<li>results: 对混合数据集进行实验，结果较 existed 方法 superiorHere’s a brief explanation of each point:* “for”: The paper is written to improve the quality of compressed videos by proposing a diffusion model based post-processing method.* “methods”: The proposed method uses a diffusion model to estimate the feature vectors of the compressed video and then uses the estimated feature vectors as prior information for a quality enhancement model to adaptively enhance the quality of the compressed video.* “results”: The proposed method outperforms existing methods on mixed datasets in terms of quality enhancement results.<details>
<summary>Abstract</summary>
Video post-processing methods can improve the quality of compressed videos at the decoder side. Most of the existing methods need to train corresponding models for compressed videos with different quantization parameters to improve the quality of compressed videos. However, in most cases, the quantization parameters of the decoded video are unknown. This makes existing methods have their limitations in improving video quality. To tackle this problem, this work proposes a diffusion model based post-processing method for compressed videos. The proposed method first estimates the feature vectors of the compressed video and then uses the estimated feature vectors as the prior information for the quality enhancement model to adaptively enhance the quality of compressed video with different quantization parameters. Experimental results show that the quality enhancement results of our proposed method on mixed datasets are superior to existing methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字符串。<</SYS>>压缩视频后处理技术可以提高压缩视频解码器 сторо的质量。大多数现有方法需要为压缩视频不同的压缩参数训练对应的模型，以提高压缩视频质量。然而，在大多数情况下，解码后的视频压缩参数未知。这限制了现有方法的应用。为解决这问题，该工作提议一种基于扩散模型的后处理方法 для压缩视频。提议方法首先估算压缩视频中的特征向量，然后使用估算的特征向量作为质量提升模型的先天信息，以适应性地提高压缩视频的质量。实验结果表明，我们提议的方法在混合数据集上的质量提升结果比现有方法更佳。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Federated-Learning-for-Clients-with-Different-Input-Image-Sizes-and-Numbers-of-Output-Categories"><a href="#Scalable-Federated-Learning-for-Clients-with-Different-Input-Image-Sizes-and-Numbers-of-Output-Categories" class="headerlink" title="Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories"></a>Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08716">http://arxiv.org/abs/2311.08716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhei Nitta, Taiji Suzuki, Albert Rodríguez Mulet, Atsushi Yaguchi, Ryusuke Hirai</li>
<li>for: 这个研究旨在提出一种适应多客户端不同输入图像大小和输出类别数量的联边学习方法，以及一个可靠地评估联边学习的通用差异关系。</li>
<li>methods: 本研究提出了一种名为ScalableFL的联边学习方法，其中每个客户端的本地模型深度和宽度将根据客户端的输入图像大小和输出类别数量进行调整。</li>
<li>results: 研究人员透过实验证明了ScalableFL在多个不同客户端设置下的实现效果，包括图像分类和物体检测任务。此外，研究人员还提出了一个新的 bounds 来评估联边学习的通用差异关系，并证明了这个 bounds 的有效性。<details>
<summary>Abstract</summary>
Federated learning is a privacy-preserving training method which consists of training from a plurality of clients but without sharing their confidential data. However, previous work on federated learning do not explore suitable neural network architectures for clients with different input images sizes and different numbers of output categories. In this paper, we propose an effective federated learning method named ScalableFL, where the depths and widths of the local models for each client are adjusted according to the clients' input image size and the numbers of output categories. In addition, we provide a new bound for the generalization gap of federated learning. In particular, this bound helps to explain the effectiveness of our scalable neural network approach. We demonstrate the effectiveness of ScalableFL in several heterogeneous client settings for both image classification and object detection tasks.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种隐私保护的训练方法，它通过多个客户端进行训练，但没有共享客户端的敏感数据。然而，先前的联邦学习工作没有探讨适合客户端不同输入图像大小和输出类别数量的适合的神经网络架构。在这篇论文中，我们提出了一种高效的联邦学习方法，名为可扩展FL（ScalableFL），其中客户端的本地模型的深度和宽度会根据客户端的输入图像大小和输出类别数量进行调整。此外，我们还提供了一个新的泛化差界，这个泛化差界可以帮助解释我们的可扩展神经网络方法的效果。我们在多个不同客户端设置下进行了多个图像分类和对象检测任务的实践，以证明可扩展FL 的效果。
</details></li>
</ul>
<hr>
<h2 id="CP-EB-Talking-Face-Generation-with-Controllable-Pose-and-Eye-Blinking-Embedding"><a href="#CP-EB-Talking-Face-Generation-with-Controllable-Pose-and-Eye-Blinking-Embedding" class="headerlink" title="CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding"></a>CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08673">http://arxiv.org/abs/2311.08673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzong Wang, Yimin Deng, Ziqi Liang, Xulong Zhang, Ning Cheng, Jing Xiao</li>
<li>for: 该研究提出了一种名为“CP-EB”的 talking face生成方法，用于生成一个基于短视频剪辑的人脸对话视频，其中人脸的头部pose和眼睛跳动都是控制的。</li>
<li>methods: 该方法使用了一种基于GAN的建模方法，通过提取音频输入和参考视频中的眼睛跳动特征，并将其embed在人脸的标识和 pose特征中，以生成真实的人脸对话视频。</li>
<li>results: 实验结果表明，该方法可以生成出真实的人脸对话视频，其中人脸的头部pose和眼睛跳动都具有自然的感觉，同时 lip 动作也具有同步的特征。<details>
<summary>Abstract</summary>
This paper proposes a talking face generation method named "CP-EB" that takes an audio signal as input and a person image as reference, to synthesize a photo-realistic people talking video with head poses controlled by a short video clip and proper eye blinking embedding. It's noted that not only the head pose but also eye blinking are both important aspects for deep fake detection. The implicit control of poses by video has already achieved by the state-of-art work. According to recent research, eye blinking has weak correlation with input audio which means eye blinks extraction from audio and generation are possible. Hence, we propose a GAN-based architecture to extract eye blink feature from input audio and reference video respectively and employ contrastive training between them, then embed it into the concatenated features of identity and poses to generate talking face images. Experimental results show that the proposed method can generate photo-realistic talking face with synchronous lips motions, natural head poses and blinking eyes.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种名为“CP-EB”的人脸讲话方法，该方法接受音频信号和人像作为输入，并生成一个包含人脸动作和自然头姿的真实人脸讲话视频。研究表明，不仅头姿，而且眼睛莲花也是深度假设检测中重要的两个方面。现有的研究已经实现了视频内容的含义控制。根据最新的研究，眼睛莲花和输入音频之间存在弱相关性，因此可以从音频和参考视频中提取眼睛莲花特征，并通过对其进行对比训练，将其embed到人脸特征和头姿特征中，以生成真实人脸讲话图像。实验结果表明，提出的方法可以生成真实人脸讲话图像，并且有同步的嘴部动作、自然的头姿和眼睛莲花。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Network-Identification-of-Limnonectes-Species-and-New-Class-Detection-Using-Image-Data"><a href="#Deep-Neural-Network-Identification-of-Limnonectes-Species-and-New-Class-Detection-Using-Image-Data" class="headerlink" title="Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data"></a>Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08661">http://arxiv.org/abs/2311.08661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Xu, Yili Hong, Eric P. Smith, David S. McLeod, Xinwei Deng, Laura J. Freeman</li>
<li>for: This paper aims to address the challenge of species complexes in biological systematics and taxonomy, specifically by developing new tools using machine learning to classify images of frogs into known species groups and detect out-of-distribution (OOD) images.</li>
<li>methods: The paper uses deep neural networks and the principles of machine learning to classify images of frogs based on a morphological character (hind limb skin texture), and to detect OOD images.</li>
<li>results: The paper demonstrates that the algorithm can successfully automate the classification of an image into a known species group for which it has been trained, and can also classify an image into a new class if the image does not belong to the existing classes. The paper also tests the performance of the OOD detection algorithm on a larger dataset.<details>
<summary>Abstract</summary>
As is true of many complex tasks, the work of discovering, describing, and understanding the diversity of life on Earth (viz., biological systematics and taxonomy) requires many tools. Some of this work can be accomplished as it has been done in the past, but some aspects present us with challenges which traditional knowledge and tools cannot adequately resolve. One such challenge is presented by species complexes in which the morphological similarities among the group members make it difficult to reliably identify known species and detect new ones. We address this challenge by developing new tools using the principles of machine learning to resolve two specific questions related to species complexes. The first question is formulated as a classification problem in statistics and machine learning and the second question is an out-of-distribution (OOD) detection problem. We apply these tools to a species complex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex) and employ a morphological character (hind limb skin texture) traditionally treated qualitatively in a quantitative and objective manner. We demonstrate that deep neural networks can successfully automate the classification of an image into a known species group for which it has been trained. We further demonstrate that the algorithm can successfully classify an image into a new class if the image does not belong to the existing classes. Additionally, we use the larger MNIST dataset to test the performance of our OOD detection algorithm. We finish our paper with some concluding remarks regarding the application of these methods to species complexes and our efforts to document true biodiversity. This paper has online supplementary materials.
</details>
<details>
<summary>摘要</summary>
如同许多复杂任务一样，发现、描述和理解地球上生物多样性（即生物系统матиcs和taxonomy）需要许多工具。一些这种工作可以通过过去的方式完成，但一些方面对传统知识和工具来说是挑战。一个这样的挑战是由生物多样性中的物种复合群产生，其中种群成员之间的形态相似性使得可靠地识别已知种和检测新种困难。我们解决这个挑战 by developing new tools using机器学习的原则。我们对一个来自东南亚短腿蛙（Limnonectes kuhlii complex）种群进行了应用。我们使用了一个形态特征（背部皮肤文化），通常被视为 качеitative的，并将其变换为一个量化和客观的方式。我们表明了深度神经网络可以成功地自动将图像分类到已知种群中。我们进一步表明了算法可以成功地分类图像到一个新的类别，如果图像不属于现有的类别。此外，我们使用了更大的MNIST数据集来测试我们的OOD检测算法。我们结束这篇论文 avec some concluding remarks regarding the application of these methods to species complexes and our efforts to document true biodiversity。这篇论文有在线补充材料。
</details></li>
</ul>
<hr>
<h2 id="ConeQuest-A-Benchmark-for-Cone-Segmentation-on-Mars"><a href="#ConeQuest-A-Benchmark-for-Cone-Segmentation-on-Mars" class="headerlink" title="ConeQuest: A Benchmark for Cone Segmentation on Mars"></a>ConeQuest: A Benchmark for Cone Segmentation on Mars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08657">http://arxiv.org/abs/2311.08657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kerner-lab/conequest">https://github.com/kerner-lab/conequest</a></li>
<li>paper_authors: Mirali Purohit, Jacob Adler, Hannah Kerner</li>
<li>for: 这个论文是为了提供一个 Mars 上的突起 cone 的专家标注数据集，以便用于计算机视觉模型的训练和测试。</li>
<li>methods: 这个论文使用了 Mars 上不同区域的卫星和探测器提供的卫星图像，并对其进行了专家标注。同时，它还使用了现有的计算机视觉模型，并对其进行了质量评估。</li>
<li>results: 这个论文的结果表明，目前存在的计算机视觉模型无法准确地识别和分割 Mars 上的突起 cone。模型在受到的数据上的平均 IoU 为 52.52% 和 42.55%。这个新的专家标注数据集将有助于开发更加准确和可靠的计算机视觉模型。<details>
<summary>Abstract</summary>
Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:年来，宇宙科学家们从卫星和探测器中收集了大量的火星数据。一个重要的特征是穹顶坑，被解释为沉积在水（如湖或海）中的泥火山。全球火星上的穹顶坑识别是非常重要，但专家地质学家无法检索庞大的卫星图像库，以确定所有示例。这个任务非常适合计算机视觉。虽然有多个计算机视觉数据集用于火星相关任务，但目前没有公共的数据集用于穹顶坑检测/分割。此外，前一些研究使用单一地区的数据进行训练，限制其全球检测和地图应用。为了解决这个问题，我们引入 ConeQuest，火星上穹顶坑的第一个专家标注公共数据集。 ConeQuest 包含 >13k 样本从三个不同的火星地区。我们提出了两个 benchmark 任务使用 ConeQuest：（i）空间总体化和（ii）坑口大小总体化。我们在两个 benchmark 任务上精细化和评估了广泛使用的分割模型。结果表明，穹顶坑分割是计算机视觉中的一个挑战性问题，现有的分割模型在具有数据集的情况下，取得了平均 IoU 为 52.52% 和 42.55%。我们认为这个新的 benchmark 数据集将促进更加准确和可靠的模型的发展。数据和代码可以在 https://github.com/kerner-lab/ConeQuest 上获取。
</details></li>
</ul>
<hr>
<h2 id="Review-of-AlexNet-for-Medical-Image-Classification"><a href="#Review-of-AlexNet-for-Medical-Image-Classification" class="headerlink" title="Review of AlexNet for Medical Image Classification"></a>Review of AlexNet for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08655">http://arxiv.org/abs/2311.08655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Wenhao Tang, Junding Sun, Shuihua Wang, Yudong Zhang</li>
<li>for: 这个论文主要用于介绍AlexNet对于医疗影像分类的应用和发展.</li>
<li>methods: 这个论文使用了dropout技术来减少适束错误，并且使用ReLU启动函数来避免梯度消失.</li>
<li>results: 这个论文发现AlexNet在2012年于医疗影像分类 tasks 中表现出色，并且在训练时使用了40多篇论文，包括期刊论文和会议论文.<details>
<summary>Abstract</summary>
In recent years, the rapid development of deep learning has led to a wide range of applications in the field of medical image classification. The variants of neural network models with ever-increasing performance share some commonalities: to try to mitigate overfitting, improve generalization, avoid gradient vanishing and exploding, etc. AlexNet first utilizes the dropout technique to mitigate overfitting and the ReLU activation function to avoid gradient vanishing. Therefore, we focus our discussion on AlexNet, which has contributed greatly to the development of CNNs in 2012. After reviewing over 40 papers, including journal papers and conference papers, we give a narrative on the technical details, advantages, and application areas of AlexNet.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习的快速发展导致医疗图像分类领域中的广泛应用。这些变种神经网络模型具有不断提高表现的共同特点：减少过拟合、提高通用性、避免梯度消失和爆炸等。AlexNet首先采用了dropout技术来减少过拟合，并使用ReLU活化函数来避免梯度消失。因此，我们在讨论AlexNet的时候会更加着重于其技术细节、优势和应用领域。经过阅读40余篇论文，包括期刊论文和会议论文，我们将为AlexNet提供一篇narative。
</details></li>
</ul>
<hr>
<h2 id="Refining-Perception-Contracts-Case-Studies-in-Vision-based-Safe-Auto-landing"><a href="#Refining-Perception-Contracts-Case-Studies-in-Vision-based-Safe-Auto-landing" class="headerlink" title="Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing"></a>Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08652">http://arxiv.org/abs/2311.08652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangge Li, Benjamin C Yang, Yixuan Jia, Daniel Zhuang, Sayan Mitra</li>
<li>For: The paper is written for evaluating the safety of control systems that use machine learning for perception, and providing a method for proving end-to-end system-level safety requirements.* Methods: The paper uses perception contracts, which are specifications for testing the machine learning components, and an algorithm for constructing data and requirement guided refinement of perception contracts (DaRePC).* Results: The paper provides testable contracts that establish the state and environment conditions under which an aircraft can safely touchdown on the runway and a drone can safely pass through a sequence of gates, and also discovers conditions that can possibly violate the safety of the vision-based control system.<details>
<summary>Abstract</summary>
Perception contracts provide a method for evaluating safety of control systems that use machine learning for perception. A perception contract is a specification for testing the ML components, and it gives a method for proving end-to-end system-level safety requirements. The feasibility of contract-based testing and assurance was established earlier in the context of straight lane keeping: a 3-dimensional system with relatively simple dynamics. This paper presents the analysis of two 6 and 12-dimensional flight control systems that use multi-stage, heterogeneous, ML-enabled perception. The paper advances methodology by introducing an algorithm for constructing data and requirement guided refinement of perception contracts (DaRePC). The resulting analysis provides testable contracts which establish the state and environment conditions under which an aircraft can safety touchdown on the runway and a drone can safely pass through a sequence of gates. It can also discover conditions (e.g., low-horizon sun) that can possibly violate the safety of the vision-based control system.
</details>
<details>
<summary>摘要</summary>
感知合约提供了评估机器学习控制系统安全性的方法。感知合约是测试ML组件的规范，它提供了系统级别的安全需求证明方法。在旁卷道保持的情况下，曾经证明了合约基于测试和保证的可行性。本文分析了使用多Stage、不同类型、ML启用的飞行控制系统，并 introduce了一种构建数据和需求导向的改进感知合约算法（DaRePC）。该分析提供了可测试的合约，以确定飞机在跑道上安全Touchdown和无人机通过序列门的安全性。它还可以发现可能违反视口控制系统安全的条件（例如低天际日）。
</details></li>
</ul>
<hr>
<h2 id="Painterly-Image-Harmonization-via-Adversarial-Residual-Learning"><a href="#Painterly-Image-Harmonization-via-Adversarial-Residual-Learning" class="headerlink" title="Painterly Image Harmonization via Adversarial Residual Learning"></a>Painterly Image Harmonization via Adversarial Residual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08646">http://arxiv.org/abs/2311.08646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xudong Wang, Li Niu, Junyan Cao, Yan Hong, Liqing Zhang</li>
<li>for: 将插入背景图像中的前景对象融合成一个自然和丰富的图像</li>
<li>methods: 使用对抗学习将前景特征图和背景特征图的领域差异bridge</li>
<li>results: 实现更自然和艺术化的混合效果，比前方法更加美观和有趣<details>
<summary>Abstract</summary>
Image compositing plays a vital role in photo editing. After inserting a foreground object into another background image, the composite image may look unnatural and inharmonious. When the foreground is photorealistic and the background is an artistic painting, painterly image harmonization aims to transfer the style of background painting to the foreground object, which is a challenging task due to the large domain gap between foreground and background. In this work, we employ adversarial learning to bridge the domain gap between foreground feature map and background feature map. Specifically, we design a dual-encoder generator, in which the residual encoder produces the residual features added to the foreground feature map from main encoder. Then, a pixel-wise discriminator plays against the generator, encouraging the refined foreground feature map to be indistinguishable from background feature map. Extensive experiments demonstrate that our method could achieve more harmonious and visually appealing results than previous methods.
</details>
<details>
<summary>摘要</summary>
图像组合在图像编辑中扮演着重要的角色。在插入一个背景图像中的前景对象后， composite image可能会看起来不自然和不协调。当前景对象是真实图像，而背景是一幅艺术油画时， painterly image harmonization的目标是将背景油画的风格传递给前景对象，这是一项具有很大领域差距的任务。在这种情况下，我们使用对抗学习来跨领域减小这个差距。我们设计了一个双encoder生成器，其中剩余编码器生成了对前景编码器的剩余特征。然后，一个像素精度分子检测器与生成器进行对抗，以便鼓励生成的涂抹前景特征图像与背景特征图像无法分辨。广泛的实验表明，我们的方法可以实现更协调、更美观的结果，比前一些方法更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.CV_2023_11_15/" data-id="clp53jwqy00moyp88hk3s5i58" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/15/cs.SD_2023_11_15/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-11-15
        
      </div>
    </a>
  
  
    <a href="/2023/11/15/cs.AI_2023_11_15/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-15</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">136</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">125</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">62</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">123</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">76</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
