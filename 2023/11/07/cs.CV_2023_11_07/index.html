
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-07 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04391 repo_url: None paper_authors: Chenfeng Xu, Huan Ling, Sanja Fidler, Or Litany for: 3">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-07">
<meta property="og:url" content="https://nullscc.github.io/2023/11/07/cs.CV_2023_11_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04391 repo_url: None paper_authors: Chenfeng Xu, Huan Ling, Sanja Fidler, Or Litany for: 3">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-07T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-09T19:38:45.385Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.CV_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T13:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-07
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3DiffTection-3D-Object-Detection-with-Geometry-Aware-Diffusion-Features"><a href="#3DiffTection-3D-Object-Detection-with-Geometry-Aware-Diffusion-Features" class="headerlink" title="3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features"></a>3DiffTection: 3D Object Detection with Geometry-Aware Diffusion Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04391">http://arxiv.org/abs/2311.04391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenfeng Xu, Huan Ling, Sanja Fidler, Or Litany</li>
<li>for: 3D object detection from single images</li>
<li>methods: uses a 3D-aware diffusion model with two specialized tuning strategies (geometric and semantic) and a novel epipolar warp operator for novel view synthesis</li>
<li>results: obtains 3D-aware features that excel in identifying cross-view point correspondences, substantially surpassing previous benchmarks (e.g., Cube-RCNN) with an improvement of 9.43% in AP3D on the Omni3D-ARkitscene dataset, and showcases robust data efficiency and generalization to cross-domain data.Here is the text in Simplified Chinese:</li>
<li>for: 用于单图像中的3D对象检测</li>
<li>methods: 使用3D感知的扩散模型，并使用两种特циали化的调整策略（几何和 semantics）和一种新的规则投影算子 для新视图创建</li>
<li>results: 获得了适应3D检测的3D感知特征，在跨视点对应点检测方面表现出色，比前一代标准（如Cube-RCNN）提高9.43%的AP3D值（在Omni3D-ARkitscene数据集上），并展现出了robust的数据效率和跨频谱数据的通用性。<details>
<summary>Abstract</summary>
We present 3DiffTection, a state-of-the-art method for 3D object detection from single images, leveraging features from a 3D-aware diffusion model. Annotating large-scale image data for 3D detection is resource-intensive and time-consuming. Recently, pretrained large image diffusion models have become prominent as effective feature extractors for 2D perception tasks. However, these features are initially trained on paired text and image data, which are not optimized for 3D tasks, and often exhibit a domain gap when applied to the target data. Our approach bridges these gaps through two specialized tuning strategies: geometric and semantic. For geometric tuning, we fine-tune a diffusion model to perform novel view synthesis conditioned on a single image, by introducing a novel epipolar warp operator. This task meets two essential criteria: the necessity for 3D awareness and reliance solely on posed image data, which are readily available (e.g., from videos) and does not require manual annotation. For semantic refinement, we further train the model on target data with detection supervision. Both tuning phases employ ControlNet to preserve the integrity of the original feature capabilities. In the final step, we harness these enhanced capabilities to conduct a test-time prediction ensemble across multiple virtual viewpoints. Through our methodology, we obtain 3D-aware features that are tailored for 3D detection and excel in identifying cross-view point correspondences. Consequently, our model emerges as a powerful 3D detector, substantially surpassing previous benchmarks, e.g., Cube-RCNN, a precedent in single-view 3D detection by 9.43\% in AP3D on the Omni3D-ARkitscene dataset. Furthermore, 3DiffTection showcases robust data efficiency and generalization to cross-domain data.
</details>
<details>
<summary>摘要</summary>
我们介绍3DiffTection，一种基于单张图像的state-of-the-art方法 для3D对象检测。我们利用了3D意识扩散模型中的特征。为了检测3D对象，手动标注大规模图像数据是费时费力的。最近，预训练的大图扩散模型在2D识别任务中表现出了remarkable的效果，但这些特征是通过paired文本和图像数据进行预训练的，这些特征通常会在目标数据上存在域 gap。我们的方法通过两种特殊的调整策略来bridges这些差异：几何调整和semantic调整。为了几何调整，我们在单张图像 Conditioned on novel epipolar warp operator进行新视图synthesis，这个任务满足了两个重要的条件：需要3D意识和仅仅基于提供的posed图像数据进行训练，不需要手动标注。为了semantic调整，我们进一步在目标数据上训练模型，使其具有检测Supervision。两个调整阶段都使用ControlNet保持原始特征的完整性。在最后一步，我们利用这些加强的特征进行多个虚拟视点的测试预测 ensemble。通过我们的方法，我们获得了适应3D检测的3D意识特征，可以准确地确定交叉视点对应关系。因此，我们的模型在Omni3D-ARkitscene dataset上的AP3D指标上表现出了9.43%的提升，胜过了先前的Benchmark，如Cube-RCNN。此外，3DiffTection还展示了robust数据效率和适用于不同频谱数据的泛化性。
</details></li>
</ul>
<hr>
<h2 id="Basis-restricted-elastic-shape-analysis-on-the-space-of-unregistered-surfaces"><a href="#Basis-restricted-elastic-shape-analysis-on-the-space-of-unregistered-surfaces" class="headerlink" title="Basis restricted elastic shape analysis on the space of unregistered surfaces"></a>Basis restricted elastic shape analysis on the space of unregistered surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04382">http://arxiv.org/abs/2311.04382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Hartman, Emery Pierson, Martin Bauer, Mohamed Daoudi, Nicolas Charon</li>
<li>for: 这个论文是为了提出一种新的数学和数值方法来进行表面分析，基于泛函空间上的弹性里曼米特特性。</li>
<li>methods: 这种方法是通过限制允许的变换空间为先导的finite维基准抽象场的估计，从而将表面空间简化为一个finite维Latent空间。</li>
<li>results: 这种方法可以有效地实现一些基于表面网格的任务，如Shape注册、Shape interpolate、动作传递和随机 pose生成，并且在人体形态和姿态数据上进行了验证，并与现有方法进行了比较，得到了更好的性能。<details>
<summary>Abstract</summary>
This paper introduces a new mathematical and numerical framework for surface analysis derived from the general setting of elastic Riemannian metrics on shape spaces. Traditionally, those metrics are defined over the infinite dimensional manifold of immersed surfaces and satisfy specific invariance properties enabling the comparison of surfaces modulo shape preserving transformations such as reparametrizations. The specificity of the approach we develop is to restrict the space of allowable transformations to predefined finite dimensional bases of deformation fields. These are estimated in a data-driven way so as to emulate specific types of surface transformations observed in a training set. The use of such bases allows to simplify the representation of the corresponding shape space to a finite dimensional latent space. However, in sharp contrast with methods involving e.g. mesh autoencoders, the latent space is here equipped with a non-Euclidean Riemannian metric precisely inherited from the family of aforementioned elastic metrics. We demonstrate how this basis restricted model can be then effectively implemented to perform a variety of tasks on surface meshes which, importantly, does not assume these to be pre-registered (i.e. with given point correspondences) or to even have a consistent mesh structure. We specifically validate our approach on human body shape and pose data as well as human face scans, and show how it generally outperforms state-of-the-art methods on problems such as shape registration, interpolation, motion transfer or random pose generation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Deep-Learning-Approach-to-Video-Anomaly-Detection-using-Convolutional-Autoencoders"><a href="#A-Deep-Learning-Approach-to-Video-Anomaly-Detection-using-Convolutional-Autoencoders" class="headerlink" title="A Deep Learning Approach to Video Anomaly Detection using Convolutional Autoencoders"></a>A Deep Learning Approach to Video Anomaly Detection using Convolutional Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04351">http://arxiv.org/abs/2311.04351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gopikrishna Pavuluri, Gayathri Annem</li>
<li>for: 这个研究旨在提出一种基于深度学习的视频异常检测方法，使用卷积自适应Encoder和解码神经网络对UCSD dataset进行分析。</li>
<li>methods: 该方法利用卷积自适应Encoder来学习正常视频的空间时间模式，然后对测试视频每帧进行比较，以确定异常。</li>
<li>results: 该方法在UCSD dataset上进行测试，实现了99.35%的总准确率（在Ped1 dataset上）和99.77%的总准确率（在Ped2 dataset上），证明了该方法在视频异常检测中的效果。<details>
<summary>Abstract</summary>
In this research we propose a deep learning approach for detecting anomalies in videos using convolutional autoencoder and decoder neural networks on the UCSD dataset.Our method utilizes a convolutional autoencoder to learn the spatiotemporal patterns of normal videos and then compares each frame of a test video to this learned representation. We evaluated our approach on the UCSD dataset and achieved an overall accuracy of 99.35% on the Ped1 dataset and 99.77% on the Ped2 dataset, demonstrating the effectiveness of our method for detecting anomalies in surveillance videos. The results show that our method outperforms other state-of-the-art methods, and it can be used in real-world applications for video anomaly detection.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种基于深度学习的视频异常检测方法，使用卷积自适应Encoder和解码神经网络在UCSD数据集上进行检测。我们的方法利用卷积自适应Encoder来学习正常视频中的空间时间模式，然后对测试视频中每帧图像与这个学习的表示进行比较。我们对UCSD数据集进行评估，实现了 Ped1 数据集的总准确率为 99.35% 和 Ped2 数据集的总准确率为 99.77%，这表明了我们的方法在视频异常检测中的有效性。结果显示，我们的方法在当前的实际应用中可以取得更高的性能。
</details></li>
</ul>
<hr>
<h2 id="SaFL-Sybil-aware-Federated-Learning-with-Application-to-Face-Recognition"><a href="#SaFL-Sybil-aware-Federated-Learning-with-Application-to-Face-Recognition" class="headerlink" title="SaFL: Sybil-aware Federated Learning with Application to Face Recognition"></a>SaFL: Sybil-aware Federated Learning with Application to Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04346">http://arxiv.org/abs/2311.04346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Ghafourian, Julian Fierrez, Ruben Vera-Rodriguez, Ruben Tolosana, Aythami Morales</li>
<li>for: 这篇论文的目的是提出一种防御腐败攻击的方法，以保护 Federated Learning（FL）中的数据隐私和安全。</li>
<li>methods: 这篇论文使用了一种名为 SaFL（Sybil-aware Federated Learning）的新防御方法，该方法使用了一种时间变量的汇集方式，以减少蜡层攻击的影响。</li>
<li>results: 该论文的实验结果表明，SaFL 方法可以有效地防止腐败攻击，并保持 FL 的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a machine learning paradigm to conduct collaborative learning among clients on a joint model. The primary goal is to share clients' local training parameters with an integrating server while preserving their privacy. This method permits to exploit the potential of massive mobile users' data for the benefit of machine learning models' performance while keeping sensitive data on local devices. On the downside, FL raises security and privacy concerns that have just started to be studied. To address some of the key threats in FL, researchers have proposed to use secure aggregation methods (e.g. homomorphic encryption, secure multiparty computation, etc.). These solutions improve some security and privacy metrics, but at the same time bring about other serious threats such as poisoning attacks, backdoor attacks, and free running attacks. This paper proposes a new defense method against poisoning attacks in FL called SaFL (Sybil-aware Federated Learning) that minimizes the effect of sybils with a novel time-variant aggregation scheme.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-Semantic-Matching-with-Hypercolumn-Correlation"><a href="#Efficient-Semantic-Matching-with-Hypercolumn-Correlation" class="headerlink" title="Efficient Semantic Matching with Hypercolumn Correlation"></a>Efficient Semantic Matching with Hypercolumn Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04336">http://arxiv.org/abs/2311.04336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungwook Kim, Juhong Min, Minsu Cho</li>
<li>for: 本研究主要针对于 semantic matching 领域，即在多个视觉特征图像之间建立Semantic correspondence。</li>
<li>methods: 本研究提出了 HCCNet，一种高效且有效的 semantic matching 方法，它利用多级对比地图中的各种信息，从低级对比地图到高级对比地图，以实现高效的Semantic correspondence establishment。 HCCNet 通过对瓶颈特征进行 feature slicing，生成了更丰富的中间特征，并使用这些中间特征建立了 hypercolumn correlation。</li>
<li>results: HCCNet 在标准的 semantic matching  bencmarks 上达到了state-of-the-art 或竞争性的性能，而且与现有的 SoTA 方法相比，它的计算负担和延迟 overhead 显著下降。<details>
<summary>Abstract</summary>
Recent studies show that leveraging the match-wise relationships within the 4D correlation map yields significant improvements in establishing semantic correspondences - but at the cost of increased computation and latency. In this work, we focus on the aspect that the performance improvements of recent methods can also largely be attributed to the usage of multi-scale correlation maps, which hold various information ranging from low-level geometric cues to high-level semantic contexts. To this end, we propose HCCNet, an efficient yet effective semantic matching method which exploits the full potential of multi-scale correlation maps, while eschewing the reliance on expensive match-wise relationship mining on the 4D correlation map. Specifically, HCCNet performs feature slicing on the bottleneck features to yield a richer set of intermediate features, which are used to construct a hypercolumn correlation. HCCNet can consequently establish semantic correspondences in an effective manner by reducing the volume of conventional high-dimensional convolution or self-attention operations to efficient point-wise convolutions. HCCNet demonstrates state-of-the-art or competitive performances on the standard benchmarks of semantic matching, while incurring a notably lower latency and computation overhead compared to the existing SoTA methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Data-Perspective-on-Enhanced-Identity-Preservation-for-Diffusion-Personalization"><a href="#A-Data-Perspective-on-Enhanced-Identity-Preservation-for-Diffusion-Personalization" class="headerlink" title="A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization"></a>A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04315">http://arxiv.org/abs/2311.04315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingzhe He, Zhiwen Cao, Nicholas Kolkin, Lantao Yu, Helge Rhodin, Ratheesh Kalarot</li>
<li>for: 该论文旨在提高大型文本到图像模型中生成图像的能力，但是它无法捕捉特定的个性或特殊视觉概念，如家中的物品或宠物。</li>
<li>methods: 该论文提出了一种数据驱动的方法，通过修改数据而不是模型来解决这个问题。它引入了一种新的常见化数据生成策略，以及文本和图像水平的常见化数据生成策略，以保持文本coherence和主体认知。</li>
<li>results: 该论文的实验结果表明，该方法可以生成高质量的图像，同时保持主体认知和多样性。它在Established benchmarks上达到了新的州OF-THE-ART WaterMark，并且在文本对应性、主体认知和多样性之间做出了最佳的平衡。<details>
<summary>Abstract</summary>
Large text-to-image models have revolutionized the ability to generate imagery using natural language. However, particularly unique or personal visual concepts, such as your pet, an object in your house, etc., will not be captured by the original model. This has led to interest in how to inject new visual concepts, bound to a new text token, using as few as 4-6 examples. Despite significant progress, this task remains a formidable challenge, particularly in preserving the subject's identity. While most researchers attempt to to address this issue by modifying model architectures, our approach takes a data-centric perspective, advocating the modification of data rather than the model itself. We introduce a novel regularization dataset generation strategy on both the text and image level; demonstrating the importance of a rich and structured regularization dataset (automatically generated) to prevent losing text coherence and better identity preservation. The better quality is enabled by allowing up to 5x more fine-tuning iterations without overfitting and degeneration. The generated renditions of the desired subject preserve even fine details such as text and logos; all while maintaining the ability to generate diverse samples that follow the input text prompt. Since our method focuses on data augmentation, rather than adjusting the model architecture, it is complementary and can be combined with prior work. We show on established benchmarks that our data-centric approach forms the new state of the art in terms of image quality, with the best trade-off between identity preservation, diversity, and text alignment.
</details>
<details>
<summary>摘要</summary>
大型文本到图像模型已经革命化了使用自然语言生成图像的能力。然而，特别是Unique或个人视觉概念，如你的宠物或家中的物品等，原始模型将不会捕捉它们。这 hath led to interest in how to inject new visual concepts, bound to a new text token, using as few as 4-6 examples. Despite significant progress, this task remains a formidable challenge, particularly in preserving the subject's identity. While most researchers attempt to address this issue by modifying model architectures, our approach takes a data-centric perspective, advocating the modification of data rather than the model itself. We introduce a novel regularization dataset generation strategy on both the text and image level; demonstrating the importance of a rich and structured regularization dataset (automatically generated) to prevent losing text coherence and better identity preservation. The better quality is enabled by allowing up to 5x more fine-tuning iterations without overfitting and degeneration. The generated renditions of the desired subject preserve even fine details such as text and logos; all while maintaining the ability to generate diverse samples that follow the input text prompt. Since our method focuses on data augmentation, rather than adjusting the model architecture, it is complementary and can be combined with prior work. We show on established benchmarks that our data-centric approach forms the new state of the art in terms of image quality, with the best trade-off between identity preservation, diversity, and text alignment.
</details></li>
</ul>
<hr>
<h2 id="Holistic-Evaluation-of-Text-To-Image-Models"><a href="#Holistic-Evaluation-of-Text-To-Image-Models" class="headerlink" title="Holistic Evaluation of Text-To-Image Models"></a>Holistic Evaluation of Text-To-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04287">http://arxiv.org/abs/2311.04287</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanford-crfm/helm">https://github.com/stanford-crfm/helm</a></li>
<li>paper_authors: Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, Minguk Kang, Taesung Park, Jure Leskovec, Jun-Yan Zhu, Li Fei-Fei, Jiajun Wu, Stefano Ermon, Percy Liang</li>
<li>For: The paper aims to provide a comprehensive evaluation of text-to-image models to better understand their capabilities and risks.* Methods: The paper introduces a new benchmark called Holistic Evaluation of Text-to-Image Models (HEIM), which assesses 12 aspects of text-to-image models, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency.* Results: The paper evaluates 26 state-of-the-art text-to-image models on the HEIM benchmark and finds that no single model excels in all aspects, with different models demonstrating different strengths. The results are released along with the generated images and human evaluation results for full transparency.Here are the three information points in Simplified Chinese text:* For: 这篇论文目的是为了提供文本映射模型的全面评估，以更好地了解它们的能力和风险。* Methods: 这篇论文引入了一个新的评估板准，名为整体评估文本映射模型（HEIM），它评估了12个方面，包括文本映射、图像质量、美学、创新、逻辑、知识、偏见、不公正、鲁莽、多语言和效率。* Results: 这篇论文评估了26种当前最佳文本映射模型，发现没有一个模型在所有方面都表现出优异，不同的模型在不同的方面都有不同的优势。结果发布在<a target="_blank" rel="noopener" href="https://crfm.stanford.edu/heim/v1.1.0%E5%92%8Chttps://github.com/stanford-crfm/helm%EF%BC%8C%E5%B9%B6%E4%B8%8EHELM%E4%BB%A3%E7%A0%81%E9%9B%86%E6%88%90%E3%80%82">https://crfm.stanford.edu/heim/v1.1.0和https://github.com/stanford-crfm/helm，并与HELM代码集成。</a><details>
<summary>Abstract</summary>
The stunning qualitative improvement of recent text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Whereas previous evaluations focus mostly on text-image alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/v1.1.0 and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase.
</details>
<details>
<summary>摘要</summary>
“文本至图模型最近的精彩质量提升，吸引了广泛的关注和应用。然而，我们缺乏全面的量化理解这些模型的能力和风险。为了填补这个空白，我们介绍了一个新的标准测试套件，即整体评估文本至图模型（HEIM）。以前的评估主要集中在文本图像对齐和图像质量上，而我们已经识别出12个方面，包括文本图像对齐、图像质量、美学、创新、理解、知识、偏见、恶意、不公正、Robustness、多语言和效率。我们编辑了62个场景，涵盖这些方面，并对26种当前最佳文本至图模型进行了评估。我们发现没有任何模型在所有方面都表现出色，不同的模型在不同的方面具有不同的优势。我们在https://crfm.stanford.edu/heim/v1.1.0发布了生成的图像和人工评估结果，以便具有全面的透明度。”Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Video-Instance-Matting"><a href="#Video-Instance-Matting" class="headerlink" title="Video Instance Matting"></a>Video Instance Matting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04212">http://arxiv.org/abs/2311.04212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shi-labs/vim">https://github.com/shi-labs/vim</a></li>
<li>paper_authors: Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Humphrey Shi<br>for:* The paper is written for the task of video instance matting, which involves estimating alpha mattes for each instance in a video frame.methods:* The proposed method, called MSG-VIM, uses a Mask Sequence Guided Video Instance Matting neural network to tackle the challenging problem of video instance matting.* The method incorporates a mixture of mask augmentations to improve the robustness of alpha matte predictions, and includes temporal mask and temporal feature guidance to improve the temporal consistency of the predictions.results:* The proposed model MSG-VIM sets a strong baseline on the VIM50 benchmark and outperforms existing methods by a large margin.Here is the information in Simplified Chinese text, as requested:for:* 这篇论文是为了解决视频实例抹发问题而写的。methods:* 提议的方法是使用Mask Sequence Guided Video Instance Matting神经网络来解决视频实例抹发问题。* 该方法使用了一种混合的面谱增强来提高抹发预测的稳定性，并包括时间面和时间特征引导来提高时间一致性。results:* 提议的模型MSG-VIM在VIM50 benchmark上设置了强大的基线，并与现有方法相比有大幅度的提升。<details>
<summary>Abstract</summary>
Conventional video matting outputs one alpha matte for all instances appearing in a video frame so that individual instances are not distinguished. While video instance segmentation provides time-consistent instance masks, results are unsatisfactory for matting applications, especially due to applied binarization. To remedy this deficiency, we propose Video Instance Matting~(VIM), that is, estimating alpha mattes of each instance at each frame of a video sequence. To tackle this challenging problem, we present MSG-VIM, a Mask Sequence Guided Video Instance Matting neural network, as a novel baseline model for VIM. MSG-VIM leverages a mixture of mask augmentations to make predictions robust to inaccurate and inconsistent mask guidance. It incorporates temporal mask and temporal feature guidance to improve the temporal consistency of alpha matte predictions. Furthermore, we build a new benchmark for VIM, called VIM50, which comprises 50 video clips with multiple human instances as foreground objects. To evaluate performances on the VIM task, we introduce a suitable metric called Video Instance-aware Matting Quality~(VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50 benchmark and outperforms existing methods by a large margin. The project is open-sourced at https://github.com/SHI-Labs/VIM.
</details>
<details>
<summary>摘要</summary>
传统的视频剪辑输出一个Alpha照明 для所有出现在视频帧中的实例，因此每个实例都无法分 differentiated。而视频实例分割提供了时间一致的实例掩码，但是结果不满足剪辑应用，特别是因为应用了二进制化。为了解决这个不足，我们提出了视频实例剪辑~(VIM)，即在每帧视频序列中对每个实例进行Alpha照明估计。为了解决这个挑战性的问题，我们提出了Mask Sequence Guided Video Instance Matting~(MSG-VIM) neural network，作为VIM的基线模型。MSG-VIM利用了混合的掩码更新，以使其预测结果具有对不准确和不一致的掩码指导的robustness。它还包括时间掩码和时间特征指导，以改进Alpha照明预测的时间一致性。此外，我们建立了一个新的VIM benchmark，称为VIM50，该benchmark包括50个视频clip，每个clip都有多个人脸作为前景对象。为了评估VIM任务的性能，我们提出了一个适合的 metric，称为视频实例相关的剪辑照明质量~(VIMQ)。我们的提案模型MSG-VIM在VIM50 bencmark上设置了一个强大的基线，并在现有方法之上出现大幅度的超越。项目开源在https://github.com/SHI-Labs/VIM。
</details></li>
</ul>
<hr>
<h2 id="Deep-Hashing-via-Householder-Quantization"><a href="#Deep-Hashing-via-Householder-Quantization" class="headerlink" title="Deep Hashing via Householder Quantization"></a>Deep Hashing via Householder Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04207">http://arxiv.org/abs/2311.04207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twistedcubic/learn-to-hash">https://github.com/twistedcubic/learn-to-hash</a></li>
<li>paper_authors: Lucas R. Schwengber, Lucas Resende, Paulo Orenstein, Roberto I. Oliveira</li>
<li>For: The paper is written for large-scale image similarity search, specifically to improve the performance of deep hashing algorithms through a new quantization strategy.* Methods: The paper proposes an alternative quantization strategy that decomposes the learning problem into two stages: first, perform similarity learning over the embedding space with no quantization, and second, find an optimal orthogonal transformation of the embeddings using Householder matrices to efficiently leverage stochastic gradient descent.* Results: The proposed algorithm leads to state-of-the-art performance on widely used image datasets, and brings consistent improvements in performance to existing deep hashing algorithms, without any hyperparameter tuning.Here’s the simplified Chinese text for the three key information points:</li>
<li>for: 这篇论文是为大规模图像相似搜索而写的，具体是为了改进深度哈希算法的性能。</li>
<li>methods: 论文提出了一种新的量化策略，即将学习问题分解成两个阶段：首先在嵌入空间上进行相似学习，然后使用抽象矩阵来高效地利用随机梯度下降来找到最佳正交变换。</li>
<li>results: 提出的算法在广泛使用的图像数据集上达到了状态级表现，并且能够在现有的深度哈希算法上带来一致的性能提升，无需进行任何超参数调整。<details>
<summary>Abstract</summary>
Hashing is at the heart of large-scale image similarity search, and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step, a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries, e.g., -1 or 1). Still, the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages: first, perform similarity learning over the embedding space with no quantization; second, find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign, and then quantize the transformed embedding through the sign function. In the second step, we parametrize orthogonal transformations using Householder matrices to efficiently leverage stochastic gradient descent. Since similarity measures are usually invariant under orthogonal transformations, this quantization strategy comes at no cost in terms of performance. The resulting algorithm is unsupervised, fast, hyperparameter-free and can be run on top of any existing deep hashing or metric learning algorithm. We provide extensive experimental results showing that this approach leads to state-of-the-art performance on widely used image datasets, and, unlike other quantization strategies, brings consistent improvements in performance to existing deep hashing algorithms.
</details>
<details>
<summary>摘要</summary>
哈希算法在大规模图像相似搜索中扮演着重要的角色，而现代方法通常通过深度学习技术进行改进。这些算法通常学习维度 embedding，以避免后续的costly binarization步骤。常见的解决方案是使用相似学习项和量化罚项的损失函数，以确保相似图像被分配到靠近 embedding 的位置。然而，这两个项的交互可能使学习更加困难，并且 embedding 更差。我们提出一种 alternating quantization 策略，即先在 embedding 空间进行相似学习，然后使用 Householder 矩阵来找到最佳的正交变换，使每个 embedding 坐标与其相应的 sign 很近。最后，对 transformed embedding 进行量化，使用 sign 函数。在第二步中，我们使用 Householder 矩阵来Parametrize orthogonal transformations，可以高效地使用批量梯度下降。由于相似度度量通常是对 orthogonal transformation 的 invariant，这种量化策略不会对性能产生损失。这种算法是无监督的，快速的，无参数的，可以在任何现有的深度哈希或度量学习算法之上运行。我们在广泛的实验中展示了这种方法可以在常用的图像 dataset 上达到状态级的表现，并且与其他量化策略不同，对现有的深度哈希算法带来了一致的性能提升。
</details></li>
</ul>
<hr>
<h2 id="High-fidelity-3D-Reconstruction-of-Plants-using-Neural-Radiance-Field"><a href="#High-fidelity-3D-Reconstruction-of-Plants-using-Neural-Radiance-Field" class="headerlink" title="High-fidelity 3D Reconstruction of Plants using Neural Radiance Field"></a>High-fidelity 3D Reconstruction of Plants using Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04154">http://arxiv.org/abs/2311.04154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kewei Hu, Ying Wei, Yaoqiang Pan, Hanwen Kang, Chao Chen</li>
<li>for: 本研究旨在探讨Neural Radiance Field（NeRF）在减少农业垦扰领域中的应用，特别是在植物形态 reconstruction和新视图图像生成等两个基本任务中。</li>
<li>methods: 本研究使用了NeRF技术，包括Instant-NGP和Instant-NSR两种state-of-the-art方法，以生成高质量的图像和精准的植物模型。</li>
<li>results: 实验结果表明，NeRF在植物形态 reconstruction和新视图图像生成任务中表现出色，能够与商业软件Reality Capture相当。然而，研究也发现NeRF在某些情况下存在较慢的训练速度、不足的样本导致的性能局限性和复杂场景中的几何质量问题。<details>
<summary>Abstract</summary>
Accurate reconstruction of plant phenotypes plays a key role in optimising sustainable farming practices in the field of Precision Agriculture (PA). Currently, optical sensor-based approaches dominate the field, but the need for high-fidelity 3D reconstruction of crops and plants in unstructured agricultural environments remains challenging. Recently, a promising development has emerged in the form of Neural Radiance Field (NeRF), a novel method that utilises neural density fields. This technique has shown impressive performance in various novel vision synthesis tasks, but has remained relatively unexplored in the agricultural context. In our study, we focus on two fundamental tasks within plant phenotyping: (1) the synthesis of 2D novel-view images and (2) the 3D reconstruction of crop and plant models. We explore the world of neural radiance fields, in particular two SOTA methods: Instant-NGP, which excels in generating high-quality images with impressive training and inference speed, and Instant-NSR, which improves the reconstructed geometry by incorporating the Signed Distance Function (SDF) during training. In particular, we present a novel plant phenotype dataset comprising real plant images from production environments. This dataset is a first-of-its-kind initiative aimed at comprehensively exploring the advantages and limitations of NeRF in agricultural contexts. Our experimental results show that NeRF demonstrates commendable performance in the synthesis of novel-view images and is able to achieve reconstruction results that are competitive with Reality Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based reconstruction. However, our study also highlights certain drawbacks of NeRF, including relatively slow training speeds, performance limitations in cases of insufficient sampling, and challenges in obtaining geometry quality in complex setups.
</details>
<details>
<summary>摘要</summary>
准确重建植物形态在精度农业中发挥关键作用。目前，光学传感器方法在这个领域占据主导地位，但是在无结构农业环境中高精度三维重建植物和植物模型的需求仍然是挑战。在最近的研究中，我们发现了一种有前途的发展：神经辐射场（NeRF）。这种技术利用神经density场，并在不同的视觉合成任务中表现出色。然而，在农业上这种技术还尚未得到广泛的探索。在我们的研究中，我们关注了两个基本的植物形态检测任务：（1）生成新视图图像，和（2）三维重建农作物和植物模型。我们深入探讨神经辐射场的世界，尤其是两种现状顶尖方法：Instant-NGP和Instant-NSR。Instant-NGP可以在快速训练和执行速度之下生成高质量图像，而Instant-NSR通过在训练中包含签名距离函数（SDF）来提高重建几何的质量。特别是，我们提供了一个新的植物形态数据集，包括来自生产环境的真实植物图像。这个数据集是农业领域的首次尝试，旨在全面探讨神经辐射场在农业上的优势和局限性。我们的实验结果表明，NeRF在生成新视图图像和三维重建农作物和植物模型方面具有很好的表现，能够与市场领先的3D多视点雷达（MVS）重建软件相匹配。然而，我们的研究也显示了NeRF的一些缺点，包括较慢的训练速度、不足的抽象情况下的表现局限性和复杂配置下的几何质量问题。
</details></li>
</ul>
<hr>
<h2 id="I2VGen-XL-High-Quality-Image-to-Video-Synthesis-via-Cascaded-Diffusion-Models"><a href="#I2VGen-XL-High-Quality-Image-to-Video-Synthesis-via-Cascaded-Diffusion-Models" class="headerlink" title="I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models"></a>I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04145">http://arxiv.org/abs/2311.04145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-vilab/i2vgen-xl">https://github.com/damo-vilab/i2vgen-xl</a></li>
<li>paper_authors: Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, Jingren Zhou</li>
<li>for: 这个论文主要是为了提高视频生成的Semantic精度和Quality。</li>
<li>methods: 这个论文提出了一种名为I2VGen-XL的方法，它包括两个阶段：基础阶段和精度阶段。基础阶段使用两个层次编码器来保持输入图像的准确性和内容，而精度阶段通过添加额外的简短文本来提高视频的细节和分辨率（1280×720）。</li>
<li>results: 经过广泛的实验，I2VGen-XL可以同时提高视频的Semantic精度、细节连续性和清晰度。与当前的顶尖方法进行比较，I2VGen-XL也表现出了其效果。<details>
<summary>Abstract</summary>
Video synthesis has recently made remarkable strides benefiting from the rapid development of diffusion models. However, it still encounters challenges in terms of semantic accuracy, clarity and spatio-temporal continuity. They primarily arise from the scarcity of well-aligned text-video data and the complex inherent structure of videos, making it difficult for the model to simultaneously ensure semantic and qualitative excellence. In this report, we propose a cascaded I2VGen-XL approach that enhances model performance by decoupling these two factors and ensures the alignment of the input data by utilizing static images as a form of crucial guidance. I2VGen-XL consists of two stages: i) the base stage guarantees coherent semantics and preserves content from input images by using two hierarchical encoders, and ii) the refinement stage enhances the video's details by incorporating an additional brief text and improves the resolution to 1280$\times$720. To improve the diversity, we collect around 35 million single-shot text-video pairs and 6 billion text-image pairs to optimize the model. By this means, I2VGen-XL can simultaneously enhance the semantic accuracy, continuity of details and clarity of generated videos. Through extensive experiments, we have investigated the underlying principles of I2VGen-XL and compared it with current top methods, which can demonstrate its effectiveness on diverse data. The source code and models will be publicly available at \url{https://i2vgen-xl.github.io}.
</details>
<details>
<summary>摘要</summary>
视频合成在最近几年内做出了很多出色的进步，受 diffusion 模型的快速发展的推动。然而，它仍然面临 semantic 精度、清晰度和空间时间连续性等挑战。这些挑战主要来自于缺乏准确的文本视频数据和视频的复杂内在结构，使得模型很难同时保证 semantic 和质量上的优秀表现。在这份报告中，我们提出了一种名为 I2VGen-XL 的方法，该方法通过分解这两个因素来提高模型性能，并使用静止图片作为关键指导。I2VGen-XL 包括两个阶段：首先，基础阶段保证文本和图片之间的协调性，并使用两层嵌入器保持输入图片的内容一致性；其次，改进阶段通过添加一个短文本和提高分辨率来提高视频的细节，并将分辨率提高到 1280×720。为了提高多样性，我们收集了约 35 万个单个文本视频对和 6 亿个文本图片对，以便优化模型。通过这些方式，I2VGen-XL 可以同时提高 semantic 精度、视频细节的连续性和清晰度。经过广泛的实验，我们发现 I2VGen-XL 的基本原理和现代顶峰方法的效果，并可以在多种数据上证明其效果。模型和代码将在 \url{https://i2vgen-xl.github.io} 上公开。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Quality-Improvement-in-Videoconferencing-using-Keyframes-based-GAN"><a href="#Perceptual-Quality-Improvement-in-Videoconferencing-using-Keyframes-based-GAN" class="headerlink" title="Perceptual Quality Improvement in Videoconferencing using Keyframes-based GAN"></a>Perceptual Quality Improvement in Videoconferencing using Keyframes-based GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04263">http://arxiv.org/abs/2311.04263</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lorenzoagnolucci/keyframes-gan">https://github.com/lorenzoagnolucci/keyframes-gan</a></li>
<li>paper_authors: Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo</li>
<li>for: 提高视频会议中的视觉质量</li>
<li>methods: 使用 Generative Adversarial Networks (GANs) 技术，基于人脸特征点进行多尺度特征提取，并在进程中进行恢复高频率细节</li>
<li>results: 实验表明，提出的方法可以提高视觉质量，并生成高度真实的结果，即使压缩率高。In English:</li>
<li>for: Improving visual quality in video conferencing</li>
<li>methods: Using Generative Adversarial Networks (GANs) technology, based on facial landmarks for multi-scale feature extraction, and progressive restoration of high-frequency details lost after video compression</li>
<li>results: Experiments show that the proposed method can improve visual quality and generate photo-realistic results, even with high compression rates.<details>
<summary>Abstract</summary>
In the latest years, videoconferencing has taken a fundamental role in interpersonal relations, both for personal and business purposes. Lossy video compression algorithms are the enabling technology for videoconferencing, as they reduce the bandwidth required for real-time video streaming. However, lossy video compression decreases the perceived visual quality. Thus, many techniques for reducing compression artifacts and improving video visual quality have been proposed in recent years. In this work, we propose a novel GAN-based method for compression artifacts reduction in videoconferencing. Given that, in this context, the speaker is typically in front of the camera and remains the same for the entire duration of the transmission, we can maintain a set of reference keyframes of the person from the higher-quality I-frames that are transmitted within the video stream and exploit them to guide the visual quality improvement; a novel aspect of this approach is the update policy that maintains and updates a compact and effective set of reference keyframes. First, we extract multi-scale features from the compressed and reference frames. Then, our architecture combines these features in a progressive manner according to facial landmarks. This allows the restoration of the high-frequency details lost after the video compression. Experiments show that the proposed approach improves visual quality and generates photo-realistic results even with high compression rates. Code and pre-trained networks are publicly available at https://github.com/LorenzoAgnolucci/Keyframes-GAN.
</details>
<details>
<summary>摘要</summary>
在最近几年，视频会议已经扮演了人际关系中的重要角色，不仅在个人交流方面，还在商业方面。损失式视频压缩算法是视频会议的关键技术，它降低了实时视频流的带宽需求。然而，损失式视频压缩会降低视频的视觉质量。因此，许多降低压缩残差和提高视频视觉质量的技术已经在最近几年被提出。在这种情况下，我们提出了一种基于GAN的新方法，用于压缩残差降低在视频会议中。由于speaker通常会在摄像头前面并保持不变的整个传输时间，我们可以维护一组高质量I帧中的人数据，并利用它们来导航视觉质量的提高。这种方法的新特点是更新策略，用于维护和更新一个高效和紧凑的人数据集。首先，我们从压缩和参照帧中提取多尺度特征。然后，我们的架构将这些特征进行进度式组合，根据人脸特征来确定。这样可以恢复压缩后丢失的高频环境信息。实验表明，我们的方法可以提高视觉质量，并生成高度真实的结果，即使压缩率很高。代码和预训练网络可以在https://github.com/LorenzoAgnolucci/Keyframes-GAN上获取。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Semantic-Map-Representation-for-Skill-based-Visual-Object-Navigation"><a href="#Interactive-Semantic-Map-Representation-for-Skill-based-Visual-Object-Navigation" class="headerlink" title="Interactive Semantic Map Representation for Skill-based Visual Object Navigation"></a>Interactive Semantic Map Representation for Skill-based Visual Object Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04107">http://arxiv.org/abs/2311.04107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatiana Zemskova, Aleksei Staroverov, Kirill Muravyev, Dmitry Yudin, Aleksandr Panov</li>
<li>for: 本研究旨在提出一种基于学习方法的移动机器人视觉对象导航技术。</li>
<li>methods: 该技术基于神经网络方法，在推理过程中调整分割模型的权重，使用反propagation计算预测的融合损失值。</li>
<li>results: 实验结果显示，该技术在Habitat环境中 Navigation metric上表现出了明显的优势，与现有方法相比。代码和自定义数据集在github.com&#x2F;AIRI-Institute&#x2F;skill-fusion上公开发布。<details>
<summary>Abstract</summary>
Visual object navigation using learning methods is one of the key tasks in mobile robotics. This paper introduces a new representation of a scene semantic map formed during the embodied agent interaction with the indoor environment. It is based on a neural network method that adjusts the weights of the segmentation model with backpropagation of the predicted fusion loss values during inference on a regular (backward) or delayed (forward) image sequence. We have implemented this representation into a full-fledged navigation approach called SkillTron, which can select robot skills from end-to-end policies based on reinforcement learning and classic map-based planning methods. The proposed approach makes it possible to form both intermediate goals for robot exploration and the final goal for object navigation. We conducted intensive experiments with the proposed approach in the Habitat environment, which showed a significant superiority in navigation quality metrics compared to state-of-the-art approaches. The developed code and used custom datasets are publicly available at github.com/AIRI-Institute/skill-fusion.
</details>
<details>
<summary>摘要</summary>
Mobile robot视觉对象导航使用学习方法是关键任务之一。这篇论文介绍了一种新的场景semantic地图表示方法，基于神经网络方法，在感知器与室内环境互动中调整分割模型的权重，通过反propagation的预测融合损失值进行推理。我们在SkillTron全面导航方法中实现了这种表示方法，可以根据反射学习和经典地图基本规划方法选择机器人技能。这种方法可以形成机器人探索的中间目标和 Navigation的最终目标。我们在Habitat环境中进行了广泛的实验，并显示了与状态艺术方法相比的导航质量指标显著superiority。我们在github.com/AIRI-Institute/skill-fusion上公开了代码和自定义数据集。
</details></li>
</ul>
<hr>
<h2 id="DeepPatent2-A-Large-Scale-Benchmarking-Corpus-for-Technical-Drawing-Understanding"><a href="#DeepPatent2-A-Large-Scale-Benchmarking-Corpus-for-Technical-Drawing-Understanding" class="headerlink" title="DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding"></a>DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04098">http://arxiv.org/abs/2311.04098</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gofigure-lanl/figure-segmentation">https://github.com/gofigure-lanl/figure-segmentation</a></li>
<li>paper_authors: Kehinde Ajayi, Xin Wei, Martin Gryder, Winston Shields, Jian Wu, Shawn M. Jones, Michal Kucer, Diane Oyen</li>
<li>for: 这个论文主要是为了提供一个大规模的技术图纸 dataset，以便进行 CV 任务的研究和开发。</li>
<li>methods: 这个论文使用了 US 设计专利文档中的技术图纸，通过自动提取对象名称和视图角度来构建 dataset。</li>
<li>results: 这个论文通过使用 DeepPatent2 dataset，实现了技术图纸中的概念描述 task。此外，这个 dataset 还有可能用于其他研究领域，如 3D 图像重建和图像检索。<details>
<summary>Abstract</summary>
Recent advances in computer vision (CV) and natural language processing have been driven by exploiting big data on practical applications. However, these research fields are still limited by the sheer volume, versatility, and diversity of the available datasets. CV tasks, such as image captioning, which has primarily been carried out on natural images, still struggle to produce accurate and meaningful captions on sketched images often included in scientific and technical documents. The advancement of other tasks such as 3D reconstruction from 2D images requires larger datasets with multiple viewpoints. We introduce DeepPatent2, a large-scale dataset, providing more than 2.7 million technical drawings with 132,890 object names and 22,394 viewpoints extracted from 14 years of US design patent documents. We demonstrate the usefulness of DeepPatent2 with conceptual captioning. We further provide the potential usefulness of our dataset to facilitate other research areas such as 3D image reconstruction and image retrieval.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的计算机视觉（CV）和自然语言处理技术的进步主要受到了实际应用的大数据驱动。然而，这些研究领域仍然受限于可用数据的量、多样性和多个视点。CV任务，如图像描述，主要在自然图像上进行，它们在科学和技术文档中出现的草图图像上仍然做出了不准确和没有意义的描述。3D重建从2D图像需要更多的数据集，包括多个视点。我们介绍DeepPatent2，一个大规模数据集，包含超过270万个技术图像，其中有132,890个物品名称和22,394个视点，从14年的美国设计专利文档中提取。我们通过概念描述来证明DeepPatent2的有用性，并提供了这些数据集可能用于其他研究领域，如3D图像重建和图像搜索。
</details></li>
</ul>
<hr>
<h2 id="Image-Pointcloud-Fusion-based-Anomaly-Detection-using-PD-REAL-Dataset"><a href="#Image-Pointcloud-Fusion-based-Anomaly-Detection-using-PD-REAL-Dataset" class="headerlink" title="Image-Pointcloud Fusion based Anomaly Detection using PD-REAL Dataset"></a>Image-Pointcloud Fusion based Anomaly Detection using PD-REAL Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04095">http://arxiv.org/abs/2311.04095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjian Qin, Chunzhi Gu, Jun Yu, Chao Zhang</li>
<li>for: 这个研究是为了开发一个大规模的无监督异常检测（AD） dataset 以测试3D空间中的异常检测能力。</li>
<li>methods: 这个研究使用了Play-Doh模型来生成15种物品类别中的异常，并在不同的照明条件下拍摄了这些物品。实验使用了一款商业可用的RealSense摄像机来拍摄RGB和深度图像。</li>
<li>results: 研究表明，使用3D信息可以增强异常检测的性能，但也存在一些挑战。与现有的3D AD dataset相比，PD-REAL的数据收集方式更加便宜、易于扩展和控制变量。<details>
<summary>Abstract</summary>
We present PD-REAL, a novel large-scale dataset for unsupervised anomaly detection (AD) in the 3D domain. It is motivated by the fact that 2D-only representations in the AD task may fail to capture the geometric structures of anomalies due to uncertainty in lighting conditions or shooting angles. PD-REAL consists entirely of Play-Doh models for 15 object categories and focuses on the analysis of potential benefits from 3D information in a controlled environment. Specifically, objects are first created with six types of anomalies, such as dent, crack, or perforation, and then photographed under different lighting conditions to mimic real-world inspection scenarios. To demonstrate the usefulness of 3D information, we use a commercially available RealSense camera to capture RGB and depth images. Compared to the existing 3D dataset for AD tasks, the data acquisition of PD-REAL is significantly cheaper, easily scalable and easier to control variables. Extensive evaluations with state-of-the-art AD algorithms on our dataset demonstrate the benefits as well as challenges of using 3D information. Our dataset can be downloaded from https://github.com/Andy-cs008/PD-REAL
</details>
<details>
<summary>摘要</summary>
我们介绍PD-REAL，一个新的大规模不监督异常检测（AD）数据集在三维领域。它受到了2D只 representation在AD任务中可能无法捕捉异常的geometry结构的不确定性的光照条件或拍摄角度的影响。PD-REAL完全由Play-Doh模型组成15种对象类，集中在控制环境中对3D信息的分析。具体来说，物体首先创建了6种异常，如损害、裂缝、或者洞，然后在不同的照明条件下拍摄，以模拟真实世界检测场景。为了证明3D信息的用于，我们使用了一个商业可用的RealSense摄像头捕摄RGB和深度图像。与现有的3D数据集 дляAD任务相比，PD-REAL数据收集的更加便宜、扩展可控和更容易控制变量。我们对state-of-the-art AD算法进行了广泛的评估，并证明了使用3D信息的好处以及挑战。PD-REAL数据集可以从https://github.com/Andy-cs008/PD-REAL下载。
</details></li>
</ul>
<hr>
<h2 id="Proceedings-of-the-5th-International-Workshop-on-Reading-Music-Systems"><a href="#Proceedings-of-the-5th-International-Workshop-on-Reading-Music-Systems" class="headerlink" title="Proceedings of the 5th International Workshop on Reading Music Systems"></a>Proceedings of the 5th International Workshop on Reading Music Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04091">http://arxiv.org/abs/2311.04091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suziai/gui-tools">https://github.com/suziai/gui-tools</a></li>
<li>paper_authors: Jorge Calvo-Zaragoza, Alexander Pacha, Elona Shatri</li>
<li>for: The workshop aims to connect researchers developing music reading systems with other researchers and practitioners who could benefit from these systems, such as librarians or musicologists.</li>
<li>methods: The workshop covers a wide range of topics, including optical music recognition, image processing on music scores, writer identification, and multi-modal systems.</li>
<li>results: The proceedings of the 5th International Workshop on Reading Music Systems, held in Milan, Italy on Nov. 4th 2023, showcase the latest research and developments in the field of music reading systems.<details>
<summary>Abstract</summary>
The International Workshop on Reading Music Systems (WoRMS) is a workshop that tries to connect researchers who develop systems for reading music, such as in the field of Optical Music Recognition, with other researchers and practitioners that could benefit from such systems, like librarians or musicologists. The relevant topics of interest for the workshop include, but are not limited to: Music reading systems; Optical music recognition; Datasets and performance evaluation; Image processing on music scores; Writer identification; Authoring, editing, storing and presentation systems for music scores; Multi-modal systems; Novel input-methods for music to produce written music; Web-based Music Information Retrieval services; Applications and projects; Use-cases related to written music.   These are the proceedings of the 5th International Workshop on Reading Music Systems, held in Milan, Italy on Nov. 4th 2023.
</details>
<details>
<summary>摘要</summary>
世界音乐读取系统国际研讨会（WoRMS）是一个研讨会，旨在联系开发音乐读取系统的研究人员（如光学音乐识别）与其他研究人员和实践者（如图书管理员或音乐学家），以便互相交流和合作。研讨会的主要关注领域包括，但不限于：音乐读取系统；光学音乐识别；数据集和性能评估；音乐手稿图像处理；作者标识；作品编写、编辑、存储和展示系统；多模态系统；新的音乐输入方法生成书面音乐；网络音乐信息检索服务；应用和项目；相关书面音乐应用场景。这是2023年11月4日在意大利米兰举行的5届世界音乐读取系统国际研讨会的论文集。
</details></li>
</ul>
<hr>
<h2 id="Restoration-of-Analog-Videos-Using-Swin-UNet"><a href="#Restoration-of-Analog-Videos-Using-Swin-UNet" class="headerlink" title="Restoration of Analog Videos Using Swin-UNet"></a>Restoration of Analog Videos Using Swin-UNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04261">http://arxiv.org/abs/2311.04261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miccunifi/analog-video-restoration">https://github.com/miccunifi/analog-video-restoration</a></li>
<li>paper_authors: Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini, Alberto Del Bimbo</li>
<li>for:  restore analog videos of historical archives</li>
<li>methods: multi-frame approach, able to deal with severe tape mistracking</li>
<li>results: effective restoration of original content, tested on real-world videos from a major historical video archive<details>
<summary>Abstract</summary>
In this paper, we present a system to restore analog videos of historical archives. These videos often contain severe visual degradation due to the deterioration of their tape supports that require costly and slow manual interventions to recover the original content. The proposed system uses a multi-frame approach and is able to deal with severe tape mistracking, which results in completely scrambled frames. Tests on real-world videos from a major historical video archive show the effectiveness of our demo system. The code and the pre-trained model are publicly available at https://github.com/miccunifi/analog-video-restoration.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种系统，用于恢复历史档案中的分析视频。这些视频经常受到媒体支持的腐蚀，导致视频内容严重损坏，需要费时费力的手动干预来恢复原始内容。我们提出的系统采用多帧方法，能够处理严重的卷积问题，这会导致整个帧完全排序。我们对历史视频档案中的真实视频进行测试，显示了我们的demo系统的效果。我们在GitHub上公开了代码和预训练模型，请参考https://github.com/miccunifi/analog-video-restoration。
</details></li>
</ul>
<hr>
<h2 id="Learning-Super-Resolution-Ultrasound-Localization-Microscopy-from-Radio-Frequency-Data"><a href="#Learning-Super-Resolution-Ultrasound-Localization-Microscopy-from-Radio-Frequency-Data" class="headerlink" title="Learning Super-Resolution Ultrasound Localization Microscopy from Radio-Frequency Data"></a>Learning Super-Resolution Ultrasound Localization Microscopy from Radio-Frequency Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04081">http://arxiv.org/abs/2311.04081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Hahne, Georges Chabouh, Olivier Couture, Raphael Sznitman</li>
<li>for: This paper aims to improve the resolution performance of Ultrasound Localization Microscopy (ULM) by bypassing the limitations of Delay-And-Sum (DAS) beamforming and using a super-resolution network to process unprocessed Radio-Frequency (RF) data.</li>
<li>methods: The proposed method involves label projection and inverse point transformation between B-mode and RF coordinate space to facilitate the use of RF data in the super-resolution network.</li>
<li>results: The results from the RF-trained network suggest that excluding DAS beamforming offers a great potential to optimize the ULM resolution performance, outperforming state-of-the-art techniques based on a public dataset featuring in silico and in vivo data.<details>
<summary>Abstract</summary>
Ultrasound Localization Microscopy (ULM) enables imaging of vascular structures in the micrometer range by accumulating contrast agent particle locations over time. Precise and efficient target localization accuracy remains an active research topic in the ULM field to further push the boundaries of this promising medical imaging technology. Existing work incorporates Delay-And-Sum (DAS) beamforming into particle localization pipelines, which ultimately determines the ULM image resolution capability. In this paper we propose to feed unprocessed Radio-Frequency (RF) data into a super-resolution network while bypassing DAS beamforming and its limitations. To facilitate this, we demonstrate label projection and inverse point transformation between B-mode and RF coordinate space as required by our approach. We assess our method against state-of-the-art techniques based on a public dataset featuring in silico and in vivo data. Results from our RF-trained network suggest that excluding DAS beamforming offers a great potential to optimize on the ULM resolution performance.
</details>
<details>
<summary>摘要</summary>
美元声波地图（ULM）可以在微米范围内成像血管结构，通过时间积累对比剂粒子的位置。在ULM领域，精准和高效的目标Localization精度仍然是活跃的研究话题，以进一步推动这种承诺的医学成像技术。现有的工作将延迟和总和（DAS）扩散加入了带粒子Localization管道， ultimately determines the ULM图像分辨率能力。在这篇文章中，我们提议将未处理的电磁波（RF）数据Feed into a super-resolution网络，而不是通过DAS扩散和其局限性。为此，我们示出了标签投影和反向点变换 междуB模式和RF坐标空间，以便实现我们的方法。我们根据公共数据集进行比较，结果表明，不包括DAS扩散可以优化ULM的分辨率性能。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Lane-Perception-and-Topology-Understanding-with-Standard-Definition-Navigation-Maps"><a href="#Augmenting-Lane-Perception-and-Topology-Understanding-with-Standard-Definition-Navigation-Maps" class="headerlink" title="Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps"></a>Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04079">http://arxiv.org/abs/2311.04079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katie Z Luo, Xinshuo Weng, Yan Wang, Shuang Wu, Jie Li, Kilian Q Weinberger, Yue Wang, Marco Pavone</li>
<li>for: 这种研究旨在替代高解度地图，提高自动驾驶系统的扩展性和可持续性。</li>
<li>methods: 该研究提出了一种新的框架，使得标准分辨率地图（SD map）可以与在线地图预测结合使用，并使用Transformer编码器来利用SD map中的知识来优化干道 topology 预测。</li>
<li>results: 研究表明，该方法可以在当前状态静态方法上提高干道检测和 topology 预测率，最高提高60%，而且不需要额外的硬件或软件。<details>
<summary>Abstract</summary>
Autonomous driving has traditionally relied heavily on costly and labor-intensive High Definition (HD) maps, hindering scalability. In contrast, Standard Definition (SD) maps are more affordable and have worldwide coverage, offering a scalable alternative. In this work, we systematically explore the effect of SD maps for real-time lane-topology understanding. We propose a novel framework to integrate SD maps into online map prediction and propose a Transformer-based encoder, SD Map Encoder Representations from transFormers, to leverage priors in SD maps for the lane-topology prediction task. This enhancement consistently and significantly boosts (by up to 60%) lane detection and topology prediction on current state-of-the-art online map prediction methods without bells and whistles and can be immediately incorporated into any Transformer-based lane-topology method. Code is available at https://github.com/NVlabs/SMERF.
</details>
<details>
<summary>摘要</summary>
自主驾驶曾然依赖高Definition（HD）地图，导致扩展性受限。相比之下，标准Definition（SD）地图更加经济，具有全球覆盖，提供了可扩展的alternative。在这种工作中，我们系统地探讨了SD地图对实时路径理解的效果。我们提出了一种将SD地图 integrate into online map prediction的方案，并提出了基于Transformer的编码器，即SD Map Encoder Representations from transFormers，以利用SD地图中的先验知识 для路径预测任务。这种提高可以在当前领先的online map prediction方法上 consistently 和 significantly 提高（最高60%）路径检测和路径预测，而不需要额外的资源和复杂的设计。代码可以在https://github.com/NVlabs/SMERF上获取。
</details></li>
</ul>
<hr>
<h2 id="Energy-based-Calibrated-VAE-with-Test-Time-Free-Lunch"><a href="#Energy-based-Calibrated-VAE-with-Test-Time-Free-Lunch" class="headerlink" title="Energy-based Calibrated VAE with Test Time Free Lunch"></a>Energy-based Calibrated VAE with Test Time Free Lunch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04071">http://arxiv.org/abs/2311.04071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Luo, Siya Qiu, Xingjian Tao, Yujun Cai, Jing Tang</li>
<li>for: 提高 Variational Autoencoders (VAEs) 的生成质量和效率，使其能够在数据和精度培aual samples中进行准确的生成。</li>
<li>methods: 提出了一种 Conditional EBM 模型，通过在训练时对生成方向进行准确的调整，使得生成模型可以在数据和精度培aual samples中进行高质量的生成，而无需在推断阶段进行MCMC抽样。</li>
<li>results: 通过了多种应用，包括图像生成和零码图像修复，并且对比了单步非对抗生成，显示了提出的方法的状态级表现。<details>
<summary>Abstract</summary>
In this paper, we propose a novel Energy-Calibrated Generative Model that utilizes a Conditional EBM for enhancing Variational Autoencoders (VAEs). VAEs are sampling efficient but often suffer from blurry generation results due to the lack of training in the generative direction. On the other hand, Energy-Based Models (EBMs) can generate high-quality samples but require expensive Markov Chain Monte Carlo (MCMC) sampling. To address these issues, we introduce a Conditional EBM for calibrating the generative direction during training, without requiring it for test time sampling. Our approach enables the generative model to be trained upon data and calibrated samples with adaptive weight, thereby enhancing efficiency and effectiveness without necessitating MCMC sampling in the inference phase. We also show that the proposed approach can be extended to calibrate normalizing flows and variational posterior. Moreover, we propose to apply the proposed method to zero-shot image restoration via neural transport prior and range-null theory. We demonstrate the effectiveness of the proposed method through extensive experiments in various applications, including image generation and zero-shot image restoration. Our method shows state-of-the-art performance over single-step non-adversarial generation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的能量均衡生成模型，该模型使用了条件的EBM来提高VAEs的效果。VAEs可以快速采样，但它们通常因缺乏生成方向的训练而导致生成结果不清晰。而EBMs可以生成高质量的样本，但它们需要昂贵的Markov Chain Monte Carlo（MCMC）采样。为了解决这些问题，我们引入了一种条件的EBM，以均衡生成方向的训练，无需测试阶段的MCMC采样。我们的方法可以让生成模型在训练和测试阶段都能够受益于高效的采样，而不需要MCMC采样。此外，我们还证明了我们的方法可以扩展到均衡流程和变量 posterior。此外，我们还提出了在zero-shot图像恢复中应用我们的方法，使用神经运输先验和范围-null理论。我们的实验结果表明，我们的方法可以在不同的应用中达到顶峰性能，包括图像生成和zero-shot图像恢复。
</details></li>
</ul>
<hr>
<h2 id="LISBET-a-self-supervised-Transformer-model-for-the-automatic-segmentation-of-social-behavior-motifs"><a href="#LISBET-a-self-supervised-Transformer-model-for-the-automatic-segmentation-of-social-behavior-motifs" class="headerlink" title="LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs"></a>LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04069">http://arxiv.org/abs/2311.04069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Chindemi, Benoit Girard, Camilla Bellone<br>for: 本研究旨在更深入地理解社交行为的核心原理，以便更好地理解社交异常的 neural 基础。methods: 本研究使用 LISBET（seLf-supervIsed Social BEhavioral Transformer）模型，通过无需人工标注和特征选择来检测和分类社交互动。results: 研究发现，使用发现驱动模式可以准确地识别和分类社交行为模式，并且与人类注释高度相关。此外，研究还发现了 dopaminergic  neurons 的电physiological 活动与社交行为模式相关。<details>
<summary>Abstract</summary>
Social behavior, defined as the process by which individuals act and react in response to others, is crucial for the function of societies and holds profound implications for mental health. To fully grasp the intricacies of social behavior and identify potential therapeutic targets for addressing social deficits, it is essential to understand its core principles. Although machine learning algorithms have made it easier to study specific aspects of complex behavior, current methodologies tend to focus primarily on single-animal behavior. In this study, we introduce LISBET (seLf-supervIsed Social BEhavioral Transformer), a model designed to detect and segment social interactions. Our model eliminates the need for feature selection and extensive human annotation by using self-supervised learning to detect and quantify social behaviors from dynamic body parts tracking data. LISBET can be used in hypothesis-driven mode to automate behavior classification using supervised finetuning, and in discovery-driven mode to segment social behavior motifs using unsupervised learning. We found that motifs recognized using the discovery-driven approach not only closely match the human annotations but also correlate with the electrophysiological activity of dopaminergic neurons in the Ventral Tegmental Area (VTA). We hope LISBET will help the community improve our understanding of social behaviors and their neural underpinnings.
</details>
<details>
<summary>摘要</summary>
社交行为，定义为个体在响应他人时的行为过程，对社会的 функion 至关重要，对 mental health 也有深刻的意义。为了全面理解社交行为的复杂性和找到可能的治疗目标，需要了解其核心原理。虽然机器学习算法已经使得研究特定方面的复杂行为变得更加容易，但现有方法ologies 往往只关注单个动物的行为。在这种研究中，我们介绍了 LISBET（seLf-supervIsed Social BEhavioral Transformer）模型，用于检测和分类社交交互。我们的模型不需要特征选择和大量的人工标注，可以通过自我超vised learning 检测和评估社交行为。LISBET可以在假设驱动模式下用于自动分类行为，以及在发现驱动模式下用于无监督学习分 segment 社交行为模式。我们发现使用发现驱动模式认定的模式和人工标注几乎完全相符，并且与 dopaminergic neurons 的电physiological 活动在 Ventral Tegmental Area (VTA) 也显示相关性。我们希望 LISBET 能帮助社区更好地理解社交行为和其神经基础。
</details></li>
</ul>
<hr>
<h2 id="mmFUSION-Multimodal-Fusion-for-3D-Objects-Detection"><a href="#mmFUSION-Multimodal-Fusion-for-3D-Objects-Detection" class="headerlink" title="mmFUSION: Multimodal Fusion for 3D Objects Detection"></a>mmFUSION: Multimodal Fusion for 3D Objects Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04058">http://arxiv.org/abs/2311.04058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javed Ahmad, Alessio Del Bue</li>
<li>for: 提高自驾车系统中3D物体检测的准确率，通过多感器融合。</li>
<li>methods: 提出了一种新的中间级多模态融合（mmFUSION）方法，使用每个感知器计算特征，然后通过交叉模态和多模态注意力机制进行融合。</li>
<li>results: 在KITTI和NuScenes dataset上测试，mmFUSION比已有的早期、中间期、晚期和两个阶段融合方案表现更好，并且可以保持多模态信息并通过注意力权重补做模态缺失。<details>
<summary>Abstract</summary>
Multi-sensor fusion is essential for accurate 3D object detection in self-driving systems. Camera and LiDAR are the most commonly used sensors, and usually, their fusion happens at the early or late stages of 3D detectors with the help of regions of interest (RoIs). On the other hand, fusion at the intermediate level is more adaptive because it does not need RoIs from modalities but is complex as the features of both modalities are presented from different points of view. In this paper, we propose a new intermediate-level multi-modal fusion (mmFUSION) approach to overcome these challenges. First, the mmFUSION uses separate encoders for each modality to compute features at a desired lower space volume. Second, these features are fused through cross-modality and multi-modality attention mechanisms proposed in mmFUSION. The mmFUSION framework preserves multi-modal information and learns to complement modalities' deficiencies through attention weights. The strong multi-modal features from the mmFUSION framework are fed to a simple 3D detection head for 3D predictions. We evaluate mmFUSION on the KITTI and NuScenes dataset where it performs better than available early, intermediate, late, and even two-stage based fusion schemes. The code with the mmdetection3D project plugin will be publicly available soon.
</details>
<details>
<summary>摘要</summary>
多感器融合是自动驾驶系统中精准三维物体检测的关键。相机和LiDAR是最常用的感知器，通常在3D检测器的早期或晚期阶段进行融合，使用区域关注（RoIs）的帮助。然而，中间阶段的融合更加适应，因为它不需要modalities中的RoIs，但是计算复杂，因为两种感知器的特征从不同的角度表达。在这篇论文中，我们提出了一种新的中间阶段多模态融合（mmFUSION）方法，以解决这些挑战。首先，mmFUSION使用每种感知器的分立编码器计算特征，以达到所需的更低的空间体积。然后，这些特征通过mmFUSION中的对话机制进行融合。mmFUSION框架保留了多模态信息，并通过注意力权重学习补做感知器的不足。从mmFUSION框架获得的强大多模态特征被传递给简单的3D检测头进行3D预测。我们在KITTI和NuScenes dataset上评估了mmFUSION，其表现比已有的早期、中间、晚期和两个阶段融合方案更好。代码将在near future通过mmdetection3D项目插件公开。
</details></li>
</ul>
<hr>
<h2 id="Generative-Structural-Design-Integrating-BIM-and-Diffusion-Model"><a href="#Generative-Structural-Design-Integrating-BIM-and-Diffusion-Model" class="headerlink" title="Generative Structural Design Integrating BIM and Diffusion Model"></a>Generative Structural Design Integrating BIM and Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04052">http://arxiv.org/abs/2311.04052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili He, Yu-Hsing Wang, Jian Zhang</li>
<li>for: 本研究旨在提出一个全面的解决方案，以帮助智能结构设计使用人工智能（AI）更加有效率，并且可能成为未来的设计新平台。</li>
<li>methods: 本研究提出了三个贡献：首先，引入建筑信息模型（BIM）到智能结构设计中，并建立了一个集成BIM和生成AI的结构设计管道，这是之前的框架仅考虑CAD图纸的唯一解决方案。其次，为了提高生成结果的感知质量和细节，本研究引入了 diffusion models（DMs）来取代广泛使用的生成对抗网络（GAN）模型，并提出了一种基于物理条件的 conditional diffusion model（PCDM）来考虑不同的设计前提。最后，本研究还设计了一个注意块（AB），包括一个自注意块（SAB）和一个并行交叉注意块（PCAB），以便跨领域数据的混合。</li>
<li>results: 对比分析表明，PCDM具有强大的生成和表示能力，并且可以考虑不同的设计前提。必要的ablation研究也进行了，以证明方法的有效性。此外，本研究还表明了DMs的潜在取代GANs的潜在性，成为未来生成问题在 civil engineering 领域的新标准。<details>
<summary>Abstract</summary>
Intelligent structural design using AI can effectively reduce time overhead and increase efficiency. It has potential to become the new design paradigm in the future to assist and even replace engineers, and so it has become a research hotspot in the academic community. However, current methods have some limitations to be addressed, whether in terms of application scope, visual quality of generated results, or evaluation metrics of results. This study proposes a comprehensive solution. Firstly, we introduce building information modeling (BIM) into intelligent structural design and establishes a structural design pipeline integrating BIM and generative AI, which is a powerful supplement to the previous frameworks that only considered CAD drawings. In order to improve the perceptual quality and details of generations, this study makes 3 contributions. Firstly, in terms of generation framework, inspired by the process of human drawing, a novel 2-stage generation framework is proposed to replace the traditional end-to-end framework to reduce the generation difficulty for AI models. Secondly, in terms of generative AI tools adopted, diffusion models (DMs) are introduced to replace widely used generative adversarial network (GAN)-based models, and a novel physics-based conditional diffusion model (PCDM) is proposed to consider different design prerequisites. Thirdly, in terms of neural networks, an attention block (AB) consisting of a self-attention block (SAB) and a parallel cross-attention block (PCAB) is designed to facilitate cross-domain data fusion. The quantitative and qualitative results demonstrate the powerful generation and representation capabilities of PCDM. Necessary ablation studies are conducted to examine the validity of the methods. This study also shows that DMs have the potential to replace GANs and become the new benchmark for generative problems in civil engineering.
</details>
<details>
<summary>摘要</summary>
智能结构设计使用人工智能可以有效减少时间开销并提高效率。它在未来可能成为新的设计 парадигмой，帮助或取代工程师，因此在学术社区中引起了广泛的研究兴趣。然而，当前方法存在一些需要解决的限制，包括应用范围、生成结果的视觉质量和评价指标。本研究提出了一个全面的解决方案。首先，我们将建筑信息模型（BIM）引入智能结构设计，并建立了结构设计管线，将BIM和生成AI相结合，这是前一个框架只考虑CAD图像的扩展。为了提高生成结果的视觉质量和细节，本研究做出了三个贡献。首先，在生成框架方面，根据人类绘制过程的灵感，我们提出了一种新的两阶段生成框架，以降低AI模型生成难度。其次，在生成AI工具方面，我们引入了扩散模型（DM），取代了广泛使用的生成对抗网络（GAN）模型，并提出了一种新的物理基于的条件扩散模型（PCDM），以考虑不同的设计前提。最后，在神经网络方面，我们设计了一个注意块（AB），包括一个自注意块（SAB）和一个平行交叉注意块（PCAB），以便跨领域数据的混合。对于PCDM的量化和质量result，我们进行了必要的ablation研究，以证明方法的有效性。此外，我们还发现了DMs在生成问题中的潜在优势，它们可能取代GANs，成为未来的生成问题的新标准。
</details></li>
</ul>
<hr>
<h2 id="3D-EAGAN-3D-edge-aware-attention-generative-adversarial-network-for-prostate-segmentation-in-transrectal-ultrasound-images"><a href="#3D-EAGAN-3D-edge-aware-attention-generative-adversarial-network-for-prostate-segmentation-in-transrectal-ultrasound-images" class="headerlink" title="3D EAGAN: 3D edge-aware attention generative adversarial network for prostate segmentation in transrectal ultrasound images"></a>3D EAGAN: 3D edge-aware attention generative adversarial network for prostate segmentation in transrectal ultrasound images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04049">http://arxiv.org/abs/2311.04049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengqing Liu, Xiao Shao, Liping Jiang, Kaizhi Wu</li>
<li>for: 这个研究的目的是为了提出一种能够高效地自动对TRUS图像中的膀胱进行分类的方法，以解决膀胱在这些图像中的不明确边界和不均匀的内散照。</li>
<li>methods: 这篇研究提出了一种基于3D edge-aware attention生成 adversarial network（3D EAGAN）的膀胱分类方法，包括一个edge-aware segmentation network（EASNet）和一个判别网络。EASNet包括一个encoder-decoder-based U-Net backbone网络、一个细节补偿模组、四个3D空间和通道对注意模组、一个边缘增强模组和一个全局特征提取器。</li>
<li>results: 这篇研究获得了高度有效地对TRUS图像中的膀胱进行分类的结果，并且比以往的方法有所改善。<details>
<summary>Abstract</summary>
Automatic prostate segmentation in TRUS images has always been a challenging problem, since prostates in TRUS images have ambiguous boundaries and inhomogeneous intensity distribution. Although many prostate segmentation methods have been proposed, they still need to be improved due to the lack of sensibility to edge information. Consequently, the objective of this study is to devise a highly effective prostate segmentation method that overcomes these limitations and achieves accurate segmentation of prostates in TRUS images. A 3D edge-aware attention generative adversarial network (3D EAGAN)-based prostate segmentation method is proposed in this paper, which consists of an edge-aware segmentation network (EASNet) that performs the prostate segmentation and a discriminator network that distinguishes predicted prostates from real prostates. The proposed EASNet is composed of an encoder-decoder-based U-Net backbone network, a detail compensation module, four 3D spatial and channel attention modules, an edge enhance module, and a global feature extractor. The detail compensation module is proposed to compensate for the loss of detailed information caused by the down-sampling process of the encoder. The features of the detail compensation module are selectively enhanced by the 3D spatial and channel attention module. Furthermore, an edge enhance module is proposed to guide shallow layers in the EASNet to focus on contour and edge information in prostates. Finally, features from shallow layers and hierarchical features from the decoder module are fused through the global feature extractor to predict the segmentation prostates.
</details>
<details>
<summary>摘要</summary>
自动脏陵脏分 segmentation 在 TRUS 图像中一直是一个挑战性的问题，因为脏陵脏在 TRUS 图像中有恒定的边缘和不均匀的Intensity 分布。虽然许多脏陵脏分 segmentation 方法已经被提出，但它们仍需要进一步改进，因为缺乏边缘信息的敏感性。这就是本研究的目标，我们提出了一种高效的脏陵脏分 segmentation 方法，能够在 TRUS 图像中准确地分 segmentation 脏陵脏。本文提出的脏陵脏分 segmentation 方法包括一个 Edge-aware 分 segmentation 网络（EASNet），该网络包括一个 U-Net 骨干网络、一个细节补偿模块、四个 3D 空间和通道注意模块、一个 Edge 增强模块和一个全局特征提取器。细节补偿模块的目的是补偿因下采样过程中的细节信息损失。特征选择性地增强了通道和空间注意模块中的特征。此外，为了引导 shallow 层在 EASNet 中注意脏陵脏的沿边信息，我们还提出了 Edge 增强模块。最后，来自 shallow 层和层次特征从 Decoder 模块中的特征被 fusion 通过全局特征提取器来预测脏陵脏的分 segmentation。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Near-Infrared-Hyperspectral-Imaging-for-Protein-Content-Regression-and-Grain-Variety-Classification-Using-Bulk-References-and-Varying-Grain-to-Background-Ratios"><a href="#Analyzing-Near-Infrared-Hyperspectral-Imaging-for-Protein-Content-Regression-and-Grain-Variety-Classification-Using-Bulk-References-and-Varying-Grain-to-Background-Ratios" class="headerlink" title="Analyzing Near-Infrared Hyperspectral Imaging for Protein Content Regression and Grain Variety Classification Using Bulk References and Varying Grain-to-Background Ratios"></a>Analyzing Near-Infrared Hyperspectral Imaging for Protein Content Regression and Grain Variety Classification Using Bulk References and Varying Grain-to-Background Ratios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04042">http://arxiv.org/abs/2311.04042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole-Christian Galbo Engstrøm, Erik Schou Dreier, Birthe Møller Jespersen, Kim Steenstrup Pedersen</li>
<li>for: 这两个数据集上，使用近红外高 spectral imaging（NIR-HSI）图像来调整模型，强调蛋白质含量预测和种类分类。</li>
<li>methods: 使用抽象和相关联合来扩展有限参考数据，但这会导致预测分布偏离和深度神经网络模型受到影响。提议修正方法来缓解这些偏差，提高蛋白质参考预测的平均值。</li>
<li>results: 研究发现，高比例的麦芽背景对两个任务都有更高的预测精度，但包含较低比例的图像在准备阶段可以增强模型对这些情况的Robustness。<details>
<summary>Abstract</summary>
Based on previous work, we assess the use of NIR-HSI images for calibrating models on two datasets, focusing on protein content regression and grain variety classification. Limited reference data for protein content is expanded by subsampling and associating it with the bulk sample. However, this method introduces significant biases due to skewed leptokurtic prediction distributions, affecting both PLS-R and deep CNN models. We propose adjustments to mitigate these biases, improving mean protein reference predictions. Additionally, we investigate the impact of grain-to-background ratios on both tasks. Higher ratios yield more accurate predictions, but including lower-ratio images in calibration enhances model robustness for such scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-exploitation-multi-task-learning-of-object-detection-and-semantic-segmentation-on-partially-annotated-data"><a href="#Data-exploitation-multi-task-learning-of-object-detection-and-semantic-segmentation-on-partially-annotated-data" class="headerlink" title="Data exploitation: multi-task learning of object detection and semantic segmentation on partially annotated data"></a>Data exploitation: multi-task learning of object detection and semantic segmentation on partially annotated data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04040">http://arxiv.org/abs/2311.04040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoàng-Ân Lê, Minh-Tan Pham</li>
<li>for: 学习对象检测和 semantic segmentation两个最受欢迎的视觉任务，从多任务数据中获得共同优化。</li>
<li>methods: 使用多任务数据，每个数据点只有一个任务的部分标注。</li>
<li>results: 通过对多任务学习和知识传播进行比较，发现多任务学习和知识传播在无法同时优化两个任务时具有优势。<details>
<summary>Abstract</summary>
Multi-task partially annotated data where each data point is annotated for only a single task are potentially helpful for data scarcity if a network can leverage the inter-task relationship. In this paper, we study the joint learning of object detection and semantic segmentation, the two most popular vision problems, from multi-task data with partial annotations. Extensive experiments are performed to evaluate each task performance and explore their complementarity when a multi-task network cannot optimize both tasks simultaneously. We propose employing knowledge distillation to leverage joint-task optimization. The experimental results show favorable results for multi-task learning and knowledge distillation over single-task learning and even full supervision scenario. All code and data splits are available at https://github.com/lhoangan/multas
</details>
<details>
<summary>摘要</summary>
多任务部分标注数据，每个数据点只有一个任务的标注，可能对数据缺乏情况有所帮助，如果网络可以利用任务之间的关系。在这篇论文中，我们研究了对象检测和 semantics 分割两个最流行的视觉问题的共同学习，从多任务数据中获得了部分标注。我们进行了广泛的实验来评估每个任务的性能，探索它们之间的补偿性。我们提议使用知识储存来利用共同优化。实验结果显示，多任务学习和知识储存的结果较单任务学习和完全监督情况更好。所有代码和数据分割可以在 GitHub 上获取：https://github.com/lhoangan/multas。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Dataset-Scale-Indicators-of-Data-Quality"><a href="#Exploring-Dataset-Scale-Indicators-of-Data-Quality" class="headerlink" title="Exploring Dataset-Scale Indicators of Data Quality"></a>Exploring Dataset-Scale Indicators of Data Quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04016">http://arxiv.org/abs/2311.04016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Feuer, Chinmay Hegde</li>
<li>for: 这研究旨在提高计算机视觉基础模型的性能，减少数据量，降低经济和环境成本。</li>
<li>methods: 研究人员提出了改善数据质量可以显著减少数据量的想法。他们认为计算机视觉数据质量可以分解为样本级别和数据集级别的两个组成部分，并且前者已经得到了更多的研究。他们还研究了两个数据集级别的关键因素：标签集设计和分类均衡。</li>
<li>results: 通过监测这些关键指标，研究人员和实践者可以更好地预测模型性能，包括准确率和分布转移robustness。<details>
<summary>Abstract</summary>
Modern computer vision foundation models are trained on massive amounts of data, incurring large economic and environmental costs. Recent research has suggested that improving data quality can significantly reduce the need for data quantity. But what constitutes data quality in computer vision? We posit that the quality of a given dataset can be decomposed into distinct sample-level and dataset-level constituents, and that the former have been more extensively studied than the latter. We ablate the effects of two important dataset-level constituents: label set design, and class balance. By monitoring these constituents using key indicators we provide, researchers and practitioners can better anticipate model performance, measured in terms of its accuracy and robustness to distribution shifts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AGNES-Abstraction-guided-Framework-for-Deep-Neural-Networks-Security"><a href="#AGNES-Abstraction-guided-Framework-for-Deep-Neural-Networks-Security" class="headerlink" title="AGNES: Abstraction-guided Framework for Deep Neural Networks Security"></a>AGNES: Abstraction-guided Framework for Deep Neural Networks Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04009">http://arxiv.org/abs/2311.04009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Dhonthi, Marcello Eiermann, Ernst Moritz Hahn, Vahid Hashemi<br>for: 这篇论文是为了检测深度神经网络（DNNs）中的后门而写的。methods: 该论文使用了一种名为AGNES的工具，用于检测DNNs中的后门。AGNES的原理基于的是。results: 论文表明，AGNES在多个有关的案例研究中表现更好于许多现有的状态 искусственный智能方法。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are becoming widespread, particularly in safety-critical areas. One prominent application is image recognition in autonomous driving, where the correct classification of objects, such as traffic signs, is essential for safe driving. Unfortunately, DNNs are prone to backdoors, meaning that they concentrate on attributes of the image that should be irrelevant for their correct classification. Backdoors are integrated into a DNN during training, either with malicious intent (such as a manipulated training process, because of which a yellow sticker always leads to a traffic sign being recognised as a stop sign) or unintentional (such as a rural background leading to any traffic sign being recognised as animal crossing, because of biased training data).   In this paper, we introduce AGNES, a tool to detect backdoors in DNNs for image recognition. We discuss the principle approach on which AGNES is based. Afterwards, we show that our tool performs better than many state-of-the-art methods for multiple relevant case studies.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在安全关键领域日益普及，特别是自动驾驶领域。在这个领域中，正确地识别对象，如交通标志，是安全驾驶的关键。然而，DNN受到后门攻击，即它们偏爱特定图像特征，而不是正确识别的对象。这些后门可以在训练过程中被植入，或者是由于偏见的训练数据而产生的。在这篇论文中，我们介绍了一个名为AGNES的工具，用于检测DNN中的后门。我们讲述了AGNES的原则方法。然后，我们展示了我们的工具在多个相关的案例研究中的性能较高。
</details></li>
</ul>
<hr>
<h2 id="Bias-and-Diversity-in-Synthetic-based-Face-Recognition"><a href="#Bias-and-Diversity-in-Synthetic-based-Face-Recognition" class="headerlink" title="Bias and Diversity in Synthetic-based Face Recognition"></a>Bias and Diversity in Synthetic-based Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03970">http://arxiv.org/abs/2311.03970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Huber, Anh Thi Luu, Fadi Boutros, Arjan Kuijper, Naser Damer</li>
<li>for: 这研究探讨了假数据的多样性如何与真实数据的多样性相比，以及生成模型训练数据的分布对假数据的分布的影响。</li>
<li>methods: 该研究使用了三种最新的假数据基于面Recognition模型，并对不同属性（性别、民族、年龄和头部位置）进行了分析。</li>
<li>results: 研究发现，生成器对不同属性的分布具有类似的分布，而且假数据基于模型和真实数据基于模型都存在类似的偏见。然而，通过降低内属性一致性，可以减少偏见。<details>
<summary>Abstract</summary>
Synthetic data is emerging as a substitute for authentic data to solve ethical and legal challenges in handling authentic face data. The current models can create real-looking face images of people who do not exist. However, it is a known and sensitive problem that face recognition systems are susceptible to bias, i.e. performance differences between different demographic and non-demographics attributes, which can lead to unfair decisions. In this work, we investigate how the diversity of synthetic face recognition datasets compares to authentic datasets, and how the distribution of the training data of the generative models affects the distribution of the synthetic data. To do this, we looked at the distribution of gender, ethnicity, age, and head position. Furthermore, we investigated the concrete bias of three recent synthetic-based face recognition models on the studied attributes in comparison to a baseline model trained on authentic data. Our results show that the generator generate a similar distribution as the used training data in terms of the different attributes. With regard to bias, it can be seen that the synthetic-based models share a similar bias behavior with the authentic-based models. However, with the uncovered lower intra-identity attribute consistency seems to be beneficial in reducing bias.
</details>
<details>
<summary>摘要</summary>
人工数据emerges as a substitute for authentic data to address ethical and legal challenges in face recognition. Current models can generate realistic face images of people who do not exist. However, face recognition systems are susceptible to bias, and there are performance differences between different demographic and non-demographic attributes, which can lead to unfair decisions.In this work, we compare the diversity of synthetic face recognition datasets to authentic datasets and investigate how the distribution of the training data of the generative models affects the distribution of the synthetic data. We examine the distribution of gender, ethnicity, age, and head position. Additionally, we compare the concrete bias of three recent synthetic-based face recognition models to a baseline model trained on authentic data.Our results show that the generator produces a similar distribution as the used training data in terms of the different attributes. With regard to bias, the synthetic-based models exhibit a similar bias behavior to the authentic-based models. However, the uncovered lower intra-identity attribute consistency may be beneficial in reducing bias.
</details></li>
</ul>
<hr>
<h2 id="CeCNN-Copula-enhanced-convolutional-neural-networks-in-joint-prediction-of-refraction-error-and-axial-length-based-on-ultra-widefield-fundus-images"><a href="#CeCNN-Copula-enhanced-convolutional-neural-networks-in-joint-prediction-of-refraction-error-and-axial-length-based-on-ultra-widefield-fundus-images" class="headerlink" title="CeCNN: Copula-enhanced convolutional neural networks in joint prediction of refraction error and axial length based on ultra-widefield fundus images"></a>CeCNN: Copula-enhanced convolutional neural networks in joint prediction of refraction error and axial length based on ultra-widefield fundus images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03967">http://arxiv.org/abs/2311.03967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Zhong, Yang Li, Danjuan Yang, Meiyan Li, Xingyao Zhou, Bo Fu, Catherine C. Liu, A. H. Welsh</li>
<li>for: 该研究旨在提高仪器诊断和诊断抑制方法的准确率，通过利用高级 statistics 方法提取数据中的信息，提高深度学习模型的预测精度。</li>
<li>methods: 该研究使用了一种高级的多变量响应回归模型，其中包括一个高级的 tensor 生成器，以及一种基于 Gaussian copula 的估计方法。这种模型可以利用数据中的相关信息，提高深度学习模型的预测精度。</li>
<li>results: 研究表明，在使用该模型时，可以提高深度学习模型的预测精度，特别是在双重任务中，例如回归预测和分类预测。此外，该模型可以在其他场景下应用，并且可以与其他背景网络结合使用。<details>
<summary>Abstract</summary>
Ultra-widefield (UWF) fundus images are replacing traditional fundus images in screening, detection, prediction, and treatment of complications related to myopia because their much broader visual range is advantageous for highly myopic eyes. Spherical equivalent (SE) is extensively used as the main myopia outcome measure, and axial length (AL) has drawn increasing interest as an important ocular component for assessing myopia. Cutting-edge studies show that SE and AL are strongly correlated. Using the joint information from SE and AL is potentially better than using either separately. In the deep learning community, though there is research on multiple-response tasks with a 3D image biomarker, dependence among responses is only sporadically taken into consideration. Inspired by the spirit that information extracted from the data by statistical methods can improve the prediction accuracy of deep learning models, we formulate a class of multivariate response regression models with a higher-order tensor biomarker, for the bivariate tasks of regression-classification and regression-regression. Specifically, we propose a copula-enhanced convolutional neural network (CeCNN) framework that incorporates the dependence between responses through a Gaussian copula (with parameters estimated from a warm-up CNN) and uses the induced copula-likelihood loss with the backbone CNNs. We establish the statistical framework and algorithms for the aforementioned two bivariate tasks. We show that the CeCNN has better prediction accuracy after adding the dependency information to the backbone models. The modeling and the proposed CeCNN algorithm are applicable beyond the UWF scenario and can be effective with other backbones beyond ResNet and LeNet.
</details>
<details>
<summary>摘要</summary>
ultra-widefield (UWF) fundus images  replacing traditional fundus images in screening, detection, prediction, and treatment of myopia complications because their much broader visual range is advantageous for highly myopic eyes。spherical equivalent (SE) is extensively used as the main myopia outcome measure, and axial length (AL) has drawn increasing interest as an important ocular component for assessing myopia。cutting-edge studies show that SE and AL are strongly correlated。using the joint information from SE and AL is potentially better than using either separately。in the deep learning community, though there is research on multiple-response tasks with a 3D image biomarker, dependence among responses is only sporadically taken into consideration。inspired by the spirit that information extracted from the data by statistical methods can improve the prediction accuracy of deep learning models，we formulate a class of multivariate response regression models with a higher-order tensor biomarker，for the bivariate tasks of regression-classification and regression-regression。specifically，we propose a copula-enhanced convolutional neural network (CeCNN) framework that incorporates the dependence between responses through a Gaussian copula (with parameters estimated from a warm-up CNN) and uses the induced copula-likelihood loss with the backbone CNNs。we establish the statistical framework and algorithms for the aforementioned two bivariate tasks。we show that the CeCNN has better prediction accuracy after adding the dependency information to the backbone models。the modeling and the proposed CeCNN algorithm are applicable beyond the UWF scenario and can be effective with other backbones beyond ResNet and LeNet。
</details></li>
</ul>
<hr>
<h2 id="Fast-Sun-aligned-Outdoor-Scene-Relighting-based-on-TensoRF"><a href="#Fast-Sun-aligned-Outdoor-Scene-Relighting-based-on-TensoRF" class="headerlink" title="Fast Sun-aligned Outdoor Scene Relighting based on TensoRF"></a>Fast Sun-aligned Outdoor Scene Relighting based on TensoRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03965">http://arxiv.org/abs/2311.03965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeonjin Chang, Yearim Kim, Seunghyeon Seo, Jung Yi, Nojun Kwak</li>
<li>for: 这个论文是为了推广Neural Radiance Fields（NeRF）中的户外场景重新照明方法，即Sun-aligned Relighting TensoRF（SR-TensoRF）。</li>
<li>methods: SR-TensoRF使用了一种轻量级、快速的管道，并将阳光方向作为输入，以实现简化的推理过程。此外，SR-TensoRF还利用了TensoRF的培育效率，通过我们提出的立方体图案，从而实现了对training和rendering过程的显著加速。</li>
<li>results: SR-TensoRF可以快速地生成高质量的户外场景图像，并且可以在不同的视点下实现高度一致的重新照明效果。<details>
<summary>Abstract</summary>
In this work, we introduce our method of outdoor scene relighting for Neural Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF). SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun, thereby achieving a simplified workflow that eliminates the need for environment maps. Our sun-alignment strategy is motivated by the insight that shadows, unlike viewpoint-dependent albedo, are determined by light direction. We directly use the sun direction as an input during shadow generation, simplifying the requirements of the inference process significantly. Moreover, SR-TensoRF leverages the training efficiency of TensoRF by incorporating our proposed cubemap concept, resulting in notable acceleration in both training and rendering processes compared to existing methods.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了一种用于外部场景照明的神经辐射场（NeRF）方法，名为太阳Alignment Relighting TensoRF（SR-TensoRF）。 SR-TensoRF提供了一个轻量级、快速的管道，同时与太阳方向相对应，因此实现了简化的工作流程，消除了环境地图的需求。我们的太阳Alignment策略受到了光线方向确定影子的直观，因此在影子生成过程中直接使用太阳方向作为输入， thereby significantly simplifying the requirements of the inference process。此外，SR-TensoRF利用了TensoRF的训练效率，通过我们的提议的立方体地图概念，从而在训练和渲染过程中实现了明显的加速。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multimodal-Compositional-Reasoning-of-Visual-Language-Models-with-Generative-Negative-Mining"><a href="#Enhancing-Multimodal-Compositional-Reasoning-of-Visual-Language-Models-with-Generative-Negative-Mining" class="headerlink" title="Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining"></a>Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03964">http://arxiv.org/abs/2311.03964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ugorsahin/Generative-Negative-Mining">https://github.com/ugorsahin/Generative-Negative-Mining</a></li>
<li>paper_authors: Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, Volker Tresp</li>
<li>for: 提高大规模视语言模型（VLM）在多模态组合理解任务中的表现。</li>
<li>methods: 提出了一种框架，该框架不仅在两个方向中挖掘负例，还可以生成多模态困难负样本，以提高VLM的多模态组合理解能力。</li>
<li>results: 通过利用这些生成的困难负样本，可以significantly enhance VLMs的表现在多模态组合理解任务中。<details>
<summary>Abstract</summary>
Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing image and text understanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused on mining negative examples from existing datasets. However, the mined negative examples might not be difficult for the model to discriminate from the positive. An alternative to mining would be negative sample generation 2) But existing generative approaches primarily focus on generating hard negative texts associated with a given image. Mining in the other direction, i.e., generating negative image samples associated with a given text has been ignored. To overcome both these limitations, we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities, i.e., images and texts. Leveraging these generative hard negative samples, we significantly enhance VLMs' performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.
</details>
<details>
<summary>摘要</summary>
现代大规模视觉语言模型（VLM）表现出强大的表示能力，使其在图像和文本理解任务中普遍应用。它们通常通过对大量和多样化的图像和相关文本描述进行对比式训练。然而，VLM frequently struggles with compositional reasoning tasks，这些任务需要细致地理解对象和其属性之间的复杂交互。这种失败可以归结于两个主要因素：1）对比式方法通常是从现有数据集中挖掘负例的，但这些挖掘出来的负例可能并不是模型很难分辨出来的。2）现有的生成方法主要是生成与给定图像关联的困难文本样本，但生成在另一个方向的负样本，即与给定文本关联的困难图像样本，被忽略了。为了超越这些限制，我们提出了一个框架，不仅挖掘在两个方向，而且生成了图像和文本中的困难负样本。利用这些生成的困难负样本，我们可以在多模态 compositional reasoning 任务中显著提高 VLM 的表现。我们的代码和数据集可以在 <https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Effectiveness-of-Deep-Generative-Data"><a href="#Improving-the-Effectiveness-of-Deep-Generative-Data" class="headerlink" title="Improving the Effectiveness of Deep Generative Data"></a>Improving the Effectiveness of Deep Generative Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03959">http://arxiv.org/abs/2311.03959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyu Wang, Sabrina Schmedding, Marco F. Huber</li>
<li>for: 本研究旨在解释在使用深度生成模型（DGM）生成的 sintetic 图像中，downstream 图像处理任务（如图像分类）时的性能下降的原因。</li>
<li>methods: 我们提出了一种新的分类法，可以更好地利用 DGM 生成的 sintetic 图像来提高 downstream 任务的性能。我们采用了一种内容差异（Content Gap）的概念，以解释在使用 sintetic 图像时，性能下降的原因。</li>
<li>results: 我们在 CIFAR-10  dataset 上进行了广泛的实验，并证明了我们的方法在 Synthetic-to-Real 和 Data Augmentation 两种情况下都能够升高 downstream 任务的性能，特别是在数据缺乏的情况下。<details>
<summary>Abstract</summary>
Recent deep generative models (DGMs) such as generative adversarial networks (GANs) and diffusion probabilistic models (DPMs) have shown their impressive ability in generating high-fidelity photorealistic images. Although looking appealing to human eyes, training a model on purely synthetic images for downstream image processing tasks like image classification often results in an undesired performance drop compared to training on real data. Previous works have demonstrated that enhancing a real dataset with synthetic images from DGMs can be beneficial. However, the improvements were subjected to certain circumstances and yet were not comparable to adding the same number of real images. In this work, we propose a new taxonomy to describe factors contributing to this commonly observed phenomenon and investigate it on the popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a large portion of the performance drop when using synthetic images from DGM and propose strategies to better utilize them in downstream tasks. Extensive experiments on multiple datasets showcase that our method outperforms baselines on downstream classification tasks both in case of training on synthetic only (Synthetic-to-Real) and training on a mix of real and synthetic data (Data Augmentation), particularly in the data-scarce scenario.
</details>
<details>
<summary>摘要</summary>
现代深度生成模型（DGM），如生成对抗网络（GAN）和扩散概率模型（DPM），已经显示出生成高品质图像的能力。虽然看上去很有趣，但是在尝试使用假图像来进行下游图像处理任务，如图像分类，通常会导致性能下降。先前的工作表明，可以通过混合真实图像和生成图像来提高模型的性能。然而，这些改进受到某些条件的限制，并且与添加相同数量的真实图像相比，未能达到相同的水平。在这项工作中，我们提出了一个新的分类法，描述了使用DGM生成的假图像在下游任务中的常见现象，并在Popular CIFAR-10 dataset上进行了广泛的实验。我们认为，内容差距占了大量的性能下降，当使用DGM生成的假图像时。我们提出了一些策略，以更好地利用它们在下游任务中。我们的方法在多个数据集上比基eline表现出色，特别是在数据缺乏的情况下。
</details></li>
</ul>
<hr>
<h2 id="CLIP-Guided-Image-perceptive-Prompt-Learning-for-Image-Enhancement"><a href="#CLIP-Guided-Image-perceptive-Prompt-Learning-for-Image-Enhancement" class="headerlink" title="CLIP Guided Image-perceptive Prompt Learning for Image Enhancement"></a>CLIP Guided Image-perceptive Prompt Learning for Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03943">http://arxiv.org/abs/2311.03943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zinuo Li, Qiuhong Ke, Weiwen Chen</li>
<li>for: 这篇论文主要旨在提出一种基于对比语言图像预训练（CLIP）引导的图像提升方法，即CLIP-LUT方法。</li>
<li>methods: 这种方法使用CLIP模型学习图像感知提示，并在这些提示基础之上建立一个非常简单的网络，用于预测三种不同的Look-up-table（LUT）权重。</li>
<li>results: 研究发现，通过将CLIP模型的先验知识引入图像提升过程，可以提高图像质量。具体来说，通过使用CLIP模型学习图像感知提示，并将其用作图像提升网络的损失函数，可以获得满意的结果。<details>
<summary>Abstract</summary>
Image enhancement is a significant research area in the fields of computer vision and image processing. In recent years, many learning-based methods for image enhancement have been developed, where the Look-up-table (LUT) has proven to be an effective tool. In this paper, we delve into the potential of Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning, proposing a simple structure called CLIP-LUT for image enhancement. We found that the prior knowledge of CLIP can effectively discern the quality of degraded images, which can provide reliable guidance. To be specific, We initially learn image-perceptive prompts to distinguish between original and target images using CLIP model, in the meanwhile, we introduce a very simple network by incorporating a simple baseline to predict the weights of three different LUT as enhancement network. The obtained prompts are used to steer the enhancement network like a loss function and improve the performance of model. We demonstrate that by simply combining a straightforward method with CLIP, we can obtain satisfactory results.
</details>
<details>
<summary>摘要</summary>
<SYS>    translate_language=zh-Hans</SYS>Image enhancement is a significant research area in the fields of computer vision and image processing. In recent years, many learning-based methods for image enhancement have been developed, where the Look-up-table (LUT) has proven to be an effective tool. In this paper, we delve into the potential of Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning, proposing a simple structure called CLIP-LUT for image enhancement. We found that the prior knowledge of CLIP can effectively discern the quality of degraded images, which can provide reliable guidance. To be specific, We initially learn image-perceptive prompts to distinguish between original and target images using CLIP model, in the meanwhile, we introduce a very simple network by incorporating a simple baseline to predict the weights of three different LUT as enhancement network. The obtained prompts are used to steer the enhancement network like a loss function and improve the performance of model. We demonstrate that by simply combining a straightforward method with CLIP, we can obtain satisfactory results.
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-NaN-Divergence-in-Training-Monocular-Depth-Estimation-Model"><a href="#Analysis-of-NaN-Divergence-in-Training-Monocular-Depth-Estimation-Model" class="headerlink" title="Analysis of NaN Divergence in Training Monocular Depth Estimation Model"></a>Analysis of NaN Divergence in Training Monocular Depth Estimation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03938">http://arxiv.org/abs/2311.03938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bum Jun Kim, Hyeonah Jang, Sang Woo Kim<br>for: 本研究旨在探讨NaN损失在单目深度估计网络训练中的发生原因，并提供了避免NaN损失的实践指南。methods: 本研究使用了深度学习的最新进展，对单目深度估计网络进行了深入分析，并发现了三种NaN损失的原因：1）使用平方根损失函数会导致梯度不稳定; 2）使用Logsigmoid函数会导致数学上的稳定问题; 3） certain variance implementations会导致计算错误。results:  experiments表明，遵循我们的指南可以提高单目深度估计网络的优化稳定性和性能。<details>
<summary>Abstract</summary>
The latest advances in deep learning have facilitated the development of highly accurate monocular depth estimation models. However, when training a monocular depth estimation network, practitioners and researchers have observed not a number (NaN) loss, which disrupts gradient descent optimization. Although several practitioners have reported the stochastic and mysterious occurrence of NaN loss that bothers training, its root cause is not discussed in the literature. This study conducted an in-depth analysis of NaN loss during training a monocular depth estimation network and identified three types of vulnerabilities that cause NaN loss: 1) the use of square root loss, which leads to an unstable gradient; 2) the log-sigmoid function, which exhibits numerical stability issues; and 3) certain variance implementations, which yield incorrect computations. Furthermore, for each vulnerability, the occurrence of NaN loss was demonstrated and practical guidelines to prevent NaN loss were presented. Experiments showed that both optimization stability and performance on monocular depth estimation could be improved by following our guidelines.
</details>
<details>
<summary>摘要</summary>
最新的深度学习技术发展已经为单目深度估计模型提供了非常高精度。然而，在训练单目深度估计网络时，实践者和研究人员经常会遇到NaN损失，这会阻碍梯度下降优化。虽然许多实践者已经报告了随机和神秘的NaN损失现象，但是在文献中没有讨论这个根本问题的原因。本研究对单目深度估计网络训练中的NaN损失进行了深入分析，并确定了三种导致NaN损失的漏洞：1）使用平方根损失，导致梯度不稳定; 2）使用对数 sigmoid 函数，存在数学上的稳定性问题; 3）某些变量实现方式会导致错误计算。此外，对每种漏洞的NaN损失的发生和避免方法都进行了实践和推荐。实验结果表明，遵循我们的指南可以提高优化稳定性和单目深度估计性能。
</details></li>
</ul>
<hr>
<h2 id="FLORA-Fine-grained-Low-Rank-Architecture-Search-for-Vision-Transformer"><a href="#FLORA-Fine-grained-Low-Rank-Architecture-Search-for-Vision-Transformer" class="headerlink" title="FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer"></a>FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03912">http://arxiv.org/abs/2311.03912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shadowpa0327/flora">https://github.com/shadowpa0327/flora</a></li>
<li>paper_authors: Chi-Chih Chang, Yuan-Yao Sung, Shixing Yu, Ning-Chi Huang, Diana Marculescu, Kai-Chiang Wu</li>
<li>for: 这个论文的目的是提出一种自动化ViT搜索空间中的低级别搜索方法，以提高ViT模型的计算效率。</li>
<li>methods: 这个论文使用了一种基于NAS的端到端自动框架， named FLORA，以解决低级别搜索空间中的设计挑战。它使用了一种基于矩阵的低级别搜索策略，并采用了一种低级别特性训练方法来提高低级别模型的质量。</li>
<li>results: 论文的实验结果表明，FLORA可以自动生成更细化的低级别配置，并可以在不同的模型和任务上实现33%左右的计算开销减少。此外，FLORA还可以与其他压缩技术或紧凑结构结合使用，实现Extra 21%-26%的计算开销减少。<details>
<summary>Abstract</summary>
Vision Transformers (ViT) have recently demonstrated success across a myriad of computer vision tasks. However, their elevated computational demands pose significant challenges for real-world deployment. While low-rank approximation stands out as a renowned method to reduce computational loads, efficiently automating the target rank selection in ViT remains a challenge. Drawing from the notable similarity and alignment between the processes of rank selection and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based on NAS. To overcome the design challenge of supernet posed by vast search space, FLORA employs a low-rank aware candidate filtering strategy. This method adeptly identifies and eliminates underperforming candidates, effectively alleviating potential undertraining and interference among subnetworks. To further enhance the quality of low-rank supernets, we design a low-rank specific training paradigm. First, we propose weight inheritance to construct supernet and enable gradient sharing among low-rank modules. Secondly, we adopt low-rank aware sampling to strategically allocate training resources, taking into account inherited information from pre-trained models. Empirical results underscore FLORA's efficacy. With our method, a more fine-grained rank configuration can be generated automatically and yield up to 33% extra FLOPs reduction compared to a simple uniform configuration. More specific, FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without performance degradtion. Importantly, FLORA boasts both versatility and orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with leading compression techniques or compact hybrid structures. Our code is publicly available at https://github.com/shadowpa0327/FLORA.
</details>
<details>
<summary>摘要</summary>
vision transformers (ViT) 在多种计算机视觉任务上已经表现出色，但它们的计算负担却是实际应用中的一大挑战。而且，可靠地自动选择目标排名在ViT中仍然是一个挑战。Drawing from the notable similarity and alignment between the processes of rank selection and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based on NAS. To overcome the design challenge of supernet posed by vast search space, FLORA employs a low-rank aware candidate filtering strategy. This method adeptly identifies and eliminates underperforming candidates, effectively alleviating potential undertraining and interference among subnetworks. To further enhance the quality of low-rank supernets, we design a low-rank specific training paradigm. First, we propose weight inheritance to construct supernet and enable gradient sharing among low-rank modules. Secondly, we adopt low-rank aware sampling to strategically allocate training resources, taking into account inherited information from pre-trained models. Empirical results underscore FLORA's efficacy. With our method, a more fine-grained rank configuration can be generated automatically and yield up to 33% extra FLOPs reduction compared to a simple uniform configuration. More specific, FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without performance degradation. Importantly, FLORA boasts both versatility and orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with leading compression techniques or compact hybrid structures. Our code is publicly available at https://github.com/shadowpa0327/FLORA.
</details></li>
</ul>
<hr>
<h2 id="RobustMat-Neural-Diffusion-for-Street-Landmark-Patch-Matching-under-Challenging-Environments"><a href="#RobustMat-Neural-Diffusion-for-Street-Landmark-Patch-Matching-under-Challenging-Environments" class="headerlink" title="RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments"></a>RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03904">http://arxiv.org/abs/2311.03904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-it-avs/robustmat">https://github.com/ai-it-avs/robustmat</a></li>
<li>paper_authors: Rui She, Qiyu Kang, Sijie Wang, Yuan-Rui Yang, Kai Zhao, Yang Song, Wee Peng Tay</li>
<li>for: 这篇论文主要针对自动驾驶车辆（AV）的视觉识别技术进行研究，以寻找在不同季节、天气和照明条件下的可靠匹配方法。</li>
<li>methods: 该论文提出了一种名为RobustMat的方法，该方法利用摄像头上的图像缓冲区域的空间邻居信息，以提高匹配的稳定性。具体来说，该方法首先使用卷积神经网络演化方法学习图像缓冲区域的特征表示。然后，使用图像街景数据库中的其他图像来归一化图像缓冲区域的信息。最后，通过对图像缓冲区域之间的特征相似性进行学习，计算出最终的匹配分数。</li>
<li>results: 该论文的实验结果表明，RobustMat方法在不同季节、天气和照明条件下能够实现高级别的匹配效果，并且比传统的方法更具有稳定性。<details>
<summary>Abstract</summary>
For autonomous vehicles (AVs), visual perception techniques based on sensors like cameras play crucial roles in information acquisition and processing. In various computer perception tasks for AVs, it may be helpful to match landmark patches taken by an onboard camera with other landmark patches captured at a different time or saved in a street scene image database. To perform matching under challenging driving environments caused by changing seasons, weather, and illumination, we utilize the spatial neighborhood information of each patch. We propose an approach, named RobustMat, which derives its robustness to perturbations from neural differential equations. A convolutional neural ODE diffusion module is used to learn the feature representation for the landmark patches. A graph neural PDE diffusion module then aggregates information from neighboring landmark patches in the street scene. Finally, feature similarity learning outputs the final matching score. Our approach is evaluated on several street scene datasets and demonstrated to achieve state-of-the-art matching results under environmental perturbations.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: For autonomous vehicles (AVs), visual perception techniques based on sensors like cameras play crucial roles in information acquisition and processing. In various computer perception tasks for AVs, it may be helpful to match landmark patches taken by an onboard camera with other landmark patches captured at a different time or saved in a street scene image database. To perform matching under challenging driving environments caused by changing seasons, weather, and illumination, we utilize the spatial neighborhood information of each patch. We propose an approach, named RobustMat, which derives its robustness to perturbations from neural differential equations. A convolutional neural ODE diffusion module is used to learn the feature representation for the landmark patches. A graph neural PDE diffusion module then aggregates information from neighboring landmark patches in the street scene. Finally, feature similarity learning outputs the final matching score. Our approach is evaluated on several street scene datasets and demonstrated to achieve state-of-the-art matching results under environmental perturbations.Translated into Traditional Chinese: For autonomous vehicles (AVs), visual perception techniques based on sensors like cameras play crucial roles in information acquisition and processing. In various computer perception tasks for AVs, it may be helpful to match landmark patches taken by an onboard camera with other landmark patches captured at a different time or saved in a street scene image database. To perform matching under challenging driving environments caused by changing seasons, weather, and illumination, we utilize the spatial neighborhood information of each patch. We propose an approach, named RobustMat, which derives its robustness to perturbations from neural differential equations. A convolutional neural ODE diffusion module is used to learn the feature representation for the landmark patches. A graph neural PDE diffusion module then aggregates information from neighboring landmark patches in the street scene. Finally, feature similarity learning outputs the final matching score. Our approach is evaluated on several street scene datasets and demonstrated to achieve state-of-the-art matching results under environmental perturbations.
</details></li>
</ul>
<hr>
<h2 id="MeVGAN-GAN-based-Plugin-Model-for-Video-Generation-with-Applications-in-Colonoscopy"><a href="#MeVGAN-GAN-based-Plugin-Model-for-Video-Generation-with-Applications-in-Colonoscopy" class="headerlink" title="MeVGAN: GAN-based Plugin Model for Video Generation with Applications in Colonoscopy"></a>MeVGAN: GAN-based Plugin Model for Video Generation with Applications in Colonoscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03884">http://arxiv.org/abs/2311.03884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Łukasz Struski, Tomasz Urbańczyk, Krzysztof Bucki, Bartłomiej Cupiał, Aneta Kaczyńska, Przemysław Spurek, Jacek Tabor</li>
<li>for: 这篇论文的目的是提出一种能够生成高分辨率视频的生成模型，以解决现有的生成模型具有大量存储需求的问题。</li>
<li>methods: 该论文提出了一种插件型架构的生成 adversarial network（GAN），称为Memory Efficient Video GAN（MeVGAN）。该模型使用预训练的2D图像GAN，并只添加了一个简单的神经网络来构造噪声空间中的轨迹，以便在GAN模型中构造真实的视频。</li>
<li>results: 该论文应用MeVGAN模型在colonoscopy视频生成任务中，并显示了MeVGAN可以生成高质量的synthetic colonoscopy视频，可能用于虚拟 simulate器中。colonoscopy是一项重要的医疗程序，尤其是在检测和治疗肠Rectal Cancer中起着重要作用。但是，因为colonoscopy是一项复杂和时间consuming的学习过程，因此colonoscopy simulators在培养年轻的colonoscopists中广泛使用。<details>
<summary>Abstract</summary>
Video generation is important, especially in medicine, as much data is given in this form. However, video generation of high-resolution data is a very demanding task for generative models, due to the large need for memory. In this paper, we propose Memory Efficient Video GAN (MeVGAN) - a Generative Adversarial Network (GAN) which uses plugin-type architecture. We use a pre-trained 2D-image GAN and only add a simple neural network to construct respective trajectories in the noise space, so that the trajectory forwarded through the GAN model constructs a real-life video. We apply MeVGAN in the task of generating colonoscopy videos. Colonoscopy is an important medical procedure, especially beneficial in screening and managing colorectal cancer. However, because colonoscopy is difficult and time-consuming to learn, colonoscopy simulators are widely used in educating young colonoscopists. We show that MeVGAN can produce good quality synthetic colonoscopy videos, which can be potentially used in virtual simulators.
</details>
<details>
<summary>摘要</summary>
视频生成对医学领域来说非常重要，因为大量数据都是以这种形式提供。然而，生成高分辨率视频是对生成模型的很大负担，因为需要很大的内存。在这篇论文中，我们提出了内存高效的视频GAN（MeVGAN）——一种生成对抗网络（GAN）。我们使用预训练的2D图像GAN，只是将简单的神经网络添加到静止图像空间中，以便在噪声空间中构造真实的视频。我们在检查colonoscopy视频生成任务中应用MeVGAN。colonoscopy是医学重要的过程，尤其是在检测和管理肠RECTAL癌中。然而，因为colonoscopy是学习复杂且时间consuming的，因此colonoscopy模拟器广泛使用在教育年轻colonoscopists。我们显示MeVGAN可以生成高质量的 sintetic colonoscopy视频，可以在虚拟模拟器中使用。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Knowledge-Transfer-Methods-for-Misaligned-Urban-Building-Labels"><a href="#A-Comparative-Study-of-Knowledge-Transfer-Methods-for-Misaligned-Urban-Building-Labels" class="headerlink" title="A Comparative Study of Knowledge Transfer Methods for Misaligned Urban Building Labels"></a>A Comparative Study of Knowledge Transfer Methods for Misaligned Urban Building Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03867">http://arxiv.org/abs/2311.03867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bipul Neupane, Jagannath Aryal, Abbas Rajabifard</li>
<li>for:  Addressing the misalignment issue in Earth observation (EO) images and building labels to train accurate convolutional neural networks (CNNs) for semantic segmentation of building footprints.</li>
<li>methods:  Comparative study of three Teacher-Student knowledge transfer methods: supervised domain adaptation (SDA), knowledge distillation (KD), and deep mutual learning (DML).</li>
<li>results:  SDA is the most effective method to address the misalignment problem, while KD and DML can efficiently compress network size without significant loss in performance. The 158 experiments and datasets developed in this study will be valuable to minimise the misaligned labels.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Misalignment in Earth observation (EO) images and building labels impact the training of accurate convolutional neural networks (CNNs) for semantic segmentation of building footprints. Recently, three Teacher-Student knowledge transfer methods have been introduced to address this issue: supervised domain adaptation (SDA), knowledge distillation (KD), and deep mutual learning (DML). However, these methods are merely studied for different urban buildings (low-rise, mid-rise, high-rise, and skyscrapers), where misalignment increases with building height and spatial resolution. In this study, we present a workflow for the systematic comparative study of the three methods. The workflow first identifies the best (with the highest evaluation scores) hyperparameters, lightweight CNNs for the Student (among 43 CNNs from Computer Vision), and encoder-decoder networks (EDNs) for both Teachers and Students. Secondly, three building footprint datasets are developed to train and evaluate the identified Teachers and Students in the three transfer methods. The results show that U-Net with VGG19 (U-VGG19) is the best Teacher, and U-EfficientNetv2B3 and U-EfficientNet-lite0 are among the best Students. With these Teacher-Student pairs, SDA could yield upto 0.943, 0.868, 0.912, and 0.697 F1 scores in the low-rise, mid-rise, high-rise, and skyscrapers respectively. KD and DML provide model compression of upto 82%, despite marginal loss in performance. This new comparison concludes that SDA is the most effective method to address the misalignment problem, while KD and DML can efficiently compress network size without significant loss in performance. The 158 experiments and datasets developed in this study will be valuable to minimise the misaligned labels.
</details>
<details>
<summary>摘要</summary>
地球观测（EO）图像和建筑标签的不一致问题会对准确的 convolutional neural networks（CNNs）的 semantic segmentation培训产生影响。最近，三种教师-学生知识传递方法被提出来解决这个问题：指导适应（SDA）、知识传递（KD）和深度相互学习（DML）。然而，这些方法只在不同的城市建筑（低层、中层、高层和尖塔）进行了研究，而这些建筑的不一致问题随着建筑高度和空间分辨率增加。在这种研究中，我们提出了一个系统性比较工作流程。首先，我们确定了最佳（最高评价分）的参数、轻量级 CNNs（从计算机视觉中选择的43个 CNNs）和编码器-解码器网络（EDNs） для两个教师和学生。其次，我们开发了三个建筑涂抹数据集，用于培训和评价确定的教师和学生。结果显示，U-Net with VGG19（U-VGG19）是最佳教师，而U-EfficientNetv2B3和U-EfficientNet-lite0是最佳学生。与这些教师-学生对照对照，SDA可以获得最高的0.943、0.868、0.912和0.697的F1分数。KD和DML可以压缩网络大小，但是只有82%的压缩。这个新的比较结论是，SDA是解决不一致标签问题的最佳方法，而KD和DML可以高效地压缩网络大小，而无损失性能。这些158个实验和开发的数据集将有助于减少不一致标签问题。
</details></li>
</ul>
<hr>
<h2 id="SCONE-GAN-Semantic-Contrastive-learning-based-Generative-Adversarial-Network-for-an-end-to-end-image-translation"><a href="#SCONE-GAN-Semantic-Contrastive-learning-based-Generative-Adversarial-Network-for-an-end-to-end-image-translation" class="headerlink" title="SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation"></a>SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03866">http://arxiv.org/abs/2311.03866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Abbasnejad, Fabio Zambetta, Flora Salim, Timothy Wiley, Jeffrey Chan, Russell Gallagher, Ehsan Abbasnejad</li>
<li>for: 这篇论文旨在实现图像翻译，以生成真实和多样化的景观图像。</li>
<li>methods: 该方法使用图像树减少网络学习图像结构和 semantics，同时维持图像的真实性。它还引入了风格引用图像，以便增加更多的多样性。</li>
<li>results: 对四个 dataset 进行评估，qualitative 和 quantitative 结果都表明该方法可以生成更真实和多样化的图像。<details>
<summary>Abstract</summary>
SCONE-GAN presents an end-to-end image translation, which is shown to be effective for learning to generate realistic and diverse scenery images. Most current image-to-image translation approaches are devised as two mappings: a translation from the source to target domain and another to represent its inverse. While successful in many applications, these approaches may suffer from generating trivial solutions with limited diversity. That is because these methods learn more frequent associations rather than the scene structures. To mitigate the problem, we propose SCONE-GAN that utilises graph convolutional networks to learn the objects dependencies, maintain the image structure and preserve its semantics while transferring images into the target domain. For more realistic and diverse image generation we introduce style reference image. We enforce the model to maximize the mutual information between the style image and output. The proposed method explicitly maximizes the mutual information between the related patches, thus encouraging the generator to produce more diverse images. We validate the proposed algorithm for image-to-image translation and stylizing outdoor images. Both qualitative and quantitative results demonstrate the effectiveness of our approach on four dataset.
</details>
<details>
<summary>摘要</summary>
SCONE-GAN 提出了一种端到端图像翻译方法，可以学习生成真实和多样化的景观图像。现有的图像到图像翻译方法通常是分为两个映射：一个从源领域到目标领域的翻译，以及另一个用于表示其 inverse。虽然在许多应用程序中成功，但这些方法可能会导致生成质量偏低、有限多样性的图像。这是因为这些方法学习更频繁的相关性，而不是场景结构。为了解决这个问题，我们提出了 SCONE-GAN，它使用图像卷积网络学习物体的依赖关系，保持图像结构，并保持图像的 semantics while transferring images into the target domain。为了更真实和多样的图像生成，我们引入了风格参照图像。我们强制模型使得输出和风格参照图像之间的共聚信息最大化。这使得生成器更有可能生成更多样的图像。我们验证了我们的方法在图像到图像翻译和图像风格化方面的效果，并取得了四个数据集的质量和量化结果，都表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Information-Integration-and-Propagation-for-Occluded-Person-Re-identification"><a href="#Multi-view-Information-Integration-and-Propagation-for-Occluded-Person-Re-identification" class="headerlink" title="Multi-view Information Integration and Propagation for Occluded Person Re-identification"></a>Multi-view Information Integration and Propagation for Occluded Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03828">http://arxiv.org/abs/2311.03828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nengdong96/mviip">https://github.com/nengdong96/mviip</a></li>
<li>paper_authors: Neng Dong, Shuanglin Yan, Hao Tang, Jinhui Tang, Liyan Zhang</li>
<li>for: 本研究 targets at the challenging task of occluded person re-identification, and aims to effectively utilize multi-view images to characterize the occluded target pedestrian.</li>
<li>methods: 提出了一个名为 Multi-view Information Integration and Propagation (MVI$^{2}$P) 的新框架，具有统一特征地图，选择性地 инте그储��unted information，以及优化的资讯传播机制。</li>
<li>results: 经过广泛的实验和分析，确认了提案的优越性和有效性，并且可以在实际应用中提高人员识别率。<details>
<summary>Abstract</summary>
Occluded person re-identification (re-ID) presents a challenging task due to occlusion perturbations. Although great efforts have been made to prevent the model from being disturbed by occlusion noise, most current solutions only capture information from a single image, disregarding the rich complementary information available in multiple images depicting the same pedestrian. In this paper, we propose a novel framework called Multi-view Information Integration and Propagation (MVI$^{2}$P). Specifically, realizing the potential of multi-view images in effectively characterizing the occluded target pedestrian, we integrate feature maps of which to create a comprehensive representation. During this process, to avoid introducing occlusion noise, we develop a CAMs-aware Localization module that selectively integrates information contributing to the identification. Additionally, considering the divergence in the discriminative nature of different images, we design a probability-aware Quantification module to emphatically integrate highly reliable information. Moreover, as multiple images with the same identity are not accessible in the testing stage, we devise an Information Propagation (IP) mechanism to distill knowledge from the comprehensive representation to that of a single occluded image. Extensive experiments and analyses have unequivocally demonstrated the effectiveness and superiority of the proposed MVI$^{2}$P. The code will be released at \url{https://github.com/nengdong96/MVIIP}.
</details>
<details>
<summary>摘要</summary>
受遮挡干扰的人脸重新识别（re-ID）问题具有挑战性，尽管大量努力已经用于避免遮挡噪音的影响，但大多数当前解决方案只是利用单个图像中的信息。在这篇论文中，我们提出了一种新的框架，即多视图信息集成和传播（MVI$^{2}$P）。具体来说，我们利用多视图图像来有效地描述受遮挡目标人脸，然后将这些特征图像集成成一个完整的表示。在这个过程中，我们开发了一个自适应 Camera-Aware Localization 模块，以选择ively 集成有助于识别的信息。此外，我们还设计了一个可靠性感知 Quantification 模块，以强调集成信息的可靠性。最后，在测试阶段不可获得多个图像的同一个标识者，我们提出了信息传播（IP）机制，以储存知识从完整表示中传递到受遮挡图像上。我们的实验和分析证明了 MVI$^{2}$P 的有效性和优势。代码将在 GitHub 上发布，链接如下：https://github.com/nengdong96/MVIIP。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Any-Human-Object-Interaction-Relationship-Universal-HOI-Detector-with-Spatial-Prompt-Learning-on-Foundation-Models"><a href="#Detecting-Any-Human-Object-Interaction-Relationship-Universal-HOI-Detector-with-Spatial-Prompt-Learning-on-Foundation-Models" class="headerlink" title="Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models"></a>Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03799">http://arxiv.org/abs/2311.03799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caoyichao/unihoi">https://github.com/caoyichao/unihoi</a></li>
<li>paper_authors: Yichao Cao, Qingfei Tang, Xiu Su, Chen Song, Shan You, Xiaobo Lu, Chang Xu</li>
<li>for: 本研究旨在开探人物互动探测（HOI）识别在开放世界 Setting中的普遍性识别，透过视觉语言基础模型和大型语言模型（LLM）的结合，提高HOI识别的精度和效率。</li>
<li>methods: 本研究提出了一个名为UniHOI的方法，包括一个HO Prompt-guided Decoder（HOPD），将高水平关系表示与不同的HO对应对象之间的关系紧密相连，以及一个大型自然语言处理器（GPT），将互动的语言理解为更加丰富和复杂的概念。</li>
<li>results: 本研究的结果显示，UniHOI可以在不同的输入类型和类别下进行开放世界 Setting中的HOI识别，并且在对照方法的超过30%的测试数据上表现出显著的改善。代码和预训练组件可以在：<a target="_blank" rel="noopener" href="https://github.com/Caoyichao/UniHOI%E3%80%82">https://github.com/Caoyichao/UniHOI。</a><details>
<summary>Abstract</summary>
Human-object interaction (HOI) detection aims to comprehend the intricate relationships between humans and objects, predicting $<human, action, object>$ triplets, and serving as the foundation for numerous computer vision tasks. The complexity and diversity of human-object interactions in the real world, however, pose significant challenges for both annotation and recognition, particularly in recognizing interactions within an open world context. This study explores the universal interaction recognition in an open-world setting through the use of Vision-Language (VL) foundation models and large language models (LLMs). The proposed method is dubbed as \emph{\textbf{UniHOI}. We conduct a deep analysis of the three hierarchical features inherent in visual HOI detectors and propose a method for high-level relation extraction aimed at VL foundation models, which we call HO prompt-based learning. Our design includes an HO Prompt-guided Decoder (HOPD), facilitates the association of high-level relation representations in the foundation model with various HO pairs within the image. Furthermore, we utilize a LLM (\emph{i.e.} GPT) for interaction interpretation, generating a richer linguistic understanding for complex HOIs. For open-category interaction recognition, our method supports either of two input types: interaction phrase or interpretive sentence. Our efficient architecture design and learning methods effectively unleash the potential of the VL foundation models and LLMs, allowing UniHOI to surpass all existing methods with a substantial margin, under both supervised and zero-shot settings. The code and pre-trained weights are available at: \url{https://github.com/Caoyichao/UniHOI}.
</details>
<details>
<summary>摘要</summary>
人物交互检测（HOI）的目标是理解人与物之间的复杂关系，预测<人,动作,物>的 triplets，并作为许多计算机视觉任务的基础。然而，在真实世界中人物交互的复杂多样性和多样性带来了annotate和识别的挑战，特别是在开放世界上下文中。本研究通过使用视力语言基础模型（VL）和大型自然语言模型（LLM）来实现开放世界上的universal交互识别。我们提出了一种名为UniHOI的方法，包括HO Prompt-based Learning（HOPD），以帮助将高级关系表示与不同的HO对在图像中相关联。此外，我们使用GPT（一种大型自然语言模型）进行交互解释，以生成更加 ric
</details></li>
</ul>
<hr>
<h2 id="Self-MI-Efficient-Multimodal-Fusion-via-Self-Supervised-Multi-Task-Learning-with-Auxiliary-Mutual-Information-Maximization"><a href="#Self-MI-Efficient-Multimodal-Fusion-via-Self-Supervised-Multi-Task-Learning-with-Auxiliary-Mutual-Information-Maximization" class="headerlink" title="Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization"></a>Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03785">http://arxiv.org/abs/2311.03785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cam-Van Thi Nguyen, Ngoc-Hoa Thi Nguyen, Duc-Trong Le, Quang-Thuy Ha</li>
<li>for: 提高多模态融合的性能，解决现有方法困难于从多个模态中提取有用和明确特征的问题。</li>
<li>methods: 提出了Self-MI方法，基于自我超vised学习的思想，同时采用了协同预测编码（CPC）技术来最大化多modal输入对多modal融合结果的mutual Information（MI）。</li>
<li>results: 在三个benchmark dataset上进行了广泛的实验，证明Self-MI方法能够有效地提高多模态融合任务的性能。<details>
<summary>Abstract</summary>
Multimodal representation learning poses significant challenges in capturing informative and distinct features from multiple modalities. Existing methods often struggle to exploit the unique characteristics of each modality due to unified multimodal annotations. In this study, we propose Self-MI in the self-supervised learning fashion, which also leverage Contrastive Predictive Coding (CPC) as an auxiliary technique to maximize the Mutual Information (MI) between unimodal input pairs and the multimodal fusion result with unimodal inputs. Moreover, we design a label generation module, $ULG_{MI}$ for short, that enables us to create meaningful and informative labels for each modality in a self-supervised manner. By maximizing the Mutual Information, we encourage better alignment between the multimodal fusion and the individual modalities, facilitating improved multimodal fusion. Extensive experiments on three benchmark datasets including CMU-MOSI, CMU-MOSEI, and SIMS, demonstrate the effectiveness of Self-MI in enhancing the multimodal fusion task.
</details>
<details>
<summary>摘要</summary>
多Modal表示学习 pose significant challenges in capturing informative and distinct features from multiple modalities. Existing methods often struggle to exploit the unique characteristics of each modality due to unified multimodal annotations. In this study, we propose Self-MI in the self-supervised learning fashion, which also leverage Contrastive Predictive Coding (CPC) as an auxiliary technique to maximize the Mutual Information (MI) between unimodal input pairs and the multimodal fusion result with unimodal inputs. Moreover, we design a label generation module, $ULG_{MI}$ for short, that enables us to create meaningful and informative labels for each modality in a self-supervised manner. By maximizing the Mutual Information, we encourage better alignment between the multimodal fusion and the individual modalities, facilitating improved multimodal fusion. Extensive experiments on three benchmark datasets including CMU-MOSI, CMU-MOSEI, and SIMS, demonstrate the effectiveness of Self-MI in enhancing the multimodal fusion task.Here's the translation in Traditional Chinese:多 modal 表示学习 pose significant challenges in capturing informative and distinct features from multiple modalities. Existing methods often struggle to exploit the unique characteristics of each modality due to unified multimodal annotations. In this study, we propose Self-MI in the self-supervised learning fashion, which also leverage Contrastive Predictive Coding (CPC) as an auxiliary technique to maximize the Mutual Information (MI) between unimodal input pairs and the multimodal fusion result with unimodal inputs. Moreover, we design a label generation module, $ULG_{MI}$ for short, that enables us to create meaningful and informative labels for each modality in a self-supervised manner. By maximizing the Mutual Information, we encourage better alignment between the multimodal fusion and the individual modalities, facilitating improved multimodal fusion. Extensive experiments on three benchmark datasets including CMU-MOSI, CMU-MOSEI, and SIMS, demonstrate the effectiveness of Self-MI in enhancing the multimodal fusion task.
</details></li>
</ul>
<hr>
<h2 id="UP-NeRF-Unconstrained-Pose-Prior-Free-Neural-Radiance-Fields"><a href="#UP-NeRF-Unconstrained-Pose-Prior-Free-Neural-Radiance-Fields" class="headerlink" title="UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields"></a>UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03784">http://arxiv.org/abs/2311.03784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Injae Kim, Minhyuk Choi, Hyunwoo J. Kim</li>
<li>for: 这篇论文的目的是提出一种不受拘束的图像集合中的NeRF优化方法，不需要摄像头pose估计。</li>
<li>methods: 这篇论文使用了一种新的替代任务来优化颜色不敏感特征场和一个独立的模块来屏蔽异常 occluders 的影响，以及一个候选头来提高pose估计的稳定性。</li>
<li>results: 对于一个在互联网上的挑战性图像集合，这篇论文的实验结果表明，我们的方法在比基elines，包括BARF和其变种的方法之上表现出了超越性。<details>
<summary>Abstract</summary>
Neural Radiance Field (NeRF) has enabled novel view synthesis with high fidelity given images and camera poses. Subsequent works even succeeded in eliminating the necessity of pose priors by jointly optimizing NeRF and camera pose. However, these works are limited to relatively simple settings such as photometrically consistent and occluder-free image collections or a sequence of images from a video. So they have difficulty handling unconstrained images with varying illumination and transient occluders. In this paper, we propose $\textbf{UP-NeRF}$ ($\textbf{U}$nconstrained $\textbf{P}$ose-prior-free $\textbf{Ne}$ural $\textbf{R}$adiance $\textbf{F}$ields) to optimize NeRF with unconstrained image collections without camera pose prior. We tackle these challenges with surrogate tasks that optimize color-insensitive feature fields and a separate module for transient occluders to block their influence on pose estimation. In addition, we introduce a candidate head to enable more robust pose estimation and transient-aware depth supervision to minimize the effect of incorrect prior. Our experiments verify the superior performance of our method compared to the baselines including BARF and its variants in a challenging internet photo collection, $\textit{Phototourism}$ dataset.
</details>
<details>
<summary>摘要</summary>
neuronal radiance field (NeRF) 已经实现了基于图像和摄像机位置的新视图合成，并且后续的工作甚至已经消除了摄像机位置的必要性，通过共同优化 NeRF 和摄像机位置。然而，这些工作受到了相对简单的设置的限制，例如具有相同的光度和没有遮挡物的图像集或视频序列。因此它们在面临不受限制的图像集和变化的照明和遮挡物时表现不佳。在这篇论文中，我们提出了 $\textbf{UP-NeRF}$（无约束 pose-prior-free neural radiance field），用于在不受限制的图像集中优化 NeRF  без摄像机位置优先。我们通过使用代表任务来优化不受光度影响的特征场和分离模块来处理变化的照明和遮挡物。此外，我们还引入了候选头来实现更加稳定的pose估计和抗不正确优先的深度监督，以降低不正确优先的影响。我们的实验证明了我们的方法比基于 BARF 和其他变体的基eline在《phototourism》数据集上表现出更高的性能。
</details></li>
</ul>
<hr>
<h2 id="CapST-An-Enhanced-and-Lightweight-Method-for-Deepfake-Video-Classification"><a href="#CapST-An-Enhanced-and-Lightweight-Method-for-Deepfake-Video-Classification" class="headerlink" title="CapST: An Enhanced and Lightweight Method for Deepfake Video Classification"></a>CapST: An Enhanced and Lightweight Method for Deepfake Video Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03782">http://arxiv.org/abs/2311.03782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wasim Ahmad, Yan-Tsung Peng, Yuan-Hao Chang, Gaddisa Olani Ganfure, Sarwar Khan, Sahibzada Adil Shahzad</li>
<li>for: 本研究旨在针对深伪视频（Deepfake video）进行分类，以满足不同领域的需求。</li>
<li>methods: 本研究提出了一种新型的深伪视频分类模型，利用VGG19bn的一部分作为底层特征提取器，并在这之上加入了卷积网络和空间时间注意力机制来提高分类能力。</li>
<li>results: 实验结果表明，与基eline模型相比，本研究的方法可以提高深伪视频的分类精度，同时减少计算资源的消耗。在DFDM数据集上，本研究的方法可以达到4%的改善率。<details>
<summary>Abstract</summary>
The proliferation of deepfake videos, synthetic media produced through advanced Artificial Intelligence techniques has raised significant concerns across various sectors, encompassing realms such as politics, entertainment, and security. In response, this research introduces an innovative and streamlined model designed to classify deepfake videos generated by five distinct encoders adeptly. Our approach not only achieves state of the art performance but also optimizes computational resources. At its core, our solution employs part of a VGG19bn as a backbone to efficiently extract features, a strategy proven effective in image-related tasks. We integrate a Capsule Network coupled with a Spatial Temporal attention mechanism to bolster the model's classification capabilities while conserving resources. This combination captures intricate hierarchies among features, facilitating robust identification of deepfake attributes. Delving into the intricacies of our innovation, we introduce an existing video level fusion technique that artfully capitalizes on temporal attention mechanisms. This mechanism serves to handle concatenated feature vectors, capitalizing on the intrinsic temporal dependencies embedded within deepfake videos. By aggregating insights across frames, our model gains a holistic comprehension of video content, resulting in more precise predictions. Experimental results on an extensive benchmark dataset of deepfake videos called DFDM showcase the efficacy of our proposed method. Notably, our approach achieves up to a 4 percent improvement in accurately categorizing deepfake videos compared to baseline models, all while demanding fewer computational resources.
</details>
<details>
<summary>摘要</summary>
深刻关注深伪视频的扩散，这种通过高级人工智能技术生成的伪造媒体，已经在不同领域引起了严重的担忧，包括政治、娱乐和安全等。为了应对这些挑战，这项研究推出了一种创新的和高效的模型，用于分类深伪视频。我们的方法不仅实现了状态机器的性能，而且还优化了计算资源。我们的解决方案在核心上采用了VGG19bn的一部分作为特征提取的后备，这是在图像相关任务中证明有效的策略。我们还将卷积网络和空间时间注意力机制相结合，以强化模型的分类能力，同时减少计算资源的消耗。这种结合使得模型能够快速和高效地分类深伪视频。具体来说，我们引入了一种现有的视频级别融合技术，通过利用时间注意力机制，处理 concatenated 特征向量。这种机制使得模型可以充分利用深伪视频中的自然时间依赖关系，从而更好地理解视频内容，并且提高了预测的精度。实验结果表明，我们的提议方法在大规模的深伪视频测试集（DFDM）上达到了4%的改善率，同时减少了计算资源的消耗。
</details></li>
</ul>
<hr>
<h2 id="Meta-Adapter-An-Online-Few-shot-Learner-for-Vision-Language-Model"><a href="#Meta-Adapter-An-Online-Few-shot-Learner-for-Vision-Language-Model" class="headerlink" title="Meta-Adapter: An Online Few-shot Learner for Vision-Language Model"></a>Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03774">http://arxiv.org/abs/2311.03774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Cheng, Lin Song, Ruoyi Xue, Hang Wang, Hongbin Sun, Yixiao Ge, Ying Shan</li>
<li>for: 提高具有少量样本的图像识别效果和泛化能力</li>
<li>methods: 使用轻量级径补器来在线调整CLIP特征，以适应少量样本学习</li>
<li>results: 与州对比，我们的方法可以在八个图像分类任务上实现竞争性的性能，同时具有更高的执行速度和灵活性，并可以直接应用于下游任务无需进一步微调。<details>
<summary>Abstract</summary>
The contrastive vision-language pre-training, known as CLIP, demonstrates remarkable potential in perceiving open-world visual concepts, enabling effective zero-shot image recognition. Nevertheless, few-shot learning methods based on CLIP typically require offline fine-tuning of the parameters on few-shot samples, resulting in longer inference time and the risk of over-fitting in certain domains. To tackle these challenges, we propose the Meta-Adapter, a lightweight residual-style adapter, to refine the CLIP features guided by the few-shot samples in an online manner. With a few training samples, our method can enable effective few-shot learning capabilities and generalize to unseen data or tasks without additional fine-tuning, achieving competitive performance and high efficiency. Without bells and whistles, our approach outperforms the state-of-the-art online few-shot learning method by an average of 3.6\% on eight image classification datasets with higher inference speed. Furthermore, our model is simple and flexible, serving as a plug-and-play module directly applicable to downstream tasks. Without further fine-tuning, Meta-Adapter obtains notable performance improvements in open-vocabulary object detection and segmentation tasks.
</details>
<details>
<summary>摘要</summary>
《对比式视觉语言预训练（CLIP）表现出了开放世界视觉概念的惊人潜力，使得零批学习成为可能。然而，基于CLIP的几批学习方法通常需要离线参数的微调，导致更长的推理时间和特定领域中的风险溢出。为解决这些挑战，我们提出了Meta-Adapter，一个轻量级的残差风格的适配器，通过指导几批样本来细化CLIP特征。只需几个训练样本，我们的方法可以实现有效的几批学习能力和扩展到未经过训练的数据或任务，达到竞争性的性能和高效性。无需额外配置或微调，我们的方法比预先 trained State-of-the-art online几批学习方法平均提高3.6%的性能在八个图像分类dataset中，同时具有更高的推理速度。此外，我们的方法简单和灵活，可以直接应用于下游任务，无需进一步微调。 Meta-Adapter在开 vocabulary对象检测和分割任务中表现出了显著的性能提高，无需进一步微调。》
</details></li>
</ul>
<hr>
<h2 id="Lightweight-Portrait-Matting-via-Regional-Attention-and-Refinement"><a href="#Lightweight-Portrait-Matting-via-Regional-Attention-and-Refinement" class="headerlink" title="Lightweight Portrait Matting via Regional Attention and Refinement"></a>Lightweight Portrait Matting via Regional Attention and Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03770">http://arxiv.org/abs/2311.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatao Zhong, Ilya Zharkov</li>
<li>for: 高解析人脸抹铺模型</li>
<li>methods: 使用两stage框架，利用视transformer（ViT）作为低分辨率网络的背景，并提出了一种新的跨区域注意力（CRA）模块来传递邻近区域的信息。</li>
<li>results: 对三个标准测试集进行比较，获得了与其他基eline模型相比的更高的抹铺质量，并且只用了$1&#x2F;20$的FLOPS。<details>
<summary>Abstract</summary>
We present a lightweight model for high resolution portrait matting. The model does not use any auxiliary inputs such as trimaps or background captures and achieves real time performance for HD videos and near real time for 4K. Our model is built upon a two-stage framework with a low resolution network for coarse alpha estimation followed by a refinement network for local region improvement. However, a naive implementation of the two-stage model suffers from poor matting quality if not utilizing any auxiliary inputs. We address the performance gap by leveraging the vision transformer (ViT) as the backbone of the low resolution network, motivated by the observation that the tokenization step of ViT can reduce spatial resolution while retain as much pixel information as possible. To inform local regions of the context, we propose a novel cross region attention (CRA) module in the refinement network to propagate the contextual information across the neighboring regions. We demonstrate that our method achieves superior results and outperforms other baselines on three benchmark datasets while only uses $1/20$ of the FLOPS compared to the existing state-of-the-art model.
</details>
<details>
<summary>摘要</summary>
我们提出了一种轻量级的高解像人脸分割模型。该模型不使用任何辅助输入，如截图或背景捕获，并在高清视频和4K视频实时性能上达到了实时性能。我们的模型基于一个两Stage框架，其中低分辨率网络用于粗略 alpha 估计，然后是一个改进网络用于本地区域改进。然而，直接使用两Stage模型会导致分割质量差，因此我们使用视transformer（ViT）作为低分辨率网络的基础，这是因为我们发现，ViT的Tokenization步骤可以减少空间分辨率，同时保留最多的像素信息。在改进网络中，我们提出了一种新的跨地区注意力（CRA）模块，用于在邻近区域之间传递Contextual信息。我们示示，我们的方法可以在三个标准数据集上实现更高的性能，并且只需要使用$1/20$ 的计算资源，相比之前的State-of-the-art模型。
</details></li>
</ul>
<hr>
<h2 id="Image-change-detection-with-only-a-few-samples"><a href="#Image-change-detection-with-only-a-few-samples" class="headerlink" title="Image change detection with only a few samples"></a>Image change detection with only a few samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03762">http://arxiv.org/abs/2311.03762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Ke Liu, Zhaoyi Song, Haoyue Bai</li>
<li>for: solving the problem of image change detection with a small number of samples, and improving the generalization ability of the model.</li>
<li>methods: using simple image processing methods to generate synthetic datasets, and designing an early fusion network based on object detection.</li>
<li>results: the model trained on the synthetic data achieves higher generalization ability than the model trained on the real-world data, and fine-tuning the model with a few samples achieves excellent results.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在解决图像变化检测 task 中的小样本问题，该问题在图像检测 task 中是一个重要的难题，因为只有很少的标注数据可用。</li>
<li>methods: 我们提出使用简单的图像处理方法生成synthetic数据，并设计一种基于对象检测的早期融合网络。</li>
<li>results: 我们的实验表明，使用synthetic数据可以提高模型的通用能力，并且使用一些（常 tens of）样本来精度调整模型可以达到优秀的结果。<details>
<summary>Abstract</summary>
This paper considers image change detection with only a small number of samples, which is a significant problem in terms of a few annotations available. A major impediment of image change detection task is the lack of large annotated datasets covering a wide variety of scenes. Change detection models trained on insufficient datasets have shown poor generalization capability. To address the poor generalization issue, we propose using simple image processing methods for generating synthetic but informative datasets, and design an early fusion network based on object detection which could outperform the siamese neural network. Our key insight is that the synthetic data enables the trained model to have good generalization ability for various scenarios. We compare the model trained on the synthetic data with that on the real-world data captured from a challenging dataset, CDNet, using six different test sets. The results demonstrate that the synthetic data is informative enough to achieve higher generalization ability than the insufficient real-world data. Besides, the experiment shows that utilizing a few (often tens of) samples to fine-tune the model trained on the synthetic data will achieve excellent results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multiclass-Segmentation-using-Teeth-Attention-Modules-for-Dental-X-ray-Images"><a href="#Multiclass-Segmentation-using-Teeth-Attention-Modules-for-Dental-X-ray-Images" class="headerlink" title="Multiclass Segmentation using Teeth Attention Modules for Dental X-ray Images"></a>Multiclass Segmentation using Teeth Attention Modules for Dental X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03749">http://arxiv.org/abs/2311.03749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afnan Ghafoor, Seong-Yong Moon, Bumshik Lee</li>
<li>for: 这篇文章的目的是提出一个高效的多类 teeth 分类架构，以帮助解决 dental panoramic 影像中 teeth 的分类问题。</li>
<li>methods: 这篇文章使用了一个 M-Net-like 架构，融合了 Swin Transformers 和一个名为 Teeth Attention Block (TAB) 的新 комponent。TAB 使用了一个特殊的注意机制，专注在 teeth 的复杂结构上，从而获得更高的准确性。</li>
<li>results: 这篇文章的实验结果显示，提出的架构在多个 benchmark dental 影像数据集上，具有较高的准确性和可靠性，并且在多类 teeth 分类任务上表现出色。<details>
<summary>Abstract</summary>
This paper proposed a cutting-edge multiclass teeth segmentation architecture that integrates an M-Net-like structure with Swin Transformers and a novel component named Teeth Attention Block (TAB). Existing teeth image segmentation methods have issues with less accurate and unreliable segmentation outcomes due to the complex and varying morphology of teeth, although teeth segmentation in dental panoramic images is essential for dental disease diagnosis. We propose a novel teeth segmentation model incorporating an M-Net-like structure with Swin Transformers and TAB. The proposed TAB utilizes a unique attention mechanism that focuses specifically on the complex structures of teeth. The attention mechanism in TAB precisely highlights key elements of teeth features in panoramic images, resulting in more accurate segmentation outcomes. The proposed architecture effectively captures local and global contextual information, accurately defining each tooth and its surrounding structures. Furthermore, we employ a multiscale supervision strategy, which leverages the left and right legs of the U-Net structure, boosting the performance of the segmentation with enhanced feature representation. The squared Dice loss is utilized to tackle the class imbalance issue, ensuring accurate segmentation across all classes. The proposed method was validated on a panoramic teeth X-ray dataset, which was taken in a real-world dental diagnosis. The experimental results demonstrate the efficacy of our proposed architecture for tooth segmentation on multiple benchmark dental image datasets, outperforming existing state-of-the-art methods in objective metrics and visual examinations. This study has the potential to significantly enhance dental image analysis and contribute to advances in dental applications.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种革新的多类牙齿分割建议，它将M-Net-like结构和Swin Transformers结合并提出了一个新的组件名为牙齿注意块（TAB）。现有的牙齿图像分割方法存在较为不准确和不可靠的分割结果，尽管牙齿图像分割在 dental 照片中是重要的 dental 疾病诊断。我们提出了一种新的牙齿分割模型，其包括M-Net-like结构、Swin Transformers和TAB。TAB 使用了一种特殊的注意机制，它准确地强调牙齿的复杂结构特征。TAB 中的注意机制可以准确地高亮牙齿图像中的关键特征，从而提高分割结果的准确性。我们的建议可以有效地捕捉局部和全局的情况概念信息，准确地定义每个牙齿和其周围结构。此外，我们采用了多尺度超级视修正策略，这将左右两个 U-Net 结构的左右脚使用，从而提高分割性能。我们使用了平方 dice 损失函数来处理类偏好问题，以确保分割结果准确性。我们的提议在一个 panoramic 牙齿 X-ray 数据集上进行验证，并在多个标准 dental 图像数据集上进行了多个基准测试。实验结果表明，我们的提议可以在多个对象指标和视觉检查中出perform state-of-the-art 牙齿分割。这篇研究有可能对 dental 图像分析产生重要影响，并为 dental 应用领域带来进步。
</details></li>
</ul>
<hr>
<h2 id="SBCFormer-Lightweight-Network-Capable-of-Full-size-ImageNet-Classification-at-1-FPS-on-Single-Board-Computers"><a href="#SBCFormer-Lightweight-Network-Capable-of-Full-size-ImageNet-Classification-at-1-FPS-on-Single-Board-Computers" class="headerlink" title="SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers"></a>SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03747">http://arxiv.org/abs/2311.03747</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyonglu/sbcformer">https://github.com/xyonglu/sbcformer</a></li>
<li>paper_authors: Xiangyong Lu, Masanori Suganuma, Takayuki Okatani</li>
<li>for: 这篇论文主要针对单板计算机（SBC）上进行图像识别任务，以解决不同领域的实际问题，如智能农业、渔业和畜牧管理。</li>
<li>methods: 该论文提出了一种名为SBCFormer的CNN-ViT混合网络，可以在低端CPU上实现高精度和快速计算。为了解决低端CPU的硬件限制，该网络采用了注意力机制而不是卷积。</li>
<li>results: 该论文在一个Raspberry Pi 4 Model B上实现了一个ImageNet-1K top-1准确率超过80%，并且达到了1.0帧&#x2F;秒的速度。这是首次在SBC上实现的。代码可以在<a target="_blank" rel="noopener" href="https://github.com/xyongLu/SBCFormer%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/xyongLu/SBCFormer中下载。</a><details>
<summary>Abstract</summary>
Computer vision has become increasingly prevalent in solving real-world problems across diverse domains, including smart agriculture, fishery, and livestock management. These applications may not require processing many image frames per second, leading practitioners to use single board computers (SBCs). Although many lightweight networks have been developed for mobile/edge devices, they primarily target smartphones with more powerful processors and not SBCs with the low-end CPUs. This paper introduces a CNN-ViT hybrid network called SBCFormer, which achieves high accuracy and fast computation on such low-end CPUs. The hardware constraints of these CPUs make the Transformer's attention mechanism preferable to convolution. However, using attention on low-end CPUs presents a challenge: high-resolution internal feature maps demand excessive computational resources, but reducing their resolution results in the loss of local image details. SBCFormer introduces an architectural design to address this issue. As a result, SBCFormer achieves the highest trade-off between accuracy and speed on a Raspberry Pi 4 Model B with an ARM-Cortex A72 CPU. For the first time, it achieves an ImageNet-1K top-1 accuracy of around 80% at a speed of 1.0 frame/sec on the SBC. Code is available at https://github.com/xyongLu/SBCFormer.
</details>
<details>
<summary>摘要</summary>
计算机视觉在解决实际问题方面日益普遍，包括智能农业、渔业和畜牧管理等领域。这些应用程序可能不需要处理多个图像帧每秒，因此实际使用单板计算机（SBC）。虽然许多轻量级网络已经为移动/边缘设备开发，但它们主要target于智能手机with更强大的处理器，而不是SBC with low-end CPU。本文介绍了一种名为SBCFormer的CNN-ViT混合网络，它在low-end CPU上实现了高准确率和快速计算。由于硬件限制，这些CPU的Transformer注意机制更加有利，但在low-end CPU上使用注意会存在一个挑战：高分辨率内部特征图需要过度的计算资源，但是降低分辨率会导致失去地方图像细节。SBCFormer提出了一种建筑设计来解决这个问题。因此，SBCFormer在raspberry Pi 4 Model B上的ARM-Cortex A72 CPU上实现了 ImageNet-1K 的top-1准确率约为80%，并且在1.0帧/秒的速度下。Code可以在https://github.com/xyongLu/SBCFormer 上获取。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Video-Summarization"><a href="#Unsupervised-Video-Summarization" class="headerlink" title="Unsupervised Video Summarization"></a>Unsupervised Video Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03745">http://arxiv.org/abs/2311.03745</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KaiyangZhou/pytorch-vsumm-reinforce">https://github.com/KaiyangZhou/pytorch-vsumm-reinforce</a></li>
<li>paper_authors: Hanqing Li, Diego Klabjan, Jean Utke</li>
<li>for: 本研究提出了一种新的、无监督的自动视频摘要方法，利用生成对抗网络的想法，但是减少了识别器，使用简单的损失函数，并将模型的不同部分分别训练。</li>
<li>methods: 本方法使用迭代训练策略，先训练恢复器，然后训练帧选择器，并在训练和评估中添加了可调 máscara вектор。</li>
<li>results: 在两个公共数据集（SumMe和TVSum）以及四个自定义数据集（足球、LoL、MLB和ShortMLB）上进行了实验，结果表明每个组件对模型性能的影响，特别是迭代训练策略。评估和比较与当前状态的方法显示了提议方法的优势在性能、稳定性和训练效率上。<details>
<summary>Abstract</summary>
This paper introduces a new, unsupervised method for automatic video summarization using ideas from generative adversarial networks but eliminating the discriminator, having a simple loss function, and separating training of different parts of the model. An iterative training strategy is also applied by alternately training the reconstructor and the frame selector for multiple iterations. Furthermore, a trainable mask vector is added to the model in summary generation during training and evaluation. The method also includes an unsupervised model selection algorithm. Results from experiments on two public datasets (SumMe and TVSum) and four datasets we created (Soccer, LoL, MLB, and ShortMLB) demonstrate the effectiveness of each component on the model performance, particularly the iterative training strategy. Evaluations and comparisons with the state-of-the-art methods highlight the advantages of the proposed method in performance, stability, and training efficiency.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的、无监督的自动视频摘要方法，利用生成对抗网络的想法，但是去掉了判断器，使用简单的损失函数，并将模型的不同部分分别训练。此外，还采用了 alternate 训练策略，在多轮训练中交替训练重构器和帧选择器。此外，在摘要生成过程中，还添加了可学习的面具向量。该方法还包括一种无监督模型选择算法。经过对两个公共数据集（SumMe和TVSum）以及我们自己创建的四个数据集（足球、LoL、MLB和ShortMLB）进行实验，结果表明每个组件对模型性能的影响，特别是 alternate 训练策略。评估和与现有方法进行比较，highlighted 提出的方法在性能、稳定性和训练效率方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="3DifFusionDet-Diffusion-Model-for-3D-Object-Detection-with-Robust-LiDAR-Camera-Fusion"><a href="#3DifFusionDet-Diffusion-Model-for-3D-Object-Detection-with-Robust-LiDAR-Camera-Fusion" class="headerlink" title="3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion"></a>3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03742">http://arxiv.org/abs/2311.03742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhao Xiang, Simon Dräger, Jiawei Zhang</li>
<li>for: 这篇论文主要target是提高3D物体检测性能，尤其是使用LiDAR-Camera感知器时的检测性能。</li>
<li>methods: 该论文提出了一种名为3DifFusionDet的框架，它将3D物体检测看作是一种杂散扩散过程，从噪声3D框架中减少噪声。在训练过程中，模型学习了反噪声过程，并在检测过程中逐渐精细化一组随机生成的框架。</li>
<li>results: 经过广泛的实验，3DifFusionDet在KITTI数据集上表现出色，与之前的高度尊敬的检测器相比，具有更高的准确率和速度。<details>
<summary>Abstract</summary>
Good 3D object detection performance from LiDAR-Camera sensors demands seamless feature alignment and fusion strategies. We propose the 3DifFusionDet framework in this paper, which structures 3D object detection as a denoising diffusion process from noisy 3D boxes to target boxes. In this framework, ground truth boxes diffuse in a random distribution for training, and the model learns to reverse the noising process. During inference, the model gradually refines a set of boxes that were generated at random to the outcomes. Under the feature align strategy, the progressive refinement method could make a significant contribution to robust LiDAR-Camera fusion. The iterative refinement process could also demonstrate great adaptability by applying the framework to various detecting circumstances where varying levels of accuracy and speed are required. Extensive experiments on KITTI, a benchmark for real-world traffic object identification, revealed that 3DifFusionDet is able to perform favorably in comparison to earlier, well-respected detectors.
</details>
<details>
<summary>摘要</summary>
好的3D物体检测性能从激光镜头感知器件中获得，需要无缝特征对应和融合策略。我们在这篇论文中提出了3DifFusionDet框架，它将3D物体检测视为一种噪声损失进程，从噪声3D框架中减噪到目标框架。在这个框架中，真实的框架在训练中随机分布的情况下噪声扩散，模型学习恢复过程。在推理中，模型逐渐精细化一组随机生成的框架，以达到最终结果。在特征对应策略下，进程式精细化方法可以对精度和速度的不同需求进行适应。另外，iterative refinement过程也能够在不同的检测情况下展现出优秀的适应性。在KITTIbenchmark上进行了广泛的实验，发现3DifFusionDet能够与以往受欢迎的检测器相比，表现出优异的性能。
</details></li>
</ul>
<hr>
<h2 id="ADFactory-Automated-Data-Factory-for-Optical-Flow-Tasks"><a href="#ADFactory-Automated-Data-Factory-for-Optical-Flow-Tasks" class="headerlink" title="ADFactory: Automated Data Factory for Optical Flow Tasks"></a>ADFactory: Automated Data Factory for Optical Flow Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04246">http://arxiv.org/abs/2311.04246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Ling</li>
<li>for: 提高现代光流方法在真实世界中的普适性， mainly due to the high cost of large real-world optical flow datasets.</li>
<li>methods: 引入一种新的光流训练框架，可以效率地在目标数据频谱上训练光流网络，无需手动标注。 Specifically, 使用高级 nerf 技术重建场景从单目镜头摄像头中收集的照片组，并计算相机pose对的光流结果。 在这基础上，对生成的训练数据进行多种筛选，如 nerf 重建质量、视觉光流标签的视觉一致性、重建深度一致性等。</li>
<li>results: 实验表明，我们的方案在 KITTI 上的普适性超过现有的自我超vised optical flow和单目场景流算法，并且可以在真实世界中超越大多数超vised方法的零点普适评估。<details>
<summary>Abstract</summary>
A major challenge faced by current optical flow methods is the difficulty in generalizing them well into the real world, mainly due to the high production cost of datasets, which currently do not have a large real-world optical flow dataset. To address this challenge, we introduce a novel optical flow training framework that can efficiently train optical flow networks on the target data domain without manual annotation. Specifically, we use advanced Nerf technology to reconstruct scenes from photo groups collected by monocular cameras, and calculate the optical flow results between camera pose pairs from the rendered results. On this basis, we screen the generated training data from various aspects such as Nerf's reconstruction quality, visual consistency of optical flow labels, reconstruction depth consistency, etc. The filtered training data can be directly used for network supervision. Experimentally, the generalization ability of our scheme on KITTI surpasses existing self-supervised optical flow and monocular scene flow algorithms. Moreover, it can always surpass most supervised methods in real-world zero-point generalization evaluation.
</details>
<details>
<summary>摘要</summary>
当前光流方法面临的一个主要挑战是将其通过到真实世界中，主要是因为光流数据集的生产成本过高，目前没有大量的真实世界光流数据集。为解决这个挑战，我们提出了一种新的光流训练框架，可以高效地训练光流网络在目标数据域中无需手动标注。特别是，我们使用了先进的Nerf技术来重建场景从单目相机采集的照片组，并计算相机 pose对的光流结果。基于这个基础，我们从多个方面筛选生成的训练数据，如Nerf的重建质量、光流标签的视觉一致性、重建深度一致性等。经验表明，我们的方案在KITTI上的总体化能力超过现有的自动化光流和单目场景流算法，同时也可以在实际世界中超越大多数指导方法的零点总成评价。
</details></li>
</ul>
<hr>
<h2 id="DeepInspect-An-AI-Powered-Defect-Detection-for-Manufacturing-Industries"><a href="#DeepInspect-An-AI-Powered-Defect-Detection-for-Manufacturing-Industries" class="headerlink" title="DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries"></a>DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03725">http://arxiv.org/abs/2311.03725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arti Kumbhar, Amruta Chougule, Priya Lokhande, Saloni Navaghane, Aditi Burud, Saee Nimbalkar</li>
<li>For: 这个系统是为了实现生产过程中的问题检测，以提高产品质量和生产效率。* Methods: 这个系统使用了卷积神经网络（CNNs）、回传神经网络（RNNs）和生成敌方模型（GANs），实现产品图像中的问题检测。它利用RNNs检测产品中的变化错误，并使用生成的伪错误数据强化模型的类型和适应性。* Results: 这个系统可以实现高精度的问题检测，并且可以在生产过程中实现即时自动化检测。这有助于减少废料和生产成本，最终提高产品质量和市场竞争力。<details>
<summary>Abstract</summary>
Utilizing Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs), our system introduces an innovative approach to defect detection in manufacturing. This technology excels in precisely identifying faults by extracting intricate details from product photographs, utilizing RNNs to detect evolving errors and generating synthetic defect data to bolster the model's robustness and adaptability across various defect scenarios. The project leverages a deep learning framework to automate real-time flaw detection in the manufacturing process. It harnesses extensive datasets of annotated images to discern complex defect patterns. This integrated system seamlessly fits into production workflows, thereby boosting efficiency and elevating product quality. As a result, it reduces waste and operational costs, ultimately enhancing market competitiveness.
</details>
<details>
<summary>摘要</summary>
我团队使用卷积神经网络（CNN）、回归神经网络（RNN）和生成抗击网络（GAN），开发了一种革新的产品瑕疵检测技术。这种技术可以准确地检测产品照片中的瑕疵，通过提取产品图像中的细节来检测错误，并使用RNN检测错误的发展趋势。此外，我们还生成了假瑕疵数据，以增强模型的可靠性和适应性。这个整体系统可以自动化生产过程中的瑕疵检测，并利用大量标注图像来识别复杂的瑕疵模式。这个系统可以轻松地整合到生产工序中，从而提高效率和产品质量。最终，它可以减少废物和运营成本，从而提高市场竞争力。
</details></li>
</ul>
<hr>
<h2 id="Inertial-Guided-Uncertainty-Estimation-of-Feature-Correspondence-in-Visual-Inertial-Odometry-SLAM"><a href="#Inertial-Guided-Uncertainty-Estimation-of-Feature-Correspondence-in-Visual-Inertial-Odometry-SLAM" class="headerlink" title="Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry&#x2F;SLAM"></a>Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry&#x2F;SLAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03722">http://arxiv.org/abs/2311.03722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongwook Yoon, Jaehyun Kim, Sanghoon Sull</li>
<li>for: This paper aims to improve the accuracy of visual odometry and Simultaneous Localization And Mapping (SLAM) methods by estimating the uncertainty of feature correspondence using an inertial guidance robust to image degradation.</li>
<li>methods: The proposed method models a guidance distribution to sample possible correspondence and fits the distribution to an energy function based on image error, yielding more robust uncertainty than conventional methods.</li>
<li>results: The authors demonstrate the feasibility of their approach by incorporating it into one of recent visual-inertial odometry&#x2F;SLAM algorithms for public datasets.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文目的是提高视觉自律和同时地图标定（SLAM）方法的准确性，通过对各个图像中的特征点进行可靠的uncertainty估计。</li>
<li>methods: 提议的方法是通过模拟可能的对应关系，并基于图像错误的能量函数来适应可靠的uncertainty估计。</li>
<li>results: 作者们在使用一种现有的视觉-各种odometry&#x2F;SLAM算法中进行实验，以证明方法的可行性。<details>
<summary>Abstract</summary>
Visual odometry and Simultaneous Localization And Mapping (SLAM) has been studied as one of the most important tasks in the areas of computer vision and robotics, to contribute to autonomous navigation and augmented reality systems. In case of feature-based odometry/SLAM, a moving visual sensor observes a set of 3D points from different viewpoints, correspondences between the projected 2D points in each image are usually established by feature tracking and matching. However, since the corresponding point could be erroneous and noisy, reliable uncertainty estimation can improve the accuracy of odometry/SLAM methods. In addition, inertial measurement unit is utilized to aid the visual sensor in terms of Visual-Inertial fusion. In this paper, we propose a method to estimate the uncertainty of feature correspondence using an inertial guidance robust to image degradation caused by motion blur, illumination change and occlusion. Modeling a guidance distribution to sample possible correspondence, we fit the distribution to an energy function based on image error, yielding more robust uncertainty than conventional methods. We also demonstrate the feasibility of our approach by incorporating it into one of recent visual-inertial odometry/SLAM algorithms for public datasets.
</details>
<details>
<summary>摘要</summary>
视觉增程和同时地图化（SLAM）已被视为计算机视觉和机器人领域中最重要的任务之一，以帮助实现自主导航和增强现实系统。在特征基于的增程/SLAM中，一个移动的视觉传感器观察到了不同视角的3D点集，通常通过特征跟踪和匹配来确定图像中的2D点对应关系。然而，由于对应点可能含误差和杂音，可靠的uncertainty估计可以提高增程/SLAM方法的准确性。此外，使用导航单元以帮助视觉传感器进行视觉-导航融合也是非常重要。在这篇论文中，我们提出了一种用于估计特征对应关系的不确定性的方法，该方法基于图像误差能量函数，可以快速和精准地估计不确定性。我们还通过将该方法与现有的视觉-导航增程/SLAM算法结合来证明其可行性。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-deep-representation-learning-for-quantum-cross-platform-verification"><a href="#Multimodal-deep-representation-learning-for-quantum-cross-platform-verification" class="headerlink" title="Multimodal deep representation learning for quantum cross-platform verification"></a>Multimodal deep representation learning for quantum cross-platform verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03713">http://arxiv.org/abs/2311.03713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Qian, Yuxuan Du, Zhenliang He, Min-hsiu Hsieh, Dacheng Tao</li>
<li>for:  Early-stage quantum computing cross-platform verification, aiming to characterize the similarity of two imperfect quantum devices executing identical algorithms with minimal measurements.</li>
<li>methods:  Multimodal learning approach, leveraging measurement outcomes and classical description of compiled circuits on explored quantum devices, fused through a neural network to create a comprehensive data representation.</li>
<li>results:  Three-orders-of-magnitude improvement in prediction accuracy compared to random measurements, demonstrating the complementary roles of each modality in cross-platform verification, and paving the way for harnessing multimodal learning in wider quantum system learning tasks.<details>
<summary>Abstract</summary>
Cross-platform verification, a critical undertaking in the realm of early-stage quantum computing, endeavors to characterize the similarity of two imperfect quantum devices executing identical algorithms, utilizing minimal measurements. While the random measurement approach has been instrumental in this context, the quasi-exponential computational demand with increasing qubit count hurdles its feasibility in large-qubit scenarios. To bridge this knowledge gap, here we introduce an innovative multimodal learning approach, recognizing that the formalism of data in this task embodies two distinct modalities: measurement outcomes and classical description of compiled circuits on explored quantum devices, both enriched with unique information. Building upon this insight, we devise a multimodal neural network to independently extract knowledge from these modalities, followed by a fusion operation to create a comprehensive data representation. The learned representation can effectively characterize the similarity between the explored quantum devices when executing new quantum algorithms not present in the training data. We evaluate our proposal on platforms featuring diverse noise models, encompassing system sizes up to 50 qubits. The achieved results demonstrate a three-orders-of-magnitude improvement in prediction accuracy compared to the random measurements and offer compelling evidence of the complementary roles played by each modality in cross-platform verification. These findings pave the way for harnessing the power of multimodal learning to overcome challenges in wider quantum system learning tasks.
</details>
<details>
<summary>摘要</summary>
cross-platform验证，在初级量子计算领域中的一项重要任务，旨在Characterize two imperfect quantum devices executing identical algorithms using minimal measurements. Although the random measurement approach has been instrumental in this context, the quasi-exponential computational demand with increasing qubit count hinders its feasibility in large-qubit scenarios. To bridge this knowledge gap, we introduce an innovative multimodal learning approach, recognizing that the formalism of data in this task embodies two distinct modalities: measurement outcomes and classical description of compiled circuits on explored quantum devices, both enriched with unique information. Building upon this insight, we devise a multimodal neural network to independently extract knowledge from these modalities, followed by a fusion operation to create a comprehensive data representation. The learned representation can effectively characterize the similarity between the explored quantum devices when executing new quantum algorithms not present in the training data. We evaluate our proposal on platforms featuring diverse noise models, encompassing system sizes up to 50 qubits. The achieved results demonstrate a three-orders-of-magnitude improvement in prediction accuracy compared to the random measurements and offer compelling evidence of the complementary roles played by each modality in cross-platform verification. These findings pave the way for harnessing the power of multimodal learning to overcome challenges in wider quantum system learning tasks.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-convolutional-neural-network-fusion-approach-for-change-detection-in-remote-sensing-images"><a href="#Unsupervised-convolutional-neural-network-fusion-approach-for-change-detection-in-remote-sensing-images" class="headerlink" title="Unsupervised convolutional neural network fusion approach for change detection in remote sensing images"></a>Unsupervised convolutional neural network fusion approach for change detection in remote sensing images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03679">http://arxiv.org/abs/2311.03679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weidong Yan, Pei Yan, Li Cao</li>
<li>for: 本研究旨在提出一种基于深度学习的无监督 shallow convolutional neural network（USCNN）Change Detection方法，以替代传统的深度学习方法，提高Change Detection的效率和可扩展性。</li>
<li>methods: 本方法首先将bi-temporal图像转化为不同的特征空间，使用不同大小的卷积核来提取图像的多尺度信息。然后，对bi-temporal图像的输出特征图像进行相同的卷积核进行减法操作，并将得到的差分特征图像 concatenate 为一个特征图像。最后，使用1*1卷积层对多尺度信息进行汇聚。</li>
<li>results: 实验结果表明，提出的方法可以有效地检测 Change，并且比传统的深度学习方法更加高效和可扩展。<details>
<summary>Abstract</summary>
With the rapid development of deep learning, a variety of change detection methods based on deep learning have emerged in recent years. However, these methods usually require a large number of training samples to train the network model, so it is very expensive. In this paper, we introduce a completely unsupervised shallow convolutional neural network (USCNN) fusion approach for change detection. Firstly, the bi-temporal images are transformed into different feature spaces by using convolution kernels of different sizes to extract multi-scale information of the images. Secondly, the output features of bi-temporal images at the same convolution kernels are subtracted to obtain the corresponding difference images, and the difference feature images at the same scale are fused into one feature image by using 1 * 1 convolution layer. Finally, the output features of different scales are concatenated and a 1 * 1 convolution layer is used to fuse the multi-scale information of the image. The model parameters are obtained by a redesigned sparse function. Our model has three features: the entire training process is conducted in an unsupervised manner, the network architecture is shallow, and the objective function is sparse. Thus, it can be seen as a kind of lightweight network model. Experimental results on four real remote sensing datasets indicate the feasibility and effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
随着深度学习的快速发展，深度学习基于的变化检测方法在最近几年出现了多种。然而，这些方法通常需要训练样本的大量，因此非常昂贵。在这篇论文中，我们介绍了一种完全无监督的浅层卷积神经网络（USCNN）融合方法 для变化检测。首先，bi-temporal图像被转换成不同的特征空间，使用不同的卷积核来提取图像的多尺度信息。然后，bi-temporal图像的输出特征在同一个卷积核上被减去，得到了对应的差图像，并将差图像的特征特征在同一个缩放因子上进行融合，使用1*1卷积层。最后，不同缩放因子的输出特征被拼接起来，并使用1*1卷积层来融合图像的多尺度信息。模型参数是通过重新设计的稀疏函数来获得。我们的模型有三个特点：整个训练过程是无监督的，网络架构是浅层的，目标函数是稀疏的。因此，它可以看作是一种轻量级的网络模型。实验结果表明，提案的方法在四个真实遥感数据集上是可行和有效的。
</details></li>
</ul>
<hr>
<h2 id="Image-Generation-and-Learning-Strategy-for-Deep-Document-Forgery-Detection"><a href="#Image-Generation-and-Learning-Strategy-for-Deep-Document-Forgery-Detection" class="headerlink" title="Image Generation and Learning Strategy for Deep Document Forgery Detection"></a>Image Generation and Learning Strategy for Deep Document Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03650">http://arxiv.org/abs/2311.03650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yamato Okamoto, Osada Genki, Iu Yahiro, Rintaro Hasegawa, Peifei Zhu, Hirokatsu Kataoka</li>
<li>for: 增强文档伪造风险的识别和防范</li>
<li>methods: 使用自然图像和文档图像自然学习，以及FD-VIED数据集来驱动模型进行训练</li>
<li>results: 实验结果表明，我们的方法可以提高识别性能<details>
<summary>Abstract</summary>
In recent years, document processing has flourished and brought numerous benefits. However, there has been a significant rise in reported cases of forged document images. Specifically, recent advancements in deep neural network (DNN) methods for generative tasks may amplify the threat of document forgery. Traditional approaches for forged document images created by prevalent copy-move methods are unsuitable against those created by DNN-based methods, as we have verified. To address this issue, we construct a training dataset of document forgery images, named FD-VIED, by emulating possible attacks, such as text addition, removal, and replacement with recent DNN-methods. Additionally, we introduce an effective pre-training approach through self-supervised learning with both natural images and document images. In our experiments, we demonstrate that our approach enhances detection performance.
</details>
<details>
<summary>摘要</summary>
Recently, document processing has made significant progress and brought many benefits. However, there has been a notable increase in reported cases of forged document images. Specifically, the recent advancements in deep neural network (DNN) methods for generative tasks have amplified the threat of document forgery. Traditional approaches for forged document images created by prevalent copy-move methods are not effective against those created by DNN-based methods, as we have verified. To address this issue, we construct a training dataset of document forgery images, named FD-VIED, by emulating possible attacks, such as text addition, removal, and replacement using recent DNN-methods. Additionally, we introduce an effective pre-training approach through self-supervised learning with both natural images and document images. In our experiments, we demonstrate that our approach enhances detection performance.
</details></li>
</ul>
<hr>
<h2 id="Instruct-Me-More-Random-Prompting-for-Visual-In-Context-Learning"><a href="#Instruct-Me-More-Random-Prompting-for-Visual-In-Context-Learning" class="headerlink" title="Instruct Me More! Random Prompting for Visual In-Context Learning"></a>Instruct Me More! Random Prompting for Visual In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03648">http://arxiv.org/abs/2311.03648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Zhang, Bowen Wang, Liangzhi Li, Yuta Nakashima, Hajime Nagahara</li>
<li>for: 这篇论文主要用于探讨如何使用启发式学习（In-Context Learning，ICL）来提高计算机视觉任务的性能。</li>
<li>methods: 该论文提出了一种名为“Instruct Me More”（InMeMo）的方法，该方法通过在输入图像对的基础上添加可学习的干扰（prompt），以便更好地地 explore 启发式学习的潜在。</li>
<li>results: 实验表明，Compared to 基eline无learnable prompt，InMeMo可以提高 foreground segmentation 和单个物体检测任务的 mIoU 分数 by 7.35 和 15.13，分别。这些结果表明，InMeMo 可以提供一种灵活和高效的方法来提高计算机视觉任务的性能。<details>
<summary>Abstract</summary>
Large-scale models trained on extensive datasets, have emerged as the preferred approach due to their high generalizability across various tasks. In-context learning (ICL), a popular strategy in natural language processing, uses such models for different tasks by providing instructive prompts but without updating model parameters. This idea is now being explored in computer vision, where an input-output image pair (called an in-context pair) is supplied to the model with a query image as a prompt to exemplify the desired output. The efficacy of visual ICL often depends on the quality of the prompts. We thus introduce a method coined Instruct Me More (InMeMo), which augments in-context pairs with a learnable perturbation (prompt), to explore its potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the current state-of-the-art performance. Specifically, compared to the baseline without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for foreground segmentation and single object detection tasks, respectively. Our findings suggest that InMeMo offers a versatile and efficient way to enhance the performance of visual ICL with lightweight training. Code is available at https://github.com/Jackieam/InMeMo.
</details>
<details>
<summary>摘要</summary>
大规模模型，训练在庞大数据集上，已成为首选方法，因其在不同任务上的高通用性。在自然语言处理中，受托学习（ICL）是一种受欢迎的策略，使用这些模型来执行不同任务，而不需要更新模型参数。在计算机视觉中，我们使用输入-输出图像对（称为内容对），将模型作为推荐图像的Prompt，以便塑造出所需的输出。图像ICL的效果通常取决于提示的质量。因此，我们引入了一种名为“ instru me more”（InMeMo）的方法，该方法将含有学习的扰动（提示），以探索其可能性。我们对主流任务进行了实验，发现InMeMo在比基eline无学习提示时的性能提高7.35和15.13个mIoU分数。我们的发现表明，InMeMo可以提供轻量级训练的高效和多样化方法，以提高图像ICL的性能。代码可以在https://github.com/Jackieam/InMeMo中找到。
</details></li>
</ul>
<hr>
<h2 id="Random-Field-Augmentations-for-Self-Supervised-Representation-Learning"><a href="#Random-Field-Augmentations-for-Self-Supervised-Representation-Learning" class="headerlink" title="Random Field Augmentations for Self-Supervised Representation Learning"></a>Random Field Augmentations for Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03629">http://arxiv.org/abs/2311.03629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Andrew Mansfield, Arash Afkanpour, Warren Richard Morningstar, Karan Singhal</li>
<li>for: 本文旨在提出一种新的本地变换方法来提高自我超vised representation learning的数据增强。</li>
<li>methods: 本文使用 Gaussian random fields 来生成图像增强，并将变换参数视为独立的 Gaussian 随机场。</li>
<li>results: 实验结果表明，新的变换方法可以提高 ImageNet 和 iNaturalist 下的自我超vised representation learning表现，但是需要权衡增强的多样性和强度。<details>
<summary>Abstract</summary>
Self-supervised representation learning is heavily dependent on data augmentations to specify the invariances encoded in representations. Previous work has shown that applying diverse data augmentations is crucial to downstream performance, but augmentation techniques remain under-explored. In this work, we propose a new family of local transformations based on Gaussian random fields to generate image augmentations for self-supervised representation learning. These transformations generalize the well-established affine and color transformations (translation, rotation, color jitter, etc.) and greatly increase the space of augmentations by allowing transformation parameter values to vary from pixel to pixel. The parameters are treated as continuous functions of spatial coordinates, and modeled as independent Gaussian random fields. Empirical results show the effectiveness of the new transformations for self-supervised representation learning. Specifically, we achieve a 1.7% top-1 accuracy improvement over baseline on ImageNet downstream classification, and a 3.6% improvement on out-of-distribution iNaturalist downstream classification. However, due to the flexibility of the new transformations, learned representations are sensitive to hyperparameters. While mild transformations improve representations, we observe that strong transformations can degrade the structure of an image, indicating that balancing the diversity and strength of augmentations is important for improving generalization of learned representations.
</details>
<details>
<summary>摘要</summary>
自适应学习表示力学 heavily dependent于数据扩充来确定表示中的不变性。 previous work 表明，对于下游性能的提高，采用多样化的数据扩充是关键，但是扩充技术还未得到充分探索。在这种工作中，我们提出了一个新的本地变换家族，基于 Gaussian random fields 来生成图像扩充。这些变换扩展了Well-established affine和色彩变换（平移、旋转、色彩牛牛、等），并大大增加了扩充的空间。变换参数 treated as continuous functions of spatial coordinates，并 modeled as independent Gaussian random fields。 empirical results show the effectiveness of the new transformations for self-supervised representation learning. Specifically, we achieve a 1.7% top-1 accuracy improvement over baseline on ImageNet downstream classification, and a 3.6% improvement on out-of-distribution iNaturalist downstream classification. However, due to the flexibility of the new transformations, learned representations are sensitive to hyperparameters. While mild transformations improve representations, we observe that strong transformations can degrade the structure of an image, indicating that balancing the diversity and strength of augmentations is important for improving generalization of learned representations.
</details></li>
</ul>
<hr>
<h2 id="FusionViT-Hierarchical-3D-Object-Detection-via-LiDAR-Camera-Vision-Transformer-Fusion"><a href="#FusionViT-Hierarchical-3D-Object-Detection-via-LiDAR-Camera-Vision-Transformer-Fusion" class="headerlink" title="FusionViT: Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion"></a>FusionViT: Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03620">http://arxiv.org/abs/2311.03620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhao Xiang, Jiawei Zhang</li>
<li>for: 提高3D物体检测性能</li>
<li>methods: 使用视Transformer模型进行多modal数据嵌入学习和数据融合</li>
<li>results: 在实际交通Scene中实现了状态的检测性能，并且超越了仅使用相机图像或激光点云的基eline方法以及最新的多Modal图像-点云深度融合方法。<details>
<summary>Abstract</summary>
For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches.
</details>
<details>
<summary>摘要</summary>
For 3D object detection, both camera и lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.CV_2023_11_07/" data-id="closbropu00lg0g88e90vgt9o" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/07/eess.AS_2023_11_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-11-07
        
      </div>
    </a>
  
  
    <a href="/2023/11/07/cs.AI_2023_11_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-07</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
