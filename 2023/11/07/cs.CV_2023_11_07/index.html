
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-07 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Video Instance Matting paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04212 repo_url: https:&#x2F;&#x2F;github.com&#x2F;shi-labs&#x2F;vim paper_authors: Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-07">
<meta property="og:url" content="https://nullscc.github.io/2023/11/07/cs.CV_2023_11_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Video Instance Matting paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04212 repo_url: https:&#x2F;&#x2F;github.com&#x2F;shi-labs&#x2F;vim paper_authors: Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-07T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-08T06:10:26.263Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.CV_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T13:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-07
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Video-Instance-Matting"><a href="#Video-Instance-Matting" class="headerlink" title="Video Instance Matting"></a>Video Instance Matting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04212">http://arxiv.org/abs/2311.04212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shi-labs/vim">https://github.com/shi-labs/vim</a></li>
<li>paper_authors: Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Humphrey Shi</li>
<li>for: 这篇论文是为了解决视频实例分割后的 alpha 环境映射问题而写的。</li>
<li>methods: 该论文提出了一种新的 Video Instance Matting<del>(VIM) 方法，即在每帧视频帧中对每个实例预测 alpha 环境映射。此外，该论文还提出了一种基于Mask Sequence Guided Video Instance Matting</del>(MSG-VIM)的基线模型，该模型利用了混合的 mask 增强来提高预测结果的Robustness。</li>
<li>results: 该论文的提出的 MSG-VIM 模型在新建的 VIM50 测试集上达到了比较高的性能，与现有方法相比，它的性能差异较大。此外，该论文还引入了一种适用于 VIM 任务的评价指标，即 Video Instance-aware Matting Quality~(VIMQ)。<details>
<summary>Abstract</summary>
Conventional video matting outputs one alpha matte for all instances appearing in a video frame so that individual instances are not distinguished. While video instance segmentation provides time-consistent instance masks, results are unsatisfactory for matting applications, especially due to applied binarization. To remedy this deficiency, we propose Video Instance Matting~(VIM), that is, estimating alpha mattes of each instance at each frame of a video sequence. To tackle this challenging problem, we present MSG-VIM, a Mask Sequence Guided Video Instance Matting neural network, as a novel baseline model for VIM. MSG-VIM leverages a mixture of mask augmentations to make predictions robust to inaccurate and inconsistent mask guidance. It incorporates temporal mask and temporal feature guidance to improve the temporal consistency of alpha matte predictions. Furthermore, we build a new benchmark for VIM, called VIM50, which comprises 50 video clips with multiple human instances as foreground objects. To evaluate performances on the VIM task, we introduce a suitable metric called Video Instance-aware Matting Quality~(VIMQ). Our proposed model MSG-VIM sets a strong baseline on the VIM50 benchmark and outperforms existing methods by a large margin. The project is open-sourced at https://github.com/SHI-Labs/VIM.
</details>
<details>
<summary>摘要</summary>
传统视频腾讯输出一个 alpha 毯幕，用于所有在视频帧中出现的实例，从而无法分辨各个实例。而视频实例分割提供了时间相关的实例掩蔽，但是由于应用了二进制化，导致结果不满足。为了解决这种不足，我们提议视频实例腾讯~(VIM)，即在每帧视频序列中为每个实例 estimating alpha 毯幕。为了解决这个挑战问题，我们提出了一种基于Mask Sequence Guided~(MSG)的新基线模型，称为MSG-VIM。MSG-VIM 利用了混合的掩蔽修饰，以使其预测更加Robust to inaccurate和不一致的掩蔽指导。它还 incorporates temporal mask和temporal feature导航，以改进 alpha 毯幕预测的时间一致性。此外，我们建立了一个新的 VIM 测试集，称为VIM50，它包含 50 个视频剪辑，每个剪辑有多个人类实例作为背景物体。为了评估 VIM 任务的性能，我们引入了一个适合的 metric，称为 Video Instance-aware Matting Quality~(VIMQ)。我们的提议模型 MSG-VIM 在 VIM50 测试集上设置了一个强大的基线，并超过现有方法的表现。项目已经开源在 GitHub 上，请参考 https://github.com/SHI-Labs/VIM。
</details></li>
</ul>
<hr>
<h2 id="Deep-Hashing-via-Householder-Quantization"><a href="#Deep-Hashing-via-Householder-Quantization" class="headerlink" title="Deep Hashing via Householder Quantization"></a>Deep Hashing via Householder Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04207">http://arxiv.org/abs/2311.04207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas R. Schwengber, Lucas Resende, Paulo Orenstein, Roberto I. Oliveira</li>
<li>For: 该文章目的是提出一种新的快速、无监督、具有优化性的图像相似性搜索算法，可以在现有的深度哈希或度量学习算法之上进行。* Methods: 该算法使用了分解学习问题的方法，首先在嵌入空间进行相似性学习，然后使用投影变换将嵌入转换为符号函数中的整数值。* Results: 对于多个图像数据集，该算法可以达到状态艺术性的表现，并且与其他归一化策略不同的是，可以在现有的深度哈希或度量学习算法上进行稳定的改进。<details>
<summary>Abstract</summary>
Hashing is at the heart of large-scale image similarity search, and recent methods have been substantially improved through deep learning techniques. Such algorithms typically learn continuous embeddings of the data. To avoid a subsequent costly binarization step, a common solution is to employ loss functions that combine a similarity learning term (to ensure similar images are grouped to nearby embeddings) and a quantization penalty term (to ensure that the embedding entries are close to binarized entries, e.g., -1 or 1). Still, the interaction between these two terms can make learning harder and the embeddings worse. We propose an alternative quantization strategy that decomposes the learning problem in two stages: first, perform similarity learning over the embedding space with no quantization; second, find an optimal orthogonal transformation of the embeddings so each coordinate of the embedding is close to its sign, and then quantize the transformed embedding through the sign function. In the second step, we parametrize orthogonal transformations using Householder matrices to efficiently leverage stochastic gradient descent. Since similarity measures are usually invariant under orthogonal transformations, this quantization strategy comes at no cost in terms of performance. The resulting algorithm is unsupervised, fast, hyperparameter-free and can be run on top of any existing deep hashing or metric learning algorithm. We provide extensive experimental results showing that this approach leads to state-of-the-art performance on widely used image datasets, and, unlike other quantization strategies, brings consistent improvements in performance to existing deep hashing algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="High-fidelity-3D-Reconstruction-of-Plants-using-Neural-Radiance-Field"><a href="#High-fidelity-3D-Reconstruction-of-Plants-using-Neural-Radiance-Field" class="headerlink" title="High-fidelity 3D Reconstruction of Plants using Neural Radiance Field"></a>High-fidelity 3D Reconstruction of Plants using Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04154">http://arxiv.org/abs/2311.04154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kewei Hu, Ying Wei, Yaoqiang Pan, Hanwen Kang, Chao Chen</li>
<li>for: 本研究旨在探讨Neural Radiance Field（NeRF）在减少农业垦戈中的应用，以实现高精度的植物形态重建。</li>
<li>methods: 本研究使用了NeRF技术，特别是Instant-NGP和Instant-NSR两种State-of-the-Art方法，以生成高质量的图像和精度地重建植物模型。</li>
<li>results: 本研究结果表明，NeRF在生成新视图图像和三维重建植物模型方面具有优秀表现，并能够与商业软件Reality Capture相当。然而，研究还发现NeRF在某些情况下存在较慢的训练速度、不足的采样导致的性能限制以及复杂设置中的几何质量困难。<details>
<summary>Abstract</summary>
Accurate reconstruction of plant phenotypes plays a key role in optimising sustainable farming practices in the field of Precision Agriculture (PA). Currently, optical sensor-based approaches dominate the field, but the need for high-fidelity 3D reconstruction of crops and plants in unstructured agricultural environments remains challenging. Recently, a promising development has emerged in the form of Neural Radiance Field (NeRF), a novel method that utilises neural density fields. This technique has shown impressive performance in various novel vision synthesis tasks, but has remained relatively unexplored in the agricultural context. In our study, we focus on two fundamental tasks within plant phenotyping: (1) the synthesis of 2D novel-view images and (2) the 3D reconstruction of crop and plant models. We explore the world of neural radiance fields, in particular two SOTA methods: Instant-NGP, which excels in generating high-quality images with impressive training and inference speed, and Instant-NSR, which improves the reconstructed geometry by incorporating the Signed Distance Function (SDF) during training. In particular, we present a novel plant phenotype dataset comprising real plant images from production environments. This dataset is a first-of-its-kind initiative aimed at comprehensively exploring the advantages and limitations of NeRF in agricultural contexts. Our experimental results show that NeRF demonstrates commendable performance in the synthesis of novel-view images and is able to achieve reconstruction results that are competitive with Reality Capture, a leading commercial software for 3D Multi-View Stereo (MVS)-based reconstruction. However, our study also highlights certain drawbacks of NeRF, including relatively slow training speeds, performance limitations in cases of insufficient sampling, and challenges in obtaining geometry quality in complex setups.
</details>
<details>
<summary>摘要</summary>
准确重建植物fenotype在精准农业（PA）中扮演了关键的角色。目前，光学感知器技术主导了该领域，但是需要高精度的3D重建植物和作物在无结构农业环境中仍然是挑战。最近，一种有前途的发展是Neural Radiance Field（NeRF）技术，它利用神经density场来实现。这种技术在视觉合成任务中表现出色，但在农业上还很少研究。在我们的研究中，我们关注了两个基本的植物fenotype任务：（1）生成2D新视图图像，和（2）3D重建作物和植物模型。我们深入探索NeRF技术，特别是两种SOTA方法：Instant-NGP和Instant-NSR。Instant-NGP可以在快速训练和推理中生成高质量图像，而Instant-NSR通过在训练中 incorporating Signed Distance Function（SDF）来提高重建的准确性。特别是，我们提供了一个全新的植物fenotype数据集，包含来自生产环境的真实植物图像。这个数据集是一项首先的尝试，旨在全面探索NeRF在农业上的优势和局限性。我们的实验结果表明，NeRF在生成新视图图像方面表现出色，并且能够与Market leader的3D Multi-View Stereo（MVS）重建软件Reality Capture竞争。然而，我们的研究也揭示了NeRF的一些缺点，包括较慢的训练速度，在不充分采样的情况下的性能局限性，以及在复杂设置下获得准确的几何困难。
</details></li>
</ul>
<hr>
<h2 id="I2VGen-XL-High-Quality-Image-to-Video-Synthesis-via-Cascaded-Diffusion-Models"><a href="#I2VGen-XL-High-Quality-Image-to-Video-Synthesis-via-Cascaded-Diffusion-Models" class="headerlink" title="I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models"></a>I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04145">http://arxiv.org/abs/2311.04145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-vilab/i2vgen-xl">https://github.com/damo-vilab/i2vgen-xl</a></li>
<li>paper_authors: Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, Jingren Zhou</li>
<li>for: 提高视频生成的semantic精度、质量和空间时间连续性</li>
<li>methods: 提出了一种叫做I2VGen-XL的方法，通过分解这两个因素来提高模型性能，并通过使用静止图像作为关键引导来保证输入数据的对齐</li>
<li>results: 通过对广泛的数据集进行优化和比较研究，显示I2VGen-XL可以同时提高视频的semantic精度、质量和空间时间连续性<details>
<summary>Abstract</summary>
Video synthesis has recently made remarkable strides benefiting from the rapid development of diffusion models. However, it still encounters challenges in terms of semantic accuracy, clarity and spatio-temporal continuity. They primarily arise from the scarcity of well-aligned text-video data and the complex inherent structure of videos, making it difficult for the model to simultaneously ensure semantic and qualitative excellence. In this report, we propose a cascaded I2VGen-XL approach that enhances model performance by decoupling these two factors and ensures the alignment of the input data by utilizing static images as a form of crucial guidance. I2VGen-XL consists of two stages: i) the base stage guarantees coherent semantics and preserves content from input images by using two hierarchical encoders, and ii) the refinement stage enhances the video's details by incorporating an additional brief text and improves the resolution to 1280$\times$720. To improve the diversity, we collect around 35 million single-shot text-video pairs and 6 billion text-image pairs to optimize the model. By this means, I2VGen-XL can simultaneously enhance the semantic accuracy, continuity of details and clarity of generated videos. Through extensive experiments, we have investigated the underlying principles of I2VGen-XL and compared it with current top methods, which can demonstrate its effectiveness on diverse data. The source code and models will be publicly available at \url{https://i2vgen-xl.github.io}.
</details>
<details>
<summary>摘要</summary>
Video合成最近几年已经取得了很大的进步，得益于扩散模型的快速发展。然而，它仍然面临 semantic accuracy、清晰度和时空连续性等问题。这些问题主要归结于缺乏准确的文本视频数据和视频的复杂内在结构，使模型具备同时保证 semantic 和质量的能力很困难。在这份报告中，我们提议一种名为 I2VGen-XL 的方法，该方法可以提高模型性能，并同时保证输入数据的对齐。I2VGen-XL 由两个阶段组成：一个基础阶段，使用两个层次编码器保证 coherent semantics 和保留输入图像中的内容，以及一个改进阶段，通过添加一个简短的文本和提高分辨率到 1280 $\times$ 720 来提高视频的细节。为了提高多样性，我们收集了约 35 万个单shot text-video 对和 6 亿个 text-image 对，用于优化模型。通过这些方法，I2VGen-XL 可以同时提高 semantic accuracy、时空连续性和视频的清晰度。经过了广泛的实验，我们发现 I2VGen-XL 的下列原则和现有的顶尖方法进行比较，可以证明其效果适用于多样的数据。模型和代码将在 \url{https://i2vgen-xl.github.io} 上公开。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Semantic-Map-Representation-for-Skill-based-Visual-Object-Navigation"><a href="#Interactive-Semantic-Map-Representation-for-Skill-based-Visual-Object-Navigation" class="headerlink" title="Interactive Semantic Map Representation for Skill-based Visual Object Navigation"></a>Interactive Semantic Map Representation for Skill-based Visual Object Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04107">http://arxiv.org/abs/2311.04107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatiana Zemskova, Aleksei Staroverov, Kirill Muravyev, Dmitry Yudin, Aleksandr Panov</li>
<li>for: 该论文旨在提出一种新的场景semantic地图表示方法，用于移动 робоットnavigation。</li>
<li>methods: 该方法基于神经网络方法，通过backpropagation算法在推理过程中调整segmentation模型的权重，以实现在各种图像序列中进行推理。</li>
<li>results: 该方法在Habitat环境中进行了广泛的实验，并与当前最佳方法进行比较，结果显示了 significiant superiority in navigation quality metrics。<details>
<summary>Abstract</summary>
Visual object navigation using learning methods is one of the key tasks in mobile robotics. This paper introduces a new representation of a scene semantic map formed during the embodied agent interaction with the indoor environment. It is based on a neural network method that adjusts the weights of the segmentation model with backpropagation of the predicted fusion loss values during inference on a regular (backward) or delayed (forward) image sequence. We have implemented this representation into a full-fledged navigation approach called SkillTron, which can select robot skills from end-to-end policies based on reinforcement learning and classic map-based planning methods. The proposed approach makes it possible to form both intermediate goals for robot exploration and the final goal for object navigation. We conducted intensive experiments with the proposed approach in the Habitat environment, which showed a significant superiority in navigation quality metrics compared to state-of-the-art approaches. The developed code and used custom datasets are publicly available at github.com/AIRI-Institute/skill-fusion.
</details>
<details>
<summary>摘要</summary>
visual 对象导航使用学习方法是移动机器人控制中的关键任务之一。这篇论文介绍了一种新的场景semantic map表示方法，基于神经网络方法在感知过程中调整分割模型的权重，并通过反propagation或延迟图像序列进行INFERENCE。我们在SkillTron navigation方法中实现了这种表示方法，可以根据权重学习和经典地图基本 планирова方法选择机器人技能。这种方法可以形成机器人探索的中间目标以及对象导航的最终目标。我们在Habitat环境中进行了广泛的实验，并表明该方法在导航质量指标上显著超过了现有方法。开发的代码和自定义数据集在github.com/AIRI-Institute/skill-fusion上公开可用。
</details></li>
</ul>
<hr>
<h2 id="DeepPatent2-A-Large-Scale-Benchmarking-Corpus-for-Technical-Drawing-Understanding"><a href="#DeepPatent2-A-Large-Scale-Benchmarking-Corpus-for-Technical-Drawing-Understanding" class="headerlink" title="DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding"></a>DeepPatent2: A Large-Scale Benchmarking Corpus for Technical Drawing Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04098">http://arxiv.org/abs/2311.04098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kehinde Ajayi, Xin Wei, Martin Gryder, Winston Shields, Jian Wu, Shawn M. Jones, Michal Kucer, Diane Oyen</li>
<li>For: The paper is written for researchers and developers in the field of computer vision and natural language processing, with a focus on improving the accuracy and versatility of image captioning and 3D reconstruction tasks.* Methods: The paper introduces a large-scale dataset called DeepPatent2, which consists of over 2.7 million technical drawings with 132,890 object names and 22,394 viewpoints extracted from 14 years of US design patent documents. The authors use this dataset to demonstrate the usefulness of conceptual captioning and highlight the potential usefulness of the dataset for other research areas such as 3D image reconstruction and image retrieval.* Results: The authors achieve state-of-the-art performance on conceptual captioning tasks using DeepPatent2, and demonstrate the potential of the dataset for other tasks such as 3D image reconstruction and image retrieval.Here’s the same information in Simplified Chinese text:* For: 这篇论文是为计算机视觉和自然语言处理领域的研究人员和开发者写的，旨在提高图像captioning和3D重建等任务的准确性和多样性。* Methods: 论文引入了一个大规模的数据集——DeepPatent2，包含14年美国设计专利文书中的270多万个技术图像，其中包含132,890个物品名称和22,394个视点。作者使用这个数据集来证明概念captioning的有用性，并 highlight了数据集的潜在用途，例如3D图像重建和图像搜索。* Results: 作者通过使用DeepPatent2数据集，实现了图像captioning任务的state-of-the-art性能，并证明了数据集的潜在用途，例如3D图像重建和图像搜索。<details>
<summary>Abstract</summary>
Recent advances in computer vision (CV) and natural language processing have been driven by exploiting big data on practical applications. However, these research fields are still limited by the sheer volume, versatility, and diversity of the available datasets. CV tasks, such as image captioning, which has primarily been carried out on natural images, still struggle to produce accurate and meaningful captions on sketched images often included in scientific and technical documents. The advancement of other tasks such as 3D reconstruction from 2D images requires larger datasets with multiple viewpoints. We introduce DeepPatent2, a large-scale dataset, providing more than 2.7 million technical drawings with 132,890 object names and 22,394 viewpoints extracted from 14 years of US design patent documents. We demonstrate the usefulness of DeepPatent2 with conceptual captioning. We further provide the potential usefulness of our dataset to facilitate other research areas such as 3D image reconstruction and image retrieval.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)latest advances in computer vision (CV) and natural language processing have been driven by exploiting big data on practical applications. However, these research fields are still limited by the sheer volume, versatility, and diversity of the available datasets. CV tasks, such as image captioning, which has primarily been carried out on natural images, still struggle to produce accurate and meaningful captions on sketched images often included in scientific and technical documents. The advancement of other tasks such as 3D reconstruction from 2D images requires larger datasets with multiple viewpoints. We introduce DeepPatent2, a large-scale dataset, providing more than 2.7 million technical drawings with 132,890 object names and 22,394 viewpoints extracted from 14 years of US design patent documents. We demonstrate the usefulness of DeepPatent2 with conceptual captioning. We further provide the potential usefulness of our dataset to facilitate other research areas such as 3D image reconstruction and image retrieval.
</details></li>
</ul>
<hr>
<h2 id="Image-Pointcloud-Fusion-based-Anomaly-Detection-using-PD-REAL-Dataset"><a href="#Image-Pointcloud-Fusion-based-Anomaly-Detection-using-PD-REAL-Dataset" class="headerlink" title="Image-Pointcloud Fusion based Anomaly Detection using PD-REAL Dataset"></a>Image-Pointcloud Fusion based Anomaly Detection using PD-REAL Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04095">http://arxiv.org/abs/2311.04095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjian Qin, Chunzhi Gu, Jun Yu, Chao Zhang</li>
<li>for: 这个论文是为了提供一个大规模的无监督异常检测（AD）数据集，用于在3D领域中进行无监督异常检测。</li>
<li>methods: 这个数据集使用了Play-Doh模型，包含15种物品类别，并在控制的环境中进行了 anomaly 的分析。Specifically, objects are first created with six types of anomalies, such as dent, crack, or perforation, and then photographed under different lighting conditions to mimic real-world inspection scenarios.</li>
<li>results:  comparing with existing 3D AD dataset, the data acquisition of PD-REAL is significantly cheaper, easily scalable and easier to control variables. Extensive evaluations with state-of-the-art AD algorithms on our dataset demonstrate the benefits as well as challenges of using 3D information.<details>
<summary>Abstract</summary>
We present PD-REAL, a novel large-scale dataset for unsupervised anomaly detection (AD) in the 3D domain. It is motivated by the fact that 2D-only representations in the AD task may fail to capture the geometric structures of anomalies due to uncertainty in lighting conditions or shooting angles. PD-REAL consists entirely of Play-Doh models for 15 object categories and focuses on the analysis of potential benefits from 3D information in a controlled environment. Specifically, objects are first created with six types of anomalies, such as dent, crack, or perforation, and then photographed under different lighting conditions to mimic real-world inspection scenarios. To demonstrate the usefulness of 3D information, we use a commercially available RealSense camera to capture RGB and depth images. Compared to the existing 3D dataset for AD tasks, the data acquisition of PD-REAL is significantly cheaper, easily scalable and easier to control variables. Extensive evaluations with state-of-the-art AD algorithms on our dataset demonstrate the benefits as well as challenges of using 3D information. Our dataset can be downloaded from https://github.com/Andy-cs008/PD-REAL
</details>
<details>
<summary>摘要</summary>
我团队现在发布了PD-REAL，一个大规模的无监督异常检测（AD）数据集在3D领域。这是因为2D只的表示在AD任务中可能无法捕捉异常的几何结构，因为光照条件或拍摄角度的不确定性。PD-REAL由Play-Doh模型组成，涵盖15种物品类别，并专注于控制环境中3D信息的分析。具体来说，物品首先被设置了6种异常，如损凹、裂缝或穿孔，然后在不同的照明条件下拍摄，以模拟实际检测场景。为了证明3D信息的有用性，我们使用了一款商业化的RealSense摄像头捕摄RGB和深度图像。与现有的3D数据集 для AD任务相比，PD-REAL的数据采集更加便宜、可扩展和更容易控制变量。我们对state-of-the-art AD算法进行了广泛的评估，并证明了使用3D信息的好处以及挑战。您可以从https://github.com/Andy-cs008/PD-REAL下载我们的数据集。
</details></li>
</ul>
<hr>
<h2 id="Proceedings-of-the-5th-International-Workshop-on-Reading-Music-Systems"><a href="#Proceedings-of-the-5th-International-Workshop-on-Reading-Music-Systems" class="headerlink" title="Proceedings of the 5th International Workshop on Reading Music Systems"></a>Proceedings of the 5th International Workshop on Reading Music Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04091">http://arxiv.org/abs/2311.04091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suziai/gui-tools">https://github.com/suziai/gui-tools</a></li>
<li>paper_authors: Jorge Calvo-Zaragoza, Alexander Pacha, Elona Shatri</li>
<li>for: 这个论文是关于音乐读取系统的研讨会论文，目的是连接发展音乐读取系统的研究人员与需要这些系统的研究人员和实践者。</li>
<li>methods: 这篇论文的研究方法包括音乐读取系统、光学音乐识别、数据集和性能评估、图像处理Music scores、多模态系统、新的音乐输入方法等。</li>
<li>results: 这篇论文提出了5个国际音乐读取系统研讨会的进展，包括音乐读取系统、光学音乐识别、数据集和性能评估、图像处理Music scores、多模态系统、新的音乐输入方法等。<details>
<summary>Abstract</summary>
The International Workshop on Reading Music Systems (WoRMS) is a workshop that tries to connect researchers who develop systems for reading music, such as in the field of Optical Music Recognition, with other researchers and practitioners that could benefit from such systems, like librarians or musicologists. The relevant topics of interest for the workshop include, but are not limited to: Music reading systems; Optical music recognition; Datasets and performance evaluation; Image processing on music scores; Writer identification; Authoring, editing, storing and presentation systems for music scores; Multi-modal systems; Novel input-methods for music to produce written music; Web-based Music Information Retrieval services; Applications and projects; Use-cases related to written music.   These are the proceedings of the 5th International Workshop on Reading Music Systems, held in Milan, Italy on Nov. 4th 2023.
</details>
<details>
<summary>摘要</summary>
世界音乐读取系统国际研讨会（WoRMS）是一个研讨会，旨在连接开发音乐读取系统的研究人员（如光学音乐识别领域）与其他研究人员和实践者（如图书馆员或音乐学家），以便共同分享和交流有关音乐读取系统的最新研究成果和应用经验。研讨会的主要研究领域包括，但不限于：* 音乐读取系统* 光学音乐识别* 数据集和性能评估* 音乐手稿图像处理* 作者识别* 作品编辑、存储和展示系统* 多Modal系统* 新的音乐输入方法生成written music* Web基于音乐信息检索服务* 应用和项目* 关于written music的使用场景这是第5届世界音乐读取系统国际研讨会的论文集，于2023年11月4日在意大利米兰举行。
</details></li>
</ul>
<hr>
<h2 id="Learning-Super-Resolution-Ultrasound-Localization-Microscopy-from-Radio-Frequency-Data"><a href="#Learning-Super-Resolution-Ultrasound-Localization-Microscopy-from-Radio-Frequency-Data" class="headerlink" title="Learning Super-Resolution Ultrasound Localization Microscopy from Radio-Frequency Data"></a>Learning Super-Resolution Ultrasound Localization Microscopy from Radio-Frequency Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04081">http://arxiv.org/abs/2311.04081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Hahne, Georges Chabouh, Olivier Couture, Raphael Sznitman</li>
<li>for: 本研究旨在提高 Ultrasound Localization Microscopy (ULM) 的解像能力，通过快速和高效地目标位置的准确地Localization。</li>
<li>methods: 本研究提议使用无处理 Radio-Frequency (RF) 数据，并通过超Resolution网络来替代延迟和总和 (DAS) 干扰。为此，我们展示了标签投影和反向点变换，以将 B-mode 和 RF 坐标空间相互转换。</li>
<li>results: 我们基于公共数据集进行比较，结果表明，不使用 DAS 干扰可以提高 ULM 的解像能力。我们的 RF 训练网络表明，禁用 DAS 干扰可以优化 ULM 的解像性能。<details>
<summary>Abstract</summary>
Ultrasound Localization Microscopy (ULM) enables imaging of vascular structures in the micrometer range by accumulating contrast agent particle locations over time. Precise and efficient target localization accuracy remains an active research topic in the ULM field to further push the boundaries of this promising medical imaging technology. Existing work incorporates Delay-And-Sum (DAS) beamforming into particle localization pipelines, which ultimately determines the ULM image resolution capability. In this paper we propose to feed unprocessed Radio-Frequency (RF) data into a super-resolution network while bypassing DAS beamforming and its limitations. To facilitate this, we demonstrate label projection and inverse point transformation between B-mode and RF coordinate space as required by our approach. We assess our method against state-of-the-art techniques based on a public dataset featuring in silico and in vivo data. Results from our RF-trained network suggest that excluding DAS beamforming offers a great potential to optimize on the ULM resolution performance.
</details>
<details>
<summary>摘要</summary>
ultrasound localization microscopy (ULM) 可以在微米级别进行血管结构成像，通过时间积累反应剂粒子位置。ULM领域中精准和高效目标Localization精度仍然是活跃的研究主题，以进一步推动这种有前途的医疗成像技术的发展。现有的方法将延迟和总和（DAS）扫描成为反射波数据的处理步骤，这 ultimately determines the ULM 图像分辨率能力。在这篇论文中，我们提议将未处理的Radio-Frequency（RF）数据传递到超分辨网络中，并 circumvent DAS 扫描的限制。为此，我们实现了标签投影和反向点变换 между B-mode 和RF 坐标空间，这些步骤是我们的方法所需。我们根据公共数据集进行比较，结果表明，不包括 DAS 扫描可以优化 ULM 的分辨率性能。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Lane-Perception-and-Topology-Understanding-with-Standard-Definition-Navigation-Maps"><a href="#Augmenting-Lane-Perception-and-Topology-Understanding-with-Standard-Definition-Navigation-Maps" class="headerlink" title="Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps"></a>Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04079">http://arxiv.org/abs/2311.04079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katie Z Luo, Xinshuo Weng, Yan Wang, Shuang Wu, Jie Li, Kilian Q Weinberger, Yue Wang, Marco Pavone</li>
<li>for: 本研究旨在探讨标准定义（SD）地图在实时车道拓扑理解方面的效果。</li>
<li>methods: 我们提出了一种新的框架，将SD地图集成到在线地图预测中，并使用Transformer编码器，即SD Map Encoder Representations from transFormers，利用SD地图中的假设来提高车道拓扑预测。</li>
<li>results: 我们的方法可以在当前领先的在线地图预测方法上显著提高（最多60%）车道检测和拓扑预测，而且不需要额外的硬件或软件支持。代码可以在<a target="_blank" rel="noopener" href="https://github.com/NVlabs/SMERF%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/NVlabs/SMERF中下载。</a><details>
<summary>Abstract</summary>
Autonomous driving has traditionally relied heavily on costly and labor-intensive High Definition (HD) maps, hindering scalability. In contrast, Standard Definition (SD) maps are more affordable and have worldwide coverage, offering a scalable alternative. In this work, we systematically explore the effect of SD maps for real-time lane-topology understanding. We propose a novel framework to integrate SD maps into online map prediction and propose a Transformer-based encoder, SD Map Encoder Representations from transFormers, to leverage priors in SD maps for the lane-topology prediction task. This enhancement consistently and significantly boosts (by up to 60%) lane detection and topology prediction on current state-of-the-art online map prediction methods without bells and whistles and can be immediately incorporated into any Transformer-based lane-topology method. Code is available at https://github.com/NVlabs/SMERF.
</details>
<details>
<summary>摘要</summary>
自适应驾驶曾经受到高Definition（HD）地图的压力，导致成本和劳动力成本较高，阻碍了扩展性。相比之下，标准Definition（SD）地图更加可Affordable和全球覆盖率，提供了可扩展的代替方案。在这种工作中，我们系统性地探索了SD地图在实时车道topology理解中的效果。我们提议了一种将SD地图integrated into online map prediction的框架，并使用Transformer基于的encoder，即SD Map Encoder Representations from transFormers，以利用SD地图中的假设 для车道topology预测任务。这种改进可以Consistently和Significantly Boost（up to 60%）车道检测和topology预测Current state-of-the-art online map prediction方法不需要额外的配置和特性。代码可以在https://github.com/NVlabs/SMERF中下载。
</details></li>
</ul>
<hr>
<h2 id="Energy-based-Calibrated-VAE-with-Test-Time-Free-Lunch"><a href="#Energy-based-Calibrated-VAE-with-Test-Time-Free-Lunch" class="headerlink" title="Energy-based Calibrated VAE with Test Time Free Lunch"></a>Energy-based Calibrated VAE with Test Time Free Lunch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04071">http://arxiv.org/abs/2311.04071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Luo, Siya Qiu, Xingjian Tao, Yujun Cai, Jing Tang</li>
<li>for: 提高Variational Autoencoders（VAEs）的描述质量和效率，使其能够生成高质量的图像和进行零shot图像修复。</li>
<li>methods: 使用Conditional Energy-Based Model（EBM）来调整VAEs的生成方向，不需要测试阶段MCMC抽样。</li>
<li>results: 通过对多个应用进行广泛的实验，包括图像生成和零shot图像修复，提出的方法实现了单步非对抗性生成的 estado-of-the-art表现。<details>
<summary>Abstract</summary>
In this paper, we propose a novel Energy-Calibrated Generative Model that utilizes a Conditional EBM for enhancing Variational Autoencoders (VAEs). VAEs are sampling efficient but often suffer from blurry generation results due to the lack of training in the generative direction. On the other hand, Energy-Based Models (EBMs) can generate high-quality samples but require expensive Markov Chain Monte Carlo (MCMC) sampling. To address these issues, we introduce a Conditional EBM for calibrating the generative direction during training, without requiring it for test time sampling. Our approach enables the generative model to be trained upon data and calibrated samples with adaptive weight, thereby enhancing efficiency and effectiveness without necessitating MCMC sampling in the inference phase. We also show that the proposed approach can be extended to calibrate normalizing flows and variational posterior. Moreover, we propose to apply the proposed method to zero-shot image restoration via neural transport prior and range-null theory. We demonstrate the effectiveness of the proposed method through extensive experiments in various applications, including image generation and zero-shot image restoration. Our method shows state-of-the-art performance over single-step non-adversarial generation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的能量准确生成模型，利用条件EBM对VAEs进行增强。VAEs的采样效率高，但通常会因为生成方向的缺失而导致生成结果模糊。而EBMs可以生成高质量的样本，但需要昂贵的MCMC采样。为了解决这些问题，我们引入了条件EBM，在训练过程中准确了生成方向，不需要测试阶段的MCMC采样。我们的方法使得生成模型可以在数据和适应性采样下接受训练，从而提高效率和效果，不需要测试阶段的MCMC采样。此外，我们还证明了我们的方法可以扩展到调整正态流和量化 posterior。此外，我们还提出了应用我们的方法于零码图像恢复，通过神经运输先验和范围-null理论。我们通过了广泛的实验，包括图像生成和零码图像恢复，证明了我们的方法的有效性。我们的方法在单步非对抗生成方面表现出了状态级的表现。
</details></li>
</ul>
<hr>
<h2 id="LISBET-a-self-supervised-Transformer-model-for-the-automatic-segmentation-of-social-behavior-motifs"><a href="#LISBET-a-self-supervised-Transformer-model-for-the-automatic-segmentation-of-social-behavior-motifs" class="headerlink" title="LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs"></a>LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04069">http://arxiv.org/abs/2311.04069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Chindemi, Benoit Girard, Camilla Bellone</li>
<li>for: 本研究旨在更好地理解社交行为的核心原理，以便更好地理解社交障碍的 neural 基础。</li>
<li>methods: 本研究使用 LISBET（seLf-supervIsed Social BEhavioral Transformer）模型，通过自我超vised learning 检测和量化社交互动，不需要特征选择和广泛的人类标注。</li>
<li>results: 研究发现，使用发现驱动模式的社交互动模式识别与人类标注高度相似，并且与 dopaminergic neurons 的电physiological 活动在 Ventral Tegmental Area (VTA) 相关。<details>
<summary>Abstract</summary>
Social behavior, defined as the process by which individuals act and react in response to others, is crucial for the function of societies and holds profound implications for mental health. To fully grasp the intricacies of social behavior and identify potential therapeutic targets for addressing social deficits, it is essential to understand its core principles. Although machine learning algorithms have made it easier to study specific aspects of complex behavior, current methodologies tend to focus primarily on single-animal behavior. In this study, we introduce LISBET (seLf-supervIsed Social BEhavioral Transformer), a model designed to detect and segment social interactions. Our model eliminates the need for feature selection and extensive human annotation by using self-supervised learning to detect and quantify social behaviors from dynamic body parts tracking data. LISBET can be used in hypothesis-driven mode to automate behavior classification using supervised finetuning, and in discovery-driven mode to segment social behavior motifs using unsupervised learning. We found that motifs recognized using the discovery-driven approach not only closely match the human annotations but also correlate with the electrophysiological activity of dopaminergic neurons in the Ventral Tegmental Area (VTA). We hope LISBET will help the community improve our understanding of social behaviors and their neural underpinnings.
</details>
<details>
<summary>摘要</summary>
社会行为，定义为个体在响应他人时的行为过程，对社会的功能和心理健康具有杰yk的意义。为了全面理解社会行为的复杂性和潜在的治疗目标，需要了解其核心原则。虽然机器学习算法已使研究特定方面的复杂行为变得更加容易，但当前方法ologies往往专注于单个动物的行为。本研究提出了 LISBET（seLf-supervIsed Social BEhavioral Transformer）模型，用于探测和分类社交互动。我们的模型不需要特定的特征选择和大量的人类标注，可以通过自动学习探测和评估社交行为的动态身体部分跟踪数据。LISBET可以在假设驱动模式下用超级visedfinetuning自动分类行为，以及在发现驱动模式下使用无监督学习分类社交行为模式。我们发现使用发现驱动模式分类的模式与人类标注非常相似，并且与苯乙胺酸细胞体内的 dopamine 神经元活动在腹主梁细胞区（VTA）也存在相似性。我们希望 LISBET 能帮助社区更好地理解社交行为和其神经基础。
</details></li>
</ul>
<hr>
<h2 id="mmFUSION-Multimodal-Fusion-for-3D-Objects-Detection"><a href="#mmFUSION-Multimodal-Fusion-for-3D-Objects-Detection" class="headerlink" title="mmFUSION: Multimodal Fusion for 3D Objects Detection"></a>mmFUSION: Multimodal Fusion for 3D Objects Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04058">http://arxiv.org/abs/2311.04058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javed Ahmad, Alessio Del Bue</li>
<li>for: 提高自驾车系统中三维物体检测的准确性，使用camera和LiDAR感知器进行多感知融合。</li>
<li>methods: 提出一种新的中间水平多感知融合方法（mmFUSION），使用每个感知器分别计算特征，并通过对各感知器的特征进行交叠和多感知注意力机制进行融合。</li>
<li>results: 在KITTI和NuScenes dataset上评估 mmFUSION，与可用的早期、中间、晚期和两个阶段融合方案相比，表现更好。<details>
<summary>Abstract</summary>
Multi-sensor fusion is essential for accurate 3D object detection in self-driving systems. Camera and LiDAR are the most commonly used sensors, and usually, their fusion happens at the early or late stages of 3D detectors with the help of regions of interest (RoIs). On the other hand, fusion at the intermediate level is more adaptive because it does not need RoIs from modalities but is complex as the features of both modalities are presented from different points of view. In this paper, we propose a new intermediate-level multi-modal fusion (mmFUSION) approach to overcome these challenges. First, the mmFUSION uses separate encoders for each modality to compute features at a desired lower space volume. Second, these features are fused through cross-modality and multi-modality attention mechanisms proposed in mmFUSION. The mmFUSION framework preserves multi-modal information and learns to complement modalities' deficiencies through attention weights. The strong multi-modal features from the mmFUSION framework are fed to a simple 3D detection head for 3D predictions. We evaluate mmFUSION on the KITTI and NuScenes dataset where it performs better than available early, intermediate, late, and even two-stage based fusion schemes. The code with the mmdetection3D project plugin will be publicly available soon.
</details>
<details>
<summary>摘要</summary>
多感器融合是自动驾驶系统中精准三维对象检测的关键。 Camera 和 LiDAR 是最常用的感知器，而其融合通常发生在检测器的 early 或 late 阶段，使用了区域兴趣（RoI）的帮助。然而，中间阶段的融合更加适应，因为它不需要 RoI 从不同的模式来，但是复杂度也较高，因为两种模式的特征从不同的角度来。在这篇论文中，我们提出了一种新的中间阶段多模态融合（mmFUSION）方法，以解决这些挑战。首先，mmFUSION 使用了每种感知器的独立编码器来计算特征，以达到所需的较低的空间体积。其次，这些特征通过 cross-modality 和 multi-modality 注意机制进行融合。mmFUSION 框架保留了多模态信息，并通过注意 веса来补做每种模式的不足。强大的多模态特征从 mmFUSION 框架中得到的是 fed 到一个简单的三维检测头进行三维预测。我们在 KITTI 和 NuScenes 数据集上评估了 mmFUSION，其表现比已有的 early、intermediate、late 和 même 阶段融合方案更好。代码将在 soon 公开。
</details></li>
</ul>
<hr>
<h2 id="Generative-Structural-Design-Integrating-BIM-and-Diffusion-Model"><a href="#Generative-Structural-Design-Integrating-BIM-and-Diffusion-Model" class="headerlink" title="Generative Structural Design Integrating BIM and Diffusion Model"></a>Generative Structural Design Integrating BIM and Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04052">http://arxiv.org/abs/2311.04052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili He, Yu-Hsing Wang, Jian Zhang</li>
<li>for: 本研究旨在提出一种涵盖所有阶段的智能结构设计方法，以减少设计过程中的时间开销和提高效率。</li>
<li>methods: 本研究引入建筑信息模型（BIM），并设计了一个 integrate BIM和生成AI的结构设计管道，以增强生成结果的可见质量和细节。</li>
<li>results: 本研究在生成框架、生成AI工具和神经网络方面做出了3个贡献，包括提出了一种新的两stage生成框架，引入了扩散模型（DM）来代替广泛使用的生成对抗网络（GAN）模型，并设计了一个新的物理基于的条件扩散模型（PCDM）。<details>
<summary>Abstract</summary>
Intelligent structural design using AI can effectively reduce time overhead and increase efficiency. It has potential to become the new design paradigm in the future to assist and even replace engineers, and so it has become a research hotspot in the academic community. However, current methods have some limitations to be addressed, whether in terms of application scope, visual quality of generated results, or evaluation metrics of results. This study proposes a comprehensive solution. Firstly, we introduce building information modeling (BIM) into intelligent structural design and establishes a structural design pipeline integrating BIM and generative AI, which is a powerful supplement to the previous frameworks that only considered CAD drawings. In order to improve the perceptual quality and details of generations, this study makes 3 contributions. Firstly, in terms of generation framework, inspired by the process of human drawing, a novel 2-stage generation framework is proposed to replace the traditional end-to-end framework to reduce the generation difficulty for AI models. Secondly, in terms of generative AI tools adopted, diffusion models (DMs) are introduced to replace widely used generative adversarial network (GAN)-based models, and a novel physics-based conditional diffusion model (PCDM) is proposed to consider different design prerequisites. Thirdly, in terms of neural networks, an attention block (AB) consisting of a self-attention block (SAB) and a parallel cross-attention block (PCAB) is designed to facilitate cross-domain data fusion. The quantitative and qualitative results demonstrate the powerful generation and representation capabilities of PCDM. Necessary ablation studies are conducted to examine the validity of the methods. This study also shows that DMs have the potential to replace GANs and become the new benchmark for generative problems in civil engineering.
</details>
<details>
<summary>摘要</summary>
使用人工智能进行智能结构设计可以有效减少时间开销并提高效率。它在未来可能成为新的设计 парадигма，帮助或者取代工程师，因此在学术界已经成为研究热点。然而，当前方法存在一些需要解决的限制，包括应用范围、生成结果的视觉质量和评价指标。本研究提出了一个完整解决方案。首先，我们将建筑信息模型（BIM）引入智能结构设计，并建立了包括BIM和生成AI的结构设计管道，这是以前的框架只考虑CAD图形的增强。为了提高生成结果的视觉质量和细节，本研究做出了三个贡献。首先，在生成框架方面，我们提出了一种新的两阶段生成框架，以replace传统的终端框架，从而降低AI模型生成难度。其次，在生成AI工具方面，我们引入了扩散模型（DM），取代了广泛使用的生成对抗网络（GAN）模型，并提出了一种新的物理基于的条件扩散模型（PCDM），以考虑不同的设计前提。最后，在神经网络方面，我们设计了一个注意块（AB），包括一个自注意块（SAB）和一个平行跨域注意块（PCAB），以便跨领域数据的混合。对于PCDM的量化和质量效果，我们进行了必要的ablation研究，以证明方法的有效性。此外，我们还发现了DMs在生成问题中的潜在可能性，可能取代GANs成为新的标准。
</details></li>
</ul>
<hr>
<h2 id="3D-EAGAN-3D-edge-aware-attention-generative-adversarial-network-for-prostate-segmentation-in-transrectal-ultrasound-images"><a href="#3D-EAGAN-3D-edge-aware-attention-generative-adversarial-network-for-prostate-segmentation-in-transrectal-ultrasound-images" class="headerlink" title="3D EAGAN: 3D edge-aware attention generative adversarial network for prostate segmentation in transrectal ultrasound images"></a>3D EAGAN: 3D edge-aware attention generative adversarial network for prostate segmentation in transrectal ultrasound images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04049">http://arxiv.org/abs/2311.04049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengqing Liu, Xiao Shao, Liping Jiang, Kaizhi Wu</li>
<li>for: 本研究的目标是提出一种高效的肾静脉图像中 prostates 的分割方法，以 overcome 当前的限制和缺点，并实现高度准确的 prostates 的分割。</li>
<li>methods: 该方法包括一个 edge-aware 分割网络（EASNet），包括一个 encoder-decoder 结构的 U-Net 背景网络、一个细节补做模块、四个 3D 空间和通道注意模块、一个 Edge 增强模块、以及一个全局特征提取器。</li>
<li>results: 实验结果表明，该方法可以准确地分割 prostates 在肾静脉图像中，并且比传统方法具有更高的准确率和更好的抗衰减性。<details>
<summary>Abstract</summary>
Automatic prostate segmentation in TRUS images has always been a challenging problem, since prostates in TRUS images have ambiguous boundaries and inhomogeneous intensity distribution. Although many prostate segmentation methods have been proposed, they still need to be improved due to the lack of sensibility to edge information. Consequently, the objective of this study is to devise a highly effective prostate segmentation method that overcomes these limitations and achieves accurate segmentation of prostates in TRUS images. A 3D edge-aware attention generative adversarial network (3D EAGAN)-based prostate segmentation method is proposed in this paper, which consists of an edge-aware segmentation network (EASNet) that performs the prostate segmentation and a discriminator network that distinguishes predicted prostates from real prostates. The proposed EASNet is composed of an encoder-decoder-based U-Net backbone network, a detail compensation module, four 3D spatial and channel attention modules, an edge enhance module, and a global feature extractor. The detail compensation module is proposed to compensate for the loss of detailed information caused by the down-sampling process of the encoder. The features of the detail compensation module are selectively enhanced by the 3D spatial and channel attention module. Furthermore, an edge enhance module is proposed to guide shallow layers in the EASNet to focus on contour and edge information in prostates. Finally, features from shallow layers and hierarchical features from the decoder module are fused through the global feature extractor to predict the segmentation prostates.
</details>
<details>
<summary>摘要</summary>
自动进行肾脏部分分割在TRUS图像中一直是一个困难的问题，因为肾脏在TRUS图像中的边界不明确，具有不均匀的强度分布。虽然许多肾脏分割方法已经被提出，但它们仍需要进一步改进，因为缺乏边缘信息的敏感性。因此，本研究的目标是开发一种高效的肾脏分割方法，可以超越这些限制，并准确地分割TRUS图像中的肾脏。本文提出的3D Edge-Aware Attention Generative Adversarial Network（3D EAGAN）基于的肾脏分割方法包括一个边缘意识分 segmentation网络（EASNet），该网络执行肾脏分割，以及一个判别网络，用于判别预测的肾脏与实际的肾脏之间的差异。EASNet由一个encoder-decoder-based U-Net底层网络、一个细节补做模块、四个3D空间和通道尺度注意模块、一个边缘增强模块以及一个全局特征提取器组成。细节补做模块的目的是补做因下采样过程而丢失的细节信息。3D空间和通道尺度注意模块可以选择性地增强细节补做模块中的特征。此外，一个边缘增强模块被提出，以引导 shallow层在EASNet中注重肾脏的沿边和边缘信息。最后， shallow层和层次特征从decoder模块中获取的特征被 fusion通过全局特征提取器来预测肾脏分割。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Near-Infrared-Hyperspectral-Imaging-for-Protein-Content-Regression-and-Grain-Variety-Classification-Using-Bulk-References-and-Varying-Grain-to-Background-Ratios"><a href="#Analyzing-Near-Infrared-Hyperspectral-Imaging-for-Protein-Content-Regression-and-Grain-Variety-Classification-Using-Bulk-References-and-Varying-Grain-to-Background-Ratios" class="headerlink" title="Analyzing Near-Infrared Hyperspectral Imaging for Protein Content Regression and Grain Variety Classification Using Bulk References and Varying Grain-to-Background Ratios"></a>Analyzing Near-Infrared Hyperspectral Imaging for Protein Content Regression and Grain Variety Classification Using Bulk References and Varying Grain-to-Background Ratios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04042">http://arxiv.org/abs/2311.04042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole-Christian Galbo Engstrøm, Erik Schou Dreier, Birthe Møller Jespersen, Kim Steenstrup Pedersen</li>
<li>for: 这个论文主要针对的是使用near-infrared hyperspectral imaging（NIR-HSI）图像来调整模型，特别是蛋白质含量预测和谷物种 classification。</li>
<li>methods: 论文使用了subsampling和association方法来扩展有限的参考数据，以便适应更多的情况。然而，这种方法会导致skewed leptokurtic prediction distributions，影响PLS-R和深度CNN模型。</li>
<li>results: 论文提出了一些修正方法来减轻这些偏见，并对蛋白质参考预测的准确性进行了改进。此外，论文还研究了不同的谷物种比例对两个任务的影响。<details>
<summary>Abstract</summary>
Based on previous work, we assess the use of NIR-HSI images for calibrating models on two datasets, focusing on protein content regression and grain variety classification. Limited reference data for protein content is expanded by subsampling and associating it with the bulk sample. However, this method introduces significant biases due to skewed leptokurtic prediction distributions, affecting both PLS-R and deep CNN models. We propose adjustments to mitigate these biases, improving mean protein reference predictions. Additionally, we investigate the impact of grain-to-background ratios on both tasks. Higher ratios yield more accurate predictions, but including lower-ratio images in calibration enhances model robustness for such scenarios.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:根据前一项研究，我们评估了使用近红外多spectral图像（NIR-HSI）来调整两个数据集上的模型，主要关注蛋白量回归和谷物种分类。但由于蛋白量参考数据的有限性，我们使用下采样和与大量样本关联来扩展参考数据。这种方法引入了显著的偏见，影响了PLS-R和深度神经网络模型的预测。为了缓解这些偏见，我们提出了调整，以提高蛋白量参考预测的均值。此外，我们还调查了芯果与背景率对两个任务的影响，发现高芯果率导致更准确的预测，但包含低芯果率图像在核心级别上进行调整可以提高模型对这种场景的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Data-exploitation-multi-task-learning-of-object-detection-and-semantic-segmentation-on-partially-annotated-data"><a href="#Data-exploitation-multi-task-learning-of-object-detection-and-semantic-segmentation-on-partially-annotated-data" class="headerlink" title="Data exploitation: multi-task learning of object detection and semantic segmentation on partially annotated data"></a>Data exploitation: multi-task learning of object detection and semantic segmentation on partially annotated data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04040">http://arxiv.org/abs/2311.04040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoàng-Ân Lê, Minh-Tan Pham</li>
<li>for: 这 paper  investigate 多任务 partially annotated data 的潜在帮助，即每个数据点只有一个任务得到标注。</li>
<li>methods: 这 paper 使用 joint learning 方法，将 object detection 和 semantic segmentation 作为两个任务，从多任务数据中学习。</li>
<li>results: 实验结果表明，多任务学习和知识传播可以获得更好的性能，比单任务学习和全supervision scenario。所有代码和数据分割可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Multi-task partially annotated data where each data point is annotated for only a single task are potentially helpful for data scarcity if a network can leverage the inter-task relationship. In this paper, we study the joint learning of object detection and semantic segmentation, the two most popular vision problems, from multi-task data with partial annotations. Extensive experiments are performed to evaluate each task performance and explore their complementarity when a multi-task network cannot optimize both tasks simultaneously. We propose employing knowledge distillation to leverage joint-task optimization. The experimental results show favorable results for multi-task learning and knowledge distillation over single-task learning and even full supervision scenario. All code and data splits are available at https://github.com/lhoangan/multas
</details>
<details>
<summary>摘要</summary>
多任务半标注数据，每个数据点只有单个任务的标注，可能对数据缺乏情况带来帮助，如果网络可以利用任务之间的关系。在这篇论文中，我们研究了对象检测和 semantic segmentation 两个最受欢迎的视觉问题的共同学习，从多任务数据中的半标注中学习。我们进行了广泛的实验来评估每个任务的性能，并探索它们之间的补充性。我们提议使用知识传播来利用共同优化。实验结果显示，多任务学习和知识传播在单任务学习和全supervision情况下具有有利的效果。所有代码和数据分割可以在 GitHub 上找到：https://github.com/lhoangan/multas。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Dataset-Scale-Indicators-of-Data-Quality"><a href="#Exploring-Dataset-Scale-Indicators-of-Data-Quality" class="headerlink" title="Exploring Dataset-Scale Indicators of Data Quality"></a>Exploring Dataset-Scale Indicators of Data Quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04016">http://arxiv.org/abs/2311.04016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Feuer, Chinmay Hegde</li>
<li>for: 这篇论文主要是为了解决现代计算机视觉基础模型的训练所需的大量数据和成本问题。</li>
<li>methods: 这篇论文使用了改进数据质量来降低模型训练所需的数据量。作者认为数据质量可以分解为样本级别和数据集级别两个部分，而样本级别的研究更为广泛。他们还研究了标签集设计和分类均衡对模型性能的影响。</li>
<li>results: 作者通过监测这些指标来评估模型性能，包括准确率和分布转移Robustness。<details>
<summary>Abstract</summary>
Modern computer vision foundation models are trained on massive amounts of data, incurring large economic and environmental costs. Recent research has suggested that improving data quality can significantly reduce the need for data quantity. But what constitutes data quality in computer vision? We posit that the quality of a given dataset can be decomposed into distinct sample-level and dataset-level constituents, and that the former have been more extensively studied than the latter. We ablate the effects of two important dataset-level constituents: label set design, and class balance. By monitoring these constituents using key indicators we provide, researchers and practitioners can better anticipate model performance, measured in terms of its accuracy and robustness to distribution shifts.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉基础模型通常在大量数据上进行训练，导致经济和环境成本增加。 latest research suggests that improving data quality can significantly reduce the need for data quantity. 但是，计算机视觉中的数据质量哪些组成部分？ we propose that the quality of a given dataset can be decomposed into two constituents: sample-level and dataset-level.  sample-level constituents have been more extensively studied than dataset-level constituents. we investigate the effects of two important dataset-level constituents: label set design and class balance. by monitoring these constituents using key indicators we provide, researchers and practitioners can better anticipate model performance, measured in terms of its accuracy and robustness to distribution shifts.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="AGNES-Abstraction-guided-Framework-for-Deep-Neural-Networks-Security"><a href="#AGNES-Abstraction-guided-Framework-for-Deep-Neural-Networks-Security" class="headerlink" title="AGNES: Abstraction-guided Framework for Deep Neural Networks Security"></a>AGNES: Abstraction-guided Framework for Deep Neural Networks Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04009">http://arxiv.org/abs/2311.04009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Dhonthi, Marcello Eiermann, Ernst Moritz Hahn, Vahid Hashemi</li>
<li>for: 这篇论文旨在检测深度神经网络（DNNs）中的后门，以确保图像识别 task 的正确性。</li>
<li>methods: 本文提出了一种名为 AGNES 的工具，用于检测 DNNs 中的后门。该工具基于一种新的检测方法，可以更好地检测多种不同类型的后门。</li>
<li>results: 作者通过多个实验示例表明，AGNES 比许多现有的方法在多个有关的案例中表现更好。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are becoming widespread, particularly in safety-critical areas. One prominent application is image recognition in autonomous driving, where the correct classification of objects, such as traffic signs, is essential for safe driving. Unfortunately, DNNs are prone to backdoors, meaning that they concentrate on attributes of the image that should be irrelevant for their correct classification. Backdoors are integrated into a DNN during training, either with malicious intent (such as a manipulated training process, because of which a yellow sticker always leads to a traffic sign being recognised as a stop sign) or unintentional (such as a rural background leading to any traffic sign being recognised as animal crossing, because of biased training data).   In this paper, we introduce AGNES, a tool to detect backdoors in DNNs for image recognition. We discuss the principle approach on which AGNES is based. Afterwards, we show that our tool performs better than many state-of-the-art methods for multiple relevant case studies.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在安全关键领域广泛应用，特别是自动驾驶图像识别。正确识别图像中的对象，如交通标志，是安全驾驶的关键。可惜，DNN容易受到后门攻击，即它们强调图像中不相关的特征，导致错误识别。后门可以在训练过程中被针对性地注入（如一个受到修改的训练过程，导致所有的交通标志都被识别为停车标志），或者不Intentional（如农村背景导致任何交通标志都被识别为动物过道）。在这篇论文中，我们介绍了AGNES，一种用于检测DNN图像识别后门的工具。我们讲述AGNES的原理方法。然后，我们展示了我们的工具在多个相关的案例研究中的性能比以前的state-of-the-art方法更好。
</details></li>
</ul>
<hr>
<h2 id="Bias-and-Diversity-in-Synthetic-based-Face-Recognition"><a href="#Bias-and-Diversity-in-Synthetic-based-Face-Recognition" class="headerlink" title="Bias and Diversity in Synthetic-based Face Recognition"></a>Bias and Diversity in Synthetic-based Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03970">http://arxiv.org/abs/2311.03970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Huber, Anh Thi Luu, Fadi Boutros, Arjan Kuijper, Naser Damer</li>
<li>for: 本研究旨在 investigate synthetic face recognition datasets 的多样性和生成模型的偏见问题。</li>
<li>methods: 我们使用了三种最新的synthetic-based face recognition模型，并对不同的人群特征（性别、种族、年龄、头部位置）进行分析。</li>
<li>results: 我们发现生成模型对不同特征的分布类似于用于训练的数据分布，而且synthetic-based模型与authentic-based模型具有相似的偏见行为。 however, we found that the lower intra-identity attribute consistency in the synthetic data can help reduce bias.<details>
<summary>Abstract</summary>
Synthetic data is emerging as a substitute for authentic data to solve ethical and legal challenges in handling authentic face data. The current models can create real-looking face images of people who do not exist. However, it is a known and sensitive problem that face recognition systems are susceptible to bias, i.e. performance differences between different demographic and non-demographics attributes, which can lead to unfair decisions. In this work, we investigate how the diversity of synthetic face recognition datasets compares to authentic datasets, and how the distribution of the training data of the generative models affects the distribution of the synthetic data. To do this, we looked at the distribution of gender, ethnicity, age, and head position. Furthermore, we investigated the concrete bias of three recent synthetic-based face recognition models on the studied attributes in comparison to a baseline model trained on authentic data. Our results show that the generator generate a similar distribution as the used training data in terms of the different attributes. With regard to bias, it can be seen that the synthetic-based models share a similar bias behavior with the authentic-based models. However, with the uncovered lower intra-identity attribute consistency seems to be beneficial in reducing bias.
</details>
<details>
<summary>摘要</summary>
现代技术在面临道德和法律问题时，人工生成的数据正在扮演一个替代者。目前的模型可以生成存在不存在的人脸图像。然而，已经知道和敏感的问题是，人脸识别系统容易受到偏见影响，即不同的民族和非民族属性之间的性能差异，这可能导致不公正的决策。在这种工作中，我们研究了生成模型训练数据的多样性与authentic数据集的多样性之间的关系，以及生成模型训练数据的分布对生成数据的分布的影响。为此，我们查看了性别、民族、年龄和头部位置的分布。此外，我们还investigated了三个最近的人工生成基于面识别模型的偏见情况，与基eline模型在不同属性上的性能差异。我们的结果显示，生成器会生成与训练数据的分布相似的分布，包括不同的属性。在偏见方面，人工生成基于模型和authentic基于模型都表现出类似的偏见行为。然而，通过降低内属性一致性，可以减少偏见。
</details></li>
</ul>
<hr>
<h2 id="CeCNN-Copula-enhanced-convolutional-neural-networks-in-joint-prediction-of-refraction-error-and-axial-length-based-on-ultra-widefield-fundus-images"><a href="#CeCNN-Copula-enhanced-convolutional-neural-networks-in-joint-prediction-of-refraction-error-and-axial-length-based-on-ultra-widefield-fundus-images" class="headerlink" title="CeCNN: Copula-enhanced convolutional neural networks in joint prediction of refraction error and axial length based on ultra-widefield fundus images"></a>CeCNN: Copula-enhanced convolutional neural networks in joint prediction of refraction error and axial length based on ultra-widefield fundus images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03967">http://arxiv.org/abs/2311.03967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Zhong, Yang Li, Danjuan Yang, Meiyan Li, Xingyao Zhou, Bo Fu, Catherine C. Liu, A. H. Welsh</li>
<li>for: The paper is written for the purpose of developing a new deep learning model that can better predict myopia outcomes using both spherical equivalent (SE) and axial length (AL) information.</li>
<li>methods: The paper proposes a copula-enhanced convolutional neural network (CeCNN) framework that incorporates the dependence between responses through a Gaussian copula and uses the induced copula-likelihood loss with the backbone CNNs.</li>
<li>results: The paper shows that the proposed CeCNN model has better prediction accuracy after adding the dependency information to the backbone models, and the modeling and proposed algorithm are applicable beyond the ultra-widefield (UWF) scenario and can be effective with other backbones beyond ResNet and LeNet.<details>
<summary>Abstract</summary>
Ultra-widefield (UWF) fundus images are replacing traditional fundus images in screening, detection, prediction, and treatment of complications related to myopia because their much broader visual range is advantageous for highly myopic eyes. Spherical equivalent (SE) is extensively used as the main myopia outcome measure, and axial length (AL) has drawn increasing interest as an important ocular component for assessing myopia. Cutting-edge studies show that SE and AL are strongly correlated. Using the joint information from SE and AL is potentially better than using either separately. In the deep learning community, though there is research on multiple-response tasks with a 3D image biomarker, dependence among responses is only sporadically taken into consideration. Inspired by the spirit that information extracted from the data by statistical methods can improve the prediction accuracy of deep learning models, we formulate a class of multivariate response regression models with a higher-order tensor biomarker, for the bivariate tasks of regression-classification and regression-regression. Specifically, we propose a copula-enhanced convolutional neural network (CeCNN) framework that incorporates the dependence between responses through a Gaussian copula (with parameters estimated from a warm-up CNN) and uses the induced copula-likelihood loss with the backbone CNNs. We establish the statistical framework and algorithms for the aforementioned two bivariate tasks. We show that the CeCNN has better prediction accuracy after adding the dependency information to the backbone models. The modeling and the proposed CeCNN algorithm are applicable beyond the UWF scenario and can be effective with other backbones beyond ResNet and LeNet.
</details>
<details>
<summary>摘要</summary>
超宽场照片（UWF）正在取代传统的眼球照片，用于检测、预测和治疗高度近视相关的疾病，因为它们的视觉范围更广。球形等效（SE）广泛使用为高度近视的主要结果指标，而轴长（AL）也在引起越来越多的关注，因为它是诊断高度近视的重要组成部分。现代研究表明，SE和AL之间存在强相关性。使用两者的共同信息可能比使用单独的SE或AL更好。在深度学习社区中，虽然有关多个响应任务的3D图像生物标志的研究，但是对响应之间的依赖关系只 occasionally被考虑。我们由于信息从数据中提取出来的统计方法可以提高深度学习模型的预测精度，因此我们提出了一类多变量响应回归模型，其中包括高级 tensor生物标志。特别是，我们提出了一种含有 Gaussian copula 的 CeCNN 框架，该框架通过将 Gaussian copula 作为生成模型的一部分，将响应之间的依赖关系传递给后续 CNNs。我们建立了这些多变量响应回归任务的统计框架和算法。我们发现，在添加依赖信息后，CeCNN 的预测精度有所提高。这种模型和提出的 CeCNN 算法可以在 UWF 场景之外应用，并且可以与其他背景模型结合使用。
</details></li>
</ul>
<hr>
<h2 id="Fast-Sun-aligned-Outdoor-Scene-Relighting-based-on-TensoRF"><a href="#Fast-Sun-aligned-Outdoor-Scene-Relighting-based-on-TensoRF" class="headerlink" title="Fast Sun-aligned Outdoor Scene Relighting based on TensoRF"></a>Fast Sun-aligned Outdoor Scene Relighting based on TensoRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03965">http://arxiv.org/abs/2311.03965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeonjin Chang, Yearim Kim, Seunghyeon Seo, Jung Yi, Nojun Kwak</li>
<li>for: 这个论文是为了提出一种用于风景场景外部重新照明的方法，以提高Neural Radiance Fields（NeRF）的效果。</li>
<li>methods: 这个方法使用了名为Sun-aligned Relighting TensoRF（SR-TensoRF），它采用了与太阳方向相对的推算策略，从而实现了简化的工作流程，并且不需要环境图。</li>
<li>results: 该方法可以在训练和渲染过程中得到显著的加速，同时也能够保持与传统方法的比较水平。<details>
<summary>Abstract</summary>
In this work, we introduce our method of outdoor scene relighting for Neural Radiance Fields (NeRF) named Sun-aligned Relighting TensoRF (SR-TensoRF). SR-TensoRF offers a lightweight and rapid pipeline aligned with the sun, thereby achieving a simplified workflow that eliminates the need for environment maps. Our sun-alignment strategy is motivated by the insight that shadows, unlike viewpoint-dependent albedo, are determined by light direction. We directly use the sun direction as an input during shadow generation, simplifying the requirements of the inference process significantly. Moreover, SR-TensoRF leverages the training efficiency of TensoRF by incorporating our proposed cubemap concept, resulting in notable acceleration in both training and rendering processes compared to existing methods.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们介绍了一种名为sun-aligned Relighting TensoRF（SR-TensoRF）的外部场景重新照明方法，用于Neural Radiance Fields（NeRF）。 SR-TensoRF提供了一个轻量级、快速的管道，与太阳方向相对应，从而实现了简化的工作流程，消除了环境地图的需求。我们的太阳Alignment策略是基于观察，阴影不同于视点依赖的反射率，阴影的方向由太阳direction决定。我们直接在阴影生成过程中使用太阳方向作为输入，从而对推理过程的需求进行了明显的简化。此外，SR-TensoRF利用了TensoRF的训练效率，通过我们提出的立方体地图概念，从而在训练和渲染过程中具有显著的加速。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multimodal-Compositional-Reasoning-of-Visual-Language-Models-with-Generative-Negative-Mining"><a href="#Enhancing-Multimodal-Compositional-Reasoning-of-Visual-Language-Models-with-Generative-Negative-Mining" class="headerlink" title="Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining"></a>Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03964">http://arxiv.org/abs/2311.03964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ugorsahin/Generative-Negative-Mining">https://github.com/ugorsahin/Generative-Negative-Mining</a></li>
<li>paper_authors: Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, Volker Tresp</li>
<li>for: 提高图文理解任务中的多modal Compositional Reasoning能力</li>
<li>methods: 使用对向采集的图片和文本描述进行对比学习，并通过生成困难的负例来提高模型的承受能力</li>
<li>results: 使用这种方法可以在多modal Compositional Reasoning任务中显著提高图文模型的表现<details>
<summary>Abstract</summary>
Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing image and text understanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and corresponding text captions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused on mining negative examples from existing datasets. However, the mined negative examples might not be difficult for the model to discriminate from the positive. An alternative to mining would be negative sample generation 2) But existing generative approaches primarily focus on generating hard negative texts associated with a given image. Mining in the other direction, i.e., generating negative image samples associated with a given text has been ignored. To overcome both these limitations, we propose a framework that not only mines in both directions but also generates challenging negative samples in both modalities, i.e., images and texts. Leveraging these generative hard negative samples, we significantly enhance VLMs' performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.
</details>
<details>
<summary>摘要</summary>
现代大规模视觉语言模型（VLM）具有强大的表达能力，使其在图像和文本理解任务中广泛应用。它们通常通过对大量和多样化的图像和相关文本描述进行对比式训练。然而，VLM 经常在 композиitional 理解任务中遇到困难，这可以归结到两个主要因素：1. 对比方法traditionally 将焦点放在挖掘现有数据集中的负例中。然而，挖掘出来的负例可能并不是模型很难归类。一种alternative 是生成负例amples。2. 现有的生成方法主要是生成与给定图像相关的困难文本样本。然而，生成在另一个方向上的负例amples，即与给定文本相关的困难图像样本，还没有得到足够的关注。为了突破这两个限制，我们提议一个框架，不仅在两个方向上挖掘负例amples，而且还生成了困难的负例amples在两个模式下，即图像和文本。通过利用这些生成的困难负例amples，我们可以大幅提高 VLM 在多模式compositional reasoning任务中的表现。我们的代码和数据集可以在 <https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Effectiveness-of-Deep-Generative-Data"><a href="#Improving-the-Effectiveness-of-Deep-Generative-Data" class="headerlink" title="Improving the Effectiveness of Deep Generative Data"></a>Improving the Effectiveness of Deep Generative Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03959">http://arxiv.org/abs/2311.03959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyu Wang, Sabrina Schmedding, Marco F. Huber</li>
<li>for: 本研究旨在探讨使用深度生成模型生成的synthetic图像在下游图像处理任务中的表现。</li>
<li>methods: 本研究使用了一种新的分类法来描述在使用深度生成模型生成的synthetic图像时常见的现象，并在CIFAR-10 dataset上进行了广泛的实验。</li>
<li>results: 研究发现，使用我们提出的策略可以更好地利用深度生成模型生成的synthetic图像，并在数据稀缺的情况下表现出优于基elines。<details>
<summary>Abstract</summary>
Recent deep generative models (DGMs) such as generative adversarial networks (GANs) and diffusion probabilistic models (DPMs) have shown their impressive ability in generating high-fidelity photorealistic images. Although looking appealing to human eyes, training a model on purely synthetic images for downstream image processing tasks like image classification often results in an undesired performance drop compared to training on real data. Previous works have demonstrated that enhancing a real dataset with synthetic images from DGMs can be beneficial. However, the improvements were subjected to certain circumstances and yet were not comparable to adding the same number of real images. In this work, we propose a new taxonomy to describe factors contributing to this commonly observed phenomenon and investigate it on the popular CIFAR-10 dataset. We hypothesize that the Content Gap accounts for a large portion of the performance drop when using synthetic images from DGM and propose strategies to better utilize them in downstream tasks. Extensive experiments on multiple datasets showcase that our method outperforms baselines on downstream classification tasks both in case of training on synthetic only (Synthetic-to-Real) and training on a mix of real and synthetic data (Data Augmentation), particularly in the data-scarce scenario.
</details>
<details>
<summary>摘要</summary>
最近的深度生成模型（DGM），如生成敌方网络（GAN）和扩散概率模型（DPM），已经展示了高效的图像生成能力。although these models can generate visually appealing images, training a model on purely synthetic images for downstream image processing tasks like image classification often leads to a drop in performance compared to training on real data. previous studies have shown that enhancing a real dataset with synthetic images from DGMs can be beneficial, but the improvements were limited and not comparable to adding the same number of real images. in this work, we propose a new taxonomy to describe the factors contributing to this commonly observed phenomenon and investigate it on the popular CIFAR-10 dataset. we hypothesize that the Content Gap accounts for a large portion of the performance drop when using synthetic images from DGM and propose strategies to better utilize them in downstream tasks. extensive experiments on multiple datasets show that our method outperforms baselines in both synthetic-only and data augmentation scenarios, particularly in data-scarce situations.
</details></li>
</ul>
<hr>
<h2 id="CLIP-Guided-Image-perceptive-Prompt-Learning-for-Image-Enhancement"><a href="#CLIP-Guided-Image-perceptive-Prompt-Learning-for-Image-Enhancement" class="headerlink" title="CLIP Guided Image-perceptive Prompt Learning for Image Enhancement"></a>CLIP Guided Image-perceptive Prompt Learning for Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03943">http://arxiv.org/abs/2311.03943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zinuo Li, Qiuhong Ke, Weiwen Chen</li>
<li>for: 本研究旨在提高计算机视觉和图像处理领域中的图像增强方法，特别是使用学习基于方法。</li>
<li>methods: 本文提出了一种简单的结构 called CLIP-LUT，通过利用CLIP模型的先前知识来提高图像增强效果。我们首先学习了图像感知的提示，使CLIP模型能够有效地识别受损图像的质量，然后引入一个简单的网络来预测三种不同的LUT的加权值，并使用这些提示来导引增强网络。</li>
<li>results: 我们示出了将简单的方法与CLIP结合起来可以获得满意的结果。<details>
<summary>Abstract</summary>
Image enhancement is a significant research area in the fields of computer vision and image processing. In recent years, many learning-based methods for image enhancement have been developed, where the Look-up-table (LUT) has proven to be an effective tool. In this paper, we delve into the potential of Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning, proposing a simple structure called CLIP-LUT for image enhancement. We found that the prior knowledge of CLIP can effectively discern the quality of degraded images, which can provide reliable guidance. To be specific, We initially learn image-perceptive prompts to distinguish between original and target images using CLIP model, in the meanwhile, we introduce a very simple network by incorporating a simple baseline to predict the weights of three different LUT as enhancement network. The obtained prompts are used to steer the enhancement network like a loss function and improve the performance of model. We demonstrate that by simply combining a straightforward method with CLIP, we can obtain satisfactory results.
</details>
<details>
<summary>摘要</summary>
Image enhancement is a significant research area in computer vision and image processing. Recently, many learning-based methods for image enhancement have been developed, and the Look-up-table (LUT) has proven to be an effective tool. In this paper, we explore the potential of Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning for image enhancement. We propose a simple structure called CLIP-LUT, which uses the prior knowledge of CLIP to effectively discern the quality of degraded images and provide reliable guidance.To be specific, we first learn image-perceptive prompts to distinguish between original and target images using the CLIP model. Then, we introduce a very simple network that incorporates a simple baseline to predict the weights of three different LUTs as an enhancement network. The obtained prompts are used to steer the enhancement network like a loss function, improving the performance of the model. We demonstrate that by simply combining a straightforward method with CLIP, we can obtain satisfactory results.
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-NaN-Divergence-in-Training-Monocular-Depth-Estimation-Model"><a href="#Analysis-of-NaN-Divergence-in-Training-Monocular-Depth-Estimation-Model" class="headerlink" title="Analysis of NaN Divergence in Training Monocular Depth Estimation Model"></a>Analysis of NaN Divergence in Training Monocular Depth Estimation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03938">http://arxiv.org/abs/2311.03938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bum Jun Kim, Hyeonah Jang, Sang Woo Kim</li>
<li>for: 帮助解释深度学习中NaN损失的原因和避免方法</li>
<li>methods: 通过对带有NaN损失的训练数据进行深入分析，并通过实验证明了NaN损失的三种原因，包括使用平方根损失、Log-sigmoid函数和certain variance实现。</li>
<li>results: 通过遵循我们的指南，可以改善优化稳定性和带有NaN损失的深度学习模型的性能。<details>
<summary>Abstract</summary>
The latest advances in deep learning have facilitated the development of highly accurate monocular depth estimation models. However, when training a monocular depth estimation network, practitioners and researchers have observed not a number (NaN) loss, which disrupts gradient descent optimization. Although several practitioners have reported the stochastic and mysterious occurrence of NaN loss that bothers training, its root cause is not discussed in the literature. This study conducted an in-depth analysis of NaN loss during training a monocular depth estimation network and identified three types of vulnerabilities that cause NaN loss: 1) the use of square root loss, which leads to an unstable gradient; 2) the log-sigmoid function, which exhibits numerical stability issues; and 3) certain variance implementations, which yield incorrect computations. Furthermore, for each vulnerability, the occurrence of NaN loss was demonstrated and practical guidelines to prevent NaN loss were presented. Experiments showed that both optimization stability and performance on monocular depth estimation could be improved by following our guidelines.
</details>
<details>
<summary>摘要</summary>
最新的深度学习技术发展，使得单目深度估计模型可以很准确。然而，在训练单目深度估计网络时，实践者和研究人员经常会遇到NaN损失，这会阻碍梯度下降优化。虽然许多实践者报道了随机和神秘的NaN损失现象，但在文献中没有讨论其根本原因。本研究对单目深度估计网络训练中NaN损失进行了深入分析，并确定了三种可能导致NaN损失的漏洞：1）使用平方根损失，导致梯度不稳定；2）使用对数 sigmoid 函数，存在数学稳定问题；3）某些变量实现方式可能会导致错误计算。此外，对每种漏洞，我们都示出了NaN损失的发生和避免NaN损失的实践指南。实验表明，遵循我们的指南可以提高优化稳定性和单目深度估计性能。
</details></li>
</ul>
<hr>
<h2 id="FLORA-Fine-grained-Low-Rank-Architecture-Search-for-Vision-Transformer"><a href="#FLORA-Fine-grained-Low-Rank-Architecture-Search-for-Vision-Transformer" class="headerlink" title="FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer"></a>FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03912">http://arxiv.org/abs/2311.03912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shadowpa0327/flora">https://github.com/shadowpa0327/flora</a></li>
<li>paper_authors: Chi-Chih Chang, Yuan-Yao Sung, Shixing Yu, Ning-Chi Huang, Diana Marculescu, Kai-Chiang Wu</li>
<li>for: 这篇论文的目的是提出一个名为FLORA的自动化框架，用于实时部署中的Computer Vision Tasks中的ViT模型，以提高其计算效率。</li>
<li>methods: 这篇论文使用了Low-rank aware candidate filtering strategy和low-rank specific training paradigm，以提高低维度超网的质量。</li>
<li>results: 实验结果显示，FLORA可以自动生成更细化的维度配置，实现约33%的FLOPs节省，并且可以与主流压缩技术或对应的短袋结构结合使用，提供额外21%-26%的FLOPs节省。<details>
<summary>Abstract</summary>
Vision Transformers (ViT) have recently demonstrated success across a myriad of computer vision tasks. However, their elevated computational demands pose significant challenges for real-world deployment. While low-rank approximation stands out as a renowned method to reduce computational loads, efficiently automating the target rank selection in ViT remains a challenge. Drawing from the notable similarity and alignment between the processes of rank selection and One-Shot NAS, we introduce FLORA, an end-to-end automatic framework based on NAS. To overcome the design challenge of supernet posed by vast search space, FLORA employs a low-rank aware candidate filtering strategy. This method adeptly identifies and eliminates underperforming candidates, effectively alleviating potential undertraining and interference among subnetworks. To further enhance the quality of low-rank supernets, we design a low-rank specific training paradigm. First, we propose weight inheritance to construct supernet and enable gradient sharing among low-rank modules. Secondly, we adopt low-rank aware sampling to strategically allocate training resources, taking into account inherited information from pre-trained models. Empirical results underscore FLORA's efficacy. With our method, a more fine-grained rank configuration can be generated automatically and yield up to 33% extra FLOPs reduction compared to a simple uniform configuration. More specific, FLORA-DeiT-B/FLORA-Swin-B can save up to 55%/42% FLOPs almost without performance degradtion. Importantly, FLORA boasts both versatility and orthogonality, offering an extra 21%-26% FLOPs reduction when integrated with leading compression techniques or compact hybrid structures. Our code is publicly available at https://github.com/shadowpa0327/FLORA.
</details>
<details>
<summary>摘要</summary>
幻 transformer (ViT) 在计算机视觉任务中表现出色，但它们的计算负担却很高，妨碍实际应用。而且，选择目标排名是一项有名望的方法来减少计算负担，但是在 ViT 中自动化目标排名选择仍然是一个挑战。通过 noticed 的 similarity and alignment between rank selection and One-Shot NAS，我们提出了 FLORA，一个基于 NAS 的端到端自动框架。尝试超网的设计挑战，FLORA 使用了低级别扩展的候选人筛选策略，可以快速地标识和消除低性能的候选人，从而避免潜在的过度训练和互相干扰。此外，我们还设计了一种低级别特有的训练方法，包括继承重量和低级别扩展的采样策略。这些方法可以提高低级别超网的质量，从而实现更好的性能和计算效率。实验结果表明，FLORA 可以自动生成更细化的排名配置，并提供更多的计算效率。具体来说，FLORA-DeiT-B/FLORA-Swin-B 可以节省至 55%/42% FLOPs，几乎不受性能下降的影响。此外，FLORA 还具有灵活和正交的特点，可以与主流压缩技术或嵌入式混合结构结合使用，提供更多的计算效率提升。我们的代码可以在 <https://github.com/shadowpa0327/FLORA> 上下载。
</details></li>
</ul>
<hr>
<h2 id="RobustMat-Neural-Diffusion-for-Street-Landmark-Patch-Matching-under-Challenging-Environments"><a href="#RobustMat-Neural-Diffusion-for-Street-Landmark-Patch-Matching-under-Challenging-Environments" class="headerlink" title="RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments"></a>RobustMat: Neural Diffusion for Street Landmark Patch Matching under Challenging Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03904">http://arxiv.org/abs/2311.03904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-it-avs/robustmat">https://github.com/ai-it-avs/robustmat</a></li>
<li>paper_authors: Rui She, Qiyu Kang, Sijie Wang, Yuan-Rui Yang, Kai Zhao, Yang Song, Wee Peng Tay</li>
<li>for: 这 paper 的目的是提出一种robust的 Visual Perception技术，用于自动驾驶车辆（AVs）的信息获取和处理。</li>
<li>methods: 这 paper 使用了一种名为 RobustMat 的方法，该方法利用摄像头上的图像数据，并通过 neural differential equations 学习 feature representation，以及 graph neural PDE diffusion module 来协同处理多个 landmark patches。</li>
<li>results: 这 paper 的实验结果表明，RobustMat 方法在不同的季节、天气和照明条件下，可以提供 state-of-the-art 的匹配结果。<details>
<summary>Abstract</summary>
For autonomous vehicles (AVs), visual perception techniques based on sensors like cameras play crucial roles in information acquisition and processing. In various computer perception tasks for AVs, it may be helpful to match landmark patches taken by an onboard camera with other landmark patches captured at a different time or saved in a street scene image database. To perform matching under challenging driving environments caused by changing seasons, weather, and illumination, we utilize the spatial neighborhood information of each patch. We propose an approach, named RobustMat, which derives its robustness to perturbations from neural differential equations. A convolutional neural ODE diffusion module is used to learn the feature representation for the landmark patches. A graph neural PDE diffusion module then aggregates information from neighboring landmark patches in the street scene. Finally, feature similarity learning outputs the final matching score. Our approach is evaluated on several street scene datasets and demonstrated to achieve state-of-the-art matching results under environmental perturbations.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)For autonomous vehicles (AVs), visual perception techniques based on sensors like cameras play crucial roles in information acquisition and processing. In various computer perception tasks for AVs, it may be helpful to match landmark patches taken by an onboard camera with other landmark patches captured at a different time or saved in a street scene image database. To perform matching under challenging driving environments caused by changing seasons, weather, and illumination, we utilize the spatial neighborhood information of each patch. We propose an approach, named RobustMat, which derives its robustness to perturbations from neural differential equations. A convolutional neural ODE diffusion module is used to learn the feature representation for the landmark patches. A graph neural PDE diffusion module then aggregates information from neighboring landmark patches in the street scene. Finally, feature similarity learning outputs the final matching score. Our approach is evaluated on several street scene datasets and demonstrated to achieve state-of-the-art matching results under environmental perturbations.Note: The translation is done in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="MeVGAN-GAN-based-Plugin-Model-for-Video-Generation-with-Applications-in-Colonoscopy"><a href="#MeVGAN-GAN-based-Plugin-Model-for-Video-Generation-with-Applications-in-Colonoscopy" class="headerlink" title="MeVGAN: GAN-based Plugin Model for Video Generation with Applications in Colonoscopy"></a>MeVGAN: GAN-based Plugin Model for Video Generation with Applications in Colonoscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03884">http://arxiv.org/abs/2311.03884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Łukasz Struski, Tomasz Urbańczyk, Krzysztof Bucki, Bartłomiej Cupiał, Aneta Kaczyńska, Przemysław Spurek, Jacek Tabor</li>
<li>for: 这个论文是为了提出一种能够生成高分辨率视频的生成模型，以解决现有的视频生成模型具有大量内存需求的问题。</li>
<li>methods: 该论文提出了一种插件式架构的生成 adversarial Network（GAN），称为 Memory Efficient Video GAN（MeVGAN），使用了预训练的2D图像GAN，并只添加了一个简单的神经网络来构造噪声空间中的轨迹，以便将轨迹通过GAN模型构造出真实的视频。</li>
<li>results: 该论文应用了MeVGAN在colonoscopy视频生成任务中，并证明了MeVGAN可以生成高质量的 sintetic colonoscopy视频，可以用于虚拟 simulate器中。 colonoscopy是一种重要的医学手术，尤其是在检测和管理肠癌方面有益。但是，因为colonoscopy是一种困难和时间consuming的学习过程，因此colonoscopy simulator在教育年轻的colonoscopists中广泛使用。<details>
<summary>Abstract</summary>
Video generation is important, especially in medicine, as much data is given in this form. However, video generation of high-resolution data is a very demanding task for generative models, due to the large need for memory. In this paper, we propose Memory Efficient Video GAN (MeVGAN) - a Generative Adversarial Network (GAN) which uses plugin-type architecture. We use a pre-trained 2D-image GAN and only add a simple neural network to construct respective trajectories in the noise space, so that the trajectory forwarded through the GAN model constructs a real-life video. We apply MeVGAN in the task of generating colonoscopy videos. Colonoscopy is an important medical procedure, especially beneficial in screening and managing colorectal cancer. However, because colonoscopy is difficult and time-consuming to learn, colonoscopy simulators are widely used in educating young colonoscopists. We show that MeVGAN can produce good quality synthetic colonoscopy videos, which can be potentially used in virtual simulators.
</details>
<details>
<summary>摘要</summary>
视频生成对医学领域来说非常重要，因为大量数据都是以视频形式提供。然而，高分辨率视频生成对生成模型来说是一项非常具有挑战性的任务，因为需要大量的内存。在这篇论文中，我们提出了 Memory Efficient Video GAN（MeVGAN），这是一种基于生成对抗网络（GAN）的插件型架构。我们使用预训练的2D图像GAN，只是添加了一个简单的神经网络，以在噪声空间中构造各自的轨迹，从而使得轨迹在GAN模型中传递的整个视频是真实的。我们在执行colonoscopy视频生成任务中应用了MeVGAN。colonoscopy是一种医学检查，尤其是在检测和管理肠癌方面非常有优势。然而，因为colonoscopy是一项困难和时间consuming的学习，因此colonoscopy模拟器在培训年轻的colonoscopist中广泛使用。我们显示MeVGAN可以生成高质量的synthetic colonoscopy视频，这些视频可能可以在虚拟模拟器中使用。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Knowledge-Transfer-Methods-for-Misaligned-Urban-Building-Labels"><a href="#A-Comparative-Study-of-Knowledge-Transfer-Methods-for-Misaligned-Urban-Building-Labels" class="headerlink" title="A Comparative Study of Knowledge Transfer Methods for Misaligned Urban Building Labels"></a>A Comparative Study of Knowledge Transfer Methods for Misaligned Urban Building Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03867">http://arxiv.org/abs/2311.03867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bipul Neupane, Jagannath Aryal, Abbas Rajabifard</li>
<li>for:  Addressing the misalignment issue in Earth observation (EO) images and building labels to train accurate convolutional neural networks (CNNs) for semantic segmentation of building footprints.</li>
<li>methods:  Comparative study of three Teacher-Student knowledge transfer methods: supervised domain adaptation (SDA), knowledge distillation (KD), and deep mutual learning (DML).</li>
<li>results:  SDA is the most effective method to address the misalignment problem, while KD and DML can efficiently compress network size without significant loss in performance. The 158 experiments and datasets developed in this study will be valuable to minimize the misaligned labels.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;<details>
<summary>Abstract</summary>
Misalignment in Earth observation (EO) images and building labels impact the training of accurate convolutional neural networks (CNNs) for semantic segmentation of building footprints. Recently, three Teacher-Student knowledge transfer methods have been introduced to address this issue: supervised domain adaptation (SDA), knowledge distillation (KD), and deep mutual learning (DML). However, these methods are merely studied for different urban buildings (low-rise, mid-rise, high-rise, and skyscrapers), where misalignment increases with building height and spatial resolution. In this study, we present a workflow for the systematic comparative study of the three methods. The workflow first identifies the best (with the highest evaluation scores) hyperparameters, lightweight CNNs for the Student (among 43 CNNs from Computer Vision), and encoder-decoder networks (EDNs) for both Teachers and Students. Secondly, three building footprint datasets are developed to train and evaluate the identified Teachers and Students in the three transfer methods. The results show that U-Net with VGG19 (U-VGG19) is the best Teacher, and U-EfficientNetv2B3 and U-EfficientNet-lite0 are among the best Students. With these Teacher-Student pairs, SDA could yield upto 0.943, 0.868, 0.912, and 0.697 F1 scores in the low-rise, mid-rise, high-rise, and skyscrapers respectively. KD and DML provide model compression of upto 82%, despite marginal loss in performance. This new comparison concludes that SDA is the most effective method to address the misalignment problem, while KD and DML can efficiently compress network size without significant loss in performance. The 158 experiments and datasets developed in this study will be valuable to minimise the misaligned labels.
</details>
<details>
<summary>摘要</summary>
地球观测（EO）图像和建筑标注的不一致问题会影响建筑 semantic 分类器的训练，特别是高层建筑。近些年，三种教师学生知识传播方法被提出来解决这个问题：指导适应领域（SDA）、知识储存（KD）和深度相互学习（DML）。然而，这些方法只被研究在不同的城市建筑（低层、中层、高层和天际线），而这些建筑的不一致程度随着建筑高度和空间分辨率增加。在这项研究中，我们提供了一个系统性比较工作流程，以评估这三种方法的效果。我们的工作流程包括：1. 选择最佳（最高评价分）的超参数、轻量级 CNN（从计算机视觉领域中选择43种 CNN）和编码器解码器网络（EDN）。2. 为教师和学生制作三种建筑块标注数据集。结果显示，U-Net with VGG19（U-VGG19）是最佳教师，而U-EfficientNetv2B3和U-EfficientNet-lite0是最佳学生之一。使用这些教师和学生对照，SDA可以达到0.943、0.868、0.912和0.697的F1分数在不同的高度建筑中。KD和DML可以压缩网络大小达82%，尽管性能下降不大。这一新的比较结论表明，SDA是解决不一致标注问题的最有效方法，而KD和DML可以高效压缩网络大小，无需重大影响性能。在这项研究中，我们开发了158个实验和数据集，这些数据集将有助于减少不一致标注。
</details></li>
</ul>
<hr>
<h2 id="SCONE-GAN-Semantic-Contrastive-learning-based-Generative-Adversarial-Network-for-an-end-to-end-image-translation"><a href="#SCONE-GAN-Semantic-Contrastive-learning-based-Generative-Adversarial-Network-for-an-end-to-end-image-translation" class="headerlink" title="SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation"></a>SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial Network for an end-to-end image translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03866">http://arxiv.org/abs/2311.03866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Abbasnejad, Fabio Zambetta, Flora Salim, Timothy Wiley, Jeffrey Chan, Russell Gallagher, Ehsan Abbasnejad</li>
<li>for: 学习生成真实和多样化的景象图像</li>
<li>methods: 使用图 convolutional neural networks 学习对象依赖关系，保持图像结构和 semantics，并使用样式参照图进行风格化</li>
<li>results: 在四个 dataset 上进行图像到图像翻译和风格化，并取得了较高的质量和多样性的图像生成 результаpts<details>
<summary>Abstract</summary>
SCONE-GAN presents an end-to-end image translation, which is shown to be effective for learning to generate realistic and diverse scenery images. Most current image-to-image translation approaches are devised as two mappings: a translation from the source to target domain and another to represent its inverse. While successful in many applications, these approaches may suffer from generating trivial solutions with limited diversity. That is because these methods learn more frequent associations rather than the scene structures. To mitigate the problem, we propose SCONE-GAN that utilises graph convolutional networks to learn the objects dependencies, maintain the image structure and preserve its semantics while transferring images into the target domain. For more realistic and diverse image generation we introduce style reference image. We enforce the model to maximize the mutual information between the style image and output. The proposed method explicitly maximizes the mutual information between the related patches, thus encouraging the generator to produce more diverse images. We validate the proposed algorithm for image-to-image translation and stylizing outdoor images. Both qualitative and quantitative results demonstrate the effectiveness of our approach on four dataset.
</details>
<details>
<summary>摘要</summary>
SCONE-GAN 提出了一种端到端图像翻译方法，可以帮助学习生成真实和多样化的景象图像。现有的图像到图像翻译方法通常是两个映射：一个从源领域到目标领域，另一个用于表示其 inverse。虽然在许多应用中成功，但这些方法可能会导致生成轻松的解决方案，具有有限的多样性。这是因为这些方法学习的更频繁是景象的关系，而不是场景的结构。为了解决这个问题，我们提议使用图 convolutional networks 来学习对象的依赖关系，保持图像的结构，并保持图像的 semantics  while 将图像转换到目标领域。为了更加真实和多样的图像生成，我们引入了风格引用图像。我们强制模型将 Style 图像和输出之间的共同信息最大化。我们的方法将相关的 patch 之间的共同信息最大化，因此激励生成器生成更多样的图像。我们验证了我们的方法在图像到图像翻译和风格化户外图像方面的效果。四个数据集的质量和量化结果都表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Information-Integration-and-Propagation-for-Occluded-Person-Re-identification"><a href="#Multi-view-Information-Integration-and-Propagation-for-Occluded-Person-Re-identification" class="headerlink" title="Multi-view Information Integration and Propagation for Occluded Person Re-identification"></a>Multi-view Information Integration and Propagation for Occluded Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03828">http://arxiv.org/abs/2311.03828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nengdong96/mviip">https://github.com/nengdong96/mviip</a></li>
<li>paper_authors: Neng Dong, Shuanglin Yan, Hao Tang, Jinhui Tang, Liyan Zhang</li>
<li>for: 提高 occluded person re-identification task 的精度，使用多视图图像来强化人脸特征表示。</li>
<li>methods: 提出 Multi-view Information Integration and Propagation (MVI$^{2}$P) 框架，integrate 多视图图像的特征图，通过 CAMs-aware Localization 模块和 probability-aware Quantification 模块来选择高可靠性信息，并使用 Information Propagation 机制储存知识。</li>
<li>results: 经验和分析表明，MVI$^{2}$P 能够提高 occluded person re-identification  task 的精度，并且在不同的 occlusion 情况下保持稳定性。<details>
<summary>Abstract</summary>
Occluded person re-identification (re-ID) presents a challenging task due to occlusion perturbations. Although great efforts have been made to prevent the model from being disturbed by occlusion noise, most current solutions only capture information from a single image, disregarding the rich complementary information available in multiple images depicting the same pedestrian. In this paper, we propose a novel framework called Multi-view Information Integration and Propagation (MVI$^{2}$P). Specifically, realizing the potential of multi-view images in effectively characterizing the occluded target pedestrian, we integrate feature maps of which to create a comprehensive representation. During this process, to avoid introducing occlusion noise, we develop a CAMs-aware Localization module that selectively integrates information contributing to the identification. Additionally, considering the divergence in the discriminative nature of different images, we design a probability-aware Quantification module to emphatically integrate highly reliable information. Moreover, as multiple images with the same identity are not accessible in the testing stage, we devise an Information Propagation (IP) mechanism to distill knowledge from the comprehensive representation to that of a single occluded image. Extensive experiments and analyses have unequivocally demonstrated the effectiveness and superiority of the proposed MVI$^{2}$P. The code will be released at \url{https://github.com/nengdong96/MVIIP}.
</details>
<details>
<summary>摘要</summary>
受遮挡干扰的人识别（re-ID）问题具有挑战性，因为遮挡干扰会对模型产生负面影响。虽然大量努力已经尝试避免遮挡干扰对模型的影响，但大多数当前解决方案仅利用单个图像信息，忽略了同一人物被多张图像捕捉的丰富补充信息。在这篇论文中，我们提出了一种新的框架，即多视图信息集成和传播（MVI$^{2}$P）。具体来说，利用多视图图像的优势，我们集成了特征地图，以创建全面的表示。在这个过程中，我们开发了一个aware的Localization模块，以选择atively интеgrate有助于识别的信息。此外，考虑到不同图像之间的分布特征的不同，我们设计了一个概率感知的量化模块，以强调高可靠性信息的集成。此外，在测试阶段无法获得多张图像同一个人物的情况下，我们提出了信息传播（IP）机制，以储存知识从全面表示中传递到受遮挡图像上。我们的实验和分析结果明显表明了我们提出的MVI$^{2}$P的有效性和优越性。代码将在 \url{https://github.com/nengdong96/MVIIP} 上发布。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Any-Human-Object-Interaction-Relationship-Universal-HOI-Detector-with-Spatial-Prompt-Learning-on-Foundation-Models"><a href="#Detecting-Any-Human-Object-Interaction-Relationship-Universal-HOI-Detector-with-Spatial-Prompt-Learning-on-Foundation-Models" class="headerlink" title="Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models"></a>Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03799">http://arxiv.org/abs/2311.03799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Caoyichao/UniHOI">https://github.com/Caoyichao/UniHOI</a></li>
<li>paper_authors: Yichao Cao, Qingfei Tang, Xiu Su, Chen Song, Shan You, Xiaobo Lu, Chang Xu</li>
<li>for: 本研究旨在开拓HOI检测领域中的开放世界Setting下的UniversalInteractionRecognition，通过使用视觉语言基础模型和大型自然语言模型（LLM）。</li>
<li>methods: 本方法包括HO Prompt-based Learning，即在视觉基础模型中协调高级关系表示，以及使用GPT进行交互解释，实现更加复杂的HOIs的语言理解。</li>
<li>results: 相比 existed方法，UniHOI在预导和零shot Setting下均能够取得显著的性能提升，并且支持多种输入类型，包括交互短语和解释句。<details>
<summary>Abstract</summary>
Human-object interaction (HOI) detection aims to comprehend the intricate relationships between humans and objects, predicting $<human, action, object>$ triplets, and serving as the foundation for numerous computer vision tasks. The complexity and diversity of human-object interactions in the real world, however, pose significant challenges for both annotation and recognition, particularly in recognizing interactions within an open world context. This study explores the universal interaction recognition in an open-world setting through the use of Vision-Language (VL) foundation models and large language models (LLMs). The proposed method is dubbed as \emph{\textbf{UniHOI}. We conduct a deep analysis of the three hierarchical features inherent in visual HOI detectors and propose a method for high-level relation extraction aimed at VL foundation models, which we call HO prompt-based learning. Our design includes an HO Prompt-guided Decoder (HOPD), facilitates the association of high-level relation representations in the foundation model with various HO pairs within the image. Furthermore, we utilize a LLM (\emph{i.e.} GPT) for interaction interpretation, generating a richer linguistic understanding for complex HOIs. For open-category interaction recognition, our method supports either of two input types: interaction phrase or interpretive sentence. Our efficient architecture design and learning methods effectively unleash the potential of the VL foundation models and LLMs, allowing UniHOI to surpass all existing methods with a substantial margin, under both supervised and zero-shot settings. The code and pre-trained weights are available at: \url{https://github.com/Caoyichao/UniHOI}.
</details>
<details>
<summary>摘要</summary>
人物交互检测（HOI）目标是理解人与物之间复杂的关系，预测<人,动作,物> triplets，并成为许多计算机视觉任务的基础。然而，现实世界中人物交互的复杂和多样性使得注释和识别具有挑战性，特别是在开放世界上下文中。这种研究通过使用视力语言基础模型（VL）和大型自然语言模型（LLM）来实现对开放世界中人物交互的识别。我们提出了一种名为UniHOI的方法，包括一个HO Prompt-based Decoder（HOPD），用于将高级关系表示assoicated with不同的HO对在图像中。此外，我们使用GPT（一种大型自然语言模型）来解释交互，生成更加详细的语言理解，以便处理复杂的HOI。我们的方法支持两种输入类型：交互短语和解释句子。我们的有效架构设计和学习方法可以有效发挥VL基础模型和LLM的潜力，使UniHOI在both supervised和零shot设置下，与所有现有方法相比，具有substantial margin。代码和预训练 веса可以在以下链接获取：<https://github.com/Caoyichao/UniHOI>。
</details></li>
</ul>
<hr>
<h2 id="Self-MI-Efficient-Multimodal-Fusion-via-Self-Supervised-Multi-Task-Learning-with-Auxiliary-Mutual-Information-Maximization"><a href="#Self-MI-Efficient-Multimodal-Fusion-via-Self-Supervised-Multi-Task-Learning-with-Auxiliary-Mutual-Information-Maximization" class="headerlink" title="Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization"></a>Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03785">http://arxiv.org/abs/2311.03785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cam-Van Thi Nguyen, Ngoc-Hoa Thi Nguyen, Duc-Trong Le, Quang-Thuy Ha<br>for:自然语言处理和计算机视觉领域中的多Modal表示学习问题，具有 capture informative和distinct特征的多Modal模式很大的挑战。methods:我们提出了一种基于自适应学习的Self-MI方法，通过利用Contrastive Predictive Coding（CPC）作为辅助技术，以 Maximize Mutual Information（MI） между单模态输入对和多模态融合结果与单模态输入。results:我们设计了一个标签生成模块，$ULG_{MI}$，可以在自适应的方式下创建每个模式的有用和信息强的标签。通过最大化MI，我们鼓励更好地对多模态融合和单 modalities进行对齐，从而提高多模态融合。我们在CMU-MOSI、CMU-MOSEI和SIMS三个benchmark dataset上进行了广泛的实验，并证明了Self-MI在多模态融合任务中的效果。<details>
<summary>Abstract</summary>
Multimodal representation learning poses significant challenges in capturing informative and distinct features from multiple modalities. Existing methods often struggle to exploit the unique characteristics of each modality due to unified multimodal annotations. In this study, we propose Self-MI in the self-supervised learning fashion, which also leverage Contrastive Predictive Coding (CPC) as an auxiliary technique to maximize the Mutual Information (MI) between unimodal input pairs and the multimodal fusion result with unimodal inputs. Moreover, we design a label generation module, $ULG_{MI}$ for short, that enables us to create meaningful and informative labels for each modality in a self-supervised manner. By maximizing the Mutual Information, we encourage better alignment between the multimodal fusion and the individual modalities, facilitating improved multimodal fusion. Extensive experiments on three benchmark datasets including CMU-MOSI, CMU-MOSEI, and SIMS, demonstrate the effectiveness of Self-MI in enhancing the multimodal fusion task.
</details>
<details>
<summary>摘要</summary>
多Modal表示学习具有很大的挑战，因为capturing informative和distinct特征从多个Modalities中很难。现有的方法通常因为各个Modalities的共同标注而难以利用每个Modalities的独特特征。在本研究中，我们提出了基于自我supervised learning的Self-MI方法，同时还利用了Contrastive Predictive Coding（CPC）作为辅助技术，以 Maximize Mutual Information（MI） между单modal输入对和多modal融合结果与单modal输入。此外，我们还设计了一个Label生成模块，简称为$ULG_{MI}$，它允许我们在自我supervised的情况下创建有意义和有用的标签 для每个Modalities。通过Maximize Mutual Information，我们鼓励了多modal融合和单modal输入之间更好的启合，从而提高多modal融合。在CMU-MOSI、CMU-MOSEI和SIMS三个 benchmark datasets上进行了广泛的实验， demonstarted Self-MI的效果iveness在提高多modal融合任务中。
</details></li>
</ul>
<hr>
<h2 id="UP-NeRF-Unconstrained-Pose-Prior-Free-Neural-Radiance-Fields"><a href="#UP-NeRF-Unconstrained-Pose-Prior-Free-Neural-Radiance-Fields" class="headerlink" title="UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields"></a>UP-NeRF: Unconstrained Pose-Prior-Free Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03784">http://arxiv.org/abs/2311.03784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Injae Kim, Minhyuk Choi, Hyunwoo J. Kim</li>
<li>for: 这篇论文旨在提出一种不需要摄像头位置估计的神经辐射场（NeRF）优化方法，以便在不受限制的图像集上实现高精度视图合成。</li>
<li>methods: 作者提出了一种叫做UP-NeRF（无constraint Pose-prior-free Neural Radiance Fields）的方法，通过用surrogate任务优化颜色不敏感特征场和分离出脏 occluder来解决不受限制图像集中的挑战。此外，作者还引入了一个候选头来提高pose估计的稳定性，以及一种适应性深度监督来减少误的估计影响。</li>
<li>results: 作者通过对著名的图像旅游 dataset（Phototourism）进行实验，证明了他们的方法在不受限制图像集上的性能比baseline（包括BARF和其他变种）更高。<details>
<summary>Abstract</summary>
Neural Radiance Field (NeRF) has enabled novel view synthesis with high fidelity given images and camera poses. Subsequent works even succeeded in eliminating the necessity of pose priors by jointly optimizing NeRF and camera pose. However, these works are limited to relatively simple settings such as photometrically consistent and occluder-free image collections or a sequence of images from a video. So they have difficulty handling unconstrained images with varying illumination and transient occluders. In this paper, we propose \textbf{UP-NeRF} (\textbf{U}nconstrained \textbf{P}ose-prior-free \textbf{Ne}ural \textbf{R}adiance \textbf{F}ields) to optimize NeRF with unconstrained image collections without camera pose prior. We tackle these challenges with surrogate tasks that optimize color-insensitive feature fields and a separate module for transient occluders to block their influence on pose estimation. In addition, we introduce a candidate head to enable more robust pose estimation and transient-aware depth supervision to minimize the effect of incorrect prior. Our experiments verify the superior performance of our method compared to the baselines including BARF and its variants in a challenging internet photo collection, \textit{Phototourism} dataset. The code of UP-NeRF is available at \url{https://github.com/mlvlab/UP-NeRF}.
</details>
<details>
<summary>摘要</summary>
neural radiance field (NeRF) 已经实现了基于图像和摄像头位置的高精度新视角合成。然而，这些工作受到了几个限制，例如：图像集的光度相对一致和没有遮蔽物。这些限制使得它们在真实世界中的应用相对较少。在这篇论文中，我们提出了\textbf{UP-NeRF} (\textbf{U}nconstrained \textbf{P}ose-prior-free \textbf{Ne}ural \textbf{R}adiance \textbf{F}ields)，它可以在无条件图像集中优化NeRF，不需要摄像头位置假设。我们在这篇论文中描述了一些挑战和解决方案，包括：* 使用代理任务来优化不受颜色影响的特征场，以及* 将短暂遮蔽物封锁在摄像头位置估计中，以避免这些遮蔽物对摄像头位置的影响。此外，我们还引入了候选头，以实现更加稳定的摄像头位置估计，并且将这些候选头与过去的摄像头位置估计进行比较，以确保更好的摄像头位置估计。我们的实验显示，UP-NeRF在具有挑战性的互联网照片集（Phototourism dataset）中表现出色，较以BARF和其他基于摄像头位置的方法为佳。UP-NeRF的代码可以在\url{https://github.com/mlvlab/UP-NeRF}获取。
</details></li>
</ul>
<hr>
<h2 id="CapST-An-Enhanced-and-Lightweight-Method-for-Deepfake-Video-Classification"><a href="#CapST-An-Enhanced-and-Lightweight-Method-for-Deepfake-Video-Classification" class="headerlink" title="CapST: An Enhanced and Lightweight Method for Deepfake Video Classification"></a>CapST: An Enhanced and Lightweight Method for Deepfake Video Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03782">http://arxiv.org/abs/2311.03782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wasim Ahmad, Yan-Tsung Peng, Yuan-Hao Chang, Gaddisa Olani Ganfure, Sarwar Khan, Sahibzada Adil Shahzad</li>
<li>for: 本研究旨在提出一种用于分类深度假视频的新型模型，以满足由多种AI技术生成的深度假视频的检测和识别需求。</li>
<li>methods: 本研究使用了一部分VGG19bn作为特征EXTRACTOR，并采用了一种卷积网络+空间时间注意力机制来增强模型的分类能力。此外，研究还利用了一种现有的视频级别融合技术，通过 temporal attention mechanism来处理 concatenated feature vectors，以便更好地利用视频中的 temporal dependencies。</li>
<li>results: 实验结果表明，本研究提出的方法可以准确地分类深度假视频，并且比基eline模型提高了4%。此外，本方法还具有更好的计算资源利用率，即使在较低的计算资源下也可以达到更高的准确率。<details>
<summary>Abstract</summary>
The proliferation of deepfake videos, synthetic media produced through advanced Artificial Intelligence techniques has raised significant concerns across various sectors, encompassing realms such as politics, entertainment, and security. In response, this research introduces an innovative and streamlined model designed to classify deepfake videos generated by five distinct encoders adeptly. Our approach not only achieves state of the art performance but also optimizes computational resources. At its core, our solution employs part of a VGG19bn as a backbone to efficiently extract features, a strategy proven effective in image-related tasks. We integrate a Capsule Network coupled with a Spatial Temporal attention mechanism to bolster the model's classification capabilities while conserving resources. This combination captures intricate hierarchies among features, facilitating robust identification of deepfake attributes. Delving into the intricacies of our innovation, we introduce an existing video level fusion technique that artfully capitalizes on temporal attention mechanisms. This mechanism serves to handle concatenated feature vectors, capitalizing on the intrinsic temporal dependencies embedded within deepfake videos. By aggregating insights across frames, our model gains a holistic comprehension of video content, resulting in more precise predictions. Experimental results on an extensive benchmark dataset of deepfake videos called DFDM showcase the efficacy of our proposed method. Notably, our approach achieves up to a 4 percent improvement in accurately categorizing deepfake videos compared to baseline models, all while demanding fewer computational resources.
</details>
<details>
<summary>摘要</summary>
“深圳技术的普及，导致深圳视频的生成和数据的处理，问题领域涵盖政治、娱乐和安全等领域。作为回应，这个研究提出了一个创新的和简化的模型，可以高效地分类深圳视频。我们的方法不仅实现了现有的性能水准，而且还可以优化计算资源。我们的解决方案是使用VGG19bn的一部分作为特征提取的基础，并与类时对应机制结合，以增强模型的分类能力。这组合使得模型能够强健地识别深圳视频的伪造特征。对于我们的创新之部分，我们引入了一个现有的视频级别融合技术，借由类时对应机制来处理 concatenated 特征向量。这种机制可以处理深圳视频中的时间相依关系，从而提供更加精确的预测。实验结果显示，我们的提案方法可以与基准模型相比，在深圳视频的分类任务中实现4%的提升，同时需要 fewer 计算资源。”
</details></li>
</ul>
<hr>
<h2 id="Meta-Adapter-An-Online-Few-shot-Learner-for-Vision-Language-Model"><a href="#Meta-Adapter-An-Online-Few-shot-Learner-for-Vision-Language-Model" class="headerlink" title="Meta-Adapter: An Online Few-shot Learner for Vision-Language Model"></a>Meta-Adapter: An Online Few-shot Learner for Vision-Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03774">http://arxiv.org/abs/2311.03774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Cheng, Lin Song, Ruoyi Xue, Hang Wang, Hongbin Sun, Yixiao Ge, Ying Shan</li>
<li>for: 这个研究旨在提高CLIP的几 shot学习能力，并且可以在线上进行调整，降低数据准确性的问题。</li>
<li>methods: 我们提出了一个名为Meta-Adapter的轻量级复合器，可以在线上使用几 shot样本来修正CLIP的特征。</li>
<li>results: 我们的方法可以在几 shot样本上取得高效的几 shot学习能力，并且可以在未见到的数据或任务上获得相对高的性能，而且具有较高的效率和简洁性。<details>
<summary>Abstract</summary>
The contrastive vision-language pre-training, known as CLIP, demonstrates remarkable potential in perceiving open-world visual concepts, enabling effective zero-shot image recognition. Nevertheless, few-shot learning methods based on CLIP typically require offline fine-tuning of the parameters on few-shot samples, resulting in longer inference time and the risk of over-fitting in certain domains. To tackle these challenges, we propose the Meta-Adapter, a lightweight residual-style adapter, to refine the CLIP features guided by the few-shot samples in an online manner. With a few training samples, our method can enable effective few-shot learning capabilities and generalize to unseen data or tasks without additional fine-tuning, achieving competitive performance and high efficiency. Without bells and whistles, our approach outperforms the state-of-the-art online few-shot learning method by an average of 3.6\% on eight image classification datasets with higher inference speed. Furthermore, our model is simple and flexible, serving as a plug-and-play module directly applicable to downstream tasks. Without further fine-tuning, Meta-Adapter obtains notable performance improvements in open-vocabulary object detection and segmentation tasks.
</details>
<details>
<summary>摘要</summary>
CLIP的对比式视觉语言预训练显示了开放世界视觉概念的惊人潜力，可以实现零shot图像识别。然而，基于CLIP的几shot学习方法通常需要离线参数的 fine-tuning，导致 longer inference time和风险过拟合在某些领域。为了解决这些挑战，我们提出了Meta-Adapter，一种轻量级的 residual-style adapter，可以在在线 manner中修改CLIP的特征，以便根据几shot样本进行修复。只需几个训练样本，我们的方法可以实现有效的几shot学习能力，并能够适应未经过训练的数据或任务，无需额外 fine-tuning，达到竞争性的性能和高效率。在八个图像分类 dataset 上，我们的方法平均提高了3.6%的性能，并且具有更高的执行速度。此外，我们的模型简单可靠，可以直接应用于下游任务，无需进一步 fine-tuning。在开放词汇Object检测和 segmentation 任务中，Meta-Adapter 也获得了显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Lightweight-Portrait-Matting-via-Regional-Attention-and-Refinement"><a href="#Lightweight-Portrait-Matting-via-Regional-Attention-and-Refinement" class="headerlink" title="Lightweight Portrait Matting via Regional Attention and Refinement"></a>Lightweight Portrait Matting via Regional Attention and Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03770">http://arxiv.org/abs/2311.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatao Zhong, Ilya Zharkov</li>
<li>for: 高解析肖像分割</li>
<li>methods: 使用两stage框架，首先使用低解析网络进行较course的α估计，然后使用改进网络进行局部区域的修正。</li>
<li>results: 比基eline模型高$1&#x2F;20$的FLOPS，在三个测试数据集上达到了更高的分割质量。<details>
<summary>Abstract</summary>
We present a lightweight model for high resolution portrait matting. The model does not use any auxiliary inputs such as trimaps or background captures and achieves real time performance for HD videos and near real time for 4K. Our model is built upon a two-stage framework with a low resolution network for coarse alpha estimation followed by a refinement network for local region improvement. However, a naive implementation of the two-stage model suffers from poor matting quality if not utilizing any auxiliary inputs. We address the performance gap by leveraging the vision transformer (ViT) as the backbone of the low resolution network, motivated by the observation that the tokenization step of ViT can reduce spatial resolution while retain as much pixel information as possible. To inform local regions of the context, we propose a novel cross region attention (CRA) module in the refinement network to propagate the contextual information across the neighboring regions. We demonstrate that our method achieves superior results and outperforms other baselines on three benchmark datasets while only uses $1/20$ of the FLOPS compared to the existing state-of-the-art model.
</details>
<details>
<summary>摘要</summary>
我们提出了一种轻量级的高分辨率肖像剪辑模型。该模型不使用任何辅助输入，如trimap或背景捕捉，并在高清视频和4K视频的实时性下运行。我们的模型基于两个阶段框架，其中低分辨率网络用于粗略alpha估计，然后是进行地域改进的改进网络。然而，如果不使用任何辅助输入，那么模型的剪辑质量很容易受到影响。我们解决这个性能差距的问题，通过利用视transformer（ViT）作为低分辨率网络的基础，这是因为ViT的tokenization步骤可以减少空间分辨率，同时保留最多的像素信息。为了在 neighouring region中传递Contextual信息，我们提出了一种新的跨区域注意力（CRA）模块，用于在邻近区域之间传递Contextual信息。我们示出，我们的方法可以在三个标准测试集上实现更好的结果，并且只需使用$1/20$的FLOPS，与现有状态之前的模型相比。
</details></li>
</ul>
<hr>
<h2 id="Image-change-detection-with-only-a-few-samples"><a href="#Image-change-detection-with-only-a-few-samples" class="headerlink" title="Image change detection with only a few samples"></a>Image change detection with only a few samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03762">http://arxiv.org/abs/2311.03762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Ke Liu, Zhaoyi Song, Haoyue Bai</li>
<li>for:  solves the problem of image change detection with a small number of samples, which is a significant problem due to the lack of large annotated datasets.</li>
<li>methods:  uses simple image processing methods to generate synthetic but informative datasets, and designs an early fusion network based on object detection to improve generalization ability.</li>
<li>results:  demonstrates that the synthetic data is informative enough to achieve higher generalization ability than the insufficient real-world data, and fine-tuning the model with a few samples can achieve excellent results.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究解决了图像变化检测具有小数量样本的问题，这是因为图像变化检测任务缺乏大量场景化标注数据。变化检测模型在不充足的数据上训练后，对各种场景的泛化能力异常差。</li>
<li>methods: 我们提出使用简单的图像处理方法生成Synthetic数据，并设计基于物体检测的早期融合网络，以提高模型的泛化能力。我们的关键发现是Synthetic数据使得训练模型在多种场景上具有良好的泛化能力。</li>
<li>results: 我们 comparing the model trained on Synthetic数据与从CDNet dataset中捕捉的实际数据，使用六个不同的测试集。结果表明Synthetic数据具有较高的泛化能力，而且通过使用几十个样本来精制模型，可以达到优秀的结果。<details>
<summary>Abstract</summary>
This paper considers image change detection with only a small number of samples, which is a significant problem in terms of a few annotations available. A major impediment of image change detection task is the lack of large annotated datasets covering a wide variety of scenes. Change detection models trained on insufficient datasets have shown poor generalization capability. To address the poor generalization issue, we propose using simple image processing methods for generating synthetic but informative datasets, and design an early fusion network based on object detection which could outperform the siamese neural network. Our key insight is that the synthetic data enables the trained model to have good generalization ability for various scenarios. We compare the model trained on the synthetic data with that on the real-world data captured from a challenging dataset, CDNet, using six different test sets. The results demonstrate that the synthetic data is informative enough to achieve higher generalization ability than the insufficient real-world data. Besides, the experiment shows that utilizing a few (often tens of) samples to fine-tune the model trained on the synthetic data will achieve excellent results.
</details>
<details>
<summary>摘要</summary>
The authors compare the model trained on the synthetic data with that on the real-world data captured from the challenging CDNet dataset using six different test sets. The results show that the synthetic data is informative enough to achieve higher generalization ability than the insufficient real-world data. Additionally, the experiment demonstrates that utilizing a few (often tens of) samples to fine-tune the model trained on the synthetic data will achieve excellent results.In simplified Chinese, the text can be translated as:这篇论文考虑了只使用少数样本进行图像变化检测，这是因为图像变化检测任务中缺乏充足的注释。图像变化检测模型在缺乏大量注释的情况下训练时表现出了差异化能力的问题。为解决这一问题，我们提出了使用简单的图像处理方法生成synthetic但具有信息的数据集，并设计了基于物体检测的早期融合网络。我们的关键发现是，synthetic数据使得训练模型具有较好的泛化能力。我们将模型训练在synthetic数据上与实际世界数据（从CDNet dataset中捕捉到的DiffNet dataset）进行比较，使用六个不同的测试集。结果表明，synthetic数据具有很好的泛化能力，而实际世界数据的泛化能力较差。此外，实验还表明，通过使用一些（通常是十几）样本来精度调整模型训练在synthetic数据上的模型，可以获得极佳的结果。
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Segmentation-using-Teeth-Attention-Modules-for-Dental-X-ray-Images"><a href="#Multiclass-Segmentation-using-Teeth-Attention-Modules-for-Dental-X-ray-Images" class="headerlink" title="Multiclass Segmentation using Teeth Attention Modules for Dental X-ray Images"></a>Multiclass Segmentation using Teeth Attention Modules for Dental X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03749">http://arxiv.org/abs/2311.03749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afnan Ghafoor, Seong-Yong Moon, Bumshik Lee</li>
<li>For: The paper proposes a novel teeth segmentation architecture that integrates an M-Net-like structure with Swin Transformers and a novel component named Teeth Attention Block (TAB) to improve the accuracy and reliability of teeth segmentation in dental panoramic images.* Methods: The proposed architecture utilizes an M-Net-like structure with Swin Transformers and TAB to capture local and global contextual information, and a unique attention mechanism that specifically highlights key elements of teeth features in panoramic images. The multiscale supervision strategy and squared Dice loss are also employed to enhance the performance of the segmentation.* Results: The proposed method was validated on a panoramic teeth X-ray dataset and outperformed existing state-of-the-art methods in objective metrics and visual examinations, demonstrating its efficacy for tooth segmentation on multiple benchmark dental image datasets.<details>
<summary>Abstract</summary>
This paper proposed a cutting-edge multiclass teeth segmentation architecture that integrates an M-Net-like structure with Swin Transformers and a novel component named Teeth Attention Block (TAB). Existing teeth image segmentation methods have issues with less accurate and unreliable segmentation outcomes due to the complex and varying morphology of teeth, although teeth segmentation in dental panoramic images is essential for dental disease diagnosis. We propose a novel teeth segmentation model incorporating an M-Net-like structure with Swin Transformers and TAB. The proposed TAB utilizes a unique attention mechanism that focuses specifically on the complex structures of teeth. The attention mechanism in TAB precisely highlights key elements of teeth features in panoramic images, resulting in more accurate segmentation outcomes. The proposed architecture effectively captures local and global contextual information, accurately defining each tooth and its surrounding structures. Furthermore, we employ a multiscale supervision strategy, which leverages the left and right legs of the U-Net structure, boosting the performance of the segmentation with enhanced feature representation. The squared Dice loss is utilized to tackle the class imbalance issue, ensuring accurate segmentation across all classes. The proposed method was validated on a panoramic teeth X-ray dataset, which was taken in a real-world dental diagnosis. The experimental results demonstrate the efficacy of our proposed architecture for tooth segmentation on multiple benchmark dental image datasets, outperforming existing state-of-the-art methods in objective metrics and visual examinations. This study has the potential to significantly enhance dental image analysis and contribute to advances in dental applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SBCFormer-Lightweight-Network-Capable-of-Full-size-ImageNet-Classification-at-1-FPS-on-Single-Board-Computers"><a href="#SBCFormer-Lightweight-Network-Capable-of-Full-size-ImageNet-Classification-at-1-FPS-on-Single-Board-Computers" class="headerlink" title="SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers"></a>SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03747">http://arxiv.org/abs/2311.03747</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyonglu/sbcformer">https://github.com/xyonglu/sbcformer</a></li>
<li>paper_authors: Xiangyong Lu, Masanori Suganuma, Takayuki Okatani</li>
<li>for: 这 paper 的目的是提出一种适用于单板计算机 (SBC) 上的 CNN-ViT 混合网络，以实现高精度和快速计算。</li>
<li>methods: 这 paper 使用了一种新的网络架构，叫做 SBCFormer，它结合了 CNN 和 ViT 两种网络，并且特意设计了一种适应 low-end CPU 的硬件约束。</li>
<li>results: 根据 paper 的实验结果，SBCFormer 可以在 Raspberry Pi 4 Model B 上 Achieve ImageNet-1K 顶部 1 准确率达到80%，并且在1.0帧&#x2F;秒的速度下运行。这是首次在 SBC 上实现这种性能。<details>
<summary>Abstract</summary>
Computer vision has become increasingly prevalent in solving real-world problems across diverse domains, including smart agriculture, fishery, and livestock management. These applications may not require processing many image frames per second, leading practitioners to use single board computers (SBCs). Although many lightweight networks have been developed for mobile/edge devices, they primarily target smartphones with more powerful processors and not SBCs with the low-end CPUs. This paper introduces a CNN-ViT hybrid network called SBCFormer, which achieves high accuracy and fast computation on such low-end CPUs. The hardware constraints of these CPUs make the Transformer's attention mechanism preferable to convolution. However, using attention on low-end CPUs presents a challenge: high-resolution internal feature maps demand excessive computational resources, but reducing their resolution results in the loss of local image details. SBCFormer introduces an architectural design to address this issue. As a result, SBCFormer achieves the highest trade-off between accuracy and speed on a Raspberry Pi 4 Model B with an ARM-Cortex A72 CPU. For the first time, it achieves an ImageNet-1K top-1 accuracy of around 80% at a speed of 1.0 frame/sec on the SBC. Code is available at https://github.com/xyongLu/SBCFormer.
</details>
<details>
<summary>摘要</summary>
计算机视觉在各种领域中日益普及，包括智能农业、渔业和畜牧管理。这些应用程序可能不需要处理大量图像帧数，因此实际使用单板计算机（SBC）。虽然许多轻量级网络已经为移动/边缘设备开发，但它们主要target着更具有力量的手机处理器，而不是SBC的低端CPU。这篇论文介绍了一种叫做SBCFormer的CNN-ViT混合网络，它在低端CPU上实现了高准确率和快速计算。由于硬件限制， transformer 的注意机制更适合SBC。但是在低端CPU上使用注意力表现出一个挑战：高分辨率内部特征图需要过度的计算资源，但是降低其分辨率会导致地方图像细节的丢失。SBCFormer 提出了一种体系设计来解决这个问题。因此，SBCFormer 在一个 Raspberry Pi 4 Model B 上实现了 ARM-Cortex A72 CPU 上的 ImageNet-1K 顶部1准确率约为 80%，并且速度为 1.0帧/秒。代码可以在 <https://github.com/xyongLu/SBCFormer> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Video-Summarization"><a href="#Unsupervised-Video-Summarization" class="headerlink" title="Unsupervised Video Summarization"></a>Unsupervised Video Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03745">http://arxiv.org/abs/2311.03745</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KaiyangZhou/pytorch-vsumm-reinforce">https://github.com/KaiyangZhou/pytorch-vsumm-reinforce</a></li>
<li>paper_authors: Hanqing Li, Diego Klabjan, Jean Utke</li>
<li>for: 这种方法用于自动视频摘要，利用生成对抗网络的想法，但是去掉识别器，使用简单的损失函数，并分解模型的不同部分的训练。</li>
<li>methods: 使用迭代训练策略，首先训练恢复器，然后训练帧选择器，并在训练和评估过程中添加可变的掩码向量。</li>
<li>results: 在两个公共数据集（SumMe和TVSum）以及四个自定义数据集（足球、LoL、MLB和ShortMLB）上进行了实验，结果显示了每个组件对模型性能的影响，特别是迭代训练策略的效果。对比现有方法，提出的方法在性能、稳定性和训练效率方面具有优势。<details>
<summary>Abstract</summary>
This paper introduces a new, unsupervised method for automatic video summarization using ideas from generative adversarial networks but eliminating the discriminator, having a simple loss function, and separating training of different parts of the model. An iterative training strategy is also applied by alternately training the reconstructor and the frame selector for multiple iterations. Furthermore, a trainable mask vector is added to the model in summary generation during training and evaluation. The method also includes an unsupervised model selection algorithm. Results from experiments on two public datasets (SumMe and TVSum) and four datasets we created (Soccer, LoL, MLB, and ShortMLB) demonstrate the effectiveness of each component on the model performance, particularly the iterative training strategy. Evaluations and comparisons with the state-of-the-art methods highlight the advantages of the proposed method in performance, stability, and training efficiency.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的无监督自动视频摘要方法，使用生成对抗网络的想法，但是去掉了识别器，使用简单的损失函数，并将模型的不同部分分解训练。此外，还采用了间歇训练STRATEGY，在多个迭代中 alternate训练重建器和帧选择器。此外，在摘要生成过程中，添加了可学习的掩码向量。方法还包括一种无监督模型选择算法。实验结果表明，每个组件对模型性能的影响，特别是间歇训练策略。对比现有方法，提出的方法在性能、稳定性和训练效率方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="3DifFusionDet-Diffusion-Model-for-3D-Object-Detection-with-Robust-LiDAR-Camera-Fusion"><a href="#3DifFusionDet-Diffusion-Model-for-3D-Object-Detection-with-Robust-LiDAR-Camera-Fusion" class="headerlink" title="3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion"></a>3DifFusionDet: Diffusion Model for 3D Object Detection with Robust LiDAR-Camera Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03742">http://arxiv.org/abs/2311.03742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhao Xiang, Simon Dräger, Jiawei Zhang</li>
<li>for: 这篇论文的目的是提出一个名为3DifFusionDet的三维物体检测框架，以实现高效精确的三维物体检测。</li>
<li>methods: 这篇论文使用了一种名为“denoising diffusion process”的方法，将受扰的三维盒子转换为目标盒子。在训练过程中，模型从真实的标准盒子中学习了逆转的过程。在推断过程中，模型逐渐精确地调整一些随机生成的盒子，以得到最终的结果。</li>
<li>results: 实验结果显示，3DifFusionDet在KITTIbenchmark上表现优异，与之前的知名检测器相比，具有更高的精确性和更高的效率。<details>
<summary>Abstract</summary>
Good 3D object detection performance from LiDAR-Camera sensors demands seamless feature alignment and fusion strategies. We propose the 3DifFusionDet framework in this paper, which structures 3D object detection as a denoising diffusion process from noisy 3D boxes to target boxes. In this framework, ground truth boxes diffuse in a random distribution for training, and the model learns to reverse the noising process. During inference, the model gradually refines a set of boxes that were generated at random to the outcomes. Under the feature align strategy, the progressive refinement method could make a significant contribution to robust LiDAR-Camera fusion. The iterative refinement process could also demonstrate great adaptability by applying the framework to various detecting circumstances where varying levels of accuracy and speed are required. Extensive experiments on KITTI, a benchmark for real-world traffic object identification, revealed that 3DifFusionDet is able to perform favorably in comparison to earlier, well-respected detectors.
</details>
<details>
<summary>摘要</summary>
好的3D物体检测性能从LiDAR-Camera感知器需要无缝特征对应和融合策略。我们在这篇论文中提出了3DifFusionDet框架，它将3D物体检测视为一个噪声扩散过程，从噪声3D盒子到目标盒子的扩散进行结构。在这个框架中，真实的盒子在训练中随机分布，模型学习反噪声过程。在推断中，模型逐渐精细化一组随机生成的盒子，以达到结果。在特征对应策略下，进行了逐渐精细化的方法，可以在不同的检测情况下提供了强大的适应性。例如，在不同的准确率和速度要求下，可以应用这种框架来进行多种不同的检测。在KITTI，一个实际的交通物体识别标准 bencmark上，我们进行了广泛的实验，发现3DifFusionDet能够与之前的高度尊敬的检测器相比，表现出优异的性能。
</details></li>
</ul>
<hr>
<h2 id="DeepInspect-An-AI-Powered-Defect-Detection-for-Manufacturing-Industries"><a href="#DeepInspect-An-AI-Powered-Defect-Detection-for-Manufacturing-Industries" class="headerlink" title="DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries"></a>DeepInspect: An AI-Powered Defect Detection for Manufacturing Industries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03725">http://arxiv.org/abs/2311.03725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arti Kumbhar, Amruta Chougale, Priya Lokhande, Saloni Navaghane, Aditi Burud, Saee Nimbalkar</li>
<li>for:  automatic defect detection in manufacturing</li>
<li>methods:  Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs)</li>
<li>results:  precise identification of faults, adaptability across various defect scenarios, reduction of waste and operational costs, enhancement of market competitiveness.<details>
<summary>Abstract</summary>
Utilizing Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Generative Adversarial Networks (GANs), our system introduces an innovative approach to defect detection in manufacturing. This technology excels in precisely identifying faults by extracting intricate details from product photographs, utilizing RNNs to detect evolving errors and generating synthetic defect data to bolster the model's robustness and adaptability across various defect scenarios. The project leverages a deep learning framework to automate real-time flaw detection in the manufacturing process. It harnesses extensive datasets of annotated images to discern complex defect patterns. This integrated system seamlessly fits into production workflows, thereby boosting efficiency and elevating product quality. As a result, it reduces waste and operational costs, ultimately enhancing market competitiveness.
</details>
<details>
<summary>摘要</summary>
我们的系统利用卷积神经网络（CNN）、循环神经网络（RNN）和生成对抗网络（GAN），引入了一种创新的瑕疵检测方法。这种技术能够准确地识别瑕疵，通过提取产品图像中的细节来检测演变性错误，并生成合成瑕疵数据来增强模型的可靠性和适应性。该系统利用深度学习框架自动化生产过程中的实时瑕疵检测。它利用大量标注图像集来识别复杂的瑕疵模式。这个整体系统顺应生产工艺流程，因此可以提高效率和产品质量。因此，它可以降低废弃和运营成本，最终提高市场竞争力。
</details></li>
</ul>
<hr>
<h2 id="Inertial-Guided-Uncertainty-Estimation-of-Feature-Correspondence-in-Visual-Inertial-Odometry-SLAM"><a href="#Inertial-Guided-Uncertainty-Estimation-of-Feature-Correspondence-in-Visual-Inertial-Odometry-SLAM" class="headerlink" title="Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry&#x2F;SLAM"></a>Inertial Guided Uncertainty Estimation of Feature Correspondence in Visual-Inertial Odometry&#x2F;SLAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03722">http://arxiv.org/abs/2311.03722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seongwook Yoon, Jaehyun Kim, Sanghoon Sull</li>
<li>for: 这个论文主要针对计算机视觉和机器人领域中的自主导航和增强现实系统，即视觉和同时地图生成（SLAM）问题。</li>
<li>methods: 该论文提出了一种基于特征跟踪和匹配的视觉增益和同时地图生成方法，并使用了陀螺仪测量单元（IMU）来帮助视觉传感器。</li>
<li>results: 论文提出的方法可以更加准确地估计特征匹配的uncertainty，并且在实验中demonstrate了其可行性。<details>
<summary>Abstract</summary>
Visual odometry and Simultaneous Localization And Mapping (SLAM) has been studied as one of the most important tasks in the areas of computer vision and robotics, to contribute to autonomous navigation and augmented reality systems. In case of feature-based odometry/SLAM, a moving visual sensor observes a set of 3D points from different viewpoints, correspondences between the projected 2D points in each image are usually established by feature tracking and matching. However, since the corresponding point could be erroneous and noisy, reliable uncertainty estimation can improve the accuracy of odometry/SLAM methods. In addition, inertial measurement unit is utilized to aid the visual sensor in terms of Visual-Inertial fusion. In this paper, we propose a method to estimate the uncertainty of feature correspondence using an inertial guidance robust to image degradation caused by motion blur, illumination change and occlusion. Modeling a guidance distribution to sample possible correspondence, we fit the distribution to an energy function based on image error, yielding more robust uncertainty than conventional methods. We also demonstrate the feasibility of our approach by incorporating it into one of recent visual-inertial odometry/SLAM algorithms for public datasets.
</details>
<details>
<summary>摘要</summary>
视觉增益和同时地理位和地图建模（SLAM）已经被视为计算机视觉和机器人领域中的一项非常重要的任务，以帮助实现自主导航和增强现实系统。在特征基于的增益/SLAM中，一个在不同视点观察的移动视觉器 observe一组3D点，通常通过特征跟踪和匹配来建立图像中每个2D点的对应关系。然而，由于对应点可能存在错误和噪声，可靠的不确定性估计可以提高增益/SLAM方法的准确性。此外，使用抗频率器来帮助视觉器，以实现视觉-抗频率融合。在这篇论文中，我们提出了一种用于估计特征对应关系的不确定性的方法，该方法基于图像误差能量函数，可以提供更加可靠的不确定性估计。我们还展示了我们的方法在一个现有的视觉-抗频率增益/SLAM算法中的可行性。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-deep-representation-learning-for-quantum-cross-platform-verification"><a href="#Multimodal-deep-representation-learning-for-quantum-cross-platform-verification" class="headerlink" title="Multimodal deep representation learning for quantum cross-platform verification"></a>Multimodal deep representation learning for quantum cross-platform verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03713">http://arxiv.org/abs/2311.03713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Qian, Yuxuan Du, Zhenliang He, Min-hsiu Hsieh, Dacheng Tao</li>
<li>for: 这篇论文的目的是为了解决早期量子计算中的跨平台验证问题，特别是在大量子比特的情况下。</li>
<li>methods: 这篇论文使用了一种创新的多模式学习方法，利用量子device上实验的量子测量结果和 classical描述的 compiled circuits 来建立一个完整的数据表示。</li>
<li>results: 这篇论文的结果显示，使用这种多模式学习方法可以对跨平台的量子device进行有效的验证，并且在不同的噪声模型下可以 achieve 三个数据的改善精度。<details>
<summary>Abstract</summary>
Cross-platform verification, a critical undertaking in the realm of early-stage quantum computing, endeavors to characterize the similarity of two imperfect quantum devices executing identical algorithms, utilizing minimal measurements. While the random measurement approach has been instrumental in this context, the quasi-exponential computational demand with increasing qubit count hurdles its feasibility in large-qubit scenarios. To bridge this knowledge gap, here we introduce an innovative multimodal learning approach, recognizing that the formalism of data in this task embodies two distinct modalities: measurement outcomes and classical description of compiled circuits on explored quantum devices, both enriched with unique information. Building upon this insight, we devise a multimodal neural network to independently extract knowledge from these modalities, followed by a fusion operation to create a comprehensive data representation. The learned representation can effectively characterize the similarity between the explored quantum devices when executing new quantum algorithms not present in the training data. We evaluate our proposal on platforms featuring diverse noise models, encompassing system sizes up to 50 qubits. The achieved results demonstrate a three-orders-of-magnitude improvement in prediction accuracy compared to the random measurements and offer compelling evidence of the complementary roles played by each modality in cross-platform verification. These findings pave the way for harnessing the power of multimodal learning to overcome challenges in wider quantum system learning tasks.
</details>
<details>
<summary>摘要</summary>
across-platform verification, a critical task in early-stage quantum computing, aims to characterize the similarity of two imperfect quantum devices executing identical algorithms using minimal measurements. Although the random measurement approach has been instrumental in this context, its computational demand increases exponentially with the number of qubits, making it unfeasible for large-qubit scenarios. To address this challenge, we propose an innovative multimodal learning approach that recognizes that the formalism of data in this task involves two distinct modalities: measurement outcomes and classical descriptions of compiled circuits on explored quantum devices, both of which contain unique information. We build on this insight to develop a multimodal neural network that independently extracts knowledge from these modalities, followed by a fusion operation to create a comprehensive data representation. The learned representation can effectively characterize the similarity between the explored quantum devices when executing new quantum algorithms not present in the training data. We evaluate our proposal on platforms with diverse noise models, including systems up to 50 qubits. The results show a three-orders-of-magnitude improvement in prediction accuracy compared to random measurements, providing compelling evidence of the complementary roles played by each modality in cross-platform verification. These findings pave the way for harnessing the power of multimodal learning to overcome challenges in wider quantum system learning tasks.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-convolutional-neural-network-fusion-approach-for-change-detection-in-remote-sensing-images"><a href="#Unsupervised-convolutional-neural-network-fusion-approach-for-change-detection-in-remote-sensing-images" class="headerlink" title="Unsupervised convolutional neural network fusion approach for change detection in remote sensing images"></a>Unsupervised convolutional neural network fusion approach for change detection in remote sensing images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03679">http://arxiv.org/abs/2311.03679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weidong Yan, Pei Yan, Li Cao</li>
<li>for: 这篇论文旨在提出一种不需要大量训练数据的深度学习改变检测方法。</li>
<li>methods: 本研究使用了一种简单的浅层神经网络（USCNN）融合方法，将双时间图像转换为不同的特征空间，并使用1*1卷积层融合不同时间图像的差异特征图像。</li>
<li>results: 实验结果显示，提出的方法可以实现不需要大量训练数据的改变检测，并且在四个实验数据中显示了可行性和有效性。<details>
<summary>Abstract</summary>
With the rapid development of deep learning, a variety of change detection methods based on deep learning have emerged in recent years. However, these methods usually require a large number of training samples to train the network model, so it is very expensive. In this paper, we introduce a completely unsupervised shallow convolutional neural network (USCNN) fusion approach for change detection. Firstly, the bi-temporal images are transformed into different feature spaces by using convolution kernels of different sizes to extract multi-scale information of the images. Secondly, the output features of bi-temporal images at the same convolution kernels are subtracted to obtain the corresponding difference images, and the difference feature images at the same scale are fused into one feature image by using 1 * 1 convolution layer. Finally, the output features of different scales are concatenated and a 1 * 1 convolution layer is used to fuse the multi-scale information of the image. The model parameters are obtained by a redesigned sparse function. Our model has three features: the entire training process is conducted in an unsupervised manner, the network architecture is shallow, and the objective function is sparse. Thus, it can be seen as a kind of lightweight network model. Experimental results on four real remote sensing datasets indicate the feasibility and effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
随着深度学习的快速发展，过去几年出现了多种基于深度学习的变化检测方法。然而，这些方法通常需要训练大量样本来训练网络模型，因此很昂贵。在这篇论文中，我们介绍了一种完全无监督的浅层卷积神经网络（USCNN）融合方法 для变化检测。首先，我们将bi-temporal图像转换为不同的特征空间，使用不同大小的卷积核来提取图像的多尺度信息。然后，同一个卷积核中的输出特征图像之间的差异图像被计算出来，并将同一个批处理的差异特征图像融合成一个特征图像使用1*1卷积层。最后，不同的批处理级别的输出特征被 concatenate 并使用1*1卷积层融合图像的多尺度信息。模型参数由一种重新定义的稀疏函数获得。我们的模型具有三个特点：整个训练过程是无监督的，网络架构是浅层的，目标函数是稀疏的。因此，它可以被视为一种轻量级的网络模型。实验结果表明，提posed方法在四个实际遥感数据集上的可行性和效果。
</details></li>
</ul>
<hr>
<h2 id="Image-Generation-and-Learning-Strategy-for-Deep-Document-Forgery-Detection"><a href="#Image-Generation-and-Learning-Strategy-for-Deep-Document-Forgery-Detection" class="headerlink" title="Image Generation and Learning Strategy for Deep Document Forgery Detection"></a>Image Generation and Learning Strategy for Deep Document Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03650">http://arxiv.org/abs/2311.03650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yamato Okamoto, Osada Genki, Iu Yahiro, Rintaro Hasegawa, Peifei Zhu, Hirokatsu Kataoka</li>
<li>for: 防止文档受到深度神经网络方法引起的伪造（document forgery）。</li>
<li>methods: 使用自然图像和文档图像自然学习，以及FD-VIED数据集来驱动文档检测模型的预训练。</li>
<li>results: 在实验中，我们的方法提高了文档检测性能。<details>
<summary>Abstract</summary>
In recent years, document processing has flourished and brought numerous benefits. However, there has been a significant rise in reported cases of forged document images. Specifically, recent advancements in deep neural network (DNN) methods for generative tasks may amplify the threat of document forgery. Traditional approaches for forged document images created by prevalent copy-move methods are unsuitable against those created by DNN-based methods, as we have verified. To address this issue, we construct a training dataset of document forgery images, named FD-VIED, by emulating possible attacks, such as text addition, removal, and replacement with recent DNN-methods. Additionally, we introduce an effective pre-training approach through self-supervised learning with both natural images and document images. In our experiments, we demonstrate that our approach enhances detection performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Instruct-Me-More-Random-Prompting-for-Visual-In-Context-Learning"><a href="#Instruct-Me-More-Random-Prompting-for-Visual-In-Context-Learning" class="headerlink" title="Instruct Me More! Random Prompting for Visual In-Context Learning"></a>Instruct Me More! Random Prompting for Visual In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03648">http://arxiv.org/abs/2311.03648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Zhang, Bowen Wang, Liangzhi Li, Yuta Nakashima, Hajime Nagahara</li>
<li>for: 这篇论文是用于探讨如何使用大规模模型进行内容学习（ICL），并且将这种方法应用到Computer Vision tasks。</li>
<li>methods: 这篇论文使用了一种名为Instruct Me More（InMeMo）的方法，它将内容组（input-output image pair）与查询图像（prompt）结合，以提高ICL的效能。</li>
<li>results: 实验结果显示，将learnable prompt加入ICL可以提高mainstream tasks的性能，包括增加了7.35和15.13的mIoU scores for foreground segmentation和single object detection tasks。<details>
<summary>Abstract</summary>
Large-scale models trained on extensive datasets, have emerged as the preferred approach due to their high generalizability across various tasks. In-context learning (ICL), a popular strategy in natural language processing, uses such models for different tasks by providing instructive prompts but without updating model parameters. This idea is now being explored in computer vision, where an input-output image pair (called an in-context pair) is supplied to the model with a query image as a prompt to exemplify the desired output. The efficacy of visual ICL often depends on the quality of the prompts. We thus introduce a method coined Instruct Me More (InMeMo), which augments in-context pairs with a learnable perturbation (prompt), to explore its potential. Our experiments on mainstream tasks reveal that InMeMo surpasses the current state-of-the-art performance. Specifically, compared to the baseline without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for foreground segmentation and single object detection tasks, respectively. Our findings suggest that InMeMo offers a versatile and efficient way to enhance the performance of visual ICL with lightweight training. Code is available at https://github.com/Jackieam/InMeMo.
</details>
<details>
<summary>摘要</summary>
大规模模型，通过它们的高通用性，在不同任务上表现出优异的成绩。在自然语言处理领域， Context Learning（ICL）是一种受欢迎的策略，使用这些模型来执行不同任务，只需提供指导性的提示，而不需更新模型参数。在计算机视觉领域，我们使用输入输出图像对（称为在 контекст对），并将查询图像作为提示，以便通过模型来示例出所需的输出。 visual ICL 的效果frequently depends on the quality of the prompts。因此，我们介绍了一种名为 Instruct Me More（InMeMo）的方法，该方法通过添加学习式的干扰（提示）来探索其 potential。我们的实验表明，Compared to the baseline without learnable prompt, InMeMo boosts mIoU scores by 7.35 and 15.13 for foreground segmentation and single object detection tasks, respectively。我们的发现表明，InMeMo 提供了一种 versatile 和高效的方式，用于增强视觉 ICLL 的性能，只需要轻量级的训练。代码可以在 <https://github.com/Jackieam/InMeMo> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Random-Field-Augmentations-for-Self-Supervised-Representation-Learning"><a href="#Random-Field-Augmentations-for-Self-Supervised-Representation-Learning" class="headerlink" title="Random Field Augmentations for Self-Supervised Representation Learning"></a>Random Field Augmentations for Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03629">http://arxiv.org/abs/2311.03629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Andrew Mansfield, Arash Afkanpour, Warren Richard Morningstar, Karan Singhal</li>
<li>for: 这篇论文是为了提高自主学习表示学习的表示学习效果而写的。</li>
<li>methods: 这篇论文提出了一新的局部变换基于 Gaussian 随机场，用于实现自主学习表示学习中的对称性。这些变换包括变形、旋转、颜色变化等，并且允许变换参数值在像素尺寸上 vary。</li>
<li>results: 这篇论文的实验结果显示，新的局部变换可以帮助自主学习表示学习中提高表示学习的效果。具体而言，在 ImageNet 下测试中，与基eline比较，我们 achieved 1.7% 的 top-1 精度提升，并在 out-of-distribution iNaturalist 下测试中 achieved 3.6% 的精度提升。但是，由于新的变换的灵活性，学习的表示受到了对称性和强度的影响，需要对对称性和强度进行平衡。<details>
<summary>Abstract</summary>
Self-supervised representation learning is heavily dependent on data augmentations to specify the invariances encoded in representations. Previous work has shown that applying diverse data augmentations is crucial to downstream performance, but augmentation techniques remain under-explored. In this work, we propose a new family of local transformations based on Gaussian random fields to generate image augmentations for self-supervised representation learning. These transformations generalize the well-established affine and color transformations (translation, rotation, color jitter, etc.) and greatly increase the space of augmentations by allowing transformation parameter values to vary from pixel to pixel. The parameters are treated as continuous functions of spatial coordinates, and modeled as independent Gaussian random fields. Empirical results show the effectiveness of the new transformations for self-supervised representation learning. Specifically, we achieve a 1.7% top-1 accuracy improvement over baseline on ImageNet downstream classification, and a 3.6% improvement on out-of-distribution iNaturalist downstream classification. However, due to the flexibility of the new transformations, learned representations are sensitive to hyperparameters. While mild transformations improve representations, we observe that strong transformations can degrade the structure of an image, indicating that balancing the diversity and strength of augmentations is important for improving generalization of learned representations.
</details>
<details>
<summary>摘要</summary>
自我指导学习中的表示学习强烈依赖于数据扩充来确定表示中的不变性。先前的工作表明，对表示学习而言，应用多样化数据扩充是关键，但是扩充技术还很少研究。在这项工作中，我们提出了一个新的本地变换家族，基于 Gaussian 随机场来生成图像扩充。这些变换超过了已知的平移、旋转和颜色扰动等变换，并大幅扩展了扩充的空间。变换参数被视为像素坐标上的连续函数，并被模型为独立的 Gaussian 随机场。实验结果表明新的变换对自我指导学习有效。特别是，我们在 ImageNet 下流类预测中 achieved 1.7% 的 top-1 精度提升，并在 iNaturalist 上流类预测中 achieved 3.6% 的精度提升。然而，由于新的变换的灵活性，学习的表示受到了 hyperparameter 的影响。虽然柔和的变换可以改进表示，但我们发现强大的变换可能会损害图像的结构，这表明在改进学习表示的通用化时，需要平衡扩充的多样性和强度。
</details></li>
</ul>
<hr>
<h2 id="FusionViT-Hierarchical-3D-Object-Detection-via-LiDAR-Camera-Vision-Transformer-Fusion"><a href="#FusionViT-Hierarchical-3D-Object-Detection-via-LiDAR-Camera-Vision-Transformer-Fusion" class="headerlink" title="FusionViT: Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion"></a>FusionViT: Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03620">http://arxiv.org/abs/2311.03620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhao Xiang, Jiawei Zhang</li>
<li>for: 3D object detection in real-world traffic scenarios, using both camera and lidar sensor data</li>
<li>methods: using a novel vision transformer-based framework called FusionViT, which embeds both images and point clouds and fuses them via a fusion vision transformer model for effective representation learning</li>
<li>results: achieving state-of-the-art performance and outperforming existing baseline methods and multi-modal image-point cloud deep fusion approaches on real-world benchmark datasets KITTI and Waymo Open<details>
<summary>Abstract</summary>
For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches.
</details>
<details>
<summary>摘要</summary>
For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches.Here's the translation in Traditional Chinese:For 3D object detection, both camera and lidar have been demonstrated to be useful sensory devices for providing complementary information about the same scenery with data representations in different modalities, e.g., 2D RGB image vs 3D point cloud. An effective representation learning and fusion of such multi-modal sensor data is necessary and critical for better 3D object detection performance. To solve the problem, in this paper, we will introduce a novel vision transformer-based 3D object detection model, namely FusionViT. Different from the existing 3D object detection approaches, FusionViT is a pure-ViT based framework, which adopts a hierarchical architecture by extending the transformer model to embed both images and point clouds for effective representation learning. Such multi-modal data embedding representations will be further fused together via a fusion vision transformer model prior to feeding the learned features to the object detection head for both detection and localization of the 3D objects in the input scenery. To demonstrate the effectiveness of FusionViT, extensive experiments have been done on real-world traffic object detection benchmark datasets KITTI and Waymo Open. Notably, our FusionViT model can achieve state-of-the-art performance and outperforms not only the existing baseline methods that merely rely on camera images or lidar point clouds, but also the latest multi-modal image-point cloud deep fusion approaches.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.CV_2023_11_07/" data-id="cloq1wl6900ld7o88hjbwgnsd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/07/eess.AS_2023_11_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-11-07
        
      </div>
    </a>
  
  
    <a href="/2023/11/07/cs.AI_2023_11_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-07</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">117</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">68</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">50</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
