
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-11-07 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Device Sampling and Resource Optimization for Federated Learning in Cooperative Edge Networks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04350 repo_url: None paper_authors: Su Wang, Roberto Morabito, Seyyed">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-11-07">
<meta property="og:url" content="https://nullscc.github.io/2023/11/07/cs.LG_2023_11_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Device Sampling and Resource Optimization for Federated Learning in Cooperative Edge Networks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04350 repo_url: None paper_authors: Su Wang, Roberto Morabito, Seyyed">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-07T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-09T19:38:45.400Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.LG_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T10:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-11-07
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Device-Sampling-and-Resource-Optimization-for-Federated-Learning-in-Cooperative-Edge-Networks"><a href="#Device-Sampling-and-Resource-Optimization-for-Federated-Learning-in-Cooperative-Edge-Networks" class="headerlink" title="Device Sampling and Resource Optimization for Federated Learning in Cooperative Edge Networks"></a>Device Sampling and Resource Optimization for Federated Learning in Cooperative Edge Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04350">http://arxiv.org/abs/2311.04350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su Wang, Roberto Morabito, Seyyedali Hosseinalipour, Mung Chiang, Christopher G. Brinton</li>
<li>for: 该研究目的是提高 Federated Learning（FedL）训练精度，并最小化数据处理和Device-to-Device（D2D）通信资源消耗。</li>
<li>methods: 该研究使用了一种新的优化方法，即智能设备采样和设备间数据下载配置选择，以最大化 FedL 训练精度，同时最小化数据处理和D2D通信资源消耗。该方法包括对 D2D 下载子问题的理论分析，并开发了一个高效的分析器。</li>
<li>results: 该研究通过使用图 convolutional neural networks（GCNs）学习网络特性、采样节点和D2D数据下载的关系，以最大化 FedL 精度。实验结果表明，该方法比Literature中常见的设备采样方法更高效，具有更好的机器学习模型性能、数据处理负担和能量消耗。<details>
<summary>Abstract</summary>
The conventional federated learning (FedL) architecture distributes machine learning (ML) across worker devices by having them train local models that are periodically aggregated by a server. FedL ignores two important characteristics of contemporary wireless networks, however: (i) the network may contain heterogeneous communication/computation resources, and (ii) there may be significant overlaps in devices' local data distributions. In this work, we develop a novel optimization methodology that jointly accounts for these factors via intelligent device sampling complemented by device-to-device (D2D) offloading. Our optimization methodology aims to select the best combination of sampled nodes and data offloading configuration to maximize FedL training accuracy while minimizing data processing and D2D communication resource consumption subject to realistic constraints on the network topology and device capabilities. Theoretical analysis of the D2D offloading subproblem leads to new FedL convergence bounds and an efficient sequential convex optimizer. Using these results, we develop a sampling methodology based on graph convolutional networks (GCNs) which learns the relationship between network attributes, sampled nodes, and D2D data offloading to maximize FedL accuracy. Through evaluation on popular datasets and real-world network measurements from our edge testbed, we find that our methodology outperforms popular device sampling methodologies from literature in terms of ML model performance, data processing overhead, and energy consumption.
</details>
<details>
<summary>摘要</summary>
传统的联邦学习（FedL）架构将机器学习（ML）分布在工作设备上，通过 periodic 将本地模型由服务器集成。然而，FedL 忽略了当代无线网络中两个重要特征：（一）网络可能包含不同的通信/计算资源，以及（二）设备的本地数据分布可能存在重叠。在这项工作中，我们开发了一种新的优化方法，通过智能设备采样和设备间通信（D2D）卸载来同时考虑这两个因素。我们的优化方法的目标是选择最佳的采样节点和数据卸载配置，以最大化 FedL 训练精度，同时最小化数据处理和D2D通信资源消耗，并且遵循网络拓扑和设备能力的实际约束。我们的分析表明，D2D卸载子问题可以得到新的 FedL 整合约束，以及一种高效的顺序凸优化器。基于图 convolutional neural networks（GCNs），我们开发了一种采样方法，可以学习网络特征、采样节点和D2D数据卸载之间的关系，以最大化 FedL 精度。通过使用实际网络测试床的数据和实际网络测量，我们发现，我们的方法可以比传统的设备采样方法在 ML 模型性能、数据处理开销和能源消耗方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="InstrumentGen-Generating-Sample-Based-Musical-Instruments-From-Text"><a href="#InstrumentGen-Generating-Sample-Based-Musical-Instruments-From-Text" class="headerlink" title="InstrumentGen: Generating Sample-Based Musical Instruments From Text"></a>InstrumentGen: Generating Sample-Based Musical Instruments From Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04339">http://arxiv.org/abs/2311.04339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahan Nercessian, Johannes Imort</li>
<li>for: 本研究目标是生成基于文本提示的样本音乐 инструментов，以推进自动化样本音乐 инструментов的研究。</li>
<li>methods: 本研究提出了 InstrumentGen 模型，它是一种基于文本提示的生成声音框架，可以根据乐器类型、源类型、音高（88键谱中的音高）、幅度和文本&#x2F;声音嵌入来conditioning。</li>
<li>results: 本研究实现了一个基本的文本-到-乐器基线，并推进了样本音乐 инструментов的自动生成研究领域。<details>
<summary>Abstract</summary>
We introduce the text-to-instrument task, which aims at generating sample-based musical instruments based on textual prompts. Accordingly, we propose InstrumentGen, a model that extends a text-prompted generative audio framework to condition on instrument family, source type, pitch (across an 88-key spectrum), velocity, and a joint text/audio embedding. Furthermore, we present a differentiable loss function to evaluate the intra-instrument timbral consistency of sample-based instruments. Our results establish a foundational text-to-instrument baseline, extending research in the domain of automatic sample-based instrument generation.
</details>
<details>
<summary>摘要</summary>
我们介绍了文本到乐器任务，该任务目的是基于文本提示生成样本化的乐器。我们提议了InstrumentGen模型，该模型是一种基于文本提示生成音频框架的扩展， condition on 乐器家族、源类型、音高（88键谱中的音高）、速度和joint文本/音频嵌入。此外，我们提出了可微分损失函数，用于评估样本化乐器内部的时间性响应。我们的结果建立了文本到乐器基线，推动了自动样本化乐器生成研究领域的发展。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Convex-Methods-for-Constrained-Linear-Bandits"><a href="#Convex-Methods-for-Constrained-Linear-Bandits" class="headerlink" title="Convex Methods for Constrained Linear Bandits"></a>Convex Methods for Constrained Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04338">http://arxiv.org/abs/2311.04338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Afsharrad, Ahmadreza Moradipari, Sanjay Lall</li>
<li>for: 这研究旨在 investigate the computational aspects of safe bandit algorithms, specifically safe linear bandits, and develop a framework that leverages convex programming tools to create computationally efficient policies.</li>
<li>methods: 该研究使用 convex programming tools to characterize the properties of the optimal policy for safe linear bandit problem, and then propose an end-to-end pipeline of safe linear bandit algorithms that only involves solving convex problems.</li>
<li>results: 研究人们提出了一个完整的安全线性帮助器框架，并进行了数值evaluation of the proposed methods.<details>
<summary>Abstract</summary>
Recently, bandit optimization has received significant attention in real-world safety-critical systems that involve repeated interactions with humans. While there exist various algorithms with performance guarantees in the literature, practical implementation of the algorithms has not received as much attention. This work presents a comprehensive study on the computational aspects of safe bandit algorithms, specifically safe linear bandits, by introducing a framework that leverages convex programming tools to create computationally efficient policies. In particular, we first characterize the properties of the optimal policy for safe linear bandit problem and then propose an end-to-end pipeline of safe linear bandit algorithms that only involves solving convex problems. We also numerically evaluate the performance of our proposed methods.
</details>
<details>
<summary>摘要</summary>
最近，匪师优化已经在实际中受到了严重的关注，特别是在安全关键系统中，这些系统需要重复地与人类进行交互。虽然文献中存在许多约束性能的算法，但实际实施这些算法却未得到了尽分的关注。本文提出了一个包括使用凸 программирова的框架，以创造高效的安全匪师策略的完整研究。特别是，我们首先描述了安全直线匪师问题的优化策略的属性，然后提出了一个综合的安全直线匪师算法链，这个链只需要解凸问题。我们还进行了数值评估我们提出的方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Lie-Point-Symmetry-and-Physics-Informed-Networks"><a href="#Lie-Point-Symmetry-and-Physics-Informed-Networks" class="headerlink" title="Lie Point Symmetry and Physics Informed Networks"></a>Lie Point Symmetry and Physics Informed Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04293">http://arxiv.org/abs/2311.04293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tara Akhound-Sadegh, Laurence Perreault-Levasseur, Johannes Brandstetter, Max Welling, Siamak Ravanbakhsh</li>
<li>for: 提高神经网络的通用性，使其能更好地解决部分 diferencial equation (PDE) 问题。</li>
<li>methods: 利用 Lie 点Symmetries 在 physics-informed neural networks (PINNs) 中进行集成，并提出一个基于 Lie 点Symmetries 的损失函数，使神经网络学习 PDE 解的同时也学习了其对应的 Lie 点Symmetries。</li>
<li>results: 实验表明，通过 Lie 点Symmetries 的 inductive bias，PINNs 的样本效率得到了大幅提高。<details>
<summary>Abstract</summary>
Symmetries have been leveraged to improve the generalization of neural networks through different mechanisms from data augmentation to equivariant architectures. However, despite their potential, their integration into neural solvers for partial differential equations (PDEs) remains largely unexplored. We explore the integration of PDE symmetries, known as Lie point symmetries, in a major family of neural solvers known as physics-informed neural networks (PINNs). We propose a loss function that informs the network about Lie point symmetries in the same way that PINN models try to enforce the underlying PDE through a loss function. Intuitively, our symmetry loss ensures that the infinitesimal generators of the Lie group conserve the PDE solutions. Effectively, this means that once the network learns a solution, it also learns the neighbouring solutions generated by Lie point symmetries. Empirical evaluations indicate that the inductive bias introduced by the Lie point symmetries of the PDEs greatly boosts the sample efficiency of PINNs.
</details>
<details>
<summary>摘要</summary>
对不同类型的神经网络进行了各种机制的同谱化，从数据扩充到等Variational architecture。然而，尽管它们的潜力很大，它们在物理学信息泛化神经网络（PINNs）中的集成仍然很少explored。我们在PINN模型中集成了解析Symmetries，即Lie点Symmetries，并提出了一个loss函数，该loss函数告诉网络关于Lie点Symmetries的信息。 intuitively，我们的Symmetry loss garantizar que los generadores infinitesimales del grupo de Lie conservan las soluciones de las Ecuaciones diferenciales parciales（PDEs）。 esto significa que, una vez que la red aprende una solución, también aprende las soluciones vecinas generadas por los symmetries de Lie. los evaluaciones empíricas indican que la bias inductivo introducido por los symmetries de Lie de las Ecuaciones diferenciales parciales mejora significativamente la eficiencia de muestras de PINNs.
</details></li>
</ul>
<hr>
<h2 id="Compilation-of-product-formula-Hamiltonian-simulation-via-reinforcement-learning"><a href="#Compilation-of-product-formula-Hamiltonian-simulation-via-reinforcement-learning" class="headerlink" title="Compilation of product-formula Hamiltonian simulation via reinforcement learning"></a>Compilation of product-formula Hamiltonian simulation via reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04285">http://arxiv.org/abs/2311.04285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leamarion/rl-for-compilation-of-product-formula-hamiltonian-simulation">https://github.com/leamarion/rl-for-compilation-of-product-formula-hamiltonian-simulation</a></li>
<li>paper_authors: Lea M. Trenkwalder, Eleanor Scerri, Thomas E. O’Brien, Vedran Dunjko</li>
<li>for: 这个研究是关于量子计算机中的汉密尔顿模拟问题，尤其是在量子计算机上实现汉密尔顿模拟时的Optimization问题。</li>
<li>methods: 这个研究使用了Trotterization方法，这是一种使用准确的approximation $e^{i\sum_jA_j}\sim \prod_je^{iA_j}$和其他更高阶 corrections的方法。然而，这还留下了一个问题：操作顺序（即在product中j的顺序）的选择。</li>
<li>results: 这个研究发现，可以通过使用机器学习技术来优化compile任务，特别是使用reinforcement learning。这个方法可以在gate counts上带来约12%的提升，相比于第二best方法，而且可以在整个问题家族上generalize。<details>
<summary>Abstract</summary>
Hamiltonian simulation is believed to be one of the first tasks where quantum computers can yield a quantum advantage. One of the most popular methods of Hamiltonian simulation is Trotterization, which makes use of the approximation $e^{i\sum_jA_j}\sim \prod_je^{iA_j}$ and higher-order corrections thereto. However, this leaves open the question of the order of operations (i.e. the order of the product over $j$, which is known to affect the quality of approximation). In some cases this order is fixed by the desire to minimise the error of approximation; when it is not the case, we propose that the order can be chosen to optimize compilation to a native quantum architecture. This presents a new compilation problem -- order-agnostic quantum circuit compilation -- which we prove is NP-hard in the worst case. In lieu of an easily-computable exact solution, we turn to methods of heuristic optimization of compilation. We focus on reinforcement learning due to the sequential nature of the compilation task, comparing it to simulated annealing and Monte Carlo tree search. While two of the methods outperform a naive heuristic, reinforcement learning clearly outperforms all others, with a gain of around 12% with respect to the second-best method and of around 50% compared to the naive heuristic in terms of the gate count. We further test the ability of RL to generalize across instances of the compilation problem, and find that a single learner is able to solve entire problem families. This demonstrates the ability of machine learning techniques to provide assistance in an order-agnostic quantum compilation task.
</details>
<details>
<summary>摘要</summary>
希amiltonian simulate是一个能量计算机可以获得量子优势的首要任务。使用Trotterization方法是最受欢迎的方法，该方法使用近似关系 $e^{i\sum_jA_j}\approx \prod_je^{iA_j}$ 和更高阶误差。然而，这还留下了操作顺序的问题（即在 $j$ 上的产品顺序，这知道会影响近似的质量）。在某些情况下，这个顺序是由望得到最小化误差而决定的；当它不是的情况下，我们提议可以选择优化到本地量子架构。这个问题被称为order-agnostic量子电路编译问题，我们证明其在最坏情况下是NP困难的。在没有可计算的精确解的情况下，我们转而使用机器学习方法进行优化编译。我们主要关注了强化学习，因为编译任务具有顺序性。我们对比了随机搜索和 Monte Carlo tree search，并发现了一个名为强化学习的方法可以准确地解决这个问题。在一个实验中，我们发现强化学习可以在gate Count方面比第二最佳方法提高约12%，并且可以在约50%的情况下比第二最佳方法提高约50%。此外，我们还测试了强化学习的泛化能力，发现一个学习者可以解决整个编译问题家族。这表明机器学习技术可以为order-agnostic量子编译任务提供帮助。
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Heavy-Tailed-Noise-Barrier-in-Stochastic-Optimization-Problems"><a href="#Breaking-the-Heavy-Tailed-Noise-Barrier-in-Stochastic-Optimization-Problems" class="headerlink" title="Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems"></a>Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04161">http://arxiv.org/abs/2311.04161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Puchkin, Eduard Gorbunov, Nikolay Kutuzov, Alexander Gasnikov</li>
<li>for: 这篇论文关注了 Stochastic Optimization 问题中具有极大尾部噪声的情况，并证明在这种情况下，可以获得更快的收敛速率，比如 $\mathcal{O}(K^{-2(\alpha - 1)&#x2F;\alpha})$。</li>
<li>methods: 作者使用了smoothed medians of means来稳定抽象 gradient，并证明了这种方法具有较低的偏误和可控的方差。</li>
<li>results: 作者通过将这种方法引入 clipped-SGD 和 clipped-SSTM 中，得到了新的高 probabilty 复杂度下界。<details>
<summary>Abstract</summary>
We consider stochastic optimization problems with heavy-tailed noise with structured density. For such problems, we show that it is possible to get faster rates of convergence than $\mathcal{O}(K^{-2(\alpha - 1)/\alpha})$, when the stochastic gradients have finite moments of order $\alpha \in (1, 2]$. In particular, our analysis allows the noise norm to have an unbounded expectation. To achieve these results, we stabilize stochastic gradients, using smoothed medians of means. We prove that the resulting estimates have negligible bias and controllable variance. This allows us to carefully incorporate them into clipped-SGD and clipped-SSTM and derive new high-probability complexity bounds in the considered setup.
</details>
<details>
<summary>摘要</summary>
我们考虑了随机估计问题，其中测量值具有重 tailed 杂质，并且具有结构化的数量分布。我们展示了在这种情况下，可以得到更快的速度增长率，比如 $\mathcal{O}(K^{-2(\alpha - 1)/\alpha})$，其中 $K$ 是迭代次数，$\alpha$ 是杂质的最大値。具体来说，我们的分析允许杂质的期望值为无限大。为了稳定随机梯度，我们使用了平滑的中值，即平滑的中值。我们证明了这些估计具有无视可控的偏差和方差。这使我们能够将其纳入 clipped-SGD 和 clipped-SSTM 中，并 deriv 出新的高机会范围内的可能性下界。
</details></li>
</ul>
<hr>
<h2 id="Computing-Approximate-ell-p-Sensitivities"><a href="#Computing-Approximate-ell-p-Sensitivities" class="headerlink" title="Computing Approximate $\ell_p$ Sensitivities"></a>Computing Approximate $\ell_p$ Sensitivities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04158">http://arxiv.org/abs/2311.04158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swati Padmanabhan, David P. Woodruff, Qiuyi, Zhang</li>
<li>for: 本文提出了一种简化方法来进行回归任务中的维度减少，通过计算数据点的敏感度来选择高敏感度数据点进行下折中值。</li>
<li>methods: 本文提供了一种高效的方法来计算$\ell_p$敏感度，包括计算$\ell_1$敏感度、全体$\ell_p$敏感度以及最大$\ell_1$敏感度。这些方法基于重要性权重的重要性抽象，可以在$O(n&#x2F;\alpha)$敏感度计算Cost中获得$\alpha$-近似的结果。</li>
<li>results: 本文的实验结果表明，在多个实际数据集中，可以快速地计算出大致相同的敏感度，并且实际的敏感度远低于理论预测，这表明实际数据集的内在效果维度较低。<details>
<summary>Abstract</summary>
Recent works in dimensionality reduction for regression tasks have introduced the notion of sensitivity, an estimate of the importance of a specific datapoint in a dataset, offering provable guarantees on the quality of the approximation after removing low-sensitivity datapoints via subsampling. However, fast algorithms for approximating $\ell_p$ sensitivities, which we show is equivalent to approximate $\ell_p$ regression, are known for only the $\ell_2$ setting, in which they are termed leverage scores.   In this work, we provide efficient algorithms for approximating $\ell_p$ sensitivities and related summary statistics of a given matrix. In particular, for a given $n \times d$ matrix, we compute $\alpha$-approximation to its $\ell_1$ sensitivities at the cost of $O(n/\alpha)$ sensitivity computations. For estimating the total $\ell_p$ sensitivity (i.e. the sum of $\ell_p$ sensitivities), we provide an algorithm based on importance sampling of $\ell_p$ Lewis weights, which computes a constant factor approximation to the total sensitivity at the cost of roughly $O(\sqrt{d})$ sensitivity computations. Furthermore, we estimate the maximum $\ell_1$ sensitivity, up to a $\sqrt{d}$ factor, using $O(d)$ sensitivity computations. We generalize all these results to $\ell_p$ norms for $p > 1$. Lastly, we experimentally show that for a wide class of matrices in real-world datasets, the total sensitivity can be quickly approximated and is significantly smaller than the theoretical prediction, demonstrating that real-world datasets have low intrinsic effective dimensionality.
</details>
<details>
<summary>摘要</summary>
近期关于维度减少的研究在回归任务中引入了敏感度的概念，即数据点的重要性度量，并提供了可证明的保证，即通过抽样除低敏感度数据点后，可以获得高质量的近似。然而，只有在 $\ell_2$ Setting 中有快速算法来approximate $\ell_p$ 敏感度，称为抓取力分数。在这个工作中，我们提供高效的算法来approximate $\ell_p$ 敏感度和相关的概要统计量。特别是，对于给定 $n \times d$ 矩阵，我们可以在 $\alpha$ approximation的成本下计算 $\ell_1$ 敏感度，需要 $O(n/\alpha)$ 敏感度计算。进一步，我们提供一种基于 $\ell_p$ Lewis 权重的重要样本计算方法，可以在 $O(\sqrt{d})$ 敏感度计算成本下计算总 $\ell_p$ 敏感度，并且可以获得 $\sqrt{d}$ 因子的近似。此外，我们可以使用 $O(d)$ 敏感度计算来估计最大 $\ell_1$ 敏感度，带有 $\sqrt{d}$ 因子的误差。我们扩展所有这些结果到 $\ell_p$  нор 中的 $p > 1$。最后，我们通过实验表明，在许多实际数据集中，总敏感度可以快速地被近似，并且与理论预测相比，实际数据集的内在有效维度很低。
</details></li>
</ul>
<hr>
<h2 id="Kernel-mean-and-noise-marginalised-Gaussian-processes-for-exoplanet-transits-and-H-0-inference"><a href="#Kernel-mean-and-noise-marginalised-Gaussian-processes-for-exoplanet-transits-and-H-0-inference" class="headerlink" title="Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet transits and $H_0$ inference"></a>Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet transits and $H_0$ inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04153">http://arxiv.org/abs/2311.04153</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zwei-beiner/transdimensional_sampler">https://github.com/zwei-beiner/transdimensional_sampler</a></li>
<li>paper_authors: Namu Kroupa, David Yallup, Will Handley, Michael Hobson</li>
<li>For: The paper extends Gaussian Process regression to include marginalization over the kernel choice and kernel hyperparameters, enabling Bayesian model comparison and inference of physical hyperparameters.* Methods: The method uses a transdimensional sampler to simultaneously sample over the discrete kernel choice and their hyperparameters, and nested sampling to sample from the joint posterior.* Results: The method was applied to synthetic data from exoplanet transit light curve simulations and real measurements of the Hubble parameter as a function of redshift, and inferred $H_0$ values were obtained. The kernel posterior of the cosmic chronometers dataset prefers a non-stationary linear kernel, and the datasets are shown to be not in tension with ln(R)&#x3D;12.17$\pm$0.02.Here’s the Chinese translation of the three points:* For: 这 paper 扩展 Gaussian Process regression 以包括 kernel 选择和 kernel 超参数的 marginalization，以实现 Bayesian 模型比较和物理超参数的推断。* Methods: 方法使用 transdimensional sampler 同时样本 discrete kernel 选择和其超参数，并使用嵌入式 sampling 从联合 posterior 中取样。* Results: 方法应用于 synthetic 数据 from exoplanet transit light curve simulations 和实际 redshift 为函数的 Hubble parameter  mesurements，并获得了 $H_0$ 值。 cosmic chronometers 数据集的 kernel  posterior 偏好 non-stationary linear kernel，而 datasets 不与 ln(R) &#x3D; 12.17$\pm$0.02 在 contradiction。<details>
<summary>Abstract</summary>
Using a fully Bayesian approach, Gaussian Process regression is extended to include marginalisation over the kernel choice and kernel hyperparameters. In addition, Bayesian model comparison via the evidence enables direct kernel comparison. The calculation of the joint posterior was implemented with a transdimensional sampler which simultaneously samples over the discrete kernel choice and their hyperparameters by embedding these in a higher-dimensional space, from which samples are taken using nested sampling. This method was explored on synthetic data from exoplanet transit light curve simulations. The true kernel was recovered in the low noise region while no kernel was preferred for larger noise. Furthermore, inference of the physical exoplanet hyperparameters was conducted. In the high noise region, either the bias in the posteriors was removed, the posteriors were broadened or the accuracy of the inference was increased. In addition, the uncertainty in mean function predictive distribution increased due to the uncertainty in the kernel choice. Subsequently, the method was extended to marginalisation over mean functions and noise models and applied to the inference of the present-day Hubble parameter, $H_0$, from real measurements of the Hubble parameter as a function of redshift, derived from the cosmologically model-independent cosmic chronometer and {\Lambda}CDM-dependent baryon acoustic oscillation observations. The inferred $H_0$ values from the cosmic chronometers, baryon acoustic oscillations and combined datasets are $H_0$ = 66$\pm$6 km/s/Mpc, $H_0$ = 67$\pm$10 km/s/Mpc and $H_0$ = 69$\pm$6 km/s/Mpc, respectively. The kernel posterior of the cosmic chronometers dataset prefers a non-stationary linear kernel. Finally, the datasets are shown to be not in tension with ln(R)=12.17$\pm$0.02.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HyperS2V-A-Framework-for-Structural-Representation-of-Nodes-in-Hyper-Networks"><a href="#HyperS2V-A-Framework-for-Structural-Representation-of-Nodes-in-Hyper-Networks" class="headerlink" title="HyperS2V: A Framework for Structural Representation of Nodes in Hyper Networks"></a>HyperS2V: A Framework for Structural Representation of Nodes in Hyper Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04149">http://arxiv.org/abs/2311.04149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liushu2019/hypers2v">https://github.com/liushu2019/hypers2v</a></li>
<li>paper_authors: Shu Liu, Cameron Lai, Fujio Toriumi</li>
<li>for: 本研究旨在提出一种基于结构相似性的节点嵌入方法（HyperS2V），以便应用vector数据处理技术于网络数据。</li>
<li>methods: 本方法首先定义了hyper网络中节点的超度概念，然后提出了一种新的结构相似性度量函数，最后使用多尺度随机步骤框架生成结构嵌入。</li>
<li>results: 对于各种简单网络和真实网络，HyperS2V表现出了较高的解释能力和应用可能性，并且在下游任务中表现出了优于其他方法的性能。<details>
<summary>Abstract</summary>
In contrast to regular (simple) networks, hyper networks possess the ability to depict more complex relationships among nodes and store extensive information. Such networks are commonly found in real-world applications, such as in social interactions. Learning embedded representations for nodes involves a process that translates network structures into more simplified spaces, thereby enabling the application of machine learning approaches designed for vector data to be extended to network data. Nevertheless, there remains a need to delve into methods for learning embedded representations that prioritize structural aspects. This research introduces HyperS2V, a node embedding approach that centers on the structural similarity within hyper networks. Initially, we establish the concept of hyper-degrees to capture the structural properties of nodes within hyper networks. Subsequently, a novel function is formulated to measure the structural similarity between different hyper-degree values. Lastly, we generate structural embeddings utilizing a multi-scale random walk framework. Moreover, a series of experiments, both intrinsic and extrinsic, are performed on both toy and real networks. The results underscore the superior performance of HyperS2V in terms of both interpretability and applicability to downstream tasks.
</details>
<details>
<summary>摘要</summary>
对比于常见（简单）网络，卷积网络具有更复杂的节点关系和大量信息存储能力。这些网络在实际应用中很常见，如社交互动。学习节点嵌入表示需要将网络结构翻译成简化的空间，以便应用于向量数据上的机器学习方法进行扩展。然而，仍然需要研究优化节点嵌入的方法，以便更好地保持结构特征。本研究提出了 HyperS2V 节点嵌入方法，强调了卷积网络中节点的结构相似性。首先，我们定义了卷积网络中节点的超度，以捕捉卷积网络中节点的结构特征。然后，我们定义了一种新的函数，用于度量不同超度值之间的结构相似性。最后，我们使用多尺度随机漫步框架生成结构嵌入。此外，我们在各种内在和外在实验中，对真实和模拟网络进行了评估。结果表明，HyperS2V 在解释性和下游任务应用性方面具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Multi-resolution-Time-Series-Transformer-for-Long-term-Forecasting"><a href="#Multi-resolution-Time-Series-Transformer-for-Long-term-Forecasting" class="headerlink" title="Multi-resolution Time-Series Transformer for Long-term Forecasting"></a>Multi-resolution Time-Series Transformer for Long-term Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04147">http://arxiv.org/abs/2311.04147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue Zhang, Mark Coates</li>
<li>for: 这个研究是为了提高时间序列预测的性能，特别是透过将时间序列分割为固定大小的块来帮助trasnformer学习时间序列中的复杂模式。</li>
<li>methods: 这个研究使用了一种名为Multi-resolution Time-Series Transformer（MTST）的新框架，这个框架包含多条分支架构，用于同时模型不同的时间模式。它还使用了相对位置编码，这样可以更好地提取不同 scales的周期性。</li>
<li>results: 实验结果显示，MTST在实际世界的数据集上比顶对方法更有效，尤其是在预测长期季节性和趋势方面。<details>
<summary>Abstract</summary>
The performance of transformers for time-series forecasting has improved significantly. Recent architectures learn complex temporal patterns by segmenting a time-series into patches and using the patches as tokens. The patch size controls the ability of transformers to learn the temporal patterns at different frequencies: shorter patches are effective for learning localized, high-frequency patterns, whereas mining long-term seasonalities and trends requires longer patches. Inspired by this observation, we propose a novel framework, Multi-resolution Time-Series Transformer (MTST), which consists of a multi-branch architecture for simultaneous modeling of diverse temporal patterns at different resolutions. In contrast to many existing time-series transformers, we employ relative positional encoding, which is better suited for extracting periodic components at different scales. Extensive experiments on several real-world datasets demonstrate the effectiveness of MTST in comparison to state-of-the-art forecasting techniques.
</details>
<details>
<summary>摘要</summary>
“transformer的时间序列预测性能有所改善，现代架构通过将时间序列分成块和使用这些块作为token来学习复杂的时间模式。块的大小控制了transformer的能力学习不同频率的时间模式：较短的块更有效地学习本地化高频率的模式，而长的块则更适合挖掘长期季节性和趋势。受到这个观察的启发，我们提出了一个新的框架──多resolution时间序列transformer（MTST），这个架构包括多条分支架构，用于同时模型不同分辨率的时间模式。不同于许多现有的时间序列transformer，我们使用相对位置编码，这种编码更适合提取不同比例的周期性。实际实验发现，MTST在多个真实世界数据集上与当前的预测技术相比，表现更加出色。”
</details></li>
</ul>
<hr>
<h2 id="Generative-learning-for-nonlinear-dynamics"><a href="#Generative-learning-for-nonlinear-dynamics" class="headerlink" title="Generative learning for nonlinear dynamics"></a>Generative learning for nonlinear dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04128">http://arxiv.org/abs/2311.04128</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Gilpin</li>
<li>for: 该论文探讨了如何使用非线性动力学理论来解释大规模生成统计学学习模型的能力创造真实的输出。</li>
<li>methods: 该论文使用了信息理论工具来推断 chaotic attractor 的性质，并提出了一些算法来 parametrize chaos in real datasets。</li>
<li>results: 该论文提出了一些新的思路，如使用 classical attractor reconstruction 和 symbolic approximations 来理解大规模生成统计学学习模型的行为，以及将 nonlinear dynamics 和学习理论相互关联。<details>
<summary>Abstract</summary>
Modern generative machine learning models demonstrate surprising ability to create realistic outputs far beyond their training data, such as photorealistic artwork, accurate protein structures, or conversational text. These successes suggest that generative models learn to effectively parametrize and sample arbitrarily complex distributions. Beginning half a century ago, foundational works in nonlinear dynamics used tools from information theory to infer properties of chaotic attractors from time series, motivating the development of algorithms for parametrizing chaos in real datasets. In this perspective, we aim to connect these classical works to emerging themes in large-scale generative statistical learning. We first consider classical attractor reconstruction, which mirrors constraints on latent representations learned by state space models of time series. We next revisit early efforts to use symbolic approximations to compare minimal discrete generators underlying complex processes, a problem relevant to modern efforts to distill and interpret black-box statistical models. Emerging interdisciplinary works bridge nonlinear dynamics and learning theory, such as operator-theoretic methods for complex fluid flows, or detection of broken detailed balance in biological datasets. We anticipate that future machine learning techniques may revisit other classical concepts from nonlinear dynamics, such as transinformation decay and complexity-entropy tradeoffs.
</details>
<details>
<summary>摘要</summary>
Half a century ago, foundational works in nonlinear dynamics used tools from information theory to infer properties of chaotic attractors from time series, which motivated the development of algorithms for parametrizing chaos in real datasets. In this perspective, we aim to connect these classical works to emerging themes in large-scale generative statistical learning.We first consider classical attractor reconstruction, which reflects the constraints on latent representations learned by state space models of time series. We then revisit early efforts to use symbolic approximations to compare minimal discrete generators underlying complex processes, a problem relevant to modern efforts to distill and interpret black-box statistical models.Emerging interdisciplinary works bridge nonlinear dynamics and learning theory, such as operator-theoretic methods for complex fluid flows, or detection of broken detailed balance in biological datasets. We anticipate that future machine learning techniques may revisit other classical concepts from nonlinear dynamics, such as transinformation decay and complexity-entropy tradeoffs.
</details></li>
</ul>
<hr>
<h2 id="Do-Language-Models-Learn-Semantics-of-Code-A-Case-Study-in-Vulnerability-Detection"><a href="#Do-Language-Models-Learn-Semantics-of-Code-A-Case-Study-in-Vulnerability-Detection" class="headerlink" title="Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection"></a>Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04109">http://arxiv.org/abs/2311.04109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Steenhoek, Md Mahbubur Rahman, Shaila Sharmin, Wei Le<br>for: This paper aims to analyze the alignment of pre-trained language models with bug semantics in vulnerability detection, and to develop annotation methods to improve the models’ performance.methods: The paper uses three distinct methods for analysis: interpretability tools, attention analysis, and interaction matrix analysis.results: The paper finds that better-performing models align better with potentially vulnerable statements (PVS), but the models fail to align strongly to buggy paths. The paper also develops two annotation methods that improve the models’ performance in the majority of settings, with up to 9.57 points improvement in F1 score compared to conventional fine-tuning. Additionally, the models aligned up to 232% better to PVS with the annotations.<details>
<summary>Abstract</summary>
Recently, pretrained language models have shown state-of-the-art performance on the vulnerability detection task. These models are pretrained on a large corpus of source code, then fine-tuned on a smaller supervised vulnerability dataset. Due to the different training objectives and the performance of the models, it is interesting to consider whether the models have learned the semantics of code relevant to vulnerability detection, namely bug semantics, and if so, how the alignment to bug semantics relates to model performance. In this paper, we analyze the models using three distinct methods: interpretability tools, attention analysis, and interaction matrix analysis. We compare the models' influential feature sets with the bug semantic features which define the causes of bugs, including buggy paths and Potentially Vulnerable Statements (PVS). We find that (1) better-performing models also aligned better with PVS, (2) the models failed to align strongly to PVS, and (3) the models failed to align at all to buggy paths. Based on our analysis, we developed two annotation methods which highlight the bug semantics inside the model's inputs. We evaluated our approach on four distinct transformer models and four vulnerability datasets and found that our annotations improved the models' performance in the majority of settings - 11 out of 16, with up to 9.57 points improvement in F1 score compared to conventional fine-tuning. We further found that with our annotations, the models aligned up to 232% better to potentially vulnerable statements. Our findings indicate that it is helpful to provide the model with information of the bug semantics, that the model can attend to it, and motivate future work in learning more complex path-based bug semantics. Our code and data are available at https://figshare.com/s/4a16a528d6874aad51a0.
</details>
<details>
<summary>摘要</summary>
近些时间，预训练语言模型在漏洞检测任务上表现出状元水平。这些模型首先在大量源代码上预训练，然后在更小的指导漏洞数据集上细化调教。由于模型的不同训练目标和性能，我们想要考虑这些模型是否学习了代码漏洞检测相关的 semantics，即漏洞 semantics，以及这种对应关系如何影响模型性能。在这篇论文中，我们使用三种不同的方法来分析模型：可读性工具、注意力分析和交互矩阵分析。我们比较模型的影响特征集与漏洞 semantics 定义的原因，包括漏洞路径和潜在漏洞语句（PVS）。我们发现：1. 性能更好的模型也更好地与 PVS 对齐。2. 模型无法强调 PVS 的对齐。3. 模型无法强调漏洞路径的对齐。根据我们的分析，我们开发了两种标注方法，用于在模型输入中标注内部的漏洞 semantics。我们在四种 transformer 模型和四个漏洞数据集上评估了我们的方法，发现我们的标注提高了模型在大多数情况下的性能，增加了11 out of 16 的情况中的 F1 分数，达到最高9.57个点的提升。我们还发现，通过我们的标注，模型可以更好地遵循潜在漏洞语句，最高达232%的提升。我们的发现表明，提供模型bug semantics信息是有帮助的，模型可以注意到它，并促进未来更多关于路径基本漏洞 semantics 的学习。我们的代码和数据可以在 figshare 上找到：https://figshare.com/s/4a16a528d6874aad51a0。
</details></li>
</ul>
<hr>
<h2 id="Time-Efficient-Reinforcement-Learning-with-Stochastic-Stateful-Policies"><a href="#Time-Efficient-Reinforcement-Learning-with-Stochastic-Stateful-Policies" class="headerlink" title="Time-Efficient Reinforcement Learning with Stochastic Stateful Policies"></a>Time-Efficient Reinforcement Learning with Stochastic Stateful Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04082">http://arxiv.org/abs/2311.04082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Firas Al-Hafez, Guoping Zhao, Jan Peters, Davide Tateo</li>
<li>for: 本文旨在提出一种新的方法来训练状态决策函数，以解决传统的Backpropagation Through Time（BPTT）方法的缺点，如训练速度慢和梯度衰减或扩散。</li>
<li>methods: 本文提出了一种将状态决策函数分解成随机内部状态核心和无状态策略的方法，并将其共同优化，以实现状态策略梯度的优化。同时，本文还提出了不同版本的状态策略梯度定理，使得可以轻松实现状态策略的变种。</li>
<li>results: 本文通过在复杂的连续控制任务中进行测试，如人型行走，并证明了新的梯度估计器可以与BPTT相比，在任务复杂性增加时能够有效扩展，并且比BPTT更快和简单。<details>
<summary>Abstract</summary>
Stateful policies play an important role in reinforcement learning, such as handling partially observable environments, enhancing robustness, or imposing an inductive bias directly into the policy structure. The conventional method for training stateful policies is Backpropagation Through Time (BPTT), which comes with significant drawbacks, such as slow training due to sequential gradient propagation and the occurrence of vanishing or exploding gradients. The gradient is often truncated to address these issues, resulting in a biased policy update. We present a novel approach for training stateful policies by decomposing the latter into a stochastic internal state kernel and a stateless policy, jointly optimized by following the stateful policy gradient. We introduce different versions of the stateful policy gradient theorem, enabling us to easily instantiate stateful variants of popular reinforcement learning and imitation learning algorithms. Furthermore, we provide a theoretical analysis of our new gradient estimator and compare it with BPTT. We evaluate our approach on complex continuous control tasks, e.g., humanoid locomotion, and demonstrate that our gradient estimator scales effectively with task complexity while offering a faster and simpler alternative to BPTT.
</details>
<details>
<summary>摘要</summary>
We propose a novel approach for training stateful policies by decomposing the policy into a stochastic internal state kernel and a stateless policy, jointly optimized by following the stateful policy gradient. We introduce different versions of the stateful policy gradient theorem, allowing us to easily instantiate stateful variants of popular reinforcement learning and imitation learning algorithms.Furthermore, we provide a theoretical analysis of our new gradient estimator and compare it with BPTT. We evaluate our approach on complex continuous control tasks, such as humanoid locomotion, and demonstrate that our gradient estimator scales effectively with task complexity while offering a faster and simpler alternative to BPTT.Here is the Simplified Chinese translation of the text:州态策略在强化学习中扮演着重要的角色，例如处理偏见环境、增强稳定性或直接在策略结构中强制一致性。传统的强化学习训练方法是Backpropagation Through Time（BPTT），但它带来了许多问题，如时间顺序梯度传播导致训练慢，以及潜在的潜在或扩散梯度问题。为解决这些问题，我们常常将梯度截断，从而导致策略更新受到偏见。我们提出了一种新的方法，即将策略分解成随机内部状态核心和无状态策略，共同由状态策略梯度优化。我们还提出了不同的州态策略梯度定理版本，使得可以轻松实现州态变体的各种强化学习和仿真学习算法。此外，我们还提供了对我们新的梯度估计器的理论分析，并与BPTT进行比较。我们在复杂的连续控制任务上，如人型步态机器人行走，进行了评估，并证明了我们的梯度估计器与任务复杂性成正比，而且对BPTT更加快速和简单。
</details></li>
</ul>
<hr>
<h2 id="Estimator-Coupled-Reinforcement-Learning-for-Robust-Purely-Tactile-In-Hand-Manipulation"><a href="#Estimator-Coupled-Reinforcement-Learning-for-Robust-Purely-Tactile-In-Hand-Manipulation" class="headerlink" title="Estimator-Coupled Reinforcement Learning for Robust Purely Tactile In-Hand Manipulation"></a>Estimator-Coupled Reinforcement Learning for Robust Purely Tactile In-Hand Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04060">http://arxiv.org/abs/2311.04060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Röstel, Johannes Pitz, Leon Sievers, Berthold Bäuml</li>
<li>for: 这篇论文 targets the problem of combining reinforcement learning-based controllers and state estimators for robotic in-hand manipulation, specifically focusing on the challenging task of purely tactile, goal-conditioned, dextrous in-hand reorientation with the hand pointing downwards.</li>
<li>methods: The authors solve the problem by coupling the control policy to the state estimator during training in simulation, leading to more robust state estimation and overall higher performance on the task while maintaining an interpretability advantage over end-to-end policy learning.</li>
<li>results: The authors demonstrate successful sim2real transfer and achieve high performance on the task of reorienting a cube to nine goals, which was beyond the reach of previous methods in this challenging setting. With their GPU-accelerated implementation, learning from scratch takes a median training time of only 6.5 hours on a single, low-cost GPU.<details>
<summary>Abstract</summary>
This paper identifies and addresses the problems with naively combining (reinforcement) learning-based controllers and state estimators for robotic in-hand manipulation. Specifically, we tackle the challenging task of purely tactile, goal-conditioned, dextrous in-hand reorientation with the hand pointing downwards. Due to the limited sensing available, many control strategies that are feasible in simulation when having full knowledge of the object's state do not allow for accurate state estimation. Hence, separately training the controller and the estimator and combining the two at test time leads to poor performance. We solve this problem by coupling the control policy to the state estimator already during training in simulation. This approach leads to more robust state estimation and overall higher performance on the task while maintaining an interpretability advantage over end-to-end policy learning. With our GPU-accelerated implementation, learning from scratch takes a median training time of only 6.5 hours on a single, low-cost GPU. In simulation experiments with the DLR-Hand II and for four significantly different object shapes, we provide an in-depth analysis of the performance of our approach. We demonstrate the successful sim2real transfer by rotating the four objects to all 24 orientations in the $\pi/2$ discretization of SO(3), which has never been achieved for such a diverse set of shapes. Finally, our method can reorient a cube consecutively to nine goals (median), which was beyond the reach of previous methods in this challenging setting.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Feature-Space-Renormalization-for-Semi-supervised-Learning"><a href="#Feature-Space-Renormalization-for-Semi-supervised-Learning" class="headerlink" title="Feature Space Renormalization for Semi-supervised Learning"></a>Feature Space Renormalization for Semi-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04055">http://arxiv.org/abs/2311.04055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Sun, Zhongjie Mao, Chao Li, Chao Zhou, Xiao-Jun Wu</li>
<li>for: 这篇论文的目的是提出一种新的半监督学习（SSL）方法，以便利用无标的数据来增强模型对于大量标签数据的依赖。</li>
<li>methods: 这篇论文使用了一种新的特征空间重整化（FSR）机制，将替代常用的一致调整机制，以提高模型对特征的学习。</li>
<li>results: 根据实验结果，这篇论文的方法可以在多个标准的SSL测试数据集上取得更好的性能，并且可以增强其他SSL方法的表现。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has been proven to be a powerful method for leveraging unlabelled data to alleviate models' dependence on large labelled datasets. The common framework among recent approaches is to train the model on a large amount of unlabelled data with consistency regularization to constrain the model predictions to be invariant to input perturbation. However, the existing SSL frameworks still have room for improvement in the consistency regularization method. Instead of regularizing category predictions in the label space as in existing frameworks, this paper proposes a feature space renormalization (FSR) mechanism for SSL. First, we propose a feature space renormalization mechanism to substitute for the commonly used consistency regularization mechanism to learn better discriminative features. To apply this mechanism, we start by building a basic model and an empirical model and then introduce our mechanism to renormalize the feature learning of the basic model with the guidance of the empirical model. Second, we combine the proposed mechanism with pseudo-labelling to obtain a novel effective SSL model named FreMatch. The experimental results show that our method can achieve better performance on a variety of standard SSL benchmark datasets, and the proposed feature space renormalization mechanism can also enhance the performance of other SSL approaches.
</details>
<details>
<summary>摘要</summary>
我们的方法包括两个步骤：1. 我们提出了一种特征空间重normalization机制，用于取代常用的一致性正则化机制，以学习更好的抽象特征。我们开始于建立基本模型和empirical模型，然后引入我们的机制，使基本模型在empirical模型的指导下进行特征学习重normalization。2. 我们将我们的机制与pseudo-labeling相结合，以获得一种新的有效SSL模型，称之为FreMatch。实验结果显示，我们的方法可以在多种标准SSL benchmark数据集上达到更好的性能，并且我们提出的特征空间重normalization机制也可以增强其他SSL方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Extracting-human-interpretable-structure-property-relationships-in-chemistry-using-XAI-and-large-language-models"><a href="#Extracting-human-interpretable-structure-property-relationships-in-chemistry-using-XAI-and-large-language-models" class="headerlink" title="Extracting human interpretable structure-property relationships in chemistry using XAI and large language models"></a>Extracting human interpretable structure-property relationships in chemistry using XAI and large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04047">http://arxiv.org/abs/2311.04047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geemi725/xpertai">https://github.com/geemi725/xpertai</a></li>
<li>paper_authors: Geemi P. Wellawatte, Philippe Schwaller</li>
<li>For: The paper aims to address the opaque nature of machine learning models in the field of Explainable Artificial Intelligence (XAI) and their applications in chemistry to understand structure-property relationships.* Methods: The paper proposes the XpertAI framework, which integrates XAI methods with large language models (LLMs) to generate accessible natural language explanations of raw chemical data automatically.* Results: The paper evaluates the performance of XpertAI through 5 case studies and shows that it combines the strengths of LLMs and XAI tools in generating specific, scientific, and interpretable explanations.Here’s the simplified Chinese version of the three key points:* For: 该论文旨在解决人工智能模型的各种不透明性问题，并在化学领域应用其结构性关系理解。* Methods: 论文提出了XpertAI框架，该框架将XAI方法与大型自然语言模型（LLMs）结合起来，自动生成化学数据的可读性语言解释。* Results: 论文通过5个案例研究证明，XpertAI结合了LLMs和XAI工具的优势，能够生成特定、科学、可解释的语言解释。<details>
<summary>Abstract</summary>
Explainable Artificial Intelligence (XAI) is an emerging field in AI that aims to address the opaque nature of machine learning models. Furthermore, it has been shown that XAI can be used to extract input-output relationships, making them a useful tool in chemistry to understand structure-property relationships. However, one of the main limitations of XAI methods is that they are developed for technically oriented users. We propose the XpertAI framework that integrates XAI methods with large language models (LLMs) accessing scientific literature to generate accessible natural language explanations of raw chemical data automatically. We conducted 5 case studies to evaluate the performance of XpertAI. Our results show that XpertAI combines the strengths of LLMs and XAI tools in generating specific, scientific, and interpretable explanations.
</details>
<details>
<summary>摘要</summary>
互助式人工智能（XAI）是一个emerging field在人工智能领域，旨在解决机器学习模型的透明性问题。此外，研究表明，XAI可以用来抽取输入-输出关系，使其成为化学领域理解结构-性质关系的有用工具。然而，XAI方法的一个主要局限性是它们是为技术 oriented 用户开发的。我们提出了 XpertAI 框架，该框架将 XAI 方法与大型自然语言模型（LLMs）相结合，自动生成可读的化学数据自然语言解释。我们进行了5个案例研究，以评估 XpertAI 的表现。我们的结果显示，XpertAI 组合了 LLMs 和 XAI 工具的优点，能够生成具体、科学和可解释的解释。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Discordance-Minimization-based-Imputation-Algorithms-for-Missing-Values-in-Rating-Data"><a href="#Discordance-Minimization-based-Imputation-Algorithms-for-Missing-Values-in-Rating-Data" class="headerlink" title="Discordance Minimization-based Imputation Algorithms for Missing Values in Rating Data"></a>Discordance Minimization-based Imputation Algorithms for Missing Values in Rating Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04035">http://arxiv.org/abs/2311.04035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young Woong Park, Jinhak Kim, Dan Zhu</li>
<li>for: 这个研究是为了解决多个评分列表合并后存在缺失评分的问题。</li>
<li>methods: 这个研究使用了六个真实世界的数据集，通过分析缺失值的特征和特性，提出了优化模型和算法，用于使用known评分信息来填充缺失评分。</li>
<li>results: 计算实验表明，提出的方法在实际和 sintetic 评分数据集上比现有的总体替换方法更高的填充精度。<details>
<summary>Abstract</summary>
Ratings are frequently used to evaluate and compare subjects in various applications, from education to healthcare, because ratings provide succinct yet credible measures for comparing subjects. However, when multiple rating lists are combined or considered together, subjects often have missing ratings, because most rating lists do not rate every subject in the combined list. In this study, we propose analyses on missing value patterns using six real-world data sets in various applications, as well as the conditions for applicability of imputation algorithms. Based on the special structures and properties derived from the analyses, we propose optimization models and algorithms that minimize the total rating discordance across rating providers to impute missing ratings in the combined rating lists, using only the known rating information. The total rating discordance is defined as the sum of the pairwise discordance metric, which can be written as a quadratic function. Computational experiments based on real-world and synthetic rating data sets show that the proposed methods outperform the state-of-the-art general imputation methods in the literature in terms of imputation accuracy.
</details>
<details>
<summary>摘要</summary>
评分 frequently 用于在不同应用中评估和比较对象，从教育到医疗，因为评分提供简洁 yet credible 的比较度量。然而，当多个评分列表合并或考虑在一起时，对象经常有缺失评分，因为大多数评分列表不会对所有对象进行评分。在这种研究中，我们对缺失值模式进行分析，使用 six 个真实世界数据集，并对适用于替换算法的条件进行分析。基于分析结果，我们提出优化模型和算法，以最小化总评分差异 across 评分提供者，使用只知道的评分信息进行替换缺失评分。总评分差异可以写作quadratic function。通过实验表明，提出的方法在实际世界和 sintetic 评分数据集上比 estado-of-the-art 通用替换方法在替换精度方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Joint-model-for-longitudinal-and-spatio-temporal-survival-data"><a href="#Joint-model-for-longitudinal-and-spatio-temporal-survival-data" class="headerlink" title="Joint model for longitudinal and spatio-temporal survival data"></a>Joint model for longitudinal and spatio-temporal survival data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04008">http://arxiv.org/abs/2311.04008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Medina-Olivares, Finn Lindgren, Raffaella Calabrese, Jonathan Crook</li>
<li>for: 预测借款人的时间到事件（如偿还），使用存生模型，特别是当时间变量covariates是内生的时候。</li>
<li>methods: 我们提出了空间-时间联合模型（STJM），该模型能够捕捉空间和时间效应以及其交互作用，并使用bayesian hierarchical模型来reckon借款人之间的不见天性差异。</li>
<li>results: 我们对大规模数据集（57,258名美国房贷借款人，超过250万个观察）应用STJM模型，结果表明包含空间效应可以顺利提高存生模型的性能，但是当再次包含空间-时间交互效应时，效果较弱。<details>
<summary>Abstract</summary>
In credit risk analysis, survival models with fixed and time-varying covariates are widely used to predict a borrower's time-to-event. When the time-varying drivers are endogenous, modelling jointly the evolution of the survival time and the endogenous covariates is the most appropriate approach, also known as the joint model for longitudinal and survival data. In addition to the temporal component, credit risk models can be enhanced when including borrowers' geographical information by considering spatial clustering and its variation over time. We propose the Spatio-Temporal Joint Model (STJM) to capture spatial and temporal effects and their interaction. This Bayesian hierarchical joint model reckons the survival effect of unobserved heterogeneity among borrowers located in the same region at a particular time. To estimate the STJM model for large datasets, we consider the Integrated Nested Laplace Approximation (INLA) methodology. We apply the STJM to predict the time to full prepayment on a large dataset of 57,258 US mortgage borrowers with more than 2.5 million observations. Empirical results indicate that including spatial effects consistently improves the performance of the joint model. However, the gains are less definitive when we additionally include spatio-temporal interactions.
</details>
<details>
<summary>摘要</summary>
在信用风险分析中，固定和时间变化的 covariates 广泛使用来预测借款人的时间事件。当时间变化驱动器是内生的时，模型同时考虑 survival 时间和内生 covariates 的演化是最合适的方法，也称为长itudinal 和存活数据共同模型。此外，信用风险模型可以通过考虑借款人的地理信息来增强，包括空间归一化和时间变化的相互作用。我们提议使用 Spatio-Temporal Joint Model (STJM) 来捕捉空间和时间效应以及其相互作用。这是一种 Bayesian 层次模型，reckons 借款人在同一地区的特定时间点上存在不见的潜在差异的存活效应。为处理大规模数据，我们考虑使用 Integrated Nested Laplace Approximation (INLA) 方法。我们应用 STJM 模型来预测57,258名美国mortgage 借款人的250万次观察数据中的时间到完全偿还。实际结果表明，包括空间效应可以持续改善共同模型的性能，但是添加空间-时间交互的效果较为不确定。
</details></li>
</ul>
<hr>
<h2 id="An-Initialization-Schema-for-Neuronal-Networks-on-Tabular-Data"><a href="#An-Initialization-Schema-for-Neuronal-Networks-on-Tabular-Data" class="headerlink" title="An Initialization Schema for Neuronal Networks on Tabular Data"></a>An Initialization Schema for Neuronal Networks on Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03996">http://arxiv.org/abs/2311.03996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wolfgang Fuhl</li>
<li>for: 本研究旨在使用神经网络进行表格数据预测，并提出了一种简单 yet effective的初始化方法。</li>
<li>methods: 本研究使用了一种binomial initialized neural network，并将Gradient Masking添加到批处理中。</li>
<li>results: 对多个公共数据集进行评估，显示了与其他神经网络方法相比的改进表现。In English:</li>
<li>for: The purpose of this study is to use neural networks for tabular data prediction, and propose a simple yet effective initialization method.</li>
<li>methods: The study uses a binomial initialized neural network and adds Gradient Masking to the batch processing.</li>
<li>results: The approach is evaluated on multiple public datasets and shows improved performance compared to other neural network-based methods.<details>
<summary>Abstract</summary>
Nowadays, many modern applications require heterogeneous tabular data, which is still a challenging task in terms of regression and classification. Many approaches have been proposed to adapt neural networks for this task, but still, boosting and bagging of decision trees are the best-performing methods for this task. In this paper, we show that a binomial initialized neural network can be used effectively on tabular data. The proposed approach shows a simple but effective approach for initializing the first hidden layer in neural networks. We also show that this initializing schema can be used to jointly train ensembles by adding gradient masking to batch entries and using the binomial initialization for the last layer in a neural network. For this purpose, we modified the hinge binary loss and the soft max loss to make them applicable for joint ensemble training. We evaluate our approach on multiple public datasets and showcase the improved performance compared to other neural network-based approaches. In addition, we discuss the limitations and possible further research of our approach for improving the applicability of neural networks to tabular data.   Link: https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FInitializationNeuronalNetworksTabularData&mode=list
</details>
<details>
<summary>摘要</summary>
现在，许多现代应用需要多元表格数据，这还是一个困难的任务，尤其是在回归和分类方面。许多方法已经被提出来适应神经网络，但是弹性和袋式的决策树仍然是最佳性能的方法。在这篇论文中，我们表明了一种使用二进制初始化神经网络的方法，可以有效地应用于表格数据。我们还提出了一种将批处理中的梯度掩码以及神经网络的最后一层的二进制初始化结合使用，以实现联合训练。为此，我们修改了梯度函数和软max损失函数，使其适用于联合集成训练。我们在多个公共数据集上评估了我们的方法，并证明了与其他神经网络基于方法相比，我们的方法具有更高的性能。此外，我们还讨论了我们的方法的局限性和可能的进一步研究，以提高神经网络对表格数据的应用。Link: <https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FInitializationNeuronalNetworksTabularData&mode=list>
</details></li>
</ul>
<hr>
<h2 id="Bandit-Pareto-Set-Identification-the-Fixed-Budget-Setting"><a href="#Bandit-Pareto-Set-Identification-the-Fixed-Budget-Setting" class="headerlink" title="Bandit Pareto Set Identification: the Fixed Budget Setting"></a>Bandit Pareto Set Identification: the Fixed Budget Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03992">http://arxiv.org/abs/2311.03992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cyrille Kone, Emilie Kaufmann, Laura Richert</li>
<li>for: 本研究旨在解决一个多目标纯exploration问题，该问题在多臂抽象模型中出现。每抽象臂都关联着一个未知多变量分布，目标是确定这些分布的mean不 worse than另一个分布：Pareto优化集。</li>
<li>methods: 我们提出并分析了首个针对fixed budget Pareto Set Identification任务的算法。我们提出了Empirical Gap Elimination家族算法，这家族算法结合了精心估计每抽象臂是否处于Pareto集的“难度分类”，以及一种通用的排除方案。我们证明了两个特定实现，EGE-SR和EGE-SH，其错误概率与预算相关，具有 exponential decay 的速率，其下界支持信息理论下界。</li>
<li>results: 我们通过实验研究使用实际世界和 sintetic 数据集，发现我们的算法具有良好的性能。<details>
<summary>Abstract</summary>
We study a multi-objective pure exploration problem in a multi-armed bandit model. Each arm is associated to an unknown multi-variate distribution and the goal is to identify the distributions whose mean is not uniformly worse than that of another distribution: the Pareto optimal set. We propose and analyze the first algorithms for the \emph{fixed budget} Pareto Set Identification task. We propose Empirical Gap Elimination, a family of algorithms combining a careful estimation of the ``hardness to classify'' each arm in or out of the Pareto set with a generic elimination scheme. We prove that two particular instances, EGE-SR and EGE-SH, have a probability of error that decays exponentially fast with the budget, with an exponent supported by an information theoretic lower-bound. We complement these findings with an empirical study using real-world and synthetic datasets, which showcase the good performance of our algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究一个多目标纯探索问题在多重投机模型中。每个武器都相关于一个未知多变量分布，目标是确定这些分布的含义，其中的平均值不比另一个分布更差。我们提出并分析了首个预算纯度集合识别任务的算法。我们提出了实验减少差异（EGE）家族算法，这些算法结合精心估计每个武器是否在纯度集合中的困难程度，以及一种通用的减少方案。我们证明了两个特定实例（EGE-SR和EGE-SH）的抽象错误概率在预算下随时间减少 exponentially，并且支持信息理论下界。我们通过实验使用真实世界和 sintetic 数据集来证明我们的算法的好效果。
</details></li>
</ul>
<hr>
<h2 id="Cup-Curriculum-Curriculum-Learning-on-Model-Capacity"><a href="#Cup-Curriculum-Curriculum-Learning-on-Model-Capacity" class="headerlink" title="Cup Curriculum: Curriculum Learning on Model Capacity"></a>Cup Curriculum: Curriculum Learning on Model Capacity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03956">http://arxiv.org/abs/2311.03956</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luca-scharr/cupcurriculum">https://github.com/luca-scharr/cupcurriculum</a></li>
<li>paper_authors: Luca Scharr, Vanessa Toborek</li>
<li>for: 提高自然语言处理任务的学习效果</li>
<li>methods: 应用特殊学习策略和迭代减少模型容量</li>
<li>results: 可靠地超越早期停止，高度抗沉淀<details>
<summary>Abstract</summary>
Curriculum learning (CL) aims to increase the performance of a learner on a given task by applying a specialized learning strategy. This strategy focuses on either the dataset, the task, or the model. There is little to no work analysing the possibilities to apply CL on the model capacity in natural language processing. To close this gap, we propose the cup curriculum. In a first phase of training we use a variation of iterative magnitude pruning to reduce model capacity. These weights are reintroduced in a second phase, resulting in the model capacity to show a cup-shaped curve over the training iterations. We empirically evaluate different strategies of the cup curriculum and show that it outperforms early stopping reliably while exhibiting a high resilience to overfitting.
</details>
<details>
<summary>摘要</summary>
学习资源（CL）目的是提高学习者对特定任务的性能，通过特殊的学习策略。这种策略可以对 dataset、任务或模型进行特化。在自然语言处理领域，有很少关于模型容量的可能性分析。为了填补这个差距，我们提出了杯资源。在第一个训练阶段，我们使用迭代幅度剪枝来降低模型容量。这些权重在第二个训练阶段重新引入，导致模型容量展示一个杯形曲线。我们对不同的杯资源策略进行实验性评估，并证明它可靠地超越早停止，同时具有高鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Blind-Federated-Learning-via-Over-the-Air-q-QAM"><a href="#Blind-Federated-Learning-via-Over-the-Air-q-QAM" class="headerlink" title="Blind Federated Learning via Over-the-Air q-QAM"></a>Blind Federated Learning via Over-the-Air q-QAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04253">http://arxiv.org/abs/2311.04253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Razavikia, José Mairton Barros Da Silva Júnior, Carlo Fischione</li>
<li>for: 这 paper 研究了 federated edge learning over a fading multiple access channel，以减少边缘设备与访问点之间的通信负担。</li>
<li>methods: 该 paper 引入了一种先进的数字上下线计算策略，使用 q-ary  quadrature amplitude modulation，实现了低延迟的通信方案。</li>
<li>results: 研究人员提出了一种新的 federated edge learning 框架，在其中边缘设备使用数字模ulation 进行对空传输，而不需要知道通道状态信息。此外，研究人员还在边缘服务器中加入多个天线，以解决无线通信中的抖音问题。研究人员分析了对抖音的影响下，需要多少天线来有效地减少影响。通过分析，研究人员得出了对不同抖音水平的非对数上限下界，并证明了在噪声和抖音条件下，该方法可以减少学习过程中的模型误差。<details>
<summary>Abstract</summary>
In this work, we investigate federated edge learning over a fading multiple access channel. To alleviate the communication burden between the edge devices and the access point, we introduce a pioneering digital over-the-air computation strategy employing q-ary quadrature amplitude modulation, culminating in a low latency communication scheme. Indeed, we propose a new federated edge learning framework in which edge devices use digital modulation for over-the-air uplink transmission to the edge server while they have no access to the channel state information. Furthermore, we incorporate multiple antennas at the edge server to overcome the fading inherent in wireless communication. We analyze the number of antennas required to mitigate the fading impact effectively. We prove a non-asymptotic upper bound for the mean squared error for the proposed federated learning with digital over-the-air uplink transmissions under both noisy and fading conditions. Leveraging the derived upper bound, we characterize the convergence rate of the learning process of a non-convex loss function in terms of the mean square error of gradients due to the fading channel. Furthermore, we substantiate the theoretical assurances through numerical experiments concerning mean square error and the convergence efficacy of the digital federated edge learning framework. Notably, the results demonstrate that augmenting the number of antennas at the edge server and adopting higher-order modulations improve the model accuracy up to 60\%.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了聚合边缘学习在淡含多access通道上。为了减轻边缘设备和访问点之间的通信负担，我们提出了一种创新的数字上下线计算策略，使用q-ary正交振荡频分 Modulation（QAM），从而实现了低延迟的通信方案。在这个框架中，边缘设备使用数字模ulation进行无线上传到边缘服务器，而无需了解通道状态信息。此外，我们在边缘服务器中添加多个天线，以抗衰减无线通信中的噪声。我们分析了需要消除噪声的天线数量，以实现有效的减轻影响。我们证明了在噪声和淡含条件下，对提议的联邦边缘学习方法的非 asymptotic 上限 bound，并且用这个上限 bound来描述联邦学习过程中梯度误差的减少率。此外，我们通过数学实验证明了 theoretical 保证的可行性和数学误差的减少率。结果表明，增加边缘服务器中天线数量和采用更高的模ulation编码可以提高模型精度达60%。
</details></li>
</ul>
<hr>
<h2 id="CNN-Based-Structural-Damage-Detection-using-Time-Series-Sensor-Data"><a href="#CNN-Based-Structural-Damage-Detection-using-Time-Series-Sensor-Data" class="headerlink" title="CNN-Based Structural Damage Detection using Time-Series Sensor Data"></a>CNN-Based Structural Damage Detection using Time-Series Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04252">http://arxiv.org/abs/2311.04252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishan Pathak, Ishan Jha, Aditya Sadana, Basuraj Bhowmik</li>
<li>for: 本研究旨在开发一种新的损害检测方法，以帮助评估结构的状况，检测结构损害。</li>
<li>methods: 该方法使用一种新的卷积神经网络算法，通过检测时间序列数据中的长期连接，提取深度空间特征。</li>
<li>results: 实验结果表明，新的卷积神经网络算法具有高度的损害检测精度，可以准确地检测结构损害。<details>
<summary>Abstract</summary>
Structural Health Monitoring (SHM) is vital for evaluating structural condition, aiming to detect damage through sensor data analysis. It aligns with predictive maintenance in modern industry, minimizing downtime and costs by addressing potential structural issues. Various machine learning techniques have been used to extract valuable information from vibration data, often relying on prior structural knowledge. This research introduces an innovative approach to structural damage detection, utilizing a new Convolutional Neural Network (CNN) algorithm. In order to extract deep spatial features from time series data, CNNs are taught to recognize long-term temporal connections. This methodology combines spatial and temporal features, enhancing discrimination capabilities when compared to methods solely reliant on deep spatial features. Time series data are divided into two categories using the proposed neural network: undamaged and damaged. To validate its efficacy, the method's accuracy was tested using a benchmark dataset derived from a three-floor structure at Los Alamos National Laboratory (LANL). The outcomes show that the new CNN algorithm is very accurate in spotting structural degradation in the examined structure.
</details>
<details>
<summary>摘要</summary>
structural health monitoring (SHM) 是现代结构监测的关键技术，通过感知器数据分析，检测结构的损害。它与predictive maintenance相结合，可以避免机器设备的停机时间和成本，早发现结构问题。多种机器学习技术已经被应用于振荡数据分析中，常常借鉴结构知识。这项研究推出了一种新的卷积神经网络算法，用于检测结构损害。这种方法将时间序列数据分解成两类：未损害和损害。为验证其准确性，这种新算法的准确率被测试使用了一个来自洛斯阿拉莫斯国家实验室（LANL）的 referential dataset。结果表明，新的卷积神经网络算法具有高度准确地检测检测结构衰老的能力。
</details></li>
</ul>
<hr>
<h2 id="Structure-of-universal-formulas"><a href="#Structure-of-universal-formulas" class="headerlink" title="Structure of universal formulas"></a>Structure of universal formulas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03910">http://arxiv.org/abs/2311.03910</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smith86n/wiki-is-mostly-fake-radom-words-word-genrationr-">https://github.com/smith86n/wiki-is-mostly-fake-radom-words-word-genrationr-</a></li>
<li>paper_authors: Dmitry Yarotsky</li>
<li>for: 这篇论文主要研究的是 Universelle Formeln（通用方程），它们可以用来拟合任意连续函数在固定区间内的表达。</li>
<li>methods: 论文使用了一系列的分析方法，包括对高度表达力模型的分析，以及对各种函数家族的分类结果。</li>
<li>results: 论文的主要结果是提出了一个层次结构的表达力分类，其中包括从全体函数到具有无限VC次元的函数家族的分类结果。此外，论文还给出了一些实际中使用的函数家族，如多层感知神经网络，并证明了它们在某些情况下可以拟合函数，但在整个定义域内无法拟合函数。<details>
<summary>Abstract</summary>
By universal formulas we understand parameterized analytic expressions that have a fixed complexity, but nevertheless can approximate any continuous function on a compact set. There exist various examples of such formulas, including some in the form of neural networks. In this paper we analyze the essential structural elements of these highly expressive models. We introduce a hierarchy of expressiveness classes connecting the global approximability property to the weaker property of infinite VC dimension, and prove a series of classification results for several increasingly complex functional families. In particular, we introduce a general family of polynomially-exponentially-algebraic functions that, as we prove, is subject to polynomial constraints. As a consequence, we show that fixed-size neural networks with not more than one layer of neurons having transcendental activations (e.g., sine or standard sigmoid) cannot in general approximate functions on arbitrary finite sets. On the other hand, we give examples of functional families, including two-hidden-layer neural networks, that approximate functions on arbitrary finite sets, but fail to do that on the whole domain of definition.
</details>
<details>
<summary>摘要</summary>
通过通用公式我们理解参数化分析表达式，它们具有固定复杂性，但可以近似任何连续函数在封闭集上。存在各种这种公式的例子，包括一些形式为神经网络。在这篇论文中，我们分析这些高表达能力模型的基本结构元素。我们建立一个表达能力层次结构，连接全球近似性质与更弱的无穷VC维度质量，并证明了一系列分类结果，其中包括几种逐渐复杂的函数家族。特别是，我们引入一个总是 полиномиаль地几何的函数家族，并证明这些函数是受限的。这表明，具有不超过一层神经元的神经网络（例如，使用极性函数如正弦或标准sigmoid）无法在一般finite集上近似函数。然而，我们给出了一些函数家族的示例，包括两层神经网络，它们可以在一般finite集上近似函数，但在整个定义域上不能够。
</details></li>
</ul>
<hr>
<h2 id="Learning-Based-Latency-Constrained-Fronthaul-Compression-Optimization-in-C-RAN"><a href="#Learning-Based-Latency-Constrained-Fronthaul-Compression-Optimization-in-C-RAN" class="headerlink" title="Learning-Based Latency-Constrained Fronthaul Compression Optimization in C-RAN"></a>Learning-Based Latency-Constrained Fronthaul Compression Optimization in C-RAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03899">http://arxiv.org/abs/2311.03899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Axel Grönland, Bleron Klaiqi, Xavier Gelabert</li>
<li>for: 这个研究旨在提出一个基于深度学习的无模型测试架构 (DRL-FC)，用于动态控制无线前端压缩 (FH) 的调整，以提高FH资料传输效率和无线通信性能。</li>
<li>methods: 本研究使用了模型自由的深度学习架构 (DRL)，通过调整不同的配置参数（如模ulation频率、预设精确度和预设量化），以影响FH负载和无线通信性能。</li>
<li>results: 实验结果显示，DRL-FC 可以在不同的FH负载水平下实现高度的FH资料传输效率和无线通信性能，并且可以遵循预先定义的FH延迟限制（在我们的情况下为260 $\mu$s）。<details>
<summary>Abstract</summary>
The evolution of wireless mobile networks towards cloudification, where Radio Access Network (RAN) functions can be hosted at either a central or distributed locations, offers many benefits like low cost deployment, higher capacity, and improved hardware utilization. Nevertheless, the flexibility in the functional deployment comes at the cost of stringent fronthaul (FH) capacity and latency requirements. One possible approach to deal with these rigorous constraints is to use FH compression techniques. To ensure that FH capacity and latency requirements are met, more FH compression is applied during high load, while less compression is applied during medium and low load to improve FH utilization and air interface performance. In this paper, a model-free deep reinforcement learning (DRL) based FH compression (DRL-FC) framework is proposed that dynamically controls FH compression through various configuration parameters such as modulation order, precoder granularity, and precoder weight quantization that affect both FH load and air interface performance. Simulation results show that DRL-FC exhibits significantly higher FH utilization (68.7% on average) and air interface throughput than a reference scheme (i.e. with no applied compression) across different FH load levels. At the same time, the proposed DRL-FC framework is able to meet the predefined FH latency constraints (in our case set to 260 $\mu$s) under various FH loads.
</details>
<details>
<summary>摘要</summary>
wireless mobile networks 向云化发展，Radio Access Network（RAN）功能可以在中央或分布式位置上hosts，这带来了许多优点，如低成本投入、更高的容量和硬件利用率。然而，功能部署的灵活性带来了前方接入（FH）容量和延迟要求的严格限制。一种可能的方法是使用FH压缩技术。要确保FH容量和延迟要求得到满足，在高负荷时应用更多的FH压缩，而在中等和低负荷时应用 menos的FH压缩，以提高FH使用率和空中接口性能。本文提出了一个基于深度学习 reinforcement learning（DRL）的FH压缩（DRL-FC）框架，通过不同的配置参数，如模调顺序、预编器粒度和预编器量化，控制FH压缩。实验结果表明，DRL-FC可以在不同的FH负荷水平上实现显著更高的FH使用率（68.7%的平均值）和空中接口吞吐量，同时能够遵循预定的FH延迟限制（我们的情况下设置为260微秒）。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Framework-for-Machine-learning-Based-Reactive-Power-Optimization-of-Distribution-Network"><a href="#An-Explainable-Framework-for-Machine-learning-Based-Reactive-Power-Optimization-of-Distribution-Network" class="headerlink" title="An Explainable Framework for Machine learning-Based Reactive Power Optimization of Distribution Network"></a>An Explainable Framework for Machine learning-Based Reactive Power Optimization of Distribution Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03863">http://arxiv.org/abs/2311.03863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlong Liao, Benjamin Schäfer, Dalin Qin, Gonghao Zhang, Zhixian Wang, Zhe Yang</li>
<li>for: 提高分布网络响应电压优化的计算效率，使用机器学习模型。</li>
<li>methods: 使用可解释机器学习框架，包括Shapley添加itive解释框架和模型独立估计Shapley值。</li>
<li>results: 通过可视分析，从全局和实例角度准确解释机器学习模型基于分布网络响应电压优化的解决方案。<details>
<summary>Abstract</summary>
To reduce the heavy computational burden of reactive power optimization of distribution networks, machine learning models are receiving increasing attention. However, most machine learning models (e.g., neural networks) are usually considered as black boxes, making it challenging for power system operators to identify and comprehend potential biases or errors in the decision-making process of machine learning models. To address this issue, an explainable machine-learning framework is proposed to optimize the reactive power in distribution networks. Firstly, a Shapley additive explanation framework is presented to measure the contribution of each input feature to the solution of reactive power optimizations generated from machine learning models. Secondly, a model-agnostic approximation method is developed to estimate Shapley values, so as to avoid the heavy computational burden associated with direct calculations of Shapley values. The simulation results show that the proposed explainable framework can accurately explain the solution of the machine learning model-based reactive power optimization by using visual analytics, from both global and instance perspectives. Moreover, the proposed explainable framework is model-agnostic, and thus applicable to various models (e.g., neural networks).
</details>
<details>
<summary>摘要</summary>
为了减轻分布网络的反应能源优化计算负担，机器学习模型在分布网络优化中收到了越来越多的关注。然而，大多数机器学习模型（例如神经网络）通常被视为黑盒模型，使得电力系统运维人员很难发现和理解机器学习模型的偏见或错误。为解决这个问题，一个可解释的机器学习框架是提议的，用于优化分布网络中的反应能源。首先，一种Shapley添加性解释框架是提出来度量每个输入特征对机器学习模型生成的反应能源优化解决方案中的贡献。其次，一种模型无关的估计方法是开发来估计Shapley值，以避免直接计算Shapley值所需的巨量计算负担。实验结果表明，提议的可解释框架可以准确地解释机器学习模型基于的反应能源优化解决方案，使用可视化分析，从全局和实例两个角度进行解释。此外，提议的可解释框架是模型无关的，因此适用于多种模型（例如神经网络）。
</details></li>
</ul>
<hr>
<h2 id="Improved-MDL-Estimators-Using-Fiber-Bundle-of-Local-Exponential-Families-for-Non-exponential-Families"><a href="#Improved-MDL-Estimators-Using-Fiber-Bundle-of-Local-Exponential-Families-for-Non-exponential-Families" class="headerlink" title="Improved MDL Estimators Using Fiber Bundle of Local Exponential Families for Non-exponential Families"></a>Improved MDL Estimators Using Fiber Bundle of Local Exponential Families for Non-exponential Families</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03852">http://arxiv.org/abs/2311.03852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kohei Miyamoto, Andrew R. Barron, Jun’ichi Takeuchi</li>
<li>for: 本文研究了使用两部分编码的最小描述长度（MDL）估计器，并对普通参数家族进行了分析。</li>
<li>methods: 本文引入了一种两部分代码，其对于一般参数家族下的某些正则条件下，可以达到与最佳目标家族M的 regret 几乎相同的水平。</li>
<li>results: 本文提出了一种基于巴顿和科вер在1991年的理论，对MDL估计器的风险和损失进行了紧密的Upper bound的分析。此外，本文还应用了结果到杂合家族中，这是非对称家族的一个典型示例。<details>
<summary>Abstract</summary>
Minimum Description Length (MDL) estimators, using two-part codes for universal coding, are analyzed. For general parametric families under certain regularity conditions, we introduce a two-part code whose regret is close to the minimax regret, where regret of a code with respect to a target family M is the difference between the code length of the code and the ideal code length achieved by an element in M. This is a generalization of the result for exponential families by Gr\"unwald. Our code is constructed by using an augmented structure of M with a bundle of local exponential families for data description, which is not needed for exponential families. This result gives a tight upper bound on risk and loss of the MDL estimators based on the theory introduced by Barron and Cover in 1991. Further, we show that we can apply the result to mixture families, which are a typical example of non-exponential families.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用两部分代码的最小描述长度（MDL）估计器被分析。对于普遍的参数家族，在某些常见的准则下，我们引入一个两部分代码，其忽略之违的 regret几乎等于最优化 regret，这是由Gr\"unwald得到的结果的推广。我们的代码使用了扩展结构的M，其中包含一个本地快满家族来描述数据，这并不需要对快满家族。这个结果给出了基于Barron和Cover在1991年引入的理论的紧张Upper bound，并且我们证明可以应用到混合家族，这些家族是非快满家族的典型示例。Note: "普遍的参数家族" (pang-ku-shi-ju-ji-ming) refers to a family of probability distributions that is not necessarily exponential, and "扩展结构" (kuang-zheng-xing) refers to an augmented structure of the parameter family.
</details></li>
</ul>
<hr>
<h2 id="User-level-Differentially-Private-Stochastic-Convex-Optimization-Efficient-Algorithms-with-Optimal-Rates"><a href="#User-level-Differentially-Private-Stochastic-Convex-Optimization-Efficient-Algorithms-with-Optimal-Rates" class="headerlink" title="User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates"></a>User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03797">http://arxiv.org/abs/2311.03797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hilal Asi, Daogao Liu</li>
<li>for: 这个论文主要是关于如何实现用户级 differentially private stochastic convex optimization (DP-SCO)，以保护每个用户的数据隐私。</li>
<li>methods: 这个论文提出了一种新的算法，基于多 passes DP-SGD 和一种私有均值估计方法，可以在 polynomial time 内实现用户级 DP-SCO，并且只需要用户数量在维度上增长 Logarithmically。</li>
<li>results: 这个论文的算法可以在 polynomial time 内实现用户级 DP-SCO，并且可以处理 convex 和强 convex 函数，以及非稠密函数。这些结果比 existing work 更优，并且不需要额外的假设。<details>
<summary>Abstract</summary>
We study differentially private stochastic convex optimization (DP-SCO) under user-level privacy, where each user may hold multiple data items. Existing work for user-level DP-SCO either requires super-polynomial runtime [Ghazi et al. (2023)] or requires the number of users to grow polynomially with the dimensionality of the problem with additional strict assumptions [Bassily et al. (2023)]. We develop new algorithms for user-level DP-SCO that obtain optimal rates for both convex and strongly convex functions in polynomial time and require the number of users to grow only logarithmically in the dimension. Moreover, our algorithms are the first to obtain optimal rates for non-smooth functions in polynomial time. These algorithms are based on multiple-pass DP-SGD, combined with a novel private mean estimation procedure for concentrated data, which applies an outlier removal step before estimating the mean of the gradients.
</details>
<details>
<summary>摘要</summary>
我们研究具有用户级隐私的可变性梯度优化（DP-SCO），每个用户可能拥有多个数据项。现有的用户级DP-SCO研究 either需要超 polynomial 时间 [Ghazi et al. (2023)] 或者需要用户数量在维度问题上增长 polynomial 方式，并且具有其他严格的假设 [Bassily et al. (2023)].我们开发了新的用户级DP-SCO算法，可以在 polynomial 时间内获得对凸和强凸函数的优化率，并且用户数量只需要在维度上增长 logarithmic。此外，我们的算法还是首次实现了非滑动函数的优化率在 polynomial 时间内。这些算法基于多个通道DP-SGD，结合了一种新的隐私性均值估计过程，用于集中的数据中心化，该过程包括一个异常 eliminating 步骤。
</details></li>
</ul>
<hr>
<h2 id="Neuro-GPT-Developing-A-Foundation-Model-for-EEG"><a href="#Neuro-GPT-Developing-A-Foundation-Model-for-EEG" class="headerlink" title="Neuro-GPT: Developing A Foundation Model for EEG"></a>Neuro-GPT: Developing A Foundation Model for EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03764">http://arxiv.org/abs/2311.03764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhui Cui, Woojae Jeong, Philipp Thölke, Takfarinas Medani, Karim Jerbi, Anand A. Joshi, Richard M. Leahy</li>
<li>for: 提高Brain-Computer Interface (BCI)任务中电энцеfalogram (EEG)数据的稀缺和多样性问题，并利用大规模公共数据来充分发挥作用。</li>
<li>methods: 基于EEG编码器和GPT模型的基础模型，通过自我超vised任务在大规模公共EEG数据集上预训练，然后在只有9名参与者的motor imagery classification任务上细化训练。</li>
<li>results: 实验表明，基于基础模型进行应用可以显著提高分类性能，证明了基础模型的进步普适性和数据稀缺和多样性问题的解决能力。<details>
<summary>Abstract</summary>
To handle the scarcity and heterogeneity of electroencephalography (EEG) data in Brain-Computer Interface (BCI) tasks, and to harness the vast public data, we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model. The foundation model is pre-trained on a large-scale public EEG dataset, using a self-supervised task which learns how to reconstruct the masked chunk in EEG. We then fine-tune the foundation model on a Motor Imagery Classification task where only 9 subjects are available. Experiments demonstrated that applying foundation model can significantly improve classification performance compared to the model trained from scratch, which provides evidence for the advanced generalizability of foundation model and the ability to address the challenges of data scarcity and heterogeneity.
</details>
<details>
<summary>摘要</summary>
为了解决电enzephalography（EEG）数据的缺乏和多样性在Brain-Computer Interface（BCI）任务中，并利用大量公共数据，我们提出了Neuro-GPT，一个基础模型，包括EEG编码器和GPT模型。基础模型在一个大规模的公共EEG数据集上自我超vised学习任务上进行预训练，学习如何重建受mask的EEG块。然后，我们在只有9名参与者的motor imagery分类任务上细化基础模型。实验表明，对基础模型进行应用可以显著提高分类性能，比起从scratch训练的模型，这提供了证据，证明了基础模型的高度通用性和能力 Addressing the challenges of data scarcity and heterogeneity。
</details></li>
</ul>
<hr>
<h2 id="Posterior-Sampling-Based-Bayesian-Optimization-with-Tighter-Bayesian-Regret-Bounds"><a href="#Posterior-Sampling-Based-Bayesian-Optimization-with-Tighter-Bayesian-Regret-Bounds" class="headerlink" title="Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds"></a>Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03760">http://arxiv.org/abs/2311.03760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shion Takeno, Yu Inatsu, Masayuki Karasuyama, Ichiro Takeuchi</li>
<li>for: This paper focuses on improving the Bayesian cumulative regret (BCR) of three acquisition functions (AFs) in Bayesian optimization (BO): Gaussian process upper confidence bound (GP-UCB), Thompson sampling (TS), and a new proposed AF called probability of improvement from the maximum of a sample path (PIMS).</li>
<li>methods: The paper uses theoretical analysis and experimental evaluation to compare the BCR bounds of GP-UCB, TS, and PIMS. The authors also propose PIMS as a new AF that achieves the tighter BCR bound and avoids manual hyperparameter tuning.</li>
<li>results: The paper shows that PIMS achieves the tighter BCR bound and mitigates the practical issues of GP-UCB and TS in a wide range of experiments. Specifically, PIMS outperforms GP-UCB and TS in terms of BCR and avoids the need for hyperparameter tuning.<details>
<summary>Abstract</summary>
Among various acquisition functions (AFs) in Bayesian optimization (BO), Gaussian process upper confidence bound (GP-UCB) and Thompson sampling (TS) are well-known options with established theoretical properties regarding Bayesian cumulative regret (BCR). Recently, it has been shown that a randomized variant of GP-UCB achieves a tighter BCR bound compared with GP-UCB, which we call the tighter BCR bound for brevity. Inspired by this study, this paper first shows that TS achieves the tighter BCR bound. On the other hand, GP-UCB and TS often practically suffer from manual hyperparameter tuning and over-exploration issues, respectively. To overcome these difficulties, we propose yet another AF called a probability of improvement from the maximum of a sample path (PIMS). We show that PIMS achieves the tighter BCR bound and avoids the hyperparameter tuning, unlike GP-UCB. Furthermore, we demonstrate a wide range of experiments, focusing on the effectiveness of PIMS that mitigates the practical issues of GP-UCB and TS.
</details>
<details>
<summary>摘要</summary>
Among various acquisition functions (AFs) in Bayesian optimization (BO), Gaussian process upper confidence bound (GP-UCB) and Thompson sampling (TS) are well-known options with established theoretical properties regarding Bayesian cumulative regret (BCR). Recently, it has been shown that a randomized variant of GP-UCB achieves a tighter BCR bound compared with GP-UCB, which we call the tighter BCR bound for brevity. Inspired by this study, this paper first shows that TS achieves the tighter BCR bound. On the other hand, GP-UCB and TS often practically suffer from manual hyperparameter tuning and over-exploration issues, respectively. To overcome these difficulties, we propose yet another AF called a probability of improvement from the maximum of a sample path (PIMS). We show that PIMS achieves the tighter BCR bound and avoids the hyperparameter tuning, unlike GP-UCB. Furthermore, we demonstrate a wide range of experiments, focusing on the effectiveness of PIMS that mitigates the practical issues of GP-UCB and TS.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Manifold-learning-what-how-and-why"><a href="#Manifold-learning-what-how-and-why" class="headerlink" title="Manifold learning: what, how, and why"></a>Manifold learning: what, how, and why</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03757">http://arxiv.org/abs/2311.03757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marina Meilă, Hanyu Zhang</li>
<li>for: 本文旨在介绍机能学（ML）的原理、方法和统计基础，以帮助读者更好地理解高维数据的低维结构。</li>
<li>methods: 本文主要介绍了ML方法的代表选择和参数选择的权衡问题，以及这些方法的统计基础。</li>
<li>results: 本文通过描述高维点云的几何形态和可视化，以及去噪和解释高维数据的方法，满足读者寻找低维结构的需求。<details>
<summary>Abstract</summary>
Manifold learning (ML), known also as non-linear dimension reduction, is a set of methods to find the low dimensional structure of data. Dimension reduction for large, high dimensional data is not merely a way to reduce the data; the new representations and descriptors obtained by ML reveal the geometric shape of high dimensional point clouds, and allow one to visualize, de-noise and interpret them. This survey presents the principles underlying ML, the representative methods, as well as their statistical foundations from a practicing statistician's perspective. It describes the trade-offs, and what theory tells us about the parameter and algorithmic choices we make in order to obtain reliable conclusions.
</details>
<details>
<summary>摘要</summary>
manifold学习（ML），也称非线性维度减少，是一组方法来找出数据的低维度结构。对大量、高维度数据进行维度减少不仅是将数据减少，新的表示和描述器由ML获得的减少结构，可以visualize、去噪和解释高维度点云的几何形状。这篇评论介绍了ML的原则、代表方法以及其统计基础，从实际统计师的角度出发，描述了参数和算法选择的交易OFF和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-physics-informed-neural-networks-with-domain-scaling-and-residual-correction-methods-for-multi-frequency-elliptic-problems"><a href="#Enhanced-physics-informed-neural-networks-with-domain-scaling-and-residual-correction-methods-for-multi-frequency-elliptic-problems" class="headerlink" title="Enhanced physics-informed neural networks with domain scaling and residual correction methods for multi-frequency elliptic problems"></a>Enhanced physics-informed neural networks with domain scaling and residual correction methods for multi-frequency elliptic problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03746">http://arxiv.org/abs/2311.03746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deok-Kyu Jang, Hyea Hyun Kim, Kyungsoo Kim</li>
<li>for: 这个论文是为了解决非线性偏微分方程的多频解决方法。</li>
<li>methods: 论文提出了基于神经网络的近似方法，具有不受不同偏微分方程的形式或问题域形状或维度的限制的优点。</li>
<li>results: 论文通过域扩展和差异修正方法提高了近似方法的效率和准确性，并在多频模拟问题中进行了证明。<details>
<summary>Abstract</summary>
In this paper, neural network approximation methods are developed for elliptic partial differential equations with multi-frequency solutions. Neural network work approximation methods have advantages over classical approaches in that they can be applied without much concerns on the form of the differential equations or the shape or dimension of the problem domain. When applied to problems with multi-frequency solutions, the performance and accuracy of neural network approximation methods are strongly affected by the contrast of the high- and low-frequency parts in the solutions. To address this issue, domain scaling and residual correction methods are proposed. The efficiency and accuracy of the proposed methods are demonstrated for multi-frequency model problems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了基于神经网络的积分方法，用于解决带多频解的几何偏微分方程。神经网络方法在比较古典方法时有优势，因为它们不受偏微分方程的形式或问题域的形状或维度的限制。当应用到具有多频解的问题时，神经网络方法的性能和准确性受到解的高频和低频部分的对比的影响。为解决这个问题，我们提出了域扩大和差异修正方法。我们对多频模拟问题进行了效率和准确性的证明。
</details></li>
</ul>
<hr>
<h2 id="Improved-weight-initialization-for-deep-and-narrow-feedforward-neural-network"><a href="#Improved-weight-initialization-for-deep-and-narrow-feedforward-neural-network" class="headerlink" title="Improved weight initialization for deep and narrow feedforward neural network"></a>Improved weight initialization for deep and narrow feedforward neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03733">http://arxiv.org/abs/2311.03733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunwoo Lee, Yunho Kim, Seungyeop Yang, Hayoung Choi</li>
<li>for: 解决深度学习中ReLU neuron死亡问题，提高深度 neural network 的训练效果和效率。</li>
<li>methods: 提出一种新的初始 веса初始化方法，并证明该方法可以有效地传递信号 вектор。</li>
<li>results: 通过一系列实验和比较 existed 方法，证明新 initialization 方法的效果。<details>
<summary>Abstract</summary>
Appropriate weight initialization settings, along with the ReLU activation function, have been a cornerstone of modern deep learning, making it possible to train and deploy highly effective and efficient neural network models across diverse artificial intelligence. The problem of dying ReLU, where ReLU neurons become inactive and yield zero output, presents a significant challenge in the training of deep neural networks with ReLU activation function. Theoretical research and various methods have been introduced to address the problem. However, even with these methods and research, training remains challenging for extremely deep and narrow feedforward networks with ReLU activation function. In this paper, we propose a new weight initialization method to address this issue. We prove the properties of the proposed initial weight matrix and demonstrate how these properties facilitate the effective propagation of signal vectors. Through a series of experiments and comparisons with existing methods, we demonstrate the effectiveness of the new initialization method.
</details>
<details>
<summary>摘要</summary>
现代深度学习的重要基础之一是适当的初始 веса设置，以及ReLU激活函数。这些设置使得可以训练和部署高效和高效的神经网络模型。ReLU激活函数的问题，即ReLU神经元变得无作用并产生零输出，对深度神经网络的训练呈现出了 significiant挑战。理论研究和各种方法已经被提出来解决这个问题。然而，即使使用这些方法和研究，训练非常深和窄的Feedforward网络仍然具有挑战性。在这篇论文中，我们提出了一种新的初始 веса方法，以解决这个问题。我们证明了提案的初始 веса矩阵的属性，并示出这些属性如何促进信号向量的有效传播。通过一系列实验和现有方法的比较，我们证明了新 initialization 方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Pipeline-Parallelism-for-DNN-Inference-with-Practical-Performance-Guarantees"><a href="#Pipeline-Parallelism-for-DNN-Inference-with-Practical-Performance-Guarantees" class="headerlink" title="Pipeline Parallelism for DNN Inference with Practical Performance Guarantees"></a>Pipeline Parallelism for DNN Inference with Practical Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03703">http://arxiv.org/abs/2311.03703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaron Archer, Matthew Fahrbach, Kuikui Liu, Prakash Prabhu</li>
<li>for: 优化深度神经网络（DNN）推理的管道并行性，通过将模型图分割成 $k$ 个阶段，最小化瓶颈阶段的运行时间，包括通信时间。</li>
<li>methods: 提出了实用的算法来解决这个NP困难的问题，并对这些算法进行了优化。同时，通过新的杂合数学Programming（MIP）形式ulation来获得更强的下界。</li>
<li>results: 通过应用这些算法和下界方法来评估生产模型，实现了比标准 combinatorial下界更好的近似保证。例如，通过 geometric means across production data with $k&#x3D;16$ pipeline stages, our MIP formulations more than double the lower bounds, improving the approximation ratio from $2.175$ to $1.058$.<details>
<summary>Abstract</summary>
We optimize pipeline parallelism for deep neural network (DNN) inference by partitioning model graphs into $k$ stages and minimizing the running time of the bottleneck stage, including communication. We design practical algorithms for this NP-hard problem and show that they are nearly optimal in practice by comparing against strong lower bounds obtained via novel mixed-integer programming (MIP) formulations. We apply these algorithms and lower-bound methods to production models to achieve substantially improved approximation guarantees compared to standard combinatorial lower bounds. For example, evaluated via geometric means across production data with $k=16$ pipeline stages, our MIP formulations more than double the lower bounds, improving the approximation ratio from $2.175$ to $1.058$. This work shows that while max-throughput partitioning is theoretically hard, we have a handle on the algorithmic side of the problem in practice and much of the remaining challenge is in developing more accurate cost models to feed into the partitioning algorithms.
</details>
<details>
<summary>摘要</summary>
我们优化深度神经网络（DNN）推断的管线并行性，通过分解模型图 into $k$ 阶段，最小化瓶颈阶段的执行时间，包括通信。我们设计了实用的算法来解决这个NP困难的问题，并证明它们在实践中几乎是最佳的。我们透过与新的混合整数程式（MIP）形式的比较，得到了强有力的下限。我们将这些算法和下限方法应用到生产模型，以取得较好的近似保证比。例如，透过生产数据中的$k=16$ 管线阶段，我们的MIP形式更是多过了下限，从$2.175$ 提高到$1.058$。这个工作显示，优化管线并行性是理论上困难的，但在实践中，我们已经掌握了算法的一侧，主要的挑战是发展更加精确的成本模型，以供分配算法使用。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Non-monotone-Submodular-Maximization"><a href="#Dynamic-Non-monotone-Submodular-Maximization" class="headerlink" title="Dynamic Non-monotone Submodular Maximization"></a>Dynamic Non-monotone Submodular Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03685">http://arxiv.org/abs/2311.03685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiarash Banihashem, Leyla Biabani, Samira Goudarzi, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, Morteza Monemizadeh</li>
<li>for: 这个论文主要针对的问题是凸覆盖函数的最大化问题，具体来说是在卡尔达ности约束$k$下最大化凸覆盖函数。</li>
<li>methods: 这篇论文使用了动态算法来解决凸覆盖函数最大化问题，并且提出了一种可以应用于非凸覆盖函数的方法。</li>
<li>results: 这篇论文提出了一种能够在卡尔达性约束$k$下实现$(8+\epsilon)$估计的动态算法，并且可以在执行多个更新后继续使用。此外，论文还应用了这种算法于视频概要和max-cut问题，并在实际数据集上 obtaint 良好的结果。<details>
<summary>Abstract</summary>
Maximizing submodular functions has been increasingly used in many applications of machine learning, such as data summarization, recommendation systems, and feature selection. Moreover, there has been a growing interest in both submodular maximization and dynamic algorithms. In 2020, Monemizadeh and Lattanzi, Mitrovic, Norouzi{-}Fard, Tarnawski, and Zadimoghaddam initiated developing dynamic algorithms for the monotone submodular maximization problem under the cardinality constraint $k$. Recently, there have been some improvements on the topic made by Banihashem, Biabani, Goudarzi, Hajiaghayi, Jabbarzade, and Monemizadeh. In 2022, Chen and Peng studied the complexity of this problem and raised an important open question: "Can we extend [fully dynamic] results (algorithm or hardness) to non-monotone submodular maximization?". We affirmatively answer their question by demonstrating a reduction from maximizing a non-monotone submodular function under the cardinality constraint $k$ to maximizing a monotone submodular function under the same constraint. Through this reduction, we obtain the first dynamic algorithms to solve the non-monotone submodular maximization problem under the cardinality constraint $k$. Our algorithms maintain an $(8+\epsilon)$-approximate of the solution and use expected amortized $O(\epsilon^{-3}k^3\log^3(n)\log(k))$ or $O(\epsilon^{-1}k^2\log^3(k))$ oracle queries per update, respectively. Furthermore, we showcase the benefits of our dynamic algorithm for video summarization and max-cut problems on several real-world data sets.
</details>
<details>
<summary>摘要</summary>
maximizing submodular functions在机器学习中越来越受欢迎，应用于数据概要、推荐系统和特征选择等领域。此外，关于submodular maximization和动态算法的研究也有增长兴趣。在2020年，Monemizadeh和Lattanzi、Mitrovic、Norouzi{-}Fard、Tarnawski和Zadimoghaddam开始开发动态算法 для偏函数最大化问题下的cardinality约束$k$。最近，有一些在这个领域进行了改进。在2022年，陈和平 studied this problem的复杂性，提出了一个重要的开问："可以将全动态结果（算法或困难）扩展到非偏函数最大化问题吗？"。我们答复了这个问题，通过示出减法函数最大化问题下的cardinality约束$k$和非偏函数最大化问题下的cardinality约束$k$之间的等价转换，从而得到了首个解决非偏函数最大化问题下的动态算法。我们的算法保证可以获得$(8+\epsilon)$-近似的解决方案，并使用预期的折衔折衔$O(\epsilon^{-3}k^3\log^3(n)\log(k))$或$O(\epsilon^{-1}k^2\log^3(k))$的缓存查询次数。此外，我们还展示了我们的动态算法在视频概要和max-cut问题中的应用效果。
</details></li>
</ul>
<hr>
<h2 id="Preventing-Arbitrarily-High-Confidence-on-Far-Away-Data-in-Point-Estimated-Discriminative-Neural-Networks"><a href="#Preventing-Arbitrarily-High-Confidence-on-Far-Away-Data-in-Point-Estimated-Discriminative-Neural-Networks" class="headerlink" title="Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks"></a>Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03683">http://arxiv.org/abs/2311.03683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Rashid, Serena Hacker, Guojun Zhang, Agustinus Kristiadi, Pascal Poupart</li>
<li>for: 这个研究旨在解决深度学习模型在对外测试数据的预测中出现的过度自信问题。</li>
<li>methods: 作者提出了一种方法，即在深度学习模型的输出加入一个类别的条件函数，以预测测试数据是否属于训练数据的类别。</li>
<li>results: 试验结果显示，这种方法可以有效地预防深度学习模型在对外测试数据的预测中出现过度自信的问题，同时仍然维持了简单的推断点估计训练。<details>
<summary>Abstract</summary>
Discriminatively trained, deterministic neural networks are the de facto choice for classification problems. However, even though they achieve state-of-the-art results on in-domain test sets, they tend to be overconfident on out-of-distribution (OOD) data. For instance, ReLU networks -- a popular class of neural network architectures -- have been shown to almost always yield high confidence predictions when the test data are far away from the training set, even when they are trained with OOD data. We overcome this problem by adding a term to the output of the neural network that corresponds to the logit of an extra class, that we design to dominate the logits of the original classes as we move away from the training data.This technique provably prevents arbitrarily high confidence on far-away test data while maintaining a simple discriminative point-estimate training. Evaluation on various benchmarks demonstrates strong performance against competitive baselines on both far-away and realistic OOD data.
</details>
<details>
<summary>摘要</summary>
通用化训练的权重链是现实中的首选 Classification 问题。然而，它们在不同 FROM 数据上表现出STATE-OF-THE-ART 的结果，但它们往往对于 OUT-OF-DISTRIBUTION 数据（OOD）表现出过于自信。例如，ReLU 网络——一种广泛使用的神经网络架构——在测试数据远离训练数据时经常提供高自信度预测，即使它们在训练 OOD 数据时被训练。我们解决了这个问题，通过将一个对应于额外类的条件添加到神经网络的输出中，使得随着测试数据的远离，预测结果呈指数趋势，从而避免了无限高的自信度。这种技术可证明地防止了无限高的自信度在测试数据远离训练数据时，同时保持简单的推学点估计训练。在多个标准 bench 上，我们的方法与高端基准值进行了有力的竞争。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-for-Power-Grid-Operational-Risk-Assessment"><a href="#Graph-Neural-Networks-for-Power-Grid-Operational-Risk-Assessment" class="headerlink" title="Graph Neural Networks for Power Grid Operational Risk Assessment"></a>Graph Neural Networks for Power Grid Operational Risk Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03661">http://arxiv.org/abs/2311.03661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yadong Zhang, Pranav M Karve, Sankaran Mahadevan</li>
<li>for: This paper is written for the purpose of investigating the utility of graph neural network (GNN) surrogates for Monte Carlo (MC) sampling-based risk quantification in daily operations of power grids.</li>
<li>methods: The paper uses GNN surrogates of optimal power flow (OPF) problems to expedite the MC simulation process, which is computationally prohibitive. The GNN surrogates are trained using supervised learning and are used to obtain MC samples of the quantities of interest (operating reserve, transmission line flow) given the (hours-ahead) probabilistic wind generation and load forecast.</li>
<li>results: The paper shows that GNN surrogates are sufficiently accurate for predicting the (bus-level, branch-level and system-level) grid state and enable fast as well as accurate operational risk quantification for power grids. The paper thus develops various tools for fast reliability and risk quantification for real-world power grids using GNNs.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了研究使用图 neural network（GNN）仿真器来实现 Monte Carlo（MC）采样基于的电力网运行风险评估的实用性。</li>
<li>methods: 这篇论文使用GNN仿真器来快速解决MC采样过程中的优化电力流问题，这是计算昂贵的。GNN仿真器通过监督学习来训练，并用于根据（小时前）风力生产和吞吐量预测获取MC采样值。</li>
<li>results: 这篇论文表明GNN仿真器可以准确地预测（电压级、电流级和系统级）电力网状态，并且可以快速而准确地评估电力网运行风险。因此，这篇论文开发了基于GNN的快速可靠性和风险评估工具 для实际电力网。<details>
<summary>Abstract</summary>
In this article, the utility of graph neural network (GNN) surrogates for Monte Carlo (MC) sampling-based risk quantification in daily operations of power grid is investigated. The MC simulation process necessitates solving a large number of optimal power flow (OPF) problems corresponding to the sample values of stochastic grid variables (power demand and renewable generation), which is computationally prohibitive. Computationally inexpensive surrogates of the OPF problem provide an attractive alternative for expedited MC simulation. GNN surrogates are especially suitable due to their superior ability to handle graph-structured data. Therefore, GNN surrogates of OPF problem are trained using supervised learning. They are then used to obtain Monte Carlo (MC) samples of the quantities of interest (operating reserve, transmission line flow) given the (hours-ahead) probabilistic wind generation and load forecast. The utility of GNN surrogates is evaluated by comparing OPF-based and GNN-based grid reliability and risk for IEEE Case118 synthetic grid. It is shown that the GNN surrogates are sufficiently accurate for predicting the (bus-level, branch-level and system-level) grid state and enable fast as well as accurate operational risk quantification for power grids. The article thus develops various tools for fast reliability and risk quantification for real-world power grids using GNNs.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们调查了graph neural network（GNN）伪函数的 utility для Monte Carlo（MC）样本基于的电力网络风险评估。MC模拟过程需要解决一个大量的优化电力流（OPF）问题，这是计算昂贵的。GNN伪函数特别适合处理图结构数据，因此GNN伪函数可以用于快速和准确地评估电力网络的可靠性和风险。在IEEE Case118 sintética网格上，我们证明了GNN伪函数是准确的预测（电网）状态的。这篇文章因此开发了各种工具，用于快速和准确地评估实际电力网络的可靠性和风险。
</details></li>
</ul>
<hr>
<h2 id="A-Physics-Guided-Bi-Fidelity-Fourier-Featured-Operator-Learning-Framework-for-Predicting-Time-Evolution-of-Drag-and-Lift-Coefficients"><a href="#A-Physics-Guided-Bi-Fidelity-Fourier-Featured-Operator-Learning-Framework-for-Predicting-Time-Evolution-of-Drag-and-Lift-Coefficients" class="headerlink" title="A Physics-Guided Bi-Fidelity Fourier-Featured Operator Learning Framework for Predicting Time Evolution of Drag and Lift Coefficients"></a>A Physics-Guided Bi-Fidelity Fourier-Featured Operator Learning Framework for Predicting Time Evolution of Drag and Lift Coefficients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03639">http://arxiv.org/abs/2311.03639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Mollaali, Izzet Sahin, Iqrar Raza, Christian Moya, Guillermo Paniagua, Guang Lin</li>
<li>for: 本研究的目的是提高计算机 experiments 的准确性和效率，并 minimize 计算资源的使用。</li>
<li>methods: 本文提出了一种基于深度学习的抽象网络框架，通过限制高精度数据的使用，实现高精度解的计算。这个框架基于一种新的物理指导的、双精度的 Fourier 特征网络（DeepONet），可以同时利用低精度和高精度数据的优点。</li>
<li>results: 研究结果表明，基于 DeepONet 的方法可以更高精度地预测振荡性解的时间轨迹，并且比数据驱动的方法更具有预测能力。<details>
<summary>Abstract</summary>
In the pursuit of accurate experimental and computational data while minimizing effort, there is a constant need for high-fidelity results. However, achieving such results often requires significant computational resources. To address this challenge, this paper proposes a deep operator learning-based framework that requires a limited high-fidelity dataset for training. We introduce a novel physics-guided, bi-fidelity, Fourier-featured Deep Operator Network (DeepONet) framework that effectively combines low and high-fidelity datasets, leveraging the strengths of each. In our methodology, we began by designing a physics-guided Fourier-featured DeepONet, drawing inspiration from the intrinsic physical behavior of the target solution. Subsequently, we train this network to primarily learn the low-fidelity solution, utilizing an extensive dataset. This process ensures a comprehensive grasp of the foundational solution patterns. Following this foundational learning, the low-fidelity deep operator network's output is enhanced using a physics-guided Fourier-featured residual deep operator network. This network refines the initial low-fidelity output, achieving the high-fidelity solution by employing a small high-fidelity dataset for training. Notably, in our framework, we employ the Fourier feature network as the Trunk network for the DeepONets, given its proficiency in capturing and learning the oscillatory nature of the target solution with high precision. We validate our approach using a well-known 2D benchmark cylinder problem, which aims to predict the time trajectories of lift and drag coefficients. The results highlight that the physics-guided Fourier-featured deep operator network, serving as a foundational building block of our framework, possesses superior predictive capability for the lift and drag coefficients compared to its data-driven counterparts.
</details>
<details>
<summary>摘要</summary>
在寻求精确的实验和计算数据的同时减少努力的挑战中，需要高精度的结果。然而，获得这些结果通常需要巨大的计算资源。为解决这个挑战，这篇论文提出了一个基于深度学习的核心运算学习框架，只需要一个有限的高精度数据集来训练。我们介绍了一种新的物理指导的、双精度的、快朗特网络（DeepONet）框架，可以有效地结合低精度和高精度数据集，利用每个数据集的优势。在我们的方法中，我们首先设计了物理指导的快朗特网络， Drawing inspiration from the intrinsic physical behavior of the target solution。然后，我们使用了一个广泛的低精度数据集来训练这个网络，以确保拥有基本解决方案的基本模式。在这个基本学习后，我们使用物理指导的快朗特网络来进一步增强低精度深度运算网络的输出，以实现高精度的解决方案。在我们的框架中，我们使用快朗特网络作为Trunk网络，因为它能够高精度地捕捉和学习目标解决方案中的律动特征。我们验证了我们的方法使用了一个已知的2D标准问题，该问题的目标是预测挡板和推力减速系数的时间轨迹。结果显示，物理指导的快朗特网络作为我们框架的基础建筑块，在预测挡板和推力减速系数方面具有更高的预测能力，比其数据驱动的对手更好。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Data-Augmentation-with-Contrastive-Learning"><a href="#Counterfactual-Data-Augmentation-with-Contrastive-Learning" class="headerlink" title="Counterfactual Data Augmentation with Contrastive Learning"></a>Counterfactual Data Augmentation with Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03630">http://arxiv.org/abs/2311.03630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Aloui, Juncheng Dong, Cat P. Le, Vahid Tarokh<br>for: 该研究旨在解决 conditional average treatment effect (CATE) 估计中的统计差异问题，提高 CATE 估计的准确性和稳定性。methods: 该研究提出了一种模型无关的数据扩充方法，使用对照学习来学习一个表示空间和一个相似度度量，以确保可靠地估计对照组中个体的可能的结果。results: 经过 theoretical analysis 和实验研究，该方法能够在 synthetic 和半synthetic 标准 benchmark 上实现显著提高 CATE 估计的性能和对抗过拟合的能力。<details>
<summary>Abstract</summary>
Statistical disparity between distinct treatment groups is one of the most significant challenges for estimating Conditional Average Treatment Effects (CATE). To address this, we introduce a model-agnostic data augmentation method that imputes the counterfactual outcomes for a selected subset of individuals. Specifically, we utilize contrastive learning to learn a representation space and a similarity measure such that in the learned representation space close individuals identified by the learned similarity measure have similar potential outcomes. This property ensures reliable imputation of counterfactual outcomes for the individuals with close neighbors from the alternative treatment group. By augmenting the original dataset with these reliable imputations, we can effectively reduce the discrepancy between different treatment groups, while inducing minimal imputation error. The augmented dataset is subsequently employed to train CATE estimation models. Theoretical analysis and experimental studies on synthetic and semi-synthetic benchmarks demonstrate that our method achieves significant improvements in both performance and robustness to overfitting across state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
“统计差异 между不同处理组是计算 conditional Average Treatment Effects（CATE）的主要挑战之一。为解决这个问题，我们介绍了一种模型无关的数据增强方法，该方法在选择的一部分个体上进行了可靠的假设替换。具体来说，我们使用了对比学习来学习一个表示空间和一种相似度度量，使得在这个表示空间中与选择的一部分个体的相似度度量相似的个体都有相似的可能结果。这个性质使得我们可以可靠地进行这些个体的假设替换，从而减少不同处理组之间的统计差异，同时减少潜在的假设替换误差。通过将这些可靠的假设替换加入原始数据集，我们可以有效地降低不同处理组之间的统计差异，同时提高计算CATE的模型性能和鲁棒性。”
</details></li>
</ul>
<hr>
<h2 id="Are-Words-Enough-On-the-semantic-conditioning-of-affective-music-generation"><a href="#Are-Words-Enough-On-the-semantic-conditioning-of-affective-music-generation" class="headerlink" title="Are Words Enough? On the semantic conditioning of affective music generation"></a>Are Words Enough? On the semantic conditioning of affective music generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03624">http://arxiv.org/abs/2311.03624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Forero, Gilberto Bernardes, Mónica Mendes</li>
<li>for: 本研究旨在分析和讨论自动生成音乐的情感表达方法。</li>
<li>methods: 本研究使用了两种主要的方法：规则式模型和机器学习模型，其中深度学习模型可以自动生成高质量的音乐从文本描述中。</li>
<li>results: 研究发现，使用深度学习模型可以有效地表达情感，但是语言中的限制和模糊性可能会影响音乐的表达。不过，使用深度学习与自然语言结合可能会对创作业务产生深见的影响，提供powerful工具来提及和生成新的音乐作品。<details>
<summary>Abstract</summary>
Music has been commonly recognized as a means of expressing emotions. In this sense, an intense debate emerges from the need to verbalize musical emotions. This concern seems highly relevant today, considering the exponential growth of natural language processing using deep learning models where it is possible to prompt semantic propositions to generate music automatically. This scoping review aims to analyze and discuss the possibilities of music generation conditioned by emotions. To address this topic, we propose a historical perspective that encompasses the different disciplines and methods contributing to this topic. In detail, we review two main paradigms adopted in automatic music generation: rules-based and machine-learning models. Of note are the deep learning architectures that aim to generate high-fidelity music from textual descriptions. These models raise fundamental questions about the expressivity of music, including whether emotions can be represented with words or expressed through them. We conclude that overcoming the limitation and ambiguity of language to express emotions through music, some of the use of deep learning with natural language has the potential to impact the creative industries by providing powerful tools to prompt and generate new musical works.
</details>
<details>
<summary>摘要</summary>
音乐已被广泛认可为表达情感的工具。在这种情况下，有一场激烈的辩论是如何用语言表达音乐中的情感。这个问题在今天更加有 relevance，因为深度学习模型在自然语言处理领域的快速发展，可以通过提出 semantics 提案来自动生成音乐。本文综述分析和讨论了基于情感的音乐生成的可能性。为此，我们提出了历史背景，涵盖不同领域和方法对这个话题的贡献。具体来说，我们回顾了两种主要的自动音乐生成方法：规则驱动和机器学习模型。特别是深度学习架构，可以生成高级别的音乐从文本描述。这些模型提出了音乐表达的基本问题，包括情感是否可以通过语言表达，或者通过语言表达出来。我们结论认为，超越语言表达情感的限制和抽象，使用深度学习与自然语言的结合可能对创作业务产生强大的影响，提供了 poderful 的工具来促进和生成新的音乐作品。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Latent-Spaces-of-Tonal-Music-using-Variational-Autoencoders"><a href="#Exploring-Latent-Spaces-of-Tonal-Music-using-Variational-Autoencoders" class="headerlink" title="Exploring Latent Spaces of Tonal Music using Variational Autoencoders"></a>Exploring Latent Spaces of Tonal Music using Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03621">http://arxiv.org/abs/2311.03621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nadiacarvalho/latent-tonal-music">https://github.com/nadiacarvalho/latent-tonal-music</a></li>
<li>paper_authors: Nádia Carvalho, Gilberto Bernardes</li>
<li>for: 这项研究用 VaR 模型来生成含义和认知价值的秘密表示。</li>
<li>methods: 研究使用了不同的 VaR 模型Encodings（Piano Roll、MIDI、ABC、Tonnetz、DFT of pitch、pitch class distributions），并对这些编码的latent space进行评估，以确定它们是否能够准确地表示音乐认知空间。</li>
<li>results: 研究发现，使用 ABC 编码的 VaR 模型能够最好地重construct原始数据，而 Pitch DFT 编码则能够从latent space中提取更多的信息。此外，通过对12个主要或小调的转调进行对比，研究发现 VAE 模型的 latent space与认知空间之间存在很好的对应关系，并且可以用来评估不同关键之间的关系。<details>
<summary>Abstract</summary>
Variational Autoencoders (VAEs) have proven to be effective models for producing latent representations of cognitive and semantic value. We assess the degree to which VAEs trained on a prototypical tonal music corpus of 371 Bach's chorales define latent spaces representative of the circle of fifths and the hierarchical relation of each key component pitch as drawn in music cognition. In detail, we compare the latent space of different VAE corpus encodings -- Piano roll, MIDI, ABC, Tonnetz, DFT of pitch, and pitch class distributions -- in providing a pitch space for key relations that align with cognitive distances. We evaluate the model performance of these encodings using objective metrics to capture accuracy, mean square error (MSE), KL-divergence, and computational cost. The ABC encoding performs the best in reconstructing the original data, while the Pitch DFT seems to capture more information from the latent space. Furthermore, an objective evaluation of 12 major or minor transpositions per piece is adopted to quantify the alignment of 1) intra- and inter-segment distances per key and 2) the key distances to cognitive pitch spaces. Our results show that Pitch DFT VAE latent spaces align best with cognitive spaces and provide a common-tone space where overlapping objects within a key are fuzzy clusters, which impose a well-defined order of structural significance or stability -- i.e., a tonal hierarchy. Tonal hierarchies of different keys can be used to measure key distances and the relationships of their in-key components at multiple hierarchies (e.g., notes and chords). The implementation of our VAE and the encodings framework are made available online.
</details>
<details>
<summary>摘要</summary>
variational autoencoders (VAEs) 已经证明是有效的模型，用于生成含义和semantic value的秘密表示。我们评估了VAEs在371首巴赫 Chorales 中的含义空间是否与音乐认知中的圆形五度和每个关键组件折射之间的层次关系相对应。具体来说，我们比较了不同的 VAE 嵌入空间 -- Piano roll, MIDI, ABC, Tonnetz, DFT of pitch, 和折射分布 -- 在提供一个折射空间来处理关键之间的关系，并使用对象metric来衡量准确性、平方误差（MSE）、KL散度和计算成本。ABC嵌入空间表现最好地重建原始数据，而折射 DFT 则能够从含义空间中提取更多的信息。此外，我们采用对象评估方法，对每个关键进行12个主小调或小调轨迹的对比，以量化每个关键与认知折射空间之间的对应关系。我们的结果表明，折射 DFT VAE 含义空间最好地与认知空间相对应，并提供一个共同频谱空间，在这里，每个关键的内部对象在一个key中是混合的不确定集合，这种结构具有明确的顺序和稳定性 -- i.e.,  tonality hierarchy。不同关键的tonality hierarchy可以用来测量关键之间的距离和关键组件在多个层次（例如，音和和弦）之间的关系。我们的 VAE 和嵌入空间框架已经在线上实现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.LG_2023_11_07/" data-id="closbrosm00td0g88at5j5hv5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/07/cs.CL_2023_11_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-11-07
        
      </div>
    </a>
  
  
    <a href="/2023/11/07/eess.IV_2023_11_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-11-07</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
