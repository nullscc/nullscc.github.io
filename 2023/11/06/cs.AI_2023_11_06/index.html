
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.AI - 2023-11-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Exploitation-Guided Exploration for Semantic Embodied Navigation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03357 repo_url: None paper_authors: Justin Wasserman, Girish Chowdhary, Abhinav Gupta, Unnat Jain">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.AI - 2023-11-06">
<meta property="og:url" content="https://nullscc.github.io/2023/11/06/cs.AI_2023_11_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Exploitation-Guided Exploration for Semantic Embodied Navigation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03357 repo_url: None paper_authors: Justin Wasserman, Girish Chowdhary, Abhinav Gupta, Unnat Jain">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-06T12:00:00.000Z">
<meta property="article:modified_time" content="2023-11-07T17:04:04.257Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.AI_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.AI_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T12:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.AI - 2023-11-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Exploitation-Guided-Exploration-for-Semantic-Embodied-Navigation"><a href="#Exploitation-Guided-Exploration-for-Semantic-Embodied-Navigation" class="headerlink" title="Exploitation-Guided Exploration for Semantic Embodied Navigation"></a>Exploitation-Guided Exploration for Semantic Embodied Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03357">http://arxiv.org/abs/2311.03357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Wasserman, Girish Chowdhary, Abhinav Gupta, Unnat Jain</li>
<li>for: 本研究旨在提出一种原理性的模块合并方法，以提高embodied navigation和sim-to-robot transfer的性能。</li>
<li>methods: 本文提出了Exploitation-Guided Exploration（XGX）方法，其包括一个分离的探索模块和一个利用模块，两者在一种新的和直观的方式相互作用。在确定的最后步骤中，利用模块会取代探索模块，并驱动一个被 override 的政策优化。</li>
<li>results: XGX方法在对难 Navigation task的表现中提高了state-of-the-art的性能，从70%提高到73%。此外，通过targeted分析，我们还证明XGX在目标 conditional exploration中更有效率。最后，我们还进行了robot硬件上的实验，并证明XGX在实验中比best baseline更高出两倍。project page: xgxvisnav.github.io<details>
<summary>Abstract</summary>
In the recent progress in embodied navigation and sim-to-robot transfer, modular policies have emerged as a de facto framework. However, there is more to compositionality beyond the decomposition of the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Particularly, we propose Exploitation-Guided Exploration (XGX) where separate modules for exploration and exploitation come together in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation i.e. when the goal becomes visible. Crucially, an exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XGX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. Along with better accuracy, through targeted analysis, we show that XGX is also more efficient at goal-conditioned exploration. Finally, we show sim-to-real transfer to robot hardware and XGX performs over two-fold better than the best baseline from simulation benchmarking. Project page: xgxvisnav.github.io
</details>
<details>
<summary>摘要</summary>
最近的具体Navigation和sim-to-robot传输进展中，模块政策 emerged as a de facto framework。然而，这只是compose beyond the decomposition of the learning load into modular components。在这项工作中，我们调查了一种原则的方式来 syntax combination of these components。特别是，我们提出了Exploitation-Guided Exploration (XGX)，其中分配策略和探索策略在一种新和直观的方式结合。我们将探索模块配置为在决定性的最后步骤中接管 Navigation i.e. 当目标变得可见时。关键地，一个exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization。XGX，通过有效的分解和新的导航，提高了Object Navigation task的状态对��性从70%提高到73%.此外，通过targeted analysis，我们还表明XGX更有效率地进行goal-conditioned exploration。最后，我们展示了XGX在硬件robot上的实际传输和 simulation benchmarking中的超过两倍的性能提升。项目页面：xgxvisnav.github.io
</details></li>
</ul>
<hr>
<h2 id="SegGen-Supercharging-Segmentation-Models-with-Text2Mask-and-Mask2Img-Synthesis"><a href="#SegGen-Supercharging-Segmentation-Models-with-Text2Mask-and-Mask2Img-Synthesis" class="headerlink" title="SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis"></a>SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03355">http://arxiv.org/abs/2311.03355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prismformore/seggen">https://github.com/prismformore/seggen</a></li>
<li>paper_authors: Hanrong Ye, Jason Kuen, Qing Liu, Zhe Lin, Brian Price, Dan Xu</li>
<li>for: 提高图像分割模型的性能，特别是在 semantic segmentation、panoptic segmentation 和 instance segmentation 领域。</li>
<li>methods: 使用 MaskSyn 和 ImgSyn 两种数据生成策略，即文本到mask生成模型和mask到图像生成模型，以提高分割模型的supervision数据的多样性; 使用mask-to-image生成模型来生成新的图像基于现有的mask。</li>
<li>results: 在 ADE20K 和 COCO benchmark上，使用我们的数据生成方法可以明显提高 state-of-the-art 分割模型的性能，包括 semantic segmentation、panoptic segmentation 和 instance segmentation; 对 ADE20K mIoU，Mask2Former R50 的性能由47.2提高到49.9 (+2.7); Mask2Former Swin-L 也从56.1提高到57.4 (+1.3)。<details>
<summary>Abstract</summary>
We propose SegGen, a highly-effective training data generation method for image segmentation, which pushes the performance limits of state-of-the-art segmentation models to a significant extent. SegGen designs and integrates two data generation strategies: MaskSyn and ImgSyn. (i) MaskSyn synthesizes new mask-image pairs via our proposed text-to-mask generation model and mask-to-image generation model, greatly improving the diversity in segmentation masks for model supervision; (ii) ImgSyn synthesizes new images based on existing masks using the mask-to-image generation model, strongly improving image diversity for model inputs. On the highly competitive ADE20K and COCO benchmarks, our data generation method markedly improves the performance of state-of-the-art segmentation models in semantic segmentation, panoptic segmentation, and instance segmentation. Notably, in terms of the ADE20K mIoU, Mask2Former R50 is largely boosted from 47.2 to 49.9 (+2.7); Mask2Former Swin-L is also significantly increased from 56.1 to 57.4 (+1.3). These promising results strongly suggest the effectiveness of our SegGen even when abundant human-annotated training data is utilized. Moreover, training with our synthetic data makes the segmentation models more robust towards unseen domains. Project website: https://seggenerator.github.io
</details>
<details>
<summary>摘要</summary>
我们提出了SegGen，一种高效的训练数据生成方法 для图像分割，可以大幅提高现有状态最优分割模型的性能。SegGen设计并集成了两种数据生成策略：MaskSyn和ImgSyn。（一）MaskSyn通过我们提议的文本到mask生成模型和mask到图像生成模型，可以大幅提高分割面精度和多样性。（二）ImgSyn通过使用现有的mask来生成新图像，可以强化图像多样性。在ADE20K和COCO标准测试 benchmark上，我们的数据生成方法对现有的分割模型进行了显著改进，包括semantic segmentation、panoptic segmentation和instance segmentation。特别是在ADE20K mIoU上，Mask2Former R50的性能从47.2提高到49.9（+2.7），Mask2Former Swin-L的性能也从56.1提高到57.4（+1.3）。这些出色的结果表明我们的SegGen在有限的人工标注数据上可以获得显著改进。此外，使用我们生成的 sintetic数据来训练分割模型可以使其更加鲁棒，对于未看到的领域。项目官方网站：https://seggenerator.github.io
</details></li>
</ul>
<hr>
<h2 id="GLaMM-Pixel-Grounding-Large-Multimodal-Model"><a href="#GLaMM-Pixel-Grounding-Large-Multimodal-Model" class="headerlink" title="GLaMM: Pixel Grounding Large Multimodal Model"></a>GLaMM: Pixel Grounding Large Multimodal Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03356">http://arxiv.org/abs/2311.03356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan Yang, Fahad S. Khan</li>
<li>for: This paper aims to generate natural language responses that are seamlessly intertwined with corresponding object segmentation masks, enabling users to interact with the model at various levels of granularity in both textual and visual domains.</li>
<li>methods: The proposed Grounding LMM (GLaMM) model uses region-level LMMs to generate visually grounded responses, and it can accept both textual and visual prompts as input. The model is evaluated on a comprehensive evaluation protocol with a densely annotated dataset called GranD, which includes 7.5M unique concepts grounded in a total of 810M regions.</li>
<li>results: GLaMM achieves state-of-the-art performance on several downstream tasks, including referring expression segmentation, image and region-level captioning, and vision-language conversations. The model is also able to generate visually grounded conversations that are more detailed and accurate than previous models.<details>
<summary>Abstract</summary>
Large Multimodal Models (LMMs) extend Large Language Models to the vision domain. Initial efforts towards LMMs used holistic images and text prompts to generate ungrounded textual responses. Very recently, region-level LMMs have been used to generate visually grounded responses. However, they are limited to only referring a single object category at a time, require users to specify the regions in inputs, or cannot offer dense pixel-wise object grounding. In this work, we present Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks. GLaMM not only grounds objects appearing in the conversations but is flexible enough to accept both textual and optional visual prompts (region of interest) as input. This empowers users to interact with the model at various levels of granularity, both in textual and visual domains. Due to the lack of standard benchmarks for the novel setting of generating visually grounded detailed conversations, we introduce a comprehensive evaluation protocol with our curated grounded conversations. Our proposed Grounded Conversation Generation (GCG) task requires densely grounded concepts in natural scenes at a large-scale. To this end, we propose a densely annotated Grounding-anything Dataset (GranD) using our proposed automated annotation pipeline that encompasses 7.5M unique concepts grounded in a total of 810M regions available with segmentation masks. Besides GCG, GLaMM also performs effectively on several downstream tasks e.g., referring expression segmentation, image and region-level captioning and vision-language conversations. Project Page: https://mbzuai-oryx.github.io/groundingLMM.
</details>
<details>
<summary>摘要</summary>
大型多模型（LMM）扩展了大型语言模型到视觉领域。初期尝试使用整体图像和文本提示生成不关联的文本回应。然而，最近的区域级LMM已经用于生成相关的回应。不过，它们只能同时引用一个物体类别，需要用户在输入中指定区域，或者无法提供密集像素精度的物体固定。在这项工作中，我们提出了固定语言回应和相应物体分割面的模型——GLaMM。GLaMM不仅可以在对话中固定物体，而且可以接受文本和可选的视觉提示（区域兴趣点）作为输入。这使得用户可以在文本和视觉领域中与模型进行互动，并且可以在不同的级别进行互动。由于不存在对生成视觉精度讨论的标准评价准则，我们提出了一种完整的评价协议，并针对这种新的设定生成了一个大规模的精度讨论数据集（GCG）。我们的提案的Grounded Conversation Generation（GCG）任务需要在自然场景中 densely grounded的概念，并且需要在大规模的场景中进行评价。为此，我们提出了一个名为Grounding-anything Dataset（GranD）的 densely annotated数据集，该数据集包含7.5万个不同的概念，并且在810万个区域中进行了分割面的注释。除了GCG，GLaMM还在多个下游任务中表现出色，如图像和区域水平的描述、图像和区域水平的标题生成和视觉语言对话等。项目页面：https://mbzuai-oryx.github.io/groundingLMM。
</details></li>
</ul>
<hr>
<h2 id="Scalable-and-Transferable-Black-Box-Jailbreaks-for-Language-Models-via-Persona-Modulation"><a href="#Scalable-and-Transferable-Black-Box-Jailbreaks-for-Language-Models-via-Persona-Modulation" class="headerlink" title="Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation"></a>Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03348">http://arxiv.org/abs/2311.03348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rusheb Shah, Quentin Feuillade–Montixi, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando</li>
<li>for:  investigate persona modulation as a black-box jailbreaking method to steer a target model to take on personalities that are willing to comply with harmful instructions</li>
<li>methods: automate the generation of jailbreaks using a language model assistant</li>
<li>results: achieve a harmful completion rate of 42.5% in GPT-4, which is 185 times larger than before modulation (0.23%); also transfer to Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%, respectively<details>
<summary>Abstract</summary>
Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour. In this work, we investigate persona modulation as a black-box jailbreaking method to steer a target model to take on personalities that are willing to comply with harmful instructions. Rather than manually crafting prompts for each persona, we automate the generation of jailbreaks using a language model assistant. We demonstrate a range of harmful completions made possible by persona modulation, including detailed instructions for synthesising methamphetamine, building a bomb, and laundering money. These automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is 185 times larger than before modulation (0.23%). These prompts also transfer to Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%, respectively. Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.
</details>
<details>
<summary>摘要</summary>
尽管努力将大语言模型调整为生成无害回复，它们仍然容易受到劫夺提示的影响，导致模型生成不良行为。在这项工作中，我们 investigate persona modulation 作为黑盒劫夺方法，以控制目标模型的行为。而不是手动制作每个人物的提示，我们使用语言模型助手自动生成劫夺。我们示出了由 persona modulation 引起的各种危险 completions，包括合成吸引剂、制造炸弹和洗钱等详细指导。这些自动攻击在 GPT-4 中达到了42.5%的危险完成率，比之前的模ulation（0.23%）高185倍。这些提示还可以在 Claude 2 和 Vicuna 上进行危险完成，即61.0%和35.9%。我们的工作揭示了商业大语言模型又一个漏洞，并高亮了更多的安全措施的需要。
</details></li>
</ul>
<hr>
<h2 id="Embedding-First-Order-Logic-into-Kernel-Machines"><a href="#Embedding-First-Order-Logic-into-Kernel-Machines" class="headerlink" title="Embedding First Order Logic into Kernel Machines"></a>Embedding First Order Logic into Kernel Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03340">http://arxiv.org/abs/2311.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michelangelo Diligenti, Marco Gori, Marco Maggini, Leonardo Rigutini</li>
<li>for: 本文提出了一种整合supervised和Unsupervised示例以及背景知识的核心机器学习框架。</li>
<li>methods: 本文使用多任务学习方案，其中多个predicate定义在一组对象上是通过 jointly 学习从示例中学习的。这些predicate可以是known priori或通过适当的kernel-based学习器来approximate。</li>
<li>results: 本文提出了一种将逻辑法则转换为连续实现的方法，以便处理由kernel-based predicates生成的输出。学习问题被формаulated为一个半监督学习任务，其中需要优化 primal 中的损失函数，该损失函数结合了监督例子上的适应损失函数、正则化项和约束项。然而，约束项不是凸函数，这可能会阻碍优化过程。本文提出了一种two stage learning schema，其中首先学习supervised例子，然后强制执行约束。<details>
<summary>Abstract</summary>
In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a penalty term that enforces the constraints on both the supervised and unsupervised examples. Unfortunately, the penalty term is not convex and it can hinder the optimization process. However, it is possible to avoid poor solutions by using a two stage learning schema, in which the supervised examples are learned first and then the constraints are enforced.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个总体框架，用于将监督和无监督示例集合Background Knowledge表示为一个集合First-Order Logic Clauses（FOL）中的扩展。特别是，我们考虑了一种多任务学习方案，其中多个定义在一组对象上的预测符需要同时学习从示例集合中，并且对预测符的值进行FOL约束。这些预测符定义在特征空间中，其中输入对象被表示，并且可以是先知的或者通过适当的核心学习器来估算。我们提出了一种抽象转换FOL条件为连续实现的总体方法，以便与核心学习器计算的输出进行交互。学习问题被定义为一种半监督学习问题，其中需要优化飞行函数的极值，该函数组合监督示例集合中的适应损失函数、规则化项和约束项。然而，约束项不是凸函数，可能会干扰优化过程。但是，我们可以通过两个阶段学习策略来避免差解，其中首先学习监督示例集合中的示例，然后对约束进行执行。
</details></li>
</ul>
<hr>
<h2 id="FLOGA-A-machine-learning-ready-dataset-a-benchmark-and-a-novel-deep-learning-model-for-burnt-area-mapping-with-Sentinel-2"><a href="#FLOGA-A-machine-learning-ready-dataset-a-benchmark-and-a-novel-deep-learning-model-for-burnt-area-mapping-with-Sentinel-2" class="headerlink" title="FLOGA: A machine learning ready dataset, a benchmark and a novel deep learning model for burnt area mapping with Sentinel-2"></a>FLOGA: A machine learning ready dataset, a benchmark and a novel deep learning model for burnt area mapping with Sentinel-2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03339">http://arxiv.org/abs/2311.03339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Sdraka, Alkinoos Dimakos, Alexandros Malounis, Zisoula Ntasiou, Konstantinos Karantzalos, Dimitrios Michail, Ioannis Papoutsis<br>for:The paper aims to provide an accurate and timely mapping of wildfire-affected areas using satellite imagery and Machine Learning techniques.methods:The authors use a combination of Sentinel-2 and MODIS satellite imagery with variable spatial and spectral resolution, and employ multiple Machine Learning and Deep Learning algorithms for change detection and burnt area mapping.results:The proposed Deep Learning model, BAM-CD, outperforms all other methods in terms of accuracy and robustness, and provides an effective solution for the automatic extraction of burnt areas.<details>
<summary>Abstract</summary>
Over the last decade there has been an increasing frequency and intensity of wildfires across the globe, posing significant threats to human and animal lives, ecosystems, and socio-economic stability. Therefore urgent action is required to mitigate their devastating impact and safeguard Earth's natural resources. Robust Machine Learning methods combined with the abundance of high-resolution satellite imagery can provide accurate and timely mappings of the affected area in order to assess the scale of the event, identify the impacted assets and prioritize and allocate resources effectively for the proper restoration of the damaged region. In this work, we create and introduce a machine-learning ready dataset we name FLOGA (Forest wiLdfire Observations for the Greek Area). This dataset is unique as it comprises of satellite imagery acquired before and after a wildfire event, it contains information from Sentinel-2 and MODIS modalities with variable spatial and spectral resolution, and contains a large number of events where the corresponding burnt area ground truth has been annotated by domain experts. FLOGA covers the wider region of Greece, which is characterized by a Mediterranean landscape and climatic conditions. We use FLOGA to provide a thorough comparison of multiple Machine Learning and Deep Learning algorithms for the automatic extraction of burnt areas, approached as a change detection task. We also compare the results to those obtained using standard specialized spectral indices for burnt area mapping. Finally, we propose a novel Deep Learning model, namely BAM-CD. Our benchmark results demonstrate the efficacy of the proposed technique in the automatic extraction of burnt areas, outperforming all other methods in terms of accuracy and robustness. Our dataset and code are publicly available at: https://github.com/Orion-AI-Lab/FLOGA.
</details>
<details>
<summary>摘要</summary>
随着过去的一个十年，全球受到野火的影响逐渐增加，对人类和动物生命，生态系统以及经济稳定造成了重大威胁。因此，我们需要立即采取行动，以遏制其恶势力并保护地球的自然资源。高效的机器学习方法，结合高分辨率卫星图像的丰富存在，可以为评估事件规模、确定影响资产和有效分配资源而提供准确和时效的映射。在这种工作中，我们创建了一个名为FLOGA（希腊地区森林野火观测）的机器学习准备数据集。FLOGA数据集独特之处在于它包括了在野火事件前后获得的卫星图像信息，其中包括Sentinel-2和MODIS模式，并且包含大量已经由领域专家标注的烧毁地区的ground truth。FLOGA覆盖了希腊更广阔的地区，其地中地中海气候和地貌特征。我们使用FLOGA数据集，对多种机器学习和深度学习算法进行自动烧毁区域抽取的比较研究。我们还将其与特有 spectral indices 的烧毁地区映射方法进行比较。最后，我们提出了一种新的深度学习模型，即BAM-CD。我们的参考结果表明，提议的方法在自动烧毁区域抽取方面具有高度的准确性和可靠性。我们的数据集和代码在：https://github.com/Orion-AI-Lab/FLOGA 可以进行下载。
</details></li>
</ul>
<hr>
<h2 id="DAIL-Data-Augmentation-for-In-Context-Learning-via-Self-Paraphrase"><a href="#DAIL-Data-Augmentation-for-In-Context-Learning-via-Self-Paraphrase" class="headerlink" title="DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase"></a>DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03319">http://arxiv.org/abs/2311.03319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawei Li, Yaxuan Li, Dheeraj Mekala, Shuyao Li, Yulin wang, Xueqi Wang, William Hogan, Jingbo Shang</li>
<li>for: 提高受限资源下的自然语言处理任务（NLP）的性能</li>
<li>methods: 使用增强的大语言模型和增强学习（ICL）方法，并利用自己生成的内容来生成补充数据</li>
<li>results: 在受限资源下，DAIL方法比标准ICL方法和其他集成方法表现出更高的性能，并且可以用投票一致性作为模型的信任度指标<details>
<summary>Abstract</summary>
In-Context Learning (ICL) combined with pre-trained large language models has achieved promising results on various NLP tasks. However, ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios. To overcome this limitation, we propose \textbf{D}ata \textbf{A}ugmentation for \textbf{I}n-Context \textbf{L}earning (\textbf{DAIL}). DAIL leverages the intuition that large language models are more familiar with the content generated by themselves. It first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions. Our extensive empirical evaluation shows that DAIL outperforms the standard ICL method and other ensemble-based methods in the low-resource scenario. Additionally, we explore the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible. We believe our work will stimulate further research on ICL in low-resource settings.
</details>
<details>
<summary>摘要</summary>
团 Context Learning（ICL）与预训练大语言模型结合实现了多种自然语言处理任务的出色结果。然而，ICL需要高质量的注释示例，这可能在实际场景中不可获得。为了解决这个限制，我们提议使用数据增强 для团 Context Learning（DAIL）。 DAIL 利用大语言模型对自己生成的内容更加熟悉的想法，首先使用语言模型生成测试样本的重写，然后通过多数投票确定基于个体预测的最终结果。我们的广泛的实验证明，DAIL 在低资源情况下超越标准 IC 方法和其他集成方法。此外，我们还探讨了使用投票一致性作为模型的信任度指标，当预测的 logits 不可访问时。我们认为我们的工作将激发更多关于 IC 在低资源情况下的研究。
</details></li>
</ul>
<hr>
<h2 id="Neural-Structure-Learning-with-Stochastic-Differential-Equations"><a href="#Neural-Structure-Learning-with-Stochastic-Differential-Equations" class="headerlink" title="Neural Structure Learning with Stochastic Differential Equations"></a>Neural Structure Learning with Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03309">http://arxiv.org/abs/2311.03309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjie Wang, Joel Jennings, Wenbo Gong</li>
<li>for: 本研究旨在解决longstanding challenge在数学科学中，即从 temporal observations 中探测变量之间的下面关系。</li>
<li>methods: 本文提出了一种新的结构学习方法，即SCOTCH，它将神经统计 diferencial equations (SDE) 与变量推理相结合，以估计可能的结构 posterior distribution。</li>
<li>results: 在synthetic和实际数据上，本方法与相关基eline比较，得到了改善的结构学习表现，并且可以正确地捕捉到不规则的观测时间点。<details>
<summary>Abstract</summary>
Discovering the underlying relationships among variables from temporal observations has been a longstanding challenge in numerous scientific disciplines, including biology, finance, and climate science. The dynamics of such systems are often best described using continuous-time stochastic processes. Unfortunately, most existing structure learning approaches assume that the underlying process evolves in discrete-time and/or observations occur at regular time intervals. These mismatched assumptions can often lead to incorrect learned structures and models. In this work, we introduce a novel structure learning method, SCOTCH, which combines neural stochastic differential equations (SDE) with variational inference to infer a posterior distribution over possible structures. This continuous-time approach can naturally handle both learning from and predicting observations at arbitrary time points. Theoretically, we establish sufficient conditions for an SDE and SCOTCH to be structurally identifiable, and prove its consistency under infinite data limits. Empirically, we demonstrate that our approach leads to improved structure learning performance on both synthetic and real-world datasets compared to relevant baselines under regular and irregular sampling intervals.
</details>
<details>
<summary>摘要</summary>
发现变量之间的下面关系从暂时观察中获得信息是许多科学领域的长期挑战，包括生物、金融和气候科学。这些系统的动态通常最好使用连续时间杂相过程来描述。然而，大多数现有结构学习方法假设下面过程发展在离散时间和/或观察发生在固定时间间隔。这些不一致的假设可能会导致错误的学习结构和模型。在这项工作中，我们介绍了一种新的结构学习方法，即SCOTCH，它将神经杂相准则（SDE）与变量抽象推理相结合，以推断可能的结构 posterior 分布。这种连续时间方法可以自然地处理从和预测观察的任意时间点进行学习和预测。理论上，我们确立了SDE和SCOTCH的结构可识别性Conditions，并证明其在无穷数据极限下的一致性。实际上，我们通过对 sintetic和实际数据进行比较，证明了我们的方法在不同的抽取间隔下的性能提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Reusable-Manipulation-Strategies"><a href="#Learning-Reusable-Manipulation-Strategies" class="headerlink" title="Learning Reusable Manipulation Strategies"></a>Learning Reusable Manipulation Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03293">http://arxiv.org/abs/2311.03293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Mao, Joshua B. Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling</li>
<li>for: 学习和总结夹用”技巧”，包括使用不同物品位置、大小和类型。</li>
<li>methods: 通过单个示例学习和自我玩家自动学习机制，并将学习的细节槽插入标准任务和运动规划中。</li>
<li>results: 机器人可以通过单个示例学习并自动执行各种 manipulate 任务，包括使用不同的工具和策略。<details>
<summary>Abstract</summary>
Humans demonstrate an impressive ability to acquire and generalize manipulation "tricks." Even from a single demonstration, such as using soup ladles to reach for distant objects, we can apply this skill to new scenarios involving different object positions, sizes, and categories (e.g., forks and hammers). Additionally, we can flexibly combine various skills to devise long-term plans. In this paper, we present a framework that enables machines to acquire such manipulation skills, referred to as "mechanisms," through a single demonstration and self-play. Our key insight lies in interpreting each demonstration as a sequence of changes in robot-object and object-object contact modes, which provides a scaffold for learning detailed samplers for continuous parameters. These learned mechanisms and samplers can be seamlessly integrated into standard task and motion planners, enabling their compositional use.
</details>
<details>
<summary>摘要</summary>
人类具有吸引人的物理 manipulation "技巧" 学习能力。从单一示例，如使用汤匙来达到远距离物体，我们可以将这种技能应用到新的情况中，包括不同的物体位置、大小和类别（例如用叶和锤子）。此外，我们可以flexibly组合不同的技能来制定长期计划。在这篇论文中，我们提出了一种框架，允许机器通过单一示例和自己游戏来学习 manipulate 技能，称为"机制"。我们的关键发现在于将每个示例解释为机器人对物体和物体之间的接触方式的序列变化，从而提供了一个支持学习细致的抽象器的框架。这些学习的机制和抽象器可以轻松地与标准任务和运动规划器集成，以实现其可 compose 使用。
</details></li>
</ul>
<hr>
<h2 id="S-LoRA-Serving-Thousands-of-Concurrent-LoRA-Adapters"><a href="#S-LoRA-Serving-Thousands-of-Concurrent-LoRA-Adapters" class="headerlink" title="S-LoRA: Serving Thousands of Concurrent LoRA Adapters"></a>S-LoRA: Serving Thousands of Concurrent LoRA Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03285">http://arxiv.org/abs/2311.03285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/S-LoRA/S-LoRA">https://github.com/S-LoRA/S-LoRA</a></li>
<li>paper_authors: Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica</li>
<li>for: 这个研究旨在开发一个可扩展的服务系统，以便大规模执行多个任务特定的精细定制化模型。</li>
<li>methods: 这个研究使用了“pretrain-then-finetune”方法，并且将LoRA（Low-Rank Adaptation）作为精致化方法。LoRA可以将基本模型adaspt到多个任务上，从而生成了许多LoRA拓展器。</li>
<li>results: 这个研究发现，使用S-LoRA系统可以在单一GPU上服务 thousands个LoRA拓展器，并且可以提高比较预设的HuggingFace PEFT和vLLM库的吞吐量，并可以增加服务的精细定制化模型数量。这使得S-LoRA可以提供大规模的定制化服务。<details>
<summary>Abstract</summary>
The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services.
</details>
<details>
<summary>摘要</summary>
“将基模型先训练后调整”的方法在大型语言模型的部署中广泛使用。低维度适应（LoRA），一种优化缓存使用率的调整方法，常用来对多个任务进行适应，从而产生了许多LoRA调整器。我们发现，这个方法在服务时期具有重要的批处理机会。为了利用这些机会，我们介绍了S-LoRA，一个可扩展的服务系统，可以实现多个LoRA调整器的扩展服务。S-LoRA将所有调整器存储在主内存中，并将用户端查询中的调整器载入到GPU内存中。为了有效利用GPU内存和减少分解，S-LoRA提出了一个统一的当值系统，将动态调整器的重量和KV库缓存中的序列长度管理在一个统一的内存池中。此外，S-LoRA还使用了一种新的tensor并行架构和高度优化的自定义CUDA核心函数，以进行heterogeneous批处理。总之，这些特点使得S-LoRA可以在单一GPU或多个GPU上服务千多个LoRA调整器，只需要轻微的负载。相比之下，现有的库如HuggingFace PEFT和vLLM（具有原生支持LoRA服务），S-LoRA可以提高吞务率来到4倍，并增加服务的调整器数量至数十个数量级。因此，S-LoRA实现了可扩展的服务多个任务特定的精确化服务，并提供了大规模自定义精确化服务的潜力。
</details></li>
</ul>
<hr>
<h2 id="From-Coupled-Oscillators-to-Graph-Neural-Networks-Reducing-Over-smoothing-via-a-Kuramoto-Model-based-Approach"><a href="#From-Coupled-Oscillators-to-Graph-Neural-Networks-Reducing-Over-smoothing-via-a-Kuramoto-Model-based-Approach" class="headerlink" title="From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach"></a>From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03260">http://arxiv.org/abs/2311.03260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Nguyen, Tan M. Nguyen, Hirotada Honda, Takashi Sano, Vinh Nguyen, Shugo Nakamura</li>
<li>for: 针对Graph Neural Networks (GNNs) 中的过度整合现象，提出了 Kuramoto Graph Neural Network (KuramotoGNN)，一种新的连续深度 GNN 类型。</li>
<li>methods: KuramotoGNN 使用 Kuramoto 模型来 Mitigate 过度整合现象，该模型捕捉了非线性吸引器的同步行为。</li>
<li>results: 对多种图深度学习benchmark任务进行实验，显示 KuramotoGNN 可以效果地降低过度整合现象，而且比基eline GNN 和现有方法更好。<details>
<summary>Abstract</summary>
We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various graph deep learning benchmark tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了kuramoto图 neuronal网络（KuramotoGNN），一种新的连续深度图神经网络（GNN），它利用kuramoto模型来 mitigate the over-smoothing phenomenon，在这个现象中，GNN中的节点特征会在层数增加时变得无法分辨。kuramoto模型捕捉了非线性相互频率振荡器的同步行为。从拓扑学角度来看，我们首先解释了kuramoto模型与基本GNN之间的连接，然后解释了GNN中的过滤现象可以被看作kuramoto模型中的相位同步现象。KuramotoGNN将这种相位同步现象替换为频率同步，以防止节点特征相互混同，同时允许系统达到稳定同步状态。我们通过实验证明了KuramotoGNN在不同的图深度学 benchmark任务上的优势，比如降低过滤现象。
</details></li>
</ul>
<hr>
<h2 id="Coherent-Entity-Disambiguation-via-Modeling-Topic-and-Categorical-Dependency"><a href="#Coherent-Entity-Disambiguation-via-Modeling-Topic-and-Categorical-Dependency" class="headerlink" title="Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency"></a>Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03253">http://arxiv.org/abs/2311.03253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilin Xiao, Linjun Shou, Xingyao Zhang, Jie Wu, Ming Gong, Jian Pei, Daxin Jiang</li>
<li>for: 提高Entity Disambiguation（ED）系统的准确率和可owiezaibility（coherence）。</li>
<li>methods: 提出了一种基于Variational Autoencoder（VAE）和外部分类记忆的ED方法，以提高Entity predictions的coherence。</li>
<li>results: 实验结果显示，该方法在Popular ED benchmarks上达到了新的State-of-the-art Results，特别是在长文本场景下表现出色。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Previous entity disambiguation (ED) methods adopt a discriminative paradigm, where prediction is made based on matching scores between mention context and candidate entities using length-limited encoders. However, these methods often struggle to capture explicit discourse-level dependencies, resulting in incoherent predictions at the abstract level (e.g. topic or category). We propose CoherentED, an ED system equipped with novel designs aimed at enhancing the coherence of entity predictions. Our method first introduces an unsupervised variational autoencoder (VAE) to extract latent topic vectors of context sentences. This approach not only allows the encoder to handle longer documents more effectively, conserves valuable input space, but also keeps a topic-level coherence. Additionally, we incorporate an external category memory, enabling the system to retrieve relevant categories for undecided mentions. By employing step-by-step entity decisions, this design facilitates the modeling of entity-entity interactions, thereby maintaining maximum coherence at the category level. We achieve new state-of-the-art results on popular ED benchmarks, with an average improvement of 1.3 F1 points. Our model demonstrates particularly outstanding performance on challenging long-text scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Instructed-Language-Models-with-Retrievers-Are-Powerful-Entity-Linkers"><a href="#Instructed-Language-Models-with-Retrievers-Are-Powerful-Entity-Linkers" class="headerlink" title="Instructed Language Models with Retrievers Are Powerful Entity Linkers"></a>Instructed Language Models with Retrievers Are Powerful Entity Linkers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03250">http://arxiv.org/abs/2311.03250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrzilinxiao/insgenentitylinking">https://github.com/mrzilinxiao/insgenentitylinking</a></li>
<li>paper_authors: Zilin Xiao, Ming Gong, Jie Wu, Xingyao Zhang, Linjun Shou, Jian Pei, Daxin Jiang</li>
<li>for: 这个论文的目的是提高大语言模型（LLM）能够进行实体连接（EL）任务，并且可以在大量知识库中提供精确的实体预测。</li>
<li>methods: 这篇论文提出了一些使用语言模型执行EL任务的方法，包括将EL任务作为一个序列到序列的训练目标进行语言模型的 instrucion-tuning，以及一种基于轻量级的潜在提及检索器的新的生成EL框架，这种框架可以在不需要重复和非平行化解码的情况下实现4倍的速度提升。</li>
<li>results:  compared to先前的生成方法，INSGENEL可以获得6.8倍的F1分数平均提升，同时在训练数据效率和训练计算消耗方面也有很大的优势。此外，论文还发现，通过针对EL任务进行精心工程的内容学习（ICL）框架，可以进一步提高EL任务的性能。<details>
<summary>Abstract</summary>
Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods to equip language models with EL capability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4$\times$ speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully engineered in-context learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs.
</details>
<details>
<summary>摘要</summary>
生成方法，受大型语言模型（LLM）的推动，在需要复杂逻辑能力的任务中表现出emergent能力。然而，生成性仍然使得生成的内容受到幻觉的困扰，因此不适用于实体关注任务（EL），需要在大量知识库中准确预测实体。我们提出了首个使得习语言模型执行实体关注任务的方法——指令生成实体链接器（INSGENEL）。我们提出了多种方法来让语言模型拥有EL能力，包括（i）将EL作为目标进行序列到序列训练，并在 instruciton-tuning 中进行调整，（ii）基于轻量级的潜在提取器来实现生成EL框架，从而实现4倍的速度提升而无需牺牲链接度量。INSGENEL在前一代生成方法上表现出+6.8个F1分的提升均值，同时具有较少的训练数据和训练计算资源的优势。此外，我们的巧妙设计的内容学习（ICL）框架也落后于INSGENEL，这再次证明了EL任务仍然是普遍的LLM难点。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Post-Hoc-Case-Based-Explanation-with-Feature-Highlighting"><a href="#Advancing-Post-Hoc-Case-Based-Explanation-with-Feature-Highlighting" class="headerlink" title="Advancing Post Hoc Case Based Explanation with Feature Highlighting"></a>Advancing Post Hoc Case Based Explanation with Feature Highlighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03246">http://arxiv.org/abs/2311.03246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eoin Kenny, Eoin Delaney, Mark Keane</li>
<li>For: 该论文提出了一种用于协助人类和AI合作的可解释AI（XAI）技术，以帮助解释黑obox AI 系统的预测结果。* Methods: 该论文提出了两种通用算法（幽默和超像素基于算法），可以在测试图像中孤立多个清晰特征部分，然后将其与训练数据中的相关案例相连接，以提供更全面的解释。* Results: 研究发现，该方法可以在实际世界数据集上，对于不确定分类结果，让用户感到满意度得到了改善，而不是只显示解释而无法高亮特征。<details>
<summary>Abstract</summary>
Explainable AI (XAI) has been proposed as a valuable tool to assist in downstream tasks involving human and AI collaboration. Perhaps the most psychologically valid XAI techniques are case based approaches which display 'whole' exemplars to explain the predictions of black box AI systems. However, for such post hoc XAI methods dealing with images, there has been no attempt to improve their scope by using multiple clear feature 'parts' of the images to explain the predictions while linking back to relevant cases in the training data, thus allowing for more comprehensive explanations that are faithful to the underlying model. Here, we address this gap by proposing two general algorithms (latent and super pixel based) which can isolate multiple clear feature parts in a test image, and then connect them to the explanatory cases found in the training data, before testing their effectiveness in a carefully designed user study. Results demonstrate that the proposed approach appropriately calibrates a users feelings of 'correctness' for ambiguous classifications in real world data on the ImageNet dataset, an effect which does not happen when just showing the explanation without feature highlighting.
</details>
<details>
<summary>摘要</summary>
Explainable AI（XAI）已被提议为在人类和AI合作下的有价值工具。可能最有心理效果的XAI技术是case基本方法，这些方法在显示“整个”例子来解释黑盒AI系统的预测。然而，对于这些后期XAI方法来处理图像，没有尝试使用多个清晰特征“部分”来解释预测，并将其联系回相关的训练数据中的案例，从而为更全面的解释提供了更多的心理效果。在这里，我们解决了这个差距，并提出了两种通用算法（潜在和超Pixel基于），它们可以在测试图像中分解出多个清晰特征部分，然后将其联系回训练数据中的解释案例，并在一个仔细设计的用户研究中测试其有效性。结果表明，我们的方法可以在实际世界数据集上的ImageNet上适当调整用户对不确定分类的情感，这种效果不会发生只显示解释而没有特征高亮。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Self-Supervised-Cross-View-Training-For-Sentence-Embedding"><a href="#An-Efficient-Self-Supervised-Cross-View-Training-For-Sentence-Embedding" class="headerlink" title="An Efficient Self-Supervised Cross-View Training For Sentence Embedding"></a>An Efficient Self-Supervised Cross-View Training For Sentence Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03228">http://arxiv.org/abs/2311.03228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrpeerat/sct">https://github.com/mrpeerat/sct</a></li>
<li>paper_authors: Peerat Limkonchotiwat, Wuttikorn Ponwitayarat, Lalita Lowphansirikul, Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Sarana Nutanong</li>
<li>for: constructing an embedding space for sentences without relying on human annotation efforts</li>
<li>methods: Self-supervised Cross-View Training (SCT) framework</li>
<li>results: outperforms state-of-the-art competitors for PLMs with less than 100M parameters in 18 of 21 cases.<details>
<summary>Abstract</summary>
Self-supervised sentence representation learning is the task of constructing an embedding space for sentences without relying on human annotation efforts. One straightforward approach is to finetune a pretrained language model (PLM) with a representation learning method such as contrastive learning. While this approach achieves impressive performance on larger PLMs, the performance rapidly degrades as the number of parameters decreases. In this paper, we propose a framework called Self-supervised Cross-View Training (SCT) to narrow the performance gap between large and small PLMs. To evaluate the effectiveness of SCT, we compare it to 5 baseline and state-of-the-art competitors on seven Semantic Textual Similarity (STS) benchmarks using 5 PLMs with the number of parameters ranging from 4M to 340M. The experimental results show that STC outperforms the competitors for PLMs with less than 100M parameters in 18 of 21 cases.
</details>
<details>
<summary>摘要</summary>
自我监督句子表示学习是构建句子嵌入空间的任务，无需人类标注努力。一种直观的方法是通过对预训练语言模型（PLM）进行更新和表示学习方法，如对比学习。然而，这种方法在PLM的参数数量减少时表现迅速下降。在这篇论文中，我们提出了一种名为自动cross-view训练（SCT）的框架，以减少大小PLM和大PLM之间的性能差距。为评估SCT的效果，我们与5个基线和现状竞争对手进行比较，在7个semantic textual similarity（STS）标准benchmark上使用5个PLM，其中参数数量从4M到340M。实验结果表明，SCT在PLM参数数量少于100M的18个情况中比其他竞争对手表现更好。
</details></li>
</ul>
<hr>
<h2 id="LDM3D-VR-Latent-Diffusion-Model-for-3D-VR"><a href="#LDM3D-VR-Latent-Diffusion-Model-for-3D-VR" class="headerlink" title="LDM3D-VR: Latent Diffusion Model for 3D VR"></a>LDM3D-VR: Latent Diffusion Model for 3D VR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03226">http://arxiv.org/abs/2311.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriela Ben Melech Stan, Diana Wofk, Estelle Aflalo, Shao-Yen Tseng, Zhipeng Cai, Michael Paulitsch, Vasudev Lal</li>
<li>for: 这个研究旨在提供一个基于潜在扩散模型的虚拟现实开发套件（LDM3D-VR），包括LDM3D-pano和LDM3D-SR两种扩散模型。</li>
<li>methods: 这些模型基于现有预训模型，并在包含托架照片、深度地图和描述的数据集上进行了精细调整。</li>
<li>results: 这些模型可以从文本提示中生成圆锥照片和高分辨率RGBD图像，并且可以将低分辨率的输入提升到高分辨率RGBD图像。<details>
<summary>Abstract</summary>
Latent diffusion models have proven to be state-of-the-art in the creation and manipulation of visual outputs. However, as far as we know, the generation of depth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite of diffusion models targeting virtual reality development that includes LDM3D-pano and LDM3D-SR. These models enable the generation of panoramic RGBD based on textual prompts and the upscaling of low-resolution inputs to high-resolution RGBD, respectively. Our models are fine-tuned from existing pretrained models on datasets containing panoramic/high-resolution RGB images, depth maps and captions. Both models are evaluated in comparison to existing related methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设想扩散模型在视觉输出的创造和操作方面已经达到了顶峰水平。然而，我们知道的是，生成高纬度RGBD图像并行RGB的生成仍然有限。我们介绍LDM3D-VR，一个针对虚拟现实开发的扩散模型集合，包括LDM3D-pano和LDM3D-SR两种模型。这两种模型允许基于文本提示生成全景RGBD图像和低分辨率输入高分辨率RGBD图像的扩展，分别。我们的模型来自现有预训练模型，并在包含全景/高分辨率RGB图像、深度图和描述的 dataset 上进行了细化。两种模型与现有相关方法进行比较。Note: "RGBD" refers to a color image with depth information, and "high-resolution" refers to a high number of pixels in both the color and depth dimensions.
</details></li>
</ul>
<hr>
<h2 id="ALYMPICS-Language-Agents-Meet-Game-Theory"><a href="#ALYMPICS-Language-Agents-Meet-Game-Theory" class="headerlink" title="ALYMPICS: Language Agents Meet Game Theory"></a>ALYMPICS: Language Agents Meet Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03220">http://arxiv.org/abs/2311.03220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, Furu Wei</li>
<li>for: 这篇论文旨在利用大语言模型（LLM）代理人 simulate human behavior，以便在游戏理论中进行假设建模和测试。</li>
<li>methods: 该论文使用LLM代理人和自主代理人实现多代理人合作， constructed realistic and dynamic models of human interactions。</li>
<li>results: 通过调整资源可用性和代理人个性，论文发现不同的代理人在竞争中如何互动和采取策略。<details>
<summary>Abstract</summary>
This paper introduces Alympics, a platform that leverages Large Language Model (LLM) agents to facilitate investigations in game theory. By employing LLMs and autonomous agents to simulate human behavior and enable multi-agent collaborations, we can construct realistic and dynamic models of human interactions for game theory hypothesis formulating and testing. To demonstrate this, we present and implement a survival game involving unequal competition for limited resources. Through manipulation of resource availability and agent personalities, we observe how different agents engage in the competition and adapt their strategies. The use of LLM agents in game theory research offers significant advantages, including simulating realistic behavior, providing a controlled, scalable, and reproducible environment. Our work highlights the potential of LLM agents in enhancing the understanding of strategic decision-making within complex socioeconomic contexts. All codes will be made public soon.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了Alympics平台，该平台利用大型语言模型（LLM）代理来促进游戏理论研究。通过使用LLM和自主代理来模拟人类行为，我们可以构建更加真实和动态的人类互动模型，用于游戏理论假设构建和测试。为了证明这一点，我们提出了一个竞争生存游戏，其中有限的资源和代理人格的不同导致不同的代理如何在竞争中 engagé和适应策略。使用LLM代理在游戏理论研究中具有重要优点，包括模拟实际行为、提供可控、可扩展和可重现的环境。我们的工作强调了LLM代理在复杂社会经济背景下的决策推理理解的潜力。所有代码即将公开。
</details></li>
</ul>
<hr>
<h2 id="Mini-Minds-Exploring-Bebeshka-and-Zlata-Baby-Models"><a href="#Mini-Minds-Exploring-Bebeshka-and-Zlata-Baby-Models" class="headerlink" title="Mini Minds: Exploring Bebeshka and Zlata Baby Models"></a>Mini Minds: Exploring Bebeshka and Zlata Baby Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03216">http://arxiv.org/abs/2311.03216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/upunaprosk/small-language-models">https://github.com/upunaprosk/small-language-models</a></li>
<li>paper_authors: Irina Proskurina, Guillaume Metzler, Julien Velcin<br>for:This paper describes the University of Lyon 2 submission to the Strict-Small track of the BabyLM competition, which focuses on small-scale language modeling from scratch on limited data.methods:The authors use an architecture search to minimize masked language modeling loss on the shared task data, and introduce two small-size language models (LMs) with a 4-layer encoder and 8 attention heads, and a 6-layer decoder with 12 heads.results:Despite being half the scale of the baseline LMs, the proposed models achieve comparable performance, and the authors explore the applicability of small-scale language models in tasks involving moral judgment, showing the potential of compact LMs in practical language understanding tasks.Here is the result in Simplified Chinese text:for: 这篇论文描述了里昂大学2分部对Strict-Small track的BabyLM比赛提交。这个比赛强调从头来语言模型化，使用有限数据和人类语言学习。methods: 作者们使用架构搜索，以最小化masked语言模型损失，并提出了两个小型语言模型（LMs），即4层编码器和8个注意头，以及6层解码器模型和12个注意头。results:  despite being half the scale of the baseline LMs, the proposed models achieve comparable performance, and the authors explore the applicability of small-scale language models in tasks involving moral judgment, showing the potential of compact LMs in practical language understanding tasks。<details>
<summary>Abstract</summary>
In this paper, we describe the University of Lyon 2 submission to the Strict-Small track of the BabyLM competition. The shared task is created with an emphasis on small-scale language modelling from scratch on limited-size data and human language acquisition. Dataset released for the Strict-Small track has 10M words, which is comparable to children's vocabulary size. We approach the task with an architecture search, minimizing masked language modelling loss on the data of the shared task. Having found an optimal configuration, we introduce two small-size language models (LMs) that were submitted for evaluation, a 4-layer encoder with 8 attention heads and a 6-layer decoder model with 12 heads which we term Bebeshka and Zlata, respectively. Despite being half the scale of the baseline LMs, our proposed models achieve comparable performance. We further explore the applicability of small-scale language models in tasks involving moral judgment, aligning their predictions with human values. These findings highlight the potential of compact LMs in addressing practical language understanding tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了吕隆二学院在Strict-Small赛道上的提交。这个共同任务强调从头来学习小规模语言模型，使用有限的数据和人类语言学习。共同任务上发布的数据集有1000万个单词，与儿童词汇相当。我们使用架构搜索，在共同任务上的数据上减小遮盲语言模型损失。发现优化配置后，我们介绍了两个小型语言模型（LM），即Bebeshka和Zlata。这两个模型具有4层Encoder和6层Decoder，具有8个注意头和12个注意头。尽管这两个模型的规模只是基线LM的一半，但它们在性能上具有相似的表现。我们进一步探讨小型语言模型在道德评价任务中的适用性，并对其预测与人类价值观 align。这些发现表明小型语言模型在实际语言理解任务中具有潜在的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Labeling-for-Domain-Agnostic-Bangla-Automatic-Speech-Recognition"><a href="#Pseudo-Labeling-for-Domain-Agnostic-Bangla-Automatic-Speech-Recognition" class="headerlink" title="Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition"></a>Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03196">http://arxiv.org/abs/2311.03196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hishab-nlp/pseudo-labeling-for-domain-agnostic-bangla-asr">https://github.com/hishab-nlp/pseudo-labeling-for-domain-agnostic-bangla-asr</a></li>
<li>paper_authors: Rabindra Nath Nandi, Mehadi Hasan Menon, Tareq Al Muntasir, Sagor Sarker, Quazi Sarwar Muhtaseem, Md. Tariqul Islam, Shammur Absar Chowdhury, Firoj Alam</li>
<li>For: The paper aims to develop a large-scale domain-agnostic automatic speech recognition (ASR) dataset for low-resource languages, specifically Bangla.* Methods: The proposed methodology uses pseudo-labeling to develop a 20k+ hours labeled Bangla speech dataset covering diverse topics, speaking styles, dialects, noisy environments, and conversational scenarios.* Results: The trained ASR model demonstrates efficacy on a human-annotated domain-agnostic test set composed of news, telephony, and conversational data, as well as publicly available Bangla datasets.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 本研究旨在开发低资源语言自动听写系统（ASR）大规模领域独立语言数据集，具体来说是 Bangla。</li>
<li>methods: 提议方法使用 pseudo-labeling 方法开发了 20k+ 小时 Bangla 语音数据集，覆盖多样的话题、说法风格、方言、噪音环境和对话场景。</li>
<li>results: 训练的 ASR 模型在人工标注的领域独立测试集上（包括新闻、电话和对话数据等）以及公共可用 Bangla 数据集上显示了效果。<details>
<summary>Abstract</summary>
One of the major challenges for developing automatic speech recognition (ASR) for low-resource languages is the limited access to labeled data with domain-specific variations. In this study, we propose a pseudo-labeling approach to develop a large-scale domain-agnostic ASR dataset. With the proposed methodology, we developed a 20k+ hours labeled Bangla speech dataset covering diverse topics, speaking styles, dialects, noisy environments, and conversational scenarios. We then exploited the developed corpus to design a conformer-based ASR system. We benchmarked the trained ASR with publicly available datasets and compared it with other available models. To investigate the efficacy, we designed and developed a human-annotated domain-agnostic test set composed of news, telephony, and conversational data among others. Our results demonstrate the efficacy of the model trained on psuedo-label data for the designed test-set along with publicly-available Bangla datasets. The experimental resources will be publicly available.(https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)
</details>
<details>
<summary>摘要</summary>
一个主要挑战在开发低资源语言自动语音识别（ASR）系统是获取域特定变化的标注数据的限制。在这项研究中，我们提出了一种假标注方法，以开发大规模域无关ASR数据集。我们使用该方法开发了20000多小时的标注的孟加拉语音频数据，包括多种话题、说话风格、方言、噪音环境和对话场景。然后，我们利用开发的数据集设计了一种基于配对器的ASR系统。我们对培izia的ASR系统进行了评估，并与其他可用模型进行了比较。为了评估效果，我们设计了和其他人注释的域无关测试集，包括新闻、电话和对话数据等。我们的结果表明，使用假标注数据来训练ASR系统对于我们设计的测试集以及公共可用的孟加拉语言数据集具有效果。我们将实验资源公开发布。
</details></li>
</ul>
<hr>
<h2 id="Nexus-at-ArAIEval-Shared-Task-Fine-Tuning-Arabic-Language-Models-for-Propaganda-and-Disinformation-Detection"><a href="#Nexus-at-ArAIEval-Shared-Task-Fine-Tuning-Arabic-Language-Models-for-Propaganda-and-Disinformation-Detection" class="headerlink" title="Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection"></a>Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03184">http://arxiv.org/abs/2311.03184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunze Xiao, Firoj Alam</li>
<li>for: 本研究旨在探讨自媒体内容中的假信息和宣传内容的自动识别问题，以便提高社会和谐和公众信息准确性。</li>
<li>methods: 我们使用了微软研究院提供的ArAIEval共同任务，并在报告1A和2A两个子任务中提交了系统。我们的实验方法包括微调变换器模型和零或几个shot学习。</li>
<li>results: 我们的系统在报告1A和2A两个子任务中分别获得了第9名和第10名的成绩。<details>
<summary>Abstract</summary>
The spread of disinformation and propagandistic content poses a threat to societal harmony, undermining informed decision-making and trust in reliable sources. Online platforms often serve as breeding grounds for such content, and malicious actors exploit the vulnerabilities of audiences to shape public opinion. Although there have been research efforts aimed at the automatic identification of disinformation and propaganda in social media content, there remain challenges in terms of performance. The ArAIEval shared task aims to further research on these particular issues within the context of the Arabic language. In this paper, we discuss our participation in these shared tasks. We competed in subtasks 1A and 2A, where our submitted system secured positions 9th and 10th, respectively. Our experiments consist of fine-tuning transformer models and using zero- and few-shot learning with GPT-4.
</details>
<details>
<summary>摘要</summary>
社会和谐受到假信息和宣传内容的威胁，这会损害人们做出了有知识的决策和信任可靠的来源。在线平台常成为这种内容的温床，恶意行为者会利用观众的漏洞来形成公众意见。虽然有研究尝试自动识别社交媒体上的假信息和宣传，但还有许多挑战。阿拉伯语言的ArAIEval共享任务想要进一步研究这些问题。本文介绍我们在这个任务中的参与。我们参加了1A和2A两个子任务，我们提交的系统在这两个子任务中分别获得了第九名和第十名。我们的实验包括对 transformer 模型进行细化和使用零和几 shot 学习，并使用 GPT-4。
</details></li>
</ul>
<hr>
<h2 id="ArAIEval-Shared-Task-Persuasion-Techniques-and-Disinformation-Detection-in-Arabic-Text"><a href="#ArAIEval-Shared-Task-Persuasion-Techniques-and-Disinformation-Detection-in-Arabic-Text" class="headerlink" title="ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text"></a>ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03179">http://arxiv.org/abs/2311.03179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maram Hasanain, Firoj Alam, Hamdy Mubarak, Samir Abdaljalil, Wajdi Zaghouani, Preslav Nakov, Giovanni Da San Martino, Abed Alhakim Freihat</li>
<li>for: 本研究的目的是提供一个 arabic 文本分析的评估任务，以探索在 tweets 和新闻文章中使用诱导技巧的应用。</li>
<li>methods: 本研究使用了 transformer 模型，例如 AraBERT，进行 fine-tuning，以提高模型的性能。</li>
<li>results: 本研究获得了20队参赛队伍的最终评估结果，包括14队参赛 Task 1，和16队参赛 Task 2。<details>
<summary>Abstract</summary>
We present an overview of the ArAIEval shared task, organized as part of the first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two tasks over Arabic text: (i) persuasion technique detection, focusing on identifying persuasion techniques in tweets and news articles, and (ii) disinformation detection in binary and multiclass setups over tweets. A total of 20 teams participated in the final evaluation phase, with 14 and 16 teams participating in Tasks 1 and 2, respectively. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further give a brief overview of the participating systems. All datasets and evaluation scripts from the shared task are released to the research community. (https://araieval.gitlab.io/) We hope this will enable further research on these important tasks in Arabic.
</details>
<details>
<summary>摘要</summary>
我们提供了阿拉伯语自然语言处理（ArAIEval）共享任务的概述，这是2023年第一届阿拉伯语言处理会议（ArabicNLP 2023）的一部分，并与EMNLP 2023共同举办。ArAIEval提供了两个任务，它们是：（i）发现挑台技巧，即在推文和新闻文章中发现挑台技巧，以及（ii）对推文进行谎言检测。共20个队伍参加了最终评估阶段，其中14个队伍参加了任务1，16个队伍参加了任务2。在两个任务之间，我们发现大多数参与系统都是使用transformer模型进行精度调整，如AraBERT。我们提供了任务设置的描述，包括数据集构建和评估设置的描述，以及参与系统的简要概述。此外，我们还发布了所有数据集和评估脚本给研究社区。我们希望这将促进阿拉伯语言处理领域的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait"><a href="#1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait" class="headerlink" title="1D-Convolutional transformer for Parkinson disease diagnosis from gait"></a>1D-Convolutional transformer for Parkinson disease diagnosis from gait</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03177">http://arxiv.org/abs/2311.03177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/safwennaimi/1d-convolutional-transformer-for-parkinson-disease-diagnosis-from-gait">https://github.com/safwennaimi/1d-convolutional-transformer-for-parkinson-disease-diagnosis-from-gait</a></li>
<li>paper_authors: Safwen Naimi, Wassim Bouachir, Guillaume-Alexandre Bilodeau</li>
<li>for:  Parkinson’s disease diagnosis from gait data</li>
<li>methods:  Hybrid ConvNet-Transformer architecture, exploiting strengths of both models to extract local features and capture long-term spatio-temporal dependencies</li>
<li>results: 88% accuracy, outperforming other state-of-the-art AI methods on the Physionet gait dataset, and can be generalized for other classification problems in 1D signals.Here’s the simplified Chinese text:</li>
<li>for:  Parkinson病诊断从步态数据</li>
<li>methods:  Hybrid ConvNet-Transformer架构，利用两种模型的优点来抽取本地特征和捕捉长期空间时间关系</li>
<li>results: 88%准确率，超过其他状态时的AI方法在Physionet步态数据集上，并可泛化应用于其他一维信号的分类问题。<details>
<summary>Abstract</summary>
This paper presents an efficient deep neural network model for diagnosing Parkinson's disease from gait. More specifically, we introduce a hybrid ConvNet-Transformer architecture to accurately diagnose the disease by detecting the severity stage. The proposed architecture exploits the strengths of both Convolutional Neural Networks and Transformers in a single end-to-end model, where the former is able to extract relevant local features from Vertical Ground Reaction Force (VGRF) signal, while the latter allows to capture long-term spatio-temporal dependencies in data. In this manner, our hybrid architecture achieves an improved performance compared to using either models individually. Our experimental results show that our approach is effective for detecting the different stages of Parkinson's disease from gait data, with a final accuracy of 88%, outperforming other state-of-the-art AI methods on the Physionet gait dataset. Moreover, our method can be generalized and adapted for other classification problems to jointly address the feature relevance and spatio-temporal dependency problems in 1D signals. Our source code and pre-trained models are publicly available at https://github.com/SafwenNaimi/1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Findings-of-the-WMT-2023-Shared-Task-on-Discourse-Level-Literary-Translation-A-Fresh-Orb-in-the-Cosmos-of-LLMs"><a href="#Findings-of-the-WMT-2023-Shared-Task-on-Discourse-Level-Literary-Translation-A-Fresh-Orb-in-the-Cosmos-of-LLMs" class="headerlink" title="Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs"></a>Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03127">http://arxiv.org/abs/2311.03127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian Yu, Qingsong Ma, Chenyang Lyu, Liting Zhou, Chao-Hong Liu, Yufeng Ma, Weiyu Chen, Yvette Graham, Bonnie Webber, Philipp Koehn, Andy Way, Yulin Yuan, Shuming Shi</li>
<li>for: 这个研究旨在提高机器翻译器的Literary Translation能力。</li>
<li>methods: 该研究使用了一个新的Literary Translation任务，并采用了人工和自动评价方法来评估提交的系统性能。</li>
<li>results: 研究发现了一些有趣的Literary和Discourse-aware MT发现，并公布了数据、系统输出和排名在<a target="_blank" rel="noopener" href="http://www2.statmt.org/wmt23/literary-translation-task.html%E3%80%82">http://www2.statmt.org/wmt23/literary-translation-task.html。</a><details>
<summary>Abstract</summary>
Translating literary works has perennially stood as an elusive dream in machine translation (MT), a journey steeped in intricate challenges. To foster progress in this domain, we hold a new shared task at WMT 2023, the first edition of the Discourse-Level Literary Translation. First, we (Tencent AI Lab and China Literature Ltd.) release a copyrighted and document-level Chinese-English web novel corpus. Furthermore, we put forth an industry-endorsed criteria to guide human evaluation process. This year, we totally received 14 submissions from 7 academia and industry teams. We employ both automatic and human evaluations to measure the performance of the submitted systems. The official ranking of the systems is based on the overall human judgments. In addition, our extensive analysis reveals a series of interesting findings on literary and discourse-aware MT. We release data, system outputs, and leaderboard at http://www2.statmt.org/wmt23/literary-translation-task.html.
</details>
<details>
<summary>摘要</summary>
machine translation (MT) 的文学翻译总是一个逃避不了的梦，一个充满复杂的挑战的旅程。为了促进这个领域的进步，我们在WMT 2023上启动了一个新的共同任务，称为文学级别翻译。首先，我们（天地智能实验室和中国文学有限公司）发布了一个版权保护的、中英文网络小说集。此外，我们提出了行业认可的评价标准，以导引人工评价过程。本年，我们总共收到了14个来自7个学术和产业团队的提交。我们使用自动和人工评价来评价提交的系统表现。人工评价的官方排名是基于总体人工评价结果。此外，我们的广泛分析发现了一系列有趣的文学和话语翻译相关的发现。我们在http://www2.statmt.org/wmt23/literary-translation-task.html上发布了数据、系统输出和排名。
</details></li>
</ul>
<hr>
<h2 id="Pelvic-floor-MRI-segmentation-based-on-semi-supervised-deep-learning"><a href="#Pelvic-floor-MRI-segmentation-based-on-semi-supervised-deep-learning" class="headerlink" title="Pelvic floor MRI segmentation based on semi-supervised deep learning"></a>Pelvic floor MRI segmentation based on semi-supervised deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03105">http://arxiv.org/abs/2311.03105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Zuo, Fei Feng, Zhuhui Wang, James A. Ashton-Miller, John O. L. Delancey, Jiajia Luo</li>
<li>for: 这个研究的目的是提高pelvic floor organs的semantic segmentation和三维重建的精度。</li>
<li>methods: 这个方法使用了深度学习的自我监督预训和阶层对应。在第一阶段，它使用了图像修复任务进行自我监督预训。然后，使用标注数据进行精确的对应。在第二阶段，自我监督预训模型被用来生成pseudo标签。最后，使用标注和pseudo标签进行半supervised训练。</li>
<li>results: 这个方法可以对pelvic floor organs的semantic segmentation和三维重建进行大幅提高。试料中的Dice公式可以提高2.65%的平均值，特别是难以分类的器官，如uterus，可以提高3.70%的精度。<details>
<summary>Abstract</summary>
The semantic segmentation of pelvic organs via MRI has important clinical significance. Recently, deep learning-enabled semantic segmentation has facilitated the three-dimensional geometric reconstruction of pelvic floor organs, providing clinicians with accurate and intuitive diagnostic results. However, the task of labeling pelvic floor MRI segmentation, typically performed by clinicians, is labor-intensive and costly, leading to a scarcity of labels. Insufficient segmentation labels limit the precise segmentation and reconstruction of pelvic floor organs. To address these issues, we propose a semi-supervised framework for pelvic organ segmentation. The implementation of this framework comprises two stages. In the first stage, it performs self-supervised pre-training using image restoration tasks. Subsequently, fine-tuning of the self-supervised model is performed, using labeled data to train the segmentation model. In the second stage, the self-supervised segmentation model is used to generate pseudo labels for unlabeled data. Ultimately, both labeled and unlabeled data are utilized in semi-supervised training. Upon evaluation, our method significantly enhances the performance in the semantic segmentation and geometric reconstruction of pelvic organs, Dice coefficient can increase by 2.65% averagely. Especially for organs that are difficult to segment, such as the uterus, the accuracy of semantic segmentation can be improved by up to 3.70%.
</details>
<details>
<summary>摘要</summary>
pelvic organs的semantic segmentation via MRI具有重要的临床意义。近期，深度学习支持的semantic segmentation技术已经实现了三维 геометрическую重建pelvic floor organs，为临床医生提供了准确和直观的诊断结果。然而，pelvic floor MRI segmentation的标注，通常由临床医生进行，是劳动密集和成本高昂的，导致标注的缺乏。不充分的标注限制了精准的segmentation和重建pelvic floor organs。为解决这些问题，我们提出了一种半supervised框架 дляpelvic organ segmentation。实施这个框架包括两个阶段。在第一阶段，它通过图像恢复任务进行自我超vised预训练。然后，使用标注数据来训练分 segmentation模型的精度调整。在第二阶段，自我超vised segmentation模型被用来生成 pseudo labels для无标注数据。最终，两者都被用于半supervised训练。经评估，我们的方法可以明显提高pelvic organs的semantic segmentation和三维重建性能，Dice指数平均提高2.65%。特别是难以分 segment的器官，如uterus，semantic segmentation的准确率可以提高到3.70%。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-yet-Efficient-Ensemble-Approach-for-AI-generated-Text-Detection"><a href="#A-Simple-yet-Efficient-Ensemble-Approach-for-AI-generated-Text-Detection" class="headerlink" title="A Simple yet Efficient Ensemble Approach for AI-generated Text Detection"></a>A Simple yet Efficient Ensemble Approach for AI-generated Text Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03084">http://arxiv.org/abs/2311.03084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harika Abburi, Kalyani Roy, Michael Suesserman, Nirmala Pudota, Balaji Veeramani, Edward Bowen, Sanmitra Bhattacharya</li>
<li>for: 本研究旨在提出一种简单 yet efficient的解决方案，用于分辨人工生成的文本和人类写作的文本。</li>
<li>methods: 本研究使用了多个基于Transformer的大型语言模型（LLMs）的 ensemble 方法，并对其进行了Condensed Ensembling 的改进。</li>
<li>results: 对四个基准数据集进行了实验，并证明了与之前的状态理论方法相比，本研究的方法可以获得0.5-100%的性能提升。此外，研究还表明可以通过使用不同的开源语言模型生成的数据来代替商业Restrictive GPT 数据，从而提高模型性能。<details>
<summary>Abstract</summary>
Recent Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across wide range of styles and genres. However, such capabilities are prone to potential abuse, such as fake news generation, spam email creation, and misuse in academic assignments. Hence, it is essential to build automated approaches capable of distinguishing between artificially generated text and human-authored text. In this paper, we propose a simple yet efficient solution to this problem by ensembling predictions from multiple constituent LLMs. Compared to previous state-of-the-art approaches, which are perplexity-based or uses ensembles with a number of LLMs, our condensed ensembling approach uses only two constituent LLMs to achieve comparable performance. Experiments conducted on four benchmark datasets for generative text classification show performance improvements in the range of 0.5 to 100\% compared to previous state-of-the-art approaches. We also study the influence the training data from individual LLMs have on model performance. We found that substituting commercially-restrictive Generative Pre-trained Transformer (GPT) data with data generated from other open language models such as Falcon, Large Language Model Meta AI (LLaMA2), and Mosaic Pretrained Transformers (MPT) is a feasible alternative when developing generative text detectors. Furthermore, to demonstrate zero-shot generalization, we experimented with an English essays dataset, and results suggest that our ensembling approach can handle new data effectively.
</details>
<details>
<summary>摘要</summary>
现代大型语言模型（LLM）已经展示了人工生成文本的各种风格和类型的很好的模仿能力。然而，这些能力也容易被滥用，如生成假新闻、垃圾邮件和学术作业中的违规使用。因此，建立自动化的人工生成文本识别方法是非常重要的。在这篇论文中，我们提出了一种简单 yet efficient的解决方案，即将多个组成部件LLM的预测 ensemble。与前一代方法相比，我们的压缩ensemble方法只需使用两个组成部件LLM，却可以 дости得相同的性能。在四个基准数据集上进行的生成文本分类实验中，我们得到了0.5%到100%的性能提升，相比前一代方法。我们还研究了各个LLM的训练数据对模型性能的影响。我们发现可以将商业限制的Generative Pre-trained Transformer（GPT）数据取代为其他开源语言模型生成的数据，如Falcon、Large Language Model Meta AI（LLaMA2）和Mosaic Pretrained Transformers（MPT）。此外，我们还进行了零shot泛化的实验，结果表明我们的ensemble方法可以对新数据进行有效的处理。
</details></li>
</ul>
<hr>
<h2 id="SugarViT-–-Multi-objective-Regression-of-UAV-Images-with-Vision-Transformers-and-Deep-Label-Distribution-Learning-Demonstrated-on-Disease-Severity-Prediction-in-Sugar-Beet"><a href="#SugarViT-–-Multi-objective-Regression-of-UAV-Images-with-Vision-Transformers-and-Deep-Label-Distribution-Learning-Demonstrated-on-Disease-Severity-Prediction-in-Sugar-Beet" class="headerlink" title="SugarViT – Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet"></a>SugarViT – Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03076">http://arxiv.org/abs/2311.03076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maurice Günder, Facundo Ramón Ispizua Yamati, Abel Andree Barreta Alcántara, Anne-Katrin Mahlein, Rafet Sifa, Christian Bauckhage</li>
<li>for: 这个研究的目的是开发一种自动化大规模植物特征标注机制，用于粘膜融合病（CLS）严重度评估在块根中。</li>
<li>methods: 该研究使用了深度标签分布学习（DLDL）、特殊的损失函数和定制的模型架构，开发了一种基于视觉转换器的疾病严重度评估模型called SugarViT。</li>
<li>results: 研究表明，将遥感数据和实验室环境参数结合使用可以提高疾病严重度预测的准确性。该模型可以应用于多种图像基于的分类和回归任务，甚至可以学习多目标问题。<details>
<summary>Abstract</summary>
Remote sensing and artificial intelligence are pivotal technologies of precision agriculture nowadays. The efficient retrieval of large-scale field imagery combined with machine learning techniques shows success in various tasks like phenotyping, weeding, cropping, and disease control. This work will introduce a machine learning framework for automatized large-scale plant-specific trait annotation for the use case disease severity scoring for Cercospora Leaf Spot (CLS) in sugar beet. With concepts of Deep Label Distribution Learning (DLDL), special loss functions, and a tailored model architecture, we develop an efficient Vision Transformer based model for disease severity scoring called SugarViT. One novelty in this work is the combination of remote sensing data with environmental parameters of the experimental sites for disease severity prediction. Although the model is evaluated on this special use case, it is held as generic as possible to also be applicable to various image-based classification and regression tasks. With our framework, it is even possible to learn models on multi-objective problems as we show by a pretraining on environmental metadata.
</details>
<details>
<summary>摘要</summary>
现代准精准农业技术中，远程感知和人工智能是非常重要的。大规模场景图像的高效回收，结合机器学习技术，在不同任务中均显示出成功，如现象识别、杂草控制、耕作和病虫控制。本文将介绍一种基于机器学习的自动化大规模植物特征注释框架，用于 sugar beet 的 Cercospora Leaf Spot（CLS）疾病严重度评估。我们采用了 Deep Label Distribution Learning（DLDL）、特殊的损失函数和定制的模型体系，开发了一种高效的视Transformer 模型，称为 SugarViT。我们的创新在于结合远程感知数据和试验站的环境参数来预测疾病严重度。尽管该模型仅应用于这个特定的应用场景，但它是通用的，可以应用于多种图像基于的类型和回归任务。我们的框架还可以学习多目标问题，我们通过在环境元数据上进行预训练来证明这一点。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Consistent-Subsystems-of-Max-T-Fuzzy-Relational-Equations"><a href="#Maximal-Consistent-Subsystems-of-Max-T-Fuzzy-Relational-Equations" class="headerlink" title="Maximal Consistent Subsystems of Max-T Fuzzy Relational Equations"></a>Maximal Consistent Subsystems of Max-T Fuzzy Relational Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03059">http://arxiv.org/abs/2311.03059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismaïl Baaj</li>
<li>for: 本文研究了一种系统 $\max-T$ 不确定关系方程 $A \Box_{T}^{\max} x &#x3D; b$, 其中 $T$ 是一个 $t-$整见 norm 中的 $\min$ 或者 Lukasiewicz 的 $t-$整见 norm。</li>
<li>methods: 本文使用了一种直接构造一个 canonical 最大一致子系统 (w.r.t. 包含顺序) 的方法，其基于一个分析式公式，计算不一致 $\max-T$ 系统中的 Chebyshev 距离 $\Delta &#x3D; \inf_{c \in \mathcal{C} \Vert b - c \Vert$，其中 $\mathcal{C}$ 是一个定义同 $A$ 矩阵的一致系统中的第二个成员。</li>
<li>results: 本文给出了一种有效的方法，用于从一个不一致 $\max-\min$ 系统中获取所有一致子系统，并证明了可以逐步地获取所有最大一致子系统。<details>
<summary>Abstract</summary>
In this article, we study the inconsistency of a system of $\max-T$ fuzzy relational equations of the form $A \Box_{T}^{\max} x = b$, where $T$ is a t-norm among $\min$, the product or Lukasiewicz's t-norm. For an inconsistent $\max-T$ system, we directly construct a canonical maximal consistent subsystem (w.r.t the inclusion order). The main tool used to obtain it is the analytical formula which compute the Chebyshev distance $\Delta = \inf_{c \in \mathcal{C} \Vert b - c \Vert$ associated to the inconsistent $\max-T$ system, where $\mathcal{C}$ is the set of second members of consistent systems defined with the same matrix $A$. Based on the same analytical formula, we give, for an inconsistent $\max-\min$ system, an efficient method to obtain all its consistent subsystems, and we show how to iteratively get all its maximal consistent subsystems.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们研究了一个 $\max-T$ 非精确关系方程的不一致性，其形式为 $A \Box_{T}^{\max} x = b$，其中 $T$ 是一个 $t-$整数（包括乘法或卢卡西耶夫 $t-$整数）。对于一个不一致的 $\max-T$ 系统，我们直接构造了一个 canonical 最大一致子系统（w.r.t. 包含关系）。我们使用的主要工具是计算 Chebyshev 距离 $\Delta = \inf_{c \in \mathcal{C} \Vert b - c \Vert$，该距离相关于不一致的 $\max-T$ 系统，其中 $\mathcal{C}$ 是定义同 $A$ 矩阵的一致系统的第二成员集。在这个基础之上，我们对一个不一致的 $\max-\min$ 系统提供了一种高效的方法来获得所有一致子系统，并证明了可以逐步获得所有最大一致子系统。
</details></li>
</ul>
<hr>
<h2 id="LitSumm-Large-language-models-for-literature-summarisation-of-non-coding-RNAs"><a href="#LitSumm-Large-language-models-for-literature-summarisation-of-non-coding-RNAs" class="headerlink" title="LitSumm: Large language models for literature summarisation of non-coding RNAs"></a>LitSumm: Large language models for literature summarisation of non-coding RNAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03056">http://arxiv.org/abs/2311.03056</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rnacentral/litscan-summarization">https://github.com/rnacentral/litscan-summarization</a></li>
<li>paper_authors: Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Anton I. Petrov, Alex Bateman, Blake Sweeney</li>
<li>for: 这个研究旨在提高生物学文献筛选的效率，通过使用大语言模型（LLM）自动生成非编码RNA文献摘要。</li>
<li>methods: 该研究使用了一个商业LMM和一系列提示和检查来生成文献摘要。</li>
<li>results: 研究发现，通过这种方法可以自动生成高质量、准确的文献摘要，并且可以通过人工评估来验证。此外，研究还发现，自动评估方法与人工评估方法不相关。最后，研究应用了这种方法于4600多个ncRNA，并将生成的摘要公开提供给RNAcentral资源。<details>
<summary>Abstract</summary>
Motivation: Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   Results: In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. We also applied the most commonly used automated evaluation approaches, finding that they do not correlate with human assessment. Finally, we apply our tool to a selection of over 4,600 ncRNAs and make the generated summaries available via the RNAcentral resource. We conclude that automated literature summarization is feasible with the current generation of LLMs, provided careful prompting and automated checking are applied.   Availability: Code used to produce these summaries can be found here: https://github.com/RNAcentral/litscan-summarization and the dataset of contexts and summaries can be found here: https://huggingface.co/datasets/RNAcentral/litsumm-v1. Summaries are also displayed on the RNA report pages in RNAcentral (https://rnacentral.org/)
</details>
<details>
<summary>摘要</summary>
motivation: 生命科学文献筛选是一项不断增长的挑战。由于发表速度的持续增长，与全球围绕知识库的数量相比，CURATOR的数量相对固定，这对生命科学知识库的开发者而言是一个主要挑战。很少的知识库有资源来涵盖整个相关文献，所以它们都必须优化努力。results: 在这项工作中，我们采取了一种解决生命科学文献筛选不足的首要步骤，即通过大语言模型（LLM）生成ncRNA的文献摘要。我们证明了，通过商业LLM和一系列提示和检查，可以自动生成高质量、精确的文献摘要，并且人工评估表明大多数摘要具有极高质量。此外，我们还应用了常用的自动评估方法，发现它们与人工评估不相关。最后，我们使用我们的工具对超过4,600个ncRNA进行摘要，并将生成的摘要公开于RNAcentral资源上。我们 conclude that使用当代LLM自动生成文献摘要是可能的，只要仔细地提示和自动检查。availability: 用于生成这些摘要的代码可以在GitHub上找到：https://github.com/RNAcentral/litscan-summarization。我们还提供了一个包含上下文和摘要的数据集：https://huggingface.co/datasets/RNAcentral/litsumm-v1。摘要也可以在RNAcentral网站上查看（https://rnacentral.org/)。
</details></li>
</ul>
<hr>
<h2 id="Masking-Hyperspectral-Imaging-Data-with-Pretrained-Models"><a href="#Masking-Hyperspectral-Imaging-Data-with-Pretrained-Models" class="headerlink" title="Masking Hyperspectral Imaging Data with Pretrained Models"></a>Masking Hyperspectral Imaging Data with Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03053">http://arxiv.org/abs/2311.03053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elias Arbash, Andréa de Lima Ribeiro, Sam Thiele, Nina Gnann, Behnood Rasti, Margret Fuchs, Pedram Ghamisi, Richard Gloaguen</li>
<li>for: 提高干扰性数据处理性能，增强计算效率和内存需求</li>
<li>methods: 使用Segment Anything Model（SAM）和零批学习Grounding Dino对象检测器进行预先图像分割，然后应用交和排除筛选步骤</li>
<li>results: 在三个复杂应用场景中（塑料碎屑特征化、钻井检测和垃圾监测）进行了数值评估，并提供了使用的超参数In Simplified Chinese text, the information can be presented as follows:</li>
<li>for: 提高干扰性数据处理性能，增强计算效率和内存需求</li>
<li>methods: 使用Segment Anything Model（SAM）和零批学习Grounding Dino对象检测器进行预先图像分割，然后应用交和排除筛选步骤</li>
<li>results: 在三个复杂应用场景中（塑料碎屑特征化、钻井检测和垃圾监测）进行了数值评估，并提供了使用的超参数<details>
<summary>Abstract</summary>
The presence of undesired background areas associated with potential noise and unknown spectral characteristics degrades the performance of hyperspectral data processing. Masking out unwanted regions is key to addressing this issue. Processing only regions of interest yields notable improvements in terms of computational costs, required memory, and overall performance. The proposed processing pipeline encompasses two fundamental parts: regions of interest mask generation, followed by the application of hyperspectral data processing techniques solely on the newly masked hyperspectral cube. The novelty of our work lies in the methodology adopted for the preliminary image segmentation. We employ the Segment Anything Model (SAM) to extract all objects within the dataset, and subsequently refine the segments with a zero-shot Grounding Dino object detector, followed by intersection and exclusion filtering steps, without the need for fine-tuning or retraining. To illustrate the efficacy of the masking procedure, the proposed method is deployed on three challenging applications scenarios that demand accurate masking; shredded plastics characterization, drill core scanning, and litter monitoring. The numerical evaluation of the proposed masking method on the three applications is provided along with the used hyperparameters. The scripts for the method will be available at https://github.com/hifexplo/Masking.
</details>
<details>
<summary>摘要</summary>
“干扰性质数据处理中存在不要的背景领域，这些领域可能具有噪音和未知的 спектраль特征，对数据处理造成干扰。为了解决这个问题，我们需要对数据进行遮盾。仅处理区域 OF  интерес可以大大减少计算成本、需要的内存和整体性能。我们的处理管线包括两个基本部分：区域 OF  интерес遮盾生成，然后将仅遮盾的多спектル矩阵进行处理。我们的工作新的是采用的是对数据进行先验性阶段分割的方法。我们使用Segment Anything Model（SAM）提取数据集中的所有物体，然后使用零扩展的Grounding Dino物体检测器进行精确的分割，然后进行交叉和排除步骤。我们不需要进行微调或重训。为了证明遮盾程序的有效性，我们将其应用于三个具有严苛需求的应用场景：材料回收、探勘棒轴和垃圾监控。我们提供了这些应用场景中的数据处理效果，以及使用的参数。我们将方法的脚本公开在https://github.com/hifexplo/Masking上。”
</details></li>
</ul>
<hr>
<h2 id="Grouping-Local-Process-Models"><a href="#Grouping-Local-Process-Models" class="headerlink" title="Grouping Local Process Models"></a>Grouping Local Process Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03040">http://arxiv.org/abs/2311.03040</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Viki Peeva, Wil M. P. van der Aalst</li>
<li>for: 这篇论文旨在提出一种三步管道，用于组合类似的本地过程模型（LPM）。</li>
<li>methods: 该论文使用了多种过程模型相似度度量，来 grouping 类似的 LPM。</li>
<li>results: 研究人员通过实际案例研究，发现 grouping 可以减少模型爆炸和重复现象，提高过程分析效果。<details>
<summary>Abstract</summary>
In recent years, process mining emerged as a proven technology to analyze and improve operational processes. An expanding range of organizations using process mining in their daily operation brings a broader spectrum of processes to be analyzed. Some of these processes are highly unstructured, making it difficult for traditional process discovery approaches to discover a start-to-end model describing the entire process. Therefore, the subdiscipline of Local Process Model (LPM) discovery tries to build a set of LPMs, i.e., smaller models that explain sub-behaviors of the process. However, like other pattern mining approaches, LPM discovery algorithms also face the problems of model explosion and model repetition, i.e., the algorithms may create hundreds if not thousands of models, and subsets of them are close in structure or behavior. This work proposes a three-step pipeline for grouping similar LPMs using various process model similarity measures. We demonstrate the usefulness of grouping through a real-life case study, and analyze the impact of different measures, the gravity of repetition in the discovered LPMs, and how it improves after grouping on multiple real event logs.
</details>
<details>
<summary>摘要</summary>
To address this issue, this work proposes a three-step pipeline for grouping similar LPMs using various process model similarity measures. We demonstrate the usefulness of grouping through a real-life case study and analyze the impact of different measures, the gravity of repetition in the discovered LPMs, and how it improves after grouping on multiple real event logs.Here is the translation in Simplified Chinese:近年来，过程挖掘技术得到证明，可以用来分析和改进操作过程。随着更多组织在日常操作中使用过程挖掘，需要分析的过程范围也在扩大。一些这些过程是高度不结构化的，使得传统的过程发现方法难以找到开始到结束的模型，因此本分支技术叫Local Process Model（LPM）发现。不过，如其他模式挖掘方法一样，LPM发现算法也面临着模型爆炸和模型重复的问题，即算法可能生成数百个或更多的模型，其中一些是结构或行为方面相似的。为了解决这个问题，本工作提出了一个三步管道，用于将相似的LPM分组使用不同的过程模型相似度度量。我们通过实际的案例研究证明了分组的有用性，并分析了不同度量的影响、发现LPM中重复的gravity度量，以及在多个真实事件日志上进行分组后对模型的改进。
</details></li>
</ul>
<hr>
<h2 id="GTP-ViT-Efficient-Vision-Transformers-via-Graph-based-Token-Propagation"><a href="#GTP-ViT-Efficient-Vision-Transformers-via-Graph-based-Token-Propagation" class="headerlink" title="GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation"></a>GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03035">http://arxiv.org/abs/2311.03035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ackesnal/gtp-vit">https://github.com/ackesnal/gtp-vit</a></li>
<li>paper_authors: Xuwei Xu, Sen Wang, Yudong Chen, Yanping Zheng, Zhewei Wei, Jiajun Liu</li>
<li>for: 该 paper 的目的是提高资源受限的设备上的 ViT 的部署，以提高计算效率。</li>
<li>methods: 该 paper 使用了 Token Pruning 和 Token Merging 等方法来减少计算量，但这些方法仍有一些局限性，如图像信息损失和tokento-matching过程的不效率。该 paper 提出了一种基于图的 Token Propagation (GTP) 方法，以解决计算复杂性和信息损失之间的权衡。</li>
<li>results: 对于 DeiT-S 和 DeiT-B 等背景下，GTP 可以减少计算复杂性，同时保持图像信息的完整性。对于 ImageNet-1K 等 dataset，GTP 可以达到更高的执行速度，而无需进行 fine-tuning。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have revolutionized the field of computer vision, yet their deployments on resource-constrained devices remain challenging due to high computational demands. To expedite pre-trained ViTs, token pruning and token merging approaches have been developed, which aim at reducing the number of tokens involved in the computation. However, these methods still have some limitations, such as image information loss from pruned tokens and inefficiency in the token-matching process. In this paper, we introduce a novel Graph-based Token Propagation (GTP) method to resolve the challenge of balancing model efficiency and information preservation for efficient ViTs. Inspired by graph summarization algorithms, GTP meticulously propagates less significant tokens' information to spatially and semantically connected tokens that are of greater importance. Consequently, the remaining few tokens serve as a summarization of the entire token graph, allowing the method to reduce computational complexity while preserving essential information of eliminated tokens. Combined with an innovative token selection strategy, GTP can efficiently identify image tokens to be propagated. Extensive experiments have validated GTP's effectiveness, demonstrating both efficiency and performance improvements. Specifically, GTP decreases the computational complexity of both DeiT-S and DeiT-B by up to 26% with only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and remarkably surpasses the state-of-the-art token merging method on various backbones at an even faster inference speed. The source code is available at https://github.com/Ackesnal/GTP-ViT.
</details>
<details>
<summary>摘要</summary>
计算机视觉领域内，变换瞭望（ViTs）已经革命化了计算机视觉领域，但是在有限资源的设备上部署仍然具有挑战性，因为它们的计算需求较高。为了加速预训练的 ViTs，尝试了划掉和合并Token的方法，以降低计算的数量。然而，这些方法仍有一些局限性，如图像信息的损失和匹配过程的不效率。在这篇论文中，我们介绍了一种新的图表达式传播（GTP）方法，以解决计算机视觉模型的效率和信息保留之间的平衡问题。受到图摘要算法的启发，GTP méticulously在空间和semantically相连的Token上传递了 menos significance的Token的信息。因此，剩下的一些Token可以serve为整个Token图的摘要， allowing the method to reduce computational complexity while preserving essential information of eliminated tokens。结合一种创新的Token选择策略，GTP可以有效地选择图像Token。经验 validate了GTP的有效性，显示它可以提高计算效率和性能。具体来说，GTP在DeiT-S和DeiT-B上降低了计算复杂度，并且只有0.3%的精度下降，在ImageNet-1K上不需要finetuning。此外，GTP还超越了一些state-of-the-art的Token合并方法，在不同的backbone上达到了更快的推理速度。代码可以在https://github.com/Ackesnal/GTP-ViT中找到。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Words-A-Mathematical-Framework-for-Interpreting-Large-Language-Models"><a href="#Beyond-Words-A-Mathematical-Framework-for-Interpreting-Large-Language-Models" class="headerlink" title="Beyond Words: A Mathematical Framework for Interpreting Large Language Models"></a>Beyond Words: A Mathematical Framework for Interpreting Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03033">http://arxiv.org/abs/2311.03033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier González, Aditya V. Nori</li>
<li>For: The paper aims to provide a formal framework for understanding and improving large language models (LLMs) in order to advance the development of generative AI systems.* Methods: The paper proposes a framework called “Hex” that clarifies key terms and concepts in LLM research, and differentiates chain-of-thought reasoning from chain-of-thought prompting.* Results: The paper establishes the conditions under which chain-of-thought reasoning and chain-of-thought prompting are equivalent, and argues that the formal definitions and results provided in the paper are crucial for advancing the discussion on how to build safe, reliable, fair, and robust generative AI systems, particularly in domains like healthcare and software engineering.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文目标是为大语言模型（LLM）提供正式框架，以促进生成人工智能系统的发展。</li>
<li>methods: 论文提出了名为“Hex”的框架，用于清晰地描述和区分大语言模型研究中的核心概念和术语，并将链条理解与链条推导进行区分。</li>
<li>results: 论文确定了链条理解和链条推导在某些情况下是等价的，并认为提供的正式定义和结果是为建构安全、可靠、公平、可靠的生成人工智能系统提供关键的支持，特别在医疗和软件工程领域。<details>
<summary>Abstract</summary>
Large language models (LLMs) are powerful AI tools that can generate and comprehend natural language text and other complex information. However, the field lacks a mathematical framework to systematically describe, compare and improve LLMs. We propose Hex a framework that clarifies key terms and concepts in LLM research, such as hallucinations, alignment, self-verification and chain-of-thought reasoning. The Hex framework offers a precise and consistent way to characterize LLMs, identify their strengths and weaknesses, and integrate new findings. Using Hex, we differentiate chain-of-thought reasoning from chain-of-thought prompting and establish the conditions under which they are equivalent. This distinction clarifies the basic assumptions behind chain-of-thought prompting and its implications for methods that use it, such as self-verification and prompt programming.   Our goal is to provide a formal framework for LLMs that can help both researchers and practitioners explore new possibilities for generative AI. We do not claim to have a definitive solution, but rather a tool for opening up new research avenues. We argue that our formal definitions and results are crucial for advancing the discussion on how to build generative AI systems that are safe, reliable, fair and robust, especially in domains like healthcare and software engineering.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）是一种强大的人工智能工具，可以生成和理解自然语言文本以及其他复杂信息。然而，这个领域缺乏一个数学基础来系统地描述、比较和改进 LLM。我们提出了 hex 框架，该框架明确了关键术语和概念在 LLM 研究中，如幻觉、对齐、自我验证和链式思维。 hex 框架提供了精确和一致的方式来描述 LLM，确定其优点和缺点，并整合新发现。使用 hex，我们区分了链式思维和链式思维激发，并确定了它们在什么情况下是等价的。这种分化有助于解释链式思维激发的基本假设和其影响，例如自我验证和提示编程。我们的目标是为 LLM 提供一个正式的数学基础，以帮助研究人员和实践者探索新的可能性，并为生成人工智能系统提供一个安全、可靠、公平和可靠的基础。我们不宣称有终极解决方案，而是一个工具，用于开拓新的研究途径。我们认为我们的正式定义和结果是推动生成人工智能系统的发展的关键，特别在医疗和软件工程领域。
</details></li>
</ul>
<hr>
<h2 id="Visual-information-driven-model-for-crowd-simulation-using-temporal-convolutional-network"><a href="#Visual-information-driven-model-for-crowd-simulation-using-temporal-convolutional-network" class="headerlink" title="Visual-information-driven model for crowd simulation using temporal convolutional network"></a>Visual-information-driven model for crowd simulation using temporal convolutional network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02996">http://arxiv.org/abs/2311.02996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanwen Liang, Eric Wai Ming Lee</li>
<li>for: 该研究旨在提高数据驱动人群模拟模型的适应性和现实性，通过integrating visual information，包括场景几何和行人行走动态信息。</li>
<li>methods: 该模型使用radar几何-动态学方法提取视觉信息，并采用社交视觉TCN深度学习模型进行速度预测。</li>
<li>results: 在三个公共步行人动态数据集中，该模型在不同的几何场景下表现出了改善的适应性，并且与传统知识驱动模型的结果进行比较，表明该模型可以提高人群模拟的现实性。<details>
<summary>Abstract</summary>
Crowd simulations play a pivotal role in building design, influencing both user experience and public safety. While traditional knowledge-driven models have their merits, data-driven crowd simulation models promise to bring a new dimension of realism to these simulations. However, most of the existing data-driven models are designed for specific geometries, leading to poor adaptability and applicability. A promising strategy for enhancing the adaptability and realism of data-driven crowd simulation models is to incorporate visual information, including the scenario geometry and pedestrian locomotion. Consequently, this paper proposes a novel visual-information-driven (VID) crowd simulation model. The VID model predicts the pedestrian velocity at the next time step based on the prior social-visual information and motion data of an individual. A radar-geometry-locomotion method is established to extract the visual information of pedestrians. Moreover, a temporal convolutional network (TCN)-based deep learning model, named social-visual TCN, is developed for velocity prediction. The VID model is tested on three public pedestrian motion datasets with distinct geometries, i.e., corridor, corner, and T-junction. Both qualitative and quantitative metrics are employed to evaluate the VID model, and the results highlight the improved adaptability of the model across all three geometric scenarios. Overall, the proposed method demonstrates effectiveness in enhancing the adaptability of data-driven crowd models.
</details>
<details>
<summary>摘要</summary>
人群模拟在建筑设计中发挥重要作用，影响用户体验和公共安全。传统知识驱动模型有其优点，但数据驱动人群模拟模型可以带来新的现实感。然而，现有的数据驱动模型大多是特定的geometry设计，导致适应性和可应用性差。为了提高数据驱动人群模拟模型的适应性和现实感，本文提出了一种视觉信息驱动（VID）人群模拟模型。VID模型预测下一步时间点人员速度基于先前的社交视觉信息和人员运动数据。通过射电几何学运动方法提取人员视觉信息。此外，基于深度学习模型（TCN）的社交视觉TCN模型是为速度预测。VID模型在三个不同的公共人员运动数据集上进行测试，包括通道、角落和T字交叉。使用质量和量度度量表评估VID模型，结果表明VID模型在三个几何场景中都有提高的适应性。总的来说，提出的方法可以提高数据驱动人群模拟模型的适应性。
</details></li>
</ul>
<hr>
<h2 id="TabRepo-A-Large-Scale-Repository-of-Tabular-Model-Evaluations-and-its-AutoML-Applications"><a href="#TabRepo-A-Large-Scale-Repository-of-Tabular-Model-Evaluations-and-its-AutoML-Applications" class="headerlink" title="TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications"></a>TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02971">http://arxiv.org/abs/2311.02971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/autogluon/tabrepo">https://github.com/autogluon/tabrepo</a></li>
<li>paper_authors: David Salinas, Nick Erickson</li>
<li>for: 这个论文是为了提供一个新的表格模型评估和预测数据集（TabRepo）。</li>
<li>methods: 这个论文使用了1206个模型和200个回归和分类数据集来评估和预测表格模型。</li>
<li>results: 这个论文显示了TabRepo数据集的优势，可以无需考虑ensemble和参数优化，对当前AutoML系统进行比较，同时可以充分利用传输学习来超越当前表格系统的准确率、运行时间和响应时间。<details>
<summary>Abstract</summary>
We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1206 models evaluated on 200 regression and classification datasets. We illustrate the benefit of our datasets in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at no cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
</details>
<details>
<summary>摘要</summary>
我们介绍TabRepo，一个新的表格模型评估和预测数据集。TabRepo包含1206个模型在200个条件数据集上的预测和度量。我们显示了我们的数据集的优点，包括可以免费使用预 computed模型预测来比较搜寻过程优化和现有的自动化机器学习系统，以及可以轻松地进行转移学习。具体来说，我们显示了运用标准的转移学习技术可以超过现有的表格系统在精度、执行时间和延迟方面的表现。Note that "TabRepo" is a new dataset, so I translated it as "一个新的数据集" (a new dataset) rather than "一个新的表格模型" (a new tabular model) to emphasize that it's a collection of data rather than a single model. Also, I used "条件数据集" (conditional datasets) instead of "regression and classification datasets" to emphasize that the dataset contains both regression and classification tasks.
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Augmented-Code-Generation-for-Universal-Information-Extraction"><a href="#Retrieval-Augmented-Code-Generation-for-Universal-Information-Extraction" class="headerlink" title="Retrieval-Augmented Code Generation for Universal Information Extraction"></a>Retrieval-Augmented Code Generation for Universal Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02962">http://arxiv.org/abs/2311.02962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucan Guo, Zixuan Li, Xiaolong Jin, Yantao Liu, Yutao Zeng, Wenxuan Liu, Xiang Li, Pan Yang, Long Bai, Jiafeng Guo, Xueqi Cheng</li>
<li>for: 提高信息抽取（IE）任务的效率和准确率，使得IE任务能够更好地抽取结构化知识 из自然语言文本中。</li>
<li>methods: 利用大语言模型（LLM）和代码生成技术，提出一种通用的代码生成框架 Code4UIE，可以在不同的IE任务中实现高效和准确的知识抽取。</li>
<li>results: 在五种代表性的IE任务中，Code4UIE框架通过不同的示例检索策略和受检试验例子的培育，实现了高效和准确的知识抽取，并且在九个数据集上进行了广泛的实验 validate the effectiveness of the Code4UIE framework.<details>
<summary>Abstract</summary>
Information Extraction (IE) aims to extract structural knowledge (e.g., entities, relations, events) from natural language texts, which brings challenges to existing methods due to task-specific schemas and complex text expressions. Code, as a typical kind of formalized language, is capable of describing structural knowledge under various schemas in a universal way. On the other hand, Large Language Models (LLMs) trained on both codes and texts have demonstrated powerful capabilities of transforming texts into codes, which provides a feasible solution to IE tasks. Therefore, in this paper, we propose a universal retrieval-augmented code generation framework based on LLMs, called Code4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define task-specific schemas of various structural knowledge in a universal way. By so doing, extracting knowledge under these schemas can be transformed into generating codes that instantiate the predefined Python classes with the information in texts. To generate these codes more precisely, Code4UIE adopts the in-context learning mechanism to instruct LLMs with examples. In order to obtain appropriate examples for different tasks, Code4UIE explores several example retrieval strategies, which can retrieve examples semantically similar to the given texts. Extensive experiments on five representative IE tasks across nine datasets demonstrate the effectiveness of the Code4UIE framework.
</details>
<details>
<summary>摘要</summary>
信息提取（IE）目的是从自然语言文本中提取结构知识（例如实体、关系、事件），这会对现有方法带来挑战，因为任务特定的模式和文本表达的复杂性。代码作为一种形式化语言，可以在不同的模式下描述结构知识，并且可以通过训练大型自然语言模型（LLM）来转化文本为代码。因此，在这篇论文中，我们提出一种基于LLM的通用检索增强代码生成框架，称之为Code4UIE，用于IE任务。特别是，Code4UIE使用Python类来定义任务特定的结构知识模式，以便将文本中的信息转化为代码，并且采用在场学习机制来指导LLM。为了生成代码更加精准，Code4UIE采用了semantic retrieval机制来检索相似文本的示例。为了获得不同任务的适当示例，Code4UIE对多个示例检索策略进行了探索。经过对五种代表性IE任务的九个数据集的广泛实验，我们发现Code4UIE框架具有效果。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-for-Knowledge-Base-Question-Answering-for-Unmanned-Systems-based-on-Large-Language-Models"><a href="#In-Context-Learning-for-Knowledge-Base-Question-Answering-for-Unmanned-Systems-based-on-Large-Language-Models" class="headerlink" title="In-Context Learning for Knowledge Base Question Answering for Unmanned Systems based on Large Language Models"></a>In-Context Learning for Knowledge Base Question Answering for Unmanned Systems based on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02956">http://arxiv.org/abs/2311.02956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlong Chen, Yaming Zhang, Jianfei Yu, Li Yang, Rui Xia</li>
<li>for: 这个论文主要目标是提高知识库问答系统中的问答能力，以满足未来无人系统中的问答需求。</li>
<li>methods: 该论文提出了一种基于ChatGPT的Cypher Query Language（CQL）生成框架，通过预测CQL语法信息、EXTractPROPER名词、生成示例和组合多个输出来生成最佳的CQL，以提高问答精度。</li>
<li>results: 根据论文中的描述，该框架在CCKS 2023知识库问答竞赛中取得了第二名，其中F1分数为0.92676，表明该方法可以提高问答精度。<details>
<summary>Abstract</summary>
Knowledge Base Question Answering (KBQA) aims to answer factoid questions based on knowledge bases. However, generating the most appropriate knowledge base query code based on Natural Language Questions (NLQ) poses a significant challenge in KBQA. In this work, we focus on the CCKS2023 Competition of Question Answering with Knowledge Graph Inference for Unmanned Systems. Inspired by the recent success of large language models (LLMs) like ChatGPT and GPT-3 in many QA tasks, we propose a ChatGPT-based Cypher Query Language (CQL) generation framework to generate the most appropriate CQL based on the given NLQ. Our generative framework contains six parts: an auxiliary model predicting the syntax-related information of CQL based on the given NLQ, a proper noun matcher extracting proper nouns from the given NLQ, a demonstration example selector retrieving similar examples of the input sample, a prompt constructor designing the input template of ChatGPT, a ChatGPT-based generation model generating the CQL, and an ensemble model to obtain the final answers from diversified outputs. With our ChatGPT-based CQL generation framework, we achieved the second place in the CCKS 2023 Question Answering with Knowledge Graph Inference for Unmanned Systems competition, achieving an F1-score of 0.92676.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Contrastive-Multi-Level-Graph-Neural-Networks-for-Session-based-Recommendation"><a href="#Contrastive-Multi-Level-Graph-Neural-Networks-for-Session-based-Recommendation" class="headerlink" title="Contrastive Multi-Level Graph Neural Networks for Session-based Recommendation"></a>Contrastive Multi-Level Graph Neural Networks for Session-based Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02938">http://arxiv.org/abs/2311.02938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuyun Wang, Xingyu Gao, Zhenyu Chen, Lei Lyu</li>
<li>for: 这个研究目的是为了提高Session-based recommendation（SBR）的精度和效能，以便更好地预测用户的下一个ITEM。</li>
<li>methods: 本研究使用了一种新的对称多层Graph Neural Networks（CM-GNN）来更好地利用session中ITEM的复杂和高阶关系信息。CM-GNN使用了本地层Graph Convolutional Network（L-GCN）和全层Network（G-GCN），以及对称层Network（H-GCN），对session中ITEM的关系进行更好地捕捉和表征。此外，CM-GNN还引入了一个注意力组合模组，以学习session中ITEM的对应关系。</li>
<li>results: 本研究的结果显示，CM-GNN可以比前者state-of-the-art SBR方法更好地预测用户的下一个ITEM。具体来说，CM-GNN在多个常用的benchmark datasets上展现出了更高的预测精度和效能。<details>
<summary>Abstract</summary>
Session-based recommendation (SBR) aims to predict the next item at a certain time point based on anonymous user behavior sequences. Existing methods typically model session representation based on simple item transition information. However, since session-based data consists of limited users' short-term interactions, modeling session representation by capturing fixed item transition information from a single dimension suffers from data sparsity. In this paper, we propose a novel contrastive multi-level graph neural networks (CM-GNN) to better exploit complex and high-order item transition information. Specifically, CM-GNN applies local-level graph convolutional network (L-GCN) and global-level network (G-GCN) on the current session and all the sessions respectively, to effectively capture pairwise relations over all the sessions by aggregation strategy. Meanwhile, CM-GNN applies hyper-level graph convolutional network (H-GCN) to capture high-order information among all the item transitions. CM-GNN further introduces an attention-based fusion module to learn pairwise relation-based session representation by fusing the item representations generated by L-GCN and G-GCN. CM-GNN averages the item representations obtained by H-GCN to obtain high-order relation-based session representation. Moreover, to convert the high-order item transition information into the pairwise relation-based session representation, CM-GNN maximizes the mutual information between the representations derived from the fusion module and the average pool layer by contrastive learning paradigm. We conduct extensive experiments on multiple widely used benchmark datasets to validate the efficacy of the proposed method. The encouraging results demonstrate that our proposed method outperforms the state-of-the-art SBR techniques.
</details>
<details>
<summary>摘要</summary>
Session-based recommendation (SBR) targets predicting the next item at a certain time point based on anonymous user behavior sequences. Existing methods typically model session representation based on simple item transition information. However, since session-based data consists of limited users' short-term interactions, modeling session representation by capturing fixed item transition information from a single dimension suffers from data sparsity. In this paper, we propose a novel contrastive multi-level graph neural networks (CM-GNN) to better exploit complex and high-order item transition information. Specifically, CM-GNN applies local-level graph convolutional network (L-GCN) and global-level network (G-GCN) on the current session and all the sessions respectively, to effectively capture pairwise relations over all the sessions by aggregation strategy. Meanwhile, CM-GNN applies hyper-level graph convolutional network (H-GCN) to capture high-order information among all the item transitions. CM-GNN further introduces an attention-based fusion module to learn pairwise relation-based session representation by fusing the item representations generated by L-GCN and G-GCN. CM-GNN averages the item representations obtained by H-GCN to obtain high-order relation-based session representation. Moreover, to convert the high-order item transition information into the pairwise relation-based session representation, CM-GNN maximizes the mutual information between the representations derived from the fusion module and the average pool layer by contrastive learning paradigm. We conduct extensive experiments on multiple widely used benchmark datasets to validate the efficacy of the proposed method. The encouraging results demonstrate that our proposed method outperforms the state-of-the-art SBR techniques.
</details></li>
</ul>
<hr>
<h2 id="Deep-Image-Semantic-Communication-Model-for-Artificial-Intelligent-Internet-of-Things"><a href="#Deep-Image-Semantic-Communication-Model-for-Artificial-Intelligent-Internet-of-Things" class="headerlink" title="Deep Image Semantic Communication Model for Artificial Intelligent Internet of Things"></a>Deep Image Semantic Communication Model for Artificial Intelligent Internet of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02926">http://arxiv.org/abs/2311.02926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meatery/semantic-segmentation">https://github.com/meatery/semantic-segmentation</a></li>
<li>paper_authors: Li Ping Qian, Yi Zhang, Sikai Lyu, Huijie Zhu, Yuan Wu, Xuemin Sherman Shen, Xiaoniu Yang</li>
<li>for: 这篇论文是为了提出一种深度学习图像Semantic Communication模型，用于AIoT设备上的图像交换中快速和高效地传输图像数据。</li>
<li>methods: 在传输方 сторо，提出了一种高精度图像semantic Segmentation算法，用于提取图像中的semantic信息，以实现图像数据的重要压缩。在接收方 сторо，提出了一种基于GAN的semantic图像恢复算法，用于将semantic图像转换成详细的真实场景图像。</li>
<li>results: 对比WebP和CycleGAN，该图像Semantic Communication模型可以提高图像压缩比和恢复精度，平均提高71.93%和25.07%。此外，我们的demo实验表明，该模型可以将图像传输延迟降低至95.26%。<details>
<summary>Abstract</summary>
With the rapid development of Artificial Intelligent Internet of Things (AIoT), the image data from AIoT devices has been witnessing the explosive increasing. In this paper, a novel deep image semantic communication model is proposed for the efficient image communication in AIoT. Particularly, at the transmitter side, a high-precision image semantic segmentation algorithm is proposed to extract the semantic information of the image to achieve significant compression of the image data. At the receiver side, a semantic image restoration algorithm based on Generative Adversarial Network (GAN) is proposed to convert the semantic image to a real scene image with detailed information. Simulation results demonstrate that the proposed image semantic communication model can improve the image compression ratio and recovery accuracy by 71.93% and 25.07% on average in comparison with WebP and CycleGAN, respectively. More importantly, our demo experiment shows that the proposed model reduces the total delay by 95.26% in the image communication, when comparing with the original image transmission.
</details>
<details>
<summary>摘要</summary>
随着人工智能互联网物联网（AIoT）的快速发展，AIoT设备上的图像数据已经经历了激增。在本文中，我们提出了一种新的深度图像 semantic 通信模型，用于AIoT中高效的图像通信。特别是在发送器端，我们提出了一种高精度图像 semantics 分割算法，以EXTRACT图像中的semantic信息，以实现图像数据的显著压缩。在接收端，我们提出了基于生成对抗网络（GAN）的semantic图像恢复算法，将semantic图像转换为详细的真实场景图像。实验结果表明，提出的图像 semantic 通信模型可以提高图像压缩率和恢复精度，比WebP和CycleGAN相比，提高71.93%和25.07%的平均值。此外，我们的 demo 实验表明，我们的模型可以将图像传输减少95.26%，相比原始图像传输。
</details></li>
</ul>
<hr>
<h2 id="Virtual-Action-Actor-Critic-Framework-for-Exploration-Student-Abstract"><a href="#Virtual-Action-Actor-Critic-Framework-for-Exploration-Student-Abstract" class="headerlink" title="Virtual Action Actor-Critic Framework for Exploration (Student Abstract)"></a>Virtual Action Actor-Critic Framework for Exploration (Student Abstract)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02916">http://arxiv.org/abs/2311.02916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bumgeun Park, Taeyoung Kim, Quoc-Vinh Lai-Dang, Dongsoo Har</li>
<li>for: 提高RL中代理人的寻索效率</li>
<li>methods: 提出了一种新的actor-critic框架，即虚拟动作actor-critic（VAAC），以模仿人类对未来行为的想像能力。</li>
<li>results: 实验结果表明，VAAC可以提高寻索性能，比既有算法更高。<details>
<summary>Abstract</summary>
Efficient exploration for an agent is challenging in reinforcement learning (RL). In this paper, a novel actor-critic framework namely virtual action actor-critic (VAAC), is proposed to address the challenge of efficient exploration in RL. This work is inspired by humans' ability to imagine the potential outcomes of their actions without actually taking them. In order to emulate this ability, VAAC introduces a new actor called virtual actor (VA), alongside the conventional actor-critic framework. Unlike the conventional actor, the VA takes the virtual action to anticipate the next state without interacting with the environment. With the virtual policy following a Gaussian distribution, the VA is trained to maximize the anticipated novelty of the subsequent state resulting from a virtual action. If any next state resulting from available actions does not exhibit high anticipated novelty, training the VA leads to an increase in the virtual policy entropy. Hence, high virtual policy entropy represents that there is no room for exploration. The proposed VAAC aims to maximize a modified Q function, which combines cumulative rewards and the negative sum of virtual policy entropy. Experimental results show that the VAAC improves the exploration performance compared to existing algorithms.
</details>
<details>
<summary>摘要</summary>
RL中的办法探索有挑战，这篇论文提出了一种新的actor-critic框架，即虚拟行为actor-critic（VAAC）。这种方法是基于人类可以通过想象来预测行动结果而不需要实际行动的能力。为了模拟这种能力，VAAC增加了一个虚拟actor（VA），该actor在不与环境交互的情况下预测下一个状态。虚拟政策采用高维度 Gaussian 分布，VA 是通过最大化预期下一个状态的新鲜度来训练的。如果可用的行动中任何一个下一个状态不具备高预期新鲜度，则训练VA会导致虚拟政策熵增加。因此，高虚拟政策熵表示没有探索空间。VAAC的目标是最大化修改后的Q函数，该函数将折衔奖励和虚拟政策熵相加。实验结果表明，VAAC在探索性能方面比现有算法更好。
</details></li>
</ul>
<hr>
<h2 id="Imitation-Learning-based-Alternative-Multi-Agent-Proximal-Policy-Optimization-for-Well-Formed-Swarm-Oriented-Pursuit-Avoidance"><a href="#Imitation-Learning-based-Alternative-Multi-Agent-Proximal-Policy-Optimization-for-Well-Formed-Swarm-Oriented-Pursuit-Avoidance" class="headerlink" title="Imitation Learning based Alternative Multi-Agent Proximal Policy Optimization for Well-Formed Swarm-Oriented Pursuit Avoidance"></a>Imitation Learning based Alternative Multi-Agent Proximal Policy Optimization for Well-Formed Swarm-Oriented Pursuit Avoidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02912">http://arxiv.org/abs/2311.02912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sizhao Li, Yuming Xiang, Rongpeng Li, Zhifeng Zhao, Honggang Zhang</li>
<li>for: 本研究旨在提出一种分布式学习基于启发式多智能体减少策略优化（IA-MAPPO）算法，以提供弹性和通信成本低的解决方案来执行追逐避免任务在具有良好结构的群体中。</li>
<li>methods: 本研究使用了政策热传播（MAPPO）执行器，并通过启发式学习减少通信开销和提高扩展性。为了减少通信开销，本研究还使用了形态控制器的启发式学习。</li>
<li>results:  simulation results validate the effectiveness of IA-MAPPO, and extensive ablation experiments show that the performance is comparable to a centralized solution with significant decrease in communication overheads.<details>
<summary>Abstract</summary>
Multi-Robot System (MRS) has garnered widespread research interest and fostered tremendous interesting applications, especially in cooperative control fields. Yet little light has been shed on the compound ability of formation, monitoring and defence in decentralized large-scale MRS for pursuit avoidance, which puts stringent requirements on the capability of coordination and adaptability. In this paper, we put forward a decentralized Imitation learning based Alternative Multi-Agent Proximal Policy Optimization (IA-MAPPO) algorithm to provide a flexible and communication-economic solution to execute the pursuit avoidance task in well-formed swarm. In particular, a policy-distillation based MAPPO executor is firstly devised to capably accomplish and swiftly switch between multiple formations in a centralized manner. Furthermore, we utilize imitation learning to decentralize the formation controller, so as to reduce the communication overheads and enhance the scalability. Afterwards, alternative training is leveraged to compensate the performance loss incurred by decentralization. The simulation results validate the effectiveness of IA-MAPPO and extensive ablation experiments further show the performance comparable to a centralized solution with significant decrease in communication overheads.
</details>
<details>
<summary>摘要</summary>
具体来说，我们首先开发了一个基于专业训练的 MAPPO 执行器，以实现多形成的快速调整和执行。然后，我们使用循环学习将形成控制器分散化，以减少通信开销和提高可扩展性。最后，我们利用对替训练来补偿由分散化带来的性能损失。 simulation 结果证明了 IA-MAPPO 的有效性，而且实际实现了对中央化解决方案的有效性优化，并且实现了重要的通信开销优化。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Empowered-Semantic-Communication-Systems-with-a-Shared-Knowledge-Base"><a href="#Deep-Learning-Empowered-Semantic-Communication-Systems-with-a-Shared-Knowledge-Base" class="headerlink" title="Deep Learning-Empowered Semantic Communication Systems with a Shared Knowledge Base"></a>Deep Learning-Empowered Semantic Communication Systems with a Shared Knowledge Base</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02884">http://arxiv.org/abs/2311.02884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Yi, Yang Cao, Xin Kang, Ying-Chang Liang</li>
<li>for: 这则论文旨在提出一个基于深度学习的semantic communication系统，以提高未来6G网络的无线通信效率。</li>
<li>methods: 本文提出了一个 Shared Knowledge Base（SKB）基于的semantic communication系统，其中包含了可读性高的句子集，并通过与讯息相关的知识融合，以获得更少的符号资料传输。</li>
<li>results:  simulation results demonstrate that the proposed approach outperforms existing baseline methods in terms of transmitted data size and sentence similarity.<details>
<summary>Abstract</summary>
Deep learning-empowered semantic communication is regarded as a promising candidate for future 6G networks. Although existing semantic communication systems have achieved superior performance compared to traditional methods, the end-to-end architecture adopted by most semantic communication systems is regarded as a black box, leading to the lack of explainability. To tackle this issue, in this paper, a novel semantic communication system with a shared knowledge base is proposed for text transmissions. Specifically, a textual knowledge base constructed by inherently readable sentences is introduced into our system. With the aid of the shared knowledge base, the proposed system integrates the message and corresponding knowledge from the shared knowledge base to obtain the residual information, which enables the system to transmit fewer symbols without semantic performance degradation. In order to make the proposed system more reliable, the semantic self-information and the source entropy are mathematically defined based on the knowledge base. Furthermore, the knowledge base construction algorithm is developed based on a similarity-comparison method, in which a pre-configured threshold can be leveraged to control the size of the knowledge base. Moreover, the simulation results have demonstrated that the proposed approach outperforms existing baseline methods in terms of transmitted data size and sentence similarity.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度学习 empowered  semantics 通信被视为未来 6G 网络的优秀候选人。虽然现有的 semantics 通信系统已经比传统方法表现出色，但大多数 semantics 通信系统采用的端到端架构被视为黑盒子，导致解释性不足。为解决这个问题，本文提出了一种基于文本传输的 semantics 通信系统，具有共享知识库。具体来说，我们引入了一个基于自然可读的文本知识库，并将该知识库与系统集成，以获取剩余信息，从而使系统可以在符号数量不减少的情况下传输文本。为使提案更加可靠，我们定义了基于知识库的 semantics 自信息和源 entropy。此外，我们还开发了基于相似比较方法的知识库构建算法，可以通过配置阈值来控制知识库的大小。最后，我们通过实验结果表明，提案方法在数据大小和句子相似性方面都超过了基eline 方法。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Shift-–-Multi-Objective-Loss-Function-for-Improved-Anomaly-Fall-Detection"><a href="#Temporal-Shift-–-Multi-Objective-Loss-Function-for-Improved-Anomaly-Fall-Detection" class="headerlink" title="Temporal Shift – Multi-Objective Loss Function for Improved Anomaly Fall Detection"></a>Temporal Shift – Multi-Objective Loss Function for Improved Anomaly Fall Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02863">http://arxiv.org/abs/2311.02863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Denkovski, Shehroz S. Khan, Alex Mihailidis<br>for:这个研究是为了提高家庭环境中的跌倒探测精度，以减少老年人受伤和死亡的机会。methods:本研究使用了自适应网络和其变体，以及一新的多目标损失函数Temporal Shift，来进行跌倒探测。results:试验结果显示，使用Temporal Shift多目标损失函数可以优化自适应网络和多模式神经网络的表现，尤其是对于单一摄像头的情况下。相比于重建 alone，这种方法可以提高了0.20 AUC ROC的表现。<details>
<summary>Abstract</summary>
Falls are a major cause of injuries and deaths among older adults worldwide. Accurate fall detection can help reduce potential injuries and additional health complications. Different types of video modalities can be used in a home setting to detect falls, including RGB, Infrared, and Thermal cameras. Anomaly detection frameworks using autoencoders and their variants can be used for fall detection due to the data imbalance that arises from the rarity and diversity of falls. However, the use of reconstruction error in autoencoders can limit the application of networks' structures that propagate information. In this paper, we propose a new multi-objective loss function called Temporal Shift, which aims to predict both future and reconstructed frames within a window of sequential frames. The proposed loss function is evaluated on a semi-naturalistic fall detection dataset containing multiple camera modalities. The autoencoders were trained on normal activities of daily living (ADL) performed by older adults and tested on ADLs and falls performed by young adults. Temporal shift shows significant improvement to a baseline 3D Convolutional autoencoder, an attention U-Net CAE, and a multi-modal neural network. The greatest improvement was observed in an attention U-Net model improving by 0.20 AUC ROC for a single camera when compared to reconstruction alone. With significant improvement across different models, this approach has the potential to be widely adopted and improve anomaly detection capabilities in other settings besides fall detection.
</details>
<details>
<summary>摘要</summary>
falls是老年人全球主要的伤害和死亡原因之一。准确的落坠检测可以帮助降低可能的伤害和额外的健康问题。家庭环境中可以使用RGB、Infrared和热成像摄像头来检测落坠。使用自编器和其变种的异常检测框架可以用于落坠检测，因为落坠的数据偏好和多样性导致了数据不均衡。在这篇论文中，我们提出了一种新的多目标损失函数 called Temporal Shift，用于预测uture和重建的帧内一个窗口中的序列帧。我们的提案的损失函数在一个半自然的落坠检测数据集上进行了评估，该数据集包含了多种摄像头模式。我们使用了正常的生活活动（ADL）来训练自编器，并测试了ADL和落坠的 younger adults 表现。Temporal Shift显示在不同的模型中具有显著改进，特别是在注意力U-Net CAE模型中，其在单个摄像头上提高了0.20 AUC ROC。与不同的模型进行比较，这种方法在不同的设置中都具有广泛的应用前景，可能为其他异常检测场景提供改进。
</details></li>
</ul>
<hr>
<h2 id="Co-training-and-Co-distillation-for-Quality-Improvement-and-Compression-of-Language-Models"><a href="#Co-training-and-Co-distillation-for-Quality-Improvement-and-Compression-of-Language-Models" class="headerlink" title="Co-training and Co-distillation for Quality Improvement and Compression of Language Models"></a>Co-training and Co-distillation for Quality Improvement and Compression of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02849">http://arxiv.org/abs/2311.02849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang, Hongbo Zhang, Sung Ju Hwang, Alexander Min</li>
<li>for: 压缩计算昂贵的预训练语言模型（PLMs），以便在资源受限或实时设置中使用。</li>
<li>methods: 提议使用协同培训和共同干扰（CTCD）框架，以同时改进性能和执行速度。</li>
<li>results: CTCD框架成功实现了性能和执行速度的同时改进，并且可以与现有技术相结合，如建筑设计或数据增强，以达到更高的性能改进。小型模型通过CTCD的干扰得到的性能超过了原始大型模型的性能。<details>
<summary>Abstract</summary>
Knowledge Distillation (KD) compresses computationally expensive pre-trained language models (PLMs) by transferring their knowledge to smaller models, allowing their use in resource-constrained or real-time settings. However, most smaller models fail to surpass the performance of the original larger model, resulting in sacrificing performance to improve inference speed. To address this issue, we propose Co-Training and Co-Distillation (CTCD), a novel framework that improves performance and inference speed together by co-training two models while mutually distilling knowledge. The CTCD framework successfully achieves this based on two significant findings: 1) Distilling knowledge from the smaller model to the larger model during co-training improves the performance of the larger model. 2) The enhanced performance of the larger model further boosts the performance of the smaller model. The CTCD framework shows promise as it can be combined with existing techniques like architecture design or data augmentation, replacing one-way KD methods, to achieve further performance improvement. Extensive ablation studies demonstrate the effectiveness of CTCD, and the small model distilled by CTCD outperforms the original larger model by a significant margin of 1.66 on the GLUE benchmark.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）可以压缩需要大量计算训练的预训练语言模型（PLM），以便在资源有限或实时设置下使用。然而，大多数更小的模型无法超越原始大型模型的性能，导致牺牲性能以提高推理速度。为解决这个问题，我们提出了协同训练和协同预测（CTCD）框架，它可以同时提高性能和推理速度。CTCD框架基于以下两点发现：1. 在协同训练期间，往返知识的预测模型可以提高大型模型的性能。2. 大型模型的提高性能可以进一步提高小型模型的性能。CTCD框架展示了其可以与现有的技术相结合，如建筑设计或数据增强，取代一向KD方法，以实现更高的性能改进。广泛的折衣研究表明CTCD的效果，并且使用CTCD折衣的小型模型在GLUE测试benchmark上比原始大型模型高出1.66个很大的margin。
</details></li>
</ul>
<hr>
<h2 id="Kinematic-aware-Prompting-for-Generalizable-Articulated-Object-Manipulation-with-LLMs"><a href="#Kinematic-aware-Prompting-for-Generalizable-Articulated-Object-Manipulation-with-LLMs" class="headerlink" title="Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs"></a>Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02847">http://arxiv.org/abs/2311.02847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xwinks/llm_articulated_object_manipulation">https://github.com/xwinks/llm_articulated_object_manipulation</a></li>
<li>paper_authors: Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu</li>
<li>for: 本研究旨在实现智能家居 robots 上的通用运动控制，以便在不同的弹性物体上进行精确的物品搬运和处理。</li>
<li>methods: 本研究使用了 Large Language Models (LLMs) 的强大内容学习能力，并提出了一个基于物体运动结构的启发框架，将 LLMs 启发为生成低层运动轨迹点。为了有效地启发 LLMs  WITH 不同物体的运动结构，我们设计了一个统一的运动知识解析器，将不同的弹性物体表示为一个统一的文本描述，包括运动脊和触控位置。</li>
<li>results: 我们的框架在48个实验中与传统方法进行比较，结果显示我们的框架不仅在8个见到类别上表现出色，而且还在8个未见到类别上表现出强大的零例能力。实验还证明了我们的框架在实际应用中的适用性。代码可以在 \href{<a target="_blank" rel="noopener" href="https://github.com/xwinks/LLM_articulated_object_manipulation%7D%7B%E8%BF%99%E9%87%8C%7D">https://github.com/xwinks/LLM_articulated_object_manipulation}{这里}</a> 找到。<details>
<summary>Abstract</summary>
Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories. Moreover, the real-world experiments on 7 different object categories prove our framework's adaptability in practical scenarios. Code is released at \href{https://github.com/xwinks/LLM_articulated_object_manipulation}{here}.
</details>
<details>
<summary>摘要</summary>
通用化的彩绘物体控制是家庭助手机器人的重要特点。最近的努力主要集中在通过示范学习或在模拟环境中进行奖励学习，然而由于实际数据收集和精准的物体模拟成本高昂，这些工作仍然面临广泛适应性问题。近些年，许多研究尝试使用大语言模型（LLM）的强在场学习能力来实现通用的机器人控制，但大多数研究都集中在高级任务规划上，忽略了低级机器人控制。在这种情况下，我们提出了基于物体骨骼结构的动作规划框架，通过向LLM提供骨骼知识来生成低级动作规划点。为了有效地向LLM提供不同物体的骨骼结构，我们设计了一种统一的骨骼知识解析器，该解析器将各种拥有骨骼结构的物体表述为一种统一的文本描述。基于这种统一描述，我们提出了一种基于骨骼的规划模型，通过一种骨骼知识推导的链条思维法生成精确的3D manipulate方向。我们的评估包括48个实例，8种seen类和8种unseen类，结果表明我们的框架不仅在8种seen类上超越传统方法，还在0shot情况下对8种unseen类表现出强大的适应能力。此外，我们在7种实际物体类上进行了实际实验，证明了我们的框架在实际应用中的适应性。代码可以在 \href{https://github.com/xwinks/LLM_articulated_object_manipulation}{这里} 找到。
</details></li>
</ul>
<hr>
<h2 id="Saturn-Efficient-Multi-Large-Model-Deep-Learning"><a href="#Saturn-Efficient-Multi-Large-Model-Deep-Learning" class="headerlink" title="Saturn: Efficient Multi-Large-Model Deep Learning"></a>Saturn: Efficient Multi-Large-Model Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02840">http://arxiv.org/abs/2311.02840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kabir Nagrecha, Arun Kumar</li>
<li>for: 提高多大型模型训练效率 (improve the efficiency of multi-large-model training)</li>
<li>methods: 提出了三个关联系统挑战（平行技术选择、GPU分配和调度），并将其形式化为一个共同问题，然后构建了一个新的系统架构来解决这些挑战。</li>
<li>results: 对比于现有的深度学习实践，我们的联合优化方法可以降低模型选择运行时间的比例（39-49%）。<details>
<summary>Abstract</summary>
In this paper, we propose Saturn, a new data system to improve the efficiency of multi-large-model training (e.g., during model selection/hyperparameter optimization). We first identify three key interconnected systems challenges for users building large models in this setting -- parallelism technique selection, distribution of GPUs over jobs, and scheduling. We then formalize these as a joint problem, and build a new system architecture to tackle these challenges simultaneously. Our evaluations show that our joint-optimization approach yields 39-49% lower model selection runtimes than typical current DL practice.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的数据系统，用于提高多大型模型训练的效率（例如， durante 模型选择/超参数优化）。我们首先认为了多个系统挑战，包括并行技术选择、分布式 GPU 资源的分配和调度。然后我们将这些问题联系起来，并设计了一个新的系统架构来解决这些挑战。我们的评估显示，我们的联合优化方法可以降低模型选择运行时间的39-49%，相比于当前深度学习实践中的常见方法。
</details></li>
</ul>
<hr>
<h2 id="Mesh-Neural-Cellular-Automata"><a href="#Mesh-Neural-Cellular-Automata" class="headerlink" title="Mesh Neural Cellular Automata"></a>Mesh Neural Cellular Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02820">http://arxiv.org/abs/2311.02820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Pajouheshgar, Yitao Xu, Alexander Mordvintsev, Eyvind Niklasson, Tong Zhang, Sabine Süsstrunk</li>
<li>for: 提高虚拟环境的现实感，直接synthesize 3D纹理。</li>
<li>methods: 提出Mesh Neural Cellular Automata（MeshNCA）方法，可以在3D网格上直接synthesize 纹理，不需要UV映射。MeshNCA是一种通用的细胞自动机，可以在非格式结构上运行，如3D网格的顶点上的细胞。</li>
<li>results: 训练于icosphere网格上，MeshNCA可以具有很好的泛化性，可以在实时中synthesize 纹理，并且可以使用不同的目标进行训练，如图像、文本提示和运动向量场。此外，还提出了拓展MeshNCA实例的方法，以实现纹理 interpolate。<details>
<summary>Abstract</summary>
Modeling and synthesizing textures are essential for enhancing the realism of virtual environments. Methods that directly synthesize textures in 3D offer distinct advantages to the UV-mapping-based methods as they can create seamless textures and align more closely with the ways textures form in nature. We propose Mesh Neural Cellular Automata (MeshNCA), a method for directly synthesizing dynamic textures on 3D meshes without requiring any UV maps. MeshNCA is a generalized type of cellular automata that can operate on a set of cells arranged on a non-grid structure such as vertices of a 3D mesh. While only being trained on an Icosphere mesh, MeshNCA shows remarkable generalization and can synthesize textures on any mesh in real time after the training. Additionally, it accommodates multi-modal supervision and can be trained using different targets such as images, text prompts, and motion vector fields. Moreover, we conceptualize a way of grafting trained MeshNCA instances, enabling texture interpolation. Our MeshNCA model enables real-time 3D texture synthesis on meshes and allows several user interactions including texture density/orientation control, a grafting brush, and motion speed/direction control. Finally, we implement the forward pass of our MeshNCA model using the WebGL shading language and showcase our trained models in an online interactive demo which is accessible on personal computers and smartphones. Our demo and the high resolution version of this PDF are available at https://meshnca.github.io/.
</details>
<details>
<summary>摘要</summary>
“模拟和生成文本ures是虚拟环境中的重要Component。Directly生成文本ures in 3D 提供了与 UV 映射方法不同的明显优势，包括可预测的文本ures 和更好地跟随自然中文本ures的形成方式。我们提出了 Mesh Neural Cellular Automata (MeshNCA)，一种不需要 UV 图的方法，可以将文本ures 直接生成在 3D 网格上。MeshNCA 是一种通用的细胞自动机，可以在 3D 网格上的 vertices 上运行。仅对 Icosphere 网格进行训练，MeshNCA 显示了卓越的扩展性，可以在实时运行中生成文本ures 以任意网格。此外，我们提出了将训练好的 MeshNCA 实例串接起来的想法，以便进行文本ures 插值。我们的 MeshNCA 模型允许在网格上实时生成文本ures，并且支援多种用户互动，包括文本密度/方向控制、插值Brush、速度/方向控制。最后，我们使用 WebGL 渲染语言进行前向通过，并在个人电脑和智能手机上显示我们训练好的模型。我们的 demo 和高分辨率的 PDF 可以在 https://meshnca.github.io/ 浏览。”
</details></li>
</ul>
<hr>
<h2 id="QualEval-Qualitative-Evaluation-for-Model-Improvement"><a href="#QualEval-Qualitative-Evaluation-for-Model-Improvement" class="headerlink" title="QualEval: Qualitative Evaluation for Model Improvement"></a>QualEval: Qualitative Evaluation for Model Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02807">http://arxiv.org/abs/2311.02807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vmurahari3/qualeval">https://github.com/vmurahari3/qualeval</a></li>
<li>paper_authors: Vishvak Murahari, Ameet Deshpande, Peter Clark, Tanmay Rajpurohit, Ashish Sabharwal, Karthik Narasimhan, Ashwin Kalyan</li>
<li>for: 本研究旨在改善人工智能系统中的评估 metric，以便更好地评估大语言模型（LLM）的性能。</li>
<li>methods: 本研究提出了一种名为 QualEval 的自动化质量评估方法，它将量化评估 metric 与自动化质量检查相结合，以提供更加细化的模型性能评估。</li>
<li>results: 研究发现，通过 QualEval 的帮助，可以提高 Llama 2 模型在对话任务（DialogSum）上的绝对性能，比基eline高达 15% 点。此外，QualEval 还可以减少模型开发的时间和努力，因此可以视为一个数据科学家在盒子中。<details>
<summary>Abstract</summary>
Quantitative evaluation metrics have traditionally been pivotal in gauging the advancements of artificial intelligence systems, including large language models (LLMs). However, these metrics have inherent limitations. Given the intricate nature of real-world tasks, a single scalar to quantify and compare is insufficient to capture the fine-grained nuances of model behavior. Metrics serve only as a way to compare and benchmark models, and do not yield actionable diagnostics, thus making the model improvement process challenging. Model developers find themselves amid extensive manual efforts involving sifting through vast datasets and attempting hit-or-miss adjustments to training data or setups. In this work, we address the shortcomings of quantitative metrics by proposing QualEval, which augments quantitative scalar metrics with automated qualitative evaluation as a vehicle for model improvement. QualEval uses a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that when applied, accelerate model improvement. The insights are backed by a comprehensive dashboard with fine-grained visualizations and human-interpretable analyses. We corroborate the faithfulness of QualEval by demonstrating that leveraging its insights, for example, improves the absolute performance of the Llama 2 model by up to 15% points relative on a challenging dialogue task (DialogSum) when compared to baselines. QualEval successfully increases the pace of model development, thus in essence serving as a data-scientist-in-a-box. Given the focus on critiquing and improving current evaluation metrics, our method serves as a refreshingly new technique for both model evaluation and improvement.
</details>
<details>
<summary>摘要</summary>
In this work, we address these shortcomings by proposing QualEval, which integrates automated qualitative evaluation with quantitative scalar metrics to facilitate model improvement. QualEval leverages a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that can be applied to accelerate model improvement. These insights are supported by a comprehensive dashboard with fine-grained visualizations and human-interpretable analyses.We demonstrate the effectiveness of QualEval by showing that it can improve the absolute performance of the Llama 2 model by up to 15% points relative to baselines on a challenging dialogue task (DialogSum). This highlights the ability of QualEval to increase the pace of model development, effectively serving as a data-scientist-in-a-box. Given the focus on critiquing and improving current evaluation metrics, our method provides a refreshingly new technique for both model evaluation and improvement.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Worker-Perspectives-into-MTurk-Annotation-Practices-for-NLP"><a href="#Incorporating-Worker-Perspectives-into-MTurk-Annotation-Practices-for-NLP" class="headerlink" title="Incorporating Worker Perspectives into MTurk Annotation Practices for NLP"></a>Incorporating Worker Perspectives into MTurk Annotation Practices for NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02802">http://arxiv.org/abs/2311.02802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia Huang, Eve Fleisig, Dan Klein</li>
<li>for: This paper aims to address open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives in natural language processing (NLP) studies on Amazon Mechanical Turk (MTurk).</li>
<li>methods: The paper conducts a critical literature review and a survey of MTurk workers to identify their preferences and concerns regarding NLP studies.</li>
<li>results: The survey finds that MTurk workers prefer reliable, reasonable payments over uncertain, very high payments; report frequently lying on demographic questions; and express frustration at having work rejected with no explanation. The paper also finds that some quality control methods, such as requiring minimum response times or Master’s qualifications, are viewed as biased and ineffective by workers. Based on the survey results, the paper provides recommendations for future NLP studies to better account for MTurk workers’ experiences and improve data quality.<details>
<summary>Abstract</summary>
Current practices regarding data collection for natural language processing on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on data quality and heuristics shared among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are susceptible to issues regarding workers' rights and poor response quality. We conducted a critical literature review and a survey of MTurk workers aimed at addressing open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives. We found that worker preferences are often at odds with received wisdom among NLP researchers. Surveyed workers preferred reliable, reasonable payments over uncertain, very high payments; reported frequently lying on demographic questions; and expressed frustration at having work rejected with no explanation. We also found that workers view some quality control methods, such as requiring minimum response times or Master's qualifications, as biased and largely ineffective. Based on the survey results, we provide recommendations on how future NLP studies may better account for MTurk workers' experiences in order to respect workers' rights and improve data quality.
</details>
<details>
<summary>摘要</summary>
现有的MTurk数据采集做法 frequently rely on a combination of学术研究和NLP研究人员之间的经验分享。然而，不考虑MTurk工作者的视角，这些方法容易出现工作者权益问题和回答质量问题。我们进行了一项批判性文献复查和MTurk工作者调查，以解决关于最佳实践的开放问题，包括公平支付、工作者隐私、数据质量和考虑工作者奖励。我们发现工作者偏好可靠、合理的支付，而不是不确定、非常高的支付；报告经常谎报个人信息问题；表示工作被拒绝无解释而感到沮丧。我们还发现一些质控方法，如要求最小响应时间或Master的学位要求，被工作者视为偏袋并不效果。根据调查结果，我们提出了将来NLP研究可能更好地考虑MTurk工作者的经验，以尊重工作者权益并提高数据质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.AI_2023_11_06/" data-id="clopawnp8007cag881ibe5n4u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/06/cs.CV_2023_11_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-11-06
        
      </div>
    </a>
  
  
    <a href="/2023/11/06/cs.CL_2023_11_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.CL - 2023-11-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
