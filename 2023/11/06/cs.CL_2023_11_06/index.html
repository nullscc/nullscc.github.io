
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CL - 2023-11-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Tackling Concept Shift in Text Classification using Entailment-style Modeling paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03320 repo_url: None paper_authors: Sumegh Roychowdhury, Karan Gupta, Siva Rajesh Kas">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CL - 2023-11-06">
<meta property="og:url" content="https://nullscc.github.io/2023/11/06/cs.CL_2023_11_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Tackling Concept Shift in Text Classification using Entailment-style Modeling paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03320 repo_url: None paper_authors: Sumegh Roychowdhury, Karan Gupta, Siva Rajesh Kas">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-06T11:00:00.000Z">
<meta property="article:modified_time" content="2023-11-07T17:04:04.265Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CL_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.CL_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T11:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CL - 2023-11-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Tackling-Concept-Shift-in-Text-Classification-using-Entailment-style-Modeling"><a href="#Tackling-Concept-Shift-in-Text-Classification-using-Entailment-style-Modeling" class="headerlink" title="Tackling Concept Shift in Text Classification using Entailment-style Modeling"></a>Tackling Concept Shift in Text Classification using Entailment-style Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03320">http://arxiv.org/abs/2311.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumegh Roychowdhury, Karan Gupta, Siva Rajesh Kasa, Prasanna Srinivasa Murthy, Alok Chandra</li>
<li>for: 这个论文是为了解决在自然语言处理（NLP）中的文本分类（TC）问题中，随着时间的推移，类别定义发生变化的情况。</li>
<li>methods: 该论文提出了一种将文本分类转化为推理问题的方法，使得只需要少量的新数据来重新训练文本分类器，以适应新的概念。</li>
<li>results: 该论文在实际世界数据集和 sintetic 数据集上进行了评测，并取得了约7%和40%的绝对 F1 提升，并在几次训练中实现了75%的标签成本减少。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have seen tremendous success in text classification (TC) problems in the context of Natural Language Processing (NLP). In many real-world text classification tasks, the class definitions being learned do not remain constant but rather change with time - this is known as Concept Shift. Most techniques for handling concept shift rely on retraining the old classifiers with the newly labelled data. However, given the amount of training data required to fine-tune large DL models for the new concepts, the associated labelling costs can be prohibitively expensive and time consuming. In this work, we propose a reformulation, converting vanilla classification into an entailment-style problem that requires significantly less data to re-train the text classifier to adapt to new concepts. We demonstrate the effectiveness of our proposed method on both real world & synthetic datasets achieving absolute F1 gains upto 7% and 40% respectively in few-shot settings. Further, upon deployment, our solution also helped save 75% of labeling costs overall.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unraveling-Downstream-Gender-Bias-from-Large-Language-Models-A-Study-on-AI-Educational-Writing-Assistance"><a href="#Unraveling-Downstream-Gender-Bias-from-Large-Language-Models-A-Study-on-AI-Educational-Writing-Assistance" class="headerlink" title="Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance"></a>Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03311">http://arxiv.org/abs/2311.03311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/epfl-ml4ed/unraveling-llm-bias">https://github.com/epfl-ml4ed/unraveling-llm-bias</a></li>
<li>paper_authors: Thiemo Wambsganss, Xiaotian Su, Vinitra Swamy, Seyed Parsa Neshaei, Roman Rietsche, Tanja Käser</li>
<li>for: 这 paper  investigate how bias transfers through an AI writing support pipeline in the context of providing writing suggestions to students.</li>
<li>methods: 这 paper 使用了大量的 user study 和多种方法来测试 AI writing support pipeline 的偏见。</li>
<li>results: 研究发现，在使用 AI writing support pipeline 时， gender bias 不会传递到学生们的回答中。这表明，使用 AI writing support 可以在教室中使用，不会因偏见而对学生们产生负面影响。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly utilized in educational tasks such as providing writing suggestions to students. Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners. Previous studies have investigated bias in models and data representations separately, neglecting the potential impact of LLM bias on human writing. In this paper, we investigate how bias transfers through an AI writing support pipeline. We conduct a large-scale user study with 231 students writing business case peer reviews in German. Students are divided into five groups with different levels of writing support: one classroom group with feature-based suggestions and four groups recruited from Prolific -- a control group with no assistance, two groups with suggestions from fine-tuned GPT-2 and GPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using GenBit gender bias analysis, Word Embedding Association Tests (WEAT), and Sentence Embedding Association Test (SEAT) we evaluate the gender bias at various stages of the pipeline: in model embeddings, in suggestions generated by the models, and in reviews written by students. Our results demonstrate that there is no significant difference in gender bias between the resulting peer reviews of groups with and without LLM suggestions. Our research is therefore optimistic about the use of AI writing support in the classroom, showcasing a context where bias in LLMs does not transfer to students' responses.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在教育任务中越来越被利用，例如为学生提供写作建议。虽然它们有潜在的潜在优势，但 LLM 也存在内置的偏见，这可能对学生有负面影响。先前的研究已经研究过模型和数据表示中的偏见，但忽略了 AI 写作支持管道中偏见的影响。本文 investigate AI 写作支持管道中偏见的传递。我们进行了大规模的学生实验，共231名德语商业案例评审学生参与了实验。学生被分为五组，每组有不同的写作支持水平：一个教室组与特定的功能基于建议，以及四个来自 Prolific 的组：一个控制组无助，两个组使用精度调整的 GPT-2 和 GPT-3 模型建议，以及一个组使用预训练 GPT-3.5 建议。我们使用 GenBit 性别偏见分析、Word Embedding Association Tests（WEAT）和 Sentence Embedding Association Test（SEAT）来评估管道中偏见的水平：模型嵌入、模型建议中的偏见和学生写作的偏见。我们的结果表明，在 LLM 建议的支持下写作的学生 peer review 中， gender 偏见并不存在显著差异。我们的研究因此optimistic about AI 写作支持的使用情况，展示了一个情况下，LLM 的偏见不会传递到学生的回答中。
</details></li>
</ul>
<hr>
<h2 id="Ziya2-Data-centric-Learning-is-All-LLMs-Need"><a href="#Ziya2-Data-centric-Learning-is-All-LLMs-Need" class="headerlink" title="Ziya2: Data-centric Learning is All LLMs Need"></a>Ziya2: Data-centric Learning is All LLMs Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03301">http://arxiv.org/abs/2311.03301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang, Kunhao Pan, Ping Yang, Qi Yang, Jiaxing Zhang, Yan Song</li>
<li>For: The paper aims to propose a new language model (Ziya2) with 13 billion parameters and improve its performance through pre-training techniques and data-centric optimization.* Methods: The paper uses LLaMA2 as the foundation model and pre-trains Ziya2 on 700 billion tokens, with a focus on pre-training techniques and data-centric optimization.* Results: Ziya2 significantly outperforms other models in multiple benchmarks, especially when compared to representative open-source models.Here are the three key points in Simplified Chinese text:* For: 这个论文目的是提出一个新的语言模型（Ziya2），其 Parameters 为 1300亿，并通过预训练技术和数据中心优化来提高其性能。* Methods: 论文使用 LLaMA2 作为基础模型，并将 Ziya2 预训练在 7000亿 个字符上，主要关注预训练技术和数据中心优化。* Results: Ziya2 在多个 bench 上显著超过其他模型，特别是与代表性的开源模型进行比较时表现出色。<details>
<summary>Abstract</summary>
Various large language models (LLMs) have been proposed in recent years, including closed- and open-source ones, continually setting new records on multiple benchmarks. However, the development of LLMs still faces several issues, such as high cost of training models from scratch, and continual pre-training leading to catastrophic forgetting, etc. Although many such issues are addressed along the line of research on LLMs, an important yet practical limitation is that many studies overly pursue enlarging model sizes without comprehensively analyzing and optimizing the use of pre-training data in their learning process, as well as appropriate organization and leveraging of such data in training LLMs under cost-effective settings. In this work, we propose Ziya2, a model with 13 billion parameters adopting LLaMA2 as the foundation model, and further pre-trained on 700 billion tokens, where we focus on pre-training techniques and use data-centric optimization to enhance the learning process of Ziya2 on different stages. Experiments show that Ziya2 significantly outperforms other models in multiple benchmarks especially with promising results compared to representative open-source ones. Ziya2 (Base) is released at https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base and https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.
</details>
<details>
<summary>摘要</summary>
各种大型语言模型（LLMs）在最近几年内已经被提出，包括关闭和开源的一些，不断创造新的纪录在多个标准测试 benchMark 上。然而，LLMs 的开发仍面临一些问题，如从零开始训练模型的高成本，以及逐渐预训练导致忘记等等。虽然这些问题在 LLamA2 的研究中得到了一定的解决，但是在实际应用中，许多研究偏重于扩大模型的大小，而忽视了预训练数据的使用和组织，以及在成本效益的情况下进行模型训练。在这项工作中，我们提出了 Ziya2，一个拥有 1300 亿参数的模型，基于 LLaMA2 基础模型，并进行了进一步的预训练，使用 700 亿个字符。我们将注重预训练技术和数据中心化优化，以提高 Ziya2 在不同阶段的学习过程。实验结果表明，Ziya2 在多个标准测试 benchMark 上显著超越其他模型，特别是与代表性的开源模型相比，显示出了极其出色的性能。Ziya2 (Base) 在 https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base 和 https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary 上发布。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Analysis-of-Hallucination-in-GPT-4V-ision-Bias-and-Interference-Challenges"><a href="#Holistic-Analysis-of-Hallucination-in-GPT-4V-ision-Bias-and-Interference-Challenges" class="headerlink" title="Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges"></a>Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03287">http://arxiv.org/abs/2311.03287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, Huaxiu Yao</li>
<li>for: 评估 GPT-4V(ision) 模型中的偏见和干扰现象</li>
<li>methods: 利用 Bingo benchmark 评估 GPT-4V(ision) 模型的偏见和干扰问题</li>
<li>results: GPT-4V(ision) 模型存在区域偏见和干扰现象，特别是在解读西方图片或图片中包含英文文本时表现较好，而其他国家的图片或其他语言的文本时表现较差。此外，GPT-4V(ision) 模型容易受到导向问题和多个图片的混乱影响。<details>
<summary>Abstract</summary>
While GPT-4V(ision) impressively models both visual and textual information simultaneously, it's hallucination behavior has not been systematically assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias and Interference Challenges in Visual Language Models (Bingo). This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models: bias and interference. Here, bias refers to the model's tendency to hallucinate certain types of responses, possibly due to imbalance in its training data. Interference pertains to scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the text prompt is phrased or how the input image is presented. We identify a notable regional bias, whereby GPT-4V(ision) is better at interpreting Western images or images with English writing compared to images from other countries or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together. Popular mitigation approaches, such as self-correction and chain-of-thought reasoning, are not effective in resolving these challenges. We also identified similar biases and interference vulnerabilities with LLaVA and Bard. Our results characterize the hallucination challenges in GPT-4V(ision) and state-of-the-art visual-language models, and highlight the need for new solutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.
</details>
<details>
<summary>摘要</summary>
GPT-4V(ision) 模型可同时处理视觉和文本信息，但它的幻觉行为尚未系统地评估。为了bridging这个差距，我们提出了一个新的标准套件，即视觉语言模型偏见和干扰挑战（Bingo）。这个套件设计用于评估和探讨视觉语言模型中两种常见的幻觉类型：偏见和干扰。其中，偏见指的是模型幻觉某些类型的回答，可能是训练数据不均衡所致。干扰指的是场景下，GPT-4V(ision) 的判断能力受到文本提示的某些方式表述或图像的展示方式的干扰。我们发现GPT-4V(ision) 对西方图像或包含英文文本的图像具有偏见，并且容易受到提示文本的leading questions的干扰。此外，GPT-4V(ision) 在处理多个图像时也存在混乱的情况。常见的修复方法，如自我检查和链式思维，无法解决这些挑战。我们还发现了LLaVA和Bard等状态OF-the-art的视觉语言模型具有相似的偏见和干扰敏感性。我们的结果描述了GPT-4V(ision) 和状态OF-the-art的视觉语言模型中幻觉挑战，并高亮了需要新的解决方案。Bingo套件可以在https://github.com/gzcch/Bingo上下载。
</details></li>
</ul>
<hr>
<h2 id="Safurai-Csharp-Harnessing-Synthetic-Data-to-improve-language-specific-Code-LLM"><a href="#Safurai-Csharp-Harnessing-Synthetic-Data-to-improve-language-specific-Code-LLM" class="headerlink" title="Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM"></a>Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03243">http://arxiv.org/abs/2311.03243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Cifarelli, Leonardo Boiardi, Alessandro Puppo, Leon Jovanovic</li>
<li>for: 这 paper 是为了开发一个专门用于生成、完成和调试 C# 代码的开源模型。</li>
<li>methods: 该模型基于 CodeLlama 34B 模型，并利用了 EvolInstruct 技术进行精细化和扩展数据集，以便进行细化的 Fine-tuning 过程。</li>
<li>results: 该模型在 Manual MultiPL-E benchmark 上表现出色，得分为 56.33% (Zero-Shot, Pass@1)，表明它具有很高的开发人员工作流程优化和代码学习帮助的能力。<details>
<summary>Abstract</summary>
This paper introduces Safurai-Csharp, an open-source model designed to specialize in the generation, completion, and debugging of C# code. Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the EvolInstruct technique, creating a refined and expanded dataset for its fine-tuning process. The results of its performance, a notable score of 56.33% on the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), signal its high capacity to streamline developers' workflows and aid code learning. It shows promise in setting new stakes in the landscape of open-source C# LLMs and hopes to inspire more inclusive and wide-ranging development in the field of language-specific LLMs.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Safurai-Csharp" is translated as "Saifulai-C#" (各 Fulai-C#), where "Saifulai" is a combination of "Safurai" (各) and "C#" (C#).* "CodeLlama 34B" is translated as "代码 llama 34B" (代码 llama 34B), where "代码 llama" (代码 llama) is a combination of "code" (代码) and "llama" ( llama), and "34B" is added to indicate the version number.* "EvolInstruct" is translated as "演进指导" (evolution guidance), where "演进" (evolution) and "指导" (guidance) are combined to indicate the process of evolving and guiding the model.* "Manual MultiPL-E benchmark" is translated as "手动多PL-E指标" (manual multi-PL-E benchmark), where "手动" (manual) and "多PL-E" (multi-PL-E) are combined to indicate the manual process of evaluating the model's performance on multiple tasks.* "Zero-Shot, Pass@1" is translated as "零枪指标@1" (zero-shot indicator@1), where "零枪" (zero-shot) and "指标@1" (indicator@1) are combined to indicate the model's performance on unseen tasks.
</details></li>
</ul>
<hr>
<h2 id="p-Laplacian-Transformer"><a href="#p-Laplacian-Transformer" class="headerlink" title="p-Laplacian Transformer"></a>p-Laplacian Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03235">http://arxiv.org/abs/2311.03235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Nguyen, Tam Nguyen, Vinh Nguyen, Tan M. Nguyen</li>
<li>for: 这个论文主要是为了提出一种基于图和图像信号处理的新的自注意机制，以及一种基于-$p$ Laplacian regularization的Transformer架构。</li>
<li>methods: 该论文使用了自注意机制，并在自注意层中引入-$p$ Laplacian regularization来控制稀疏性和缓和效果。</li>
<li>results: 论文通过实验表明，使用$p$-Laplacian Transformer（p-LaT）可以在各种 benchmark datasets 上提高表现，并且可以更好地处理不同距离的token之间的关系。<details>
<summary>Abstract</summary>
$p$-Laplacian regularization, rooted in graph and image signal processing, introduces a parameter $p$ to control the regularization effect on these data. Smaller values of $p$ promote sparsity and interpretability, while larger values encourage smoother solutions. In this paper, we first show that the self-attention mechanism obtains the minimal Laplacian regularization ($p=2$) and encourages the smoothness in the architecture. However, the smoothness is not suitable for the heterophilic structure of self-attention in transformers where attention weights between tokens that are in close proximity and non-close ones are assigned indistinguishably. From that insight, we then propose a novel class of transformers, namely the $p$-Laplacian Transformer (p-LaT), which leverages $p$-Laplacian regularization framework to harness the heterophilic features within self-attention layers. In particular, low $p$ values will effectively assign higher attention weights to tokens that are in close proximity to the current token being processed. We empirically demonstrate the advantages of p-LaT over the baseline transformers on a wide range of benchmark datasets.
</details>
<details>
<summary>摘要</summary>
$p$- Laplacian REGULARIZATION，基于图像和图像信号处理，引入一个参数 $p$ 来控制这些数据的REGULARIZATION效果。小于 $p$ 的值推动简洁性和可读性，而大于 $p$ 的值激励更平滑的解决方案。在这篇论文中，我们首先表明了自注意机制可以获得最小 Laplacian REGULARIZATION ($p=2$)，并且激励了 Architecture 中的平滑性。然而，这种平滑性不适合 transformers 中的自注意机制，因为自注意机制中的注意量在 tokens 之间的距离和非距离之间被赋予无法分辨的权重。从这一点出发，我们then propose a novel class of transformers，即 $p$-Laplacian Transformer (p-LaT)，该框架利用 $p$-Laplacian REGULARIZATION框架来利用 transformers 中的自注意机制中的异质特征。具体来说，小于 $p$ 的值会将更高的注意量赋予 tokens 和当前被处理的token之间的close proximity。我们经验性地证明了 p-LaT 对基eline transformers 的优势在各种 benchmark 数据集上。
</details></li>
</ul>
<hr>
<h2 id="Model-based-Counterfactual-Generator-for-Gender-Bias-Mitigation"><a href="#Model-based-Counterfactual-Generator-for-Gender-Bias-Mitigation" class="headerlink" title="Model-based Counterfactual Generator for Gender Bias Mitigation"></a>Model-based Counterfactual Generator for Gender Bias Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03186">http://arxiv.org/abs/2311.03186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ewoenam Kwaku Tokpo, Toon Calders</li>
<li>for:  mitigating gender bias in natural language models</li>
<li>methods:  combination of data processing techniques and bi-objective training regime</li>
<li>results:  alleviates the shortcomings of dictionary-based solutions and improves the mitigation of gender bias<details>
<summary>Abstract</summary>
Counterfactual Data Augmentation (CDA) has been one of the preferred techniques for mitigating gender bias in natural language models. CDA techniques have mostly employed word substitution based on dictionaries. Although such dictionary-based CDA techniques have been shown to significantly improve the mitigation of gender bias, in this paper, we highlight some limitations of such dictionary-based counterfactual data augmentation techniques, such as susceptibility to ungrammatical compositions, and lack of generalization outside the set of predefined dictionary words. Model-based solutions can alleviate these problems, yet the lack of qualitative parallel training data hinders development in this direction. Therefore, we propose a combination of data processing techniques and a bi-objective training regime to develop a model-based solution for generating counterfactuals to mitigate gender bias. We implemented our proposed solution and performed an empirical evaluation which shows how our model alleviates the shortcomings of dictionary-based solutions.
</details>
<details>
<summary>摘要</summary>
<SYS>Translate the following text into Simplified Chinese:</SYS> counterfactual data augmentation (CDA) 是一种常用的技术来减轻自然语言模型中的性别偏见。 CDA 技术主要使用词替换，基于词典。 although such dictionary-based CDA techniques have been shown to significantly improve the mitigation of gender bias, in this paper, we highlight some limitations of such dictionary-based counterfactual data augmentation techniques, such as susceptibility to ungrammatical compositions, and lack of generalization outside the set of predefined dictionary words. model-based solutions can alleviate these problems, yet the lack of qualitative parallel training data hinders development in this direction. therefore, we propose a combination of data processing techniques and a bi-objective training regime to develop a model-based solution for generating counterfactuals to mitigate gender bias. we implemented our proposed solution and performed an empirical evaluation which shows how our model alleviates the shortcomings of dictionary-based solutions.Translation: counterfactual data augmentation (CDA) 是一种常用的技术来减轻自然语言模型中的性别偏见。 CDA 技术主要使用词替换，基于词典。 although such dictionary-based CDA techniques have been shown to significantly improve the mitigation of gender bias, in this paper, we highlight some limitations of such dictionary-based counterfactual data augmentation techniques, such as susceptibility to ungrammatical compositions, and lack of generalization outside the set of predefined dictionary words. 模型基本解决方案可以解决这些问题，但是缺乏质量平行训练数据限制了发展。 therefore, we propose a combination of data processing techniques and a bi-objective training regime to develop a model-based solution for generating counterfactuals to mitigate gender bias. 我们实现了我们的提议解决方案，并进行了 empirical evaluation，表明我们的模型可以解决 Dictionary-based 的缺点。
</details></li>
</ul>
<hr>
<h2 id="Architectural-Sweet-Spots-for-Modeling-Human-Label-Variation-by-the-Example-of-Argument-Quality-It’s-Best-to-Relate-Perspectives"><a href="#Architectural-Sweet-Spots-for-Modeling-Human-Label-Variation-by-the-Example-of-Argument-Quality-It’s-Best-to-Relate-Perspectives" class="headerlink" title="Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It’s Best to Relate Perspectives!"></a>Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It’s Best to Relate Perspectives!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03153">http://arxiv.org/abs/2311.03153</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phhei/relateperspectives-sweetspots">https://github.com/phhei/relateperspectives-sweetspots</a></li>
<li>paper_authors: Philipp Heinisch, Matthias Orlikowski, Julia Romberg, Philipp Cimiano</li>
<li>for: 本研究旨在探讨自然语言处理中多个注解者之间的关系如何影响注解质量。</li>
<li>methods: 研究使用了一种从推荐系统中启发的建议方法，包括模型注解者之间的关系以提高单个注解者的标签预测精度。</li>
<li>results: 研究发现，使用建议方法可以提高单个注解者的标签预测精度，最高提高$43%$。这表明，关注个体注解者的视角可以提高Subjectivity的方法。<details>
<summary>Abstract</summary>
Many annotation tasks in natural language processing are highly subjective in that there can be different valid and justified perspectives on what is a proper label for a given example. This also applies to the judgment of argument quality, where the assignment of a single ground truth is often questionable. At the same time, there are generally accepted concepts behind argumentation that form a common ground. To best represent the interplay of individual and shared perspectives, we consider a continuum of approaches ranging from models that fully aggregate perspectives into a majority label to "share nothing"-architectures in which each annotator is considered in isolation from all other annotators. In between these extremes, inspired by models used in the field of recommender systems, we investigate the extent to which architectures that include layers to model the relations between different annotators are beneficial for predicting single-annotator labels. By means of two tasks of argument quality classification (argument concreteness and validity/novelty of conclusions), we show that recommender architectures increase the averaged annotator-individual F$_1$-scores up to $43\%$ over a majority label model. Our findings indicate that approaches to subjectivity can benefit from relating individual perspectives.
</details>
<details>
<summary>摘要</summary>
In between these extremes, inspired by models used in the field of recommender systems, we investigate the extent to which architectures that include layers to model the relations between different annotators are beneficial for predicting single-annotator labels. By means of two tasks of argument quality classification (argument concreteness and validity/novelty of conclusions), we show that recommender architectures increase the averaged annotator-individual F$_1$-scores up to 43% over a majority label model. Our findings indicate that approaches to subjectivity can benefit from relating individual perspectives.Translation notes:* "Many annotation tasks" is translated as "多个标注任务" (duō gè bāng xiǎng yì xiǎng)* "natural language processing" is translated as "自然语言处理" (zì rán yǔ yán jí)* "subjective" is translated as "主观的" (zhǔ qiǎo de)* "perspectives" is translated as "视点" (shì diǎn)* "common ground" is translated as "共同基础" (gòng dòng jī chū)* "approaches" is translated as "方法" (fāng fá)* "ranging from" is translated as "从..." (cong...）* "share nothing" is translated as "没有共享" (méi yǒu gòng jiāo)* "individual F$_1$-scores" is translated as "个人 F$_1$-分数" (个人 F$_1$-分数)* "recommender architectures" is translated as "推荐建模" (tuī yù jiàn mó)* "increase" is translated as "提高" (tí gāo)* "averaged annotator-individual F$_1$-scores" is translated as "平均个人 F$_1$-分数" (píng jūn gòng rén F$_1$-分数)* "benefit from" is translated as "得益于" (de yǐ yú)* "approaches to subjectivity" is translated as "对主观的方法" (duì zhǔ qiǎo de fāng fá)
</details></li>
</ul>
<hr>
<h2 id="Injecting-Categorical-Labels-and-Syntactic-Information-into-Biomedical-NER"><a href="#Injecting-Categorical-Labels-and-Syntactic-Information-into-Biomedical-NER" class="headerlink" title="Injecting Categorical Labels and Syntactic Information into Biomedical NER"></a>Injecting Categorical Labels and Syntactic Information into Biomedical NER</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03113">http://arxiv.org/abs/2311.03113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumam Francis, Marie-Francine Moens</li>
<li>for: 提高生物医学Named EntityRecognition(NER)的精度</li>
<li>methods: 使用分类器模型和BERT模型，并将分类标签和语法信息注入到NER模型中</li>
<li>results: 对三个 benchmark dataset进行实验，结果显示将分类标签和语法信息注入到NER模型中可以提高精度，并且超过基elineBERT模型的性能<details>
<summary>Abstract</summary>
We present a simple approach to improve biomedical named entity recognition (NER) by injecting categorical labels and Part-of-speech (POS) information into the model. We use two approaches, in the first approach, we first train a sequence-level classifier to classify the sentences into categories to obtain the sentence-level tags (categorical labels). The sequence classifier is modeled as an entailment problem by modifying the labels as a natural language template. This helps to improve the accuracy of the classifier. Further, this label information is injected into the NER model. In this paper, we demonstrate effective ways to represent and inject these labels and POS attributes into the NER model. In the second approach, we jointly learn the categorical labels and NER labels. Here we also inject the POS tags into the model to increase the syntactic context of the model. Experiments on three benchmark datasets show that incorporating categorical label information with syntactic context is quite useful and outperforms baseline BERT-based models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的方法来改进生物医学命名实体识别（NER），并将分类标签和语法信息注入到模型中。我们采用两种方法：第一种方法是首先训练一个序列级分类器，以将句子分类为类别获得句子级标签（分类标签）。这个序列分类器是通过修改标签为自然语言模板来实现的，这有助于提高分类器的准确率。此外，这些标签信息还被注入到NER模型中。在这篇论文中，我们展示了如何有效地表示和注入这些标签和语法信息到NER模型中。第二种方法是同时学习分类标签和NER标签。在这里，我们还将POS标签注入到模型中，以增加语法上下文。实验表明，将分类标签和语法信息注入到BERT基于模型中可以提高NER模型的性能，并超过基eline模型。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-are-Super-Mario-Absorbing-Abilities-from-Homologous-Models-as-a-Free-Lunch"><a href="#Language-Models-are-Super-Mario-Absorbing-Abilities-from-Homologous-Models-as-a-Free-Lunch" class="headerlink" title="Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch"></a>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03099">http://arxiv.org/abs/2311.03099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yule-buaa/mergelm">https://github.com/yule-buaa/mergelm</a></li>
<li>paper_authors: Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li</li>
<li>for: 这个论文目的是探索语言模型（LM）可以通过吸收同类模型参数而获得新的能力，无需重新训练或GPU。</li>
<li>methods: 这个论文使用的方法包括超级vised Fine-Tuning（SFT）和Drop And REscale（DARE）操作来简化 delta 参数，以便将多个任务特定的 LM 合并成一个多能力模型。</li>
<li>results: 实验结果表明，DARE 可以快速地减少 delta 参数的大部分，但是继续预训练后 delta 参数的范围可以增大到约 0.03，使 DARE 无法实施。此外， merged 多个任务特定的 LM 可以创造出更高的性能。<details>
<summary>Abstract</summary>
In this paper, we uncover that Language Models (LMs), either encoder- or decoder-based, can obtain new capabilities by assimilating the parameters of homologous models without retraining or GPUs. Typically, new abilities of LMs can be imparted by Supervised Fine-Tuning (SFT), reflected in the disparity between fine-tuned and pre-trained parameters (i.e., delta parameters). We initially observe that by introducing a novel operation called DARE (Drop And REscale), most delta parameters can be directly set to zeros without affecting the capabilities of SFT LMs and larger models can tolerate a higher proportion of discarded parameters. Based on this observation, we further sparsify delta parameters of multiple SFT homologous models with DARE and subsequently merge them into a single model by parameter averaging. We conduct experiments on eight datasets from the GLUE benchmark with BERT and RoBERTa. We also merge WizardLM, WizardMath, and Code Alpaca based on Llama 2. Experimental results show that: (1) The delta parameter value ranges for SFT models are typically small, often within 0.005, and DARE can eliminate 99% of them effortlessly. However, once the models are continuously pre-trained, the value ranges can grow to around 0.03, making DARE impractical. We have also tried to remove fine-tuned instead of delta parameters and find that a 10% reduction can lead to drastically decreased performance (even to 0). This highlights that SFT merely stimulates the abilities via delta parameters rather than injecting new abilities into LMs; (2) DARE can merge multiple task-specific LMs into one LM with diverse abilities. For instance, the merger of WizardLM and WizardMath improves the GSM8K zero-shot accuracy of WizardLM from 2.2 to 66.3, retaining its instruction-following ability while surpassing WizardMath's original 64.2 performance. Codes are available at https://github.com/yule-BUAA/MergeLM.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们发现了一种新的方法，即将语言模型（LM）的参数吸收到同类模型的参数中，无需重新训练或GPU。通常，LM可以通过监督精细调整（SFT）获得新的能力，而这种能力差异可以通过 delta 参数来表示。我们发现，通过一种新的操作 called DARE（Drop And REscale），大多数 delta 参数可以直接设置为零，而不会影响 SFT LM 的能力。基于这个发现，我们进一步减轻 delta 参数的多个 SFT 同类模型，并将它们合并成一个单独的模型。我们在 GLUE  bencmark 上进行了八个数据集的实验，并将 WizardLM、WizardMath 和 Code Alpaca 基于 Llama 2 进行了合并。实验结果表明：（1）SFT 模型的 delta 参数范围通常在 0.005 之间，DARE 可以轻松地消除 99% 的 delta 参数。然而，一旦模型进行了连续预训练， delta 参数的范围可以增长到约 0.03，使 DARE 不太实用。我们还尝试了从 fine-tuned 参数中删除 delta 参数，发现可以将其减少到 10%，但这会导致性能锐减（甚至为 0）。这显示出 SFT 仅仅激活 LM 的能力，而不是把新的能力注入到 LM 中；（2）DARE 可以将多个任务特定 LM 合并成一个多能力 LM。例如，将 WizardLM 和 WizardMath 合并的 GSM8K 零shot准确率从 2.2 提高到 66.3，保留 WizardLM 的指令遵从能力，同时超越 WizardMath 的原始 64.2 性能。代码可以在 GitHub 上找到：https://github.com/yule-BUAA/MergeLM。
</details></li>
</ul>
<hr>
<h2 id="BanLemma-A-Word-Formation-Dependent-Rule-and-Dictionary-Based-Bangla-Lemmatizer"><a href="#BanLemma-A-Word-Formation-Dependent-Rule-and-Dictionary-Based-Bangla-Lemmatizer" class="headerlink" title="BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer"></a>BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03078">http://arxiv.org/abs/2311.03078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eblict-gigatech/BanLemma">https://github.com/eblict-gigatech/BanLemma</a></li>
<li>paper_authors: Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed</li>
<li>for: 本研究旨在提出一个特别设计 для Bangla 的 lemmatizer，以提高 Bangla 自然语言处理 (NLP) 的效能。</li>
<li>methods: 本研究使用了语言规则来定义 lemmatization 规则，并使用字典和规则来设计一个特别的 lemmatizer。它以 sentence 中每个词的 part of speech 类型为基础，对每个词进行 lemmatization。不同于先前的规则based方法，本研究分析了 Bangla 文本中各种词干的 suffix marker 的出现，并使用了 suffix marker 的序列而不是整个 suffix。</li>
<li>results: 本研究的 lemmatizer 在一个 manually annotated 的 test dataset 上 achiev 了 96.36% 的准确率，并在三个先前发表的 Bangla lemmatization dataset 上显示了竞争力的性能。<details>
<summary>Abstract</summary>
Lemmatization holds significance in both natural language processing (NLP) and linguistics, as it effectively decreases data density and aids in comprehending contextual meaning. However, due to the highly inflected nature and morphological richness, lemmatization in Bangla text poses a complex challenge. In this study, we propose linguistic rules for lemmatization and utilize a dictionary along with the rules to design a lemmatizer specifically for Bangla. Our system aims to lemmatize words based on their parts of speech class within a given sentence. Unlike previous rule-based approaches, we analyzed the suffix marker occurrence according to the morpho-syntactic values and then utilized sequences of suffix markers instead of entire suffixes. To develop our rules, we analyze a large corpus of Bangla text from various domains, sources, and time periods to observe the word formation of inflected words. The lemmatizer achieves an accuracy of 96.36% when tested against a manually annotated test dataset by trained linguists and demonstrates competitive performance on three previously published Bangla lemmatization datasets. We are making the code and datasets publicly available at https://github.com/eblict-gigatech/BanLemma in order to contribute to the further advancement of Bangla NLP.
</details>
<details>
<summary>摘要</summary>
lemmatization在自然语言处理（NLP）和语言学中具有重要意义，因为它有效地减少数据密度，并帮助理解上下文中的含义。然而，由于孟加拉语的高度变格和 morphological richness，孟加拉语 lemmatization 提出了复杂的挑战。在这种研究中，我们提出了语言规则 для lemmatization 并使用词典和规则来设计特有的孟加拉语 lemmatizer。我们的系统 aimsto lemmatize words based on their parts of speech class within a given sentence。不同于前一些规则基本的方法，我们分析了 suffix marker 的出现根据 morpho-syntactic values，然后使用 sequences of suffix markers instead of entire suffixes。为了开发我们的规则，我们分析了大量的孟加拉语文本，从不同的领域、来源和时期，以观察word formation of inflected words。lemmatizer 在一个手动标注的测试集上达到了 96.36% 的准确率，并在三个之前发布的孟加拉语 lemmatization 数据集上表现了竞争性。我们将代码和数据集公开发布在 GitHub 上，以便贡献到孟加拉语 NLP 的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Bilingual-App-Reviews-Mining-with-Large-Language-Models"><a href="#Zero-shot-Bilingual-App-Reviews-Mining-with-Large-Language-Models" class="headerlink" title="Zero-shot Bilingual App Reviews Mining with Large Language Models"></a>Zero-shot Bilingual App Reviews Mining with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03058">http://arxiv.org/abs/2311.03058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jl-wei/mini-bar">https://github.com/jl-wei/mini-bar</a></li>
<li>paper_authors: Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, Gérard Dray</li>
<li>for: 这个论文的目的是提高软件需求的改进，通过自动EXTRACT和SUMMARIZE用户评论来帮助开发人员更好地理解用户需求。</li>
<li>methods: 这个论文使用了大型自然语言处理（NLP）模型，包括分类、聚合和摘要等方法，以自动挖掘用户评论中的有用信息。</li>
<li>results: 实验结果表明，Mini-BAR可以有效地分类、聚合和摘要用户评论，并且可以在英文和法语两种语言中进行零shot学习。<details>
<summary>Abstract</summary>
App reviews from app stores are crucial for improving software requirements. A large number of valuable reviews are continually being posted, describing software problems and expected features. Effectively utilizing user reviews necessitates the extraction of relevant information, as well as their subsequent summarization. Due to the substantial volume of user reviews, manual analysis is arduous. Various approaches based on natural language processing (NLP) have been proposed for automatic user review mining. However, the majority of them requires a manually crafted dataset to train their models, which limits their usage in real-world scenarios. In this work, we propose Mini-BAR, a tool that integrates large language models (LLMs) to perform zero-shot mining of user reviews in both English and French. Specifically, Mini-BAR is designed to (i) classify the user reviews, (ii) cluster similar reviews together, (iii) generate an abstractive summary for each cluster and (iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we created a dataset containing 6,000 English and 6,000 French annotated user reviews and conducted extensive experiments. Preliminary results demonstrate the effectiveness and efficiency of Mini-BAR in requirement engineering by analyzing bilingual app reviews. (Replication package containing the code, dataset, and experiment setups on https://github.com/Jl-wei/mini-bar )
</details>
<details>
<summary>摘要</summary>
应用商店中的用户评论是软件需求的关键来源。大量有价值的评论不断上传，描述软件问题和期望功能。有效利用用户评论需要提取有用信息，并将其概括。由于用户评论的数量太多，人工分析是困难的。基于自然语言处理（NLP）的多种方法已经被提议用于自动用户评论挖掘。然而，大多数其中需要手动创建数据集来训练其模型，这限制了它们在实际场景中的使用。在这项工作中，我们提出了“小BAR”工具，它通过大型自然语言模型（LLM）来完成零shot用户评论挖掘。具体来说，小BAR是设计来：（i）分类用户评论，（ii）将相似的评论集成在一起，（iii）为每个分组生成抽象摘要，以及（iv）对用户评论分组进行排名。为了评估小BAR的性能，我们创建了包含6,000个英语和6,000个法语用户评论的数据集，并进行了广泛的实验。初步结果表明小BAR在需求工程中的效果和效率，通过分析双语应用评论。（复制包含代码、数据集和实验设置的代码在https://github.com/Jl-wei/mini-bar）
</details></li>
</ul>
<hr>
<h2 id="Detecting-Agreement-in-Multi-party-Conversational-AI"><a href="#Detecting-Agreement-in-Multi-party-Conversational-AI" class="headerlink" title="Detecting Agreement in Multi-party Conversational AI"></a>Detecting Agreement in Multi-party Conversational AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03026">http://arxiv.org/abs/2311.03026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Schauer, Jason Sweeney, Charlie Lyttle, Zein Said, Aron Szeles, Cale Clark, Katie McAskill, Xander Wickham, Tom Byars, Daniel Hernández Garcia, Nancie Gunson, Angus Addlesee, Oliver Lemon</li>
<li>for: 这个论文的目的是提出一种多方会话系统，用于 Socially Assistive Robots (SARs) 中的多方会话。</li>
<li>methods: 该论文使用了一种基于协调识别和复杂转接的多方会话系统，并在评估中使用了两名用户参与的评估测试。</li>
<li>results: 论文的评估结果显示，该系统可以准确地检测用户的一致或不一致，并根据用户的回答进行相应的响应。<details>
<summary>Abstract</summary>
Today, conversational systems are expected to handle conversations in multi-party settings, especially within Socially Assistive Robots (SARs). However, practical usability remains difficult as there are additional challenges to overcome, such as speaker recognition, addressee recognition, and complex turn-taking. In this paper, we present our work on a multi-party conversational system, which invites two users to play a trivia quiz game. The system detects users' agreement or disagreement on a final answer and responds accordingly. Our evaluation includes both performance and user assessment results, with a focus on detecting user agreement. Our annotated transcripts and the code for the proposed system have been released open-source on GitHub.
</details>
<details>
<summary>摘要</summary>
今天，对话系统需要在多方会话中处理对话，特别是在社交辅助机器人（SARs）中。然而，实际用户体验仍然具有困难，因为需要解决多种挑战，如说话人识别、接收人识别和复杂的回答交互。在这篇论文中，我们介绍了一种多方对话系统，其中两名用户参与一场智能答题游戏。系统可以检测用户的同意或不同意 final 答案，并根据此进行响应。我们的评估包括性能评估和用户评估结果，强调检测用户同意的能力。我们已经在 GitHub 上公开发布了注释过的讯息记录和提议的系统代码。
</details></li>
</ul>
<hr>
<h2 id="Detecting-agreement-in-multi-party-dialogue-evaluating-speaker-diarisation-versus-a-procedural-baseline-to-enhance-user-engagement"><a href="#Detecting-agreement-in-multi-party-dialogue-evaluating-speaker-diarisation-versus-a-procedural-baseline-to-enhance-user-engagement" class="headerlink" title="Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement"></a>Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03021">http://arxiv.org/abs/2311.03021</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddenley/multi-person-quiz">https://github.com/ddenley/multi-person-quiz</a></li>
<li>paper_authors: Angus Addlesee, Daniel Denley, Andy Edmondson, Nancie Gunson, Daniel Hernández Garcia, Alexandre Kha, Oliver Lemon, James Ndubuisi, Neil O’Reilly, Lia Perochaud, Raphaël Valeri, Miebaka Worika</li>
<li>for: 本研究旨在探讨对话状态跟踪在多方交互中的挑战，以及使用扩声模型和频率和 proximity 方法来确定对话内容。</li>
<li>methods: 本研究使用了合作测验，其中对话机器人扮演了答题游戏的主持人，以确定响应者是否达成一致。</li>
<li>results: 实验结果表明，我们的程序体系更加有趣，并且更准确地确定了一致，其准确率为 0.44，而对比的扩声系统的准确率为 0.28。<details>
<summary>Abstract</summary>
Conversational agents participating in multi-party interactions face significant challenges in dialogue state tracking, since the identity of the speaker adds significant contextual meaning. It is common to utilise diarisation models to identify the speaker. However, it is not clear if these are accurate enough to correctly identify specific conversational events such as agreement or disagreement during a real-time interaction. This study uses a cooperative quiz, where the conversational agent acts as quiz-show host, to determine whether diarisation or a frequency-and-proximity-based method is more accurate at determining agreement, and whether this translates to feelings of engagement from the players. Experimental results show that our procedural system was more engaging to players, and was more accurate at detecting agreement, reaching an average accuracy of 0.44 compared to 0.28 for the diarised system.
</details>
<details>
<summary>摘要</summary>
多个人party交流中的对话管理器面临着很大的挑战，因为发言人的身份带有丰富的上下文含义。通常使用 диари化模型来确定发言人。然而，不清楚这些模型是否具有准确地确定对话事件的能力，如同意或不同意在实时交流中。本研究使用合作测验，将对话管理器 acted as quiz show 主持人，以确定 диари化或频率和距离基于方法是哪一种更准确地确定一致，以及这种精度是否对玩家产生了参与感。实验结果表明，我们的过程系统更加有趣，并且更准确地确定一致，达到了0.44的平均准确率，比0.28的 диари化系统更高。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Transformer-Based-Reverse-Dictionary-Model-for-Quality-Estimation-of-Definitions"><a href="#Towards-a-Transformer-Based-Reverse-Dictionary-Model-for-Quality-Estimation-of-Definitions" class="headerlink" title="Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions"></a>Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02985">http://arxiv.org/abs/2311.02985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guité-Vinet Julien, Blondin Massé Alexandre, Sadat Fatiha</li>
<li>for: 这篇论文是为了研究不同变体的transformer模型在解决反词典任务上的表现，以及这些模型在严肃游戏《词典游戏》中的应用。</li>
<li>methods: 本文使用了多种transformer模型，包括Bert、RoBERTa、XLNet等，进行对比研究。</li>
<li>results: 研究结果显示，Bert模型在解决反词典任务上表现最佳，而XLNet模型在某些任务上表现较差。<details>
<summary>Abstract</summary>
In the last years, several variants of transformers have emerged. In this paper, we compare different transformer-based models for solving the reverse dictionary task and explore their use in the context of a serious game called The Dictionary Game.
</details>
<details>
<summary>摘要</summary>
最近几年，transformer的多种变体出现了。本文比较了不同基于transformer的模型，用于解决反ictionary任务，并在serious game《词典游戏》的context中 explore their use。Here's the breakdown of the translation:* 最近几年 (last years) becomes 最近几年 (last years)* transformer的多种变体 (variants of transformers) becomes  transformer的多种变体 (variants of transformers)* 出现了 (emerged) becomes 出现了 (emerged)* 本文 (this paper) becomes 本文 (this paper)* 比较 (compare) becomes 比较 (compare)* 不同基于 (different based on) becomes 不同基于 (different based on)* transformer (transformer) becomes transformer (transformer)* 模型 (model) becomes 模型 (model)* 用于 (for) becomes 用于 (for)* 解决 (solve) becomes 解决 (solve)* 反ictionary (reverse dictionary) becomes 反ictionary (reverse dictionary)* 任务 (task) becomes 任务 (task)* 并 (and) becomes 并 (and)* 在 (in) becomes 在 (in)* serious game (serious game) becomes 严肃游戏 (serious game)* 《词典游戏》 (The Dictionary Game) becomes 《词典游戏》 (The Dictionary Game)
</details></li>
</ul>
<hr>
<h2 id="Adapting-Pre-trained-Generative-Models-for-Extractive-Question-Answering"><a href="#Adapting-Pre-trained-Generative-Models-for-Extractive-Question-Answering" class="headerlink" title="Adapting Pre-trained Generative Models for Extractive Question Answering"></a>Adapting Pre-trained Generative Models for Extractive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02961">http://arxiv.org/abs/2311.02961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prabirmallick/GenAI4EQA">https://github.com/prabirmallick/GenAI4EQA</a></li>
<li>paper_authors: Prabir Mallick, Tapas Nayak, Indrajit Bhattacharya</li>
<li>for: 本文旨在探讨将先进的生成模型应用于抽取式问答任务中，以提高问答效果。</li>
<li>methods: 本文提出了一种新的方法，通过使用预训练的生成模型生成上下文字符或句子的索引，以便更好地解决抽取式问答任务。</li>
<li>results: 经过对多个抽取式问答数据集的详细评估，包括MultiSpanQA、BioASQ、MASHQA和WikiQA，本文的提议方法表现出了较高的性能，比之前的状态艺术模型更好。<details>
<summary>Abstract</summary>
Pre-trained Generative models such as BART, T5, etc. have gained prominence as a preferred method for text generation in various natural language processing tasks, including abstractive long-form question answering (QA) and summarization. However, the potential of generative models in extractive QA tasks, where discriminative models are commonly employed, remains largely unexplored. Discriminative models often encounter challenges associated with label sparsity, particularly when only a small portion of the context contains the answer. The challenge is more pronounced for multi-span answers. In this work, we introduce a novel approach that uses the power of pre-trained generative models to address extractive QA tasks by generating indexes corresponding to context tokens or sentences that form part of the answer. Through comprehensive evaluations on multiple extractive QA datasets, including MultiSpanQA, BioASQ, MASHQA, and WikiQA, we demonstrate the superior performance of our proposed approach compared to existing state-of-the-art models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PhoGPT-Generative-Pre-training-for-Vietnamese"><a href="#PhoGPT-Generative-Pre-training-for-Vietnamese" class="headerlink" title="PhoGPT: Generative Pre-training for Vietnamese"></a>PhoGPT: Generative Pre-training for Vietnamese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02945">http://arxiv.org/abs/2311.02945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinairesearch/phogpt">https://github.com/vinairesearch/phogpt</a></li>
<li>paper_authors: Dat Quoc Nguyen, Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Nhung Nguyen, Thien Huu Nguyen, Dinh Phung, Hung Bui</li>
<li>for: 这个论文是为了介绍一种新的开源generative模型系列PhoGPT，用于越南语言处理。</li>
<li>methods: 这个模型使用了7.5亿个参数，包括基础预训练单语言模型PhoGPT-7B5以及其 instrucion-following变体PhoGPT-7B5-Instruct。</li>
<li>results: 作者通过人工评估实验表明，这个模型在前一代开源模型的比较中显示出了更高的性能。Here’s the English version for reference:</li>
<li>for: This paper introduces a new open-source generative model series named PhoGPT for Vietnamese language processing.</li>
<li>methods: The model uses 7.5 billion parameters, including the base pre-trained monolingual model PhoGPT-7B5 and its instruction-following variant, PhoGPT-7B5-Instruct.</li>
<li>results: The authors demonstrate the superior performance of the model compared to previous open-source models through a human evaluation experiment.<details>
<summary>Abstract</summary>
We open-source a state-of-the-art 7.5B-parameter generative model series named PhoGPT for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-7B5 and its instruction-following variant, PhoGPT-7B5-Instruct. In addition, we also demonstrate its superior performance compared to previous open-source models through a human evaluation experiment. GitHub: https://github.com/VinAIResearch/PhoGPT
</details>
<details>
<summary>摘要</summary>
我们开源了一种高度进步的7.5亿参数生成模型系列，名为 PhoGPT，用于越南语言。该系列包括基础预训练单语言模型 PhoGPT-7B5 和其 instrucion-following 变体 PhoGPT-7B5-Instruct。此外，我们还通过人工评估实验证明了该模型与前一代开源模型的superior表现。GitHub：https://github.com/VinAIResearch/PhoGPT。Note: "PhoGPT" is the name of the model series, and "PhoGPT-7B5" and "PhoGPT-7B5-Instruct" are the specific models within the series.
</details></li>
</ul>
<hr>
<h2 id="SQLPrompt-In-Context-Text-to-SQL-with-Minimal-Labeled-Data"><a href="#SQLPrompt-In-Context-Text-to-SQL-with-Minimal-Labeled-Data" class="headerlink" title="SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data"></a>SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02883">http://arxiv.org/abs/2311.02883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruoxi Sun, Sercan Ö. Arik, Rajarishi Sinha, Hootan Nakhost, Hanjun Dai, Pengcheng Yin, Tomas Pfister</li>
<li>for: 提高 Text-to-SQL 模型在大型自然语言处理器（LLM）上的几 shot 提示能力</li>
<li>methods: 创新的提示设计、执行基于一致性选择策略、以及在一致性选择中使用不同提示设计和基础模型进行多样化提案策略</li>
<li>results: 在具有少量标注数据的情况下，SQLPrompt 可以大幅提高 Text-to-SQL 模型的几 shot 学习性能，并且可以追近训练 state-of-the-art 的性能水平，但需要使用数千个标注数据进行训练。<details>
<summary>Abstract</summary>
Text-to-SQL aims to automate the process of generating SQL queries on a database from natural language text. In this work, we propose "SQLPrompt", tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs). Our methods include innovative prompt design, execution-based consistency decoding strategy which selects the SQL with the most consistent execution outcome among other SQL proposals, and a method that aims to improve performance by diversifying the SQL proposals during consistency selection with different prompt designs ("MixPrompt") and foundation models ("MixLLMs"). We show that \emph{SQLPrompt} outperforms previous approaches for in-context learning with few labeled data by a large margin, closing the gap with finetuning state-of-the-art with thousands of labeled data.
</details>
<details>
<summary>摘要</summary>
文本到SQL是一个自动生成SQL查询语句的数据库的过程。在这个工作中，我们提出了“SQLPrompt”，用于改进文本到SQL中大语言模型（LLM）的几个shot提示能力。我们的方法包括创新的提示设计、执行基于一致性解码策略和多种提示设计和基础模型“混合提示”以提高性能。我们示出了 compared to previous approaches，我们的方法可以在受限的培auoted data上进行Context learning，大幅提高SQLPrompt的性能，并且可以追赶到finetuning状态的水平。
</details></li>
</ul>
<hr>
<h2 id="Less-than-One-shot-Named-Entity-Recognition-via-Extremely-Weak-Supervision"><a href="#Less-than-One-shot-Named-Entity-Recognition-via-Extremely-Weak-Supervision" class="headerlink" title="Less than One-shot: Named Entity Recognition via Extremely Weak Supervision"></a>Less than One-shot: Named Entity Recognition via Extremely Weak Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02861">http://arxiv.org/abs/2311.02861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KomeijiForce/X-NER">https://github.com/KomeijiForce/X-NER</a></li>
<li>paper_authors: Letian Peng, Zihan Wang, Jingbo Shang</li>
<li>for: 本研究targets the named entity recognition (NER) problem under the extremely weak supervision (XWS) setting, where only one example entity per type is given in a context-free way.</li>
<li>methods: 我们提出了一种新的方法X-NER，它可以在XWS setting下超过现有的一shot NER方法的性能。我们首先从无标注训练集中挖掘 Entity span，然后使用这些 span 的上下文分布来训练 NER 标注器。</li>
<li>results: 我们在4个 NER  dataset上进行了广泛的实验和分析，结果显示 X-NER 可以具有state-of-the-art的 few-shot NER 性能，并且在1shot supervision和ChatGPT标注下显著超过现有方法。此外，X-NER 还具有跨语言能力。<details>
<summary>Abstract</summary>
We study the named entity recognition (NER) problem under the extremely weak supervision (XWS) setting, where only one example entity per type is given in a context-free way. While one can see that XWS is lighter than one-shot in terms of the amount of supervision, we propose a novel method X-NER that can outperform the state-of-the-art one-shot NER methods. We first mine entity spans that are similar to the example entities from an unlabelled training corpus. Instead of utilizing entity span representations from language models, we find it more effective to compare the context distributions before and after the span is replaced by the entity example. We then leverage the top-ranked spans as pseudo-labels to train an NER tagger. Extensive experiments and analyses on 4 NER datasets show the superior end-to-end NER performance of X-NER, outperforming the state-of-the-art few-shot methods with 1-shot supervision and ChatGPT annotations significantly. Finally, our X-NER possesses several notable properties, such as inheriting the cross-lingual abilities of the underlying language models.
</details>
<details>
<summary>摘要</summary>
我们研究名实体识别（NER）问题在极其轻量级指导（XWS） Setting下，只有一个例子实体每种类型。虽然XWS比一shot更轻量级，但我们提出了一种新方法X-NER，可以超越现状最佳一shot NER方法。我们首先在无标注训练集中挖掘类似实体示例的 span。而不是使用语言模型中的实体span表示，我们发现更有效的是在 span 前后的上下文分布比较。然后，我们利用top排名 span 作为pseudo标签来训练 NER 标注器。我们对4个NER数据集进行了广泛的实验和分析，显示 X-NER 具有显著的端到端 NER性能优势，比现状最佳几个shot方法和ChatGPT标注更加显著。最后，我们的 X-NER 具有一些注意的性能特性，如继承下来语言模型的cross-Lingual能力。
</details></li>
</ul>
<hr>
<h2 id="Improving-Machine-Translation-with-Large-Language-Models-A-Preliminary-Study-with-Cooperative-Decoding"><a href="#Improving-Machine-Translation-with-Large-Language-Models-A-Preliminary-Study-with-Cooperative-Decoding" class="headerlink" title="Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"></a>Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02851">http://arxiv.org/abs/2311.02851</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lemon0830/CoDec">https://github.com/lemon0830/CoDec</a></li>
<li>paper_authors: Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou</li>
<li>for: 这 paper 是为了探讨 NMT 系统和 MT-oriented LLMs 在翻译中的竞争关系，以及如何使用这两种技术来提高翻译质量。</li>
<li>methods: 这 paper 使用了多种商业 NMT 系统和 MT-oriented LLMs 进行比较分析，以了解它们在不同的翻译场景中的优劣点。然后，基于这些发现，提出了一种 hybrid 方法——Cooperative Decoding (CoDec)，它将 NMT 系统作为预翻译模型，并将 MT-oriented LLMs 作为补充解决方案来处理复杂的翻译问题。</li>
<li>results: 试验结果表明，CoDec 能够有效地和高效地结合 NMT 系统和 MT-oriented LLMs，在 WMT22 测试集和一个新收集的 WebCrawl 测试集上达到了显著的改进。这些结果表明，CoDec 可以作为一种可靠的解决方案，用于将 NMT 系统和 MT-oriented LLMs 结合使用于翻译中。<details>
<summary>Abstract</summary>
Contemporary translation engines built upon the encoder-decoder framework have reached a high level of development, while the emergence of Large Language Models (LLMs) has disrupted their position by offering the potential for achieving superior translation quality. Therefore, it is crucial to understand in which scenarios LLMs outperform traditional NMT systems and how to leverage their strengths. In this paper, we first conduct a comprehensive analysis to assess the strengths and limitations of various commercial NMT systems and MT-oriented LLMs. Our findings indicate that neither NMT nor MT-oriented LLMs alone can effectively address all the translation issues, but MT-oriented LLMs can serve as a promising complement to the NMT systems. Building upon these insights, we explore hybrid methods and propose Cooperative Decoding (CoDec), which treats NMT systems as a pretranslation model and MT-oriented LLMs as a supplemental solution to handle complex scenarios beyond the capability of NMT alone. The results on the WMT22 test sets and a newly collected test set WebCrawl demonstrate the effectiveness and efficiency of CoDec, highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in machine translation.
</details>
<details>
<summary>摘要</summary>
当代翻译引擎，基于编码器-解码器框架，已经到了高度的发展阶段，而大语言模型（LLMs）的出现，对传统的新型机器翻译（NMT）系统产生了冲击，提供了可以实现更高质量翻译的潜在力量。因此，理解LLMs在哪些场景下表现出色，并如何利用其优势，是非常重要的。在这篇论文中，我们首先进行了全面的分析，以评估不同的商业NMT系统和MT-oriented LLMs的优缺点。我们的发现表明，NMT系统和MT-oriented LLMs单独无法有效地解决所有翻译问题，但MT-oriented LLMs可以作为NMT系统的补充解决方案，处理NMT无法涵盖的复杂场景。基于这些发现，我们探索了 гибрид方法，并提出了合作解码（CoDec），它将NMT系统作为先translation模型，MT-oriented LLMs作为补充解决方案，用于处理NMT无法涵盖的复杂场景。results on WMT22 test sets和我们新收集的WebCrawl test set表明，CoDec是一种有效和高效的解决方案，强调其在组合NMT系统和MT-oriented LLMs的机器翻译中的潜在力量。
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Self-Rationalizers-with-Multi-Reward-Distillation"><a href="#Tailoring-Self-Rationalizers-with-Multi-Reward-Distillation" class="headerlink" title="Tailoring Self-Rationalizers with Multi-Reward Distillation"></a>Tailoring Self-Rationalizers with Multi-Reward Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02805">http://arxiv.org/abs/2311.02805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ink-usc/rationalemultirewarddistillation">https://github.com/ink-usc/rationalemultirewarddistillation</a></li>
<li>paper_authors: Sahana Ramnath, Brihi Joshi, Skyler Hallinan, Ximing Lu, Liunian Harold Li, Aaron Chan, Jack Hessel, Yejin Choi, Xiang Ren</li>
<li>for: 这个论文的目的是提高小型语言模型（LMs）的问答能力，并且可以生成更有用的自我合理化来帮助 humans 理解模型的决策过程。</li>
<li>methods: 这个论文使用了一种名为 MaRio（Multi-rewArd RatIOnalization）的多奖准则自我合理化算法，通过优化多个特征（如可能性、多样性和一致性）来提高小LMs 的问答能力和自我合理化质量。</li>
<li>results: 实验结果表明，MaRio 可以在五个复杂的问答任务上提高任务的准确率，同时也提高小LMs 的自我合理化质量，比如可能性、多样性和一致性。  besides, human评估表明，MaRio 的合理化推理比 SFT 基线更受欢迎，并且有质量上的提升。<details>
<summary>Abstract</summary>
Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale LMs (approx. 200x smaller than GPT-3) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, MaRio (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on five difficult question-answering datasets StrategyQA, QuaRel, OpenBookQA, NumerSense and QASC show that not only does MaRio improve task accuracy, but it also improves the self-rationalization quality of small LMs across the aforementioned axes better than a supervised fine-tuning (SFT) baseline. Extensive human evaluations confirm that MaRio rationales are preferred vs. SFT rationales, as well as qualitative improvements in plausibility and consistency.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LM）能够生成自由文本论据以帮助问答。然而，先前的研究表明，有用的自我合理化只有在较大的Scale（例如175B参数的GPT-3）下才能出现；而且大多数研究都将注意力集中在下游性能上，忽略论据本身的 semantics，例如它们是否 faithful、true 和有用 для人类？在这项工作中，我们使得小型语言模型（约200倍小于GPT-3）能够生成不 только提高下游任务性能的论据，而且也更可靠、多样和一致，通过自动和人类评估。我们的方法，MaRio（多重rewArd RatIOnalization），是一种多重奖励条件自我合理化算法，该算法优化多个不同的属性，如可能性、多样性和一致性。结果表明，MaRio在五个复杂的问答任务StrategYQA、QuaRel、OpenBookQA、NumerSense和QASC上表现出色，不仅提高了任务准确率，而且也在论据质量上超越基eline。人类评估表明，MaRio的论据比SFT基eline更受欢迎，并且有质量上的改进。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.CL_2023_11_06/" data-id="clopawnr600dyag885lnwcj2z" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/06/cs.AI_2023_11_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.AI - 2023-11-06
        
      </div>
    </a>
  
  
    <a href="/2023/11/06/cs.LG_2023_11_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-11-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
