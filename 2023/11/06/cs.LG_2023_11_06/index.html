
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-11-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03615 repo_url: None paper_authors: Jieming Bian, Shaolei Ren, Jie Xu for: This pa">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-11-06">
<meta property="og:url" content="https://nullscc.github.io/2023/11/06/cs.LG_2023_11_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03615 repo_url: None paper_authors: Jieming Bian, Shaolei Ren, Jie Xu for: This pa">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-06T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-08T05:49:48.963Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.LG_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T10:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-11-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CAFE-Carbon-Aware-Federated-Learning-in-Geographically-Distributed-Data-Centers"><a href="#CAFE-Carbon-Aware-Federated-Learning-in-Geographically-Distributed-Data-Centers" class="headerlink" title="CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers"></a>CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03615">http://arxiv.org/abs/2311.03615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieming Bian, Shaolei Ren, Jie Xu</li>
<li>for: This paper aims to address the challenges of training large-scale AI models while minimizing their carbon footprint.</li>
<li>methods: The paper proposes a new framework called CAFE (Carbon-Aware Federated Learning) that incorporates coreset selection, Lyapunov drift-plus-penalty, and efficient algorithm to optimize training within a fixed carbon footprint budget.</li>
<li>results: The paper demonstrates the efficacy of the proposed algorithm through extensive simulations using real-world carbon intensity data, showing its superiority over existing methods in optimizing learning performance while minimizing environmental impact.<details>
<summary>Abstract</summary>
Training large-scale artificial intelligence (AI) models demands significant computational power and energy, leading to increased carbon footprint with potential environmental repercussions. This paper delves into the challenges of training AI models across geographically distributed (geo-distributed) data centers, emphasizing the balance between learning performance and carbon footprint. We consider Federated Learning (FL) as a solution, which prioritizes model parameter exchange over raw data, ensuring data privacy and compliance with local regulations. Given the variability in carbon intensity across regions, we propose a new framework called CAFE (short for Carbon-Aware Federated Learning) to optimize training within a fixed carbon footprint budget. Our approach incorporates coreset selection to assess learning performance, employs the Lyapunov drift-plus-penalty framework to address the unpredictability of future carbon intensity, and devises an efficient algorithm to address the combinatorial complexity of the data center selection. Through extensive simulations using real-world carbon intensity data, we demonstrate the efficacy of our algorithm, highlighting its superiority over existing methods in optimizing learning performance while minimizing environmental impact.
</details>
<details>
<summary>摘要</summary>
训练大规模人工智能（AI）模型需要巨大的计算能力和能源，导致增加碳脚印的可能性，有潜在的环境影响。这篇论文探讨跨地区分布的数据中心（geo-distributed）训练AI模型所遇到的挑战，强调学习性和碳脚印之间的平衡。我们认为 Federated Learning（FL）是一种解决方案，它强调模型参数交换而不是原始数据，保证数据隐私和当地法规的遵守。由于地区碳Intensity的变化，我们提出了一个新的框架called CAFE（缩写为 Carbon-Aware Federated Learning），以优化在固定碳脚印预算内进行训练。我们的方法包括核心选择来评估学习性能，使用Lyapunov逸偏离策略来Address未来碳Intensity的不可预知性，并开发了高效的数据中心选择算法。通过使用实际碳Intensity数据进行大规模的 simulations，我们证明了我们的算法的有效性， highlighting its superiority over existing methods in optimizing learning performance while minimizing environmental impact.
</details></li>
</ul>
<hr>
<h2 id="Plug-and-Play-Stability-for-Intracortical-Brain-Computer-Interfaces-A-One-Year-Demonstration-of-Seamless-Brain-to-Text-Communication"><a href="#Plug-and-Play-Stability-for-Intracortical-Brain-Computer-Interfaces-A-One-Year-Demonstration-of-Seamless-Brain-to-Text-Communication" class="headerlink" title="Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication"></a>Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03611">http://arxiv.org/abs/2311.03611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cffan/corp">https://github.com/cffan/corp</a></li>
<li>paper_authors: Chaofei Fan, Nick Hahn, Foram Kamdar, Donald Avansino, Guy H. Wilson, Leigh Hochberg, Krishna V. Shenoy, Jaimie M. Henderson, Francis R. Willett</li>
<li>for: This paper aims to address the issue of frequent recalibration required for intracortical brain-computer interfaces (iBCIs) to maintain high performance, which can be time-consuming and interrupt the user’s experience.</li>
<li>methods: The proposed method, called Continual Online Recalibration with Pseudo-labels (CORP), leverages large language models to automatically correct errors in iBCI outputs and update the decoder online.</li>
<li>results: The CORP framework achieved a stable decoding accuracy of 93.84% in an online handwriting iBCI task over a period of 403 days, significantly outperforming other baseline methods. This is the longest-running iBCI stability demonstration involving a human participant, providing evidence for the long-term stabilization of a plug-and-play, high-performance communication iBCI.<details>
<summary>Abstract</summary>
Intracortical brain-computer interfaces (iBCIs) have shown promise for restoring rapid communication to people with neurological disorders such as amyotrophic lateral sclerosis (ALS). However, to maintain high performance over time, iBCIs typically need frequent recalibration to combat changes in the neural recordings that accrue over days. This requires iBCI users to stop using the iBCI and engage in supervised data collection, making the iBCI system hard to use. In this paper, we propose a method that enables self-recalibration of communication iBCIs without interrupting the user. Our method leverages large language models (LMs) to automatically correct errors in iBCI outputs. The self-recalibration process uses these corrected outputs ("pseudo-labels") to continually update the iBCI decoder online. Over a period of more than one year (403 days), we evaluated our Continual Online Recalibration with Pseudo-labels (CORP) framework with one clinical trial participant. CORP achieved a stable decoding accuracy of 93.84% in an online handwriting iBCI task, significantly outperforming other baseline methods. Notably, this is the longest-running iBCI stability demonstration involving a human participant. Our results provide the first evidence for long-term stabilization of a plug-and-play, high-performance communication iBCI, addressing a major barrier for the clinical translation of iBCIs.
</details>
<details>
<summary>摘要</summary>
干预 cortical 脑机器交互（iBCI）已经表现出了恢复快速通信的潜力，尤其是对于amyotrophic lateral sclerosis（ALS）等神经系统疾病。然而，以保持高性能而言，iBCI通常需要频繁重新寻常化，以避免因日期而增加的神经记录变化。这要求iBCI用户停止使用iBCI并进行监控数据采集，使iBCI系统变得困难使用。在这篇论文中，我们提议了一种方法，可以让通信iBCI进行自动重新寻常化，无需中断用户。我们的方法利用大型语言模型（LM）来自动更正iBCI输出中的错误。自动重新寻常化过程使用这些更正后的输出（ Pseudo-labels）来在线更新iBCI解码器。在403天的测试期间，我们通过我们的Continual Online Recalibration with Pseudo-labels（CORP）框架进行了一项临床试验。CORP在在线手写iBCI任务中实现了93.84%的稳定解码率，与其他基线方法相比，显著超越。这也是人类参与者的 longest-running iBCI稳定展示。我们的结果提供了首次的长期稳定性证明，解决了iBCI在临床翻译的主要障碍。
</details></li>
</ul>
<hr>
<h2 id="Testing-RadiX-Nets-Advances-in-Viable-Sparse-Topologies"><a href="#Testing-RadiX-Nets-Advances-in-Viable-Sparse-Topologies" class="headerlink" title="Testing RadiX-Nets: Advances in Viable Sparse Topologies"></a>Testing RadiX-Nets: Advances in Viable Sparse Topologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03609">http://arxiv.org/abs/2311.03609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Kwak, Zack West, Hayden Jananthan, Jeremy Kepner</li>
<li>for: 这篇论文主要针对hyper-parametrized deep neural networks (DNNs) 的简化，以减少 computation demands in ML research and industry use.</li>
<li>methods: 论文使用RadiX-Nets，一种 subgroup of sparse DNNs，以维持 uniformity 并且获得更好的性能。</li>
<li>results: 论文通过实验发现，RadiX-Nets 在 TensorFlow 中的表现不受初始化和训练方法的影响，并且发现了一些 “strange models” 的现象，这些模型在训练时间较长且精度较低。<details>
<summary>Abstract</summary>
The exponential growth of data has sparked computational demands on ML research and industry use. Sparsification of hyper-parametrized deep neural networks (DNNs) creates simpler representations of complex data. Past research has shown that some sparse networks achieve similar performance as dense ones, reducing runtime and storage. RadiX-Nets, a subgroup of sparse DNNs, maintain uniformity which counteracts their lack of neural connections. Generation, independent of a dense network, yields faster asymptotic training and removes the need for costly pruning. However, little work has been done on RadiX-Nets, making testing challenging. This paper presents a testing suite for RadiX-Nets in TensorFlow. We test RadiX-Net performance to streamline processing in scalable models, revealing relationships between network topology, initialization, and training behavior. We also encounter "strange models" that train inconsistently and to lower accuracy while models of similar sparsity train well.
</details>
<details>
<summary>摘要</summary>
“数据的 exponential 增长对 machine learning 研究和实际应用带来了 computation 的需求。通过减少 hyper-parametrized deep neural networks（DNNs）中的参数，可以创造简洁的数据表示。过去的研究表明，一些简洁网络可以与密集网络具有相同的性能，同时降低 runtime 和存储成本。RadiX-Nets 是一 subgroup of sparse DNNs，它们保持了 uniformity，并且独立于密集网络进行生成，从而减少了训练的时间和成本。然而，对 RadiX-Nets 的研究不多，测试很困难。这篇文章提出了一个基于 TensorFlow 的 RadiX-Nets 测试 suite。我们测试 RadiX-Net 的性能，以便在扩展性好的模型中进行流线处理。我们还发现了一些“strange models”，它们在不同的初始化和训练方法下存在不一致的训练行为和较低的准确率。”Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Generative-Diffusion-Models-for-Lattice-Field-Theory"><a href="#Generative-Diffusion-Models-for-Lattice-Field-Theory" class="headerlink" title="Generative Diffusion Models for Lattice Field Theory"></a>Generative Diffusion Models for Lattice Field Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03578">http://arxiv.org/abs/2311.03578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingxiao Wang, Gert Aarts, Kai Zhou</li>
<li>for: 这个研究paper explores the connection between machine learning and lattice field theory by linking generative diffusion models (DMs) with stochastic quantization, from a stochastic differential equation perspective.</li>
<li>methods: The paper shows that DMs can be conceptualized by reversing a stochastic process driven by the Langevin equation, which produces samples from an initial distribution to approximate the target distribution.</li>
<li>results: The paper demonstrates the capability of DMs to learn effective actions and its feasibility to act as a global sampler for generating configurations in the two-dimensional $\phi^4$ quantum lattice field theory.<details>
<summary>Abstract</summary>
This study delves into the connection between machine learning and lattice field theory by linking generative diffusion models (DMs) with stochastic quantization, from a stochastic differential equation perspective. We show that DMs can be conceptualized by reversing a stochastic process driven by the Langevin equation, which then produces samples from an initial distribution to approximate the target distribution. In a toy model, we highlight the capability of DMs to learn effective actions. Furthermore, we demonstrate its feasibility to act as a global sampler for generating configurations in the two-dimensional $\phi^4$ quantum lattice field theory.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Graph-Theoretic-Framework-for-Understanding-Open-World-Semi-Supervised-Learning"><a href="#A-Graph-Theoretic-Framework-for-Understanding-Open-World-Semi-Supervised-Learning" class="headerlink" title="A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning"></a>A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03524">http://arxiv.org/abs/2311.03524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deeplearning-wisc/sorl">https://github.com/deeplearning-wisc/sorl</a></li>
<li>paper_authors: Yiyou Sun, Zhenmei Shi, Yixuan Li</li>
<li>for: 这篇论文目的是提出一种基于图论的开放世界 semi-supervised 学习方法，以便在无标签数据中推断已知和新类，并且提供了理论基础。</li>
<li>methods: 该论文使用了图论来形式化开放世界设定下的卷积问题，并提出了一种名为spectral open-world representation learning（SORL）的算法。SORL 算法的核心思想是将图形式化为一个 Spectral 问题，并使用spectral decomposition来解决卷积问题。</li>
<li>results: 该论文通过实验表明，SORL 算法可以与一些强基线相比或超越它们，并且可以提供理论上的保证。具体来说，SORL 算法可以在常见的 benchmark 数据集上实现比较好的 clustering 性能，而且可以在实际应用中具有理论上的保证。<details>
<summary>Abstract</summary>
Open-world semi-supervised learning aims at inferring both known and novel classes in unlabeled data, by harnessing prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for this problem. This paper bridges the gap by formalizing a graph-theoretic framework tailored for the open-world setting, where the clustering can be theoretically characterized by graph factorization. Our graph-theoretic framework illuminates practical algorithms and provides guarantees. In particular, based on our graph formulation, we apply the algorithm called Spectral Open-world Representation Learning (SORL), and show that minimizing our loss is equivalent to performing spectral decomposition on the graph. Such equivalence allows us to derive a provable error bound on the clustering performance for both known and novel classes, and analyze rigorously when labeled data helps. Empirically, SORL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
Open-world semi-supervised learning目标是在无标签数据中推断已知和新类，通过利用已知类标注集的知识来提供优化。然而，这个问题在理论基础上存在缺失。这篇论文填补了这个鸿蒙，通过 graf-based 框架来形式化开放世界设置下的划分。我们的 graf-based 框架可以 theoretically Characterize 划分，并且提供了实用的算法和保证。具体来说，基于我们的 graf 表示法，我们应用了 Spectral Open-world Representation Learning（SORL）算法，并证明了将我们的损失函数下降等价于在 graf 上进行 spectral decomposition。这种等价性允许我们 derive 划分性能的证明性 bound，并且分析了在已知类标注集的帮助下，labeled 数据的作用。实验表明，SORL 可以与一些强大的基线算法相匹配或超越，这是在实践中获得理论保证的愉悦。
</details></li>
</ul>
<hr>
<h2 id="The-Fairness-Stitch-Unveiling-the-Potential-of-Model-Stitching-in-Neural-Network-De-Biasing"><a href="#The-Fairness-Stitch-Unveiling-the-Potential-of-Model-Stitching-in-Neural-Network-De-Biasing" class="headerlink" title="The Fairness Stitch: Unveiling the Potential of Model Stitching in Neural Network De-Biasing"></a>The Fairness Stitch: Unveiling the Potential of Model Stitching in Neural Network De-Biasing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03532">http://arxiv.org/abs/2311.03532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/modar7/the_fairness_stitch">https://github.com/modar7/the_fairness_stitch</a></li>
<li>paper_authors: Modar Sulaiman, Kallol Roy</li>
<li>for: 这篇研究的目的是提高机器学习模型的公平性，以解决不同应用中的偏见和歧视问题。</li>
<li>methods: 本研究提出了一个名为“公平缝纫（The Fairness Stitch，TFS）”的新方法，它结合了模型缝纫和训练，同时将公平性约束组入。</li>
<li>results: 这篇研究的结果显示，TFS方法能够实现更好的公平性和性能之间的平衡，比较于现有的基eline方法。<details>
<summary>Abstract</summary>
The pursuit of fairness in machine learning models has emerged as a critical research challenge in different applications ranging from bank loan approval to face detection. Despite the widespread adoption of artificial intelligence algorithms across various domains, concerns persist regarding the presence of biases and discrimination within these models. To address this pressing issue, this study introduces a novel method called "The Fairness Stitch (TFS)" to enhance fairness in deep learning models. This method combines model stitching and training jointly, while incorporating fairness constraints. In this research, we assess the effectiveness of our proposed method by conducting a comprehensive evaluation of two well-known datasets, CelebA and UTKFace. We systematically compare the performance of our approach with the existing baseline method. Our findings reveal a notable improvement in achieving a balanced trade-off between fairness and performance, highlighting the promising potential of our method to address bias-related challenges and foster equitable outcomes in machine learning models. This paper poses a challenge to the conventional wisdom of the effectiveness of the last layer in deep learning models for de-biasing.
</details>
<details>
<summary>摘要</summary>
“机器学习模型中的公平性追求已经成为不同应用领域的重要研究挑战，从银行贷款批准到面部识别。尽管人工智能算法在各个领域得到了广泛的采用，但是存在偏见和歧视的担忧仍然存在。为解决这个紧要的问题，本研究提出了一种新的方法 called“公平缝纫（TFS）”，用于增强机器学习模型的公平性。这个方法结合了模型缝纫和训练的过程，并将公平性约束纳入到模型中。在这个研究中，我们对两个常用的数据集CelebA和UTKFace进行了全面的评估，并与现有的基准方法进行比较。我们发现，我们的方法可以更好地实现公平性和性能之间的平衡，这显示了我们的方法具有对偏见相关挑战的应对能力，并可以实现更加公平的结果。本研究挑战了传统的机器学习模型中最后一层的偏见处理方法的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Local-Computations-in-Distributed-Bayesian-Learning"><a href="#Asynchronous-Local-Computations-in-Distributed-Bayesian-Learning" class="headerlink" title="Asynchronous Local Computations in Distributed Bayesian Learning"></a>Asynchronous Local Computations in Distributed Bayesian Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03496">http://arxiv.org/abs/2311.03496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kinjal Bhar, He Bai, Jemin George, Carl Busart<br>for: 这篇论文主要针对于分布式机器学习（ML）中的分布式推理算法，即多个智能 Device 之间的协同学习。methods: 该论文提出了一种基于干扰通信的异步推理算法，使用了本地计算和between successive inter-agent communications。results: 论文通过使用 Bayesian sampling via unadjusted Langevin algorithm (ULA) MCMC 和 theoretically quantify the convergence rates，证明了该算法的效果。在一个小型问题和实际数据集上进行了丰富的 simulations，并观察到了更快的初始快速收敛和改善的性能精度，特别是在低数据范围内。在Gamma Telescope和mHealth数据集上，共计达到了78%和90%的分类精度。<details>
<summary>Abstract</summary>
Due to the expanding scope of machine learning (ML) to the fields of sensor networking, cooperative robotics and many other multi-agent systems, distributed deployment of inference algorithms has received a lot of attention. These algorithms involve collaboratively learning unknown parameters from dispersed data collected by multiple agents. There are two competing aspects in such algorithms, namely, intra-agent computation and inter-agent communication. Traditionally, algorithms are designed to perform both synchronously. However, certain circumstances need frugal use of communication channels as they are either unreliable, time-consuming, or resource-expensive. In this paper, we propose gossip-based asynchronous communication to leverage fast computations and reduce communication overhead simultaneously. We analyze the effects of multiple (local) intra-agent computations by the active agents between successive inter-agent communications. For local computations, Bayesian sampling via unadjusted Langevin algorithm (ULA) MCMC is utilized. The communication is assumed to be over a connected graph (e.g., as in decentralized learning), however, the results can be extended to coordinated communication where there is a central server (e.g., federated learning). We theoretically quantify the convergence rates in the process. To demonstrate the efficacy of the proposed algorithm, we present simulations on a toy problem as well as on real world data sets to train ML models to perform classification tasks. We observe faster initial convergence and improved performance accuracy, especially in the low data range. We achieve on average 78% and over 90% classification accuracy respectively on the Gamma Telescope and mHealth data sets from the UCI ML repository.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-High-Level-Synthesis-and-Large-Language-Models-to-Generate-Simulate-and-Deploy-a-Uniform-Random-Number-Generator-Hardware-Design"><a href="#Leveraging-High-Level-Synthesis-and-Large-Language-Models-to-Generate-Simulate-and-Deploy-a-Uniform-Random-Number-Generator-Hardware-Design" class="headerlink" title="Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design"></a>Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03489">http://arxiv.org/abs/2311.03489</a></li>
<li>repo_url: None</li>
<li>paper_authors: James T. Meech</li>
<li>for: 用于开发具有特定应用场景的集成电路设计。</li>
<li>methods: 使用大语言模型工具，仅使用开源工具（ exclude 大语言模型），实现硬件设计生成。</li>
<li>results: 通过使用大语言模型生成的 simulate 和 Dieharder 随机性测试盘，验证了Permuted Congruential Random Number Generator 设计的正常性和质量。<details>
<summary>Abstract</summary>
We present a new high-level synthesis methodology for using large language model tools to generate hardware designs. The methodology uses exclusively open-source tools excluding the large language model. As a case study, we use our methodology to generate a permuted congruential random number generator design with a wishbone interface. We verify the functionality and quality of the random number generator design using large language model-generated simulations and the Dieharder randomness test suite. We document all the large language model chat logs, Python scripts, Verilog scripts, and simulation results used in the case study. We believe that our method of hardware design generation coupled with the open source silicon 130 nm design tools will revolutionize application-specific integrated circuit design. Our methodology significantly lowers the bar to entry when building domain-specific computing accelerators for the Internet of Things and proof of concept prototypes for later fabrication in more modern process nodes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的高级合成方法，使用大型自然语言模型工具生成硬件设计。这种方法使用exclusively开源工具，排除了大型自然语言模型。作为一个案例研究，我们使用了我们的方法生成一个卷积排序随机数生成器设计，具有愿望形接口。我们使用大型自然语言模型生成的 simulations和Dieharder随机性测试集来验证随机数生成器设计的功能和质量。我们记录了所有的大型自然语言模型对话记录、Python脚本、Verilog脚本和 simulations 结果，以便在案例研究中进行参考。我们认为，我们的硬件设计生成方法，结合开源130nm设计工具，将重塑应用特定集成电路设计领域。我们的方法可以大大降低在建立领域特定计算加速器和互联网物联网设备的门槛，并为后续更高级的处理节点fabrication提供证明。
</details></li>
</ul>
<hr>
<h2 id="Uni-O4-Unifying-Online-and-Offline-Deep-Reinforcement-Learning-with-Multi-Step-On-Policy-Optimization"><a href="#Uni-O4-Unifying-Online-and-Offline-Deep-Reinforcement-Learning-with-Multi-Step-On-Policy-Optimization" class="headerlink" title="Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization"></a>Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03351">http://arxiv.org/abs/2311.03351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, Huazhe Xu</li>
<li>for: 本研究旨在提出一种 straightforward yet effective 的 offline和online reinforcement learning 方法，以便在各种实际应用中快速部署。</li>
<li>methods: 该方法使用了一种单一的对象函数来结合offline和online学习，从而实现了灵活的学习模式，允许任意的预训练、细化和offline&#x2F;online学习组合。在offline阶段，该方法使用了多个ensemble政策来解决行为策略与offline数据集之间的差异问题。</li>
<li>results: 该方法可以在真实世界 robot 任务上实现superior的offline初始化以及稳定的在线细化 capacities。通过多个 simulate benchmark 的全面评估，我们证明了该方法在offline和offline-to-online学习中具有state-of-the-art表现。<details>
<summary>Abstract</summary>
Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization? In this study, we propose Uni-o4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-o4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE) approach, Uni-o4 can achieve multi-step policy improvement safely. We demonstrate that by employing the method above, the fusion of these two paradigms can yield superior offline initialization as well as stable and rapid online fine-tuning capabilities. Through real-world robot tasks, we highlight the benefits of this paradigm for rapid deployment in challenging, previously unseen real-world environments. Additionally, through comprehensive evaluations using numerous simulated benchmarks, we substantiate that our method achieves state-of-the-art performance in both offline and offline-to-online fine-tuning learning. Our website: https://lei-kun.github.io/uni-o4/ .
</details>
<details>
<summary>摘要</summary>
combining 线上和线下强化学习 (RL) 是关键 для高效和安全的学习。然而，先前的方法将线上和线下学习视为分开的过程，导致了重复的设计和限制性能。我们问：可以达到简单又有效的线上和线下学习，不需要额外的保守性或正则化吗？在这种研究中，我们提出了Uni-o4，它利用了在两个阶段中的同步目标，使RL机器人可以在线上和线下学习无缝转换。这种性能的可更换性，允许学习模式的任意组合，包括预训练、细化、线上和线下学习。在线上阶段，Uni-o4特别利用了多个ensemble政策来解决对行为策略估计和线上数据集的匹配问题。通过简单的线上策略评估（OPE）方法，Uni-o4可以安全地实现多步策略改进。我们示出，通过上述方法，线上和线下学习的融合可以实现出色的初始化以及稳定和快速的在线细化能力。通过实际的 робоット任务，我们强调了这种 Paradigma 在面临实际、以前未看到的挑战时的快速部署的优势。此外，通过大量的模拟 benchmark 的全面评估，我们证明了我们的方法在线上和线下初始化学习以及在线-到-线上细化学习中具有状态 искус力表现。更多信息请访问我们的网站：https://lei-kun.github.io/uni-o4/。
</details></li>
</ul>
<hr>
<h2 id="Learning-Hard-Constrained-Models-with-One-Sample"><a href="#Learning-Hard-Constrained-Models-with-One-Sample" class="headerlink" title="Learning Hard-Constrained Models with One Sample"></a>Learning Hard-Constrained Models with One Sample</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03332">http://arxiv.org/abs/2311.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Galanis, Alkis Kalavasis, Anthimos Vardis Kandiros</li>
<li>for: 这篇论文主要研究了用单个样本来估计Markov随机场的参数，并应用于$k$-SAT、正确颜色模型和通用$H$-颜色模型等问题。</li>
<li>methods: 该论文使用pseudo-likelihood estimator，并使用了 coupling技术来获得变量 bounds。</li>
<li>results: 研究发现，在soft-constrained情况下不 siempre可以使用单个样本来估计参数，而且存在非满足性INSTances的问题。对于$k$-SAT和正确颜色模型，提供了正确的估计器，而对于通用$H$-颜色模型，则需要更强的condition来 garantizar sampling。<details>
<summary>Abstract</summary>
We consider the problem of estimating the parameters of a Markov Random Field with hard-constraints using a single sample. As our main running examples, we use the $k$-SAT and the proper coloring models, as well as general $H$-coloring models; for all of these we obtain both positive and negative results. In contrast to the soft-constrained case, we show in particular that single-sample estimation is not always possible, and that the existence of an estimator is related to the existence of non-satisfiable instances.   Our algorithms are based on the pseudo-likelihood estimator. We show variance bounds for this estimator using coupling techniques inspired, in the case of $k$-SAT, by Moitra's sampling algorithm (JACM, 2019); our positive results for colorings build on this new coupling approach. For $q$-colorings on graphs with maximum degree $d$, we give a linear-time estimator when $q>d+1$, whereas the problem is non-identifiable when $q\leq d+1$. For general $H$-colorings, we show that standard conditions that guarantee sampling, such as Dobrushin's condition, are insufficient for one-sample learning; on the positive side, we provide a general condition that is sufficient to guarantee linear-time learning and obtain applications for proper colorings and permissive models. For the $k$-SAT model on formulas with maximum degree $d$, we provide a linear-time estimator when $k\gtrsim 6.45\log d$, whereas the problem becomes non-identifiable when $k\lesssim \log d$.
</details>
<details>
<summary>摘要</summary>
我们考虑一个推估Markov随机场景中的参数，使用单一样本。我们的主要Running例是$k$-SAT和proper颜色模型，以及一般的$H$-颜色模型。我们获得了both positive和negative结果。在不同于软链接的情况下，我们表明单一样本推估不一定可行，并且存在非满足性的实例。我们的算法基于伪贝氏可能性推估器。我们使用对Moitra的抽样算法（JACM, 2019）的对抗技术来获得变数上下限。在$k$-SAT模型中，我们给出了线性时间的推估器，当$q>d+1$时，而当$q\leq d+1$时，问题是非归一性的。对于一般的$H$-颜色模型，我们表明了样本推估是不可能的，因为Dobrushin的condition是不充分的。然而，我们提供了一个一般的condition，可以保证线性时间的学习，并且有应用于正颜色和允许模型。在$k$-SAT模型中，我们给出了线性时间的推估器，当$k\gtrsim 6.45\log d$时，而当$k\lesssim \log d$时，问题是非归一性的。
</details></li>
</ul>
<hr>
<h2 id="Practical-considerations-for-variable-screening-in-the-Super-Learner"><a href="#Practical-considerations-for-variable-screening-in-the-Super-Learner" class="headerlink" title="Practical considerations for variable screening in the Super Learner"></a>Practical considerations for variable screening in the Super Learner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03313">http://arxiv.org/abs/2311.03313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bdwilliamson/sl_screening_supplementary">https://github.com/bdwilliamson/sl_screening_supplementary</a></li>
<li>paper_authors: Brian D. Williamson, Drew King, Ying Huang</li>
<li>for: 本研究旨在探讨Super Learner ensemble在数据分析中的应用，以及使用变量选择算法（如lasso）进行维度减少的性能。</li>
<li>methods: 本研究使用Super Learner ensemble和变量选择算法进行数据分析，并对不同的候选creening算法进行比较。</li>
<li>results: 研究发现，使用多种候选creening算法可以保证Super Learner的性能，类似于选择多种预测算法来保证Super Learner的性能。<details>
<summary>Abstract</summary>
Estimating a prediction function is a fundamental component of many data analyses. The Super Learner ensemble, a particular implementation of stacking, has desirable theoretical properties and has been used successfully in many applications. Dimension reduction can be accomplished by using variable screening algorithms, including the lasso, within the ensemble prior to fitting other prediction algorithms. However, the performance of a Super Learner using the lasso for dimension reduction has not been fully explored in cases where the lasso is known to perform poorly. We provide empirical results that suggest that a diverse set of candidate screening algorithms should be used to protect against poor performance of any one screen, similar to the guidance for choosing a library of prediction algorithms for the Super Learner.
</details>
<details>
<summary>摘要</summary>
估算预测函数是数据分析中的基本组成部分。跨学习ensemble（Super Learner）具有优秀的理论性质，在许多应用中得到了成功。使用变量选择算法，如lasso，来实现维度减少，可以在ensemble之前进行。然而，使用lasso进行维度减少的Super Learner表现没有得到完整的探索。我们提供了实证结果，表明应用多种候选屏选择算法，类似于选择预测算法库，以保证预测表现不受任何一个屏选的影响。
</details></li>
</ul>
<hr>
<h2 id="TS-Diffusion-Generating-Highly-Complex-Time-Series-with-Diffusion-Models"><a href="#TS-Diffusion-Generating-Highly-Complex-Time-Series-with-Diffusion-Models" class="headerlink" title="TS-Diffusion: Generating Highly Complex Time Series with Diffusion Models"></a>TS-Diffusion: Generating Highly Complex Time Series with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03303">http://arxiv.org/abs/2311.03303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangming Li<br>for:这个论文的目的是处理具有 sampling irregularities、缺失和大的特征-时间维度的时间序列。methods:这个模型使用了点处理框架，包括一个神经网络 Ordinary Differential Equation（ODE）编码器、一个 diffusion 模型和另一个 ODE 编码器。results:这个模型在多个时间序列数据集上进行了广泛的实验，并达到了在传统和复杂时间序列上的出色表现，在比较之下显著超越了先前的基eline。<details>
<summary>Abstract</summary>
While current generative models have achieved promising performances in time-series synthesis, they either make strong assumptions on the data format (e.g., regularities) or rely on pre-processing approaches (e.g., interpolations) to simplify the raw data. In this work, we consider a class of time series with three common bad properties, including sampling irregularities, missingness, and large feature-temporal dimensions, and introduce a general model, TS-Diffusion, to process such complex time series. Our model consists of three parts under the framework of point process. The first part is an encoder of the neural ordinary differential equation (ODE) that converts time series into dense representations, with the jump technique to capture sampling irregularities and self-attention mechanism to handle missing values; The second component of TS-Diffusion is a diffusion model that learns from the representation of time series. These time-series representations can have a complex distribution because of their high dimensions; The third part is a decoder of another ODE that generates time series with irregularities and missing values given their representations. We have conducted extensive experiments on multiple time-series datasets, demonstrating that TS-Diffusion achieves excellent results on both conventional and complex time series and significantly outperforms previous baselines.
</details>
<details>
<summary>摘要</summary>
当前的生成模型已经取得了时间序列合成的可靠表现，但它们 Either 对数据格式做出了强大的假设（例如，常规）或者通过预处理方法（例如， interpolations）来简化原始数据。在这项工作中，我们考虑一类时间序列具有三种常见的坏属性，包括采样不均、缺失和大功能-时间维度，并引入一种通用模型，TS-Diffusion，来处理这些复杂的时间序列。我们的模型包括三部分，即encoder、diffusion模型和decoder。首先，TS-Diffusion的encoder部分使用神经ordinary differential equation（ODE）将时间序列转换为稠密表示，并使用跳技术 capture采样不均和自动注意机制处理缺失值。第二部分是一个学习从时间序列表示的diffusion模型，这些时间序列表示可能具有复杂的分布，因为它们的维度很高。最后，TS-Diffusion的decoder部分使用另一个ODE将时间序列生成器，带有采样不均和缺失值，给出其表示。我们在多个时间序列 datasets 进行了广泛的实验，并证明TS-Diffusion在传统和复杂时间序列上取得了优秀的结果，并在前一个基线模型 Significantly outperform。
</details></li>
</ul>
<hr>
<h2 id="Risk-of-Transfer-Learning-and-its-Applications-in-Finance"><a href="#Risk-of-Transfer-Learning-and-its-Applications-in-Finance" class="headerlink" title="Risk of Transfer Learning and its Applications in Finance"></a>Risk of Transfer Learning and its Applications in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03283">http://arxiv.org/abs/2311.03283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Cao, Haotian Gu, Xin Guo, Mathieu Rosenbaum</li>
<li>for: 这篇论文是为了提出一种新的转移风险概念，以评估转移学习的转移性。</li>
<li>methods: 本论文使用转移学习技术和转移风险概念来解决股票回报预测和资产 allocate问题。</li>
<li>results: 数据结果显示，转移风险与转移学习性能之间存在强相关关系，而转移风险可以提供一种 computationally efficient 的方法来选择合适的源任务。<details>
<summary>Abstract</summary>
Transfer learning is an emerging and popular paradigm for utilizing existing knowledge from previous learning tasks to improve the performance of new ones. In this paper, we propose a novel concept of transfer risk and and analyze its properties to evaluate transferability of transfer learning. We apply transfer learning techniques and this concept of transfer risk to stock return prediction and portfolio optimization problems. Numerical results demonstrate a strong correlation between transfer risk and overall transfer learning performance, where transfer risk provides a computationally efficient way to identify appropriate source tasks in transfer learning, including cross-continent, cross-sector, and cross-frequency transfer for portfolio optimization.
</details>
<details>
<summary>摘要</summary>
通过学习转移是一种迅速升起的并且受欢迎的方法，可以利用之前学习任务中的知识来提高新任务的性能。在这篇论文中，我们提出了一种新的转移风险概念，并分析其性质以评估转移学习的可行性。我们运用转移学习技术和这种转移风险概念来解决股票回报预测和股票组合优化问题。 numerically 的结果表明，转移风险和总转移学习性能之间存在强相关性，而转移风险提供了一种 computationally efficient 的方法来确定合适的源任务，包括跨洲、跨领域和跨频率的转移学习，以便进行股票组合优化。
</details></li>
</ul>
<hr>
<h2 id="Discretizing-Numerical-Attributes-An-Analysis-of-Human-Perceptions"><a href="#Discretizing-Numerical-Attributes-An-Analysis-of-Human-Perceptions" class="headerlink" title="Discretizing Numerical Attributes: An Analysis of Human Perceptions"></a>Discretizing Numerical Attributes: An Analysis of Human Perceptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03278">http://arxiv.org/abs/2311.03278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minakshi Kaushik, Rahul Sharma, Dirk Draheim</li>
<li>for: 本研究旨在提供一个标准方法 для数值属性分割。</li>
<li>methods: 本研究使用人类对数值属性分割的认知来提出两个指标，并与专家的看法进行比较。</li>
<li>results: 分析结果显示，68.7%的人类回答与我们提出的两个指标吻合较好。这表明我们的指标可能可以用于数值属性分割。<details>
<summary>Abstract</summary>
Machine learning (ML) has employed various discretization methods to partition numerical attributes into intervals. However, an effective discretization technique remains elusive in many ML applications, such as association rule mining. Moreover, the existing discretization techniques do not reflect best the impact of the independent numerical factor on the dependent numerical target factor. This research aims to establish a benchmark approach for numerical attribute partitioning. We conduct an extensive analysis of human perceptions of partitioning a numerical attribute and compare these perceptions with the results obtained from our two proposed measures. We also examine the perceptions of experts in data science, statistics, and engineering by employing numerical data visualization techniques. The analysis of collected responses reveals that $68.7\%$ of human responses approximately closely align with the values generated by our proposed measures. Based on these findings, our proposed measures may be used as one of the methods for discretizing the numerical attributes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploiting-Latent-Attribute-Interaction-with-Transformer-on-Heterogeneous-Information-Networks"><a href="#Exploiting-Latent-Attribute-Interaction-with-Transformer-on-Heterogeneous-Information-Networks" class="headerlink" title="Exploiting Latent Attribute Interaction with Transformer on Heterogeneous Information Networks"></a>Exploiting Latent Attribute Interaction with Transformer on Heterogeneous Information Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03275">http://arxiv.org/abs/2311.03275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyuan Zhao, Qingqing Ge, Anfeng Cheng, Yiding Liu, Xiang Li, Shuaiqiang Wang</li>
<li>for: 这篇论文是为了提出一种新的多型图模型（MULAN），用于处理实际应用中存在多种不同特征的图像。</li>
<li>methods: 该模型包括两个主要组成部分：一个是对节点类型信息的考虑，另一个是用于捕捉各个节点特征之间的高阶交互信息的维度感知器。</li>
<li>results: 对六个多型图标准数据集进行了广泛的实验，结果显示MULAN在与其他当前状态的竞争对手比较之下表现出色，同时也证明了MULAN的效率。<details>
<summary>Abstract</summary>
Heterogeneous graph neural networks (HGNNs) have recently shown impressive capability in modeling heterogeneous graphs that are ubiquitous in real-world applications. Due to the diversity of attributes of nodes in different types, most existing models first align nodes by mapping them into the same low-dimensional space. However, in this way, they lose the type information of nodes. In addition, most of them only consider the interactions between nodes while neglecting the high-order information behind the latent interactions among different node features. To address these problems, in this paper, we propose a novel heterogeneous graph model MULAN, including two major components, i.e., a type-aware encoder and a dimension-aware encoder. Specifically, the type-aware encoder compensates for the loss of node type information and better leverages graph heterogeneity in learning node representations. Built upon transformer architecture, the dimension-aware encoder is capable of capturing the latent interactions among the diverse node features. With these components, the information of graph heterogeneity, node features and graph structure can be comprehensively encoded in node representations. We conduct extensive experiments on six heterogeneous benchmark datasets, which demonstrates the superiority of MULAN over other state-of-the-art competitors and also shows that MULAN is efficient.
</details>
<details>
<summary>摘要</summary>
“异构图 neural network (HGNN) 最近已经表现出模型异构图的出色能力，这些图在实际应用中非常普遍。由于节点属性的多样性，大多数现有模型都会将节点映射到同一低维度空间中，从而产生节点类型信息的丢失。此外，大多数模型只考虑节点之间的交互，而忽略节点特征之间的高阶信息。为了解决这些问题，本文提出了一种新的异构图模型名为 MULAN，包括两个主要组件：类型意识编码器和维度意识编码器。特别是，类型意识编码器可以补偿节点类型信息的丢失，更好地利用图中的异构性。基于 transformer 架构，维度意识编码器可以捕捉节点特征之间的隐藏交互。通过这两个组件，图中的异构性、节点特征和图结构可以完整地编码在节点表示中。我们在六个异构 benchmark 数据集进行了广泛的实验， demonstarted  MULAN 的超越性，以及其高效性。”
</details></li>
</ul>
<hr>
<h2 id="Parameter-Agnostic-Optimization-under-Relaxed-Smoothness"><a href="#Parameter-Agnostic-Optimization-under-Relaxed-Smoothness" class="headerlink" title="Parameter-Agnostic Optimization under Relaxed Smoothness"></a>Parameter-Agnostic Optimization under Relaxed Smoothness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03252">http://arxiv.org/abs/2311.03252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Hübler, Junchi Yang, Xiang Li, Niao He</li>
<li>for: 本研究旨在探讨训练机器学习模型时，调整参数的问题。</li>
<li>methods: 本研究使用Normalized Stochastic Gradient Descent with Momentum（NSGD-M）方法，并提出了一种新的框架来下降bounds。</li>
<li>results: 研究发现，NSGD-M方法可以在没有任何问题参数知识的情况下达到（近似）最优复杂性，但是这会带来一个具有$L_1$幂函数的扩展项。在探讨的设定下，这个扩展项可以被消除。此外，研究还发现，这种扩展项是不可避免的，因为任何parameter-agnostic算法都会面临这种问题。<details>
<summary>Abstract</summary>
Tuning hyperparameters, such as the stepsize, presents a major challenge of training machine learning models. To address this challenge, numerous adaptive optimization algorithms have been developed that achieve near-optimal complexities, even when stepsizes are independent of problem-specific parameters, provided that the loss function is $L$-smooth. However, as the assumption is relaxed to the more realistic $(L_0, L_1)$-smoothness, all existing convergence results still necessitate tuning of the stepsize. In this study, we demonstrate that Normalized Stochastic Gradient Descent with Momentum (NSGD-M) can achieve a (nearly) rate-optimal complexity without prior knowledge of any problem parameter, though this comes at the cost of introducing an exponential term dependent on $L_1$ in the complexity. We further establish that this exponential term is inevitable to such schemes by introducing a theoretical framework of lower bounds tailored explicitly for parameter-agnostic algorithms. Interestingly, in deterministic settings, the exponential factor can be neutralized by employing Gradient Descent with a Backtracking Line Search. To the best of our knowledge, these findings represent the first parameter-agnostic convergence results under the generalized smoothness condition. Our empirical experiments further confirm our theoretical insights.
</details>
<details>
<summary>摘要</summary>
调整 гиперпараметров，如步长，对机器学习模型的训练呈poses major challenge。为 Addressing this challenge, numerous adaptive optimization algorithms have been developed that achieve near-optimal complexities, even when the stepsizes are independent of problem-specific parameters, provided that the loss function is $L$-smooth。然而，如果放弃这个假设，所有现有的收敛结果都仍然需要调整步长。在这种研究中，我们示出了Normalized Stochastic Gradient Descent with Momentum（NSGD-M）可以实现一个（近似）率optimal complexity without prior knowledge of any problem parameter，但是这会导致在 $L_1$ 上增加一个对数函数。我们还证明这个对数函数是不可避免的，通过引入特定于无参数算法的理论框架的下界。在束定性 Settings，这个对数因子可以被中和 employing Gradient Descent with a Backtracking Line Search。我们认为这些发现是parameter-agnostic convergence results under the generalized smoothness condition的首次。我们的实验也证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Langevin-Monte-Carlo-with-ResNet-like-Neural-Network-architectures"><a href="#Approximating-Langevin-Monte-Carlo-with-ResNet-like-Neural-Network-architectures" class="headerlink" title="Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures"></a>Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03242">http://arxiv.org/abs/2311.03242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Eigel, Charles Miranda, Janina Schütte, David Sommer</li>
<li>for: 这个论文的目的是构建一种用于采样目标分布的神经网络，以便从简单的参照分布（例如标准正态分布）采样目标分布。</li>
<li>methods: 该论文提出一种基于Langevin Monte Carlo（LMC）算法的神经网络建模方法，并通过LMC干扰结果的评估来评估该方法的收敛率。</li>
<li>results: 该论文的分析表明，在不同的干扰假设下，使用该方法可以在 Wasserstein-$2$ 距离下收敛到目标分布。此外，文章还提出了一种类似于深度差分神经网络的建模方法，并 derivated expressivity 结果表明该方法可以准确地表示采样到目标分布的映射。<details>
<summary>Abstract</summary>
We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.
</details>
<details>
<summary>摘要</summary>
我们从一个目标分布中抽取样本，通过建立一个对应于标准正常分布的神经网络，将标准正常分布中的样本转换为目标分布中的样本。我们提议使用对应于兰杰维尔 Monte Carlo（LMC）算法的神经网络架构。基于LMC扰动结果，我们显示了对预设的均匀、对数凹陷分布的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的��
</details></li>
</ul>
<hr>
<h2 id="Out-of-distribution-Detection-Learning-with-Unreliable-Out-of-distribution-Sources"><a href="#Out-of-distribution-Detection-Learning-with-Unreliable-Out-of-distribution-Sources" class="headerlink" title="Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources"></a>Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03236">http://arxiv.org/abs/2311.03236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Zheng, Qizhou Wang, Zhen Fang, Xiaobo Xia, Feng Liu, Tongliang Liu, Bo Han</li>
<li>for: 这个研究的目的是提高开放世界分类中预测器的可靠性，通过实现极少量的真实外部数据训练。</li>
<li>methods: 这个研究使用的方法是基于数据生成器，通过将ID数据生成为外部数据，以提高预测器对外部数据的预测能力。</li>
<li>results: 研究发现，使用auxiliary task可以帮助预测器更好地识别外部数据，并且与先前的方法相比，这个方法可以更好地避免伪阳性识别。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection discerns OOD data where the predictor cannot make valid predictions as in-distribution (ID) data, thereby increasing the reliability of open-world classification. However, it is typically hard to collect real out-of-distribution (OOD) data for training a predictor capable of discerning ID and OOD patterns. This obstacle gives rise to data generation-based learning methods, synthesizing OOD data via data generators for predictor training without requiring any real OOD data. Related methods typically pre-train a generator on ID data and adopt various selection procedures to find those data likely to be the OOD cases. However, generated data may still coincide with ID semantics, i.e., mistaken OOD generation remains, confusing the predictor between ID and OOD data. To this end, we suggest that generated data (with mistaken OOD generation) can be used to devise an auxiliary OOD detection task to facilitate real OOD detection. Specifically, we can ensure that learning from such an auxiliary task is beneficial if the ID and the OOD parts have disjoint supports, with the help of a well-designed training procedure for the predictor. Accordingly, we propose a powerful data generation-based learning method named Auxiliary Task-based OOD Learning (ATOL) that can relieve the mistaken OOD generation. We conduct extensive experiments under various OOD detection setups, demonstrating the effectiveness of our method against its advanced counterparts.
</details>
<details>
<summary>摘要</summary>
外部数据（OOD）检测可以将OOD数据与内部数据（ID）进行分类，从而提高开放世界分类的可靠性。然而，收集真实的OOD数据用于训练预测器是困难的。这种困难引起了数据生成基于学习方法，通过数据生成器来训练预测器，无需任何真实的OOD数据。这些方法通常是在ID数据上预训练数据生成器，并采用不同的选择过程来找到可能是OOD的数据。然而，生成的数据可能仍然与ID semantics相同，即生成的OOD数据仍然与ID数据匹配。为了解决这个问题，我们建议使用生成的数据（包括 mistaken OOD generation）来设计辅助OOD检测任务，以便在真正的OOD检测中帮助预测器分辨ID和OOD数据。具体来说，我们可以确保学习这种辅助任务是有益的，只要ID和OOD部分具有不同的支持，并且通过适当的预测器训练程序来保证这一点。因此，我们提出了一种强大的数据生成基于学习方法，名为辅助任务基于OOD学习（ATOL），可以减少 mistaken OOD generation。我们在不同的OOD检测设置下进行了广泛的实验，并证明了我们的方法在相比先进的方法之上具有更高的效果。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Process-Approximations-Assessing-Their-Necessity"><a href="#Spatial-Process-Approximations-Assessing-Their-Necessity" class="headerlink" title="Spatial Process Approximations: Assessing Their Necessity"></a>Spatial Process Approximations: Assessing Their Necessity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03201">http://arxiv.org/abs/2311.03201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang</li>
<li>for: 这篇论文主要是为了解决大样本大Matrix的不稳定性问题。</li>
<li>methods: 论文使用了多种优化算法来解决这种不稳定性问题，包括低级分解、卷积分解等。</li>
<li>results: 论文通过对各种优化算法进行比较，发现其中的一些方法可以有效地解决大样本大Matrix的不稳定性问题，但是还有一些方法无法减轻这种不稳定性。<details>
<summary>Abstract</summary>
In spatial statistics and machine learning, the kernel matrix plays a pivotal role in prediction, classification, and maximum likelihood estimation. A thorough examination reveals that for large sample sizes, the kernel matrix becomes ill-conditioned, provided the sampling locations are fairly evenly distributed. This condition poses significant challenges to numerical algorithms used in prediction and estimation computations and necessitates an approximation to prediction and the Gaussian likelihood. A review of current methodologies for managing large spatial data indicates that some fail to address this ill-conditioning problem. Such ill-conditioning often results in low-rank approximations of the stochastic processes. This paper introduces various optimality criteria and provides solutions for each.
</details>
<details>
<summary>摘要</summary>
在空间统计学和机器学习中，kernel矩阵在预测、分类和最大可能性估计中扮演着重要的角色。经过全面的检查，发现在大样本大小下，如果抽样点 distribution 相对均匀，那么kernel矩阵就会变得不整合，这会对数学计算中用到的数值算法提出 significan challenges。这种不整合情况通常会导致低级别的随机过程的预测和高斯可能性函数的 Approximation。本文评估了当前的大 spatial data 管理方法，发现一些方法并不能解决这个不整合问题。这种不整合情况通常会导致低级别的随机过程的预测和高斯可能性函数的 Approximation。本文介绍了多种优化性riteria和解决方案。
</details></li>
</ul>
<hr>
<h2 id="Stable-Linear-Subspace-Identification-A-Machine-Learning-Approach"><a href="#Stable-Linear-Subspace-Identification-A-Machine-Learning-Approach" class="headerlink" title="Stable Linear Subspace Identification: A Machine Learning Approach"></a>Stable Linear Subspace Identification: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03197">http://arxiv.org/abs/2311.03197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cemempamoi/simba">https://github.com/cemempamoi/simba</a></li>
<li>paper_authors: Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp Heer, Giancarlo Ferrari Trecate, Colin N. Jones</li>
<li>for: 这篇论文主要是为了提出一种基于机器学习（ML）和线性系统适应（SI）的新的状态空间Identification方法（SIMBA），以提高现有方法的稳定性和适应性。</li>
<li>methods: 该方法使用了自动微分框架，并使用了一种新的线性矩阵不等式（LMI）来保证模型的稳定性。</li>
<li>results: 对比 traditional的线性状态空间SI方法，SIMBA通常有更高的性能，尤其是在保证模型稳定性的情况下。在许多输入输出系统和真实数据上，SIMBA显示出了良好的抗预测性和灵活性。<details>
<summary>Abstract</summary>
Machine Learning (ML) and linear System Identification (SI) have been historically developed independently. In this paper, we leverage well-established ML tools - especially the automatic differentiation framework - to introduce SIMBa, a family of discrete linear multi-step-ahead state-space SI methods using backpropagation. SIMBa relies on a novel Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure the stability of the identified model.   We show how SIMBa generally outperforms traditional linear state-space SI methods, and sometimes significantly, although at the price of a higher computational burden. This performance gap is particularly remarkable compared to other SI methods with stability guarantees, where the gain is frequently above 25% in our investigations, hinting at SIMBa's ability to simultaneously achieve state-of-the-art fitting performance and enforce stability. Interestingly, these observations hold for a wide variety of input-output systems and on both simulated and real-world data, showcasing the flexibility of the proposed approach. We postulate that this new SI paradigm presents a great extension potential to identify structured nonlinear models from data, and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeepInception-Hypnotize-Large-Language-Model-to-Be-Jailbreaker"><a href="#DeepInception-Hypnotize-Large-Language-Model-to-Be-Jailbreaker" class="headerlink" title="DeepInception: Hypnotize Large Language Model to Be Jailbreaker"></a>DeepInception: Hypnotize Large Language Model to Be Jailbreaker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03191">http://arxiv.org/abs/2311.03191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han<br>for: 这篇论文旨在揭示大语言模型（LLM）在防御性监禁方面的漏洞，并提出一种轻量级的方法来利用LLM的人格化能力实现攻击。methods: 该方法基于Milgram实验，利用LLM的人格化能力构建一个嵌入式场景，使LLM在正常情况下逃脱使用控制，并提供了进一步的直接监禁机会。results: 实验结果显示，DeepInception可以与之前的对手竞争，并在后续互动中实现连续监禁。研究发现，自失的问题存在于多个开源&#x2F;关闭源LLM上，如Falcon、Vicuna、Llama-2和GPT-3.5&#x2F;4&#x2F;4V。<details>
<summary>Abstract</summary>
Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment that individuals can harm another person if they are told to do so by an authoritative figure, we disclose a lightweight method, termed as DeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock its misusing risks. Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario and provides the possibility for further direct jailbreaks. Empirically, we conduct comprehensive experiments to show its efficacy. Our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open/closed-source LLMs like Falcon, Vicuna, Llama-2, and GPT-3.5/4/4V. Our investigation appeals that people should pay more attention to the safety aspects of LLMs and a stronger defense against their misuse risks. The code is publicly available at: https://github.com/tmlr-group/DeepInception.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）在不同应用中具有惊人的成功，但它们却容易受到恶意破坏的威胁。然而，前一代的研究通常采用粗糙优化或高计算成本的推理，这可能不是实用或有效的。在这篇论文中，我们受到米凯尔实验的启发，该实验表明了人们可以通过权威人士的指令而使别人产生危害。因此，我们提出了一种轻量级的方法，称为深度引入，可以轻松地使LLM变成破坏者，并暴露其不当使用的风险。具体来说，深度引入利用LLM的人格化能力构建了一个新的嵌入式场景，实现了在常规情况下适应的方式，并提供了进一步的直接破坏的可能性。我们进行了广泛的实验，证明了深度引入的效果。我们的深度引入可以与前一代的对手相比，并在后续交互中实现连续破坏，揭示了开源/关闭源LLM like Falcon、Vicuna、Llama-2和GPT-3.5/4/4V的潜在漏洞。我们的调查表明，人们应该更加关注LLM的安全问题，并采取更加有力的防御措施。代码可以在 GitHub 上找到：https://github.com/tmlr-group/DeepInception。
</details></li>
</ul>
<hr>
<h2 id="Hopfield-Enhanced-Deep-Neural-Networks-for-Artifact-Resilient-Brain-State-Decoding"><a href="#Hopfield-Enhanced-Deep-Neural-Networks-for-Artifact-Resilient-Brain-State-Decoding" class="headerlink" title="Hopfield-Enhanced Deep Neural Networks for Artifact-Resilient Brain State Decoding"></a>Hopfield-Enhanced Deep Neural Networks for Artifact-Resilient Brain State Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03421">http://arxiv.org/abs/2311.03421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arnaumarin/hdnn-artifactbrainstate">https://github.com/arnaumarin/hdnn-artifactbrainstate</a></li>
<li>paper_authors: Arnau Marin-Llobet, Arnau Manasanch, Maria V. Sanchez-Vives</li>
<li>for: 这项研究的目的是提高脑动态图像的识别精度，并探索脑动态图像与行为之间的关系。</li>
<li>methods: 该研究使用了两个阶段的计算机框架，首先使用了抽象网络对数据进行噪声除掉，然后使用了卷积神经网络进行分类。</li>
<li>results: 研究发现，这种混合的方法可以有效地减少噪声的影响，使模型在噪声水平较低时达到与清洁数据 CNN 的性能。<details>
<summary>Abstract</summary>
The study of brain states, ranging from highly synchronous to asynchronous neuronal patterns like the sleep-wake cycle, is fundamental for assessing the brain's spatiotemporal dynamics and their close connection to behavior. However, the development of new techniques to accurately identify them still remains a challenge, as these are often compromised by the presence of noise, artifacts, and suboptimal recording quality. In this study, we propose a two-stage computational framework combining Hopfield Networks for artifact data preprocessing with Convolutional Neural Networks (CNNs) for classification of brain states in rat neural recordings under different levels of anesthesia. To evaluate the robustness of our framework, we deliberately introduced noise artifacts into the neural recordings. We evaluated our hybrid Hopfield-CNN pipeline by benchmarking it against two comparative models: a standalone CNN handling the same noisy inputs, and another CNN trained and tested on artifact-free data. Performance across various levels of data compression and noise intensities showed that our framework can effectively mitigate artifacts, allowing the model to reach parity with the clean-data CNN at lower noise levels. Although this study mainly benefits small-scale experiments, the findings highlight the necessity for advanced deep learning and Hopfield Network models to improve scalability and robustness in diverse real-world settings.
</details>
<details>
<summary>摘要</summary>
研究大脑状态的研究，从高度同步到不同步的神经元模式，如睡卫休眠-醒目的周期，是评估大脑的空间时间动态和其与行为的紧密关系的基础。然而，开发新的技术来准确识别这些状态仍然是一个挑战，因为这些经常受到噪音、artefacts和低质量记录的干扰。在这种研究中，我们提出了一个两个阶段的计算框架，其中首先使用束缚网络来处理噪音数据，然后使用卷积神经网络（CNN）来分类大脑状态。为评估我们的框架的可靠性，我们故意将噪音 artifacts添加到神经记录中。我们对我们的混合束缚-CNN pipeline进行了比较，并与两个参照模型进行比较：一个只处理同样噪音输入的单独CNN，另一个在噪音自由数据上训练和测试CNN。我们在不同的数据压缩和噪音强度下测试了我们的框架，结果表明，我们的框架可以有效地 mitigate artifacts，使模型在噪音水平下达到与干净数据CNN的性能。尽管这种研究主要是为小规模实验而设计的，但发现的结果 highlights the necessity for advanced deep learning and束缚网络模型，以提高可扩展性和可靠性在多样化的实际场景中。**Simplified Chinese Translation:**研究大脑状态的研究，从高度同步到不同步的神经元模式，如睡卫休眠-醒目的周期，是评估大脑的空间时间动态和其与行为的紧密关系的基础。然而，开发新的技术来准确识别这些状态仍然是一个挑战，因为这些经常受到噪音、artefacts和低质量记录的干扰。在这种研究中，我们提出了一个两个阶段的计算框架，其中首先使用束缚网络来处理噪音数据，然后使用卷积神经网络（CNN）来分类大脑状态。为评估我们的框架的可靠性，我们故意将噪音 artifacts添加到神经记录中。我们对我们的混合束缚-CNN pipeline进行了比较，并与两个参照模型进行比较：一个只处理同样噪音输入的单独CNN，另一个在噪音自由数据上训练和测试CNN。我们在不同的数据压缩和噪音强度下测试了我们的框架，结果表明，我们的框架可以有效地 mitigate artifacts，使模型在噪音水平下达到与干净数据CNN的性能。尽管这种研究主要是为小规模实验而设计的，但发现的结果 highlights the necessity for advanced deep learning and束缚网络模型，以提高可扩展性和可靠性在多样化的实际场景中。
</details></li>
</ul>
<hr>
<h2 id="Preserving-Privacy-in-GANs-Against-Membership-Inference-Attack"><a href="#Preserving-Privacy-in-GANs-Against-Membership-Inference-Attack" class="headerlink" title="Preserving Privacy in GANs Against Membership Inference Attack"></a>Preserving Privacy in GANs Against Membership Inference Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03172">http://arxiv.org/abs/2311.03172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadhadi Shateri, Francisco Messina, Fabrice Labeau, Pablo Piantanida</li>
<li>for: 本研究旨在提高生成 adversarial Networks (GANs) 对于 Membership Inference Attacks (MIAs) 的Robustness，并且提出了两种防御策略。</li>
<li>methods: 本研究使用了 GANs 生成 Synthetic Data，并且定义了一种基于 Bhattacharyya 度的泛化过度溯源检测方法，以及一种基于 Fano 不等式的最大 entropy GAN (MEGAN) 防御策略。</li>
<li>results: 对于一些常用的数据集，应用了提出的防御策略后，可以将 adversaries 的准确率降低到随机猜测率水平，而且减少了生成样本中关于训练数据点的信息泄露。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) have been widely used for generating synthetic data for cases where there is a limited size real-world dataset or when data holders are unwilling to share their data samples. Recent works showed that GANs, due to overfitting and memorization, might leak information regarding their training data samples. This makes GANs vulnerable to Membership Inference Attacks (MIAs). Several defense strategies have been proposed in the literature to mitigate this privacy issue. Unfortunately, defense strategies based on differential privacy are proven to reduce extensively the quality of the synthetic data points. On the other hand, more recent frameworks such as PrivGAN and PAR-GAN are not suitable for small-size training datasets. In the present work, the overfitting in GANs is studied in terms of the discriminator, and a more general measure of overfitting based on the Bhattacharyya coefficient is defined. Then, inspired by Fano's inequality, our first defense mechanism against MIAs is proposed. This framework, which requires only a simple modification in the loss function of GANs, is referred to as the maximum entropy GAN or MEGAN and significantly improves the robustness of GANs to MIAs. As a second defense strategy, a more heuristic model based on minimizing the information leaked from generated samples about the training data points is presented. This approach is referred to as mutual information minimization GAN (MIMGAN) and uses a variational representation of the mutual information to minimize the information that a synthetic sample might leak about the whole training data set. Applying the proposed frameworks to some commonly used data sets against state-of-the-art MIAs reveals that the proposed methods can reduce the accuracy of the adversaries to the level of random guessing accuracy with a small reduction in the quality of the synthetic data samples.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GANs）广泛应用于生成synthetic数据，特别是当实际数据集规模受限或数据持有者不愿意分享数据样本时。然而，GANs可能因过拟合和记忆而泄露training数据样本信息，从而使GANs面临会员推断攻击（MIAs）的隐私问题。文献中已经提出了一些防御策略，但是这些策略基于差分隐私会导致synthetic数据点质量降低 extensively。相反，更新的框架如PrivGAN和PAR-GAN在小型训练集上不适用。在 presente 的研究中，我们研究了GANs中的过拟合，并定义了基于 Bhattacharyya 系数的更一般的过拟合度量。然后，以Fano的不等式为 inspiration，我们提出了一种防御机制，称为最大 entropy GAN（MEGAN）。这种方法需要在GANs的损失函数中进行简单的修改，可以减轻GANs对MIAs的抗性。此外，我们还提出了一种更具体的防御策略，即使generated samples about the training data points中的信息泄露的最小化。这种方法称为mutual information minimization GAN（MIMGAN），使用了变量表示的mutual information来最小化generated samples中对全部训练数据集的信息泄露。通过应用我们提出的方法到一些常用的数据集上，发现可以将敌对者的准确率降低到随机猜测率水平，同时减少generated samples的质量下降。
</details></li>
</ul>
<hr>
<h2 id="An-Examination-of-the-Alleged-Privacy-Threats-of-Confidence-Ranked-Reconstruction-of-Census-Microdata"><a href="#An-Examination-of-the-Alleged-Privacy-Threats-of-Confidence-Ranked-Reconstruction-of-Census-Microdata" class="headerlink" title="An Examination of the Alleged Privacy Threats of Confidence-Ranked Reconstruction of Census Microdata"></a>An Examination of the Alleged Privacy Threats of Confidence-Ranked Reconstruction of Census Microdata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03171">http://arxiv.org/abs/2311.03171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NajeebJebreel/CRR-analysis">https://github.com/NajeebJebreel/CRR-analysis</a></li>
<li>paper_authors: David Sánchez, Najeeb Jebreel, Josep Domingo-Ferrer, Krishnamurty Muralidhar, Alberto Blanco-Justicia</li>
<li>for: 本研究旨在探讨美国人口普查局（USCB）在2020年人口普查中使用 differential privacy（DP） instead of traditional statistical disclosure limitation based on rank swapping，并对这种更改的影响。</li>
<li>methods: 本研究使用了一种新的重建攻击，即 confidence-ranked reconstruction，以评估 USCB 是否应该使用 DP-based solutions。</li>
<li>results: 研究结果表明， confidence-ranked reconstruction 无法准确地重建 Census 记录，并且无法帮助攻击者进行身份透露或属性透露攻击。此外，由于人口普查数据的编译、处理和发布方式，无法通过任何方法重建原始和完整的记录。<details>
<summary>Abstract</summary>
The alleged threat of reconstruction attacks has led the U.S. Census Bureau (USCB) to replace in the Decennial Census 2020 the traditional statistical disclosure limitation based on rank swapping with one based on differential privacy (DP). This has resulted in substantial accuracy loss of the released statistics. Worse yet, it has been shown that the reconstruction attacks used as an argument to move to DP are very far from allowing unequivocal reidentification of the respondents, because in general there are a lot of reconstructions compatible with the released statistics. In a very recent paper, a new reconstruction attack has been proposed, whose goal is to indicate the confidence that a reconstructed record was in the original respondent data. The alleged risk of serious disclosure entailed by such confidence-ranked reconstruction has renewed the interest of the USCB to use DP-based solutions. To forestall the potential accuracy loss in future data releases resulting from adoption of these solutions, we show in this paper that the proposed confidence-ranked reconstruction does not threaten privacy. Specifically, we report empirical results showing that the proposed ranking cannot guide reidentification or attribute disclosure attacks, and hence it fails to warrant the USCB's move towards DP. Further, we also demonstrate that, due to the way the Census data are compiled, processed and released, it is not possible to reconstruct original and complete records through any methodology, and the confidence-ranked reconstruction not only is completely ineffective at accurately reconstructing Census records but is trivially outperformed by an adequate interpretation of the released aggregate statistics.
</details>
<details>
<summary>摘要</summary>
美国人口普查局（USCB）在2020年人口普查中取代了传统的统计隐私技术，改用Diffusion Privacy（DP）。这导致了发布统计数据的准确性下降。尽管如此，有人提出了重建攻击，即使用DP来保护个人隐私。然而，这些重建攻击并不能 garantuee  unequivocal 的重建记录，因为通常有多个重建方案与发布统计数据兼容。在最近的论文中，一种新的重建攻击方法被提出，其目标是指出重建记录中是否包含原始回答数据。美国人口普查局对这种重建攻击表示兴趣，以免Future数据发布中可能出现的准确性下降。我们在这篇论文中展示，提posed confidence-ranked reconstruction不会威胁隐私。我们的实验结果表明，这种排名不能导guide 重建或 attribute 透露攻击，因此无法准确重建人口普查记录。此外，我们还证明，由于人口普查数据的编译、处理和发布方式，无法通过任何方法重建原始和完整的记录。 confidence-ranked reconstruction不仅完全无法准确重建人口普查记录，而且也远远下降于对发布统计数据进行合理解读的能力。
</details></li>
</ul>
<hr>
<h2 id="Convergence-Analysis-of-Sequential-Federated-Learning-on-Heterogeneous-Data"><a href="#Convergence-Analysis-of-Sequential-Federated-Learning-on-Heterogeneous-Data" class="headerlink" title="Convergence Analysis of Sequential Federated Learning on Heterogeneous Data"></a>Convergence Analysis of Sequential Federated Learning on Heterogeneous Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03154">http://arxiv.org/abs/2311.03154</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyipeng00/convergence">https://github.com/liyipeng00/convergence</a></li>
<li>paper_authors: Yipeng Li, Xinchen Lyu</li>
<li>for: 本文为了研究 Federated Learning (FL) 中多客户端的并发训练，以及对不同数据的敏感性。</li>
<li>methods: 本文使用了两种类型的方法：一是平行 FL (PFL)， где客户端在平行方式进行模型训练；另一种是顺序 FL (SFL)， donde客户端在顺序方式进行模型训练。 在对异构数据进行训练时，SFL的整合理论仍然缺乏。本文为SFL在异构数据上的强健&#x2F;通用&#x2F;非对称目标下的整合 garantías。</li>
<li>results: 实验结果表明，在异构数据上，SFL比PFL在跨设备情况下表现更好，并且SFL在完全和偏参与客户端情况下的整合 garantías比PFL更好。<details>
<summary>Abstract</summary>
There are two categories of methods in Federated Learning (FL) for joint training across multiple clients: i) parallel FL (PFL), where clients train models in a parallel manner; and ii) sequential FL (SFL), where clients train models in a sequential manner. In contrast to that of PFL, the convergence theory of SFL on heterogeneous data is still lacking. In this paper, we establish the convergence guarantees of SFL for strongly/general/non-convex objectives on heterogeneous data. The convergence guarantees of SFL are better than that of PFL on heterogeneous data with both full and partial client participation. Experimental results validate the counterintuitive analysis result that SFL outperforms PFL on extremely heterogeneous data in cross-device settings.
</details>
<details>
<summary>摘要</summary>
在聚合学习（Federated Learning，FL）中，有两类方法 для共同训练多个客户端：一是平行聚合学习（Parallel Federated Learning，PFL），其中客户端在平行的方式进行模型训练；另一是顺序聚合学习（Sequential Federated Learning，SFL），其中客户端在顺序的方式进行模型训练。与PFL相比，SFL在不同数据上的整合理论仍然缺失。在这篇论文中，我们建立了SFL在强不同数据上的整合保证，并且比PFL在不同数据上的整合保证更好。实验结果证明了对于非常不同的数据，SFL在跨设备的场景下超越PFL。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Material-Thermal-Conductivity-Prediction-through-Machine-Learning"><a href="#End-to-end-Material-Thermal-Conductivity-Prediction-through-Machine-Learning" class="headerlink" title="End-to-end Material Thermal Conductivity Prediction through Machine Learning"></a>End-to-end Material Thermal Conductivity Prediction through Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03139">http://arxiv.org/abs/2311.03139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yagyank Srivastava, Ankit Jain</li>
<li>for: 这个论文是为了提高材料热导率预测的速度而写的。</li>
<li>methods: 这个论文使用了结构基于的机器学习方法来预测材料热导率，包括使用基本原理和博尔tz滤波方程进行高通量计算，并评估了现有的状态艺术模型。</li>
<li>results: 研究发现，由于数据质量的问题，所有使用的机器学习模型都会过拟合。为解决这个问题，这篇论文提出了一种图граaph neural network模型，该模型在所有评估 datasets 上展现了更加一致和规范的性能。然而，在测试 datasets 上，最佳的相对百分比误差仍然在50-60%的范围内。这表明，虽然这些模型可以帮助加速材料屏选，但其当前准确性仍有限。<details>
<summary>Abstract</summary>
We investigated the accelerated prediction of the thermal conductivity of materials through end- to-end structure-based approaches employing machine learning methods. Due to the non-availability of high-quality thermal conductivity data, we first performed high-throughput calculations based on first principles and the Boltzmann transport equation for 225 materials, effectively more than doubling the size of the existing dataset. We assessed the performance of state-of-the-art machine learning models for thermal conductivity prediction on this expanded dataset and observed that all these models suffered from overfitting. To address this issue, we introduced a novel graph-based neural network model, which demonstrated more consistent and regularized performance across all evaluated datasets. Nevertheless, the best mean absolute percentage error achieved on the test dataset remained in the range of 50-60%. This suggests that while these models are valuable for expediting material screening, their current accuracy is still limited.
</details>
<details>
<summary>摘要</summary>
我们研究了通过终端结构基于方法加速预测材料的热导率。由于热导率数据的不可得性，我们首先通过基本原理和博尔ツ曼传输方程进行了225种材料的高通过率计算，实际上更 чем doubling了现有数据集的大小。我们评估了现有状态的机器学习模型在这个扩展数据集上的表现，并发现所有这些模型都受到了过拟合。为解决这个问题，我们提出了一种新的图表基于神经网络模型，该模型在所有评估数据集上显示了更一致和规范的表现。然而，在测试数据集上最佳的 mean absolute percentage error 仍然在50-60%的范围内，这表明这些模型可以快速屏选材料，但其当前精度仍然有限。
</details></li>
</ul>
<hr>
<h2 id="Reservoir-Computing-Model-for-Mapping-and-Forecasting-Neuronal-Interactions-from-Electrophysiological-Data"><a href="#Reservoir-Computing-Model-for-Mapping-and-Forecasting-Neuronal-Interactions-from-Electrophysiological-Data" class="headerlink" title="Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data"></a>Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03131">http://arxiv.org/abs/2311.03131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilya Auslender, Giorgio Letti, Yasaman Heydari, Lorenzo Pavesi</li>
<li>for: 研究 neuronal network 的电生物学性质，以揭示不同单元之间的互动。</li>
<li>methods: 使用 Reservoir Computing Network 建立计算模型，从电生物学测量数据中提取网络结构。</li>
<li>results: 模型可以准确预测网络结构图和响应特定输入。<details>
<summary>Abstract</summary>
Electrophysiological nature of neuronal networks allows to reveal various interactions between different cell units at a very short time-scales. One of the many challenges in analyzing these signals is to retrieve the morphology and functionality of a given network. In this work we developed a computational model, based on Reservoir Computing Network (RCN) architecture, which decodes the spatio-temporal data from electro-physiological measurements of neuronal cultures and reconstructs the network structure on a macroscopic domain, representing the connectivity between neuronal units. We demonstrate that the model can predict the connectivity map of the network with higher accuracy than the common methods such as Cross-Correlation and Transfer-Entropy. In addition, we experimentally demonstrate the ability of the model to predict a network response to a specific input, such as localized stimulus.
</details>
<details>
<summary>摘要</summary>
electrophysiological nature of neuronal networks allows for revealing various interactions between different cell units at very short time-scales. one of the many challenges in analyzing these signals is to retrieve the morphology and functionality of a given network. in this work, we developed a computational model based on reservoir computing network (RCN) architecture, which decodes the spatio-temporal data from electro-physiological measurements of neuronal cultures and reconstructs the network structure on a macroscopic domain, representing the connectivity between neuronal units. we demonstrate that the model can predict the connectivity map of the network with higher accuracy than common methods such as cross-correlation and transfer-entropy. in addition, we experimentally demonstrate the ability of the model to predict a network response to a specific input, such as localized stimulus.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-modeling-of-the-composite-effect-of-multiple-nutrients-on-blood-glucose-dynamics"><a href="#Nonparametric-modeling-of-the-composite-effect-of-multiple-nutrients-on-blood-glucose-dynamics" class="headerlink" title="Nonparametric modeling of the composite effect of multiple nutrients on blood glucose dynamics"></a>Nonparametric modeling of the composite effect of multiple nutrients on blood glucose dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03129">http://arxiv.org/abs/2311.03129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jularina/trcmed-kit">https://github.com/jularina/trcmed-kit</a></li>
<li>paper_authors: Arina Odnoblyudova, Çağlar Hizli, ST John, Andrea Cognolato, Anne Juuti, Simo Särkkä, Kirsi Pietiläinen, Pekka Marttinen</li>
<li>for: 估计多 ком成分治疗的生物physiological response, 并分离各组件的影响。</li>
<li>methods: 扩展现有的 probabilistic nonparametric方法, 使其能够直接面对这个问题。 开发了一种基于卷积的composite treatment-response曲线模型，更易于生物预测。</li>
<li>results: 通过对卡路里和脂肪在餐食中的影响来预测血糖响应，并且通过分解治疗组件、 incorporating dosages, 和在患者之间共享统计信息来提高预测精度。<details>
<summary>Abstract</summary>
In biomedical applications it is often necessary to estimate a physiological response to a treatment consisting of multiple components, and learn the separate effects of the components in addition to the joint effect. Here, we extend existing probabilistic nonparametric approaches to explicitly address this problem. We also develop a new convolution-based model for composite treatment-response curves that is more biologically interpretable. We validate our models by estimating the impact of carbohydrate and fat in meals on blood glucose. By differentiating treatment components, incorporating their dosages, and sharing statistical information across patients via a hierarchical multi-output Gaussian process, our method improves prediction accuracy over existing approaches, and allows us to interpret the different effects of carbohydrates and fat on the overall glucose response.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在生物医学应用中，经常需要估算具有多个组成部分的治疗的生物学响应，并了解每个组成部分的独立效应以及它们之间的共同效应。在这里，我们扩展了现有的概率非 Parametric方法，以便直接解决这个问题。我们还开发了一种基于卷积的治疗响应曲线模型，这种模型更加容易理解生物学意义。我们验证了我们的模型，通过估算吃到抗酵素和脂肪在饭物中的血糖响应。通过分解治疗组成部分，包括它们的剂量，并在患者之间共享统计信息via层次多输出 Gaussian process，我们的方法可以提高预测精度，并允许我们解释抗酵素和脂肪对总血糖响应的不同效应。
</details></li>
</ul>
<hr>
<h2 id="Algebraic-Dynamical-Systems-in-Machine-Learning"><a href="#Algebraic-Dynamical-Systems-in-Machine-Learning" class="headerlink" title="Algebraic Dynamical Systems in Machine Learning"></a>Algebraic Dynamical Systems in Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03118">http://arxiv.org/abs/2311.03118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iolo Jones, Jerry Swan, Jeffrey Giansiracusa</li>
<li>for: 本研究旨在开发一种基于符号 rewrite 的动态系统分析模型，用于描述动态机器学习模型的组合性和扩展性。</li>
<li>methods: 本研究使用了一种 recursive function 来定义动态模型的Output，并通过Category theory 的框架来描述这些模型的结构和特性。</li>
<li>results: 本研究显示了一种将 recurrent neural networks, graph neural networks, 和 diffusion models 等动态模型嵌入到一个抽象的 formally defined 模型中的方法，并提出了一种将这些模型扩展到适用于结构化或非数字数据的 ‘hybrid symbolic-numeric’ 模型的方法。<details>
<summary>Abstract</summary>
We introduce an algebraic analogue of dynamical systems, based on term rewriting. We show that a recursive function applied to the output of an iterated rewriting system defines a formal class of models into which all the main architectures for dynamic machine learning models (including recurrent neural networks, graph neural networks, and diffusion models) can be embedded. Considered in category theory, we also show that these algebraic models are a natural language for describing the compositionality of dynamic models. Furthermore, we propose that these models provide a template for the generalisation of the above dynamic models to learning problems on structured or non-numerical data, including 'hybrid symbolic-numeric' models.
</details>
<details>
<summary>摘要</summary>
我们介绍一个运算方程的数学同源，基于字串重写。我们显示出一个递回函数对迭代重写系统的输出所定义的一个正式的模型类别，这个类别包括了大多数运算机器学习模型（包括回传神经网络、格raph神经网络和扩散模型）的所有主要架构。在category theory中考虑，我们还显示出这些运算模型是动态模型的自然语言描述。此外，我们建议这些模型可以用来对于结构化或非数据的学习问题进行通用化，包括"混合 символиic-numeric"模型。
</details></li>
</ul>
<hr>
<h2 id="RELand-Risk-Estimation-of-Landmines-via-Interpretable-Invariant-Risk-Minimization"><a href="#RELand-Risk-Estimation-of-Landmines-via-Interpretable-Invariant-Risk-Minimization" class="headerlink" title="RELand: Risk Estimation of Landmines via Interpretable Invariant Risk Minimization"></a>RELand: Risk Estimation of Landmines via Interpretable Invariant Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03115">http://arxiv.org/abs/2311.03115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Dulce Rubio, Siqi Zeng, Qi Wang, Didier Alvarado, Francisco Moreno, Hoda Heidari, Fei Fang</li>
<li>for: 支持人道主义排雷操作，提高排雷效率和准确率。</li>
<li>methods: 提供普适的特征工程和标签分配指南，并基于稀缺特征掩模和不变风险最小化算法设计了一种可解释性模型。</li>
<li>results: 在遵循实际排雷操作协议进行了广泛的评估，并显示在比较实际排雷操作中获得了显著提高。<details>
<summary>Abstract</summary>
Landmines remain a threat to war-affected communities for years after conflicts have ended, partly due to the laborious nature of demining tasks. Humanitarian demining operations begin by collecting relevant information from the sites to be cleared, which is then analyzed by human experts to determine the potential risk of remaining landmines. In this paper, we propose RELand system to support these tasks, which consists of three major components. We (1) provide general feature engineering and label assigning guidelines to enhance datasets for landmine risk modeling, which are widely applicable to global demining routines, (2) formulate landmine presence as a classification problem and design a novel interpretable model based on sparse feature masking and invariant risk minimization, and run extensive evaluation under proper protocols that resemble real-world demining operations to show a significant improvement over the state-of-the-art, and (3) build an interactive web interface to suggest priority areas for demining organizations. We are currently collaborating with a humanitarian demining NGO in Colombia that is using our system as part of their field operations in two areas recently prioritized for demining.
</details>
<details>
<summary>摘要</summary>
土地雷 remained a threat to war-affected communities for years after conflicts have ended, partly due to the laborious nature of demining tasks. Humanitarian demining operations begin by collecting relevant information from the sites to be cleared, which is then analyzed by human experts to determine the potential risk of remaining landmines. In this paper, we propose RELand system to support these tasks, which consists of three major components. We (1) provide general feature engineering and label assigning guidelines to enhance datasets for landmine risk modeling, which are widely applicable to global demining routines, (2) formulate landmine presence as a classification problem and design a novel interpretable model based on sparse feature masking and invariant risk minimization, and run extensive evaluation under proper protocols that resemble real-world demining operations to show a significant improvement over the state-of-the-art, and (3) build an interactive web interface to suggest priority areas for demining organizations. We are currently collaborating with a humanitarian demining NGO in Colombia that is using our system as part of their field operations in two areas recently prioritized for demining.Here's the translation breakdown:1. 土地雷 (tǔdì zhú) - landmines2.  remained (jiàn) - remained3. a threat (wēn) - a threat4. to war-affected communities (zhòu yì zhī qīng kè xìng) - to war-affected communities5. for years (nián) - for years6. after conflicts have ended (hòu yì zhī qīng kè xìng) - after conflicts have ended7. partly due to (bù yǐ) - partly due to8. the laborious nature (gōng yì) - the laborious nature9. of demining tasks (zhòu yì zhī qīng kè xìng) - of demining tasks10. Humanitarian demining operations (jīn yì zhī qīng kè xìng) - humanitarian demining operations11. begin (dào) - begin12. by collecting (jī) - by collecting13. relevant information (xiēng yè) - relevant information14. from the sites (zhòu yì zhī qīng kè xìng) - from the sites15. to be cleared (dīng kě) - to be cleared16. which is then analyzed (dào) - which is then analyzed17. by human experts (rén zhī) - by human experts18. to determine (dì) - to determine19. the potential risk (fāng yì) - the potential risk20. of remaining landmines (dào) - of remaining landmines21. In this paper (zhe zhèng) - In this paper22. we propose (dào) - we propose23. RELand system (RELand zhì) - RELand system24. to support (yuè) - to support25. these tasks (zhòu yì zhī qīng kè xìng) - these tasks26. which consists of (bù yǐ) - which consists of27. three major components (sān zhòng zhī qīng kè xìng) - three major components28. We (wǒ) - We29. provide (dào) - provide30. general feature engineering (gōng yì) - general feature engineering31. and label assigning (jiào zhì) - and label assigning32. guidelines (guīdàng) - guidelines33. to enhance (yì) - to enhance34. datasets (zhòu yì zhī qīng kè xìng) - datasets35. for landmine risk modeling (zhòu yì zhī qīng kè xìng) - for landmine risk modeling36. which are widely applicable (bù yǐ) - which are widely applicable37. to global demining routines (qīng kè xìng) - to global demining routines38. formulate (xíng) - formulate39. landmine presence (zhòu yì zhī qīng kè xìng) - landmine presence40. as a classification problem (bǎo yì zhī qīng kè xìng) - as a classification problem41. and design (dì) - and design42. a novel interpretable model (xīn xiǎng) - a novel interpretable model43. based on (bù yǐ) - based on44. sparse feature masking (shū zhì) - sparse feature masking45. and invariant risk minimization (bì yì) - and invariant risk minimization46. run extensive evaluation (dào) - run extensive evaluation47. under proper protocols (zhèng zhì) - under proper protocols48. that resemble (xiǎng) - that resemble49. real-world demining operations (zhòu yì zhī qīng kè xìng) - real-world demining operations50. to show (dì) - to show51. a significant improvement (fāng yì) - a significant improvement52. over the state-of-the-art (zhèng zhì) - over the state-of-the-art53. and (bù yǐ) - and54. build (dào) - build55. an interactive web interface (yuè) - an interactive web interface56. to suggest (jiào) - to suggest57. priority areas (jī) - priority areas58. for demining organizations (zhòu yì zhī qīng kè xìng) - for demining organizations59. We are currently (zài yǐ) - We are currently60. collaborating (xīn xiǎng) - collaborating61. with (bù yǐ) - with62. a humanitarian demining NGO (jīn yì zhī qīng kè xìng) - a humanitarian demining NGO63. in Colombia (Kòlumbī) - in Colombia64. that is using (yǐ) - that is using65. our system (zì) - our system66. as part of their field operations (zhòu yì zhī qīng kè xìng) - as part of their field operations67. in two areas (liǎng yù) - in two areas68. recently prioritized for demining (zhòu yì zhī qīng kè xìng) - recently prioritized for deminingNote that some words and phrases have been shortened or modified for brevity, and some grammatical structures have been adjusted for clarity and consistency.
</details></li>
</ul>
<hr>
<h2 id="Weight-Sharing-Regularization"><a href="#Weight-Sharing-Regularization" class="headerlink" title="Weight-Sharing Regularization"></a>Weight-Sharing Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03096">http://arxiv.org/abs/2311.03096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/motahareh-sohrabi/weight-sharing-regularization">https://github.com/motahareh-sohrabi/weight-sharing-regularization</a></li>
<li>paper_authors: Mehran Shakerinava, Motahareh Sohrabi, Siamak Ravanbakhsh, Simon Lacoste-Julien</li>
<li>for: 这 paper 是为了研究深度学习中的 weight-sharing  regularization，并提出了一种基于 this 的 proximal mapping 算法。</li>
<li>methods: 这 paper 使用了 proximal gradient descent 来训练 weight-sharing  regularized deep neural networks，并提供了一种基于物理系统的解释，以及一种 exponential speedup 的并行算法。</li>
<li>results: 实验表明，weight-sharing regularization 可以使得全连接网络学习 convolution-like 滤波器，并且 proximal mapping 算法可以提供 exponential speedup。<details>
<summary>Abstract</summary>
Weight-sharing is ubiquitous in deep learning. Motivated by this, we introduce ''weight-sharing regularization'' for neural networks, defined as $R(w) = \frac{1}{d - 1}\sum_{i > j}^d |w_i - w_j|$. We study the proximal mapping of $R$ and provide an intuitive interpretation of it in terms of a physical system of interacting particles. Using this interpretation, we design a novel parallel algorithm for $\operatorname{prox}_R$ which provides an exponential speedup over previous algorithms, with a depth of $O(\log^3 d)$. Our algorithm makes it feasible to train weight-sharing regularized deep neural networks with proximal gradient descent. Experiments reveal that weight-sharing regularization enables fully-connected networks to learn convolution-like filters.
</details>
<details>
<summary>摘要</summary>
“深度学习中的权重共享是普遍存在的。为了解决这个问题，我们介绍了一种新的“权重共享规则”($R(w) = \frac{1}{d - 1}\sum_{i > j}^d |w_i - w_j|$)，并研究了这个规则的距离映射。我们通过 физи学系中的互动粒子来解释这个映射的含义，并设计了一种新的并行算法来实现 $\operatorname{prox}_R$，它可以在 $O(\log^3 d)$ 深度下提供 exponential 的加速。我们的算法使得可以使用 proximal 梯度下降来训练权重共享规则的深度神经网络。实验表明，权重共享规则可以使得全连接网络学习类似于 convolution 的滤波器。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Equivariance-Is-Not-All-You-Need-Characterizing-the-Utility-of-Equivariant-Graph-Neural-Networks-for-Particle-Physics-Tasks"><a href="#Equivariance-Is-Not-All-You-Need-Characterizing-the-Utility-of-Equivariant-Graph-Neural-Networks-for-Particle-Physics-Tasks" class="headerlink" title="Equivariance Is Not All You Need: Characterizing the Utility of Equivariant Graph Neural Networks for Particle Physics Tasks"></a>Equivariance Is Not All You Need: Characterizing the Utility of Equivariant Graph Neural Networks for Particle Physics Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03094">http://arxiv.org/abs/2311.03094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savannah Thais, Daniel Murnane</li>
<li>for: This paper evaluates the benefits of using equivariant graph neural networks (GNNs) for real-world particle physics reconstruction tasks, and explores the limitations of these networks in realistic systems.</li>
<li>methods: The paper uses real-world particle physics reconstruction tasks to evaluate the performance of equivariant GNNs, and draws on the relevant literature around group equivariant networks to provide a comprehensive assessment of their benefits.</li>
<li>results: The paper demonstrates that many of the theoretical benefits of equivariant GNNs do not hold for realistic systems, and introduces compelling directions for future research that will benefit both the scientific theory of machine learning and physics applications.<details>
<summary>Abstract</summary>
Incorporating inductive biases into ML models is an active area of ML research, especially when ML models are applied to data about the physical world. Equivariant Graph Neural Networks (GNNs) have recently become a popular method for learning from physics data because they directly incorporate the symmetries of the underlying physical system. Drawing from the relevant literature around group equivariant networks, this paper presents a comprehensive evaluation of the proposed benefits of equivariant GNNs by using real-world particle physics reconstruction tasks as an evaluation test-bed. We demonstrate that many of the theoretical benefits generally associated with equivariant networks may not hold for realistic systems and introduce compelling directions for future research that will benefit both the scientific theory of ML and physics applications.
</details>
<details>
<summary>摘要</summary>
研究在机器学习（ML）模型中涵入推导性偏见是一个活跃领域，尤其是当ML模型应用于物理世界的数据时。最近，对物理系统数据进行学习的等效幂论神经网络（GNNs）已成为一种受欢迎的方法，因为它们直接涵入物理系统的Symmetries。从相关文献中提取到的关于群equivariant网络的知识，这篇论文对提出的优点进行了全面的评估，并使用实际的 particle physics重建任务作为评估测试床。我们发现了许多理论上关于等效网络的优点可能不适用于实际系统，并提出了有吸引力的未来研究方向，这些研究将有助于 both the scientific theory of ML和物理应用。
</details></li>
</ul>
<hr>
<h2 id="Persistent-homology-for-high-dimensional-data-based-on-spectral-methods"><a href="#Persistent-homology-for-high-dimensional-data-based-on-spectral-methods" class="headerlink" title="Persistent homology for high-dimensional data based on spectral methods"></a>Persistent homology for high-dimensional data based on spectral methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03087">http://arxiv.org/abs/2311.03087</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berenslab/eff-ph">https://github.com/berenslab/eff-ph</a></li>
<li>paper_authors: Sebastian Damrich, Philipp Berens, Dmitry Kobak</li>
<li>For: 检测点云数据中的非凡拓扑结构，如循环或空隙，而vanilla persistent homology在高维空间中容易受到噪声影响并错误检测拓扑结构。* Methods: 使用$k$-nearest-neighbor图的特征距离（如扩散距离和有效抗电阻）来改进 persistent homology，并 derive了一个新的关闭形式表达式，用于计算有效抗电阻，并描述了它们与扩散距离之间的关系。* Results: 应用这些方法于多个高维单元核酸测量数据集，并显示了使用$k$-nearest-neighbor图的特征距离可以robust地检测细胞周期循环。<details>
<summary>Abstract</summary>
Persistent homology is a popular computational tool for detecting non-trivial topology of point clouds, such as the presence of loops or voids. However, many real-world datasets with low intrinsic dimensionality reside in an ambient space of much higher dimensionality. We show that in this case vanilla persistent homology becomes very sensitive to noise and fails to detect the correct topology. The same holds true for most existing refinements of persistent homology. As a remedy, we find that spectral distances on the $k$-nearest-neighbor graph of the data, such as diffusion distance and effective resistance, allow persistent homology to detect the correct topology even in the presence of high-dimensional noise. Furthermore, we derive a novel closed-form expression for effective resistance in terms of the eigendecomposition of the graph Laplacian, and describe its relation to diffusion distances. Finally, we apply these methods to several high-dimensional single-cell RNA-sequencing datasets and show that spectral distances on the $k$-nearest-neighbor graph allow robust detection of cell cycle loops.
</details>
<details>
<summary>摘要</summary>
持续同态是一种广泛使用的计算工具，用于探测点云的不同几何结构，如循环或空隙。然而，许多真实世界数据集具有低自然维度，并且存在高维度的噪声。我们表明，在这种情况下，普通的持续同态会受到噪声的影响，无法正确探测几何结构。同时，大多数现有的持续同态改进方法也无法在高维度噪声下工作。为了解决这个问题，我们发现了一种新的方法，即基于数据集的k最近邻 Graph Laplacian的特征距离，如扩散距离和有效抗量。我们发现这些特征距离可以使持续同态在高维度噪声下正确探测几何结构。此外，我们还提出了一个新的关闭式表达式，用于计算有效抗量，并与扩散距离之间的关系。最后，我们应用这些方法到了一些高维单元细胞RNA-seq数据集，并显示了spectral distances on the $k$-nearest-neighbor graph可以robustly探测细胞周期循环。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-the-value-of-information-transfer-in-population-based-SHM"><a href="#Quantifying-the-value-of-information-transfer-in-population-based-SHM" class="headerlink" title="Quantifying the value of information transfer in population-based SHM"></a>Quantifying the value of information transfer in population-based SHM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03083">http://arxiv.org/abs/2311.03083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidan J. Hughes, Jack Poole, Nikolaos Dervilis, Paul Gardner, Keith Worden</li>
<li>For: The paper aims to develop a transfer-strategy decision process for a classification task in a population-based structural health monitoring (PBSHM) framework, using domain adaptation.* Methods: The paper uses a transfer decision framework based on the expected value of information transfer, which involves making predictions about classification performance in the target domain after information transfer. A probabilistic regression is used to predict classification performance from a proxy for structural similarity based on the modal assurance criterion.* Results: The paper demonstrates the effectiveness of the proposed transfer-strategy decision process in a simulated SHM maintenance problem, and shows that the expected value of information transfer can be used to guide the transfer of information between structures.<details>
<summary>Abstract</summary>
Population-based structural health monitoring (PBSHM), seeks to address some of the limitations associated with data scarcity that arise in traditional SHM. A tenet of the population-based approach to SHM is that information can be shared between sufficiently-similar structures in order to improve predictive models. Transfer learning techniques, such as domain adaptation, have been shown to be a highly-useful technology for sharing information between structures when developing statistical classifiers for PBSHM. Nonetheless, transfer-learning techniques are not without their pitfalls. In some circumstances, for example if the data distributions associated with the structures within a population are dissimilar, applying transfer-learning methods can be detrimental to classification performance -- this phenomenon is known as negative transfer. Given the potentially-severe consequences of negative transfer, it is prudent for engineers to ask the question `when, what, and how should one transfer between structures?'.   The current paper aims to demonstrate a transfer-strategy decision process for a classification task for a population of simulated structures in the context of a representative SHM maintenance problem, supported by domain adaptation. The transfer decision framework is based upon the concept of expected value of information transfer. In order to compute the expected value of information transfer, predictions must be made regarding the classification (and decision performance) in the target domain following information transfer. In order to forecast the outcome of transfers, a probabilistic regression is used here to predict classification performance from a proxy for structural similarity based on the modal assurance criterion.
</details>
<details>
<summary>摘要</summary>
Population-based结构健康监测（PBSHM）想要解决传统结构健康监测中的数据稀缺问题。PBSHM的一个基本思想是在相似的结构之间共享信息，以改进预测模型。但是转移学习技术，如领域适应，也有一些缺点。在某些情况下，如结构内Population的数据分布不同，应用转移学习方法可能会导致分类性能下降，这种现象被称为负向转移。为了避免负向转移的严重后果，工程师应该问到“何时、何种、如何进行转移？”。本文旨在提出一种转移策略决策过程，用于一种基于预测性能的分类任务，在一个代表性的维保问题上。转移决策框架基于预测信息转移的预期价值。为计算预测信息转移的预期价值，需要对目标领域中的分类性能进行预测。为预测转移的结果，这里使用了一种概率回归方法，来预测基于模式保证因子的结构相似度。
</details></li>
</ul>
<hr>
<h2 id="SoK-Memorisation-in-machine-learning"><a href="#SoK-Memorisation-in-machine-learning" class="headerlink" title="SoK: Memorisation in machine learning"></a>SoK: Memorisation in machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03075">http://arxiv.org/abs/2311.03075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dmitrii Usynin, Moritz Knolle, Georgios Kaissis</li>
<li>for: 本研究旨在探讨机器学习模型中个体数据样本的影响，特别是在深度学习中，对于有限数据生成分布中学习复杂和高维关系时。</li>
<li>methods: 本研究系统化了多种前期研究和视角，探讨memorization在机器学习中的定义和其与模型泛化的关系，以及这些现象对数据隐私的影响。同时，本研究还提供了识别和评估memorization的方法，以及在多种机器学习学习环境中应用这些方法的情况。</li>
<li>results: 本研究结果表明，memorization在机器学习中存在广泛的定义和视角，并且与模型泛化有Close关系。此外，本研究还发现了一些方法可以识别和评估memorization，并且在多种机器学习学习环境中应用这些方法可以提高模型的隐私性。<details>
<summary>Abstract</summary>
Quantifying the impact of individual data samples on machine learning models is an open research problem. This is particularly relevant when complex and high-dimensional relationships have to be learned from a limited sample of the data generating distribution, such as in deep learning. It was previously shown that, in these cases, models rely not only on extracting patterns which are helpful for generalisation, but also seem to be required to incorporate some of the training data more or less as is, in a process often termed memorisation. This raises the question: if some memorisation is a requirement for effective learning, what are its privacy implications? In this work we unify a broad range of previous definitions and perspectives on memorisation in ML, discuss their interplay with model generalisation and their implications of these phenomena on data privacy. Moreover, we systematise methods allowing practitioners to detect the occurrence of memorisation or quantify it and contextualise our findings in a broad range of ML learning settings. Finally, we discuss memorisation in the context of privacy attacks, differential privacy (DP) and adversarial actors.
</details>
<details>
<summary>摘要</summary>
量化机器学习模型中个别数据样本的影响是一个开放的研究问题。特别是在深度学习中，当需要从有限的数据生成分布中学习复杂和高维的关系时，模型不仅需要抽取有助于泛化的模式，还似乎需要直接将一些训练数据纳入模型中，这种现象通常被称为memorization。这引发了一个问题：如果一定程度的memorization是有效学习的必要条件，那么这些现象具有什么隐私含义？在这个工作中，我们将结合各种前期定义和视角来解释memorization在ML中的含义，评估其与模型泛化的关系，并对这些现象在各种ML学习设置下进行系统化分析。此外，我们还会讨论memorization在隐私攻击、数据隐私（DP）和对抗攻击者的情况下的影响。
</details></li>
</ul>
<hr>
<h2 id="Imaging-through-multimode-fibres-with-physical-prior"><a href="#Imaging-through-multimode-fibres-with-physical-prior" class="headerlink" title="Imaging through multimode fibres with physical prior"></a>Imaging through multimode fibres with physical prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03062">http://arxiv.org/abs/2311.03062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuncheng Zhang, Yingjie Shi, Zheyi Yao, Xiubao Sui, Qian Cheng</li>
<li>for: 这篇论文主要用于提出一种physics-assisted, unsupervised, learning-based fibre imaging scheme，用于在扰动多模式纤维中进行图像重建。</li>
<li>methods: 该方法使用了深度学习，并利用物理优化方法简化了扰动纤维图像与目标图像之间的映射关系，从而降低计算复杂性。</li>
<li>results: 该方法可以通过在线学习，只需要几个扰动纤维的图像和无标目标图像，就可以重建高质量的图像。此外，该方法还提高了学习基于方法在扰动多模式纤维中的通用能力。<details>
<summary>Abstract</summary>
Imaging through perturbed multimode fibres based on deep learning has been widely researched. However, existing methods mainly use target-speckle pairs in different configurations. It is challenging to reconstruct targets without trained networks. In this paper, we propose a physics-assisted, unsupervised, learning-based fibre imaging scheme. The role of the physical prior is to simplify the mapping relationship between the speckle pattern and the target image, thereby reducing the computational complexity. The unsupervised network learns target features according to the optimized direction provided by the physical prior. Therefore, the reconstruction process of the online learning only requires a few speckle patterns and unpaired targets. The proposed scheme also increases the generalization ability of the learning-based method in perturbed multimode fibres. Our scheme has the potential to extend the application of multimode fibre imaging.
</details>
<details>
<summary>摘要</summary>
依靠受扰多模式纤维的成像通过深度学习已经广泛研究。现有方法主要使用不同配置的目标-特点对。很难无需训练网络来重建目标。在本文中，我们提出了一种基于物理优先的、无监督学习基于纤维成像方案。物理优先的作用是将特点图像和扰乱模式图像之间的映射关系简化，从而降低计算复杂性。无监督网络根据优化的方向学习目标特征，因此在线学习只需要几个扰乱模式和无对应的目标。我们的方案还能够提高深度学习基于多模式纤维成像的泛化能力。我们的方案具有扩展多模式纤维成像的潜力。
</details></li>
</ul>
<hr>
<h2 id="Learned-layered-coding-for-Successive-Refinement-in-the-Wyner-Ziv-Problem"><a href="#Learned-layered-coding-for-Successive-Refinement-in-the-Wyner-Ziv-Problem" class="headerlink" title="Learned layered coding for Successive Refinement in the Wyner-Ziv Problem"></a>Learned layered coding for Successive Refinement in the Wyner-Ziv Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03061">http://arxiv.org/abs/2311.03061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Joukovsky, Brent De Weerdt, Nikos Deligiannis</li>
<li>for: 学习进行逐步编码的维нер- Живу coding问题，并在不同的质量水平下逐步解码。</li>
<li>methods: 使用循环神经网络（RNN）学习层次编码器和解码器，并通过最小化变量约束来训练模型。</li>
<li>results: 实验表明，RNN可以直接获得层次归一化解决方案，并且Rate-Distortion性能与单一的维нер- Живу coding方法相当，并且几乎与Rate-Distortion约束相符。<details>
<summary>Abstract</summary>
We propose a data-driven approach to explicitly learn the progressive encoding of a continuous source, which is successively decoded with increasing levels of quality and with the aid of correlated side information. This setup refers to the successive refinement of the Wyner-Ziv coding problem. Assuming ideal Slepian-Wolf coding, our approach employs recurrent neural networks (RNNs) to learn layered encoders and decoders for the quadratic Gaussian case. The models are trained by minimizing a variational bound on the rate-distortion function of the successively refined Wyner-Ziv coding problem. We demonstrate that RNNs can explicitly retrieve layered binning solutions akin to scalable nested quantization. Moreover, the rate-distortion performance of the scheme is on par with the corresponding monolithic Wyner-Ziv coding approach and is close to the rate-distortion bound.
</details>
<details>
<summary>摘要</summary>
我们提出了一种数据驱动的方法，用于显式地学习不断编码一个连续源，并在不同的质量水平下逐步解码，并且使用相关的侧信息。这种设置与Wyner-Ziv编码问题的逐步精化相关。假设理想的Slepian-Wolf编码，我们使用循环神经网络（RNN）学习层次编码器和解码器，并在二阶均勋函数的极限下进行训练。我们示出了RNN可以直接获取层次归一化解决方案，类似于扩展的嵌套量化。此外，我们的方案的比特率-质量表现与对应的庞大Wyner-Ziv编码方法几乎相同，并且几乎与比特率-质量函数的上限相同。
</details></li>
</ul>
<hr>
<h2 id="Personalizing-Keyword-Spotting-with-Speaker-Information"><a href="#Personalizing-Keyword-Spotting-with-Speaker-Information" class="headerlink" title="Personalizing Keyword Spotting with Speaker Information"></a>Personalizing Keyword Spotting with Speaker Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03419">http://arxiv.org/abs/2311.03419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beltrán Labrador, Pai Zhu, Guanlong Zhao, Angelo Scorza Scarpati, Quan Wang, Alicia Lozano-Diez, Alex Park, Ignacio López Moreno</li>
<li>for: 提高关键词检测精度，特别是面对多元化的人群和年龄组团时。</li>
<li>methods: 利用Feature-wise Linear Modulation（FiLM）方法，将说话人信息集成到关键词检测中，实现Text-Dependent和Text-Independent说话人识别系统。</li>
<li>results: 在多元化的数据集上进行测试，实现了关键词检测精度的显著提高，特别是面对下 Represented speaker groups。此外，该方法只需增加1%的参数数量，并不影响延迟和计算成本，适用于实际应用。<details>
<summary>Abstract</summary>
Keyword spotting systems often struggle to generalize to a diverse population with various accents and age groups. To address this challenge, we propose a novel approach that integrates speaker information into keyword spotting using Feature-wise Linear Modulation (FiLM), a recent method for learning from multiple sources of information. We explore both Text-Dependent and Text-Independent speaker recognition systems to extract speaker information, and we experiment on extracting this information from both the input audio and pre-enrolled user audio. We evaluate our systems on a diverse dataset and achieve a substantial improvement in keyword detection accuracy, particularly among underrepresented speaker groups. Moreover, our proposed approach only requires a small 1% increase in the number of parameters, with a minimum impact on latency and computational cost, which makes it a practical solution for real-world applications.
</details>
<details>
<summary>摘要</summary>
��<<SYS>>���racle�спот�系统��� newline�struggle� generalize� diverse� population� various� accents� age�groups. To address� this challenge, we propose� novel approach� integrates� speaker information� into keyword spotting using Feature-wise Linear Modulation (FiLM), a recent method for learning from multiple sources of information. We explore both Text-Dependent and Text-Independent speaker recognition systems to extract speaker information, and we experiment on extracting this information from both the input audio and pre-enrolled user audio. We evaluate our systems on a diverse dataset and achieve a substantial improvement in keyword detection accuracy, particularly among underrepresented speaker groups. Moreover, our proposed approach only requires a small 1% increase in the number of parameters, with a minimum impact on latency and computational cost, which makes it a practical solution for real-world applications.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="DRAUC-An-Instance-wise-Distributionally-Robust-AUC-Optimization-Framework"><a href="#DRAUC-An-Instance-wise-Distributionally-Robust-AUC-Optimization-Framework" class="headerlink" title="DRAUC: An Instance-wise Distributionally Robust AUC Optimization Framework"></a>DRAUC: An Instance-wise Distributionally Robust AUC Optimization Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03055">http://arxiv.org/abs/2311.03055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siran Dai, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, Qingming Huang</li>
<li>for: The paper focuses on improving the Area Under the ROC Curve (AUC) in long-tailed classification scenarios, where the distribution of the data is not uniform.</li>
<li>methods: The paper proposes an instance-wise surrogate loss of Distributionally Robust AUC (DRAUC) to optimize the AUC under worst-case distributional shift. The proposed method is built on top of Distributionally Robust Optimization (DRO) to enhance model performance.</li>
<li>results: The paper shows that the proposed method outperforms existing methods in terms of AUC under distributional shift. The authors also highlight that conventional DRAUC may induce label bias and propose a distribution-aware DRAUC as a more suitable metric for robust AUC learning. Theoretical analysis and empirical experiments on corrupted benchmark datasets demonstrate the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
The Area Under the ROC Curve (AUC) is a widely employed metric in long-tailed classification scenarios. Nevertheless, most existing methods primarily assume that training and testing examples are drawn i.i.d. from the same distribution, which is often unachievable in practice. Distributionally Robust Optimization (DRO) enhances model performance by optimizing it for the local worst-case scenario, but directly integrating AUC optimization with DRO results in an intractable optimization problem. To tackle this challenge, methodically we propose an instance-wise surrogate loss of Distributionally Robust AUC (DRAUC) and build our optimization framework on top of it. Moreover, we highlight that conventional DRAUC may induce label bias, hence introducing distribution-aware DRAUC as a more suitable metric for robust AUC learning. Theoretically, we affirm that the generalization gap between the training loss and testing error diminishes if the training set is sufficiently large. Empirically, experiments on corrupted benchmark datasets demonstrate the effectiveness of our proposed method. Code is available at: https://github.com/EldercatSAM/DRAUC.
</details>
<details>
<summary>摘要</summary>
“区域下ROC曲线（AUC）是长尾分类任务中广泛使用的衡量指标。然而，现有的方法几乎都假设训练和测试例子是从同一个分布中随机抽出的，这在实际应用中通常是不可能的。分布强健优化（DRO）可以提高模型性能，但是直接将AUC优化融入DRO中会导致问题不可解。为了解决这个挑战，我们提出了实例别的代理损失函数（surrogate loss），将其组合成一个优化框架。此外，我们点出了传统的DRAUC可能会导致标签偏调，因此提出了分布意识的DRAUC作为更适合的稳健AUC学习指标。理论上，我们证明了训练集大 enough会使得测试集误差与训练集误差之间的差异减少。实验结果显示了我们的提案的效果，codes可以在https://github.com/EldercatSAM/DRAUC上获取。”Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Validity-problems-in-clinical-machine-learning-by-indirect-data-labeling-using-consensus-definitions"><a href="#Validity-problems-in-clinical-machine-learning-by-indirect-data-labeling-using-consensus-definitions" class="headerlink" title="Validity problems in clinical machine learning by indirect data labeling using consensus definitions"></a>Validity problems in clinical machine learning by indirect data labeling using consensus definitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03037">http://arxiv.org/abs/2311.03037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/statnlp/ml4h_validity_problems">https://github.com/statnlp/ml4h_validity_problems</a></li>
<li>paper_authors: Michael Hagmann, Shigehiko Schamoni, Stefan Riezler</li>
<li>for: 预测疾病发展（disease diagnosis）的重要应用领域中存在机器学习的有效性问题。</li>
<li>methods: 研究者使用了一种通用的方法来检测问题数据集和黑盒机器学习模型是否存在问题。</li>
<li>results: 研究者发现了一种检测方法，可以帮助检测问题数据集和黑盒机器学习模型，并且在预测疾病发展任务中进行了实践。<details>
<summary>Abstract</summary>
We demonstrate a validity problem of machine learning in the vital application area of disease diagnosis in medicine. It arises when target labels in training data are determined by an indirect measurement, and the fundamental measurements needed to determine this indirect measurement are included in the input data representation. Machine learning models trained on this data will learn nothing else but to exactly reconstruct the known target definition. Such models show perfect performance on similarly constructed test data but will fail catastrophically on real-world examples where the defining fundamental measurements are not or only incompletely available. We present a general procedure allowing identification of problematic datasets and black-box machine learning models trained on them, and exemplify our detection procedure on the task of early prediction of sepsis.
</details>
<details>
<summary>摘要</summary>
我们描述了机器学习在医学领域的重要应用中的有效性问题。这种问题发生在训练数据中的目标标签是基于间接测量的，而根本测量 needed to determine this indirect measurement 是包含在输入数据表示中的。机器学习模型在这种数据上训练后会只学习重建已知的目标定义，并在同构测试数据上达到完美的性能。但在真实世界中，这些定义基本测量不可能或只有部分可用时，这些模型会catastrophically fail。我们提出了一种通用的数据集问题标识和黑盒机器学习模型训练数据集问题的方法，并在预测性 septic 任务中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="On-regularized-polynomial-functional-regression"><a href="#On-regularized-polynomial-functional-regression" class="headerlink" title="On regularized polynomial functional regression"></a>On regularized polynomial functional regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03036">http://arxiv.org/abs/2311.03036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Holzleitner, Sergei Pereverzyev</li>
<li>for: 该论文涉及多项式函数回归，并提出了一个新的finite sample bound，该bound涵盖了一般的光滑条件、容量条件以及规则化技术等方面。</li>
<li>methods: 该论文使用了多项式函数回归，并采用了一些常用的规则化技术和容量条件。</li>
<li>results: 该论文的numerical evidence表明，使用高阶多项式函数可以提高回归性能。<details>
<summary>Abstract</summary>
This article offers a comprehensive treatment of polynomial functional regression, culminating in the establishment of a novel finite sample bound. This bound encompasses various aspects, including general smoothness conditions, capacity conditions, and regularization techniques. In doing so, it extends and generalizes several findings from the context of linear functional regression as well. We also provide numerical evidence that using higher order polynomial terms can lead to an improved performance.
</details>
<details>
<summary>摘要</summary>
这篇文章提供了多元函数回归的全面征识，最终得出了一个新的finite sample bound。这个 bound 包括了一般的光滑条件、容量条件以及规则化技术。因此，它扩展和总结了 linear functional regression 的一些发现。我们还提供了数字证明，使用更高阶的多元函数项可以提高性能。Note: "finite sample bound" in the original text is translated as "finite sample bound" (finite sample bound) in Simplified Chinese, as there is no direct translation for "finite sample bound" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Estimating-treatment-effects-from-single-arm-trials-via-latent-variable-modeling"><a href="#Estimating-treatment-effects-from-single-arm-trials-via-latent-variable-modeling" class="headerlink" title="Estimating treatment effects from single-arm trials via latent-variable modeling"></a>Estimating treatment effects from single-arm trials via latent-variable modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03002">http://arxiv.org/abs/2311.03002</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manuelhaussmann/lvm_singlearm">https://github.com/manuelhaussmann/lvm_singlearm</a></li>
<li>paper_authors: Manuel Haussmann, Tran Minh Son Le, Viivi Halla-aho, Samu Kurki, Jussi Leinonen, Miika Koskinen, Samuel Kaski, Harri Lähdesmäki</li>
<li>for: 代替Randomized controlled trials (RCTs)，用于估计治疗效果。</li>
<li>methods: 使用深度嵌入变量模型，可以考虑欠拟合观察的 Patterns。</li>
<li>results: 比前方法提高了直接治疗效果估计和病人匹配估计的性能。<details>
<summary>Abstract</summary>
Randomized controlled trials (RCTs) are the accepted standard for treatment effect estimation but they can be infeasible due to ethical reasons and prohibitive costs. Single-arm trials, where all patients belong to the treatment group, can be a viable alternative but require access to an external control group. We propose an identifiable deep latent-variable model for this scenario that can also account for missing covariate observations by modeling their structured missingness patterns. Our method uses amortized variational inference to learn both group-specific and identifiable shared latent representations, which can subsequently be used for (i) patient matching if treatment outcomes are not available for the treatment group, or for (ii) direct treatment effect estimation assuming outcomes are available for both groups. We evaluate the model on a public benchmark as well as on a data set consisting of a published RCT study and real-world electronic health records. Compared to previous methods, our results show improved performance both for direct treatment effect estimation as well as for effect estimation via patient matching.
</details>
<details>
<summary>摘要</summary>
Randomized controlled trials (RCTs) 是评估治疗效果的标准方法，但它们可能因为伦理和成本因素而无法实施。单臂试验，其中所有患者都属于治疗组，可以作为一种可行的替代方案。我们提议一种可识别深度隐藏变量模型来解决这种情况，该模型还可以考虑欠拟合的 covariate 观察数据的结构化欠拟合模式。我们的方法使用摘要变量推断来学习群体特定和可识别的共享隐藏变量表示，这些表示可以用于（i）如果治疗结果不可用于治疗组，则进行患者匹配，或者（ii）直接计算治疗效果的推断。我们在一个公共标准和一个包含已发表RCT研究和真实电子医疗记录的数据集上评估了我们的方法。相比之前的方法，我们的结果显示了改进的性能，包括直接治疗效果推断和通过患者匹配来计算治疗效果。
</details></li>
</ul>
<hr>
<h2 id="Variational-Weighting-for-Kernel-Density-Ratios"><a href="#Variational-Weighting-for-Kernel-Density-Ratios" class="headerlink" title="Variational Weighting for Kernel Density Ratios"></a>Variational Weighting for Kernel Density Ratios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03001">http://arxiv.org/abs/2311.03001</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swyoon/variationally-weighted-kernel-density-estimation">https://github.com/swyoon/variationally-weighted-kernel-density-estimation</a></li>
<li>paper_authors: Sangwoong Yoon, Frank C. Park, Gunsu S Yun, Iljung Kim, Yung-Kyun Noh</li>
<li>for: 提高机器学习中的生成和推理任务中的kernel density estimation（KDE）的精度。</li>
<li>methods: 利用多维 calculus of variations  derive 标准kernel density estimate中的优化Weight函数，以减少预测 posterior 和信息论量度的偏差。</li>
<li>results: 提高了 KDE 中的预测 posterior 和信息论量度的估计精度。<details>
<summary>Abstract</summary>
Kernel density estimation (KDE) is integral to a range of generative and discriminative tasks in machine learning. Drawing upon tools from the multidimensional calculus of variations, we derive an optimal weight function that reduces bias in standard kernel density estimates for density ratios, leading to improved estimates of prediction posteriors and information-theoretic measures. In the process, we shed light on some fundamental aspects of density estimation, particularly from the perspective of algorithms that employ KDEs as their main building blocks.
</details>
<details>
<summary>摘要</summary>
kernel density estimation（KDE）是机器学习中多种生成和识别任务的关键技术之一。我们通过多维Calculus of variations中的工具，得出了改善标准kernel density estimate的优化权重函数，从而提高预测 posterior和信息论量的估计。在这个过程中，我们还探讨了density estimation的一些基本问题，特别是那些使用KDE作为主要构建件的算法。Here's a breakdown of the translation:* Kernel density estimation (KDE) is an important technique in machine learning for generative and discriminative tasks.* We use tools from multidimensional calculus of variations to derive an optimal weight function that reduces bias in standard kernel density estimates for density ratios.* This leads to improved estimates of prediction posteriors and information-theoretic measures.* In the process, we shed light on some fundamental aspects of density estimation, particularly from the perspective of algorithms that employ KDEs as their main building blocks.
</details></li>
</ul>
<hr>
<h2 id="Strong-statistical-parity-through-fair-synthetic-data"><a href="#Strong-statistical-parity-through-fair-synthetic-data" class="headerlink" title="Strong statistical parity through fair synthetic data"></a>Strong statistical parity through fair synthetic data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03000">http://arxiv.org/abs/2311.03000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivona Krchova, Michael Platzer, Paul Tiwald</li>
<li>for: 保护原始数据隐私和符合公平性标准</li>
<li>methods: 使用人工智能生成的合成数据，通过对敏感属性的学习目标概率分布进行平衡，使下游模型在各个阈值下做出公平预测。</li>
<li>results: 通过将公平性定义为统计平衡，生成的合成数据能够提供强公平预测，即从偏见原始数据中做出公平预测。此公平调整可以直接 integrate into 生成器的采样过程或作为后处理步骤进行。<details>
<summary>Abstract</summary>
AI-generated synthetic data, in addition to protecting the privacy of original data sets, allows users and data consumers to tailor data to their needs. This paper explores the creation of synthetic data that embodies Fairness by Design, focusing on the statistical parity fairness definition. By equalizing the learned target probability distributions of the synthetic data generator across sensitive attributes, a downstream model trained on such synthetic data provides fair predictions across all thresholds, that is, strong fair predictions even when inferring from biased, original data. This fairness adjustment can be either directly integrated into the sampling process of a synthetic generator or added as a post-processing step. The flexibility allows data consumers to create fair synthetic data and fine-tune the trade-off between accuracy and fairness without any previous assumptions on the data or re-training the synthetic data generator.
</details>
<details>
<summary>摘要</summary>
人工生成的数据，除了保护原始数据集的隐私，还允许用户和数据消费者根据需要修改数据。这篇论文探讨了基于 Fairness by Design 的synthetic数据的创造，专注于统计平均性公平定义。通过在敏感特征上均衡synthetic数据生成器学习的目标概率分布，下游模型在such synthetic数据上训练时提供了公平预测结果，无论推理自偏向的原始数据。这种公平调整可以直接集成到synthetic生成器的抽样过程中，或作为后处理步骤进行。这种灵活性允许数据消费者创造公平的synthetic数据，并微调准确性和公平之间的负荷平衡，无需对数据或synthetic数据生成器进行任何先前假设或重新训练。
</details></li>
</ul>
<hr>
<h2 id="Hacking-Cryptographic-Protocols-with-Advanced-Variational-Quantum-Attacks"><a href="#Hacking-Cryptographic-Protocols-with-Advanced-Variational-Quantum-Attacks" class="headerlink" title="Hacking Cryptographic Protocols with Advanced Variational Quantum Attacks"></a>Hacking Cryptographic Protocols with Advanced Variational Quantum Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02986">http://arxiv.org/abs/2311.02986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Borja Aizpurua, Pablo Bermejo, Josu Etxezarreta Martinez, Roman Orus</li>
<li>for: 这个论文旨在提出一种改进的量子变换攻击算法（VQAA），用于攻击密码协议。</li>
<li>methods: 这篇论文使用了新的量子攻击方法，可以更有效地攻击一些知名的密码协议，比如S-DES、S-AES和Blowfish。这些攻击方法使用了更少的量子比特数，并且可以更快地完成攻击任务。</li>
<li>results: 论文的实验结果表明，使用这种新的量子攻击方法可以在一些小型量子计算机上模拟一个32位Blowfish实例的秘钥找到，只需要24次更少的迭代次数 than一个普通攻击。此外，这种攻击方法也可以在其他一些轻量级加密协议中提高攻击成功率。<details>
<summary>Abstract</summary>
Here we introduce an improved approach to Variational Quantum Attack Algorithms (VQAA) on crytographic protocols. Our methods provide robust quantum attacks to well-known cryptographic algorithms, more efficiently and with remarkably fewer qubits than previous approaches. We implement simulations of our attacks for symmetric-key protocols such as S-DES, S-AES and Blowfish. For instance, we show how our attack allows a classical simulation of a small 8-qubit quantum computer to find the secret key of one 32-bit Blowfish instance with 24 times fewer number of iterations than a brute-force attack. Our work also shows improvements in attack success rates for lightweight ciphers such as S-DES and S-AES. Further applications beyond symmetric-key cryptography are also discussed, including asymmetric-key protocols and hash functions. In addition, we also comment on potential future improvements of our methods. Our results bring one step closer assessing the vulnerability of large-size classical cryptographic protocols with Noisy Intermediate-Scale Quantum (NISQ) devices, and set the stage for future research in quantum cybersecurity.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Pursuit-of-Human-Labeling-A-New-Perspective-on-Unsupervised-Learning"><a href="#The-Pursuit-of-Human-Labeling-A-New-Perspective-on-Unsupervised-Learning" class="headerlink" title="The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning"></a>The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02940">http://arxiv.org/abs/2311.02940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlbio-epfl/hume">https://github.com/mlbio-epfl/hume</a></li>
<li>paper_authors: Artyom Gadetsky, Maria Brbic</li>
<li>for: 本研究旨在提出一种无需外部监督的推理方法，用于推测给定数据集的人类标注。</li>
<li>methods: 该方法基于人类标注的类别之间的线性分离性，通过搜索所有可能的标注来找出下面的人类标注。</li>
<li>results: 该方法可以与任何大型预训练和自动监督模型兼容，并在STL-10和CIFAR-10 datasets上实现了显著的提升和相当的性能。相比现有的无监督基elines，本研究在四个图像分类数据集上达到了状态级表现。<details>
<summary>Abstract</summary>
We present HUME, a simple model-agnostic framework for inferring human labeling of a given dataset without any external supervision. The key insight behind our approach is that classes defined by many human labelings are linearly separable regardless of the representation space used to represent a dataset. HUME utilizes this insight to guide the search over all possible labelings of a dataset to discover an underlying human labeling. We show that the proposed optimization objective is strikingly well-correlated with the ground truth labeling of the dataset. In effect, we only train linear classifiers on top of pretrained representations that remain fixed during training, making our framework compatible with any large pretrained and self-supervised model. Despite its simplicity, HUME outperforms a supervised linear classifier on top of self-supervised representations on the STL-10 dataset by a large margin and achieves comparable performance on the CIFAR-10 dataset. Compared to the existing unsupervised baselines, HUME achieves state-of-the-art performance on four benchmark image classification datasets including the large-scale ImageNet-1000 dataset. Altogether, our work provides a fundamentally new view to tackle unsupervised learning by searching for consistent labelings between different representation spaces.
</details>
<details>
<summary>摘要</summary>
我团队今天发布了一种名为HUME的简单的模型无关框架，用于无外部监督的数据集标注推断。我们的思路是，由多个人标注的类将在不同的表示空间中Linearly separable。HUME利用这一点来导引对所有可能的标注集合的搜索，以找到下面的人类标注。我们表明，提出的优化目标与实际的数据集标注高度相关。因此，我们只需在固定的表示空间中训练线性分类器，使得我们的框架与任何大型预训练和自动标注模型兼容。尽管其简单，但HUME在STL-10 dataset上大幅超越了基于自动标注的线性分类器，并在CIFAR-10 dataset上达到了相似的性能。与现有的无监督基准之间比较，HUME在四个基准图像分类dataset上达到了状态的最佳性能，包括大规模的ImageNet-1000 dataset。总的来说，我们的工作提供了一种新的视角来解决无监督学习，通过在不同的表示空间之间寻找一致的标注。
</details></li>
</ul>
<hr>
<h2 id="Edge2Node-Reducing-Edge-Prediction-to-Node-Classification"><a href="#Edge2Node-Reducing-Edge-Prediction-to-Node-Classification" class="headerlink" title="Edge2Node: Reducing Edge Prediction to Node Classification"></a>Edge2Node: Reducing Edge Prediction to Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02921">http://arxiv.org/abs/2311.02921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arahmatiiii/E2N">https://github.com/arahmatiiii/E2N</a></li>
<li>paper_authors: Zahed Rahmati, Ali Rahmati, Dariush Kazemi</li>
<li>for: 本文是针对图像预测缺失或潜在关系的任务进行研究，提出了一种新的 Edge2Node（E2N）方法，可以直接从图像中提取边的嵌入，不需要预先定义评分函数。</li>
<li>methods: 本文提出了一种新的 E2N 方法，包括在基于给定图像的新图像 H 中创建一个新的图像，并将图像预测任务转换为图像分类任务。</li>
<li>results: 在 ogbl-ddi 和 ogbl-collab 数据集上，E2N 方法与现有的状态艺术方法进行比较，实现了较高的 Hits@20 和 Hits@50 分数。在 ogbl-ddi 数据集上，我们在验证集上达到了 Hits@20 分数为 98.79%，并在测试集上达到了 Hits@20 分数为 98.11%。在 ogbl-collab 数据集上，我们在验证集上达到了 Hits@50 分数为 95.46%，并在测试集上达到了 Hits@50 分数为 95.15%。<details>
<summary>Abstract</summary>
Despite the success of graph neural network models in node classification, edge prediction (the task of predicting missing or potential relationships between nodes in a graph) remains a challenging problem for these models. A common approach for edge prediction is to first obtain the embeddings of two nodes, and then a predefined scoring function is used to predict the existence of an edge between the two nodes. In this paper, we introduce a new approach called E2N (Edge2Node) which directly obtains an embedding for each edge, without the need for a scoring function. To do this, we create a new graph H based on the graph G given for the edge prediction task, and then reduce the edge prediction task on G to a node classification task on H. Our E2N method can be easily applied to any edge prediction task with superior performance and lower computational costs.   For the ogbl-ddi and ogbl-collab datasets, our E2N method outperforms the state-of-the-art methods listed on the leaderboards. Our experiments on the ogbl-ddi dataset achieved a Hits@20 score of 98.79% on the validation set and 98.11% on the test set. On the ogbl-collab dataset, we achieved a Hits@50 score of 95.46% on the validation set and 95.15% on the test set.
</details>
<details>
<summary>摘要</summary>
尽管图 neural network 模型在节点分类任务上得到了成功，但Edge prediction（预测图中缺失或可能存在的两个节点之间的关系）仍然是这些模型中的一个挑战。一般来说，用于Edge prediction的方法是先获取两个节点的嵌入，然后使用预定的分数函数来预测这两个节点之间的关系是否存在。在本文中，我们引入了一种新的方法，即E2N（Edge2Node），可以直接获取每个边的嵌入，不需要预定的分数函数。我们创建了一个新的图H，基于给定的图G，以便在G上进行边预测任务。然后，我们将边预测任务转化为图H上的节点分类任务。我们的E2N方法可以轻松应用于任何边预测任务，并且能够提供更高的性能和更低的计算成本。在ogbl-ddi和ogbl-collab datasets上，我们的E2N方法超过了当前的状态码列表。我们对ogbl-ddi dataset进行了验证集和测试集的验证， validation set上的Hits@20分数为98.79%，测试集上的Hits@20分数为98.11%。在ogbl-collab dataset上，我们对验证集和测试集进行了验证， validation set上的Hits@50分数为95.46%，测试集上的Hits@50分数为95.15%。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Matrix-Based-Sampling-for-Graph-Neural-Network-Training"><a href="#Distributed-Matrix-Based-Sampling-for-Graph-Neural-Network-Training" class="headerlink" title="Distributed Matrix-Based Sampling for Graph Neural Network Training"></a>Distributed Matrix-Based Sampling for Graph Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02909">http://arxiv.org/abs/2311.02909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Alok Tripathy, Katherine Yelick, Aydin Buluc</li>
<li>for: 本文提出了一种新的分布式GNN训练中的采样方法，用于减少通信量。</li>
<li>methods: 本文提出了一种基于矩阵的批量采样方法，可以同时采样多个批处理。在输入图不能在单个设备内存中的情况下，我们将图分布到多个设备上，并使用通信避免的SpGEMM算法来扩展GNN批处理的训练。此外，我们还提出了一种用于代表不同采样算法的简单matrix构造方法。</li>
<li>results: 我们在最大Open Graph Benchmark（OGB）集合上使用128个GPU进行实验，并证明了我们的管道在一个3层GraphSAGE网络上比Quiver（一种分布式PyTorch-Geometric扩展）快$2.5\times$。在OGB集合之外，我们在128个GPU上实现了一个epoch时间的提高$8.46\times$。最后，我们还展示了将图分布在GPU上的扩展和采样算法的扩展。<details>
<summary>Abstract</summary>
The primary contribution of this paper is new methods for reducing communication in the sampling step for distributed GNN training. Here, we propose a matrix-based bulk sampling approach that expresses sampling as a sparse matrix multiplication (SpGEMM) and samples multiple minibatches at once. When the input graph topology does not fit on a single device, our method distributes the graph and use communication-avoiding SpGEMM algorithms to scale GNN minibatch sampling, enabling GNN training on much larger graphs than those that can fit into a single device memory. When the input graph topology (but not the embeddings) fits in the memory of one GPU, our approach (1) performs sampling without communication, (2) amortizes the overheads of sampling a minibatch, and (3) can represent multiple sampling algorithms by simply using different matrix constructions. In addition to new methods for sampling, we show that judiciously replicating feature data with a simple all-to-all exchange can outperform current methods for the feature extraction step in distributed GNN training. We provide experimental results on the largest Open Graph Benchmark (OGB) datasets on $128$ GPUs, and show that our pipeline is $2.5\times$ faster Quiver (a distributed extension to PyTorch-Geometric) on a $3$-layer GraphSAGE network. On datasets outside of OGB, we show a $8.46\times$ speedup on $128$ GPUs in-per epoch time. Finally, we show scaling when the graph is distributed across GPUs and scaling for both node-wise and layer-wise sampling algorithms
</details>
<details>
<summary>摘要</summary>
主要贡献 OF 这篇论文在于提出了用于降低分布式 GNN 训练中采样步骤中的新方法。我们提议了一种基于矩阵的批量采样方法，将采样表示为稀疏矩阵乘法（SpGEMM），并在一次多个批处理多个批处理。当输入图 topology 不能在单个设备内存中存储时，我们将图分布在多个设备上，并使用通信快速的 SpGEMM 算法来扩展 GNN 批处理的训练范围。当输入图 topology 可以在单个 GPU 内存中存储时，我们的方法可以无需通信进行采样，并且可以吃费采样批处理的开销。此外，我们还提出了一种使用简单的所有对所有交换来快速复制特征数据的方法，以提高分布式 GNN 训练中的特征提取步骤。我们在 $128$ 个 GPU 上进行了实验，并证明了我们的管道在 $3$-layer GraphSAGE 网络上比 Quiver （分布式 PyTorch-Geometric 扩展）快 $2.5\times$。在 OGB  datasets 外，我们在 $128$ 个 GPU 上实现了在一个epoch时间内的快速速度为 $8.46\times$。最后，我们还展示了分布在 GPU 上的图和层wise 采样算法的扩展。
</details></li>
</ul>
<hr>
<h2 id="HDGL-A-hierarchical-dynamic-graph-representation-learning-model-for-brain-disorder-classification"><a href="#HDGL-A-hierarchical-dynamic-graph-representation-learning-model-for-brain-disorder-classification" class="headerlink" title="HDGL: A hierarchical dynamic graph representation learning model for brain disorder classification"></a>HDGL: A hierarchical dynamic graph representation learning model for brain disorder classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02903">http://arxiv.org/abs/2311.02903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parniyan Jalali, Mehran Safayani</li>
<li>For: 本研究旨在提出一种 Hierarchical Dynamic Graph Representation Learning（HDGL）模型，用于分类大脑疾病样本和健康样本。* Methods:  HDGL模型包括两个层次，第一层构建大脑网络图和学习其空间和时间嵌入，第二层组建人口图并进行分类。此外，为了降低内存复杂性，提出了四种方法。* Results: 研究在 ABIDE 和 ADHD-200 数据集上进行了评估，结果表明 HDGL 模型在多种评价指标上比一些状态顶模型表现更好。<details>
<summary>Abstract</summary>
The human brain can be considered as complex networks, composed of various regions that continuously exchange their information with each other, forming the brain network graph, from which nodes and edges are extracted using resting-state functional magnetic resonance imaging (rs-fMRI). Therefore, this graph can potentially depict abnormal patterns that have emerged under the influence of brain disorders. So far, numerous studies have attempted to find embeddings for brain network graphs and subsequently classify samples with brain disorders from healthy ones, which include limitations such as: not considering the relationship between samples, not utilizing phenotype information, lack of temporal analysis, using static functional connectivity (FC) instead of dynamic ones and using a fixed graph structure. We propose a hierarchical dynamic graph representation learning (HDGL) model, which is the first model designed to address all the aforementioned challenges. HDGL consists of two levels, where at the first level, it constructs brain network graphs and learns their spatial and temporal embeddings, and at the second level, it forms population graphs and performs classification after embedding learning. Furthermore, based on how these two levels are trained, four methods have been introduced, some of which are suggested for reducing memory complexity. We evaluated the performance of the proposed model on the ABIDE and ADHD-200 datasets, and the results indicate the improvement of this model compared to several state-of-the-art models in terms of various evaluation metrics.
</details>
<details>
<summary>摘要</summary>
人类大脑可以视为复杂的网络，由多个区域组成，这些区域不断地交换信息，形成大脑网络图，从而可以承载着脑病的异常模式。因此，这个图可能能够描述出脑病的异常模式。迄今为止，许多研究已经尝试使用大脑网络图的嵌入和分类样本，但是这些研究存在一些限制，如：不考虑样本之间的关系、不使用类型信息、缺乏时间分析、使用静态功能连接（FC）而不是动态连接、使用固定图结构。我们提出了层次动态图表学习（HDGL）模型，这是第一个解决以上所有挑战的模型。HDGL包括两层，在第一层中，它构建大脑网络图并学习其空间和时间嵌入，在第二层中，它形成人口图并进行分类 після嵌入学习。此外，根据这两层的训练方式，我们引入了四种方法来降低内存复杂性。我们对ABIDE和ADHD-200 dataset进行了评估，结果表明，提案的模型在多种评估指标上比多个状态的艺术模型表现出色。
</details></li>
</ul>
<hr>
<h2 id="Transduce-and-Speak-Neural-Transducer-for-Text-to-Speech-with-Semantic-Token-Prediction"><a href="#Transduce-and-Speak-Neural-Transducer-for-Text-to-Speech-with-Semantic-Token-Prediction" class="headerlink" title="Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction"></a>Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02898">http://arxiv.org/abs/2311.02898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Dongjune Lee, Nam Soo Kim</li>
<li>for: 提出了一个基于神经抽象器的文本读取调变（TTS）框架，以便实现文本读取调变的简洁整合。</li>
<li>methods: 使用wav2vec2.0嵌入表示，从而获得精确的语意标识，然后透过神经抽象器生成对应的语意标识，最后使用非autoregressive（NAR）算法生成语音讯号。</li>
<li>results: 实验结果显示，提案的模型在零基础适应TTS中 exceeds 基于的benchmarks 的语音质量和话者相似度，并且 investigate了单位处理时间和语速控制的可能性。<details>
<summary>Abstract</summary>
We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.
</details>
<details>
<summary>摘要</summary>
我们提出了基于神经转化器的文本到语音（TTS）框架。我们使用从wav2vec2.0嵌入中获得的分词，这使得我们可以轻松地采用神经转化器来实现TTS框架，并且受到它的幂等约束。我们的提案的模型首先使用神经转化器生成了对齐的 semantic tokens，然后使用非自然语言生成器（NAR）来synthesize语音样本。这种解体的框架可以减轻TTS训练复杂度，让每个阶段都可以专注于1) 语言和对齐模型化和2) 细腻的声音模型化。我们的实验结果表明，我们的提案模型在零shot适应TTS中超越了基线值，在语音质量和发音相似性方面通过对象和主观度量表现出色。我们还调查了我们的提案模型的推理速度和幂制控性，表明神经转化器在TTS框架中的潜在力量。
</details></li>
</ul>
<hr>
<h2 id="AdaFlood-Adaptive-Flood-Regularization"><a href="#AdaFlood-Adaptive-Flood-Regularization" class="headerlink" title="AdaFlood: Adaptive Flood Regularization"></a>AdaFlood: Adaptive Flood Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02891">http://arxiv.org/abs/2311.02891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wonho Bae, Yi Ren, Mohamad Osama Ahmed, Frederick Tung, Danica J. Sutherland, Gabriel L. Oliveira</li>
<li>for: 提高模型的测验时间一致性（test time generalization）</li>
<li>methods: 使用 AdaFlood 方法，将训练数据中每个示例的净训练损失阈值（flood level）灵活地调整为每个示例的难度（difficulty）</li>
<li>results: 在文本、图像、异步事件序列和表格等多种输入模式下，实验结果显示 AdaFlood 可以在数据领域和噪音水平上展现弹性。<details>
<summary>Abstract</summary>
Although neural networks are conventionally optimized towards zero training loss, it has been recently learned that targeting a non-zero training loss threshold, referred to as a flood level, often enables better test time generalization. Current approaches, however, apply the same constant flood level to all training samples, which inherently assumes all the samples have the same difficulty. We present AdaFlood, a novel flood regularization method that adapts the flood level of each training sample according to the difficulty of the sample. Intuitively, since training samples are not equal in difficulty, the target training loss should be conditioned on the instance. Experiments on datasets covering four diverse input modalities - text, images, asynchronous event sequences, and tabular - demonstrate the versatility of AdaFlood across data domains and noise levels.
</details>
<details>
<summary>摘要</summary>
尽管神经网络通常被优化为零训练损失，但最近的研究发现，目标一个非零训练损失水平，称为洪水水平，经常能提高测试时的泛化性。然而，现有的方法通常对所有训练样本应用相同的定点洪水水平，这种假设所有样本具有相同的难度。我们介绍了 AdaFlood，一种新的洪水规范方法，可以根据训练样本的难度来适应洪水水平。这种 intuition 是，由于训练样本不是平等的难度，因此训练损失应该基于实例。经验表明， AdaFlood 可以在不同的输入模式 - 文本、图像、异步事件序列和表格 - 中进行多样化应用，并在噪音水平上进行适应。
</details></li>
</ul>
<hr>
<h2 id="MultiSPANS-A-Multi-range-Spatial-Temporal-Transformer-Network-for-Traffic-Forecast-via-Structural-Entropy-Optimization"><a href="#MultiSPANS-A-Multi-range-Spatial-Temporal-Transformer-Network-for-Traffic-Forecast-via-Structural-Entropy-Optimization" class="headerlink" title="MultiSPANS: A Multi-range Spatial-Temporal Transformer Network for Traffic Forecast via Structural Entropy Optimization"></a>MultiSPANS: A Multi-range Spatial-Temporal Transformer Network for Traffic Forecast via Structural Entropy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02880">http://arxiv.org/abs/2311.02880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/selgroup/multispans">https://github.com/selgroup/multispans</a></li>
<li>paper_authors: Dongcheng Zou, Senzhang Wang, Xuefeng Li, Hao Peng, Yuandong Wang, Chunyang Liu, Kehua Sheng, Bo Zhang</li>
<li>for: 本文提出了一种基于多 span 的嵌入式 transformer 模型，用于解决现有方法在模型复杂多范围关系和路网层次知识方面的缺陷。</li>
<li>methods: 本文使用了多filter convolution 模块生成有用的 ST-token 嵌入，以便计算注意力计算。然后，通过 ST-token 和空间时间位编码，使用 transformers 捕捉长范围的时间和空间关系。此外，本文引入了结构 entropy 理论来优化空间注意力机制。</li>
<li>results: 对多个实际交通数据集进行了广泛的实验，并证明了提出的框架在多种状况下具有较好的性能，并能够有效利用更长的历史窗口。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/SELGroup/MultiSPANS">https://github.com/SELGroup/MultiSPANS</a> 上下载。<details>
<summary>Abstract</summary>
Traffic forecasting is a complex multivariate time-series regression task of paramount importance for traffic management and planning. However, existing approaches often struggle to model complex multi-range dependencies using local spatiotemporal features and road network hierarchical knowledge. To address this, we propose MultiSPANS. First, considering that an individual recording point cannot reflect critical spatiotemporal local patterns, we design multi-filter convolution modules for generating informative ST-token embeddings to facilitate attention computation. Then, based on ST-token and spatial-temporal position encoding, we employ the Transformers to capture long-range temporal and spatial dependencies. Furthermore, we introduce structural entropy theory to optimize the spatial attention mechanism. Specifically, The structural entropy minimization algorithm is used to generate optimal road network hierarchies, i.e., encoding trees. Based on this, we propose a relative structural entropy-based position encoding and a multi-head attention masking scheme based on multi-layer encoding trees. Extensive experiments demonstrate the superiority of the presented framework over several state-of-the-art methods in real-world traffic datasets, and the longer historical windows are effectively utilized. The code is available at https://github.com/SELGroup/MultiSPANS.
</details>
<details>
<summary>摘要</summary>
宽泛预测是一个复杂的多变量时间序列回归任务，对交通管理和规划都具有极高的重要性。然而，现有的方法 oftentimes 难以模型复杂的多范围依赖关系使用本地空间时间特征和路网层次知识。为Addressing this challenge, we propose MultiSPANS.First, we recognize that an individual recording point cannot fully capture critical spatiotemporal local patterns, so we design multi-filter convolution modules to generate informative ST-token embeddings for facilitating attention computation. Then, based on ST-token and spatial-temporal position encoding, we employ Transformers to capture long-range temporal and spatial dependencies. Furthermore, we introduce structural entropy theory to optimize the spatial attention mechanism. Specifically, we use the structural entropy minimization algorithm to generate optimal road network hierarchies, i.e., encoding trees. Based on this, we propose a relative structural entropy-based position encoding and a multi-head attention masking scheme based on multi-layer encoding trees.Extensive experiments demonstrate the superiority of the presented framework over several state-of-the-art methods in real-world traffic datasets, and the longer historical windows are effectively utilized. The code is available at https://github.com/SELGroup/MultiSPANS.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Active-Learning-in-Meta-Learning-Enhancing-Context-Set-Labeling"><a href="#Exploring-Active-Learning-in-Meta-Learning-Enhancing-Context-Set-Labeling" class="headerlink" title="Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling"></a>Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02879">http://arxiv.org/abs/2311.02879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wonho Bae, Jing Wang, Danica J. Sutherland</li>
<li>for: 本文针对Meta-learning中的active learning问题做出了研究，特别是在Context Set中选择哪些点进行标注。</li>
<li>methods: 本文提出了一种基于 Gaussian mixture 的自然算法，用于选择需要标注的点。这种算法的选择基于meta-learning过程中active learning的各个部分。</li>
<li>results:  Comparing with现有的active learning方法，本文的提出的算法在各种 benchmark 数据集上表现出了更好的性能。<details>
<summary>Abstract</summary>
Most meta-learning methods assume that the (very small) context set used to establish a new task at test time is passively provided. In some settings, however, it is feasible to actively select which points to label; the potential gain from a careful choice is substantial, but the setting requires major differences from typical active learning setups. We clarify the ways in which active meta-learning can be used to label a context set, depending on which parts of the meta-learning process use active learning. Within this framework, we propose a natural algorithm based on fitting Gaussian mixtures for selecting which points to label; though simple, the algorithm also has theoretical motivation. The proposed algorithm outperforms state-of-the-art active learning methods when used with various meta-learning algorithms across several benchmark datasets.
</details>
<details>
<summary>摘要</summary>
大多数元学习方法假设测试时使用的上下文集（非常小）是被提供的，而不是主动选择。然而，在某些情况下，可以通过选择哪些点标注来获得更大的提升，但这需要与常见的活动学习设置作出重大差异。我们在这种框架下解释了如何使用活动元学习来标注上下文集，具体来说是根据哪些部分使用活动学习。我们提议一种自然的 Gaussian 混合函数来选择需要标注的点，尽管简单，但也有理论上的推动。我们的提议算法在考试数据集上与现有的活动学习方法进行比较，表现出色。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-Bounds-for-Estimating-Probability-Divergences-under-Invariances"><a href="#Sample-Complexity-Bounds-for-Estimating-Probability-Divergences-under-Invariances" class="headerlink" title="Sample Complexity Bounds for Estimating Probability Divergences under Invariances"></a>Sample Complexity Bounds for Estimating Probability Divergences under Invariances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02868">http://arxiv.org/abs/2311.02868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behrooz Tahmasebi, Stefanie Jegelka</li>
<li>for: 这篇论文旨在研究如何使用流chartsgroup的自然对称性提高样本复杂度估计 Wasserstein距离、Sobolev集成概率度量（Sobolev IPMs）、最大均方差（MMD）以及density估计问题的复杂度。</li>
<li>methods: 该论文使用了流chartsgroup的自然对称性来提高样本复杂度估计的效率。</li>
<li>results: 该论文的结果表明，使用流chartsgroup的自然对称性可以提高样本复杂度估计的效率， Specifically, the paper shows that there is a two-fold gain: (1) reducing the sample complexity by a multiplicative factor corresponding to the group size (for finite groups) or the normalized volume of the quotient space (for groups of positive dimension); (2) improving the exponent in the convergence rate (for groups of positive dimension). These results are completely new for groups of positive dimension and extend recent bounds for finite group actions.<details>
<summary>Abstract</summary>
Group-invariant probability distributions appear in many data-generative models in machine learning, such as graphs, point clouds, and images. In practice, one often needs to estimate divergences between such distributions. In this work, we study how the inherent invariances, with respect to any smooth action of a Lie group on a manifold, improve sample complexity when estimating the Wasserstein distance, the Sobolev Integral Probability Metrics (Sobolev IPMs), the Maximum Mean Discrepancy (MMD), and also the complexity of the density estimation problem (in the $L^2$ and $L^\infty$ distance). Our results indicate a two-fold gain: (1) reducing the sample complexity by a multiplicative factor corresponding to the group size (for finite groups) or the normalized volume of the quotient space (for groups of positive dimension); (2) improving the exponent in the convergence rate (for groups of positive dimension). These results are completely new for groups of positive dimension and extend recent bounds for finite group actions.
</details>
<details>
<summary>摘要</summary>
群体共轭的概率分布出现在机器学习中的数据生成模型中，如图、点云和图像。在实践中，我们经常需要估算这些分布之间的差异。在这项工作中，我们研究了在任意平滑 Lie 群动作下的拓扑 manifold 上的自然协变性如何提高样本复杂性when estimating Wasserstein distance, Sobolev Integral Probability Metrics (Sobolev IPMs), Maximum Mean Discrepancy (MMD), 以及density estimation问题的复杂性（在 $L^2$ 和 $L^\infty$ 距离中）。我们的结果表明有两种新的优点：1. 通过 GROUP 的大小（对于有限群）或投影空间的归一化体积来减少样本复杂性的因子；2. 在有限维度的群动作中，提高了对数减少率的 exponent。这些结果对于有限维度的群动作是完全新的，并推广了最近的 finite 群动作 bounds。
</details></li>
</ul>
<hr>
<h2 id="Barron-Space-for-Graph-Convolution-Neural-Networks"><a href="#Barron-Space-for-Graph-Convolution-Neural-Networks" class="headerlink" title="Barron Space for Graph Convolution Neural Networks"></a>Barron Space for Graph Convolution Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02838">http://arxiv.org/abs/2311.02838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seok-Young Chung, Qiyu Sun</li>
<li>for: 这个论文是为了研究图像领域中的图 convolutional neural network (GCNN) 的性能和可学习性而写的。</li>
<li>methods: 这篇论文提出了一个 Barron 空间的函数在紧密的域上的研究，并证明了该空间是一个 reproduce kernel Banach space，可以分解为一系列的 reproduce kernel Hilbert space with neuron kernels，并且可以densely embed在连续函数空间中。</li>
<li>results: 论文显示了 GCNN 的输出可以包含在 Barron 空间中，而 Barron 空间中的函数可以由一些 GCNN 的输出在积分平坦和固定量的测量中进行有效地学习。此外，论文还估计了 Barron 空间中函数的 Rademacher 复杂性，并证明了 functions 在 Barron 空间中可以由Random sampling efficiently learning。<details>
<summary>Abstract</summary>
Graph convolutional neural network (GCNN) operates on graph domain and it has achieved a superior performance to accomplish a wide range of tasks. In this paper, we introduce a Barron space of functions on a compact domain of graph signals. We prove that the proposed Barron space is a reproducing kernel Banach space, it can be decomposed into the union of a family of reproducing kernel Hilbert spaces with neuron kernels, and it could be dense in the space of continuous functions on the domain. Approximation property is one of the main principles to design neural networks. In this paper, we show that outputs of GCNNs are contained in the Barron space and functions in the Barron space can be well approximated by outputs of some GCNNs in the integrated square and uniform measurements. We also estimate the Rademacher complexity of functions with bounded Barron norm and conclude that functions in the Barron space could be learnt from their random samples efficiently.
</details>
<details>
<summary>摘要</summary>
“图 convolutional neural network（GCNN）在图域上运行，并实现了广泛的任务。在这篇论文中，我们介绍了一个巴隆空间函数在封闭域上的图信号。我们证明了提议的巴隆空间是一个复制kernel Banach空间，可以分解为一家族 reproduce kernel Hilbert space with neuron kernels，并且可以在域上 dense 的函数空间中。对于 neural network 的设计，精度是一个关键原则。在这篇论文中，我们表明了 GCNN 的输出在巴隆空间中，而巴隆空间中的函数可以由一些 GCNN 的集成平方和均匀测量良好地 aproximate。我们还估算了 bounded 巴隆 нор的函数的拉德玛勒复杂度，并结论了可以由它们的随机样本进行高效地学习。”Note: The translation is in Simplified Chinese, which is one of the two standardized forms of Chinese. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Prioritized-Propagation-in-Graph-Neural-Networks"><a href="#Prioritized-Propagation-in-Graph-Neural-Networks" class="headerlink" title="Prioritized Propagation in Graph Neural Networks"></a>Prioritized Propagation in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02832">http://arxiv.org/abs/2311.02832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Cheng, Minjie Chen, Xiang Li, Caihua Shan, Ming Gao</li>
<li>for: 本文旨在提出一种可与现有的图 neural network (GNN) 模型结合使用的框架，以学习图中节点的个性化消息传递步骤。</li>
<li>methods: 该框架包含三个组件：基础 GNN 模型、传递控制器和重量控制器。 transmitController 用于确定节点的优化传递步骤，weightController 用于计算节点的优先级分数。</li>
<li>results: 通过对 8 个 benchmark 数据集进行广泛的实验比较，我们发现我们的框架可以在传递策略和节点表示方面达到更高的性能。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have recently received significant attention. Learning node-wise message propagation in GNNs aims to set personalized propagation steps for different nodes in the graph. Despite the success, existing methods ignore node priority that can be reflected by node influence and heterophily. In this paper, we propose a versatile framework PPro, which can be integrated with most existing GNN models and aim to learn prioritized node-wise message propagation in GNNs. Specifically, the framework consists of three components: a backbone GNN model, a propagation controller to determine the optimal propagation steps for nodes, and a weight controller to compute the priority scores for nodes. We design a mutually enhanced mechanism to compute node priority, optimal propagation step and label prediction. We also propose an alternative optimization strategy to learn the parameters in the backbone GNN model and two parametric controllers. We conduct extensive experiments to compare our framework with other 11 state-of-the-art competitors on 8 benchmark datasets. Experimental results show that our framework can lead to superior performance in terms of propagation strategies and node representations.
</details>
<details>
<summary>摘要</summary>
graf neuronetWORKS (GNNs) 在最近几年received significant attention。learning node-wise message propagation in GNNs aims to set personalized propagation steps for different nodes in the graph。Despite the success，existing methods ignore node priority that can be reflected by node influence and heterophily。In this paper，we propose a versatile framework PPro，which can be integrated with most existing GNN models and aim to learn prioritized node-wise message propagation in GNNs。Specifically，the framework consists of three components：a backbone GNN model，a propagation controller to determine the optimal propagation steps for nodes，and a weight controller to compute the priority scores for nodes。We design a mutually enhanced mechanism to compute node priority，optimal propagation step and label prediction。We also propose an alternative optimization strategy to learn the parameters in the backbone GNN model and two parametric controllers。We conduct extensive experiments to compare our framework with other 11 state-of-the-art competitors on 8 benchmark datasets。Experimental results show that our framework can lead to superior performance in terms of propagation strategies and node representations。
</details></li>
</ul>
<hr>
<h2 id="On-Subagging-Boosted-Probit-Model-Trees"><a href="#On-Subagging-Boosted-Probit-Model-Trees" class="headerlink" title="On Subagging Boosted Probit Model Trees"></a>On Subagging Boosted Probit Model Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02827">http://arxiv.org/abs/2311.02827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Qin, Wei-Min Huang</li>
<li>For: 这个研究旨在提出一个新的混合式袋包-提升算法，以解决分类问题。* Methods: 这个算法使用了新的搜寻树模型（Probit Model Tree，PMT）作为提升部分的基本分类器，并在袋包部分使用了增强后的PMTS。* Results: 我们的理论分析显示了这个算法的一致性和缩小普通误差的可能性，并且显示了增加袋包次数可以减少这个算法的一般化误差。实验结果显示了这个算法具有优秀的预测力，并且在一些情况下表现更好。<details>
<summary>Abstract</summary>
With the insight of variance-bias decomposition, we design a new hybrid bagging-boosting algorithm named SBPMT for classification problems. For the boosting part of SBPMT, we propose a new tree model called Probit Model Tree (PMT) as base classifiers in AdaBoost procedure. For the bagging part, instead of subsampling from the dataset at each step of boosting, we perform boosted PMTs on each subagged dataset and combine them into a powerful "committee", which can be viewed an incomplete U-statistic. Our theoretical analysis shows that (1) SBPMT is consistent under certain assumptions, (2) Increase the subagging times can reduce the generalization error of SBPMT to some extent and (3) Large number of ProbitBoost iterations in PMT can benefit the performance of SBPMT with fewer steps in the AdaBoost part. Those three properties are verified by a famous simulation designed by Mease and Wyner (2008). The last two points also provide a useful guidance in model tuning. A comparison of performance with other state-of-the-art classification methods illustrates that the proposed SBPMT algorithm has competitive prediction power in general and performs significantly better in some cases.
</details>
<details>
<summary>摘要</summary>
针对分类问题，我们基于差异-偏见分解的视角设计了一种新的杂合袋包-提升算法，称为SBPMT。在提升部分，我们提出了一种新的树模型，称为概率模型树（PMT），作为AdaBoost过程中的基准分类器。在袋包部分，而不是从数据集中随机抽样，我们在每次提升过程中运行了提升PMТ，并将其组合成一个强大的“委员会”，可以视为不完全的U统计。我们的理论分析表明，SBPMT在满足 certain assumptions 的情况下是一个一致的算法，并且可以降低提升PMТ的总体化风险。此外，我们还发现，增加袋包次数可以减少SBPMT的总体化风险，并且大量的概率提升迭代可以提高SBPMT的性能，但是需要 fewer steps 在AdaBoost部分。这些发现都得到了Mease和Wyner（2008）的著名的模拟 verify。最后，我们还对SBPMT的性能进行了比较，并发现它在总的来说具有竞争力，并在一些情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Signal-Processing-Meets-SGD-From-Momentum-to-Filter"><a href="#Signal-Processing-Meets-SGD-From-Momentum-to-Filter" class="headerlink" title="Signal Processing Meets SGD: From Momentum to Filter"></a>Signal Processing Meets SGD: From Momentum to Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02818">http://arxiv.org/abs/2311.02818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhipeng Yao, Guisong Chang, Jiaqi Zhang, Qi Zhang, Yu Zhang, Dazhou Li</li>
<li>for: 本研究旨在探讨降低历史梯度方差对当前梯度估计的影响，以便优化器更快地趋向于平滑解。</li>
<li>methods: 本文提出了一种基于减少历史梯度方差的新优化方法，使用维ener filter理论来增强SGD的首 moments estimator，并在深度学习模型训练中采用适应性的权重调整。</li>
<li>results: 实验结果表明，提出的SGDF优化器可以与现有的优化器相比，在深度学习模型训练中达到满意的性能。<details>
<summary>Abstract</summary>
In the field of deep learning, Stochastic Gradient Descent (SGD) and its momentum-based variants are the predominant choices for optimization algorithms. Despite all that, these momentum strategies, which accumulate historical gradients by using a fixed $\beta$ hyperparameter to smooth the optimization processing, often neglect the potential impact of the variance of historical gradients on the current gradient estimation. In the gradient variance during training, fluctuation indicates the objective function does not meet the Lipschitz continuity condition at all time, which raises the troublesome optimization problem. This paper aims to explore the potential benefits of reducing the variance of historical gradients to make optimizer converge to flat solutions. Moreover, we proposed a new optimization method based on reducing the variance. We employed the Wiener filter theory to enhance the first moment estimation of SGD, notably introducing an adaptive weight to optimizer. Specifically, the adaptive weight dynamically changes along with temporal fluctuation of gradient variance during deep learning model training. Experimental results demonstrated our proposed adaptive weight optimizer, SGDF (Stochastic Gradient Descent With Filter), can achieve satisfactory performance compared with state-of-the-art optimizers.
</details>
<details>
<summary>摘要</summary>
在深度学习领域，随机梯度下降（SGD）和其带有动量的变体是主要的优化算法。尽管如此，这些动量策略通常忽视了历史梯度的方差对当前梯度估计的影响。在训练过程中，梯度方差的变化指示目标函数不满足 lipschitz连续性条件，这会导致优化问题。本文旨在探索减少历史梯度方差的可能性，以使优化器 converge to 平滑解。此外，我们还提出了一种基于减少方差的新的优化方法。我们利用了wiener filter理论来增强SGD中的首个oment estimation，特别是引入了适应量到优化器。具体来说，适应量在时间上随着梯度方差的 temporal fluctuation而改变。实验结果表明我们提出的适应量优化器（SGDF）可以与当前的优化器相比，实现满意的性能。
</details></li>
</ul>
<hr>
<h2 id="APGL4SR-A-Generic-Framework-with-Adaptive-and-Personalized-Global-Collaborative-Information-in-Sequential-Recommendation"><a href="#APGL4SR-A-Generic-Framework-with-Adaptive-and-Personalized-Global-Collaborative-Information-in-Sequential-Recommendation" class="headerlink" title="APGL4SR: A Generic Framework with Adaptive and Personalized Global Collaborative Information in Sequential Recommendation"></a>APGL4SR: A Generic Framework with Adaptive and Personalized Global Collaborative Information in Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02816">http://arxiv.org/abs/2311.02816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-team/apgl4sr">https://github.com/graph-team/apgl4sr</a></li>
<li>paper_authors: Mingjia Yin, Hao Wang, Xiang Xu, Likang Wu, Sirui Zhao, Wei Guo, Yong Liu, Ruiming Tang, Defu Lian, Enhong Chen</li>
<li>for: 提高Sequential Recommendation系统的效果， capture dynamic preferences和global collaborative information</li>
<li>methods: 使用自适应和个性化的全局合作信息，自适应全球图 constructions， relative positional encoding， multi-task learning paradigm</li>
<li>results: 可以outperform其他基eline with significant margins， improve recommendation performance<details>
<summary>Abstract</summary>
The sequential recommendation system has been widely studied for its promising effectiveness in capturing dynamic preferences buried in users' sequential behaviors. Despite the considerable achievements, existing methods usually focus on intra-sequence modeling while overlooking exploiting global collaborative information by inter-sequence modeling, resulting in inferior recommendation performance. Therefore, previous works attempt to tackle this problem with a global collaborative item graph constructed by pre-defined rules. However, these methods neglect two crucial properties when capturing global collaborative information, i.e., adaptiveness and personalization, yielding sub-optimal user representations. To this end, we propose a graph-driven framework, named Adaptive and Personalized Graph Learning for Sequential Recommendation (APGL4SR), that incorporates adaptive and personalized global collaborative information into sequential recommendation systems. Specifically, we first learn an adaptive global graph among all items and capture global collaborative information with it in a self-supervised fashion, whose computational burden can be further alleviated by the proposed SVD-based accelerator. Furthermore, based on the graph, we propose to extract and utilize personalized item correlations in the form of relative positional encoding, which is a highly compatible manner of personalizing the utilization of global collaborative information. Finally, the entire framework is optimized in a multi-task learning paradigm, thus each part of APGL4SR can be mutually reinforced. As a generic framework, APGL4SR can outperform other baselines with significant margins. The code is available at https://github.com/Graph-Team/APGL4SR.
</details>
<details>
<summary>摘要</summary>
“对称推荐系统已经广泛研究，因为它能够充分捕捉用户的动态喜好。然而，现有的方法通常专注于内部序列模型，忽略了将全局协力信息应用于推荐，从而导致推荐性能不佳。因此，先前的工作尝试使用全局协力项目Graph来解决这个问题，但这些方法忽略了两个重要的属性，即适应和个性化。为了解决这个问题，我们提出了一个图驱动的框架，名为适应和个性化图驱动推荐系统（APGL4SR）。 Specifically, we first learn an adaptive global graph among all items and capture global collaborative information with it in a self-supervised fashion, whose computational burden can be further alleviated by the proposed SVD-based accelerator. Furthermore, based on the graph, we propose to extract and utilize personalized item correlations in the form of relative positional encoding, which is a highly compatible manner of personalizing the utilization of global collaborative information. Finally, the entire framework is optimized in a multi-task learning paradigm, thus each part of APGL4SR can be mutually reinforced. As a generic framework, APGL4SR can outperform other baselines with significant margins. 可以在https://github.com/Graph-Team/APGL4SR上下载代码。”
</details></li>
</ul>
<hr>
<h2 id="On-the-Intersection-of-Self-Correction-and-Trust-in-Language-Models"><a href="#On-the-Intersection-of-Self-Correction-and-Trust-in-Language-Models" class="headerlink" title="On the Intersection of Self-Correction and Trust in Language Models"></a>On the Intersection of Self-Correction and Trust in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02801">http://arxiv.org/abs/2311.02801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyapriya Krishna</li>
<li>for: 这个论文旨在检验自我更正机制是否能够提高语言模型的可靠性。</li>
<li>methods: 研究人员使用了自我更正机制来提高语言模型的可靠性，并对两个关键方面进行了实验：谏性和毒性。</li>
<li>results: 研究发现，自我更正可以提高语言模型的谏性和真实性，但这些改进的程度因任务的具体需求和自我更正的方式而异。此外，研究还发现了一些“自我犹豫”现象在自我更正过程中，这引入了一些新的挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex cognitive tasks. However, their complexity and lack of transparency have raised several trustworthiness concerns, including the propagation of misinformation and toxicity. Recent research has explored the self-correction capabilities of LLMs to enhance their performance. In this work, we investigate whether these self-correction capabilities can be harnessed to improve the trustworthiness of LLMs. We conduct experiments focusing on two key aspects of trustworthiness: truthfulness and toxicity. Our findings reveal that self-correction can lead to improvements in toxicity and truthfulness, but the extent of these improvements varies depending on the specific aspect of trustworthiness and the nature of the task. Interestingly, our study also uncovers instances of "self-doubt" in LLMs during the self-correction process, introducing a new set of challenges that need to be addressed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.LG_2023_11_06/" data-id="cloq1wl8z00st7o887swc6fyc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/06/cs.CL_2023_11_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-11-06
        
      </div>
    </a>
  
  
    <a href="/2023/11/06/eess.IV_2023_11_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-11-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">117</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">68</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">50</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
