
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03354 repo_url: None paper_authors: Junyan Li, Delin Chen, Y">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-06">
<meta property="og:url" content="https://nullscc.github.io/2023/11/06/cs.CV_2023_11_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03354 repo_url: None paper_authors: Junyan Li, Delin Chen, Y">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-06T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-07T04:52:03.171Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.CV_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T13:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CoVLM-Composing-Visual-Entities-and-Relationships-in-Large-Language-Models-Via-Communicative-Decoding"><a href="#CoVLM-Composing-Visual-Entities-and-Relationships-in-Large-Language-Models-Via-Communicative-Decoding" class="headerlink" title="CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding"></a>CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03354">http://arxiv.org/abs/2311.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyan Li, Delin Chen, Yining Hong, Zhenfang Chen, Peihao Chen, Yikang Shen, Chuang Gan</li>
<li>for: 提高大型视言语基础模型（VLM）的 Compositional Reasoning 能力，使其能够更好地捕捉视觉和语言之间的关系。</li>
<li>methods: 提出了一种新的 Communication Token 机制，通过动态地与视觉检测系统和语言系统进行交互，使 LLM 可以更好地组合视觉元素和关系。</li>
<li>results: 对 compositional reasoning benchmark 进行评测，CoVLM 表现出色，较前一代 VLM 提高了大约 20% 的 mAP 分数，较前一代 VLM 提高了大约 14% 的 top-1 准确率，并在 ARO 上提高了大约 3% 的 top-1 准确率。同时，在传统的视觉语言任务中，如引用表达理解和视觉问答任务中，也达到了状态略进行表现。<details>
<summary>Abstract</summary>
A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make "infinite use of finite means". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their "bag-of-words" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose CoVLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.
</details>
<details>
<summary>摘要</summary>
人类有一种杰出的能力是 compositional reasoning，即将 "无限用 finite means" 的能力。然而，当前的大规模视语言基础模型 (VLM) 因其 "bag-of-words" 行为和无法正确表示视觉元素和关系而做出缺陷。为此，我们提议 CoVLM，可以引导 LLM 进行视语言交互性解oding。specifically，我们首先设计了一组新的交流符，用于在视觉检测系统和语言系统之间的动态交流。一个交流符是由 LLM 根据视觉元素或关系生成的，以告知检测网络提交相关的区域。提交的区域是feedback到 LLM，以便更好地根据相关区域生成语言。 LLM 因此可以通过交流符compose视觉元素和关系。视觉到语言和语言到视觉的交流是 iterative 进行，直到整个句子被生成。我们的框架可以准确地跨越视觉感知和 LLMs，并在 compositional reasoning benchmark 上大幅提高 VLM 的性能（例如，~ 20% 在 HICO-DET mAP 中，~ 14% 在 Cola 顶部精度中，和 ~ 3% 在 ARO 顶部精度中）。我们还达到了传统视觉语言任务的州际性表现，如 Referring Expression Comprehension 和 Visual Question Answering。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Evaluation-Metrics-of-Open-Vocabulary-Segmentaion"><a href="#Rethinking-Evaluation-Metrics-of-Open-Vocabulary-Segmentaion" class="headerlink" title="Rethinking Evaluation Metrics of Open-Vocabulary Segmentaion"></a>Rethinking Evaluation Metrics of Open-Vocabulary Segmentaion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03352">http://arxiv.org/abs/2311.03352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhou, Tiancheng Shen, Xu Yang, Hai Huang, Xiangtai Li, Lu Qi, Ming-Hsuan Yang</li>
<li>for: 本研究探讨了开放词汇分割评价指标的问题，即评价过程仍然强调关闭集成指标在零shot或跨数据集管道上，而不考虑预测和真实分类类别之间的相似性。</li>
<li>methods: 我们首先对 eleven 种 между两个概念的相似度测量使用WordNet语言统计、文本嵌入和语言模型进行了全面的量化分析和用户研究。基于这些探索的测量，我们设计了 noval 评价指标，包括 Open mIoU、Open AP 和 Open PQ，适用于三种开放词汇分割任务。</li>
<li>results: 我们在 12 种开放词汇分割方法上 benchmarked 我们的评价指标，即使相对Subjective的相似距离，我们 still 能够通过我们的指标来评价现有的开放词汇分割方法的开放能力。我们希望通过我们的工作，能够带领社区新的思想，以评价开放能力的方法。评价代码已经发布到github。<details>
<summary>Abstract</summary>
In this paper, we highlight a problem of evaluation metrics adopted in the open-vocabulary segmentation. That is, the evaluation process still heavily relies on closed-set metrics on zero-shot or cross-dataset pipelines without considering the similarity between predicted and ground truth categories. To tackle this issue, we first survey eleven similarity measurements between two categorical words using WordNet linguistics statistics, text embedding, and language models by comprehensive quantitative analysis and user study. Built upon those explored measurements, we designed novel evaluation metrics, namely Open mIoU, Open AP, and Open PQ, tailored for three open-vocabulary segmentation tasks. We benchmarked the proposed evaluation metrics on 12 open-vocabulary methods of three segmentation tasks. Even though the relative subjectivity of similarity distance, we demonstrate that our metrics can still well evaluate the open ability of the existing open-vocabulary segmentation methods. We hope that our work can bring with the community new thinking about how to evaluate the open ability of models. The evaluation code is released in github.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Long-Term-Invariant-Local-Features-via-Implicit-Cross-Domain-Correspondences"><a href="#Long-Term-Invariant-Local-Features-via-Implicit-Cross-Domain-Correspondences" class="headerlink" title="Long-Term Invariant Local Features via Implicit Cross-Domain Correspondences"></a>Long-Term Invariant Local Features via Implicit Cross-Domain Correspondences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03345">http://arxiv.org/abs/2311.03345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zador Pataki, Mohammad Altillawi, Menelaos Kanakis, Rémi Pautrat, Fengyi Shen, Ziyuan Liu, Luc Van Gool, Marc Pollefeys</li>
<li>for: 本文研究了现代学习基于的视觉特征提取网络在跨域地理位置ocalization中的性能问题。</li>
<li>methods: 我们提出了一种新的数据驱动方法，即偏振交叉域对应（iCDC），以提高现代特征提取网络的跨域ocalization性能。iCDC使用不同视觉域下的场景表示，通过下推到场景的3D表示来生成准确的对应关系。</li>
<li>results: 我们的提议方法在评估于流行的长期地理位置 benchmark 上表现出色，与现有方法相比，具有显著的性能优势。这些结果表明，我们的方法可以为长期部署的视觉地理位置ocalization领域提供更加稳定和可靠的性能。<details>
<summary>Abstract</summary>
Modern learning-based visual feature extraction networks perform well in intra-domain localization, however, their performance significantly declines when image pairs are captured across long-term visual domain variations, such as different seasonal and daytime variations. In this paper, our first contribution is a benchmark to investigate the performance impact of long-term variations on visual localization. We conduct a thorough analysis of the performance of current state-of-the-art feature extraction networks under various domain changes and find a significant performance gap between intra- and cross-domain localization. We investigate different methods to close this gap by improving the supervision of modern feature extractor networks. We propose a novel data-centric method, Implicit Cross-Domain Correspondences (iCDC). iCDC represents the same environment with multiple Neural Radiance Fields, each fitting the scene under individual visual domains. It utilizes the underlying 3D representations to generate accurate correspondences across different long-term visual conditions. Our proposed method enhances cross-domain localization performance, significantly reducing the performance gap. When evaluated on popular long-term localization benchmarks, our trained networks consistently outperform existing methods. This work serves as a substantial stride toward more robust visual localization pipelines for long-term deployments, and opens up research avenues in the development of long-term invariant descriptors.
</details>
<details>
<summary>摘要</summary>
现代学习基于的视觉特征提取网络在同一个频谱域中的本地化表现良好，但是当图像对被捕捉到不同季节和日间变化时，其表现显著下降。在这篇论文中，我们的首要贡献是设立了考察长期变化对视觉本地化表现的 bencmark。我们进行了对当前状态艺术特征提取网络在不同频谱变化下的完整分析，并发现了 intra- 和 cross-频谱本地化之间的表现差距非常大。我们 investigate了不同的方法来减少这个差距，包括改进现代特征提取器网络的超vision。我们提出了一种新的数据中心方法，即隐式跨频谱对应（iCDC）。iCDC通过对各个视觉频谱下的场景使用多个神经辐射场，来生成高精度的对应关系。我们的提出方法可以大幅提高跨频谱本地化表现，并减少表现差距。当我们的训练网络被评估于popular long-term本地化benchmark上，它们一直表现出色，胜过现有方法。这种工作是Visual本地化管道中更Robust的解决方案的一大步进，同时开启了长期不变特征表示器的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Cross-Image-Attention-for-Zero-Shot-Appearance-Transfer"><a href="#Cross-Image-Attention-for-Zero-Shot-Appearance-Transfer" class="headerlink" title="Cross-Image Attention for Zero-Shot Appearance Transfer"></a>Cross-Image Attention for Zero-Shot Appearance Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03335">http://arxiv.org/abs/2311.03335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/garibida/cross-image-attention">https://github.com/garibida/cross-image-attention</a></li>
<li>paper_authors: Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, Daniel Cohen-Or</li>
<li>for: 将图像之间的Semantic知识传递到图像中，以实现图像的Visual表现转换。</li>
<li>methods: 基于文本生成模型的自我注意层，引入cross-image注意机制，在denoising过程中利用已经建立的semantic对应关系，将结构图像的查询与外观图像的键值相结合，以生成结构和外观相符的图像。</li>
<li>results: 在各种物体类别和不同的形状、大小和视点变化下，实现了图像的高质量生成，无需优化或训练。<details>
<summary>Abstract</summary>
Recent advancements in text-to-image generative models have demonstrated a remarkable ability to capture a deep semantic understanding of images. In this work, we leverage this semantic knowledge to transfer the visual appearance between objects that share similar semantics but may differ significantly in shape. To achieve this, we build upon the self-attention layers of these generative models and introduce a cross-image attention mechanism that implicitly establishes semantic correspondences across images. Specifically, given a pair of images -- one depicting the target structure and the other specifying the desired appearance -- our cross-image attention combines the queries corresponding to the structure image with the keys and values of the appearance image. This operation, when applied during the denoising process, leverages the established semantic correspondences to generate an image combining the desired structure and appearance. In addition, to improve the output image quality, we harness three mechanisms that either manipulate the noisy latent codes or the model's internal representations throughout the denoising process. Importantly, our approach is zero-shot, requiring no optimization or training. Experiments show that our method is effective across a wide range of object categories and is robust to variations in shape, size, and viewpoint between the two input images.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Robust-Bi-Directional-Algorithm-For-People-Count-In-Crowded-Areas"><a href="#A-Robust-Bi-Directional-Algorithm-For-People-Count-In-Crowded-Areas" class="headerlink" title="A Robust Bi-Directional Algorithm For People Count In Crowded Areas"></a>A Robust Bi-Directional Algorithm For People Count In Crowded Areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03323">http://arxiv.org/abs/2311.03323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyanarayana Penke, Gopikrishna Pavuluri, Soukhya Kunda, Satvik M, CharanKumar Y<br>for: 这个论文的目的是计算人群流动的方法。methods: 这篇论文使用了blob分析方法来计算人群流动。results: 论文得出的结果是可以实时计算人群流动的方法，可以用于统计人群流动的方向和数量。<details>
<summary>Abstract</summary>
People counting system in crowded places has become a very useful practical application that can be accomplished in various ways which include many traditional methods using sensors. Examining the case of real time scenarios, the algorithm espoused should be steadfast and accurate. People counting algorithm presented in this paper, is centered on blob assessment, devoted to yield the count of the people through a path along with the direction of traversal. The system depicted is often ensconced at the entrance of a building so that the unmitigated frequency of visitors can be recorded. The core premise of this work is to extricate count of people inflow and outflow pertaining to a particular area. The tot-up achieved can be exploited for purpose of statistics in the circumstances of any calamity occurrence in that zone. Relying upon the count totaled, the population in that vicinity can be assimilated in order to take on relevant measures to rescue the people.
</details>
<details>
<summary>摘要</summary>
人数计数系统在拥挤的地方已成为非常有用的实际应用，可以通过多种传统方法使用感测器进行实现。在实时场景中，算法应该是不动摇的和准确的。本文所提出的人数计算算法基于块评估，可以通过路径和旋转方向来计算人数。这种系统通常安装在建筑物入口处，以记录入场人数的不断变化。本文的核心思想是提取入口和出口人数的计数，以便在紧急情况下对该区域的人口进行统计，并根据统计结果采取相应的救援措施。
</details></li>
</ul>
<hr>
<h2 id="FATE-Feature-Agnostic-Transformer-based-Encoder-for-learning-generalized-embedding-spaces-in-flow-cytometry-data"><a href="#FATE-Feature-Agnostic-Transformer-based-Encoder-for-learning-generalized-embedding-spaces-in-flow-cytometry-data" class="headerlink" title="FATE: Feature-Agnostic Transformer-based Encoder for learning generalized embedding spaces in flow cytometry data"></a>FATE: Feature-Agnostic Transformer-based Encoder for learning generalized embedding spaces in flow cytometry data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03314">http://arxiv.org/abs/2311.03314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lisa Weijler, Florian Kowarsch, Michael Reiter, Pedro Hermosilla, Margarita Maurer-Granofszky, Michael Dworzak</li>
<li>for: 这个研究目的是提出一个能够处理不同特征空间数据的新型架构，以便在实验室数据中检测白细胞抗原表达的自动化方法。</li>
<li>methods: 这个方法使用了一个名为set-transformer的新型架构，并与特征编解层合作，以学习内部特征空间的共同对应关系。</li>
<li>results: 这个方法在检测淋巴细胞抗原表达中表现出色，能够处理不同特征空间的数据，并且不需要对特征空间进行调整或扩展。<details>
<summary>Abstract</summary>
While model architectures and training strategies have become more generic and flexible with respect to different data modalities over the past years, a persistent limitation lies in the assumption of fixed quantities and arrangements of input features. This limitation becomes particularly relevant in scenarios where the attributes captured during data acquisition vary across different samples. In this work, we aim at effectively leveraging data with varying features, without the need to constrain the input space to the intersection of potential feature sets or to expand it to their union. We propose a novel architecture that can directly process data without the necessity of aligned feature modalities by learning a general embedding space that captures the relationship between features across data samples with varying sets of features. This is achieved via a set-transformer architecture augmented by feature-encoder layers, thereby enabling the learning of a shared latent feature space from data originating from heterogeneous feature spaces. The advantages of the model are demonstrated for automatic cancer cell detection in acute myeloid leukemia in flow cytometry data, where the features measured during acquisition often vary between samples. Our proposed architecture's capacity to operate seamlessly across incongruent feature spaces is particularly relevant in this context, where data scarcity arises from the low prevalence of the disease. The code is available for research purposes at https://github.com/lisaweijler/FATE.
</details>
<details>
<summary>摘要</summary>
而模型的建立和训练策略在过去几年来变得更加一般化和灵活，能够处理不同数据类型的数据。然而，一个 persistent limitation 是假设输入特征的数量和排列是固定的。这个限制在样本中 captured 的特征不同时 particualrly relevant。在这个工作中，我们想要有效地利用不同特征的数据，不需要尺寸输入空间到可能的特征集的交叉点或者到其union。我们提出了一种新的架构，可以直接处理数据，不需要对特征模式进行aligned。这是通过在数据样本之间学习一个通用的嵌入空间，以捕捉特征之间的关系。这个嵌入空间是通过set-transformer架构和特征编码层来学习的。这种架构可以在不同特征空间中学习共享的凉特征空间。我们的提出的架构在 automatic cancer cell detection 中得到了应用， particularly relevant 在 flow cytometry 数据中，因为在这里的特征通常在不同的样本中不同。我们的提出的架构可以在不同的特征空间中操作无缝，这是 particualrly relevant 在这个疾病的低发病率下。代码可以在https://github.com/lisaweijler/FATE 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Single-2D-Pose-with-Context-is-Worth-Hundreds-for-3D-Human-Pose-Estimation"><a href="#A-Single-2D-Pose-with-Context-is-Worth-Hundreds-for-3D-Human-Pose-Estimation" class="headerlink" title="A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation"></a>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03312">http://arxiv.org/abs/2311.03312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/QitaoZhao/ContextAware-PoseFormer">https://github.com/QitaoZhao/ContextAware-PoseFormer</a></li>
<li>paper_authors: Qitao Zhao, Ce Zheng, Mengyuan Liu, Chen Chen</li>
<li>for: 提高3D人姿估计精度，不需要使用大量视频帧</li>
<li>methods: 利用 pré-train 的2D pose检测器生成的中间视觉表示，无需训练</li>
<li>results: 与Context-Aware PoseFormer比较，无需使用任何时间信息，在速度和精度两个方面显著提高表现<details>
<summary>Abstract</summary>
The dominant paradigm in 3D human pose estimation that lifts a 2D pose sequence to 3D heavily relies on long-term temporal clues (i.e., using a daunting number of video frames) for improved accuracy, which incurs performance saturation, intractable computation and the non-causal problem. This can be attributed to their inherent inability to perceive spatial context as plain 2D joint coordinates carry no visual cues. To address this issue, we propose a straightforward yet powerful solution: leveraging the readily available intermediate visual representations produced by off-the-shelf (pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed. The key observation is that, while the pose detector learns to localize 2D joints, such representations (e.g., feature maps) implicitly encode the joint-centric spatial context thanks to the regional operations in backbone networks. We design a simple baseline named Context-Aware PoseFormer to showcase its effectiveness. Without access to any temporal information, the proposed method significantly outperforms its context-agnostic counterpart, PoseFormer, and other state-of-the-art methods using up to hundreds of video frames regarding both speed and precision. Project page: https://qitaozhao.github.io/ContextAware-PoseFormer
</details>
<details>
<summary>摘要</summary>
主流方法在3D人姿估计中将2D姿势序列提升到3D，对准的使用了严重的长期时间讯息（例如使用大量的影像帧），从而导致性能暴露、计算复杂和非 causa 问题。这可以被归因于它们的内在无法感受到空间上下文的缺陷，因为普通的2D JOINT坐标并不含任何视觉提示。为解决这个问题，我们提出了一个简单 yet 具有强大的解决方案：利用可以通过Off-the-shelf（预训）的2D姿势探测器生成的Ready-to-use中间显示表示，不需要任何调整。关键观察是，对于2D JOINT的探测器而言，其生成的表示（例如特征图）会隐式地传递 JOINT-centric的空间上下文，这是由背景网络的区域操作所带来的。我们设计了一个简单的基eline名为Context-Aware PoseFormer，以示其效iveness。在没有任何时间讯息的情况下，我们的提案明显超越了它的context-agnostic counterpart，PoseFormer，以及其他使用到百分之百的影像帧的现有方法， regardind both speed和精度。Project page: <https://qitaozhao.github.io/ContextAware-PoseFormer>
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Based-Tea-Leaf-Disease-Detection-A-Comprehensive-Review"><a href="#Machine-Learning-Based-Tea-Leaf-Disease-Detection-A-Comprehensive-Review" class="headerlink" title="Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive Review"></a>Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03240">http://arxiv.org/abs/2311.03240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faruk Ahmed, Md. Taimur Ahad, Yousuf Rayhan Emon</li>
<li>for: 这个研究旨在探讨机器学习方法在农业生产力提高中的应用，尤其是在茶叶疾病诊断方面。</li>
<li>methods: 这个研究使用了多种机器学习方法，包括视觉 трансформа器模型（ICVT、GreenViT、PlantXViT、PlantViT、MSCVT、TLMViT、IterationViT、IEM-ViT）以及其他模型（ dense convolutional network（DenseNet）、Residual Neural Network（ResNet）-50V2、YOLOv5、YOLOv7、Convolutional Neural Network（CNN）、Deep CNN、Non-dominated Sorting Genetic Algorithm（NSGA-II）、MobileNetv2、Lesion-Aware Visual Transformer）。</li>
<li>results: 这些机器学习模型在不同的数据集上进行了训练和测试，并在实际应用中得到了良好的结果。<details>
<summary>Abstract</summary>
Tea leaf diseases are a major challenge to agricultural productivity, with far-reaching implications for yield and quality in the tea industry. The rise of machine learning has enabled the development of innovative approaches to combat these diseases. Early detection and diagnosis are crucial for effective crop management. For predicting tea leaf disease, several automated systems have already been developed using different image processing techniques. This paper delivers a systematic review of the literature on machine learning methodologies applied to diagnose tea leaf disease via image classification. It thoroughly evaluates the strengths and constraints of various Vision Transformer models, including Inception Convolutional Vision Transformer (ICVT), GreenViT, PlantXViT, PlantViT, MSCVT, Transfer Learning Model & Vision Transformer (TLMViT), IterationViT, IEM-ViT. Moreover, this paper also reviews models like Dense Convolutional Network (DenseNet), Residual Neural Network (ResNet)-50V2, YOLOv5, YOLOv7, Convolutional Neural Network (CNN), Deep CNN, Non-dominated Sorting Genetic Algorithm (NSGA-II), MobileNetv2, and Lesion-Aware Visual Transformer. These machine-learning models have been tested on various datasets, demonstrating their real-world applicability. This review study not only highlights current progress in the field but also provides valuable insights for future research directions in the machine learning-based detection and classification of tea leaf diseases.
</details>
<details>
<summary>摘要</summary>
茶叶疾病是现代农业生产的主要挑战，对茶业生产的产量和质量有着深远的影响。随着机器学习的发展，开发了一些创新的方法来解决这些疾病。早期检测和诊断是 tea 生产中的关键环节。为预测茶叶疾病，已经开发了多种自动化系统，使用了不同的图像处理技术。本文提供了机器学习方法在预测茶叶疾病方面的系统性回顾，全面评估了各种视Transformer模型的优势和缺陷，包括Inception Convolutional Vision Transformer (ICVT)、GreenViT、PlantXViT、PlantViT、MSCVT、Transfer Learning Model & Vision Transformer (TLMViT)、IterationViT、IEM-ViT等。此外，本文还回顾了其他模型，如 dense convolutional network (DenseNet)、Residual Neural Network (ResNet)-50V2、YOLOv5、YOLOv7、Convolutional Neural Network (CNN)、Deep CNN、Non-dominated Sorting Genetic Algorithm (NSGA-II)、MobileNetv2、Lesion-Aware Visual Transformer等。这些机器学习模型在不同的数据集上进行测试，示出了它们在实际应用中的可行性。本文不仅概括了当前领域的进展，还为未来在机器学习基于茶叶疾病检测和分类方面的研究提供了价值的指导。
</details></li>
</ul>
<hr>
<h2 id="Navigating-Scaling-Laws-Accelerating-Vision-Transformer’s-Training-via-Adaptive-Strategies"><a href="#Navigating-Scaling-Laws-Accelerating-Vision-Transformer’s-Training-via-Adaptive-Strategies" class="headerlink" title="Navigating Scaling Laws: Accelerating Vision Transformer’s Training via Adaptive Strategies"></a>Navigating Scaling Laws: Accelerating Vision Transformer’s Training via Adaptive Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03233">http://arxiv.org/abs/2311.03233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sotiris Anagnostidis, Gregor Bachmann, Thomas Hofmann</li>
<li>for: 本研究旨在提出一种可以适应不同任务和 dataset 的 compute-optimal 模型，通过适应 scaling laws 来减少计算资源的需求，以提高模型的性能。</li>
<li>methods: 本研究使用了 vision transformers 家族的模型，通过自适应的方式调整模型的 shape 参数，以实现 compute-optimal 性能。</li>
<li>results: 研究发现，通过适应 scaling laws 和自适应的方式调整模型的 shape 参数，可以创建一种更高性能的模型，并且可以在不同的任务和 dataset 上达到更好的性能。<details>
<summary>Abstract</summary>
In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: Investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a "compute-optimal" model, i.e. a model that allocates a given level of compute during training optimally to maximise performance. In this work, we extend the concept of optimality by allowing for an "adaptive" model, i.e. a model that can change its shape during the course of training. By allowing the shape to adapt, we can optimally traverse between the underlying scaling laws, leading to a significant reduction in the required compute to reach a given target performance. We focus on vision tasks and the family of Vision Transformers, where the patch size as well as the width naturally serve as adaptive shape parameters. We demonstrate that, guided by scaling laws, we can design compute-optimal adaptive models that beat their "static" counterparts.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习领域的状态艺术被大型模型所主导。这种模型的概念很简单：投入更多的计算资源（理想情况下）会提高性能，甚至可预测性能会提高。基于这个概念，我们可以定义“计算优化”模型，即在训练期间优化计算资源的分配，以最大化性能。在这种工作中，我们扩展了优化的概念，允许模型在训练过程中改变其形状。通过让形状适应变化，我们可以优化地 traverse between 的下面法则，从而减少达到给定目标性能所需的计算量。我们将注意力集中在视觉任务上，特别是家族中的视觉转换器，其patch size和宽度自然成为适应形参数。我们示示了，通过适应法则引导，我们可以设计计算优化的适应模型，超过其“静态”对手。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-of-Drone-Collision-Hazards-in-Airborne-RADAR-Point-Clouds-Using-PointNet"><a href="#Segmentation-of-Drone-Collision-Hazards-in-Airborne-RADAR-Point-Clouds-Using-PointNet" class="headerlink" title="Segmentation of Drone Collision Hazards in Airborne RADAR Point Clouds Using PointNet"></a>Segmentation of Drone Collision Hazards in Airborne RADAR Point Clouds Using PointNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03221">http://arxiv.org/abs/2311.03221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Arroyo, Paul Kier, Dylan Angus, Santiago Matalonga, Svetlozar Georgiev, Mehdi Goli, Gerard Dooly, James Riordan<br>for: 本研究旨在增强无人航空器（UAV）在共享空域中的安全运行，特别是在 beyond visual line of sight（BVLOS）操作中。methods: 本研究使用激光技术，实现了一个独特的端到端含义分类方法，可同时识别多个撞击威胁。results: 本研究获得了94%的准确率，成功地同时识别了多个撞击威胁，包括机动无人机（DJI M300和DJI Mini）和飞机（Ikarus C42），以及静止返回（地面和基础设施）。<details>
<summary>Abstract</summary>
The integration of unmanned aerial vehicles (UAVs) into shared airspace for beyond visual line of sight (BVLOS) operations presents significant challenges but holds transformative potential for sectors like transportation, construction, energy and defense. A critical prerequisite for this integration is equipping UAVs with enhanced situational awareness to ensure safe operations. Current approaches mainly target single object detection or classification, or simpler sensing outputs that offer limited perceptual understanding and lack the rapid end-to-end processing needed to convert sensor data into safety-critical insights. In contrast, our study leverages radar technology for novel end-to-end semantic segmentation of aerial point clouds to simultaneously identify multiple collision hazards. By adapting and optimizing the PointNet architecture and integrating aerial domain insights, our framework distinguishes five distinct classes: mobile drones (DJI M300 and DJI Mini) and airplanes (Ikarus C42), and static returns (ground and infrastructure) which results in enhanced situational awareness for UAVs. To our knowledge, this is the first approach addressing simultaneous identification of multiple collision threats in an aerial setting, achieving a robust 94% accuracy. This work highlights the potential of radar technology to advance situational awareness in UAVs, facilitating safe and efficient BVLOS operations.
</details>
<details>
<summary>摘要</summary>
“无人航空器（UAV）在共享空域进行超过视程操作（BVLOS）具有重要挑战，但具有传换性潜力，尤其是交通、建筑、能源和国防等领域。一个重要的前提是将UAV设备了更好的情感知识，以确保安全运作。现有的方法主要针对单一物体检测或分类，或更简单的感知输入，它们具有限制的感知理解和缺乏快速端到端处理，无法将感知资料转换为安全数据。相比之下，我们的研究利用激光技术，实现了首个同时识别多个冲击威胁的对 aerial point clouds 的终端性数据分类框架。我们运用了PointNet架构，并将空中领域知识融入，分别识别了多架 DJI M300 和 DJI Mini，以及机场（Ikarus C42）和静止返回（地面和基础设施），从而提高UAV的情感知识。根据我们所知，这是首个同时识别多个冲击威胁的 aerial 设置，精度高达 94%。这项研究显示了激光技术对 UAV 的情感知识提升具有巨大潜力，促进安全和效率的 BVLOS 操作。”
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Transformers-to-Improve-Breast-Cancer-Classification-and-Risk-Assessment-with-Multi-modal-and-Longitudinal-Data"><a href="#Leveraging-Transformers-to-Improve-Breast-Cancer-Classification-and-Risk-Assessment-with-Multi-modal-and-Longitudinal-Data" class="headerlink" title="Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data"></a>Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03217">http://arxiv.org/abs/2311.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqiu Shen, Jungkyu Park, Frank Yeung, Eliana Goldberg, Laura Heacock, Farah Shamout, Krzysztof J. Geras</li>
<li>for: 这份研究的目的是为了使用多Modal的显像技术来识别有现在的乳癌患者以及估算有现在无癌的患者会在未来癌症的风险。</li>
<li>methods: 这篇研究使用了多Modal Transformer（MMT），一种神经网络，将ammography和ultrasound联合利用，以实现多Modal的显像数据的联合分析，并且跟踪当前影像与前一次的影像进行比较，以探测当前的变化。</li>
<li>results: 这篇研究获得了1.3百万个检查数据，使用MMT神经网络，实现了AUROC0.943的存在癌患者检测，超过了单一Modal的基准值。此外，这篇研究还可以对5年的风险预测，AUROC0.826，超过了以往的乳癌预测模型。<details>
<summary>Abstract</summary>
Breast cancer screening, primarily conducted through mammography, is often supplemented with ultrasound for women with dense breast tissue. However, existing deep learning models analyze each modality independently, missing opportunities to integrate information across imaging modalities and time. In this study, we present Multi-modal Transformer (MMT), a neural network that utilizes mammography and ultrasound synergistically, to identify patients who currently have cancer and estimate the risk of future cancer for patients who are currently cancer-free. MMT aggregates multi-modal data through self-attention and tracks temporal tissue changes by comparing current exams to prior imaging. Trained on 1.3 million exams, MMT achieves an AUROC of 0.943 in detecting existing cancers, surpassing strong uni-modal baselines. For 5-year risk prediction, MMT attains an AUROC of 0.826, outperforming prior mammography-based risk models. Our research highlights the value of multi-modal and longitudinal imaging in cancer diagnosis and risk stratification.
</details>
<details>
<summary>摘要</summary>
乳癌检测通常通过胸部X光来进行，但是存在一些问题，如 dense breast tissue 的影响。现有的深度学习模型会分别分析每种成像模式，这会miss opportunities to integrate information across imaging modalities and time。在本研究中，我们介绍 Multi-modal Transformer（MMT），一种神经网络，可以利用胸部X光和ultrasound synergistically，以识别患有乳癌的患者和评估无癌患者是否有未来发癌的风险。MMT通过自注意力和比较当前检测与过去成像来聚合多Modal数据，并且可以跟踪时间变化。我们在 1.3 万个检测数据上训练 MMT，其 AUROC 在检测现有癌病为 0.943，超过了强大的单Modal 基线。而在5年风险预测方面，MMT 的 AUROC 为 0.826，超过了以往基于胸部X光的风险模型。我们的研究表明多Modal和 longitudinal 成像在诊断和风险 stratification 中具有价值。
</details></li>
</ul>
<hr>
<h2 id="PainSeeker-An-Automated-Method-for-Assessing-Pain-in-Rats-Through-Facial-Expressions"><a href="#PainSeeker-An-Automated-Method-for-Assessing-Pain-in-Rats-Through-Facial-Expressions" class="headerlink" title="PainSeeker: An Automated Method for Assessing Pain in Rats Through Facial Expressions"></a>PainSeeker: An Automated Method for Assessing Pain in Rats Through Facial Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03205">http://arxiv.org/abs/2311.03205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Liu, Guang Li, Dingfan Deng, Jinhua Yu, Yuan Zong</li>
<li>for: 本研究的目的是 investigate whether laboratory rats’ pain can be automatically assessed through their facial expressions.</li>
<li>methods: 本研究使用了一个新的深度学习方法called PainSeeker，该方法可以自动从动物脸部表情图像中识别痛苦。</li>
<li>results: 实验结果表明，可以通过脸部表情图像来评估啮齿动物的痛苦，并且提出了一种新的痛苦识别方法PainSeeker，该方法可以帮助解决这个新的问题。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In this letter, we aim to investigate whether laboratory rats' pain can be automatically assessed through their facial expressions. To this end, we began by presenting a publicly available dataset called RatsPain, consisting of 1,138 facial images captured from six rats that underwent an orthodontic treatment operation. Each rat' facial images in RatsPain were carefully selected from videos recorded either before or after the operation and well labeled by eight annotators according to the Rat Grimace Scale (RGS). We then proposed a novel deep learning method called PainSeeker for automatically assessing pain in rats via facial expressions. PainSeeker aims to seek pain-related facial local regions that facilitate learning both pain discriminative and head pose robust features from facial expression images. To evaluate the PainSeeker, we conducted extensive experiments on the RatsPain dataset. The results demonstrate the feasibility of assessing rats' pain from their facial expressions and also verify the effectiveness of the proposed PainSeeker in addressing this emerging but intriguing problem. The RasPain dataset can be freely obtained from https://github.com/xhzongyuan/RatsPain.
</details>
<details>
<summary>摘要</summary>
在这封信中，我们想 investigate  Whether laboratory rats' pain can be automatically assessed through their facial expressions. To this end, we began by presenting a publicly available dataset called RatsPain, consisting of 1,138 facial images captured from six rats that underwent an orthodontic treatment operation. Each rat's facial images in RatsPain were carefully selected from videos recorded either before or after the operation and well labeled by eight annotators according to the Rat Grimace Scale (RGS). We then proposed a novel deep learning method called PainSeeker for automatically assessing pain in rats via facial expressions. PainSeeker aims to seek pain-related facial local regions that facilitate learning both pain discriminative and head pose robust features from facial expression images. To evaluate the PainSeeker, we conducted extensive experiments on the RatsPain dataset. The results demonstrate the feasibility of assessing rats' pain from their facial expressions and also verify the effectiveness of the proposed PainSeeker in addressing this emerging but intriguing problem. The RasPain dataset can be freely obtained from https://github.com/xhzongyuan/RatsPain.
</details></li>
</ul>
<hr>
<h2 id="LCPR-A-Multi-Scale-Attention-Based-LiDAR-Camera-Fusion-Network-for-Place-Recognition"><a href="#LCPR-A-Multi-Scale-Attention-Based-LiDAR-Camera-Fusion-Network-for-Place-Recognition" class="headerlink" title="LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for Place Recognition"></a>LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03198">http://arxiv.org/abs/2311.03198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhouZijie77/LCPR">https://github.com/ZhouZijie77/LCPR</a></li>
<li>paper_authors: Zijie Zhou, Jingyi Xu, Guangming Xiong, Junyi Ma</li>
<li>for: 本研究旨在提高自动驾驶车辆在GPS无效环境中识别已经前期访问的地方。</li>
<li>methods: 本研究使用了一种新的神经网络模型，即LCPR，来实现多模式地点识别。LCPR模型将激光点云和多视图RGB图像进行融合，生成特征性强、旋转不变的环境表示。</li>
<li>results: 实验结果表明，LCPR模型能够有效地利用多视图相机和激光数据来提高地点识别性能，同时保持强大的视点变化Robustness。<details>
<summary>Abstract</summary>
Place recognition is one of the most crucial modules for autonomous vehicles to identify places that were previously visited in GPS-invalid environments. Sensor fusion is considered an effective method to overcome the weaknesses of individual sensors. In recent years, multimodal place recognition fusing information from multiple sensors has gathered increasing attention. However, most existing multimodal place recognition methods only use limited field-of-view camera images, which leads to an imbalance between features from different modalities and limits the effectiveness of sensor fusion. In this paper, we present a novel neural network named LCPR for robust multimodal place recognition, which fuses LiDAR point clouds with multi-view RGB images to generate discriminative and yaw-rotation invariant representations of the environment. A multi-scale attention-based fusion module is proposed to fully exploit the panoramic views from different modalities of the environment and their correlations. We evaluate our method on the nuScenes dataset, and the experimental results show that our method can effectively utilize multi-view camera and LiDAR data to improve the place recognition performance while maintaining strong robustness to viewpoint changes. Our open-source code and pre-trained models are available at https://github.com/ZhouZijie77/LCPR .
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>自动驾驶车辆需要进行地点识别以确定先前访问过的地点，尤其在GPS无效环境下。感知融合是一种有效的方法来解决单个感知器的缺陷。在过去几年，多模态地点识别已经吸引了越来越多的关注。然而，大多数现有的多模态地点识别方法只使用有限的视场镜像，这会导致不同模态之间的特征不匹配，限制感知融合的效iveness。在这篇论文中，我们提出了一种新的神经网络模型名为LCPR，它将LiDAR点云与多视角RGB图像融合以生成特征化和旋转变化不敏感的环境表示。我们提出了一种多级注意力基于的混合模块，以便充分利用不同模态环境的全景视图和其相关性。我们在nuScenes数据集上进行了实验，实验结果表明，我们的方法可以有效地利用多视角镜像和LiDAR数据，提高地点识别性能，同时保持强的旋转变化鲁棒性。我们的开源代码和预训练模型可以在https://github.com/ZhouZijie77/LCPR 上获取。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Learning-using-Data-Augmentation-and-Time-Frequency-Transformation-for-Time-Series-Classification"><a href="#Few-shot-Learning-using-Data-Augmentation-and-Time-Frequency-Transformation-for-Time-Series-Classification" class="headerlink" title="Few-shot Learning using Data Augmentation and Time-Frequency Transformation for Time Series Classification"></a>Few-shot Learning using Data Augmentation and Time-Frequency Transformation for Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03194">http://arxiv.org/abs/2311.03194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Zhendong Pang, Jiangpeng Wang, Teng Li<br>for: 这个研究旨在解决时间序列分类任务中的几少数据问题。methods: 本研究提出了一个新的几少数学习框架，通过资料增强的方法来解决这个问题。这个方法包括时间频域的转换和随机删除来生成伪象图像。此外，我们还开发了一个序列几何agram（SSNN）神经网络模型，这个模型由两个子网络组成：一个使用1D遗留层来提取输入序列中的特征，另一个使用2D遗留层来提取几何agram表示中的特征。results: 在实验中，我们与不同的现有DNN模型进行比较，包括对于amyotrophic lateral sclerosis（ALS）dataset和风 турбі缺陷（WTF）dataset的评估。结果显示，我们的提案方法在ALSdataset上 achieve 93.75% F1 score和93.33% accuracy，在WTFdataset上 achieve 95.48% F1 score和95.59% accuracy。这些结果表明了我们的方法可以对时间序列分类任务中的几少数据问题进行解决。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) that tackle the time series classification (TSC) task have provided a promising framework in signal processing. In real-world applications, as a data-driven model, DNNs are suffered from insufficient data. Few-shot learning has been studied to deal with this limitation. In this paper, we propose a novel few-shot learning framework through data augmentation, which involves transformation through the time-frequency domain and the generation of synthetic images through random erasing. Additionally, we develop a sequence-spectrogram neural network (SSNN). This neural network model composes of two sub-networks: one utilizing 1D residual blocks to extract features from the input sequence while the other one employing 2D residual blocks to extract features from the spectrogram representation. In the experiments, comparison studies of different existing DNN models with/without data augmentation are conducted on an amyotrophic lateral sclerosis (ALS) dataset and a wind turbine fault (WTF) dataset. The experimental results manifest that our proposed method achieves 93.75% F1 score and 93.33% accuracy on the ALS datasets while 95.48% F1 score and 95.59% accuracy on the WTF datasets. Our methodology demonstrates its applicability of addressing the few-shot problems for time series classification.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在时序分类任务中提供了一个有前途的框架，在实际应用中，作为数据驱动模型，DNN受到了不足的数据的限制。几个步学习被研究以解决这个问题。在这篇论文中，我们提出了一种新的几个步学习框架，通过数据扩充，该框架包括时间频率域的变换和随机磁化生成的 sintetic 图像。此外，我们开发了一种序列spectrogram神经网络（SSNN）模型，该模型由两个子网络组成：一个使用1D residual块来提取输入序列中的特征，另一个使用2D residual块来提取spectrogram表示中的特征。在实验中，我们对不同的现有DNN模型进行了带/无数据扩充的比较研究，并在amyotrophic lateral sclerosis（ALS）数据集和风电机综合缺陷（WTF）数据集上进行了实验。实验结果表明，我们的提议方法在ALS数据集上达到了93.75%的F1分数和93.33%的准确率，在WTF数据集上达到了95.48%的F1分数和95.59%的准确率。我们的方法可以应对几个步学习问题。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-Decomposition-Translation-for-Enhanced-Medical-Image-Translation-Using-GANs"><a href="#Frequency-Domain-Decomposition-Translation-for-Enhanced-Medical-Image-Translation-Using-GANs" class="headerlink" title="Frequency Domain Decomposition Translation for Enhanced Medical Image Translation Using GANs"></a>Frequency Domain Decomposition Translation for Enhanced Medical Image Translation Using GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03175">http://arxiv.org/abs/2311.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuhui Wang, Jianwei Zuo, Xuliang Deng, Jiajia Luo</li>
<li>for: 这篇论文主要针对的是医学影像转换，即将一个医学影像转换为另一个医学影像，并且保持原始影像的特征信息。</li>
<li>methods: 这篇论文提出了一种新的方法，即频率域分解转换（FDDT），它可以将原始影像分解为高频率和低频率两部分，并将这两部分转换为新的影像。</li>
<li>results: 这篇论文的实验结果显示，FDDT可以比基准模型（GAN）更好地保持原始影像的特征信息，并且可以降低转换后的 Fréchet 实验距离、结构相似度、峰值信号吸引比和平均平方误差。<details>
<summary>Abstract</summary>
Medical Image-to-image translation is a key task in computer vision and generative artificial intelligence, and it is highly applicable to medical image analysis. GAN-based methods are the mainstream image translation methods, but they often ignore the variation and distribution of images in the frequency domain, or only take simple measures to align high-frequency information, which can lead to distortion and low quality of the generated images. To solve these problems, we propose a novel method called frequency domain decomposition translation (FDDT). This method decomposes the original image into a high-frequency component and a low-frequency component, with the high-frequency component containing the details and identity information, and the low-frequency component containing the style information. Next, the high-frequency and low-frequency components of the transformed image are aligned with the transformed results of the high-frequency and low-frequency components of the original image in the same frequency band in the spatial domain, thus preserving the identity information of the image while destroying as little stylistic information of the image as possible. We conduct extensive experiments on MRI images and natural images with FDDT and several mainstream baseline models, and we use four evaluation metrics to assess the quality of the generated images. Compared with the baseline models, optimally, FDDT can reduce Fr\'echet inception distance by up to 24.4%, structural similarity by up to 4.4%, peak signal-to-noise ratio by up to 5.8%, and mean squared error by up to 31%. Compared with the previous method, optimally, FDDT can reduce Fr\'echet inception distance by up to 23.7%, structural similarity by up to 1.8%, peak signal-to-noise ratio by up to 6.8%, and mean squared error by up to 31.6%.
</details>
<details>
<summary>摘要</summary>
医疗图像翻译是计算机视觉和生成人工智能的关键任务，具有广泛的应用前景。GAN基本方法是主流图像翻译方法，但它们经常忽视图像频谱频率域的变化和分布，或者只是对高频信息进行简单的对齐，这可能导致图像生成的质量低下。为解决这些问题，我们提出了一种新的方法called频谱频域分解翻译（FDDT）。这种方法将原始图像分解成高频组件和低频组件，其中高频组件包含细节和标识信息，而低频组件包含风格信息。接着，高频和低频组件的转换结果与原始图像的高频和低频组件在同一频谱域的空间域进行对齐，以保持图像的标识信息，同时尽量少破坏图像的风格信息。我们在MRI图像和自然图像上进行了广泛的实验，并使用了多种主流基线模型。我们使用了四个评价指标来评估生成图像的质量。相比主流基线模型，FDDT最佳情况下可以降低Fréchet抽象距离24.4%、结构相似度4.4%、峰值信号噪声比5.8%和平均方差31%。相比前一方法，FDDT最佳情况下可以降低Fréchet抽象距离23.7%、结构相似度1.8%、峰值信号噪声比6.8%和平均方差31.6%。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-Masked-Distillation-for-Pre-Training-Small-Foundation-Models"><a href="#Asymmetric-Masked-Distillation-for-Pre-Training-Small-Foundation-Models" class="headerlink" title="Asymmetric Masked Distillation for Pre-Training Small Foundation Models"></a>Asymmetric Masked Distillation for Pre-Training Small Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03149">http://arxiv.org/abs/2311.03149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Zhao, Bingkun Huang, Sen Xing, Gangshan Wu, Yu Qiao, Limin Wang</li>
<li>For: This paper focuses on pre-training relatively small vision transformer models that could be efficiently adapted to downstream tasks, with the goal of reducing computational cost and improving deployment.* Methods: The paper proposes a new asymmetric masked distillation (AMD) framework for pre-training relatively small models with autoencoding, which involves an asymmetric masking strategy and customized multi-layer feature alignment between the teacher and student models.* Results: The paper achieves 84.6% classification accuracy on IN1K using the ViT-B model with AMD, and achieves 73.3% classification accuracy on the Something-in-Something V2 dataset with a 3.7% improvement over the original ViT-B model from VideoMAE. Additionally, the paper shows consistent performance improvement over the standard pre-training when transferring AMD pre-trained models to downstream tasks.Here is the simplified Chinese text for the three key information points:* For: 这篇论文关注于预训练相对小的视觉变换器模型，以便更好地适应下游任务，降低计算成本并提高部署。* Methods: 论文提出了一种新的偏向隐藏填充（AMD）框架，用于预训练相对小的模型，其中师模型可以更好地看到更多的上下文信息，而学生模型仍然使用高度隐藏率来原始隐藏预训练。* Results: 论文在IN1K上使用ViT-B模型达到84.6%的分类精度，在Something-in-Something V2数据集上使用ViT-B模型达到73.3%的分类精度，比原始ViT-B模型从VideoMAE中提高3.7%。此外，论文还将预训练AMD模型传输到下游任务，并实现了一致性的性能提升。<details>
<summary>Abstract</summary>
Self-supervised foundation models have shown great potential in computer vision thanks to the pre-training paradigm of masked autoencoding. Scale is a primary factor influencing the performance of these foundation models. However, these large foundation models often result in high computational cost that might limit their deployment. This paper focuses on pre-training relatively small vision transformer models that could be efficiently adapted to downstream tasks. Specifically, taking inspiration from knowledge distillation in model compression, we propose a new asymmetric masked distillation(AMD) framework for pre-training relatively small models with autoencoding. The core of AMD is to devise an asymmetric masking strategy, where the teacher model is enabled to see more context information with a lower masking ratio, while the student model still with high masking ratio to the original masked pre-training. We design customized multi-layer feature alignment between the teacher encoder and student encoder to regularize the pre-training of student MAE. To demonstrate the effectiveness and versatility of AMD, we apply it to both ImageMAE and VideoMAE for pre-training relatively small ViT models. AMD achieved 84.6% classification accuracy on IN1K using the ViT-B model. And AMD achieves 73.3% classification accuracy using the ViT-B model on the Something-in-Something V2 dataset, a 3.7% improvement over the original ViT-B model from VideoMAE. We also transfer AMD pre-trained models to downstream tasks and obtain consistent performance improvement over the standard pre-training.
</details>
<details>
<summary>摘要</summary>
自我超vised基础模型在计算机视觉领域表现出了很大的潜力，这主要归功于预训练 paradigma的masked autoencoding。 however, these large foundation models often result in high computational cost, which may limit their deployment. This paper focuses on pre-training relatively small vision transformer models that could be efficiently adapted to downstream tasks. Specifically, taking inspiration from knowledge distillation in model compression, we propose a new asymmetric masked distillation(AMD) framework for pre-training relatively small models with autoencoding. The core of AMD is to devise an asymmetric masking strategy, where the teacher model is enabled to see more context information with a lower masking ratio, while the student model still with high masking ratio to the original masked pre-training. We design customized multi-layer feature alignment between the teacher encoder and student encoder to regularize the pre-training of student MAE. To demonstrate the effectiveness and versatility of AMD, we apply it to both ImageMAE and VideoMAE for pre-training relatively small ViT models. AMD achieved 84.6% classification accuracy on IN1K using the ViT-B model. And AMD achieves 73.3% classification accuracy using the ViT-B model on the Something-in-Something V2 dataset, a 3.7% improvement over the original ViT-B model from VideoMAE. We also transfer AMD pre-trained models to downstream tasks and obtain consistent performance improvement over the standard pre-training.
</details></li>
</ul>
<hr>
<h2 id="Animating-NeRFs-from-Texture-Space-A-Framework-for-Pose-Dependent-Rendering-of-Human-Performances"><a href="#Animating-NeRFs-from-Texture-Space-A-Framework-for-Pose-Dependent-Rendering-of-Human-Performances" class="headerlink" title="Animating NeRFs from Texture Space: A Framework for Pose-Dependent Rendering of Human Performances"></a>Animating NeRFs from Texture Space: A Framework for Pose-Dependent Rendering of Human Performances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03140">http://arxiv.org/abs/2311.03140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Knoll, Wieland Morgenstern, Anna Hilsmann, Peter Eisert</li>
<li>for: 这个论文的目的是提出一种基于NeRF的高质量、可控的3D人体模型生成方法，用于多视图RGB视频中的人体动作 synthesis。</li>
<li>methods: 该方法使用NeRF来描述人体的形状和动作，并通过skeletal JOINT参数来控制人体的pose和表现。</li>
<li>results: 实验表明，该方法可以生成高质量的人体动作 renderings，包括novel-view和novel-pose synthesis。<details>
<summary>Abstract</summary>
Creating high-quality controllable 3D human models from multi-view RGB videos poses a significant challenge. Neural radiance fields (NeRFs) have demonstrated remarkable quality in reconstructing and free-viewpoint rendering of static as well as dynamic scenes. The extension to a controllable synthesis of dynamic human performances poses an exciting research question. In this paper, we introduce a novel NeRF-based framework for pose-dependent rendering of human performances. In our approach, the radiance field is warped around an SMPL body mesh, thereby creating a new surface-aligned representation. Our representation can be animated through skeletal joint parameters that are provided to the NeRF in addition to the viewpoint for pose dependent appearances. To achieve this, our representation includes the corresponding 2D UV coordinates on the mesh texture map and the distance between the query point and the mesh. To enable efficient learning despite mapping ambiguities and random visual variations, we introduce a novel remapping process that refines the mapped coordinates. Experiments demonstrate that our approach results in high-quality renderings for novel-view and novel-pose synthesis.
</details>
<details>
<summary>摘要</summary>
In our approach, the radiance field is warped around an SMPL body mesh, creating a new surface-aligned representation. Our representation can be animated through skeletal joint parameters provided to the NeRF, in addition to the viewpoint, for pose-dependent appearances. To achieve this, our representation includes the corresponding 2D UV coordinates on the mesh texture map and the distance between the query point and the mesh.To enable efficient learning despite mapping ambiguities and random visual variations, we introduce a novel remapping process that refines the mapped coordinates. Experiments demonstrate that our approach results in high-quality renderings for novel-view and novel-pose synthesis.
</details></li>
</ul>
<hr>
<h2 id="TAMPAR-Visual-Tampering-Detection-for-Parcel-Logistics-in-Postal-Supply-Chains"><a href="#TAMPAR-Visual-Tampering-Detection-for-Parcel-Logistics-in-Postal-Supply-Chains" class="headerlink" title="TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains"></a>TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03124">http://arxiv.org/abs/2311.03124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Naumann, Felix Hertlein, Laura Dörr, Kai Furmans</li>
<li>for: 该论文主要关注的是最后一英里配送阶段，只有一张RGB图像，与现有数据库中的参考图像进行比较，检测 probable appearance changes 可能指示骚乱。</li>
<li>methods: 该论文提出了一个骚乱检测管道，利用锚点检测来确定包裹的八个角点，然后应用投影变换创建正规化的前后平行视图。</li>
<li>results: 该论文的实验结果表明，锚点检测和骚乱检测分别具有75.76%的准确率和81%的准确率，并进行了骚乱类型、镜头扭曲和视角的敏感性分析。Here’s the full text in Simplified Chinese:</li>
<li>for: 该论文主要关注的是最后一英里配送阶段，只有一张RGB图像，与现有数据库中的参考图像进行比较，检测 probable appearance changes 可能指示骚乱。</li>
<li>methods: 该论文提出了一个骚乱检测管道，利用锚点检测来确定包裹的八个角点，然后应用投影变换创建正规化的前后平行视图。</li>
<li>results: 该论文的实验结果表明，锚点检测和骚乱检测分别具有75.76%的准确率和81%的准确率，并进行了骚乱类型、镜头扭曲和视角的敏感性分析。<details>
<summary>Abstract</summary>
Due to the steadily rising amount of valuable goods in supply chains, tampering detection for parcels is becoming increasingly important. In this work, we focus on the use-case last-mile delivery, where only a single RGB image is taken and compared against a reference from an existing database to detect potential appearance changes that indicate tampering. We propose a tampering detection pipeline that utilizes keypoint detection to identify the eight corner points of a parcel. This permits applying a perspective transformation to create normalized fronto-parallel views for each visible parcel side surface. These viewpoint-invariant parcel side surface representations facilitate the identification of signs of tampering on parcels within the supply chain, since they reduce the problem to parcel side surface matching with pair-wise appearance change detection. Experiments with multiple classical and deep learning-based change detection approaches are performed on our newly collected TAMpering detection dataset for PARcels, called TAMPAR. We evaluate keypoint and change detection separately, as well as in a unified system for tampering detection. Our evaluation shows promising results for keypoint (Keypoint AP 75.76) and tampering detection (81% accuracy, F1-Score 0.83) on real images. Furthermore, a sensitivity analysis for tampering types, lens distortion and viewing angles is presented. Code and dataset are available at https://a-nau.github.io/tampar.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unified-Multi-modal-Unsupervised-Representation-Learning-for-Skeleton-based-Action-Understanding"><a href="#Unified-Multi-modal-Unsupervised-Representation-Learning-for-Skeleton-based-Action-Understanding" class="headerlink" title="Unified Multi-modal Unsupervised Representation Learning for Skeleton-based Action Understanding"></a>Unified Multi-modal Unsupervised Representation Learning for Skeleton-based Action Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03106">http://arxiv.org/abs/2311.03106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HuiGuanLab/UmURL">https://github.com/HuiGuanLab/UmURL</a></li>
<li>paper_authors: Shengkai Sun, Daizong Liu, Jianfeng Dong, Xiaoye Qu, Junyu Gao, Xun Yang, Xun Wang, Meng Wang</li>
<li>for: 提高skeleton-based动作理解的灵活性和可靠性</li>
<li>methods: 提出了一种单流承载多modal自supervised学习框架，通过早期融合策略将多modal输入 feed到同一个流中，并通过内部和外部一致性学习来避免模态偏袋问题</li>
<li>results: 实验表明， compared to单modal方法，该方法可以减少模型复杂度，同时在不同的下游任务场景中实现新的state-of-the-art表现<details>
<summary>Abstract</summary>
Unsupervised pre-training has shown great success in skeleton-based action understanding recently. Existing works typically train separate modality-specific models, then integrate the multi-modal information for action understanding by a late-fusion strategy. Although these approaches have achieved significant performance, they suffer from the complex yet redundant multi-stream model designs, each of which is also limited to the fixed input skeleton modality. To alleviate these issues, in this paper, we propose a Unified Multimodal Unsupervised Representation Learning framework, called UmURL, which exploits an efficient early-fusion strategy to jointly encode the multi-modal features in a single-stream manner. Specifically, instead of designing separate modality-specific optimization processes for uni-modal unsupervised learning, we feed different modality inputs into the same stream with an early-fusion strategy to learn their multi-modal features for reducing model complexity. To ensure that the fused multi-modal features do not exhibit modality bias, i.e., being dominated by a certain modality input, we further propose both intra- and inter-modal consistency learning to guarantee that the multi-modal features contain the complete semantics of each modal via feature decomposition and distinct alignment. In this manner, our framework is able to learn the unified representations of uni-modal or multi-modal skeleton input, which is flexible to different kinds of modality input for robust action understanding in practical cases. Extensive experiments conducted on three large-scale datasets, i.e., NTU-60, NTU-120, and PKU-MMD II, demonstrate that UmURL is highly efficient, possessing the approximate complexity with the uni-modal methods, while achieving new state-of-the-art performance across various downstream task scenarios in skeleton-based action representation learning.
</details>
<details>
<summary>摘要</summary>
近年来，无监督预训练在skeleton基于动作理解中取得了 significiant success。现有的方法通常是在不同的感知模式上分别训练单独的模型，然后通过晚期的融合策略将多modal信息融合为动作理解。虽然这些方法已经实现了显著的性能提升，但它们受到复杂且重复的多流程模型设计的限制，每个模型都受到固定输入skeleton模式的限制。为了解决这些问题，在这篇论文中，我们提出了一种统一多modal无监督学习框架，叫做UmURL，它利用了高效的早期融合策略来同时处理多modal特征。具体来说，而不是为每个感知模式单独进行单 modal无监督学习，我们将不同的感知输入Feed into同一个流程中，通过早期融合策略来学习它们的多modal特征，以降低模型复杂性。为确保多modal特征不受某种感知输入的偏见，我们还提出了内部和外部协调学习，以确保每个感知输入的多modal特征都包含完整的 semantics。这种方法可以学习单 modal或多modal skeleton输入的统一表示，这是对实际应用中不同类型的感知输入的灵活应用。我们在NTU-60、NTU-120和PKU-MMD II等三个大规模数据集上进行了广泛的实验，结果显示，UmURL在性能和复杂度两个方面具有优于单 modal方法，同时在不同的下游任务场景中实现了新的国际纪录。
</details></li>
</ul>
<hr>
<h2 id="A-survey-and-classification-of-face-alignment-methods-based-on-face-models"><a href="#A-survey-and-classification-of-face-alignment-methods-based-on-face-models" class="headerlink" title="A survey and classification of face alignment methods based on face models"></a>A survey and classification of face alignment methods based on face models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03082">http://arxiv.org/abs/2311.03082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jagmohan Meher, Hector Allende-Cid, Torbjörn E. M. Nordling</li>
<li>for: 本研究旨在提供一个面对面的涵义分析，旨在帮助Beginner、实践者和研究人员更好地理解不同的面模型，以及如何在不同情况下使用这些面模型进行面对适应。</li>
<li>methods: 本研究使用了多种不同的面模型，包括基于3D的面模型和基于深度学习的方法，并进行了对这些面模型的解释和训练。</li>
<li>results: 研究发现，在极端面 pose 情况下，3D-based面模型被更多地使用，而深度学习-based方法通常使用热图进行面对适应。  Additionally, the paper discusses the possible future directions of face models in the field of face alignment.<details>
<summary>Abstract</summary>
A face model is a mathematical representation of the distinct features of a human face. Traditionally, face models were built using a set of fiducial points or landmarks, each point ideally located on a facial feature, i.e., corner of the eye, tip of the nose, etc. Face alignment is the process of fitting the landmarks in a face model to the respective ground truth positions in an input image containing a face. Despite significant research on face alignment in the past decades, no review analyses various face models used in the literature. Catering to three types of readers - beginners, practitioners and researchers in face alignment, we provide a comprehensive analysis of different face models used for face alignment. We include the interpretation and training of the face models along with the examples of fitting the face model to a new face image. We found that 3D-based face models are preferred in cases of extreme face pose, whereas deep learning-based methods often use heatmaps. Moreover, we discuss the possible future directions of face models in the field of face alignment.
</details>
<details>
<summary>摘要</summary>
一个人脸模型是一种数学表达人脸特征的模型。过去，人脸模型通常是使用一组定点或标记点建立的，每个点位于人脸特征处，例如眼角、鼻尖等。人脸对alignment是指将这些标记点与输入图像中的真实位置进行对应。虽然过去数十年来有很多关于人脸对alignment的研究，但是没有任何文献对不同的人脸模型进行了全面的分析。为了适应不同类型的读者（ BEGINNER、实践者和研究人员），我们提供了对不同人脸模型的全面分析，包括这些模型的解释和训练，以及将人脸模型适应新的 face image 的示例。我们发现在极端面 pose 的情况下，3D-based人脸模型被更常用，而深度学习基于方法通常使用热图。此外，我们还讨论了人脸模型在面对alignment领域的可能的未来方向。
</details></li>
</ul>
<hr>
<h2 id="CogVLM-Visual-Expert-for-Pretrained-Language-Models"><a href="#CogVLM-Visual-Expert-for-Pretrained-Language-Models" class="headerlink" title="CogVLM: Visual Expert for Pretrained Language Models"></a>CogVLM: Visual Expert for Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03079">http://arxiv.org/abs/2311.03079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang</li>
<li>for: 本研究旨在开发一个强大的开源视觉语言基础模型（CogVLM），以便深度融合视觉语言特征。</li>
<li>methods: 本研究使用了一种名为“可训练视觉专家模块”的方法，将图像特征与静止预处理语言模型和图像编码器相连接，从而实现深度融合视觉语言特征。</li>
<li>results: 根据10个跨模态测试benchmark，CogVLM-17B实现了状态的最佳性能，并在VQAv2、OKVQA、TextVQA、COCOcaptioning等测试中名列第二，超过或与PaLI-X 55B匹配。<details>
<summary>Abstract</summary>
We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.
</details>
<details>
<summary>摘要</summary>
我们介绍CogVLM，一种强大的开源视觉语言基础模型。与流行的浅层对应方法不同，CogVLM通过在注意力和FFN层中添加可学习的视觉专家模块，将图像特征与冻结预训练语言模型的输入空间连接起来。这使得CogVLM可以深度融合视觉语言特征，而无需牺牲任何NLPTask的性能。CogVLM-17B在10个经典跨模态benchmark测试中获得了状态机器人表现，包括NoCaps、Flicker30k captioning、RefCOCO、RefCOCO+、RefCOCOg、Visual7W、GQA、ScienceQA、VizWiz VQA和TDIUC，并在VQAv2、OKVQA、TextVQA、COCO captioning等测试中排名第二，超过或匹配PaLI-X 55B。代码和检查点可以在https://github.com/THUDM/CogVLM中获取。
</details></li>
</ul>
<hr>
<h2 id="A-Two-Stage-Generative-Model-with-CycleGAN-and-Joint-Diffusion-for-MRI-based-Brain-Tumor-Detection"><a href="#A-Two-Stage-Generative-Model-with-CycleGAN-and-Joint-Diffusion-for-MRI-based-Brain-Tumor-Detection" class="headerlink" title="A Two-Stage Generative Model with CycleGAN and Joint Diffusion for MRI-based Brain Tumor Detection"></a>A Two-Stage Generative Model with CycleGAN and Joint Diffusion for MRI-based Brain Tumor Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03074">http://arxiv.org/abs/2311.03074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxin Wang, Zhuo-Xu Cui, Guanxun Cheng, Chentao Cao, Xi Xu, Ziwei Liu, Haifeng Wang, Yulong Qi, Dong Liang, Yanjie Zhu</li>
<li>for: 这个论文的目的是提高脑癌检测和分类的精度。</li>
<li>methods: 这个方法使用了两个关键技术：CycleGAN和VE-JP。CycleGAN在无标的数据上训练，将健康图像转换为异常图像，作为数据先验。VE-JP则是使用共同分布来重建健康图像，将异常区域修复为健康区域。</li>
<li>results: 这个方法在三个数据集上进行验证，与其他无标的方法进行比较。结果显示，这个方法在脑癌检测和分类中表现出色，DSC分数为0.8590、0.6226和0.7403。<details>
<summary>Abstract</summary>
Accurate detection and segmentation of brain tumors is critical for medical diagnosis. However, current supervised learning methods require extensively annotated images and the state-of-the-art generative models used in unsupervised methods often have limitations in covering the whole data distribution. In this paper, we propose a novel framework Two-Stage Generative Model (TSGM) that combines Cycle Generative Adversarial Network (CycleGAN) and Variance Exploding stochastic differential equation using joint probability (VE-JP) to improve brain tumor detection and segmentation. The CycleGAN is trained on unpaired data to generate abnormal images from healthy images as data prior. Then VE-JP is implemented to reconstruct healthy images using synthetic paired abnormal images as a guide, which alters only pathological regions but not regions of healthy. Notably, our method directly learned the joint probability distribution for conditional generation. The residual between input and reconstructed images suggests the abnormalities and a thresholding method is subsequently applied to obtain segmentation results. Furthermore, the multimodal results are weighted with different weights to improve the segmentation accuracy further. We validated our method on three datasets, and compared with other unsupervised methods for anomaly detection and segmentation. The DSC score of 0.8590 in BraTs2020 dataset, 0.6226 in ITCS dataset and 0.7403 in In-house dataset show that our method achieves better segmentation performance and has better generalization.
</details>
<details>
<summary>摘要</summary>
严格的脑肿检测和分割是医学诊断中的关键。然而，当前的指导学习方法需要大量的标注图像，而状态之前的生成模型通常有覆盖整个数据分布的限制。在这篇论文中，我们提出了一种新的框架 Two-Stage Generative Model (TSGM)，它将 Cycle Generative Adversarial Network (CycleGAN) 和 Variance Exploding stochastic differential equation using joint probability (VE-JP) 结合以提高脑肿检测和分割。CycleGAN 在没有对应的数据上训练，将健康图像转换成病态图像作为数据先验。然后，VE-JP 被实现以使用生成的合理的假样图像作为引导，重构健康图像，只有病理区域受到改变，而不是健康区域。值得注意的是，我们的方法直接学习了联合分布的联合概率分布。输入图像和重构图像之间的差异指示了病理，并且使用了阈值法来获得分割结果。此外，我们还使用了不同的权重来进一步提高分割精度。我们在三个数据集上验证了我们的方法，并与其他无监督方法进行了对比。BraTs2020 数据集的 DSC 分数为 0.8590，ITCS 数据集的 DSC 分数为 0.6226，In-house 数据集的 DSC 分数为 0.7403，这些结果表明我们的方法在分割性能方面表现出色，并且具有更好的普适性。
</details></li>
</ul>
<hr>
<h2 id="OrthoNets-Orthogonal-Channel-Attention-Networks"><a href="#OrthoNets-Orthogonal-Channel-Attention-Networks" class="headerlink" title="OrthoNets: Orthogonal Channel Attention Networks"></a>OrthoNets: Orthogonal Channel Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03071">http://arxiv.org/abs/2311.03071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Salman, Caleb Parks, Matthew Swan, John Gauch</li>
<li>For: The paper aims to design an effective channel attention mechanism that can find optimal feature representation using a lossy-compression method.* Methods: The paper uses Discrete Cosine Transforms (DCTs) to find information-rich compression and randomly initialized orthogonal filters to construct an attention mechanism.* Results: The paper shows superior performance compared to other attention mechanisms on several datasets, including Birds, MS-COCO, and Places356, and competes with the current state-of-the-art on the ImageNet dataset. The results suggest that an optimal choice of filter is elusive, and generalization can be achieved with a sufficiently large number of orthogonal filters.<details>
<summary>Abstract</summary>
Designing an effective channel attention mechanism implores one to find a lossy-compression method allowing for optimal feature representation. Despite recent progress in the area, it remains an open problem. FcaNet, the current state-of-the-art channel attention mechanism, attempted to find such an information-rich compression using Discrete Cosine Transforms (DCTs). One drawback of FcaNet is that there is no natural choice of the DCT frequencies. To circumvent this issue, FcaNet experimented on ImageNet to find optimal frequencies. We hypothesize that the choice of frequency plays only a supporting role and the primary driving force for the effectiveness of their attention filters is the orthogonality of the DCT kernels. To test this hypothesis, we construct an attention mechanism using randomly initialized orthogonal filters. Integrating this mechanism into ResNet, we create OrthoNet. We compare OrthoNet to FcaNet (and other attention mechanisms) on Birds, MS-COCO, and Places356 and show superior performance. On the ImageNet dataset, our method competes with or surpasses the current state-of-the-art. Our results imply that an optimal choice of filter is elusive and generalization can be achieved with a sufficiently large number of orthogonal filters. We further investigate other general principles for implementing channel attention, such as its position in the network and channel groupings.
</details>
<details>
<summary>摘要</summary>
设计有效的通道注意机制需要一种lossy压缩方法，以便最佳化特征表示。尽管最近在这个领域的进步，但这问题仍然未解决。FcaNet，当前领先的通道注意机制，使用Discrete Cosine Transforms（DCTs）来找到信息密集的压缩。FcaNet的一个缺点是没有自然的DCT频率选择。为了解决这个问题，FcaNet在ImageNet上进行了实验。我们假设选择频率只是支持性的角色，主要驱动力是DCT核函数的正交性。为测试这个假设，我们构建了一个使用随机初始化的正交滤波器的注意机制。将这种机制integrated into ResNet，我们创建了OrthoNet。我们与FcaNet（以及其他注意机制）在Birds、MS-COCO和Places356上进行比较，并显示出超越性。在ImageNet dataset上，我们的方法与当前领先的方法竞争。我们的结果表明，选择最佳的滤波器是抽象的，通过一个充分大的正交滤波器数量，可以实现泛化。我们进一步调查了其他实现通道注意的一般原则，如其位置在网络中和通道分组。
</details></li>
</ul>
<hr>
<h2 id="Forest-aboveground-biomass-estimation-using-GEDI-and-earth-observation-data-through-attention-based-deep-learning"><a href="#Forest-aboveground-biomass-estimation-using-GEDI-and-earth-observation-data-through-attention-based-deep-learning" class="headerlink" title="Forest aboveground biomass estimation using GEDI and earth observation data through attention-based deep learning"></a>Forest aboveground biomass estimation using GEDI and earth observation data through attention-based deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03067">http://arxiv.org/abs/2311.03067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenquan Dong, Edward T. A. Mitchard, Hao Yu, Steven Hancock, Casey M. Ryan</li>
<li>for: 这个研究的目的是用深度学习方法来估计遥感数据中的森林上空生物质量（AGB），以便更好地理解气候变化中的碳会计。</li>
<li>methods: 这个研究使用了一种新的注意力深度学习方法来估计森林AGB，主要使用了公开 accessible的遥感数据，包括GEDI LiDAR数据、C-band Sentinel-1 SAR数据、ALOS-2 PALSAR-2数据和Sentinel-2多spectral数据。</li>
<li>results: 这个研究发现，使用注意力深度学习方法可以对森林AGB估计得到明显更高的准确性，比传统的 Random Forest 算法高。特别是，AU 模型在 R2 方面达到了 0.66，RMSE 为 43.66 Mg ha-1，偏差为 0.14 Mg ha-1，而 Random Forest 算法只有 R2 为 0.62，RMSE 为 45.87 Mg ha-1，偏差为 1.09 Mg ha-1。此外，这个研究还发现，使用注意力深度学习方法可以减少遥感数据中的空间信息，从而提高了 AGB 估计的准确性。<details>
<summary>Abstract</summary>
Accurate quantification of forest aboveground biomass (AGB) is critical for understanding carbon accounting in the context of climate change. In this study, we presented a novel attention-based deep learning approach for forest AGB estimation, primarily utilizing openly accessible EO data, including: GEDI LiDAR data, C-band Sentinel-1 SAR data, ALOS-2 PALSAR-2 data, and Sentinel-2 multispectral data. The attention UNet (AU) model achieved markedly higher accuracy for biomass estimation compared to the conventional RF algorithm. Specifically, the AU model attained an R2 of 0.66, RMSE of 43.66 Mg ha-1, and bias of 0.14 Mg ha-1, while RF resulted in lower scores of R2 0.62, RMSE 45.87 Mg ha-1, and bias 1.09 Mg ha-1. However, the superiority of the deep learning approach was not uniformly observed across all tested models. ResNet101 only achieved an R2 of 0.50, an RMSE of 52.93 Mg ha-1, and a bias of 0.99 Mg ha-1, while the UNet reported an R2 of 0.65, an RMSE of 44.28 Mg ha-1, and a substantial bias of 1.84 Mg ha-1. Moreover, to explore the performance of AU in the absence of spatial information, fully connected (FC) layers were employed to eliminate spatial information from the remote sensing data. AU-FC achieved intermediate R2 of 0.64, RMSE of 44.92 Mgha-1, and bias of -0.56 Mg ha-1, outperforming RF but underperforming AU model using spatial information. We also generated 10m forest AGB maps across Guangdong for the year 2019 using AU and compared it with that produced by RF. The AGB distributions from both models showed strong agreement with similar mean values; the mean forest AGB estimated by AU was 102.18 Mg ha-1 while that of RF was 104.84 Mg ha-1. Additionally, it was observed that the AGB map generated by AU provided superior spatial information. Overall, this research substantiates the feasibility of employing deep learning for biomass estimation based on satellite data.
</details>
<details>
<summary>摘要</summary>
“精确量化森林上空生物质量（AGB）是气候变化研究中非常重要的。本研究发表了一种基于深度学习的新型注意力模型（AU），用于森林AGB估计，主要使用开放式的卫星数据，包括：GEDI LiDAR数据、C-band Sentinel-1 SAR数据、ALOS-2 PALSAR-2数据和Sentinel-2多spectral数据。AU模型在AGB估计中表现出色，相比于传统的RF算法，具有更高的准确性。具体来说，AU模型的R2值为0.66，RMSE值为43.66吨 ha-1，偏差值为0.14吨 ha-1，而RF的R2值为0.62，RMSE值为45.87吨 ha-1，偏差值为1.09吨 ha-1。然而，深度学习方法的优势不uniform地遍布所有测试模型。ResNet101模型只有R2值为0.50，RMSE值为52.93吨 ha-1，偏差值为0.99吨 ha-1，而UNet模型则有R2值为0.65，RMSE值为44.28吨 ha-1，并且具有较大的偏差值1.84吨 ha-1。此外，为了探讨AU在没有空间信息的情况下的表现，FC层被用来删除卫星数据中的空间信息。AU-FC模型在R2值为0.64，RMSE值为44.92吨 ha-1，偏差值为-0.56吨 ha-1，与RF模型相比，具有更高的准确性。此外，我们还生成了2019年在广东省的10米森林AGB地图，使用AU模型，并与RF模型生成的地图进行比较。AU模型生成的AGB分布和RF模型生成的AGB分布之间有强相似性，两者的mean值几乎相同。”
</details></li>
</ul>
<hr>
<h2 id="AnyText-Multilingual-Visual-Text-Generation-And-Editing"><a href="#AnyText-Multilingual-Visual-Text-Generation-And-Editing" class="headerlink" title="AnyText: Multilingual Visual Text Generation And Editing"></a>AnyText: Multilingual Visual Text Generation And Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03054">http://arxiv.org/abs/2311.03054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, Xuansong Xie</li>
<li>for: 这个论文主要写于如何使用扩散模型实现高精度的文本图像生成和修改。</li>
<li>methods: 该模型包括一个扩散管道，包括auxiliary latent module和text embedding module。auxiliary latent module使用文本字形、位置和masked image生成latent特征，而text embedding module使用OCR模型将笔画数据编码为嵌入，这些嵌入与图像标签器生成的文本嵌入混合以生成高一致的文本。</li>
<li>results: 经过训练并测试，这种方法在多种语言下能够高精度地生成和修改文本图像，并且在多种评价指标上与其他方法有着显著的差异。此外，我们还提供了一个大规模的多语言文本图像集，名为AnyWord-3M，以及基于这个集的AnyText-benchmark评价指标。<details>
<summary>Abstract</summary>
Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced on https://github.com/tyxsspa/AnyText to improve and promote the development of text generation technology.
</details>
<details>
<summary>摘要</summary>
“传播模型基于文本至图像技术在最近得到了很多成就。 although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText consists of a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs such as text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We used text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced on https://github.com/tyxsspa/AnyText to improve and promote the development of text generation technology.”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="MixUp-MIL-A-Study-on-Linear-Multilinear-Interpolation-Based-Data-Augmentation-for-Whole-Slide-Image-Classification"><a href="#MixUp-MIL-A-Study-on-Linear-Multilinear-Interpolation-Based-Data-Augmentation-for-Whole-Slide-Image-Classification" class="headerlink" title="MixUp-MIL: A Study on Linear &amp; Multilinear Interpolation-Based Data Augmentation for Whole Slide Image Classification"></a>MixUp-MIL: A Study on Linear &amp; Multilinear Interpolation-Based Data Augmentation for Whole Slide Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03052">http://arxiv.org/abs/2311.03052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Gadermayr, Lukas Koller, Maximilian Tschuchnig, Lea Maria Stangassinger, Christina Kreutzer, Sebastien Couillard-Despres, Gertie Janneke Oostingh, Anton Hittmair</li>
<li>for: 这篇论文探讨了在没有像素级标注的情况下，如何类别数位全面图像。</li>
<li>methods: 本论文使用了多个例子学习方法，并调查了线性和多线性 interpolate 技术作为数据增强技术的影响。</li>
<li>results: 实验结果显示了高度的数据分布差异，并发现了一些有趣的特点和探索新的研究方向。<details>
<summary>Abstract</summary>
For classifying digital whole slide images in the absence of pixel level annotation, typically multiple instance learning methods are applied. Due to the generic applicability, such methods are currently of very high interest in the research community, however, the issue of data augmentation in this context is rarely explored. Here we investigate linear and multilinear interpolation between feature vectors, a data augmentation technique, which proved to be capable of improving the generalization performance classification networks and also for multiple instance learning. Experiments, however, have been performed on only two rather small data sets and one specific feature extraction approach so far and a strong dependence on the data set has been identified. Here we conduct a large study incorporating 10 different data set configurations, two different feature extraction approaches (supervised and self-supervised), stain normalization and two multiple instance learning architectures. The results showed an extraordinarily high variability in the effect of the method. We identified several interesting aspects to bring light into the darkness and identified novel promising fields of research.
</details>
<details>
<summary>摘要</summary>
为了在无像素级标注的情况下分类数字整张图像，通常采用多实例学习方法。由于其普适性，这些方法目前在研究 сообществе具有非常高的兴趣度，但数据增强在这个上下文中的问题却rarely explored。我们在这里调查了线性和多线性 interpolate between feature vectors，一种数据增强技术，并证明其能够提高分类网络的泛化性能和多实例学习。然而，我们在只有两个较小数据集和一种特定的特征提取方法上进行了实验，并且发现了数据集强度的依赖关系。在这里，我们进行了大规模的研究，包括10种不同的数据集配置、两种不同的特征提取方法（有监督和无监督）、染料 норmalization和两种多实例学习架构。结果显示了非常高的变化性，我们 indentified several interesting aspects to bring light into the darkness, and identified novel promising fields of research.
</details></li>
</ul>
<hr>
<h2 id="COLA-COarse-LAbel-multi-source-LiDAR-semantic-segmentation-for-autonomous-driving"><a href="#COLA-COarse-LAbel-multi-source-LiDAR-semantic-segmentation-for-autonomous-driving" class="headerlink" title="COLA: COarse-LAbel multi-source LiDAR semantic segmentation for autonomous driving"></a>COLA: COarse-LAbel multi-source LiDAR semantic segmentation for autonomous driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03017">http://arxiv.org/abs/2311.03017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jules Sanchez, Jean-Emmanuel Deschaud, François Goulette</li>
<li>for: 提高 LiDAR semantic segmentation 的Results，特别是在领域扩展、源到源 segmentation 和预训练方面。</li>
<li>methods: 提出了一种新的多源训练方法，利用不同的数据集合在一起进行训练，以提高 LiDAR semantic segmentation 的Results。</li>
<li>results: 在不同的应用方面（领域扩展、源到源 segmentation 和预训练）中，提出了系统性的改进（最高提升 +12%），证明了多源训练的效果。<details>
<summary>Abstract</summary>
LiDAR semantic segmentation for autonomous driving has been a growing field of interest in the past few years. Datasets and methods have appeared and expanded very quickly, but methods have not been updated to exploit this new availability of data and continue to rely on the same classical datasets.   Different ways of performing LIDAR semantic segmentation training and inference can be divided into several subfields, which include the following: domain generalization, the ability to segment data coming from unseen domains ; source-to-source segmentation, the ability to segment data coming from the training domain; and pre-training, the ability to create re-usable geometric primitives.   In this work, we aim to improve results in all of these subfields with the novel approach of multi-source training. Multi-source training relies on the availability of various datasets at training time and uses them together rather than relying on only one dataset.   To overcome the common obstacles found for multi-source training, we introduce the coarse labels and call the newly created multi-source dataset COLA. We propose three applications of this new dataset that display systematic improvement over single-source strategies: COLA-DG for domain generalization (up to +10%), COLA-S2S for source-to-source segmentation (up to +5.3%), and COLA-PT for pre-training (up to +12%).
</details>
<details>
<summary>摘要</summary>
隐藏文本LiDAR语义分割 для自动驾驶在最近几年来得到了越来越多的关注。数据集和方法在不断增加，但方法没有适应新的数据可用性，仍然依赖于传统的数据集。不同的LiDAR语义分割训练和推断方法可以分为以下几个子领域：领域泛化、源到源语义分割和预训练。在这项工作中，我们目的是提高这些子领域的结果，使用新的多源训练方法。多源训练利用训练时可用的多个数据集，而不是仅仅依赖于一个数据集。为了解决多源训练中常见的障碍，我们引入粗略标签，并将其与多源数据集组合而成的新数据集命名为COLA。我们提出了三种应用COLA数据集，显示了对单源策略的系统性提高（最多+10%）：COLA-DG для领域泛化（最多+10%）、COLA-S2S для源到源语义分割（最多+5.3%）和COLA-PT для预训练（最多+12%）。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Capability-of-Text-to-Image-Diffusion-Models-with-Structural-Edge-Guidance-for-Multi-Spectral-Satellite-Image-Inpainting"><a href="#Exploring-the-Capability-of-Text-to-Image-Diffusion-Models-with-Structural-Edge-Guidance-for-Multi-Spectral-Satellite-Image-Inpainting" class="headerlink" title="Exploring the Capability of Text-to-Image Diffusion Models with Structural Edge Guidance for Multi-Spectral Satellite Image Inpainting"></a>Exploring the Capability of Text-to-Image Diffusion Models with Structural Edge Guidance for Multi-Spectral Satellite Image Inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03008">http://arxiv.org/abs/2311.03008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikolaj Czerkawski, Christos Tachtatzis</li>
<li>for: 这个论文探讨了文本到图像填充模型在卫星图像数据上的可用性。</li>
<li>methods: 论文提出了一种基于StableDiffusion和ControlNet的新填充框架，以及一种RGB-to-MSI的翻译方法来解决填充过程中的两个技术挑战。</li>
<li>results: 研究结果表明，通过StableDiffusion进行填充会出现不жела的artefacts，而一种简单的自动内部填充方法可以达到更高质量的填充Synthesis。<details>
<summary>Abstract</summary>
The paper investigates the utility of text-to-image inpainting models for satellite image data. Two technical challenges of injecting structural guiding signals into the generative process as well as translating the inpainted RGB pixels to a wider set of MSI bands are addressed by introducing a novel inpainting framework based on StableDiffusion and ControlNet as well as a novel method for RGB-to-MSI translation. The results on a wider set of data suggest that the inpainting synthesized via StableDiffusion suffers from undesired artefacts and that a simple alternative of self-supervised internal inpainting achieves higher quality of synthesis.
</details>
<details>
<summary>摘要</summary>
文章研究了使用文本到图像恢复模型来处理卫星图像数据的可用性。两个技术挑战，即在生成过程中插入结构导向信号以及将恢复后的RGB像素翻译到更广泛的MSI频谱中，通过引入稳定扩散和控制网络等新框架，以及一种新的RGB-to-MSI翻译方法来解决。对于更广泛的数据集，研究发现使用稳定扩散恢复后会出现不良特征，而自动内部填充方法可以达到更高质量的恢复。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Enhancement-of-Low-Light-Image-Based-on-Retinex-Decomposition"><a href="#Zero-Shot-Enhancement-of-Low-Light-Image-Based-on-Retinex-Decomposition" class="headerlink" title="Zero-Shot Enhancement of Low-Light Image Based on Retinex Decomposition"></a>Zero-Shot Enhancement of Low-Light Image Based on Retinex Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02995">http://arxiv.org/abs/2311.02995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenchao Li, Bangshu Xiong, Qiaofeng Ou, Xiaoyun Long, Jinhao Zhu, Jiabao Chen, Shuyuan Wen</li>
<li>for: 提高低光照图像的修剪和增强</li>
<li>methods: 提出了一种基于学习的Retinex分解方法，即ZERRINNet，通过对原始低光照图像进行降噪、估计雷达组件和照明组件，并通过тексту预测损失和分割平滑损失来约束雷达组件和照明组件，以解决低光照图像中的颜色扭曲、对比度和雷达问题。</li>
<li>results: 在homemade实际低光照数据集和高级视觉任务上进行了有效验证，并与多个公共数据集进行了比较性试验，结果表明我们的方法与当前状态的艺术方法竞争。代码可以在<a target="_blank" rel="noopener" href="https://github.com/liwenchao0615/ZERRINNet%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/liwenchao0615/ZERRINNet中获取。</a><details>
<summary>Abstract</summary>
Two difficulties here make low-light image enhancement a challenging task; firstly, it needs to consider not only luminance restoration but also image contrast, image denoising and color distortion issues simultaneously. Second, the effectiveness of existing low-light enhancement methods depends on paired or unpaired training data with poor generalization performance.   To solve these difficult problems, we propose in this paper a new learning-based Retinex decomposition of zero-shot low-light enhancement method, called ZERRINNet. To this end, we first designed the N-Net network, together with the noise loss term, to be used for denoising the original low-light image by estimating the noise of the low-light image. Moreover, RI-Net is used to estimate the reflection component and illumination component, and in order to solve the color distortion and contrast, we use the texture loss term and segmented smoothing loss to constrain the reflection component and illumination component. Finally, our method is a zero-reference enhancement method that is not affected by the training data of paired and unpaired datasets, so our generalization performance is greatly improved, and in the paper, we have effectively validated it with a homemade real-life low-light dataset and additionally with advanced vision tasks, such as face detection, target recognition, and instance segmentation. We conducted comparative experiments on a large number of public datasets and the results show that the performance of our method is competitive compared to the current state-of-the-art methods. The code is available at:https://github.com/liwenchao0615/ZERRINNet
</details>
<details>
<summary>摘要</summary>
两个难点使低光照图像增强成为一项困难的任务，一是需要同时考虑照度还原、图像对比度、雷达噪声和颜色扭曲问题。二是现有的低光照增强方法的效果受到对照或无照训练数据的影响，导致对新数据的泛化性能不佳。为解决这两个难题，我们在本文提出了一种新的学习基于Retinex decomposición的零参估低光照增强方法，称为ZERRINNet。为此，我们首先设计了N-Net网络，并与噪声损失项一起用于对原始低光照图像进行降噪。其次，RI-Net用于估计反射组件和照明组件，以解决颜色扭曲和对比度问题。 finally，我们的方法是一种零参增强方法，不受照或无照训练数据的影响，因此我们的泛化性能得到了大幅提高。在论文中，我们有效地验证了我们的方法，使用自己制作的实际低光照 dataset 以及高级视觉任务，如人脸检测、目标识别和实例 segmentation。我们对大量公共数据进行了比较 эксперименты，结果显示我们的方法与当前状态的艺术方法竞争力。代码可以在：https://github.com/liwenchao0615/ZERRINNet 获取。
</details></li>
</ul>
<hr>
<h2 id="NEURO-HAND-A-weakly-supervised-Hierarchical-Attention-Network-for-neuroimaging-abnormality-Detection"><a href="#NEURO-HAND-A-weakly-supervised-Hierarchical-Attention-Network-for-neuroimaging-abnormality-Detection" class="headerlink" title="NEURO HAND: A weakly supervised Hierarchical Attention Network for neuroimaging abnormality Detection"></a>NEURO HAND: A weakly supervised Hierarchical Attention Network for neuroimaging abnormality Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02992">http://arxiv.org/abs/2311.02992</a></li>
<li>repo_url: None</li>
<li>paper_authors: David A. Wood</li>
<li>for: 用于临床神经成像数据中的异常检测</li>
<li>methods: 使用幂等注意力网络，适用于非体积数据（即高分辨率MRI扫描堆），可以从二进制检查级别标签进行训练</li>
<li>results: 提高分类精度，并提供了解释性的幂等扫描和序列级别异常localization，或者给出不同扫描和序列的重要性分数，适用于自动化 radiology 部门的检测系统<details>
<summary>Abstract</summary>
Clinical neuroimaging data is naturally hierarchical. Different magnetic resonance imaging (MRI) sequences within a series, different slices covering the head, and different regions within each slice all confer different information. In this work we present a hierarchical attention network for abnormality detection using MRI scans obtained in a clinical hospital setting. The proposed network is suitable for non-volumetric data (i.e. stacks of high-resolution MRI slices), and can be trained from binary examination-level labels. We show that this hierarchical approach leads to improved classification, while providing interpretability through either coarse inter- and intra-slice abnormality localisation, or giving importance scores for different slices and sequences, making our model suitable for use as an automated triaging system in radiology departments.
</details>
<details>
<summary>摘要</summary>
临床神经成像数据自然归于层次结构。不同的核磁共振成像（MRI）序列内一系列、不同的剖面覆盖头部、和每个剖面中的不同区域都提供不同的信息。在这个工作中，我们提出了一种层次注意力网络用于临床MRI扫描中的异常检测。我们的提案的网络适用于非量化数据（即高分辨率MRI剖面栈），并可以从二进制评估级别标签进行训练。我们显示了这种层次方法可以提高分类，同时提供可解释性，通过粗略的跨剖面和内部剖面异常Localization，或者给出不同剖面和序列的重要性分数，使我们的模型适用于软件自动排查系统中。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Radiotherapy-Dose-Prediction-Guided-by-Inter-slice-Aware-Structure-Encoding"><a href="#Diffusion-based-Radiotherapy-Dose-Prediction-Guided-by-Inter-slice-Aware-Structure-Encoding" class="headerlink" title="Diffusion-based Radiotherapy Dose Prediction Guided by Inter-slice Aware Structure Encoding"></a>Diffusion-based Radiotherapy Dose Prediction Guided by Inter-slice Aware Structure Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02991">http://arxiv.org/abs/2311.02991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghao Feng, Lu Wen, Jianghong Xiao, Yuanyuan Xu, Xi Wu, Jiliu Zhou, Xingchen Peng, Yan Wang</li>
<li>for: 这个研究旨在提高 radiotherapy 规划中的剂量分布预测精度，并且解决常用的 L1 或 L2 损失函数导致的过滤问题。</li>
<li>methods: 本研究提出了一种基于扩散模型的剂量分布预测方法（DiffDose），它包括一个前向过程和一个反向过程。在前向过程中，DiffDose 将剂量分布图像转化为纯 Gaussian 噪声图像，并同时训练一个噪声预测器来估算添加的噪声。在反向过程中，它逐步除去噪声，并最终输出预测的剂量分布图像。</li>
<li>results: 研究结果表明，DiffDose 可以减少过滤问题，并提高 radiotherapy 规划中剂量分布预测精度。<details>
<summary>Abstract</summary>
Deep learning (DL) has successfully automated dose distribution prediction in radiotherapy planning, enhancing both efficiency and quality. However, existing methods suffer from the over-smoothing problem for their commonly used L1 or L2 loss with posterior average calculations. To alleviate this limitation, we propose a diffusion model-based method (DiffDose) for predicting the radiotherapy dose distribution of cancer patients. Specifically, the DiffDose model contains a forward process and a reverse process. In the forward process, DiffDose transforms dose distribution maps into pure Gaussian noise by gradually adding small noise and a noise predictor is simultaneously trained to estimate the noise added at each timestep. In the reverse process, it removes the noise from the pure Gaussian noise in multiple steps with the well-trained noise predictor and finally outputs the predicted dose distribution maps...
</details>
<details>
<summary>摘要</summary>
深度学习（DL）已成功地自动预测放疗规划中的剂量分布，提高了效率和质量。然而，现有方法受到L1或L2损失函数的过滤问题的限制。为解决这个局限性，我们提出了基于扩散模型的放疗剂量预测方法（DiffDose）。具体来说，DiffDose模型包括前向过程和反向过程。在前向过程中，DiffDose将剂量分布图转换成纯 Gaussian 噪声，通过逐步添加小噪声并同时训练噪声预测器来估算添加的噪声。在反向过程中，它逐步从纯 Gaussian 噪声中除噪，使用已经训练好的噪声预测器，并最终输出预测的剂量分布图。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Deep-Representation-Learning-via-Layerwise-Feature-Compression-and-Discrimination"><a href="#Understanding-Deep-Representation-Learning-via-Layerwise-Feature-Compression-and-Discrimination" class="headerlink" title="Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination"></a>Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02960">http://arxiv.org/abs/2311.02960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, Qing Qu</li>
<li>for: 本研究旨在探讨深度学习网络在层次特征学习中的具体过程，即如何在深度网络中层次抽象特征。</li>
<li>methods: 本研究使用深度线性网络进行实验研究，并定义了内部特征压缩和between-class特征分化两个指标来量化特征的演化趋势。</li>
<li>results: 研究结果表明，在近正交输入数据和最小 нор减重、平衡和低矩的网络参数下，每层的线性网络都会压缩内部特征 geometric rate，并在数据流经多层后进行 linear rate 的 between-class特征分化。这是深度线性网络层次特征学习的首次量化特征描述。<details>
<summary>Abstract</summary>
Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is nearly orthogonal and the network weights are minimum-norm, balanced, and approximate low-rank: Each layer of the linear network progressively compresses within-class features at a geometric rate and discriminates between-class features at a linear rate with respect to the number of layers that data have passed through. To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep linear networks. Empirically, our extensive experiments not only validate our theoretical results numerically but also reveal a similar pattern in deep nonlinear networks which aligns well with recent empirical studies. Moreover, we demonstrate the practical implications of our results in transfer learning. Our code is available at \url{https://github.com/Heimine/PNC_DLN}.
</details>
<details>
<summary>摘要</summary>
过去一个十年，深度学习已经证明是一种非常有效的工具来学习原始数据中的有意义特征。然而，仍然是一个开放的问题，深度网络在层次上如何进行层次特征学习。在这项工作中，我们尝试揭示这个谜题，通过investigating深度网络中间特征的结构。我们的实际发现表明，深度线性网络中的线性层可以模拟深度网络中非线性层的特征学习功能，因此我们可以通过调查每层训练后的输出（即特征）来研究深度网络如何将输入数据转换成输出。为达到这个目标，我们首先定义了内部特征压缩和 между类差异度的两个度量，然后通过理论分析这两个度量，我们显示了深度网络中的特征演化遵循简单和量化的规律：每层深度网络将内部特征压缩到 геометрический率，并且在数据流经多层后，对于不同类型的特征进行线性差异分化。我们认为这是深度线性网络中特征演化的首次量化 caracterization。我们的实验证明了我们的理论结果，并且发现这种特征演化的趋势也存在于深度非线性网络中，与最近的实验研究相吻合。此外，我们还展示了我们的结果在传输学习中的实践意义。我们的代码可以在 <https://github.com/Heimine/PNC_DLN> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-learning-for-automatic-classification-of-multi-wavelength-auroral-images"><a href="#Multi-view-learning-for-automatic-classification-of-multi-wavelength-auroral-images" class="headerlink" title="Multi-view learning for automatic classification of multi-wavelength auroral images"></a>Multi-view learning for automatic classification of multi-wavelength auroral images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02947">http://arxiv.org/abs/2311.02947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuju Yang, Hang Su, Lili Liu, Yixuan Wang, Ze-Jun Hu</li>
<li>for:  auroral classification studies in polar research, especially using images taken at multiple wavelengths</li>
<li>methods:  lightweight feature extraction backbone (LCTNet) and multi-scale reconstructed feature module (MSRM) to improve classification rate and highlight discriminative information between auroral classes</li>
<li>results:  fusion of multi-wavelength information effectively improves auroral classification performance, achieving state-of-the-art accuracy and computational efficiency compared to previous studies and existing multi-view methods.Here’s the summary in Traditional Chinese:</li>
<li>for: 极地研究中的auroral分类研究，特别是使用多波长的影像</li>
<li>methods: 轻量级特征提取后门 (LCTNet) 和多尺度重建特征模组 (MSRM) 以提高分类率并强调auroralclasses中的差异</li>
<li>results: 多波长资讯的融合有效地提高了auroral分类性能，与前一代研究和现有的多视野方法相比，获得了更高的准确率和计算效率。<details>
<summary>Abstract</summary>
Auroral classification plays a crucial role in polar research. However, current auroral classification studies are predominantly based on images taken at a single wavelength, typically 557.7 nm. Images obtained at other wavelengths have been comparatively overlooked, and the integration of information from multiple wavelengths remains an underexplored area. This limitation results in low classification rates for complex auroral patterns. Furthermore, these studies, whether employing traditional machine learning or deep learning approaches, have not achieved a satisfactory trade-off between accuracy and speed. To address these challenges, this paper proposes a lightweight auroral multi-wavelength fusion classification network, MLCNet, based on a multi-view approach. Firstly, we develop a lightweight feature extraction backbone, called LCTNet, to improve the classification rate and cope with the increasing amount of auroral observation data. Secondly, considering the existence of multi-scale spatial structures in auroras, we design a novel multi-scale reconstructed feature module named MSRM. Finally, to highlight the discriminative information between auroral classes, we propose a lightweight attention feature enhancement module called LAFE. The proposed method is validated using observational data from the Arctic Yellow River Station during 2003-2004. Experimental results demonstrate that the fusion of multi-wavelength information effectively improves the auroral classification performance. In particular, our approach achieves state-of-the-art classification accuracy compared to previous auroral classification studies, and superior results in terms of accuracy and computational efficiency compared to existing multi-view methods.
</details>
<details>
<summary>摘要</summary>
极地研究中的 auroral 分类扮演着关键性的角色，但现有的 auroral 分类研究主要基于单一波长的图像，通常为 557.7 nm。其他波长的图像尚未得到了足够的研究，而 integrating 多波长信息的研究仍然处于未开掘阶段。这种限制导致了复杂的 auroral 图像分类率较低。此外，这些研究， whether 使用传统机器学习或深度学习方法，尚未达到了满意的精度和速度之间的平衡。为了解决这些挑战，本文提出了一种轻量级 auroral 多波长融合分类网络， MLCCNet，基于多视图方法。首先，我们开发了一种轻量级特征提取核心， called LCTNet，以提高分类率和处理大量极地观测数据。其次，考虑到极地 auroras 中存在多尺度空间结构，我们设计了一种新的多尺度重构特征模块， named MSRM。最后，为了强调 auroral 类别之间的区别特征，我们提出了一种轻量级注意力特征增强模块， called LAFE。我们对于2003-2004年由北极黄河站收集的观测数据进行验证。实验结果表明，将多波长信息融合分类效果显著提高了极地分类性能。尤其是，我们的方法在前一代 auroral 分类研究中达到了状态机器学习和现有多视图方法的最高精度和计算效率。
</details></li>
</ul>
<hr>
<h2 id="Truly-Scale-Equivariant-Deep-Nets-with-Fourier-Layers"><a href="#Truly-Scale-Equivariant-Deep-Nets-with-Fourier-Layers" class="headerlink" title="Truly Scale-Equivariant Deep Nets with Fourier Layers"></a>Truly Scale-Equivariant Deep Nets with Fourier Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02922">http://arxiv.org/abs/2311.02922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Ashiqur Rahman, Raymond A. Yeh</li>
<li>for: 这篇论文的目的是解决计算机视觉领域中模型如何适应图像分辨率变化以进行图像分割等任务？</li>
<li>methods: 这篇论文使用了weight-sharing和kernel resizing等方法来实现缩放平衡的 convolutional neural networks，但这些网络并没有真正具备缩放平衡性。</li>
<li>results: 该模型在MNIST-scale和STL-10 datasets上实现了竞争力的分类性能，同时保持了缩放平衡性。<details>
<summary>Abstract</summary>
In computer vision, models must be able to adapt to changes in image resolution to effectively carry out tasks such as image segmentation; This is known as scale-equivariance. Recent works have made progress in developing scale-equivariant convolutional neural networks, e.g., through weight-sharing and kernel resizing. However, these networks are not truly scale-equivariant in practice. Specifically, they do not consider anti-aliasing as they formulate the down-scaling operation in the continuous domain. To address this shortcoming, we directly formulate down-scaling in the discrete domain with consideration of anti-aliasing. We then propose a novel architecture based on Fourier layers to achieve truly scale-equivariant deep nets, i.e., absolute zero equivariance-error. Following prior works, we test this model on MNIST-scale and STL-10 datasets. Our proposed model achieves competitive classification performance while maintaining zero equivariance-error.
</details>
<details>
<summary>摘要</summary>
在计算机视觉领域，模型需要适应图像分辨率的变化以有效实现图像分割等任务。这被称为尺度对称性。最近的研究已经在发展尺度对称的卷积神经网络，例如通过Weight-sharing和kernel重新大小化。但这些网络在实践中并不是真正的尺度对称的。具体来说，它们没有考虑抗锯齿处理，因为它们在连续领域中表述下降操作。为了解决这个缺陷，我们直接在离散领域中表述下降操作，并考虑抗锯齿处理。我们then proposes一种基于Fourier层的新架构，以实现真正的尺度对称的深度网络，即绝对零对称性错误。following prior works, we test this model on MNIST-scale和STL-10 datasets。我们的提议的模型实现了竞争力的分类性能，同时保持零对称性错误。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Deep-Facial-Expression-Recognition-An-Extensive-Protocol-with-Balanced-Dataset-in-the-Wild"><a href="#Benchmarking-Deep-Facial-Expression-Recognition-An-Extensive-Protocol-with-Balanced-Dataset-in-the-Wild" class="headerlink" title="Benchmarking Deep Facial Expression Recognition: An Extensive Protocol with Balanced Dataset in the Wild"></a>Benchmarking Deep Facial Expression Recognition: An Extensive Protocol with Balanced Dataset in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02910">http://arxiv.org/abs/2311.02910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianmarco Ipinze Tutuianu, Yang Liu, Ari Alamäki, Janne Kauttonen</li>
<li>for: 本研究旨在提高人computer交互中的表情识别（FER）精度和通用性，探讨现有方法在实际设置下的表现。</li>
<li>methods: 本研究使用了23种常见的网络架构，按照一种统一的协议进行评估。此外，研究还检验了不同的输入分辨率、类别负荷管理和预训练策略的影响。</li>
<li>results: 经过广泛的实验和实际交叉验证，研究发现了不同网络架构的表现，并提出了在真实场景中部署深度FER方法的建议。此外，研究还讨论了实际应用中的道德规则、隐私问题和法规。<details>
<summary>Abstract</summary>
Facial expression recognition (FER) is a crucial part of human-computer interaction. Existing FER methods achieve high accuracy and generalization based on different open-source deep models and training approaches. However, the performance of these methods is not always good when encountering practical settings, which are seldom explored. In this paper, we collected a new in-the-wild facial expression dataset for cross-domain validation. Twenty-three commonly used network architectures were implemented and evaluated following a uniform protocol. Moreover, various setups, in terms of input resolutions, class balance management, and pre-trained strategies, were verified to show the corresponding performance contribution. Based on extensive experiments on three large-scale FER datasets and our practical cross-validation, we ranked network architectures and summarized a set of recommendations on deploying deep FER methods in real scenarios. In addition, potential ethical rules, privacy issues, and regulations were discussed in practical FER applications such as marketing, education, and entertainment business.
</details>
<details>
<summary>摘要</summary>
人机交互中的表情识别（FER）是一项非常重要的技术。现有的FER方法在不同的开源深度学习模型和训练方法上达到了高度的准确率和泛化性。然而，这些方法在实际应用中的性能不 invariantly是好的，这些应用场景很少被探讨。在这篇论文中，我们收集了一个新的在野 Facial Expression 数据集，用于跨频训练验证。我们实现了23种常用的网络架构，并按照一个固定的协议进行评估。此外，我们还采用了不同的输入分辨率、类别减少管理和预训练策略，以确定它们对性能的贡献。基于大量的实验和我们的实际跨验证，我们对深度FER方法的部署在实际场景中进行了排名和总结，并讨论了实际应用中的伦理规则、隐私问题和法规。
</details></li>
</ul>
<hr>
<h2 id="Human-as-Points-Explicit-Point-based-3D-Human-Reconstruction-from-Single-view-RGB-Images"><a href="#Human-as-Points-Explicit-Point-based-3D-Human-Reconstruction-from-Single-view-RGB-Images" class="headerlink" title="Human as Points: Explicit Point-based 3D Human Reconstruction from Single-view RGB Images"></a>Human as Points: Explicit Point-based 3D Human Reconstruction from Single-view RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02892">http://arxiv.org/abs/2311.02892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingzhi Tang, Qijian Zhang, Junhui Hou, Yebin Liu</li>
<li>for: 这个论文的目的是解决单视图人体重建领域中的一些问题，包括灵活性、普适性、稳定性和表达能力等方面的限制。</li>
<li>methods: 这个论文使用的方法是一种基于点云的Explicit Point-based Human Reconstruction（HaP）框架，通过全面地利用点云来表示目标几何结构，而不是使用传统的启发式学习过程。</li>
<li>results: 实验结果表明，HaP框架可以比现有的方法提供20%到40%的量化性能提升，并且可以提供更好的质量结果。这些结果表明了fully-explicit和几何中心的算法设计的可行性，并且可以激发更多的强大点云模型化架构和处理技术的发展。<details>
<summary>Abstract</summary>
The latest trends in the research field of single-view human reconstruction devote to learning deep implicit functions constrained by explicit body shape priors. Despite the remarkable performance improvements compared with traditional processing pipelines, existing learning approaches still show different aspects of limitations in terms of flexibility, generalizability, robustness, and/or representation capability. To comprehensively address the above issues, in this paper, we investigate an explicit point-based human reconstruction framework called HaP, which adopts point clouds as the intermediate representation of the target geometric structure. Technically, our approach is featured by fully-explicit point cloud estimation, manipulation, generation, and refinement in the 3D geometric space, instead of an implicit learning process that can be ambiguous and less controllable. The overall workflow is carefully organized with dedicated designs of the corresponding specialized learning components as well as processing procedures. Extensive experiments demonstrate that our framework achieves quantitative performance improvements of 20% to 40% over current state-of-the-art methods, and better qualitative results. Our promising results may indicate a paradigm rollback to the fully-explicit and geometry-centric algorithm design, which enables to exploit various powerful point cloud modeling architectures and processing techniques. We will make our code and data publicly available at https://github.com/yztang4/HaP.
</details>
<details>
<summary>摘要</summary>
最新的研究方向在单视人重建领域是学习深入隐藏函数，受到明确的体形约束。despite remarkable performance improvements over traditional processing pipelines, existing learning approaches still have limitations in terms of flexibility, generalizability, robustness, and/or representation capability. To comprehensively address these issues, in this paper, we investigate an explicit point-based human reconstruction framework called HaP, which adopts point clouds as the intermediate representation of the target geometric structure. Technically, our approach is characterized by fully-explicit point cloud estimation, manipulation, generation, and refinement in the 3D geometric space, rather than an implicit learning process that can be ambiguous and less controllable. The overall workflow is carefully organized with dedicated designs of the corresponding specialized learning components as well as processing procedures. Extensive experiments show that our framework achieves quantitative performance improvements of 20% to 40% over current state-of-the-art methods, and better qualitative results. Our promising results may indicate a paradigm rollback to the fully-explicit and geometry-centric algorithm design, which enables us to exploit various powerful point cloud modeling architectures and processing techniques. We will make our code and data publicly available at <https://github.com/yztang4/HaP>.
</details></li>
</ul>
<hr>
<h2 id="Stacked-Autoencoder-Based-Feature-Extraction-and-Superpixel-Generation-for-Multifrequency-PolSAR-Image-Classification"><a href="#Stacked-Autoencoder-Based-Feature-Extraction-and-Superpixel-Generation-for-Multifrequency-PolSAR-Image-Classification" class="headerlink" title="Stacked Autoencoder Based Feature Extraction and Superpixel Generation for Multifrequency PolSAR Image Classification"></a>Stacked Autoencoder Based Feature Extraction and Superpixel Generation for Multifrequency PolSAR Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02887">http://arxiv.org/abs/2311.02887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tushar Gadhiya, Sumanth Tangirala, Anil K. Roy</li>
<li>for: 本研究提出了一种用于多频波束成像（PolSAR）图像的分类算法。</li>
<li>methods: 使用波束剖分算法提取了每个频率带的33个特征，然后使用两层自适应编码器减少输入特征向量的维度，保留有用的输入特征。接着，使用SLIC算法生成了超像素，并使用这些超像素构建了一个强健的特征表示。最后，使用softmax分类器进行分类任务。</li>
<li>results: 实验表明，提出的方法在Flevoland dataset上比其他文献中的方法更为有效。<details>
<summary>Abstract</summary>
In this paper we are proposing classification algorithm for multifrequency Polarimetric Synthetic Aperture Radar (PolSAR) image. Using PolSAR decomposition algorithms 33 features are extracted from each frequency band of the given image. Then, a two-layer autoencoder is used to reduce the dimensionality of input feature vector while retaining useful features of the input. This reduced dimensional feature vector is then applied to generate superpixels using simple linear iterative clustering (SLIC) algorithm. Next, a robust feature representation is constructed using both pixel as well as superpixel information. Finally, softmax classifier is used to perform classification task. The advantage of using superpixels is that it preserves spatial information between neighbouring PolSAR pixels and therefore minimises the effect of speckle noise during classification. Experiments have been conducted on Flevoland dataset and the proposed method was found to be superior to other methods available in the literature.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种多频段Polarimetric Synthetic Aperture Radar（PolSAR）图像分类算法。使用PolSAR分解算法，从每个频段图像中提取了33个特征。然后，使用两层自适应卷积神经网络减少输入特征向量的维度，保留输入特征的有用信息。这个减少后的特征向量然后用simple linear iterative clustering（SLIC）算法生成超像素。接着，通过像素和超像素信息建立了一个强健的特征表示。最后，使用softmax分类器进行分类任务。使用超像素的优势在于它保留了相邻PolSAR像素之间的空间信息，因此减少了雷达噪声的影响，提高了分类的精度。我们在Flevoland数据集上进行了实验，并发现提出的方法在文献中所有方法中显示出优势。
</details></li>
</ul>
<hr>
<h2 id="Inner-IoU-More-Effective-Intersection-over-Union-Loss-with-Auxiliary-Bounding-Box"><a href="#Inner-IoU-More-Effective-Intersection-over-Union-Loss-with-Auxiliary-Bounding-Box" class="headerlink" title="Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box"></a>Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02877">http://arxiv.org/abs/2311.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Cong Xu, Shuaijie Zhang</li>
<li>for: 提高 bounding box regression 过程的效率和精度</li>
<li>methods: 使用不同的缩放因子和辅助 bounding box 来计算损失函数，并引入 Inner-IoU 损失函数</li>
<li>results: 实验结果表明， combining Inner-IoU 损失函数与现有的 IoU-based 损失函数可以进一步提高检测性能，并且在不同的 dataset 和检测器上具有广泛的应用性和通用性。<details>
<summary>Abstract</summary>
With the rapid development of detectors, Bounding Box Regression (BBR) loss function has constantly updated and optimized. However, the existing IoU-based BBR still focus on accelerating convergence by adding new loss terms, ignoring the limitations of IoU loss term itself. Although theoretically IoU loss can effectively describe the state of bounding box regression,in practical applications, it cannot adjust itself according to different detectors and detection tasks, and does not have strong generalization. Based on the above, we first analyzed the BBR model and concluded that distinguishing different regression samples and using different scales of auxiliary bounding boxes to calculate losses can effectively accelerate the bounding box regression process. For high IoU samples, using smaller auxiliary bounding boxes to calculate losses can accelerate convergence, while larger auxiliary bounding boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which calculates IoU loss through auxiliary bounding boxes. For different datasets and detectors, we introduce a scaling factor ratio to control the scale size of the auxiliary bounding boxes for calculating losses. Finally, integrate Inner-IoU into the existing IoU-based loss functions for simulation and comparative experiments. The experiment result demonstrate a further enhancement in detection performance with the utilization of the method proposed in this paper, verifying the effectiveness and generalization ability of Inner IoU loss.
</details>
<details>
<summary>摘要</summary>
随着检测器的快速发展，矩形框回归（BBR）损失函数已经不断地更新和优化。然而，现有的IoU基于的BBR仍然ocuses on accelerating convergence by adding new loss terms, ignoring the limitations of IoU loss term itself. Although theoretically IoU loss can effectively describe the state of bounding box regression, in practical applications, it cannot adjust itself according to different detectors and detection tasks, and does not have strong generalization.根据以上分析，我们首先分析了BBR模型，并结论出，可以通过 distinguish different regression samples and use different scales of auxiliary bounding boxes to calculate losses来加速矩形框回归过程。 For high IoU samples, using smaller auxiliary bounding boxes to calculate losses can accelerate convergence, while larger auxiliary bounding boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which calculates IoU loss through auxiliary bounding boxes. For different datasets and detectors, we introduce a scaling factor ratio to control the scale size of the auxiliary bounding boxes for calculating losses. Finally, we integrate Inner-IoU into the existing IoU-based loss functions for simulation and comparative experiments. The experiment results demonstrate a further enhancement in detection performance with the utilization of the method proposed in this paper, verifying the effectiveness and generalization ability of Inner IoU loss.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Neural-Fields-for-Learning-Atlases-of-4D-Fetal-MRI-Time-series"><a href="#Dynamic-Neural-Fields-for-Learning-Atlases-of-4D-Fetal-MRI-Time-series" class="headerlink" title="Dynamic Neural Fields for Learning Atlases of 4D Fetal MRI Time-series"></a>Dynamic Neural Fields for Learning Atlases of 4D Fetal MRI Time-series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02874">http://arxiv.org/abs/2311.02874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeen Chi, Zhongxiao Cong, Clinton J. Wang, Yingcheng Liu, Esra Abaci Turk, P. Ellen Grant, S. Mazdak Abulnaga, Polina Golland, Neel Dey</li>
<li>for:  rapidement construire des atlas médicaux biomimétiques à l’aide de champs neuraux.</li>
<li>methods: utiliser des champs neuraux pour apprendre des observations déformables spatiotemporelles pour construire des atlas subject-spécifiques.</li>
<li>results: obtenir des atlas de haute qualité de séries de données BOLD MRI dynamiques de fœtus en utéro avec une convergence environ 5-7 fois plus rapide que les méthodes existantes.<details>
<summary>Abstract</summary>
We present a method for fast biomedical image atlas construction using neural fields. Atlases are key to biomedical image analysis tasks, yet conventional and deep network estimation methods remain time-intensive. In this preliminary work, we frame subject-specific atlas building as learning a neural field of deformable spatiotemporal observations. We apply our method to learning subject-specific atlases and motion stabilization of dynamic BOLD MRI time-series of fetuses in utero. Our method yields high-quality atlases of fetal BOLD time-series with $\sim$5-7$\times$ faster convergence compared to existing work. While our method slightly underperforms well-tuned baselines in terms of anatomical overlap, it estimates templates significantly faster, thus enabling rapid processing and stabilization of large databases of 4D dynamic MRI acquisitions. Code is available at https://github.com/Kidrauh/neural-atlasing
</details>
<details>
<summary>摘要</summary>
我们提出了一种快速生成医学影像 Atlases 的方法，使用神经场。 Atlases 是生物医学影像分析任务的关键，但是传统的深度网络估计方法和深度网络估计方法仍然很时间消耗。在这项先导性工作中，我们将主动Specific atlas 的建立Equate 为学习扭变的时空观察的神经场。我们应用我们的方法到学习主动Specific atlas 和动态BOLD MRI时序列的运动稳定。我们的方法可以快速生成高质量的胎儿BOLD时序列的 Atlases，相比已有的工作，它的连续变换速度约为5-7倍。虽然我们的方法在注意力调整后的基eline上略有下降，但它可以更快地估计模板，因此可以快速处理和稳定大量的4D动态MRI数据库。代码可以在https://github.com/Kidrauh/neural-atlasing 上获取。
</details></li>
</ul>
<hr>
<h2 id="OVIR-3D-Open-Vocabulary-3D-Instance-Retrieval-Without-Training-on-3D-Data"><a href="#OVIR-3D-Open-Vocabulary-3D-Instance-Retrieval-Without-Training-on-3D-Data" class="headerlink" title="OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data"></a>OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02873">http://arxiv.org/abs/2311.02873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shiyoung77/OVIR-3D">https://github.com/shiyoung77/OVIR-3D</a></li>
<li>paper_authors: Shiyang Lu, Haonan Chang, Eric Pu Jing, Abdeslam Boularias, Kostas Bekris</li>
<li>for: 开发了一种无需使用任何3D数据进行训练的开 vocabulary 3D对象实例检索方法。</li>
<li>methods: 使用文本查询和多视图拟合2D区域提档网络，将2D区域提档拟合到3D空间，实现文本查询和3D对象实例检索的联合。</li>
<li>results: 实验表明，该方法可以快速和高效地在大多数室内3D场景中进行实时多视图拟合，并且不需要额外训练在3D空间。<details>
<summary>Abstract</summary>
This work presents OVIR-3D, a straightforward yet effective method for open-vocabulary 3D object instance retrieval without using any 3D data for training. Given a language query, the proposed method is able to return a ranked set of 3D object instance segments based on the feature similarity of the instance and the text query. This is achieved by a multi-view fusion of text-aligned 2D region proposals into 3D space, where the 2D region proposal network could leverage 2D datasets, which are more accessible and typically larger than 3D datasets. The proposed fusion process is efficient as it can be performed in real-time for most indoor 3D scenes and does not require additional training in 3D space. Experiments on public datasets and a real robot show the effectiveness of the method and its potential for applications in robot navigation and manipulation.
</details>
<details>
<summary>摘要</summary>
这项工作介绍了一种简单又有效的方法，可以在开放词汇3D对象实例检索中不使用任何3D数据进行训练。给定一个语言查询，提posed方法可以返回一个相似度排序的3D对象实例分割，基于实例和文本查询的特征相似性。这是通过多视图融合文本对齐2D区域提案到3D空间中进行实现的，其中2D区域提案网络可以利用2D数据集，这些数据集通常更容易获取和更大规模。我们的融合过程具有实时性，可以适用于大多数室内3D场景，而且不需要额外训练在3D空间。我们的实验表明，这种方法在公共数据集和真实的 робоット上具有有效性，并且它在机器人导航和 manipulate 中具有潜在的应用前景。
</details></li>
</ul>
<hr>
<h2 id="FocusTune-Tuning-Visual-Localization-through-Focus-Guided-Sampling"><a href="#FocusTune-Tuning-Visual-Localization-through-Focus-Guided-Sampling" class="headerlink" title="FocusTune: Tuning Visual Localization through Focus-Guided Sampling"></a>FocusTune: Tuning Visual Localization through Focus-Guided Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02872">http://arxiv.org/abs/2311.02872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Son Tung Nguyen, Alejandro Fontan, Michael Milford, Tobias Fischer</li>
<li>for: 提高视觉地标算法的性能</li>
<li>methods: FocusTune使用重点导航的抽象技术，将场景坐标回归模型引导向关键的3D点三角形计算中的区域</li>
<li>results: FocusTune可以提高或与状态机器学习模型匹配的性能，同时保持ACE模型的低存储和计算需求，例如在 cambridge 地标集上减少翻译错误从25到19和17到15cm。<details>
<summary>Abstract</summary>
We propose FocusTune, a focus-guided sampling technique to improve the performance of visual localization algorithms. FocusTune directs a scene coordinate regression model towards regions critical for 3D point triangulation by exploiting key geometric constraints. Specifically, rather than uniformly sampling points across the image for training the scene coordinate regression model, we instead re-project 3D scene coordinates onto the 2D image plane and sample within a local neighborhood of the re-projected points. While our proposed sampling strategy is generally applicable, we showcase FocusTune by integrating it with the recently introduced Accelerated Coordinate Encoding (ACE) model. Our results demonstrate that FocusTune both improves or matches state-of-the-art performance whilst keeping ACE's appealing low storage and compute requirements, for example reducing translation error from 25 to 19 and 17 to 15 cm for single and ensemble models, respectively, on the Cambridge Landmarks dataset. This combination of high performance and low compute and storage requirements is particularly promising for applications in areas like mobile robotics and augmented reality. We made our code available at \url{https://github.com/sontung/focus-tune}.
</details>
<details>
<summary>摘要</summary>
我们提出了FocusTune，一种帮助视觉地标定算法提高性能的集中样本技术。FocusTune通过利用关键的几何约束来导引Scene coordinate regression模型对于3D点Triangulation的重要区域进行样本。具体来说，我们不是 uniformly sampling点在图像上进行Scene coordinate regression模型的训练，而是将3D场景坐标重project onto 2D图像平面，然后在本地 neighborhood中采样。我们的提议的样本策略是通用的，但我们在ACE模型中展示了FocusTune。我们的结果表明，FocusTune可以提高或与状态艺术性能匹配，同时保持ACE模型的吸引人低存储和计算需求，例如将翻译错误从25减少到19和17减少到15 cm，分别在Cambridge Landmarks dataset上。这种高性能低计算存储需求的组合非常有前途，特别是在移动 робо扮和增强现实中。我们的代码可以在https://github.com/sontung/focus-tune中下载。
</details></li>
</ul>
<hr>
<h2 id="Neural-based-Compression-Scheme-for-Solar-Image-Data"><a href="#Neural-based-Compression-Scheme-for-Solar-Image-Data" class="headerlink" title="Neural-based Compression Scheme for Solar Image Data"></a>Neural-based Compression Scheme for Solar Image Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02855">http://arxiv.org/abs/2311.02855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Jeremy A. Grajeda, Piyush M. Mehta, Nasser M. Nasrabadi, Laura E. Boucheron, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva<br>for: This paper proposes a neural network-based lossy compression method for data-intensive imagery missions, specifically targeting NASA’s SDO mission.methods: The proposed method uses an adversarially trained neural network with local and non-local attention modules to capture the local and global structure of the image, resulting in a better trade-off in rate-distortion (RD) compared to conventional hand-engineered codecs. The RD variational autoencoder is jointly trained with a channel-dependent entropy model to make the entropy coding of the latent code more effective.results: The proposed algorithm outperforms currently-in-use and state-of-the-art codecs such as JPEG and JPEG-2000 in terms of RD performance when compressing extreme-ultraviolet (EUV) data. The algorithm is able to achieve consistent segmentations of coronal holes (CH) in compressed EUV images from SDO, even at a compression rate of $\sim0.1$ bits per pixel.<details>
<summary>Abstract</summary>
Studying the solar system and especially the Sun relies on the data gathered daily from space missions. These missions are data-intensive and compressing this data to make them efficiently transferable to the ground station is a twofold decision to make. Stronger compression methods, by distorting the data, can increase data throughput at the cost of accuracy which could affect scientific analysis of the data. On the other hand, preserving subtle details in the compressed data requires a high amount of data to be transferred, reducing the desired gains from compression. In this work, we propose a neural network-based lossy compression method to be used in NASA's data-intensive imagery missions. We chose NASA's SDO mission which transmits 1.4 terabytes of data each day as a proof of concept for the proposed algorithm. In this work, we propose an adversarially trained neural network, equipped with local and non-local attention modules to capture both the local and global structure of the image resulting in a better trade-off in rate-distortion (RD) compared to conventional hand-engineered codecs. The RD variational autoencoder used in this work is jointly trained with a channel-dependent entropy model as a shared prior between the analysis and synthesis transforms to make the entropy coding of the latent code more effective. Our neural image compression algorithm outperforms currently-in-use and state-of-the-art codecs such as JPEG and JPEG-2000 in terms of the RD performance when compressing extreme-ultraviolet (EUV) data. As a proof of concept for use of this algorithm in SDO data analysis, we have performed coronal hole (CH) detection using our compressed images, and generated consistent segmentations, even at a compression rate of $\sim0.1$ bits per pixel (compared to 8 bits per pixel on the original data) using EUV data from SDO.
</details>
<details>
<summary>摘要</summary>
研究太阳系和特别是太阳需要每天从空间任务中收集数据。这些任务是数据敏感的，压缩这些数据以使其可以高效地传输到地面站点是一个两fold的决策。使用更强的压缩方法可以提高数据传输速率，但是这将会影响科学分析中的精度。当然，保留图像中的细节需要大量数据进行传输，这会降低所希望的压缩率。在这种情况下，我们提出了一种基于神经网络的损失压缩方法，用于NASA的数据敏感成像任务。我们选择了NASA的SDO任务，每天传输1.4 terabytes的数据作为证明。在这种情况下，我们提出了一种适应学习神经网络，具有本地和非本地注意模块，以捕捉图像的本地和全局结构，从而实现更好的Rate-Distortion（RD）质量比。我们的神经图像压缩算法在JPEG和JPEG-2000等现有的编码器之上具有更高的RD性能，特别是在激光ultraviolet（EUV）数据压缩中。作为SDO数据分析的证明，我们对压缩后的图像进行了核心孔（CH）检测，并获得了一致的分 segmentation，即使压缩率为0.1 bits/像素（相比8 bits/像素的原始数据）。
</details></li>
</ul>
<hr>
<h2 id="Consistent4D-Consistent-360°-Dynamic-Object-Generation-from-Monocular-Video"><a href="#Consistent4D-Consistent-360°-Dynamic-Object-Generation-from-Monocular-Video" class="headerlink" title="Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video"></a>Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02848">http://arxiv.org/abs/2311.02848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanqinJiang/Consistent4D">https://github.com/yanqinJiang/Consistent4D</a></li>
<li>paper_authors: Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, Yao Yao</li>
<li>for: 这个论文是为了从不准确的单视视频中生成4D动态对象而设计的。</li>
<li>methods: 该论文使用了一种新的方法，即Casting the 360-degree dynamic object reconstruction as a 4D generation problem，即使用物体层次3D意识图像扩散模型作为训练动态神经辐射场（DyNeRF）的主要监督信号。具体来说，该论文提出了一种协调 DyNeRF 来实现稳定的收敛和时间连续性，并引入了一种 interpolate-driven consistency loss 来保证空间和时间一致性。</li>
<li>results: 该论文的实验表明，Consistent4D 可以与之前的同类方法相比，在生成4D动态对象方面表现出 competitive 的性能，同时也在传统的文本到3D生成任务中表现出优势。具体来说，Consistent4D 可以在不需要多视图数据收集和摄像头准确性调整的情况下，从单视视频中生成高质量的4D动态对象。<details>
<summary>Abstract</summary>
In this paper, we present Consistent4D, a novel approach for generating 4D dynamic objects from uncalibrated monocular videos. Uniquely, we cast the 360-degree dynamic object reconstruction as a 4D generation problem, eliminating the need for tedious multi-view data collection and camera calibration. This is achieved by leveraging the object-level 3D-aware image diffusion model as the primary supervision signal for training Dynamic Neural Radiance Fields (DyNeRF). Specifically, we propose a Cascade DyNeRF to facilitate stable convergence and temporal continuity under the supervision signal which is discrete along the time axis. To achieve spatial and temporal consistency, we further introduce an Interpolation-driven Consistency Loss. It is optimized by minimizing the discrepancy between rendered frames from DyNeRF and interpolated frames from a pre-trained video interpolation model. Extensive experiments show that our Consistent4D can perform competitively to prior art alternatives, opening up new possibilities for 4D dynamic object generation from monocular videos, whilst also demonstrating advantage for conventional text-to-3D generation tasks. Our project page is https://consistent4d.github.io/.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的方法---Consistent4D，可以从无投影照片中生成4D动态对象。这种方法通过将360度动态对象重建视为4D生成问题来消除需要繁琐的多视图数据收集和摄像头准确性调整的需求。我们利用对象层3D意识图像扩散模型作为训练动态光谱场（DyNeRF）的主要超视度信号。我们还提出了一种协调 DyNeRF，以便在超视度信号上稳定地迁移并保持时间连续性。为实现空间和时间一致性，我们还引入了一种 interpolate-driven 一致损失。它通过对 DyNeRF 生成的帧和 interpolated 视频 interpolated 模型生成的帧之间的差异进行最小化来优化。我们的实验结果表明，我们的 Consistent4D 可以与先前的方法相比竞争，开启了从无投影照片中生成4D动态对象的新可能性，同时也在传统的文本到3D生成任务中显示了优势。我们的项目页面是 <https://consistent4d.github.io/>。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Multi-Generator-Model-with-Fused-Spatiotemporal-Graph-for-Trajectory-Prediction"><a href="#Flexible-Multi-Generator-Model-with-Fused-Spatiotemporal-Graph-for-Trajectory-Prediction" class="headerlink" title="Flexible Multi-Generator Model with Fused Spatiotemporal Graph for Trajectory Prediction"></a>Flexible Multi-Generator Model with Fused Spatiotemporal Graph for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02835">http://arxiv.org/abs/2311.02835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyuan Zhu, Fengxia Han, Hao Deng</li>
<li>for: 这个研究是为了提高自动驾驶系统中的路径预测精度，以便更 precisel y track和决策。</li>
<li>methods: 本研究使用了生成对抗网络，能够学习未来路径的分布。但是，这些分布通常包含了不同的数据构造，例如人行道上的行人可能有不同的社交互动方式。为了解决这个问题，我们提出了一个路径预测框架，可以更好地模型场景中的人行道交互。</li>
<li>results: 我们的框架在不同的挑战性数据集上实现了比基eline更高的表现。<details>
<summary>Abstract</summary>
Trajectory prediction plays a vital role in automotive radar systems, facilitating precise tracking and decision-making in autonomous driving. Generative adversarial networks with the ability to learn a distribution over future trajectories tend to predict out-of-distribution samples, which typically occurs when the distribution of forthcoming paths comprises a blend of various manifolds that may be disconnected. To address this issue, we propose a trajectory prediction framework, which can capture the social interaction variations and model disconnected manifolds of pedestrian trajectories. Our framework is based on a fused spatiotemporal graph to better model the complex interactions of pedestrians in a scene, and a multi-generator architecture that incorporates a flexible generator selector network on generated trajectories to learn a distribution over multiple generators. We show that our framework achieves state-of-the-art performance compared with several baselines on different challenging datasets.
</details>
<details>
<summary>摘要</summary>
几何预测在汽车射频系统中扮演着重要的角色，帮助汽车在自动驾驶中精确追踪和做出决策。生成对抗网络，能够学习未来几何分布，对于非常量标本进行预测，通常发生在未来路径分布中包含多个不同构造的混合体。为解决这个问题，我们提出了一个几何预测框架，可以捕捉人员在场景中的社交互动变化，并且可以模型对于步行人的跟踪轨迹的不连续构造。我们的框架基于融合的空间时间图，更好地模型人员在场景中的复杂互动，并且具有一个灵活的生成器选择器网络，可以学习多个生成器之间的分布。我们证明了我们的框架在不同的挑战性数据集上具有现代水准的表现。
</details></li>
</ul>
<hr>
<h2 id="SemanticTopoLoop-Semantic-Loop-Closure-With-3D-Topological-Graph-Based-on-Quadric-Level-Object-Map"><a href="#SemanticTopoLoop-Semantic-Loop-Closure-With-3D-Topological-Graph-Based-on-Quadric-Level-Object-Map" class="headerlink" title="SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map"></a>SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02831">http://arxiv.org/abs/2311.02831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenzhong Cao</li>
<li>for: 增强SLAM系统的精度和稳定性，解决传统的bag-of-words模型在实际场景中的局限性和不稳定性问题。</li>
<li>methods: 提出基于多级验证的对象水平数据关联方法，使得当前帧的2D语义特征与地图中的3D对象标记之间建立关联关系。然后，通过比较场景的对象图形图表 topology，实现精准的循环关闭。</li>
<li>results: 经验和抽象研究表明，提出的对象水平数据关联算法具有高效性和稳定性，而semantic循环关闭方法在宽视野下具有高精度和高准确性。<details>
<summary>Abstract</summary>
Loop closure, as one of the crucial components in SLAM, plays an essential role in correcting the accumulated errors. Traditional appearance-based methods, such as bag-of-words models, are often limited by local 2D features and the volume of training data, making them less versatile and robust in real-world scenarios, leading to missed detections or false positives detections in loop closure. To address these issues, we first propose a object-level data association method based on multi-level verification, which can associate 2D semantic features of current frame with 3D objects landmarks of map. Next, taking advantage of these association relations, we introduce a semantic loop closure method based on quadric-level object map topology, which represents scenes through the topological graph of objects and achieves accurate loop closure at a wide field of view by comparing differences in the topological graphs. Finally, we integrate these two methods into a complete object-aware SLAM system. Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm. Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall and localization accuracy metrics.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>Loop closure, as one of the crucial components in SLAM, plays an essential role in correcting the accumulated errors. Traditional appearance-based methods, such as bag-of-words models, are often limited by local 2D features and the volume of training data, making them less versatile and robust in real-world scenarios, leading to missed detections or false positives detections in loop closure. To address these issues, we first propose a object-level data association method based on multi-level verification, which can associate 2D semantic features of current frame with 3D objects landmarks of map. Next, taking advantage of these association relations, we introduce a semantic loop closure method based on quadric-level object map topology, which represents scenes through the topological graph of objects and achieves accurate loop closure at a wide field of view by comparing differences in the topological graphs. Finally, we integrate these two methods into a complete object-aware SLAM system. Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm. Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall and localization accuracy metrics.Here's the translation in Simplified Chinese:<<SYS>>将文本翻译成简化中文。<</SYS>>loop closure，作为SLAM中关键的一部分，对correcting accumulated errors起到关键作用。传统的出现基于方法，如bag-of-words模型，frequently limited by local 2D features和training data量，这会使其在实际场景中 menos versatile和Robust，导致loop closure中的 missed detections或false positives detections。为解决这些问题，我们首先提出了一种基于多级验证的object-level数据关联方法，可以将当前帧的2Dsemantic features与地图中的3Dobject landmarks相关联。然后，通过这些关联关系，我们引入了一种基于quadric-level object map topology的semantic loop closure方法，可以通过比较不同的topological graphs来实现准确的loop closure。最后，我们将这两种方法集成到了一个完整的object-aware SLAM系统中。Qualitative experiments和ablation studies表明了我们提出的object-level数据关联算法的效iveness和Robustness。Quantitative experiments表明，我们的semantic loop closure方法在精度、回归率和本地化精度 metric上表现出色，超越了现有的state-of-the-art方法。
</details></li>
</ul>
<hr>
<h2 id="InstructPix2NeRF-Instructed-3D-Portrait-Editing-from-a-Single-Image"><a href="#InstructPix2NeRF-Instructed-3D-Portrait-Editing-from-a-Single-Image" class="headerlink" title="InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image"></a>InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02826">http://arxiv.org/abs/2311.02826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhui Li, Shilong Liu, Zidong Liu, Yikai Wang, Kaiwen Zheng, Jinghui Xu, Jianmin Li, Jun Zhu</li>
<li>for: This paper aims to solve the problem of human-instructed 3D-aware portrait editing for open-world images, which has been under-explored due to the lack of labeled human face 3D datasets and effective architectures.</li>
<li>methods: The proposed method, InstructPix2NeRF, uses a conditional latent 3D diffusion process to lift 2D editing to 3D space, and learns the correlation between the paired images’ difference and the instructions via triplet data. The method also uses a token position randomization strategy to achieve multi-semantic editing with the portrait identity well-preserved.</li>
<li>results: The proposed method is effective in achieving human-instructed 3D-aware portrait editing, and shows superiority against strong baselines quantitatively and qualitatively.<details>
<summary>Abstract</summary>
With the success of Neural Radiance Field (NeRF) in 3D-aware portrait editing, a variety of works have achieved promising results regarding both quality and 3D consistency. However, these methods heavily rely on per-prompt optimization when handling natural language as editing instructions. Due to the lack of labeled human face 3D datasets and effective architectures, the area of human-instructed 3D-aware editing for open-world portraits in an end-to-end manner remains under-explored. To solve this problem, we propose an end-to-end diffusion-based framework termed InstructPix2NeRF, which enables instructed 3D-aware portrait editing from a single open-world image with human instructions. At its core lies a conditional latent 3D diffusion process that lifts 2D editing to 3D space by learning the correlation between the paired images' difference and the instructions via triplet data. With the help of our proposed token position randomization strategy, we could even achieve multi-semantic editing through one single pass with the portrait identity well-preserved. Besides, we further propose an identity consistency module that directly modulates the extracted identity signals into our diffusion process, which increases the multi-view 3D identity consistency. Extensive experiments verify the effectiveness of our method and show its superiority against strong baselines quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
成功的神经辐射场（NeRF）在3D意识图像编辑方面取得了显著成果，许多作品在质量和3D一致性方面都取得了显著的进步。然而，这些方法却依赖于每个提示进行优化，对于自然语言作为编辑指令来说是一个挑战。由于人脸3D数据集缺乏标注和有效的建筑，在开放世界图像上的人 instruктирован3D意识图像编辑仍然是一个未解决的问题。为解决这个问题，我们提出了一种终端扩散基于的框架，称为InstructPix2NeRF，它可以从单个开放世界图像中进行人 instruктирован3D意识图像编辑。核心 liegt于一种受控 latent 3D扩散过程，通过学习对带有匹配图像差异和指令的对应关系来提升2D编辑到3D空间。通过我们提出的 токен位随机化策略，我们可以在一个单 pass 中实现多Semantic编辑，并且保持人脸标识的稳定性。此外，我们还提出了一种标识一致性模块，它直接将提取的标识信号与我们的扩散过程集成，从而提高多视图3D标识一致性。广泛的实验证明了我们的方法的效果，并与强基线相比，我们的方法在量和质量上具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Self-Supervised-Human-Pose-Estimation-with-Inductive-Prior-Tuning"><a href="#Efficient-Self-Supervised-Human-Pose-Estimation-with-Inductive-Prior-Tuning" class="headerlink" title="Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning"></a>Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02815">http://arxiv.org/abs/2311.02815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nobline Yoo, Olga Russakovsky</li>
<li>for: 本研究的目的是提高无监督人体pose估计（HPE）的精度。</li>
<li>methods: 本文使用自我监督方法，将HPE任务重新定义为一个重构问题，以便利用大量未标注的视觉数据。</li>
<li>results: 本研究提出了一种新的度量方法，可以在无监督情况下衡量人体部分长度的一致性。此外，文章还提出了一种新的模型架构，可以在使用 fewer than one-third 的训练数据的情况下超越基eline。<details>
<summary>Abstract</summary>
The goal of 2D human pose estimation (HPE) is to localize anatomical landmarks, given an image of a person in a pose. SOTA techniques make use of thousands of labeled figures (finetuning transformers or training deep CNNs), acquired using labor-intensive crowdsourcing. On the other hand, self-supervised methods re-frame the HPE task as a reconstruction problem, enabling them to leverage the vast amount of unlabeled visual data, though at the present cost of accuracy. In this work, we explore ways to improve self-supervised HPE. We (1) analyze the relationship between reconstruction quality and pose estimation accuracy, (2) develop a model pipeline that outperforms the baseline which inspired our work, using less than one-third the amount of training data, and (3) offer a new metric suitable for self-supervised settings that measures the consistency of predicted body part length proportions. We show that a combination of well-engineered reconstruction losses and inductive priors can help coordinate pose learning alongside reconstruction in a self-supervised paradigm.
</details>
<details>
<summary>摘要</summary>
文章目标是提高二维人姿估计（HPE）的自主学习性能。现状的最佳技术使用了千余个标注的人像（finetuning transformers或训练深度CNN），通过劳动密集的人工投票获得。然而，无监督方法将HPE任务重新定义为重建问题，可以利用大量未标注的视觉数据，但当前精度较低。本文探讨如何提高自主学习HPE。我们（1）分析重建质量和姿势估计准确性之间的关系，（2）开发一个超过基eline的模型管道，使用较少的训练数据，并（3）提出一个适合自主设置的度量，用于测量预测身体部分长度的一致性。我们显示，通过合理的重建损失和推导约束，可以协调姿势学习与重建在无监督情况下协同进行。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Interpretable-Face-Identification-for-Out-Of-Distribution-Data-Using-Vision-Transformers"><a href="#Fast-and-Interpretable-Face-Identification-for-Out-Of-Distribution-Data-Using-Vision-Transformers" class="headerlink" title="Fast and Interpretable Face Identification for Out-Of-Distribution Data Using Vision Transformers"></a>Fast and Interpretable Face Identification for Out-Of-Distribution Data Using Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02803">http://arxiv.org/abs/2311.02803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Phan, Cindy Le, Vu Le, Yihui He, Anh Totti Nguyen</li>
<li>for: 提高Face Identification的精度和准确性，特别是对于 occlusion 和 out-of-distribution 数据。</li>
<li>methods: 使用 two-image Vision Transformers (ViTs)  Compares two images at the patch level using cross-attention。</li>
<li>results: 在 CASIA Webface 数据集上训练，与 DeepFace-EMD 相比，我们的模型在 out-of-distribution 数据上达到了相同的准确率，但在推理速度上高于 DeepFace-EMD 的两倍以上。此外，我们通过人类研究发现，我们的模型具有良好的解释性，可以通过视觉化的 cross-attention 来visualize。<details>
<summary>Abstract</summary>
Most face identification approaches employ a Siamese neural network to compare two images at the image embedding level. Yet, this technique can be subject to occlusion (e.g. faces with masks or sunglasses) and out-of-distribution data. DeepFace-EMD (Phan et al. 2022) reaches state-of-the-art accuracy on out-of-distribution data by first comparing two images at the image level, and then at the patch level. Yet, its later patch-wise re-ranking stage admits a large $O(n^3 \log n)$ time complexity (for $n$ patches in an image) due to the optimal transport optimization. In this paper, we propose a novel, 2-image Vision Transformers (ViTs) that compares two images at the patch level using cross-attention. After training on 2M pairs of images on CASIA Webface (Yi et al. 2014), our model performs at a comparable accuracy as DeepFace-EMD on out-of-distribution data, yet at an inference speed more than twice as fast as DeepFace-EMD (Phan et al. 2022). In addition, via a human study, our model shows promising explainability through the visualization of cross-attention. We believe our work can inspire more explorations in using ViTs for face identification.
</details>
<details>
<summary>摘要</summary>
大多数面部识别方法使用奶爸 neural network 对两个图像进行比较，并在图像嵌入层进行比较。然而，这种技术可能会受到遮盾（例如面具或镜片）和非标本数据的影响。深度Face-EMD（phan et al. 2022）可以在非标本数据上达到状态机器的准确率，但是其后来的 patch-wise 重新排名阶段具有大 O（n^3 log n）的时间复杂度（对于图像中的 n 个 patch），这是因为优化运输问题。在这篇文章中，我们提出了一种新的， 两个图像 vision transformers（ViTs），它将两个图像的 patch 进行比较，使用 crossed attention。经过在 CASIA Webface（yi et al. 2014）上训练 200 万对图像，我们的模型在不标本数据上达到与 DeepFace-EMD 相同的准确率，但是在推断速度上比 DeepFace-EMD 快上了两倍。此外，通过人类研究，我们的模型表现出了可见的解释性，通过跨层 attention 的视觉化。我们认为我们的工作可以激励更多的人们进行 ViTs 的探索。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.CV_2023_11_06/" data-id="cloojsmgc00l2re888cn3cw73" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/06/cs.SD_2023_11_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-11-06
        
      </div>
    </a>
  
  
    <a href="/2023/11/06/cs.AI_2023_11_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
