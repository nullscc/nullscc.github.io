
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-06 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03572 repo_url: None paper_authors: Dehao Qin, Ripon Saha, Suren Jayasuriya,">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-06">
<meta property="og:url" content="https://nullscc.github.io/2023/11/06/cs.CV_2023_11_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.03572 repo_url: None paper_authors: Dehao Qin, Ripon Saha, Suren Jayasuriya,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-06T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-08T18:15:54.478Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.CV_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T13:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-06
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-Region-Growing-Network-for-Object-Segmentation-in-Atmospheric-Turbulence"><a href="#Unsupervised-Region-Growing-Network-for-Object-Segmentation-in-Atmospheric-Turbulence" class="headerlink" title="Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence"></a>Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03572">http://arxiv.org/abs/2311.03572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dehao Qin, Ripon Saha, Suren Jayasuriya, Jinwei Ye, Nianyi Li</li>
<li>for: 这篇论文旨在提出一个不需要训练数据的二阶段无监督前景物分类网络，用于应对受到大气抖扰影响的动态场景中的前景物分类。</li>
<li>methods: 这篇论文使用了均值运动流的数据来验证一个新的区域生长算法，将每个影像序列中的移动物体构成为先验的mask。然后，这些mask会被用来训练一个U-Net架构，以进一步精确化这些mask的空间时间Alignment。</li>
<li>results: 这篇论文的方法在新的动态场景中的前景物分类 зада问中证明了superior的准确性和Robustness，并且可以运行在不同的抖扰强度下，并且可以跨越不同的训练数据。<details>
<summary>Abstract</summary>
In this paper, we present a two-stage unsupervised foreground object segmentation network tailored for dynamic scenes affected by atmospheric turbulence. In the first stage, we utilize averaged optical flow from turbulence-distorted image sequences to feed a novel region-growing algorithm, crafting preliminary masks for each moving object in the video. In the second stage, we employ a U-Net architecture with consistency and grouping losses to further refine these masks optimizing their spatio-temporal alignment. Our approach does not require labeled training data and works across varied turbulence strengths for long-range video. Furthermore, we release the first moving object segmentation dataset of turbulence-affected videos, complete with manually annotated ground truth masks. Our method, evaluated on this new dataset, demonstrates superior segmentation accuracy and robustness as compared to current state-of-the-art unsupervised methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种适用于气象干扰影像序列的两stage无监督前景 объек 分割网络。在第一stage中，我们利用气象干扰扭曲图像序列的平均运动流场来驱动一种新的区域增长算法，生成每个视频中的移动对象预制掩码。在第二stage中，我们使用U-Net架构，并添加一致性和分组损失来进一步纠正这些掩码，以便在视频中实现更好的空间时间Alignment。我们的方法不需要标注训练数据，并在不同的气象强度下工作，可以处理长距离视频。此外，我们发布了第一个受气象干扰影像序列影响的移动对象分割数据集，该数据集包括手动标注的真实掩码标准。我们的方法在这个新数据集上进行评估，与当前无监督方法相比，示出了更高的分割精度和Robustness。
</details></li>
</ul>
<hr>
<h2 id="Cal-DETR-Calibrated-Detection-Transformer"><a href="#Cal-DETR-Calibrated-Detection-Transformer" class="headerlink" title="Cal-DETR: Calibrated Detection Transformer"></a>Cal-DETR: Calibrated Detection Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03570">http://arxiv.org/abs/2311.03570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akhtarvision/cal-detr">https://github.com/akhtarvision/cal-detr</a></li>
<li>paper_authors: Muhammad Akhtar Munir, Salman Khan, Muhammad Haris Khan, Mohsen Ali, Fahad Shahbaz Khan</li>
<li>for: 这个研究的目的是为了将深度神经网络（DNNs）训练为更加正确的物体检测器。</li>
<li>methods: 这个研究使用了训练时间calibration（Cal-DETR） Mechanism，包括量化uncertainty、 uncertainty-guided logit modulation和logit mixing等方法，以提高物体检测器的准确性。</li>
<li>results: 实验结果显示，Cal-DETR可以对内部和外部预测进行准确的调整，同时维持或以上改善物体检测器的检测性能。<details>
<summary>Abstract</summary>
Albeit revealing impressive predictive performance for several computer vision tasks, deep neural networks (DNNs) are prone to making overconfident predictions. This limits the adoption and wider utilization of DNNs in many safety-critical applications. There have been recent efforts toward calibrating DNNs, however, almost all of them focus on the classification task. Surprisingly, very little attention has been devoted to calibrating modern DNN-based object detectors, especially detection transformers, which have recently demonstrated promising detection performance and are influential in many decision-making systems. In this work, we address the problem by proposing a mechanism for calibrated detection transformers (Cal-DETR), particularly for Deformable-DETR, UP-DETR and DINO. We pursue the train-time calibration route and make the following contributions. First, we propose a simple yet effective approach for quantifying uncertainty in transformer-based object detectors. Second, we develop an uncertainty-guided logit modulation mechanism that leverages the uncertainty to modulate the class logits. Third, we develop a logit mixing approach that acts as a regularizer with detection-specific losses and is also complementary to the uncertainty-guided logit modulation technique to further improve the calibration performance. Lastly, we conduct extensive experiments across three in-domain and four out-domain scenarios. Results corroborate the effectiveness of Cal-DETR against the competing train-time methods in calibrating both in-domain and out-domain detections while maintaining or even improving the detection performance. Our codebase and pre-trained models can be accessed at \url{https://github.com/akhtarvision/cal-detr}.
</details>
<details>
<summary>摘要</summary>
尽管深度神经网络（DNN）在计算机视觉任务上表现出了卓越的预测能力，但它们又容易出现过于自信的预测问题。这限制了DNN在一些安全关键应用程序中的应用和利用。随着这些应用的广泛使用，对DNN的准确性和可靠性的需求也在不断增长。在这种情况下，我们提出了一种名为Cal-DETR的机制，用于calibrating modern DNN-based object detectors，特别是Deformable-DETR、UP-DETR和DINO。我们采用了训练时calibration的路径，并在这里提供了以下贡献：首先，我们提出了一种简单 yet effective的方法来评估转换器基本对象检测器的uncertainty。其次，我们开发了一种基于uncertainty的logit模ulation机制，可以利用uncertainty来修改类logits。最后，我们开发了一种logit混合approach，可以作为一种权重补做和检测特有的损失函数，并且与uncertainty-guided logit modulation技术相结合，以进一步提高准确性表现。我们在三个域内和四个外域场景进行了广泛的实验。结果证明，Cal-DETR在与竞争的训练时calibration方法相比，可以更好地准确地调整域内和外域检测。我们的代码基和预训练模型可以在 GitHub上获取，地址为 \url{https://github.com/akhtarvision/cal-detr}.
</details></li>
</ul>
<hr>
<h2 id="Sea-You-Later-Metadata-Guided-Long-Term-Re-Identification-for-UAV-Based-Multi-Object-Tracking"><a href="#Sea-You-Later-Metadata-Guided-Long-Term-Re-Identification-for-UAV-Based-Multi-Object-Tracking" class="headerlink" title="Sea You Later: Metadata-Guided Long-Term Re-Identification for UAV-Based Multi-Object Tracking"></a>Sea You Later: Metadata-Guided Long-Term Re-Identification for UAV-Based Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03561">http://arxiv.org/abs/2311.03561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Yen Yang, Hsiang-Wei Huang, Zhongyu Jiang, Heng-Cheng Kuo, Jie Mei, Chung-I Huang, Jenq-Neng Hwang</li>
<li>for: 这篇论文主要针对的是用于无人机上的多对目标跟踪（MOT）问题，具体来说是在海上环境中实现稍微检测（ReID）的问题。</li>
<li>methods: 该论文提出了一种适应性 metadata 导航 MOT 算法（MG-MOT），该算法可以将短期跟踪数据融合成一个可靠的长期跟踪数据，并且利用无人机的 GPS 位置、飞行高度和摄像头方向等metadata来帮助实现更好的跟踪效果。</li>
<li>results: 经验表明，该算法在使用 SeaDroneSee 跟踪数据集进行测试时可以 дости得到比较出色的性能，其中 HOTA 达到了 69.5%，IDF1 达到了 85.9%。<details>
<summary>Abstract</summary>
Re-identification (ReID) in multi-object tracking (MOT) for UAVs in maritime computer vision has been challenging for several reasons. More specifically, short-term re-identification (ReID) is difficult due to the nature of the characteristics of small targets and the sudden movement of the drone's gimbal. Long-term ReID suffers from the lack of useful appearance diversity. In response to these challenges, we present an adaptable motion-based MOT algorithm, called Metadata Guided MOT (MG-MOT). This algorithm effectively merges short-term tracking data into coherent long-term tracks, harnessing crucial metadata from UAVs, including GPS position, drone altitude, and camera orientations. Extensive experiments are conducted to validate the efficacy of our MOT algorithm. Utilizing the challenging SeaDroneSee tracking dataset, which encompasses the aforementioned scenarios, we achieve a much-improved performance in the latest edition of the UAV-based Maritime Object Tracking Challenge with a state-of-the-art HOTA of 69.5% and an IDF1 of 85.9% on the testing split.
</details>
<details>
<summary>摘要</summary>
多bject tracking (MOT) for UAVs in maritime computer vision 是一个挑战性的问题，主要是短期重标识 (ReID) 和长期重标识 (ReID) 均存在挑战。特别是，短期ReID 困难由小目标特征的自然特性和飞机摄像头的突然运动所带来。而长期ReID 则受到有用的外观多样性的缺乏的影响。为了应对这些挑战，我们提出了适应性的运动基于MOT算法，称为Metadata驱动MOT（MG-MOT）。这种算法能够将短期跟踪数据与合理的长期跟踪轨迹相结合，利用飞机的GPS位置、高度和摄像头方向等重要metadata。我们对这种MOT算法进行了广泛的实验，以验证其效果。使用 SeaDroneSee 跟踪数据集，该数据集包括以上所述的场景，我们在最新的UAV-based Maritime Object Tracking Challenge中取得了很好的表现，其中HOTA为69.5%，IDF1为85.9%。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Similarity-Measure-based-Multi-Task-Learning-for-Predicting-Alzheimer’s-Disease-Progression-using-MRI-Data"><a href="#Spatio-Temporal-Similarity-Measure-based-Multi-Task-Learning-for-Predicting-Alzheimer’s-Disease-Progression-using-MRI-Data" class="headerlink" title="Spatio-Temporal Similarity Measure based Multi-Task Learning for Predicting Alzheimer’s Disease Progression using MRI Data"></a>Spatio-Temporal Similarity Measure based Multi-Task Learning for Predicting Alzheimer’s Disease Progression using MRI Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03557">http://arxiv.org/abs/2311.03557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xulong Wang, Yu Zhang, Menghui Zhou, Tong Liu, Jun Qi, Po Yang</li>
<li>for: 预测阿尔茨曼病（AD）进程，并且捕捉脑区域之间的相互关系</li>
<li>methods: 基于多任务学习的新尺度 Similarity Measure 方法，能够有效地预测 AD 进程，并捕捉脑区域之间的相互关系</li>
<li>results: 与直接使用 ROI 学习相比，提出的方法更有效地预测疾病进程，并能够进行 longitudinal stability selection，捕捉脑区域之间的变化关系，这些关系在疾病进程中发挥关键作用。<details>
<summary>Abstract</summary>
Identifying and utilising various biomarkers for tracking Alzheimer's disease (AD) progression have received many recent attentions and enable helping clinicians make the prompt decisions. Traditional progression models focus on extracting morphological biomarkers in regions of interest (ROIs) from MRI/PET images, such as regional average cortical thickness and regional volume. They are effective but ignore the relationships between brain ROIs over time, which would lead to synergistic deterioration. For exploring the synergistic deteriorating relationship between these biomarkers, in this paper, we propose a novel spatio-temporal similarity measure based multi-task learning approach for effectively predicting AD progression and sensitively capturing the critical relationships between biomarkers. Specifically, we firstly define a temporal measure for estimating the magnitude and velocity of biomarker change over time, which indicate a changing trend(temporal). Converting this trend into the vector, we then compare this variability between biomarkers in a unified vector space(spatial). The experimental results show that compared with directly ROI based learning, our proposed method is more effective in predicting disease progression. Our method also enables performing longitudinal stability selection to identify the changing relationships between biomarkers, which play a key role in disease progression. We prove that the synergistic deteriorating biomarkers between cortical volumes or surface areas have a significant effect on the cognitive prediction.
</details>
<details>
<summary>摘要</summary>
检测和跟踪阿尔茨海默病（AD）进展的不同生物标志物（biomarker）在Received recent attention and enable clinicians to make prompt decisions. 传统的进展模型将注意力集中在提取ROI（区域 интерес点）的MRI/PET图像中的形态生长指标和区域体积。尽管有效，但忽略了跨时间ROI之间的关系，这会导致相互恶化。为了探索这些生物标志物之间的相互恶化关系，在这篇论文中，我们提出了一种基于多任务学习的新的空间-时间相似度度量方法。 Specifically，我们首先定义了一种时间度量，用于估计生物标志物变化的大小和速度，这些变化指示了时间的趋势（temporal）。将这种趋势转换为向量，然后在一个统一的向量空间中比较这些变化的相互关系。实验结果表明，与直接使用ROI基于学习相比，我们的提议方法更有效地预测疾病进程。我们的方法还允许进行长期稳定选择，以确定变化的关系 между生物标志物，这些关系在疾病进程中扮演着关键的角色。我们证明了脑区表面积或体积之间的相互恶化生物标志物对认知预测具有重要的效果。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-point-annotations-in-segmentation-learning-with-boundary-loss"><a href="#Leveraging-point-annotations-in-segmentation-learning-with-boundary-loss" class="headerlink" title="Leveraging point annotations in segmentation learning with boundary loss"></a>Leveraging point annotations in segmentation learning with boundary loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03537">http://arxiv.org/abs/2311.03537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eva Breznik, Hoel Kervadec, Filip Malmberg, Joel Kullberg, Håkan Ahlström, Marleen de Bruijne, Robin Strand</li>
<li>for: 这个论文研究了在点指导下的semantic segmentation中使用Intensity-based距离地图和边界损失。</li>
<li>methods: 该论文使用了边界损失，以便更好地强制实施false positives的约束。</li>
<li>results: 该论文在ACDC和POEM两个多类数据集上进行了实验，并获得了鼓舞人心的结果。在ACDC数据集上，它超越了基于CRF损失的方法，而在POEM数据集上与CRF损失相当。代码所有实验都公开可用。<details>
<summary>Abstract</summary>
This paper investigates the combination of intensity-based distance maps with boundary loss for point-supervised semantic segmentation. By design the boundary loss imposes a stronger penalty on the false positives the farther away from the object they occur. Hence it is intuitively inappropriate for weak supervision, where the ground truth label may be much smaller than the actual object and a certain amount of false positives (w.r.t. the weak ground truth) is actually desirable. Using intensity-aware distances instead may alleviate this drawback, allowing for a certain amount of false positives without a significant increase to the training loss. The motivation for applying the boundary loss directly under weak supervision lies in its great success for fully supervised segmentation tasks, but also in not requiring extra priors or outside information that is usually required -- in some form -- with existing weakly supervised methods in the literature. This formulation also remains potentially more attractive than existing CRF-based regularizers, due to its simplicity and computational efficiency. We perform experiments on two multi-class datasets; ACDC (heart segmentation) and POEM (whole-body abdominal organ segmentation). Preliminary results are encouraging and show that this supervision strategy has great potential. On ACDC it outperforms the CRF-loss based approach, and on POEM data it performs on par with it. The code for all our experiments is openly available.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="High-resolution-power-equipment-recognition-based-on-improved-self-attention"><a href="#High-resolution-power-equipment-recognition-based-on-improved-self-attention" class="headerlink" title="High-resolution power equipment recognition based on improved self-attention"></a>High-resolution power equipment recognition based on improved self-attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03518">http://arxiv.org/abs/2311.03518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyi Zhang, Cheng Liu, Xiang Li, Xin Zhai, Zhen Wei, Sizhe Li, Xun Ma</li>
<li>For: 提高变压器图像识别精度，应对现有模型参数数量限制。* Methods: 提出了一种基于深度自注意网络的改进方法，包括基础网络、区域提议网络、目标区域提取和 segmentation 模块，以及最终预测网络。* Results: 比较实验表明，该方法在变压器图像识别 task 上表现出色，超过了两种常见目标识别模型的表现，为自动化电气设备检测带来新的思路。<details>
<summary>Abstract</summary>
The current trend of automating inspections at substations has sparked a surge in interest in the field of transformer image recognition. However, due to restrictions in the number of parameters in existing models, high-resolution images can't be directly applied, leaving significant room for enhancing recognition accuracy. Addressing this challenge, the paper introduces a novel improvement on deep self-attention networks tailored for this issue. The proposed model comprises four key components: a foundational network, a region proposal network, a module for extracting and segmenting target areas, and a final prediction network. The innovative approach of this paper differentiates itself by decoupling the processes of part localization and recognition, initially using low-resolution images for localization followed by high-resolution images for recognition. Moreover, the deep self-attention network's prediction mechanism uniquely incorporates the semantic context of images, resulting in substantially improved recognition performance. Comparative experiments validate that this method outperforms the two other prevalent target recognition models, offering a groundbreaking perspective for automating electrical equipment inspections.
</details>
<details>
<summary>摘要</summary>
现在的 substation 自动检测趋势使得transformer 图像识别领域受到了广泛的关注。然而，由于现有模型中参数的限制，高分辨率图像直接应用是不可能的，留下了大量的提高识别精度的空间。为解决这个挑战，本文提出了一种新的深度自注意网络改进方法。该模型包括四个关键组件：基础网络、区域提议网络、目标区域提取和分割模块、最终预测网络。本文的创新方法在分解部分localization和识别两个过程，首先使用低分辨率图像进行localization，然后使用高分辨率图像进行识别。此外，深度自注意网络的预测机制唯一地包含图像的semantic上下文，导致识别性能得到了显著提高。对比试验证明了这种方法在两种常见的目标识别模型之上表现出色，提供了一个创新的自动电气设备检测方法。
</details></li>
</ul>
<hr>
<h2 id="SoundCam-A-Dataset-for-Finding-Humans-Using-Room-Acoustics"><a href="#SoundCam-A-Dataset-for-Finding-Humans-Using-Room-Acoustics" class="headerlink" title="SoundCam: A Dataset for Finding Humans Using Room Acoustics"></a>SoundCam: A Dataset for Finding Humans Using Room Acoustics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03517">http://arxiv.org/abs/2311.03517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mason Wang, Samuel Clarke, Jui-Hsien Wang, Ruohan Gao, Jiajun Wu</li>
<li>for: 这篇论文旨在描述一个室内的声学特性是室内的 geometry、objects和它们的具体位置之间的互动的结果。</li>
<li>methods: 这篇论文使用了声学响应函数（RIR）来描述室内的声学特性，并通过不同的物体位置变化来研究室内的声学特性的变化。</li>
<li>results: 这篇论文提供了5000个真实世界的声学响应函数记录和3000个音乐记录，并在三个不同的房间中（包括一个控制的声学实验室、一个野生的客厅和一个会议室）进行了不同的人员的位置变化。研究表明这些记录可以用于检测和识别人员，以及跟踪他们的位置。<details>
<summary>Abstract</summary>
A room's acoustic properties are a product of the room's geometry, the objects within the room, and their specific positions. A room's acoustic properties can be characterized by its impulse response (RIR) between a source and listener location, or roughly inferred from recordings of natural signals present in the room. Variations in the positions of objects in a room can effect measurable changes in the room's acoustic properties, as characterized by the RIR. Existing datasets of RIRs either do not systematically vary positions of objects in an environment, or they consist of only simulated RIRs. We present SoundCam, the largest dataset of unique RIRs from in-the-wild rooms publicly released to date. It includes 5,000 10-channel real-world measurements of room impulse responses and 2,000 10-channel recordings of music in three different rooms, including a controlled acoustic lab, an in-the-wild living room, and a conference room, with different humans in positions throughout each room. We show that these measurements can be used for interesting tasks, such as detecting and identifying humans, and tracking their positions.
</details>
<details>
<summary>摘要</summary>
Room 的听音性能是由房间的几何结构、房间内的物品和它们的具体位置相互作用而决定的。 Room 的听音性能可以通过源和听者位置之间的冲激响应（RIR）来描述，或者通过房间中自然的信号记录来粗略地推断。 变化房间内物品的位置会导致可观测的改变房间的听音性能，这可以通过 RIR 来证明。现有的数据集中的 RIR  either do not systematically vary positions of objects in an environment, or they consist of only simulated RIRs。我们提出了 SoundCam，迄今为止最大的在野room impulse responses的公共数据集。它包括5000个10通道的真实世界测量室冲激响应和3个不同的房间中的2000个10通道的音乐录音，包括控制的听音实验室、在野的客厦和会议室，每个房间中有不同的人在不同的位置。我们显示这些测量可以用于有趣的任务，如检测和识别人类，并跟踪他们的位置。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Age-from-White-Matter-Diffusivity-with-Residual-Learning"><a href="#Predicting-Age-from-White-Matter-Diffusivity-with-Residual-Learning" class="headerlink" title="Predicting Age from White Matter Diffusivity with Residual Learning"></a>Predicting Age from White Matter Diffusivity with Residual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03500">http://arxiv.org/abs/2311.03500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyu Gao, Michael E. Kim, Ho Hin Lee, Qi Yang, Nazirah Mohd Khairi, Praitayini Kanakaraj, Nancy R. Newlin, Derek B. Archer, Angela L. Jefferson, Warren D. Taylor, Brian D. Boyd, Lori L. Beason-Held, Susan M. Resnick, The BIOCARD Study Team, Yuankai Huo, Katherine D. Van Schaik, Kurt G. Schilling, Daniel Moyer, Ivana Išgum, Bennett A. Landman</li>
<li>for: 这个研究旨在开发白 matter 特定的年龄估计方法，以捕捉不同于正常白 matter 年龄发展的异常。</li>
<li>methods: 这个研究使用了两种不同的方法来预测年龄：一种是提取区域兴趣点中的微结构特征，另一种是使用3D差异神经网络（ResNets）学习直接从图像中的特征。</li>
<li>results: 测试数据上，使用提取微结构特征的方法可以得到平均绝对误差（MAE）为6.11年和6.62年，而使用ResNets学习特征的方法可以得到MAE为4.69年和4.96年。<details>
<summary>Abstract</summary>
Imaging findings inconsistent with those expected at specific chronological age ranges may serve as early indicators of neurological disorders and increased mortality risk. Estimation of chronological age, and deviations from expected results, from structural MRI data has become an important task for developing biomarkers that are sensitive to such deviations. Complementary to structural analysis, diffusion tensor imaging (DTI) has proven effective in identifying age-related microstructural changes within the brain white matter, thereby presenting itself as a promising additional modality for brain age prediction. Although early studies have sought to harness DTI's advantages for age estimation, there is no evidence that the success of this prediction is owed to the unique microstructural and diffusivity features that DTI provides, rather than the macrostructural features that are also available in DTI data. Therefore, we seek to develop white-matter-specific age estimation to capture deviations from normal white matter aging. Specifically, we deliberately disregard the macrostructural information when predicting age from DTI scalar images, using two distinct methods. The first method relies on extracting only microstructural features from regions of interest. The second applies 3D residual neural networks (ResNets) to learn features directly from the images, which are non-linearly registered and warped to a template to minimize macrostructural variations. When tested on unseen data, the first method yields mean absolute error (MAE) of 6.11 years for cognitively normal participants and MAE of 6.62 years for cognitively impaired participants, while the second method achieves MAE of 4.69 years for cognitively normal participants and MAE of 4.96 years for cognitively impaired participants. We find that the ResNet model captures subtler, non-macrostructural features for brain age prediction.
</details>
<details>
<summary>摘要</summary>
医学成像发现与期望的年龄范围不符的现象可能是脑神经疾病的早期指标和增加死亡风险的预示器。测量年龄和与期望结果的差异从结构MRI数据中得到了重要的任务，以开发敏感于这些差异的生物标志物。与结构分析相加，Diffusion Tensor Imaging（DTI）已经证明了在脑白 matter中年龄相关的微结构变化，因此成为了脑年龄预测的有力的其他可能性。虽然早期研究尝试使用DTI来预测年龄，但没有证据表明这种预测的成功归功于DTI提供的特有微结构和diffusivity特征，而不是Macrostructural特征。因此，我们想要开发白 matter特有的年龄预测方法，以捕捉不同于正常白 matter年龄的异常。我们专门忽略了Macrostructural信息，使用两种不同的方法来预测年龄从DTI扁平图像。第一种方法是从区域关心提取微结构特征。第二种方法是使用3D差分神经网络（ResNets）学习图像直接特征，使用非线性对齐和折叠将图像与模板进行非线性对齐和折叠，以最小化Macrostructural变化。当测试在未见数据时，第一种方法的平均绝对误差（MAE）为6.11年，对于正常认知参与者，MAE为6.62年，对于认知障碍参与者。而第二种方法的MAE为4.69年，对于正常认知参与者，MAE为4.96年，对于认知障碍参与者。我们发现ResNet模型可以捕捉更细微、非Macrostructural特征，用于脑年龄预测。
</details></li>
</ul>
<hr>
<h2 id="CoVLM-Composing-Visual-Entities-and-Relationships-in-Large-Language-Models-Via-Communicative-Decoding"><a href="#CoVLM-Composing-Visual-Entities-and-Relationships-in-Large-Language-Models-Via-Communicative-Decoding" class="headerlink" title="CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding"></a>CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03354">http://arxiv.org/abs/2311.03354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UMass-Foundation-Model/CoVLM">https://github.com/UMass-Foundation-Model/CoVLM</a></li>
<li>paper_authors: Junyan Li, Delin Chen, Yining Hong, Zhenfang Chen, Peihao Chen, Yikang Shen, Chuang Gan</li>
<li>for: 提高大型视言语基本模型（VLM）的可 compose 能力，以便更好地理解和生成视语语言。</li>
<li>methods: 提出了一种新的通信标识符，通过动态与视觉检测系统和语言系统之间的交互，使得语言模型能够正确地组合视觉实体和关系。</li>
<li>results: 与先前的 VLM 相比，提高了大约20%的 HICO-DET mAP 分数、14%的 Cola 顶部准确率和3%的 ARO 顶部准确率，并在传统的视语语言任务中达到了状态略优的表现。<details>
<summary>Abstract</summary>
A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make "infinite use of finite means". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their "bag-of-words" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose CoVLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.
</details>
<details>
<summary>摘要</summary>
人类具有惊人的能力，即使用有限的资源来创造无限的表达。然而，目前的大型视言语基本模型（VLM）缺乏这种 Compositional 能力，因为它们的“袋子行为”和无法正确地构成图像元素和关系。为此，我们提出了 CoVLM，可以导引 LLM 进行 Explicit 的视图语言组合，并在视觉Encoder和检测网络之间动态交流来实现视语言交流编码。具体来说，我们首先设计了一组新的通信标识符，以便 LLM 与视觉检测系统之间的动态交流。通信标识符是由 LLM 根据视图元素或关系生成的，以告诉检测网络提出相关的区域。提出的区域关注（ROIs）然后被 fed 回 LLM，以便更好地基于相关区域进行语言生成。LLM 因此可以通过通信标识符来组合视图元素和关系。视觉语言和语言视觉之间的交流是相互进行的，直到整个句子被生成。我们的框架凝聚了视觉感知和 LLM 之间的差距，并在 Compositional 理解标准差（例如 HICO-DET mAP、Cola top-1 准确率和 ARO top-1 准确率）上大幅超越先前的 VLM。我们还在传统的视觉语言任务中实现了状态的最佳表现，如图像描述和视觉问答。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Evaluation-Metrics-of-Open-Vocabulary-Segmentaion"><a href="#Rethinking-Evaluation-Metrics-of-Open-Vocabulary-Segmentaion" class="headerlink" title="Rethinking Evaluation Metrics of Open-Vocabulary Segmentaion"></a>Rethinking Evaluation Metrics of Open-Vocabulary Segmentaion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03352">http://arxiv.org/abs/2311.03352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qqlu/entity">https://github.com/qqlu/entity</a></li>
<li>paper_authors: Hao Zhou, Tiancheng Shen, Xu Yang, Hai Huang, Xiangtai Li, Lu Qi, Ming-Hsuan Yang</li>
<li>for: 本文提出了一个关于开放词汇分割评估方法的问题，即现有评估方法仍然强调关闭集成度metric在零shot或跨数据集管道上，而不考虑预测和实际分类类别之间的相似性。</li>
<li>methods: 本文首先对eleven种语言学统计、文本嵌入和语言模型中的类比度量进行了抽象和用户研究，然后基于这些探索的量建立了一些新的评估指标，包括开放mIoU、开放AP和开放PQ，这些指标适用于三种开放词汇分割任务。</li>
<li>results: 对于12种开放词汇分割方法，我们使用了我们提出的评估指标进行了比较，并证明了我们的指标可以有效评估现有的开放词汇分割方法的开放能力，即使相对的类比度距离存在一定的主观性。<details>
<summary>Abstract</summary>
In this paper, we highlight a problem of evaluation metrics adopted in the open-vocabulary segmentation. That is, the evaluation process still heavily relies on closed-set metrics on zero-shot or cross-dataset pipelines without considering the similarity between predicted and ground truth categories. To tackle this issue, we first survey eleven similarity measurements between two categorical words using WordNet linguistics statistics, text embedding, and language models by comprehensive quantitative analysis and user study. Built upon those explored measurements, we designed novel evaluation metrics, namely Open mIoU, Open AP, and Open PQ, tailored for three open-vocabulary segmentation tasks. We benchmarked the proposed evaluation metrics on 12 open-vocabulary methods of three segmentation tasks. Even though the relative subjectivity of similarity distance, we demonstrate that our metrics can still well evaluate the open ability of the existing open-vocabulary segmentation methods. We hope that our work can bring with the community new thinking about how to evaluate the open ability of models. The evaluation code is released in github.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Long-Term-Invariant-Local-Features-via-Implicit-Cross-Domain-Correspondences"><a href="#Long-Term-Invariant-Local-Features-via-Implicit-Cross-Domain-Correspondences" class="headerlink" title="Long-Term Invariant Local Features via Implicit Cross-Domain Correspondences"></a>Long-Term Invariant Local Features via Implicit Cross-Domain Correspondences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03345">http://arxiv.org/abs/2311.03345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zador Pataki, Mohammad Altillawi, Menelaos Kanakis, Rémi Pautrat, Fengyi Shen, Ziyuan Liu, Luc Van Gool, Marc Pollefeys</li>
<li>for: 本研究旨在 investigating the performance impact of long-term visual domain variations on modern learning-based visual feature extraction networks, and proposing a novel method to improve cross-domain localization performance.</li>
<li>methods: 我们使用 current state-of-the-art feature extraction networks, and conduct a thorough analysis of their performance under various domain changes. We also propose a novel data-centric method, Implicit Cross-Domain Correspondences (iCDC), which utilizes Neural Radiance Fields to generate accurate correspondences across different long-term visual conditions.</li>
<li>results: 我们的 proposed method significantly enhances cross-domain localization performance, reducing the performance gap between intra- and cross-domain localization. When evaluated on popular long-term localization benchmarks, our trained networks consistently outperform existing methods.<details>
<summary>Abstract</summary>
Modern learning-based visual feature extraction networks perform well in intra-domain localization, however, their performance significantly declines when image pairs are captured across long-term visual domain variations, such as different seasonal and daytime variations. In this paper, our first contribution is a benchmark to investigate the performance impact of long-term variations on visual localization. We conduct a thorough analysis of the performance of current state-of-the-art feature extraction networks under various domain changes and find a significant performance gap between intra- and cross-domain localization. We investigate different methods to close this gap by improving the supervision of modern feature extractor networks. We propose a novel data-centric method, Implicit Cross-Domain Correspondences (iCDC). iCDC represents the same environment with multiple Neural Radiance Fields, each fitting the scene under individual visual domains. It utilizes the underlying 3D representations to generate accurate correspondences across different long-term visual conditions. Our proposed method enhances cross-domain localization performance, significantly reducing the performance gap. When evaluated on popular long-term localization benchmarks, our trained networks consistently outperform existing methods. This work serves as a substantial stride toward more robust visual localization pipelines for long-term deployments, and opens up research avenues in the development of long-term invariant descriptors.
</details>
<details>
<summary>摘要</summary>
现代学习基于的视觉特征提取网络在同一个频谱下的本地化表现良好，但是当图像对被捕捉到不同季节和时间变化的长期视觉频谱上时，其表现会有很大下降。在这篇论文中，我们的首要贡献是一个评估长期变化对视本地化表现的影响的benchmark。我们进行了现代特征提取网络在不同频谱下的广泛分析，并发现了跨频谱本地化表现和同频谱本地化表现之间的显著性能差距。我们研究了不同的方法来减少这个差距，包括改进现代特征提取器网络的监督。我们提出了一种数据驱动的方法，即隐式跨频谱对匹配（iCDC）。iCDC使用场景下的多个神经辐射场，每个场景适应不同的视觉频谱，以生成准确的跨频谱对匹配。我们的提议方法可以提高跨频谱本地化表现，显著减少性能差距。当我们的训练网络被评估在知名的长期本地化benchmark上时，它们一直表现出色，超过了现有方法。这种工作为长期部署的更加Robust的视本地化管道奠定了基础，并开启了长期不变描述器的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Cross-Image-Attention-for-Zero-Shot-Appearance-Transfer"><a href="#Cross-Image-Attention-for-Zero-Shot-Appearance-Transfer" class="headerlink" title="Cross-Image Attention for Zero-Shot Appearance Transfer"></a>Cross-Image Attention for Zero-Shot Appearance Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03335">http://arxiv.org/abs/2311.03335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/garibida/cross-image-attention">https://github.com/garibida/cross-image-attention</a></li>
<li>paper_authors: Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, Daniel Cohen-Or</li>
<li>For: The paper is written for transferring the visual appearance between objects that share similar semantics but may differ significantly in shape.* Methods: The paper uses a cross-image attention mechanism that combines the queries corresponding to the structure image with the keys and values of the appearance image during the denoising process. Additionally, the paper uses three mechanisms to manipulate the noisy latent codes or the model’s internal representations throughout the denoising process.* Results: The paper demonstrates that its method is effective across a wide range of object categories and is robust to variations in shape, size, and viewpoint between the two input images.<details>
<summary>Abstract</summary>
Recent advancements in text-to-image generative models have demonstrated a remarkable ability to capture a deep semantic understanding of images. In this work, we leverage this semantic knowledge to transfer the visual appearance between objects that share similar semantics but may differ significantly in shape. To achieve this, we build upon the self-attention layers of these generative models and introduce a cross-image attention mechanism that implicitly establishes semantic correspondences across images. Specifically, given a pair of images -- one depicting the target structure and the other specifying the desired appearance -- our cross-image attention combines the queries corresponding to the structure image with the keys and values of the appearance image. This operation, when applied during the denoising process, leverages the established semantic correspondences to generate an image combining the desired structure and appearance. In addition, to improve the output image quality, we harness three mechanisms that either manipulate the noisy latent codes or the model's internal representations throughout the denoising process. Importantly, our approach is zero-shot, requiring no optimization or training. Experiments show that our method is effective across a wide range of object categories and is robust to variations in shape, size, and viewpoint between the two input images.
</details>
<details>
<summary>摘要</summary>
最近的文本到图像生成模型的进步已经展示了深刻的semantic理解能力。在这项工作中，我们利用这种semantic知识来传递图像中的视觉特征。为此，我们基于这些生成模型中的自注意层，并引入了跨图像注意机制，以便在图像之间建立semantic对应关系。具体来说，给定一对图像——一个显示目标结构的图像和一个指定愿望的外观图像——我们的跨图像注意机制将目标结构图像中的查询与愿望图像中的键和值进行结合。这个操作，当应用于吸吧过程中，利用建立的semantic对应关系来生成一个组合了目标结构和愿望外观的图像。此外，为了提高输出图像质量，我们利用三种机制来操作噪声缺失代码或模型内部表示在吸吧过程中。值得一提的是，我们的方法是零值的，不需要优化或训练。实验表明，我们的方法在对象类型的广泛范围内都是有效的，并且对图像之间的形状、大小和视角的变化具有较高的Robustness。
</details></li>
</ul>
<hr>
<h2 id="TSP-Transformer-Task-Specific-Prompts-Boosted-Transformer-for-Holistic-Scene-Understanding"><a href="#TSP-Transformer-Task-Specific-Prompts-Boosted-Transformer-for-Holistic-Scene-Understanding" class="headerlink" title="TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding"></a>TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03427">http://arxiv.org/abs/2311.03427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tb2-sy/tsp-transformer">https://github.com/tb2-sy/tsp-transformer</a></li>
<li>paper_authors: Shuo Wang, Jing Li, Zibo Zhao, Dongze Lian, Binbin Huang, Xiaomei Wang, Zhengxin Li, Shenghua Gao</li>
<li>for: 本研究旨在提出一种基于任务特定提示的转换器模型，用于实现整体场景理解。</li>
<li>methods: 该模型由一个普通的变换器层和一个任务特定提示变换器层组成，其中任务特定提示变换器层通过增强任务特定的概念来增强模型的任务特定能力。</li>
<li>results: 通过对NYUD-v2和PASCAL-Context datasets进行广泛的实验，我们的方法得到了状态对抗的性能，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Holistic scene understanding includes semantic segmentation, surface normal estimation, object boundary detection, depth estimation, etc. The key aspect of this problem is to learn representation effectively, as each subtask builds upon not only correlated but also distinct attributes. Inspired by visual-prompt tuning, we propose a Task-Specific Prompts Transformer, dubbed TSP-Transformer, for holistic scene understanding. It features a vanilla transformer in the early stage and tasks-specific prompts transformer encoder in the lateral stage, where tasks-specific prompts are augmented. By doing so, the transformer layer learns the generic information from the shared parts and is endowed with task-specific capacity. First, the tasks-specific prompts serve as induced priors for each task effectively. Moreover, the task-specific prompts can be seen as switches to favor task-specific representation learning for different tasks. Extensive experiments on NYUD-v2 and PASCAL-Context show that our method achieves state-of-the-art performance, validating the effectiveness of our method for holistic scene understanding. We also provide our code in the following link https://github.com/tb2-sy/TSP-Transformer.
</details>
<details>
<summary>摘要</summary>
整体场景理解包括semantic segmentation、surface normal估计、物体边界检测、深度估计等多个任务。关键问题在于学习表示效果，因为每个子任务受到不仅相关的还有独特的特征影响。以视觉提示调整为引入，我们提出了任务特定提示变换器（TSP-Transformer） для整体场景理解。它包括普通变换器的早期阶段和后期阶段的任务特定提示变换器 Encoder，其中任务特定提示被增强。通过这种方式，变换层学习了通用信息从共享部分，同时受到任务特定能力的激励。首先，任务特定提示作为各任务的预设效果地启用了变换层。此外，任务特定提示可以看作是为不同任务的表示学习偏好的开关。我们的方法在NYUD-v2和PASCAL-Context上进行了广泛的实验，并达到了当前最佳性能，证明了我们的方法的有效性。我们还提供了我们的代码，请参考以下链接<https://github.com/tb2-sy/TSP-Transformer>。
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Bi-Directional-Algorithm-For-People-Count-In-Crowded-Areas"><a href="#A-Robust-Bi-Directional-Algorithm-For-People-Count-In-Crowded-Areas" class="headerlink" title="A Robust Bi-Directional Algorithm For People Count In Crowded Areas"></a>A Robust Bi-Directional Algorithm For People Count In Crowded Areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03323">http://arxiv.org/abs/2311.03323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyanarayana Penke, Gopikrishna Pavuluri, Soukhya Kunda, Satvik M, CharanKumar Y</li>
<li>for: 这篇论文是为了计算人群流动量而写的。</li>
<li>methods: 这篇论文使用了聚合物探测方法来计算人群流动量。</li>
<li>results: 这篇论文得到了实时人群流动量的计数结果。In English, this translates to:</li>
<li>for: This paper is written for counting the flow of people.</li>
<li>methods: This paper uses blob detection methods to count the flow of people.</li>
<li>results: This paper obtains real-time results of the number of people flowing through a particular area.<details>
<summary>Abstract</summary>
People counting system in crowded places has become a very useful practical application that can be accomplished in various ways which include many traditional methods using sensors. Examining the case of real time scenarios, the algorithm espoused should be steadfast and accurate. People counting algorithm presented in this paper, is centered on blob assessment, devoted to yield the count of the people through a path along with the direction of traversal. The system depicted is often ensconced at the entrance of a building so that the unmitigated frequency of visitors can be recorded. The core premise of this work is to extricate count of people inflow and outflow pertaining to a particular area. The tot-up achieved can be exploited for purpose of statistics in the circumstances of any calamity occurrence in that zone. Relying upon the count totaled, the population in that vicinity can be assimilated in order to take on relevant measures to rescue the people.
</details>
<details>
<summary>摘要</summary>
人Counter系统在拥挤的地方已成为一项非常有用的实用应用，可以通过多种传统方法使用感测器实现。在实时场景中，算法应该坚定稳定，准确。本文所描述的人Counter算法是基于物体评估，用于计算游客通过一条路径的方向和人数。这种系统通常会被安装在建筑物的入口处，以记录进出的游客频率。本研究的核心思想是计算某个区域的人流量，以便在紧急情况下统计人口。根据记录的人数，可以对该区域的人口进行相应的整合，以采取救援措施。
</details></li>
</ul>
<hr>
<h2 id="FATE-Feature-Agnostic-Transformer-based-Encoder-for-learning-generalized-embedding-spaces-in-flow-cytometry-data"><a href="#FATE-Feature-Agnostic-Transformer-based-Encoder-for-learning-generalized-embedding-spaces-in-flow-cytometry-data" class="headerlink" title="FATE: Feature-Agnostic Transformer-based Encoder for learning generalized embedding spaces in flow cytometry data"></a>FATE: Feature-Agnostic Transformer-based Encoder for learning generalized embedding spaces in flow cytometry data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03314">http://arxiv.org/abs/2311.03314</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lisaweijler/fate">https://github.com/lisaweijler/fate</a></li>
<li>paper_authors: Lisa Weijler, Florian Kowarsch, Michael Reiter, Pedro Hermosilla, Margarita Maurer-Granofszky, Michael Dworzak</li>
<li>for:  This paper aims to effectively leverage data with varying features in scenarios where the attributes captured during data acquisition vary across different samples.</li>
<li>methods: The proposed architecture uses a set-transformer architecture augmented by feature-encoder layers to learn a shared latent feature space from data originating from heterogeneous feature spaces.</li>
<li>results: The proposed architecture is demonstrated to operate seamlessly across incongruent feature spaces, particularly relevant in the context of automatic cancer cell detection in acute myeloid leukemia in flow cytometry data, where data scarcity arises from the low prevalence of the disease.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是在不同样本中采集的数据中有变量特征的情况下，有效地利用数据。</li>
<li>methods: 提议的架构使用集成变换器架构和特征编码层来学习不同特征空间中的共享准则空间。</li>
<li>results: 提议的架构在不同特征空间之间操作无缝，特别在某些抑郁白血病细胞检测中， где数据稀缺性由疾病的低发生率导致。<details>
<summary>Abstract</summary>
While model architectures and training strategies have become more generic and flexible with respect to different data modalities over the past years, a persistent limitation lies in the assumption of fixed quantities and arrangements of input features. This limitation becomes particularly relevant in scenarios where the attributes captured during data acquisition vary across different samples. In this work, we aim at effectively leveraging data with varying features, without the need to constrain the input space to the intersection of potential feature sets or to expand it to their union. We propose a novel architecture that can directly process data without the necessity of aligned feature modalities by learning a general embedding space that captures the relationship between features across data samples with varying sets of features. This is achieved via a set-transformer architecture augmented by feature-encoder layers, thereby enabling the learning of a shared latent feature space from data originating from heterogeneous feature spaces. The advantages of the model are demonstrated for automatic cancer cell detection in acute myeloid leukemia in flow cytometry data, where the features measured during acquisition often vary between samples. Our proposed architecture's capacity to operate seamlessly across incongruent feature spaces is particularly relevant in this context, where data scarcity arises from the low prevalence of the disease. The code is available for research purposes at https://github.com/lisaweijler/FATE.
</details>
<details>
<summary>摘要</summary>
而模型的建立和训练策略在过去几年变得更加通用和灵活，对不同数据类型的处理也变得更加容易。然而，一个持续存在的限制是假设输入特征的量和排序是固定的。这个限制在样本中的特征在不同时会变化的场景中特别 relevante。在这项工作中，我们想使用不同特征的数据，不需要固定输入空间为样本之间的交集或者 union。我们提出了一种新的架构，可以直接处理数据，不需要对特征模式进行对齐。这是通过在扩展set-transformer架构中添加特征编码层来实现的，从而学习样本间的共享隐藏特征空间。这些优势在抑制针对恶性白细胞的自动检测中得到了证明，特别是在流细胞分析数据中， где特征的测量通常会在样本之间异常。我们的提出的架构能够无缝地处理不同特征空间，特别在这种疾病的低发生率下，数据的缺乏问题更加突出。代码可以在https://github.com/lisaweijler/FATE上获取 для研究用途。
</details></li>
</ul>
<hr>
<h2 id="A-Single-2D-Pose-with-Context-is-Worth-Hundreds-for-3D-Human-Pose-Estimation"><a href="#A-Single-2D-Pose-with-Context-is-Worth-Hundreds-for-3D-Human-Pose-Estimation" class="headerlink" title="A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation"></a>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03312">http://arxiv.org/abs/2311.03312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/QitaoZhao/ContextAware-PoseFormer">https://github.com/QitaoZhao/ContextAware-PoseFormer</a></li>
<li>paper_authors: Qitao Zhao, Ce Zheng, Mengyuan Liu, Chen Chen</li>
<li>for: 提高3D人体pose估计精度，不需要大量视频帧</li>
<li>methods: 利用 pré-trained 2D pose检测器生成的中间视觉表示，不需要精度调整</li>
<li>results: 与Context-Aware PoseFormer比较，无需使用视频帧，可以达到更高的速度和精度<details>
<summary>Abstract</summary>
The dominant paradigm in 3D human pose estimation that lifts a 2D pose sequence to 3D heavily relies on long-term temporal clues (i.e., using a daunting number of video frames) for improved accuracy, which incurs performance saturation, intractable computation and the non-causal problem. This can be attributed to their inherent inability to perceive spatial context as plain 2D joint coordinates carry no visual cues. To address this issue, we propose a straightforward yet powerful solution: leveraging the readily available intermediate visual representations produced by off-the-shelf (pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed. The key observation is that, while the pose detector learns to localize 2D joints, such representations (e.g., feature maps) implicitly encode the joint-centric spatial context thanks to the regional operations in backbone networks. We design a simple baseline named Context-Aware PoseFormer to showcase its effectiveness. Without access to any temporal information, the proposed method significantly outperforms its context-agnostic counterpart, PoseFormer, and other state-of-the-art methods using up to hundreds of video frames regarding both speed and precision. Project page: https://qitaozhao.github.io/ContextAware-PoseFormer
</details>
<details>
<summary>摘要</summary>
dominant 模式在三维人姿估算中，将二维姿势序列提升到三维的强调很重视长期时间凭据（即使用大量视频帧），从而导致性能峰值、不可持续计算和非因果问题。这可以归结于它们的内置无法感知空间上下文的问题，因为平面的二维关节坐标不含视觉提示。为解决这个问题，我们提出了一个简单 yet powerful的解决方案：利用可用的 intermediate visual representations 生成的 off-the-shelf (预训练) 二维姿势检测器生成的可用性。关键观察是，虽然姿势检测器学习到了二维关节的位置，但这些表示（例如特征图）在后向网络中的区域操作中隐式地编码了关节-中心空间上下文。我们设计了一个简单的基线方案，名为 Context-Aware PoseFormer，以示其效iveness。无需访问任何时间信息，我们的提案significantly outperform其上下文无关的对手，以及使用Up to hundreds of video frames的其他现状顶峰方法。项目页面：https://qitaozhao.github.io/ContextAware-PoseFormer
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Based-Tea-Leaf-Disease-Detection-A-Comprehensive-Review"><a href="#Machine-Learning-Based-Tea-Leaf-Disease-Detection-A-Comprehensive-Review" class="headerlink" title="Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive Review"></a>Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03240">http://arxiv.org/abs/2311.03240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faruk Ahmed, Md. Taimur Ahad, Yousuf Rayhan Emon</li>
<li>for: 本研究旨在对茶叶病虫的早期检测和诊断进行机器学习方法的系统性回顾。</li>
<li>methods: 本文回顾了各种机器学习模型，包括Inception Convolutional Vision Transformer (ICVT)、GreenViT、PlantXViT、PlantViT、MSCVT、Transfer Learning Model &amp; Vision Transformer (TLMViT)、IterationViT、IEM-ViT等。此外，还回顾了 dense convolutional network (DenseNet)、Residual Neural Network (ResNet)-50V2、YOLOv5、YOLOv7、Convolutional Neural Network (CNN)、Deep CNN、Non-dominated Sorting Genetic Algorithm (NSGA-II)、MobileNetv2等模型。</li>
<li>results: 本文通过对各种数据集进行测试，证明了这些机器学习模型在实际应用中的可行性。<details>
<summary>Abstract</summary>
Tea leaf diseases are a major challenge to agricultural productivity, with far-reaching implications for yield and quality in the tea industry. The rise of machine learning has enabled the development of innovative approaches to combat these diseases. Early detection and diagnosis are crucial for effective crop management. For predicting tea leaf disease, several automated systems have already been developed using different image processing techniques. This paper delivers a systematic review of the literature on machine learning methodologies applied to diagnose tea leaf disease via image classification. It thoroughly evaluates the strengths and constraints of various Vision Transformer models, including Inception Convolutional Vision Transformer (ICVT), GreenViT, PlantXViT, PlantViT, MSCVT, Transfer Learning Model & Vision Transformer (TLMViT), IterationViT, IEM-ViT. Moreover, this paper also reviews models like Dense Convolutional Network (DenseNet), Residual Neural Network (ResNet)-50V2, YOLOv5, YOLOv7, Convolutional Neural Network (CNN), Deep CNN, Non-dominated Sorting Genetic Algorithm (NSGA-II), MobileNetv2, and Lesion-Aware Visual Transformer. These machine-learning models have been tested on various datasets, demonstrating their real-world applicability. This review study not only highlights current progress in the field but also provides valuable insights for future research directions in the machine learning-based detection and classification of tea leaf diseases.
</details>
<details>
<summary>摘要</summary>
茶叶疾病是现代农业生产的主要挑战，对茶业产量和质量有广泛的影响。随着机器学习的发展，开发了一些创新的方法来控制这些疾病。早期检测和诊断是有效管理茶叶疾病的关键。为预测茶叶疾病，已经开发了一些自动化系统，使用不同的图像处理技术。本文提供了一种系统性的文献评审，检索了不同的机器学习方法，包括启发征值网络（ICVT）、绿色值网络（GreenViT）、植物值网络（PlantXViT）、植物值网络（PlantViT）、多种特征值网络（MSCVT）、传输学习模型与视觉Transformer（TLMViT）、迭代值网络（IterationViT）、IEM-ViT等。此外，本文还评估了各种模型，如密集卷积网络（DenseNet）、径回神经网络（ResNet）-50V2、YOLOv5、YOLOv7、图像卷积神经网络（CNN）、深度CNN、非主导排序遗传算法（NSGA-II）、MobileNetv2、损论值网络（Lesion-Aware）。这些机器学习模型在不同的数据集上进行测试，表明了它们在实际应用中的可行性。本评估研究不仅总结了当前领域的进展，还为未来机器学习基于茶叶疾病检测和分类的研究提供了有价值的思路。
</details></li>
</ul>
<hr>
<h2 id="Navigating-Scaling-Laws-Accelerating-Vision-Transformer’s-Training-via-Adaptive-Strategies"><a href="#Navigating-Scaling-Laws-Accelerating-Vision-Transformer’s-Training-via-Adaptive-Strategies" class="headerlink" title="Navigating Scaling Laws: Accelerating Vision Transformer’s Training via Adaptive Strategies"></a>Navigating Scaling Laws: Accelerating Vision Transformer’s Training via Adaptive Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03233">http://arxiv.org/abs/2311.03233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sotiris Anagnostidis, Gregor Bachmann, Thomas Hofmann</li>
<li>for: 这篇论文目的是提出一种可以适应不同 Compute 水平的深度学习模型，以便在不同的训练 dataset 上实现最佳性能。</li>
<li>methods: 这篇论文使用了一种名为“adaptive model”的方法，允许模型在训练过程中改变其形状，以便根据不同的 Compute 水平来实现最佳性能。</li>
<li>results: 论文的结果显示，使用这种 adaptive model 可以比传统的“静态”模型（static model）更好地适应不同 Compute 水平，并在视觉任务上实现更高的性能。<details>
<summary>Abstract</summary>
In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: Investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a "compute-optimal" model, i.e. a model that allocates a given level of compute during training optimally to maximise performance. In this work, we extend the concept of optimality by allowing for an "adaptive" model, i.e. a model that can change its shape during the course of training. By allowing the shape to adapt, we can optimally traverse between the underlying scaling laws, leading to a significant reduction in the required compute to reach a given target performance. We focus on vision tasks and the family of Vision Transformers, where the patch size as well as the width naturally serve as adaptive shape parameters. We demonstrate that, guided by scaling laws, we can design compute-optimal adaptive models that beat their "static" counterparts.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近年来，深度学习的州前掌握被庞大的模型所主导，这些模型在大量数据上进行预训练。这种方法的核心思想是：投入更多的计算资源（优化）会导致性能更好，甚至可预测性；神经网络的扩展法律已经 derivation 了可以准确预测模型在给定的计算水平下的性能。这导致了“计算优化”模型的概念，即在训练期间优化计算资源以 Maximize 性能。在这个工作中，我们将 extend 计算优化的概念，允许模型在训练过程中改变其形态。通过允许形态改变，我们可以优化地 traverse  между神经网络的下面各个缩放法律，从而减少到达目标性能所需的计算量。我们将注意力集中在视觉任务上，以及家族中的视觉变换器，其中补丁大小以及宽度自然成为 adaptive 形态参数。我们示示，根据缩放法律指导，我们可以设计计算优化的 adaptive 模型， beat 其“静止”对手。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-of-Drone-Collision-Hazards-in-Airborne-RADAR-Point-Clouds-Using-PointNet"><a href="#Segmentation-of-Drone-Collision-Hazards-in-Airborne-RADAR-Point-Clouds-Using-PointNet" class="headerlink" title="Segmentation of Drone Collision Hazards in Airborne RADAR Point Clouds Using PointNet"></a>Segmentation of Drone Collision Hazards in Airborne RADAR Point Clouds Using PointNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03221">http://arxiv.org/abs/2311.03221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Arroyo, Paul Kier, Dylan Angus, Santiago Matalonga, Svetlozar Georgiev, Mehdi Goli, Gerard Dooly, James Riordan</li>
<li>for: 这个研究旨在将无人航空器（UAV）integre into shared airspace for beyond visual line of sight（BVLOS）操作中提供更高度的 situational awareness，以确保安全的运作。</li>
<li>methods: 本研究使用激光技术进行 novel end-to-end semantic segmentation of aerial point clouds，以同时识别多个 Collision Hazards。研究人员运用和优化PointNet架构，并integrate aerial domain insights，以分别识别机动无人机（DJI M300和DJI Mini）和机场（Ikarus C42），以及静止返回（地面和基础设施）。</li>
<li>results: 本研究获得了robust 94%的准确率，成功地同时识别多个 Collision Hazards。这项研究显示激光技术在UAV situational awareness中的应用潜力，并促进了安全和效率的BVLOS操作。<details>
<summary>Abstract</summary>
The integration of unmanned aerial vehicles (UAVs) into shared airspace for beyond visual line of sight (BVLOS) operations presents significant challenges but holds transformative potential for sectors like transportation, construction, energy and defense. A critical prerequisite for this integration is equipping UAVs with enhanced situational awareness to ensure safe operations. Current approaches mainly target single object detection or classification, or simpler sensing outputs that offer limited perceptual understanding and lack the rapid end-to-end processing needed to convert sensor data into safety-critical insights. In contrast, our study leverages radar technology for novel end-to-end semantic segmentation of aerial point clouds to simultaneously identify multiple collision hazards. By adapting and optimizing the PointNet architecture and integrating aerial domain insights, our framework distinguishes five distinct classes: mobile drones (DJI M300 and DJI Mini) and airplanes (Ikarus C42), and static returns (ground and infrastructure) which results in enhanced situational awareness for UAVs. To our knowledge, this is the first approach addressing simultaneous identification of multiple collision threats in an aerial setting, achieving a robust 94% accuracy. This work highlights the potential of radar technology to advance situational awareness in UAVs, facilitating safe and efficient BVLOS operations.
</details>
<details>
<summary>摘要</summary>
integrating unmanned aerial vehicles (UAVs) into shared airspace for beyond visual line of sight (BVLOS) operations poses significant challenges, but it also has transformative potential for sectors like transportation, construction, energy, and defense. a critical prerequisite for this integration is equipping UAVs with enhanced situational awareness to ensure safe operations. current approaches mainly focus on single object detection or classification, or simpler sensing outputs that offer limited perceptual understanding and lack the rapid end-to-end processing needed to convert sensor data into safety-critical insights. in contrast, our study leverages radar technology for novel end-to-end semantic segmentation of aerial point clouds to simultaneously identify multiple collision hazards. by adapting and optimizing the PointNet architecture and integrating aerial domain insights, our framework distinguishes five distinct classes: mobile drones (DJI M300 and DJI Mini) and airplanes (Ikarus C42), and static returns (ground and infrastructure) which results in enhanced situational awareness for UAVs. to our knowledge, this is the first approach addressing simultaneous identification of multiple collision threats in an aerial setting, achieving a robust 94% accuracy. this work highlights the potential of radar technology to advance situational awareness in UAVs, facilitating safe and efficient BVLOS operations.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Transformers-to-Improve-Breast-Cancer-Classification-and-Risk-Assessment-with-Multi-modal-and-Longitudinal-Data"><a href="#Leveraging-Transformers-to-Improve-Breast-Cancer-Classification-and-Risk-Assessment-with-Multi-modal-and-Longitudinal-Data" class="headerlink" title="Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data"></a>Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03217">http://arxiv.org/abs/2311.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqiu Shen, Jungkyu Park, Frank Yeung, Eliana Goldberg, Laura Heacock, Farah Shamout, Krzysztof J. Geras</li>
<li>for: 这个研究旨在验证多modal imaging 技术，以帮助预后预测乳腺癌的可能性。</li>
<li>methods: 这个研究使用了多modal transformer（MMT），一种神经网络，将� Mammo graphy 和 ultrasound 资料融合，以提高癌症检测和预后预测的精度。</li>
<li>results: 在130万个检查数据中，MMT 获得了AUROC 0.943，surpassing strong uni-modal baselines，并且在5年的预后预测中，MMT 获得了AUROC 0.826，outperforming prior mammography-based risk models。<details>
<summary>Abstract</summary>
Breast cancer screening, primarily conducted through mammography, is often supplemented with ultrasound for women with dense breast tissue. However, existing deep learning models analyze each modality independently, missing opportunities to integrate information across imaging modalities and time. In this study, we present Multi-modal Transformer (MMT), a neural network that utilizes mammography and ultrasound synergistically, to identify patients who currently have cancer and estimate the risk of future cancer for patients who are currently cancer-free. MMT aggregates multi-modal data through self-attention and tracks temporal tissue changes by comparing current exams to prior imaging. Trained on 1.3 million exams, MMT achieves an AUROC of 0.943 in detecting existing cancers, surpassing strong uni-modal baselines. For 5-year risk prediction, MMT attains an AUROC of 0.826, outperforming prior mammography-based risk models. Our research highlights the value of multi-modal and longitudinal imaging in cancer diagnosis and risk stratification.
</details>
<details>
<summary>摘要</summary>
乳癌检测通常通过胸部X射线扫描进行，但现有的深度学习模型通常只分析每种成像Modal separately， missed opportunities to integrate信息 across imaging modalities and time。在这项研究中，我们介绍了多Modal Transformer（MMT），一种神经网络，利用胸部X射线和ultrasound synergistically，用于识别当前有 cancer 的患者和估计当前没有 cancer 的患者将来癌症风险。MMT通过自注意力和比较当前检测和前一次检测来聚合多modal数据，并跟踪 temporal tissue changes。我们在130万次检测数据上训练MMT，其AUROC为0.943，超过了强的uni-modal基线。此外，MMT在5年风险预测方面的AUROC为0.826，超过了以往基于胸部X射线的风险模型。我们的研究强调了多modal和长期成像在肿瘤诊断和风险分级中的价值。
</details></li>
</ul>
<hr>
<h2 id="PainSeeker-An-Automated-Method-for-Assessing-Pain-in-Rats-Through-Facial-Expressions"><a href="#PainSeeker-An-Automated-Method-for-Assessing-Pain-in-Rats-Through-Facial-Expressions" class="headerlink" title="PainSeeker: An Automated Method for Assessing Pain in Rats Through Facial Expressions"></a>PainSeeker: An Automated Method for Assessing Pain in Rats Through Facial Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03205">http://arxiv.org/abs/2311.03205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Liu, Guang Li, Dingfan Deng, Jinhua Yu, Yuan Zong</li>
<li>for: 研究是用来检测实验室老鼠的痛苦吗？</li>
<li>methods: 使用一种名为PainSeeker的深度学习方法，通过脸部表情图像来自动评估老鼠的痛苦程度。</li>
<li>results: 研究表明，可以通过脸部表情图像来评估实验室老鼠的痛苦程度，并且PainSeeker方法可以有效地解决这个问题。<details>
<summary>Abstract</summary>
In this letter, we aim to investigate whether laboratory rats' pain can be automatically assessed through their facial expressions. To this end, we began by presenting a publicly available dataset called RatsPain, consisting of 1,138 facial images captured from six rats that underwent an orthodontic treatment operation. Each rat' facial images in RatsPain were carefully selected from videos recorded either before or after the operation and well labeled by eight annotators according to the Rat Grimace Scale (RGS). We then proposed a novel deep learning method called PainSeeker for automatically assessing pain in rats via facial expressions. PainSeeker aims to seek pain-related facial local regions that facilitate learning both pain discriminative and head pose robust features from facial expression images. To evaluate the PainSeeker, we conducted extensive experiments on the RatsPain dataset. The results demonstrate the feasibility of assessing rats' pain from their facial expressions and also verify the effectiveness of the proposed PainSeeker in addressing this emerging but intriguing problem. The RasPain dataset can be freely obtained from https://github.com/xhzongyuan/RatsPain.
</details>
<details>
<summary>摘要</summary>
《这封信中，我们想 investigating  Whether laboratory rats' pain can be automatically assessed through their facial expressions. To this end, we began by presenting a publicly available dataset called RatsPain, consisting of 1,138 facial images captured from six rats that underwent an orthodontic treatment operation. Each rat's facial images in RatsPain were carefully selected from videos recorded either before or after the operation and well labeled by eight annotators according to the Rat Grimace Scale (RGS). We then proposed a novel deep learning method called PainSeeker for automatically assessing pain in rats via facial expressions. PainSeeker aims to seek pain-related facial local regions that facilitate learning both pain discriminative and head pose robust features from facial expression images. To evaluate the PainSeeker, we conducted extensive experiments on the RatsPain dataset. The results demonstrate the feasibility of assessing rats' pain from their facial expressions and also verify the effectiveness of the proposed PainSeeker in addressing this emerging but intriguing problem. The RasPain dataset can be freely obtained from https://github.com/xhzongyuan/RatsPain.》Here's the word-for-word translation:“这封信中，我们想 investigating  Whether laboratory rats' pain can be automatically assessed through their facial expressions. To this end, we began by presenting a publicly available dataset called RatsPain, consisting of 1,138 facial images captured from six rats that underwent an orthodontic treatment operation. Each rat's facial images in RatsPain were carefully selected from videos recorded either before or after the operation and well labeled by eight annotators according to the Rat Grimace Scale (RGS). We then proposed a novel deep learning method called PainSeeker for automatically assessing pain in rats via facial expressions. PainSeeker aims to seek pain-related facial local regions that facilitate learning both pain discriminative and head pose robust features from facial expression images. To evaluate the PainSeeker, we conducted extensive experiments on the RatsPain dataset. The results demonstrate the feasibility of assessing rats' pain from their facial expressions and also verify the effectiveness of the proposed PainSeeker in addressing this emerging but intriguing problem. The RasPain dataset can be freely obtained from https://github.com/xhzongyuan/RatsPain.”
</details></li>
</ul>
<hr>
<h2 id="LCPR-A-Multi-Scale-Attention-Based-LiDAR-Camera-Fusion-Network-for-Place-Recognition"><a href="#LCPR-A-Multi-Scale-Attention-Based-LiDAR-Camera-Fusion-Network-for-Place-Recognition" class="headerlink" title="LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for Place Recognition"></a>LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03198">http://arxiv.org/abs/2311.03198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhouZijie77/LCPR">https://github.com/ZhouZijie77/LCPR</a></li>
<li>paper_authors: Zijie Zhou, Jingyi Xu, Guangming Xiong, Junyi Ma</li>
<li>for: 提高自动驾驶车辆在GPS无效环境中识别环境的精度。</li>
<li>methods: 利用多模态感知器件的拟合技术，将LiDAR点云和多视图RGB图像 fusion 为特征表示。</li>
<li>results: 在nuScenes数据集上测试，我们的方法可以有效地利用多视图摄像头和LiDAR数据进行环境识别，同时具有强大的抗视点变化性。<details>
<summary>Abstract</summary>
Place recognition is one of the most crucial modules for autonomous vehicles to identify places that were previously visited in GPS-invalid environments. Sensor fusion is considered an effective method to overcome the weaknesses of individual sensors. In recent years, multimodal place recognition fusing information from multiple sensors has gathered increasing attention. However, most existing multimodal place recognition methods only use limited field-of-view camera images, which leads to an imbalance between features from different modalities and limits the effectiveness of sensor fusion. In this paper, we present a novel neural network named LCPR for robust multimodal place recognition, which fuses LiDAR point clouds with multi-view RGB images to generate discriminative and yaw-rotation invariant representations of the environment. A multi-scale attention-based fusion module is proposed to fully exploit the panoramic views from different modalities of the environment and their correlations. We evaluate our method on the nuScenes dataset, and the experimental results show that our method can effectively utilize multi-view camera and LiDAR data to improve the place recognition performance while maintaining strong robustness to viewpoint changes. Our open-source code and pre-trained models are available at https://github.com/ZhouZijie77/LCPR .
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CN Place recognition is one of the most crucial modules for autonomous vehicles to identify places that were previously visited in GPS-invalid environments. Sensor fusion is considered an effective method to overcome the weaknesses of individual sensors. In recent years, multimodal place recognition fusing information from multiple sensors has gathered increasing attention. However, most existing multimodal place recognition methods only use limited field-of-view camera images, which leads to an imbalance between features from different modalities and limits the effectiveness of sensor fusion. In this paper, we present a novel neural network named LCPR for robust multimodal place recognition, which fuses LiDAR point clouds with multi-view RGB images to generate discriminative and yaw-rotation invariant representations of the environment. A multi-scale attention-based fusion module is proposed to fully exploit the panoramic views from different modalities of the environment and their correlations. We evaluate our method on the nuScenes dataset, and the experimental results show that our method can effectively utilize multi-view camera and LiDAR data to improve the place recognition performance while maintaining strong robustness to viewpoint changes. Our open-source code and pre-trained models are available at https://github.com/ZhouZijie77/LCPR .<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Learning-using-Data-Augmentation-and-Time-Frequency-Transformation-for-Time-Series-Classification"><a href="#Few-shot-Learning-using-Data-Augmentation-and-Time-Frequency-Transformation-for-Time-Series-Classification" class="headerlink" title="Few-shot Learning using Data Augmentation and Time-Frequency Transformation for Time Series Classification"></a>Few-shot Learning using Data Augmentation and Time-Frequency Transformation for Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03194">http://arxiv.org/abs/2311.03194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Zhendong Pang, Jiangpeng Wang, Teng Li</li>
<li>for: 这 paper 的目的是解决时间序列分类任务中的少量数据问题。</li>
<li>methods: 该 paper 提出了一种基于数据扩展的新少量学习框架，包括时域频域转换和随机磁化生成的 synthetic 图像。此外，paper 还提出了一种序列spectrogram神经网络模型，该模型包括一个使用 1D 径向块 Extract 特征的子网络和另一个使用 2D 径向块 Extract 特征的spectrogram 表示。</li>
<li>results: 在一个amyotrophic lateral sclerosis 数据集和一个风力机磨碎问题数据集上进行了对比研究，结果显示，我们提posed 方法可以达到 93.75% F1 分数和 93.33% 准确率在amyotrophic lateral sclerosis 数据集上，以及 95.48% F1 分数和 95.59% 准确率在风力机磨碎问题数据集上。这表明了我们的方法可以有效地解决时间序列分类中的少量数据问题。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) that tackle the time series classification (TSC) task have provided a promising framework in signal processing. In real-world applications, as a data-driven model, DNNs are suffered from insufficient data. Few-shot learning has been studied to deal with this limitation. In this paper, we propose a novel few-shot learning framework through data augmentation, which involves transformation through the time-frequency domain and the generation of synthetic images through random erasing. Additionally, we develop a sequence-spectrogram neural network (SSNN). This neural network model composes of two sub-networks: one utilizing 1D residual blocks to extract features from the input sequence while the other one employing 2D residual blocks to extract features from the spectrogram representation. In the experiments, comparison studies of different existing DNN models with/without data augmentation are conducted on an amyotrophic lateral sclerosis (ALS) dataset and a wind turbine fault (WTF) dataset. The experimental results manifest that our proposed method achieves 93.75% F1 score and 93.33% accuracy on the ALS datasets while 95.48% F1 score and 95.59% accuracy on the WTF datasets. Our methodology demonstrates its applicability of addressing the few-shot problems for time series classification.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在时间序列分类任务中提供了一个有前途的框架，在实际应用中，作为数据驱动模型，DNNs受到有限的数据的限制。少数例学习已经被研究以解决这种限制。在这篇论文中，我们提出了一种新的少数例学习框架，通过数据扩充来实现，包括在时间频域和频谱域中进行变换，以及随机缺失生成的synthetic图像生成。此外，我们开发了一种序列spectrogram神经网络（SSNN）。这个神经网络模型由两个子网络组成：一个使用1D径向块来提取输入序列中的特征，另一个使用2D径向块来提取spectrogram表示中的特征。在实验中，我们对不同的现有DNN模型进行了与/无数据扩充的比较研究，并在amyotrophic lateral sclerosis（ALS）数据集和风力机缺陷（WTF）数据集上进行了实验。实验结果表明，我们提出的方法可以在时间序列分类 task中解决少数例学习问题，并达到了93.75%的F1分数和93.33%的准确率在ALS数据集上，以及95.48%的F1分数和95.59%的准确率在WTF数据集上。
</details></li>
</ul>
<hr>
<h2 id="Efficient-and-Low-Footprint-Object-Classification-using-Spatial-Contrast"><a href="#Efficient-and-Low-Footprint-Object-Classification-using-Spatial-Contrast" class="headerlink" title="Efficient and Low-Footprint Object Classification using Spatial Contrast"></a>Efficient and Low-Footprint Object Classification using Spatial Contrast</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03422">http://arxiv.org/abs/2311.03422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Belding, Daniel C. Stumpp, Rajkumar Kubendran</li>
<li>for: 这个研究探讨了一种基于本地空间差异（SC）的事件驱动视觉传感器，并使用了两种不同的阈值技术来实现。</li>
<li>methods: 研究人员使用了论述模拟器来研究这种硬件传感器的性能，并使用了德国交通标志图像集（GTSRB）和知名的深度神经网络（DNN）进行车道标志分类。</li>
<li>results: 研究结果表明，使用空间差异可以有效地捕捉图像中关键的特征，并且可以使用一个简化的二进制神经网络（Binarized MicronNet）来实现高达94.4%的F1分数，相比之下，使用高精度RGB图像和DNN，只能达到56.3%的F1分数。这表明，SC在资源受限的边缘计算环境中具有极大的推荐性。<details>
<summary>Abstract</summary>
Event-based vision sensors traditionally compute temporal contrast that offers potential for low-power and low-latency sensing and computing. In this research, an alternative paradigm for event-based sensors using localized spatial contrast (SC) under two different thresholding techniques, relative and absolute, is investigated. Given the slow maturity of spatial contrast in comparison to temporal-based sensors, a theoretical simulated output of such a hardware sensor is explored. Furthermore, we evaluate traffic sign classification using the German Traffic Sign dataset (GTSRB) with well-known Deep Neural Networks (DNNs). This study shows that spatial contrast can effectively capture salient image features needed for classification using a Binarized DNN with significant reduction in input data usage (at least 12X) and memory resources (17.5X), compared to high precision RGB images and DNN, with only a small loss (~2%) in macro F1-score. Binarized MicronNet achieves an F1-score of 94.4% using spatial contrast, compared to only 56.3% when using RGB input images. Thus, SC offers great promise for deployment in power and resource constrained edge computing environments.
</details>
<details>
<summary>摘要</summary>
事件基于视觉传感器传统上计算时间对比，具有低功耗和低延迟感知和计算的潜在潜力。本研究提出了基于地方化空间对比（SC）的事件基于传感器的一种 alternativa  парадиг，并通过两种不同的阈值技术（相对和绝对）进行调查。由于空间对比 slower 于时间基于传感器，我们使用理论模拟的硬件传感器输出进行研究。此外，我们还使用德国交通标志数据集（GTSRB）和知名的深度神经网络（DNN）进行评估交通标志分类。研究结果表明，使用空间对比可以有效地捕捉图像中重要的特征，并使用 binarized DNN 对输入数据进行压缩，从而降低输入数据的使用量（至少12倍）和内存资源（17.5倍），与高精度RGB图像和 DNN 相比，只有 slight loss（约2%）的macro F1 分数。使用空间对比，Binarized MicronNet 的 F1 分数达94.4%，与使用 RGB 输入图像的56.3%相比，SC 对于具有限制的边缘计算环境而言，具有极大的承诺。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-Decomposition-Translation-for-Enhanced-Medical-Image-Translation-Using-GANs"><a href="#Frequency-Domain-Decomposition-Translation-for-Enhanced-Medical-Image-Translation-Using-GANs" class="headerlink" title="Frequency Domain Decomposition Translation for Enhanced Medical Image Translation Using GANs"></a>Frequency Domain Decomposition Translation for Enhanced Medical Image Translation Using GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03175">http://arxiv.org/abs/2311.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuhui Wang, Jianwei Zuo, Xuliang Deng, Jiajia Luo</li>
<li>for: 这篇论文的目的是提出一种新的医学图像转换方法，以提高现有的GAN基于方法的图像质量。</li>
<li>methods: 该方法称为频域分解翻译（FDDT），它将原始图像分解成高频成分和低频成分，其中高频成分包含图像细节和特征信息，而低频成分包含图像风格信息。然后，将高频和低频分量的转换结果与原始图像的高频和低频分量进行对齐，以保持图像的标识信息，同时尽量避免风格信息的损害。</li>
<li>results: 作者对MRI图像和自然图像进行了广泛的实验，并使用了多种主流基eline模型进行比较。结果显示，FDDT可以在四个评价指标中提高图像质量，比如Fr&#39;echet医学分割距离、结构相似度、峰值信号噪声比和平均差异。相比主流基eline模型，FDDT可以降低Fr&#39;echet医学分割距离、结构相似度、峰值信号噪声比和平均差异的值，最大降低值分别为24.4%, 4.4%, 5.8%和31%.<details>
<summary>Abstract</summary>
Medical Image-to-image translation is a key task in computer vision and generative artificial intelligence, and it is highly applicable to medical image analysis. GAN-based methods are the mainstream image translation methods, but they often ignore the variation and distribution of images in the frequency domain, or only take simple measures to align high-frequency information, which can lead to distortion and low quality of the generated images. To solve these problems, we propose a novel method called frequency domain decomposition translation (FDDT). This method decomposes the original image into a high-frequency component and a low-frequency component, with the high-frequency component containing the details and identity information, and the low-frequency component containing the style information. Next, the high-frequency and low-frequency components of the transformed image are aligned with the transformed results of the high-frequency and low-frequency components of the original image in the same frequency band in the spatial domain, thus preserving the identity information of the image while destroying as little stylistic information of the image as possible. We conduct extensive experiments on MRI images and natural images with FDDT and several mainstream baseline models, and we use four evaluation metrics to assess the quality of the generated images. Compared with the baseline models, optimally, FDDT can reduce Fr\'echet inception distance by up to 24.4%, structural similarity by up to 4.4%, peak signal-to-noise ratio by up to 5.8%, and mean squared error by up to 31%. Compared with the previous method, optimally, FDDT can reduce Fr\'echet inception distance by up to 23.7%, structural similarity by up to 1.8%, peak signal-to-noise ratio by up to 6.8%, and mean squared error by up to 31.6%.
</details>
<details>
<summary>摘要</summary>
医学图像转换是计算机视觉和生成人工智能领域的关键任务，具有广泛的应用前景。GAN基于方法是主流图像转换方法，但它们经常忽视图像在频率域中的变化和分布，或者只是进行简单的高频信息对齐，这可能导致图像生成的质量低下。为解决这些问题，我们提出了一种新的方法 called frequency domain decomposition translation (FDDT)。这种方法将原始图像分解成高频组件和低频组件，其中高频组件包含细节和标识信息，而低频组件包含风格信息。然后，将高频和低频组件的转换结果与原始图像的高频和低频组件在同一频率域的空间域进行对齐，以保持图像的标识信息，同时尽量保持图像的风格信息。我们对MRI图像和自然图像进行了广泛的实验，并使用了四个评价指标来评估生成图像的质量。与基eline模型相比，FDDT可以降低Fréchet吸引距离、结构相似度、峰值信号噪声比和平均平方误差，分别降低24.4%、4.4%、5.8%和31%。与之前的方法相比，FDDT可以降低Fréchet吸引距离23.7%、结构相似度1.8%、峰值信号噪声比6.8%和31.6%。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-Masked-Distillation-for-Pre-Training-Small-Foundation-Models"><a href="#Asymmetric-Masked-Distillation-for-Pre-Training-Small-Foundation-Models" class="headerlink" title="Asymmetric Masked Distillation for Pre-Training Small Foundation Models"></a>Asymmetric Masked Distillation for Pre-Training Small Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03149">http://arxiv.org/abs/2311.03149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Zhao, Bingkun Huang, Sen Xing, Gangshan Wu, Yu Qiao, Limin Wang</li>
<li>For: This paper focuses on pre-training relatively small vision transformer models for computer vision tasks, with the goal of improving their efficiency and deployment.* Methods: The proposed method is called asymmetric masked distillation (AMD), which uses an asymmetric masking strategy and customized multi-layer feature alignment to pre-train small vision transformer models.* Results: The AMD method achieves 84.6% classification accuracy on IN1K using the ViT-B model, and 73.3% classification accuracy on the Something-in-Something V2 dataset, outperforming the original ViT-B model from VideoMAE. Additionally, the AMD pre-trained models show consistent performance improvement on downstream tasks.<details>
<summary>Abstract</summary>
Self-supervised foundation models have shown great potential in computer vision thanks to the pre-training paradigm of masked autoencoding. Scale is a primary factor influencing the performance of these foundation models. However, these large foundation models often result in high computational cost that might limit their deployment. This paper focuses on pre-training relatively small vision transformer models that could be efficiently adapted to downstream tasks. Specifically, taking inspiration from knowledge distillation in model compression, we propose a new asymmetric masked distillation(AMD) framework for pre-training relatively small models with autoencoding. The core of AMD is to devise an asymmetric masking strategy, where the teacher model is enabled to see more context information with a lower masking ratio, while the student model still with high masking ratio to the original masked pre-training. We design customized multi-layer feature alignment between the teacher encoder and student encoder to regularize the pre-training of student MAE. To demonstrate the effectiveness and versatility of AMD, we apply it to both ImageMAE and VideoMAE for pre-training relatively small ViT models. AMD achieved 84.6% classification accuracy on IN1K using the ViT-B model. And AMD achieves 73.3% classification accuracy using the ViT-B model on the Something-in-Something V2 dataset, a 3.7% improvement over the original ViT-B model from VideoMAE. We also transfer AMD pre-trained models to downstream tasks and obtain consistent performance improvement over the standard pre-training.
</details>
<details>
<summary>摘要</summary>
自我监督基础模型在计算机视觉领域表现出色，这主要归功于预训练方法的掩码自动编码。但是这些大型基础模型经常具有高计算成本，可能限制其部署。本文关注预训练相对小型视图转换器模型，以提高下游任务适应性。具体来说，我们提出了一种新的偏向掩码采样（AMD）框架，用于预训练相对小型模型。AMD框架的核心是设计偏向掩码策略，使老师模型在低掩码比例下看到更多的上下文信息，而学生模型仍然具有高掩码比例。我们设计了特定的多层特征对齐方法，以规范学生MAE的预训练。为证明AMD的效果和多样性，我们应用其于图像MAE和视频MAE中的预训练相对小型ViT模型。AMD在IN1K上达到84.6%的分类精度，而在Something-in-Something V2集合上达到73.3%的分类精度，比原始ViT-B模型提高3.7%。我们还将AMD预训练模型传递到下游任务，并获得了常见性的性能改进。
</details></li>
</ul>
<hr>
<h2 id="Animating-NeRFs-from-Texture-Space-A-Framework-for-Pose-Dependent-Rendering-of-Human-Performances"><a href="#Animating-NeRFs-from-Texture-Space-A-Framework-for-Pose-Dependent-Rendering-of-Human-Performances" class="headerlink" title="Animating NeRFs from Texture Space: A Framework for Pose-Dependent Rendering of Human Performances"></a>Animating NeRFs from Texture Space: A Framework for Pose-Dependent Rendering of Human Performances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03140">http://arxiv.org/abs/2311.03140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Knoll, Wieland Morgenstern, Anna Hilsmann, Peter Eisert</li>
<li>for: 本研究旨在提供一种基于NeRF的高质量、可控的3D人体模型，用于多视图RGB视频中的人体表演控制。</li>
<li>methods: 我们提出了一种新的NeRF基本框架，其中NeRF的辐射场被扭曲到SMPL人体模型上，创建了一个新的表面对应表示。我们的表示可以通过skeletal关节参数来动画，并且可以根据视角来提供pose-dependent的外观。</li>
<li>results: 我们的方法可以在多视图RGB视频中生成高质量的人体表演控制图像，并且可以处理不同的pose和视角。<details>
<summary>Abstract</summary>
Creating high-quality controllable 3D human models from multi-view RGB videos poses a significant challenge. Neural radiance fields (NeRFs) have demonstrated remarkable quality in reconstructing and free-viewpoint rendering of static as well as dynamic scenes. The extension to a controllable synthesis of dynamic human performances poses an exciting research question. In this paper, we introduce a novel NeRF-based framework for pose-dependent rendering of human performances. In our approach, the radiance field is warped around an SMPL body mesh, thereby creating a new surface-aligned representation. Our representation can be animated through skeletal joint parameters that are provided to the NeRF in addition to the viewpoint for pose dependent appearances. To achieve this, our representation includes the corresponding 2D UV coordinates on the mesh texture map and the distance between the query point and the mesh. To enable efficient learning despite mapping ambiguities and random visual variations, we introduce a novel remapping process that refines the mapped coordinates. Experiments demonstrate that our approach results in high-quality renderings for novel-view and novel-pose synthesis.
</details>
<details>
<summary>摘要</summary>
创建高质量可控3D人体模型从多视图RGB视频中是一项重要挑战。神经辐射场（NeRF）已经表现出了惊人的质量，在重建和自由视点渲染的静止和动态场景中。在这篇论文中，我们介绍了一种基于NeRF的新的框架，用于pose-dependent渲染人体表演。在我们的方法中，辐射场被扭曲到SMPL人体模型上，创造了一个新的表面对应表示。我们的表示可以通过提供skeletal关节参数，并将观察点传递给NeRF，以便根据pose而改变外观。为了实现这一点，我们的表示包括UV坐标在 texture map上，以及 query点和 mesh之间的距离。为了实现高效的学习，我们引入了一种新的重映射过程，以修正映射的坐标。实验表明，我们的方法可以实现高质量的新视角和新pose sintesis。
</details></li>
</ul>
<hr>
<h2 id="TAMPAR-Visual-Tampering-Detection-for-Parcel-Logistics-in-Postal-Supply-Chains"><a href="#TAMPAR-Visual-Tampering-Detection-for-Parcel-Logistics-in-Postal-Supply-Chains" class="headerlink" title="TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains"></a>TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03124">http://arxiv.org/abs/2311.03124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Naumann, Felix Hertlein, Laura Dörr, Kai Furmans</li>
<li>for: 这个论文主要针对的是 послед一个分配阶段的物流货物伪装检测，使用单个RGB图像和现有数据库中的参考图像进行比较，检测可能存在的外观变化。</li>
<li>methods: 该检测管道使用关键点检测来确定包裹的八个角点，并应用投影变换将每个可见的包裹面变换成平行视图。这些视图不受视角的影响，使得在物流链中检测包裹的外观变化变得更加容易。</li>
<li>results: 在我们新收集的TAMPAR数据集上，我们使用了多种经典和深度学习基于变化检测方法进行实验。我们分别测试了关键点和伪装检测，以及它们在一起的检测系统。我们的评估结果显示关键点检测（Keypoint AP 75.76）和伪装检测（81%准确率，F1-Score 0.83）在实际图像上具有扎实的性能。此外，我们还提供了伪装类型、镜头扭曲和视图角度的敏感分析。代码和数据集可以在<a target="_blank" rel="noopener" href="https://a-nau.github.io/tampar%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://a-nau.github.io/tampar中下载。</a><details>
<summary>Abstract</summary>
Due to the steadily rising amount of valuable goods in supply chains, tampering detection for parcels is becoming increasingly important. In this work, we focus on the use-case last-mile delivery, where only a single RGB image is taken and compared against a reference from an existing database to detect potential appearance changes that indicate tampering. We propose a tampering detection pipeline that utilizes keypoint detection to identify the eight corner points of a parcel. This permits applying a perspective transformation to create normalized fronto-parallel views for each visible parcel side surface. These viewpoint-invariant parcel side surface representations facilitate the identification of signs of tampering on parcels within the supply chain, since they reduce the problem to parcel side surface matching with pair-wise appearance change detection. Experiments with multiple classical and deep learning-based change detection approaches are performed on our newly collected TAMpering detection dataset for PARcels, called TAMPAR. We evaluate keypoint and change detection separately, as well as in a unified system for tampering detection. Our evaluation shows promising results for keypoint (Keypoint AP 75.76) and tampering detection (81% accuracy, F1-Score 0.83) on real images. Furthermore, a sensitivity analysis for tampering types, lens distortion and viewing angles is presented. Code and dataset are available at https://a-nau.github.io/tampar.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unified-Multi-modal-Unsupervised-Representation-Learning-for-Skeleton-based-Action-Understanding"><a href="#Unified-Multi-modal-Unsupervised-Representation-Learning-for-Skeleton-based-Action-Understanding" class="headerlink" title="Unified Multi-modal Unsupervised Representation Learning for Skeleton-based Action Understanding"></a>Unified Multi-modal Unsupervised Representation Learning for Skeleton-based Action Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03106">http://arxiv.org/abs/2311.03106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huiguanlab/umurl">https://github.com/huiguanlab/umurl</a></li>
<li>paper_authors: Shengkai Sun, Daizong Liu, Jianfeng Dong, Xiaoye Qu, Junyu Gao, Xun Yang, Xun Wang, Meng Wang</li>
<li>for: 提高skeleton-based动作理解的灵活性和可扩展性，适应实际场景中的多种模式输入。</li>
<li>methods: 提出了一种单流承载多模态特征学习框架（UmURL），通过早期融合策略将多 modal 输入融合到单个流中，并通过内部和外部一致性学习约束保证多modal特征具备完整的semantics。</li>
<li>results: EXTENSIVE experiments在三个大规模数据集（NTU-60、NTU-120和PKU-MMD II）上表明，UmURL可以具有高效性和可扩展性，同时在多种下游任务场景中实现新的state-of-the-art表现。<details>
<summary>Abstract</summary>
Unsupervised pre-training has shown great success in skeleton-based action understanding recently. Existing works typically train separate modality-specific models, then integrate the multi-modal information for action understanding by a late-fusion strategy. Although these approaches have achieved significant performance, they suffer from the complex yet redundant multi-stream model designs, each of which is also limited to the fixed input skeleton modality. To alleviate these issues, in this paper, we propose a Unified Multimodal Unsupervised Representation Learning framework, called UmURL, which exploits an efficient early-fusion strategy to jointly encode the multi-modal features in a single-stream manner. Specifically, instead of designing separate modality-specific optimization processes for uni-modal unsupervised learning, we feed different modality inputs into the same stream with an early-fusion strategy to learn their multi-modal features for reducing model complexity. To ensure that the fused multi-modal features do not exhibit modality bias, i.e., being dominated by a certain modality input, we further propose both intra- and inter-modal consistency learning to guarantee that the multi-modal features contain the complete semantics of each modal via feature decomposition and distinct alignment. In this manner, our framework is able to learn the unified representations of uni-modal or multi-modal skeleton input, which is flexible to different kinds of modality input for robust action understanding in practical cases. Extensive experiments conducted on three large-scale datasets, i.e., NTU-60, NTU-120, and PKU-MMD II, demonstrate that UmURL is highly efficient, possessing the approximate complexity with the uni-modal methods, while achieving new state-of-the-art performance across various downstream task scenarios in skeleton-based action representation learning.
</details>
<details>
<summary>摘要</summary>
Recently, unsupervised pre-training has shown great success in skeleton-based action understanding. Existing methods typically train separate modality-specific models and then use a late-fusion strategy to integrate multi-modal information for action understanding. Although these approaches have achieved significant performance, they suffer from complex and redundant multi-stream model designs and are limited to fixed input skeleton modalities.To address these issues, we propose a Unified Multimodal Unsupervised Representation Learning (UmURL) framework, which exploits an efficient early-fusion strategy to jointly encode multi-modal features in a single-stream manner. Instead of designing separate modality-specific optimization processes for uni-modal unsupervised learning, we feed different modality inputs into the same stream with an early-fusion strategy to learn their multi-modal features and reduce model complexity.To ensure that the fused multi-modal features do not exhibit modality bias, we propose both intra- and inter-modal consistency learning to guarantee that the multi-modal features contain the complete semantics of each modal. Our framework is able to learn unified representations of uni-modal or multi-modal skeleton input, which is flexible to different kinds of modality input for robust action understanding in practical cases.Extensive experiments conducted on three large-scale datasets, i.e., NTU-60, NTU-120, and PKU-MMD II, demonstrate that UmURL is highly efficient and achieves new state-of-the-art performance across various downstream task scenarios in skeleton-based action representation learning, with an approximate complexity comparable to uni-modal methods.
</details></li>
</ul>
<hr>
<h2 id="A-survey-and-classification-of-face-alignment-methods-based-on-face-models"><a href="#A-survey-and-classification-of-face-alignment-methods-based-on-face-models" class="headerlink" title="A survey and classification of face alignment methods based on face models"></a>A survey and classification of face alignment methods based on face models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03082">http://arxiv.org/abs/2311.03082</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nordlinglab/facealignment-survey">https://github.com/nordlinglab/facealignment-survey</a></li>
<li>paper_authors: Jagmohan Meher, Hector Allende-Cid, Torbjörn E. M. Nordling</li>
<li>for: 这篇论文旨在对不同的面部模型在面部姿态定位方面进行总结，以适应不同类型的读者（beginner、实践者和研究人员）。</li>
<li>methods: 本文包括不同类型的面部模型的解释和训练，以及如何将面部模型适应新的面部图像。另外，文中还介绍了3D基于的面部模型和深度学习基于的方法，以及它们在极端面 pose 中的应用。</li>
<li>results: 文中发现，3D基于的面部模型在极端面 pose 中具有优势，而深度学习基于的方法则常用热图来表示面部特征。此外，文章还提出了未来面部模型在面部姿态定位领域的可能发展方向。<details>
<summary>Abstract</summary>
A face model is a mathematical representation of the distinct features of a human face. Traditionally, face models were built using a set of fiducial points or landmarks, each point ideally located on a facial feature, i.e., corner of the eye, tip of the nose, etc. Face alignment is the process of fitting the landmarks in a face model to the respective ground truth positions in an input image containing a face. Despite significant research on face alignment in the past decades, no review analyses various face models used in the literature. Catering to three types of readers - beginners, practitioners and researchers in face alignment, we provide a comprehensive analysis of different face models used for face alignment. We include the interpretation and training of the face models along with the examples of fitting the face model to a new face image. We found that 3D-based face models are preferred in cases of extreme face pose, whereas deep learning-based methods often use heatmaps. Moreover, we discuss the possible future directions of face models in the field of face alignment.
</details>
<details>
<summary>摘要</summary>
一个面模型是人脸特征的数学表达。在过去的几十年中，人们使用了一组 fiducial point或标记点来建立面模型，每个点理想地位于人脸中的一个特征处，例如眼角、鼻子的tip等。面对齐是将面模型的标记点与输入图像中的面部特征进行匹配的过程。尽管在过去的几十年中有很大的研究努力，但是没有任何文献对不同的面模型进行了总的分析。为了针对不同类型的读者（ Beginner、实践者和研究者），我们提供了对不同的面模型进行了全面的分析。我们包括面模型的解释和训练以及将面模型适应到新的面图像中的示例。我们发现在极端面pose时，3D基于的面模型被首选，而深度学习基于的方法通常使用热图。此外，我们还讨论了面模型在面对齐领域的未来发展方向。
</details></li>
</ul>
<hr>
<h2 id="CogVLM-Visual-Expert-for-Pretrained-Language-Models"><a href="#CogVLM-Visual-Expert-for-Pretrained-Language-Models" class="headerlink" title="CogVLM: Visual Expert for Pretrained Language Models"></a>CogVLM: Visual Expert for Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03079">http://arxiv.org/abs/2311.03079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/cogvlm">https://github.com/thudm/cogvlm</a></li>
<li>paper_authors: Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang</li>
<li>for: 这个论文的目的是提出一个有力量的开源视觉语言基础模型（CogVLM），以便将图像特征与语言模型进行深度融合。</li>
<li>methods: 这篇论文使用了一种名为“可训练的视觉专家模块”，这种模块在语言模型的注意力和FFN层中嵌入，以 bridging the gap between 静止预训练的语言模型和图像Encoder。</li>
<li>results: 根据论文的描述，CogVLM-17B在10个跨模态测试 benchmarck上达到了状态之最好的表现，包括NoCaps、Flicker30k captioning、RefCOCO、RefCOCO+、RefCOCOg、Visual7W、GQA、ScienceQA、VizWiz VQA 和 TDIUC等，并在VQAv2、OKVQA、TextVQA、COCO captioning等测试中排名第2，超过或匹配 PaLI-X 55B。<details>
<summary>Abstract</summary>
We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.
</details>
<details>
<summary>摘要</summary>
我们介绍CogVLM，一款强大的开源视觉语言基础模型。与流行的浅层对应方法不同，CogVLM通过在注意力和FFN层中添加可训练的视觉专家模块，将图像特征与静止预训练语言模型的输入空间连接起来。这使得CogVLM可以深度融合视觉语言特征，而无需牺牲任何批处语言任务的性能。CogVLM-17B在10个经典跨模态测试 benchmark上达到了状态机器人的表现，包括NoCaps、Flicker30k captioning、RefCOCO、RefCOCO+、RefCOCOg、Visual7W、GQA、ScienceQA、VizWiz VQA和TDIUC，并在VQAv2、OKVQA、TextVQA、COCO captioning等测试上排名第二，超过或匹配PaLI-X 55B。代码和检查点可以在https://github.com/THUDM/CogVLM中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Two-Stage-Generative-Model-with-CycleGAN-and-Joint-Diffusion-for-MRI-based-Brain-Tumor-Detection"><a href="#A-Two-Stage-Generative-Model-with-CycleGAN-and-Joint-Diffusion-for-MRI-based-Brain-Tumor-Detection" class="headerlink" title="A Two-Stage Generative Model with CycleGAN and Joint Diffusion for MRI-based Brain Tumor Detection"></a>A Two-Stage Generative Model with CycleGAN and Joint Diffusion for MRI-based Brain Tumor Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03074">http://arxiv.org/abs/2311.03074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhyjsiat/a-two-stage-cyclegan-ve-brats2020">https://github.com/zhyjsiat/a-two-stage-cyclegan-ve-brats2020</a></li>
<li>paper_authors: Wenxin Wang, Zhuo-Xu Cui, Guanxun Cheng, Chentao Cao, Xi Xu, Ziwei Liu, Haifeng Wang, Yulong Qi, Dong Liang, Yanjie Zhu</li>
<li>for: 提高脑肿检测和分割精度</li>
<li>methods:  combinatorial CycleGAN和VE-JP方法</li>
<li>results: 提高检测和分割精度，比如DSC分数为0.8590、0.6226和0.7403等<details>
<summary>Abstract</summary>
Accurate detection and segmentation of brain tumors is critical for medical diagnosis. However, current supervised learning methods require extensively annotated images and the state-of-the-art generative models used in unsupervised methods often have limitations in covering the whole data distribution. In this paper, we propose a novel framework Two-Stage Generative Model (TSGM) that combines Cycle Generative Adversarial Network (CycleGAN) and Variance Exploding stochastic differential equation using joint probability (VE-JP) to improve brain tumor detection and segmentation. The CycleGAN is trained on unpaired data to generate abnormal images from healthy images as data prior. Then VE-JP is implemented to reconstruct healthy images using synthetic paired abnormal images as a guide, which alters only pathological regions but not regions of healthy. Notably, our method directly learned the joint probability distribution for conditional generation. The residual between input and reconstructed images suggests the abnormalities and a thresholding method is subsequently applied to obtain segmentation results. Furthermore, the multimodal results are weighted with different weights to improve the segmentation accuracy further. We validated our method on three datasets, and compared with other unsupervised methods for anomaly detection and segmentation. The DSC score of 0.8590 in BraTs2020 dataset, 0.6226 in ITCS dataset and 0.7403 in In-house dataset show that our method achieves better segmentation performance and has better generalization.
</details>
<details>
<summary>摘要</summary>
当前的超级vised学习方法需要大量的标注图像，而状态对的生成模型通常只能覆盖数据分布的一部分。在这篇论文中，我们提出了一种新的框架Two-Stage Generative Model（TSGM），它将Cycling Generative Adversarial Network（CycleGAN）和Variance Exploding stochastic differential equation using joint probability（VE-JP）相结合以提高脑肿检测和分 segmentation。CycleGAN在无对应数据上训练，将健康图像转化为病态图像作为数据先验。然后，VE-JP被实现，通过使用生成的合理对数据作为引导，重construct健康图像，只有病态区域被修改，而不是健康区域。值得注意的是，我们的方法直接学习了联合分布，从而实现了条件生成。输入图像与重construct图像之间的差异提示病态区域，并采用阈值分割方法获得分 segmentation 结果。此外，我们还将多modal结果权重为不同的权重，以进一步提高分 segmentation 精度。我们在三个数据集上验证了我们的方法，并与其他无supervised方法进行比较。BraTs2020数据集的DSC分数为0.8590，ITCS数据集的DSC分数为0.6226，In-house数据集的DSC分数为0.7403，表明我们的方法在检测和分 segmentation方面表现出色，并且具有更好的泛化性。
</details></li>
</ul>
<hr>
<h2 id="OrthoNets-Orthogonal-Channel-Attention-Networks"><a href="#OrthoNets-Orthogonal-Channel-Attention-Networks" class="headerlink" title="OrthoNets: Orthogonal Channel Attention Networks"></a>OrthoNets: Orthogonal Channel Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03071">http://arxiv.org/abs/2311.03071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hady1011/orthonets">https://github.com/hady1011/orthonets</a></li>
<li>paper_authors: Hadi Salman, Caleb Parks, Matthew Swan, John Gauch</li>
<li>for: 提高频道注意力机制的效iveness，以实现更好的图像识别和分类。</li>
<li>methods: 使用 randomly initialized orthogonal filters construct an attention mechanism，并将其 integrate into ResNet 网络中。</li>
<li>results: 相比 FcaNet 和其他注意力机制，OrthoNet 在 Birds、MS-COCO 和 Places356  dataset 上显示出较好的性能，并在 ImageNet  dataset 上与当前状态的艺术品级别竞争。 Our results suggest that a sufficient number of orthogonal filters can achieve generalization, and we also explore other general principles for implementing channel attention.<details>
<summary>Abstract</summary>
Designing an effective channel attention mechanism implores one to find a lossy-compression method allowing for optimal feature representation. Despite recent progress in the area, it remains an open problem. FcaNet, the current state-of-the-art channel attention mechanism, attempted to find such an information-rich compression using Discrete Cosine Transforms (DCTs). One drawback of FcaNet is that there is no natural choice of the DCT frequencies. To circumvent this issue, FcaNet experimented on ImageNet to find optimal frequencies. We hypothesize that the choice of frequency plays only a supporting role and the primary driving force for the effectiveness of their attention filters is the orthogonality of the DCT kernels. To test this hypothesis, we construct an attention mechanism using randomly initialized orthogonal filters. Integrating this mechanism into ResNet, we create OrthoNet. We compare OrthoNet to FcaNet (and other attention mechanisms) on Birds, MS-COCO, and Places356 and show superior performance. On the ImageNet dataset, our method competes with or surpasses the current state-of-the-art. Our results imply that an optimal choice of filter is elusive and generalization can be achieved with a sufficiently large number of orthogonal filters. We further investigate other general principles for implementing channel attention, such as its position in the network and channel groupings. Our code is publicly available at https://github.com/hady1011/OrthoNets/
</details>
<details>
<summary>摘要</summary>
设计有效的通道注意机制需要一种lossy压缩方法，以便获得最佳特征表示。尽管最近在这一领域的进步很大，但这仍然是一个开放的问题。FcaNet，当前状态的ARTchannel attention机制，使用Discrete Cosine Transforms（DCTs）来找到信息充足的压缩。FcaNet的一个缺点是没有自然的DCT频率选择。为了解决这个问题，FcaNet在ImageNet上进行了实验以找到最佳频率。我们假设，选择的频率只是支持角色，主要的驱动力是DCT核心的正交性。为了测试这个假设，我们构建了一个使用随机初始化的正交滤波器的注意机制。将这种机制 integrate into ResNet，我们创建了OrthoNet。我们比较OrthoNet与FcaNet（以及其他注意机制）在Birds、MS-COCO和Places356上的性能，并显示其性能较好。在ImageNet dataset上，我们的方法与当前状态的艺术品级别竞争。我们的结果表明，选择的滤波器是找不到的，并且通过 sufficient number of orthogonal filters 可以实现总体化。我们进一步调查了其他实现 channel attention 的一般原则，如其在网络中的位置和通道分组。我们的代码在https://github.com/hady1011/OrthoNets/上公开 available。
</details></li>
</ul>
<hr>
<h2 id="Forest-aboveground-biomass-estimation-using-GEDI-and-earth-observation-data-through-attention-based-deep-learning"><a href="#Forest-aboveground-biomass-estimation-using-GEDI-and-earth-observation-data-through-attention-based-deep-learning" class="headerlink" title="Forest aboveground biomass estimation using GEDI and earth observation data through attention-based deep learning"></a>Forest aboveground biomass estimation using GEDI and earth observation data through attention-based deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03067">http://arxiv.org/abs/2311.03067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenquan Dong, Edward T. A. Mitchard, Hao Yu, Steven Hancock, Casey M. Ryan</li>
<li>for: 这个研究的目的是精确量度森林上空的生物质量（AGB），以了解气候变革的碳会计。</li>
<li>methods: 这个研究使用了开源的卫星数据，包括GEDI LiDAR数据、C-band Sentinel-1 SAR数据、ALOS-2 PALSAR-2数据和Sentinel-2多спектル数据，并使用了注意力对应深度学习模型（AU）进行森林AGB估算。</li>
<li>results: 这个研究发现，使用AU模型可以实现森林AGB估算的高精度，其R2值为0.66，RMSE值为43.66吨&#x2F;公顷，偏差值为0.14吨&#x2F;公顷，而传统的RF算法则有较低的R2值、RMSE值和偏差值。此外，这个研究还发现AU模型在不使用空间信息时也可以获得高精度的结果。<details>
<summary>Abstract</summary>
Accurate quantification of forest aboveground biomass (AGB) is critical for understanding carbon accounting in the context of climate change. In this study, we presented a novel attention-based deep learning approach for forest AGB estimation, primarily utilizing openly accessible EO data, including: GEDI LiDAR data, C-band Sentinel-1 SAR data, ALOS-2 PALSAR-2 data, and Sentinel-2 multispectral data. The attention UNet (AU) model achieved markedly higher accuracy for biomass estimation compared to the conventional RF algorithm. Specifically, the AU model attained an R2 of 0.66, RMSE of 43.66 Mg ha-1, and bias of 0.14 Mg ha-1, while RF resulted in lower scores of R2 0.62, RMSE 45.87 Mg ha-1, and bias 1.09 Mg ha-1. However, the superiority of the deep learning approach was not uniformly observed across all tested models. ResNet101 only achieved an R2 of 0.50, an RMSE of 52.93 Mg ha-1, and a bias of 0.99 Mg ha-1, while the UNet reported an R2 of 0.65, an RMSE of 44.28 Mg ha-1, and a substantial bias of 1.84 Mg ha-1. Moreover, to explore the performance of AU in the absence of spatial information, fully connected (FC) layers were employed to eliminate spatial information from the remote sensing data. AU-FC achieved intermediate R2 of 0.64, RMSE of 44.92 Mgha-1, and bias of -0.56 Mg ha-1, outperforming RF but underperforming AU model using spatial information. We also generated 10m forest AGB maps across Guangdong for the year 2019 using AU and compared it with that produced by RF. The AGB distributions from both models showed strong agreement with similar mean values; the mean forest AGB estimated by AU was 102.18 Mg ha-1 while that of RF was 104.84 Mg ha-1. Additionally, it was observed that the AGB map generated by AU provided superior spatial information. Overall, this research substantiates the feasibility of employing deep learning for biomass estimation based on satellite data.
</details>
<details>
<summary>摘要</summary>
准确量化林地上部生物质（AGB）的估算对于气候变化的 carbon 账户有着重要的意义。在这个研究中，我们提出了一种基于 Deep Learning 的新型注意力模型（AU），主要利用开放访问的 Earth Observation（EO）数据，包括：GEDI LiDAR 数据、C-band Sentinel-1 SAR 数据、ALOS-2 PALSAR-2 数据和 Sentinel-2 多spectral 数据。AU 模型在生物质估算中显示出了明显更高的准确度，与传统的 Random Forest 算法相比。具体来说，AU 模型的 R2 为 0.66，RMSE 为 43.66 Mg ha-1，偏差为 0.14 Mg ha-1，而 Random Forest 算法的 R2 为 0.62，RMSE 为 45.87 Mg ha-1，偏差为 1.09 Mg ha-1。然而，深度学习方法的优势不uniformly 分布于所测试的모дели中。ResNet101 模型只有 R2 为 0.50，RMSE 为 52.93 Mg ha-1，偏差为 0.99 Mg ha-1，而 UNet 模型 Reported R2 为 0.65，RMSE 为 44.28 Mg ha-1，并且偏差为 1.84 Mg ha-1。此外，为了探讨 AU 模型在不含空间信息的情况下的性能，我们使用了全连接（FC）层来消除 remote sensing 数据中的空间信息。AU-FC 模型的 R2 为 0.64，RMSE 为 44.92 Mg ha-1，偏差为 -0.56 Mg ha-1，与 Random Forest 算法相比，表现较佳。此外，我们还使用 AU 模型生成了2019年广东省的10米森林AGB地图，并与 Random Forest 算法生成的地图进行了比较。两个模型的 AGB 分布显示了强相关性，两者的平均森林AGB估算值也类似，AU 模型的 Mean Forest AGB 为 102.18 Mg ha-1，Random Forest 算法的 Mean Forest AGB 为 104.84 Mg ha-1。此外，AU 模型生成的 AGB 地图提供了较好的空间信息。总的来说，这项研究证明了深度学习可以为基于卫星数据的生物质估算提供可靠的方法。
</details></li>
</ul>
<hr>
<h2 id="AnyText-Multilingual-Visual-Text-Generation-And-Editing"><a href="#AnyText-Multilingual-Visual-Text-Generation-And-Editing" class="headerlink" title="AnyText: Multilingual Visual Text Generation And Editing"></a>AnyText: Multilingual Visual Text Generation And Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03054">http://arxiv.org/abs/2311.03054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, Xuansong Xie</li>
<li>for: 这个研究旨在提高文本与图像之间的融合，以实现高精度的文本生成和修改。</li>
<li>methods: 这个研究使用了一个散射管道，包括两个主要元素：auxiliary latent module和text embedding module。auxiliary latent module使用文本glyph、位置和对像图像来生成潜在特征，而text embedding module使用OCR模型将roke数据转换为嵌入，与图像描述嵌入结合以生成文本。</li>
<li>results: 这个研究的方法在训练后，与其他方法相比，具有了很大的进步。此外，这个研究还提供了一个大规模的多种语言文本图像集（AnyWord-3M），并提出了AnyText-benchmark来评估文本生成质量和精度。<details>
<summary>Abstract</summary>
Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced on https://github.com/tyxsspa/AnyText to improve and promote the development of text generation technology.
</details>
<details>
<summary>摘要</summary>
simplified Chinese:现代文本描述技术在最近几年来已经取得了很大的进步，尤其是在生成图像时的文本描述方面。然而，当注意力集中在生成图像中的文本区域时，仍然存在可能给出显示的问题。为解决这个问题，我们介绍了 AnyText，一种基于扩散的多语言视觉文本生成和编辑模型，它专注于在图像中生成准确和一致的文本。AnyText包括一个扩散管道，其中包括两个主要元素：一个辅助隐藏模块和一个文本嵌入模块。前者使用文本字形、位置和遮盲图像作为输入，生成隐藏特征 для文本生成或编辑。后者使用 OCR 模型对笔画数据进行编码，将其混合到图像caption embedding 中，以生成文本与背景相协同的文本。我们在训练时使用文本扩散损失和文本感知损失，以进一步提高文本准确性。AnyText 可以在多种语言中写字符，我们知道这是首次对多语言视觉文本生成进行研究。值得一提的是，AnyText 可以与社区已有的扩散模型结合使用，以提供更高的文本准确性。经过广泛的评估实验，我们的方法在所有其他方法之上出现了显著的优势。此外，我们还提供了首个大规模多语言文本图像集 AnyWord-3M，包含 3000 万个图像文本对，其中每个对包含多种语言的 OCR 注释。基于 AnyWord-3M 数据集，我们提出 AnyText-benchmark，用于评估视觉文本生成准确性和质量。我们的项目将在 https://github.com/tyxsspa/AnyText 上开源，以便改进和推动文本生成技术的发展。
</details></li>
</ul>
<hr>
<h2 id="MixUp-MIL-A-Study-on-Linear-Multilinear-Interpolation-Based-Data-Augmentation-for-Whole-Slide-Image-Classification"><a href="#MixUp-MIL-A-Study-on-Linear-Multilinear-Interpolation-Based-Data-Augmentation-for-Whole-Slide-Image-Classification" class="headerlink" title="MixUp-MIL: A Study on Linear &amp; Multilinear Interpolation-Based Data Augmentation for Whole Slide Image Classification"></a>MixUp-MIL: A Study on Linear &amp; Multilinear Interpolation-Based Data Augmentation for Whole Slide Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03052">http://arxiv.org/abs/2311.03052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Gadermayr, Lukas Koller, Maximilian Tschuchnig, Lea Maria Stangassinger, Christina Kreutzer, Sebastien Couillard-Despres, Gertie Janneke Oostingh, Anton Hittmair</li>
<li>for: 本研究旨在 investigate linear and multilinear interpolation between feature vectors, a data augmentation technique, 用于提高分类网络和多例学习的总体表现。</li>
<li>methods: 本研究使用了10个不同的数据集配置、两种不同的特征提取方法（一种是supervised，另一种是self-supervised）、染料normalization，以及两种多例学习架构进行实验。</li>
<li>results: 研究发现了该方法在不同的数据集和特征提取方法下 exhibit EXTRAORDINARILY HIGH variability，并且identified several interesting aspects to bring light into the darkness, 以及 novelfields of research。<details>
<summary>Abstract</summary>
For classifying digital whole slide images in the absence of pixel level annotation, typically multiple instance learning methods are applied. Due to the generic applicability, such methods are currently of very high interest in the research community, however, the issue of data augmentation in this context is rarely explored. Here we investigate linear and multilinear interpolation between feature vectors, a data augmentation technique, which proved to be capable of improving the generalization performance classification networks and also for multiple instance learning. Experiments, however, have been performed on only two rather small data sets and one specific feature extraction approach so far and a strong dependence on the data set has been identified. Here we conduct a large study incorporating 10 different data set configurations, two different feature extraction approaches (supervised and self-supervised), stain normalization and two multiple instance learning architectures. The results showed an extraordinarily high variability in the effect of the method. We identified several interesting aspects to bring light into the darkness and identified novel promising fields of research.
</details>
<details>
<summary>摘要</summary>
Experiments have been performed on only two small data sets and one specific feature extraction approach so far, and a strong dependence on the data set has been identified. In this study, we conduct a large study incorporating 10 different data set configurations, two different feature extraction approaches (supervised and self-supervised), stain normalization, and two multiple instance learning architectures.The results showed an extraordinarily high variability in the effect of the method. We identified several interesting aspects to bring light into the darkness and identified novel promising fields of research.Translation in Simplified Chinese:为了针对无法获得像素级别标注的数字整体图像进行分类，通常使用多个实例学习方法。由于其普适性，这些方法目前在研究社区中具有很高的兴趣。然而，在这种情况下数据增强的问题很少被探讨。我们在这里investigate linear和多线性 interpolate between feature vectors，一种数据增强技术，已经证明可以提高分类网络和多个实例学习的总体化性能。尝试在只有两个较小数据集和一种特定的特征提取方法上进行了实验，并且发现数据集强度的依赖性。在这里，我们进行了一项大规模的研究，包括10个不同的数据集配置、两种不同的特征提取方法（可视和自动化）、染色 нормализа化以及两种多个实例学习架构。结果表明了非常高的变化程度，我们分析了一些有趣的方面，并发现了一些新的可能性。
</details></li>
</ul>
<hr>
<h2 id="COLA-COarse-LAbel-multi-source-LiDAR-semantic-segmentation-for-autonomous-driving"><a href="#COLA-COarse-LAbel-multi-source-LiDAR-semantic-segmentation-for-autonomous-driving" class="headerlink" title="COLA: COarse-LAbel multi-source LiDAR semantic segmentation for autonomous driving"></a>COLA: COarse-LAbel multi-source LiDAR semantic segmentation for autonomous driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03017">http://arxiv.org/abs/2311.03017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jules Sanchez, Jean-Emmanuel Deschaud, François Goulette</li>
<li>for: 本研究旨在提高LiDAR semantic segmentation中的自动驾驶性能。</li>
<li>methods: 本研究使用多源训练方法，利用多个数据集进行训练，以提高域泛化、源到源分类和预训练等三个子领域的结果。</li>
<li>results: 研究显示，使用多源训练方法可以提高域泛化 (+10%), source-to-source分类 (+5.3%), 和预训练 (+12%)等三个方面的结果。<details>
<summary>Abstract</summary>
LiDAR semantic segmentation for autonomous driving has been a growing field of interest in the past few years. Datasets and methods have appeared and expanded very quickly, but methods have not been updated to exploit this new availability of data and continue to rely on the same classical datasets.   Different ways of performing LIDAR semantic segmentation training and inference can be divided into several subfields, which include the following: domain generalization, the ability to segment data coming from unseen domains ; source-to-source segmentation, the ability to segment data coming from the training domain; and pre-training, the ability to create re-usable geometric primitives.   In this work, we aim to improve results in all of these subfields with the novel approach of multi-source training. Multi-source training relies on the availability of various datasets at training time and uses them together rather than relying on only one dataset.   To overcome the common obstacles found for multi-source training, we introduce the coarse labels and call the newly created multi-source dataset COLA. We propose three applications of this new dataset that display systematic improvement over single-source strategies: COLA-DG for domain generalization (up to +10%), COLA-S2S for source-to-source segmentation (up to +5.3%), and COLA-PT for pre-training (up to +12%).
</details>
<details>
<summary>摘要</summary>
隐藏的文本雷达semantic segmentation для自动驾驶在过去几年内得到了广泛关注。 datasets和方法在短时间内出现和扩展了，但方法没有更新以利用这些新的数据，而是仍然依赖于传统的数据集。 不同的雷达semantic segmentation训练和推断方法可以分为以下几个子领域：领域泛化、来自训练领域的数据排序和预训练。 在这项工作中，我们想要在所有这些子领域中提高结果，使用新的多源训练方法。 多源训练利用训练时可用的多个数据集，而不是仅仅依赖于一个数据集。 为了解决多源训练中常见的障碍，我们引入了粗略标签，并将其与多源数据集称为COLA。 我们提出了三个COLA数据集的应用，其中COLA-DG用于领域泛化（最高提高+10%），COLA-S2S用于来自源到源的排序（最高提高+5.3%），COLA-PT用于预训练（最高提高+12%）。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Capability-of-Text-to-Image-Diffusion-Models-with-Structural-Edge-Guidance-for-Multi-Spectral-Satellite-Image-Inpainting"><a href="#Exploring-the-Capability-of-Text-to-Image-Diffusion-Models-with-Structural-Edge-Guidance-for-Multi-Spectral-Satellite-Image-Inpainting" class="headerlink" title="Exploring the Capability of Text-to-Image Diffusion Models with Structural Edge Guidance for Multi-Spectral Satellite Image Inpainting"></a>Exploring the Capability of Text-to-Image Diffusion Models with Structural Edge Guidance for Multi-Spectral Satellite Image Inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03008">http://arxiv.org/abs/2311.03008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikolaj Czerkawski, Christos Tachtatzis</li>
<li>for:  investigate the utility of text-to-image inpainting models for satellite image data</li>
<li>methods: introduce a novel inpainting framework based on StableDiffusion and ControlNet, as well as a novel method for RGB-to-MSI translation</li>
<li>results: the inpainting synthesized via StableDiffusion suffers from undesired artefacts, and a simple alternative of self-supervised internal inpainting achieves higher quality of synthesis<details>
<summary>Abstract</summary>
The paper investigates the utility of text-to-image inpainting models for satellite image data. Two technical challenges of injecting structural guiding signals into the generative process as well as translating the inpainted RGB pixels to a wider set of MSI bands are addressed by introducing a novel inpainting framework based on StableDiffusion and ControlNet as well as a novel method for RGB-to-MSI translation. The results on a wider set of data suggest that the inpainting synthesized via StableDiffusion suffers from undesired artefacts and that a simple alternative of self-supervised internal inpainting achieves higher quality of synthesis.
</details>
<details>
<summary>摘要</summary>
文章研究文本到图像填充模型在卫星图像数据中的实用性。两个技术挑战，即在生成过程中涂入结构导向信号以及将RGB像素翻译到更广泛的MSI频谱中，通过引入稳定扩散和控制网络以及RGB到MSI翻译方法来解决。研究结果表明，通过稳定扩散生成的图像填充存在不良artefacts，而自我超vised内部填充方法可以获得更高质量的生成结果。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Enhancement-of-Low-Light-Image-Based-on-Retinex-Decomposition"><a href="#Zero-Shot-Enhancement-of-Low-Light-Image-Based-on-Retinex-Decomposition" class="headerlink" title="Zero-Shot Enhancement of Low-Light Image Based on Retinex Decomposition"></a>Zero-Shot Enhancement of Low-Light Image Based on Retinex Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02995">http://arxiv.org/abs/2311.02995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenchao Li, Bangshu Xiong, Qiaofeng Ou, Xiaoyun Long, Jinhao Zhu, Jiabao Chen, Shuyuan Wen</li>
<li>for: 提高低光照图像的显示质量，同时解决阴影、噪声、色偏和对比等问题。</li>
<li>methods: 提出了一种基于学习的Retinex分解方法，称为ZERRINNet，使用N-Net网络和噪声损失函数来除噪推优原始低光照图像，同时使用RI-Net网络和текxture损失函数来约束反射组件和照明组件的优化。</li>
<li>results: 在一个自制的实际低光照数据集上进行了有效验证，并在面部检测、目标识别和实例分割等高级视觉任务上达到了比较出色的表现。与现有状态艺术方法相比，其总体性能得到了进一步提高。代码可以在<a target="_blank" rel="noopener" href="https://github.com/liwenchao0615/ZERRINNet%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/liwenchao0615/ZERRINNet中下载。</a><details>
<summary>Abstract</summary>
Two difficulties here make low-light image enhancement a challenging task; firstly, it needs to consider not only luminance restoration but also image contrast, image denoising and color distortion issues simultaneously. Second, the effectiveness of existing low-light enhancement methods depends on paired or unpaired training data with poor generalization performance.   To solve these difficult problems, we propose in this paper a new learning-based Retinex decomposition of zero-shot low-light enhancement method, called ZERRINNet. To this end, we first designed the N-Net network, together with the noise loss term, to be used for denoising the original low-light image by estimating the noise of the low-light image. Moreover, RI-Net is used to estimate the reflection component and illumination component, and in order to solve the color distortion and contrast, we use the texture loss term and segmented smoothing loss to constrain the reflection component and illumination component. Finally, our method is a zero-reference enhancement method that is not affected by the training data of paired and unpaired datasets, so our generalization performance is greatly improved, and in the paper, we have effectively validated it with a homemade real-life low-light dataset and additionally with advanced vision tasks, such as face detection, target recognition, and instance segmentation. We conducted comparative experiments on a large number of public datasets and the results show that the performance of our method is competitive compared to the current state-of-the-art methods. The code is available at:https://github.com/liwenchao0615/ZERRINNet
</details>
<details>
<summary>摘要</summary>
两个问题使得低光照图像改善成为一项具有挑战性的任务：首先，它需要同时考虑照明修复、图像对比、雷达噪声和颜色扭曲问题。其次，现有的低光照改善方法的效果取决于对配对或非配对训练数据的训练，而且其泛化性不强。为解决这些困难问题，我们在本文提出了一种新的学习基于的Retinex分解 zero-shot 低光照改善方法，称为ZERRINNet。为此，我们首先设计了N-Net网络，并与噪声损失项一起用于对原始低光照图像进行降噪。此外，我们使用RI-Net来估算反射Component和照明Component，并使用文本损失项和分割平滑损失项来约束反射Component和照明Component，以解决颜色扭曲和对比问题。最后，我们的方法是一种零参考改善方法，不受配对和非配对训练数据的影响，因此我们的泛化性得到了大幅提高。在论文中，我们有效地验证了它的可靠性，使用自己制作的实际低光照 dataset 和高级视觉任务，如人脸检测、目标识别和实例 segmentation。我们进行了大量的公共数据集的比较实验，结果显示，我们的方法与当前状态艺的方法竞争。代码可以在：https://github.com/liwenchao0615/ZERRINNet 中获取
</details></li>
</ul>
<hr>
<h2 id="NEURO-HAND-A-weakly-supervised-Hierarchical-Attention-Network-for-neuroimaging-abnormality-Detection"><a href="#NEURO-HAND-A-weakly-supervised-Hierarchical-Attention-Network-for-neuroimaging-abnormality-Detection" class="headerlink" title="NEURO HAND: A weakly supervised Hierarchical Attention Network for neuroimaging abnormality Detection"></a>NEURO HAND: A weakly supervised Hierarchical Attention Network for neuroimaging abnormality Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02992">http://arxiv.org/abs/2311.02992</a></li>
<li>repo_url: None</li>
<li>paper_authors: David A. Wood</li>
<li>for: 这份研究是为了探讨临床神经成像数据的自然层次结构，并提出了一种 Hierarchical Attention Network 来检测 MRI 扫描数据。</li>
<li>methods: 这个方法使用了 Hierarchical Attention Network，适用于非体积数据（即高分辨率 MRI 扫描堆），并可以从二分类评估级别 labels 进行训练。</li>
<li>results: 研究结果显示，这个方法可以提高分类精度，并提供了解释性的 coarse inter-和 intra-slice 异常地点位置，或者给出了不同层次的 slice 和序列之间的重要性分数，使得这个模型适合用于自动化诊断系统。<details>
<summary>Abstract</summary>
Clinical neuroimaging data is naturally hierarchical. Different magnetic resonance imaging (MRI) sequences within a series, different slices covering the head, and different regions within each slice all confer different information. In this work we present a hierarchical attention network for abnormality detection using MRI scans obtained in a clinical hospital setting. The proposed network is suitable for non-volumetric data (i.e. stacks of high-resolution MRI slices), and can be trained from binary examination-level labels. We show that this hierarchical approach leads to improved classification, while providing interpretability through either coarse inter- and intra-slice abnormality localisation, or giving importance scores for different slices and sequences, making our model suitable for use as an automated triaging system in radiology departments.
</details>
<details>
<summary>摘要</summary>
临床神经成像数据自然具有层次结构。不同的磁共振成像（MRI）序列内一系列、不同的slice覆盖头部、和每个slice中的不同区域都提供不同的信息。在这项工作中，我们提出了一种层次注意力网络用于使用临床医院设备获取的MRI扫描数据中的异常检测。我们的提案的网络适用于非核Volume数据（即高分辨率MRI slice栈），并可以从二进制诊断级别标签进行训练。我们表明，这种层次方法可以提高分类效果，同时提供可读性，通过 Either coarse inter-和intra-slice异常定位或给出不同的slice和序列的重要性分数，使我们的模型适用于 radiology部门的自动化排序系统。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Radiotherapy-Dose-Prediction-Guided-by-Inter-slice-Aware-Structure-Encoding"><a href="#Diffusion-based-Radiotherapy-Dose-Prediction-Guided-by-Inter-slice-Aware-Structure-Encoding" class="headerlink" title="Diffusion-based Radiotherapy Dose Prediction Guided by Inter-slice Aware Structure Encoding"></a>Diffusion-based Radiotherapy Dose Prediction Guided by Inter-slice Aware Structure Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02991">http://arxiv.org/abs/2311.02991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghao Feng, Lu Wen, Jianghong Xiao, Yuanyuan Xu, Xi Wu, Jiliu Zhou, Xingchen Peng, Yan Wang</li>
<li>for: 这个研究目的是为了提高放射治疗规划中的剂量分布预测，并且提高效率和质量。</li>
<li>methods: 这种方法使用了散射模型，包括前向过程和反向过程。在前向过程中，DiffDose将剂量分布图Transform into pure Gaussian noise，并且同时训练了噪音预测器来估计添加的噪音。在反向过程中，它逐步除去噪音，最后输出预测的剂量分布图。</li>
<li>results: 这种方法可以有效地解决传统方法中的过度平滑问题，并且提高预测的精度和稳定性。<details>
<summary>Abstract</summary>
Deep learning (DL) has successfully automated dose distribution prediction in radiotherapy planning, enhancing both efficiency and quality. However, existing methods suffer from the over-smoothing problem for their commonly used L1 or L2 loss with posterior average calculations. To alleviate this limitation, we propose a diffusion model-based method (DiffDose) for predicting the radiotherapy dose distribution of cancer patients. Specifically, the DiffDose model contains a forward process and a reverse process. In the forward process, DiffDose transforms dose distribution maps into pure Gaussian noise by gradually adding small noise and a noise predictor is simultaneously trained to estimate the noise added at each timestep. In the reverse process, it removes the noise from the pure Gaussian noise in multiple steps with the well-trained noise predictor and finally outputs the predicted dose distribution maps...
</details>
<details>
<summary>摘要</summary>
深度学习（DL）已成功地自动预测辐射规划中的剂量分布，提高了效率和质量。然而，现有方法受到L1或L2损失函数的过滤问题的限制。为了解决这个限制，我们提出了基于扩散模型的剂量分布预测方法（DiffDose）。具体来说，DiffDose模型包括一个前进过程和一个反向过程。在前进过程中，DiffDose将剂量分布图转换成纯 Gaussian 噪声，通过逐步添加小噪声并同时训练噪声预测器来预测噪声添加的每个时间步。在反向过程中，它将纯 Gaussian 噪声中的噪声除掉，并在多个步骤中使用已经训练好的噪声预测器来除掉噪声，最后输出预测的剂量分布图。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Deep-Representation-Learning-via-Layerwise-Feature-Compression-and-Discrimination"><a href="#Understanding-Deep-Representation-Learning-via-Layerwise-Feature-Compression-and-Discrimination" class="headerlink" title="Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination"></a>Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02960">http://arxiv.org/abs/2311.02960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, Qing Qu</li>
<li>for: 本研究探讨了深度学习网络在层次结构中如何实现特征学习。</li>
<li>methods: 作者使用了深度Linear Network（DLN）来研究输入数据如何在每层训练后变换为特征。</li>
<li>results: 研究发现，在输入数据几乎正交的情况下，每层DLN都会在内类压缩和间类分化方面进行抽象，并且这种抽象会逐层增长。此外，研究还发现这种特征演化pattern在深度非线性网络中也存在，并且与现有的实验研究相吻合。此外，研究还展示了这些结果在转移学习中的实际应用。<details>
<summary>Abstract</summary>
Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is nearly orthogonal and the network weights are minimum-norm, balanced, and approximate low-rank: Each layer of the linear network progressively compresses within-class features at a geometric rate and discriminates between-class features at a linear rate with respect to the number of layers that data have passed through. To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep linear networks. Empirically, our extensive experiments not only validate our theoretical results numerically but also reveal a similar pattern in deep nonlinear networks which aligns well with recent empirical studies. Moreover, we demonstrate the practical implications of our results in transfer learning. Our code is available at \url{https://github.com/Heimine/PNC_DLN}.
</details>
<details>
<summary>摘要</summary>
过去一个 décennia，深度学习已经证明是一种非常有效的工具，可以从原始数据中学习有意义的特征。然而，仍然是一个打开的问题，深度网络在层次结构上如何进行层次特征学习。在这项工作中，我们尝试了解这个谜题，通过分析层次结构的中间特征来研究深度网络如何将输入数据转化为输出。为此，我们首先定义了内部特征的压缩和 между类特征的分化度量，并通过理论分析这两个度量，证明了深度线性网络中每层的特征演化遵循简单和量化的模式：在输入数据几乎正交的情况下，网络参数是最小二乘、均衡的和近似低维的情况下，每层的线性网络会逐步压缩内类特征，并在数据流过多层的情况下，分化between类特征。我们认为这是深度线性网络层次表示的特征演化的首次量化特征化。我们的实验结果不仅verify了我们的理论结果，还发现深度非线性网络中的特征演化呈现类似的模式，与最近的实验研究相吻合。此外，我们还展示了我们的结果在转移学习中的实践意义。我们的代码可以在<https://github.com/Heimine/PNC_DLN>上获取。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-learning-for-automatic-classification-of-multi-wavelength-auroral-images"><a href="#Multi-view-learning-for-automatic-classification-of-multi-wavelength-auroral-images" class="headerlink" title="Multi-view learning for automatic classification of multi-wavelength auroral images"></a>Multi-view learning for automatic classification of multi-wavelength auroral images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02947">http://arxiv.org/abs/2311.02947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuju Yang, Hang Su, Lili Liu, Yixuan Wang, Ze-Jun Hu</li>
<li>for:  auroral classification  auroral classification  Auroral classification plays a crucial role in polar research.</li>
<li>methods:  multi-wavelength fusion classification network, MLCNet, based on a multi-view approach.  including lightweight feature extraction backbone, called LCTNet, and a novel multi-scale reconstructed feature module named MSRM.</li>
<li>results:  fusion of multi-wavelength information effectively improves the auroral classification performance.  Specifically, our approach achieves state-of-the-art classification accuracy compared to previous auroral classification studies, and superior results in terms of accuracy and computational efficiency compared to existing multi-view methods.<details>
<summary>Abstract</summary>
Auroral classification plays a crucial role in polar research. However, current auroral classification studies are predominantly based on images taken at a single wavelength, typically 557.7 nm. Images obtained at other wavelengths have been comparatively overlooked, and the integration of information from multiple wavelengths remains an underexplored area. This limitation results in low classification rates for complex auroral patterns. Furthermore, these studies, whether employing traditional machine learning or deep learning approaches, have not achieved a satisfactory trade-off between accuracy and speed. To address these challenges, this paper proposes a lightweight auroral multi-wavelength fusion classification network, MLCNet, based on a multi-view approach. Firstly, we develop a lightweight feature extraction backbone, called LCTNet, to improve the classification rate and cope with the increasing amount of auroral observation data. Secondly, considering the existence of multi-scale spatial structures in auroras, we design a novel multi-scale reconstructed feature module named MSRM. Finally, to highlight the discriminative information between auroral classes, we propose a lightweight attention feature enhancement module called LAFE. The proposed method is validated using observational data from the Arctic Yellow River Station during 2003-2004. Experimental results demonstrate that the fusion of multi-wavelength information effectively improves the auroral classification performance. In particular, our approach achieves state-of-the-art classification accuracy compared to previous auroral classification studies, and superior results in terms of accuracy and computational efficiency compared to existing multi-view methods.
</details>
<details>
<summary>摘要</summary>
极光分类对极地研究起到关键作用，但目前的极光分类研究主要基于单一波长的图像，通常为557.7纳米。其他波长的图像得到了相对较少的关注，汇集多波长信息的研究仍处于不足explored领域。这种限制导致复杂的极光模式的分类率较低。此外，这些研究，无论使用传统机器学习还是深度学习方法，尚未达到了满意的准确率和速度均衡。为解决这些挑战，本文提出了一种轻量级极光多波长融合分类网络，称为MLCNet，基于多视图approach。首先，我们开发了一种轻量级特征提取Backbone，称为LCTNet，以提高分类率和应对增加的极光观测数据量。其次，考虑到极光中存在多尺度空间结构，我们设计了一种新的多尺度重建特征模块，称为MSRM。最后，为强调极光类别之间的区别信息，我们提出了一种轻量级注意特征增强模块，称为LAFE。我们的方法在Arctic Yellow River Station于2003-2004年的观测数据上进行验证。实验结果表明，将多波长信息融合分类效果显著提高了极光分类性能。尤其是，我们的方法与前一些极光分类研究相比，实现了状态机器学习和计算效率的优秀结果。
</details></li>
</ul>
<hr>
<h2 id="Truly-Scale-Equivariant-Deep-Nets-with-Fourier-Layers"><a href="#Truly-Scale-Equivariant-Deep-Nets-with-Fourier-Layers" class="headerlink" title="Truly Scale-Equivariant Deep Nets with Fourier Layers"></a>Truly Scale-Equivariant Deep Nets with Fourier Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02922">http://arxiv.org/abs/2311.02922</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ashiq24/scale_equivarinat_fourier_layer">https://github.com/ashiq24/scale_equivarinat_fourier_layer</a></li>
<li>paper_authors: Md Ashiqur Rahman, Raymond A. Yeh</li>
<li>for: 这个论文是为了研究如何实现scale-equivariant deep neural networks，以便在图像分割等任务中实现更好的性能。</li>
<li>methods: 该论文使用了Weight-sharing和kernel resizing等方法来实现scale-equivariant convolutional neural networks，但这些网络并不是在实践中真正具备scale-equivariance。</li>
<li>results: 该论文提出了一种基于Fourier层的新架构，可以实现绝对零equivariance-error的深度网络。该模型在MNIST-scale和STL-10 datasets上实现了竞争力的分类性能，同时保持了零equivariance-error。<details>
<summary>Abstract</summary>
In computer vision, models must be able to adapt to changes in image resolution to effectively carry out tasks such as image segmentation; This is known as scale-equivariance. Recent works have made progress in developing scale-equivariant convolutional neural networks, e.g., through weight-sharing and kernel resizing. However, these networks are not truly scale-equivariant in practice. Specifically, they do not consider anti-aliasing as they formulate the down-scaling operation in the continuous domain. To address this shortcoming, we directly formulate down-scaling in the discrete domain with consideration of anti-aliasing. We then propose a novel architecture based on Fourier layers to achieve truly scale-equivariant deep nets, i.e., absolute zero equivariance-error. Following prior works, we test this model on MNIST-scale and STL-10 datasets. Our proposed model achieves competitive classification performance while maintaining zero equivariance-error.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在计算机视觉中，模型需要适应图像分辨率变化以实现任务 such as 图像分割;这称为尺度对称性。近期工作已经在开发尺度对称 convolutional neural networks (CNNs)，例如通过共享权重和核心大小调整。但是，这些网络并不是实际上的尺度对称的。具体来说，它们没有考虑抗锯齿处理。为了解决这个缺陷，我们直接在精度领域中定义下降操作，并考虑抗锯齿处理。我们then proposed a novel architecture based on Fourier layers to achieve truly scale-equivariant deep nets, i.e., absolute zero equivariance-error. Following prior works, we test this model on MNIST-scale and STL-10 datasets. Our proposed model achieves competitive classification performance while maintaining zero equivariance-error.Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Deep-Facial-Expression-Recognition-An-Extensive-Protocol-with-Balanced-Dataset-in-the-Wild"><a href="#Benchmarking-Deep-Facial-Expression-Recognition-An-Extensive-Protocol-with-Balanced-Dataset-in-the-Wild" class="headerlink" title="Benchmarking Deep Facial Expression Recognition: An Extensive Protocol with Balanced Dataset in the Wild"></a>Benchmarking Deep Facial Expression Recognition: An Extensive Protocol with Balanced Dataset in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02910">http://arxiv.org/abs/2311.02910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianmarco Ipinze Tutuianu, Yang Liu, Ari Alamäki, Janne Kauttonen</li>
<li>for: 本研究旨在提高人机交互中的表情识别精度。</li>
<li>methods: 本研究使用了23种常用的网络架构，并采用了一致的协议进行评估。而且，研究者们还在不同的输入分辨率、类别平衡管理和预训练策略下进行了多种设置的测试，以确定它们对表情识别的影响。</li>
<li>results: 经过广泛的实验和十三个实际交互领域的跨频评估，研究者们得出了一份网络架构排名和深度表情识别方法的推荐。此外，研究者们还讨论了在实际应用中可能会遇到的伦理规则、隐私问题和法规。<details>
<summary>Abstract</summary>
Facial expression recognition (FER) is a crucial part of human-computer interaction. Existing FER methods achieve high accuracy and generalization based on different open-source deep models and training approaches. However, the performance of these methods is not always good when encountering practical settings, which are seldom explored. In this paper, we collected a new in-the-wild facial expression dataset for cross-domain validation. Twenty-three commonly used network architectures were implemented and evaluated following a uniform protocol. Moreover, various setups, in terms of input resolutions, class balance management, and pre-trained strategies, were verified to show the corresponding performance contribution. Based on extensive experiments on three large-scale FER datasets and our practical cross-validation, we ranked network architectures and summarized a set of recommendations on deploying deep FER methods in real scenarios. In addition, potential ethical rules, privacy issues, and regulations were discussed in practical FER applications such as marketing, education, and entertainment business.
</details>
<details>
<summary>摘要</summary>
人机交互中的表情识别（FER）是一项非常重要的技术。现有的FER方法在不同的开源深度学习模型和训练方法下已经实现了高准确率和泛化性。然而，这些方法在实际设置中的性能并不总是好很，这些实际设置通常被忽略。在这篇论文中，我们收集了一个新的在野外的表情数据集，用于跨频训练验证。我们实现了23种常用的网络架构，并按照一个统一的协议进行评估。此外，我们还验证了不同的输入分辨率、类别均衡管理和预训练策略的影响。基于大规模的FER数据集和我们的实际跨频验证，我们对深度FER方法的部署在实际场景中进行了排名和总结。此外，我们还讨论了实际应用中的伦理规则、隐私问题和法规。
</details></li>
</ul>
<hr>
<h2 id="Human-as-Points-Explicit-Point-based-3D-Human-Reconstruction-from-Single-view-RGB-Images"><a href="#Human-as-Points-Explicit-Point-based-3D-Human-Reconstruction-from-Single-view-RGB-Images" class="headerlink" title="Human as Points: Explicit Point-based 3D Human Reconstruction from Single-view RGB Images"></a>Human as Points: Explicit Point-based 3D Human Reconstruction from Single-view RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02892">http://arxiv.org/abs/2311.02892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yztang4/hap">https://github.com/yztang4/hap</a></li>
<li>paper_authors: Yingzhi Tang, Qijian Zhang, Junhui Hou, Yebin Liu</li>
<li>for: 这个论文的目的是为了提出一种基于点云的人体重建框架，以解决现有的深度学习人体重建方法存在的一些局限性和缺陷。</li>
<li>methods: 该方法使用了点云作为人体重建的中间表示，并采用了全部表示的点云估计、处理、生成和精度调整等技术。</li>
<li>results: 对于现有的state-of-the-art方法，该方法实现了20%到40%的性能提升，并且有更好的质量结果。这些结果表明，使用点云作为中间表示可以提高人体重建的灵活性和泛化能力。<details>
<summary>Abstract</summary>
The latest trends in the research field of single-view human reconstruction devote to learning deep implicit functions constrained by explicit body shape priors. Despite the remarkable performance improvements compared with traditional processing pipelines, existing learning approaches still show different aspects of limitations in terms of flexibility, generalizability, robustness, and/or representation capability. To comprehensively address the above issues, in this paper, we investigate an explicit point-based human reconstruction framework called HaP, which adopts point clouds as the intermediate representation of the target geometric structure. Technically, our approach is featured by fully-explicit point cloud estimation, manipulation, generation, and refinement in the 3D geometric space, instead of an implicit learning process that can be ambiguous and less controllable. The overall workflow is carefully organized with dedicated designs of the corresponding specialized learning components as well as processing procedures. Extensive experiments demonstrate that our framework achieves quantitative performance improvements of 20% to 40% over current state-of-the-art methods, and better qualitative results. Our promising results may indicate a paradigm rollback to the fully-explicit and geometry-centric algorithm design, which enables to exploit various powerful point cloud modeling architectures and processing techniques. We will make our code and data publicly available at https://github.com/yztang4/HaP.
</details>
<details>
<summary>摘要</summary>
最新的研究方向在单视人重建领域是学习深度隐函数，受到Explicit体形规范的约束。虽然现有的学习方法已经提高了处理管道的性能，但还有不同方面的局限性，包括灵活性、通用性、稳定性和/或表示能力。为全面解决以上问题，在这篇论文中，我们 investigate了一种叫做HaP的点云重建框架，该框架采用点云作为目标几何结构的中间表示。技术上，我们的方法具有完全Explicit的点云估计、处理、生成和精细化在3D几何空间中，而不是一种含糊不清的学习过程。整个 workflow 采用特定的专门设计的学习组件和处理程序。广泛的实验表明，我们的框架可以与当前状态艺术方法相比，提高了20%-40%的量化性能，并且有更好的质量结果。我们的成功结果可能意味着回归到完全Explicit和几何中心的算法设计，以便利用各种强大的点云模型化架构和处理技术。我们将在 GitHub 上公开代码和数据，请参考 https://github.com/yztang4/HaP。
</details></li>
</ul>
<hr>
<h2 id="Stacked-Autoencoder-Based-Feature-Extraction-and-Superpixel-Generation-for-Multifrequency-PolSAR-Image-Classification"><a href="#Stacked-Autoencoder-Based-Feature-Extraction-and-Superpixel-Generation-for-Multifrequency-PolSAR-Image-Classification" class="headerlink" title="Stacked Autoencoder Based Feature Extraction and Superpixel Generation for Multifrequency PolSAR Image Classification"></a>Stacked Autoencoder Based Feature Extraction and Superpixel Generation for Multifrequency PolSAR Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02887">http://arxiv.org/abs/2311.02887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tushar Gadhiya, Sumanth Tangirala, Anil K. Roy</li>
<li>for: 本研究提出了一种用于多频波特Synthetic Aperture Radar（PolSAR）图像的分类算法。</li>
<li>methods: 该算法使用PolSAR分解算法提取了每个频率带的33个特征，然后使用两层自适应归一化器减少输入特征向量中的维度，保留了输入特征的有用信息。接着，使用SLIC算法生成了超 пикsel，并使用了这些超 пикsel来构建一个强健的特征表示。</li>
<li>results: 在Flevoland数据集上进行了实验，并发现该方法在文献中已知的方法中显示出优异性。<details>
<summary>Abstract</summary>
In this paper we are proposing classification algorithm for multifrequency Polarimetric Synthetic Aperture Radar (PolSAR) image. Using PolSAR decomposition algorithms 33 features are extracted from each frequency band of the given image. Then, a two-layer autoencoder is used to reduce the dimensionality of input feature vector while retaining useful features of the input. This reduced dimensional feature vector is then applied to generate superpixels using simple linear iterative clustering (SLIC) algorithm. Next, a robust feature representation is constructed using both pixel as well as superpixel information. Finally, softmax classifier is used to perform classification task. The advantage of using superpixels is that it preserves spatial information between neighbouring PolSAR pixels and therefore minimises the effect of speckle noise during classification. Experiments have been conducted on Flevoland dataset and the proposed method was found to be superior to other methods available in the literature.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种多频率折射 Synthetic Aperture Radar（PolSAR）图像的分类算法。使用PolSAR分解算法提取了每个频率带的图像中的33个特征。然后，使用两层自适应神经网络减少输入特征向量的维度，保留输入特征的有用信息。这个减少后的特征向量然后用简单线性迭代归一化算法生成超像素。接下来，使用像素和超像素信息构建了一个稳定的特征表示。最后，使用softmax分类器进行分类任务。使用超像素的优点是它保留了邻近PolSAR像素之间的空间信息，因此减少了speckle噪声的影响，提高了分类的精度。在Flevoland dataset上进行了实验，并发现该方法与文献中其他方法相比，表现更优异。
</details></li>
</ul>
<hr>
<h2 id="Inner-IoU-More-Effective-Intersection-over-Union-Loss-with-Auxiliary-Bounding-Box"><a href="#Inner-IoU-More-Effective-Intersection-over-Union-Loss-with-Auxiliary-Bounding-Box" class="headerlink" title="Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box"></a>Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02877">http://arxiv.org/abs/2311.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Cong Xu, Shuaijie Zhang</li>
<li>for: 提高检测性能和精度</li>
<li>methods: 使用不同的缩放因子来控制auxiliary bounding box的大小，以适应不同的检测器和检测任务</li>
<li>results: 提出Inner-IoU损失函数，通过auxiliary bounding box计算IoU损失，可以further enhance detection performance and improve generalization ability<details>
<summary>Abstract</summary>
With the rapid development of detectors, Bounding Box Regression (BBR) loss function has constantly updated and optimized. However, the existing IoU-based BBR still focus on accelerating convergence by adding new loss terms, ignoring the limitations of IoU loss term itself. Although theoretically IoU loss can effectively describe the state of bounding box regression,in practical applications, it cannot adjust itself according to different detectors and detection tasks, and does not have strong generalization. Based on the above, we first analyzed the BBR model and concluded that distinguishing different regression samples and using different scales of auxiliary bounding boxes to calculate losses can effectively accelerate the bounding box regression process. For high IoU samples, using smaller auxiliary bounding boxes to calculate losses can accelerate convergence, while larger auxiliary bounding boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which calculates IoU loss through auxiliary bounding boxes. For different datasets and detectors, we introduce a scaling factor ratio to control the scale size of the auxiliary bounding boxes for calculating losses. Finally, integrate Inner-IoU into the existing IoU-based loss functions for simulation and comparative experiments. The experiment result demonstrate a further enhancement in detection performance with the utilization of the method proposed in this paper, verifying the effectiveness and generalization ability of Inner-IoU loss.
</details>
<details>
<summary>摘要</summary>
通过检测器的快速发展，矩形框回归（BBR）损失函数不断更新和优化。然而，现有的 IoU 基于的 BBR 仍然强调加入新的损失项，忽视 IoU 损失项本身的限制。虽然理论上 IoU 损失可以有效描述矩形框回归的状态，但在实际应用中，它无法根据不同的检测器和检测任务自适应调整，也不具备强大的通用性。基于以上分析，我们首先分析了 BBR 模型，并结论出了分类不同 regression 样本和使用不同缩放的帮助 bounding box 来计算损失可以有效加速矩形框回归过程。对高 IoU 样本，使用更小的帮助 bounding box 来计算损失可以加速收敛，而对低 IoU 样本，使用更大的帮助 bounding box 是合适的。然后，我们提出了 Inner-IoU 损失，它通过帮助 bounding box 来计算 IoU 损失。对不同的数据集和检测器，我们引入了涂抹因子比例来控制帮助 bounding box 的缩放大小。最后，我们将 Inner-IoU 集成到现有的 IoU 基于的损失函数中进行模拟和比较实验。实验结果表明，通过使用我们提出的方法，可以进一步提高检测性能，证明了 Inner-IoU 损失的有效性和通用性。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Neural-Fields-for-Learning-Atlases-of-4D-Fetal-MRI-Time-series"><a href="#Dynamic-Neural-Fields-for-Learning-Atlases-of-4D-Fetal-MRI-Time-series" class="headerlink" title="Dynamic Neural Fields for Learning Atlases of 4D Fetal MRI Time-series"></a>Dynamic Neural Fields for Learning Atlases of 4D Fetal MRI Time-series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02874">http://arxiv.org/abs/2311.02874</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kidrauh/neural-atlasing">https://github.com/kidrauh/neural-atlasing</a></li>
<li>paper_authors: Zeen Chi, Zhongxiao Cong, Clinton J. Wang, Yingcheng Liu, Esra Abaci Turk, P. Ellen Grant, S. Mazdak Abulnaga, Polina Golland, Neel Dey</li>
<li>for: 这种方法用于快速构建生物医学影像 Atlases，以便生物医学影像分析任务。</li>
<li>methods: 这种方法使用神经场来学习可变的空间时间观察。</li>
<li>results: 该方法可以快速构建高质量的生物医学影像 Atlases，并且比现有方法快得多。<details>
<summary>Abstract</summary>
We present a method for fast biomedical image atlas construction using neural fields. Atlases are key to biomedical image analysis tasks, yet conventional and deep network estimation methods remain time-intensive. In this preliminary work, we frame subject-specific atlas building as learning a neural field of deformable spatiotemporal observations. We apply our method to learning subject-specific atlases and motion stabilization of dynamic BOLD MRI time-series of fetuses in utero. Our method yields high-quality atlases of fetal BOLD time-series with $\sim$5-7$\times$ faster convergence compared to existing work. While our method slightly underperforms well-tuned baselines in terms of anatomical overlap, it estimates templates significantly faster, thus enabling rapid processing and stabilization of large databases of 4D dynamic MRI acquisitions. Code is available at https://github.com/Kidrauh/neural-atlasing
</details>
<details>
<summary>摘要</summary>
我们提出了一种快速生成医学图像 Atlases 的方法，使用神经场。 Atlases 是生物医学图像分析任务中关键的，但是传统的深度网络估计方法和深度网络估计方法仍然耗时很长。在这项初步工作中，我们将主动Specific atlas 的建立视为学习一个可变的空间时间观察的神经场。我们应用我们的方法来学习主动Specific atlas 和动态 BOLD MRI 时序列的运动稳定。我们的方法可以高质量地生成受孕妈妈动态 BOLD MRI 时序列的Atlas，并且比现有的工作快速 $\sim$5-7 $\times$。虽然我们的方法对于骨骼的覆盖率有所下降，但它可以更快地估计模板，从而实现大规模的动态 MRI 数据库的快速处理和稳定。代码可以在 <https://github.com/Kidrauh/neural-atlasing> 上获取。
</details></li>
</ul>
<hr>
<h2 id="OVIR-3D-Open-Vocabulary-3D-Instance-Retrieval-Without-Training-on-3D-Data"><a href="#OVIR-3D-Open-Vocabulary-3D-Instance-Retrieval-Without-Training-on-3D-Data" class="headerlink" title="OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data"></a>OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02873">http://arxiv.org/abs/2311.02873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shiyoung77/ovir-3d">https://github.com/shiyoung77/ovir-3d</a></li>
<li>paper_authors: Shiyang Lu, Haonan Chang, Eric Pu Jing, Abdeslam Boularias, Kostas Bekris</li>
<li>for: 该论文旨在实现无需使用任何3D数据进行训练的开 vocabulary 3D物体实例检索。</li>
<li>methods: 该方法使用文本查询和多视图融合来实现3D物体实例检索。文本查询和2D区域提档网络可以共同使用2D数据集，这些数据集通常更易доступible和更大。</li>
<li>results: 实验表明，该方法可以快速和有效地在大多数indoor 3D场景中进行实时多视图融合，并且不需要额外训练在3D空间。实验结果还表明，该方法在 робоット导航和制造中具有潜在应用前景。<details>
<summary>Abstract</summary>
This work presents OVIR-3D, a straightforward yet effective method for open-vocabulary 3D object instance retrieval without using any 3D data for training. Given a language query, the proposed method is able to return a ranked set of 3D object instance segments based on the feature similarity of the instance and the text query. This is achieved by a multi-view fusion of text-aligned 2D region proposals into 3D space, where the 2D region proposal network could leverage 2D datasets, which are more accessible and typically larger than 3D datasets. The proposed fusion process is efficient as it can be performed in real-time for most indoor 3D scenes and does not require additional training in 3D space. Experiments on public datasets and a real robot show the effectiveness of the method and its potential for applications in robot navigation and manipulation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FocusTune-Tuning-Visual-Localization-through-Focus-Guided-Sampling"><a href="#FocusTune-Tuning-Visual-Localization-through-Focus-Guided-Sampling" class="headerlink" title="FocusTune: Tuning Visual Localization through Focus-Guided Sampling"></a>FocusTune: Tuning Visual Localization through Focus-Guided Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02872">http://arxiv.org/abs/2311.02872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sontung/focus-tune">https://github.com/sontung/focus-tune</a></li>
<li>paper_authors: Son Tung Nguyen, Alejandro Fontan, Michael Milford, Tobias Fischer</li>
<li>for: 提高视觉地标算法的性能</li>
<li>methods: 使用FocusTune方法，通过利用关键 геометрические约束来导引场景坐标回归模型的训练</li>
<li>results: 在 Cambridge Landmarks 数据集上，与或超过了状态前的性能，而且保持了ACE模型的低存储和计算需求，例如将翻译错误从 25 到 19 和 17 到 15 cm，并且在单个和 ensemble 模型中实现了这一点。<details>
<summary>Abstract</summary>
We propose FocusTune, a focus-guided sampling technique to improve the performance of visual localization algorithms. FocusTune directs a scene coordinate regression model towards regions critical for 3D point triangulation by exploiting key geometric constraints. Specifically, rather than uniformly sampling points across the image for training the scene coordinate regression model, we instead re-project 3D scene coordinates onto the 2D image plane and sample within a local neighborhood of the re-projected points. While our proposed sampling strategy is generally applicable, we showcase FocusTune by integrating it with the recently introduced Accelerated Coordinate Encoding (ACE) model. Our results demonstrate that FocusTune both improves or matches state-of-the-art performance whilst keeping ACE's appealing low storage and compute requirements, for example reducing translation error from 25 to 19 and 17 to 15 cm for single and ensemble models, respectively, on the Cambridge Landmarks dataset. This combination of high performance and low compute and storage requirements is particularly promising for applications in areas like mobile robotics and augmented reality. We made our code available at \url{https://github.com/sontung/focus-tune}.
</details>
<details>
<summary>摘要</summary>
我们提出了FocusTune，一种帮助视觉本地化算法性能提高的集中样本技术。FocusTune利用场景坐标回归模型中关键的 геометрические约束，从而导引样本点在图像平面上的分布。具体来说，我们不再在图像上均匀采样点进行场景坐标回归模型的训练，而是将3D场景坐标重 проек到图像平面上，然后在当地邻域内采样。我们的提议的采样策略可以通用，但我们在ACE模型中示cases。我们的结果表明，FocusTune可以同时提高或与当前最佳性能匹配，而且ACE模型的存储和计算成本仍然很低，例如在 cambridge 标记集上，单个和集成模型的平均翻译误差从25降低到19和17cm。这种高性能低存储计算成本的组合特别有前途，特别是在移动 робо特和扩展现实中。我们的代码可以在 \url{https://github.com/sontung/focus-tune} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Neural-based-Compression-Scheme-for-Solar-Image-Data"><a href="#Neural-based-Compression-Scheme-for-Solar-Image-Data" class="headerlink" title="Neural-based Compression Scheme for Solar Image Data"></a>Neural-based Compression Scheme for Solar Image Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02855">http://arxiv.org/abs/2311.02855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Jeremy A. Grajeda, Piyush M. Mehta, Nasser M. Nasrabadi, Laura E. Boucheron, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva<br>for: 这个论文的目的是提出一种基于神经网络的丢弃式压缩方法，用于快速和高效地传输 NASA 的数据敏感图像数据。methods: 该方法使用一个 adversarially 培养的神经网络，具有本地和非本地注意模块，以捕捉图像的本地和全局结构，从而实现更好的Rate-Distortion 比较。此外，该方法还使用一个共同的前提模型来进行通道相关的 entropy 编码。results: 该方法在压缩极紫外线（EUV）数据时表现出了更高的Rate-Distortion 性能，比现有的 JPEG 和 JPEG-2000 编码器更高。此外，通过对 SDO 数据进行极紫外线洞（CH）检测和分割，实现了高效的压缩和分析。<details>
<summary>Abstract</summary>
Studying the solar system and especially the Sun relies on the data gathered daily from space missions. These missions are data-intensive and compressing this data to make them efficiently transferable to the ground station is a twofold decision to make. Stronger compression methods, by distorting the data, can increase data throughput at the cost of accuracy which could affect scientific analysis of the data. On the other hand, preserving subtle details in the compressed data requires a high amount of data to be transferred, reducing the desired gains from compression. In this work, we propose a neural network-based lossy compression method to be used in NASA's data-intensive imagery missions. We chose NASA's SDO mission which transmits 1.4 terabytes of data each day as a proof of concept for the proposed algorithm. In this work, we propose an adversarially trained neural network, equipped with local and non-local attention modules to capture both the local and global structure of the image resulting in a better trade-off in rate-distortion (RD) compared to conventional hand-engineered codecs. The RD variational autoencoder used in this work is jointly trained with a channel-dependent entropy model as a shared prior between the analysis and synthesis transforms to make the entropy coding of the latent code more effective. Our neural image compression algorithm outperforms currently-in-use and state-of-the-art codecs such as JPEG and JPEG-2000 in terms of the RD performance when compressing extreme-ultraviolet (EUV) data. As a proof of concept for use of this algorithm in SDO data analysis, we have performed coronal hole (CH) detection using our compressed images, and generated consistent segmentations, even at a compression rate of $\sim0.1$ bits per pixel (compared to 8 bits per pixel on the original data) using EUV data from SDO.
</details>
<details>
<summary>摘要</summary>
studying the solar system and especially the Sun 需要每天从空间任务中获取数据。这些任务是数据极其杂的，压缩这些数据以使其高效地传输到地面站是一个两重决策。更强大的压缩方法可以通过扭曲数据来增加数据传输速率，但是这会影响科学分析中的精度。另一方面，保留图像中的细节需要大量的数据被传输，这会降低愿意吸取的收益。在这种情况下，我们提出了基于神经网络的损失压缩方法，用于NASA的数据极其杂的图像任务。我们选择了NASA的SDO任务，每天传输1.4 terrabytes的数据作为证明。在这种情况下，我们提出了一个对抗学习神经网络，具有本地和非本地注意模块，以捕捉图像的本地和全局结构，从而实现更好的Rate-Distortion（RD）质量比。我们的神经图像压缩算法在JPEG和JPEG-2000等现有和状态最佳的编码器中表现出色，特别是在激发ultraviolet（EUV）数据压缩中。作为SDO数据分析的证明，我们通过我们压缩后的图像进行了核心孔（CH）检测，并获得了一致的分割，甚至在0.1 bits/像素的压缩率下（与原始数据的8 bits/像素相比）。
</details></li>
</ul>
<hr>
<h2 id="Consistent4D-Consistent-360°-Dynamic-Object-Generation-from-Monocular-Video"><a href="#Consistent4D-Consistent-360°-Dynamic-Object-Generation-from-Monocular-Video" class="headerlink" title="Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video"></a>Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02848">http://arxiv.org/abs/2311.02848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanqinJiang/Consistent4D">https://github.com/yanqinJiang/Consistent4D</a></li>
<li>paper_authors: Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, Yao Yao</li>
<li>for: 这 paper 的目的是生成基于单视图视频的4D动态对象。</li>
<li>methods: 该方法使用 Dynamic Neural Radiance Fields (DyNeRF) 模型，并提出了一种 Cascade DyNeRF 来确保稳定的启发和时间连续性。另外，该方法还引入了一种 Interpolation-driven Consistency Loss 来保证空间和时间一致性。</li>
<li>results: 经过广泛的实验，该方法可以与先前的方法相比竞争，同时也在文本到3D生成任务中表现优异。<details>
<summary>Abstract</summary>
In this paper, we present Consistent4D, a novel approach for generating 4D dynamic objects from uncalibrated monocular videos. Uniquely, we cast the 360-degree dynamic object reconstruction as a 4D generation problem, eliminating the need for tedious multi-view data collection and camera calibration. This is achieved by leveraging the object-level 3D-aware image diffusion model as the primary supervision signal for training Dynamic Neural Radiance Fields (DyNeRF). Specifically, we propose a Cascade DyNeRF to facilitate stable convergence and temporal continuity under the supervision signal which is discrete along the time axis. To achieve spatial and temporal consistency, we further introduce an Interpolation-driven Consistency Loss. It is optimized by minimizing the discrepancy between rendered frames from DyNeRF and interpolated frames from a pre-trained video interpolation model. Extensive experiments show that our Consistent4D can perform competitively to prior art alternatives, opening up new possibilities for 4D dynamic object generation from monocular videos, whilst also demonstrating advantage for conventional text-to-3D generation tasks. Our project page is https://consistent4d.github.io/.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，即Consistent4D，用于从无推定照视视频中生成4D动态对象。这种方法独特地将360度动态对象重建视为4D生成问题，从而消除了繁琐的多视图数据收集和摄像头准确性的需求。这是通过利用物体层3D意识图像扩散模型作为主要监督信号来训练动态神经频谱场（DyNeRF）来实现的。我们提出了一种升级的DyNeRF来促进稳定的受益和时间连续性，并在监督信号是时间轴上离散的情况下进行稳定的训练。为保证空间和时间一致，我们还引入了一种 interpolate-driven 一致损失。它通过将 DyNeRF 生成的帧与 interpolated 的帧进行比较，来减少生成的差异。我们进行了广泛的实验，结果表明，我们的 Consistent4D 可以与之前的方法相比竞争，开 up 新的可能性 для 4D动态对象生成从照视视频，同时也 demonstate 优势在传统文本到3D生成任务中。我们的项目页面是 <https://consistent4d.github.io/>。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Multi-Generator-Model-with-Fused-Spatiotemporal-Graph-for-Trajectory-Prediction"><a href="#Flexible-Multi-Generator-Model-with-Fused-Spatiotemporal-Graph-for-Trajectory-Prediction" class="headerlink" title="Flexible Multi-Generator Model with Fused Spatiotemporal Graph for Trajectory Prediction"></a>Flexible Multi-Generator Model with Fused Spatiotemporal Graph for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02835">http://arxiv.org/abs/2311.02835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyuan Zhu, Fengxia Han, Hao Deng</li>
<li>for: 这个研究旨在提高自动驾驶系统中的路径预测能力，以便更精确地追踪和决策。</li>
<li>methods: 我们提出了一个统合类型几何图表来更好地模型人群之间的复杂互动，以及一个多生成器架构，具有一个可选的生成器网络，以学习多个生成器的分布。</li>
<li>results: 我们的框架在不同的挑战性数据集上比较了几个基准之间表现出色，并且比以往的方法更好地预测了路径。<details>
<summary>Abstract</summary>
Trajectory prediction plays a vital role in automotive radar systems, facilitating precise tracking and decision-making in autonomous driving. Generative adversarial networks with the ability to learn a distribution over future trajectories tend to predict out-of-distribution samples, which typically occurs when the distribution of forthcoming paths comprises a blend of various manifolds that may be disconnected. To address this issue, we propose a trajectory prediction framework, which can capture the social interaction variations and model disconnected manifolds of pedestrian trajectories. Our framework is based on a fused spatiotemporal graph to better model the complex interactions of pedestrians in a scene, and a multi-generator architecture that incorporates a flexible generator selector network on generated trajectories to learn a distribution over multiple generators. We show that our framework achieves state-of-the-art performance compared with several baselines on different challenging datasets.
</details>
<details>
<summary>摘要</summary>
几何预测在自动驾驶系统中扮演着关键角色，帮助汽车实现精准跟踪和决策。生成对抗网络，能够学习未来轨迹的分布，通常会预测外部样本，这通常发生在未来路径分布中包含多种不同拓扑的混合。为解决这个问题，我们提出了一个几何预测框架，可以捕捉社交互动变化和模型断开拓扑的行人轨迹。我们的框架基于 fusion spatiotemporal graph来更好地模型场景中人员之间的复杂互动，并在生成的轨迹上使用灵活的生成器选择网络来学习多个生成器的分布。我们表明，我们的框架可以与多个基准达到最佳性能在不同的具有挑战性的数据集上。
</details></li>
</ul>
<hr>
<h2 id="SemanticTopoLoop-Semantic-Loop-Closure-With-3D-Topological-Graph-Based-on-Quadric-Level-Object-Map"><a href="#SemanticTopoLoop-Semantic-Loop-Closure-With-3D-Topological-Graph-Based-on-Quadric-Level-Object-Map" class="headerlink" title="SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map"></a>SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02831">http://arxiv.org/abs/2311.02831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenzhong Cao</li>
<li>For: 提高 loop closure 精度和可靠性，使 SLAM 系统在实际场景中更加稳定和准确。* Methods: 提出了一种基于多级验证的对象水平数据协调方法，并基于这种协调关系提出了一种基于对象地图拓扑的 semantics loop closure 方法。* Results: 对比 existed 状态艺术方法，我们的 semantic loop closure 方法在精度、准确性和本地化精度指标上具有明显的优势。<details>
<summary>Abstract</summary>
Loop closure, as one of the crucial components in SLAM, plays an essential role in correcting the accumulated errors. Traditional appearance-based methods, such as bag-of-words models, are often limited by local 2D features and the volume of training data, making them less versatile and robust in real-world scenarios, leading to missed detections or false positives detections in loop closure. To address these issues, we first propose a object-level data association method based on multi-level verification, which can associate 2D semantic features of current frame with 3D objects landmarks of map. Next, taking advantage of these association relations, we introduce a semantic loop closure method based on quadric-level object map topology, which represents scenes through the topological graph of objects and achieves accurate loop closure at a wide field of view by comparing differences in the topological graphs. Finally, we integrate these two methods into a complete object-aware SLAM system. Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm. Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall and localization accuracy metrics.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT Loop closure, as one of the crucial components in SLAM, plays an essential role in correcting the accumulated errors. Traditional appearance-based methods, such as bag-of-words models, are often limited by local 2D features and the volume of training data, making them less versatile and robust in real-world scenarios, leading to missed detections or false positives detections in loop closure. To address these issues, we first propose a object-level data association method based on multi-level verification, which can associate 2D semantic features of current frame with 3D objects landmarks of map. Next, taking advantage of these association relations, we introduce a semantic loop closure method based on quadric-level object map topology, which represents scenes through the topological graph of objects and achieves accurate loop closure at a wide field of view by comparing differences in the topological graphs. Finally, we integrate these two methods into a complete object-aware SLAM system. Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm. Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall and localization accuracy metrics.TRANSLATE_TEXTHere's the translation in Simplified Chinese:<<SYS>>环closure，作为SLAM中关键组件，对各种累积错误进行更正。传统的外观基于方法，如袋子模型，常被当地2D特征所限制，并且需要大量训练数据，从而使其在实际场景中变得更加不灵活和稳定，导致环closure中的错误检测。为解决这些问题，我们首先提出了基于多级验证的对象水平数据归一化方法，可以将当前帧的2D semantics特征与地图中的3D对象标记相关联。然后，利用这些归一化关系，我们引入了基于对象图像拓扑的语义环closure方法，可以通过比较对象图像拓扑的差异来实现精度的环closure。最后，我们将这两种方法集成到了一个完整的对象意识SLAM系统中。Qualitative experiments和ablation study表明我们提出的对象水平数据归一化算法的效果和稳定性。Quantitative experiments表明我们的语义环closure方法在精度、检测率和本地化精度等指标上超过现有状态 искусственный智能方法。TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="InstructPix2NeRF-Instructed-3D-Portrait-Editing-from-a-Single-Image"><a href="#InstructPix2NeRF-Instructed-3D-Portrait-Editing-from-a-Single-Image" class="headerlink" title="InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image"></a>InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02826">http://arxiv.org/abs/2311.02826</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mybabyyh/instructpix2nerf">https://github.com/mybabyyh/instructpix2nerf</a></li>
<li>paper_authors: Jianhui Li, Shilong Liu, Zidong Liu, Yikai Wang, Kaiwen Zheng, Jinghui Xu, Jianmin Li, Jun Zhu<br>for:This paper focuses on the problem of human-instructed 3D-aware portrait editing for open-world images, which has not been well-explored due to the lack of labeled human face 3D datasets and effective architectures.methods:The proposed method, InstructPix2NeRF, is an end-to-end diffusion-based framework that lifts 2D editing to 3D space by learning the correlation between the paired images’ difference and the instructions via triplet data. The method uses a conditional latent 3D diffusion process and a proposed token position randomization strategy to achieve multi-semantic editing with portrait identity preservation.results:Extensive experiments show the effectiveness of the proposed method and its superiority against strong baselines quantitatively and qualitatively. The method is able to achieve multi-semantic editing with portrait identity preservation, and the proposed identity consistency module increases the multi-view 3D identity consistency.<details>
<summary>Abstract</summary>
With the success of Neural Radiance Field (NeRF) in 3D-aware portrait editing, a variety of works have achieved promising results regarding both quality and 3D consistency. However, these methods heavily rely on per-prompt optimization when handling natural language as editing instructions. Due to the lack of labeled human face 3D datasets and effective architectures, the area of human-instructed 3D-aware editing for open-world portraits in an end-to-end manner remains under-explored. To solve this problem, we propose an end-to-end diffusion-based framework termed InstructPix2NeRF, which enables instructed 3D-aware portrait editing from a single open-world image with human instructions. At its core lies a conditional latent 3D diffusion process that lifts 2D editing to 3D space by learning the correlation between the paired images' difference and the instructions via triplet data. With the help of our proposed token position randomization strategy, we could even achieve multi-semantic editing through one single pass with the portrait identity well-preserved. Besides, we further propose an identity consistency module that directly modulates the extracted identity signals into our diffusion process, which increases the multi-view 3D identity consistency. Extensive experiments verify the effectiveness of our method and show its superiority against strong baselines quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
成功的神经辐射场（NeRF）在3D意识编辑中取得了出色的成果，许多研究在质量和3D一致性方面均取得了显著进步。然而，这些方法在处理自然语言编辑指令时仍然依赖于每个提示的优化。由于人脸3D数据集缺乏标注和有效的建筑，人类给出的3D意识编辑在开放世界照片上的端到端方式仍然受到了尚未探索的问题。为解决这个问题，我们提出了一个终端扩散基于的框架，称为InstructPix2NeRF，它允许根据单个开放世界图像和人类指令来进行指定3D意识编辑。核心在于一种 conditional 粒子扩散过程，通过学习对彩色图像差异和指令之间的相关性来提升2D编辑到3D空间。通过我们提出的token位置随机策略，我们可以在一个单个过程中实现多个Semantic编辑，并且保持人脸identität。此外，我们还提出了一种标识一致模块，该模块直接将提取的标识信号加入我们的扩散过程中，从而提高多视图3D标识一致性。广泛的实验证明了我们的方法的有效性，并在量和质量上与强基线相比较。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Self-Supervised-Human-Pose-Estimation-with-Inductive-Prior-Tuning"><a href="#Efficient-Self-Supervised-Human-Pose-Estimation-with-Inductive-Prior-Tuning" class="headerlink" title="Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning"></a>Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02815">http://arxiv.org/abs/2311.02815</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princetonvisualai/hpe-inductive-prior-tuning">https://github.com/princetonvisualai/hpe-inductive-prior-tuning</a></li>
<li>paper_authors: Nobline Yoo, Olga Russakovsky</li>
<li>for: 本研究旨在提高无监督人体 pose 估计（HPE）的自动化精度。</li>
<li>methods: 我们使用了一种自适应的模型架构，并对其进行了优化，以使其在使用少量训练数据时可以达到更高的准确率。</li>
<li>results: 我们的实验结果显示，我们的模型可以在使用少量训练数据时达到比基eline更高的准确率，并且可以更好地考虑人体 pose 的各个部分之间的相对长度差异。<details>
<summary>Abstract</summary>
The goal of 2D human pose estimation (HPE) is to localize anatomical landmarks, given an image of a person in a pose. SOTA techniques make use of thousands of labeled figures (finetuning transformers or training deep CNNs), acquired using labor-intensive crowdsourcing. On the other hand, self-supervised methods re-frame the HPE task as a reconstruction problem, enabling them to leverage the vast amount of unlabeled visual data, though at the present cost of accuracy. In this work, we explore ways to improve self-supervised HPE. We (1) analyze the relationship between reconstruction quality and pose estimation accuracy, (2) develop a model pipeline that outperforms the baseline which inspired our work, using less than one-third the amount of training data, and (3) offer a new metric suitable for self-supervised settings that measures the consistency of predicted body part length proportions. We show that a combination of well-engineered reconstruction losses and inductive priors can help coordinate pose learning alongside reconstruction in a self-supervised paradigm.
</details>
<details>
<summary>摘要</summary>
目标是二维人体姿势估计（HPE）是将人体在姿势中的 анатомиче�� landmarks 的 localization。现有最佳技术使用了 thousands of 标注的图像（finetuning transformers 或 training deep CNNs），通过劳动密集的人工投票获得。然而，自主学习方法将 HPE 任务重新定义为一个重建问题，可以利用大量的无标注视觉数据，但目前精度相对较低。在这项工作中，我们探索了如何提高自主学习HPE。我们（1）分析了重建质量和姿势估计准确性之间的关系，（2）开发了一个超过基线的模型管道，使用 less than one-third 的训练数据，（3）提出了适合自主学习设置的一个新的度量，可以测量预测人体部分长度的一致性。我们表明，将高效的重建损失和推导约束结合在一起可以协调姿势学习和重建在自主学习模式下。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Interpretable-Face-Identification-for-Out-Of-Distribution-Data-Using-Vision-Transformers"><a href="#Fast-and-Interpretable-Face-Identification-for-Out-Of-Distribution-Data-Using-Vision-Transformers" class="headerlink" title="Fast and Interpretable Face Identification for Out-Of-Distribution Data Using Vision Transformers"></a>Fast and Interpretable Face Identification for Out-Of-Distribution Data Using Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02803">http://arxiv.org/abs/2311.02803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Phan, Cindy Le, Vu Le, Yihui He, Anh Totti Nguyen</li>
<li>for: 提高face identification的准确率和可解释性</li>
<li>methods: 使用2个图像的Vision Transformers（ViTs）对比两个图像的patch уровень，通过cross-attention进行比较</li>
<li>results: 与DeepFace-EMD相当的准确率，但速度比DeepFace-EMD快得多，并且通过人类研究显示出了可解释性的优势<details>
<summary>Abstract</summary>
Most face identification approaches employ a Siamese neural network to compare two images at the image embedding level. Yet, this technique can be subject to occlusion (e.g. faces with masks or sunglasses) and out-of-distribution data. DeepFace-EMD (Phan et al. 2022) reaches state-of-the-art accuracy on out-of-distribution data by first comparing two images at the image level, and then at the patch level. Yet, its later patch-wise re-ranking stage admits a large $O(n^3 \log n)$ time complexity (for $n$ patches in an image) due to the optimal transport optimization. In this paper, we propose a novel, 2-image Vision Transformers (ViTs) that compares two images at the patch level using cross-attention. After training on 2M pairs of images on CASIA Webface (Yi et al. 2014), our model performs at a comparable accuracy as DeepFace-EMD on out-of-distribution data, yet at an inference speed more than twice as fast as DeepFace-EMD (Phan et al. 2022). In addition, via a human study, our model shows promising explainability through the visualization of cross-attention. We believe our work can inspire more explorations in using ViTs for face identification.
</details>
<details>
<summary>摘要</summary>
大多数面部识别方法使用同一个神经网络来比较两个图像的图像嵌入水平。然而，这种技术可能会受到遮盖物（如面具或太阳镜）和非典型数据的影响。深度Face-EMD（phan et al. 2022）达到了非典型数据上的状态态顶峰性能，通过首先比较两个图像的图像水平，然后是图像中的小块水平。然而，其后的小块排序阶段的优化问题采用了最优运输算符，导致了 $O(n^3 \log n)$ 时间复杂度（其中 $n$ 是图像中的小块数）。在这篇论文中，我们提出了一种新的、两个图像的视图转换器（ViTs），用于比较两个图像的小块水平。经过在 CASIA Webface（Yi et al. 2014）上训练 200 万对图像，我们的模型在非典型数据上达到了与 DeepFace-EMD 相当的准确率，并且在推断速度方面比 DeepFace-EMD 更快速。此外，通过人类研究，我们的模型表现出了可解释的特点，可以通过跨域抗注意力的视觉化来解释。我们认为，我们的工作可以激励更多的人们在使用 ViTs 进行面部识别。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.CV_2023_11_06/" data-id="cloqtaesa00ldgh88d6bs9qk6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/06/eess.AS_2023_11_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-11-06
        
      </div>
    </a>
  
  
    <a href="/2023/11/06/cs.AI_2023_11_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-06</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">117</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">68</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">50</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
