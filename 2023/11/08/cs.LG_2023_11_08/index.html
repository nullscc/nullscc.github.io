
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-11-08 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Optimized measurements of chaotic dynamical systems via the information bottleneck paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04896 repo_url: None paper_authors: Kieran A. Murphy, Dani S. Bassett for: 这篇论文目">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-11-08">
<meta property="og:url" content="https://nullscc.github.io/2023/11/08/cs.LG_2023_11_08/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Optimized measurements of chaotic dynamical systems via the information bottleneck paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04896 repo_url: None paper_authors: Kieran A. Murphy, Dani S. Bassett for: 这篇论文目">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-08T10:00:00.000Z">
<meta property="article:modified_time" content="2023-11-09T07:48:31.916Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/cs.LG_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T10:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-11-08
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Optimized-measurements-of-chaotic-dynamical-systems-via-the-information-bottleneck"><a href="#Optimized-measurements-of-chaotic-dynamical-systems-via-the-information-bottleneck" class="headerlink" title="Optimized measurements of chaotic dynamical systems via the information bottleneck"></a>Optimized measurements of chaotic dynamical systems via the information bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04896">http://arxiv.org/abs/2311.04896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kieran A. Murphy, Dani S. Bassett</li>
<li>for: 这篇论文目的是提出一种基于机器学习的方法来提取漫步数据中的信息，以提高信息的效率和可靠性。</li>
<li>methods: 这篇论文使用了一种 variants of the information bottleneck 来实现一种准确的测量方法，并使用机器学习来优化测量过程。</li>
<li>results: 研究人员通过使用这种方法在多种杂态图中获得了约等效的测量结果，并为普通时间序列提供了一种有效的信息提取方法。<details>
<summary>Abstract</summary>
Deterministic chaos permits a precise notion of a "perfect measurement" as one that, when obtained repeatedly, captures all of the information created by the system's evolution with minimal redundancy. Finding an optimal measurement is challenging, and has generally required intimate knowledge of the dynamics in the few cases where it has been done. We establish an equivalence between a perfect measurement and a variant of the information bottleneck. As a consequence, we can employ machine learning to optimize measurement processes that efficiently extract information from trajectory data. We obtain approximately optimal measurements for multiple chaotic maps and lay the necessary groundwork for efficient information extraction from general time series.
</details>
<details>
<summary>摘要</summary>
tranlate_input = "Deterministic chaos permits a precise notion of a 'perfect measurement' as one that, when obtained repeatedly, captures all of the information created by the system's evolution with minimal redundancy. Finding an optimal measurement is challenging, and has generally required intimate knowledge of the dynamics in the few cases where it has been done. We establish an equivalence between a perfect measurement and a variant of the information bottleneck. As a consequence, we can employ machine learning to optimize measurement processes that efficiently extract information from trajectory data. We obtain approximately optimal measurements for multiple chaotic maps and lay the necessary groundwork for efficient information extraction from general time series."translate_output = translate_input.translate(to_language='zh-CN').font_substitute(    keywords=['deterministic chaos', 'perfect measurement', 'information bottleneck', 'machine learning', 'trajectory data', 'chaotic maps', 'time series'],    to_language='zh-CN')print(translate_output)# Output:<<SYS>通用化的混沌允许我们对系统演化过程中创造的信息进行精确的捕捉，并且可以通过重复获得这种准确的测量，以Minimize redundancy。找到最佳测量是一项挑战，通常需要对系统动力学有深入的了解。我们证明了一种等价关系，即完美测量与信息瓶颈之间的等价关系。这意味着我们可以通过机器学习来优化测量过程，以 efficiently Extract information from trajectory data。我们对多个混沌图进行了 approximately optimal measurements，并为通用时间序列提供了必要的基础。Note: The above output is in Simplified Chinese. The `font_substitute` function is used to replace certain keywords with their Simplified Chinese equivalents. The `to_language` parameter is set to `'zh-CN'` to specify that the output should be in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Computing-with-Residue-Numbers-in-High-Dimensional-Representation"><a href="#Computing-with-Residue-Numbers-in-High-Dimensional-Representation" class="headerlink" title="Computing with Residue Numbers in High-Dimensional Representation"></a>Computing with Residue Numbers in High-Dimensional Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04872">http://arxiv.org/abs/2311.04872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cjkymn/residuehdcomputing">https://github.com/cjkymn/residuehdcomputing</a></li>
<li>paper_authors: Christopher J. Kymn, Denis Kleyko, E. Paxon Frady, Connor Bybee, Pentti Kanerva, Friedrich T. Sommer, Bruno A. Olshausen</li>
<li>for: 这篇论文是为了提出一种新的计算框架，即循环高维计算（Residue Hyperdimensional Computing），用于解决计算复杂的问题。</li>
<li>methods: 该框架使用了循环数字系统和高维随机向量的代数运算，并使用高维向量的分解方法来实现对数值的表示和操作。这些操作可以是分组、并行的，需要 fewer 资源 than previous methods。</li>
<li>results: 研究人员通过应用这种框架解决了一些计算难题，包括视觉认知和 combinatorial 优化问题，并与基eline方法进行比较。此外，这种框架还可能地解释大脑Grid cells的计算操作，并提供了新的机器学习架构来表示和操作数字数据。<details>
<summary>Abstract</summary>
We introduce Residue Hyperdimensional Computing, a computing framework that unifies residue number systems with an algebra defined over random, high-dimensional vectors. We show how residue numbers can be represented as high-dimensional vectors in a manner that allows algebraic operations to be performed with component-wise, parallelizable operations on the vector elements. The resulting framework, when combined with an efficient method for factorizing high-dimensional vectors, can represent and operate on numerical values over a large dynamic range using vastly fewer resources than previous methods, and it exhibits impressive robustness to noise. We demonstrate the potential for this framework to solve computationally difficult problems in visual perception and combinatorial optimization, showing improvement over baseline methods. More broadly, the framework provides a possible account for the computational operations of grid cells in the brain, and it suggests new machine learning architectures for representing and manipulating numerical data.
</details>
<details>
<summary>摘要</summary>
我团队介绍了剩余超维计算框架，这是一种将剩余数系统与随机高维向量上定义的代数结合起来的计算框架。我们示示了如何将剩余数转换为高维向量的方式，以便在向量元素上进行分 Component-wise、并行化的运算。这种框架，当与高维向量分解方法相结合时，可以使用较少的资源来表示和处理大Dynamic range的数值，并且具有很好的鲁棒性 against noise。我们示示了这种框架可以解决计算困难的问题，如视觉recognition和组合优化问题，并且比基eline方法有所提高。更重要的是，这种框架可能对大脑Grid cells的计算操作提供了一种可能的解释，并且建议了一种新的机器学习架构来表示和操作数字数据。
</details></li>
</ul>
<hr>
<h2 id="Algorithms-for-Non-Negative-Matrix-Factorization-on-Noisy-Data-With-Negative-Values"><a href="#Algorithms-for-Non-Negative-Matrix-Factorization-on-Noisy-Data-With-Negative-Values" class="headerlink" title="Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values"></a>Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04855">http://arxiv.org/abs/2311.04855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dylan Green, Stephen Bailey</li>
<li>for: This paper is written for analyzing noisy data, especially astronomical data, which may contain negative values due to noise.</li>
<li>methods: The paper presents two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and any introduced negativity without clipping the data.</li>
<li>results: The algorithms are proven to have monotonically decreasing update rules and can correctly recover non-negative signals without any introduced positive offset. The paper demonstrates the effectiveness of the algorithms on both simple and more realistic examples.<details>
<summary>Abstract</summary>
Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotonically decreasing update rules.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Incorporating-temporal-dynamics-of-mutations-to-enhance-the-prediction-capability-of-antiretroviral-therapy’s-outcome-for-HIV-1"><a href="#Incorporating-temporal-dynamics-of-mutations-to-enhance-the-prediction-capability-of-antiretroviral-therapy’s-outcome-for-HIV-1" class="headerlink" title="Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy’s outcome for HIV-1"></a>Incorporating temporal dynamics of mutations to enhance the prediction capability of antiretroviral therapy’s outcome for HIV-1</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04846">http://arxiv.org/abs/2311.04846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulia Di Teodoro, Martin Pirkl, Francesca Incardona, Ilaria Vicenti, Anders Sönnerborg, Rolf Kaiser, Laura Palagi, Maurizio Zazzi, Thomas Lengauer</li>
<li>for: This paper aims to determine whether using historical information can improve the accuracy of predicting HIV therapy outcomes compared to using only current data.</li>
<li>methods: The authors introduce a method to weigh viral mutations based on their temporal occurrence and concomitant viral load measurements, and compare a model that includes historical information (H-model) with one that does not (NH-model).</li>
<li>results: The H-model demonstrates superior discriminative ability, with a higher ROC-AUC score (76.34%) than the NH-model (74.98%). Incorporating historical information improves consistently predictive accuracy for treatment outcomes, and the better performance of the H-model may be attributed to its consideration of latent HIV reservoirs.Here are the three points in Simplified Chinese text:</li>
<li>for: 这项研究的目的是判断 whether 使用历史信息可以提高 HIV 治疗结果预测的准确性，相比使用只有当前数据。</li>
<li>methods: 作者们引入了一种基于时间的权重方法，用于评估病毒变异的重要性，并对比一个不包含历史信息的模型（NH-model）和一个包含历史信息的模型（H-model）。</li>
<li>results: H-model 表现出了更高的 ROC-AUC 分数（76.34%），与 NH-model 的分数（74.98%）相比，这表明 incorporating 历史信息 可以提高预测结果的准确性。<details>
<summary>Abstract</summary>
Motivation: In predicting HIV therapy outcomes, a critical clinical question is whether using historical information can enhance predictive capabilities compared with current or latest available data analysis. This study analyses whether historical knowledge, which includes viral mutations detected in all genotypic tests before therapy, their temporal occurrence, and concomitant viral load measurements, can bring improvements. We introduce a method to weigh mutations, considering the previously enumerated factors and the reference mutation-drug Stanford resistance tables. We compare a model encompassing history (H) with one not using it (NH). Results: The H-model demonstrates superior discriminative ability, with a higher ROC-AUC score (76.34%) than the NH-model (74.98%). Significant Wilcoxon test results confirm that incorporating historical information improves consistently predictive accuracy for treatment outcomes. The better performance of the H-model might be attributed to its consideration of latent HIV reservoirs, probably obtained when leveraging historical information. The findings emphasize the importance of temporal dynamics in mutations, offering insights into HIV infection complexities. However, our result also shows that prediction accuracy remains relatively high even when no historical information is available. Supplementary information: Supplementary material is available.
</details>
<details>
<summary>摘要</summary>
目的：判断HIV治疗结果的估计，一个关键的临床问题是 Whether using historical information can enhance predictive capabilities compared with current or latest available data analysis. This study examines whether historical knowledge, including viral mutations detected in all genotypic tests before therapy, their temporal occurrence, and concomitant viral load measurements, can bring improvements. We propose a method to weigh mutations, considering the previously enumerated factors and the reference mutation-drug Stanford resistance tables. We compare a model encompassing history (H) with one not using it (NH).结果：H-model 的抑制能力显示出优于 NH-model 的 ROC-AUC 分数 (76.34% vs. 74.98%), 并且 Wilcoxon 测试结果表明，使用历史信息可逐止预测结果的准确性。 H-model 的更好表现可能是由于其考虑了潜在的 HIV 储存库，可能是通过历史信息获得的。 这些结果强调了HIV感染的复杂性，并且表明了时间动态的病毒变异对预测结果的重要性。 然而，我们的结果也表明，即使没有历史信息，预测结果的准确性仍然相对高。补充信息：补充材料可以在附录中找到。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Dimensions-Confident-Reachability-for-High-Dimensional-Controllers"><a href="#Bridging-Dimensions-Confident-Reachability-for-High-Dimensional-Controllers" class="headerlink" title="Bridging Dimensions: Confident Reachability for High-Dimensional Controllers"></a>Bridging Dimensions: Confident Reachability for High-Dimensional Controllers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04843">http://arxiv.org/abs/2311.04843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuang Geng, Souradeep Dutta, Ivan Ruchkin</li>
<li>for: 这 paper 的目的是将高维控制器的验证方法与高维图像感知相连接。</li>
<li>methods: 这 paper 使用了验证感知知识填充来将高维控制器的行为approximation为低维控制器的行为。</li>
<li>results: 这 paper 的研究结果表明，通过使用验证感知知识填充，可以提供高维控制器的高 confidence 的可达性保证。两种inflation技术（基于轨迹和行为）在 OpenAI gym  benchmark 中表现出色。<details>
<summary>Abstract</summary>
Autonomous systems are increasingly implemented using end-end-end trained controllers. Such controllers make decisions that are executed on the real system with images as one of the primary sensing modalities. Deep neural networks form a fundamental building block of such controllers. Unfortunately, the existing neural-network verification tools do not scale to inputs with thousands of dimensions. Especially when the individual inputs (such as pixels) are devoid of clear physical meaning. This paper takes a step towards connecting exhaustive closed-loop verification with high-dimensional controllers. Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space. To balance approximation and verifiability, we leverage the latest verification-aware knowledge distillation. Then, if low-dimensional reachability results are inflated with statistical approximation errors, they yield a high-confidence reachability guarantee for the high-dimensional controller. We investigate two inflation techniques -- based on trajectories and actions -- both of which show convincing performance in two OpenAI gym benchmarks.
</details>
<details>
<summary>摘要</summary>
Our key insight is that the behavior of a high-dimensional controller can be approximated with several low-dimensional controllers in different regions of the state space. To balance approximation and verifiability, we leverage the latest verification-aware knowledge distillation. If low-dimensional reachability results are inflated with statistical approximation errors, they yield a high-confidence reachability guarantee for the high-dimensional controller.We investigate two inflation techniques based on trajectories and actions, both of which show convincing performance in two OpenAI gym benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Toward-Rapid-Optimal-and-Feasible-Power-Dispatch-through-Generalized-Neural-Mapping"><a href="#Toward-Rapid-Optimal-and-Feasible-Power-Dispatch-through-Generalized-Neural-Mapping" class="headerlink" title="Toward Rapid, Optimal, and Feasible Power Dispatch through Generalized Neural Mapping"></a>Toward Rapid, Optimal, and Feasible Power Dispatch through Generalized Neural Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04838">http://arxiv.org/abs/2311.04838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meiyi Li, Javad Mohammadi<br>for: 这个论文的目的是提高大规模的电力系统优化过程中的决策效率，使用机器学习（ML）技术来改善优化过程。methods: 本文使用的方法是一种名为“学习来优化优化过程”的学习型方法，并且提出了一新的“通用措准图”方法来将不可行的解析转换为可行的解析。results: 本文的结果显示，使用LOOP-LC 2.0方法可以实现近乎最佳的解析结果，并且可以在硬件条件范围内确保解析的可行性。另外，与现有方法相比，LOOP-LC 2.0方法的训练速度、计算时间、优化性和可行性都有所提高。<details>
<summary>Abstract</summary>
The evolution towards a more distributed and interconnected grid necessitates large-scale decision-making within strict temporal constraints. Machine learning (ML) paradigms have demonstrated significant potential in improving the efficacy of optimization processes. However, the feasibility of solutions derived from ML models continues to pose challenges. It's imperative that ML models produce solutions that are attainable and realistic within the given system constraints of power systems. To address the feasibility issue and expedite the solution search process, we proposed LOOP-LC 2.0(Learning to Optimize the Optimization Process with Linear Constraints version 2.0) as a learning-based approach for solving the power dispatch problem. A notable advantage of the LOOP-LC 2.0 framework is its ability to ensure near-optimality and strict feasibility of solutions without depending on computationally intensive post-processing procedures, thus eliminating the need for iterative processes. At the heart of the LOOP-LC 2.0 model lies the newly proposed generalized gauge map method, capable of mapping any infeasible solution to a feasible point within the linearly-constrained domain. The proposed generalized gauge map method improves the traditional gauge map by exhibiting reduced sensitivity to input variances while increasing search speeds significantly. Utilizing the IEEE-200 test case as a benchmark, we demonstrate the effectiveness of the LOOP-LC 2.0 methodology, confirming its superior performance in terms of training speed, computational time, optimality, and solution feasibility compared to existing methodologies.
</details>
<details>
<summary>摘要</summary>
随着grid的分布化和连接性的演化，大规模决策在 строги时间限制下进行了大规模决策。机器学习（ML） paradigms 已经表现出了提高优化过程的潜在能力。然而，ML模型生成的解决方案的可行性仍然存在挑战。为了解决可行性问题并加速解决过程，我们提出了LOOP-LC 2.0（学习优化优化过程的学习方法版2.0）作为一种基于学习的电力调度问题的解决方案。LOOP-LC 2.0框架的一个重要优势是它可以保证解决方案的准确性和可行性，而不需要 computationally intensive post-processing procedures，因此可以消除迭代过程。LOOP-LC 2.0模型的核心 lies the newly proposed generalized gauge map method, capable of mapping any infeasible solution to a feasible point within the linearly-constrained domain.这种新的总体监测方法比传统的监测方法具有更低的输入方差敏感度，同时提高了搜索速度。使用IEEE-200测试集作为标准，我们证明了LOOP-LC 2.0方法的效iveness，其比现有方法ologies具有更高的训练速度、计算时间、优化性和可行性。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Recurrent-Reinforcement-Learning"><a href="#Real-Time-Recurrent-Reinforcement-Learning" class="headerlink" title="Real-Time Recurrent Reinforcement Learning"></a>Real-Time Recurrent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04830">http://arxiv.org/abs/2311.04830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Julian Lemmel, Radu Grosu</li>
<li>for: 解决 partially-observable Markov decision processes (POMDPs) 中的奖励学习问题</li>
<li>methods: 使用 random feedback local online learning (RFLO) 和 temporaldifference reinforcement learning with eligibility traces (TD($\lambda$)) 组成一种生物学可能的，循环神经网络算法</li>
<li>results: RFLO 可以与 RTRL 相当，而且在复杂性方面超过 BPTT，并且可以解决离散和连续控制任务在 POMDPs 中Here is the same information in Simplified Chinese text:</li>
<li>for: 解决 partially-observable Markov decision processes (POMDPs) 中的奖励学习问题</li>
<li>methods: 使用 random feedback local online learning (RFLO) 和 temporaldifference reinforcement learning with eligibility traces (TD($\lambda$)) 组成一种生物学可能的，循环神经网络算法</li>
<li>results: RFLO 可以与 RTRL 相当，而且在复杂性方面超过 BPTT，并且可以解决离散和连续控制任务在 POMDPs 中<details>
<summary>Abstract</summary>
Recent advances in reinforcement learning, for partially-observable Markov decision processes (POMDPs), rely on the biologically implausible backpropagation through time algorithm (BPTT) to perform gradient-descent optimisation. In this paper we propose a novel reinforcement learning algorithm that makes use of random feedback local online learning (RFLO), a biologically plausible approximation of realtime recurrent learning (RTRL) to compute the gradients of the parameters of a recurrent neural network in an online manner. By combining it with TD($\lambda$), a variant of temporaldifference reinforcement learning with eligibility traces, we create a biologically plausible, recurrent actor-critic algorithm, capable of solving discrete and continuous control tasks in POMDPs. We compare BPTT, RTRL and RFLO as well as different network architectures, and find that RFLO can perform just as well as RTRL while exceeding even BPTT in terms of complexity. The proposed method, called real-time recurrent reinforcement learning (RTRRL), serves as a model of learning in biological neural networks mimicking reward pathways in the mammalian brain.
</details>
<details>
<summary>摘要</summary>
近期在 partially-observable Markov decision processes (POMDPs) 中的进步， rely on biologically implausible backpropagation through time algorithm (BPTT) 来进行 gradient-descent 优化。在这篇论文中，我们提出了一种新的 reinforcement learning 算法，使用 random feedback local online learning (RFLO)，一种 biologically plausible 的 realtime recurrent learning (RTRL) 来计算 recurrent neural network 的参数 Gradient。通过将其与 temporaldifference reinforcement learning with eligibility traces (TD($\lambda$)) 结合，我们创造了一种 biologically plausible， recurrent actor-critic 算法，可以解决 POMDPs 中的离散和连续控制任务。我们对 BPTT, RTRL 和 RFLO 进行比较，以及不同的网络架构，发现 RFLO 可以与 RTRL 相当，而且even exceed BPTT 的复杂性。我们提出的方法，called real-time recurrent reinforcement learning (RTRRL)，可以作为生物 neural networks 中学习的模型，模拟奖 PATHways 在哺乳动物大脑中的学习过程。
</details></li>
</ul>
<hr>
<h2 id="Functional-Bayesian-Tucker-Decomposition-for-Continuous-indexed-Tensor-Data"><a href="#Functional-Bayesian-Tucker-Decomposition-for-Continuous-indexed-Tensor-Data" class="headerlink" title="Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data"></a>Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04829">http://arxiv.org/abs/2311.04829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikai Fang, Xin Yu, Zheng Wang, Shibo Li, Mike Kirby, Shandian Zhe</li>
<li>for: 该论文旨在扩展tensor decomposition，以处理不符合维度的多方面数据。</li>
<li>methods: 该论文提出了Functional Bayesian Tucker Decomposition（FunBaT）方法，将连续索引的数据视为Tucker核和一组隐函数之间的互动。使用 Gaussian processes（GP）作为函数假设，并将GP转换为状态空间假设，以降低计算成本。</li>
<li>results: 论文通过使用高效的消息传递技术进行可扩展 posterior  aproximation，实现了可扩展的 posterior  aproximation。在 sintetic 数据和实际应用中，论文示出了其优势。<details>
<summary>Abstract</summary>
Tucker decomposition is a powerful tensor model to handle multi-aspect data. It demonstrates the low-rank property by decomposing the grid-structured data as interactions between a core tensor and a set of object representations (factors). A fundamental assumption of such decomposition is that there were finite objects in each aspect or mode, corresponding to discrete indexes of data entries. However, many real-world data are not naturally posed in the setting. For example, geographic data is represented as continuous indexes of latitude and longitude coordinates, and cannot fit tensor models directly. To generalize Tucker decomposition to such scenarios, we propose Functional Bayesian Tucker Decomposition (FunBaT). We treat the continuous-indexed data as the interaction between the Tucker core and a group of latent functions. We use Gaussian processes (GP) as functional priors to model the latent functions, and then convert the GPs into a state-space prior by constructing an equivalent stochastic differential equation (SDE) to reduce computational cost. An efficient inference algorithm is further developed for scalable posterior approximation based on advanced message-passing techniques. The advantage of our method is shown in both synthetic data and several real-world applications.
</details>
<details>
<summary>摘要</summary>
图克 decompositions 是一种强大的维度模型，用于处理多方面数据。它示出了低级别性质，通过将格子结构数据分解为核tensor和一组对象表示（因子）之间的交互。基本假设是，每个方面或模式中都有限多个对象，与数据项的独立标识符相对应。然而，许多实际世界数据并不适合直接适用tensor模型。例如，地理数据通常表示为纬度和经度坐标的连续标识符，无法直接适用tensor模型。为推广图克 decompositions 到这些场景，我们提出了函数 Bayesian Tucker decompositions（FunBaT）。我们将连续标识符数据视为核tensor和一组隐函数之间的交互。我们使用 Gaussian processes（GP）作为函数先验来模型隐函数，然后将GP转换为状态空间先验，通过构建相应的随机振荡方程（SDE）来减少计算成本。我们还开发了一种高效的 posterior 近似算法，以便扩展可扩展的 posterior 估计。我们的方法在 sintetic 数据和一些实际应用中的优点被证明。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Architecture-for-Real-Time-Neuronal-Spike-Classification"><a href="#A-Lightweight-Architecture-for-Real-Time-Neuronal-Spike-Classification" class="headerlink" title="A Lightweight Architecture for Real-Time Neuronal-Spike Classification"></a>A Lightweight Architecture for Real-Time Neuronal-Spike Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04808">http://arxiv.org/abs/2311.04808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ali Siddiqi, David Vrijenhoek, Lennart P. L. Landsmeer, Job van der Kleij, Anteneh Gebregiorgis, Vincenzo Romano, Rajendra Bishnoi, Said Hamdioui, Christos Strydis</li>
<li>for: 这项研究旨在提高电physiological recording的灵活性和数据存储方式，以便更好地理解大脑功能。</li>
<li>methods: 该研究提出了一种轻量级的 neuronal-spike detection和分类架构，利用精雕细胞的特点来排除不必要的神经数据，并在实时进行数据压缩存储。</li>
<li>results: 该架构在实验中达到了&gt;95%的总分类精度，同时具有小型化设计和低功耗特性，使得头stage可以靠一个小电池运行达4天。<details>
<summary>Abstract</summary>
Electrophysiological recordings of neural activity in a mouse's brain are very popular among neuroscientists for understanding brain function. One particular area of interest is acquiring recordings from the Purkinje cells in the cerebellum in order to understand brain injuries and the loss of motor functions. However, current setups for such experiments do not allow the mouse to move freely and, thus, do not capture its natural behaviour since they have a wired connection between the animal's head stage and an acquisition device. In this work, we propose a lightweight neuronal-spike detection and classification architecture that leverages on the unique characteristics of the Purkinje cells to discard unneeded information from the sparse neural data in real time. This allows the (condensed) data to be easily stored on a removable storage device on the head stage, alleviating the need for wires. Our proposed implementation shows a >95% overall classification accuracy while still resulting in a small-form-factor design, which allows for the free movement of mice during experiments. Moreover, the power-efficient nature of the design and the usage of STT-RAM (Spin Transfer Torque Magnetic Random Access Memory) as the removable storage allows the head stage to easily operate on a tiny battery for up to approximately 4 days.
</details>
<details>
<summary>摘要</summary>
neuroscientists 非常感兴趣获取 Purkinje 细胞记录在脑室中以了解脑功能。一个具体的领域是获取 Purkinje 细胞记录，以了解脑 lesions 和失去运动功能。然而，现有的实验设置不允许鼠标自由移动，因此不能捕捉其自然行为，因为它们有一个从动物头阶段到获取设备的硬件连接。在这种工作中，我们提议一种轻量级神经元脉冲检测和分类架构，利用 Purkinje 细胞的特殊特征来抛弃不必要的神经数据。这使得（缩减）数据可以轻松地存储在动物头阶段的可 removable 存储设备上，解决了需要电缆的问题。我们的提议实现显示了 >95% 的总分类精度，同时仍保持了小型设计，允许鼠标在实验中自由移动。此外，设计的能效性和使用 STT-RAM（扭转磁铁随机存储）作为可 removable 存储，使得头阶段可以轻松地在 tiny 电池上运行，可以达到约 4 天的运行时间。
</details></li>
</ul>
<hr>
<h2 id="The-PetShop-Dataset-–-Finding-Causes-of-Performance-Issues-across-Microservices"><a href="#The-PetShop-Dataset-–-Finding-Causes-of-Performance-Issues-across-Microservices" class="headerlink" title="The PetShop Dataset – Finding Causes of Performance Issues across Microservices"></a>The PetShop Dataset – Finding Causes of Performance Issues across Microservices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04806">http://arxiv.org/abs/2311.04806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaela Hardt, William Orchard, Patrick Blöbaum, Shiva Kasiviswanathan, Elke Kirschbaum</li>
<li>for: 本文旨在提供一个特有的分析根本原因的数据集，用于评估微服务应用中的 Root Cause Analysis（RCA）方法的准确性。</li>
<li>methods: 本文使用了一个分析Root Cause Analysis问题的多种方法，包括不同的 causal 和 non-causal 特征。</li>
<li>results: 本文通过使用这个新的数据集，证明了这些方法的准确性和可靠性。<details>
<summary>Abstract</summary>
Identifying root causes for unexpected or undesirable behavior in complex systems is a prevalent challenge. This issue becomes especially crucial in modern cloud applications that employ numerous microservices. Although the machine learning and systems research communities have proposed various techniques to tackle this problem, there is currently a lack of standardized datasets for quantitative benchmarking. Consequently, research groups are compelled to create their own datasets for experimentation. This paper introduces a dataset specifically designed for evaluating root cause analyses in microservice-based applications. The dataset encompasses latency, requests, and availability metrics emitted in 5-minute intervals from a distributed application. In addition to normal operation metrics, the dataset includes 68 injected performance issues, which increase latency and reduce availability throughout the system. We showcase how this dataset can be used to evaluate the accuracy of a variety of methods spanning different causal and non-causal characterisations of the root cause analysis problem. We hope the new dataset, available at https://github.com/amazon-science/petshop-root-cause-analysis/ enables further development of techniques in this important area.
</details>
<details>
<summary>摘要</summary>
“找到复杂系统中不望或不适的行为的根本原因是一个普遍的挑战。这个问题在现代云应用中使用多个微服务时变得更加重要。尽管机器学习和系统研究社区已经提出了多种技术来解决这个问题，但目前没有标准化的数据集用于量化比较。因此，研究组织被迫创建自己的数据集来进行实验。这篇论文介绍了一个专门为评估微服务基建应用中的根本原因分析问题而设计的数据集。该数据集包括5分钟间隔发生的延迟、请求和可用性指标，以及68个注入性性能问题，这些问题会在系统中增加延迟和降低可用性。我们示出了如何使用这个数据集来评估多种不同的 causa和非 causa 的根本原因分析问题的准确性。我们希望这个新的数据集，可以在 https://github.com/amazon-science/petshop-root-cause-analysis/ 上下载，能够促进这个重要领域的技术发展。”Note: "causa" and "非 causa" are Chinese terms that roughly translate to "causal" and "non-causal" respectively.
</details></li>
</ul>
<hr>
<h2 id="Why-Do-Clinical-Probabilistic-Models-Fail-To-Transport-Between-Sites"><a href="#Why-Do-Clinical-Probabilistic-Models-Fail-To-Transport-Between-Sites" class="headerlink" title="Why Do Clinical Probabilistic Models Fail To Transport Between Sites?"></a>Why Do Clinical Probabilistic Models Fail To Transport Between Sites?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04787">http://arxiv.org/abs/2311.04787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas A. Lasko, Eric V. Strobl, William W. Stead</li>
<li>for: 这篇论文目的是解释医疗人工智能的应用中出现的问题，即模型在训练站上达到超人类性能后，在新的站点上表现差强。</li>
<li>methods: 本文使用了实验控制和资料生成过程中的因素来分析这些问题，并提出了一个解决方案，即隔离训练站上的临床实践对数据的影响。</li>
<li>results: 本文预测了这种问题的原因，并提出了一个解决方案，可以将训练站上的临床实践对数据的影响隔离开来。<details>
<summary>Abstract</summary>
The rising popularity of artificial intelligence in healthcare is highlighting the problem that a computational model achieving super-human clinical performance at its training sites may perform substantially worse at new sites. In this perspective, we present common sources for this failure to transport, which we divide into sources under the control of the experimenter and sources inherent to the clinical data-generating process. Of the inherent sources we look a little deeper into site-specific clinical practices that can affect the data distribution, and propose a potential solution intended to isolate the imprint of those practices on the data from the patterns of disease cause and effect that are the usual target of clinical models.
</details>
<details>
<summary>摘要</summary>
人工智能在医疗领域的普及化正在吸引着关注，但同时也暴露了一个问题：一个计算模型在训练站上达到超人至尊的临床性能后，在新的站点上可能表现出很差的性能。在这个视角下，我们描述了不能传输的常见原因，分为实验者可控的源和临床数据生成过程中的内在源。其中内在源中，我们做了一些深入的分析，包括站点特有的临床实践，这些实践可能会影响数据分布。我们还提出了一种解决方案，用于孤立临床实践对数据的影响，从而更好地隔离疾病原因和疾病效应的模式。
</details></li>
</ul>
<hr>
<h2 id="FetMRQC-an-open-source-machine-learning-framework-for-multi-centric-fetal-brain-MRI-quality-control"><a href="#FetMRQC-an-open-source-machine-learning-framework-for-multi-centric-fetal-brain-MRI-quality-control" class="headerlink" title="FetMRQC: an open-source machine learning framework for multi-centric fetal brain MRI quality control"></a>FetMRQC: an open-source machine learning framework for multi-centric fetal brain MRI quality control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04780">http://arxiv.org/abs/2311.04780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Sanchez, Oscar Esteban, Yvan Gomez, Alexandre Pron, Mériam Koob, Vincent Dunet, Nadine Girard, Andras Jakab, Elisenda Eixarch, Guillaume Auzias, Meritxell Bach Cuadra</li>
<li>for: 这个研究旨在提供一个开源的机器学习框架，用于自动评估和控制胎儿脑部MRI的影像质量，以扩大胎儿脑部发展研究的可能性。</li>
<li>methods: 这个研究使用了一个开源的机器学习框架（FetMRQC），它可以自动提取胎儿脑部MRI影像中的质量指标，并结合这些指标使用随机树来预测专家的评分。</li>
<li>results: 这个研究发现，FetMRQC的预测结果能够在不同的胎儿脑部MRI影像数据中稳定地表现出来，并且可以对未见过的数据进行预测，同时保持可解释性。<details>
<summary>Abstract</summary>
Fetal brain MRI is becoming an increasingly relevant complement to neurosonography for perinatal diagnosis, allowing fundamental insights into fetal brain development throughout gestation. However, uncontrolled fetal motion and heterogeneity in acquisition protocols lead to data of variable quality, potentially biasing the outcome of subsequent studies. We present FetMRQC, an open-source machine-learning framework for automated image quality assessment and quality control that is robust to domain shifts induced by the heterogeneity of clinical data. FetMRQC extracts an ensemble of quality metrics from unprocessed anatomical MRI and combines them to predict experts' ratings using random forests. We validate our framework on a pioneeringly large and diverse dataset of more than 1600 manually rated fetal brain T2-weighted images from four clinical centers and 13 different scanners. Our study shows that FetMRQC's predictions generalize well to unseen data while being interpretable. FetMRQC is a step towards more robust fetal brain neuroimaging, which has the potential to shed new insights on the developing human brain.
</details>
<details>
<summary>摘要</summary>
《胎儿脑MRI在产前诊断中日益重要，允许深入了解胎儿脑发育过程。但是，不受控制的胎儿运动和数据采集协议的多样性导致数据质量变化，可能影响后续研究的结果。我们介绍FetMRQC，一个开源的机器学习框架，用于自动评估和控制图像质量，对域Shift具有抗针对性。FetMRQC从未处理的 анатомичеMRI中提取了一个ensemble的质量指标，并使用Random Forest组合以预测专家评分。我们验证了我们的框架，使用了1600多个手动评分的胎儿脑T2强化MRI图像，来自四个临床中心和13个不同的扫描仪。我们的研究表明，FetMRQC的预测能够在未看到数据上进行良好的泛化，同时具有可解释性。FetMRQC是更加Robust的胎儿脑神经成像的一步，它有可能为人类脑发育带来新的发现。》
</details></li>
</ul>
<hr>
<h2 id="Optimal-Deep-Neural-Network-Approximation-for-Korobov-Functions-with-respect-to-Sobolev-Norms"><a href="#Optimal-Deep-Neural-Network-Approximation-for-Korobov-Functions-with-respect-to-Sobolev-Norms" class="headerlink" title="Optimal Deep Neural Network Approximation for Korobov Functions with respect to Sobolev Norms"></a>Optimal Deep Neural Network Approximation for Korobov Functions with respect to Sobolev Norms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04779">http://arxiv.org/abs/2311.04779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahong Yang, Yulong Lu</li>
<li>for: 这个论文设计了深度神经网络（DNN）在科罗波夫函数上的近似率，有效战胜了维度的咒语。</li>
<li>methods: 这个论文使用了深度神经网络对科罗波夫函数进行近似，并使用了$L_p$ norm和$H^1$ norm来测量近似结果。</li>
<li>results: 这个论文的结果表明，深度神经网络可以在科罗波夫函数上实现非常高的近似率，超过传统方法和任何连续函数近似器。这些结果是非阿塞拜 ME 的，即不含任何假设或偏好。<details>
<summary>Abstract</summary>
This paper establishes the nearly optimal rate of approximation for deep neural networks (DNNs) when applied to Korobov functions, effectively overcoming the curse of dimensionality. The approximation results presented in this paper are measured with respect to $L_p$ norms and $H^1$ norms. Our achieved approximation rate demonstrates a remarkable "super-convergence" rate, outperforming traditional methods and any continuous function approximator. These results are non-asymptotic, providing error bounds that consider both the width and depth of the networks simultaneously.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文确立了深度神经网络（DNNs）应用于科罗波夫函数的近似率，有效地绕过维度的咒语。我们的近似结果使用 $L_p$ 范数和 $H^1$ 范数来度量，并demonstrates一个非常出众的"超 converges" 率，超过传统方法和任何连续函数近似器。这些结果是非 asymptotic，同时考虑了网络的宽度和深度。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Unified-Framework-of-Contrastive-Learning-for-Disentangled-Representations"><a href="#Towards-a-Unified-Framework-of-Contrastive-Learning-for-Disentangled-Representations" class="headerlink" title="Towards a Unified Framework of Contrastive Learning for Disentangled Representations"></a>Towards a Unified Framework of Contrastive Learning for Disentangled Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04774">http://arxiv.org/abs/2311.04774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Matthes, Zhiwei Han, Hao Shen</li>
<li>for: 本研究探讨了对数据表示学习的抽象方法，即对数据分解和分离的因素。</li>
<li>methods: 本研究使用了一种更加广泛的对照方法家族，而不是单独关注噪音对照估计（NCE）和InfoNCE。</li>
<li>results: 研究证明了这些对照方法可以很好地分解和分离数据中的因素，而不需要假设特定的数据分布。<details>
<summary>Abstract</summary>
Contrastive learning has recently emerged as a promising approach for learning data representations that discover and disentangle the explanatory factors of the data. Previous analyses of such approaches have largely focused on individual contrastive losses, such as noise-contrastive estimation (NCE) and InfoNCE, and rely on specific assumptions about the data generating process. This paper extends the theoretical guarantees for disentanglement to a broader family of contrastive methods, while also relaxing the assumptions about the data distribution. Specifically, we prove identifiability of the true latents for four contrastive losses studied in this paper, without imposing common independence assumptions. The theoretical findings are validated on several benchmark datasets. Finally, practical limitations of these methods are also investigated.
</details>
<details>
<summary>摘要</summary>
“对比学习” recent  emerge  as  a  promising  approach  for  learning  data  representations  that  discover  and  disentangle  the  explanatory  factors  of  the  data. Previous  analyses  of  such  approaches  have  largely  focused  on  individual  contrastive  losses,  such  as  noise-contrastive  estimation  (NCE)  and  InfoNCE,  and  rely  on  specific  assumptions  about  the  data  generating  process. This  paper  extends  the  theoretical  guarantees  for  disentanglement  to  a  broader  family  of  contrastive  methods,  while  also  relaxing  the  assumptions  about  the  data  distribution. Specifically, we prove identifiability  of  the  true  latents  for  four  contrastive  losses  studied  in  this  paper,  without  imposing  common  independence  assumptions. The  theoretical  findings  are  validated  on  several  benchmark  datasets. Finally, practical  limitations  of  these  methods  are  also  investigated.
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-world-Cross-Domain-Sequential-Recommendation-A-Model-Agnostic-Contrastive-Denoising-Approach"><a href="#Towards-Open-world-Cross-Domain-Sequential-Recommendation-A-Model-Agnostic-Contrastive-Denoising-Approach" class="headerlink" title="Towards Open-world Cross-Domain Sequential Recommendation: A Model-Agnostic Contrastive Denoising Approach"></a>Towards Open-world Cross-Domain Sequential Recommendation: A Model-Agnostic Contrastive Denoising Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04760">http://arxiv.org/abs/2311.04760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wujiang Xu, Xuying Ning, Wenfang Lin, Mingming Ha, Qiongxu Ma, Linxun Chen, Bing Han, Minnan Luo</li>
<li>for: 本研究旨在 addresses the data sparsity problem in traditional sequential recommendation (SR) systems, particularly in open-world cross-domain scenarios where most users have sparse behaviors and cold-start users only exist in one domain.</li>
<li>methods: 现有的 CDSR 方法强调设计特定的 cross-domain unit，以传递和协调多个领域中的用户行为信息。然而，在实际业务平台上， CDSR 场景通常存在大量的长尾用户和缺乏行为的情况，这导致了现有 CDSR 方法在实际场景中的性能下降。因此，构建高效的 CDSR 模型在开放世界 CDSR 场景中是非常重要的 (\textit{1st} CH).</li>
<li>results: 总之，这些 SR 方法无法在 CDSR 场景中提供出色的表现，因为它们忽视了目标行为和辅助行为之间的semantic gap，以及用户兴趣的偏移 across domains (\textit{2nd} CH).<details>
<summary>Abstract</summary>
Cross-domain sequential recommendation (CDSR) aims to address the data sparsity problems that exist in traditional sequential recommendation (SR) systems.   The existing approaches aim to design a specific cross-domain unit that can transfer and propagate information across multiple domains by relying on overlapping users with abundant behaviors. However, in real-world recommender systems, CDSR scenarios usually consist of a majority of long-tailed users with sparse behaviors and cold-start users who only exist in one domain. This leads to a drop in the performance of existing CDSR methods in the real-world industry platform. Therefore, improving the consistency and effectiveness of models in open-world CDSR scenarios is crucial for constructing CDSR models (\textit{1st} CH). Recently, some SR approaches have utilized auxiliary behaviors to complement the information for long-tailed users. However, these multi-behavior SR methods cannot deliver promising performance in CDSR, as they overlook the semantic gap between target and auxiliary behaviors, as well as user interest deviation across domains (\textit{2nd} CH).
</details>
<details>
<summary>摘要</summary>
Recently, some SR approaches have utilized auxiliary behaviors to complement the information for long-tailed users. However, these multi-behavior SR methods cannot deliver promising performance in CDSR, as they overlook the semantic gap between target and auxiliary behaviors, as well as user interest deviation across domains.In Chinese:跨DomainSequential recommendation (CDSR) 目标是解决传统Sequential recommendation (SR) 系统中的数据稀缺问题。现有的方法是通过跨多个Domain的用户 overlap来设计特定的跨Domain单元，以传递和宣传信息。然而，在实际的 recommender 系统中， CDSR 场景通常由大量的长尾用户和缺乏行为组成，以及唯一存在于一个Domain的冷启用户。这导致了现有CDSR方法在实际行业平台上的性能下降。因此，在开放世界CDSR场景中提高模型的一致性和效果是构建CDSR模型的关键。当前，一些SR方法已经利用 auxillary 行为来补充长尾用户的信息。然而，这些多行为SR方法在 CDSR 中无法达到可靠的性能，因为它们忽视了目标行为和 auxillary 行为之间的semantic gap，以及用户兴趣在不同Domain中的偏移。
</details></li>
</ul>
<hr>
<h2 id="Natural-Bayesian-Cramer-Rao-Bound-with-an-Application-to-Covariance-Estimation"><a href="#Natural-Bayesian-Cramer-Rao-Bound-with-an-Application-to-Covariance-Estimation" class="headerlink" title="Natural Bayesian Cramér-Rao Bound with an Application to Covariance Estimation"></a>Natural Bayesian Cramér-Rao Bound with an Application to Covariance Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04748">http://arxiv.org/abs/2311.04748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Bouchard, Alexandre Renaux, Guillaume Ginolhac, Arnaud Breloy</li>
<li>for: 这个论文是为了开发一种新的克拉默-拉托 bound（CRB），当参数需要估计的拓扑在拓扑上并且遵循一个先验分布时。</li>
<li>methods: 这个论文使用的方法包括 derive a new Cramér-Rao bound (CRB) based on the manifold of the parameter and the prior distribution.</li>
<li>results: 该论文的主要贡献是通过一种自然的不等式关系 между一个Error criteria和该新的 bound，并且通过numerical simulation表明了该新的 CRB 可以展示一些MAP estimator的有趣性质，这些质量不存在于经典的极大似然估计 bound 中。<details>
<summary>Abstract</summary>
In this paper, we propose to develop a new Cram\'er-Rao Bound (CRB) when the parameter to estimate lies in a manifold and follows a prior distribution. This derivation leads to a natural inequality between an error criteria based on geometrical properties and this new bound. This main contribution is illustrated in the problem of covariance estimation when the data follow a Gaussian distribution and the prior distribution is an inverse Wishart. Numerical simulation shows new results where the proposed CRB allows to exhibit interesting properties of the MAP estimator which are not observed with the classical Bayesian CRB.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的卡尔-拉奥 bound (CRB)，当参数估计的拟合体系是一个拟合 manifold 并且遵循一个先验分布时。这种 derivation 导致了一种自然的不等式 между一个基于 геометрические属性的误差标准和这个新的 bound。我们的主要贡献在covariance估计问题中，数据遵循 Gaussian 分布，先验分布是 inverse Wishart。数值仪表示我们的提案CRB具有不同于传统极大似然估计CRB的有趣性质。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Multi-Agent-Coordination-through-Common-Operating-Picture-Integration"><a href="#Enhancing-Multi-Agent-Coordination-through-Common-Operating-Picture-Integration" class="headerlink" title="Enhancing Multi-Agent Coordination through Common Operating Picture Integration"></a>Enhancing Multi-Agent Coordination through Common Operating Picture Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04740">http://arxiv.org/abs/2311.04740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peihong Yu, Bhoram Lee, Aswin Raghavan, Supun Samarasekara, Pratap Tokekar, James Zachary Hare</li>
<li>for: 这 paper 是为了提高多智能体系统中代理的协调性和鲁棒性，具体来说是通过融合代理的历史观察、行动和消息来构建共同运作图像（COP），并在这个过程中考虑环境的动态性和共同任务。</li>
<li>methods: 该 paper 使用了多智能体强化学习（MARL）方法，并在这些方法中引入了共同运作图像（COP）的概念。</li>
<li>results: 实验结果表明，使用 COP 融合学习方法可以提高代理的协调性和鲁棒性，并在 faced with out-of-distribution 初始状态时表现更加稳定和有效。<details>
<summary>Abstract</summary>
In multi-agent systems, agents possess only local observations of the environment. Communication between teammates becomes crucial for enhancing coordination. Past research has primarily focused on encoding local information into embedding messages which are unintelligible to humans. We find that using these messages in agent's policy learning leads to brittle policies when tested on out-of-distribution initial states. We present an approach to multi-agent coordination, where each agent is equipped with the capability to integrate its (history of) observations, actions and messages received into a Common Operating Picture (COP) and disseminate the COP. This process takes into account the dynamic nature of the environment and the shared mission. We conducted experiments in the StarCraft2 environment to validate our approach. Our results demonstrate the efficacy of COP integration, and show that COP-based training leads to robust policies compared to state-of-the-art Multi-Agent Reinforcement Learning (MARL) methods when faced with out-of-distribution initial states.
</details>
<details>
<summary>摘要</summary>
在多代理系统中，代理具有只有本地环境观察的能力。代理之间的交流成为协调的关键。过去的研究主要集中在编码本地信息到嵌入消息中，这些消息不可读人类。我们发现，使用这些消息在代理的政策学习中导致粗糙的策略，对于非标准初始状态测试时表现不稳定。我们提出了一种多代理协调方法，其中每个代理具有积极融合其观察历史、行动和接收的消息的能力，并将这些信息整合成共同运行图像（COP）。这个过程考虑了环境的动态性和共同任务。我们在StarCraft2环境中进行了实验，以验证我们的方法。我们的结果表明COP融合的有效性，并示出COP基于培训在面对非标准初始状态时比state-of-the-art多代理学习方法（MARL）更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Robust-Best-arm-Identification-in-Linear-Bandits"><a href="#Robust-Best-arm-Identification-in-Linear-Bandits" class="headerlink" title="Robust Best-arm Identification in Linear Bandits"></a>Robust Best-arm Identification in Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04731">http://arxiv.org/abs/2311.04731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Wang, Sattar Vakili, Ilija Bogunovic</li>
<li>for: 本研究旨在解决线性奖励情况下的Robust Best-Arm Identification问题（RBAI）。</li>
<li>methods: 我们提出了一种实例dependent的下界，以及静态和适应式bandit算法，以实现与下界相同的样本复杂度。</li>
<li>results: 在synthetic实验中，我们的算法能够有效地标识最佳 robust arm，并与oracle策略相似。在应用中，我们研究了使用这些算法来学习不准确标准计算器的药物剂量建议，并证明了这些算法在不同年龄段患者中实现了有效的剂量值标识。<details>
<summary>Abstract</summary>
We study the robust best-arm identification problem (RBAI) in the case of linear rewards. The primary objective is to identify a near-optimal robust arm, which involves selecting arms at every round and assessing their robustness by exploring potential adversarial actions. This approach is particularly relevant when utilizing a simulator and seeking to identify a robust solution for real-world transfer. To this end, we present an instance-dependent lower bound for the robust best-arm identification problem with linear rewards. Furthermore, we propose both static and adaptive bandit algorithms that achieve sample complexity that matches the lower bound. In synthetic experiments, our algorithms effectively identify the best robust arm and perform similarly to the oracle strategy. As an application, we examine diabetes care and the process of learning insulin dose recommendations that are robust with respect to inaccuracies in standard calculators. Our algorithms prove to be effective in identifying robust dosage values across various age ranges of patients.
</details>
<details>
<summary>摘要</summary>
我们研究了线性奖励情况下的强 robust arm 标识问题（RBAI）。我们的主要目标是找到一个近似优化的强 robust arm，这里是在每个轮次选择 arms 并评估其强健性，包括探索敌对行为。这种方法特别 relevante 当使用模拟器并寻求实际世界中的强解。为此，我们提出了一个实例 dependent 下界 для强 robust arm 标识问题。此外，我们还提出了静态和适应bandit算法，它们的样本复杂度与下界相同。在synthetic实验中，我们的算法能够有效地确定最佳强 robust arm，并与oracle策略相似。在应用中，我们研究了 диабеtes 护理，即通过学习不准确的标准计算机确定具有robustness的药物剂量。我们的算法在不同年龄层的患者中显示出效果。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Properties-of-Nodes-via-Community-Aware-Features"><a href="#Predicting-Properties-of-Nodes-via-Community-Aware-Features" class="headerlink" title="Predicting Properties of Nodes via Community-Aware Features"></a>Predicting Properties of Nodes via Community-Aware Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04730">http://arxiv.org/abs/2311.04730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bogumił Kamiński, Paweł Prałat, François Théberge, Sebastian Zając</li>
<li>for: 这 paper 的目的是提出一家族community-aware node features，并研究其性质。</li>
<li>methods: 该 paper 使用了 classical node features 和 node embeddings（包括类传统的 node embeddings 和 structural node embeddings）来描述节点的特征，并提出了一种新的community-aware node features 家族。</li>
<li>results: 研究发现，这种新的community-aware node features 具有高的预测力，可以用于分类任务。此外，它们还包含了不可recover的信息，与传统的节点特征和节点嵌入不同。<details>
<summary>Abstract</summary>
A community structure that is often present in complex networks plays an important role not only in their formation but also shapes dynamics of these networks, affecting properties of their nodes. In this paper, we propose a family of community-aware node features and then investigate their properties. We show that they have high predictive power for classification tasks. We also verify that they contain information that cannot be recovered neither by classical node features nor by node embeddings (both classical as well as structural).
</details>
<details>
<summary>摘要</summary>
Complex networks often exhibit a community structure, which not only affects their formation but also shapes their dynamics, influencing the properties of their nodes. In this paper, we propose a family of community-aware node features and investigate their properties. We show that they have high predictive power for classification tasks. We also verify that they contain information that cannot be recovered by classical node features or node embeddings (both classical and structural).Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other countries.
</details></li>
</ul>
<hr>
<h2 id="Robust-and-Communication-Efficient-Federated-Domain-Adaptation-via-Random-Features"><a href="#Robust-and-Communication-Efficient-Federated-Domain-Adaptation-via-Random-Features" class="headerlink" title="Robust and Communication-Efficient Federated Domain Adaptation via Random Features"></a>Robust and Communication-Efficient Federated Domain Adaptation via Random Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04686">http://arxiv.org/abs/2311.04686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sadangelf/fedrf-tca">https://github.com/sadangelf/fedrf-tca</a></li>
<li>paper_authors: Zhanbo Feng, Yuanjie Wang, Jie Li, Fan Yang, Jiong Lou, Tiebin Mi, Robert. C. Qiu, Zhenyu Liao</li>
<li>for: 这个论文的目的是提出一种基于聚合学习的分布式适应预测方法，以适应域shift问题。</li>
<li>methods: 该方法基于传输组件分析（TCA），通过减少数据的通信复杂度来加速计算。</li>
<li>results: 试验结果表明，提出的 FedRF-TCA 协议在不同的网络条件下具有稳定高性能，并且与现有的 FDA 方法相比，具有独立于样本大小的通信复杂度。<details>
<summary>Abstract</summary>
Modern machine learning (ML) models have grown to a scale where training them on a single machine becomes impractical. As a result, there is a growing trend to leverage federated learning (FL) techniques to train large ML models in a distributed and collaborative manner. These models, however, when deployed on new devices, might struggle to generalize well due to domain shifts. In this context, federated domain adaptation (FDA) emerges as a powerful approach to address this challenge.   Most existing FDA approaches typically focus on aligning the distributions between source and target domains by minimizing their (e.g., MMD) distance. Such strategies, however, inevitably introduce high communication overheads and can be highly sensitive to network reliability.   In this paper, we introduce RF-TCA, an enhancement to the standard Transfer Component Analysis approach that significantly accelerates computation without compromising theoretical and empirical performance. Leveraging the computational advantage of RF-TCA, we further extend it to FDA setting with FedRF-TCA. The proposed FedRF-TCA protocol boasts communication complexity that is \emph{independent} of the sample size, while maintaining performance that is either comparable to or even surpasses state-of-the-art FDA methods. We present extensive experiments to showcase the superior performance and robustness (to network condition) of FedRF-TCA.
</details>
<details>
<summary>摘要</summary>
现代机器学习（ML）模型已经增长到了一个Scale，训练它们在单个机器上已经成为不切实际的。因此，有一个增长的趋势是使用联合学习（FL）技术来训练大型ML模型在分布式和合作的方式下。这些模型在新设备上部署时可能会遇到领域变化，因此联合领域适应（FDA）作为一种强大的方法来解决这个挑战。现有的FDA方法通常是通过最小化源和目标领域之间的分布差（如MMD）来对align distribution。这些策略会导致高度的通信开销和网络可靠性的敏感性。在这篇论文中，我们介绍了RF-TCA，一种改进了标准传输组件分析（Transfer Component Analysis，TCA）的方法，可以快速计算而不会损失理论和实验性能。我们还将RF-TCA扩展到FDA设置，得到了 FedRF-TCA协议。FedRF-TCA协议的通信复杂度是独立于样本大小的，同时保持与状态 искусственный智能（AI）方法相当或者超越的性能。我们进行了广泛的实验，以示出FedRF-TCA的superior性能和网络condition的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Compressive-Recovery-of-Sparse-Precision-Matrices"><a href="#Compressive-Recovery-of-Sparse-Precision-Matrices" class="headerlink" title="Compressive Recovery of Sparse Precision Matrices"></a>Compressive Recovery of Sparse Precision Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04673">http://arxiv.org/abs/2311.04673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Titouan Vayer, Etienne Lasalle, Rémi Gribonval, Paulo Gonçalves</li>
<li>for: 本研究的目的是提出一种高维数据图模型学习方法，以优化数据的统计关系。</li>
<li>methods: 该方法采用了压缩视角，通过非线性随机特征来采样数据，从而降低存储负担。</li>
<li>results: 研究表明，可以通过采样大小$m &#x3D; O((d+2k)\log(d))$来估算稀疏图模型。这些信息理论保证基于压缩感知论和特定的幂等均值抑制器。<details>
<summary>Abstract</summary>
We consider the problem of learning a graph modeling the statistical relations of the $d$ variables of a dataset with $n$ samples $X \in \mathbb{R}^{n \times d}$. Standard approaches amount to searching for a precision matrix $\Theta$ representative of a Gaussian graphical model that adequately explains the data. However, most maximum likelihood-based estimators usually require storing the $d^{2}$ values of the empirical covariance matrix, which can become prohibitive in a high-dimensional setting. In this work, we adopt a compressive viewpoint and aim to estimate a sparse $\Theta$ from a sketch of the data, i.e. a low-dimensional vector of size $m \ll d^{2}$ carefully designed from $X$ using nonlinear random features. Under certain assumptions on the spectrum of $\Theta$ (or its condition number), we show that it is possible to estimate it from a sketch of size $m=\Omega((d+2k)\log(d))$ where $k$ is the maximal number of edges of the underlying graph. These information-theoretic guarantees are inspired by compressed sensing theory and involve restricted isometry properties and instance optimal decoders. We investigate the possibility of achieving practical recovery with an iterative algorithm based on the graphical lasso, viewed as a specific denoiser. We compare our approach and graphical lasso on synthetic datasets, demonstrating its favorable performance even when the dataset is compressed.
</details>
<details>
<summary>摘要</summary>
我们考虑了一个图模型，用于描述一个 dataset 中变量的统计关系。标准方法通常是寻找一个精度矩阵 $\Theta$，用于描述一个 Gaussian 图模型，并且能够准确地描述数据。然而，大多数最大可能似然值基于的估计器通常需要存储 $d^2$ 个实际协方差矩阵的值，这在高维度设置中可能变得离谱。在这个工作中，我们采用了压缩视角，并且尝试从数据的快照中估计一个稀疏 $\Theta$。我们在 certain 假设下（或其condition number），证明可以从数据的快照中估计 $\Theta$，并且这个快照的大小可以是 $m = \Omega((d+2k)\log(d))$，where $k$ 是图中的最大边数。这些信息理论保证是基于压缩感知理论和有限幂均值属性，以及实例最优解码器。我们 investigate 一种实际恢复的可能性，使用图ical lasso 作为特定的排除器。我们在 synthetic 数据上对我们的方法和图ical lasso 进行比较，并证明它在压缩数据时仍然有优势。
</details></li>
</ul>
<hr>
<h2 id="Learning-Linear-Gaussian-Polytree-Models-with-Interventions"><a href="#Learning-Linear-Gaussian-Polytree-Models-with-Interventions" class="headerlink" title="Learning Linear Gaussian Polytree Models with Interventions"></a>Learning Linear Gaussian Polytree Models with Interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04636">http://arxiv.org/abs/2311.04636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emduart2/polytrees">https://github.com/emduart2/polytrees</a></li>
<li>paper_authors: D. Tramontano, L. Waldmann, M. Drton, E. Duarte</li>
<li>For: 学习线性 Gaussian 树的 causal 结构，使用来自干预实验的数据，known intervention targets。* Methods: 首先学习树的桠架，然后对其边 Orient。输出是一个 CPDAG，表示真实的下面分布的 intervenational 等价类。* Results: 在不同场景下进行了synthetic数据集的测试，并在一个基因表达干预数据集中应用了算法，实现了高速、高精度的结构哈姆钦距离。<details>
<summary>Abstract</summary>
We present a consistent and highly scalable local approach to learn the causal structure of a linear Gaussian polytree using data from interventional experiments with known intervention targets. Our methods first learn the skeleton of the polytree and then orient its edges. The output is a CPDAG representing the interventional equivalence class of the polytree of the true underlying distribution. The skeleton and orientation recovery procedures we use rely on second order statistics and low-dimensional marginal distributions. We assess the performance of our methods under different scenarios in synthetic data sets and apply our algorithm to learn a polytree in a gene expression interventional data set. Our simulation studies demonstrate that our approach is fast, has good accuracy in terms of structural Hamming distance, and handles problems with thousands of nodes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种一致性和可扩展性很强的本地方法，用于学习一个线性 Gaussian 树的 causal 结构，使用来自干预实验的数据，其中干预目标已知。我们的方法首先学习树的架构，然后将其边orientation。输出是一个 CPDAG，表示实验性Equivalence class of the true underlying distribution的树。我们使用第二阶 statistics和低维度分布来进行架构和边orientation的回归。我们在不同情况下进行了 simulate 研究，并将方法应用于一个基因表达干预数据集中。我们的 simulate 研究表明，我们的方法快速、精度高，能够处理 thousands of nodes 的问题。
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Tolerant-Methods-for-Distributed-Variational-Inequalities"><a href="#Byzantine-Tolerant-Methods-for-Distributed-Variational-Inequalities" class="headerlink" title="Byzantine-Tolerant Methods for Distributed Variational Inequalities"></a>Byzantine-Tolerant Methods for Distributed Variational Inequalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04611">http://arxiv.org/abs/2311.04611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nazya/sgda-ra">https://github.com/nazya/sgda-ra</a></li>
<li>paper_authors: Nazarii Tupitsa, Abdulla Jasem Almansoori, Yanlin Wu, Martin Takáč, Karthik Nandakumar, Samuel Horváth, Eduard Gorbunov</li>
<li>for: 提高分布式学习中的Byzantine攻击Robustness，特别是在解决variational inequality问题时。</li>
<li>methods: 提供了多种(可证明)ByzantineRobust的方法，并进行了全面的理论分析和数值比较，以支持理论发现。</li>
<li>results: 研究结果表明，提出的方法能够有效地提高分布式学习中的Byzantine攻击Robustness，并且超过了之前的研究所具有的限制。<details>
<summary>Abstract</summary>
Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work (Adibi et al., 2022) addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings.
</details>
<details>
<summary>摘要</summary>
Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work (Adibi et al., 2022) addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings.Here's the translation in Traditional Chinese:Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work (Adibi et al., 2022) addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings.
</details></li>
</ul>
<hr>
<h2 id="Accurate-Autism-Spectrum-Disorder-prediction-using-Support-Vector-Classifier-based-on-Federated-Learning-SVCFL"><a href="#Accurate-Autism-Spectrum-Disorder-prediction-using-Support-Vector-Classifier-based-on-Federated-Learning-SVCFL" class="headerlink" title="Accurate Autism Spectrum Disorder prediction using Support Vector Classifier based on Federated Learning (SVCFL)"></a>Accurate Autism Spectrum Disorder prediction using Support Vector Classifier based on Federated Learning (SVCFL)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04606">http://arxiv.org/abs/2311.04606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Mohammadifar, Hasan Samadbin, Arman Daliri</li>
<li>for: 早期诊断autism spectrum disorder (ASD)的方法</li>
<li>methods: 使用 federated learning 方法和支持向量分类器方法对数据进行识别</li>
<li>results: 实现了 99% 的准确率和 13% 的提升Here’s a breakdown of each point:1. for: The paper is written for the purpose of exploring the use of artificial intelligence (AI) for early diagnosis of autism spectrum disorder (ASD).2. methods: The paper uses four datasets and combines them using federated learning, a machine learning method that allows multiple devices or datasets to work together without sharing their data. The paper also uses a support vector classifier method to diagnose the data.3. results: The paper achieves 99% accuracy in predicting ASD and a 13% improvement in results using this method.<details>
<summary>Abstract</summary>
The path to an autism diagnosis can be long and difficult, and delays can have serious consequences. Artificial intelligence can completely change the way autism is diagnosed, especially when it comes to situations where it is difficult to see the first signs of the disease. AI-based diagnostic tools may help confirm a diagnosis or highlight the need for further testing by analyzing large volumes of data and uncovering patterns that may not be immediately apparent to human evaluators. After a successful and timely diagnosis, autism can be treated through artificial intelligence using various methods. In this article, by using four datasets and gathering them with the federated learning method and diagnosing them with the support vector classifier method, the early diagnosis of this disorder has been discussed. In this method, we have achieved 99% accuracy for predicting autism spectrum disorder and we have achieved 13% improvement in the results.
</details>
<details>
<summary>摘要</summary>
“患有自关症诊断的路程可能很长而困难，而延误可能会有严重的后果。人工智能可能将您诊断自关症的方式完全改变，特别是在您difficult to spot the first signs of the disease的情况下。人工智能基本的诊断工具可能会帮助确认诊断或显示需要进一步测试的需求，通过分析大量数据和找出不同于人类评估者所能找到的模式。在这篇文章中，我们使用了四个数据集和使用联邦学习方法和支持向量分类方法进行诊断。在这种方法中，我们已经达成了99%的自关症诊断精度，并取得了13%的改善。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Zeroth-order-Asynchronous-Learning-with-Bounded-Delays-with-a-Use-case-in-Resource-Allocation-in-Communication-Networks"><a href="#Zeroth-order-Asynchronous-Learning-with-Bounded-Delays-with-a-Use-case-in-Resource-Allocation-in-Communication-Networks" class="headerlink" title="Zeroth-order Asynchronous Learning with Bounded Delays with a Use-case in Resource Allocation in Communication Networks"></a>Zeroth-order Asynchronous Learning with Bounded Delays with a Use-case in Resource Allocation in Communication Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04604">http://arxiv.org/abs/2311.04604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pourya Behmandpoor, Marc Moonen, Panagiotis Patrinos</li>
<li>for: 这 paper 的目的是探讨分布式优化在分布式学习和适应中的应用。</li>
<li>methods: 该 paper 使用的方法包括分布式优化和 asynchronous learning。</li>
<li>results: 该 paper 提出了一种基于 zeroth-order oracle 的分布式优化方法，并提供了关于该方法 的理论收敛分析和实验结果。<details>
<summary>Abstract</summary>
Distributed optimization has experienced a significant surge in interest due to its wide-ranging applications in distributed learning and adaptation. While various scenarios, such as shared-memory, local-memory, and consensus-based approaches, have been extensively studied in isolation, there remains a need for further exploration of their interconnections. This paper specifically concentrates on a scenario where agents collaborate toward a unified mission while potentially having distinct tasks. Each agent's actions can potentially impact other agents through interactions. Within this context, the objective for the agents is to optimize their local parameters based on the aggregate of local reward functions, where only local zeroth-order oracles are available. Notably, the learning process is asynchronous, meaning that agents update and query their zeroth-order oracles asynchronously while communicating with other agents subject to bounded but possibly random communication delays. This paper presents theoretical convergence analyses and establishes a convergence rate for the proposed approach. Furthermore, it addresses the relevant issue of deep learning-based resource allocation in communication networks and conducts numerical experiments in which agents, acting as transmitters, collaboratively train their individual (possibly unique) policies to maximize a common performance metric.
</details>
<details>
<summary>摘要</summary>
分布式优化在分布式学习和适应中受到了广泛的关注，因为它们在各种应用场景中具有广泛的应用前景。虽然总共有分布式内存、本地内存和共识基本方法等多种场景被广泛研究，但还需要进一步探索这些场景之间的交互。这篇论文专门关注在多个代理协同完成一个统一任务的情况下，每个代理的行为可能会影响其他代理的交互中。在这个上下文中，代理的目标是基于所有代理的本地奖励函数的积和优化本地参数，但只有本地零次见识是可用的。备注，学习过程是异步的，代理在不同时间更新和查询本地零次见识，并在与其他代理通信时受到可能是随机的 bound 的通信延迟。本篇文章提供了理论快速整合分析和确定了快速整合率，并解决了深度学习基于通信网络的资源分配问题。此外，文章还进行了数值实验，在其中代理作为发送器，协同训练它们的个人（可能是唯一的）策略，以最大化一个共同性表现指标。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Based-Resource-Allocator-for-Communication-Systems-with-Dynamic-User-Utility-Demands"><a href="#A-Deep-Learning-Based-Resource-Allocator-for-Communication-Systems-with-Dynamic-User-Utility-Demands" class="headerlink" title="A Deep Learning Based Resource Allocator for Communication Systems with Dynamic User Utility Demands"></a>A Deep Learning Based Resource Allocator for Communication Systems with Dynamic User Utility Demands</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04600">http://arxiv.org/abs/2311.04600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pourya Behmandpoor, Panagiotis Patrinos, Marc Moonen</li>
<li>for: 这个论文主要用于研究一种基于深度学习的资源分配（RA）算法，以提高系统性能和灵活性。</li>
<li>methods: 该算法使用深度神经网络（DNN）来实现批处理优化算法，并在不同的分布式环境下进行资源分配。</li>
<li>results: 实验结果表明，该算法可以在不同的应用层和分布式环境下提供更高的系统性能和灵活性，并且可以适应不同的用户需求。<details>
<summary>Abstract</summary>
Deep learning (DL) based resource allocation (RA) has recently gained a lot of attention due to its performance efficiency. However, most of the related studies assume an ideal case where the number of users and their utility demands, e.g., data rate constraints, are fixed and the designed DL based RA scheme exploits a policy trained only for these fixed parameters. A computationally complex policy retraining is required whenever these parameters change. Therefore, in this paper, a DL based resource allocator (ALCOR) is introduced, which allows users to freely adjust their utility demands based on, e.g., their application layer. ALCOR employs deep neural networks (DNNs), as the policy, in an iterative optimization algorithm. The optimization algorithm aims to optimize the on-off status of users in a time-sharing problem to satisfy their utility demands in expectation. The policy performs unconstrained RA (URA) -- RA without taking into account user utility demands -- among active users to maximize the sum utility (SU) at each time instant. Based on the chosen URA scheme, ALCOR can perform RA in a model-based or model-free manner and in a centralized or distributed scenario. Derived convergence analyses provide guarantees for the convergence of ALCOR, and numerical experiments corroborate its effectiveness.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）基于资源分配（RA）在最近几年得到了很多关注，因为它的性能效率非常高。然而，大多数相关研究假设了一个理想的情况，在那里用户和他们的利用需求，例如数据速率限制，是固定的，而设计的 DL 基于 RA 方案仅仅利用一个已经训练过的策略来解决这些固定参数。在这种情况下，每当这些参数发生变化时，需要进行复杂的策略重新训练。因此，在本文中，一种基于 DL 的资源分配器（ALCOR）被引入，允许用户自由地调整他们的利用需求，例如应用层。ALCOR 使用深度神经网络（DNN）作为策略，并在迭代优化算法中使用。这个算法的目标是在时间分享问题中，通过调整用户的在线状态，来满足用户的利用需求。策略在活跃用户中进行不受限制的资源分配（URA），以 Maximize 时刻的总用户（SU）。根据选择的 URA 方案，ALCOR 可以进行模型基于或模型自由的 RA，以及中央或分布式的分配方式。从选择的 URA 方案而得出的算法可以提供 ALCOR 的 converges  garanties，并且数值实验证明了它的有效性。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Market-Value-in-Professional-Soccer-Insights-from-Explainable-Machine-Learning-Models"><a href="#Predicting-Market-Value-in-Professional-Soccer-Insights-from-Explainable-Machine-Learning-Models" class="headerlink" title="Predicting Market Value in Professional Soccer: Insights from Explainable Machine Learning Models"></a>Predicting Market Value in Professional Soccer: Insights from Explainable Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04599">http://arxiv.org/abs/2311.04599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyang Huang, Shaoliang Zhang</li>
<li>for: 这个研究旨在预测职业足球运动员市场价值使用可解释机器学习模型。</li>
<li>methods: 我们使用FIFA网站上的数据集合 ensemble机器学习方法和SHAP可解释方法进行预测。GBDT模型在评估中得分最高（0.8780）， Root Mean Squared Error最低（3,221,632.175），表明其在评估中的优异性。</li>
<li>results: 我们的分析发现，技能维度中的球控、短传、完成、抢断、踢球和攻击等特点具有重要性，而身体维度中的短跑速度和加速度具有重要性，认知维度中的反应具有先锋性。这些结果为职业足球运动员市场价值的更加准确、 объектив和一致的评估提供了有用的洞察。<details>
<summary>Abstract</summary>
This study presents an innovative method for predicting the market value of professional soccer players using explainable machine learning models. Using a dataset curated from the FIFA website, we employ an ensemble machine learning approach coupled with Shapley Additive exPlanations (SHAP) to provide detailed explanations of the models' predictions. The GBDT model achieves the highest mean R-Squared (0.8780) and the lowest mean Root Mean Squared Error (3,221,632.175), indicating its superior performance among the evaluated models. Our analysis reveals that specific skills such as ball control, short passing, finishing, interceptions, dribbling, and tackling are paramount within the skill dimension, whereas sprint speed and acceleration are critical in the fitness dimension, and reactions are preeminent in the cognitive dimension. Our results offer a more accurate, objective, and consistent framework for market value estimation, presenting useful insights for managerial decisions in player transfers.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "market value" is translated as "市场价值" (shìchǎng jīnyà)* "professional soccer players" is translated as "职业足球运动员" (zhíyè zúqiú yùndòng yuán)* "explainable machine learning models" is translated as "可解释机器学习模型" (kějìjué jīshì wùxíhuì módelì)* "ensemble machine learning approach" is translated as "集成机器学习方法" (jíshēn jīshì wùxíhuì fāngshì)* "Shapley Additive exPlanations" is translated as "沙普利添加式解释" (shāpǔlì tiānjiā xiěyì)* "GBDT model" is translated as "GBDT模型" (GBDT módelì)* "mean R-Squared" is translated as "平均R平方" (píngjì R píngfāng)* "mean Root Mean Squared Error" is translated as "平均根平方误差" (píngjì gēn píngfāng yuèshì)
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-as-a-tool-for-quantum-error-reduction-in-quantum-image-processing"><a href="#Deep-learning-as-a-tool-for-quantum-error-reduction-in-quantum-image-processing" class="headerlink" title="Deep learning as a tool for quantum error reduction in quantum image processing"></a>Deep learning as a tool for quantum error reduction in quantum image processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04575">http://arxiv.org/abs/2311.04575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krzysztof Werner, Kamil Wereszczyński, Rafał Potempa, Krzysztof Cyran</li>
<li>for: 图像表示的量子计算研究</li>
<li>methods: 使用量子排序、维度缩放和阶段缩放等方法实现图像表示</li>
<li>results: 通过将生成器推荐网络与阶段误差摘要方法相结合，实现图像表示中的总误差减少<details>
<summary>Abstract</summary>
Despite the limited availability and quantum volume of quantum computers, quantum image representation is a widely researched area. Currently developed methods use quantum entanglement to encode information about pixel positions. These methods range from using the angle parameter of the rotation gate (e.g., the Flexible Representation of Quantum Images, FRQI), sequences of qubits (e.g., Novel Enhanced Quantum Representation, NEQR), or the angle parameter of the phase shift gates (e.g., Local Phase Image Quantum Encoding, LPIQE) for storing color information. All these methods are significantly affected by decoherence and other forms of quantum noise, which is an inseparable part of quantum computing in the noisy intermediate-scale quantum era. These phenomena can highly influence the measurements and result in extracted images that are visually dissimilar to the originals. Because this process is at its foundation quantum, the computational reversal of this process is possible. There are many methods for error correction, mitigation, and reduction, but all of them use quantum computer time or additional qubits to achieve the desired result. We report the successful use of a generative adversarial network trained for image-to-image translation, in conjunction with Phase Distortion Unraveling error reduction method, for reducing overall error in images encoded using LPIQE.
</details>
<details>
<summary>摘要</summary>
尽管量子计算机的可用性和量子量都有限，量子图像表示仍然是广泛研究的领域。目前已经开发出的方法使用量子束缚来编码图像元素的信息。这些方法包括使用旋转门的角度参数（例如，自适应量子图像表示法，FRQI）、序列位域（例如，改进量子表示法，NEQR）或扩散门的角度参数（例如，本地阶段图像量子编码法，LPIQE）来存储颜色信息。这些方法都受到辐射噪和其他形式的量子噪声的影响，这些噪声是量子计算在不稳定中等级量子时期的不可避免的一部分。这些现象会高度影响测量结果，导致提取的图像与原始图像视觉上不同。由于这是基于量子的过程，因此可以进行量子计算的反转。有许多方法用于错误纠正、缓解和减少，但所有这些方法均需要量子计算时间或额外的寄存器来实现所需的结果。我们报道了使用基于图像到图像翻译的生成 adversarial network，与phas Distortion Unraveling错误纠正方法相结合，以减少LPIQE编码图像的总错误。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretic-Generalization-Bounds-for-Transductive-Learning-and-its-Applications"><a href="#Information-Theoretic-Generalization-Bounds-for-Transductive-Learning-and-its-Applications" class="headerlink" title="Information-Theoretic Generalization Bounds for Transductive Learning and its Applications"></a>Information-Theoretic Generalization Bounds for Transductive Learning and its Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04561">http://arxiv.org/abs/2311.04561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huayi Tang, Yong Liu</li>
<li>for: 这 paper 是为了研究逻辑学上的推导学习算法的通用 bounds。</li>
<li>methods: 该 paper 使用了数据依赖和算法依赖的通用 bounds，并引入了转ductive supersamples 的概念，以推广 inductive learning 的 Setting。</li>
<li>results: 该 paper 得到了在不同信息度下的通用 bounds，并derived novel PAC-Bayesian bounds和loss landscape flatness的连接。最后，paper 还提出了 adaptive optimization algorithms 的 upper bounds，并 validate 了 results 在 semi-supervised learning 和 graph learning 场景中。<details>
<summary>Abstract</summary>
In this paper, we develop data-dependent and algorithm-dependent generalization bounds for transductive learning algorithms in the context of information theory for the first time. We show that the generalization gap of transductive learning algorithms can be bounded by the mutual information between training labels and hypothesis. By innovatively proposing the concept of transductive supersamples, we go beyond the inductive learning setting and establish upper bounds in terms of various information measures. Furthermore, we derive novel PAC-Bayesian bounds and build the connection between generalization and loss landscape flatness under the transductive learning setting. Finally, we present the upper bounds for adaptive optimization algorithms and demonstrate the applications of results on semi-supervised learning and graph learning scenarios. Our theoretic results are validated on both synthetic and real-world datasets.
</details>
<details>
<summary>摘要</summary>
在本文中，我们为推论学习算法在信息理论上开发了数据依赖和算法依赖的通用泛化上限。我们证明了推论学习算法的泛化差可以通过训练标签和假设之间的共同信息来边界。我们创新地提出了推论超采样的概念，以超过步骤学习设定，并在不同信息度量下建立上限。此外，我们 derivated novel PAC-Bayesian bound和泛化与损失地形平坦的连接。最后，我们提供了适应优化算法的上限，并在半无监督学习和图学习场景中应用了结果。我们的理论结果在Synthetic和实际数据上得到验证。Note that Simplified Chinese is a written form of Chinese that uses simpler grammar and vocabulary than Traditional Chinese. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Regression-with-Cost-based-Rejection"><a href="#Regression-with-Cost-based-Rejection" class="headerlink" title="Regression with Cost-based Rejection"></a>Regression with Cost-based Rejection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04550">http://arxiv.org/abs/2311.04550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Cheng, Yuzhou Cao, Haobo Wang, Hongxin Wei, Bo An, Lei Feng</li>
<li>for: 避免错误预测，对于某些示例进行拒绝预测</li>
<li>methods: 使用成本基于的拒绝，对于某些示例进行拒绝预测</li>
<li>results: 提出了一个新的回归问题—— regression with cost-based rejection，并提出了一个可以解决这个问题的方法，并且进行了广泛的实验证明了方法的有效性。<details>
<summary>Abstract</summary>
Learning with rejection is an important framework that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
学习 WITH 拒绝是一种重要的框架，可以避免在预测时发生重要的误预测，通过平衡预测和拒绝来做到这一点。在过去的研究中，cost-based rejection仅限于分类设置，无法处理连续的无穷目标空间在回归设置下。在这篇论文中，我们研究了一种新的回归问题：基于成本的拒绝回归，其中模型可以根据certain rejection costs拒绝对一些示例进行预测。为解决这个问题，我们首先计算这个问题的预期风险，然后 derivate Bayes 优质解决方案，显示了优质模型应该在variance 大于 rejection cost 的示例上拒绝进行预测，当使用mean squared error 作为评价指标时。此外，我们提议使用surrogate 损失函数来训练模型，并提供了条件，以确保模型的一致性，这意味着我们的提议的surrogate损失函数可以回归 Bayes 优质解决方案。我们的实验结果表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="FEIR-Quantifying-and-Reducing-Envy-and-Inferiority-for-Fair-Recommendation-of-Limited-Resources"><a href="#FEIR-Quantifying-and-Reducing-Envy-and-Inferiority-for-Fair-Recommendation-of-Limited-Resources" class="headerlink" title="FEIR: Quantifying and Reducing Envy and Inferiority for Fair Recommendation of Limited Resources"></a>FEIR: Quantifying and Reducing Envy and Inferiority for Fair Recommendation of Limited Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04542">http://arxiv.org/abs/2311.04542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Li, Bo Kang, Jefrey Lijffijt, Tijl De Bie</li>
<li>for: 本研究旨在提出一种新的不平等度量，以解决电子招聘和在线约会中的推荐问题。</li>
<li>methods: 本文引入了一种新的不平等度量，称为“劣等”，用于量化用户对推荐的Item的竞争性益处。此外，本文还结合了“嫉妒”和“实用性”这三种度量，并将它们转化为可导数学函数。</li>
<li>results: 实验表明，本文提出的方法可以在 synthetic 数据和实际数据上提高不平等、嫉妒和实用性的融合，比静态推荐和基eline方法更优。<details>
<summary>Abstract</summary>
In settings such as e-recruitment and online dating, recommendation involves distributing limited opportunities, calling for novel approaches to quantify and enforce fairness. We introduce \emph{inferiority}, a novel (un)fairness measure quantifying a user's competitive disadvantage for their recommended items. Inferiority complements \emph{envy}, a fairness notion measuring preference for others' recommendations. We combine inferiority and envy with \emph{utility}, an accuracy-related measure of aggregated relevancy scores. Since these measures are non-differentiable, we reformulate them using a probabilistic interpretation of recommender systems, yielding differentiable versions. We combine these loss functions in a multi-objective optimization problem called \texttt{FEIR} (Fairness through Envy and Inferiority Reduction), applied as post-processing for standard recommender systems. Experiments on synthetic and real-world data demonstrate that our approach improves trade-offs between inferiority, envy, and utility compared to naive recommendations and the baseline methods.
</details>
<details>
<summary>摘要</summary>
在电子招聘和在线约会等设置中，推荐具有限制的机会，需要开发新的方法来衡量和保证公平。我们介绍了“劣等感”，一种新的公平度量量， quantifying 用户对推荐的物品的竞争性劣势。劣等感与“嫉妒”（envy）、“用处”（utility）这两种公平概念相结合，用于衡量用户对推荐的物品的喜好程度和准确性。由于这些度量函数不导数 differentiable，我们使用概率解释器来重新表示它们，得到了导数函数。我们将这些损失函数组合成了一个多目标优化问题 called \texttt{FEIR}（公平性 через 劣等感和嫉妒减少），用于补做标准推荐系统的后处理。实验表明，我们的方法可以在synthetic和实际数据上提高劣等感、嫉妒和用处之间的负荷平衡，比标准推荐和基eline方法更好。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Assisted-Multiuser-MIMO-Load-Modulated-Systems-for-Enhanced-Downlink-mmWave-Communications"><a href="#Deep-Learning-Assisted-Multiuser-MIMO-Load-Modulated-Systems-for-Enhanced-Downlink-mmWave-Communications" class="headerlink" title="Deep Learning Assisted Multiuser MIMO Load Modulated Systems for Enhanced Downlink mmWave Communications"></a>Deep Learning Assisted Multiuser MIMO Load Modulated Systems for Enhanced Downlink mmWave Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04537">http://arxiv.org/abs/2311.04537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ercong Yu, Jinle Zhu, Qiang Li, Zilong Liu, Hongyang Chen, Shlomo Shamai, H. Vincent Poor</li>
<li>For: The paper is focused on improving the performance of multiuser load modulation arrays (MU-LMAs) in millimeter wave (mmWave) multi-input multi-output (MIMO) systems.* Methods: The paper proposes two algorithms for MU-LMA systems employing a full-array structured (FAS) transmitter: an FAS-based normalized block diagonalization (FAS-NBD) algorithm and a Deep Learning-enhanced (FAS-DL-NBD) algorithm for adaptive codebook design and codebook-independent decoding.* Results: The proposed algorithms are shown to be robust to imperfect knowledge of channel state information and yield excellent error performance, with the FAS-DL-NBD algorithm enabling signal detection with low complexity as the number of bits per codeword increases.<details>
<summary>Abstract</summary>
This paper is focused on multiuser load modulation arrays (MU-LMAs) which are attractive due to their low system complexity and reduced cost for millimeter wave (mmWave) multi-input multi-output (MIMO) systems. The existing precoding algorithm for downlink MU-LMA relies on a sub-array structured (SAS) transmitter which may suffer from decreased degrees of freedom and complex system configuration. Furthermore, a conventional LMA codebook with codewords uniformly distributed on a hypersphere may not be channel-adaptive and may lead to increased signal detection complexity. In this paper, we conceive an MU-LMA system employing a full-array structured (FAS) transmitter and propose two algorithms accordingly. The proposed FAS-based system addresses the SAS structural problems and can support larger numbers of users. For LMA-imposed constant-power downlink precoding, we propose an FAS-based normalized block diagonalization (FAS-NBD) algorithm. However, the forced normalization may result in performance degradation. This degradation, together with the aforementioned codebook design problems, is difficult to solve analytically. This motivates us to propose a Deep Learning-enhanced (FAS-DL-NBD) algorithm for adaptive codebook design and codebook-independent decoding. It is shown that the proposed algorithms are robust to imperfect knowledge of channel state information and yield excellent error performance. Moreover, the FAS-DL-NBD algorithm enables signal detection with low complexity as the number of bits per codeword increases.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose an MU-LMA system employing a full-array structured (FAS) transmitter and put forward two algorithms accordingly. The proposed FAS-based system addresses the SAS structural problems and can support larger numbers of users. For LMA-imposed constant-power downlink precoding, we propose an FAS-based normalized block diagonalization (FAS-NBD) algorithm. However, the forced normalization may result in performance degradation. This degradation, together with the aforementioned codebook design problems, is difficult to solve analytically.To address these issues, we propose a Deep Learning-enhanced (FAS-DL-NBD) algorithm for adaptive codebook design and codebook-independent decoding. It is shown that the proposed algorithms are robust to imperfect knowledge of channel state information and yield excellent error performance. Moreover, the FAS-DL-NBD algorithm enables signal detection with low complexity as the number of bits per codeword increases.
</details></li>
</ul>
<hr>
<h2 id="An-Unsupervised-Deep-Learning-Approach-for-the-Wave-Equation-Inverse-Problem"><a href="#An-Unsupervised-Deep-Learning-Approach-for-the-Wave-Equation-Inverse-Problem" class="headerlink" title="An Unsupervised Deep Learning Approach for the Wave Equation Inverse Problem"></a>An Unsupervised Deep Learning Approach for the Wave Equation Inverse Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04531">http://arxiv.org/abs/2311.04531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiong-Bin Yan, Keke Wu, Zhi-Qin John Xu, Zheng Ma</li>
<li>for: 实时地震波形描述高精度地层物理参数</li>
<li>methods: 利用深度神经网络和部分数学方程来解决全波形描述问题</li>
<li>results: 比较 conventinal方法更好地重建地层物理速度参数<details>
<summary>Abstract</summary>
Full-waveform inversion (FWI) is a powerful geophysical imaging technique that infers high-resolution subsurface physical parameters by solving a non-convex optimization problem. However, due to limitations in observation, e.g., limited shots or receivers, and random noise, conventional inversion methods are confronted with numerous challenges, such as the local-minimum problem. In recent years, a substantial body of work has demonstrated that the integration of deep neural networks and partial differential equations for solving full-waveform inversion problems has shown promising performance. In this work, drawing inspiration from the expressive capacity of neural networks, we provide an unsupervised learning approach aimed at accurately reconstructing subsurface physical velocity parameters. This method is founded on a re-parametrization technique for Bayesian inference, achieved through a deep neural network with random weights. Notably, our proposed approach does not hinge upon the requirement of the labeled training dataset, rendering it exceedingly versatile and adaptable to diverse subsurface models. Extensive experiments show that the proposed approach performs noticeably better than existing conventional inversion methods.
</details>
<details>
<summary>摘要</summary>
全波形推算（FWI）是一种具有高分辨率地球物理参数推算技术，通过解决不对称优化问题来推算地球物理参数。然而，由于观测限制，如有限的发射器或接收器，以及随机噪声，传统的推算方法会遇到许多挑战，如本地最优问题。在过去几年，一大量的研究表明，将深度神经网络和部分偏微分方程相结合，可以在全波形推算问题中显著提高性能。在这种情况下，我们提出了一种无监督学习方法，旨在准确地重construct地球物理速度参数。这种方法基于 Bayesian 推理中的重arametrization技术，通过一个随机权重的深度神经网络来实现。吸引注意的是，我们提posed方法不依赖于标注训练数据集，因此非常灵活和适应性强，适用于多种不同的地球模型。广泛的实验表明，我们的方法在现有的推算方法之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Bandit-Learning-to-Rank-with-Position-Based-Click-Models-Personalized-and-Equal-Treatments"><a href="#Bandit-Learning-to-Rank-with-Position-Based-Click-Models-Personalized-and-Equal-Treatments" class="headerlink" title="Bandit Learning to Rank with Position-Based Click Models: Personalized and Equal Treatments"></a>Bandit Learning to Rank with Position-Based Click Models: Personalized and Equal Treatments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04528">http://arxiv.org/abs/2311.04528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchen Zhou, Jia Liu, Yang Jiao, Chaosheng Dong, Yetian Chen, Yan Gao, Yi Sun</li>
<li>for: 本研究的目的是提出一个基于多重投掷机(MAB)的在线学习排名算法，用于推荐系统中的排名问题。</li>
<li>methods: 本研究使用了MAB框架和位置基于点击模型来模型在线学习排名问题。</li>
<li>results: 本研究提出了一个全面的MAB模型，并开发了两种名为GreedyRank和UCBRank的政策，这两种政策可以应用于个性化和平等的排名待遇。此外，研究还证明了这两种政策在不同的问题设定下都可以达到$O(\sqrt{t}\ln t)$和$O(\sqrt{t\ln t})$的任何线性 regret。<details>
<summary>Abstract</summary>
Online learning to rank (ONL2R) is a foundational problem for recommender systems and has received increasing attention in recent years. Among the existing approaches for ONL2R, a natural modeling architecture is the multi-armed bandit framework coupled with the position-based click model. However, developing efficient online learning policies for MAB-based ONL2R with position-based click models is highly challenging due to the combinatorial nature of the problem, and partial observability in the position-based click model. To date, results in MAB-based ONL2R with position-based click models remain rather limited, which motivates us to fill this gap in this work. Our main contributions in this work are threefold: i) We propose the first general MAB framework that captures all key ingredients of ONL2R with position-based click models. Our model considers personalized and equal treatments in ONL2R ranking recommendations, both of which are widely used in practice; ii) Based on the above analytical framework, we develop two unified greed- and UCB-based policies called GreedyRank and UCBRank, each of which can be applied to personalized and equal ranking treatments; and iii) We show that both GreedyRank and UCBRank enjoy $O(\sqrt{t}\ln t)$ and $O(\sqrt{t\ln t})$ anytime sublinear regret for personalized and equal treatment, respectively. For the fundamentally hard equal ranking treatment, we identify classes of collective utility functions and their associated sufficient conditions under which $O(\sqrt{t}\ln t)$ and $O(\sqrt{t\ln t})$ anytime sublinear regrets are still achievable for GreedyRank and UCBRank, respectively. Our numerical experiments also verify our theoretical results and demonstrate the efficiency of GreedyRank and UCBRank in seeking the optimal action under various problem settings.
</details>
<details>
<summary>摘要</summary>
“在线学习排名（ONL2R）是推荐系统的基础问题，在最近几年内得到了越来越多的关注。现有的ONL2R方法中，一种自然的建模架构是多重投掷机制（MAB）结合位置基于点击模型。然而，开发高效的在线学习策略 дляMAB基于ONL2R的位置基于点击模型是极其困难的，因为这个问题具有 combinatorial 性和部分可见性。到目前为止，关于MAB基于ONL2R的位置基于点击模型的结果尚未充分，这种挑战我们在这个工作中填补。我们的主要贡献是三fold：1. 我们提出了第一个涵盖ONL2R所有关键因素的MAB框架。我们的模型考虑了个性化和平等的排名推荐，这两种方法在实践中广泛使用；2. 基于上述分析框架，我们开发了两种总是优于UCRL（Upper Confidence Bound with Lookahead）的政策，即GreedyRank和UCBRank，每个政策都可以应用于个性化和平等的排名推荐；3. 我们证明了GreedyRank和UCBRank在个性化和平等排名下都可以获得$O(\sqrt{t}\ln t)$和$O(\sqrt{t\ln t})$的任何时间下折Linear regret，而且对于平等排名，我们还提出了一些类型的集合性能函数和其相应的充分条件，这些条件下，GreedyRank和UCBRank仍然可以获得$O(\sqrt{t}\ln t)$和$O(\sqrt{t\ln t})$的任何时间下折Linear regret。我们的numerical experiments也验证了我们的理论结果，并证明了GreedyRank和UCBRank在不同的问题设置下都能够有效地寻找优化的行动。”
</details></li>
</ul>
<hr>
<h2 id="Long-term-Time-Series-Forecasting-based-on-Decomposition-and-Neural-Ordinary-Differential-Equations"><a href="#Long-term-Time-Series-Forecasting-based-on-Decomposition-and-Neural-Ordinary-Differential-Equations" class="headerlink" title="Long-term Time Series Forecasting based on Decomposition and Neural Ordinary Differential Equations"></a>Long-term Time Series Forecasting based on Decomposition and Neural Ordinary Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04522">http://arxiv.org/abs/2311.04522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonkyu Lim, Jaehyeon Park, Seojin Kim, Hyowon Wi, Haksoo Lim, Jinsung Jeon, Jeongwhan Choi, Noseong Park</li>
<li>for: 这个研究是为了解决长期时间序列预测（LTSF） tasks 中的挑战，例如财务投资、健康照顾、交通预测和天气预测等领域。</li>
<li>methods: 本研究提出了一个基于线性常微分方程（ODEs）和时间序列分解方法的模型，以优化时间序列预测的性能。</li>
<li>results:  compared with基于Transformer的方法，LTSF-DNODE模型在多个真实世界数据集上表现出色，并且在不同数据集上进行了各种调整的研究。<details>
<summary>Abstract</summary>
Long-term time series forecasting (LTSF) is a challenging task that has been investigated in various domains such as finance investment, health care, traffic, and weather forecasting. In recent years, Linear-based LTSF models showed better performance, pointing out the problem of Transformer-based approaches causing temporal information loss. However, Linear-based approach has also limitations that the model is too simple to comprehensively exploit the characteristics of the dataset. To solve these limitations, we propose LTSF-DNODE, which applies a model based on linear ordinary differential equations (ODEs) and a time series decomposition method according to data statistical characteristics. We show that LTSF-DNODE outperforms the baselines on various real-world datasets. In addition, for each dataset, we explore the impacts of regularization in the neural ordinary differential equation (NODE) framework.
</details>
<details>
<summary>摘要</summary>
长期时系列预测（LTSF）是一项复杂的任务，在不同领域如金融投资、医疗、交通和天气预测中都有广泛的研究。在最近几年，线性基于的LTSF模型表现更好，表明Transformer基于的方法会导致时间信息损失。然而，线性基于的方法也有限制，模型太简单，无法充分利用数据集的特点。为解决这些限制，我们提出了LTSF-DNODE，它使用基于线性常微分方程（ODEs）的模型和根据数据统计特点的时间序列分解方法。我们表明，LTSF-DNODE在各种真实世界数据集上表现出色，并且对每个数据集进行了减少正则化的影响分析。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Mirror-Descent-Bilevel-Optimization"><a href="#Adaptive-Mirror-Descent-Bilevel-Optimization" class="headerlink" title="Adaptive Mirror Descent Bilevel Optimization"></a>Adaptive Mirror Descent Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04520">http://arxiv.org/abs/2311.04520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feihu Huang</li>
<li>for: 这种纸子是为了解决非 convex 二级优化问题而写的。</li>
<li>methods: 这种方法使用了镜像下降法，并且使用了投影援助加速器（i.e., AdaPAG）和随机投影援助加速器（i.e., AdaVSPAG）来解决非 convex 二级优化问题。</li>
<li>results: 这种方法可以在 $O(\epsilon^{-1})$ 和 $O(\epsilon^{-3&#x2F;2})$ 两种情况下取得最佳known的加速率，并且可以用于解决非 convex 强 convex 二级优化问题。<details>
<summary>Abstract</summary>
In the paper, we propose a class of efficient adaptive bilevel methods based on mirror descent for nonconvex bilevel optimization, where its upper-level problem is nonconvex possibly with nonsmooth regularization, and its lower-level problem is also nonconvex while satisfies Polyak-{\L}ojasiewicz (PL) condition. To solve these deterministic bilevel problems, we present an efficient adaptive projection-aid gradient (i.e., AdaPAG) method based on mirror descent, and prove that it obtains the best known gradient complexity of $O(\epsilon^{-1})$ for finding an $\epsilon$-stationary solution of nonconvex bilevel problems. To solve these stochastic bilevel problems, we propose an efficient adaptive stochastic projection-aid gradient (i.e., AdaVSPAG) methods based on mirror descent and variance-reduced techniques, and prove that it obtains the best known gradient complexity of $O(\epsilon^{-3/2})$ for finding an $\epsilon$-stationary solution. Since the PL condition relaxes the strongly convex, our algorithms can be used to nonconvex strongly-convex bilevel optimization. Theoretically, we provide a useful convergence analysis framework for our methods under some mild conditions, and prove that our methods have a fast convergence rate of $O(\frac{1}{T})$, where $T$ denotes the number of iterations.
</details>
<details>
<summary>摘要</summary>
在论文中，我们提出了一种高效的适应双层方法基于镜像极小值算法，用于非 convex 双层优化问题，其上层问题为非 convex 可能具有非精炤规则化，下层问题则满足波佩-{\L}ojasiewicz（PL）条件。为解决这些权重 deterministic 双层问题，我们提出了一种高效的适应投影帮助梯度算法（i.e., AdaPAG），并证明其可以在 $O(\epsilon^{-1})$ 的梯度复杂度下找到一个 $\epsilon$-站点解决方案。为解决这些随机双层问题，我们提出了一种高效的适应随机投影帮助梯度算法（i.e., AdaVSPAG），并证明其可以在 $O(\epsilon^{-3/2})$ 的梯度复杂度下找到一个 $\epsilon$-站点解决方案。由于 PL 条件宽松了强 convex，我们的算法可以应用于非 convex 强 convex 双层优化问题。从理论角度来看，我们提供了一个有用的 convergence 分析框架，并证明我们的方法在某些轻量级的 услови下具有 $O(\frac{1}{T})$ 的快速收敛率，其中 $T$ 表示迭代次数。
</details></li>
</ul>
<hr>
<h2 id="Towards-Democratizing-AI-A-Comparative-Analysis-of-AI-as-a-Service-Platforms-and-the-Open-Space-for-Machine-Learning-Approach"><a href="#Towards-Democratizing-AI-A-Comparative-Analysis-of-AI-as-a-Service-Platforms-and-the-Open-Space-for-Machine-Learning-Approach" class="headerlink" title="Towards Democratizing AI: A Comparative Analysis of AI as a Service Platforms and the Open Space for Machine Learning Approach"></a>Towards Democratizing AI: A Comparative Analysis of AI as a Service Platforms and the Open Space for Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04518">http://arxiv.org/abs/2311.04518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dennis Rall, Bernhard Bauer, Thomas Fraunholz</li>
<li>for: 这篇论文的目的是实现人工智能的民主化，对于无技术背景的人员提供更多的机会使用人工智能。</li>
<li>methods: 这篇论文使用了对多个流行的人工智能服务平台进行比较，并评估了这些平台是否能够实现人工智能的民主化。</li>
<li>results: 论文提出了一个新的平台“Open Space for Machine Learning”，这个平台使用了最新的技术如Kubernetes、Kubeflow Pipelines和Ludwig，可以更好地满足人工智能的民主化需求。<details>
<summary>Abstract</summary>
Recent AI research has significantly reduced the barriers to apply AI, but the process of setting up the necessary tools and frameworks can still be a challenge. While AI-as-a-Service platforms have emerged to simplify the training and deployment of AI models, they still fall short of achieving true democratization of AI. In this paper, we aim to address this gap by comparing several popular AI-as-a-Service platforms and identifying the key requirements for a platform that can achieve true democratization of AI. Our analysis highlights the need for self-hosting options, high scalability, and openness. To address these requirements, we propose our approach: the "Open Space for Machine Learning" platform. Our platform is built on cutting-edge technologies such as Kubernetes, Kubeflow Pipelines, and Ludwig, enabling us to overcome the challenges of democratizing AI. We argue that our approach is more comprehensive and effective in meeting the requirements of democratizing AI than existing AI-as-a-Service platforms.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的人工智能研究已经大大降低了应用人工智能的门槛，但是设置必要的工具和框架仍然是一个挑战。人工智能为服务平台已经出现，以便简化人工智能模型的训练和部署，但是它们仍然无法实现真正的人工智能民主化。在这篇论文中，我们想要解决这个差距，因此我们比较了一些流行的人工智能为服务平台，并确定了实现人工智能民主化的关键要求。我们的分析表明，自主主机选项、高可扩展性和开放性是必要的。为了解决这些要求，我们提出了我们的方法：“机器学习开放空间”平台。我们的平台基于最新的技术，如Kubernetes、Kubeflow Pipelines和Ludwig，这使得我们可以超越民主化人工智能的挑战。我们认为，我们的方法比现有的人工智能为服务平台更全面和有效地实现了人工智能民主化的要求。
</details></li>
</ul>
<hr>
<h2 id="Strategies-for-Parallelizing-the-Big-Means-Algorithm-A-Comprehensive-Tutorial-for-Effective-Big-Data-Clustering"><a href="#Strategies-for-Parallelizing-the-Big-Means-Algorithm-A-Comprehensive-Tutorial-for-Effective-Big-Data-Clustering" class="headerlink" title="Strategies for Parallelizing the Big-Means Algorithm: A Comprehensive Tutorial for Effective Big Data Clustering"></a>Strategies for Parallelizing the Big-Means Algorithm: A Comprehensive Tutorial for Effective Big Data Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04517">http://arxiv.org/abs/2311.04517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravil Mussabayev, Rustam Mussabayev</li>
<li>for: 本研究探讨了对大规模数据集进行归一化的大均值算法优化问题，探索了四种不同的并行策略。</li>
<li>methods: 本研究采用了广泛的实验来评估每种策略的计算效率、可扩展性和归一化性能，并探讨了各种因素对归一化质量的影响。</li>
<li>results: 研究发现了不同策略的优缺点，并提供了实际的资源和数据特点选择最佳并行策略的指导原则，为并行技术的深入理解做出贡献。<details>
<summary>Abstract</summary>
This study focuses on the optimization of the Big-means algorithm for clustering large-scale datasets, exploring four distinct parallelization strategies. We conducted extensive experiments to assess the computational efficiency, scalability, and clustering performance of each approach, revealing their benefits and limitations. The paper also delves into the trade-offs between computational efficiency and clustering quality, examining the impacts of various factors. Our insights provide practical guidance on selecting the best parallelization strategy based on available resources and dataset characteristics, contributing to a deeper understanding of parallelization techniques for the Big-means algorithm.
</details>
<details>
<summary>摘要</summary>
这项研究关注大规模数据集 clustering 的大平均算法优化，探讨了四种不同的并行策略。我们进行了广泛的实验，评估了每种方法的计算效率、可扩展性和归一化性，并分析了各种因素对结果的影响。这些发现可以为我们在具有不同资源和数据特点的情况下选择最佳并行策略提供实践性的指导， deepened our understanding of parallelization techniques for the Big-means algorithm。
</details></li>
</ul>
<hr>
<h2 id="Solution-of-FPK-Equation-for-Stochastic-Dynamics-Subjected-to-Additive-Gaussian-Noise-via-Deep-Learning-Approach"><a href="#Solution-of-FPK-Equation-for-Stochastic-Dynamics-Subjected-to-Additive-Gaussian-Noise-via-Deep-Learning-Approach" class="headerlink" title="Solution of FPK Equation for Stochastic Dynamics Subjected to Additive Gaussian Noise via Deep Learning Approach"></a>Solution of FPK Equation for Stochastic Dynamics Subjected to Additive Gaussian Noise via Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04511">http://arxiv.org/abs/2311.04511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir H. Khodabakhsh, Seid H. Pourtakdoust</li>
<li>for: 解决高维度泊松方程的精炼问题</li>
<li>methods: 基于物理法则的深度学习网络（FPK-DP Net）</li>
<li>results: 可以解决高维度实际问题，无需依赖于先 simulate 数据，并且可以作为高效的准确模型使用<details>
<summary>Abstract</summary>
The Fokker-Plank-Kolmogorov (FPK) equation is an idealized model representing many stochastic systems commonly encountered in the analysis of stochastic structures as well as many other applications. Its solution thus provides an invaluable insight into the performance of many engineering systems. Despite its great importance, the solution of the FPK equation is still extremely challenging. For systems of practical significance, the FPK equation is usually high dimensional, rendering most of the numerical methods ineffective. In this respect, the present work introduces the FPK-DP Net as a physics-informed network that encodes the physical insights, i.e. the governing constrained differential equations emanated out of physical laws, into a deep neural network. FPK-DP Net is a mesh-free learning method that can solve the density evolution of stochastic dynamics subjected to additive white Gaussian noise without any prior simulation data and can be used as an efficient surrogate model afterward. FPK-DP Net uses the dimension-reduced FPK equation. Therefore, it can be used to address high-dimensional practical problems as well. To demonstrate the potential applicability of the proposed framework, and to study its accuracy and efficacy, numerical implementations on five different benchmark problems are investigated.
</details>
<details>
<summary>摘要</summary>
“福克-普朗-科穆洛夫（FPK）方程是一种理想化的模型，广泛应用于杂相随机系统的分析和其他多种应用。它的解释能提供各种工程系统的性能的有价值信息。然而，解决FPK方程仍然是非常困难的。实际应用中的FPK方程通常是高维的，使得大多数数字方法无效。在这个问题上，本工作提出了FPK-DP网，它是一种基于物理法则的深度学习网络，可以解决受加性白噪声影响的杂相动态系统的浓度演化。FPK-DP网是一种无网格学习方法，不需要任何先前的仪器数据，可以作为高效的代理模型。由于FPK-DP网使用缩少的FPK方程，因此可以应用于高维实际问题。为了证明提出的框架的可能应用性和准确性，本文对五个不同的 benchmark 问题进行了数学实现。”Note that Simplified Chinese is a written language, and the translation is based on the standardized grammar and vocabulary of Simplified Chinese. The translation may vary depending on the specific dialect or regional variation of Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Constrained-Adaptive-Attacks-Realistic-Evaluation-of-Adversarial-Examples-and-Robust-Training-of-Deep-Neural-Networks-for-Tabular-Data"><a href="#Constrained-Adaptive-Attacks-Realistic-Evaluation-of-Adversarial-Examples-and-Robust-Training-of-Deep-Neural-Networks-for-Tabular-Data" class="headerlink" title="Constrained Adaptive Attacks: Realistic Evaluation of Adversarial Examples and Robust Training of Deep Neural Networks for Tabular Data"></a>Constrained Adaptive Attacks: Realistic Evaluation of Adversarial Examples and Robust Training of Deep Neural Networks for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04503">http://arxiv.org/abs/2311.04503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thibault Simonetto, Salah Ghamizi, Antoine Desjardins, Maxime Cordy, Yves Le Traon</li>
<li>for: 这篇论文的目的是为了评估深度学习模型在表格数据上的抗辐射性能。</li>
<li>methods: 这篇论文提出了CAA，首个适用于受限表格深度学习模型的谱攻击方法，它结合了梯度和搜索攻击来生成受限制的攻击示例。</li>
<li>results: 研究人员使用CAA建立了三个受欢迎的应用场景（信用评分、骗取和机器人攻击检测）的深度表格模型的攻击 benchmark，并在不同的威胁模型下进行了测试。结果显示了不同的领域知识、抗辐射训练和攻击预算对深度表格模型的抗辐射性能产生了不同的影响。<details>
<summary>Abstract</summary>
State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings. However, the robustness of these models remains scarcely explored. Contrary to computer vision, there is to date no realistic protocol to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data such as categorical features, immutability, and feature relationship constraints. To fill this gap, we propose CAA, the first efficient evasion attack for constrained tabular deep learning models. CAA is an iterative parameter-free attack that combines gradient and search attacks to generate adversarial examples under constraints. We leverage CAA to build a benchmark of deep tabular models across three popular use cases: credit scoring, phishing and botnet attacks detection. Our benchmark supports ten threat models with increasing capabilities of the attacker, and reflects real-world attack scenarios for each use case. Overall, our results demonstrate how domain knowledge, adversarial training, and attack budgets impact the robustness assessment of deep tabular models and provide security practitioners with a set of recommendations to improve the robustness of deep tabular models against various evasion attack scenarios.
</details>
<details>
<summary>摘要</summary>
现代深度学习模型在表格数据上已经实现了可接受的性能，可以在工业环境中部署。然而，这些模型的可靠性仍未得到充分探索。与计算机视觉不同，到目前为止没有一个实用协议来评估深表格模型的攻击Robustness，这主要归结于表格数据的内在特性，如分类特征、不可变性和特征关系约束。为了填补这一漏洞，我们提出了CAA，首个可效的练习攻击方法 для受限表格深度学习模型。CAA是一种循环 parameter-free攻击，将梯度和搜索攻击结合起来生成攻击示例，并且具有约束。我们利用CAA建立了深表格模型的权威 benchmark，包括银行评分、骗取和机器人攻击检测等三个Popular use case。我们的 benchmark 支持了十种威胁模型，每种威胁模型都有不同的攻击能力，并且反映了实际攻击enario。总的来说，我们的结果表明了domain knowledge、对抗训练和攻击预算对深表格模型的可靠性评估产生了重要的影响，并提供了一组建议来改善深表格模型对不同攻击enario的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Advanced-Aerial-Mobility-–-An-End-to-end-Autonomy-Framework-for-UAVs-and-Beyond"><a href="#Autonomous-Advanced-Aerial-Mobility-–-An-End-to-end-Autonomy-Framework-for-UAVs-and-Beyond" class="headerlink" title="Autonomous Advanced Aerial Mobility – An End-to-end Autonomy Framework for UAVs and Beyond"></a>Autonomous Advanced Aerial Mobility – An End-to-end Autonomy Framework for UAVs and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04472">http://arxiv.org/abs/2311.04472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sakshi Mishra, Praveen Palanisamy</li>
<li>for: 这篇论文的目的是提供一个权威的 perspective on the emerging field of autonomous advanced aerial mobility, 包括使用无人航空器（UAV）和电动垂直起降（eVTOL）飞机进行各种应用程序，如城市空中交通、快递和监测。</li>
<li>methods: 该论文提出了一个可扩展和可控的自主框架，包括感知、识别、规划和控制四个主要块。此外，论文还讨论了多机体队伍运行和管理的挑战和机遇，以及自主飞行系统的测试、验证和证明方面的问题。</li>
<li>results: 该论文探讨了自主高空 mobilty 领域的未来方向，并分析了MONOLITHIC 模型的优势和局限性。<details>
<summary>Abstract</summary>
Developing aerial robots that can both safely navigate and execute assigned mission without any human intervention - i.e., fully autonomous aerial mobility of passengers and goods - is the larger vision that guides the research, design, and development efforts in the aerial autonomy space. However, it is highly challenging to concurrently operationalize all types of aerial vehicles that are operating fully autonomously sharing the airspace. Full autonomy of the aerial transportation sector includes several aspects, such as design of the technology that powers the vehicles, operations of multi-agent fleets, and process of certification that meets stringent safety requirements of aviation sector. Thereby, Autonomous Advanced Aerial Mobility is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we present a comprehensive perspective on the emerging field of autonomous advanced aerial mobility, which involves the use of unmanned aerial vehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft for various applications, such as urban air mobility, package delivery, and surveillance. The article proposes a scalable and extensible autonomy framework consisting of four main blocks: sensing, perception, planning, and controls. Furthermore, the article discusses the challenges and opportunities in multi-agent fleet operations and management, as well as the testing, validation, and certification aspects of autonomous aerial systems. Finally, the article explores the potential of monolithic models for aerial autonomy and analyzes their advantages and limitations. The perspective aims to provide a holistic picture of the autonomous advanced aerial mobility field and its future directions.
</details>
<details>
<summary>摘要</summary>
发展可以安全地导航并完成任务无需人类干预的空中机器人，即全自动空中 mobilit y 的既客货运输 - 是研究、设计和开发团队的大vision。然而，同时操作具有完全自动化功能的所有空中 Vehicle 共享空间是非常挑战性的。全自动化空运行业包括许多方面，如机器人技术的设计、多机器人队列的运作和遵循 strict 航空行业安全要求的证明过程。因此，自主高级空中 mobilit y 的概念仍然是混乱的，对研究人员和专业人员来说是抽象的。为了填补这个空白，我们提出了一篇全面的自主高级空中 mobilit y 领域的视角，该领域包括使用无人航空器（UAV）和电动垂直起降（eVTOL）飞机进行多种应用，如城市空中 mobilit y、快递和监测。文章提出了可扩展和可重复的自主框架，包括感知、识别、规划和控制四个主要块。此外，文章还讨论了多机器人队列的运作和管理挑战和机器人自主系统的测试、验证和证明方面的问题。最后，文章探讨了单体模型在空中自主领域的优势和局限性。该视角的目的是为自主高级空中 mobilit y 领域提供一个整体的图像，以及未来方向。
</details></li>
</ul>
<hr>
<h2 id="Solving-High-Frequency-and-Multi-Scale-PDEs-with-Gaussian-Processes"><a href="#Solving-High-Frequency-and-Multi-Scale-PDEs-with-Gaussian-Processes" class="headerlink" title="Solving High Frequency and Multi-Scale PDEs with Gaussian Processes"></a>Solving High Frequency and Multi-Scale PDEs with Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04465">http://arxiv.org/abs/2311.04465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikai Fang, Madison Cooley, Da Long, Shibo Li, Robert Kirby, Shandian Zhe</li>
<li>for: 用于解决高频和多尺度的 partial differential equations (PDEs) 问题，以提高物理 simulations 和科学计算的精度和效率。</li>
<li>methods: 使用 Gaussian process (GP) 框架，模型 PDE 解的力спектrum 使用 student t 混合物或 Gaussian 混合物，并通过 inverse Fourier transform 获取 covariance function。使用 Jeffreys prior 来估计混合物的权重，并使用 GP  conditional mean 来预测解和其导数，以满足边界条件和方程本身。</li>
<li>results: 通过系统性的实验，提出了一种基于 GP 的高效和可扩展的 PDE 解法，可以处理大量的 collocation points，并且不需要低级 approximations。实验结果表明，该方法可以提高解的精度和效率，并且可以处理高频和多尺度的 PDEs。<details>
<summary>Abstract</summary>
Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student t mixture or Gaussian mixture. We then apply the inverse Fourier transform to obtain the covariance function (according to the Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. We are the first to discover its rationale and effectiveness for PDE solving. Next,we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remaining toward the ground truth. Third, to enable efficient and scalable computation on massive collocation points, which are critical to capture high frequencies, we place the collocation points on a grid, and multiply our covariance function at each input dimension. We use the GP conditional mean to predict the solution and its derivatives so as to fit the boundary condition and the equation itself. As a result, we can derive a Kronecker product structure in the covariance matrix. We use Kronecker product properties and multilinear algebra to greatly promote computational efficiency and scalability, without any low-rank approximations. We show the advantage of our method in systematic experiments.
</details>
<details>
<summary>摘要</summary>
《机器学习基于的解决方案在物理 simulate 和科学计算中备受关注，例如物理学习神经网络（PINNs）。但是，PINNs 经常遇到高频和多尺度的 partial differential equations（PDEs）解决问题，这可能是在神经网络训练过程中的spectral bias。为了解决这个问题，我们使用 Gaussian process（GP）框架。我们模型 PDE 解的能量谱，使用学生 t 混合或 Gaussian 混合来灵活地捕捉解的主要频率。然后，我们应用 inverse Fourier transform 获得 covariance function（根据 Wiener-Khinchin 定理）。这个 covariance function 与known spectral mixture kernel相对应。我们是第一个发现其理由和效果，并且在 PDE 解决中应用。接下来，我们在循环域中估计混合Weight，我们表明这equivalent to placing a Jeffreys prior。它自动带来稀疏性，排除过度频率，并调整剩下来向真实值。三、为了efficiently和可扩展地计算大量的 collocation points，这些点 kritical 捕捉高频，我们将 collocation points 放在网格上，并将我们的 covariance function 乘以每个输入维度。我们使用 GP conditional mean 预测解和其Derivatives，以适应边界条件和方程本身。因此，我们可以 deriv a Kronecker product structure 在 covariance matrix 中。我们使用 Kronecker product properties 和多线性代数，无需低级 Approximations，提高了计算效率和可扩展性。我们在系统性实验中展示了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Spatial-Transformer-for-Massive-Point-Samples-in-Continuous-Space"><a href="#A-Hierarchical-Spatial-Transformer-for-Massive-Point-Samples-in-Continuous-Space" class="headerlink" title="A Hierarchical Spatial Transformer for Massive Point Samples in Continuous Space"></a>A Hierarchical Spatial Transformer for Massive Point Samples in Continuous Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04434">http://arxiv.org/abs/2311.04434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spatialdatasciencegroup/hst">https://github.com/spatialdatasciencegroup/hst</a></li>
<li>paper_authors: Wenchong He, Zhe Jiang, Tingsong Xiao, Zelin Xu, Shigang Chen, Ronald Fick, Miles Medina, Christine Angelini</li>
<li>for: This paper proposes a novel transformer model for massive point samples in continuous space, addressing challenges such as implicit long-range and multi-scale dependency, non-uniform point distribution, and high computational costs.</li>
<li>methods: The proposed hierarchical spatial transformer model includes multi-resolution representation learning within a quad-tree hierarchy and efficient spatial attention via coarse approximation, as well as an uncertainty quantification branch to estimate prediction confidence.</li>
<li>results: The proposed method outperforms multiple baselines in prediction accuracy and can scale up to one million points on one NVIDIA A100 GPU, as demonstrated through extensive experiments on both real-world and synthetic datasets.<details>
<summary>Abstract</summary>
Transformers are widely used deep learning architectures. Existing transformers are mostly designed for sequences (texts or time series), images or videos, and graphs. This paper proposes a novel transformer model for massive (up to a million) point samples in continuous space. Such data are ubiquitous in environment sciences (e.g., sensor observations), numerical simulations (e.g., particle-laden flow, astrophysics), and location-based services (e.g., POIs and trajectories). However, designing a transformer for massive spatial points is non-trivial due to several challenges, including implicit long-range and multi-scale dependency on irregular points in continuous space, a non-uniform point distribution, the potential high computational costs of calculating all-pair attention across massive points, and the risks of over-confident predictions due to varying point density. To address these challenges, we propose a new hierarchical spatial transformer model, which includes multi-resolution representation learning within a quad-tree hierarchy and efficient spatial attention via coarse approximation. We also design an uncertainty quantification branch to estimate prediction confidence related to input feature noise and point sparsity. We provide a theoretical analysis of computational time complexity and memory costs. Extensive experiments on both real-world and synthetic datasets show that our method outperforms multiple baselines in prediction accuracy and our model can scale up to one million points on one NVIDIA A100 GPU. The code is available at \url{https://github.com/spatialdatasciencegroup/HST}.
</details>
<details>
<summary>摘要</summary>
transformers 是深度学习架构的广泛使用。现有的 transformers 主要是 для文本、图像或视频，以及图。这篇论文提出了一种新的 transformer 模型，用于处理大量（达到一百万）的连续空间点样本。这些数据在环境科学（例如感知器观察）、数值仿真（例如含有粒子的流体和天文物理）以及地理位置服务（例如 POI 和轨迹）中都很常见。然而，为大量的空间点设计 transformer 是非常困难的，因为存在许多挑战，包括不明确的长距离和多尺度依赖，不均匀的点分布，计算所有对对关注的计算成本可能很高，以及输入特征噪声和点稀缺性可能导致不确定的预测。为解决这些挑战，我们提出了一种新的层次空间 transformer 模型，包括在 Quad-tree 层次结构中进行多尺度表示学习和高效的空间关注。我们还设计了一个 uncertainty 评估分支，用于评估输入特征噪声和点稀缺性导致的预测信度。我们还提供了计算时间复杂度和内存成本的理论分析。在实际实验中，我们的方法在多个实际和 sintetic 数据集上表现出色，并且可以在一个 NVIDIA A100 GPU 上处理一百万点。代码可以在 <https://github.com/spatialdatasciencegroup/HST> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Emerging-AI-ML-Accelerators-IPU-RDU-and-NVIDIA-AMD-GPUs"><a href="#Evaluating-Emerging-AI-ML-Accelerators-IPU-RDU-and-NVIDIA-AMD-GPUs" class="headerlink" title="Evaluating Emerging AI&#x2F;ML Accelerators: IPU, RDU, and NVIDIA&#x2F;AMD GPUs"></a>Evaluating Emerging AI&#x2F;ML Accelerators: IPU, RDU, and NVIDIA&#x2F;AMD GPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04417">http://arxiv.org/abs/2311.04417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongwu Peng, Caiwen Ding, Tong Geng, Sutanay Choudhury, Kevin Barker, Ang Li</li>
<li>for: 这个研究旨在评估和比较现有的商业人工智能&#x2F;机器学习加速器，了解它们的硬件和软件设计特点，以及它们在常见的深度神经网络操作和其他人工智能&#x2F;机器学习任务中的表现。</li>
<li>methods: 本研究使用了一系列的单元测试，评估了不同的商业加速器在深度神经网络操作和其他人工智能&#x2F;机器学习任务中的表现。</li>
<li>results: 研究发现，使用数据流Architecture的加速器在深度神经网络操作中表现出色，并且在其他人工智能&#x2F;机器学习任务中也表现了优秀的表现。<details>
<summary>Abstract</summary>
The relentless advancement of artificial intelligence (AI) and machine learning (ML) applications necessitates the development of specialized hardware accelerators capable of handling the increasing complexity and computational demands. Traditional computing architectures, based on the von Neumann model, are being outstripped by the requirements of contemporary AI/ML algorithms, leading to a surge in the creation of accelerators like the Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU), and enhanced GPU platforms. These hardware accelerators are characterized by their innovative data-flow architectures and other design optimizations that promise to deliver superior performance and energy efficiency for AI/ML tasks.   This research provides a preliminary evaluation and comparison of these commercial AI/ML accelerators, delving into their hardware and software design features to discern their strengths and unique capabilities. By conducting a series of benchmark evaluations on common DNN operators and other AI/ML workloads, we aim to illuminate the advantages of data-flow architectures over conventional processor designs and offer insights into the performance trade-offs of each platform. The findings from our study will serve as a valuable reference for the design and performance expectations of research prototypes, thereby facilitating the development of next-generation hardware accelerators tailored for the ever-evolving landscape of AI/ML applications. Through this analysis, we aspire to contribute to the broader understanding of current accelerator technologies and to provide guidance for future innovations in the field.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）和机器学习（ML）应用的不断发展需要特化的硬件加速器来处理不断增长的复杂性和计算需求。传统的计算架构，基于 von Neumann 模型，由于当今 AI/ML 算法的要求而落后，导致加速器的出现，如 Graphcore 智能处理单元（IPU）、Sambanova 可重新配置数据流单元（RDU）以及增强的 GPU 平台。这些硬件加速器具有创新的数据流架构和其他设计优化，承诺提供更高的性能和能效率 для AI/ML 任务。本研究提供了这些商业 AI/ML 加速器的初步评估和比较，探讨了他们的硬件和软件设计特点，以了解它们的优势和特殊能力。通过对常见深度神经网络（DNN）操作和其他 AI/ML 任务的benchmark评估，我们希望探明数据流架构在传统处理器设计方面的优势，并提供每台平台的性能交易OFF。研究结果将成为未来硬件加速器设计和性能预期的重要参考，以便为AI/ML应用场景不断演化的硬件加速器设计下一代。通过这种分析，我们 aspire to 贡献到当前加速器技术的更广泛理解，并为未来的创新提供指导。
</details></li>
</ul>
<hr>
<h2 id="Likelihood-Ratio-Confidence-Sets-for-Sequential-Decision-Making"><a href="#Likelihood-Ratio-Confidence-Sets-for-Sequential-Decision-Making" class="headerlink" title="Likelihood Ratio Confidence Sets for Sequential Decision Making"></a>Likelihood Ratio Confidence Sets for Sequential Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04402">http://arxiv.org/abs/2311.04402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Emmenegger, Mojmír Mutný, Andreas Krause</li>
<li>for: 这篇论文是为了提供一种可证明、适应uncertainty estimate的方法，用于Sequential decision-making算法。</li>
<li>methods: 该方法基于 likelihood-based inference principle，使用 likelihood ratio 构建 any-time 有效的 confidence sequences，不需要特殊的处理在具体应用场景中。</li>
<li>results: 该方法适用于具有准确specified likelihood的问题，其结果始终保持预定的覆盖率，并且可以在model-agnostic manner中进行any-time有效的 confidence estimation。<details>
<summary>Abstract</summary>
Certifiable, adaptive uncertainty estimates for unknown quantities are an essential ingredient of sequential decision-making algorithms. Standard approaches rely on problem-dependent concentration results and are limited to a specific combination of parameterization, noise family, and estimator. In this paper, we revisit the likelihood-based inference principle and propose to use likelihood ratios to construct any-time valid confidence sequences without requiring specialized treatment in each application scenario. Our method is especially suitable for problems with well-specified likelihoods, and the resulting sets always maintain the prescribed coverage in a model-agnostic manner. The size of the sets depends on a choice of estimator sequence in the likelihood ratio. We discuss how to provably choose the best sequence of estimators and shed light on connections to online convex optimization with algorithms such as Follow-the-Regularized-Leader. To counteract the initially large bias of the estimators, we propose a reweighting scheme that also opens up deployment in non-parametric settings such as RKHS function classes. We provide a non-asymptotic analysis of the likelihood ratio confidence sets size for generalized linear models, using insights from convex duality and online learning. We showcase the practical strength of our method on generalized linear bandit problems, survival analysis, and bandits with various additive noise distributions.
</details>
<details>
<summary>摘要</summary>
证明可靠的不确定性估计对不确定量是sequential decision-making算法的重要组件。既定方法取决于问题依赖的集中结果，而且只能在特定的参数化、噪声家族和估计器上下采取特定的方法。在这篇论文中，我们回顾了概率基于的推理原理，并提议使用likelihood比率构建任何时间有效的信心集，无需特殊对各应用场景进行处理。我们的方法特别适用于具有明确概率的问题，并且得到的集 siempre maintain the prescribed coverage in a model-agnostic manner。likelihood ratio confidence sets的大小取决于选择的估计器序列。我们讨论了如何选择最佳的估计器序列，并 shed light on connections to online convex optimization algorithms such as Follow-the-Regularized-Leader。为了减少初始偏差，我们提议了一种重weighting scheme，该方案还可以在非 Parametric setting such as RKHS function classes中应用。我们提供了非假设分析likelihood ratio confidence sets的大小 для泛化线性模型，使用了 convex duality和在线学习的 Insights。我们在 generalized linear bandit problems, survival analysis, and bandits with various additive noise distributions中示cases the practical strength of our method.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/cs.LG_2023_11_08/" data-id="clorjzlab00syf18884qpae9u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/08/cs.CL_2023_11_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CL - 2023-11-08
        
      </div>
    </a>
  
  
    <a href="/2023/11/08/eess.IV_2023_11_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-11-08</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
