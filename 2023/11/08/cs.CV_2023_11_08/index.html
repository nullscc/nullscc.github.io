
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-11-08 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04901 repo_url: None paper_authors: Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Ho">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-11-08">
<meta property="og:url" content="https://nullscc.github.io/2023/11/08/cs.CV_2023_11_08/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2311.04901 repo_url: None paper_authors: Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Ho">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-11-08T13:00:00.000Z">
<meta property="article:modified_time" content="2023-11-09T19:58:08.699Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_11_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/08/cs.CV_2023_11_08/" class="article-date">
  <time datetime="2023-11-08T13:00:00.000Z" itemprop="datePublished">2023-11-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-11-08
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GENOME-GenerativE-Neuro-symbOlic-visual-reasoning-by-growing-and-reusing-ModulEs"><a href="#GENOME-GenerativE-Neuro-symbOlic-visual-reasoning-by-growing-and-reusing-ModulEs" class="headerlink" title="GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs"></a>GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04901">http://arxiv.org/abs/2311.04901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, Chuang Gan</li>
<li>for: 提高视觉理解能力，使用大语言模型（LLMs）推动传统的神经 символиック模型，以实现强大的视觉理解结果，同时保持模型的透明度和效率。</li>
<li>methods: 使用LLMs对每个任务进行分析，判断是否可以重用和扩展现有模块来解决该任务。如果不可以，则初始化一个新模块，并指定输入和输出。然后，通过查询LLMs来生成匹配要求的代码段，并对新模块进行几何训练。</li>
<li>results: 提出了一种生成神经符号学模型，可以在几何训练例程中学习和 reuse 现有模块，以实现新任务的视觉理解。模型在标准任务上表现竞争力强，并且可以将学习到的模块转移到新任务上，同时能够适应新的视觉理解任务。<details>
<summary>Abstract</summary>
Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate language into module descriptions, thus achieving strong visual reasoning results while maintaining the model's transparency and efficiency. However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. We propose generative neuro-symbolic visual reasoning by growing and reusing modules. Specifically, our model consists of three unique stages, module initialization, module generation, and module execution. First, given a vision-language task, we adopt LLMs to examine whether we could reuse and grow over established modules to handle this new task. If not, we initialize a new module needed by the task and specify the inputs and outputs of this new module. After that, the new module is created by querying LLMs to generate corresponding code snippets that match the requirements. In order to get a better sense of the new module's ability, we treat few-shot training examples as test cases to see if our new module could pass these cases. If yes, the new module is added to the module library for future reuse. Finally, we evaluate the performance of our model on the testing set by executing the parsed programs with the newly made visual modules to get the results. We find the proposed model possesses several advantages. First, it performs competitively on standard tasks like visual question answering and referring expression comprehension; Second, the modules learned from one task can be seamlessly transferred to new tasks; Last but not least, it is able to adapt to new visual reasoning tasks by observing a few training examples and reusing modules.
</details>
<details>
<summary>摘要</summary>
现有研究表明，大语言模型（LLM）可以使传统的神经符号模型通过编程功能来理解语言，从而实现强大的视觉逻辑结果，同时保持模型的透明度和效率。然而，这些模型通常会浪费大量时间来生成整个代码块给每个新任务，这是非常不高效。我们提议使用生成的神经符号视觉逻辑。具体来说，我们的模型包括三个独特的阶段：模块初始化、模块生成和模块执行。首先，给定一个视觉语言任务，我们采用LLM来检查是否可以重用和增长现有的模块来处理这个新任务。如果不可以，我们将新增一个需要的模块，并指定这个模块的输入和输出。然后，我们通过查询LLM来生成匹配要求的代码块，以创建这个新模块。为了更好地了解这个新模块的能力，我们将几个干净训练例作为测试用例，以判断这个新模块是否可以通过这些测试。如果可以，则将这个新模块添加到模块库中，以供未来的重用。最后，我们使用解析后的程序来评估模型的性能，并通过执行这些新生成的视觉模块来获得结果。我们发现我们提议的模型具有以下优点：一、在标准任务如视觉问答和表达理解方面表现竞争力强；二、学习到的模块可以很好地转移到新任务上；三、可以通过几个干净训练例来适应新的视觉逻辑任务。
</details></li>
</ul>
<hr>
<h2 id="Are-foundation-models-efficient-for-medical-image-segmentation"><a href="#Are-foundation-models-efficient-for-medical-image-segmentation" class="headerlink" title="Are foundation models efficient for medical image segmentation?"></a>Are foundation models efficient for medical image segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04847">http://arxiv.org/abs/2311.04847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danielle Ferreira, Rima Arnaout</li>
<li>for: 这个论文主要是为了评估Segment Anything模型（SAM）在医学影像Segmentation方面的性能和资源养成。</li>
<li>methods: 这个论文使用了SAM模型和一种模式特定、标签自由的自然学习（SSL）方法，对100次医学ultrasound图像进行25个测量。</li>
<li>results: SAM模型的性能相对较差，需要更多的标签和计算资源，而SSL方法则显示出了更高的效率。<details>
<summary>Abstract</summary>
Foundation models are experiencing a surge in popularity. The Segment Anything model (SAM) asserts an ability to segment a wide spectrum of objects but required supervised training at unprecedented scale. We compared SAM's performance (against clinical ground truth) and resources (labeling time, compute) to a modality-specific, label-free self-supervised learning (SSL) method on 25 measurements for 100 cardiac ultrasounds. SAM performed poorly and required significantly more labeling and computing resources, demonstrating worse efficiency than SSL.
</details>
<details>
<summary>摘要</summary>
基础模型目前正在受欢迎。Segment Anything模型（SAM）表明可以对广泛的对象进行分割，但需要前所未有的大规模指导式训练。我们比较了SAM的表现（与临床真实标注）和资源（标注时间、计算）与一种特定Modalitate、标签自由自学习（SSL）方法在25个测量上对100个心脏超声图像。SAM表现不佳，需要更多的标注和计算资源，示出了与SSL的更差效率。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Visual-Relationship-Detection-through-Masked-Bounding-Box-Reconstruction"><a href="#Self-Supervised-Learning-for-Visual-Relationship-Detection-through-Masked-Bounding-Box-Reconstruction" class="headerlink" title="Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction"></a>Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04834">http://arxiv.org/abs/2311.04834</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deeplab-ai/selfsupervisedvrd">https://github.com/deeplab-ai/selfsupervisedvrd</a></li>
<li>paper_authors: Zacharias Anastasakis, Dimitrios Mallis, Markos Diomataris, George Alexandridis, Stefanos Kollias, Vassilis Pitsikalis</li>
<li>for: 这篇论文是为了提出一种自然语言生成模型，尤其是用于视觉关系检测任务（VRD）的自我超vised学习方法。</li>
<li>methods: 这篇论文提出了一种基于Masked Image Modeling（MIM）的变种，称为Masked Bounding Box Reconstruction（MBBR），其中一定比例的场景中的实体&#x2F;对象会被遮盖，然后通过未遮盖的对象进行重建。这种方法的核心思想是通过对象级别的遮盖模型，使网络学习场景中对象之间的交互的上下文感知，并且通过这种方法学习出高度预测视觉对象关系的表示。</li>
<li>results: 作者们在一些几个shot设定下进行了质量和量тив的评估，并证明了MBBR在VRD任务中比静态方法更高效，使用只需要几个标注样本就能够超越现有的VRD方法。<details>
<summary>Abstract</summary>
We present a novel self-supervised approach for representation learning, particularly for the task of Visual Relationship Detection (VRD). Motivated by the effectiveness of Masked Image Modeling (MIM), we propose Masked Bounding Box Reconstruction (MBBR), a variation of MIM where a percentage of the entities/objects within a scene are masked and subsequently reconstructed based on the unmasked objects. The core idea is that, through object-level masked modeling, the network learns context-aware representations that capture the interaction of objects within a scene and thus are highly predictive of visual object relationships. We extensively evaluate learned representations, both qualitatively and quantitatively, in a few-shot setting and demonstrate the efficacy of MBBR for learning robust visual representations, particularly tailored for VRD. The proposed method is able to surpass state-of-the-art VRD methods on the Predicate Detection (PredDet) evaluation setting, using only a few annotated samples. We make our code available at https://github.com/deeplab-ai/SelfSupervisedVRD.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的自助学习方法，尤其适用于视觉关系检测（VRD）任务。我们的方法受到Masked Image Modeling（MIM）的成功所 inspirited，我们提议Masked Bounding Box Reconstruction（MBBR），这是MIM中一种变种，其中场景中一部分对象被遮盖，然后通过不遮盖的对象进行重建。核心思想是通过对象级别的遮盖模型，网络学习场景中对象之间的交互，从而学习出高度预测性的视觉对象关系表示。我们广泛评估学习的表示，包括质量和量度的评估，在几个预测设定下，并证明MBBR可以在几个预测设定下超越当前VRD方法，只需使用几个注释样本。我们的代码可以在https://github.com/deeplab-ai/SelfSupervisedVRD中获取。
</details></li>
</ul>
<hr>
<h2 id="Anonymizing-medical-case-based-explanations-through-disentanglement"><a href="#Anonymizing-medical-case-based-explanations-through-disentanglement" class="headerlink" title="Anonymizing medical case-based explanations through disentanglement"></a>Anonymizing medical case-based explanations through disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04833">http://arxiv.org/abs/2311.04833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helena Montenegro, Jaime S. Cardoso</li>
<li>for: 这个论文是为了解释深度学习模型在医疗上的决策过程。</li>
<li>methods: 论文提出了一种新的方法，用于分离图像中的人脸和医学特征，以便在医疗上分享图像。这种方法使用了一种替换特征向量的机制，以保持图像的医学特征，同时将人脸信息替换为Synthetic Privacy-Preserving Identity。</li>
<li>results: 实验表明，这种方法可以生成真实的、适用于医疗的隐私保护图像，同时保持原始医学内容。此外，实验还发现网络内置了生成对抗图像的能力。<details>
<summary>Abstract</summary>
Case-based explanations are an intuitive method to gain insight into the decision-making process of deep learning models in clinical contexts. However, medical images cannot be shared as explanations due to privacy concerns. To address this problem, we propose a novel method for disentangling identity and medical characteristics of images and apply it to anonymize medical images. The disentanglement mechanism replaces some feature vectors in an image while ensuring that the remaining features are preserved, obtaining independent feature vectors that encode the images' identity and medical characteristics. We also propose a model to manufacture synthetic privacy-preserving identities to replace the original image's identity and achieve anonymization. The models are applied to medical and biometric datasets, demonstrating their capacity to generate realistic-looking anonymized images that preserve their original medical content. Additionally, the experiments show the network's inherent capacity to generate counterfactual images through the replacement of medical features.
</details>
<details>
<summary>摘要</summary>
可以使用 случаي explanations 来了解深度学习模型在医疗场景中做出决策的过程。然而，医疗图像不能被分享作为解释，因为隐私问题。为解决这个问题，我们提议一种新的方法，即分离图像的身份和医疗特征。我们称之为“图像分离”。这种方法可以在图像中替换一些特征向量，以保证图像的其余特征被保留，从而获得独立的特征向量，这些特征向量编码着图像的身份和医疗特征。我们还提议一种模型，用于生成隐私保护的虚拟人工身份来替换原始图像的身份，以实现医疗图像的匿名化。这些模型在医疗和生物ometrics dataset上进行了应用，并在生成真实看起来的匿名化图像，同时保留图像的原始医疗内容。此外，实验还示出了网络的内置Counterfactual图像生成能力，通过替换医疗特征来生成对抗性图像。
</details></li>
</ul>
<hr>
<h2 id="SODAWideNet-–-Salient-Object-Detection-with-an-Attention-augmented-Wide-Encoder-Decoder-network-without-ImageNet-pre-training"><a href="#SODAWideNet-–-Salient-Object-Detection-with-an-Attention-augmented-Wide-Encoder-Decoder-network-without-ImageNet-pre-training" class="headerlink" title="SODAWideNet – Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training"></a>SODAWideNet – Salient Object Detection with an Attention augmented Wide Encoder Decoder network without ImageNet pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04828">http://arxiv.org/abs/2311.04828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VimsLab/SODAWideNet">https://github.com/VimsLab/SODAWideNet</a></li>
<li>paper_authors: Rohit Venkata Sai Dulam, Chandra Kambhamettu</li>
<li>for: 本研究旨在提出一种新的突出物检测模型（SOD），而不需要在ImageNet预训练上进行复杂的 retrained。</li>
<li>methods: 我们提出了一种自适应的干扰检测模型，包括MRFFAM模块和多 scales attention（MSA）模块，以及一种自适应的卷积搅拌扩展。</li>
<li>results: 我们在五个数据集上进行了比较，并达到了与现有模型相当的竞争性表现。<details>
<summary>Abstract</summary>
Developing a new Salient Object Detection (SOD) model involves selecting an ImageNet pre-trained backbone and creating novel feature refinement modules to use backbone features. However, adding new components to a pre-trained backbone needs retraining the whole network on the ImageNet dataset, which requires significant time. Hence, we explore developing a neural network from scratch directly trained on SOD without ImageNet pre-training. Such a formulation offers full autonomy to design task-specific components. To that end, we propose SODAWideNet, an encoder-decoder-style network for Salient Object Detection. We deviate from the commonly practiced paradigm of narrow and deep convolutional models to a wide and shallow architecture, resulting in a parameter-efficient deep neural network. To achieve a shallower network, we increase the receptive field from the beginning of the network using a combination of dilated convolutions and self-attention. Therefore, we propose Multi Receptive Field Feature Aggregation Module (MRFFAM) that efficiently obtains discriminative features from farther regions at higher resolutions using dilated convolutions. Next, we propose Multi-Scale Attention (MSA), which creates a feature pyramid and efficiently computes attention across multiple resolutions to extract global features from larger feature maps. Finally, we propose two variants, SODAWideNet-S (3.03M) and SODAWideNet (9.03M), that achieve competitive performance against state-of-the-art models on five datasets.
</details>
<details>
<summary>摘要</summary>
开发新的突出 объек特征检测（SOD）模型需要选择一个ImageNet预训练后置和创建专门的特征修正模块，以使用后置特征。但是，添加新组件到预训练后置需要重新训练整个网络在ImageNet dataset上，这需要很长时间。因此，我们探索直接从 scratch 开发一个神经网络，用于 Salient Object Detection 而不需要 ImageNet 预训练。这种设计方式允许我们完全自主地设计任务特定的组件。为此，我们提出 SODAWideNet，一种编码器-解码器式神经网络，用于 Salient Object Detection。我们与常见的窄深 convolutional 模型不同，采用了宽浅的架构，从而实现参数效率的深度神经网络。为了增加网络的宽度，我们在网络的开始处使用扩展 convolutions 和自注意力来增加感知场。因此，我们提出 Multi Receptive Field Feature Aggregation Module (MRFFAM)，它可以高效地从较远区域获取更高分辨率的特征。然后，我们提出 Multi-Scale Attention (MSA)，它可以快速计算多尺度的注意力，从而提取更大的特征图。最后，我们提出 SODAWideNet-S (3.03M) 和 SODAWideNet (9.03M) 两种变种，它们在五个数据集上实现了与当前状态的竞争性性能。
</details></li>
</ul>
<hr>
<h2 id="Cross-Silo-Federated-Learning-Across-Divergent-Domains-with-Iterative-Parameter-Alignment"><a href="#Cross-Silo-Federated-Learning-Across-Divergent-Domains-with-Iterative-Parameter-Alignment" class="headerlink" title="Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment"></a>Cross-Silo Federated Learning Across Divergent Domains with Iterative Parameter Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04818">http://arxiv.org/abs/2311.04818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mattgorb/iterative_parameter_alignment">https://github.com/mattgorb/iterative_parameter_alignment</a></li>
<li>paper_authors: Matt Gorbett, Hossein Shirazi, Indrakshi Ray</li>
<li>for: 本研究旨在提高神经网络的总结能力，通过在私有数据源中散布的数据收集知识来启发神经网络。</li>
<li>methods: 本研究使用联合训练方法，通过中心服务器的调度来将客户端模型相互结合。</li>
<li>results: 研究发现，当client domains suficiently不同时，现有的方法很难以收敛，并且现有的汇集技术会生成每个客户端的同样的全球模型。本研究解决这两个问题，并提出了一种新的框架，即迭代参数对齐。这种框架可以在cross-silo设置下自然地应用，具有以下特点：（i）每个参与者都有唯一的解决方案，并且可以在联邦中每个模型都globally converge。（ii）可选的早期停止机制，以便在合作学习设置下让每个参与者具有公平的学习机会。这些特点共同提供了一种可Iteratively学习从peer模型在不同数据集上进行训练的灵活新框架。我们发现，该技术在多个数据分区比例上与当前的方法竞争。此外，我们还证明了该方法在不同领域（i.e. 客户端数据集中的分离类）的情况下具有良好的Robustness。<details>
<summary>Abstract</summary>
Learning from the collective knowledge of data dispersed across private sources can provide neural networks with enhanced generalization capabilities. Federated learning, a method for collaboratively training a machine learning model across remote clients, achieves this by combining client models via the orchestration of a central server. However, current approaches face two critical limitations: i) they struggle to converge when client domains are sufficiently different, and ii) current aggregation techniques produce an identical global model for each client. In this work, we address these issues by reformulating the typical federated learning setup: rather than learning a single global model, we learn N models each optimized for a common objective. To achieve this, we apply a weighted distance minimization to model parameters shared in a peer-to-peer topology. The resulting framework, Iterative Parameter Alignment, applies naturally to the cross-silo setting, and has the following properties: (i) a unique solution for each participant, with the option to globally converge each model in the federation, and (ii) an optional early-stopping mechanism to elicit fairness among peers in collaborative learning settings. These characteristics jointly provide a flexible new framework for iteratively learning from peer models trained on disparate datasets. We find that the technique achieves competitive results on a variety of data partitions compared to state-of-the-art approaches. Further, we show that the method is robust to divergent domains (i.e. disjoint classes across peers) where existing approaches struggle.
</details>
<details>
<summary>摘要</summary>
通过收集分散在私有源数据上的共同知识，神经网络可以获得增强的通用能力。联邦学习方法可以在远程客户端之间协同训练机器学习模型，并将客户端模型集成到中央服务器的指挥下。然而，现有的方法面临两个严重的限制：一是在客户端领域差异足够大时困难 converge，二是现有的汇集技术会生成每个客户端上的相同全球模型。在这项工作中，我们解决这些问题，通过在参数之间分配权重，来实现参数的对照Alignment。这种方法可以自然地应用于跨私有集成（cross-silo）的情况，并具有以下特点：（i）每个参与者都有独特的解决方案，可以在联邦中每个模型的全球化进程中选择地 globally converge 每个模型，（ii）在合作学习设置中，可以使用可选的早期停止机制来寻求参与者之间的公平性。这些特点共同提供了一种灵活的新框架，可以逐步学习来自不同数据集的 peer 模型。我们发现，这种技术可以与当前的状态艺术方法竞争，并且在不同的数据分区上表现出色。此外，我们还证明了该方法在不同领域（i.e. 客户端领域中的分类）中具有强大的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptive-Object-Detection-via-Balancing-Between-Self-Training-and-Adversarial-Learning"><a href="#Domain-Adaptive-Object-Detection-via-Balancing-Between-Self-Training-and-Adversarial-Learning" class="headerlink" title="Domain Adaptive Object Detection via Balancing Between Self-Training and Adversarial Learning"></a>Domain Adaptive Object Detection via Balancing Between Self-Training and Adversarial Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04815">http://arxiv.org/abs/2311.04815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, Mohsen Ali</li>
<li>for: 这篇论文的目的是提高深度学习基础的物体检测器在新目标领域中的整合能力，因为这些检测器在面临新的领域时通常会受到许多物品和背景的变化所困扰。</li>
<li>methods: 这篇论文使用了两种方法来实现这个目的：一是使用图像或实例对抗性特征Alignment，但这经常会因为背景的影响而受到限制，而且缺乏对特定类别的Alignment。另一方面，这篇论文提出了一个简单的方法，即使用高信任率的预测值作为pseudo-label，并且使用预测值的不确定度来调节对抗特征Alignment和类别Alignment之间的平衡。</li>
<li>results: 这篇论文的结果显示，使用这种方法可以比据面临新的领域时，与现有的方法相比，有较高的整合能力和性能。实验结果显示，这种方法可以在五个多样化和挑战性的领域中实现较好的整合和性能。<details>
<summary>Abstract</summary>
Deep learning based object detectors struggle generalizing to a new target domain bearing significant variations in object and background. Most current methods align domains by using image or instance-level adversarial feature alignment. This often suffers due to unwanted background and lacks class-specific alignment. A straightforward approach to promote class-level alignment is to use high confidence predictions on unlabeled domain as pseudo-labels. These predictions are often noisy since model is poorly calibrated under domain shift. In this paper, we propose to leverage model's predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. We develop a technique to quantify predictive uncertainty on class assignments and bounding-box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-training, whereas the ones with higher uncertainty are used to generate tiles for adversarial feature alignment. This synergy between tiling around uncertain object regions and generating pseudo-labels from highly certain object regions allows capturing both image and instance-level context during the model adaptation. We report thorough ablation study to reveal the impact of different components in our approach. Results on five diverse and challenging adaptation scenarios show that our approach outperforms existing state-of-the-art methods with noticeable margins.
</details>
<details>
<summary>摘要</summary>
深度学习基于对象检测器在新目标领域中总是难以泛化，主要原因是对象和背景中的变化很大。现有方法通常使用图像或实例级别的对抗特征对齐。然而，这经常会受到背景的影响，并且缺乏类别匹配。为了促进类别匹配，我们提出使用高信息content的预测值作为pseudo-标签。这些预测值经常是噪音的，因为模型在频率变换下不具备良好的准确性。在这篇论文中，我们提出使用模型预测的不确定性来平衡对抗特征对齐和类别匹配。我们开发了一种方法来衡量模型预测中的不确定性，并将其用于生成pseudo-标签和对抗特征对齐。这种对不确定性范围内的瓦解和高信息content预测值生成pseudo-标签的方法，使得在模型适应过程中捕捉到图像和实例级别的上下文。我们进行了系统的ablation研究，以便了解不同组件的影响。我们的方法在五种复杂和挑战性的适应场景中显著超过了现有状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Be-Careful-When-Evaluating-Explanations-Regarding-Ground-Truth"><a href="#Be-Careful-When-Evaluating-Explanations-Regarding-Ground-Truth" class="headerlink" title="Be Careful When Evaluating Explanations Regarding Ground Truth"></a>Be Careful When Evaluating Explanations Regarding Ground Truth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04813">http://arxiv.org/abs/2311.04813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mi2datalab/be-careful-evaluating-explanations">https://github.com/mi2datalab/be-careful-evaluating-explanations</a></li>
<li>paper_authors: Hubert Baniecki, Maciej Chrabaszcz, Andreas Holzinger, Bastian Pfeifer, Anna Saranti, Przemyslaw Biecek</li>
<li>for: 这个论文的目的是评估深度神经网络和解释方法的可靠性，它们在实际应用中如医学图像分析和机器人控制中越来越广泛使用。</li>
<li>methods: 这个论文提出了一种框架，可以同时评估这些系统的可靠性和解释方法的质量。它们使用了一种精度调整过程来（不）对模型和解释方法进行对齐，并使用这种过程来衡量模型和解释方法之间的差异。</li>
<li>results: 经过多种模型体系和后处地本地解释方法的实验，研究发现了视Transformers的可靠性和AI系统总体受到可能的敌意攻击的抵触性。<details>
<summary>Abstract</summary>
Evaluating explanations of image classifiers regarding ground truth, e.g. segmentation masks defined by human perception, primarily evaluates the quality of the models under consideration rather than the explanation methods themselves. Driven by this observation, we propose a framework for $\textit{jointly}$ evaluating the robustness of safety-critical systems that $\textit{combine}$ a deep neural network with an explanation method. These are increasingly used in real-world applications like medical image analysis or robotics. We introduce a fine-tuning procedure to (mis)align model$\unicode{x2013}$explanation pipelines with ground truth and use it to quantify the potential discrepancy between worst and best-case scenarios of human alignment. Experiments across various model architectures and post-hoc local interpretation methods provide insights into the robustness of vision transformers and the overall vulnerability of such AI systems to potential adversarial attacks.
</details>
<details>
<summary>摘要</summary>
evaluating explanations of image classifiers regarding ground truth, e.g. segmentation masks defined by human perception, primarily evaluates the quality of the models under consideration rather than the explanation methods themselves. driven by this observation, we propose a framework for jointly evaluating the robustness of safety-critical systems that combine a deep neural network with an explanation method. these are increasingly used in real-world applications like medical image analysis or robotics. we introduce a fine-tuning procedure to (mis)align model-explanation pipelines with ground truth and use it to quantify the potential discrepancy between worst and best-case scenarios of human alignment. experiments across various model architectures and post-hoc local interpretation methods provide insights into the robustness of vision transformers and the overall vulnerability of such AI systems to potential adversarial attacks.Here's the text with some notes on the translation:* "image classifiers" is translated as "图像分类器" (tú zhǐ bǐng lèi jī)* "ground truth" is translated as "真实的事实" (zhēn shí de shí shí)* "segmentation masks" is translated as "分割 маSK" (fēn zhī ma sk)* "human perception" is translated as "人类的感知" (rén xìng de gǎn zhī)* "primarily evaluates" is translated as "主要评估" (zhǔ yào píng yào)* "robustness" is translated as "鲜度" (xiān duō)* "safety-critical systems" is translated as "安全关键系统" (ān què yuè xiàng jì tǒng)* "deep neural network" is translated as "深度神经网络" (shēn dào shén zhì wǎng wǎn)* "explanation method" is translated as "解释方法" (jiě jiě fāng fá)* "post-hoc local interpretation methods" is translated as "后期本地解释方法" (hòu qǐ běn dì jiě jiě fāng fá)* "adversarial attacks" is translated as "抗击攻击" (kàng zhì gōng jī)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. Traditional Chinese is used in Taiwan and other countries, and the translation may be slightly different in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Image-Based-Virtual-Try-On-A-Survey"><a href="#Image-Based-Virtual-Try-On-A-Survey" class="headerlink" title="Image-Based Virtual Try-On: A Survey"></a>Image-Based Virtual Try-On: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04811">http://arxiv.org/abs/2311.04811</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/little-misfit/survey-of-virtual-try-on">https://github.com/little-misfit/survey-of-virtual-try-on</a></li>
<li>paper_authors: Dan Song, Xuanpu Zhang, Juan Zhou, Weizhi Nie, Ruofeng Tong, An-An Liu</li>
<li>for: 这篇论文主要目标是对图像基 Virtual Try-On 技术进行概述和分析，并提出未来研究方向。</li>
<li>methods: 论文使用了现有的图像生成技术，如人体匠心技术和图像涂抹技术，以及新的Semantic CLIP 技术。</li>
<li>results: 论文通过对现有方法的量化和质量评估，以及ControlNet 等新技术的应用，展示了图像基 Virtual Try-On 技术的未来发展潜力。<details>
<summary>Abstract</summary>
Image-based virtual try-on aims to synthesize a naturally dressed person image with a clothing image, which revolutionizes online shopping and inspires related topics within image generation, showing both research significance and commercial potentials. However, there is a great gap between current research progress and commercial applications and an absence of comprehensive overview towards this field to accelerate the development. In this survey, we provide a comprehensive analysis of the state-of-the-art techniques and methodologies in aspects of pipeline architecture, person representation and key modules such as try-on indication, clothing warping and try-on stage. We propose a new semantic criteria with CLIP, and evaluate representative methods with uniformly implemented evaluation metrics on the same dataset. In addition to quantitative and qualitative evaluation of current open-source methods, we also utilize ControlNet to fine-tune a recent large image generation model (PBE) to show future potentials of large-scale models on image-based virtual try-on task. Finally, unresolved issues are revealed and future research directions are prospected to identify key trends and inspire further exploration. The uniformly implemented evaluation metrics, dataset and collected methods will be made public available at https://github.com/little-misfit/Survey-Of-Virtual-Try-On.
</details>
<details>
<summary>摘要</summary>
文本摘要：图像基于虚拟试穿涉及将人像图像与衣服图像合成成一个自然地穿着的人像图像，这种技术有前所未有的在线购物和图像生成方面的潜在应用前景。然而，目前研究进步和商业应用之间存在很大的差距，而且该领域的总体评估缺乏，这阻碍了该领域的快速发展。在本调查中，我们提供了图像基于虚拟试穿领域的完整分析，包括管道架构、人物表示和关键模块 such as 试穿指示、衣服扭曲和试穿阶段。我们还提出了一种新的Semantic CLIP标准，并对代表方法进行了统一的评估。除了对当前开源方法的量化和质量评估外，我们还使用ControlNet来精度地调整最近一个大型图像生成模型（PBE），以示未来大规模模型在图像基于虚拟试穿任务中的潜在潜力。最后，我们揭示了未解的问题和未来研究方向，以便识别关键趋势并鼓励进一步的探索。我们在 GitHub 上将 uniformly 实现的评估指标、数据集和收集到的方法公开可用。
</details></li>
</ul>
<hr>
<h2 id="VioLA-Aligning-Videos-to-2D-LiDAR-Scans"><a href="#VioLA-Aligning-Videos-to-2D-LiDAR-Scans" class="headerlink" title="VioLA: Aligning Videos to 2D LiDAR Scans"></a>VioLA: Aligning Videos to 2D LiDAR Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04783">http://arxiv.org/abs/2311.04783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun-Jee Chao, Selim Engin, Nikhil Chavan-Dafle, Bhoram Lee, Volkan Isler</li>
<li>for: 本研究旨在将视频中捕捉的本地环境与2D LiDAR扫描图匹配。</li>
<li>methods: 我们提出了一种方法（VioLA），它首先从图像序列中构建了本地场景的semantic map，然后从图像序列中提取了一个固定高度的点，用于与LiDAR图匹配。由于恢复错误或相机扫描部分的覆盖，可能导致重构的semantic map中缺失足够的信息 для姿态注册。为解决这个问题，VioLA利用了一个预训练的文本到图像填充模型和深度完成模型，以在 geometrically 一致的方式填充缺失的场景内容，从而支持姿态注册。</li>
<li>results: 我们在两个真实的RGB-D标准测试集和一个自拍取的大办公室场景测试集上评估了VioLA。结果显示，我们的提出的场景填充模块可以提高姿态注册性能，最高提高20%。<details>
<summary>Abstract</summary>
We study the problem of aligning a video that captures a local portion of an environment to the 2D LiDAR scan of the entire environment. We introduce a method (VioLA) that starts with building a semantic map of the local scene from the image sequence, then extracts points at a fixed height for registering to the LiDAR map. Due to reconstruction errors or partial coverage of the camera scan, the reconstructed semantic map may not contain sufficient information for registration. To address this problem, VioLA makes use of a pre-trained text-to-image inpainting model paired with a depth completion model for filling in the missing scene content in a geometrically consistent fashion to support pose registration. We evaluate VioLA on two real-world RGB-D benchmarks, as well as a self-captured dataset of a large office scene. Notably, our proposed scene completion module improves the pose registration performance by up to 20%.
</details>
<details>
<summary>摘要</summary>
我们研究将视频捕捉当地环境与2D LiDAR扫描的全环境进行对应的问题。我们提出了一种方法（VioLA），它从图像序列中构建了本地场景的semantic地图，然后从图像序列中提取了一个固定高度的点来与LiDAR地图进行对应。由于恢复错误或摄像头扫描的部分覆盖，可能无法从重构的semantic地图中获取足够的信息进行注册。为解决这个问题，VioLA利用了预训练的文本到图像填充模型和深度完成模型，以在 geometrically 一致的方式填充缺失的场景内容，从而支持pose注册。我们在两个真实的RGB-D标准测试集和我们自己拍摄的大办公室场景测试集上评估了VioLA。可以看到，我们提出的场景完成模块可以提高pose注册性能，最高提高20%。
</details></li>
</ul>
<hr>
<h2 id="Lidar-Annotation-Is-All-You-Need"><a href="#Lidar-Annotation-Is-All-You-Need" class="headerlink" title="Lidar Annotation Is All You Need"></a>Lidar Annotation Is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04777">http://arxiv.org/abs/2311.04777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evocargo/lidar-annotation-is-all-you-need">https://github.com/evocargo/lidar-annotation-is-all-you-need</a></li>
<li>paper_authors: Dinar Sharafutdinov, Stanislav Kuskov, Saian Protasov, Alexey Voropaev</li>
<li>for: 提高图像分割的效率，用于自动驾驶系统中的道路表面分割任务。</li>
<li>methods: 使用卷积神经网络，利用激光探测器（Lidar）精度测量道路表面，并将其用作图像分割模型的地面标注。</li>
<li>results: 实验 validate the approach on 多个benchmark dataset，与高质量图像分割模型的性能相当。<details>
<summary>Abstract</summary>
In recent years, computer vision has transformed fields such as medical imaging, object recognition, and geospatial analytics. One of the fundamental tasks in computer vision is semantic image segmentation, which is vital for precise object delineation. Autonomous driving represents one of the key areas where computer vision algorithms are applied. The task of road surface segmentation is crucial in self-driving systems, but it requires a labor-intensive annotation process in several data domains. The work described in this paper aims to improve the efficiency of image segmentation using a convolutional neural network in a multi-sensor setup. This approach leverages lidar (Light Detection and Ranging) annotations to directly train image segmentation models on RGB images. Lidar supplements the images by emitting laser pulses and measuring reflections to provide depth information. However, lidar's sparse point clouds often create difficulties for accurate object segmentation. Segmentation of point clouds requires time-consuming preliminary data preparation and a large amount of computational resources. The key innovation of our approach is the masked loss, addressing sparse ground-truth masks from point clouds. By calculating loss exclusively where lidar points exist, the model learns road segmentation on images by using lidar points as ground truth. This approach allows for blending of different ground-truth data types during model training. Experimental validation of the approach on benchmark datasets shows comparable performance to a high-quality image segmentation model. Incorporating lidar reduces the load on annotations and enables training of image-segmentation models without loss of segmentation quality. The methodology is tested on diverse datasets, both publicly available and proprietary. The strengths and weaknesses of the proposed method are also discussed in the paper.
</details>
<details>
<summary>摘要</summary>
近年来，计算机视觉技术已经对医学影像、物体识别和地球分析等领域产生了深见的影响。计算机视觉中的一项基本任务是semantic image segmentation，它是精确物体定义的关键。自动驾驶是计算机视觉算法的一个关键应用领域，但是道路表面 segmentation 是自动驾驶系统中的关键任务，但是需要大量的注释过程。这篇文章描述的工作是使用多感器设置中的卷积神经网络进行图像分割的提高。这种方法利用激光探测器（Lidar）的注释来直接将图像分割模型训练到RGB图像上。Lidar 补充图像，通过发射激光脉冲并测量反射来提供深度信息。然而，Lidar 的稀疏点云经常对精确物体分割造成困难。图像分割需要耗时的先前数据准备和大量的计算资源。我们的方法的关键创新是使用Masked loss来 Addressing sparse ground truth masks from point clouds。通过在激光点云存在的地方进行损失计算，模型可以通过激光点云作为真实的ground truth来学习道路分割。这种方法允许在训练过程中混合不同的ground truth数据类型。实验 validate our approach on benchmark datasets shows comparable performance to a high-quality image segmentation model。将激光点云纳入训练过程可以减轻注释的负担，并使图像分割模型不会失去分割质量。我们的方法在多个dataset上进行了测试，包括公共 dataset 和专有 dataset。文章还讨论了我们的方法的优缺点。
</details></li>
</ul>
<hr>
<h2 id="GCS-ICHNet-Assessment-of-Intracerebral-Hemorrhage-Prognosis-using-Self-Attention-with-Domain-Knowledge-Integration"><a href="#GCS-ICHNet-Assessment-of-Intracerebral-Hemorrhage-Prognosis-using-Self-Attention-with-Domain-Knowledge-Integration" class="headerlink" title="GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using Self-Attention with Domain Knowledge Integration"></a>GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using Self-Attention with Domain Knowledge Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04772">http://arxiv.org/abs/2311.04772</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Windbelll/Prognosis-analysis-of-cerebral-hemorrhage">https://github.com/Windbelll/Prognosis-analysis-of-cerebral-hemorrhage</a></li>
<li>paper_authors: Xuhao Shan, Xinyang Li, Ruiquan Ge, Shibin Wu, Ahmed Elazab, Jichao Zhu, Lingyan Zhang, Gangyong Jia, Qingying Xiao, Xiang Wan, Changmiao Wang</li>
<li>for: 该论文旨在提高脑内出血（ICH）的预测和管理，提高患者的生存率。</li>
<li>methods: 该论文提出了一种新的深度学习算法——GCS-ICHNet，该算法通过将多Modal脑CT图像数据和格拉斯哥康复分数（GCS）值融合，以提高ICH的诊断精度。</li>
<li>results: 该论文的结果显示，GCS-ICHNet能够达到81.03%的敏感性和91.59%的特异性，超过了平均的临床医生和其他当前的方法。<details>
<summary>Abstract</summary>
Intracerebral Hemorrhage (ICH) is a severe condition resulting from damaged brain blood vessel ruptures, often leading to complications and fatalities. Timely and accurate prognosis and management are essential due to its high mortality rate. However, conventional methods heavily rely on subjective clinician expertise, which can lead to inaccurate diagnoses and delays in treatment. Artificial intelligence (AI) models have been explored to assist clinicians, but many prior studies focused on model modification without considering domain knowledge. This paper introduces a novel deep learning algorithm, GCS-ICHNet, which integrates multimodal brain CT image data and the Glasgow Coma Scale (GCS) score to improve ICH prognosis. The algorithm utilizes a transformer-based fusion module for assessment. GCS-ICHNet demonstrates high sensitivity 81.03% and specificity 91.59%, outperforming average clinicians and other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Intracerebral Hemorrhage (ICH) 是一种严重的疾病，由于脑血管损害而导致，经常会导致复杂性和致死率高。因此，时效精准的诊断和治疗是非常重要。然而，传统的方法强调专家式的诊断，可能会导致诊断不准确和治疗延迟。人工智能（AI）模型已经被探讨，但多数前 studies 强调模型修改而未考虑域知识。这篇论文介绍了一种新的深度学习算法，GCS-ICHNet，该算法利用多modal脑CT图像数据和格拉斯哥昏迷度评分（GCS）来提高ICH诊断。该算法使用转换器基于的融合模块进行评估。GCS-ICHNet 在敏感性和特异性方面均达到了81.03%和91.59%，高于平均的临床医生和其他当前状态的方法。
</details></li>
</ul>
<hr>
<h2 id="An-attention-based-deep-learning-network-for-predicting-Platinum-resistance-in-ovarian-cancer"><a href="#An-attention-based-deep-learning-network-for-predicting-Platinum-resistance-in-ovarian-cancer" class="headerlink" title="An attention-based deep learning network for predicting Platinum resistance in ovarian cancer"></a>An attention-based deep learning network for predicting Platinum resistance in ovarian cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04769">http://arxiv.org/abs/2311.04769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoming Zhuang, Beibei Li, Jingtong Ma, Patrice Monkam, Shouliang Qi, Wei Qian, Dianning He</li>
<li>For: This study aims to propose a deep learning-based method to determine whether a patient with high-grade serous ovarian cancer (HGSOC) is platinum-resistant using multimodal positron emission tomography&#x2F;computed tomography (PET&#x2F;CT) images.* Methods: The proposed method uses an end-to-end SE-SPP-DenseNet model, which combines Squeeze-Excitation Block (SE Block) and Spatial Pyramid Pooling Layer (SPPLayer) with Dense Convolutional Network (DenseNet) to analyze multimodal PET&#x2F;CT images of the regions of interest (ROI) and predict platinum resistance in patients.* Results: The study achieved a high accuracy rate and an area under the curve (AUC) of 0.93 in predicting platinum resistance in patients, indicating that the proposed deep learning framework can help gynecologists make better treatment decisions.<details>
<summary>Abstract</summary>
Background: Ovarian cancer is among the three most frequent gynecologic cancers globally. High-grade serous ovarian cancer (HGSOC) is the most common and aggressive histological type. Guided treatment for HGSOC typically involves platinum-based combination chemotherapy, necessitating an assessment of whether the patient is platinum-resistant. The purpose of this study is to propose a deep learning-based method to determine whether a patient is platinum-resistant using multimodal positron emission tomography/computed tomography (PET/CT) images. Methods: 289 patients with HGSOC were included in this study. An end-to-end SE-SPP-DenseNet model was built by adding Squeeze-Excitation Block (SE Block) and Spatial Pyramid Pooling Layer (SPPLayer) to Dense Convolutional Network (DenseNet). Multimodal data from PET/CT images of the regions of interest (ROI) were used to predict platinum resistance in patients. Results: Through five-fold cross-validation, SE-SPP-DenseNet achieved a high accuracy rate and an area under the curve (AUC) in predicting platinum resistance in patients, which were 92.6% and 0.93, respectively. The importance of incorporating SE Block and SPPLayer into the deep learning model, and considering multimodal data was substantiated by carrying out ablation studies and experiments with single modality data. Conclusions: The obtained classification results indicate that our proposed deep learning framework performs better in predicting platinum resistance in patients, which can help gynecologists make better treatment decisions. Keywords: PET/CT, CNN, SE Block, SPP Layer, Platinum resistance, Ovarian cancer
</details>
<details>
<summary>摘要</summary>
背景：子宫癌是全球三大妇科癌症之一，高度精度膜癌（HGSOC）是最常见且侵袭性最高的 histological 型。对 HGSOC 患者，通常采用 platinum-based 组合化学疗法，需要评估患者是否 platinum-resistant。本研究的目的是使用深度学习方法来判断 HGSOC 患者是否 platinum-resistant，并使用 multimodal PET/CT 图像进行预测。方法：本研究包括 289 名 HGSOC 患者。我们建立了一个 end-to-end SE-SPP-DenseNet 模型，其中包括 Squeeze-Excitation Block（SE Block）和 Spatial Pyramid Pooling Layer（SPPLayer）。我们使用 PET/CT 图像的多Modal 数据来预测患者是否 platinum-resistant。结果：通过五fold 十字验证，SE-SPP-DenseNet 模型在预测患者是否 platinum-resistant 中取得了高精度率和抑肿曲线（AUC），具体数据如下：精度率 92.6%，AUC 0.93。我们还进行了减少和单 modal 数据实验，以证明 incorporating SE Block 和 SPPLayer 到深度学习模型以及使用 multimodal 数据的重要性。结论：我们的深度学习框架在预测患者是否 platinum-resistant 中取得了更高的精度率，这可以帮助妇科医生更好地制定治疗方案。关键词：PET/CT, CNN, SE Block, SPP Layer, Platinum resistance, Ovarian cancer
</details></li>
</ul>
<hr>
<h2 id="DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation"><a href="#DualTalker-A-Cross-Modal-Dual-Learning-Approach-for-Speech-Driven-3D-Facial-Animation" class="headerlink" title="DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation"></a>DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04766">http://arxiv.org/abs/2311.04766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guinan Su, Yanwu Yang, Zhifeng Li</li>
<li>for: 增强数据使用效率和跨模态关系</li>
<li>methods: 跨模态双学习框架（DualTalker）和跨模态一致性损失</li>
<li>results: 在VOCA和BIWI数据集上，与当前状态艺术方法进行比较，得到了较好的性能表现，并通过用户测试得到了更高的评价。<details>
<summary>Abstract</summary>
In recent years, audio-driven 3D facial animation has gained significant attention, particularly in applications such as virtual reality, gaming, and video conferencing. However, accurately modeling the intricate and subtle dynamics of facial expressions remains a challenge. Most existing studies approach the facial animation task as a single regression problem, which often fail to capture the intrinsic inter-modal relationship between speech signals and 3D facial animation and overlook their inherent consistency. Moreover, due to the limited availability of 3D-audio-visual datasets, approaches learning with small-size samples have poor generalizability that decreases the performance. To address these issues, in this study, we propose a cross-modal dual-learning framework, termed DualTalker, aiming at improving data usage efficiency as well as relating cross-modal dependencies. The framework is trained jointly with the primary task (audio-driven facial animation) and its dual task (lip reading) and shares common audio/motion encoder components. Our joint training framework facilitates more efficient data usage by leveraging information from both tasks and explicitly capitalizing on the complementary relationship between facial motion and audio to improve performance. Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate the potential over-smoothing underlying the cross-modal complementary representations, enhancing the mapping of subtle facial expression dynamics. Through extensive experiments and a perceptual user study conducted on the VOCA and BIWI datasets, we demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. We have made our code and video demonstrations available at https://github.com/sabrina-su/iadf.git.
</details>
<details>
<summary>摘要</summary>
近年来，受音频驱动的3D面部动画技术受到了广泛关注，特别是在虚拟现实、游戏和视频会议等应用中。然而，准确地模型面部表达的细微和复杂动态仍然是一大挑战。大多数现有研究将面部动画任务视为单一回归问题，容易忽视面部表达和音频信号之间的内在关系，以及它们的共同一致性。此外，由于3D-audio-visual数据的有限可用性，使用小型样本学习的方法的性能差异很大，降低了性能。为解决这些问题，我们在本研究中提出了一种跨Modal dual学习框架，称之为DualTalker，旨在提高数据使用效率以及跨Modal相关性。该框架在同时进行主任务（音频驱动面部动画）和其双任务（ lip reading）的 JOINT 训练中，共享音频/动作编码器组件。我们的 JOINT 训练框架可以更好地利用两个任务之间的信息，并通过跨Modal的相关性来提高表达性能。此外，我们还引入了辅助的跨Modal一致性损失，以遏制可能出现的跨Modal相关表示过度简化，提高表达动态的映射。经过广泛的实验和基于 VOCA 和 BIWI 数据集的感知用户研究，我们表明，我们的方法在质量和量上都超过了当前领导方法，并提供了相关视频展示。我们在 GitHub 上提供了代码和视频展示，请参考 https://github.com/sabrina-su/iadf.git。
</details></li>
</ul>
<hr>
<h2 id="Social-Motion-Prediction-with-Cognitive-Hierarchies"><a href="#Social-Motion-Prediction-with-Cognitive-Hierarchies" class="headerlink" title="Social Motion Prediction with Cognitive Hierarchies"></a>Social Motion Prediction with Cognitive Hierarchies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04726">http://arxiv.org/abs/2311.04726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Walter0807/Social-CH">https://github.com/Walter0807/Social-CH</a></li>
<li>paper_authors: Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, Hai Ci, Yizhou Wang</li>
<li>for: 本研究旨在复制人类在预测他人行为方面的能力，通过解决社交动作预测问题。</li>
<li>methods: 我们引入了一个新的数据集Wusi，基于团队运动的3D多人动作数据集，并提出了一种新的形式化和认知层次框架。我们还使用了行为快照和生成对抗学习来提高学习效率和通用性。</li>
<li>results: 我们进行了广泛的实验 validate our proposed dataset and approach的效果。<details>
<summary>Abstract</summary>
Humans exhibit a remarkable capacity for anticipating the actions of others and planning their own actions accordingly. In this study, we strive to replicate this ability by addressing the social motion prediction problem. We introduce a new benchmark, a novel formulation, and a cognition-inspired framework. We present Wusi, a 3D multi-person motion dataset under the context of team sports, which features intense and strategic human interactions and diverse pose distributions. By reformulating the problem from a multi-agent reinforcement learning perspective, we incorporate behavioral cloning and generative adversarial imitation learning to boost learning efficiency and generalization. Furthermore, we take into account the cognitive aspects of the human social action planning process and develop a cognitive hierarchy framework to predict strategic human social interactions. We conduct comprehensive experiments to validate the effectiveness of our proposed dataset and approach. Code and data are available at https://walter0807.github.io/Social-CH/.
</details>
<details>
<summary>摘要</summary>
人类具有惊人的他人行为预测能力和相应的行动规划能力。在本研究中，我们努力复制这种能力，解决社交动作预测问题。我们介绍了一个新的标准 bencmark，一种新的形ulation，以及一个基于认知的框架。我们发布了一个3D多人动作数据集，名为Wusi，这个数据集在球类运动中展示了人类之间的激烈和策略性的互动，以及多种姿态分布。我们通过将问题转化为多智能激励学习的视角，并采用行为做模仿学习和生成敌对偶护学习，以提高学习效率和泛化能力。此外，我们考虑了人类社交行为规划过程中的认知方面，并开发了一个认知层次框架，以预测人类社交互动的策略性。我们进行了全面的实验 validate our proposed dataset and approach。代码和数据可以在https://walter0807.github.io/Social-CH/ 获取。
</details></li>
</ul>
<hr>
<h2 id="Training-CLIP-models-on-Data-from-Scientific-Papers"><a href="#Training-CLIP-models-on-Data-from-Scientific-Papers" class="headerlink" title="Training CLIP models on Data from Scientific Papers"></a>Training CLIP models on Data from Scientific Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04711">http://arxiv.org/abs/2311.04711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nopperl/clip_arxiv_pmc">https://github.com/nopperl/clip_arxiv_pmc</a></li>
<li>paper_authors: Calvin Metzger</li>
<li>for: 是否可以使用特定领域高质量数据来提高CLIP模型的总体性能？</li>
<li>methods: 使用arXiv和PubMed Central repositories中的科学论文文本图像数据进行采集和实验。</li>
<li>results: 模型性能平均提高，但只是moderate。这表明使用这些数据来训练大规模CLIP模型是一个值得研究的方向。<details>
<summary>Abstract</summary>
Contrastive Language-Image Pretraining (CLIP) models are able to capture the semantic relationship of images and texts and have enabled a wide range of applications, from image retrieval to classification. These models are trained with datasets extracted from web crawls, which are of large quantity but limited quality. This paper explores whether limited amounts higher quality data in a specific domain improve the general performance of CLIP models. To this purpose, we extract text-image data from scientific papers hosted in the arXiv and PubMed Central repositories. Experiments on small-scale CLIP models (ViT B/32) show that model performance increases on average, but only moderately. This result indicates that using the data sources considered in the paper to train large-scale CLIP models is a worthwile research direction.
</details>
<details>
<summary>摘要</summary>
CLIP模型能够捕捉图像和文本之间的含义关系，并且开发了许多应用程序，从图像检索到分类。这些模型通常通过网络抓取而训练，但这些数据量虽然庞大，但质量有限。这篇论文探讨了是否有限量但高质量数据在特定领域中提高CLIP模型的总性表现。为此，我们从arXiv和PubMed Central数据Repositories中提取了文本-图像数据。实验结果表明，使用这些数据来训练小规模CLIP模型（ViT B/32），模型的性能平均提高，但只是 Moderate。这个结果表明，使用论文中所考虑的数据来训练大规模CLIP模型是一个值得研究的方向。
</details></li>
</ul>
<hr>
<h2 id="3D-Pose-Estimation-of-Tomato-Peduncle-Nodes-using-Deep-Keypoint-Detection-and-Point-Cloud"><a href="#3D-Pose-Estimation-of-Tomato-Peduncle-Nodes-using-Deep-Keypoint-Detection-and-Point-Cloud" class="headerlink" title="3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud"></a>3D Pose Estimation of Tomato Peduncle Nodes using Deep Keypoint Detection and Point Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04699">http://arxiv.org/abs/2311.04699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianchao Ci, Xin Wang, David Rapado-Rincón, Akshay K. Burusa, Gert Kootstra</li>
<li>for: 这个论文的目的是解决发达国家的greenhouse生产中的劳动力短缺和高成本问题，通过使用机器人技术实现可持续可靠的生产。</li>
<li>methods: 这篇论文提出了一种基于关键点检测的方法，使用RGB-D摄像头数据进行 Tomatoes的3D姿态估计。该方法首先在色彩图像中检测四个解剖特征点，然后将这些点与3D点云信息集成，以确定 Tomatoes的3D姿态。</li>
<li>results: 该方法的测试结果显示：一、对象检测精度高，AP&#x3D;0.96；二、关键点检测精度高，<a href="mailto:&#x50;&#x44;&#x4a;&#x40;&#x30;&#x2e;&#x32;">&#x50;&#x44;&#x4a;&#x40;&#x30;&#x2e;&#x32;</a>&#x3D;94.31%；三、3D姿态估计精度低，MAE&#x3D;11.38o和9.93o。此外，该方法能够适应视点变化，但 canonical和高视点视角表现略高一些。该方法可以应用于其他greenhouse作物，如辣椒。<details>
<summary>Abstract</summary>
Greenhouse production of fruits and vegetables in developed countries is challenged by labor 12 scarcity and high labor costs. Robots offer a good solution for sustainable and cost-effective 13 production. Acquiring accurate spatial information about relevant plant parts is vital for 14 successful robot operation. Robot perception in greenhouses is challenging due to variations in 15 plant appearance, viewpoints, and illumination. This paper proposes a keypoint-detection-based 16 method using data from an RGB-D camera to estimate the 3D pose of peduncle nodes, which 17 provides essential information to harvest the tomato bunches. 18 19 Specifically, this paper proposes a method that detects four anatomical landmarks in the color 20 image and then integrates 3D point-cloud information to determine the 3D pose. A 21 comprehensive evaluation was conducted in a commercial greenhouse to gain insight into the 22 performance of different parts of the method. The results showed: (1) high accuracy in object 23 detection, achieving an Average Precision (AP) of AP@0.5=0.96; (2) an average Percentage of 24 Detected Joints (PDJ) of the keypoints of PhDJ@0.2=94.31%; and (3) 3D pose estimation 25 accuracy with mean absolute errors (MAE) of 11.38o and 9.93o for the relative upper and lower 26 angles between the peduncle and main stem, respectively. Furthermore, the capability to handle 27 variations in viewpoint was investigated, demonstrating the method was robust to view changes. 28 However, canonical and higher views resulted in slightly higher performance compared to other 29 views. Although tomato was selected as a use case, the proposed method is also applicable to 30 other greenhouse crops like pepper.
</details>
<details>
<summary>摘要</summary>
developed countries 的greenhouse production of fruits and vegetables 面临劳动力短缺和高成本问题。Robots 提供了一个可持续和经济的解决方案。获取有关重要植物部分的准确空间信息是成功Robot操作的关键。在greenhouse中Robot感知是因植物外观、视角和照明变化而具有挑战性。这篇文章提出了基于颜色图像中的关键点检测方法，使用RGB-D摄像头获取 Tomatoes 的3D姿态。具体来说，这篇文章将在颜色图像中检测四个 анатомиче特征点，然后将3D点云信息与颜色图像进行集成，以确定 Tomatoes 的3D姿态。进行了在商业greenhouse中广泛的评估，以获得不同方法部分的性能评估结果。结果显示：1. 对象检测精度高，AP@0.5=0.96;2. 平均关节检测精度（PDJ）为94.31%;3. 3D姿态估计精度（MAE）为11.38°和9.93°。此外，文章还 investigate了视角变化的影响，并证明方法具有视角变化的Robustness。although Tomato 是使用case，提出的方法还适用于其他greenhouse 作物如辣椒。
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-cross-model-learning-in-high-content-screening"><a href="#Weakly-supervised-cross-model-learning-in-high-content-screening" class="headerlink" title="Weakly supervised cross-model learning in high-content screening"></a>Weakly supervised cross-model learning in high-content screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04678">http://arxiv.org/abs/2311.04678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Watkinson Gabriel, Cohen Ethan, Bourriez Nicolas, Bendidi Ihab, Bollot Guillaume, Genovesio Auguste</li>
<li>for: 这个研究旨在搭桥不同类型的数据，以便在药物探索中更好地利用各种数据来源。</li>
<li>methods: 本研究提出了一个新的方法，使用CLIP建立跨模式表示，并使用弱监督和跨站重复来验证。</li>
<li>results: 比较知名的基eline，我们的提案方法可以学习更好的表示，帮助减少批次效应。此外，我们还提出了一种适合JUMP-CP dataset的预处理方法，从85Tb减少到7Tb，保留所有干扰和大部分资讯内容。<details>
<summary>Abstract</summary>
With the surge in available data from various modalities, there is a growing need to bridge the gap between different data types. In this work, we introduce a novel approach to learn cross-modal representations between image data and molecular representations for drug discovery. We propose EMM and IMM, two innovative loss functions built on top of CLIP that leverage weak supervision and cross sites replicates in High-Content Screening. Evaluating our model against known baseline on cross-modal retrieval, we show that our proposed approach allows to learn better representations and mitigate batch effect. In addition, we also present a preprocessing method for the JUMP-CP dataset that effectively reduce the required space from 85Tb to a mere usable 7Tb size, still retaining all perturbations and most of the information content.
</details>
<details>
<summary>摘要</summary>
随着不同数据模式的数据量的增加，需要桥渡不同数据类型之间的 gap。在这项工作中，我们介绍了一种新的方法，用于在图像数据和分子表示之间学习交叉模式表示。我们提出了两种创新的损失函数：EMM和IMM，它们基于CLIP且利用弱监督和跨站复制在高内容检测中。我们评估了我们的模型与已知基eline之间的交叉模式检索，并显示了我们的提议方法可以学习更好的表示，并减轻批处效应。此外，我们还提出了对JUMP-CP数据集的预处理方法，可以将85Tb的数据减少到7Tb的可用大小，保留所有干扰和大部分信息内容。
</details></li>
</ul>
<hr>
<h2 id="VET-Visual-Error-Tomography-for-Point-Cloud-Completion-and-High-Quality-Neural-Rendering"><a href="#VET-Visual-Error-Tomography-for-Point-Cloud-Completion-and-High-Quality-Neural-Rendering" class="headerlink" title="VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering"></a>VET: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04634">http://arxiv.org/abs/2311.04634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lfranke/vet">https://github.com/lfranke/vet</a></li>
<li>paper_authors: Linus Franke, Darius Rückert, Laura Fink, Matthias Innmann, Marc Stamminger</li>
<li>for: 提高点云 Synthesize 质量</li>
<li>methods: 使用点云 Rendering 和 Visual Error Tomography (VET) 技术</li>
<li>results: 可以提高点云 Synthesize 质量，fix 大规模坑洞和缺失细长结构，并且可以在实时帧率下进行 Rendering，舒适性得到了显著改善。<details>
<summary>Abstract</summary>
In the last few years, deep neural networks opened the doors for big advances in novel view synthesis. Many of these approaches are based on a (coarse) proxy geometry obtained by structure from motion algorithms. Small deficiencies in this proxy can be fixed by neural rendering, but larger holes or missing parts, as they commonly appear for thin structures or for glossy regions, still lead to distracting artifacts and temporal instability. In this paper, we present a novel neural-rendering-based approach to detect and fix such deficiencies. As a proxy, we use a point cloud, which allows us to easily remove outlier geometry and to fill in missing geometry without complicated topological operations. Keys to our approach are (i) a differentiable, blending point-based renderer that can blend out redundant points, as well as (ii) the concept of Visual Error Tomography (VET), which allows us to lift 2D error maps to identify 3D-regions lacking geometry and to spawn novel points accordingly. Furthermore, (iii) by adding points as nested environment maps, our approach allows us to generate high-quality renderings of the surroundings in the same pipeline. In our results, we show that our approach can improve the quality of a point cloud obtained by structure from motion and thus increase novel view synthesis quality significantly. In contrast to point growing techniques, the approach can also fix large-scale holes and missing thin structures effectively. Rendering quality outperforms state-of-the-art methods and temporal stability is significantly improved, while rendering is possible at real-time frame rates.
</details>
<details>
<summary>摘要</summary>
最近几年，深度神经网络开启了新视角合成的大进步。许多这些方法基于结构从运动算法获得的（粗糙）代理几何。小落差在这个代理上可以通过神经渲染修复，但更大的孔隙或缺失部分，如随着精细结构或光滑区域，仍会导致干扰 artifacts 和时间不稳定。在这篇论文中，我们提出了一种基于神经渲染的新方法，用于检测和修复这些落差。作为代理，我们使用点云，这使得我们可以轻松地移除点云中的异常 geometry 并填充缺失 geometry 而无需复杂的 topological 操作。我们方法的关键是：1. 可导的点云渲染器，可以将重复的点消除掉，以及2. 视觉错误板准（VET）的概念，允许我们将2D 错误地图提升到3D 空间中，以识别缺失 geometry 并生成新的点。3. 通过添加点作为嵌入环境图，我们的方法可以在同一个管道中生成高质量的周围 renderings。在我们的结果中，我们发现我们的方法可以提高由结构从运动算法获得的点云质量，从而提高新视角合成质量。与点生长技术相比，我们的方法可以更有效地修复大规模孔隙和缺失细腻结构。rendering 质量超出了状态机器人方法，同时实现了实时帧率。
</details></li>
</ul>
<hr>
<h2 id="General-Framework-to-Evaluate-Unlinkability-in-Biometric-Template-Protection-Systems"><a href="#General-Framework-to-Evaluate-Unlinkability-in-Biometric-Template-Protection-Systems" class="headerlink" title="General Framework to Evaluate Unlinkability in Biometric Template Protection Systems"></a>General Framework to Evaluate Unlinkability in Biometric Template Protection Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04633">http://arxiv.org/abs/2311.04633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Gomez-Barrero, Javier Galbally, Christian Rathgeb, Christoph Busch<br>for: 本研究旨在提供一种系统性的评估生物特征模板保护技术的不可逆性和分识性的框架。methods: 本研究使用了一种新的框架来评估生物特征模板保护技术的不可逆性和分识性。这种框架包括了四个州chart下的生物特征模板保护技术的评估：生物盐、Bloom filter、同构加密和块重新映射。results: 研究发现，使用了提议的框架来评估生物特征模板保护技术的不可逆性和分识性，可以帮助更好地了解这些技术的性能。特别是，对块重新映射技术的评估表明，提议的方法在其他现有的度量方法中具有优势。<details>
<summary>Abstract</summary>
The wide deployment of biometric recognition systems in the last two decades has raised privacy concerns regarding the storage and use of biometric data. As a consequence, the ISO/IEC 24745 international standard on biometric information protection has established two main requirements for protecting biometric templates: irreversibility and unlinkability. Numerous efforts have been directed to the development and analysis of irreversible templates. However, there is still no systematic quantitative manner to analyse the unlinkability of such templates. In this paper we address this shortcoming by proposing a new general framework for the evaluation of biometric templates' unlinkability. To illustrate the potential of the approach, it is applied to assess the unlinkability of four state-of-the-art techniques for biometric template protection: biometric salting, Bloom filters, Homomorphic Encryption and block re-mapping. For the last technique, the proposed framework is compared with other existing metrics to show its advantages.
</details>
<details>
<summary>摘要</summary>
在过去二十年中，生物认证系统的广泛部署已引起了隐私问题的存储和使用生物特征数据。因此，ISO/IEC 24745国际标准 для生物特征信息保护确立了两个主要要求：不可逆和不可关联。许多努力已经专注于不可逆模板的开发和分析。然而，目前还没有系统化的量化方法来分析不可关联模板。在这篇论文中，我们解决了这个缺陷，提出了一个新的通用框架来评估生物模板的不可关联性。为了证明方法的潜力，我们对四种现状顶尖技术进行了生物模板保护的评估：生物盐、Bloom滤波、同时加密和块重映射。对于最后一种技术，我们的框架与其他现有的指标进行了比较，以显示其优势。
</details></li>
</ul>
<hr>
<h2 id="Image-Patch-Matching-with-Graph-Based-Learning-in-Street-Scenes"><a href="#Image-Patch-Matching-with-Graph-Based-Learning-in-Street-Scenes" class="headerlink" title="Image Patch-Matching with Graph-Based Learning in Street Scenes"></a>Image Patch-Matching with Graph-Based Learning in Street Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04617">http://arxiv.org/abs/2311.04617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui She, Qiyu Kang, Sijie Wang, Wee Peng Tay, Yong Liang Guan, Diego Navarro Navarro, Andreas Hartmannsgruber</li>
<li>for: 本文主要用于提高自动驾驶计算机视觉任务中的匹配性能。</li>
<li>methods: 本文提出了一种基于图学习的 JOINT 特征和度量学习模型，利用图structure capture了图像区域之间的空间相互关系。</li>
<li>results: 对多个街景数据集进行评估，表明我们的方法可以达到当前最佳匹配效果。<details>
<summary>Abstract</summary>
Matching landmark patches from a real-time image captured by an on-vehicle camera with landmark patches in an image database plays an important role in various computer perception tasks for autonomous driving. Current methods focus on local matching for regions of interest and do not take into account spatial neighborhood relationships among the image patches, which typically correspond to objects in the environment. In this paper, we construct a spatial graph with the graph vertices corresponding to patches and edges capturing the spatial neighborhood information. We propose a joint feature and metric learning model with graph-based learning. We provide a theoretical basis for the graph-based loss by showing that the information distance between the distributions conditioned on matched and unmatched pairs is maximized under our framework. We evaluate our model using several street-scene datasets and demonstrate that our approach achieves state-of-the-art matching results.
</details>
<details>
<summary>摘要</summary>
Current methods focus on local matching for regions of interest and do not take into account spatial neighborhood relationships among the image patches, which typically correspond to objects in the environment. In this paper, we construct a spatial graph with the graph vertices corresponding to patches and edges capturing the spatial neighborhood information. We propose a joint feature and metric learning model with graph-based learning. We provide a theoretical basis for the graph-based loss by showing that the information distance between the distributions conditioned on matched and unmatched pairs is maximized under our framework. We evaluate our model using several street-scene datasets and demonstrate that our approach achieves state-of-the-art matching results.Here's the translation in Traditional Chinese:现有的方法仅专注于区域 interess 的本地匹配，而不考虑图像块之间的空间邻居关系，通常是环境中的物体。在这篇文章中，我们建立了一个空间图，其顶点对应图像块，并通过空间邻居信息来连接顶点。我们提出了一个共同特征和距离学习模型，并基于图形学习。我们提供了图形基础下的损失函数的理论基础，详细说明了在我们的框架下，匹配的信息距离between matched和unmatched pairs的最大化。我们使用了多个街景数据集进行评估，并证明了我们的方法可以实现顶尖匹配结果。
</details></li>
</ul>
<hr>
<h2 id="On-Characterizing-the-Evolution-of-Embedding-Space-of-Neural-Networks-using-Algebraic-Topology"><a href="#On-Characterizing-the-Evolution-of-Embedding-Space-of-Neural-Networks-using-Algebraic-Topology" class="headerlink" title="On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology"></a>On Characterizing the Evolution of Embedding Space of Neural Networks using Algebraic Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04592">http://arxiv.org/abs/2311.04592</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cross-caps/dnntopology">https://github.com/cross-caps/dnntopology</a></li>
<li>paper_authors: Suryaka Suresh, Bishshoy Das, Vinayak Abrol, Sumantra Dutta Roy</li>
<li>for: 研究深度神经网络（DNN）中特征表示空间的topology如何随层数变化，通过Betti数来量化。</li>
<li>methods: 使用Cubical homology对各种流行深度网络和实际图像数据进行扩展分析。</li>
<li>results: 随着depth层数的增加，复杂的特征表示空间变为简单的空间，Betti数达到最低值。这个过程中的复杂性衰减率可以衡量不同架构的泛化能力。此外，我们还发现了一些不变性，如（1）同类 dataset上不同架构的 topological invariance;（2） embedding space中的数据尺寸&#x2F;分辨率不变性;（3） embedding space到输入尺寸&#x2F;分辨率的不变性，以及（4）数据采样不变性。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
We study how the topology of feature embedding space changes as it passes through the layers of a well-trained deep neural network (DNN) through Betti numbers. Motivated by existing studies using simplicial complexes on shallow fully connected networks (FCN), we present an extended analysis using Cubical homology instead, with a variety of popular deep architectures and real image datasets. We demonstrate that as depth increases, a topologically complicated dataset is transformed into a simple one, resulting in Betti numbers attaining their lowest possible value. The rate of decay in topological complexity (as a metric) helps quantify the impact of architectural choices on the generalization ability. Interestingly from a representation learning perspective, we highlight several invariances such as topological invariance of (1) an architecture on similar datasets; (2) embedding space of a dataset for architectures of variable depth; (3) embedding space to input resolution/size, and (4) data sub-sampling. In order to further demonstrate the link between expressivity \& the generalization capability of a network, we consider the task of ranking pre-trained models for downstream classification task (transfer learning). Compared to existing approaches, the proposed metric has a better correlation to the actually achievable accuracy via fine-tuning the pre-trained model.
</details>
<details>
<summary>摘要</summary>
我们研究了深度神经网络（DNN）中层次结构的变化，通过瓶颈数学（Betti numbers）来量化。受激发于先前使用简单的全连接网络（FCN）的研究，我们在不同的深度和实际图像数据集上进行了扩展分析，并示出了随depth增加而导致数据空间的topologically complicated转变为简单的现象。这种转变导致Betti numbers achieves its lowest possible value，这也是一个度量化深度神经网络的泛化能力的重要指标。从表示学学习的角度来看，我们发现了一些对称性，包括：1. 不同dataset上的同一个架构具有类似的topological invariance。2. 具有不同深度的架构的 embedding space 在不同dataset上具有相同的topological invariance。3. embedding space 对于不同的input resolution/size具有 topological invariance。4. 数据采样的对称性。为了进一步证明深度神经网络的表达能力和泛化能力之间的关系，我们考虑了将预训练模型排名为下游分类任务（转移学习）。与现有方法相比，我们的指标具有更好的与实际可 achievable accuracy 的相关性。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Event-based-Human-Pose-Estimation-with-3D-Event-Representations"><a href="#Rethinking-Event-based-Human-Pose-Estimation-with-3D-Event-Representations" class="headerlink" title="Rethinking Event-based Human Pose Estimation with 3D Event Representations"></a>Rethinking Event-based Human Pose Estimation with 3D Event Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04591">http://arxiv.org/abs/2311.04591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masterhow/eventpointpose">https://github.com/masterhow/eventpointpose</a></li>
<li>paper_authors: Xiaoting Yin, Hao Shi, Jiaan Chen, Ze Wang, Yaozu Ye, Huajian Ni, Kailun Yang, Kaiwei Wang</li>
<li>for: 本研究旨在提高自动驾驶和停车安全性，通过人体动作预测。</li>
<li>methods: 该研究使用事件摄像头，并开发了两种3D事件表示方法：Rasterized Event Point Cloud (RasEPC) 和 Decoupled Event Voxel (DEV)。</li>
<li>results: 研究发现，使用事件点云方法可以在实时移动预测中取得优秀表现，而使用分离事件矩阵方法可以获得最高准确率。实验表明，提posed的3D表示方法在对传统RGB图像和事件帧技术进行比较时具有更高的总体化能力。<details>
<summary>Abstract</summary>
Human pose estimation is a critical component in autonomous driving and parking, enhancing safety by predicting human actions. Traditional frame-based cameras and videos are commonly applied, yet, they become less reliable in scenarios under high dynamic range or heavy motion blur. In contrast, event cameras offer a robust solution for navigating these challenging contexts. Predominant methodologies incorporate event cameras into learning frameworks by accumulating events into event frames. However, such methods tend to marginalize the intrinsic asynchronous and high temporal resolution characteristics of events. This disregard leads to a loss in essential temporal dimension data, crucial for safety-critical tasks associated with dynamic human activities. To address this issue and to unlock the 3D potential of event information, we introduce two 3D event representations: the Rasterized Event Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). The RasEPC collates events within concise temporal slices at identical positions, preserving 3D attributes with statistical cues and markedly mitigating memory and computational demands. Meanwhile, the DEV representation discretizes events into voxels and projects them across three orthogonal planes, utilizing decoupled event attention to retrieve 3D cues from the 2D planes. Furthermore, we develop and release EV-3DPW, a synthetic event-based dataset crafted to facilitate training and quantitative analysis in outdoor scenes. On the public real-world DHP19 dataset, our event point cloud technique excels in real-time mobile predictions, while the decoupled event voxel method achieves the highest accuracy. Experiments reveal our proposed 3D representation methods' superior generalization capacities against traditional RGB images and event frame techniques. Our code and dataset are available at https://github.com/MasterHow/EventPointPose.
</details>
<details>
<summary>摘要</summary>
人体姿态估计是自动驾驶和停车中的关键组件，提高安全性 by 预测人类动作。传统的帧基camera和视频通常应用，但在高动态范围或重重运动抑制下变得不可靠。相比之下，事件摄像头提供了一种可靠的解决方案。大多数方法将事件摄像头集成到学习框架中，但这些方法通常忽略事件的本质异步和高时间分辨率特性，导致数据损失。为解决这个问题，我们介绍了两种3D事件表示方法：分割事件点云（RasEPC）和解决事件立方体（DEV）。RasEPC将事件集中在同一个时间位置内，保留3D特征参数，并remarkably减少内存和计算负担。DEV表示方法将事件分解为立方体，并在三个正交平面上 проек，使用解决事件注意力来检索3D准确性。此外，我们开发了EV-3DPWSynthetic事件基于数据集，用于训练和量化分析。在公共实际世界DHP19数据集上，我们的事件点云技术在实时移动预测中表现出色，而DEV表示方法在准确性方面达到了最高水平。实验表明我们的提posed 3D表示方法在传统RGB图像和事件帧技术上有superior的总体化能力。我们的代码和数据集可以在https://github.com/MasterHow/EventPointPose上获取。
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-deepfake-localization-in-diffusion-generated-images"><a href="#Weakly-supervised-deepfake-localization-in-diffusion-generated-images" class="headerlink" title="Weakly-supervised deepfake localization in diffusion-generated images"></a>Weakly-supervised deepfake localization in diffusion-generated images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04584">http://arxiv.org/abs/2311.04584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dragos Tantaru, Elisabeta Oneata, Dan Oneata<br>for: 这个论文的目的是提出一种弱监督的图像假象检测方法，以提供更加具有信息的输出，包括图像假象的地方化映射。methods: 这个论文使用的方法包括基于解释、本地分数和注意力的三类方法，并且使用Xception网络作为共同背景 arquitectura。results: 研究结果显示，弱监督的图像假象检测是可能的，并且使用本地分数的方法比其他两类方法更为敏感于监督质量，而不是 dataset 或 generator 的不同。<details>
<summary>Abstract</summary>
The remarkable generative capabilities of denoising diffusion models have raised new concerns regarding the authenticity of the images we see every day on the Internet. However, the vast majority of existing deepfake detection models are tested against previous generative approaches (e.g. GAN) and usually provide only a "fake" or "real" label per image. We believe a more informative output would be to augment the per-image label with a localization map indicating which regions of the input have been manipulated. To this end, we frame this task as a weakly-supervised localization problem and identify three main categories of methods (based on either explanations, local scores or attention), which we compare on an equal footing by using the Xception network as the common backbone architecture. We provide a careful analysis of all the main factors that parameterize the design space: choice of method, type of supervision, dataset and generator used in the creation of manipulated images; our study is enabled by constructing datasets in which only one of the components is varied. Our results show that weakly-supervised localization is attainable, with the best performing detection method (based on local scores) being less sensitive to the looser supervision than to the mismatch in terms of dataset or generator.
</details>
<details>
<summary>摘要</summary>
“denoising扩散模型的杰出生成能力已经引起了网络上每天见到的图像真实性的新忧虑。然而，现有的深圳检测模型大多是根据先前的生成方法（如GAN）进行测试，通常只会提供每个图像的“伪”或“真”标签。我们认为，更有用的输出将是附加每个图像的地方几何映射，以指示哪些输入区域被修改。为了实现这一目标，我们将这个任务描述为一个弱监督的地方化推导问题，并分别找出三种方法（基于解释、本地分数或注意力），并使用Xception网络作为共同背景架构。我们对所有主要设计空间的因素进行了谨慎的分析：方法选择、监督方式、数据集和生成器。我们的研究受惠于将单一的元素变化为多个数据集。我们的结果显示，弱监督的地方化推导是可能的，并且基于本地分数的检测方法比较不敏感于监督质量的变化，也比较不敏感于数据集或生成器的差异。”
</details></li>
</ul>
<hr>
<h2 id="A-3D-generative-model-of-pathological-multi-modal-MR-images-and-segmentations"><a href="#A-3D-generative-model-of-pathological-multi-modal-MR-images-and-segmentations" class="headerlink" title="A 3D generative model of pathological multi-modal MR images and segmentations"></a>A 3D generative model of pathological multi-modal MR images and segmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04552">http://arxiv.org/abs/2311.04552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/virginiafdez/brainspade3d_rel">https://github.com/virginiafdez/brainspade3d_rel</a></li>
<li>paper_authors: Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Mark S. Graham, Tom Vercauteren, M. Jorge Cardoso</li>
<li>for: 本研究旨在提供一种基于生成器的3D磁共振成像（MRI）和相关的分割数据，用于训练具有高特异性的深度学习模型，并可以根据具体的疾病现象和对比进行 conditioning。</li>
<li>methods: 本研究使用的是生成对抗网络（GANs）和扩散模型（DMs），并实现了对核磁共振成像（MRI）和相关的分割数据的生成。用户可以通过Specifying pathological phenotypes and contrasts来控制生成的图像和分割数据的内容和对比。</li>
<li>results: 本研究的实验结果表明，brainSPADE3D可以生成高品质的Synthetic MRI和相关的分割数据，并且可以同时捕捉多种疾病现象。此外，brainSPADE3D还可以提高分割模型的性能，并且可以适应不计划的疾病现象。<details>
<summary>Abstract</summary>
Generative modelling and synthetic data can be a surrogate for real medical imaging datasets, whose scarcity and difficulty to share can be a nuisance when delivering accurate deep learning models for healthcare applications. In recent years, there has been an increased interest in using these models for data augmentation and synthetic data sharing, using architectures such as generative adversarial networks (GANs) or diffusion models (DMs). Nonetheless, the application of synthetic data to tasks such as 3D magnetic resonance imaging (MRI) segmentation remains limited due to the lack of labels associated with the generated images. Moreover, many of the proposed generative MRI models lack the ability to generate arbitrary modalities due to the absence of explicit contrast conditioning. These limitations prevent the user from adjusting the contrast and content of the images and obtaining more generalisable data for training task-specific models. In this work, we propose brainSPADE3D, a 3D generative model for brain MRI and associated segmentations, where the user can condition on specific pathological phenotypes and contrasts. The proposed joint imaging-segmentation generative model is shown to generate high-fidelity synthetic images and associated segmentations, with the ability to combine pathologies. We demonstrate how the model can alleviate issues with segmentation model performance when unexpected pathologies are present in the data.
</details>
<details>
<summary>摘要</summary>
生成模型和 sintetic 数据可以作为实际医疗影像数据的代理，解决医疗应用中深度学习模型的准确性问题。过去几年，有越来越多的人们关注使用这些模型进行数据增强和 sintetic 数据共享，使用生成对抗网络（GANs）或扩散模型（DMs）等建筑。然而，在应用 sintetic 数据进行3D磁共振成像（MRI）分割任务时，还是有一些限制，主要是生成图像无标签，以及生成图像不能按照用户需求进行调整。此外，许多已经提出的生成MRI模型无法生成多个模式，因为缺少显式对比条件。这些限制使得用户无法根据任务需求进行图像的调整和对比，从而获得更加普适的数据。在这项工作中，我们提出了brainSPADE3D，一种3D生成模型，用于脑MRI和相关的分割。用户可以根据特定的疾病现象和对比条件来conditioning。我们的结合成像和分割生成模型能够生成高质量的 sintetic 图像和相关的分割，并且可以组合疾病。我们示示了该模型如何解决预期疾病存在于数据时，影响分割模型性能的问题。
</details></li>
</ul>
<hr>
<h2 id="Learning-Robust-Multi-Scale-Representation-for-Neural-Radiance-Fields-from-Unposed-Images"><a href="#Learning-Robust-Multi-Scale-Representation-for-Neural-Radiance-Fields-from-Unposed-Images" class="headerlink" title="Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images"></a>Learning Robust Multi-Scale Representation for Neural Radiance Fields from Unposed Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04521">http://arxiv.org/abs/2311.04521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nishant Jain, Suryansh Kumar, Luc Van Gool<br>for: 这个论文旨在解决计算机视觉中的神经图像基于渲染问题，即从一个自由移动的摄像头 captured 的图像集中生成一个真实的场景图像，并且在测试时使用。methods: 该论文使用了以下方法：1. 使用 robust pipeline  recuperate 摄像头参数，以确保在神经图像基于渲染问题中准确地synthesize 场景图像。2. 使用多尺度神经场景表示，以处理对象内容的不同分辨率。3. 使用单个图像深度预测，以估计图像中对象的深度。results: 该论文的结果表明，在不考虑 Camera 参数估计的情况下，模型对象内容的多尺度神经场景表示可能会导致模型缺失精度。而通过在Scene 理论基础之上引入 Camera 参数估计，可以提高模型的精度。例如，通过使用多个视图图像，可以在测试时Synthesize 一个真实的场景图像。<details>
<summary>Abstract</summary>
We introduce an improved solution to the neural image-based rendering problem in computer vision. Given a set of images taken from a freely moving camera at train time, the proposed approach could synthesize a realistic image of the scene from a novel viewpoint at test time. The key ideas presented in this paper are (i) Recovering accurate camera parameters via a robust pipeline from unposed day-to-day images is equally crucial in neural novel view synthesis problem; (ii) It is rather more practical to model object's content at different resolutions since dramatic camera motion is highly likely in day-to-day unposed images. To incorporate the key ideas, we leverage the fundamentals of scene rigidity, multi-scale neural scene representation, and single-image depth prediction. Concretely, the proposed approach makes the camera parameters as learnable in a neural fields-based modeling framework. By assuming per view depth prediction is given up to scale, we constrain the relative pose between successive frames. From the relative poses, absolute camera pose estimation is modeled via a graph-neural network-based multiple motion averaging within the multi-scale neural-fields network, leading to a single loss function. Optimizing the introduced loss function provides camera intrinsic, extrinsic, and image rendering from unposed images. We demonstrate, with examples, that for a unified framework to accurately model multiscale neural scene representation from day-to-day acquired unposed multi-view images, it is equally essential to have precise camera-pose estimates within the scene representation framework. Without considering robustness measures in the camera pose estimation pipeline, modeling for multi-scale aliasing artifacts can be counterproductive. We present extensive experiments on several benchmark datasets to demonstrate the suitability of our approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种改进的解决方案 для计算机视觉中的神经图像基于渲染问题。给定一组由自由移动相机在训练时拍摄的图像，我们的方法可以在测试时生成出真实的图像场景从一个新的视点。我们的关键想法是：（i）在不带pose的日常图像中恢复高精度相机参数，是神经novel view sintesis问题的关键因素。（ii）因为日常不带pose图像中的相机运动可能很剧烈，因此更加实用地模型对象的内容在不同分辨率上。为了激活这两个想法，我们利用场景刚性、多尺度神经场景表示和单个图像深度预测的基础知识。具体来说，我们将相机参数作为神经场景中的学习参数。通过假设每个视图的深度预测在某种程度上是固定的，我们使用图像之间的相对pose进行多个运动平均，从而模型维度的相机pose估计。通过这种方式，我们可以从日常不带pose图像中恢复相机参数，并同时实现图像渲染。我们通过多个实验证明，在不忽视 robustness measure的情况下，模型多尺度神经场景表示是不可避免的。我们的方法可以在多个标准 datasets上进行广泛的实验，以证明我们的方法的适用性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Discriminative-Features-for-Crowd-Counting"><a href="#Learning-Discriminative-Features-for-Crowd-Counting" class="headerlink" title="Learning Discriminative Features for Crowd Counting"></a>Learning Discriminative Features for Crowd Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04509">http://arxiv.org/abs/2311.04509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuehai Chen</li>
<li>for: 提高人群计数模型在高度拥挤的区域中的准确性</li>
<li>methods: 提出了一种学习权重特征框架，包括遮盖特征预测模块（MPM）和监督像素级对比学习模块（CLM）</li>
<li>results: 实现了提高人群计数模型的准确性，并可以应用于其他计算机视觉任务，如人群计数和物体检测，在高度拥挤的环境下提高了模型的性能<details>
<summary>Abstract</summary>
Crowd counting models in highly congested areas confront two main challenges: weak localization ability and difficulty in differentiating between foreground and background, leading to inaccurate estimations. The reason is that objects in highly congested areas are normally small and high-level features extracted by convolutional neural networks are less discriminative to represent small objects. To address these problems, we propose a learning discriminative features framework for crowd counting, which is composed of a masked feature prediction module (MPM) and a supervised pixel-level contrastive learning module (CLM). The MPM randomly masks feature vectors in the feature map and then reconstructs them, allowing the model to learn about what is present in the masked regions and improving the model's ability to localize objects in high-density regions. The CLM pulls targets close to each other and pushes them far away from background in the feature space, enabling the model to discriminate foreground objects from background. Additionally, the proposed modules can be beneficial in various computer vision tasks, such as crowd counting and object detection, where dense scenes or cluttered environments pose challenges to accurate localization. The proposed two modules are plug-and-play, incorporating the proposed modules into existing models can potentially boost their performance in these scenarios.
</details>
<details>
<summary>摘要</summary>
人群计数模型在高度拥堵的区域面临两大挑战：一是局部化能力弱和对背景和前景分化不具有分辨力，导致估计不准确。这是因为在高度拥堵的区域中，物体通常是小型，高级特征提取网络的特征更难以表示小型物体。为解决这些问题，我们提出了一种学习特征分布框架 для人群计数，该框架包括偏挥特征预测模块（MPM）和监督像素级对比学习模块（CLM）。MPM模块随机屏蔽特征向量在特征地图中，然后重建它们，使模型能够学习面积内的物体存在的地方，提高模型的局部化能力。CLM模块将目标紧挨着并将它们与背景分离开来，使模型能够在特征空间中分辨前景和背景。此外，我们提出的两个模块可以在各种计算机视觉任务中有助于提高精度，如人群计数和物体检测，在拥堵的环境下面临精度下降的挑战。我们的两个模块可以与现有模型集成，可能提高其性能在这些场景中。
</details></li>
</ul>
<hr>
<h2 id="NITEC-Versatile-Hand-Annotated-Eye-Contact-Dataset-for-Ego-Vision-Interaction"><a href="#NITEC-Versatile-Hand-Annotated-Eye-Contact-Dataset-for-Ego-Vision-Interaction" class="headerlink" title="NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction"></a>NITEC: Versatile Hand-Annotated Eye Contact Dataset for Ego-Vision Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04505">http://arxiv.org/abs/2311.04505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thorsten Hempel, Magnus Jung, Ahmed A. Abdelrahman, Ayoub Al-Hamadi</li>
<li>for: 本研究的目的是提供一个大型、多样化的人egoscopic eye contact dataset，以便进一步推进人机交互、计算机视觉和社交机器人等领域的研究。</li>
<li>methods: 本研究使用了人工精心标注的方法，将许多不同的人类表情、社会情境和照明条件 integrate into dataset，以满足不同场景下的研究需求。</li>
<li>results: 研究者通过广泛的评估表明，NITEC dataset在多种场景下具有强大的横跨数据集性能，表明其适用性和可重用性在计算机视觉、人机交互和社交机器人等领域。<details>
<summary>Abstract</summary>
Eye contact is a crucial non-verbal interaction modality and plays an important role in our everyday social life. While humans are very sensitive to eye contact, the capabilities of machines to capture a person's gaze are still mediocre. We tackle this challenge and present NITEC, a hand-annotated eye contact dataset for ego-vision interaction. NITEC exceeds existing datasets for ego-vision eye contact in size and variety of demographics, social contexts, and lighting conditions, making it a valuable resource for advancing ego-vision-based eye contact research. Our extensive evaluations on NITEC demonstrate strong cross-dataset performance, emphasizing its effectiveness and adaptability in various scenarios, that allows seamless utilization to the fields of computer vision, human-computer interaction, and social robotics. We make our NITEC dataset publicly available to foster reproducibility and further exploration in the field of ego-vision interaction. https://github.com/thohemp/nitec
</details>
<details>
<summary>摘要</summary>
眼接触是非言语交互模式中的重要一环，在我们每天社交生活中扮演着重要的角色。然而，机器人的眼接触捕捉能力仍然较差。我们解决这个挑战，并提供了NITEC数据集，这是一个手动注释的眼接触数据集，用于EGO视觉交互。NITEC的数据集比现有的EGO视觉眼接触数据集更大，包括更多的人种、社会背景、照明条件等，这使得它成为了EGO视觉交互领域的价值质量资源。我们对NITEC数据集进行了广泛的评估，并证明了其适用性和可变性在各种场景中，这使得它可以轻松地应用于计算机视觉、人机交互和社交机器人等领域。我们将NITEC数据集公开发布，以便促进复现性和进一步的探索在EGO视觉交互领域。更多信息请访问<https://github.com/thohemp/nitec>。
</details></li>
</ul>
<hr>
<h2 id="PRED-Pre-training-via-Semantic-Rendering-on-LiDAR-Point-Clouds"><a href="#PRED-Pre-training-via-Semantic-Rendering-on-LiDAR-Point-Clouds" class="headerlink" title="PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds"></a>PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04501">http://arxiv.org/abs/2311.04501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Yang, Haiyang Wang, Di Dai, Liwei Wang</li>
<li>for: This paper is written for improving point cloud pre-training in outdoor scenarios, addressing the issue of incompleteness by incorporating images and leveraging semantic information.</li>
<li>methods: The proposed method, PRED, uses a Birds-Eye-View (BEV) feature map conditioned semantic rendering to supervise the pre-training process, and incorporates point-wise masking with a high mask ratio (95%) to enhance performance.</li>
<li>results: The paper demonstrates significant improvements over prior point cloud pre-training methods on various large-scale datasets for 3D perception tasks, providing a superior framework for outdoor point cloud pre-training.<details>
<summary>Abstract</summary>
Pre-training is crucial in 3D-related fields such as autonomous driving where point cloud annotation is costly and challenging. Many recent studies on point cloud pre-training, however, have overlooked the issue of incompleteness, where only a fraction of the points are captured by LiDAR, leading to ambiguity during the training phase. On the other hand, images offer more comprehensive information and richer semantics that can bolster point cloud encoders in addressing the incompleteness issue inherent in point clouds. Yet, incorporating images into point cloud pre-training presents its own challenges due to occlusions, potentially causing misalignments between points and pixels. In this work, we propose PRED, a novel image-assisted pre-training framework for outdoor point clouds in an occlusion-aware manner. The main ingredient of our framework is a Birds-Eye-View (BEV) feature map conditioned semantic rendering, leveraging the semantics of images for supervision through neural rendering. We further enhance our model's performance by incorporating point-wise masking with a high mask ratio (95%). Extensive experiments demonstrate PRED's superiority over prior point cloud pre-training methods, providing significant improvements on various large-scale datasets for 3D perception tasks. Codes will be available at https://github.com/PRED4pc/PRED.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>在3D相关领域中，如自动驾驶，点云注解是昂贵而困难的。许多最近的点云预训练研究，然而，忽略了 incomplete 问题，只有一部分点云被 LiDAR 捕获，导致训练阶段的混淆。然而，图像具有更全面的信息和更丰富的 semantics，可以增强点云编码器对 incomplete 问题的应对。然而，将图像 incorporated 到点云预训练中又存在遮挡问题，可能导致点云和像素之间的misalignment。在这个工作中，我们提出了一个novel的图像助け预训练框架，称为 PRED。我们的框架的主要组成部分是 Birds-Eye-View（BEV）特征地图 conditioned semantic rendering，通过图像 semantics 来对点云进行超参。此外，我们还通过点 wise 掩码（mask ratio 95%）进一步提高我们的模型性能。广泛的实验证明，PRED 在多个大规模数据集上对 3D 识别任务具有显著的改进。代码将在 GitHub 上提供。
</details></li>
</ul>
<hr>
<h2 id="PersonMAE-Person-Re-Identification-Pre-Training-with-Masked-AutoEncoders"><a href="#PersonMAE-Person-Re-Identification-Pre-Training-with-Masked-AutoEncoders" class="headerlink" title="PersonMAE: Person Re-Identification Pre-Training with Masked AutoEncoders"></a>PersonMAE: Person Re-Identification Pre-Training with Masked AutoEncoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04496">http://arxiv.org/abs/2311.04496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hezhen Hu, Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Lu Yuan, Dong Chen, Houqiang Li</li>
<li>for: 这篇论文主要目标是提高人识别任务中的特征表示，具体来说是实现人识别中的多层意识、遮挡Robustness和跨区域一致性。</li>
<li>methods: 这篇论文提出了一种简单 yet effective的预训练框架，即PersonMAE，它包括两个核心设计：1) 将图像分成两个区域， RegionA 和 RegionB，其中 RegionA 会被块级干扰，以模拟人识别中的常见遮挡；2) 使用掩码自适应网络来预测 RegionB 的整个像素水平和 semantic feature 水平。</li>
<li>results: 这篇论文在四个下游人识别任务中达到了状态机器人的性能，包括supervised（整体和 occluded 设置）和无监督（UDA 和 USL 设置）。特别是在通常采用的supervised设置下，PersonMAE 与 ViT-B 底层结构相结合，在 MSMT17 和 OccDuke 数据集上 achiev 79.8% 和 69.5% mAP，比前一个状态机器人提高了大幅度的 +8.0 mAP 和 +5.3 mAP， соответivamente。<details>
<summary>Abstract</summary>
Pre-training is playing an increasingly important role in learning generic feature representation for Person Re-identification (ReID). We argue that a high-quality ReID representation should have three properties, namely, multi-level awareness, occlusion robustness, and cross-region invariance. To this end, we propose a simple yet effective pre-training framework, namely PersonMAE, which involves two core designs into masked autoencoders to better serve the task of Person Re-ID. 1) PersonMAE generates two regions from the given image with RegionA as the input and \textit{RegionB} as the prediction target. RegionA is corrupted with block-wise masking to mimic common occlusion in ReID and its remaining visible parts are fed into the encoder. 2) Then PersonMAE aims to predict the whole RegionB at both pixel level and semantic feature level. It encourages its pre-trained feature representations with the three properties mentioned above. These properties make PersonMAE compatible with downstream Person ReID tasks, leading to state-of-the-art performance on four downstream ReID tasks, i.e., supervised (holistic and occluded setting), and unsupervised (UDA and USL setting). Notably, on the commonly adopted supervised setting, PersonMAE with ViT-B backbone achieves 79.8% and 69.5% mAP on the MSMT17 and OccDuke datasets, surpassing the previous state-of-the-art by a large margin of +8.0 mAP, and +5.3 mAP, respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>预训练在人识别（ReID）中的重要性不断增长。我们认为高质量的ReID表示应具备三种属性，即多级意识、遮挡Robustness和跨地域一致性。为此，我们提出了一个简单 yet 高效的预训练框架，即PersonMAE，其包括两个核心设计：1. PersonMAE从给定的图像中生成两个区域， RegionA 作为输入， RegionB 作为预测目标。RegionA 被块状遮盖，以模拟 ReID 中常见的遮挡，其可见部分被编码器接受。2. PersonMAE 目标是在像素级和semantic feature级别预测 RegionB。它鼓励预训练的特征表示具备三种属性，使其与下游 Person ReID 任务兼容，从而达到了状态之最的性能。这些属性使得PersonMAE 与下游任务一起进行训练，以达到最佳的表达。在四个下游 ReID 任务上（包括超级vised setting，holistic setting和occluded setting），PersonMAE 均达到了状态之最的性能。特别是在通常采用的超级vised setting上，PersonMAE 与 ViT-B 基础结构组合得到了79.8%和69.5%的mAP值，大幅超越了之前的状态之最，升高了+8.0 mAP和+5.3 mAP，分别。>>>
</details></li>
</ul>
<hr>
<h2 id="Non-Rigid-Shape-Registration-via-Deep-Functional-Maps-Prior"><a href="#Non-Rigid-Shape-Registration-via-Deep-Functional-Maps-Prior" class="headerlink" title="Non-Rigid Shape Registration via Deep Functional Maps Prior"></a>Non-Rigid Shape Registration via Deep Functional Maps Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04494">http://arxiv.org/abs/2311.04494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rqhuang88/DFR">https://github.com/rqhuang88/DFR</a></li>
<li>paper_authors: Puhua Jiang, Mingze Sun, Ruqi Huang</li>
<li>for: 本研究提出了一种基于学习的非静止形态注册方法，无需协调监督。</li>
<li>methods: 本方法使用深度函数图（DFM）来学习高维空间中的非线性映射，并使用这些映射来引导源网格受力向目标点云进行变换。此外，该方法还使用了一种动态更新和约束过滤的方法来提高注册稳定性。</li>
<li>results: 实验结果表明，使用只需几十个有限变化的训练形状，该方法可以达到现有最佳效果在非静止点云匹配中，同时也可以处理一些具有大量外部和内部变换的挑战性形状对。<details>
<summary>Abstract</summary>
In this paper, we propose a learning-based framework for non-rigid shape registration without correspondence supervision. Traditional shape registration techniques typically rely on correspondences induced by extrinsic proximity, therefore can fail in the presence of large intrinsic deformations. Spectral mapping methods overcome this challenge by embedding shapes into, geometric or learned, high-dimensional spaces, where shapes are easier to align. However, due to the dependency on abstract, non-linear embedding schemes, the latter can be vulnerable with respect to perturbed or alien input. In light of this, our framework takes the best of both worlds. Namely, we deform source mesh towards the target point cloud, guided by correspondences induced by high-dimensional embeddings learned from deep functional maps (DFM). In particular, the correspondences are dynamically updated according to the intermediate registrations and filtered by consistency prior, which prominently robustify the overall pipeline. Moreover, in order to alleviate the requirement of extrinsically aligned input, we train an orientation regressor on a set of aligned synthetic shapes independent of the training shapes for DFM. Empirical results show that, with as few as dozens of training shapes of limited variability, our pipeline achieves state-of-the-art results on several benchmarks of non-rigid point cloud matching, but also delivers high-quality correspondences between unseen challenging shape pairs that undergo both significant extrinsic and intrinsic deformations, in which case neither traditional registration methods nor intrinsic methods work. The code is available at https://github.com/rqhuang88/DFR.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种学习基于的非固定形态注册框架，无需协调监督。传统的形态注册技术通常会基于外部 proximity 引起的协调，因此在大型内部扭曲存在时会失败。 spectral mapping 方法可以将形状嵌入高维空间中，从而使形状更容易对齐。然而，由于这些空间是抽象的、非线性的，因此可能受到输入的干扰或外来输入的影响。为了解决这个问题，我们的框架结合了两者的优点。具体来说，我们将源网格扭曲到目标点云，并且以高维函数映射（DFM）学习的高维嵌入来引导协调。特别是，协调在中间注册中进行动态更新，并且通过具有一致性约束的约束策略来强化整体管道。此外，为了避免外部对齐的需求，我们在独立于训练形状的同aligned synthetic shapes上训练了一个旋转回归器。实验结果表明，只需几十个有限变化的训练形状，我们的管道可以在多个非固定点云匹配 benchmark 上达到领先的Result，同时还能够在未看过的挑战性形状对应中提供高质量的协调。代码可以在 <https://github.com/rqhuang88/DFR> 上获取。
</details></li>
</ul>
<hr>
<h2 id="All-Optical-Phase-Conjugation-Using-Diffractive-Wavefront-Processing"><a href="#All-Optical-Phase-Conjugation-Using-Diffractive-Wavefront-Processing" class="headerlink" title="All-Optical Phase Conjugation Using Diffractive Wavefront Processing"></a>All-Optical Phase Conjugation Using Diffractive Wavefront Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04473">http://arxiv.org/abs/2311.04473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Che-Yung Shen, Jingxi Li, Tianyi Gan, Mona Jarrahi, Aydogan Ozcan</li>
<li>for: 这种研究旨在设计一种可以实现全光学相位逆 conjugation 操作的干涉波front处理器，用于对输入场的相位偏移进行补偿。</li>
<li>methods: 该研究使用深度学习来优化一组不活跃的干涉层，以实现对输入场的全光学相位逆 conjugation 操作。</li>
<li>results: 实验表明，使用这种diffractive wavefront processor可以成功地实现对相位偏移的补偿，并且可以在不同的光学 Spectrum中实现。<details>
<summary>Abstract</summary>
Optical phase conjugation (OPC) is a nonlinear technique used for counteracting wavefront distortions, with various applications ranging from imaging to beam focusing. Here, we present the design of a diffractive wavefront processor to approximate all-optical phase conjugation operation for input fields with phase aberrations. Leveraging deep learning, a set of passive diffractive layers was optimized to all-optically process an arbitrary phase-aberrated coherent field from an input aperture, producing an output field with a phase distribution that is the conjugate of the input wave. We experimentally validated the efficacy of this wavefront processor by 3D fabricating diffractive layers trained using deep learning and performing OPC on phase distortions never seen by the diffractive processor during its training. Employing terahertz radiation, our physical diffractive processor successfully performed the OPC task through a shallow spatially-engineered volume that axially spans tens of wavelengths. In addition to this transmissive OPC configuration, we also created a diffractive phase-conjugate mirror by combining deep learning-optimized diffractive layers with a standard mirror. Given its compact, passive and scalable nature, our diffractive wavefront processor can be used for diverse OPC-related applications, e.g., turbidity suppression and aberration correction, and is also adaptable to different parts of the electromagnetic spectrum, especially those where cost-effective wavefront engineering solutions do not exist.
</details>
<details>
<summary>摘要</summary>
《光学阶段 conjugation（OPC）是一种非线性技术，用于纠正波前弯曲，具有多种应用，包括成像和聚焦。在这里，我们提出了一种Diffractive wavefront processor，用于 aproximate all-optical phase conjugation操作，对输入场具有偏移的phas aberrations。通过深度学习，我们对输入距离场的arbitrary phase-aberrated coherent field进行了all-optical处理，并生成了输出场的phas distribution，与输入场的phas distribution是 conjugate。我们经验验证了这种wavefront processor的有效性，通过3D制造深度学习训练的Diffractive layers并在输入场中实现OPC操作。使用teraHertz radiation，我们的物理Diffractive processor成功完成了OPC任务，并在axially span tens of wavelengths的浅空间工程ered volume中完成了这个任务。此外，我们还创建了一个Diffractive phase-conjugate mirror，通过将深度学习优化的Diffractive layers与标准镜相结合。由于它的 компакт、被动和可扩展性，我们的Diffractive wavefront processor可以用于多种OPC相关应用，如受杂率减少和偏移 correction，并可以适应不同的电磁波谱，尤其是那些没有cost-effective wavefront engineering解决方案。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Few-shot-CLIP-with-Semantic-Aware-Fine-Tuning"><a href="#Enhancing-Few-shot-CLIP-with-Semantic-Aware-Fine-Tuning" class="headerlink" title="Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning"></a>Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04464">http://arxiv.org/abs/2311.04464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Zhu, Yuefeng Chen, Wei Wang, Xiaofeng Mao, Yue Wang, Zhigang Li, Wang lu, Jindong Wang, Xiangyang Ji</li>
<li>for: 本研究旨在提高由少量训练样本学习的深度神经网络在低资源enario下应用。</li>
<li>methods: 我们修改了CLIP的视觉编码器，特别是它的强调池化层，以适应不同的少量任务。在训练过程中，我们 fine-tune 强调池化层的参数，以便让模型强调任务特有的 semantics。在推理过程中，我们使用差异混合来组合原始强调池化层和 fine-tuned 强调池化层的特征，以便结合几架shot知识和预先训练的 CLIP 知识。</li>
<li>results: 我们的方法可以增强传统的几架shot CLIP，并且与现有的 adapter 方法相容（称为 SAFE-A）。<details>
<summary>Abstract</summary>
Learning generalized representations from limited training samples is crucial for applying deep neural networks in low-resource scenarios. Recently, methods based on Contrastive Language-Image Pre-training (CLIP) have exhibited promising performance in few-shot adaptation tasks. To avoid catastrophic forgetting and overfitting caused by few-shot fine-tuning, existing works usually freeze the parameters of CLIP pre-trained on large-scale datasets, overlooking the possibility that some parameters might not be suitable for downstream tasks. To this end, we revisit CLIP's visual encoder with a specific focus on its distinctive attention pooling layer, which performs a spatial weighted-sum of the dense feature maps. Given that dense feature maps contain meaningful semantic information, and different semantics hold varying importance for diverse downstream tasks (such as prioritizing semantics like ears and eyes in pet classification tasks rather than side mirrors), using the same weighted-sum operation for dense features across different few-shot tasks might not be appropriate. Hence, we propose fine-tuning the parameters of the attention pooling layer during the training process to encourage the model to focus on task-specific semantics. In the inference process, we perform residual blending between the features pooled by the fine-tuned and the original attention pooling layers to incorporate both the few-shot knowledge and the pre-trained CLIP's prior knowledge. We term this method as Semantic-Aware FinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot CLIP and is compatible with the existing adapter approach (termed SAFE-A).
</details>
<details>
<summary>摘要</summary>
学习通用表示法从有限的训练样本中学习是深度神经网络在具有限制的情况下应用的关键。最近，基于对比语言图像预训练（CLIP）的方法在少shot适应任务中表现出色。然而，由于少shot精度调整和过拟合，现有的方法通常将CLIP预训练在大规模数据集上冻结参数，忽略了可能一些参数不适合下游任务。为此，我们重新审视CLIP的视觉编码器，尤其是其独特的注意力汇聚层，该层通过空间权重汇聚密集特征图来完成。由于密集特征图包含有意义的semantic信息，而不同的semantic在不同的下游任务中具有不同的重要性（例如在宠物分类任务中更重视耳朵和眼睛而不是侧镜），因此在不同的少shot任务中使用同样的权重汇聚操作可能并不适合。因此，我们提议在训练过程中根据任务的semantic特征进行attenetion pooling层的细化调整，以便让模型在不同的下游任务中强调相应的semantic信息。在推理过程中，我们通过卷积残差融合法将 Fine-tuned的注意力汇聚层和原始注意力汇聚层的特征进行混合，以便同时利用CLIP的先前知识和少shot任务的知识。我们称这种方法为Semantic-Aware FinE-tuning（SAFE）。SAFE效果良好地提高了传统的少shot CLIP，并与现有的adapter方法（称为SAFE-A）兼容。
</details></li>
</ul>
<hr>
<h2 id="Retargeting-video-with-an-end-to-end-framework"><a href="#Retargeting-video-with-an-end-to-end-framework" class="headerlink" title="Retargeting video with an end-to-end framework"></a>Retargeting video with an end-to-end framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04458">http://arxiv.org/abs/2311.04458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thi-Ngoc-Hanh Le, HuiGuang Huang, Yi-Ru Chen, Tong-Yee Lee</li>
<li>For: 本研究旨在提高计算机图形应用中视频的播放体验，因为现代数字设备具有不同的屏幕比例，视频重新调整成不同的比例成为必要的功能。* Methods: 本研究使用了一种端到端的 RETVI 方法，解决了传统方法中的计算瓶颈问题，通过设计了内容特征分析器 (CFA) 和适应型变形估计器 (ADE) 两个模块来实现。* Results: 实验和评估表明，我们的系统在质量和运行时间两个方面都超过了前一代的工作。您可以通过访问我们的项目网站 $\href{<a target="_blank" rel="noopener" href="http://graphics.csie.ncku.edu.tw/RETVI%7D%7Bhttp://graphics.csie.ncku.edu.tw/RETVI%7D$">http://graphics.csie.ncku.edu.tw/RETVI}{http://graphics.csie.ncku.edu.tw/RETVI}$</a> 了解更多结果。<details>
<summary>Abstract</summary>
Video holds significance in computer graphics applications. Because of the heterogeneous of digital devices, retargeting videos becomes an essential function to enhance user viewing experience in such applications. In the research of video retargeting, preserving the relevant visual content in videos, avoiding flicking, and processing time are the vital challenges. Extending image retargeting techniques to the video domain is challenging due to the high running time. Prior work of video retargeting mainly utilizes time-consuming preprocessing to analyze frames. Plus, being tolerant of different video content, avoiding important objects from shrinking, and the ability to play with arbitrary ratios are the limitations that need to be resolved in these systems requiring investigation. In this paper, we present an end-to-end RETVI method to retarget videos to arbitrary aspect ratios. We eliminate the computational bottleneck in the conventional approaches by designing RETVI with two modules, content feature analyzer (CFA) and adaptive deforming estimator (ADE). The extensive experiments and evaluations show that our system outperforms previous work in quality and running time. Visit our project website for more results at $\href{http://graphics.csie.ncku.edu.tw/RETVI}{http://graphics.csie.ncku.edu.tw/RETVI}$.
</details>
<details>
<summary>摘要</summary>
视频具有计算机图形应用中的重要性。由于数字设备的多样性，视频重定向成为提高用户观看体验的关键功能。在视频重定向研究中，保持视频中重要的视觉内容，避免抖动和减少处理时间是挑战的关键。将图像重定向技术应用到视频领域是困难的，因为运行时间较长。现有的视频重定向方法主要通过时间consuming的预处理分析帧来解决这些挑战。此外，承受不同视频内容、避免重要对象缩小以及可以自由调整比例是这些系统需要解决的问题。在本文中，我们提出了一种终端到终点的 RETVI 方法，用于重定向视频到任意比例。我们通过设计 CFA 和 ADE 两个模块来消除传统方法中的计算瓶颈。我们进行了广泛的实验和评估，结果显示，我们的系统在质量和运行时间方面都超过了前一代的工作。欢迎访问我们的项目网站，了解更多的结果：<http://graphics.csie.ncku.edu.tw/RETVI>。
</details></li>
</ul>
<hr>
<h2 id="SS-MAE-Spatial-Spectral-Masked-Auto-Encoder-for-Multi-Source-Remote-Sensing-Image-Classification"><a href="#SS-MAE-Spatial-Spectral-Masked-Auto-Encoder-for-Multi-Source-Remote-Sensing-Image-Classification" class="headerlink" title="SS-MAE: Spatial-Spectral Masked Auto-Encoder for Multi-Source Remote Sensing Image Classification"></a>SS-MAE: Spatial-Spectral Masked Auto-Encoder for Multi-Source Remote Sensing Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04442">http://arxiv.org/abs/2311.04442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyan Lin, Feng Gao, Xiaocheng Shi, Junyu Dong, Qian Du</li>
<li>for: 这个研究旨在提出一种基于自我监督学习的图像理解方法，具体来说是一种基于遮盖图像模型（Masked Image Modeling，MIM）的图像和激光&#x2F;雷达数据联合分类方法。</li>
<li>methods: 本研究使用的方法包括一种空间分支和一种频谱分支，其中空间分支遮盖随机的补做patches并重建缺失的像素，而频谱分支遮盖随机的频谱通道并重建缺失的通道。此外，为了补做本地特征，我们添加了两个轻量级CNN来提取特征。</li>
<li>results: 我们的实验结果表明，SS-MAE比基于MIM的一些状态艺术基eline（包括一些使用Transporter进行特征提取）更有效果，并且可以更好地利用输入数据的空间和频谱表示。我们在三个公共可用的数据集上进行了广泛的实验，并证明了我们的SS-MAE在三个多源数据集上的优越性。<details>
<summary>Abstract</summary>
Masked image modeling (MIM) is a highly popular and effective self-supervised learning method for image understanding. Existing MIM-based methods mostly focus on spatial feature modeling, neglecting spectral feature modeling. Meanwhile, existing MIM-based methods use Transformer for feature extraction, some local or high-frequency information may get lost. To this end, we propose a spatial-spectral masked auto-encoder (SS-MAE) for HSI and LiDAR/SAR data joint classification. Specifically, SS-MAE consists of a spatial-wise branch and a spectral-wise branch. The spatial-wise branch masks random patches and reconstructs missing pixels, while the spectral-wise branch masks random spectral channels and reconstructs missing channels. Our SS-MAE fully exploits the spatial and spectral representations of the input data. Furthermore, to complement local features in the training stage, we add two lightweight CNNs for feature extraction. Both global and local features are taken into account for feature modeling. To demonstrate the effectiveness of the proposed SS-MAE, we conduct extensive experiments on three publicly available datasets. Extensive experiments on three multi-source datasets verify the superiority of our SS-MAE compared with several state-of-the-art baselines. The source codes are available at \url{https://github.com/summitgao/SS-MAE}.
</details>
<details>
<summary>摘要</summary>
高度流行和有效的自我监督学习方法之一是遮盖图像模型（MIM），它主要关注空间特征模型化，忽略 spectral特征模型化。同时，现有的MIM基于方法通常使用Transformer来提取特征，可能会导致部分本地或高频信息丢失。为此，我们提议一种具有空间和 spectral特征的掩码自动编码器（SS-MAE），用于高等分辨率遥感数据和 LiDAR/SAR 数据的联合分类。具体来说，SS-MAE包括一个空间分支和一个spectral分支。空间分支遮盖随机的补丁，重建缺失的像素，而spectral分支遮盖随机的频率通道，重建缺失的通道。我们的SS-MAE充分利用输入数据的空间和spectral表示。此外，为了补充训练阶段的本地特征，我们添加了两个轻量级的 CNN，用于特征提取。通过将全局和本地特征进行共同特征模型化，我们的SS-MAE可以更好地利用输入数据的特征。为了证明我们提出的SS-MAE的有效性，我们进行了广泛的实验，并与多个数据集进行比较。实验结果表明，我们的SS-MAE在三个公共可用的数据集上具有明显的优势，超越了多个状态 искусственный基线。代码可以在 \url{https://github.com/summitgao/SS-MAE} 中获取。
</details></li>
</ul>
<hr>
<h2 id="Blurry-Video-Compression-A-Trade-off-between-Visual-Enhancement-and-Data-Compression"><a href="#Blurry-Video-Compression-A-Trade-off-between-Visual-Enhancement-and-Data-Compression" class="headerlink" title="Blurry Video Compression: A Trade-off between Visual Enhancement and Data Compression"></a>Blurry Video Compression: A Trade-off between Visual Enhancement and Data Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04430">http://arxiv.org/abs/2311.04430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawit Mureja Argaw, Junsik Kim, In So Kweon</li>
<li>for: 该论文主要针对视频压缩（VC）方法在不同的场景下提供高质量视频压缩。</li>
<li>methods: 该论文提出了一种基于视觉改进和数据压缩的VC方法，通过利用视觉特征之间的自然负荷关系，将VC问题转化为一个最小化最大化问题，并提出了一种有效的框架和训练策略。</li>
<li>results: 对多个标准数据集进行了广泛的实验，证明了该方法比多种现有的VC方法更有效。<details>
<summary>Abstract</summary>
Existing video compression (VC) methods primarily aim to reduce the spatial and temporal redundancies between consecutive frames in a video while preserving its quality. In this regard, previous works have achieved remarkable results on videos acquired under specific settings such as instant (known) exposure time and shutter speed which often result in sharp videos. However, when these methods are evaluated on videos captured under different temporal priors, which lead to degradations like motion blur and low frame rate, they fail to maintain the quality of the contents. In this work, we tackle the VC problem in a general scenario where a given video can be blurry due to predefined camera settings or dynamics in the scene. By exploiting the natural trade-off between visual enhancement and data compression, we formulate VC as a min-max optimization problem and propose an effective framework and training strategy to tackle the problem. Extensive experimental results on several benchmark datasets confirm the effectiveness of our method compared to several state-of-the-art VC approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-the-What-and-How-of-Annotation-in-Video-Object-Segmentation"><a href="#Learning-the-What-and-How-of-Annotation-in-Video-Object-Segmentation" class="headerlink" title="Learning the What and How of Annotation in Video Object Segmentation"></a>Learning the What and How of Annotation in Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04414">http://arxiv.org/abs/2311.04414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanos Delatolas, Vicky Kalogeiton, Dim P. Papadopoulos</li>
<li>for: 这个研究是为了提高视频物件分割（VOS）模型的训练效率，并且实现人工费时的减少。</li>
<li>methods: 我们提出了一个人工辅助的标注框架（EVA-VOS），让机器学习模型预测每帧框架的标注类型和标注内容。</li>
<li>results: 我们在MOSE和DAVIS datasets上进行了实验，结果显示：EVA-VOS可以与人工标注相互比较，并且比标准方法快得多（3.5倍）；我们的帧选择性能优秀；EVA-VOS在标注时间方面具有优秀的表现。<details>
<summary>Abstract</summary>
Video Object Segmentation (VOS) is crucial for several applications, from video editing to video data generation. Training a VOS model requires an abundance of manually labeled training videos. The de-facto traditional way of annotating objects requires humans to draw detailed segmentation masks on the target objects at each video frame. This annotation process, however, is tedious and time-consuming. To reduce this annotation cost, in this paper, we propose EVA-VOS, a human-in-the-loop annotation framework for video object segmentation. Unlike the traditional approach, we introduce an agent that predicts iteratively both which frame ("What") to annotate and which annotation type ("How") to use. Then, the annotator annotates only the selected frame that is used to update a VOS module, leading to significant gains in annotation time. We conduct experiments on the MOSE and the DAVIS datasets and we show that: (a) EVA-VOS leads to masks with accuracy close to the human agreement 3.5x faster than the standard way of annotating videos; (b) our frame selection achieves state-of-the-art performance; (c) EVA-VOS yields significant performance gains in terms of annotation time compared to all other methods and baselines.
</details>
<details>
<summary>摘要</summary>
干脆视频对象分割（VOS）在许多应用中扮演着关键角色，从视频编辑到视频数据生成。训练VOS模型需要大量的手动标注视频。传统的标注方法需要人类 manually draw对象分割masks在目标视频帧上。这个Annotation process， however，是时间consuming和耗时的。为了降低这个标注成本，在这篇论文中，我们提出了EVA-VOS，一个人类在 loop的标注框架 для视频对象分割。与传统方法不同，我们引入了一个智能预测器，可以预测iteratively Which frame("What") to annotate和Which annotation type("How") to use。然后，annotator仅需要标注选择的帧，并将其用于更新VOS模块，从而 achieve significant gains in annotation time。我们在MOSE和DAVIS datasets上进行了实验，并证明了：（a）EVA-VOS可以在3.5倍 faster than标准视频标注方法得到masks with accuracy close to human agreement;（b）我们的帧选择性能在state-of-the-art水平;（c）EVA-VOS在所有方法和基准点上 yields significant performance gains in terms of annotation time。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/08/cs.CV_2023_11_08/" data-id="closbropt00le0g8811lkhdw2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/11/08/eess.AS_2023_11_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-11-08
        
      </div>
    </a>
  
  
    <a href="/2023/11/08/cs.AI_2023_11_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-11-08</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
