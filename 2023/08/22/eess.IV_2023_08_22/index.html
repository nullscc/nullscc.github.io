
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-22 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.11468 repo_url: None paper_authors: Ma">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-22 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/22/eess.IV_2023_08_22/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.11468 repo_url: None paper_authors: Ma">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-21T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:35.514Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/eess.IV_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-22 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multitemporal-analysis-in-Google-Earth-Engine-for-detecting-urban-changes-using-optical-data-and-machine-learning-algorithms"><a href="#Multitemporal-analysis-in-Google-Earth-Engine-for-detecting-urban-changes-using-optical-data-and-machine-learning-algorithms" class="headerlink" title="Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms"></a>Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11468">http://arxiv.org/abs/2308.11468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariapia Rita Iandolo, Francesca Razzano, Chiara Zarro, G. S. Yogesh, Silvia Liberata Ullo</li>
<li>for: 本研究使用Google Earth Engine（GEE）平台进行多时间分析，检测城市区域的变化使用光学数据和专门的机器学习（ML）算法。</li>
<li>methods: 作为案例研究，选择了埃及国家的开罗市，为过去一个十年内世界上最多人口的五个超大城市之一。从2013年7月到2021年7月，对研究区域进行分类和变化检测分析。</li>
<li>results: 结果表明提出的方法有效地确定了在选定时间间发生变化和不发生变化的城市区域。此外，本研究也证明了GEE作为云端解决方案，可以有效地处理大量的卫星数据。<details>
<summary>Abstract</summary>
The aim of this work is to perform a multitemporal analysis using the Google Earth Engine (GEE) platform for the detection of changes in urban areas using optical data and specific machine learning (ML) algorithms. As a case study, Cairo City has been identified, in Egypt country, as one of the five most populous megacities of the last decade in the world. Classification and change detection analysis of the region of interest (ROI) have been carried out from July 2013 to July 2021. Results demonstrate the validity of the proposed method in identifying changed and unchanged urban areas over the selected period. Furthermore, this work aims to evidence the growing significance of GEE as an efficient cloud-based solution for managing large quantities of satellite data.
</details>
<details>
<summary>摘要</summary>
目的是使用Google Earth Engine（GEE）平台进行多时间分析，以探测城市区域的变化使用光学数据和专门的机器学习（ML）算法。作为案例研究，埃及的开罗城市被选为全球最后一个十年最为人口稠密的五个超级城市之一。从2013年7月至2021年7月的时间段进行了区域兴趣（ROI）的分类和变化检测分析。结果表明提出的方法有效地标识了在选定时间段内发生变化和不发生变化的城市区域。此外，本研究也旨在证明GEE作为云计算平台，对快速处理大量卫星数据表现出了高效的能力。
</details></li>
</ul>
<hr>
<h2 id="Integration-of-Sentinel-1-and-Sentinel-2-data-for-Earth-surface-classification-using-Machine-Learning-algorithms-implemented-on-Google-Earth-Engine"><a href="#Integration-of-Sentinel-1-and-Sentinel-2-data-for-Earth-surface-classification-using-Machine-Learning-algorithms-implemented-on-Google-Earth-Engine" class="headerlink" title="Integration of Sentinel-1 and Sentinel-2 data for Earth surface classification using Machine Learning algorithms implemented on Google Earth Engine"></a>Integration of Sentinel-1 and Sentinel-2 data for Earth surface classification using Machine Learning algorithms implemented on Google Earth Engine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11340">http://arxiv.org/abs/2308.11340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesca Razzano, Mariapia Rita Iandolo, Chiara Zarro, G. S. Yogesh, Silvia Liberata Ullo</li>
<li>for: 这个研究旨在使用Synthetic Aperture Radar (SAR)和光学数据进行地球表面分类。</li>
<li>methods: 这个研究使用了Sentinel-1 (S-1)和Sentinel-2 (S-2)数据的集成，通过超vision机器学习算法在Google Earth Engine (GEE)平台上进行分类。</li>
<li>results: 研究结果表明，Radar和光学Remote探测提供了补充性信息，有利于地表覆盖分类，通常导致映射精度提高。此外，这篇论文也证明了GEE在处理大量卫星数据方面的emerging角色。<details>
<summary>Abstract</summary>
In this study, Synthetic Aperture Radar (SAR) and optical data are both considered for Earth surface classification. Specifically, the integration of Sentinel-1 (S-1) and Sentinel-2 (S-2) data is carried out through supervised Machine Learning (ML) algorithms implemented on the Google Earth Engine (GEE) platform for the classification of a particular region of interest. Achieved results demonstrate how in this case radar and optical remote detection provide complementary information, benefiting surface cover classification and generally leading to increased mapping accuracy. In addition, this paper works in the direction of proving the emerging role of GEE as an effective cloud-based tool for handling large amounts of satellite data.
</details>
<details>
<summary>摘要</summary>
这个研究中，使用Synthetic Aperture Radar（SAR）和光学数据进行地面分类。特别是通过监督式机器学习（ML）算法在Google Earth Engine（GEE）平台上结合Sentinel-1（S-1）和Sentinel-2（S-2）数据进行地面覆盖分类。实际结果表明在这种情况下，雷达和光学Remote探测提供了补充性信息，有利于地面覆盖分类，通常导致增加的地图精度。此外，这篇论文还证明了GEE在处理巨量卫星数据方面的emerging角色。
</details></li>
</ul>
<hr>
<h2 id="PCMC-T1-Free-breathing-myocardial-T1-mapping-with-Physically-Constrained-Motion-Correction"><a href="#PCMC-T1-Free-breathing-myocardial-T1-mapping-with-Physically-Constrained-Motion-Correction" class="headerlink" title="PCMC-T1: Free-breathing myocardial T1 mapping with Physically-Constrained Motion Correction"></a>PCMC-T1: Free-breathing myocardial T1 mapping with Physically-Constrained Motion Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11281">http://arxiv.org/abs/2308.11281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eyal Hanania, Ilya Volovik, Lilach Barkat, Israel Cohen, Moti Freiman</li>
<li>for: 这篇论文旨在描述一种基于深度学习的自由呼吸T1映像注射技术，以增强diffuse myocardial disease的诊断。</li>
<li>methods: 该技术使用了深度学习模型来 correction of motion artifacts in free-breathing T1 mapping. The model incorporates the signal decay model to encourage physically-plausible deformations along the longitudinal relaxation axis.</li>
<li>results: 对于5个 эксперименталь setup，PCMC-T1表现出最高的模型适应质量(R2: 0.955)和最高的клиниче影响(clinical score: 3.93)，在比较基eline方法时表现出了优异性。<details>
<summary>Abstract</summary>
T1 mapping is a quantitative magnetic resonance imaging (qMRI) technique that has emerged as a valuable tool in the diagnosis of diffuse myocardial diseases. However, prevailing approaches have relied heavily on breath-hold sequences to eliminate respiratory motion artifacts. This limitation hinders accessibility and effectiveness for patients who cannot tolerate breath-holding. Image registration can be used to enable free-breathing T1 mapping. Yet, inherent intensity differences between the different time points make the registration task challenging. We introduce PCMC-T1, a physically-constrained deep-learning model for motion correction in free-breathing T1 mapping. We incorporate the signal decay model into the network architecture to encourage physically-plausible deformations along the longitudinal relaxation axis. We compared PCMC-T1 to baseline deep-learning-based image registration approaches using a 5-fold experimental setup on a publicly available dataset of 210 patients. PCMC-T1 demonstrated superior model fitting quality (R2: 0.955) and achieved the highest clinical impact (clinical score: 3.93) compared to baseline methods (0.941, 0.946 and 3.34, 3.62 respectively). Anatomical alignment results were comparable (Dice score: 0.9835 vs. 0.984, 0.988). Our code and trained models are available at https://github.com/eyalhana/PCMC-T1.
</details>
<details>
<summary>摘要</summary>
T1映射是一种量化核磁共振成像（qMRI）技术，在慢性心肺疾病诊断中发挥了重要作用。然而，现有的方法都是基于呼吸停止序列来消除呼吸运动artefacts，这限制了患者可以tolerate的范围。图像 registrtion可以使得自由呼吸T1映射成为可能。然而，不同时点的信号强度之间的自然差异使得注册任务变得困难。我们介绍了PCMC-T1，一种基于深度学习的物理约束模型，用于自由呼吸T1映射中的运动 corrections。我们在网络架构中包含了信号衰减模型，以便鼓励物理可能的扭曲 along the longitudinal relaxation axis。我们与基eline deep learning based image registrtion方法进行了5-fold实验，结果显示PCMC-T1的模型适应质量（R2：0.955）和临床影响（临床分数：3.93）都高于基eline方法（0.941、0.946和3.34、3.62分别）。解剖对应结果相似（Dice分数：0.9835 vs. 0.984、0.988）。我们的代码和训练模型可以在https://github.com/eyalhana/PCMC-T1上获取。
</details></li>
</ul>
<hr>
<h2 id="Validation-of-apparent-intra-and-extra-myocellular-lipid-content-indicator-using-spiral-spectroscopic-imaging-at-3T"><a href="#Validation-of-apparent-intra-and-extra-myocellular-lipid-content-indicator-using-spiral-spectroscopic-imaging-at-3T" class="headerlink" title="Validation of apparent intra-and extra-myocellular lipid content indicator using spiral spectroscopic imaging at 3T"></a>Validation of apparent intra-and extra-myocellular lipid content indicator using spiral spectroscopic imaging at 3T</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11668">http://arxiv.org/abs/2308.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Naëgel, Magalie Viallon, Jabrane Karkouri, Thomas Troalen, Pierre Croisille, Hélène Ratiney</li>
<li>for: 本研究旨在开发一种快速简单的征方法，用于映射IMCL和EMCL显示的内容，这是一项复杂的任务，并与经典质量量化结果进行比较。</li>
<li>methods: 本研究使用旋转MRSI技术来实现快速简单的征方法，并对肌肉区域进行研究。</li>
<li>results: 研究结果表明，本方法可以快速、简单地映射IMCL和EMCL显示的内容，并与经典质量量化结果相符。<details>
<summary>Abstract</summary>
This work presents a fast and simple method based on spiral MRSI for mapping the IMCL and EMCL apparent content, which is a challenging task and it compares this indicator to classical quantification results in muscles of interest.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种快速简单的方法，基于螺旋MRSI测量IMCL和EMCL显示的内容，这是一项复杂的任务，并与经典量化结果进行比较。Here's a breakdown of the translation:* 这个研究 (zhè ge yánjiū) - This study* 提出 (tīshuō) - proposes* 一种 (yī zhī) - a kind of* 方法 (fāngfa) - method* 基于 (jīyù) - based on* 螺旋MRSI (lúnshēn MRSI) - spiral MRSI* 测量 (cèliàng) - measurement* IMCL (yìmín cluò) - intramuscular adipose tissue* 和 (hé) - and* EMCL (èrmín cluò) - extramuscular adipose tissue* 显示 (xiǎnshì) - display* 的 (de) - possessive particle* 内容 (néngyòng) - content* 是 (shì) - is* 一项 (yījiàn) - a task* 复杂 (fùzé) - complex* 与 (yǔ) - and* 经典 (jīngdiǎn) - classical* 量化 (liàngzhèng) - quantification* 结果 (jiéguò) - result* 进行 (jìnxiàng) - to conduct* 比较 (bǐjiào) - comparisonNote that the translation of "IMCL" and "EMCL" as "内容" (content) is a bit ambiguous, as these terms typically refer to specific types of tissue, rather than a general term for content. However, in the context of the sentence, it seems to be using "内容" to refer to the apparent content of the tissue, rather than the tissue itself.
</details></li>
</ul>
<hr>
<h2 id="Phase-Aberration-Correction-A-Deep-Learning-Based-Aberration-to-Aberration-Approach"><a href="#Phase-Aberration-Correction-A-Deep-Learning-Based-Aberration-to-Aberration-Approach" class="headerlink" title="Phase Aberration Correction: A Deep Learning-Based Aberration to Aberration Approach"></a>Phase Aberration Correction: A Deep Learning-Based Aberration to Aberration Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11149">http://arxiv.org/abs/2308.11149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Sobhan Goudarzi, An Tang, Habib Benali, Hassan Rivaz</li>
<li>for: 本研究旨在开发一种不需要真实ground truth的深度学习方法，用于correcting phase aberration问题在ultrasound imaging中。</li>
<li>methods: 我们提出了一种使用深度学习方法，并使用Randomly Aberrated RF Data作为输入和目标输出。我们还提出了一种适应性混合损失函数，以便更高效地训练这种网络。</li>
<li>results: 我们在实验中发现，使用我们提出的方法可以高效地correct phase aberration问题，并且可以在实际场景中使用。此外，我们还发布了一个包含161,701个单平面波图像（RF数据）的数据集，以便解决深度学习方法的数据不足问题。<details>
<summary>Abstract</summary>
One of the primary sources of suboptimal image quality in ultrasound imaging is phase aberration. It is caused by spatial changes in sound speed over a heterogeneous medium, which disturbs the transmitted waves and prevents coherent summation of echo signals. Obtaining non-aberrated ground truths in real-world scenarios can be extremely challenging, if not impossible. This challenge hinders training of deep learning-based techniques' performance due to the presence of domain shift between simulated and experimental data. Here, for the first time, we propose a deep learning-based method that does not require ground truth to correct the phase aberration problem, and as such, can be directly trained on real data. We train a network wherein both the input and target output are randomly aberrated radio frequency (RF) data. Moreover, we demonstrate that a conventional loss function such as mean square error is inadequate for training such a network to achieve optimal performance. Instead, we propose an adaptive mixed loss function that employs both B-mode and RF data, resulting in more efficient convergence and enhanced performance. Finally, we publicly release our dataset, including 161,701 single plane-wave images (RF data). This dataset serves to mitigate the data scarcity problem in the development of deep learning-based techniques for phase aberration correction.
</details>
<details>
<summary>摘要</summary>
For the first time, we propose a deep learning-based method that does not require ground truth to correct the phase aberration problem, and can be directly trained on real data. We train a network where the input and target output are randomly aberrated radio frequency (RF) data. Moreover, we find that a conventional loss function such as mean square error is inadequate for training such a network to achieve optimal performance. Instead, we propose an adaptive mixed loss function that employs both B-mode and RF data, resulting in more efficient convergence and enhanced performance.We publicly release our dataset, including 161,701 single plane-wave images (RF data), which serves to mitigate the data scarcity problem in the development of deep learning-based techniques for phase aberration correction.
</details></li>
</ul>
<hr>
<h2 id="Hey-That’s-Mine-Imperceptible-Watermarks-are-Preserved-in-Diffusion-Generated-Outputs"><a href="#Hey-That’s-Mine-Imperceptible-Watermarks-are-Preserved-in-Diffusion-Generated-Outputs" class="headerlink" title="Hey That’s Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs"></a>Hey That’s Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11123">http://arxiv.org/abs/2308.11123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke Ditria, Tom Drummond</li>
<li>for: 保护内容在线分享</li>
<li>methods: 使用隐藏水印技术训练生成模型，并测试模型是否可以检测水印并恢复水印特征</li>
<li>results: 通过统计测试，确认模型可以检测和恢复水印，提供了一种保护知识产权的解决方案<details>
<summary>Abstract</summary>
Generative models have seen an explosion in popularity with the release of huge generative Diffusion models like Midjourney and Stable Diffusion to the public. Because of this new ease of access, questions surrounding the automated collection of data and issues regarding content ownership have started to build. In this paper we present new work which aims to provide ways of protecting content when shared to the public. We show that a generative Diffusion model trained on data that has been imperceptibly watermarked will generate new images with these watermarks present. We further show that if a given watermark is correlated with a certain feature of the training data, the generated images will also have this correlation. Using statistical tests we show that we are able to determine whether a model has been trained on marked data, and what data was marked. As a result our system offers a solution to protect intellectual property when sharing content online.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>生成模型在发布大量生成扩散模型 Midjourney 和 Stable Diffusion 后得到了广泛的投入。由于这种新的访问权，人们开始关注自动收集数据的问题以及内容所有权问题。在这篇论文中，我们提出了一种保护内容的新方法。我们证明了一个基于不可见水印的生成扩散模型，会在生成新图像时包含这些水印。此外，如果给定的水印与训练数据中的某个特征相关，那么生成的图像也将具有这种相关性。通过统计测试，我们证明了我们是否可以判断模型是否训练用到了水印数据，以及这些数据是什么。因此，我们的系统可以保护在线分享内容的知识产权。
</details></li>
</ul>
<hr>
<h2 id="Switched-auxiliary-loss-for-robust-training-of-transformer-models-for-histopathological-image-segmentation"><a href="#Switched-auxiliary-loss-for-robust-training-of-transformer-models-for-histopathological-image-segmentation" class="headerlink" title="Switched auxiliary loss for robust training of transformer models for histopathological image segmentation"></a>Switched auxiliary loss for robust training of transformer models for histopathological image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10994">http://arxiv.org/abs/2308.10994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustaffa Hussain, Saharsh Barve</li>
<li>for: 本研究旨在开发一种用于多器官功能组织单元（FTUs）分割模型，以帮助病理学家更好地理解人体疾病的 Cellular 水平信息。</li>
<li>methods: 我们使用了HuBMAP + HPA - Hacking the Human Body竞赛数据集，并提出了使用偏移式助理损失来解决深度模型衰减问题，以便在训练深度模型时获得最佳result。</li>
<li>results: 我们的模型在公共数据集上得到了0.793的dice分数，与私有数据集上的0.778的dice分数相比，表明使用我们提议的方法可以提高模型的表现。这些发现也证明了transformers模型在医学图像分析中的紧密预测任务中的可靠性。<details>
<summary>Abstract</summary>
Functional tissue Units (FTUs) are cell population neighborhoods local to a particular organ performing its main function. The FTUs provide crucial information to the pathologist in understanding the disease affecting a particular organ by providing information at the cellular level. In our research, we have developed a model to segment multi-organ FTUs across 5 organs namely: the kidney, large intestine, lung, prostate and spleen by utilizing the HuBMAP + HPA - Hacking the Human Body competition dataset. We propose adding shifted auxiliary loss for training models like the transformers to overcome the diminishing gradient problem which poses a challenge towards optimal training of deep models. Overall, our model achieved a dice score of 0.793 on the public dataset and 0.778 on the private dataset and shows a 1% improvement with the use of the proposed method. The findings also bolster the use of transformers models for dense prediction tasks in the field of medical image analysis. The study assists in understanding the relationships between cell and tissue organization thereby providing a useful medium to look at the impact of cellular functions on human health.
</details>
<details>
<summary>摘要</summary>
Functional tissue Units (FTUs) 是指器官当地的细胞群聚地区，它们提供了病理学家理解器官疾病的关键信息。在我们的研究中，我们已经开发了一种方法来在5个器官（肾脏、大小肠、肺、生殖腺和脾膜）的多个Functional tissue Units（FTUs）中进行分割，使用了HuBMAP + HPA - Hacking the Human Body competition dataset。我们提议在训练模型时使用偏移 auxiliary loss，以解决深度模型训练中的减少梯度问题，从而提高模型的训练效果。总的来说，我们的模型在公共数据集上达到了0.793的 dice score，在私有数据集上达到了0.778的 dice score，与使用我们提议的方法相比，提高了1%。这些结果也证明了使用 transformers 模型在医学图像分析中进行紧密预测任务是有效的。这个研究帮助我们更好地理解细胞和组织之间的关系，从而更好地了解人类健康的影响因素。
</details></li>
</ul>
<hr>
<h2 id="Debiasing-Counterfactuals-In-the-Presence-of-Spurious-Correlations"><a href="#Debiasing-Counterfactuals-In-the-Presence-of-Spurious-Correlations" class="headerlink" title="Debiasing Counterfactuals In the Presence of Spurious Correlations"></a>Debiasing Counterfactuals In the Presence of Spurious Correlations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10984">http://arxiv.org/abs/2308.10984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amar Kumar, Nima Fathi, Raghav Mehta, Brennan Nichyporuk, Jean-Pierre R. Falet, Sotirios Tsaftaris, Tal Arbel</li>
<li>for: This paper aims to improve the performance of deep learning models in medical imaging classification tasks by addressing the issue of spurious correlations in the training data.</li>
<li>methods: The proposed method integrates two techniques: (1) popular debiasing classifiers, such as distributionally robust optimization (DRO), to avoid relying on spurious correlations, and (2) counterfactual image generation to unveil generalizable imaging markers of relevance to the task.</li>
<li>results: The proposed method is evaluated on two public datasets with simulated and real visual artifacts, and the results show that it (1) learns generalizable markers across the population and (2) successfully ignores spurious correlations and focuses on the underlying disease pathology.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是解决医学成像分类任务中的干扰因素问题，使深度学习模型能够更好地在人口中学习。</li>
<li>methods: 提议的方法组合了两种技术：（1）流行的偏差修正分类器（DRO），以避免基于干扰因素的推断，和（2）对应ifactual图像生成，以揭示有关任务的可靠成像标记。</li>
<li>results: 在两个公共数据集上（包括模拟和实际视觉artefacts）进行了评估，结果显示：（1）可以在人口中学习普遍的标记，和（2）成功忽略干扰因素，关注下面疾病趋势。<details>
<summary>Abstract</summary>
Deep learning models can perform well in complex medical imaging classification tasks, even when basing their conclusions on spurious correlations (i.e. confounders), should they be prevalent in the training dataset, rather than on the causal image markers of interest. This would thereby limit their ability to generalize across the population. Explainability based on counterfactual image generation can be used to expose the confounders but does not provide a strategy to mitigate the bias. In this work, we introduce the first end-to-end training framework that integrates both (i) popular debiasing classifiers (e.g. distributionally robust optimization (DRO)) to avoid latching onto the spurious correlations and (ii) counterfactual image generation to unveil generalizable imaging markers of relevance to the task. Additionally, we propose a novel metric, Spurious Correlation Latching Score (SCLS), to quantify the extent of the classifier reliance on the spurious correlation as exposed by the counterfactual images. Through comprehensive experiments on two public datasets (with the simulated and real visual artifacts), we demonstrate that the debiasing method: (i) learns generalizable markers across the population, and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BundleSeg-A-versatile-reliable-and-reproducible-approach-to-white-matter-bundle-segmentation"><a href="#BundleSeg-A-versatile-reliable-and-reproducible-approach-to-white-matter-bundle-segmentation" class="headerlink" title="BundleSeg: A versatile, reliable and reproducible approach to white matter bundle segmentation"></a>BundleSeg: A versatile, reliable and reproducible approach to white matter bundle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10958">http://arxiv.org/abs/2308.10958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne St-Onge, Kurt G Schilling, Francois Rheault</li>
<li>for: 提供一种可靠、可重现、快速的白 matter 通路EXTRACTION方法</li>
<li>methods: 利用迭代注册程序和精确流线搜索算法，能够高效地分 Segment 流线，无需进行追gram clustering或简化假设</li>
<li>results: 在repeatability和 reproduceability方面，BundleSeg 表现更好于现状的分Segmentation方法，并且具有显著的速度提升。提高了白 matter 连接的精度和减少了变化，为 neuroscience 研究提供了一种有价值的工具，从而提高了追gram-based 研究的敏感性和特点。<details>
<summary>Abstract</summary>
This work presents BundleSeg, a reliable, reproducible, and fast method for extracting white matter pathways. The proposed method combines an iterative registration procedure with a recently developed precise streamline search algorithm that enables efficient segmentation of streamlines without the need for tractogram clustering or simplifying assumptions. We show that BundleSeg achieves improved repeatability and reproducibility than state-of-the-art segmentation methods, with significant speed improvements. The enhanced precision and reduced variability in extracting white matter connections offer a valuable tool for neuroinformatic studies, increasing the sensitivity and specificity of tractography-based studies of white matter pathways.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种可靠、可重复、快速的白 matter 路径提取方法，称为 BundleSeg。该方法结合了一种迭代注册过程和最近发展的精确流线搜索算法，可以高效地分 segments 流线无需进行 tractogram 归类或简化假设。我们示出了 BundleSeg 在比较州-of-the-art 分 segmentation 方法的重复性和可重复性有所提高，同时速度也有显著提高。增强的精度和流线分 segments 的变化率提供了一种有价值的工具 для neuroscience 研究，提高了追踪性-based 研究中白 matter 路径的敏感性和特点。
</details></li>
</ul>
<hr>
<h2 id="Pixel-Adaptive-Deep-Unfolding-Transformer-for-Hyperspectral-Image-Reconstruction"><a href="#Pixel-Adaptive-Deep-Unfolding-Transformer-for-Hyperspectral-Image-Reconstruction" class="headerlink" title="Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction"></a>Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10820">http://arxiv.org/abs/2308.10820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoyu Li, Ying Fu, Ji Liu, Yulun Zhang</li>
<li>for: 高光谱像 (HSI) 重建，解决深度折衣框架中的匹配问题。</li>
<li>methods: 提出了一种Pixel Adaptive Deep Unfolding Transformer (PADUT)，在数据模块中使用了像素适应下降步骤，在假设模块中引入了Non-local Spectral Transformer (NST)，并在不同阶段和深度中的特征表达中进行了改进。</li>
<li>results: 对比于现有的HSI重建方法，实验结果表明PADUT方法在实验场景中具有更高的重建质量。代码可以在<a target="_blank" rel="noopener" href="https://github.com/MyuLi/PADUT%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/MyuLi/PADUT中下载。</a><details>
<summary>Abstract</summary>
Hyperspectral Image (HSI) reconstruction has made gratifying progress with the deep unfolding framework by formulating the problem into a data module and a prior module. Nevertheless, existing methods still face the problem of insufficient matching with HSI data. The issues lie in three aspects: 1) fixed gradient descent step in the data module while the degradation of HSI is agnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3) stage interaction ignoring the differences in features at different stages. To address these issues, in this work, we propose a Pixel Adaptive Deep Unfolding Transformer (PADUT) for HSI reconstruction. In the data module, a pixel adaptive descent step is employed to focus on pixel-level agnostic degradation. In the prior module, we introduce the Non-local Spectral Transformer (NST) to emphasize the 3D characteristics of HSI for recovering. Moreover, inspired by the diverse expression of features in different stages and depths, the stage interaction is improved by the Fast Fourier Transform (FFT). Experimental results on both simulated and real scenes exhibit the superior performance of our method compared to state-of-the-art HSI reconstruction methods. The code is released at: https://github.com/MyuLi/PADUT.
</details>
<details>
<summary>摘要</summary>
高spectral像素（HSI）重建已经取得了满意的进步，透过深度发散框架，将问题转化为数据模组和假设模组。然而，现有方法仍然面临HSI数据不足的问题，这些问题包括：1）固定的梯度下降步骤在数据模组中，而HSI质量下降是无知的Pixel水平。2）不够的假设模组 для3D HSI立方体。3）阶段交互忽略了不同阶段和层级的特征之间的不同。为了解决这些问题，在这个研究中，我们提出了适应Pixel深度 unfolding transformer（PADUT） дляHSI重建。在数据模组中，我们使用适应梯度下降步骤，以注意到Pixel水平的不知质量下降。在假设模组中，我们引入了Non-local Spectral Transformer（NST），以强调3D特征的HSI重建。此外，受到不同阶段和层级的特征表达的多样性的惊叹，我们改进了阶段交互，使用Fast Fourier Transform（FFT）。实验结果显示，我们的方法与现有的HSI重建方法相比，在 simulated和RealScene 上具有更高的性能。代码可以在：https://github.com/MyuLi/PADUT 获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/eess.IV_2023_08_22/" data-id="clly3606z00exdd88327cdtjn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/22/eess.AS_2023_08_22/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-08-22
        
      </div>
    </a>
  
  
    <a href="/2023/08/21/cs.LG_2023_08_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-21 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
