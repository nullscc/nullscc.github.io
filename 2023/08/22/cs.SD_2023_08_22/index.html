
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-22 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Furnishing Sound Event Detection with Language Model Abilities paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.11530 repo_url: None paper_authors: Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xia">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-22 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/22/cs.SD_2023_08_22/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Furnishing Sound Event Detection with Language Model Abilities paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.11530 repo_url: None paper_authors: Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xia">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-21T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:35.478Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.SD_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-22 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Furnishing-Sound-Event-Detection-with-Language-Model-Abilities"><a href="#Furnishing-Sound-Event-Detection-with-Language-Model-Abilities" class="headerlink" title="Furnishing Sound Event Detection with Language Model Abilities"></a>Furnishing Sound Event Detection with Language Model Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11530">http://arxiv.org/abs/2308.11530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xiangdong Wang</li>
<li>for: 本文further探讨语言模型（LMs）在视觉交互中的能力，特别是声事件检测（SED）。</li>
<li>methods: 提议一种精炼的方法，将音频特征和文本特征进行对应，以实现声事件分类和时间位置检测。该框架包括声学编码器、对应模块和解 Coupled语言解码器，从 audio 特征中生成时间和事件序列。与传统方法相比，我们的模型更简洁和全面，因为语言模型直接利用其语义能力来生成序列。</li>
<li>results: 研究表明，提议的方法可以准确地检测声事件。<details>
<summary>Abstract</summary>
Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification. Evaluation results show that the proposed method achieves accurate sequences of sound event detection.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Recently" is translated as "最近" (most recent)* "ability" is translated as "能力" (capability)* "language models" is translated as "语言模型" (language models)* "visual cross-modality" is translated as "视觉交叠" (visual intersection)* "sound event detection" is translated as "声音事件检测" (sound event detection)* "acoustic encoder" is translated as "声音编码器" (acoustic encoder)* "contrastive module" is translated as "对比模块" (contrastive module)* "decoupled language decoder" is translated as "分离语言解码器" (decoupled language decoder)* "temporal and event sequences" is translated as "时间和事件序列" (temporal and event sequences)* "compared with conventional works" is translated as "与传统工作比较" (compared with conventional works)* "require complicated processing" is translated as "需要复杂处理" (require complicated processing)* "barely utilize limited audio features" is translated as "几乎只使用有限的音频特征" (barely utilize limited audio features)* "our model is more concise and comprehensive" is translated as "我们的模型更简洁和全面" (our model is more concise and comprehensive)* "directly leverage its semantic capabilities" is translated as "直接利用它的 semantic 能力" (directly leverage its semantic capabilities)* "generate the sequences" is translated as "生成序列" (generate the sequences)* "timestamps capture" is translated as "时间捕捉" (timestamps capture)* "event classification" is translated as "事件分类" (event classification)* "evaluation results show" is translated as "评估结果表明" (evaluation results show)* "accurate sequences of sound event detection" is translated as "准确的声音事件检测序列" (accurate sequences of sound event detection)
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-denoising-streamed-from-mobile-phones-improves-speech-in-noise-understanding-for-hearing-aid-users"><a href="#Deep-learning-based-denoising-streamed-from-mobile-phones-improves-speech-in-noise-understanding-for-hearing-aid-users" class="headerlink" title="Deep learning-based denoising streamed from mobile phones improves speech-in-noise understanding for hearing aid users"></a>Deep learning-based denoising streamed from mobile phones improves speech-in-noise understanding for hearing aid users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11456">http://arxiv.org/abs/2308.11456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Udo Diehl, Hannes Zilly, Felix Sattler, Yosef Singer, Kevin Kepp, Mark Berry, Henning Hasemann, Marlene Zippel, Müge Kaya, Paul Meyer-Rachner, Annett Pudszuhn, Veit M. Hofmann, Matthias Vormann, Elias Sprengel<br>for:The paper is written for people with hearing loss who use hearing aids, particularly those in real-world noisy environments.methods:The paper uses deep learning-based denoising systems that run in real-time on mobile devices (iPhone 7 and Samsung Galaxy S10) with a total delay of around 75ms.results:The denoising system improves audio quality, as measured by subjective ratings, speech intelligibility, and live conversations in noisy environments. Subjective ratings increase by more than 40%, and speech reception thresholds improve by 1.6 dB SRT compared to a fitted hearing aid as a baseline. The system is the first of its kind to be implemented on a mobile device and streamed directly to users’ hearing aids using only a single channel of audio input, resulting in improved user satisfaction.<details>
<summary>Abstract</summary>
The hearing loss of almost half a billion people is commonly treated with hearing aids. However, current hearing aids often do not work well in real-world noisy environments. We present a deep learning based denoising system that runs in real time on iPhone 7 and Samsung Galaxy S10 (25ms algorithmic latency). The denoised audio is streamed to the hearing aid, resulting in a total delay of around 75ms. In tests with hearing aid users having moderate to severe hearing loss, our denoising system improves audio across three tests: 1) listening for subjective audio ratings, 2) listening for objective speech intelligibility, and 3) live conversations in a noisy environment for subjective ratings. Subjective ratings increase by more than 40%, for both the listening test and the live conversation compared to a fitted hearing aid as a baseline. Speech reception thresholds, measuring speech understanding in noise, improve by 1.6 dB SRT. Ours is the first denoising system that is implemented on a mobile device, streamed directly to users' hearing aids using only a single channel as audio input while improving user satisfaction on all tested aspects, including speech intelligibility. This includes overall preference of the denoised and streamed signal over the hearing aid, thereby accepting the higher latency for the significant improvement in speech understanding.
</details>
<details>
<summary>摘要</summary>
现有约一亿人的听力问题，通常通过听觉器进行治疗。然而，现有的听觉器 frequently 在实际噪音环境中不够有效。我们提出了基于深度学习的噪音除除系统，可以在 iPhone 7 和 Samsung Galaxy S10 上进行实时运行（25ms 的算法遅延）。噪音除除系统将运算到听觉器上，总延迟约 75ms。在听觉器用户进行 moderate 至 severe 的听力损伤时，我们的噪音除除系统在三个测试中表现出色：1）聆听Subjective 音乐评分，2）聆听Speech 智能度测试，3）在噪音环境中进行生活对话的Subjective 评分。聆听者的评分提高了 más de 40%，包括聆听测试和生活对话。Speech 收集阈值（SRT）也提高了1.6 dB。我们的噪音除除系统是首个在 mobil device 上实现的，直接将清晰的音频流传递到用户的听觉器，使用单一通道的音频输入，而不需要听觉器的额外配件。在所有测试项目中，包括聆听者对清晰音频的偏好和生活对话中的听力理解，我们的系统获得了更高的使用者满意度。
</details></li>
</ul>
<hr>
<h2 id="Convoifilter-A-case-study-of-doing-cocktail-party-speech-recognition"><a href="#Convoifilter-A-case-study-of-doing-cocktail-party-speech-recognition" class="headerlink" title="Convoifilter: A case study of doing cocktail party speech recognition"></a>Convoifilter: A case study of doing cocktail party speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11380">http://arxiv.org/abs/2308.11380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thai-Binh Nguyen, Alexander Waibel</li>
<li>for: 提高自动语音识别（ASR）的精度，特别是在响度高、听录环境中。</li>
<li>methods: 使用单通道speech减少模块减少干扰声音，同时使用ASR模块。通过这种方法，模型可以降低ASR的单词错误率（WER）从80%降至26.4%。</li>
<li>results: 通过对这两个组件进行联合细调，降低WER从26.4%下降至14.5%。<details>
<summary>Abstract</summary>
This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluation-of-the-Speech-Resynthesis-Capabilities-of-the-VoicePrivacy-Challenge-Baseline-B1"><a href="#Evaluation-of-the-Speech-Resynthesis-Capabilities-of-the-VoicePrivacy-Challenge-Baseline-B1" class="headerlink" title="Evaluation of the Speech Resynthesis Capabilities of the VoicePrivacy Challenge Baseline B1"></a>Evaluation of the Speech Resynthesis Capabilities of the VoicePrivacy Challenge Baseline B1</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11337">http://arxiv.org/abs/2308.11337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ünal Ege Gaznepoglu, Nils Peters</li>
<li>for: 这项研究旨在评估VPC基线B1是否能够生成人性化的语音。</li>
<li>methods: 这项研究使用了VPC基线B1中的神经 vocoder 将 F0、x-vectors 和瓶颈特征转换为语音示例。</li>
<li>results: 研究发现，神经 vocoder 在语音示例中引入了 artifacts，导致语音具有不自然的感觉。 MUSHRA-like 听试中，18名参与者也证实了这一点，促使进一步研究VPC基线B1的分析和 sintesis 组件。<details>
<summary>Abstract</summary>
Speaker anonymization systems continue to improve their ability to obfuscate the original speaker characteristics in a speech signal, but often create processing artifacts and unnatural sounding voices as a tradeoff. Many of those systems stem from the VoicePrivacy Challenge (VPC) Baseline B1, using a neural vocoder to synthesize speech from an F0, x-vectors and bottleneck features-based speech representation. Inspired by this, we investigate the reproduction capabilities of the aforementioned baseline, to assess how successful the shared methodology is in synthesizing human-like speech. We use four objective metrics to measure speech quality, waveform similarity, and F0 similarity. Our findings indicate that both the speech representation and the vocoder introduces artifacts, causing an unnatural perception. A MUSHRA-like listening test on 18 subjects corroborate our findings, motivating further research on the analysis and synthesis components of the VPC Baseline B1.
</details>
<details>
<summary>摘要</summary>
对话匿名系统继续提高了对话者特征的隐蔽能力，但经常会产生处理 artifacts 和不自然的声音作为交易。这些系统多数来自于 VoicePrivacy Challenge（VPC）基线B1，使用神经 vocoder 将 F0、x-vectors 和瓶颈特征转化为语音。受这些基线的启发，我们调查了论文中的复制能力，以评估这种方法是否能够 sinthez human-like 语音。我们使用四个对象指标测量语音质量、波形相似性和 F0 相似性。我们的发现表明， tanto 语音表示法 quanto  vocoder 都会产生缺陷，导致不自然的感觉。一个 MUSHRA-like 听众测试中，18名参与者证实了我们的发现，并促使了对分析和 sinthez 组件的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning"><a href="#Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning" class="headerlink" title="Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning"></a>Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11276">http://arxiv.org/abs/2308.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan</li>
<li>for: 这篇论文旨在解决文本到音乐生成（T2M-Gen）领域中的一个主要障碍，即公共可用的大规模音乐数据集中的缺乏。</li>
<li>methods: 我们提出了一种名为Music Understanding LLaMA（MU-LLaMA）的模型，可以回答音乐相关的问题并生成音乐文件的标题。我们的模型使用一个预训练的MERT模型来提取音乐特征。</li>
<li>results: 我们的实验表明，使用我们设计的MusicQA数据集训练MU-LLaMA模型可以在多个维度上达到出色的表现，包括音乐问答和音乐标题生成。我们的模型在不同的维度上的表现都高于当前状态的艺术模型，并为T2M-Gen领域的进步带来了希望。<details>
<summary>Abstract</summary>
Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.
</details>
<details>
<summary>摘要</summary>
文本转换为乐曲生成（T2M-Gen）遇到了一个主要的障碍，即公共可用的大规模乐曲数据集中的自然语言标签缺乏。为解决这一问题，我们提议了Music Understanding LLaMA（MU-LLaMA），能够回答乐曲相关的问题并生成乐曲文件的标签。我们的模型利用了预训练的MERT模型来提取乐曲特征。但是，为了训练MU-LLaMA模型，获得合适的数据集仍然是一个挑战，因为现有的公共可用的音频问答数据集缺乏必要的深度来回答开放式乐曲问题。为了填补这一漏洞，我们提出了一种方法，可以从现有的音频描述数据集中生成问题答案对。我们还介绍了MusicQA数据集，用于回答开放式乐曲相关的问题。实验结果表明，我们提出的MU-LLaMA模型，使用我们设计的MusicQA数据集进行训练，在不同的纪录metric上取得了极佳的表现，超过了当前状态的前方模型在乐曲问题回答和乐曲描述生成方面，并提供了T2M-Gen研究领域的一个可能的进步。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Bends-in-Popular-Music-Guitar-Tablatures"><a href="#Modeling-Bends-in-Popular-Music-Guitar-Tablatures" class="headerlink" title="Modeling Bends in Popular Music Guitar Tablatures"></a>Modeling Bends in Popular Music Guitar Tablatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12307">http://arxiv.org/abs/2308.12307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/adhooge1/bend-prediction">https://gitlab.com/adhooge1/bend-prediction</a></li>
<li>paper_authors: Alexandre D’Hooge, Louis Bigo, Ken Déguernel</li>
<li>For: 本研究旨在提高电 guitar tablature 中的折衣预测，以便帮助将其他乐器的乐谱转换成 guitar tablature。* Methods: 本研究使用了一组25个高级特征，对每个 tablature 进行分析，以预测下一个折衣的发生。* Results: 实验结果显示，使用决策树可以成功预测折衣的发生，F1 分数为 0.71，false positive 预测很少，这表明这种方法可以有效地帮助将非 guitar 乐谱转换成 guitar tablature。<details>
<summary>Abstract</summary>
Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.
</details>
<details>
<summary>摘要</summary>
Tablature notation widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information, including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.
</details></li>
</ul>
<hr>
<h2 id="PMVC-Data-Augmentation-Based-Prosody-Modeling-for-Expressive-Voice-Conversion"><a href="#PMVC-Data-Augmentation-Based-Prosody-Modeling-for-Expressive-Voice-Conversion" class="headerlink" title="PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion"></a>PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11084">http://arxiv.org/abs/2308.11084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Deng, Huaizhen Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</li>
<li>for: 本研究旨在提出一个新的声音转换框架，以实现无需文本转换的自然声音转换。</li>
<li>methods: 本研究使用了一新的声音增强算法来提取robust的调音信息，并应用了mask和predict机制来分离调音和内容信息。</li>
<li>results: 实验结果显示，PMVC框架能够提高声音转换的自然性和相似度。<details>
<summary>Abstract</summary>
Voice conversion as the style transfer task applied to speech, refers to converting one person's speech into a new speech that sounds like another person's. Up to now, there has been a lot of research devoted to better implementation of VC tasks. However, a good voice conversion model should not only match the timbre information of the target speaker, but also expressive information such as prosody, pace, pause, etc. In this context, prosody modeling is crucial for achieving expressive voice conversion that sounds natural and convincing. Unfortunately, prosody modeling is important but challenging, especially without text transcriptions. In this paper, we firstly propose a novel voice conversion framework named 'PMVC', which effectively separates and models the content, timbre, and prosodic information from the speech without text transcriptions. Specially, we introduce a new speech augmentation algorithm for robust prosody extraction. And building upon this, mask and predict mechanism is applied in the disentanglement of prosody and content information. The experimental results on the AIShell-3 corpus supports our improvement of naturalness and similarity of converted speech.
</details>
<details>
<summary>摘要</summary>
声音转换作为样式传递任务应用于语音，指的是将一个人的语音转换成另一个人的语音，以致 зву量和样式相似。至今，对于更好地实现声音转换任务的研究已经很多。然而，一个好的声音转换模型应该不仅匹配目标说话者的时刻信息，还应该包括表达信息 such as 味道、速度、停顿等。在这种情况下，味道模型化是关键的，以实现自然和吸引人的声音转换。可惜，味道模型化是重要但困难的，特别是无文本词汇。在这篇论文中，我们首先提出了一种新的声音转换框架，名为PMVC，可以有效地从语音中分离和模型内容、时刻和表达信息。特别是，我们提出了一种新的语音增强算法，用于Robust prosody extraction。然后，我们在内容和味道信息之间进行遮盖和预测，以实现味道和内容信息的分离。实验结果表示，PMVC框架在AIShell-3 corpus上提高了转换后语音的自然性和相似性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/cs.SD_2023_08_22/" data-id="clm0t8e1f00amv788fj513y95" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/22/cs.LG_2023_08_22/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-22 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/22/eess.AS_2023_08_22/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-08-22</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
