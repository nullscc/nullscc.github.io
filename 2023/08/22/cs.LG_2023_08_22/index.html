
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-22 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="A free from local minima algorithm for training regressive MLP neural networks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.11532 repo_url: None paper_authors: Augusto Montisci for: 本文提出了一种创新的多层感知网络训练方法，以避免地方">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-22 18:00:00">
<meta property="og:url" content="http://nullscc.github.io/2023/08/22/cs.LG_2023_08_22/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="A free from local minima algorithm for training regressive MLP neural networks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.11532 repo_url: None paper_authors: Augusto Montisci for: 本文提出了一种创新的多层感知网络训练方法，以避免地方">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-21T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:51.350Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.LG_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-22 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-free-from-local-minima-algorithm-for-training-regressive-MLP-neural-networks"><a href="#A-free-from-local-minima-algorithm-for-training-regressive-MLP-neural-networks" class="headerlink" title="A free from local minima algorithm for training regressive MLP neural networks"></a>A free from local minima algorithm for training regressive MLP neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11532">http://arxiv.org/abs/2308.11532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Augusto Montisci</li>
<li>for: 本文提出了一种创新的多层感知网络训练方法，以避免地方最小值问题。</li>
<li>methods: 该方法基于训练集分布的性质，或者说是其内部图像，以避免地方最小值问题。</li>
<li>results: 本文对一个知名的基准数据集进行了表现示例，并证明了该方法可以减少地方最小值问题。<details>
<summary>Abstract</summary>
In this article an innovative method for training regressive MLP networks is presented, which is not subject to local minima. The Error-Back-Propagation algorithm, proposed by William-Hinton-Rummelhart, has had the merit of favouring the development of machine learning techniques, which has permeated every branch of research and technology since the mid-1980s. This extraordinary success is largely due to the black-box approach, but this same factor was also seen as a limitation, as soon more challenging problems were approached. One of the most critical aspects of the training algorithms was that of local minima of the loss function, typically the mean squared error of the output on the training set. In fact, as the most popular training algorithms are driven by the derivatives of the loss function, there is no possibility to evaluate if a reached minimum is local or global. The algorithm presented in this paper avoids the problem of local minima, as the training is based on the properties of the distribution of the training set, or better on its image internal to the neural network. The performance of the algorithm is shown for a well-known benchmark.
</details>
<details>
<summary>摘要</summary>
本文提出了一种创新的多层感知网络训练方法，不受地方最小值限制。惠威姆-希茨-鲁姆哈特提出的错误反传播算法在1980年代中期以来，在机器学习领域取得了杰出成就，这一成就主要归功于黑盒模型，但同时也被视为一个限制，因为随着问题的增加 complexity，这种方法的应用逐渐受限。训练算法中最 kritical的问题是搜索函数的地方最小值，通常是训练集上输出的平均方差。因为现有训练算法都是根据损失函数的导数进行驱动，因此无法评估是否到达了地方最小值。本文所提出的方法解决了本问题，基于训练集的分布特性或更好地说，是基于其内部图像的。文中所示的性能表现对一个知名的测试集进行了展示。
</details></li>
</ul>
<hr>
<h2 id="ReLiCADA-–-Reservoir-Computing-using-Linear-Cellular-Automata-Design-Algorithm"><a href="#ReLiCADA-–-Reservoir-Computing-using-Linear-Cellular-Automata-Design-Algorithm" class="headerlink" title="ReLiCADA – Reservoir Computing using Linear Cellular Automata Design Algorithm"></a>ReLiCADA – Reservoir Computing using Linear Cellular Automata Design Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11522">http://arxiv.org/abs/2308.11522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Kantic, Fabian C. Legl, Walter Stechele, Jakob Hermann</li>
<li>for: 优化Reservoir Computing的设计用于时间序列应用。</li>
<li>methods: 使用Cellular Automata模型选择规则，并解决线性Cellular Automaton规则选择问题。</li>
<li>results: 对相关的标准数据集进行了严格的测试，选择的规则在总规则空间中排名在前5%，并且与其他当前领域模型相比，提供了更低的计算复杂性和训练时间，同时实现了更低的错误率。<details>
<summary>Abstract</summary>
In this paper, we present a novel algorithm to optimize the design of Reservoir Computing using Cellular Automata models for time series applications. Besides selecting the models' hyperparameters, the proposed algorithm particularly solves the open problem of linear Cellular Automaton rule selection. The selection method pre-selects only a few promising candidate rules out of an exponentially growing rule space. When applied to relevant benchmark datasets, the selected rules achieve low errors, with the best rules being among the top 5% of the overall rule space. The algorithm was developed based on mathematical analysis of linear Cellular Automaton properties and is backed by almost one million experiments, adding up to a computational runtime of nearly one year. Comparisons to other state-of-the-art time series models show that the proposed Reservoir Computing using Cellular Automata models have lower computational complexity, at the same time, achieve lower errors. Hence, our approach reduces the time needed for training and hyperparameter optimization by up to several orders of magnitude.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的算法优化储量计算机使用细胞自动机模型进行时间序列应用。除了选择模型的超参数之外，我们的算法特别解决了线性细胞自动机规则选择的开放问题。选择方法先于搜索极其庞大的规则空间中的一些有潜力的候选规则。应用到相关的基准数据集上，选择的规则可以 дости到低的错误率，最好的规则在总规则空间中排名前5%。我们的算法基于细胞自动机的数学分析和大约一百万个实验，计算时间近一年。与其他当前最佳时间序列模型比较，我们的方法具有更低的计算复杂度，同时可以实现更低的错误率。因此，我们的方法可以减少训练和超参数优化所需的时间，可能是数量级减少。
</details></li>
</ul>
<hr>
<h2 id="EM-for-Mixture-of-Linear-Regression-with-Clustered-Data"><a href="#EM-for-Mixture-of-Linear-Regression-with-Clustered-Data" class="headerlink" title="EM for Mixture of Linear Regression with Clustered Data"></a>EM for Mixture of Linear Regression with Clustered Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11518">http://arxiv.org/abs/2308.11518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Reisizadeh, Khashayar Gatmiry, Asuman Ozdaglar</li>
<li>for: 这种问题的答案是如何利用分布式学习框架中的数据归一化来提高学习效果。</li>
<li>methods: 作者使用了Expectation-Maximization（EM）方法来估计二元混合线性回归模型中的参数。</li>
<li>results: 作者表明，如果 Initialize EM 方法正确，并且 $m$ 增长为 $e^{o(n)}$，那么 EM 方法只需要 $O(1)$ 轮次来达到同样的统计准确性，并且提供了新的 asymptotic optimization 和通用的验证保证。<details>
<summary>Abstract</summary>
Modern data-driven and distributed learning frameworks deal with diverse massive data generated by clients spread across heterogeneous environments. Indeed, data heterogeneity is a major bottleneck in scaling up many distributed learning paradigms. In many settings however, heterogeneous data may be generated in clusters with shared structures, as is the case in several applications such as federated learning where a common latent variable governs the distribution of all the samples generated by a client. It is therefore natural to ask how the underlying clustered structures in distributed data can be exploited to improve learning schemes. In this paper, we tackle this question in the special case of estimating $d$-dimensional parameters of a two-component mixture of linear regressions problem where each of $m$ nodes generates $n$ samples with a shared latent variable. We employ the well-known Expectation-Maximization (EM) method to estimate the maximum likelihood parameters from $m$ batches of dependent samples each containing $n$ measurements. Discarding the clustered structure in the mixture model, EM is known to require $O(\log(mn/d))$ iterations to reach the statistical accuracy of $O(\sqrt{d/(mn)})$. In contrast, we show that if initialized properly, EM on the structured data requires only $O(1)$ iterations to reach the same statistical accuracy, as long as $m$ grows up as $e^{o(n)}$. Our analysis establishes and combines novel asymptotic optimization and generalization guarantees for population and empirical EM with dependent samples, which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
现代数据驱动和分布式学习框架面临各种各样的大规模数据，这些数据来自客户端分布在不同环境中。实际上，数据的差异性是规模化多个分布式学习模式的主要障碍。然而，在一些应用中，客户端生成的数据可能会具有共同结构，例如联邦学习中，每个客户端都生成的所有样本均具有共同的隐藏变量。因此，可以问到如何利用分布式数据中的层次结构来改进学习方案。在这篇论文中，我们解决这个问题，特别是在估计$d$-维参数的两组线性回归问题中。我们使用常见的期望-最大化（EM）方法来估计最大 LIKELIHOOD 参数，从$m$ 批次的相互依赖样本中获取 $n$ 个测量。不考虑分布式数据中的层次结构，EM 知道需要 $O(\log(mn/d))$ 迭代来达到同样的统计准确性，其中 $m$ 是 Client 数量，$n$ 是每个客户端生成的样本数量，$d$ 是维度。然而，我们显示，如果INITIALIZED 正确， THEN EM 在结构化数据上只需要 $O(1)$ 迭代来达到同样的统计准确性，只要 $m$ 增长为 $e^{o(n)}$。我们的分析建立了和更新了可opus和总体Optimization guarantees for population和empirical EM with dependent samples，这可能是独立的兴趣。
</details></li>
</ul>
<hr>
<h2 id="Mode-Combinability-Exploring-Convex-Combinations-of-Permutation-Aligned-Models"><a href="#Mode-Combinability-Exploring-Convex-Combinations-of-Permutation-Aligned-Models" class="headerlink" title="Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models"></a>Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11511">http://arxiv.org/abs/2308.11511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrián Csiszárik, Melinda F. Kiss, Péter Kőrösi-Szabó, Márton Muntag, Gergely Papp, Dániel Varga</li>
<li>for: 本研究探讨了两个具有相同结构的神经网络参数向量 $\Theta_A$ 和 $\Theta_B$ 的元素积体weighted组合的可能性。</li>
<li>methods: 作者采用了广泛的实验方法，检查了不同的模型组合parametrized by elements of the hypercube $[0,1]^d$ 和其附近的区域。</li>
<li>results: 研究发现，大部分的hypercube区域具有低损失值，表明了模型连接性的扩展到一般现象，称为模式可连接性。 作者还发现了一些新的线性模式连接性和模型重新定位的观察结果。<details>
<summary>Abstract</summary>
We explore element-wise convex combinations of two permutation-aligned neural network parameter vectors $\Theta_A$ and $\Theta_B$ of size $d$. We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its vicinity. Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability. We also make several novel observations regarding linear mode connectivity and model re-basin. We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model. Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are non-vacuous in the sense that there are significant functional differences between the resulting models.
</details>
<details>
<summary>摘要</summary>
我们探索element-wise convex combinations of two permutation-aligned neural network parameter vectors $\Theta_A$和$\Theta_B$ of size $d$. We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its vicinity. Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability. We also make several novel observations regarding linear mode connectivity and model re-basin. We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model. Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are non-vacuous in the sense that there are significant functional differences between the resulting models.Here's the translation in Traditional Chinese:我们探索element-wise convex combinations of two permutation-aligned neural network parameter vectors $\Theta_A$和$\Theta_B$ of size $d$. We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its vicinity. Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability. We also make several novel observations regarding linear mode connectivity and model re-basin. We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model. Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are non-vacuous in the sense that there are significant functional differences between the resulting models.
</details></li>
</ul>
<hr>
<h2 id="Designing-an-attack-defense-game-how-to-increase-robustness-of-financial-transaction-models-via-a-competition"><a href="#Designing-an-attack-defense-game-how-to-increase-robustness-of-financial-transaction-models-via-a-competition" class="headerlink" title="Designing an attack-defense game: how to increase robustness of financial transaction models via a competition"></a>Designing an attack-defense game: how to increase robustness of financial transaction models via a competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11406">http://arxiv.org/abs/2308.11406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexey Zaytsev, Alex Natekin, Evgeni Vorsin, Valerii Smirnov, Georgii Smirnov, Oleg Sidorshin, Alexander Senin, Alexander Dudin, Dmitry Berestnev</li>
<li>for: 本研究旨在 investigate 机器学习模型在金融交易数据上的护盾机制和攻击策略，以提高金融领域机器学习模型的安全性。</li>
<li>methods: 我们采用了一场竞赛来实现这一目标，其中参与者之间进行直接竞争，以模拟现实生活中的攻击和防御情况。我们还进行了一系列的数学实验和相关的ablation study，以评估不同方法的效果。</li>
<li>results: 我们的分析表明，我们开发的攻击和防御策略在实际执行中表现出色，并且在不同领域中都能够得到显著的提升。此外，我们还发现了一些现有方法的局限性和不足，并提出了一些改进的建议。<details>
<summary>Abstract</summary>
Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.   To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break it, and what techniques one should use to make it more robust, and introduction additional way to attack models or increase their robustness.   Our analysis continues with a meta-study on the used approaches with their power, numerical experiments, and accompanied ablations studies. We show that the developed attacks and defenses outperform existing alternatives from the literature while being practical in terms of execution, proving the validity of the competition as a tool for uncovering vulnerabilities of machine learning models and mitigating them in various domains.
</details>
<details>
<summary>摘要</summary>
随着金融领域中邪恶攻击的升级风险和相应严重的损害，对机器学习模型的敌对策略和鲁棒防御机制的全面理解变得非常重要。随着银行更广泛采用更准确但可能脆弱的神经网络，这种威胁变得更加严重。我们希望通过 investigate现有的敌对攻击和防御策略，以及对神经网络模型使用时间序列金融数据的影响。为实现这个目标，我们设计了一项竞赛，allowing participants to directly compete against each other, allowing for realistic and detailed investigation of problems in modern financial transaction data. Our main contributions include:1. 分析竞赛动态，回答如何隐藏模型从恶意用户，如何打砸它，以及如何使它更加鲁棒。2.  introduce additional way to attack models or increase their robustness.3.  conduct a meta-study on the used approaches, including their power, numerical experiments, and accompanied ablation studies.我们的分析表明，我们提出的攻击和防御策略在实现方面具有优势，并且在不同领域中能够有效地抵御邪恶攻击。这些策略的实现可以在实际应用中进行 praktikable 的执行，证明了竞赛的有效性作为机器学习模型的漏洞探测和 mitigation 工具。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-assisted-exploration-for-affine-Deligne-Lusztig-varieties"><a href="#Machine-learning-assisted-exploration-for-affine-Deligne-Lusztig-varieties" class="headerlink" title="Machine learning assisted exploration for affine Deligne-Lusztig varieties"></a>Machine learning assisted exploration for affine Deligne-Lusztig varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11355">http://arxiv.org/abs/2308.11355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinpf314/ml4adlv">https://github.com/jinpf314/ml4adlv</a></li>
<li>paper_authors: Bin Dong, Xuhua He, Pengfei Jin, Felix Schremmer, Qingchao Yu</li>
<li>for: 本研究使用机器学习助手框架研究非齐次Deligne-Lusztig变换（ADLV）的几何结构，主要目标是研究ADLV中不同维度和分解成分的非空性模式。</li>
<li>methods: 本研究提出了一种数据生成、模型训练、模式分析和人工审阅的可重复管道，表现出机器学习和纯 математиче研究之间的细腻互动。数据生成过程强调选择 significative 子集和适当的特征集。</li>
<li>results: 本研究通过这种框架， rediscovered 虚维度公式并提供了一个新的问题的完整数学证明，同时还扩展了一个已经开放的问题，即一个certain lower bound of dimension的下界。此外，我们还提供了计算ADLV和机器学习模型的源代码，以便进一步的探索。<details>
<summary>Abstract</summary>
This paper presents a novel, interdisciplinary study that leverages a Machine Learning (ML) assisted framework to explore the geometry of affine Deligne-Lusztig varieties (ADLV). The primary objective is to investigate the nonemptiness pattern, dimension and enumeration of irreducible components of ADLV. Our proposed framework demonstrates a recursive pipeline of data generation, model training, pattern analysis, and human examination, presenting an intricate interplay between ML and pure mathematical research. Notably, our data-generation process is nuanced, emphasizing the selection of meaningful subsets and appropriate feature sets. We demonstrate that this framework has a potential to accelerate pure mathematical research, leading to the discovery of new conjectures and promising research directions that could otherwise take significant time to uncover. We rediscover the virtual dimension formula and provide a full mathematical proof of a newly identified problem concerning a certain lower bound of dimension. Furthermore, we extend an open invitation to the readers by providing the source code for computing ADLV and the ML models, promoting further explorations. This paper concludes by sharing valuable experiences and highlighting lessons learned from this collaboration.
</details>
<details>
<summary>摘要</summary>
Notably, our data generation process is sophisticated, focusing on selecting meaningful subsets and appropriate feature sets. We show that this framework has the potential to accelerate pure mathematical research, leading to the discovery of new conjectures and promising research directions that might otherwise take a long time to uncover.In this paper, we rediscover the virtual dimension formula and provide a full mathematical proof of a newly identified problem concerning a certain lower bound of dimension. Furthermore, we extend an open invitation to readers by providing the source code for computing ADLV and the ML models, encouraging further explorations.This paper concludes by sharing valuable experiences and highlighting lessons learned from this collaboration.
</details></li>
</ul>
<hr>
<h2 id="WEARS-Wearable-Emotion-AI-with-Real-time-Sensor-data"><a href="#WEARS-Wearable-Emotion-AI-with-Real-time-Sensor-data" class="headerlink" title="WEARS: Wearable Emotion AI with Real-time Sensor data"></a>WEARS: Wearable Emotion AI with Real-time Sensor data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11673">http://arxiv.org/abs/2308.11673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Limbani, Daketi Yatin, Nitish Chaturvedi, Vaishnavi Moorthy, Pushpalatha M, Harichandana BSS, Sumit Kumar</li>
<li>for: 这 paper 的目的是提出一种基于智能手表传感器的情绪预测系统，以便在日常生活中识别用户的情绪。</li>
<li>methods: 这 paper 使用了一种混合英文和地方语言的视频来采集真实数据，并使用多种机器学习模型进行模型化。</li>
<li>results: 实验结果显示，使用多层感知器模型可以达到93.75%的最高准确率，并且对心率、加速度和自转仪器数据进行了ablation研究，以便更好地理解各种传感器数据对情绪识别的影响。<details>
<summary>Abstract</summary>
Emotion prediction is the field of study to understand human emotions. Existing methods focus on modalities like text, audio, facial expressions, etc., which could be private to the user. Emotion can be derived from the subject's psychological data as well. Various approaches that employ combinations of physiological sensors for emotion recognition have been proposed. Yet, not all sensors are simple to use and handy for individuals in their daily lives. Thus, we propose a system to predict user emotion using smartwatch sensors. We design a framework to collect ground truth in real-time utilizing a mix of English and regional language-based videos to invoke emotions in participants and collect the data. Further, we modeled the problem as binary classification due to the limited dataset size and experimented with multiple machine-learning models. We also did an ablation study to understand the impact of features including Heart Rate, Accelerometer, and Gyroscope sensor data on mood. From the experimental results, Multi-Layer Perceptron has shown a maximum accuracy of 93.75 percent for pleasant-unpleasant (high/low valence classification) moods.
</details>
<details>
<summary>摘要</summary>
预测人类情感是一个研究领域，旨在理解人类情感的变化。现有方法主要基于模式如文本、音频、脸部表达等，这些模式可能是用户私有的。另外，情感也可以从主体的心理数据中 derivation。多种方法使用组合的生物学传感器进行情感识别的建议。然而，不是所有传感器都是容易使用和日常生活中手动使用的。因此，我们提出了使用智能手表传感器预测用户情感的系统。我们设计了一个框架，通过混合英文和地区语言基于视频诱发情感并收集数据。此外，我们将问题设置为二分类问题，因为数据集的限制性，并试用多种机器学习模型。我们还进行了减少研究，以了解特定传感器数据的影响。从实验结果来看，多层感知网络在高/低积极情感（愉悦-不愉悦）类别上达到了最高的准确率为93.75%。
</details></li>
</ul>
<hr>
<h2 id="Careful-at-Estimation-and-Bold-at-Exploration"><a href="#Careful-at-Estimation-and-Bold-at-Exploration" class="headerlink" title="Careful at Estimation and Bold at Exploration"></a>Careful at Estimation and Bold at Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11348">http://arxiv.org/abs/2308.11348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xing Chen, Yijun Liu, Zhaogeng Liu, Hechang Chen, Hengshuai Yao, Yi Chang</li>
<li>for: 该 paper 的目的是提出一种新的探索策略来解决DPRL中的探索问题，以提高DPRL的性能。</li>
<li>methods: 该 paper 使用了double-Q函数框架，提出了一种新的探索策略，包括greedy Q softmax更新 schema和action探索与Q值更新的组合。</li>
<li>results: 该 paper 在Mujoco benchmark上进行了实验，并证明了其在不同环境中的优秀表现，特别是在最复杂的人工智能环境中。<details>
<summary>Abstract</summary>
Exploration strategies in continuous action space are often heuristic due to the infinite actions, and these kinds of methods cannot derive a general conclusion. In prior work, it has been shown that policy-based exploration is beneficial for continuous action space in deterministic policy reinforcement learning(DPRL). However, policy-based exploration in DPRL has two prominent issues: aimless exploration and policy divergence, and the policy gradient for exploration is only sometimes helpful due to inaccurate estimation. Based on the double-Q function framework, we introduce a novel exploration strategy to mitigate these issues, separate from the policy gradient. We first propose the greedy Q softmax update schema for Q value update. The expected Q value is derived by weighted summing the conservative Q value over actions, and the weight is the corresponding greedy Q value. Greedy Q takes the maximum value of the two Q functions, and conservative Q takes the minimum value of the two different Q functions. For practicality, this theoretical basis is then extended to allow us to combine action exploration with the Q value update, except for the premise that we have a surrogate policy that behaves like this exploration policy. In practice, we construct such an exploration policy with a few sampled actions, and to meet the premise, we learn such a surrogate policy by minimizing the KL divergence between the target policy and the exploration policy constructed by the conservative Q. We evaluate our method on the Mujoco benchmark and demonstrate superior performance compared to previous state-of-the-art methods across various environments, particularly in the most complex Humanoid environment.
</details>
<details>
<summary>摘要</summary>
<SYS><translation>探索策略在连续动作空间 frequently 是随机的，因为动作的数量是无限的，这类方法无法得出一般结论。在先前的工作中，已经证明了在 deterministic policy reinforcement learning（DPRL） 中的策略基于探索是有利。然而，DPRL 中的策略基于探索存在两个明显的问题：无目的探索和策略分化，并且Policy gradient for exploration 只有在不准确的估计时才能帮助。基于 double-Q 函数框架，我们介绍了一种新的探索策略，与 Policy gradient 分离。我们首先提出了 Q 值更新 schema，将 Q 值更新为 greedy Q softmax Weighted sum。预期 Q 值由 actions 的 greedy Q 值Weighted sum  derive，greedy Q 取得最大值，conservative Q 取得最小值。为了实用，我们将这种理论基础EXTEND TO ALLOW US TO COMBINE ACTION EXPLORATION WITH Q VALUE UPDATE，只要我们有一个 substitute policy ，该策略在探索过程中与目标策略类似。在实践中，我们构建了一个这种探索策略，使用了一些采样的动作，并为了满足这种前提，我们学习了一个 substitute policy ，通过最小化 KL divergence  между target policy 和探索策略来减少 KL divergence。我们在 Mujoco  benchmark 上评估了我们的方法，并在不同的环境中表现出优于先前的状态艺术方法，特别是在最复杂的人类形态环境中。</translation></SYS>Note: Simplified Chinese is used in this translation, as it is more widely used in mainland China and is the standard for most online content. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Generalising-sequence-models-for-epigenome-predictions-with-tissue-and-assay-embeddings"><a href="#Generalising-sequence-models-for-epigenome-predictions-with-tissue-and-assay-embeddings" class="headerlink" title="Generalising sequence models for epigenome predictions with tissue and assay embeddings"></a>Generalising sequence models for epigenome predictions with tissue and assay embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11671">http://arxiv.org/abs/2308.11671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Deasy, Ron Schwessinger, Ferran Gonzalez, Stephen Young, Kim Branson</li>
<li>for: 预测epigenetic profile的Sequence模型方法在最近几年内得到了广泛发展，包括序列长度、模型大小以及profile多样性。</li>
<li>methods: 我们使用Contextualised Genomic Network（CGN）来集成组织和assay嵌入，从而利用上下文信息来增强长距离序列嵌入。与之前的方法不同，我们在输入空间中使用上下文信息来增强长距离序列嵌入，而不是扩展输出空间。</li>
<li>results: 我们在一个广泛的epigenetic profile集合中展示了我们的方法的效果，并为genetic variant对epigenetic sequence模型训练的影响提供了第一个洞见。我们的总体方法在多个设置中超越了现状，并且采用了更严格的验证过程。<details>
<summary>Abstract</summary>
Sequence modelling approaches for epigenetic profile prediction have recently expanded in terms of sequence length, model size, and profile diversity. However, current models cannot infer on many experimentally feasible tissue and assay pairs due to poor usage of contextual information, limiting $\textit{in silico}$ understanding of regulatory genomics. We demonstrate that strong correlation can be achieved across a large range of experimental conditions by integrating tissue and assay embeddings into a Contextualised Genomic Network (CGN). In contrast to previous approaches, we enhance long-range sequence embeddings with contextual information in the input space, rather than expanding the output space. We exhibit the efficacy of our approach across a broad set of epigenetic profiles and provide the first insights into the effect of genetic variants on epigenetic sequence model training. Our general approach to context integration exceeds state of the art in multiple settings while employing a more rigorous validation procedure.
</details>
<details>
<summary>摘要</summary>
Sequence模型方法 дляepigeneticprofile预测最近几年来有所扩展，包括序列长度、模型大小和profile多样性。然而，当前的模型无法涵盖许多实验可行的组织和测试对 pair，因为各种Contextualinformation的使用不佳，限制了$\textit{in silico}$理解的规则 genomics。我们示出，可以在各种实验条件下 achieves强相关性，通过在输入空间中 integrating组织和测试嵌入（CGN）。与之前的方法不同，我们在长距离序列嵌入中增强了 Contextualinformation，而不是扩展输出空间。我们在多种epigeneticprofile中展示了我们的方法的效果，并提供了遗传变种对epigenetic sequence模型训练的首次研究。我们的通用方法可以在多个设置中超越当前状态，并采用更严格的验证过程。
</details></li>
</ul>
<hr>
<h2 id="Protect-Federated-Learning-Against-Backdoor-Attacks-via-Data-Free-Trigger-Generation"><a href="#Protect-Federated-Learning-Against-Backdoor-Attacks-via-Data-Free-Trigger-Generation" class="headerlink" title="Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation"></a>Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11333">http://arxiv.org/abs/2308.11333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanxin Yang, Ming Hu, Yue Cao, Jun Xia, Yihao Huang, Yang Liu, Mingsong Chen<br>for: This paper aims to address the vulnerability of Federated Learning (FL) to poisoning attacks, specifically backdoor attacks, by proposing a novel data-free trigger-generation-based defense approach.methods: The proposed approach uses two characteristics of backdoor attacks to generate trigger images that can eliminate poisoned models and ensure the updated global model is benign. These trigger images are generated by identifying the differences between the old and new global models and evaluating the effect of these generated images.results: The proposed approach can defend against almost all existing types of backdoor attacks and outperform all seven state-of-the-art defense methods with both IID and non-IID scenarios. In addition, the approach can successfully defend against backdoor attacks even when 80% of the clients are malicious.<details>
<summary>Abstract</summary>
As a distributed machine learning paradigm, Federated Learning (FL) enables large-scale clients to collaboratively train a model without sharing their raw data. However, due to the lack of data auditing for untrusted clients, FL is vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned data for local training or directly changing the model parameters, attackers can easily inject backdoors into the model, which can trigger the model to make misclassification of targeted patterns in images. To address these issues, we propose a novel data-free trigger-generation-based defense approach based on the two characteristics of backdoor attacks: i) triggers are learned faster than normal knowledge, and ii) trigger patterns have a greater effect on image classification than normal class patterns. Our approach generates the images with newly learned knowledge by identifying the differences between the old and new global models, and filters trigger images by evaluating the effect of these generated images. By using these trigger images, our approach eliminates poisoned models to ensure the updated global model is benign. Comprehensive experiments demonstrate that our approach can defend against almost all the existing types of backdoor attacks and outperform all the seven state-of-the-art defense methods with both IID and non-IID scenarios. Especially, our approach can successfully defend against the backdoor attack even when 80\% of the clients are malicious.
</details>
<details>
<summary>摘要</summary>
如一种分布式机器学习 paradigma，联邦学习（FL）允许大规模客户端共同训练模型，而无需分享 raw 数据。然而，由于没有数据审核 для不可信客户端，FL 容易受到毒素攻击，特别是后门攻击。攻击者可以使用毒素数据进行本地训练或直接修改模型参数，轻松地植入后门到模型中，导致模型对预期的图像模式进行误分类。为解决这些问题，我们提出了一种基于两个特征的数据-自由触发器生成防御方法：一、触发器更快学习Normal Knowledge，二、触发器图像在图像分类中具有更大的效果。我们的方法生成新学习的图像，并将触发器图像筛选出来。通过使用这些触发器图像，我们的方法可以消除毒素模型，以确保更新的全球模型是善意的。我们的实验表明，我们的方法可以防御大多数现有的后门攻击，并在IID和非IID场景下超过所有七种现有的防御方法。特别是，我们的方法可以成功防御80%的客户端是恶意的情况下的后门攻击。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Positioning-using-Multivariate-Time-Series-Classification-for-Factory-Environments"><a href="#Machine-Learning-based-Positioning-using-Multivariate-Time-Series-Classification-for-Factory-Environments" class="headerlink" title="Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments"></a>Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11670">http://arxiv.org/abs/2308.11670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nisal Hemadasa Manikku Badu, Marcus Venzke, Volker Turau, Yanqiu Huang</li>
<li>for: 这篇论文是用于解决室内定位系统（IPS）在各种产业应用中的问题的。</li>
<li>methods: 这篇论文使用了机器学习（ML）技术，通过使用固件设备的动态和环境感知器来实现室内定位。</li>
<li>results: 论文通过对多个机器学习模型进行比较，发现了一个名为 CNN-1D 的模型，它在精度、内存占用率和推理速度三个方面均有优秀的性能。<details>
<summary>Abstract</summary>
Indoor Positioning Systems (IPS) gained importance in many industrial applications. State-of-the-art solutions heavily rely on external infrastructures and are subject to potential privacy compromises, external information requirements, and assumptions, that make it unfavorable for environments demanding privacy and prolonged functionality. In certain environments deploying supplementary infrastructures for indoor positioning could be infeasible and expensive. Recent developments in machine learning (ML) offer solutions to address these limitations relying only on the data from onboard sensors of IoT devices. However, it is unclear which model fits best considering the resource constraints of IoT devices. This paper presents a machine learning-based indoor positioning system, using motion and ambient sensors, to localize a moving entity in privacy concerned factory environments. The problem is formulated as a multivariate time series classification (MTSC) and a comparative analysis of different machine learning models is conducted in order to address it. We introduce a novel time series dataset emulating the assembly lines of a factory. This dataset is utilized to assess and compare the selected models in terms of accuracy, memory footprint and inference speed. The results illustrate that all evaluated models can achieve accuracies above 80 %. CNN-1D shows the most balanced performance, followed by MLP. DT was found to have the lowest memory footprint and inference latency, indicating its potential for a deployment in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
室内定位系统（IPS）在多个工业应用中升级到了重要地位。当前的解决方案大多依赖于外部基础设施，并且受到外部信息要求、假设和隐私问题的限制，这使得在需要隐私和长期可用性的环境中部署室内定位系统变得不可能或者太过昂贵。当前的机器学习（ML）技术提供了解决这些限制的解决方案，不需要外部基础设施，仅仅使用宠物设备上的固件传感器数据。然而，选择最佳的机器学习模型仍然存在问题，特别是考虑到宠物设备的资源限制。本文提出了一种基于机器学习的室内定位系统，使用运动和 ambient 传感器来确定移动实体在隐私关注的制造厂房环境中的位置。问题被形式化为多变量时间序列分类（MTSC），并进行了不同机器学习模型的比较分析，以解决其中的问题。我们提供了一个新的时间序列数据集，模拟了制造厂房的生产线。这个数据集被用来评估和比较选择的模型，以确定它们在精度、内存占用量和推理速度等方面的表现。结果表明所有评估模型均可以达到上百分之八十的准确率。CNN-1D 表现最平衡，其次是 MLP。DT 具有最低的内存占用量和推理延迟，表明其在实际场景中的部署潜在性。
</details></li>
</ul>
<hr>
<h2 id="Class-Label-aware-Graph-Anomaly-Detection"><a href="#Class-Label-aware-Graph-Anomaly-Detection" class="headerlink" title="Class Label-aware Graph Anomaly Detection"></a>Class Label-aware Graph Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11669">http://arxiv.org/abs/2308.11669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jhkim611/clad">https://github.com/jhkim611/clad</a></li>
<li>paper_authors: Junghoon Kim, Yeonjun In, Kanghoon Yoon, Junmo Lee, Chanyoung Park</li>
<li>for: 检测图像中的结构异常（structural anomalies）</li>
<li>methods: 使用有限的标注节点来增强无监督图像异常检测（unsupervised graph anomaly detection）的性能</li>
<li>results: 在十个数据集上进行了广泛的实验，并证明了CLAD在缺乏类别标签信息的情况下也能够显著提高异常检测性能，比较于现有的无监督图像异常检测方法。<details>
<summary>Abstract</summary>
Unsupervised GAD methods assume the lack of anomaly labels, i.e., whether a node is anomalous or not. One common observation we made from previous unsupervised methods is that they not only assume the absence of such anomaly labels, but also the absence of class labels (the class a node belongs to used in a general node classification task). In this work, we study the utility of class labels for unsupervised GAD; in particular, how they enhance the detection of structural anomalies. To this end, we propose a Class Label-aware Graph Anomaly Detection framework (CLAD) that utilizes a limited amount of labeled nodes to enhance the performance of unsupervised GAD. Extensive experiments on ten datasets demonstrate the superior performance of CLAD in comparison to existing unsupervised GAD methods, even in the absence of ground-truth class label information. The source code for CLAD is available at \url{https://github.com/jhkim611/CLAD}.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Unsupervised GAD methods assume the lack of anomaly labels, i.e., whether a node is anomalous or not. One common observation we made from previous unsupervised methods is that they not only assume the absence of such anomaly labels, but also the absence of class labels (the class a node belongs to used in a general node classification task). In this work, we study the utility of class labels for unsupervised GAD; in particular, how they enhance the detection of structural anomalies. To this end, we propose a Class Label-aware Graph Anomaly Detection framework (CLAD) that utilizes a limited amount of labeled nodes to enhance the performance of unsupervised GAD. Extensive experiments on ten datasets demonstrate the superior performance of CLAD in comparison to existing unsupervised GAD methods, even in the absence of ground-truth class label information. The source code for CLAD is available at \url{https://github.com/jhkim611/CLAD}." into Simplified Chinese.<<SYS>>不监督的GAD方法假设缺乏异常标签，即节点是否异常。我们从前一些不监督方法中所观察到的一个常见现象是，它们不仅假设缺乏异常标签，还假设缺乏类标签（用于普通的节点分类任务中的节点类别）。在这种情况下，我们研究了基于类标签的不监督GAD框架（CLAD）的utilidad，特别是如何通过限制数量的标签节点来提高不监督GAD的性能。我们对10个数据集进行了广泛的实验，结果表明CLAD在对不监督GAD方法进行比较时表现更优秀，即使没有地面真实的类标签信息。CLAD的源代码可以在\url{https://github.com/jhkim611/CLAD}中找到。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-of-Transformers’-Predictions-via-Topological-Analysis-of-the-Attention-Matrices"><a href="#Uncertainty-Estimation-of-Transformers’-Predictions-via-Topological-Analysis-of-the-Attention-Matrices" class="headerlink" title="Uncertainty Estimation of Transformers’ Predictions via Topological Analysis of the Attention Matrices"></a>Uncertainty Estimation of Transformers’ Predictions via Topological Analysis of the Attention Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11295">http://arxiv.org/abs/2308.11295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizaveta Kostenok, Daniil Cherniavskii, Alexey Zaytsev</li>
<li>for: 本研究旨在解决深度学习模型在预测中的信任度评估问题，这是自然语言处理领域的一个开放问题。</li>
<li>methods: 我们使用基于Transformer架构的神经网络，并利用Topological Data Analysis方法来探索内部表示关系，以估计模型的信任度。</li>
<li>results: 我们的方法比传统方法更高质量，并开启了新的应用领域，但是需要选择 topological 特征。<details>
<summary>Abstract</summary>
Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires the selection of topological features.
</details>
<details>
<summary>摘要</summary>
确定深度学习模型的预测结果的可信度是自然语言处理领域的一个开放问题。大多数经典方法对文本分类模型的不确定性估计非常弱。我们将任务设置为基于Transformer架构的神经网络不确定性估计。Transformer模型的一个关键特点是听力机制，可以支持神经网络中token的隐藏表示之间的信息流动。我们通过使用Topological Data Analysis方法探索神经网络内部表示的关系，并利用它们来预测模型的可信度。在这篇论文中，我们提出了基于Topological Properties的不确定性估计方法，并与经典方法进行比较。结果显示，我们的方法质量高于现有方法，开启了新的应用领域，但需要选择Topological特征。
</details></li>
</ul>
<hr>
<h2 id="Network-Momentum-across-Asset-Classes"><a href="#Network-Momentum-across-Asset-Classes" class="headerlink" title="Network Momentum across Asset Classes"></a>Network Momentum across Asset Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11294">http://arxiv.org/abs/2308.11294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Pu, Stephen Roberts, Xiaowen Dong, Stefan Zohren</li>
<li>for: 这篇论文探讨了网络势头概念，即资产之间的势头延伸效应。</li>
<li>methods: 该论文使用了一种可读性强的网络学习模型，揭示了不同资产类别之间势头延伸的网络效应。</li>
<li>results: 研究发现，基于网络势头的投资策略可以实现年化返报率为22%，并且具有1.5的希尔帕雷利率。<details>
<summary>Abstract</summary>
We investigate the concept of network momentum, a novel trading signal derived from momentum spillover across assets. Initially observed within the confines of pairwise economic and fundamental ties, such as the stock-bond connection of the same company and stocks linked through supply-demand chains, momentum spillover implies a propagation of momentum risk premium from one asset to another. The similarity of momentum risk premium, exemplified by co-movement patterns, has been spotted across multiple asset classes including commodities, equities, bonds and currencies. However, studying the network effect of momentum spillover across these classes has been challenging due to a lack of readily available common characteristics or economic ties beyond the company level. In this paper, we explore the interconnections of momentum features across a diverse range of 64 continuous future contracts spanning these four classes. We utilise a linear and interpretable graph learning model with minimal assumptions to reveal the intricacies of the momentum spillover network. By leveraging the learned networks, we construct a network momentum strategy that exhibits a Sharpe ratio of 1.5 and an annual return of 22%, after volatility scaling, from 2000 to 2022. This paper pioneers the examination of momentum spillover across multiple asset classes using only pricing data, presents a multi-asset investment strategy based on network momentum, and underscores the effectiveness of this strategy through robust empirical analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Test-Time-Embedding-Normalization-for-Popularity-Bias-Mitigation"><a href="#Test-Time-Embedding-Normalization-for-Popularity-Bias-Mitigation" class="headerlink" title="Test Time Embedding Normalization for Popularity Bias Mitigation"></a>Test Time Embedding Normalization for Popularity Bias Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11288">http://arxiv.org/abs/2308.11288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-postech/tten">https://github.com/ml-postech/tten</a></li>
<li>paper_authors: Dain Kim, Jinhyeok Park, Dongwoo Kim</li>
<li>For:  Mitigating popularity bias in recommender systems* Methods:  Test Time Embedding Normalization (TTEN)* Results:  Surpasses previous mitigation approaches by a significant margin, effectively reducing popularity biasHere’s the full text in Simplified Chinese:* 为： Mitigating popularity bias in recommender systems* 方法： Test Time Embedding Normalization (TTEN)* 结果： Surpasses previous mitigation approaches by a significant margin, effectively reducing popularity biasI hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Popularity bias is a widespread problem in the field of recommender systems, where popular items tend to dominate recommendation results. In this work, we propose 'Test Time Embedding Normalization' as a simple yet effective strategy for mitigating popularity bias, which surpasses the performance of the previous mitigation approaches by a significant margin. Our approach utilizes the normalized item embedding during the inference stage to control the influence of embedding magnitude, which is highly correlated with item popularity. Through extensive experiments, we show that our method combined with the sampled softmax loss effectively reduces popularity bias compare to previous approaches for bias mitigation. We further investigate the relationship between user and item embeddings and find that the angular similarity between embeddings distinguishes preferable and non-preferable items regardless of their popularity. The analysis explains the mechanism behind the success of our approach in eliminating the impact of popularity bias. Our code is available at https://github.com/ml-postech/TTEN.
</details>
<details>
<summary>摘要</summary>
受欢迎偏见是推荐系统领域中的一个广泛存在的问题，即受欢迎的项目往往占据推荐结果的主导地位。在这项工作中，我们提议了“测试时嵌入normalization”作为一种简单而高效的策略来缓解受欢迎偏见，该策略在前一些缓解方法的性能上超越了很大的差距。我们的方法在推荐阶段使用normalized的项目嵌入来控制嵌入量的影响，这与项目受欢迎程度高度相关。通过广泛的实验，我们证明了我们的方法与采样的softmax损失相结合可以有效缓解受欢迎偏见，而且比前一些缓解方法高效得多。我们进一步调查用户和项目嵌入之间的关系，发现angular相似性可以分辨用户喜欢和不喜欢的项目，无论它们的受欢迎程度如何。这种分析解释了我们的方法在消除受欢迎偏见的机制。我们的代码可以在https://github.com/ml-postech/TTEN中找到。
</details></li>
</ul>
<hr>
<h2 id="FoX-Formation-aware-exploration-in-multi-agent-reinforcement-learning"><a href="#FoX-Formation-aware-exploration-in-multi-agent-reinforcement-learning" class="headerlink" title="FoX: Formation-aware exploration in multi-agent reinforcement learning"></a>FoX: Formation-aware exploration in multi-agent reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11272">http://arxiv.org/abs/2308.11272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghyeon Jo, Sunwoo Lee, Junghyuk Yum, Seungyul Han</li>
<li>for: 解决多智能体 MARL 中的探索问题，提高多智能体任务的可行性和稳定性。</li>
<li>methods: 定义基于形态关系的探索空间减少法，并提出基于形态意识的探索框架 FoX，使部分可见智能体更好地了解自己的当前形态。</li>
<li>results: 在 GRF 和 SMAC 任务上，提出的 FoX 框架与现状册列 MARL 算法相比，显著提高了多智能体任务的性能。<details>
<summary>Abstract</summary>
Recently, deep multi-agent reinforcement learning (MARL) has gained significant popularity due to its success in various cooperative multi-agent tasks. However, exploration still remains a challenging problem in MARL due to the partial observability of the agents and the exploration space that can grow exponentially as the number of agents increases. Firstly, in order to address the scalability issue of the exploration space, we define a formation-based equivalence relation on the exploration space and aim to reduce the search space by exploring only meaningful states in different formations. Then, we propose a novel formation-aware exploration (FoX) framework that encourages partially observable agents to visit the states in diverse formations by guiding them to be well aware of their current formation solely based on their own observations. Numerical results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC) tasks.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:近期，深度多智能游戏学习（MARL）获得了重要的 популяр性，因为它在多种合作多智能任务中取得了成功。然而，探索仍然是 MARL 中的挑战，因为智能机器人的partial observability和探索空间的增长速度可以 exponential with the number of agents。为了解决探索空间的扩展问题，我们定义了基于formation的等价关系在探索空间上，并且尝试将搜索空间减少到只包含有意义的状态。然后，我们提出了一种新的formation-aware探索（FoX）框架，该框架鼓励部分可见的智能机器人 посеща多种formation中的状态，基于它们自己的观察结果而不是全局视角。数值结果显示，我们的 FoX 框架在 Google Research Football (GRF) 和 sparse Starcraft II multi-agent challenge (SMAC) 任务上明显超过了现状MARL算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Inspired-Machine-Learning-a-Survey"><a href="#Quantum-Inspired-Machine-Learning-a-Survey" class="headerlink" title="Quantum-Inspired Machine Learning: a Survey"></a>Quantum-Inspired Machine Learning: a Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11269">http://arxiv.org/abs/2308.11269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Larry Huynh, Jin Hong, Ajmal Mian, Hajime Suzuki, Yanqiu Wu, Seyit Camtepe</li>
<li>For: This paper provides an integrated and comprehensive examination of Quantum-inspired Machine Learning (QiML), exploring its diverse research domains and recent advancements.* Methods: The paper uses a concrete definition of QiML, established by analyzing various prior interpretations of the term and their inherent ambiguities. It also showcases practical applications and illuminates potential future research avenues.* Results: The paper provides a holistic understanding of QiML’s current landscape and future directions, serving as a guide for researchers and practitioners alike.<details>
<summary>Abstract</summary>
Quantum-inspired Machine Learning (QiML) is a burgeoning field, receiving global attention from researchers for its potential to leverage principles of quantum mechanics within classical computational frameworks. However, current review literature often presents a superficial exploration of QiML, focusing instead on the broader Quantum Machine Learning (QML) field. In response to this gap, this survey provides an integrated and comprehensive examination of QiML, exploring QiML's diverse research domains including tensor network simulations, dequantized algorithms, and others, showcasing recent advancements, practical applications, and illuminating potential future research avenues. Further, a concrete definition of QiML is established by analyzing various prior interpretations of the term and their inherent ambiguities. As QiML continues to evolve, we anticipate a wealth of future developments drawing from quantum mechanics, quantum computing, and classical machine learning, enriching the field further. This survey serves as a guide for researchers and practitioners alike, providing a holistic understanding of QiML's current landscape and future directions.
</details>
<details>
<summary>摘要</summary>
量子逻辑机器学习（QiML）是一个迅速发展的领域，在全球的研究者中备受关注，因为它可以在类比量子机制的计算框架中利用量子力学原理。然而，现有的文献综述通常会对QiML进行 superficiel的探讨，而不是对量子机器学习（QML）领域进行深入的探讨。为了填补这一空白，本调查提供了一个综合和完整的QiML的检视，探讨了QiML的多样化研究领域，包括张量网络仿真、量子化算法等，介绍了最新的进步、实践应用以及未来研究方向。此外，本文还提供了QiML的具体定义，通过分析各种先前的定义和其内在的歧义，为读者提供一个准确的理解。随着QiML的进一步发展，我们预计未来将有许多基于量子力学、量子计算和经典机器学习的发展，使得QiML领域变得更加丰富。本调查作为研究者和实践者的指南，为读者提供了QiML当前的领域景观和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Minwise-Independent-Permutations-with-Insertion-and-Deletion-of-Features"><a href="#Minwise-Independent-Permutations-with-Insertion-and-Deletion-of-Features" class="headerlink" title="Minwise-Independent Permutations with Insertion and Deletion of Features"></a>Minwise-Independent Permutations with Insertion and Deletion of Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11240">http://arxiv.org/abs/2308.11240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rameshwar Pratap, Raghav Kulkarni</li>
<li>for: 本文研究的目的是提出一种能够适应高维数据动态特征添加和删除的MINHash算法，以提高MINHash的运行效率和精度。</li>
<li>methods: 本文提出了一种基于批处理和快速搜索的方法，使MINHash sketches可以适应动态添加和删除特征。此外，本文还提供了一种基于哈希函数的方法，可以快速地更新MINHash sketches。</li>
<li>results: 本文通过了严格的理论分析和广泛的实验研究，证明了提出的方法可以快速地更新MINHash sketches，同时保持与静态MINHash的性能相似。实验结果表明，在几个实际 datasets 上，提出的方法可以获得显著的运行时间加速，而无需重新生成随机排序。<details>
<summary>Abstract</summary>
In their seminal work, Broder \textit{et. al.}~\citep{BroderCFM98} introduces the $\mathrm{minHash}$ algorithm that computes a low-dimensional sketch of high-dimensional binary data that closely approximates pairwise Jaccard similarity. Since its invention, $\mathrm{minHash}$ has been commonly used by practitioners in various big data applications. Further, the data is dynamic in many real-life scenarios, and their feature sets evolve over time. We consider the case when features are dynamically inserted and deleted in the dataset. We note that a naive solution to this problem is to repeatedly recompute $\mathrm{minHash}$ with respect to the updated dimension. However, this is an expensive task as it requires generating fresh random permutations. To the best of our knowledge, no systematic study of $\mathrm{minHash}$ is recorded in the context of dynamic insertion and deletion of features. In this work, we initiate this study and suggest algorithms that make the $\mathrm{minHash}$ sketches adaptable to the dynamic insertion and deletion of features. We show a rigorous theoretical analysis of our algorithms and complement it with extensive experiments on several real-world datasets. Empirically we observe a significant speed-up in the running time while simultaneously offering comparable performance with respect to running $\mathrm{minHash}$ from scratch. Our proposal is efficient, accurate, and easy to implement in practice.
</details>
<details>
<summary>摘要</summary>
布罗德等人在 seminal 论文中提出了 $\minHash$ 算法，该算法可以计算高维二分数据的低维笔记，并且可以准确地近似对应对 Jaccard 相似性。自其发明以来， $\minHash$ 已经广泛地应用于各种大数据应用场景中。然而，在许多实际场景中，数据的特征集是动态变化的，因此需要对数据进行不断的更新。在这种情况下，直接重新计算 $\minHash$ 是一个昂贵的任务，因为需要生成新的随机排序。在这篇文章中，我们开始了对 $\minHash$ 在动态插入和删除特征时的研究。我们提出了一些可靠的算法，使得 $\minHash$ 笔记可以适应动态插入和删除特征。我们提供了严谨的理论分析，并补充了大量实验数据，证明了我们的方法可以减少运行时间，同时保持与从头开始计算 $\minHash$ 的性能相似。我们的提议是高效、准确、易于实现。
</details></li>
</ul>
<hr>
<h2 id="Hamiltonian-GAN"><a href="#Hamiltonian-GAN" class="headerlink" title="Hamiltonian GAN"></a>Hamiltonian GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11216">http://arxiv.org/abs/2308.11216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koritsky/hamiltonian_learning">https://github.com/koritsky/hamiltonian_learning</a></li>
<li>paper_authors: Christine Allen-Blanchette</li>
<li>for: 这个论文是为了探讨使用泊松方程来学习视频生成的可能性。</li>
<li>methods: 该论文使用了一种基于GAN的视频生成管道，其中包含一个学习的配置空间地图和泊松 нейрон网络运动模型，以从数据中学习配置空间的表示。</li>
<li>results: 该论文通过使用一种物理启发的循环坐标损失函数，以提高模型的解释性和可靠性。并在Hamiltonian Dynamics Suite Toy Physics dataset上进行了证明。<details>
<summary>Abstract</summary>
A growing body of work leverages the Hamiltonian formalism as an inductive bias for physically plausible neural network based video generation. The structure of the Hamiltonian ensures conservation of a learned quantity (e.g., energy) and imposes a phase-space interpretation on the low-dimensional manifold underlying the input video. While this interpretation has the potential to facilitate the integration of learned representations in downstream tasks, existing methods are limited in their applicability as they require a structural prior for the configuration space at design time. In this work, we present a GAN-based video generation pipeline with a learned configuration space map and Hamiltonian neural network motion model, to learn a representation of the configuration space from data. We train our model with a physics-inspired cyclic-coordinate loss function which encourages a minimal representation of the configuration space and improves interpretability. We demonstrate the efficacy and advantages of our approach on the Hamiltonian Dynamics Suite Toy Physics dataset.
</details>
<details>
<summary>摘要</summary>
一种增长的研究利用 Hamiltonian  formalism 作为 физи学 plausible 的神经网络视频生成的 inductive bias。 Hamiltonian 的结构保证学习的一个量（例如，能量）的保守和在输入视频的低维度抽象空间中强制phase-space interpretation。这种解释有助于在下游任务中集成学习的表示，但现有方法受限于需要在设计时提供结构的 prior  для配置空间。在这篇文章中，我们提出了一个基于 GAN 的视频生成管道，其中包含学习的配置空间地图和 Hamiltonian 神经网络动力学模型，以从数据中学习配置空间的表示。我们使用一种 физи学 inspires 的循环坐标损失函数来驱动我们的模型，该函数鼓励了配置空间的最小表示，并提高了解释性。我们在 Hamiltonian Dynamics Suite Toy Physics 数据集上证明了我们的方法的有效性和优势。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Framework-for-Multi-mode-Spatial-Temporal-Data-Modeling"><a href="#A-Simple-Framework-for-Multi-mode-Spatial-Temporal-Data-Modeling" class="headerlink" title="A Simple Framework for Multi-mode Spatial-Temporal Data Modeling"></a>A Simple Framework for Multi-mode Spatial-Temporal Data Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11204">http://arxiv.org/abs/2308.11204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lzhmarkk/simmst">https://github.com/lzhmarkk/simmst</a></li>
<li>paper_authors: Zihang Liu, Le Yu, Tongyu Zhu, Leiei Sun</li>
<li>for: 本研究旨在提出一种简单 yet effective 的多模式空间时间数据模型，以捕捉多种空间关系和时间依赖关系。</li>
<li>methods: 我们提出了一种通用的cross-mode空间关系学习组件，可以适应多种模式之间的连接，并在这些连接上传递信息。此外，我们采用多层感知器来捕捉时间依赖关系和通道相关性。</li>
<li>results: 我们在三个实际数据集上进行了实验，结果显示，我们的模型可以在空间和时间复杂度下 consistently 高于基elines，并且具有更好的一致性和泛化能力。<details>
<summary>Abstract</summary>
Spatial-temporal data modeling aims to mine the underlying spatial relationships and temporal dependencies of objects in a system. However, most existing methods focus on the modeling of spatial-temporal data in a single mode, lacking the understanding of multiple modes. Though very few methods have been presented to learn the multi-mode relationships recently, they are built on complicated components with higher model complexities. In this paper, we propose a simple framework for multi-mode spatial-temporal data modeling to bring both effectiveness and efficiency together. Specifically, we design a general cross-mode spatial relationships learning component to adaptively establish connections between multiple modes and propagate information along the learned connections. Moreover, we employ multi-layer perceptrons to capture the temporal dependencies and channel correlations, which are conceptually and technically succinct. Experiments on three real-world datasets show that our model can consistently outperform the baselines with lower space and time complexity, opening up a promising direction for modeling spatial-temporal data. The generalizability of the cross-mode spatial relationships learning module is also validated.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SegRNN-Segment-Recurrent-Neural-Network-for-Long-Term-Time-Series-Forecasting"><a href="#SegRNN-Segment-Recurrent-Neural-Network-for-Long-Term-Time-Series-Forecasting" class="headerlink" title="SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting"></a>SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11200">http://arxiv.org/abs/2308.11200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengsheng Lin, Weiwei Lin, Wentai Wu, Feiyu Zhao, Ruichao Mo, Haotong Zhang</li>
<li>for: 这个研究是为了解决长期时间序列预测（LTSF）领域中RNN方法面临的挑战，特别是在处理极长回顾窗口和预测时间 horizon时。</li>
<li>methods: 本研究提出了两种新的策略来减少RNN在LTSF任务中的迭代次数：Segment-wise Iterations和Parallel Multi-step Forecasting（PMF）。</li>
<li>results: 实验结果显示，SegRNN不仅能够超越现有的Transformer-based模型，而且能够降低运行时间和内存使用量超过78%。这些成果证明RNN仍然在LTSF任务中 excel，并且鼓励更多的RNN-based方法在这个领域进行更多的探索。<details>
<summary>Abstract</summary>
RNN-based methods have faced challenges in the Long-term Time Series Forecasting (LTSF) domain when dealing with excessively long look-back windows and forecast horizons. Consequently, the dominance in this domain has shifted towards Transformer, MLP, and CNN approaches. The substantial number of recurrent iterations are the fundamental reasons behind the limitations of RNNs in LTSF. To address these issues, we propose two novel strategies to reduce the number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies, namely SegRNN, significantly reduce the required recurrent iterations for LTSF, resulting in notable improvements in forecast accuracy and inference speed. Extensive experiments demonstrate that SegRNN not only outperforms SOTA Transformer-based models but also reduces runtime and memory usage by more than 78%. These achievements provide strong evidence that RNNs continue to excel in LTSF tasks and encourage further exploration of this domain with more RNN-based approaches. The source code is coming soon.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Toward-Generalizable-Machine-Learning-Models-in-Speech-Language-and-Hearing-Sciences-Power-Analysis-and-Sample-Size-Estimation"><a href="#Toward-Generalizable-Machine-Learning-Models-in-Speech-Language-and-Hearing-Sciences-Power-Analysis-and-Sample-Size-Estimation" class="headerlink" title="Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation"></a>Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11197">http://arxiv.org/abs/2308.11197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamzeh Ghasemzadeh, Robert E. Hillman, Daryush D. Mehta</li>
<li>For: The paper aims to provide quantitative evidence to incentivize researchers to use the more robust method of nested cross-validation in machine learning-based analysis, and to present methods and MATLAB codes for power analysis during the design of a study.* Methods: The paper uses Monte Carlo simulations to compare the performance of four different cross-validation methods (single holdout, 10-fold, train-validation-test, and nested 10-fold) in terms of statistical power and statistical confidence.* Results: The paper finds that the nested 10-fold cross-validation method results in the highest statistical confidence and statistical power, while providing an unbiased estimate of the accuracy. The required sample size with a single holdout is found to be 50% higher than what would be needed with nested cross-validation, and confidence in the model based on nested cross-validation is found to be as much as four times higher than the confidence in the single holdout-based model.<details>
<summary>Abstract</summary>
This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust method of nested cross-validation. The second purpose is to present methods and MATLAB codes for doing power analysis for ML-based analysis during the design of a study. Monte Carlo simulations were used to quantify the interactions between the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, and the dimensionality of the model. Four different cross-validations (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and statistical confidence of the ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome ({\alpha}=0.05, 1-\b{eta}=0.8). Statistical confidence of the model was defined as the probability of correct features being selected and hence being included in the final model. Our analysis showed that the model generated based on the single holdout method had very low statistical power and statistical confidence and that it significantly overestimated the accuracy. Conversely, the nested 10-fold cross-validation resulted in the highest statistical confidence and the highest statistical power, while providing an unbiased estimate of the accuracy. The required sample size with a single holdout could be 50% higher than what would be needed if nested cross-validation were used. Confidence in the model based on nested cross-validation was as much as four times higher than the confidence in the single holdout-based model. A computational model, MATLAB codes, and lookup tables are provided to assist researchers with estimating the sample size during the design of their future studies.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这项研究的主要目标是提供量化证据，以便激励研究人员使用更加稳定的嵌套十进制验证方法。第二个目标是提供方法和MATLAB代码，以便在设计研究时进行能量分析。 Monte Carlo仿真被用来比较不同的十进制验证方法（单个弃置、10进制、训练验证测试和嵌套10进制）的统计能力和统计信任度。研究发现，使用嵌套10进制验证方法可以获得最高的统计信任度和统计能力，同时提供不偏估的准确率估计。单个弃置方法的模型有very low的统计信任度和统计能力，并且显著地过分估计准确率。相比之下，嵌套10进制验证方法可以降低样本大小，同时提供更高的统计信任度和统计能力。研究提供了一个计算模型、MATLAB代码和lookup表来帮助研究人员在设计未来的研究时估计样本大小。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Task-Parallelization-of-Dataflow-Graphs-in-ML-DL-models"><a href="#Automatic-Task-Parallelization-of-Dataflow-Graphs-in-ML-DL-models" class="headerlink" title="Automatic Task Parallelization of Dataflow Graphs in ML&#x2F;DL models"></a>Automatic Task Parallelization of Dataflow Graphs in ML&#x2F;DL models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11192">http://arxiv.org/abs/2308.11192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srinjoy Das, Lawrence Rauchwerger</li>
<li>for: 提高机器学习（ML）或深度学习（DL）模型的训练和推理性能</li>
<li>methods: 使用批处理、卷积和常量传递优化方法，并使用一新建的工具 called Ramiel 生成可读可执行的平行 PyTorch+Python 代码</li>
<li>results: 在多个 ML 图表中达到了最高的 1.9 倍的速度提升，并在编译和运行时间方面表现出色，超过了一些当前的机制，同时可以在资源和功能限制的设备上使用<details>
<summary>Abstract</summary>
Several methods exist today to accelerate Machine Learning(ML) or Deep-Learning(DL) model performance for training and inference. However, modern techniques that rely on various graph and operator parallelism methodologies rely on search space optimizations which are costly in terms of power and hardware usage. Especially in the case of inference, when the batch size is 1 and execution is on CPUs or for power-constrained edge devices, current techniques can become costly, complicated or inapplicable. To ameliorate this, we present a Critical-Path-based Linear Clustering approach to exploit inherent parallel paths in ML dataflow graphs. Our task parallelization approach further optimizes the structure of graphs via cloning and prunes them via constant propagation and dead-code elimination. Contrary to other work, we generate readable and executable parallel Pytorch+Python code from input ML models in ONNX format via a new tool that we have built called {\bf Ramiel}. This allows us to benefit from other downstream acceleration techniques like intra-op parallelism and potentially pipeline parallelism. Our preliminary results on several ML graphs demonstrate up to 1.9$\times$ speedup over serial execution and outperform some of the current mechanisms in both compile and runtimes. Lastly, our methods are lightweight and fast enough so that they can be used effectively for power and resource-constrained devices, while still enabling downstream optimizations.
</details>
<details>
<summary>摘要</summary>
Unlike other work, we generate readable and executable parallel PyTorch+Python code from input ML models in ONNX format via a new tool we have built called Ramiel. This allows us to benefit from other downstream acceleration techniques like intra-op parallelism and potentially pipeline parallelism. Our preliminary results on several ML graphs demonstrate up to 1.9 times speedup over serial execution and outperform some of the current mechanisms in both compile and runtimes. Lastly, our methods are lightweight and fast enough to be used effectively for power and resource-constrained devices, while still enabling downstream optimizations.Translated into Simplified Chinese:有很多方法可以加速机器学习（ML）或深度学习（DL）模型的训练和推理性能。然而，现代技术，它们基于各种图和运算符并行方法，它们需要搜索空间优化，这些优化对于硬件使用量和电力占用来说是昂贵的。尤其是在推理时，批处理数为1，执行在CPUs或有限功能边缘设备上时，当前技术可能变得昂贵、复杂或不可靠。为了改善这种情况，我们提出了一种基于执行路径的线性聚类方法，以利用ML数据流图中的自然并行路径。我们的任务并行方法还可以优化图的结构via副本和减少via常量卷积和死代码消除。与其他工作不同，我们可以将输入ML模型的ONNX格式转换为可读可执行的并行PyTorch+Python代码，通过我们新建的工具Ramiel。这样，我们就可以从其他下游加速技术，如内部操作并行和可能的管道并行中受益。我们的初步结果表明，对于一些ML图，我们可以在序列执行下获得最高达1.9倍的加速，并且超越一些当前机制。此外，我们的方法具有轻量级和快速的特点，可以有效地在功率和资源有限的设备上使用，而仍可以开启下游优化。
</details></li>
</ul>
<hr>
<h2 id="Mobility-Aware-Computation-Offloading-for-Swarm-Robotics-using-Deep-Reinforcement-Learning"><a href="#Mobility-Aware-Computation-Offloading-for-Swarm-Robotics-using-Deep-Reinforcement-Learning" class="headerlink" title="Mobility-Aware Computation Offloading for Swarm Robotics using Deep Reinforcement Learning"></a>Mobility-Aware Computation Offloading for Swarm Robotics using Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11154">http://arxiv.org/abs/2308.11154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiucheng Wang, Hongzhi Guo</li>
<li>for: 自动化废弃、危险、无聊任务</li>
<li>methods: 利用移动边计算减轻计算负担</li>
<li>results: 提出了一种基于移动端深度学习算法的计算调度和资源分配解决方案，可以 garantizar computation preciseness 并使用最少机器人能量。<details>
<summary>Abstract</summary>
Swarm robotics is envisioned to automate a large number of dirty, dangerous, and dull tasks. Robots have limited energy, computation capability, and communication resources. Therefore, current swarm robotics have a small number of robots, which can only provide limited spatio-temporal information. In this paper, we propose to leverage the mobile edge computing to alleviate the computation burden. We develop an effective solution based on a mobility-aware deep reinforcement learning model at the edge server side for computing scheduling and resource. Our results show that the proposed approach can meet delay requirements and guarantee computation precision by using minimum robot energy.
</details>
<details>
<summary>摘要</summary>
群体 робототехника预见到自动执行大量污秽危险讨厌任务。机器人有限的能源、计算能力和通信资源，因此当前的群体 робототехника只有一小数量的机器人，可以提供有限的空间时间信息。在这篇论文中，我们提议利用移动边缘计算来减轻计算负担。我们开发了一种基于移动性意识深度学习模型的边缘服务器端解决方案，以实现计算调度和资源管理。我们的结果表明，我们的方法可以遵循延迟要求，保证计算精度，并使最小化机器人能量消耗。
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-On-Board-Radio-Resource-Management-for-Satellite-Communications-via-Neuromorphic-Computing"><a href="#Energy-Efficient-On-Board-Radio-Resource-Management-for-Satellite-Communications-via-Neuromorphic-Computing" class="headerlink" title="Energy-Efficient On-Board Radio Resource Management for Satellite Communications via Neuromorphic Computing"></a>Energy-Efficient On-Board Radio Resource Management for Satellite Communications via Neuromorphic Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11152">http://arxiv.org/abs/2308.11152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Flor Ortiz, Nicolas Skatchkovsky, Eva Lagunas, Wallace A. Martins, Geoffrey Eappen, Saed Daoud, Osvaldo Simeone, Bipin Rajendran, Symeon Chatzinotas</li>
<li>for: 这个论文是为了研究使用能效的大脑发现学模型来管理空间通信系统中的电磁资源。</li>
<li>methods: 本论文使用了软件仿真和实验方法来评估提议的模型。在实验中，使用了最新的英特尔洛瑞2芯片。</li>
<li>results: 研究发现，使用神经元发现学模型可以提高准确率和能效性，而且可以降低电力消耗。相比传统的卷积神经网络，使用神经元发现学模型可以降低电力消耗超过100倍。这些结果表明，使用神经元发现学模型可以在空间通信系统中提供更高的效率和可持续性。<details>
<summary>Abstract</summary>
The latest satellite communication (SatCom) missions are characterized by a fully reconfigurable on-board software-defined payload, capable of adapting radio resources to the temporal and spatial variations of the system traffic. As pure optimization-based solutions have shown to be computationally tedious and to lack flexibility, machine learning (ML)-based methods have emerged as promising alternatives. We investigate the application of energy-efficient brain-inspired ML models for on-board radio resource management. Apart from software simulation, we report extensive experimental results leveraging the recently released Intel Loihi 2 chip. To benchmark the performance of the proposed model, we implement conventional convolutional neural networks (CNN) on a Xilinx Versal VCK5000, and provide a detailed comparison of accuracy, precision, recall, and energy efficiency for different traffic demands. Most notably, for relevant workloads, spiking neural networks (SNNs) implemented on Loihi 2 yield higher accuracy, while reducing power consumption by more than 100$\times$ as compared to the CNN-based reference platform. Our findings point to the significant potential of neuromorphic computing and SNNs in supporting on-board SatCom operations, paving the way for enhanced efficiency and sustainability in future SatCom systems.
</details>
<details>
<summary>摘要</summary>
最新的卫星通信（SatCom）任务 caracterized by a fully reconfigurable on-board software-defined payload, capable of adapting radio resources to the temporal and spatial variations of the system traffic. As pure optimization-based solutions have shown to be computationally tedious and to lack flexibility, machine learning (ML)-based methods have emerged as promising alternatives. We investigate the application of energy-efficient brain-inspired ML models for on-board radio resource management. Apart from software simulation, we report extensive experimental results leveraging the recently released Intel Loihi 2 chip. To benchmark the performance of the proposed model, we implement conventional convolutional neural networks (CNN) on a Xilinx Versal VCK5000, and provide a detailed comparison of accuracy, precision, recall, and energy efficiency for different traffic demands. Most notably, for relevant workloads, spiking neural networks (SNNs) implemented on Loihi 2 yield higher accuracy, while reducing power consumption by more than 100 times as compared to the CNN-based reference platform. Our findings point to the significant potential of neuromorphic computing and SNNs in supporting on-board SatCom operations, paving the way for enhanced efficiency and sustainability in future SatCom systems.
</details></li>
</ul>
<hr>
<h2 id="Graph-Encoding-and-Neural-Network-Approaches-for-Volleyball-Analytics-From-Game-Outcome-to-Individual-Play-Predictions"><a href="#Graph-Encoding-and-Neural-Network-Approaches-for-Volleyball-Analytics-From-Game-Outcome-to-Individual-Play-Predictions" class="headerlink" title="Graph Encoding and Neural Network Approaches for Volleyball Analytics: From Game Outcome to Individual Play Predictions"></a>Graph Encoding and Neural Network Approaches for Volleyball Analytics: From Game Outcome to Individual Play Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11142">http://arxiv.org/abs/2308.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rhys Tracy, Haotian Xia, Alex Rasla, Yuan-Fang Wang, Ambuj Singh</li>
<li>For: 本研究旨在提高复杂排球预测精度，为教练和运动员提供更有意义的洞察。* Methods: 我们引入特殊的图编码技术，将已有的排球数据集添加了更多的接触接触排球上下文。我们使用图神经网络（GNNs）进行三个不同的排球预测任务的预测：排球结果预测、场地预测和击TYPE预测。* Results: 我们的图基模型比基线模型表现更出色，特别是在 removing blocked hits 这个简单的调整下。我们还发现，选择合适的模型结构可以更好地提取重要信息，从而提高预测结果。总的来说，我们的研究展示了使用图编码在体育数据分析中的潜在优势和局限性，并希望未来的机器学习策略在体育和应用程序中使用图基编码。<details>
<summary>Abstract</summary>
This research aims to improve the accuracy of complex volleyball predictions and provide more meaningful insights to coaches and players. We introduce a specialized graph encoding technique to add additional contact-by-contact volleyball context to an already available volleyball dataset without any additional data gathering. We demonstrate the potential benefits of using graph neural networks (GNNs) on this enriched dataset for three different volleyball prediction tasks: rally outcome prediction, set location prediction, and hit type prediction. We compare the performance of our graph-based models to baseline models and analyze the results to better understand the underlying relationships in a volleyball rally. Our results show that the use of GNNs with our graph encoding yields a much more advanced analysis of the data, which noticeably improves prediction results overall. We also show that these baseline tasks can be significantly improved with simple adjustments, such as removing blocked hits. Lastly, we demonstrate the importance of choosing a model architecture that will better extract the important information for a certain task. Overall, our study showcases the potential strengths and weaknesses of using graph encodings in sports data analytics and hopefully will inspire future improvements in machine learning strategies across sports and applications by using graphbased encodings.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文翻译，不同于正式中文）这个研究的目标是提高复杂的排球预测精度，为教练和运动员提供更有意义的洞察。我们引入了一种特殊的图编码技术，将已有的排球数据集添加了更多的接触点接触之间的排球上下文。我们示出了使用图神经网络（GNNs）在这些丰富的数据集上进行三种不同的排球预测任务：赛点预测、场地预测和击打类型预测。我们对基线模型和我们的图基于模型进行比较，分析结果，以更好地理解排球赛中的下面关系。我们的结果显示，使用GNNs和我们的图编码可以提供更高级的数据分析，全面提高预测结果。我们还示出了一些简单的调整，如移除堵塞的击球，可以大幅提高基线任务的性能。最后，我们证明选择适合的模型架构可以更好地提取关键信息，以便更好地进行特定任务。总之，我们的研究展示了使用图编码在体育数据分析中的潜在优势和缺点，希望这些研究结果可以激励未来在体育和应用领域的机器学习策略的改进。
</details></li>
</ul>
<hr>
<h2 id="Towards-Validating-Long-Term-User-Feedbacks-in-Interactive-Recommendation-Systems"><a href="#Towards-Validating-Long-Term-User-Feedbacks-in-Interactive-Recommendation-Systems" class="headerlink" title="Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems"></a>Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11137">http://arxiv.org/abs/2308.11137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Hojoon Lee, Dongyoon Hwang, Kyushik Min, Jaegul Choo</li>
<li>for: 这paper的目的是检验RL算法在Interactive Recommender Systems (IRSs) 中的性能，以及公共数据集是否适合评估IRS算法。</li>
<li>methods: 该paper使用了Reinforcement Learning (RL)算法，并 compare它们与简单的奖励模型。</li>
<li>results: 主要发现结果包括：1) 简单的奖励模型在 maximizing 累积奖励方面一直处于领先地位; 2) 应用更高的长期奖励权重会导致推荐性能下降; 3) 用户反馈对 benchmark datasets 中的长期影响很小。<details>
<summary>Abstract</summary>
Interactive Recommender Systems (IRSs) have attracted a lot of attention, due to their ability to model interactive processes between users and recommender systems. Numerous approaches have adopted Reinforcement Learning (RL) algorithms, as these can directly maximize users' cumulative rewards. In IRS, researchers commonly utilize publicly available review datasets to compare and evaluate algorithms. However, user feedback provided in public datasets merely includes instant responses (e.g., a rating), with no inclusion of delayed responses (e.g., the dwell time and the lifetime value). Thus, the question remains whether these review datasets are an appropriate choice to evaluate the long-term effects of the IRS. In this work, we revisited experiments on IRS with review datasets and compared RL-based models with a simple reward model that greedily recommends the item with the highest one-step reward. Following extensive analysis, we can reveal three main findings: First, a simple greedy reward model consistently outperforms RL-based models in maximizing cumulative rewards. Second, applying higher weighting to long-term rewards leads to a degradation of recommendation performance. Third, user feedbacks have mere long-term effects on the benchmark datasets. Based on our findings, we conclude that a dataset has to be carefully verified and that a simple greedy baseline should be included for a proper evaluation of RL-based IRS approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>使用简单的奖励模型可以在总奖励方面 consistently 高于 RL 基eline。2. 对 RL 模型应用更高的长期奖励权重会导致推荐性能下降。3. 在 benchmark 数据集上，用户反馈的长期效果很少。根据我们的发现，我们结论是一个数据集需要仔细验证，而且一个简单的奖励基eline应该包含在评估 RL 基eline 方法的评估中。</details></li>
</ol>
<hr>
<h2 id="How-Expressive-are-Graph-Neural-Networks-in-Recommendation"><a href="#How-Expressive-are-Graph-Neural-Networks-in-Recommendation" class="headerlink" title="How Expressive are Graph Neural Networks in Recommendation?"></a>How Expressive are Graph Neural Networks in Recommendation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11127">http://arxiv.org/abs/2308.11127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkuds/gte">https://github.com/hkuds/gte</a></li>
<li>paper_authors: Xuheng Cai, Lianghao Xia, Xubin Ren, Chao Huang</li>
<li>for: 本研究旨在提供对graph neural networks（GNNs）在推荐任务中的理论分析，包括GNNs的表达能力、难度和可解性。</li>
<li>methods: 本研究使用了message passing GNNs和random node initialization来评估GNNs的表达能力，并提出了一种新的表达能力度量—— topological closeness，用于评估GNNs在推荐任务中的能力。</li>
<li>results: 研究发现，GNNs在推荐任务中的表达能力可以通过三种度量来评估：graph isomorphism（图像同构）、node automorphism（节点自同构）和topological closeness（结构距离）。其中，topological closeness度量能够更好地评估GNNs在推荐任务中的能力，并且可以与node-level度量相比较。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in recommendation, considering three levels of expressiveness metrics: graph isomorphism (graph-level), node automorphism (node-level), and topological closeness (link-level). We propose the topological closeness metric to evaluate GNNs' ability to capture the structural distance between nodes, which aligns closely with the objective of recommendation. To validate the effectiveness of this new metric in evaluating recommendation performance, we introduce a learning-less GNN algorithm that is optimal on the new metric and can be optimal on the node-level metric with suitable modification. We conduct extensive experiments comparing the proposed algorithm against various types of state-of-the-art GNN models to explore the explainability of the new metric in the recommendation task. For reproducibility, implementation codes are available at https://github.com/HKUDS/GTE.
</details>
<details>
<summary>摘要</summary>
格Nodes Neural Networks (GNNs) 在不同的图学任务中表现出色，包括推荐，其利用用户ITEM的协同过滤信号在图中。然而，GNNs的理论表述仍然缺乏，即使它们在现有的推荐模型中有较高的实际效果。最近，研究人员已经开始探讨GNNs的表达能力，并证明了message passing GNNs 是 Weisfeiler-Lehman test 的最多Equivalent，并且GNNs 与随机节点初始化相结合是 universal。然而，GNNs 的“表达能力”概念仍然具有模糊的定义。大多数现有的工作采用图 isomorphism test 作为表达能力的度量，但这可能并不是在推荐任务中效果最高的度量，因为推荐任务的目标是分辨不同的节点之间的邻域关系。在这篇论文中，我们提供了对GNNs在推荐任务中的表达能力进行了全面的理论分析，考虑了三级别的表达能力度量：图 isomorphism（图级）、节点自动机制（节点级）和topological closeness（链级）。我们提出了topological closeness度量，用于评估GNNs在推荐任务中 capture 节点之间的结构距离，这与推荐任务的目标吻合。为验证这种新的度量在推荐任务中的效果，我们提出了一种不含学习的GNN算法，该算法在新的度量上是最优的，并且可以通过修改而在节点级度量上达到最优性。我们在多种现有的state-of-the-art GNN模型的比较中进行了广泛的实验，以探索这种新的度量在推荐任务中的解释性。为保持可重现性，我们在 GitHub 上提供了实现代码。
</details></li>
</ul>
<hr>
<h2 id="Explicability-and-Inexplicability-in-the-Interpretation-of-Quantum-Neural-Networks"><a href="#Explicability-and-Inexplicability-in-the-Interpretation-of-Quantum-Neural-Networks" class="headerlink" title="Explicability and Inexplicability in the Interpretation of Quantum Neural Networks"></a>Explicability and Inexplicability in the Interpretation of Quantum Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11098">http://arxiv.org/abs/2308.11098</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lirandepira/interpret-qnn">https://github.com/lirandepira/interpret-qnn</a></li>
<li>paper_authors: Lirandë Pira, Chris Ferrie</li>
<li>for: 研究人工智能（AI）方法的可解释性，尤其是深度神经网络，因为AI后台系统的使用很普遍，但它们的行为往往无法解释。</li>
<li>methods: 使用本地模型独立可解释性测试方法来评估Quantum和Classical神经网络的可解释性。</li>
<li>results: 发现Quantum神经网络的可解释性遭受困难，尤其是在数据样本的很多情况下，这些样本被称为“不可解释的带”（band of inexplicability）。这些结果表明了如何建立可靠和负责任的量子AI模型。<details>
<summary>Abstract</summary>
Interpretability of artificial intelligence (AI) methods, particularly deep neural networks, is of great interest due to the widespread use of AI-backed systems, which often have unexplainable behavior. The interpretability of such models is a crucial component of building trusted systems. Many methods exist to approach this problem, but they do not obviously generalize to the quantum setting. Here we explore the interpretability of quantum neural networks using local model-agnostic interpretability measures of quantum and classical neural networks. We introduce the concept of the band of inexplicability, representing the interpretable region in which data samples have no explanation, likely victims of inherently random quantum measurements. We see this as a step toward understanding how to build responsible and accountable quantum AI models.
</details>
<details>
<summary>摘要</summary>
artifical intelligence（AI）方法的解释性（interpretability）是非常有价值的，因为AI支持的系统在使用中经常表现出不可解释的行为。解释性是建立可信系统的重要组成部分。许多方法可以解决这个问题，但它们不明显推广到量子设置。在这里，我们研究了量子神经网络的解释性，使用本地模型独立可解释性度量来评估量子和类传统神经网络的解释性。我们引入了带彩色不可解释区域的概念，表示无法解释的数据样本， probable victims of inherently random quantum measurements。我们认为这是一步 towards understanding how to build responsible and accountable quantum AI models。Here's the breakdown of the translation:*  artifical intelligence (AI) 是指人工智能的全称，用于描述使用机器学习和深度学习等技术实现的智能系统。* 解释性 (interpretability) 是指可以理解和解释的程度，在这种情况下是指可以理解和解释的AI模型。* 量子设置 (quantum setting) 是指使用量子计算机和量子信息处理技术实现的系统。* 本地模型独立可解释性度量 (local model-agnostic interpretability measures) 是指可以评估不同模型的解释性的度量，不同于特定的模型或算法。* 带彩色不可解释区域 (band of inexplicability) 是指无法解释的数据样本， probable victims of inherently random quantum measurements。这个概念是在量子神经网络中引入的，表示在量子计算中，由于随机性的原因，一些数据样本无法被解释。* responsible and accountable quantum AI models (responsible and accountable quantum AI models) 是指可以被信任和负责的量子AI模型，即可以被解释和控制的模型。
</details></li>
</ul>
<hr>
<h2 id="Stress-representations-for-tensor-basis-neural-networks-alternative-formulations-to-Finger-Rivlin-Ericksen"><a href="#Stress-representations-for-tensor-basis-neural-networks-alternative-formulations-to-Finger-Rivlin-Ericksen" class="headerlink" title="Stress representations for tensor basis neural networks: alternative formulations to Finger-Rivlin-Ericksen"></a>Stress representations for tensor basis neural networks: alternative formulations to Finger-Rivlin-Ericksen</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11080">http://arxiv.org/abs/2308.11080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan N. Fuhg, Nikolaos Bouklas, Reese E. Jones</li>
<li>for: 这个论文主要研究了数据驱动的 constitutive 模型框架，以及其中的神经网络和经典表述定理的结合使用。</li>
<li>methods: 这个论文使用了各种神经网络模型来模拟弹性材料的弹性行为，包括一些尚未探讨过的表述形式。它们使用了同等的 invariants 和生成器来替代经典的 Rivlin 和 Ericksen 表述。</li>
<li>results: 论文测试了九种不同的变体，并对三种不同的材料进行了测试。结果表明，各种表述形式之间存在差异，而且具有不同的优劣点。<details>
<summary>Abstract</summary>
Data-driven constitutive modeling frameworks based on neural networks and classical representation theorems have recently gained considerable attention due to their ability to easily incorporate constitutive constraints and their excellent generalization performance. In these models, the stress prediction follows from a linear combination of invariant-dependent coefficient functions and known tensor basis generators. However, thus far the formulations have been limited to stress representations based on the classical Rivlin and Ericksen form, while the performance of alternative representations has yet to be investigated. In this work, we survey a variety of tensor basis neural network models for modeling hyperelastic materials in a finite deformation context, including a number of so far unexplored formulations which use theoretically equivalent invariants and generators to Finger-Rivlin-Ericksen. Furthermore, we compare potential-based and coefficient-based approaches, as well as different calibration techniques. Nine variants are tested against both noisy and noiseless datasets for three different materials. Theoretical and practical insights into the performance of each formulation are given.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate(given_text, to='zh-CN', language='zh-CN')</SYS>>Here's the translated text: Data-driven constitutive modeling frameworks based on neural networks and classical representation theorems have recently gained considerable attention due to their ability to easily incorporate constitutive constraints and their excellent generalization performance. In these models, the stress prediction follows from a linear combination of invariant-dependent coefficient functions and known tensor basis generators. However, thus far the formulations have been limited to stress representations based on the classical Rivlin and Ericksen form, while the performance of alternative representations has yet to be investigated. In this work, we survey a variety of tensor basis neural network models for modeling hyperelastic materials in a finite deformation context, including a number of so far unexplored formulations which use theoretically equivalent invariants and generators to Finger-Rivlin-Ericksen. Furthermore, we compare potential-based and coefficient-based approaches, as well as different calibration techniques. Nine variants are tested against both noisy and noiseless datasets for three different materials. Theoretical and practical insights into the performance of each formulation are given.Here's the translation in Traditional Chinese:<<SYS>>translate(given_text, to='zh-TW', language='zh-TW')</SYS>>Here's the translated text:Data-driven constitutive modeling frameworks based on neural networks and classical representation theorems have recently gained considerable attention due to their ability to easily incorporate constitutive constraints and their excellent generalization performance. In these models, the stress prediction follows from a linear combination of invariant-dependent coefficient functions and known tensor basis generators. However, thus far the formulations have been limited to stress representations based on the classical Rivlin and Ericksen form, while the performance of alternative representations has yet to be investigated. In this work, we survey a variety of tensor basis neural network models for modeling hyperelastic materials in a finite deformation context, including a number of so far unexplored formulations which use theoretically equivalent invariants and generators to Finger-Rivlin-Ericksen. Furthermore, we compare potential-based and coefficient-based approaches, as well as different calibration techniques. Nine variants are tested against both noisy and noiseless datasets for three different materials. Theoretical and practical insights into the performance of each formulation are given.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Dive-into-the-Connections-Between-the-Renormalization-Group-and-Deep-Learning-in-the-Ising-Model"><a href="#A-Deep-Dive-into-the-Connections-Between-the-Renormalization-Group-and-Deep-Learning-in-the-Ising-Model" class="headerlink" title="A Deep Dive into the Connections Between the Renormalization Group and Deep Learning in the Ising Model"></a>A Deep Dive into the Connections Between the Renormalization Group and Deep Learning in the Ising Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11075">http://arxiv.org/abs/2308.11075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kelsie Taylor</li>
<li>for: 这个论文的目的是探讨深度学习和 renormalization group（RG）之间的联系，以及深度学习是否可以被视为 RG 流。</li>
<li>methods: 作者使用了 Restricted Boltzmann Machines（RBMs）进行深度学习，并开发了一系列的 renormalization 技术来对 1D 和 2D Ising 模型进行比较。</li>
<li>results: 作者成功地使用 Adam 优化器和 correlation length 损失函数来学习 RG 流，并对 2D Ising 模型进行了验证。他们还发现了层次结构的学习，这与 RG 的层次结构有相似之处。但是，对于 simplest 的 nearest-neighbor Ising 模型，作者发现了量化上的不一致。<details>
<summary>Abstract</summary>
The renormalization group (RG) is an essential technique in statistical physics and quantum field theory, which considers scale-invariant properties of physical theories and how these theories' parameters change with scaling. Deep learning is a powerful computational technique that uses multi-layered neural networks to solve a myriad of complicated problems. Previous research suggests the possibility that unsupervised deep learning may be a form of RG flow, by being a layer-by-layer coarse graining of the original data. We examined this connection on a more rigorous basis for the simple example of Kadanoff block renormalization of the 2D nearest-neighbor Ising model, with our deep learning accomplished via Restricted Boltzmann Machines (RBMs). We developed extensive renormalization techniques for the 1D and 2D Ising model to provide a baseline for comparison. For the 1D Ising model, we successfully used Adam optimization on a correlation length loss function to learn the group flow, yielding results consistent with the analytical model for infinite N. For the 2D Ising model, we successfully generated Ising model samples using the Wolff algorithm, and performed the group flow using a quasi-deterministic method, validating these results by calculating the critical exponent \nu. We then examined RBM learning of the Ising model layer by layer, finding a blocking structure in the learning that is qualitatively similar to RG. Lastly, we directly compared the weights of each layer from the learning to Ising spin renormalization, but found quantitative inconsistencies for the simple case of nearest-neighbor Ising models.
</details>
<details>
<summary>摘要</summary>
“ renormalization group（RG）是物理学和量子场论中的一种重要技术，它考虑物理理论中的尺度无关性和参数如何随缩放变化。深度学习是一种强大的计算技术，使用多层神经网络解决了许多复杂的问题。之前的研究表明，无监督的深度学习可能是一种RG流，通过层次的粗化来描述原始数据。我们在更加严格的基础上进行了这种连接的研究，使用了Restricted Boltzmann Machines（RBMs）来实现深度学习。我们还开发了对1D和2D Ising模型的广泛的renoormalization技术，以提供一个基准 для比较。对于1D Ising模型，我们使用Adam优化器和尺度长度损失函数来学习群流，得到了与分析模型相符的结果。对于2D Ising模型，我们使用了Wolff算法生成样本，并使用 quasi-deterministic方法进行群流，并计算了扩散 exponent ν。然后，我们查看了RBM学习中每层的权重和Ising模型的renoormalization，发现了一种块结构，与RG具有相似的特征。最后，我们直接比较了每层权重的 weights 和Ising spin renormalization，发现了简单的 nearest-neighbor Ising 模型中的量化差异。”
</details></li>
</ul>
<hr>
<h2 id="Ultra-Dual-Path-Compression-For-Joint-Echo-Cancellation-And-Noise-Suppression"><a href="#Ultra-Dual-Path-Compression-For-Joint-Echo-Cancellation-And-Noise-Suppression" class="headerlink" title="Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression"></a>Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11053">http://arxiv.org/abs/2308.11053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangting Chen, Jianwei Yu, Yi Luo, Rongzhi Gu, Weihua Li, Zhuocheng Lu, Chao Weng</li>
<li>for: 提高全双工通信中的干扰抑制和听频补做的灵活性和效率。</li>
<li>methods: 提出时空双路压缩法，通过可调 filters 实现维度减少，并通过后处理网络实现全序数据模型化来解决时间压缩导致的性能下降。</li>
<li>results: 实现了各种压缩比例（4x-32x）的灵活性，并与 FastFullSubNet 和 DeepFilterNet 的表现竞争。<details>
<summary>Abstract</summary>
Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be found at hangtingchen.github.io/ultra_dual_path_compression.github.io/.
</details>
<details>
<summary>摘要</summary>
双路压缩是全二重通信中不可或缺的，但大多数现有的神经网络具有高计算成本和不可调整的模型复杂度。在这篇论文中，我们引入时间频率双路压缩以 дости持续各种压缩比率的 Computational Cost。具体来说，用于频率压缩的专案网络取代了手动设计的范本网络，以进行维度缩减。而用于时间压缩的专案网络则使用几个几个帧的预测来缓和大量的预测误差，以免受几个几个帧的预测误差对准确性的影响。我们发现，具有固定压缩比率的双路压缩可以提供更好的性能，覆盖压缩比率 від 4x 到 32x 之间，而且模型大小几乎不变。此外，我们的模型也与快速FullSubNet和DeepFilterNet的性能相当。更多详情可以查看hangtingchen.github.io/ultra_dual_path_compression.github.io/.
</details></li>
</ul>
<hr>
<h2 id="Spurious-Correlations-and-Where-to-Find-Them"><a href="#Spurious-Correlations-and-Where-to-Find-Them" class="headerlink" title="Spurious Correlations and Where to Find Them"></a>Spurious Correlations and Where to Find Them</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11043">http://arxiv.org/abs/2308.11043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gautam Sreekumar, Vishnu Naresh Boddeti</li>
<li>for: 本研究旨在探讨数据驱动学习中存在的假 correlate 问题，并提出一种基于 causal graph 生成的 Synthetic dataset，以便研究这种问题的影响。</li>
<li>methods: 本研究使用了一些常见的假 correlate 假设，并对标准 ERM 基elines 进行比较，以评估这些假设对模型性能的影响。</li>
<li>results: 研究发现，假 correlate 的存在会导致模型学习到不可靠的特征，从而降低模型的性能。同时，研究还发现了一些模型设计选择和假 correlate 之间的相互关系。<details>
<summary>Abstract</summary>
Spurious correlations occur when a model learns unreliable features from the data and are a well-known drawback of data-driven learning. Although there are several algorithms proposed to mitigate it, we are yet to jointly derive the indicators of spurious correlations. As a result, the solutions built upon standalone hypotheses fail to beat simple ERM baselines. We collect some of the commonly studied hypotheses behind the occurrence of spurious correlations and investigate their influence on standard ERM baselines using synthetic datasets generated from causal graphs. Subsequently, we observe patterns connecting these hypotheses and model design choices.
</details>
<details>
<summary>摘要</summary>
非常有用的关系会发生假冒险，这是数据驱动学习的一个重要问题。虽然有许多算法提出来解决这个问题，但我们还未能同时 derivate 假冒险的指标。因此，基于独立的假设建立的解决方案不能超过简单的 ERM 基elines。我们收集了一些通常研究的假设，这些假设会导致假冒险的发生，并通过使用 causal 图生成的 sintetic 数据进行调查。我们发现了这些假设和模型设计选择之间的征 patterns。
</details></li>
</ul>
<hr>
<h2 id="Split-Learning-for-Distributed-Collaborative-Training-of-Deep-Learning-Models-in-Health-Informatics"><a href="#Split-Learning-for-Distributed-Collaborative-Training-of-Deep-Learning-Models-in-Health-Informatics" class="headerlink" title="Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics"></a>Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11027">http://arxiv.org/abs/2308.11027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuohang Li, Chao Yan, Xinmeng Zhang, Gharib Gharibi, Zhijun Yin, Xiaoqian Jiang, Bradley A. Malin</li>
<li>for: 这篇论文旨在解决医疗机构内部数据分散和隐私要求的问题，使得深度学习模型能够在不同医疗机构之间分享数据并进行模型训练。</li>
<li>methods: 该论文提出了一种新的隐私保护分布式学习框架，该框架可以在不同医疗机构之间分享数据，而不需要将原始数据或模型参数泄露出去。</li>
<li>results: 根据实验结果，使用这种分布式学习框架可以让深度学习模型在不同医疗机构的数据上进行合作训练，而不会产生隐私泄露或计算效率的问题。<details>
<summary>Abstract</summary>
Deep learning continues to rapidly evolve and is now demonstrating remarkable potential for numerous medical prediction tasks. However, realizing deep learning models that generalize across healthcare organizations is challenging. This is due, in part, to the inherent siloed nature of these organizations and patient privacy requirements. To address this problem, we illustrate how split learning can enable collaborative training of deep learning models across disparate and privately maintained health datasets, while keeping the original records and model parameters private. We introduce a new privacy-preserving distributed learning framework that offers a higher level of privacy compared to conventional federated learning. We use several biomedical imaging and electronic health record (EHR) datasets to show that deep learning models trained via split learning can achieve highly similar performance to their centralized and federated counterparts while greatly improving computational efficiency and reducing privacy risks.
</details>
<details>
<summary>摘要</summary>
深度学习继续迅速发展，现在表现出了各种医疗预测任务的惊人潜力。然而，实现通用于医疗机构的深度学习模型却是一大挑战。这主要归结于医疗机构的自然分化性和患者隐私要求。为解决这问题，我们介绍了一种新的隐私保护分布式学习框架，可以在不同的私有保持健康数据集上进行分布式学习，同时保持原始记录和模型参数的私有性。我们使用了多个生物医学成像和电子医疗记录（EHR）数据集，展示了通过Split Learning训练的深度学习模型可以与中央化和联邦化模型相比，达到同等或更高的性能水平，同时大幅提高计算效率和降低隐私风险。
</details></li>
</ul>
<hr>
<h2 id="Extreme-Multilabel-Classification-for-Specialist-Doctor-Recommendation-with-Implicit-Feedback-and-Limited-Patient-Metadata"><a href="#Extreme-Multilabel-Classification-for-Specialist-Doctor-Recommendation-with-Implicit-Feedback-and-Limited-Patient-Metadata" class="headerlink" title="Extreme Multilabel Classification for Specialist Doctor Recommendation with Implicit Feedback and Limited Patient Metadata"></a>Extreme Multilabel Classification for Specialist Doctor Recommendation with Implicit Feedback and Limited Patient Metadata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11022">http://arxiv.org/abs/2308.11022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filipa Valdeira, Stevo Racković, Valeria Danalachi, Qiwei Han, Cláudia Soares</li>
<li>for: This paper aims to predict medical referrals for both new patients and those with a consultation history, using Extreme Multilabel Classification (XML) to encode available features and explore different scenarios.</li>
<li>methods: The authors use XML, commonly employed in text-based classification tasks, to recast a traditional recommender setting into a multilabel classification problem that current XML methods can solve. They propose a unified model leveraging patient history across different specialties.</li>
<li>results: Compared to state-of-the-art recommendation systems using the same features, the authors’ approach consistently improves standard recommendation metrics up to approximately $10%$ for patients with a previous consultation history. For new patients, XML proves better at exploiting available features, outperforming the benchmark in favorable scenarios, with particular emphasis on recall metrics.<details>
<summary>Abstract</summary>
Recommendation Systems (RS) are often used to address the issue of medical doctor referrals. However, these systems require access to patient feedback and medical records, which may not always be available in real-world scenarios. Our research focuses on medical referrals and aims to predict recommendations in different specialties of physicians for both new patients and those with a consultation history. We use Extreme Multilabel Classification (XML), commonly employed in text-based classification tasks, to encode available features and explore different scenarios. While its potential for recommendation tasks has often been suggested, this has not been thoroughly explored in the literature. Motivated by the doctor referral case, we show how to recast a traditional recommender setting into a multilabel classification problem that current XML methods can solve. Further, we propose a unified model leveraging patient history across different specialties. Compared to state-of-the-art RS using the same features, our approach consistently improves standard recommendation metrics up to approximately $10\%$ for patients with a previous consultation history. For new patients, XML proves better at exploiting available features, outperforming the benchmark in favorable scenarios, with particular emphasis on recall metrics. Thus, our approach brings us one step closer to creating more effective and personalized doctor referral systems. Additionally, it highlights XML as a promising alternative to current hybrid or content-based RS, while identifying key aspects to take into account when using XML for recommendation tasks.
</details>
<details>
<summary>摘要</summary>
医疗专家 recombination系统（RS）经常用于解决医疗专家推荐的问题。然而，这些系统可能需要患者反馈和医疗记录，却不一定在实际场景中可用。我们的研究关注医疗推荐，并 стремятся预测不同专业医生的建议，包括新患者和已有咨询历史。我们使用极端多标签分类（XML），通常用于文本基于分类任务，来编码可用的特征并探索不同场景。虽然XML在推荐任务中的潜在优势已经得到了一些研究，但这还没有得到充分的探讨。受医生推荐 случа件 inspirited，我们提出了一种将传统推荐设定转化为多标签分类问题的方法，使得现有的XML方法可以解决。此外，我们提议一种综合模型，利用患者历史跨不同专业。与现有的RS使用同样的特征相比，我们的方法在拥有历史记录的患者上 consistently 改善标准推荐指标，最高提高约10%。对新患者来说，XML表现出色地利用可用的特征，在有利场景下超越 benchmark，尤其是在回溯率指标上。因此，我们的方法使得我们更近一步到创造更有效和个性化的医生推荐系统。此外，它还证明了XML作为推荐任务的一种有前途的alternative，而且标识了在使用XML时需要注意的关键方面。
</details></li>
</ul>
<hr>
<h2 id="Instance-based-Learning-with-Prototype-Reduction-for-Real-Time-Proportional-Myocontrol-A-Randomized-User-Study-Demonstrating-Accuracy-preserving-Data-Reduction-for-Prosthetic-Embedded-Systems"><a href="#Instance-based-Learning-with-Prototype-Reduction-for-Real-Time-Proportional-Myocontrol-A-Randomized-User-Study-Demonstrating-Accuracy-preserving-Data-Reduction-for-Prosthetic-Embedded-Systems" class="headerlink" title="Instance-based Learning with Prototype Reduction for Real-Time Proportional Myocontrol: A Randomized User Study Demonstrating Accuracy-preserving Data Reduction for Prosthetic Embedded Systems"></a>Instance-based Learning with Prototype Reduction for Real-Time Proportional Myocontrol: A Randomized User Study Demonstrating Accuracy-preserving Data Reduction for Prosthetic Embedded Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11019">http://arxiv.org/abs/2308.11019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Sziburis, Markus Nowak, Davide Brunelli</li>
<li>for:  Gesture detection in prosthetic control</li>
<li>methods:  kNN scheme with dataset reduction techniques (Decision Surface Mapping)</li>
<li>results:  Significantly better performance than regression techniques, with a reduction rate of over 99% and no statistically significant difference between kNN and kNN with DSM.<details>
<summary>Abstract</summary>
This work presents the design, implementation and validation of learning techniques based on the kNN scheme for gesture detection in prosthetic control. To cope with high computational demands in instance-based prediction, methods of dataset reduction are evaluated considering real-time determinism to allow for the reliable integration into battery-powered portable devices. The influence of parameterization and varying proportionality schemes is analyzed, utilizing an eight-channel-sEMG armband. Besides offline cross-validation accuracy, success rates in real-time pilot experiments (online target achievement tests) are determined. Based on the assessment of specific dataset reduction techniques' adequacy for embedded control applications regarding accuracy and timing behaviour, Decision Surface Mapping (DSM) proves itself promising when applying kNN on the reduced set. A randomized, double-blind user study was conducted to evaluate the respective methods (kNN and kNN with DSM-reduction) against Ridge Regression (RR) and RR with Random Fourier Features (RR-RFF). The kNN-based methods performed significantly better (p<0.0005) than the regression techniques. Between DSM-kNN and kNN, there was no statistically significant difference (significance level 0.05). This is remarkable in consideration of only one sample per class in the reduced set, thus yielding a reduction rate of over 99% while preserving success rate. The same behaviour could be confirmed in an extended user study. With k=1, which turned out to be an excellent choice, the runtime complexity of both kNN (in every prediction step) as well as DSM-kNN (in the training phase) becomes linear concerning the number of original samples, favouring dependable wearable prosthesis applications.
</details>
<details>
<summary>摘要</summary>
Based on the assessment of specific dataset reduction techniques for embedded control applications regarding accuracy and timing behavior, Decision Surface Mapping (DSM) proves to be promising when applied to the reduced set using kNN. A randomized, double-blind user study was conducted to evaluate the kNN-based methods against Ridge Regression (RR) and RR with Random Fourier Features (RR-RFF). The kNN-based methods performed significantly better (p < 0.0005) than the regression techniques. Between DSM-kNN and kNN, there was no statistically significant difference (significance level 0.05). This is noteworthy, considering only one sample per class in the reduced set, resulting in a reduction rate of over 99% while preserving success rate. The same behavior was confirmed in an extended user study.With k = 1, which was found to be an excellent choice, the runtime complexity of both kNN (in every prediction step) and DSM-kNN (in the training phase) becomes linear concerning the number of original samples, favoring dependable wearable prosthesis applications.
</details></li>
</ul>
<hr>
<h2 id="Fat-Shattering-Joint-Measurability-and-PAC-Learnability-of-POVM-Hypothesis-Classes"><a href="#Fat-Shattering-Joint-Measurability-and-PAC-Learnability-of-POVM-Hypothesis-Classes" class="headerlink" title="Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes"></a>Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12304">http://arxiv.org/abs/2308.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abram Magner, Arun Padakandla</li>
<li>for: 这篇论文主要研究了量子测量类的学习可能性，并提出了匹配的必要和 suficient conditions，以及相应的样本复杂性上限。</li>
<li>methods: 作者使用了empirical risk和denoised ERM来学习量子测量类，并证明了这两种方法的universal性和有效性。</li>
<li>results: 作者证明了一些量子测量类是可学习的，并提供了对应的样本复杂性上限。此外，作者还证明了一些previous works中的result不正确，并提供了一种新的学习规则—denoised ERM。<details>
<summary>Abstract</summary>
We characterize learnability for quantum measurement classes by establishing matching necessary and sufficient conditions for their PAC learnability, along with corresponding sample complexity bounds, in the setting where the learner is given access only to prepared quantum states. We first probe the results from previous works on this setting. We show that the empirical risk defined in previous works and matching the definition in the classical theory fails to satisfy the uniform convergence property enjoyed in the classical setting for some learnable classes. Moreover, we show that VC dimension generalization upper bounds in previous work are frequently infinite, even for finite-dimensional POVM classes. To surmount the failure of the standard ERM to satisfy uniform convergence, we define a new learning rule -- denoised ERM. We show this to be a universal learning rule for POVM and probabilistically observed concept classes, and the condition for it to satisfy uniform convergence is finite fat shattering dimension of the class. We give quantitative sample complexity upper and lower bounds for learnability in terms of finite fat-shattering dimension and a notion of approximate finite partitionability into approximately jointly measurable subsets, which allow for sample reuse. We then show that finite fat shattering dimension implies finite coverability by approximately jointly measurable subsets, leading to our matching conditions. We also show that every measurement class defined on a finite-dimensional Hilbert space is PAC learnable. We illustrate our results on several example POVM classes.
</details>
<details>
<summary>摘要</summary>
我们characterize学习可能性 для量子测量类by establishing匹配的必需和充分条件，以及相应的样本复杂度上限，在learner只有访问准备好的量子状态的设置下。我们首先探讨previous works中的结果。我们发现，employmedical risk定义在previous works和classical theory中匹配的不满足uniform convergence性 enjoyed in the classical setting for some learnable classes。此外，我们发现VC dimension generalization upper bounds in previous work are frequently infinite, even for finite-dimensional POVM classes。为了缺乏标准ERM的uniform convergence，我们定义了一个新的学习规则——denoised ERM。我们证明这是一个universal learning rule for POVM和 probabilistically observed concept classes，并且condition for it to satisfy uniform convergence is finite fat shattering dimension of the class。我们给出了量子sample complexity upper和lower bounds in terms offinite fat-shattering dimension和一种notion of approximate finite partitionability into approximately jointly measurable subsets，which allow for sample reuse。然后，我们证明finite fat shattering dimension implies finite coverability by approximately jointly measurable subsets, leading to our matching conditions。最后，我们证明每个定义在finite-dimensional Hilbert space上的测量类都是PAC可学习的。我们在several example POVM classes中ILLustrate our results。
</details></li>
</ul>
<hr>
<h2 id="Majorana-Demonstrator-Data-Release-for-AI-ML-Applications"><a href="#Majorana-Demonstrator-Data-Release-for-AI-ML-Applications" class="headerlink" title="Majorana Demonstrator Data Release for AI&#x2F;ML Applications"></a>Majorana Demonstrator Data Release for AI&#x2F;ML Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10856">http://arxiv.org/abs/2308.10856</a></li>
<li>repo_url: None</li>
<li>paper_authors: I. J. Arnquist, F. T. Avignone III, A. S. Barabash, C. J. Barton, K. H. Bhimani, E. Blalock, B. Bos, M. Busch, M. Buuck, T. S. Caldwell, Y. -D. Chan, C. D. Christofferson, P. -H. Chu, M. L. Clark, C. Cuesta, J. A. Detwiler, Yu. Efremenko, H. Ejiri, S. R. Elliott, N. Fuad, G. K. Giovanetti, M. P. Green, J. Gruszko, I. S. Guinn, V. E. Guiseppe, C. R. Haufe, R. Henning, D. Hervas Aguilar, E. W. Hoppe, A. Hostiuc, M. F. Kidd, I. Kim, R. T. Kouzes, T. E. Lannen V, A. Li, J. M. Lopez-Castano, R. D. Martin, R. Massarczyk, S. J. Meijer, S. Mertens, T. K. Oli, L. S. Paudel, W. Pettus, A. W. P. Poon, B. Quenallata, D. C. Radford, A. L. Reine, K. Rielage, N. W. Ruof, D. C. Schaper, S. J. Schleich, D. Tedeschi, R. L. Varner, S. Vasilyev, S. L. Watkins, J. F. Wilkerson, C. Wiseman, W. Xu, C. -H. Yu, B. X. Zhu<br>for:This paper is written for the purpose of releasing a subset of calibration data from the Majorana Demonstrator experiment to support the training and testing of Artificial Intelligence (AI) and Machine Learning (ML) algorithms.methods:The paper uses HDF5 file format to store the raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, along with relevant metadata.results:The paper provides a dataset that includes a subset of the calibration data from the Majorana Demonstrator experiment, which is specifically designed for the training and testing of AI and ML algorithms. The dataset includes raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, along with relevant metadata.<details>
<summary>Abstract</summary>
The enclosed data release consists of a subset of the calibration data from the Majorana Demonstrator experiment. Each Majorana event is accompanied by raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, all shared in an HDF5 file format along with relevant metadata. This release is specifically designed to support the training and testing of Artificial Intelligence (AI) and Machine Learning (ML) algorithms upon our data. This document is structured as follows. Section I provides an overview of the dataset's content and format; Section II outlines the location of this dataset and the method for accessing it; Section III presents the NPML Machine Learning Challenge associated with this dataset; Section IV contains a disclaimer from the Majorana collaboration regarding the use of this dataset; Appendix A contains technical details of this data release. Please direct questions about the material provided within this release to liaobo77@ucsd.edu (A. Li).
</details>
<details>
<summary>摘要</summary>
附件的数据释放包含 Majorana 实验中的一个子集 calibration 数据。每个 Majorana 事件都包含Raw Germanium 探测器波形、振荡形态分离cuts 和加工后的最终能量， alles 共同存储在 HDF5 文件格式中，并同时包含相关的元数据。这个释放是为支持使用人工智能（AI）和机器学习（ML）算法进行训练和测试而设计的。本文结构如下：Section I 提供了数据集的内容和格式的概述；Section II 描述了数据集的位置和访问方法；Section III 介绍了NPML 机器学习挑战与此数据集相关的详细信息；Section IV 包含 Majorana 团队的声明关于使用这个数据集； Appendix A 包含这个数据释放的技术详细信息。如有关于附件中提供的内容的问题，请邮件 liaobo77@ucsd.edu (A. Li)。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-quantum-generative-models-via-imbalanced-data-classification-benchmarks"><a href="#Evaluating-quantum-generative-models-via-imbalanced-data-classification-benchmarks" class="headerlink" title="Evaluating quantum generative models via imbalanced data classification benchmarks"></a>Evaluating quantum generative models via imbalanced data classification benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10847">http://arxiv.org/abs/2308.10847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Graham R. Enos, Matthew J. Reagor, Eric Hulburd</li>
<li>for: 这个论文是为了研究量子机器学习模型的异常行为是否可以用现实世界数据集来评估的。</li>
<li>methods: 该论文使用可解释人工智能技术来分析生成自量子-类型 нейрон网络的合成数据，并对这些数据进行比较与现有的措施来mitigate类别偏好。</li>
<li>results: 该论文发现，对于不同的数据集，量子生成的数据会具有不同的特征，这些特征可以用来评估量子机器学习模型的性能。<details>
<summary>Abstract</summary>
A limited set of tools exist for assessing whether the behavior of quantum machine learning models diverges from conventional models, outside of abstract or theoretical settings. We present a systematic application of explainable artificial intelligence techniques to analyze synthetic data generated from a hybrid quantum-classical neural network adapted from twenty different real-world data sets, including solar flares, cardiac arrhythmia, and speech data. Each of these data sets exhibits varying degrees of complexity and class imbalance. We benchmark the quantum-generated data relative to state-of-the-art methods for mitigating class imbalance for associated classification tasks. We leverage this approach to elucidate the qualities of a problem that make it more or less likely to be amenable to a hybrid quantum-classical generative model.
</details>
<details>
<summary>摘要</summary>
有限的工具存在用于判断量子机器学习模型的行为与常规模型 diverges，外部的抽象或理论设置之外。我们们提出了一种系统的应用 explainable artificial intelligence 技术来分析量子-классиical 神经网络在 twenty 个真实世界数据集中生成的synthetic数据。这些数据集包括太阳风暴、心脏 arrhythmia 和语音数据，每个数据集都具有不同的复杂度和类别偏度。我们将量子生成的数据与现有的方法进行比较，以衡量对类别偏度的影响。我们利用这种方法来描述问题的特质，使其更或 menos 可能适用于量子-классиical 生成模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/22/cs.LG_2023_08_22/" data-id="cllshr3660030o988gula2uya" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/22/cs.CV_2023_08_22/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-08-22
        
      </div>
    </a>
  
  
    <a href="/2023/08/22/cs.SD_2023_08_22/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-22 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
