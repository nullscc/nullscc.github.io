
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-20 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Boosting Adversarial Transferability by Block Shuffle and Rotation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10299 repo_url: None paper_authors: Kunyu Wang, Xuanran He, Wenxuan Wang, Xiaosen Wang for: 防御深度">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-20 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/20/eess.IV_2023_08_20/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Boosting Adversarial Transferability by Block Shuffle and Rotation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10299 repo_url: None paper_authors: Kunyu Wang, Xuanran He, Wenxuan Wang, Xiaosen Wang for: 防御深度">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-19T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:34.189Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/20/eess.IV_2023_08_20/" class="article-date">
  <time datetime="2023-08-19T16:00:00.000Z" itemprop="datePublished">2023-08-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-20 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Boosting-Adversarial-Transferability-by-Block-Shuffle-and-Rotation"><a href="#Boosting-Adversarial-Transferability-by-Block-Shuffle-and-Rotation" class="headerlink" title="Boosting Adversarial Transferability by Block Shuffle and Rotation"></a>Boosting Adversarial Transferability by Block Shuffle and Rotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10299">http://arxiv.org/abs/2308.10299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunyu Wang, Xuanran He, Wenxuan Wang, Xiaosen Wang</li>
<li>for: 防御深度学习模型受到攻击，即使使用黑盒 Setting 进行攻击。</li>
<li>methods: 使用输入变换基于的攻击方法，包括块洗混淆和旋转（BSR）。</li>
<li>results: BSR 可以在单模型和ensemble模型 Setting 下达到更高的传输性能，并且可以与现有的输入变换方法相结合，以获得更好的传输性能。<details>
<summary>Abstract</summary>
Adversarial examples mislead deep neural networks with imperceptible perturbations and have brought significant threats to deep learning. An important aspect is their transferability, which refers to their ability to deceive other models, thus enabling attacks in the black-box setting. Though various methods have been proposed to boost transferability, the performance still falls short compared with white-box attacks. In this work, we observe that existing input transformation based attacks, one of the mainstream transfer-based attacks, result in different attention heatmaps on various models, which might limit the transferability. We also find that breaking the intrinsic relation of the image can disrupt the attention heatmap of the original image. Based on this finding, we propose a novel input transformation based attack called block shuffle and rotation (BSR). Specifically, BSR splits the input image into several blocks, then randomly shuffles and rotates these blocks to construct a set of new images for gradient calculation. Empirical evaluations on the ImageNet dataset demonstrate that BSR could achieve significantly better transferability than the existing input transformation based methods under single-model and ensemble-model settings. Combining BSR with the current input transformation method can further improve the transferability, which significantly outperforms the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
深度学习中的对抗性示例会使深度神经网络受到不可见的扰动，对深度学习带来了重要的威胁。其中一个重要方面是它们的传输性，即它们能够在黑盒Setting中欺骗其他模型，因此可以实现黑盒攻击。虽然多种方法已经被提出来提高传输性，但其性能仍然落后于白盒攻击。在这项工作中，我们发现现有的输入变换基于攻击，一种主流的传输基于攻击，会导致不同的注意度热图在不同的模型上。我们还发现，打破图像的内在关系可以阻断原始图像的注意度热图。基于这一发现，我们提出了一种新的输入变换基于攻击方法，即块拼接和旋转（BSR）。具体来说，BSR将输入图像分成多个块，然后随机拼接和旋转这些块来构建一组新的图像，用于计算梯度。我们的实验表明，BSR可以在单模型和集成模型设置下 achieves significantly better transferability than现有的输入变换基于攻击方法。同时，BSR与现有的输入变换方法相结合可以进一步提高传输性，并且与当前的状态态-of-the-art方法相比，显著超越。
</details></li>
</ul>
<hr>
<h2 id="Domain-Reduction-Strategy-for-Non-Line-of-Sight-Imaging"><a href="#Domain-Reduction-Strategy-for-Non-Line-of-Sight-Imaging" class="headerlink" title="Domain Reduction Strategy for Non Line of Sight Imaging"></a>Domain Reduction Strategy for Non Line of Sight Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10269">http://arxiv.org/abs/2308.10269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunbo Shim, In Cho, Daekyu Kwon, Seon Joo Kim</li>
<li>for: 非直线视野（NLOS）图像重建</li>
<li>methods: 利用独立计算各点隐藏物变数的方法，并将处理隐藏表面的函数组合成一个更加简洁的问题</li>
<li>results: 在多种NLOS情况下，包括非平面镜壁、稀疏扫描图、对焦和非对焦图像重建等，该方法能够提供高效且稳定的重建结果<details>
<summary>Abstract</summary>
This paper presents a novel optimization-based method for non-line-of-sight (NLOS) imaging that aims to reconstruct hidden scenes under various setups. Our method is built upon the observation that photons returning from each point in hidden volumes can be independently computed if the interactions between hidden surfaces are trivially ignored. We model the generalized light propagation function to accurately represent the transients as a linear combination of these functions. Moreover, our proposed method includes a domain reduction procedure to exclude empty areas of the hidden volumes from the set of propagation functions, thereby improving computational efficiency of the optimization. We demonstrate the effectiveness of the method in various NLOS scenarios, including non-planar relay wall, sparse scanning patterns, confocal and non-confocal, and surface geometry reconstruction. Experiments conducted on both synthetic and real-world data clearly support the superiority and the efficiency of the proposed method in general NLOS scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Crucial-Feature-Capture-and-Discrimination-for-Limited-Training-Data-SAR-ATR"><a href="#Crucial-Feature-Capture-and-Discrimination-for-Limited-Training-Data-SAR-ATR" class="headerlink" title="Crucial Feature Capture and Discrimination for Limited Training Data SAR ATR"></a>Crucial Feature Capture and Discrimination for Limited Training Data SAR ATR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10911">http://arxiv.org/abs/2308.10911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cwwangsaratr/saratr_feacapture_discrimination">https://github.com/cwwangsaratr/saratr_feacapture_discrimination</a></li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 本研究旨在提高SAR ATR的表现，对于受限 Training 数据的情况下。</li>
<li>methods: 本研究使用了两个分支和两个模组，包括全球帮助分支和本地增强分支、特征捕捉模组和特征分别模组。在每次训练中，全球帮助分支首先完成了初始识别，基于整个图像。然后，特征捕捉模组自动搜寻并锁定了重要的图像区域，我们称之为“图像金钥”。最后，本地增强分支对于这些捕捉的图像区域进行了本地特征抽取，并将其与全局特征进行了联合识别。</li>
<li>results: 本研究的模型验证实验和比较显示，在受限 Training 数据的情况下，我们的方法实现了更好的识别表现，包括改善特征分布和识别概率。在MSTAR和OPENSAR上进行的实验和比较显示，我们的方法已经 дости得了superior的识别表现。<details>
<summary>Abstract</summary>
Although deep learning-based methods have achieved excellent performance on SAR ATR, the fact that it is difficult to acquire and label a lot of SAR images makes these methods, which originally performed well, perform weakly. This may be because most of them consider the whole target images as input, but the researches find that, under limited training data, the deep learning model can't capture discriminative image regions in the whole images, rather focus on more useless even harmful image regions for recognition. Therefore, the results are not satisfactory. In this paper, we design a SAR ATR framework under limited training samples, which mainly consists of two branches and two modules, global assisted branch and local enhanced branch, feature capture module and feature discrimination module. In every training process, the global assisted branch first finishes the initial recognition based on the whole image. Based on the initial recognition results, the feature capture module automatically searches and locks the crucial image regions for correct recognition, which we named as the golden key of image. Then the local extract the local features from the captured crucial image regions. Finally, the overall features and local features are input into the classifier and dynamically weighted using the learnable voting parameters to collaboratively complete the final recognition under limited training samples. The model soundness experiments demonstrate the effectiveness of our method through the improvement of feature distribution and recognition probability. The experimental results and comparisons on MSTAR and OPENSAR show that our method has achieved superior recognition performance.
</details>
<details>
<summary>摘要</summary>
尽管深度学习基本方法在 Synthetic Aperture Radar（SAR）特征识别（ATR）中表现出色，但由于获取和标注大量SAR图像困难，这些方法在限制性训练样本的情况下表现弱。这可能是因为大多数方法将整个目标图像作为输入，但研究人员发现，在有限训练样本下，深度学习模型无法在整个图像中捕捉有利特征区域，而是偏向更无用或甚至有害的图像区域进行识别。因此，结果不满足要求。在这篇论文中，我们设计了一个基于有限训练样本的SAR ATR框架，它包括两个支线和两个模块：全球协助支线和本地增强支线，特征捕捉模块和特征分类模块。在每次训练过程中，全球协助支线首先根据整个图像完成初步识别。基于初步识别结果，特征捕捉模块自动搜索和锁定图像中重要的关键区域，我们称之为图像的“金钥匙”。然后，本地EXTRACT本地特征从捕捉到的关键图像区域。最后，总特征和本地特征被输入到分类器并使用学习投票参数进行协同完成最终识别。实验证明我们的方法的有效性通过特征分布和识别概率的改进。实验结果和相对比较表明，我们的方法在MSTAR和OPENSAR上达到了最高的识别性能。
</details></li>
</ul>
<hr>
<h2 id="An-Entropy-Awareness-Meta-Learning-Method-for-SAR-Open-Set-ATR"><a href="#An-Entropy-Awareness-Meta-Learning-Method-for-SAR-Open-Set-ATR" class="headerlink" title="An Entropy-Awareness Meta-Learning Method for SAR Open-Set ATR"></a>An Entropy-Awareness Meta-Learning Method for SAR Open-Set ATR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10251">http://arxiv.org/abs/2308.10251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Xiaoyu Liu, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 提高Synthetic Aperture Radar自动目标识别（SAR ATR）方法的泛化能力，特别是处理未知类目标的开放集成识别（OSR）问题。</li>
<li>methods: 提出一种基于自适应学习和熵觉知的元学习方法，通过元学习任务学习构建动态分配的知识类别特征空间，并通过熵觉知损失进一步增强特征空间的效果和Robustness。</li>
<li>results: 在MSTAR数据集上进行实验，显示了方法的效果性和可靠性。<details>
<summary>Abstract</summary>
Existing synthetic aperture radar automatic target recognition (SAR ATR) methods have been effective for the classification of seen target classes. However, it is more meaningful and challenging to distinguish the unseen target classes, i.e., open set recognition (OSR) problem, which is an urgent problem for the practical SAR ATR. The key solution of OSR is to effectively establish the exclusiveness of feature distribution of known classes. In this letter, we propose an entropy-awareness meta-learning method that improves the exclusiveness of feature distribution of known classes which means our method is effective for not only classifying the seen classes but also encountering the unseen other classes. Through meta-learning tasks, the proposed method learns to construct a feature space of the dynamic-assigned known classes. This feature space is required by the tasks to reject all other classes not belonging to the known classes. At the same time, the proposed entropy-awareness loss helps the model to enhance the feature space with effective and robust discrimination between the known and unknown classes. Therefore, our method can construct a dynamic feature space with discrimination between the known and unknown classes to simultaneously classify the dynamic-assigned known classes and reject the unknown classes. Experiments conducted on the moving and stationary target acquisition and recognition (MSTAR) dataset have shown the effectiveness of our method for SAR OSR.
</details>
<details>
<summary>摘要</summary>
现有的Synthetic Aperture Radar自动目标识别（SAR ATR）方法已经有效地对seen标签类进行分类。然而，更重要和挑战性的是分inguished the unseen target classes，即open set recognition（OSR）问题，这是实际SAR ATR中的紧迫问题。OSR的关键解决方案是有效地建立known classes的特征分布的 exclusiveness。在这封信中，我们提出了一种基于entropy的meta-学习方法，该方法可以提高known classes的特征分布的 exclusiveness，这意味着我们的方法不仅能够分类seen classes，还能够遇到未seen的其他类。通过meta-学习任务，我们的方法学习了construct a feature space of the dynamic-assigned known classes。这个feature space是需要由任务拒绝所有不属于known classes的其他类。同时，我们的entropy-awareness loss帮助模型增强feature space的有效和Robust的 distinguish between known和unknown classes。因此，我们的方法可以construct a dynamic feature space with discrimination between known和unknown classes，同时分类dynamic-assigned known classes和拒绝unknown classes。在MSTAR dataset上进行的实验表明了我们的方法对SAR OSR的效iveness。
</details></li>
</ul>
<hr>
<h2 id="SAR-Ship-Target-Recognition-via-Selective-Feature-Discrimination-and-Multifeature-Center-Classifier"><a href="#SAR-Ship-Target-Recognition-via-Selective-Feature-Discrimination-and-Multifeature-Center-Classifier" class="headerlink" title="SAR Ship Target Recognition via Selective Feature Discrimination and Multifeature Center Classifier"></a>SAR Ship Target Recognition via Selective Feature Discrimination and Multifeature Center Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10250">http://arxiv.org/abs/2308.10250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Jifang Pei, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 这 paper 是为了提高 SAR 图像识别精度，特别是在减少训练样本数量的情况下。</li>
<li>methods: 本 paper 提出了一种基于选择性特征强制和多特征中心分类器的 SAR 船target 识别方法。该方法包括自动选择与最相似 Inter-class 图像对的相似特征，以及与最不相似 Inner-class 图像对的不相似特征，然后对这些特征进行增强。此外，本方法还使用分类器分配多个学习可能的特征中心，以 conquering 大 inner-class variance。</li>
<li>results: 实验结果表明，本方法在 OpenSARShip 和 FUSAR-Ship 数据集上 achieved 较高的识别精度，而且可以在减少训练 SAR 船样本数量的情况下保持高精度。<details>
<summary>Abstract</summary>
Maritime surveillance is not only necessary for every country, such as in maritime safeguarding and fishing controls, but also plays an essential role in international fields, such as in rescue support and illegal immigration control. Most of the existing automatic target recognition (ATR) methods directly send the extracted whole features of SAR ships into one classifier. The classifiers of most methods only assign one feature center to each class. However, the characteristics of SAR ship images, large inner-class variance, and small interclass difference lead to the whole features containing useless partial features and a single feature center for each class in the classifier failing with large inner-class variance. We proposes a SAR ship target recognition method via selective feature discrimination and multifeature center classifier. The selective feature discrimination automatically finds the similar partial features from the most similar interclass image pairs and the dissimilar partial features from the most dissimilar inner-class image pairs. It then provides a loss to enhance these partial features with more interclass separability. Motivated by divide and conquer, the multifeature center classifier assigns multiple learnable feature centers for each ship class. In this way, the multifeature centers divide the large inner-class variance into several smaller variances and conquered by combining all feature centers of one ship class. Finally, the probability distribution over all feature centers is considered comprehensively to achieve an accurate recognition of SAR ship images. The ablation experiments and experimental results on OpenSARShip and FUSAR-Ship datasets show that our method has achieved superior recognition performance under decreasing training SAR ship samples.
</details>
<details>
<summary>摘要</summary>
海上监控不仅是每个国家的必需，如海上安全和渔业控制，而且在国际领域也扮演着重要角色，如搜救支持和非法移民控制。现有的自动目标识别（ATR）方法大多直接将抽取的整个特征集送入一个分类器。但是SAR船图像的特征是巨大的内类差异和小的间类差异，导致整个特征集含有无用的部分特征和分类器中的每个特征中心都是一个。我们提出了一种基于选择性特征分化和多特征中心分类器的SAR船目标识别方法。选择性特征分化自动从最相似的Interclass图像对中找到相似的部分特征和最不相似的内类图像对中找到不相似的部分特征，然后为这些部分特征提供损失来提高它们的Interclass分离度。受分和胜利的思想 inspirited by divide and conquer，多特征中心分类器将每个船类分配多个学习的特征中心。这样，多特征中心将大内类差异分解成多个小差异，并且通过所有特征中心的组合来实现一个船类的准确识别。最后，对所有特征中心的概率分布进行全面考虑以实现高精度的SAR船图像识别。ablation experiment和OpenSARShip和FUSAR-Ship数据集的实验结果表明，我们的方法在减少的训练SAR船样本下实现了优秀的识别性能。
</details></li>
</ul>
<hr>
<h2 id="SAR-Ship-Target-Recognition-Via-Multi-Scale-Feature-Attention-and-Adaptive-Weighed-Classifier"><a href="#SAR-Ship-Target-Recognition-Via-Multi-Scale-Feature-Attention-and-Adaptive-Weighed-Classifier" class="headerlink" title="SAR Ship Target Recognition Via Multi-Scale Feature Attention and Adaptive-Weighed Classifier"></a>SAR Ship Target Recognition Via Multi-Scale Feature Attention and Adaptive-Weighed Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10247">http://arxiv.org/abs/2308.10247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Siyi Luo, Weibo Huo, Yulin Huang, Yin Zhang, Jianyu Yang</li>
<li>for: 本研究旨在提高Synthetic Aperture Radar（SAR）船target recognition的精度，提供了一种基于多scale feature attention和自适应权重分类器的方法。</li>
<li>methods: 该方法首先在网络中构建了一个内网络特征 piramid，以EXTRACT多scale特征从SAR船图像中。然后，使用多scale feature attention可以提取和增强多scale特征中的主要成分，具有更高的内类紧凑性和 между类分离性。最后，使用自适应权重分类器选择有效的特征层次，以实现最终的精度 recognition。</li>
<li>results: 通过对OpenSARship数据集进行实验和比较，该方法被证明可以达到SAR船target recognition的 estado-of-the-art表现。<details>
<summary>Abstract</summary>
Maritime surveillance is indispensable for civilian fields, including national maritime safeguarding, channel monitoring, and so on, in which synthetic aperture radar (SAR) ship target recognition is a crucial research field. The core problem to realizing accurate SAR ship target recognition is the large inner-class variance and inter-class overlap of SAR ship features, which limits the recognition performance. Most existing methods plainly extract multi-scale features of the network and utilize equally each feature scale in the classification stage. However, the shallow multi-scale features are not discriminative enough, and each scale feature is not equally effective for recognition. These factors lead to the limitation of recognition performance. Therefore, we proposed a SAR ship recognition method via multi-scale feature attention and adaptive-weighted classifier to enhance features in each scale, and adaptively choose the effective feature scale for accurate recognition. We first construct an in-network feature pyramid to extract multi-scale features from SAR ship images. Then, the multi-scale feature attention can extract and enhance the principal components from the multi-scale features with more inner-class compactness and inter-class separability. Finally, the adaptive weighted classifier chooses the effective feature scales in the feature pyramid to achieve the final precise recognition. Through experiments and comparisons under OpenSARship data set, the proposed method is validated to achieve state-of-the-art performance for SAR ship recognition.
</details>
<details>
<summary>摘要</summary>
海上监测是民用领域不可或缺的，包括国家海上安全、水道监测等， Synthetic Aperture Radar（SAR）船TargetRecognition是一个重要的研究领域。 however， Due to the large inner-class variance and inter-class overlap of SAR ship features, accurate recognition is limited. Most existing methods simply extract multi-scale features from the network and use each feature scale equally in the classification stage. However, the shallow multi-scale features are not discriminative enough, and each scale feature is not equally effective for recognition. These factors limit the recognition performance. Therefore, we proposed a SAR ship recognition method based on multi-scale feature attention and adaptive-weighted classifiers to enhance features in each scale and adaptively choose the effective feature scale for accurate recognition.First, we construct an in-network feature pyramid to extract multi-scale features from SAR ship images. Then, the multi-scale feature attention can extract and enhance the principal components from the multi-scale features with more inner-class compactness and inter-class separability. Finally, the adaptive weighted classifier chooses the effective feature scales in the feature pyramid to achieve the final precise recognition. Through experiments and comparisons under OpenSARship dataset, the proposed method is validated to achieve state-of-the-art performance for SAR ship recognition.
</details></li>
</ul>
<hr>
<h2 id="SAR-ATR-Method-with-Limited-Training-Data-via-an-Embedded-Feature-Augmenter-and-Dynamic-Hierarchical-Feature-Refiner"><a href="#SAR-ATR-Method-with-Limited-Training-Data-via-an-Embedded-Feature-Augmenter-and-Dynamic-Hierarchical-Feature-Refiner" class="headerlink" title="SAR ATR Method with Limited Training Data via an Embedded Feature Augmenter and Dynamic Hierarchical-Feature Refiner"></a>SAR ATR Method with Limited Training Data via an Embedded Feature Augmenter and Dynamic Hierarchical-Feature Refiner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10243">http://arxiv.org/abs/2308.10243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Siyi Luo, Yulin Huang, Jifang Pei, Yin Zhang, Jianyu Yang</li>
<li>for: 增进 Synthetic Aperture Radar（SAR）自动目标识别（ATR）性能，特别是在有限的训练数据情况下。</li>
<li>methods: 提出了一种新的方法，包括嵌入特征增强器和动态层次特征细化器。嵌入特征增强器通过吸引远离分类中心的虚拟特征，提高了可用于超vised学习的信息量。动态层次特征细化器通过生成动态核函数，捕捉到不同维度的特征，进一步提高了内类紧凑度和对类分离度。</li>
<li>results: 实验结果表明，提出的方法在有限SAR训练数据情况下实现了优秀的ATR性能，并在MSTAR、OpenSARShip和FUSAR-Ship数据集上达到了robust性和稳定性。<details>
<summary>Abstract</summary>
Without sufficient data, the quantity of information available for supervised training is constrained, as obtaining sufficient synthetic aperture radar (SAR) training data in practice is frequently challenging. Therefore, current SAR automatic target recognition (ATR) algorithms perform poorly with limited training data availability, resulting in a critical need to increase SAR ATR performance. In this study, a new method to improve SAR ATR when training data are limited is proposed. First, an embedded feature augmenter is designed to enhance the extracted virtual features located far away from the class center. Based on the relative distribution of the features, the algorithm pulls the corresponding virtual features with different strengths toward the corresponding class center. The designed augmenter increases the amount of information available for supervised training and improves the separability of the extracted features. Second, a dynamic hierarchical-feature refiner is proposed to capture the discriminative local features of the samples. Through dynamically generated kernels, the proposed refiner integrates the discriminative local features of different dimensions into the global features, further enhancing the inner-class compactness and inter-class separability of the extracted features. The proposed method not only increases the amount of information available for supervised training but also extracts the discriminative features from the samples, resulting in superior ATR performance in problems with limited SAR training data. Experimental results on the moving and stationary target acquisition and recognition (MSTAR), OpenSARShip, and FUSAR-Ship benchmark datasets demonstrate the robustness and outstanding ATR performance of the proposed method in response to limited SAR training data.
</details>
<details>
<summary>摘要</summary>
无 suficiente datos de entrenamiento，量度信息可用于超vised学习是受限的，因为在实践中获得 suficiente Synthetic Aperture Radar（SAR）training data是困难的。因此，当前SAR自动目标识别（ATR）算法在有限的training data availability时表现糟糕。在这种情况下，一种新的方法是提出，以提高SAR ATR的性能。首先，一种嵌入式特征增强器是设计的，以增强提取的虚拟特征，特别是那些远离类中心的特征。根据特征的相对分布，算法将相应的虚拟特征与不同强度拖向相应的类中心。这种增强器提高了用于超vised学习的信息量，并提高了提取特征的分类能力。其次，一种动态层次特征级化器是提出的，以捕捉不同维度的特征。通过动态生成的核函数，提出的级化器将不同维度的特征级化到全维度特征中，进一步提高了类内紧密性和类间分离性。这种方法不仅提高了用于超vised学习的信息量，还提取了样本中的特征，从而实现了在有限SAR training data情况下的出色ATR性能。实验结果表明，在MSTAR、OpenSARShip和FUSAR-Ship benchmark dataset上，提出的方法具有强大的Robustness和出色的ATR性能，在有限SAR training data情况下表现优秀。
</details></li>
</ul>
<hr>
<h2 id="Blind-Face-Restoration-for-Under-Display-Camera-via-Dictionary-Guided-Transformer"><a href="#Blind-Face-Restoration-for-Under-Display-Camera-via-Dictionary-Guided-Transformer" class="headerlink" title="Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer"></a>Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10196">http://arxiv.org/abs/2308.10196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingfan Tan, Xiaoxu Chen, Tao Wang, Kaihao Zhang, Wenhan Luo, Xiaocun Cao</li>
<li>for: 提供全屏体验，但是由于显示器的特点，UDC图像会受到质量下降的影响。</li>
<li>methods: 提出了一种两stage网络UDC退化模型网络（UDC-DMNet），通过模拟UDC拍摄过程来synthesize UDC图像。还使用高质量的face图像从FFHQ和CelebA-Test创建了UDC face测试集FFHQ-P&#x2F;T和CelebA-Test-P&#x2F;T。</li>
<li>results: 提出了一种基于字典指导 transformer网络（DGFormer），通过引入面部组合字典和UDC图像特点来实现盲目face restauration。实验表明，我们的DGFormer和UDC-DMNet均达到了当前最佳性能。<details>
<summary>Abstract</summary>
By hiding the front-facing camera below the display panel, Under-Display Camera (UDC) provides users with a full-screen experience. However, due to the characteristics of the display, images taken by UDC suffer from significant quality degradation. Methods have been proposed to tackle UDC image restoration and advances have been achieved. There are still no specialized methods and datasets for restoring UDC face images, which may be the most common problem in the UDC scene. To this end, considering color filtering, brightness attenuation, and diffraction in the imaging process of UDC, we propose a two-stage network UDC Degradation Model Network named UDC-DMNet to synthesize UDC images by modeling the processes of UDC imaging. Then we use UDC-DMNet and high-quality face images from FFHQ and CelebA-Test to create UDC face training datasets FFHQ-P/T and testing datasets CelebA-Test-P/T for UDC face restoration. We propose a novel dictionary-guided transformer network named DGFormer. Introducing the facial component dictionary and the characteristics of the UDC image in the restoration makes DGFormer capable of addressing blind face restoration in UDC scenarios. Experiments show that our DGFormer and UDC-DMNet achieve state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过将前置摄像头嵌入到显示板上，Under-Display Camera（UDC）为用户提供了全屏体验。然而，由于显示器的特点，UDC拍摄的图像受到了显著的质量下降。已经提出了修复UDC图像的方法，并取得了进步。然而，还没有专门的方法和数据集用于修复UDC脸图像，这可能是UDC场景中最常见的问题。为此，我们考虑了颜色滤波、亮度减弱和干涉的影像捕捉过程，并提出了一个两stage网络名为UDC-DMNet，用于模拟UDC拍摄过程。然后，我们使用UDC-DMNet和高质量的脸图像从FFHQ和CelebA-Test创建了UDC脸培训集FFHQ-P/T和测试集CelebA-Test-P/T。我们还提出了一种新的字典引导变换网络名为DGFormer，通过引入人脸组件字典和UDC图像的特点，DGFormer可以实现盲目脸修复在UDC场景中。实验表明，我们的DGFormer和UDC-DMNet实现了状态盘的性能。Note: Simplified Chinese is used here, which is a more common writing system in China. If you prefer Traditional Chinese, I can also provide the translation.
</details></li>
</ul>
<hr>
<h2 id="WMFormer-Nested-Transformer-for-Visible-Watermark-Removal-via-Implict-Joint-Learning"><a href="#WMFormer-Nested-Transformer-for-Visible-Watermark-Removal-via-Implict-Joint-Learning" class="headerlink" title="WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning"></a>WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10195">http://arxiv.org/abs/2308.10195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjian Huo, Zehong Zhang, Hanjing Su, Guanbin Li, Chaowei Fang, Qingyao Wu</li>
<li>for: 本研究旨在提高水印去除技术的精度和效率，以提高水印保护的可靠性。</li>
<li>methods: 该研究提出了一种新的 JOINT 学习方法，通过自动控制信息流动来Integrate 多个分支知识，从而提高水印localization和背景 restore 的精度。</li>
<li>results: 实验结果表明，该方法可以具有remarkable superiority，比existingsota-of-the-art方法大幅提高精度和效率。<details>
<summary>Abstract</summary>
Watermarking serves as a widely adopted approach to safeguard media copyright. In parallel, the research focus has extended to watermark removal techniques, offering an adversarial means to enhance watermark robustness and foster advancements in the watermarking field. Existing watermark removal methods mainly rely on UNet with task-specific decoder branches--one for watermark localization and the other for background image restoration. However, watermark localization and background restoration are not isolated tasks; precise watermark localization inherently implies regions necessitating restoration, and the background restoration process contributes to more accurate watermark localization. To holistically integrate information from both branches, we introduce an implicit joint learning paradigm. This empowers the network to autonomously navigate the flow of information between implicit branches through a gate mechanism. Furthermore, we employ cross-channel attention to facilitate local detail restoration and holistic structural comprehension, while harnessing nested structures to integrate multi-scale information. Extensive experiments are conducted on various challenging benchmarks to validate the effectiveness of our proposed method. The results demonstrate our approach's remarkable superiority, surpassing existing state-of-the-art methods by a large margin.
</details>
<details>
<summary>摘要</summary>
水印技术是现代版权保护的广泛采用方法之一。同时，研究焦点已经扩展到水印去除技术，提供了一种对抗性的方法，以提高水印的鲁棒性和推动水印技术的发展。现有的水印去除方法主要基于UNet架构，其中一个任务特定的解码分支用于水印localization，另一个分支用于背景图像修复。然而，水印localization和背景修复不是独立的任务，准确的水印localization直接 imply了需要修复的区域，而背景修复过程也会提高水印localization的准确性。为了整合这两个分支的信息，我们提出了隐式联合学习 paradigm。这种方法使得网络可以自动地在两个分支之间流动信息，通过门控制机制来自动调节信息的流动。此外，我们还使用了跨通道注意力来促进地方细节修复和整体结构认知，同时利用嵌入结构来整合多尺度信息。我们在多种复杂的 benchmark 上进行了广泛的实验，以验证我们的提出方法的有效性。结果表明，我们的方法在与现有状态顶的方法进行比较时表现出了惊人的优势，凌驱了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="EDDense-Net-Fully-Dense-Encoder-Decoder-Network-for-Joint-Segmentation-of-Optic-Cup-and-Disc"><a href="#EDDense-Net-Fully-Dense-Encoder-Decoder-Network-for-Joint-Segmentation-of-Optic-Cup-and-Disc" class="headerlink" title="EDDense-Net: Fully Dense Encoder Decoder Network for Joint Segmentation of Optic Cup and Disc"></a>EDDense-Net: Fully Dense Encoder Decoder Network for Joint Segmentation of Optic Cup and Disc</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10192">http://arxiv.org/abs/2308.10192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehwish Mehmood, Khuram Naveed, Haroon Ahmed Khan, Syed S. Naqvi<br>for: 这篇论文是为了检测和诊断眼内疾病 glaucoma 而写的。methods: 这篇论文提出了一种基于 dense block 的 EDDense-Net 分割网络，用于同时 segmentation of OC 和 OD。该网络使用 grouped convolutional layer 以获取和传递图像中的空间信息，并同时减少网络的复杂性。在 semantic segmentation 阶段，使用 dice pixel classification 来缓解类别不均衡问题。results: 该方法在两个公共可用的数据集上进行评估，并在精度和效率方面超越了现有的状态图像方法。这种方法可以用于医学眼科专业人员的助手，帮助他们诊断和分析眼内疾病。<details>
<summary>Abstract</summary>
Glaucoma is an eye disease that causes damage to the optic nerve, which can lead to visual loss and permanent blindness. Early glaucoma detection is therefore critical in order to avoid permanent blindness. The estimation of the cup-to-disc ratio (CDR) during an examination of the optical disc (OD) is used for the diagnosis of glaucoma. In this paper, we present the EDDense-Net segmentation network for the joint segmentation of OC and OD. The encoder and decoder in this network are made up of dense blocks with a grouped convolutional layer in each block, allowing the network to acquire and convey spatial information from the image while simultaneously reducing the network's complexity. To reduce spatial information loss, the optimal number of filters in all convolution layers were utilised. In semantic segmentation, dice pixel classification is employed in the decoder to alleviate the problem of class imbalance. The proposed network was evaluated on two publicly available datasets where it outperformed existing state-of-the-art methods in terms of accuracy and efficiency. For the diagnosis and analysis of glaucoma, this method can be used as a second opinion system to assist medical ophthalmologists.
</details>
<details>
<summary>摘要</summary>
Glaucoma 是一种眼病，可以导致视网膜损害，从而导致视力下降和永久失明。 Early detection of glaucoma 是非常重要，以避免永久失明。在诊断 glaucoma 时，可以使用 optical disc 的 cup-to-disc ratio（CDR）的估算。在这篇论文中，我们提出了 EDDense-Net 分割网络，用于对 optical disc 和 optic cup 的同时分割。这个网络包括 dense block 和 grouped convolutional layer，可以同时保持图像的空间信息并减少网络的复杂性。为了避免空间信息损失，我们在所有卷积层中使用了最佳的筛选器数量。在 semantic segmentation 中，我们使用 dice pixel classification 来缓解类别偏见问题。我们提出的网络在两个公共可用的数据集上进行了评估，并与现有的状态之arte方法相比，在准确性和效率方面表现出色。这种方法可以用于帮助医学眼科医生进行诊断和分析。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Diffusion-Model-with-Auxiliary-Guidance-for-Coarse-to-Fine-PET-Reconstruction"><a href="#Contrastive-Diffusion-Model-with-Auxiliary-Guidance-for-Coarse-to-Fine-PET-Reconstruction" class="headerlink" title="Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction"></a>Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10157">http://arxiv.org/abs/2308.10157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/show-han/pet-reconstruction">https://github.com/show-han/pet-reconstruction</a></li>
<li>paper_authors: Zeyu Han, Yuhan Wang, Luping Zhou, Peng Wang, Binyu Yan, Jiliu Zhou, Yan Wang, Dinggang Shen</li>
<li>for: 提高标准剂量 позитроном发射tomography（PET）成像质量，降低人体对PET成像的辐射暴露。</li>
<li>methods: 使用生成对抗网络（GANs）和扩散概率模型（DPMs）重建标准剂量PET（SPET）图像从低剂量PET（LPET）图像中。</li>
<li>results: 提出一种块级预测模块（CPM）和一种迭代纠正模块（IRM）组成的粗细预测架构，可以大幅提高样本速度，同时保持LPET图像和重建PET（RPET）图像之间的协同关系，提高临床可靠性。<details>
<summary>Abstract</summary>
To obtain high-quality positron emission tomography (PET) scans while reducing radiation exposure to the human body, various approaches have been proposed to reconstruct standard-dose PET (SPET) images from low-dose PET (LPET) images. One widely adopted technique is the generative adversarial networks (GANs), yet recently, diffusion probabilistic models (DPMs) have emerged as a compelling alternative due to their improved sample quality and higher log-likelihood scores compared to GANs. Despite this, DPMs suffer from two major drawbacks in real clinical settings, i.e., the computationally expensive sampling process and the insufficient preservation of correspondence between the conditioning LPET image and the reconstructed PET (RPET) image. To address the above limitations, this paper presents a coarse-to-fine PET reconstruction framework that consists of a coarse prediction module (CPM) and an iterative refinement module (IRM). The CPM generates a coarse PET image via a deterministic process, and the IRM samples the residual iteratively. By delegating most of the computational overhead to the CPM, the overall sampling speed of our method can be significantly improved. Furthermore, two additional strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, are proposed and integrated into the reconstruction process, which can enhance the correspondence between the LPET image and the RPET image, further improving clinical reliability. Extensive experiments on two human brain PET datasets demonstrate that our method outperforms the state-of-the-art PET reconstruction methods. The source code is available at \url{https://github.com/Show-han/PET-Reconstruction}.
</details>
<details>
<summary>摘要</summary>
为了获得高质量的 позиトрон发射Tomography（PET）扫描图像，而同时降低人体对X射线暴露，Various approaches have been proposed to reconstruct standard-dose PET（SPET）images from low-dose PET（LPET）images. One widely adopted technique is the generative adversarial networks（GANs）, yet recently, diffusion probabilistic models（DPMs）have emerged as a compelling alternative due to their improved sample quality and higher log-likelihood scores compared to GANs. Despite this, DPMs suffer from two major drawbacks in real clinical settings, i.e., the computationally expensive sampling process and the insufficient preservation of correspondence between the conditioning LPET image and the reconstructed PET（RPET）image. To address the above limitations, this paper presents a coarse-to-fine PET reconstruction framework that consists of a coarse prediction module（CPM）and an iterative refinement module（IRM）. The CPM generates a coarse PET image via a deterministic process, and the IRM samples the residual iteratively. By delegating most of the computational overhead to the CPM, the overall sampling speed of our method can be significantly improved. Furthermore, two additional strategies, i.e., an auxiliary guidance strategy and a contrastive diffusion strategy, are proposed and integrated into the reconstruction process, which can enhance the correspondence between the LPET image and the RPET image, further improving clinical reliability. Extensive experiments on two human brain PET datasets demonstrate that our method outperforms the state-of-the-art PET reconstruction methods. The source code is available at \url{https://github.com/Show-han/PET-Reconstruction}.
</details></li>
</ul>
<hr>
<h2 id="Federated-Pseudo-Modality-Generation-for-Incomplete-Multi-Modal-MRI-Reconstruction"><a href="#Federated-Pseudo-Modality-Generation-for-Incomplete-Multi-Modal-MRI-Reconstruction" class="headerlink" title="Federated Pseudo Modality Generation for Incomplete Multi-Modal MRI Reconstruction"></a>Federated Pseudo Modality Generation for Incomplete Multi-Modal MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10910">http://arxiv.org/abs/2308.10910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlu Yan, Chun-Mei Feng, Yuexiang Li, Rick Siow Mong Goh, Lei Zhu</li>
<li>for:  Addressing the missing modality challenge in federated multi-modal MRI reconstruction.</li>
<li>methods:  Utilizes a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space, and introduces a clustering scheme to reduce communication costs.</li>
<li>results:  Can effectively complete the missing modality within an acceptable communication cost, and attains similar performance with the ideal scenario, i.e., all clients have the full set of modalities.Here’s the Chinese translation of the three points:</li>
<li>for:  Addressing the 缺失多个Modalities的问题在分布式多modal MRI重建中。</li>
<li>methods:  Utilizes a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space, and introduces a clustering scheme to reduce communication costs.</li>
<li>results:  Can effectively complete the missing modality within an acceptable communication cost, and attains similar performance with the ideal scenario, i.e., all clients have the full set of modalities.<details>
<summary>Abstract</summary>
While multi-modal learning has been widely used for MRI reconstruction, it relies on paired multi-modal data which is difficult to acquire in real clinical scenarios. Especially in the federated setting, the common situation is that several medical institutions only have single-modal data, termed the modality missing issue. Therefore, it is infeasible to deploy a standard federated learning framework in such conditions. In this paper, we propose a novel communication-efficient federated learning framework, namely Fed-PMG, to address the missing modality challenge in federated multi-modal MRI reconstruction. Specifically, we utilize a pseudo modality generation mechanism to recover the missing modality for each single-modal client by sharing the distribution information of the amplitude spectrum in frequency space. However, the step of sharing the original amplitude spectrum leads to heavy communication costs. To reduce the communication cost, we introduce a clustering scheme to project the set of amplitude spectrum into finite cluster centroids, and share them among the clients. With such an elaborate design, our approach can effectively complete the missing modality within an acceptable communication cost. Extensive experiments demonstrate that our proposed method can attain similar performance with the ideal scenario, i.e., all clients have the full set of modalities. The source code will be released.
</details>
<details>
<summary>摘要</summary>
多Modal学习已经广泛应用于MRI重建，但它需要对配套多Modal数据进行训练，这在实际临床情况下很Difficult to acquire.特别是在联邦设置下，一些医疗机构只有单Modal数据，称为模式缺失问题。因此，使用标准联邦学习框架是不可能的。在这篇论文中，我们提出了一种新的通信效率高的联邦学习框架，即Fed-PMG，用于解决联邦多Modal MRI重建中的模式缺失问题。特别是，我们利用 pseudo Modality生成机制来为每个单Modal客户端 recuperate 缺失的模式。然而，在分享原始振荡谱的步骤中，会导致重大的通信成本。为了降低通信成本，我们引入了分区 schemes来将振荡谱集合投影到有限的群集中心，然后分享这些中心。通过这种精心的设计，我们的方法可以在接受ABLE的通信成本下完成缺失模式。EXTensive experiments表明，我们提出的方法可以 дости到与理想情况（即所有客户端具有完整的模式）相同的性能。源代码将被发布。
</details></li>
</ul>
<hr>
<h2 id="Polymerized-Feature-based-Domain-Adaptation-for-Cervical-Cancer-Dose-Map-Prediction"><a href="#Polymerized-Feature-based-Domain-Adaptation-for-Cervical-Cancer-Dose-Map-Prediction" class="headerlink" title="Polymerized Feature-based Domain Adaptation for Cervical Cancer Dose Map Prediction"></a>Polymerized Feature-based Domain Adaptation for Cervical Cancer Dose Map Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10142">http://arxiv.org/abs/2308.10142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Zeng, Zeyu Han, Xingchen Peng, Jianghong Xiao, Peng Wang, Yan Wang</li>
<li>for: 这篇研究是为了提高乳腺癌治疗规划的精确度，使用深度学习（DL）方法。</li>
<li>methods: 本研究使用对另一种肝癌（rectum cancer）进行了预测，并将其中学习到的丰富知识转移到乳腺癌上，以优化对乳腺癌的辐射规划预测性。</li>
<li>results: 实验结果显示，提案的方法比州先进方法更高效，并在两个实验数据集上显示出超过州先进方法的性能。<details>
<summary>Abstract</summary>
Recently, deep learning (DL) has automated and accelerated the clinical radiation therapy (RT) planning significantly by predicting accurate dose maps. However, most DL-based dose map prediction methods are data-driven and not applicable for cervical cancer where only a small amount of data is available. To address this problem, this paper proposes to transfer the rich knowledge learned from another cancer, i.e., rectum cancer, which has the same scanning area and more clinically available data, to improve the dose map prediction performance for cervical cancer through domain adaptation. In order to close the congenital domain gap between the source (i.e., rectum cancer) and the target (i.e., cervical cancer) domains, we develop an effective Transformer-based polymerized feature module (PFM), which can generate an optimal polymerized feature distribution to smoothly align the two input distributions. Experimental results on two in-house clinical datasets demonstrate the superiority of the proposed method compared with state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
To bridge the inherent domain gap between the source (rectum cancer) and target (cervical cancer) domains, we develop an effective Transformer-based polymerized feature module (PFM) that generates an optimal polymerized feature distribution to smoothly align the two input distributions. Experimental results on two in-house clinical datasets demonstrate the superiority of the proposed method compared with state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-analysis-of-AI-based-algorithms-for-autonomous-driving-on-optical-wavefront-aberrations-induced-by-the-windshield"><a href="#Sensitivity-analysis-of-AI-based-algorithms-for-autonomous-driving-on-optical-wavefront-aberrations-induced-by-the-windshield" class="headerlink" title="Sensitivity analysis of AI-based algorithms for autonomous driving on optical wavefront aberrations induced by the windshield"></a>Sensitivity analysis of AI-based algorithms for autonomous driving on optical wavefront aberrations induced by the windshield</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11711">http://arxiv.org/abs/2308.11711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Werner Wolf, Markus Ulrich, Nikhil Kapoor</li>
<li>for: 本研究旨在解决自动驾驶视觉技术中的领域转移问题，即使用不同汽车型号和镜头配置训练模型后，在不同镜头配置下运行模型的性能下降问题。</li>
<li>methods: 本研究采用了一种基于 Fourier optics 的威胁模型，对两种视觉模型进行评估，以确定它们在不同镜头配置下的性能敏感性。</li>
<li>results: 研究结果表明，镜头配置会引入性能差距，而现有的光学指标可能不够用于评估模型在不同镜头配置下的性能。<details>
<summary>Abstract</summary>
Autonomous driving perception techniques are typically based on supervised machine learning models that are trained on real-world street data. A typical training process involves capturing images with a single car model and windshield configuration. However, deploying these trained models on different car types can lead to a domain shift, which can potentially hurt the neural networks performance and violate working ADAS requirements. To address this issue, this paper investigates the domain shift problem further by evaluating the sensitivity of two perception models to different windshield configurations. This is done by evaluating the dependencies between neural network benchmark metrics and optical merit functions by applying a Fourier optics based threat model. Our results show that there is a performance gap introduced by windshields and existing optical metrics used for posing requirements might not be sufficient.
</details>
<details>
<summary>摘要</summary>
自主驾驶感知技术通常基于指导学习模型，这些模型通常是基于实际街道数据进行训练。一般训练过程中会使用单车型和顶部配置拍摄图像。但是，将这些训练模型应用于不同车型可能会导致领域shift，这可能会影响神经网络的性能，并违反工作ADAS要求。为解决这个问题，本文进一步探讨领域shift问题，并评估两种感知模型对不同顶部配置的敏感性。我们通过应用 Fourier optics 基于威胁模型来评估神经网络指标和光学功能之间的依赖关系。我们的结果表明，顶部配置会引入性能差异，现有的光学指标可能并不够。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/20/eess.IV_2023_08_20/" data-id="clmjn91qo00h30j88cv4495wm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/20/cs.SD_2023_08_20/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-20 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/19/cs.LG_2023_08_19/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-19 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
