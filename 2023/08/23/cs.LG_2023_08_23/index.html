
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-23 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12210 repo_url: https:&#x2F;&#x2F;github.com&#x2F;fumiyukikato&#x2F;uldp-fl paper_authors: Fumiyuki Kato,">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-23 18:00:00">
<meta property="og:url" content="http://nullscc.github.io/2023/08/23/cs.LG_2023_08_23/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12210 repo_url: https:&#x2F;&#x2F;github.com&#x2F;fumiyukikato&#x2F;uldp-fl paper_authors: Fumiyuki Kato,">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-22T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:51.846Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.LG_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-23 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy"><a href="#ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy" class="headerlink" title="ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy"></a>ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12210">http://arxiv.org/abs/2308.12210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fumiyukikato/uldp-fl">https://github.com/fumiyukikato/uldp-fl</a></li>
<li>paper_authors: Fumiyuki Kato, Li Xiong, Shun Takagi, Yang Cao, Masatoshi Yoshikawa</li>
<li>for: 保障用户级数据隐私在跨存储节点 federated learning 中（cross-silo federated learning）。</li>
<li>methods: 我们提出了一种新的 federated learning 框架（ULDP-FL），该框架通过每个用户Weighted clipping来直接保证用户级数据隐私，而不是基于组级隐私方法。我们也提供了对算法的理论分析和实践实现。</li>
<li>results: 我们的方法在实际 dataset 上实现了显著的隐私Utility 质量比较，特别是在用户级数据隐私下。此外，我们还对比了我们的方法与基eline方法，并证明了它们在隐私Utility 质量上的优势。<details>
<summary>Abstract</summary>
Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under user-level DP compared to baseline methods. To the best of our knowledge, our work is the first FL framework that effectively provides user-level DP in the general cross-silo FL setting.
</details>
<details>
<summary>摘要</summary>
differentially private federated learning (DP-FL) 已经吸引了关注，是一种合作机器学习方法，确保正式隐私。大多数 DP-FL 方法在每个隔离空间内保证 DP 记录级，但是一个用户的数据可能会涵盖多个隔离空间， Desired 用户级 DP 保证尚未得到解决。在这项研究中，我们提出了 ULDP-FL，一种新的 Federated Learning 框架，用于在跨隔离空间的 Federated Learning 中保证用户级隐私。我们的提议的算法直接确保用户级隐私通过每个用户Weighted clipping，与集体隐私方法不同。我们提供了对算法的隐私和实用性的理论分析。此外，我们提高了算法的实用性，并使用 криптографическиеbuilding blocks 实现私有的实现。实验表明，在真实的数据集上，我们的方法在隐私Utility 质量中具有显著提高，相比基eline方法。到目前为止，我们的工作是跨隔离空间 Federated Learning 中第一个有效提供用户级隐私的框架。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network"><a href="#Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network" class="headerlink" title="Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network"></a>Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12325">http://arxiv.org/abs/2308.12325</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Ho, Zhao-Heng Yin, Colin Zhang, Henry Overhauser, Kyle Swanson, Yang Ha</li>
<li>for: 这个研究旨在使用现代计算资源来预测药物的溶解度，以便在药品设计中更好地理解化学结构对化学性质的影响。</li>
<li>methods: 这个研究使用了两种机器学习模型：线性回归模型和图 convolutional neural network 模型，并在多个实验数据集上应用这两种模型。</li>
<li>results: 研究发现，GCNN 模型的性能最高，但是现在GCNN模型是一个黑盒模型，而线性回归模型的功能重要性分析可以提供更多有关化学结构下的化学性质的影响的信息。<details>
<summary>Abstract</summary>
Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advances in next generation high throughput screening.
</details>
<details>
<summary>摘要</summary>
预测药物的溶解性是医药工业中重要的任务，因此这是一个已经很好地研究的话题。在这项研究中，我们利用现代计算资源重新探讨了这个问题。我们使用了两种机器学习模型：线性回归模型和图 convolutional neural network 模型，在多个实验数据集上应用。两种方法都可以做出合理的预测，但GCNN模型表现最佳。然而，当前的GCNN模型是一个黑盒模型，而线性回归模型的特征重要性分析提供了更多的化学影响的下达。使用线性回归模型，我们显示了每个 функциональ组对总溶解性的影响。最终，了解化学结构对化学性质的影响是设计新药物的关键。未来的工作应该努力将GCNNs的高性能与线性回归的可读性结合起来，开启新的高通过率屏选择。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion"><a href="#Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion" class="headerlink" title="Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion"></a>Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12193">http://arxiv.org/abs/2308.12193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinshuo Li, Zhuo Jia, Wenkai Lu, Cao Song<br>for: 这个论文主要目标是提出一种基于自我超vis学习的三维磁场反转方法（SSKMI），用于估计地表磁场异常数据下的地下异常分布。methods: 该方法使用了一种封闭的循环反向模型和优化方法，通过最小化观察到重新估计的地表磁场异常差分的平均绝对错误来优化反转模型。此外，该方法还包括一个知识驱动模块，使得深度学习方法更加可读性。results: 对比实验表明，提出的方法能够在磁场反转任务中获得出色的性能，并且可以快速训练和更好地估计地下异常分布。<details>
<summary>Abstract</summary>
The magnetic inversion method is one of the non-destructive geophysical methods, which aims to estimate the subsurface susceptibility distribution from surface magnetic anomaly data. Recently, supervised deep learning methods have been widely utilized in lots of geophysical fields including magnetic inversion. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magnetic inversion by self-supervised deep learning. The proposed self-supervised knowledge-driven 3D magnetic inversion method (SSKMI) learns on the target field data by a closed loop of the inversion and forward models. Given that the parameters of the forward model are preset, SSKMI can optimize the inversion model by minimizing the mean absolute error between observed and re-estimated surface magnetic anomalies. Besides, there is a knowledge-driven module in the proposed inversion model, which makes the deep learning method more explicable. Meanwhile, comparative experiments demonstrate that the knowledge-driven module can accelerate the training of the proposed method and achieve better results. Since magnetic inversion is an ill-pose task, SSKMI proposed to constrain the inversion model by a guideline in the auxiliary loop. The experimental results demonstrate that the proposed method is a reliable magnetic inversion method with outstanding performance.
</details>
<details>
<summary>摘要</summary>
magnetic inversion method是一种非破坏地球物理方法，目的是从地面磁场异常数据中估计地下分布的吸引性分布。在最近的几年，深度学习方法在地球物理领域中广泛应用，但这些方法受到制作数据的限制，因为制作数据不是独立同分布的。因此，我们提出了基于自我监督的深度学习方法来实现磁场反Transform。我们的提议的自我监督知识驱动3D磁场反Transform方法（SSKMI）通过closed loop的反Transform和前向模型来学习目标场数据。给定前向模型参数固定，SSKMI可以通过最小化观察到重新估计的地面磁场异常值的平均绝对误差来优化反Transform模型。此外，知识驱动模块在提议的反Transform模型中，使得深度学习方法更加可解释。同时，对比 экспериментах表明，知识驱动模块可以加速提议方法的训练和获得更好的结果。由于磁场反Transform是一个不定系统，我们提出了在auxiliary loop中使用导向线来约束反Transform模型。实验结果表明，我们的方法是一种可靠的磁场反Transform方法，性能卓越。
</details></li>
</ul>
<hr>
<h2 id="Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting"><a href="#Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting" class="headerlink" title="Development and external validation of a lung cancer risk estimation tool using gradient-boosting"></a>Development and external validation of a lung cancer risk estimation tool using gradient-boosting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12188">http://arxiv.org/abs/2308.12188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plbenveniste/lungcancerrisk">https://github.com/plbenveniste/lungcancerrisk</a></li>
<li>paper_authors: Pierre-Louis Benveniste, Julie Alberge, Lei Xing, Jean-Emmanuel Bibault<br>for: 这个研究的目的是为了开发一个基于机器学习算法的 lung cancer 的发生可能性估计工具，以帮助患有肺癌的人士提前发现疾病并采取适当的预防措施。methods: 这个研究使用了两个数据集：PLCO 和 NLST，其中 PLCO 数据集包含了肺癌的风险因素、临床测量和结果，而 NLST 数据集则是一个 Validation 数据集。研究人员使用了 XGBoost 算法进行特征选择、超参数优化和模型补做，并将模型训练在 PLCO 数据集上，然后测试在 NLST 数据集上。results: 研究人员发现，使用 XGBoost 算法可以准确地估计肺癌的发生可能性，并且模型具有良好的准确性（Brier 分数为 0.044）和 ROC-AUC 值（PLCO 数据集上为 82%，NLST 数据集上为 70%）。此外，研究还发现，与美国医疗保险基金会（USPSTF）的指南相比，这种机器学习工具可以提供同等的回快，但是准确性更高（PLCO 数据集上的精度为 13.1%，对比 USPSTF 的 9.3%；NLST 数据集上的精度为 3.2%，对比 USPSTF 的 3.1%）。<details>
<summary>Abstract</summary>
Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates. In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years. The study utilized two datasets, the PLCO (n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer. Additionally, a focus was placed on mitigating bias caused by censored data. Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees. The ML model was trained on the pre-processed PLCO dataset and tested on the NLST dataset. The model incorporated features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer. The model was well-calibrated (Brier score=0.044). ROC-AUC was 82% on the PLCO dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When compared to the USPSTF guidelines for lung cancer screening, our model provided the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2% vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years. By utilizing risk factors and clinical data, individuals can assess their risk and make informed decisions regarding lung cancer screening. This research contributes to the efforts in early detection and prevention strategies, aiming to reduce lung cancer-related mortality rates.
</details>
<details>
<summary>摘要</summary>
肺癌是全球一大死亡原因，强调了早期检测的重要性，以提高生存率。本研究提出了一种基于机器学习（ML）技术，利用PLCO癌症creening试验和NLST试验数据，计算肺癌发生的可能性在5年内。研究使用了两个数据集：PLCO（n=55,161）和NLST（n=48,595），其中包括肺癌风险因素、临床测量和结果等信息。数据处理包括移除不是现任或前任烟民和不 relacionados于肺癌的死亡病例。此外，还尽量减少因 censored data 引起的偏见。在Feature selection、超参数优化和模型校准方面，使用了XGBoost ensemble学习算法，其 combine了梯度提升和决策树。模型在PLCO数据集上训练，并在NLST数据集上测试。模型包括年龄、性别、烟草历史、医疗诊断和家族史肺癌风险因素。模型具有良好的准确性（Brier score=0.044），ROC-AUC在PLCO数据集上为82%，在NLST数据集上为70%。PR-AUC分别为29%和11%。与美国Preventive Services Task Force（USPSTF）的肺癌检测指南相比，我们的模型具有同等的回归率，但精度为13.1% vs. 9.3%在PLCO数据集上和3.2% vs. 3.1%在NLST数据集上。已经开发出了一个可以在线使用的自由web应用程序，用于计算肺癌发生的可能性在5年内。通过利用风险因素和临床数据，个人可以评估自己的风险，并做出了有知识的决定 regarding肺癌检测。这些研究贡献到了早期检测和预防策略中，以减少肺癌相关的死亡率。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-decision-focused-surrogate-modeling"><a href="#Data-driven-decision-focused-surrogate-modeling" class="headerlink" title="Data-driven decision-focused surrogate modeling"></a>Data-driven decision-focused surrogate modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12161">http://arxiv.org/abs/2308.12161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddolab/decfocsurrmod">https://github.com/ddolab/decfocsurrmod</a></li>
<li>paper_authors: Rishabh Gupta, Qi Zhang<br>for:  solving computationally challenging nonlinear optimization problems in real-time settingsmethods: decision-focused surrogate modeling, data-driven framework, bilevel program, decomposition-based solution algorithmresults: significantly more data-efficient, simple surrogate models with high decision prediction accuracy<details>
<summary>Abstract</summary>
We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our approach is significantly more data-efficient while producing simple surrogate models with high decision prediction accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种专注决策的代理模型化技术，用于解决具有计算挑战的非线性优化问题。我们提议的数据驱动框架是通过寻找一个更简单的、例如几何的优化模型，来减少决策预测误差，这种误差定义为原始优化模型和代理优化模型的优化解的差异。我们将问题视为一个数据驱动的反向优化问题，并应用之前的研究中的分解解决方法。我们通过对化学反应器、热交换网络和材料混合系统等常见非线性化学过程进行数值实验，来验证我们的框架。我们还对决策受ocus surrogate模型与标准数据驱动代理模型进行了详细比较，并证明了我们的方法在数据效率方面明显高于标准方法，同时生成的决策简单优化模型具有高决策预测准确性。
</details></li>
</ul>
<hr>
<h2 id="An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization"><a href="#An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization" class="headerlink" title="An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization"></a>An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12126">http://arxiv.org/abs/2308.12126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weifeng Yang, Wenwen Min</li>
<li>for: 非 convex 和非平滑优化问题</li>
<li>methods: 使用加速版块 proximal 线性框架（ABPL$^+$），并提高比较过程来解决某些算法中的扩rapolation步骤失败问题</li>
<li>results: 可 monotonically 降低函数值，并且不需要严格限制扩rapolation参数和步长，同时可随机更新变量块更新顺序，并且可以在多种不同的场景下进行应用。<details>
<summary>Abstract</summary>
We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point set. Moreover, we demonstrate the global convergence as well as the linear and sublinear convergence rates of our algorithm by utilizing the Kurdyka-Lojasiewicz (K{\L}) condition. To enhance the effectiveness and flexibility of our algorithm, we also expand the study to the imprecise version of our algorithm and construct an adaptive extrapolation parameter strategy, which improving its overall performance. We apply our algorithm to multiple non-negative matrix factorization with the $\ell_0$ norm, nonnegative tensor decomposition with the $\ell_0$ norm, and perform extensive numerical experiments to validate its effectiveness and efficiency.
</details>
<details>
<summary>摘要</summary>
我们提出一种加速的块邻接线性框架（ABPL$^+$）用于非对称和非光滑优化问题。我们分析了一些算法中的扩rapolation步骤失败的可能原因，并解决这个问题 by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm。此外，我们扩展了我们的算法，以便在每个ecycle中随机排序变量块的更新顺序。此外，我们在某些假设下证明ABPL$^+$可以不Restricting the extrapolation parameters and step size monotonically decrease the function value，并且可以Randomly shuffle the update order of the variable blocks。此外，我们还证明了我们的算法的全面性和灵活性，并提出了一种 adaptive extrapolation parameter strategy。我们在多个非正式矩阵因子化、非正式矩阵归一化中应用了我们的算法，并进行了广泛的numerical experiments to validate its effectiveness and efficiency。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. The Traditional Chinese form may be used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators"><a href="#An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators" class="headerlink" title="An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators"></a>An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12120">http://arxiv.org/abs/2308.12120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Esmaeilzadeh, Soroush Ghodrati, Andrew B. Kahng, Joon Kyung Kim, Sean Kinzer, Sayak Kundu, Rohan Mahapatra, Susmita Dey Manasi, Sachin Sapatnekar, Zhiang Wang, Ziqing Zeng</li>
<li>for: 本研究旨在探讨物理设计驱动、学习基于的含义预测框架，以便对含义预测和硬件加速的深度学习和非深度学习机器学习算法进行可能的设计空间探索。</li>
<li>methods: 该研究提出了一种统一的方法，将backend power, performance,和面积（PPA）分析与前端性能估计结合起来，从而实现可靠地预测backend PPA和系统指标，如运行时间和能耗。此外，该研究还包括一种自动化的设计空间探索技术，通过自动化搜索Architecture和backend参数，以提高backend和系统指标。</li>
<li>results: 实验表明，该预测框架可以准确预测backend PPA和系统指标，在两个深度学习加速器平台（VTA和VeriGOOD-ML）的ASIC实现中，在商用12nm工艺和研究 oriented 45nm工艺中，平均预测误差为7%或更小。<details>
<summary>Abstract</summary>
Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a research-oriented 45 nm process.
</details>
<details>
<summary>摘要</summary>
现代机器学习（ML）加速器的开发，受到最近的 ML 突破所带来的启发。我们提议一个物理设计驱动、学习式预测框架，用于硬件加速深度学习网络（DNN）和非深度学习 ML 算法。这个方法结合了后端的电力、性能和面积（PPA）分析，与前端的性能模拟，以达成实际的后端 PPA 和系统特性，如运行时间和能源消耗。此外，我们的框架还包括一个完全自动化的设计空间探索技术，可以通过自动搜索架构和后端参数，来优化后端和系统特性。实验研究显示，我们的方法可以对 ASIC 实现二个深度学习加速器平台，VTA 和 VeriGOOD-ML，在商业 12 nm 制程和研究oriented 45 nm 制程中，平均预测后端 PPA 和系统特性的误差为7%或以下。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Stein-Variational-Trajectory-Optimization"><a href="#Constrained-Stein-Variational-Trajectory-Optimization" class="headerlink" title="Constrained Stein Variational Trajectory Optimization"></a>Constrained Stein Variational Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12110">http://arxiv.org/abs/2308.12110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Power, Dmitry Berenson</li>
<li>for: 本研究开发了一种可以同时处理多条路径的受限制曲线优化算法（Constrained Stein Variational Trajectory Optimization，CSVTO），以便在受限制的情况下进行曲线优化。</li>
<li>methods: 本研究使用stein可变 Gradient Descent（SVGD）来找到一组粒子，这组粒子可以近似一个低成本曲线的分布，同时遵循着组件的受限制。</li>
<li>results: 在实验中，CSVTO比基eline的表现更好，在7DoF钳操作任务中，CSVTO成功了20&#x2F;20次试验，而基eline只成功了13&#x2F;20次。这示示了，通过生成多个受限制的路径，可以提高初始化和干扰耐性。<details>
<summary>Abstract</summary>
We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO succeeds in 20/20 trials vs 13/20 for the closest baseline. Our results demonstrate that generating diverse constraint-satisfying trajectories improves robustness to disturbances and initialization over baselines.
</details>
<details>
<summary>摘要</summary>
我们介绍Constrained Stein Variational Trajectory Optimization（CSVTO）算法，用于并行进行有约束的轨迹优化。我们将受约束的轨迹优化视为一种新的约束函数最小化问题，这种方法可以避免对约束进行处理为对象中的罚 penalty，从而生成一些符合约束的多个轨迹。我们使用Stein Variational Gradient Descent（SVGD）算法来找到一组粒子，这些粒子用于近似一个低成本轨迹分布，同时遵循约束。CSVTO可以应用于具有平等和不平等约束的问题，并包括一种新的粒子重采样步骤以逃脱本地极小值。由于直接生成多个约束符合的轨迹，CSVTO更好地避免 initialization 和干扰的问题，并且更加稳定。我们在一个7DoF 工具捕获任务中展示了CSVTO的表现，其在20/20次试验中成功，而最接近的基eline只有13/20次成功。我们的结果表明，通过生成多个约束符合的轨迹，可以提高对干扰和 initialization 的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training"><a href="#Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training" class="headerlink" title="Cached Operator Reordering: A Unified View for Fast GNN Training"></a>Cached Operator Reordering: A Unified View for Fast GNN Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12093">http://arxiv.org/abs/2308.12093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Bazinska, Andrei Ivanov, Tal Ben-Nun, Nikoli Dryden, Maciej Besta, Siyuan Shen, Torsten Hoefler</li>
<li>for: 这篇论文的目的是提出一种可以优化Graph Neural Networks（GNNs）性能的方法，以Addressing tasks such as node classification, graph classification, and clustering。</li>
<li>methods: 这篇论文使用了一种统一的视图来描述GNN计算、输入&#x2F;输出和内存，并分析了Graph Convolutional Network（GCN）和Graph Attention（GAT）两种广泛使用的GNN层的计算图。</li>
<li>results: 这篇论文提出了一种称为adaptive operator reordering with caching的计算策略，可以将GCN的计算速度提高到2.43倍，并对GAT进行了不同的缓存方案，可以提高计算速度到1.94倍。这些优化方法可以降低训练大规模GNN模型的内存占用和性能瓶颈。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks in training large-scale GNN models.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 是一种强大的工具，用于处理结构化图数据，并解决节点分类、图分类和聚类等任务。然而，GNN 的稀疏性带来了新的性能优化挑战，与传统的深度神经网络不同。我们通过提供一个统一的视图，对 GNN 计算、输入和存储进行分析。我们分析了图几何网络（GCN）和图注意力（GAT）两种广泛使用的 GNN 层的计算图，并提出了一些代替计算策略。我们提出了自适应操作重新排序与缓存，可以将 GCN 的计算速度提高至最多 2.43 倍，而 GAT 的计算速度提高至最多 1.94 倍。这些优化可以降低内存占用量，易于实现多种硬件平台，并有助于减轻训练大规模 GNN 模型的性能瓶颈。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks"><a href="#Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks" class="headerlink" title="Sample Complexity of Robust Learning against Evasion Attacks"></a>Sample Complexity of Robust Learning against Evasion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12054">http://arxiv.org/abs/2308.12054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascale Gourdeau<br>for:  This paper focuses on understanding the vulnerability of machine learning models to adversarial attacks, specifically in the context of exact-in-the-ball robustness and sample complexity.methods:  The paper explores various learning models, including learning from random examples, learning with membership queries, and learning with local equivalence queries. The authors provide upper and lower bounds on the sample complexity of these models in the presence of adversarial attacks.results:  The paper shows that, under certain assumptions, it is possible to robustly learn conjunctions and decision lists w.r.t. log-Lipschitz distributions with sample complexity at least exponential in the adversary’s budget. However, if the adversary is restricted to perturbing $O(\log n)$ bits, then one can robustly learn conjunctions and decision lists w.r.t. log-Lipschitz distributions. The paper also introduces a local equivalence query oracle, which allows for robust empirical risk minimization algorithms in the distribution-free setting.<details>
<summary>Abstract</summary>
It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibility of adversarially robust learning from the perspective of learning theory, considering sample complexity.   We first explore the setting where the learner has access to random examples only, and show that distributional assumptions are essential. We then focus on learning problems with distributions on the input data that satisfy a Lipschitz condition and show that robustly learning monotone conjunctions has sample complexity at least exponential in the adversary's budget (the maximum number of bits it can perturb on each input). However, if the adversary is restricted to perturbing $O(\log n)$ bits, then one can robustly learn conjunctions and decision lists w.r.t. log-Lipschitz distributions.   We then study learning models where the learner is given more power. We first consider local membership queries, where the learner can query the label of points near the training sample. We show that, under the uniform distribution, the exponential dependence on the adversary's budget to robustly learn conjunctions remains inevitable. We then introduce a local equivalence query oracle, which returns whether the hypothesis and target concept agree in a given region around a point in the training sample, and a counterexample if it exists. We show that if the query radius is equal to the adversary's budget, we can develop robust empirical risk minimization algorithms in the distribution-free setting. We give general query complexity upper and lower bounds, as well as for concrete concept classes.
</details>
<details>
<summary>摘要</summary>
“机器学习模型对抗攻击的漏洞日益重要。一个基本问题是在恶作势攻击下如何评估训练数据的量。在这个论文中，我们使用精确在球体中的不可变性来测量模型的抗攻击能力，并从学习理论的角度研究抗攻击学习的可行性。我们首先考虑learner只有随机样本的存在，并证明了需要分布假设。然后，我们专注于具有 lipschitz 分布的学习问题，并证明了在恶作势攻击下可以robustly learning conjunctions和决策列表，并且其 sample complexity 至少是对抗者的预算（最大可以随意对 input 进行修改的位数）的对数。但是，如果抗敌者仅仅可以随意对 input 进行 $O(\log n)$ 位的修改，那么可以robustly learning conjunctions 和决策列表，并且其 sample complexity 是对抗者的预算的对数。然后，我们考虑learner更有实力的情况。我们首先考虑了本地会员询问，learner可以询问训练样本中的标签。我们证明了，在对于 uniform distribution 的情况下， exponential dependence on the adversary's budget to robustly learn conjunctions 是不可避免的。然后，我们引入了本地等价询问 oracle，它可以返回训练样本中的标签，并且可以返回一个条件是否存在Counterexample。我们证明了，如果询问半径等于抗敌者的预算，那么可以开发出在分布自由设定下的robust empirical risk minimization algorithm。我们还给出了一般的询问量上限和下限，以及具体的概念类别上的结果。”
</details></li>
</ul>
<hr>
<h2 id="Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD"><a href="#Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD" class="headerlink" title="Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD"></a>Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12018">http://arxiv.org/abs/2308.12018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Knolle, Robert Dorfman, Alexander Ziller, Daniel Rueckert, Georgios Kaissis</li>
<li>for: 这个研究旨在提高具有隐私保证的SGD（DP-SGD）的模型可用性和安全性。</li>
<li>methods: 本研究使用了一种新的渐进搜索策略——偏轨度识别（BAM），可以减少DP-SGD中私度估计器的偏轨度。</li>
<li>results:  empirical results demonstrate that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on several benchmark datasets.Here’s the full translation of the paper’s abstract in Simplified Chinese:</li>
<li>for: 这个研究旨在提高具有隐私保证的SGD（DP-SGD）的模型可用性和安全性。</li>
<li>methods: 本研究使用了一种新的渐进搜索策略——偏轨度识别（BAM），可以减少DP-SGD中私度估计器的偏轨度。</li>
<li>results:  empirical results demonstrate that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on several benchmark datasets.<details>
<summary>Abstract</summary>
Differentially private SGD (DP-SGD) holds the promise of enabling the safe and responsible application of machine learning to sensitive datasets. However, DP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This renders optimisation steps less effective and limits model utility as a result. With this work, we show a connection between per-sample gradient norms and the estimation bias of the private gradient oracle used in DP-SGD. Here, we propose Bias-Aware Minimisation (BAM) that allows for the provable reduction of private gradient estimator bias. We show how to efficiently compute quantities needed for BAM to scale to large neural networks and highlight similarities to closely related methods such as Sharpness-Aware Minimisation. Finally, we provide empirical evidence that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32 datasets.
</details>
<details>
<summary>摘要</summary>
diferencialmente privado SGD (DP-SGD) promete permitir la aplicación segura y responsable de aprendizaje automático a conjuntos de datos sensibles. Sin embargo, DP-SGD solo proporciona una estimación sesgada y ruidoso de un gradiente de mini-batch. Esto hace que los pasos de optimización sean menos efectivos y limite la utilidad del modelo como resultado. En este trabajo, mostramos una conexión entre las normas de gradiente por muestra y la bias del oracle de gradiente privado utilizado en DP-SGD. Aquí, propomos Bias-Aware Minimisation (BAM) que permite reducir provablemente la bias del estimador de gradiente privado. Mostramos cómo computar cuantidades necesarias para BAM escalable a redes neuronales grandes y highlight las similitudes con métodos relacionados como Sharpness-Aware Minimisation. Por último, proporcionamos evidencia empírica de que BAM no solo reduce la bias, sino que también mejora significativamente los trade-offs de privacidad-utilidad en los conjuntos de datos CIFAR-10, CIFAR-100 y ImageNet-32.
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Stochastic-Differential-Equations"><a href="#Graph-Neural-Stochastic-Differential-Equations" class="headerlink" title="Graph Neural Stochastic Differential Equations"></a>Graph Neural Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12316">http://arxiv.org/abs/2308.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Bergna, Felix Opolka, Pietro Liò, Jose Miguel Hernandez-Lobato</li>
<li>for: This paper is written for enhancing the current models of graph neural networks by incorporating randomness into the data representation to assess prediction uncertainty.</li>
<li>methods: The paper proposes a novel model called Graph Neural Stochastic Differential Equations (Graph Neural SDEs), which embeds Brownian motion into the data representation to improve the assessment of prediction uncertainty.</li>
<li>results: The paper shows that the proposed Latent Graph Neural SDE variant outperforms conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making it more effective in handling out-of-distribution detection in both static and spatio-temporal contexts.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提高现有的图 neural network模型，通过嵌入随机性到数据表示来评估预测uncertainty。</li>
<li>methods: 论文提出了一种新的模型——图 neural Stochastic Differential Equations（图 neural SDEs），通过嵌入 Браун运动来提高预测uncertainty的评估。</li>
<li>results: 论文显示，提出的 Latent Graph Neural SDE 变体在 confidence prediction 方面比现有的 Graph Convolutional Networks 和 Graph Neural ODEs 更高效，尤其在静态和空间-时间上。<details>
<summary>Abstract</summary>
We present a novel model Graph Neural Stochastic Differential Equations (Graph Neural SDEs). This technique enhances the Graph Neural Ordinary Differential Equations (Graph Neural ODEs) by embedding randomness into data representation using Brownian motion. This inclusion allows for the assessment of prediction uncertainty, a crucial aspect frequently missed in current models. In our framework, we spotlight the \textit{Latent Graph Neural SDE} variant, demonstrating its effectiveness. Through empirical studies, we find that Latent Graph Neural SDEs surpass conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making them superior in handling out-of-distribution detection across both static and spatio-temporal contexts.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的模型——图像神经随机微分方程（图像神经SDE）。这种技术将随机性embedded到数据表示中，使用布朗运动来实现。这种包含允许我们评估预测uncertainty，这是现有模型中常常缺失的一个重要方面。在我们的框架中，我们特别强调了《隐藏图像神经SDE》变体，并证明其效果。通过实验研究，我们发现Latent Graph Neural SDEs在预测 confidence 方面表现出色，特别是在面对非标准数据时，比如图像神经网络和图像神经ODEs。这使得Latent Graph Neural SDEs在处理out-of-distribution检测中表现出优异，特别是在静止和空间时间上。
</details></li>
</ul>
<hr>
<h2 id="MKL-L-0-1-SVM"><a href="#MKL-L-0-1-SVM" class="headerlink" title="MKL-$L_{0&#x2F;1}$-SVM"></a>MKL-$L_{0&#x2F;1}$-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12016">http://arxiv.org/abs/2308.12016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxis1718/simplemkl">https://github.com/maxis1718/simplemkl</a></li>
<li>paper_authors: Bin Zhu, Yijie Shi</li>
<li>for: 这 paper 是为了提出一个基于多 kernel 学习（简称 MKL）的支持向量机（SVM）模型，使用 $(0, 1)$ 损失函数。</li>
<li>methods: 这 paper 使用了一些首选条件，然后利用这些条件来开发一个快速的 ADMM 解决方案来处理非对称和非光滑优化问题。</li>
<li>results: 数据实验表明，我们的 MKL-$L_{0&#x2F;1}$-SVM 与 SimpleMKL 的性能相似，这个 SimpleMKL 是 Rakotomamonjy et al. 在 Journal of Machine Learning Research 上发表的一篇论文 [vol. 9, pp. 2491-2521, 2008] 中提出的一种方法。<details>
<summary>Abstract</summary>
This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-oscillators-for-magnetic-hysteresis-modeling"><a href="#Neural-oscillators-for-magnetic-hysteresis-modeling" class="headerlink" title="Neural oscillators for magnetic hysteresis modeling"></a>Neural oscillators for magnetic hysteresis modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12002">http://arxiv.org/abs/2308.12002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Chandra, Taniya Kapoor, Bram Daniels, Mitrofan Curti, Koen Tiels, Daniel M. Tartakovsky, Elena A. Lomonova<br>for: 模型和诊断材料科学和工程中的滞后现象，这种现象在系统的行为中具有重要作用。methods: 我们开发了基于Ordinary Differential Equation（ODE）的循环神经网络方法（RNN），这种方法可以模型和评估滞后现象的各种系统。results: 我们的研究表明，HystRNN可以在新的训练区域中展现出总的行为，这是一种必需的特征，因为滞后模型必须能够在不同的条件下进行扩展。这种研究展示了神经抗振荡器在描述磁materials中的滞后模式时的优势，因为传统的速率依赖方法在这些材料中是不充分的。<details>
<summary>Abstract</summary>
Hysteresis is a ubiquitous phenomenon in science and engineering; its modeling and identification are crucial for understanding and optimizing the behavior of various systems. We develop an ordinary differential equation-based recurrent neural network (RNN) approach to model and quantify the hysteresis, which manifests itself in sequentiality and history-dependence. Our neural oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states. The performance of HystRNN is evaluated to predict generalized scenarios, involving first-order reversal curves and minor loops. The findings show the ability of HystRNN to generalize its behavior to previously untrained regions, an essential feature that hysteresis models must have. This research highlights the advantage of neural oscillators over the traditional RNN-based methods in capturing complex hysteresis patterns in magnetic materials, where traditional rate-dependent methods are inadequate to capture intrinsic nonlinearity.
</details>
<details>
<summary>摘要</summary>
频率依赖现象（干扰）是科学和工程中的一种普遍现象，其模型化和识别是理解和优化不同系统的行为的关键。我们开发了基于常微分方程的循环神经网络（RNN）方法来模型和评估干扰，这种干扰在时间序列和历史相互关联的情况下表现出来。我们的神经普朗（HystRNN） drawing inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states。我们评估了HystRNN的性能，可以预测多个扩展场景，包括第一顺序反转曲线和小循环。发现HystRNN可以在未经训练的区域中展现出普适的行为，这是干扰模型中的一个重要特点。这项研究指出了神经普朗比传统的RNN基本方法更有优势，在捕捉磁材料中的内在非线性方面。
</details></li>
</ul>
<hr>
<h2 id="On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget"><a href="#On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget" class="headerlink" title="On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget"></a>On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12000">http://arxiv.org/abs/2308.12000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-An Wang, Kaito Ariu, Alexandre Proutiere</li>
<li>for: 这个论文研究了固定预算下最佳臂 identification问题的权衡问题，具体来说是在随机两臂抽奖机中的 Bernoulli 奖励中。</li>
<li>methods: 作者使用了consistent和stable算法的自然类型，并证明了这种算法在所有情况下都能达到 uniform 抽奖算法的性能，并且不能超过它在任何情况下。</li>
<li>results: 作者证明了surprisingly，不存在一个能够在所有情况下比 uniform 抽奖算法更好的算法，同时也不存在一个能够在任何情况下超过 uniform 抽奖算法的算法。<details>
<summary>Abstract</summary>
We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
</details>
<details>
<summary>摘要</summary>
我们研究了固定预算下最佳臂标识问题，具体是在随机二臂抽奖机中，具有 Bernoulli 奖励。我们证明了，奇怪地，没有算法可以在所有情况下和uniform sampling 算法（即每次平均选择一个臂）相当或更好，并且在至少一个情况下 strictly 超越 uniform sampling 算法。简而抽象，uniform sampling 算法是无可比的。我们引入了自然的consistent 和stable 算法集，并证明任何可以在所有情况下与 uniform sampling 算法相当的算法都属于这个集。证明完毕后，我们 derivation 了一个any consistent和stable算法的下界，并证明 uniform sampling 算法符合这个下界。我们的结果解决了 \cite{qin2022open} 中的两个开问题。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting"><a href="#Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting"></a>Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11946">http://arxiv.org/abs/2308.11946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhang, Rui Wu, Sergiu M. Dascalu, Frederick C. Harris Jr</li>
<li>for: 这篇论文旨在提出一种基于Transformer的多时间序列（MTS）预测方法，以捕捉多种不同的时季特征，包括小时间步和日径模式。</li>
<li>methods: 本文提出了一种维度不变的嵌入技术，用于捕捉MTS资料中的短时间相互dependencies，并将MTS资料转换为一个更高维度的空间中。此外，本文还提出了一种新的多尺度Transformer弹簧网络（MTPNet），用于有效地捕捉MTS资料中的多个不定比例的时间相互dependencies。</li>
<li>results: 实验结果显示，提出的MTPNet方法在九个benchmark dataset上比较 latest state-of-the-art方法更高的预测性能。<details>
<summary>Abstract</summary>
Multivariate Time Series (MTS) forecasting involves modeling temporal dependencies within historical records. Transformers have demonstrated remarkable performance in MTS forecasting due to their capability to capture long-term dependencies. However, prior work has been confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2). This limitation hinders their effectiveness in capturing diverse seasonalities, such as hourly and daily patterns. In this paper, we introduce a dimension invariant embedding technique that captures short-term temporal dependencies and projects MTS data into a higher-dimensional space, while preserving the dimensions of time steps and variables in MTS data. Furthermore, we present a novel Multi-scale Transformer Pyramid Network (MTPNet), specifically designed to effectively capture temporal dependencies at multiple unconstrained scales. The predictions are inferred from multi-scale latent representations obtained from transformers at various scales. Extensive experiments on nine benchmark datasets demonstrate that the proposed MTPNet outperforms recent state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）预测 involve 模型历史记录中的时间相关性。 transformers 已经表现出了很好的表现，因为它们可以捕捉长期相关性。然而，先前的工作受限于模型时间相关性的固定缩放或多个缩放因子，其中大多数以基数2为底。这些限制碍着它们在捕捉多样化的季节性，如每小时和每天的模式。在这篇论文中，我们介绍了一种维度不变的嵌入技术，该技术可以捕捉短期时间相关性，并将 MTS 数据 проек到一个更高维度的空间中，保持时间步骤和变量的维度在 MTS 数据中。此外，我们提出了一种专门为了有效地捕捉多个不受限制的缩放尺度而设计的多尺度 transformer  пирамиidal 网络（MTPNet）。该网络可以从多个缩放级别的 transformers 中获得多个尺度的干扰表示，并将其用于预测。广泛的实验表明，提议的 MTPNet 可以超越最近的状态艺术方法。
</details></li>
</ul>
<hr>
<h2 id="System-Identification-for-Continuous-time-Linear-Dynamical-Systems"><a href="#System-Identification-for-Continuous-time-Linear-Dynamical-Systems" class="headerlink" title="System Identification for Continuous-time Linear Dynamical Systems"></a>System Identification for Continuous-time Linear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11933">http://arxiv.org/abs/2308.11933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jonas-Nicodemus/phdmd">https://github.com/Jonas-Nicodemus/phdmd</a></li>
<li>paper_authors: Peter Halmos, Jonathan Pillow, David A. Knowles</li>
<li>for: 这篇论文主要针对系统识别问题，即使 Observations 是不规则的时间点抽样。</li>
<li>methods: 该论文使用了 expectation-maximization（EM）程序来学习下落系统的参数。</li>
<li>results: 该论文提出了一种新的两filter方法，可以efficiently计算 posterior，并且可以自适应地处理不规则的抽样。这种方法可以扩展kalman filter的学习范围，并且可以扩展非线性系统识别方法，如 switching LDS。<details>
<summary>Abstract</summary>
The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurements. Generalizing the learning of latent linear dynamical systems (LDS) to continuous-time may extend the use of the hybrid Kalman filter to data which is not regularly sampled or has intermittent missing values, and can extend the power of non-linear system identification methods such as switching LDS (SLDS), which rely on EM for the linear discrete-time Kalman filter as a sub-unit for learning locally linearized behavior of a non-linear system. We apply the method by learning the parameters of a latent, multivariate Fokker-Planck SDE representing a toggle-switch genetic circuit using biologically realistic parameters, and compare the efficacy of learning relative to the discrete-time Kalman filter as the step-size irregularity and spectral-radius of the dynamics-matrix increases.
</details>
<details>
<summary>摘要</summary>
系统识别问题 для卡尔曼筛，利用期望最大化（EM）过程学习下落系统的参数，已经广泛研究，假设观测数据点为均匀时间点抽样。然而，在许多应用中，这是一种过于狭隘和不切实际的假设。本文解决了继续-抽样筛的系统识别问题，通过基于继续时间Itô抽Diffusion Equation（SDE）的latent状态和covariance动态学来扩展卡尔曼筛的学习。我们提出了一种新的两filteranalytical形式的 posterior，使用权重 derive Bayesian derivation，这些analytical更新不需要先computed forward-pass。使用这种analytical和高效的计算 posterior，我们提供了一种EM过程，可以Estimate parameters SDE，自然地包括不规则的观测数据。扩展hybrid Kalman filter的学习到继续时间可能扩展其应用范围，包括不Regularly sampled measurements或具有间歇性的缺失值。此外，我们可以通过EM学习非线性系统的本地化行为，例如拓扑switching LDS（SLDS），这些方法依赖于EM来学习紧定时间Kalman筛的参数。我们在learning toggle-switch生物学Circuit的参数使用生物学实际参数，并与精度随时间增长的步长和动态矩阵的spectral-radius增长相比较评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas"><a href="#Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas" class="headerlink" title="Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas"></a>Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11929">http://arxiv.org/abs/2308.11929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cli-de/d_lsm">https://github.com/cli-de/d_lsm</a></li>
<li>paper_authors: Peifeng Ma, Li Chen, Chang Yu, Qing Zhu, Yulin Ding<br>for: 这个研究旨在提供一个能够适应不同时间间隔的Dynamic Landslide Susceptibility Mapping（DLSM）方法，以便更好地评估滑坡风险。methods: 本研究使用多个预测模型来进行年度预测，并使用SHAP来解释每个模型的预测结果和滑坡特征的 permutation。此外，本研究还使用MT-InSAR来强化和验证预测结果。results: 研究结果显示，在香港大屿山的滑坡诱因主要是地形坡度和极端雨量，而这些滑坡诱因的变化主要受到全球气候变革和香港政府实施的LPMitP计划的影响。<details>
<summary>Abstract</summary>
Landslide susceptibility assessment (LSA) is of paramount importance in mitigating landslide risks. Recently, there has been a surge in the utilization of data-driven methods for predicting landslide susceptibility due to the growing availability of aerial and satellite data. Nonetheless, the rapid oscillations within the landslide-inducing environment (LIE), primarily due to significant changes in external triggers such as rainfall, pose difficulties for contemporary data-driven LSA methodologies to accommodate LIEs over diverse timespans. This study presents dynamic landslide susceptibility mapping that simply employs multiple predictive models for annual LSA. In practice, this will inevitably encounter small sample problems due to the limited number of landslide samples in certain years. Another concern arises owing to the majority of the existing LSA approaches train black-box models to fit distinct datasets, yet often failing in generalization and providing comprehensive explanations concerning the interactions between input features and predictions. Accordingly, we proposed to meta-learn representations with fast adaptation ability using a few samples and gradient updates; and apply SHAP for each model interpretation and landslide feature permutation. Additionally, we applied MT-InSAR for LSA result enhancement and validation. The chosen study area is Lantau Island, Hong Kong, where we conducted a comprehensive dynamic LSA spanning from 1992 to 2019. The model interpretation results demonstrate that the primary factors responsible for triggering landslides in Lantau Island are terrain slope and extreme rainfall. The results also indicate that the variation in landslide causes can be primarily attributed to extreme rainfall events, which result from global climate change, and the implementation of the Landslip Prevention and Mitigation Programme (LPMitP) by the Hong Kong government.
</details>
<details>
<summary>摘要</summary>
陡峰风险评估（LSA）在降低山崩风险方面具有重要的意义。在最近几年，由于飞地和卫星数据的更加普遍的使用，数据驱动方法在预测山崩风险方面得到了广泛的应用。然而，由于山崩 triggers环境（LIE）中的快速振荡，主要归因于外部触发因素的重要变化，如降水量的增加，这会使当今数据驱动LSA方法难以涵盖LIE的多样性。本研究提出了动态山崩风险地图，使用多个预测模型来实现年度LSA。在实践中，这将遇到小样本问题，因为certain years的山崩样本数量有限。另一个问题在于现有LSA方法通常将黑盒模型训练到特定数据集，然后在总体化和提供全面的输入特征和预测之间的交互关系。为此，我们提出了使用少量样本和梯度更新来学习代表，并应用SHAP来解释每个模型和山崩特征的 permutation。此外，我们还应用MT-InSAR来增强和验证LSA结果。我们选择的研究区是香港大屿山岛，我们在1992年至2019年之间进行了全面的动态LSA。模型解释结果表明，lantau岛山崩的主要诱因是地形坡度和极端降水量。结果还显示，由于全球气候变化而导致的极端降水事件和香港政府实施的LPMitP计划，对山崩诱因的变化产生了主要影响。
</details></li>
</ul>
<hr>
<h2 id="Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks"><a href="#Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks" class="headerlink" title="Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks"></a>Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11925">http://arxiv.org/abs/2308.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangti Jin, Ramesh Sau, Luowei Yin, Zhi Zhou</li>
<li>for: 这篇论文是关于优化控制问题的数值解决方案，包括不包括箱Constraints的情况。</li>
<li>methods: 该方法基于优化控制问题的第一个最优化系统，并使用物理学知识引入神经网络解决coupled系统。</li>
<li>results: 该paper提供了深度神经网络参数（例如深度、宽度和参数 bound）和样本点的数量对$L^2(\Omega)$ error bounds的分析，并提供了several numerical examples来说明方法和与其他三种方法进行比较。<details>
<summary>Abstract</summary>
In this work, we present and analyze a numerical solver for optimal control problems (without / with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍和分析了一种数值解决方法 для优化控制问题（无框约束和带框约束）的线性和半线性二次几何问题。该方法基于优化控制问题的第一个可能性系统而 derivation，并应用物理学 Informed Neural Networks (PINNs) 解决这个联系系统。我们提供 $L^2(\Omega)$ 误差分析，并在深度神经网络参数（例如深度、宽度和参数范围）和样本点数量上提供误差界限。主要分析工具包括偏移Rademacher复杂度和稳定性和 lipschitz连续性神经网络函数。我们提供了几个数据示例，用于说明方法和与三种现有方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach"><a href="#Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach" class="headerlink" title="Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach"></a>Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11912">http://arxiv.org/abs/2308.11912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/riiid/useraif">https://github.com/riiid/useraif</a></li>
<li>paper_authors: Soonwoo Kwon, Sojung Kim, Seunghyun Lee, Jin-Young Kim, Suyeong An, Kyuseok Kim<br>for:  This paper aims to address the issue of obtaining item profiles for Computerized Adaptive Testing (CAT) by leveraging response data collected in the CAT service.methods:  The proposed method uses the user-wise aggregate influence function to filter out users whose response data is heavily biased in an aggregate manner.results:  The proposed method is shown to be superior to naive training of the diagnostic model using CAT response data, with minimal bias introduced to the item profiles. The method is demonstrated through extensive experiments on four datasets, including one with real-world CAT response data.<details>
<summary>Abstract</summary>
Computerized Adaptive Testing (CAT) is a widely used, efficient test mode that adapts to the examinee's proficiency level in the test domain. CAT requires pre-trained item profiles, for CAT iteratively assesses the student real-time based on the registered items' profiles, and selects the next item to administer using candidate items' profiles. However, obtaining such item profiles is a costly process that involves gathering a large, dense item-response data, then training a diagnostic model on the collected data. In this paper, we explore the possibility of leveraging response data collected in the CAT service. We first show that this poses a unique challenge due to the inherent selection bias introduced by CAT, i.e., more proficient students will receive harder questions. Indeed, when naively training the diagnostic model using CAT response data, we observe that item profiles deviate significantly from the ground-truth. To tackle the selection bias issue, we propose the user-wise aggregate influence function method. Our intuition is to filter out users whose response data is heavily biased in an aggregate manner, as judged by how much perturbation the added data will introduce during parameter estimation. This way, we may enhance the performance of CAT while introducing minimal bias to the item profiles. We provide extensive experiments to demonstrate the superiority of our proposed method based on the three public datasets and one dataset that contains real-world CAT response data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models"><a href="#Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models" class="headerlink" title="Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models"></a>Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11890">http://arxiv.org/abs/2308.11890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, Xia Ning</li>
<li>for: 这篇论文的目的是设计类似形状的药物候选者。</li>
<li>methods: 这篇论文使用了一个名为ShapeMol的实验方法，这个方法包括一个等态的形状Encoder和一个等态的扩散模型，它们可以将形状转换为几何特征，并将这些几何特征用于生成3D分子结构。</li>
<li>results: 实验结果显示ShapeMol可以生成新、多样的、适合药物的3D分子结构，这些结构具有与原始形状相似的3D分子形状。这些结果显示ShapeMol可以实现设计需要特定3D形状的药物候选者。<details>
<summary>Abstract</summary>
Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本中文翻译为简化字符串。<</SYS>>ligand-based drug design的目标是找到与已知活跃分子相似的新药候选者。在这篇论文中，我们定义了一个在 silico  shape-conditioned molecule generation问题，以生成具有给定分子形状的3D分子结构。为解决这个问题，我们开发了一种具有平衡性和旋转平衡性的形态引导生成模型ShapeMol。ShapeMol包括一种具有平衡性的形态编码器，该编码器将分子表面形状映射到缓存中的嵌入中，以及一种具有平衡性的扩散模型，该模型基于这些嵌入生成3D分子。实验结果表明，ShapeMol可以生成新、多样、药理学上适用的3D分子结构，这些结构具有与给定形状相似的3D分子形态。这些结果表明ShapeMol在设计拥有感兴趣3D形态的药物候选者方面具有潜在的潜力。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-Using-Feedback-Loops"><a href="#Adversarial-Training-Using-Feedback-Loops" class="headerlink" title="Adversarial Training Using Feedback Loops"></a>Adversarial Training Using Feedback Loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11881">http://arxiv.org/abs/2308.11881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Haisam Muhammad Rafid, Adrian Sandu</li>
<li>for: 防御黑客攻击（adversarial attacks）</li>
<li>methods: 使用控制理论和反馈控制（feedback control），提出一种基于反馈控制的强化方法（Feedback Looped Adversarial Training，FLAT）</li>
<li>results: 对标准测试问题进行数值实验，比STATE-OF-THE-ART更有效地防御黑客攻击<details>
<summary>Abstract</summary>
Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.   This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numerical results on standard test problems empirically show that our FLAT method is more effective than the state-of-the-art to guard against adversarial attacks.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）因其能够准确地学习复杂的输入输出关系而在各个领域得到广泛应用。然而，由于它们的局限性，DNN受到了针对性攻击的威胁。为了在这个领域取得Future进展，建立能够抵抗数据点上任何类型的杂散的DNN是非常重要。在过去，许多技术被提议来强化DNN的Robustness，包括使用first-orderDerivative信息。这篇论文提出了一种基于控制理论的新的Robustification方法，称为Feedback Looped Adversarial Training（FLAT）。该方法基于反馈控制架构，其中一个是一个神经网络，通过常规和针对性数据进行训练，以稳定系统输出。我们的FLAT方法在标准测试问题上的数值实验结果表明，与当前最佳方法相比，它更有效地防止针对性攻击。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations"><a href="#Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations" class="headerlink" title="Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations"></a>Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11873">http://arxiv.org/abs/2308.11873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp1511unsw/dcc">https://github.com/comp1511unsw/dcc</a></li>
<li>paper_authors: Andrew Taylor, Alexandra Vassar, Jake Renzella, Hammond Pearce</li>
<li>for: 这个论文旨在使用大语言模型（LLM）生成改进的编译器错误说明，以便在我们的调试C编译器（DCC）中提高初学者学习编程的经验。</li>
<li>methods: 该论文使用了LLM生成错误说明，并通过专家评估发现LLM生成的错误说明概念准确率为90%，运行时错误说明准确率为75%。</li>
<li>results: 该论文发现，通过LLM生成的错误说明可以更好地帮助初学者理解编程错误，并且DCC-help工具在学生中得到了普遍的采用，每周1047次唯一运行，表明使用LLM complement compiler output可以有效地提高初学者编程教育。<details>
<summary>Abstract</summary>
This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, demonstrating a promising initial assessment of using LLMs to complement compiler output to enhance programming education for beginners. We release our tool as open-source to the community.
</details>
<details>
<summary>摘要</summary>
Through expert evaluation, we found that LLM-generated explanations for compiler errors were conceptually accurate 90% of the time at compile-time and 75% of the time at run-time. Additionally, the new DCC-help tool has been widely adopted by students, with an average of 1047 unique runs per week, indicating a promising initial assessment of using LLMs to complement compiler output and enhance programming education for beginners. We are releasing our tool as open-source to the community.
</details></li>
</ul>
<hr>
<h2 id="Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form"><a href="#Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form" class="headerlink" title="Fast Exact NPN Classification with Influence-aided Canonical Form"></a>Fast Exact NPN Classification with Influence-aided Canonical Form</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12311">http://arxiv.org/abs/2308.12311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhang, Liwei Ni, Jiaxi Zhang, Guojie Luo, Huawei Li, Shenggen Zheng</li>
<li>for: 这个论文的目的是为了提高NPNN类型的分类速度。</li>
<li>methods: 这个论文使用了Boolean影响来改进NPNN类型的 canonical form 计算算法，并通过Introducing Boolean influence to NPN classification, 提高了NPNN类型的分类速度。</li>
<li>results: 实验结果表明，Boolean影响在NPNN类型的分类中扮演着重要的角色，可以减少转换枚举的计算量，并与ABC算法相比，我们的影响帮助的canonical form  для精确NPNN分类可以获得到5.5倍的速度提升。<details>
<summary>Abstract</summary>
NPN classification has many applications in the synthesis and verification of digital circuits. The canonical-form-based method is the most common approach, designing a canonical form as representative for the NPN equivalence class first and then computing the transformation function according to the canonical form. Most works use variable symmetries and several signatures, mainly based on the cofactor, to simplify the canonical form construction and computation. This paper describes a novel canonical form and its computation algorithm by introducing Boolean influence to NPN classification, which is a basic concept in analysis of Boolean functions. We show that influence is input-negation-independent, input-permutation-dependent, and has other structural information than previous signatures for NPN classification. Therefore, it is a significant ingredient in speeding up NPN classification. Experimental results prove that influence plays an important role in reducing the transformation enumeration in computing the canonical form. Compared with the state-of-the-art algorithm implemented in ABC, our influence-aided canonical form for exact NPN classification gains up to 5.5x speedup.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
<li>for: 本研究旨在提高基于坦桑尼亚语的语音识别性能。</li>
<li>methods: 我们使用自然语言模型进行自动预训练，然后在精度训练中采用简单的课程计划，并使用半监督学习来利用大量未标注的语音数据。</li>
<li>results: 我们的最终模型在新的studio质量语音数据集上达到了3.2%词错率（WER），在Mozilla Common Voicebenchmark上达到了15.9%WER，这是目前最佳的性能。我们的实验还表明，使用 syllabic而非字符基本tokenization可以提高基于坦桑尼亚语的语音识别性能。<details>
<summary>Abstract</summary>
Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.
</details>
<details>
<summary>摘要</summary>
尽管最近有大量的坦语 recording数据可用，但实现坦语 speech recognition仍然是一项挑战。在这个工作中，我们显示了使用自我超vision预训练、在精度训练中采用简单的课程计划以及使用半有 Label 学习来利用大量无标的语音数据，可以显著提高坦语 speech recognition性能。我们的方法仅使用公共领域数据。我们首先收集了一个新的studio质量语音dataset，然后使用这个dataset来训练一个clean基线模型。然后，我们使用这个基线模型来排序来自更多和噪音的公共数据集中的示例，定义了一个简单的课程训练计划。最后，我们应用半有 Label 学习来标注和学习大量无标的数据，并在四个Successive Generation中进行了四次Generation。我们的最终模型在新dataset上达到了3.2%的单词错误率（WER），并在Mozilla Common Voice标准测试集上达到了15.9%的WER，这与我们所知道的最新的状态均为最佳。我们的实验还表明，使用 syllabic 而非字符基本的分词会提高坦语 speech recognition性能。
</details></li>
</ul>
<hr>
<h2 id="SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks"><a href="#SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks" class="headerlink" title="SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks"></a>SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11845">http://arxiv.org/abs/2308.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Gao, Ilia Shumailov, Kassem Fawaz</li>
<li>for: 这篇论文旨在提供一种基于隐马尔可夫模型的机器学习安全系统，以帮助掌握黑盒攻击和分析攻击者的行为。</li>
<li>methods: 该系统使用隐马尔可夫模型框架来将观察到的查询序列归类为已知的攻击。它可以理解攻击的演进，而不仅仅是Focus on the final adversarial examples。</li>
<li>results: 评估表明，该系统能够有效地归类攻击，即使是第二次出现。它还能够抵御适应攻击策略，并且可以提供人类可解释的情报分享。例如，它可以揭示特定的攻击库中的小实现漏洞。<details>
<summary>Abstract</summary>
Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, there is a need for a more comprehensive approach to logging, analyzing, and sharing evidence of attacks. While classic security benefits from well-established forensics and intelligence sharing, Machine Learning is yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages the Hidden Markov Models framework to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on their second occurrence, and is robust to adaptive strategies designed to evade forensics analysis. Interestingly, SEA's explanations of the attack behavior allow us even to fingerprint specific minor implementation bugs in attack libraries. For example, we discover that the SignOPT and Square attacks implementation in ART v1.14 sends over 50% specific zero difference queries. We thoroughly evaluate SEA on a variety of settings and demonstrate that it can recognize the same attack's second occurrence with 90+% Top-1 and 95+% Top-3 accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures"><a href="#A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures" class="headerlink" title="A Survey for Federated Learning Evaluations: Goals and Measures"></a>A Survey for Federated Learning Evaluations: Goals and Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11841">http://arxiv.org/abs/2308.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Chai, Leye Wang, Liu Yang, Junxue Zhang, Kai Chen, Qiang Yang</li>
<li>for: 本研究的目的是对 Federated Learning（FL）的评估方法进行系统性评估，以确定FL在实现其目标时的表现。</li>
<li>methods: 本研究使用了多种评估方法，包括 utility、效率和安全性等多个目标的评估。同时，本研究还介绍了一个开源的评估平台—FedEval，用于对FL算法进行标准化和全面的评估。</li>
<li>results: 本研究结果表明，FL在实现Utility、效率和安全性等多个目标方面的表现具有一定的挑战性。同时，本研究还提出了一些未来研究方向，以更好地评估FL的表现。<details>
<summary>Abstract</summary>
Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.
</details>
<details>
<summary>摘要</summary>
评估是一种系统的方法，用于评估系统是否实现了其意图的目的。联邦学习（FL）是一种新的隐私保护机器学习方法，允许多方共同训练模型无需分享敏感数据。然而，评估FL很困难，因为它的交叉学科性和多样化目标，如实用性、效率和安全性。在这种调查中，我们首先审查了现有研究中采用的主要评估目标，然后探索每个目标的评估指标。我们还介绍了FedEval，一个开源平台，提供了一个标准化和全面的评估框架，用于评估FL算法的实用性、效率和安全性。最后，我们讨论了一些挑战和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection"><a href="#Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection" class="headerlink" title="Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection"></a>Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11834">http://arxiv.org/abs/2308.11834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tosin Ige, Christopher Kiekintveld</li>
<li>for: 这个研究的目的是实现和比较不同版本的潘氏分类器（多个分类、 Берну利分类和高斯分类）在网络入侵探测中的性能，并investigate whether there is any association between each variant assumption and their performance.</li>
<li>methods: 这个研究使用了不同版本的潘氏分类器，包括多个分类、 Bernoulli 分类和高斯分类，并通过实验测试它们在网络入侵探测中的性能。</li>
<li>results: 实验结果显示， Gaussian 分类器在网络入侵探测中的性能最好，其中的准确率为 81.69% 测试集（82.84% 训练集）；Multinomial 分类器的性能很差，其中的准确率仅为 31.2% 测试集（31.2% 训练集）；Bernoulli 分类器的性能介于两者之间，其中的准确率为 69.9% 测试集（71% 训练集）。<details>
<summary>Abstract</summary>
Bayesian classifiers perform well when each of the features is completely independent of the other which is not always valid in real world application. The aim of this study is to implement and compare the performances of each variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on anomaly detection in network intrusion, and to investigate whether there is any association between each variant assumption and their performance. Our investigation showed that each variant of Bayesian algorithm blindly follows its assumption regardless of feature property, and that the assumption is the single most important factor that influences their accuracy. Experimental results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69% test (82.84% train). Going deeper, we investigated and found that each Naive Bayes variants performances and accuracy is largely due to each classifier assumption, Gaussian classifier performed best on anomaly detection due to its assumption that features follow normal distributions which are continuous, while multinomial classifier have a dismal performance as it simply assumes discreet and multinomial distribution.
</details>
<details>
<summary>摘要</summary>
bayesian 分类器在实际应用中表现良好时，每个特征都应该完全独立于其他特征，这并不总是正确的。本研究的目标是实现和比较bayesian分类器（多omial、bernoulli和gaussian）在网络侵入异常检测中的表现，并investigate各种variant假设和其表现之间的关系。我们的调查发现，bayesian算法的每个变体都会遵循自己的假设，不管特征的性质如何。假设是影响准确率的最重要因素。实验结果显示，bernoulli的准确率为69.9%（71%训练），多omial的准确率为31.2%（31.2%训练），而gaussian的准确率为81.69%（82.84%训练）。进一步调查发现，bayesian算法的每种变体表现和准确率主要归结于各自假设。gaussian分类器在异常检测中表现最佳，因为它假设特征遵循正态分布，而多omial分类器表现很差，因为它只假设离散和多omial分布。
</details></li>
</ul>
<hr>
<h2 id="Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks"><a href="#Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks" class="headerlink" title="Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks"></a>Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11825">http://arxiv.org/abs/2308.11825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiexi1990/iccad-accel-gnn">https://github.com/xiexi1990/iccad-accel-gnn</a></li>
<li>paper_authors: Xi Xie, Hongwu Peng, Amit Hasan, Shaoyi Huang, Jiahui Zhao, Haowen Fang, Wei Zhang, Tong Geng, Omer Khan, Caiwen Ding</li>
<li>for: 本文是为了提高图数据中GCNs的计算效率。</li>
<li>methods: 本文使用了一种新的加速器架构 Accel-GCN，包括度 sorting 阶段、块级分配策略和共同卷积策略等。</li>
<li>results: 对于 18 个测试图，Accel-GCN 比 cuSPARSE、GNNAdvisor 和 graph-BLAST 高效性提高了1.17倍、1.86倍和2.94倍。<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.   Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memory efficiency and optimizes memory bandwidth by exploiting memory coalescing and alignment. Evaluation of Accel-GCN across 18 benchmark graphs reveals that it outperforms cuSPARSE, GNNAdvisor, and graph-BLAST by factors of 1.17 times, 1.86 times, and 2.94 times respectively. The results underscore Accel-GCN as an effective solution for enhancing GCN computational efficiency.
</details>
<details>
<summary>摘要</summary>
格raph Convolutional Networks (GCNs) 是在不同领域中提取隐藏信息的关键工具，但它们在主流GPU上的加速受到工作负载不均和内存访问不规则的挑战。为 Addressing these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. Accel-GCN的设计包括：1. 轻量级度排序阶段，用于将节点按度分组;2. 基于块级别的分配策略，动态调整卷积工作负载大小，提高共享内存地方位性和工作负载均衡，并减少metadata overhead相比GNNAdvisor;3. 结合卷积策略，以提高内存块化和计算并行性在纵向 dense 矩阵中。基于这些原则，我们定制了GCNs中的稀疏矩阵乘法（SpMM）的核心，使用块级别的分配和结合卷积策略。这种方法可以提高性能和多级内存效率，并通过利用内存块化和对齐来增加内存带宽。对Accel-GCN在18个测试图进行评估，发现它在cuSPARSE、GNNAdvisor和graph-BLAST的基础上提高性能，分别提高了1.17倍、1.86倍和2.94倍。这些结果证明Accel-GCN是GCN计算效率的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder"><a href="#Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder" class="headerlink" title="Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder"></a>Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11819">http://arxiv.org/abs/2308.11819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Xiaohan Li, Philip Yu</li>
<li>for: 该论文旨在解决电子医疗记录（EHR）模型中的公平问题，特别是对于EHR的复杂隐藏结构和可能存在的选择偏见。</li>
<li>methods: 该论文提出了一种新的模型 called Fair Longitudinal Medical Deconfounder (FLMD)，以实现在长期EHR模型中的公平和准确性。 FLMD采用了两个阶段的训练过程，第一阶段是捕捉每次观察的隐藏因素，第二阶段是将学习的隐藏表示与其他相关特征结合以进行预测。</li>
<li>results: 该论文通过对两个实际EHR数据集进行了广泛的实验，证明了FLMD的效果。 除了与基准方法和FLMD变体进行公平和准确性的比较外，该论文还评估了所有模型在受损&#x2F;不均衡和人工数据集上的性能，以验证FLMD在不同的设定下的超越性和提供有价值的意见。<details>
<summary>Abstract</summary>
The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial for addressing the accuracy/fairness dilemma. In the second stage, FLMD combines the learned latent representation with other relevant features to make predictions. By incorporating appropriate fairness criteria, such as counterfactual fairness, FLMD ensures that it maintains high prediction accuracy while simultaneously minimizing health disparities. We conducted comprehensive experiments on two real-world EHR datasets to demonstrate the effectiveness of FLMD. Apart from the comparison of baseline methods and FLMD variants in terms of fairness and accuracy, we assessed the performance of all models on disturbed/imbalanced and synthetic datasets to showcase the superiority of FLMD across different settings and provide valuable insights into its capabilities.
</details>
<details>
<summary>摘要</summary>
临床数据模型中的公平问题，尤其是在电子医疗记录（EHR）方面，是非常重要的，因为EHR具有复杂的潜在结构和可能存在选择偏见。在实践中，需要同时解决健康差距和模型的总准确率问题。然而，传统的方法经常面临精度和公平之间的负面选择，因为它们无法捕捉下观数据的底层因素。为解决这个挑战，我们提出了一种新的模型 called Fair Longitudinal Medical Deconfounder (FLMD)，旨在在长期EHR模型中同时实现公平和精度。受到干扰器理论的启发，FLMD采用了两个阶段的训练过程。在第一阶段，FLMD捕捉每次观察的隐藏因素，这些隐藏因素表示了在观察数据之外的医学因素，如患者的基因和生活习惯。这些隐藏因素是解决精度和公平之间的负面选择的关键。在第二阶段，FLMD将学习的底层表示与其他相关特征结合以进行预测。通过包含合适的公平标准，如对比公平，FLMD确保了它在实践中维护高预测精度，同时减少健康差距。我们在两个真实的EHR数据集上进行了广泛的实验，以证明FLMD的有效性。除了对基eline方法和FLMD变体的公平和精度进行比较外，我们还评估了所有模型在干扰/不均衡和 sintetic 数据集上的性能，以展示FLMD在不同的设置下的超越性和提供有价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks"><a href="#Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks" class="headerlink" title="Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks"></a>Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11818">http://arxiv.org/abs/2308.11818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Archie J. Huang, Animesh Biswas, Shaurya Agarwal</li>
<li>for: 这个研究的目的是提高交通状况估算方法的精度和可靠性，通过非本地LWR模型在物理学 Informed Deep Learning框架中进行增强。</li>
<li>methods: 这篇论文提出了一种新的PIDL框架，该框架包括非本地LWR模型，并引入了固定长度和变量长度的核心。文章还提供了相关的数学基础。</li>
<li>results: 对比基eline PIDL方法，提出的PIDL框架得到了改进，具体来说，使用NGSIM和CitySim数据集进行评估，实验结果表明，该方法可以提高交通状况估算的精度和可靠性。<details>
<summary>Abstract</summary>
This research contributes to the advancement of traffic state estimation methods by leveraging the benefits of the nonlocal LWR model within a physics-informed deep learning framework. The classical LWR model, while useful, falls short of accurately representing real-world traffic flows. The nonlocal LWR model addresses this limitation by considering the speed as a weighted mean of the downstream traffic density. In this paper, we propose a novel PIDL framework that incorporates the nonlocal LWR model. We introduce both fixed-length and variable-length kernels and develop the required mathematics. The proposed PIDL framework undergoes a comprehensive evaluation, including various convolutional kernels and look-ahead windows, using data from the NGSIM and CitySim datasets. The results demonstrate improvements over the baseline PIDL approach using the local LWR model. The findings highlight the potential of the proposed approach to enhance the accuracy and reliability of traffic state estimation, enabling more effective traffic management strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting"><a href="#Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting" class="headerlink" title="Evaluation of Deep Neural Operator Models toward Ocean Forecasting"></a>Evaluation of Deep Neural Operator Models toward Ocean Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11814">http://arxiv.org/abs/2308.11814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellery Rajagopal, Anantha N. S. Babu, Tony Ryu, Patrick J. Haley Jr., Chris Mirabito, Pierre F. J. Lermusiaux</li>
<li>for: 研究使用深度学习模型来预测时间序列数据的有效性，并应用于大气和海洋领域。</li>
<li>methods: 使用深度 neural operator 模型来模拟和预测经典流体流动和实际海洋动力学 simulate。</li>
<li>results: 训练深度 neural operator 模型可以预测理想化 periodic eddy shedding，并在实际海洋表面流动中预测一些特征，提供未来研究和应用的潜在可能性。<details>
<summary>Abstract</summary>
Data-driven, deep-learning modeling frameworks have been recently developed for forecasting time series data. Such machine learning models may be useful in multiple domains including the atmospheric and oceanic ones, and in general, the larger fluids community. The present work investigates the possible effectiveness of such deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics. We first briefly evaluate the capabilities of such deep neural operator models when trained on a simulated two-dimensional fluid flow past a cylinder. We then investigate their application to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay, learning from high-resolution data-assimilative simulations employed for real sea experiments. We confirm that trained deep neural operator models are capable of predicting idealized periodic eddy shedding. For realistic ocean surface flows and our preliminary study, they can predict several of the features and show some skill, providing potential for future research and applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Data-driven, deep-learning modeling frameworks have been recently developed for forecasting time series data. Such machine learning models may be useful in multiple domains including the atmospheric and oceanic ones, and in general, the larger fluids community. The present work investigates the possible effectiveness of such deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics. We first briefly evaluate the capabilities of such deep neural operator models when trained on a simulated two-dimensional fluid flow past a cylinder. We then investigate their application to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay, learning from high-resolution data-assimilative simulations employed for real sea experiments. We confirm that trained deep neural operator models are capable of predicting idealized periodic eddy shedding. For realistic ocean surface flows and our preliminary study, they can predict several of the features and show some skill, providing potential for future research and applications." into Simplified Chinese.以下是文本的简化中文翻译：现代数据驱动、深度学习模型框架在时间序列预测方面得到了最近的发展。这些机器学习模型在大气和海洋领域以及更广泛的流体社区中可能有用。本文 исследова了使用深度神经运算器模型来重现和预测经典的流体流动和实际海洋动力学 simulations。我们首先简要评估了在模拟的二维流体流 past cylinder 上训练的深度神经运算器模型的能力。然后，我们调查了它们在中大西洋盆地和麻萨诸塞湾的海面 circulation 预测方面的应用。我们证实了训练后的深度神经运算器模型可以预测理想化 periodic eddy shedding。对于实际的海面流动和我们的初步研究，它们可以预测一些特征并显示一定的技巧，提供未来研究和应用的潜在可能性。
</details></li>
</ul>
<hr>
<h2 id="Variational-Density-Propagation-Continual-Learning"><a href="#Variational-Density-Propagation-Continual-Learning" class="headerlink" title="Variational Density Propagation Continual Learning"></a>Variational Density Propagation Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11801">http://arxiv.org/abs/2308.11801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Angelini, Nidhal Bouaynaya, Ghulam Rasool</li>
<li>for: 这篇论文的目的是提出一个应用于实际世界中的深度神经网络（DNNs），应对不同类型的噪声和数据分布迁移的案例，并且避免严重遗传问题。</li>
<li>methods: 这篇论文提出了一个基于Benchmark Continual Learning datasets的框架，并开发了一种利用uncertainty quantification from Bayesian Inference来 mitigate catastrophic forgetting的方法。这个方法不需要运行模型 weights的 Monte Carlo sampling，而是使用关键� concatenate 的closed-form Evidence Lower Bound (ELBO) 目标，以便精确地预测分布。</li>
<li>results: 这篇论文的结果显示，这个方法可以帮助DNNs在实际世界中应对数据分布迁移和不同类型的噪声，并且可以避免严重遗传问题。具体来说，这篇论文在对应用于实际世界中的density propagated fully-connected和convolutional neural networks across multiple sequential benchmark datasets with varying task sequence lengths时，获得了良好的成果。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) deployed to the real world are regularly subject to out-of-distribution (OoD) data, various types of noise, and shifting conceptual objectives. This paper proposes a framework for adapting to data distribution drift modeled by benchmark Continual Learning datasets. We develop and evaluate a method of Continual Learning that leverages uncertainty quantification from Bayesian Inference to mitigate catastrophic forgetting. We expand on previous approaches by removing the need for Monte Carlo sampling of the model weights to sample the predictive distribution. We optimize a closed-form Evidence Lower Bound (ELBO) objective approximating the predictive distribution by propagating the first two moments of a distribution, i.e. mean and covariance, through all network layers. Catastrophic forgetting is mitigated by using the closed-form ELBO to approximate the Minimum Description Length (MDL) Principle, inherently penalizing changes in the model likelihood by minimizing the KL Divergence between the variational posterior for the current task and the previous task's variational posterior acting as the prior. Leveraging the approximation of the MDL principle, we aim to initially learn a sparse variational posterior and then minimize additional model complexity learned for subsequent tasks. Our approach is evaluated for the task incremental learning scenario using density propagated versions of fully-connected and convolutional neural networks across multiple sequential benchmark datasets with varying task sequence lengths. Ultimately, this procedure produces a minimally complex network over a series of tasks mitigating catastrophic forgetting.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在实际世界中部署时常会遇到不同类型的噪音、批处理数据和目标概念的变化。这篇论文提出了一种基于Continual Learning benchmark datasets的数据分布漂移适应框架。我们开发了和评估了一种基于 bayesian 推理的Continual Learning方法，以减轻忘却现象。我们在前一些方法的基础上，去除了对模型重量的蒙те Carlo 抽样，而是使用closed-form Evidence Lower Bound（ELBO）目标来 aproximate 预测分布。通过在所有网络层次上传递mean和covariance两个分布的第一两个 moment，我们可以 aproximate 预测分布。这种方法可以减轻忘却现象，因为它使用closed-form ELBO目标来 aproximate Minimum Description Length（MDL）原理，这种原理自然地强制对模型可能性的变化进行惩罚，使得模型避免忘却现象。我们的方法在任务增量学习场景中进行了评估，使用激活propagated的全连接和卷积神经网络，在多个顺序 benchmark datasets 上进行了多个任务的增量学习。最终，这种方法可以生成一个最小化任务学习的神经网络，减轻忘却现象。
</details></li>
</ul>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
<li>For: This paper proposes a new approach for anti-spoofing and audio deepfake detection that combines the benefits of both magnitude spectrogram-based features and raw audio processed through convolution or sinc-layers.* Methods: The proposed approach uses complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio, which retains phase information and allows for explainable AI methods.* Results: The results show that this approach outperforms previous methods on the “In-the-Wild” anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.Here is the information in Simplified Chinese text:* 为: 这篇论文提出了一种新的反假报告和音频深层负拟合方法，这种方法结合了 магниту德spectrogram-based特征和raw音频通过卷积或sinc层处理的优点。* 方法: 提议的方法使用复数值神经网络来处理输入音频的复数值CQT频域表示，保留相位信息，并允许使用解释性AI方法。* 结果: 结果显示，这种方法在”In-the-Wild”反假报告集上表现出色，并且可以通过解释性AI方法解释结果。剥离学研究表明，模型学习了相位信息以探测声笔 spoofing。<details>
<summary>Abstract</summary>
Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the "In-the-Wild" anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.
</details>
<details>
<summary>摘要</summary>
当前反伪措施和听音深度伪措施系统使用 either 大小 spectrogram 基于特征 (如 CQT 或 Melspectrograms) 或 raw 音频经过 convolution 或 sinc-layer 处理。两种方法均有缺点：大小 spectrogram 抛弃相位信息，影响音频自然性，而 raw-特征基本模型无法使用传统可解释 AI 方法。这篇论文提出了一种新的方法，通过使用复杂值神经网络处理输入音频的复杂值 CQT 频域表示。这种方法保留相位信息，并允许使用可解释 AI 方法。结果表明这种方法在 "In-the-Wild" 反伪措施数据集上表现出色，并且可以通过可解释 AI 方法解释结果。缺除研究表明模型已经学习使用相位信息探测声音伪措施。
</details></li>
</ul>
<hr>
<h2 id="Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics"><a href="#Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics" class="headerlink" title="Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics"></a>Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11792">http://arxiv.org/abs/2308.11792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Scheinert, Philipp Wiesner, Thorsten Wittkopp, Lauritz Thamsen, Jonathan Will, Odej Kao<br>for: 这个论文的目的是提高大数据分析任务中选择资源的自动化方法。methods: 该论文使用了分享数据和 ensemble 方法来解决冷开始问题，并同时优化多个目标。results: 论文通过对多种工作负荷的性能数据进行分析，显示了Karasu 可以在公共云环境中显著提高现有方法的性能、搜索时间和成本，即使共享 Profiling 运行只有部分相似特征。<details>
<summary>Abstract</summary>
Selecting the right resources for big data analytics jobs is hard because of the wide variety of configuration options like machine type and cluster size. As poor choices can have a significant impact on resource efficiency, cost, and energy usage, automated approaches are gaining popularity. Most existing methods rely on profiling recurring workloads to find near-optimal solutions over time. Due to the cold-start problem, this often leads to lengthy and costly profiling phases. However, big data analytics jobs across users can share many common properties: they often operate on similar infrastructure, using similar algorithms implemented in similar frameworks. The potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored.   We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information of collaborators and combines them into an ensemble method to exploit inherent knowledge of the configuration search space. Moreover, Karasu allows the optimization of multiple objectives simultaneously. Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job.
</details>
<details>
<summary>摘要</summary>
Big data analytics tasks often share common properties, such as operating on similar infrastructure, using similar algorithms implemented in similar frameworks, and working with similar datasets. This presents an opportunity to collaboratively address the cold-start problem by sharing aggregated profiling runs.We propose an approach called Karasu, which promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information from collaborators and combines them into an ensemble method to exploit the inherent knowledge of the configuration search space. Additionally, Karasu allows for the optimization of multiple objectives simultaneously.Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job.
</details></li>
</ul>
<hr>
<h2 id="HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials"><a href="#HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials" class="headerlink" title="HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials"></a>HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11787">http://arxiv.org/abs/2308.11787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoulatif Cisse, Xenophon Evangelopoulos, Sam Carruthers, Vladimir V. Gusev, Andrew I. Cooper</li>
<li>for: 这种研究是为了提高bayesian优化的效率和准确性，使其能更好地应用于新的科学任务。</li>
<li>methods: 该研究使用了人类专家知识来导引bayesian搜索，并通过自动排除无用的样本和使用有用的样本来提高模型数据的质量。</li>
<li>results: 实验结果表明，该方法可以在synthetic函数和化学设计任务中提高搜索效率和准确性，并且可以快速地应用于新的科学任务。<details>
<summary>Abstract</summary>
Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling. This process continues in a global versus local search fashion, organized in a bilevel optimization framework. We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.
</details>
<details>
<summary>摘要</summary>
робототехника и автоматизация提供了巨大的加速器，用于解决不可解的多变量科学问题，如材料发现。但可用搜索空间可能非常大。 bayesian优化（BO）已经成为一种流行的高效搜索引擎，在没有known analytic形式的目标函数/属性时表现出色。在我们的方法中，我们利用专家人类知识，即假设，以导引bayesian搜索更快地前往有投资可能的区域。先前的方法使用了基于现有实验尺度的下面分布，这是新的科学任务不可行。此外，这些分布无法捕捉复杂的假设。我们的提议的方法，即hypBO，使用专家人类假设来生成改进的种子样本。无用的种子样本会被排除，而有投资可能的种子样本将被用来补充surrogate模型数据，从而实现更好的搜索。这个过程继续进行在全球与本地搜索的biLevel优化框架中。我们验证了我们的方法的性能在一系列的synthetic函数上，并在一个真实的化学设计任务中示出了它的实用性。在这个任务中，使用专家假设可以加速搜索性能。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables"><a href="#Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables" class="headerlink" title="Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables"></a>Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11781">http://arxiv.org/abs/2308.11781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirban Mukherjee, Hannah H. Chang</li>
<li>for: 这种新的框架用于将质量数据 incorporated 入量化模型中，以估计 causal 效应。</li>
<li>methods: 我们使用函数分析创建了一个更加灵活和细化的框架，将观察到的类别嵌入到一个Baire空间中，并引入一个连续线性映射，将Baire空间中的类别映射到一个征表空间（RKHS）中。</li>
<li>results: 我们通过实验证明了我们的模型在具有复杂和细化的质量数据时表现出优异的性能，特别是在传统模型中的 embedding 与 kernel 技巧相结合时。<details>
<summary>Abstract</summary>
We propose a novel framework for incorporating qualitative data into quantitative models for causal estimation. Previous methods use categorical variables derived from qualitative data to build quantitative models. However, this approach can lead to data-sparse categories and yield inconsistent (asymptotically biased) and imprecise (finite sample biased) estimates if the qualitative information is dynamic and intricate. We use functional analysis to create a more nuanced and flexible framework. We embed the observed categories into a latent Baire space and introduce a continuous linear map -- a Hilbert space embedding -- from the Baire space of categories to a Reproducing Kernel Hilbert Space (RKHS) of representation functions. Through the Riesz representation theorem, we establish that the canonical treatment of categorical variables in causal models can be transformed into an identified structure in the RKHS. Transfer learning acts as a catalyst to streamline estimation -- embeddings from traditional models are paired with the kernel trick to form the Hilbert space embedding. We validate our model through comprehensive simulation evidence and demonstrate its relevance in a real-world study that contrasts theoretical predictions from economics and psychology in an e-commerce marketplace. The results confirm the superior performance of our model, particularly in scenarios where qualitative information is nuanced and complex.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的框架，用于将质量数据 integrate into量化模型以估计 causality。先前的方法使用 categorical 变量来建立量化模型，但这种方法可能会导致数据缺失和训练数据稀疏，从而导致估计结果偏差和不精准。我们使用函数分析来创建一个更加灵活和精细的框架。我们将观察到的类划分入一个 latent Baire 空间，并引入一个连续线性映射——一个 reproducing kernel Hilbert space (RKHS) 的表达函数 embeddings。通过 Riesz 表示定理，我们证明了传统的 categorical 变量在 causal 模型中的径向处理可以在 RKHS 中转化为一个唯一标识结构。在使用 transfer learning 作为助手的情况下，我们可以使用 kernel 技巧来形成一个 Hilbert 空间 embeddings。我们通过了全面的 simulation 证明和一个实际的案例研究，证明了我们的模型在 nuanced 和复杂的质量信息的情况下表现出优于传统的方法。结果表明，我们的模型在质量数据是缺失的情况下具有更高的准确性和稳定性。
</details></li>
</ul>
<hr>
<h2 id="When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making"><a href="#When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making" class="headerlink" title="When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making"></a>When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11721">http://arxiv.org/abs/2308.11721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kpdonahue/benefits_harms_joint_decision_making">https://github.com/kpdonahue/benefits_harms_joint_decision_making</a></li>
<li>paper_authors: Kate Donahue, Kostas Kollias, Sreenivas Gollapudi</li>
<li>for: 这个研究旨在优化人机合作的机器学习算法性能。</li>
<li>methods: 研究使用了一种特定的人机合作模型，其中算法可以访问一组 $n$ 个项目，并将其中的一个 subset of size $k$ 展示给人类，人类选择最终的一个项目。这种场景可以应用于内容推荐、路径规划或任何类型的标签任务。由于人类和算法均有不准确、噪音信息，关键问题是：哪个值得到 $k$ 最大化人类选择最佳项目的概率？</li>
<li>results: 研究发现，在某些噪音模型下，最佳的 $k $ 值在 $[2, n-1] $ 之间，即在人机合作下有着明显的优势。然而，当人类固定在算法提供的顺序上时，人机合作系统总是表现更差。此外，研究还发现，在人类和算法准确性水平不同时，存在一些情况下，更准确的代表会受益于与更准确的代表合作。<details>
<summary>Abstract</summary>
Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is reversed when the human is anchored on the algorithm's presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithm's accuracy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:历史上，机器学习研究中 большинство时间都是关注算法本身的性能，但最近更多的注意力都是关注人机算法协作的性能。在这里，我们分析了一种特定的人机算法协作方式，其中算法有一组$n$个项目，并将其中的一个集合Size$k$提供给人类，他们从中选择最终的一个项目。这种方式可以模型内容推荐、路径规划或任何类型的标注任务。由于人类和算法都有不准确、噪音信息关于真正的项目顺序，因此关键问题是：哪个值的$k$最大化最终选择最佳项目的概率呢？ For $k=1$, 性能是由算法单独行动优化的，而 For $k=n$ 则是由人类单独行动优化的。 surprisingly，我们显示出，对多种噪音模型，最佳的$k$在[2, n-1]范围内，即在人类和算法之间协作时存在着紧跟的好处。我们在Mallocks模型和Random Utilities模型上证明了这一点，并在实验中证明了这一点。然而，我们发现在人类固定在算法提供的顺序上的情况下，共同系统总是有着劣化性能。我们扩展了这些结果到人类和算法的准确程度不同的情况下，显示在某些情况下，更准确的代表会受益于和更不准确的代表协作。但这些情况是人类和算法准确程度之间的非对称的。
</details></li>
</ul>
<hr>
<h2 id="Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection"><a href="#Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection" class="headerlink" title="Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection"></a>Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12885">http://arxiv.org/abs/2308.12885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oana Inel, Tim Draws, Lora Aroyo</li>
<li>for: 这个论文的目的是提出一种负责任AI（RAI）方法来导向数据收集，以便对数据质量进行系统atic的评估和改进。</li>
<li>methods: 这个论文使用了一些 metrics来评估数据的内在可靠性和外在稳定性，并进行了对九个现有数据集和注释任务以及四种内容模式的验证。</li>
<li>results: 研究发现，使用这些 metrics可以对数据集的质量进行深入的分析和评估，并且可以提高AI应用中数据的可靠性和公平性。<details>
<summary>Abstract</summary>
The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models' robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized and measured through a systematic set of appropriate metrics. In this paper, we propose a Responsible AI (RAI) methodology designed to guide the data collection with a set of metrics for an iterative in-depth analysis of the factors influencing the quality and reliability} of the generated data. We propose a granular set of measurements to inform on the internal reliability of a dataset and its external stability over time. We validate our approach across nine existing datasets and annotation tasks and four content modalities. This approach impacts the assessment of data robustness used for AI applied in the real world, where diversity of users and content is eminent. Furthermore, it deals with fairness and accountability aspects in data collection by providing systematic and transparent quality analysis for data collections.
</details>
<details>
<summary>摘要</summary>
machine learning技术在我们日常生活和高度关键领域的快速普及需要透明度和审核其公正性和可靠性。为了衡量机器学习模型的稳定性，研究通常将焦点放在massive dataset上，例如创建和维护对其起源、开发过程和伦理考虑的文档。然而，AI数据收集仍然是一次性的做法，并且经常 reuse datasets collected for certain purpose或应用。此外，数据集的注释可能不会在时间上保持表示性，包含歧义或错误的注释，或者无法泛化到问题或领域。最新的研究表明这些做法可能会导致不公正、偏见或不准确的结果。我们认为AI数据收集应该采取负责任的方式，审核数据质量的系统性和合适的度量。在这篇论文中，我们提出了一种负责任AI（RAI）方法ологи，用于指导数据收集，并提供了一个细化的度量来评估数据生成的内部可靠性和外部稳定性。我们验证了我们的方法在9个现有的数据集和注释任务中，以及4种内容模式。这种方法对实际应用中的AI进行评估数据坚实性，并处理公正和责任的方面。
</details></li>
</ul>
<hr>
<h2 id="SuperCalo-Calorimeter-shower-super-resolution"><a href="#SuperCalo-Calorimeter-shower-super-resolution" class="headerlink" title="SuperCalo: Calorimeter shower super-resolution"></a>SuperCalo: Calorimeter shower super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11700">http://arxiv.org/abs/2308.11700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ian-pang/supercalo">https://github.com/ian-pang/supercalo</a></li>
<li>paper_authors: Ian Pang, John Andrew Raine, David Shih</li>
<li>for: 这篇论文是为了解决大型夸克碰撞机器的计算管道中的喷流计算瓶颈问题。</li>
<li>methods: 这篇论文使用了深度生成替身模型来超越这个挑战。</li>
<li>results: 作者介绍了一种名为SuperCalo的流基super-resolution模型，可以快速将高维度细腻的喷流显示为高维度精细的喷流。这种新的方法可以减少喷流计算成本、内存需求和生成时间。此外，作者还证明了这些喷流具有高度的变化程度，可以从少量的粗细喷流中生成大量的高维度精细喷流，从而进一步减少生成时间。<details>
<summary>Abstract</summary>
Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
</details>
<details>
<summary>摘要</summary>
喷涂计数器模拟是大型夸克粒子加速器计算管道中的一个主要瓶颈。在最近的努力下，employs deep-generative surrogate models来突破这个挑战。然而，许多最高性能的模型的训练和生成时间不太适合高维度喷涂计数器。在这项工作中，我们引入SuperCalo，一种流基的超解析模型，并证明了高维度细致喷涂计数器可以快速upsample自粗细的喷涂计数器。这种新的方法可以降低计算成本、内存需求和生成时间相关的快速喷涂计数器模拟模型。此外，我们表明了upsampled的喷涂计数器具有高度的变化度。这使得可以从 relativamente few coarse showers 中生成大量高维度喷涂计数器，从而再次降低生成时间。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Multi-Resolution-Communications"><a href="#Semantic-Multi-Resolution-Communications" class="headerlink" title="Semantic Multi-Resolution Communications"></a>Semantic Multi-Resolution Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11604">http://arxiv.org/abs/2308.11604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus</li>
<li>for: 提高数据重建和保留 semantics 特征</li>
<li>methods: 使用深度学习多任务学习框架，具有层次结构，用于编码数据，并通过Current和 Past层编码数据进行解码</li>
<li>results: 在 MNIST 和 CIFAR10 数据集上进行实验，表明该方法可以在不同的分辨率下重建数据，并且可以提高Successive层中的semantic特征提取精度，从而在数据传输过程中保留更重要的semantic特征。<details>
<summary>Abstract</summary>
Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC). This superiority arises from the suboptimality of SSCC when dealing with finite block-length data. Moreover, SSCC falls short in reconstructing data in a multi-user and/or multi-resolution fashion, as it only tries to satisfy the worst channel and/or the highest quality data. To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL). This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data. Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process. These semantic features could be crucial elements such as class labels, essential for classification tasks, or other key attributes that require preservation. Within this framework, each level of encoded data can be carefully designed to retain specific data semantics. As a result, the precision of a semantic classifier can be progressively enhanced across successive layers, emphasizing the preservation of targeted semantics throughout the encoding and decoding stages. We conduct experiments on MNIST and CIFAR10 dataset. The experiment with both datasets illustrates that our proposed method is capable of surpassing the SSCC method in reconstructing data with different resolutions, enabling the extraction of semantic features with heightened confidence in successive layers. This capability is particularly advantageous for prioritizing and preserving more crucial semantic features within the datasets.
</details>
<details>
<summary>摘要</summary>
深度学习基于的共同源渠道编码（JSCC）已经在数据重建方面展示出了明显的进步，相比于分离源渠道编码（SSCC）。这种超越来自于分离源渠道编码在有限块长度数据时的不优化。此外，SSCC无法在多用户和/或多分辨率下重建数据，因为它只是尝试满足最差的渠道和/或最高质量的数据。为了超越这些限制，我们提议了一种基于多任务学习（MTL）的深度学习多分辨率JSCC框架。这个提议的框架通过层次结构来编码数据，并通过同一个批处理来解码它。此外，这个框架具有很大的潜在应用前提，即semantic communication，其目标不仅是重建数据，而且是保留特定的 semantics。这些semantic features可能是分类任务中的关键特征，或者是其他需要保留的关键特征。在这个框架中，每个层的编码数据可以非常精心地设计，以保留特定的数据 semantics。因此，在successive层中，精度的semantic classifier可以不断提高，强调在编码和解码过程中保留目标 semantics。我们在MNIST和CIFAR10 dataset上进行了实验，实验结果表明，我们的提议方法可以在不同的分辨率下重建数据，并且可以在successive层中提高semantic features的预测精度。这种能力特别有利于在数据中优先保留更加关键的semantic features。
</details></li>
</ul>
<hr>
<h2 id="Low-Tensor-Rank-Learning-of-Neural-Dynamics"><a href="#Low-Tensor-Rank-Learning-of-Neural-Dynamics" class="headerlink" title="Low Tensor Rank Learning of Neural Dynamics"></a>Low Tensor Rank Learning of Neural Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11567">http://arxiv.org/abs/2308.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Pellegrino, N Alex Cayco-Gajic, Angus Chadwick</li>
<li>For: 本研究探讨了人工神经网络（RNN）在学习过程中对连接性的演化，尤其是学习过程中weight矩阵的低矩阵结构是如何发展的。* Methods: 本研究使用了不同矩阵秩的RNN进行适应，并通过分析大规模神经记录数据来探讨学习过程中weight矩阵的演化。* Results: 研究发现，在学习过程中，RNN的weight矩阵都是低矩阵，并且这些低矩阵结构在整个学习过程中不断演化。此外，研究还发现，通过对真实权重矩阵进行低矩阵分解，可以 faithful地回归出学习过程中的低矩阵结构。最后，研究还提出了一些数学结论，用于界定在学习过程中RNN的矩阵和tensor秩的演化。<details>
<summary>Abstract</summary>
Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applied to the data faithfully recovers this low rank structure. Finally, we present a set of mathematical results bounding the matrix and tensor ranks of gradient descent learning dynamics which show that low-tensor-rank weights emerge naturally in RNNs trained to solve low-dimensional tasks. Taken together, our findings provide novel constraints on the evolution of population connectivity over learning in both biological and artificial neural networks, and enable reverse engineering of learning-induced changes in recurrent network dynamics from large-scale neural recordings.
</details>
<details>
<summary>摘要</summary>
学习依赖在相关联的神经元之间发生协调的synaptic变化。因此，理解学习过程中集体 synaptic 连接的演化是 neuroscience 和机器学习领域中关键的挑战。特别是，最近的研究表明，任务训练的 RNN 的weight 矩阵通常是低级数的，但是这种低级数结构如何在学习过程中发展未知。为了解决这个问题，我们调查了学习过程中weight矩阵的三个维度 tensor 的rank。通过将 RNN 的rank 追求到大规模神经记录中的学习任务中，我们发现了低级数学习的观察。我们随后验证了这一观察，通过直接将ground truth weights中的低级数结构应用于 RNN 中，并证明了我们对数据进行的方法可以准确地恢复这种低级数结构。最后，我们提供了一些数学结果， bounding 矩阵和tensor 的rank，这些结果表明，在解决低维度任务时，RNN 中的低级数 weights 是自然的。总之，我们的发现提供了对学习过程中 population connectivity 的新的约束，并允许从大规模神经记录中反向工程学习induced的变化。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/23/cs.LG_2023_08_23/" data-id="cllshr363002po988bu9n97r1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/23/cs.CV_2023_08_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-08-23
        
      </div>
    </a>
  
  
    <a href="/2023/08/23/cs.SD_2023_08_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-23 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
