
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-23 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12215 repo_url: https:&#x2F;&#x2F;github.com&#x2F;ramybaly&#x2F;News-Media-Reliabilit">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-23 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/23/cs.LG_2023_08_23/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12215 repo_url: https:&#x2F;&#x2F;github.com&#x2F;ramybaly&#x2F;News-Media-Reliabilit">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-22T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:36.140Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.LG_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-23 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection"><a href="#The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection" class="headerlink" title="The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection"></a>The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12215">http://arxiv.org/abs/2308.12215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ramybaly/News-Media-Reliability">https://github.com/ramybaly/News-Media-Reliability</a></li>
<li>paper_authors: Madelyne Xiao, Jonathan Mayer</li>
<li>for: 本研究旨在探讨机器学习在信任和安全问题上的应用，以推识信息检测为例。</li>
<li>methods: 研究者通过系мати化文献检测自动推识谣言的方法，从270篇引用次多的论文中检测出了 significan t shortcomings。</li>
<li>results: 研究发现，现有的论文中的检测任务与实际应用中的挑战存在 significan t的差异，数据和代码的可用性差、模型评价方法不独立、模型在不同领域数据上的泛化能力差。<details>
<summary>Abstract</summary>
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is for future work to avoid the pitfalls that we identify.
</details>
<details>
<summary>摘要</summary>
我们检查机器学习在保障和安全问题上的应用中存在的偏差，用误信息探测作为例子。我们系统化了 relate field 中270篇最具影响力的论文，然后对这些论文中的数据和代码可用性、设计异常、可重现性和泛化能力进行了检查。我们发现了 significante 缺陷，这些缺陷可能会质疑已经宣称的性能和实用性。检测任务与在线服务实际面临的挑战有很大差异，数据集和模型评估 часто不符合实际情况，评估 часто与模型训练无关。数据和代码可用性很差，模型无法在域外数据上Generalization well。基于这些结果，我们提出了评估机器学习应用到保障和安全问题的建议。我们的目标是为未来的工作避免这些偏差。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies"><a href="#Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies" class="headerlink" title="Learning to Learn Financial Networks for Optimising Momentum Strategies"></a>Learning to Learn Financial Networks for Optimising Momentum Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12212">http://arxiv.org/abs/2308.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Pu, Stefan Zohren, Stephen Roberts, Xiaowen Dong</li>
<li>for: 这篇论文旨在提供一种新的风险豁免，即基于金融网络的资产关系来预测未来的回报。</li>
<li>methods: 本论文提出了一种基于机器学习的综合学习框架，即L2GMOM，可以同时学习金融网络和优化交易信号。L2GMOM的模型是一种高度可读性的神经网络，其架构来自于算法抽象。</li>
<li>results: 根据64个连续未来合约的回报测试，L2GMOM可以显著提高股票资产的利润和风险控制，Sharpe比率为1.74，覆盖20年的时间段。<details>
<summary>Abstract</summary>
Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 64 continuous future contracts demonstrates a significant improvement in portfolio profitability and risk control, with a Sharpe ratio of 1.74 across a 20-year period.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy"><a href="#ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy" class="headerlink" title="ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy"></a>ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12210">http://arxiv.org/abs/2308.12210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fumiyukikato/uldp-fl">https://github.com/fumiyukikato/uldp-fl</a></li>
<li>paper_authors: Fumiyuki Kato, Li Xiong, Shun Takagi, Yang Cao, Masatoshi Yoshikawa</li>
<li>for: This paper aims to provide user-level differential privacy (DP) in cross-silo federated learning (FL) settings, where a single user’s data may belong to multiple silos.</li>
<li>methods: The proposed algorithm, called ULDP-FL, uses per-user weighted clipping to directly ensure user-level DP, departing from group-privacy approaches. The algorithm also utilizes cryptographic building blocks to enhance its utility and provide private implementation.</li>
<li>results: The authors provide a theoretical analysis of the algorithm’s privacy and utility, and showcase substantial improvements in privacy-utility trade-offs under user-level DP compared to baseline methods through empirical experiments on real-world datasets.<details>
<summary>Abstract</summary>
Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under user-level DP compared to baseline methods. To the best of our knowledge, our work is the first FL framework that effectively provides user-level DP in the general cross-silo FL setting.
</details>
<details>
<summary>摘要</summary>
受众级 differentially private federated learning（DP-FL）已经吸引了关注，这是一种合作机器学习方法，确保正式隐私。大多数 DP-FL 方法在每个缓存中保证了DP，但是一个用户的数据可能会分布在多个缓存中，并且未知用户级DP保证。在这种情况下，我们提出了ULDP-FL，一种新的 federated learning 框架，确保了用户级DP在跨缓存FL中。我们的提议算法直接确保用户级DP通过每个用户的质量截断，而不是GROUP-privacy方法。我们提供了算法的理论分析，包括隐私和用户性能的分析。此外，我们还提高了算法的用户性能，并使用密码学建筑块实现私有的实现。实验表明，我们的方法在保证用户级DP的情况下，与基eline方法相比，在隐私-用户性能质量上具有显著提升。我们知道，我们的工作是首次在通用跨缓存FL设置中提供了用户级DP的FL框架。
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details"><a href="#Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details" class="headerlink" title="Curriculum Learning with Adam: The Devil Is in the Wrong Details"></a>Curriculum Learning with Adam: The Devil Is in the Wrong Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12202">http://arxiv.org/abs/2308.12202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Weber, Jaap Jumelet, Paul Michel, Elia Bruni, Dieuwke Hupkes</li>
<li>for: 本研究旨在探讨CURRENT LEARNING PROGRESS（CL）方法在自然语言处理（NLP）领域的应用。</li>
<li>methods: 本研究使用了许多最近的CURRENT LEARNING PROGRESS方法的复制和扩展，并发现这些方法在NLP领域的结果具有许多不稳定性。</li>
<li>results: 研究发现，当CURRENT LEARNING PROGRESS方法与受欢迎的Adam优化算法结合使用时，它们经常学习到不适合选择的优化参数，导致结果不佳。研究还发现，不同的手动和自动CL方法在不同的场景下的表现都不佳， none of them outperforms optimisation with only Adam with well-chosen hyperparameters。<details>
<summary>Abstract</summary>
Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chosen hyperparameters. As such, our results contribute to understanding why CL methods work, but at the same time urge caution when claiming positive results.
</details>
<details>
<summary>摘要</summary>
学习课程（CL）认为机器学习模型，类似于人类，可能更有效地从匹配其当前学习进程的数据中学习。然而，CL方法仍然不够了解，特别是在自然语言处理（NLP）领域，只有有限的成功。在这篇论文中，我们探究了这一点。从尝试复制和扩展一些最近的课程方法开始，我们发现它们在NLP领域的结果很有限制。我们进行了深入的分析，发现在使用Adam优化算法时，课程经常学习适应不合适的优化参数。我们通过不同的常见手动编制和自动生成CL方法的几个案例研究，发现 none of them outperforms 仅使用Adam算法和合适的超参数。因此，我们的结果对CL方法的工作 Mechanism 提供了更深入的理解，同时也警告使用CL方法时应该有谨慎。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network"><a href="#Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network" class="headerlink" title="Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network"></a>Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12325">http://arxiv.org/abs/2308.12325</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Ho, Zhao-Heng Yin, Colin Zhang, Henry Overhauser, Kyle Swanson, Yang Ha</li>
<li>for: 预测药物的溶解度是药品研发领域中重要的任务，这个问题在现代计算机资源的帮助下得到了进一步研究。</li>
<li>methods: 本研究使用了两种机器学习模型：一个线性回归模型和一个图 convolutional neural network 模型，在多个实验数据集上应用了这两种方法。两种方法都可以做出合理的预测，但GCNN模型的性能最好。</li>
<li>results: 使用GCNN模型可以获得比较好的预测结果，但目前GCNN模型是一个黑盒模型，而线性回归模型的特征重要性分析可以提供更多有关化学结构下的物理性质的信息。使用线性回归模型，我们显示了各种 функциональ组在总溶解度上的影响。<details>
<summary>Abstract</summary>
Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advances in next generation high throughput screening.
</details>
<details>
<summary>摘要</summary>
预测给定分子的溶解度是药物工业中重要的任务，因此这是一个广泛研究的话题。在这项研究中，我们利用现代计算资源重新探讨了这个问题。我们使用了两种机器学习模型：线性回归模型和图 convolutional neural network 模型，并在多个实验数据集上应用了它们。两种方法都可以做出合理的预测，但GCNN模型的性能最佳。然而，当前GCNN模型是黑盒模型，而线性回归模型的功能重要性分析却可以提供更多有关化学影响的启示。使用线性回归模型，我们展示了每个 функциональ组如何影响总的溶解度。最终，知道化学结构如何影响化学性质是设计新药物的关键。未来的工作应该努力将高性能的 GCNN 与可解释的线性回归结合起来，这将开启下一代高通过筛选的新进展。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion"><a href="#Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion" class="headerlink" title="Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion"></a>Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12193">http://arxiv.org/abs/2308.12193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinshuo Li, Zhuo Jia, Wenkai Lu, Cao Song</li>
<li>for: 这个研究的目的是提出一种基于自我监督深度学习的非破坏性地磁探测方法，以便估算地下 Distribution of susceptibility.</li>
<li>methods: 这个方法使用了自我监督深度学习，并且具有知识驱动模组，以便更好地解释模型的运作。</li>
<li>results: 实验结果显示，提出的方法可以实现高效的地磁探测，并且可以提供更好的结果。<details>
<summary>Abstract</summary>
The magnetic inversion method is one of the non-destructive geophysical methods, which aims to estimate the subsurface susceptibility distribution from surface magnetic anomaly data. Recently, supervised deep learning methods have been widely utilized in lots of geophysical fields including magnetic inversion. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magnetic inversion by self-supervised deep learning. The proposed self-supervised knowledge-driven 3D magnetic inversion method (SSKMI) learns on the target field data by a closed loop of the inversion and forward models. Given that the parameters of the forward model are preset, SSKMI can optimize the inversion model by minimizing the mean absolute error between observed and re-estimated surface magnetic anomalies. Besides, there is a knowledge-driven module in the proposed inversion model, which makes the deep learning method more explicable. Meanwhile, comparative experiments demonstrate that the knowledge-driven module can accelerate the training of the proposed method and achieve better results. Since magnetic inversion is an ill-pose task, SSKMI proposed to constrain the inversion model by a guideline in the auxiliary loop. The experimental results demonstrate that the proposed method is a reliable magnetic inversion method with outstanding performance.
</details>
<details>
<summary>摘要</summary>
magnetische inversie-methode is een van de niet-verwoestende geofysische methodes, die wordt gebruikt om de subsurface gelijkstroomsverdeling te schatten vanaf bovenstaande magneet-anomaliedata. Recent hebben supervisionele diepe-lerenmethodes werden breed toegepast in verschillende geofysische velden, waaronder magneet-inversie. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magneet-inversie by self-supervised deep learning.De voorgestelde self-supervised knowledge-driven 3D magneet-inversie-methode (SSKMI) leert op het doelveld data van de inversie en de voorwaardelijke modellen. Given that the parameters of the voorwaardelijke model are preset, SSKMI can optimize the inversie model by minimizing the mean absolute error between observed and re-estimated surface magneet-anomalieën. Besides, there is a knowledge-driven module in the proposed inversie model, which makes the diepe-lerenmethode more explicable. Meanwhile, comparative experiments demonstrate that the knowledge-driven module can accelerate the training of the proposed method and achieve better results.Since magneet-inversie is an ill-pose task, SSKMI proposed to constrain the inversie model by a guideline in the auxiliary loop. The experimental results demonstrate that the proposed method is a reliable magneet-inversie-methode with outstanding performance.
</details></li>
</ul>
<hr>
<h2 id="Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques"><a href="#Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques" class="headerlink" title="Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques"></a>Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12192">http://arxiv.org/abs/2308.12192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie A. Neubauer, Radu Grosu</li>
<li>for: 这篇论文旨在统一地描述某些时间连续过程的有序和随机拉格朗日方法，以确定这些过程的行为稳定性。</li>
<li>methods: 这篇论文使用了LRT-NG、SLR和GoTube算法来构建紧凑的可达集，即在给定时间范围内可达的状态集。这些算法提供了确定性和随机性的保证。</li>
<li>results: 实验表明，使用拉格朗日方法可以比LRT、Flow*和CAPD更高效地分析不同类型的时间连续模型的稳定性。<details>
<summary>Abstract</summary>
This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep. Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种统一的方法，即抽象的Lagrangian验证技术，可以确定时间连续过程的行为Robustness。这些技术基于构建一个紧缩的reachtube，即在给定时间范围内可达的状态的覆盖，并提供了reachtube bound的保证。文章介绍了LRT-NG、SLR和GoTube三种算法，并比较它们在使用变量方程、mean value theorem和Lipschitz常数来实现束缚和统计保证方面的表现。实验表明LAGRANGIAN技术在对LRT、Flow*和CAPD等方法的比较中表现出了superiority，并 ilustrated its use in various continuous-depth models的Robustness分析。
</details></li>
</ul>
<hr>
<h2 id="Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting"><a href="#Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting" class="headerlink" title="Development and external validation of a lung cancer risk estimation tool using gradient-boosting"></a>Development and external validation of a lung cancer risk estimation tool using gradient-boosting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12188">http://arxiv.org/abs/2308.12188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plbenveniste/lungcancerrisk">https://github.com/plbenveniste/lungcancerrisk</a></li>
<li>paper_authors: Pierre-Louis Benveniste, Julie Alberge, Lei Xing, Jean-Emmanuel Bibault</li>
<li>For: The paper aims to develop a machine learning tool for estimating the likelihood of developing lung cancer within five years, which can help individuals make informed decisions regarding lung cancer screening.* Methods: The study uses two datasets, PLCO and NLST, consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. The ML model is trained on the pre-processed PLCO dataset and tested on the NLST dataset, using XGBoost algorithm. The model incorporates features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer.* Results: The model is well-calibrated (Brier score&#x3D;0.044) and shows good performance on both datasets, with ROC-AUC of 82% on the PLCO dataset and 70% on the NLST dataset. The PR-AUC is 29% and 11% respectively. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years.<details>
<summary>Abstract</summary>
Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates. In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years. The study utilized two datasets, the PLCO (n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer. Additionally, a focus was placed on mitigating bias caused by censored data. Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees. The ML model was trained on the pre-processed PLCO dataset and tested on the NLST dataset. The model incorporated features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer. The model was well-calibrated (Brier score=0.044). ROC-AUC was 82% on the PLCO dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When compared to the USPSTF guidelines for lung cancer screening, our model provided the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2% vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years. By utilizing risk factors and clinical data, individuals can assess their risk and make informed decisions regarding lung cancer screening. This research contributes to the efforts in early detection and prevention strategies, aiming to reduce lung cancer-related mortality rates.
</details>
<details>
<summary>摘要</summary>
全球范围内，肺癌是一种重要的死亡原因，因此早期检测的重要性得到了更多的注意。在本研究中，我们提出了一种基于机器学习（ML）技术的工具，用于在5年内lung cancer的发生可能性的估计。该工具基于PLCO癌症检测试验和NLST试验数据进行训练和验证。研究使用了两个数据集：PLCO（n=55,161）和NLST（n=48,595），这两个数据集包含了肺癌的风险因素、临床测量和结果的全面信息。数据处理包括移除不是当前或前任吸烟者和不相关于肺癌的死亡病人，以及减少偏见的报告数据。我们使用XGBoost算法进行特征选择、超参数优化和模型约束。模型在PLCO数据集上训练，并在NLST数据集上测试。模型包含了年龄、性别、吸烟历史、医疗诊断和家族史肺癌的特征。模型具有良好的准备（Brier分数=0.044），ROC-AUC为82%、PLCO数据集上和70%、NLST数据集上。PR-AUC分别为29%和11%。与美国预防服务委员会（USPSTF）肺癌检测指南相比，我们的模型具有同等的回快，但精度更高（9.3% vs. 13.1%、PLCO数据集上，3.1% vs. 3.2%、NLST数据集上）。我们开发的ML工具提供了一个免费的网上应用程序，用于在5年内lung cancer的发生可能性的估计。通过利用风险因素和临床数据，个人可以评估自己的风险，并做出了有知情的决策关于肺癌检测。这些研究贡献到了早期检测和预防策略的努力，以减少肺癌相关的死亡率。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning"><a href="#Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning" class="headerlink" title="Unsupervised anomalies detection in IIoT edge devices networks using federated learning"></a>Unsupervised anomalies detection in IIoT edge devices networks using federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12175">http://arxiv.org/abs/2308.12175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niyomukiza Thamar, Hossam Samy Elsaid Sharara<br>for: 这个研究是为了解决跨多个IoT设备之间的数据共享问题，以及保护数据的隐私问题。methods: 这个研究使用了联邦学习（Federated Learning）的分布式机器学习方法，并使用了FedAvg算法。results: 研究发现，使用FedAvg算法可以在IoT&#x2F;IIoT设备上进行机器学习模型训练，并且结果与中央化机器学习方法相似。但是，研究也发现了一些缺陷，例如在训练过程中不稳定的设备可能会导致伪阳性警示。为了解决这个问题，研究人员提出了一个公平的FedAvg算法，将在未来的工作中进行评估。<details>
<summary>Abstract</summary>
In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT/ IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT/IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.
</details>
<details>
<summary>摘要</summary>
在许多物联网设备之间的连接中，每个设备都会收集数据，通常需要将数据传输到中央服务器进行机器学习模型训练。然而，一些业主可能会拒绝将数据传输到公司外due to data security concerns。基于分布式机器学习的 Federated learning（FL）方法可以在设备上训练机器学习模型，从而解决数据隐私问题。在这种情况下，数据不会在训练过程中传输到网络。Fedavg是FL算法的一种实现，允许在训练过程中将模型复制到参与设备上。这些设备可以随机选择，并且设备可以被中止。获得的模型将被送往协调服务器，然后平均处理参与设备完成训练的模型。这个过程会重复，直到达到所需的模型精度。通过这种方法，FL方法解决了物联网/IIoT设备所拥有的敏感数据所有者的隐私问题。在这篇论文中，我们利用了FL的优点，并在当今物联网/IIoT设备网络上使用Fedavg算法进行实验。结果与中央机器学习方法的结果几乎相同。我们还评估了Fedavg的一些缺陷，如训练过程中不参与的设备会导致不公平性。这可能会导致在物联网/IIoT设备中的预测报警系统中出现高达数百个假报警。因此，我们在后续的工作中提出了一种公平的Fedavg算法，它将在未来进行评估。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-decision-focused-surrogate-modeling"><a href="#Data-driven-decision-focused-surrogate-modeling" class="headerlink" title="Data-driven decision-focused surrogate modeling"></a>Data-driven decision-focused surrogate modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12161">http://arxiv.org/abs/2308.12161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddolab/decfocsurrmod">https://github.com/ddolab/decfocsurrmod</a></li>
<li>paper_authors: Rishabh Gupta, Qi Zhang</li>
<li>for: 这个论文是为了解决计算复杂非线性优化问题而设计的。</li>
<li>methods: 论文使用了数据驱动的框架，通过学习一个简单的、例如几何的优化模型，来减少决策预测错误。</li>
<li>results: 研究通过数字实验 validate了该框架，并与标准数据驱动模型相比较，表明该方法更加数据有效，并且生成了高决策预测精度的简单优化模型。<details>
<summary>Abstract</summary>
We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our approach is significantly more data-efficient while producing simple surrogate models with high decision prediction accuracy.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种决策抽象模型的概念，用于在实时设置下解决复杂非线性优化问题。我们的数据驱动框架寻求学习一个更简单的，例如几何，优化模型，以减少决策预测错误，即原始优化模型和代理优化模型的优化解的差异。我们将这种学习问题视为一个数据驱动的反向优化问题，并应用之前的研究中的分解法。我们通过数值实验，包括化学反应器、热交换网络和材料混合系统的优化问题，证明了我们的方法的有效性。此外，我们还进行了标准数据驱动模型和我们方法的比较，发现我们的方法在数据效率方面明显高于标准方法，并生成了高决策预测精度的简单模型。
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models"><a href="#A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models" class="headerlink" title="A Probabilistic Fluctuation based Membership Inference Attack for Generative Models"></a>A Probabilistic Fluctuation based Membership Inference Attack for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
<li>for: 该研究旨在开发一种黑盒式会员推断攻击（MIA），用于检测生成模型中是否存在记录。</li>
<li>methods: 该研究使用了现有的生成模型，并采用了多种正则化技术来避免过拟合。另外，该研究还使用了概率随机变量来检测生成模型中的记录分布变化。</li>
<li>results: 研究结果表明，在多个生成模型和数据集上，PFAMI可以提高攻击成功率（ASR）约27.9%，相比最佳基线。<details>
<summary>Abstract</summary>
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.
</details>
<details>
<summary>摘要</summary>
具有简化的中文翻译如下：机器学习模型训练集中记录是否存在的问题被称为成员推断攻击（MIA）。在经典分类模型上，MIA已经得到了广泛的研究，而最近的研究又开始探索如何将MIA应用于生成模型。我们的调查表明，现有的生成模型MIA主要依赖于目标模型的过拟合。然而，过拟合可以通过多种正则化技术来避免，而现有的MIAs在实践中表现不佳。与过拟合不同的是，记忆是深度学习模型取得优化性能的关键因素，因此在生成模型中更常见。记忆导致生成记录随member记录的概率分布增长趋势，因此我们提出了一种基于概率变动的成员推断攻击方法（PFAMI）。PFAMI是一种黑盒子MIA，通过分析给定记录的总probabilistic fluctuations来推断成员。我们在多个生成模型和数据集上进行了广泛的实验，实验结果表明，PFAMI可以提高攻击成功率（ASR）约27.9%，相比最佳基准。
</details></li>
</ul>
<hr>
<h2 id="Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models"><a href="#Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models" class="headerlink" title="Masking Strategies for Background Bias Removal in Computer Vision Models"></a>Masking Strategies for Background Bias Removal in Computer Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal">https://github.com/ananthu-aniraj/masking_strategies_bias_removal</a></li>
<li>paper_authors: Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos</li>
<li>for: 本研究探讨了细化图像分类任务中背景引起的偏见问题，特别是当一些类别之间的差异非常微小，并且每个类别的样本数很少时，模型容易受到背景相关的偏见。</li>
<li>methods: 我们 investigate了两种遮盾策略来mitigate背景引起的偏见：早期遮盾（ removes background information at the input image level）和晚期遮盾（ selectively masks high-level spatial features corresponding to the background）。</li>
<li>results: 我们的实验结果表明，两种遮盾策略都能够提高模型对非典型背景的抗性，其中早期遮盾 consistently exhibiting the best OOD performance。另外，一种基于GAP-Pooled Patch token的ViT变体，combined with early masking, achieves the highest OOD robustness。<details>
<summary>Abstract</summary>
Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.
</details>
<details>
<summary>摘要</summary>
fine-grained 图像分类任务中，某些类别之间的差异可能非常微妙，同时每个类型的样本数往往很少，因此模型容易受到背景相关的偏见。为了更深入地理解这个重要问题，我们的研究investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT).我们探索了两种遮盖策略来 mitigate background-induced bias：早期遮盖， removes background information at the (input) image level，和晚期遮盖， selectively masks high-level spatial features corresponding to the background。我们在不同的遮盖策略下进行了广泛的实验，专注于其对于不同背景的泛化性能。结果显示，我们所提出的两种策略都能够提高对于不同背景的性能，其中 early masking  consistently exhibits the best OOD performance。宁可是，一种基于 GAP-Pooled Patch token-based classification 的 ViT 变体，结合 early masking， achieved the highest OOD robustness。
</details></li>
</ul>
<hr>
<h2 id="An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization"><a href="#An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization" class="headerlink" title="An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization"></a>An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12126">http://arxiv.org/abs/2308.12126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weifeng Yang, Wenwen Min</li>
<li>For: 非对称和非光滑优化问题* Methods: 加速块 proximal 线性框架（ABPL$^+$），以及随机排序更新阶段的扩展* Results: 可以 monotonically 降低函数值，并且可以在某些情况下提高步长和扩展参数，以及在多个实验中证明了其效果和灵活性。<details>
<summary>Abstract</summary>
We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point set. Moreover, we demonstrate the global convergence as well as the linear and sublinear convergence rates of our algorithm by utilizing the Kurdyka-Lojasiewicz (K{\L}) condition. To enhance the effectiveness and flexibility of our algorithm, we also expand the study to the imprecise version of our algorithm and construct an adaptive extrapolation parameter strategy, which improving its overall performance. We apply our algorithm to multiple non-negative matrix factorization with the $\ell_0$ norm, nonnegative tensor decomposition with the $\ell_0$ norm, and perform extensive numerical experiments to validate its effectiveness and efficiency.
</details>
<details>
<summary>摘要</summary>
我们提出一种加速的块距离 próxima线性框架（ABPL$^+$）用于非拟合和非光滑优化。我们分析了一些算法中扩rapolation步骤失败的可能原因，并解决这个问题 by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm。此外，我们扩展了我们的算法，以便在更多的enario中更新块变量，并允许每个 цикла随机洗牌更新变量块的顺序。此外，在某些假设下，我们证明了ABPL$^+$可以 monotonically decrease the function value without strictly restricting the extrapolation parameters and step size，并且可以更加明确地示出该序列生成的 derivative set 是一个critical point set。此外，我们还证明了我们的算法的全球收敛性以及其线性和非线性收敛率，并使用Kurdyka-Lojasiewicz（K{\L））条件。为了提高我们的算法的效iveness和灵活性，我们还扩展了它的不精确版本，并构建了一种 adaptive extrapolation parameter strategy。我们应用我们的算法到多个非负矩阵因子化with the $\ell_0$ norm，非负tensor decomposition with the $\ell_0$ norm，并进行了广泛的数值实验来验证其效果和效率。
</details></li>
</ul>
<hr>
<h2 id="An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators"><a href="#An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators" class="headerlink" title="An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators"></a>An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12120">http://arxiv.org/abs/2308.12120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Esmaeilzadeh, Soroush Ghodrati, Andrew B. Kahng, Joon Kyung Kim, Sean Kinzer, Sayak Kundu, Rohan Mahapatra, Susmita Dey Manasi, Sachin Sapatnekar, Zhiang Wang, Ziqing Zeng</li>
<li>for: 这 paper 是为了探讨 físical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN machine learning (ML) algorithms。</li>
<li>methods: 这 paper 使用了 backend power, performance, and area (PPA) analysis 和 frontend performance simulation，以实现backend PPA 和系统指标如运行时和能耗的实际估计。此外，这 paper 还提出了一种自动化的设计空间探索技术，通过自动化搜索架构和后端参数，优化backend和系统指标。</li>
<li>results: 实验表明，这 paper 的方法可以准确预测backend PPA 和系统指标，平均预测错误为7%或更低，并在商业12nm进程和研究 oriented 45nm进程中实现了两个深度学习加速器平台（VTA和VeriGOOD-ML）的ASIC实现。<details>
<summary>Abstract</summary>
Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a research-oriented 45 nm process.
</details>
<details>
<summary>摘要</summary>
现代机器学习（ML）加速器的 Parameterizable 机制是 ML 的最新突破。为了充分利用设计空间探索（DSE），我们提议一种物理设计驱动、学习基于预测框架，用于硬件加速深度神经网络（DNN）和非 DNN ML 算法。它采用一种统一的方法，结合后端能力、性能和面积（PPA）分析与前端性能仿真，从而实现真实的 backend PPA 和系统指标（如运行时间和能耗）的估计。此外，我们的框架还包括一种完全自动化 DSE 技术，通过自动搜索设计和后端参数，实现最佳化 backend 和系统指标。实验研究显示，我们的方法可预测 backend PPA 和系统指标的平均差异为 7% 或更小，对 ASIC 实现的两种深度学习加速器平台（VTA 和 VeriGOOD-ML）在商用 12 nm 进程和研究 oriented 45 nm 进程中进行了可靠的预测。
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity"><a href="#Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity" class="headerlink" title="Less is More – Towards parsimonious multi-task models using structured sparsity"></a>Less is More – Towards parsimonious multi-task models using structured sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12114">http://arxiv.org/abs/2308.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, Marcus Liwicki</li>
<li>for: 本研究旨在在多任务学习（MTL）框架中 incorporate 结构化群 sparse 性，以开发 fewer 参数的模型，能够有效地解决多个任务，同时保持与权重 dense 模型的性能相似或更高。</li>
<li>methods: 我们使用 channel-wise L1&#x2F;L2 群 sparse 性在共享层中，通过减少模型的内存占用量、计算需求和预测时间来降低模型的资源占用。</li>
<li>results: 我们通过对单任务和多任务实验表明，在 group sparsity 下，模型的性能与 dense 模型相似或更高，同时可以减少模型的参数数量。 我们还发现，适当减少 sparse 度可以提高模型的性能和简洁度。<details>
<summary>Abstract</summary>
Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also investigate how changing the sparsification degree impacts both the performance of the model and the sparsity of groups.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:group sparsity in machine learning (ML) 推动更加简单、更易理解的模型，具有 fewer 活动参数组。本工作想要将结构化组简陋性 incorporated 到多任务学习 (MTL) 框架中，以开发更具有经济性的模型，可以更好地解决多个任务，而且具有更好的性能。在训练中减少模型的内存占用、计算需求和预测时间，可以提高模型的效率。我们在共享层中使用通道级 L1/L2 组简陋性，这种方法不仅可以消除无用的通道，还对权重进行 penalty，从而提高所有任务的学习。我们在 NYU-v2 和 CelebAMask-HQ 两个公开的 MTL 数据集上进行了单任务和多任务的实验，并 investigate 如何改变简陋化度对模型性能和组简陋性的影响。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Continual-Category-Discovery"><a href="#Generalized-Continual-Category-Discovery" class="headerlink" title="Generalized Continual Category Discovery"></a>Generalized Continual Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12112">http://arxiv.org/abs/2308.12112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Marczak, Grzegorz Rypeść, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski</li>
<li>for: 这个论文是为了探讨一种新的常规学习（Continual Learning，CL）框架，它允许在任务之间学习新的分类和保持之前的知识。</li>
<li>methods: 该论文使用了一种基于常规分类发现（Generalized Category Discovery，GCD）的方法，允许在任务中存在新的分类和已知的分类，并使用了一种新的混合指导方法来减少忘记。</li>
<li>results: 实验表明，该方法可以在存在新分类的情况下积累知识，并且表现比较好，超过了一些强大的CL方法。<details>
<summary>Abstract</summary>
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of novel classes are present. In light of these limitations, we propose a method that incorporates both supervised and unsupervised signals and mitigates the forgetting through the use of centroid adaptation. Our method surpasses strong CL methods adopted for GCD techniques and presents a superior representation learning performance.
</details>
<details>
<summary>摘要</summary>
大多数 continual learning (CL) 方法都是在Supervised learning  Setting 中进行学习，agent 需要学习新的标注任务，而不是忘记之前的知识。然而，这些设置并不是实际生活中的enario ，因为学习 Agent 可以访问大量的无标注数据，包括未知类和已知类的示例。 drawing inspiration from Generalized Category Discovery (GCD)，我们介绍了一个新的框架，允许在任务中存在未知和已知类，并且使用 continual 版本的无标注学习方法来发现它们。我们称这种设置为 Generalized Continual Category Discovery (GCCD)。它将 CL 和 GCD 融合起来， bridge  Synthetic  benchmarks 和实际生活中的enario。我们通过一系列实验表明，现有的方法在 Subsequent 任务中不能够从无标注样本中积累知识。在这些限制下，我们提出了一种方法，该方法将 supervised 和无标注信号相互作用，以避免忘记。我们的方法超越了Strong CL 方法，并在 representation learning 中表现出优于其他方法。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Stein-Variational-Trajectory-Optimization"><a href="#Constrained-Stein-Variational-Trajectory-Optimization" class="headerlink" title="Constrained Stein Variational Trajectory Optimization"></a>Constrained Stein Variational Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12110">http://arxiv.org/abs/2308.12110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Power, Dmitry Berenson</li>
<li>for: 这篇论文的目的是提出一种具有约束的轨迹优化算法，以便在多条轨迹上同时满足约束。</li>
<li>methods: 这篇论文使用stein可变数 gradient descent（SVGD）来找到一组粒子，这组粒子可以 aproximate一个低成本轨迹分布，并且遵循约束。</li>
<li>results: 实验结果显示，这篇论文的算法可以在具有高度约束的任务中，比基eline更好地避免损坏和初始化问题，并且在7DoF夹寸推进任务中取得了20&#x2F;20次成功，比基eline的13&#x2F;20次成功率高。<details>
<summary>Abstract</summary>
We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO succeeds in 20/20 trials vs 13/20 for the closest baseline. Our results demonstrate that generating diverse constraint-satisfying trajectories improves robustness to disturbances and initialization over baselines.
</details>
<details>
<summary>摘要</summary>
我们介绍了受限制 Stein 变量梯度下降（CSVTO）算法，用于并行执行具有约束的轨迹优化。我们将受限制轨迹优化视为一种新的约束函数最小化问题，这种方法可以避免对约束进行处理，并使我们可以生成一个多样化的约束满足轨迹集。我们使用 Stein 变量梯度下降（SVGD）来找到一组粒子，这些粒子可以近似一个低成本轨迹分布，同时遵循约束。CSVTO 适用于具有平等和不平等约束的问题，并包括一种新的粒子重采样步骤，以避免局部最优解。通过生成多样化的约束满足轨迹集，CSVTO 能够更好地避免初始化和干扰的影响，并且更加稳定。我们在一个高度约束的 7DoF 工具抓取任务中，证明了CSVTO 比基eline更高效，CSVTO 在 20/20 次试验中成功，而基eline 只有 13/20 次成功。我们的结果表明，通过生成多样化的约束满足轨迹集，可以提高对干扰和初始化的 robustness。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient"><a href="#Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient" class="headerlink" title="Quantifying degeneracy in singular models via the learning coefficient"></a>Quantifying degeneracy in singular models via the learning coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12108">http://arxiv.org/abs/2308.12108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edmundlth/scalable_learning_coefficient_with_sgld">https://github.com/edmundlth/scalable_learning_coefficient_with_sgld</a></li>
<li>paper_authors: Edmund Lau, Daniel Murfet, Susan Wei</li>
<li>for: 这篇论文的目的是解释深度神经网络（DNN）中的复杂的异常性。</li>
<li>methods: 这篇论文使用了 singular learning theory 中的一个量称为学习系数（learning coefficient），来量化 DNN 中的异常性。 它们还提出了一种可扩展的 Approximation 方法，使用游化 gradient Langevin dynamics，以便计算 localized 版本的学习系数。</li>
<li>results: 该论文的结果表明，local learning coefficient 可以准确地回归不同参数区域的异常性的排序。在 MNIST 实验中，local learning coefficient 能够揭示随机优化器对不同异常点的吸引力。<details>
<summary>Abstract</summary>
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）是单一的统计模型，具有复杂的多重性。在这项工作中，我们展示了singular学习理论中的一个量——学习系数，可以准确地量化深度神经网络中的多重性。重要的是，我们将证明 dass多重性在DNN中不可以通过简单地计数"平坦"方向来解释。我们提出了一种可扩展的本地化学习系数的计算方法，使用随机梯度质子泊利动力学。为验证我们的方法，我们在低维模型中展示了其准确性，并且可以正确地推断多重性的顺序。在MNIST实验中，本地学习系数可以揭示随机优化器对更或少多重极点的 inductive bias。
</details></li>
</ul>
<hr>
<h2 id="Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training"><a href="#Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training" class="headerlink" title="Cached Operator Reordering: A Unified View for Fast GNN Training"></a>Cached Operator Reordering: A Unified View for Fast GNN Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12093">http://arxiv.org/abs/2308.12093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Bazinska, Andrei Ivanov, Tal Ben-Nun, Nikoli Dryden, Maciej Besta, Siyuan Shen, Torsten Hoefler</li>
<li>for: 本文旨在提高图像网络（Graph Neural Networks，GNNs）的性能优化，以满足大规模图像模型的训练。</li>
<li>methods: 本文使用了一种统一的视角，对图像网络计算、输入&#x2F;输出和内存进行分析。基于图像 convolutional network（GCN）和图像注意力（GAT）层的计算图的分析，提出了一些 alternating computation strategies。</li>
<li>results: 提出的优化策略可以达到GCN中的速度提高（最高达2.43倍）和GAT中的速度提高（最高达1.94倍），同时减少内存占用。这些优化可以在不同的硬件平台上实现，并且有助于减轻训练大规模GNN模型的性能瓶颈。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks in training large-scale GNN models.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 是一种有力的工具，用于处理结构化图数据，并解决节点分类、图分类和聚类等任务。然而，GNN 的稀疏性导致了性能优化的新挑战，与传统深度神经网络相比。我们通过提供一种统一的视图，对 GNN 计算、输入和存储进行分析。通过分析图 convolutional network (GCN) 和 graph attention (GAT) 两种广泛使用的 GNN 层的计算图，我们提出了 alternate computation strategies。我们的提案包括 adaptive operator reordering with caching，可以达到 GCN 比现状态 искус的最大速度提升率为 2.43倍。此外，对 GAT 的缓存 schemes 的探索，可以达到最大速度提升率为 1.94倍。我们的优化措施可以降低训练大规模 GNN 模型的内存占用量，易于在不同硬件平台上实现，并有 Potential to alleviate performance bottlenecks in training large-scale GNN models。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-RNN-Gradients-through-Pre-training"><a href="#Stabilizing-RNN-Gradients-through-Pre-training" class="headerlink" title="Stabilizing RNN Gradients through Pre-training"></a>Stabilizing RNN Gradients through Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12075">http://arxiv.org/abs/2308.12075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Herranz-Celotti, Jean Rouat</li>
<li>for: 本研究旨在稳定和改进深度学习模型的训练，以避免梯度变化的扩散式增长。</li>
<li>methods: 本研究使用了先验学习稳定性的理论，并扩展了知名的稳定性条件（LSC）至更广泛的深度循环神经网络。</li>
<li>results: 研究发现，在应用классиical Glorot、He和Orthogonal初始化方案时， feed-forward fully-connected神经网络和深度循环神经网络都可以满足LSC。此外，研究还发现了一种新的权重加权问题，并提出了一种新的方法来解决这个问题。<details>
<summary>Abstract</summary>
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponential explosion that emerges from counting gradient paths in a rectangular grid in depth and time. We propose a new approach to mitigate this issue, that consists on giving a weight of a half to the time and depth contributions to the gradient, instead of the classical weight of one. Our empirical results confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. This study contributes to the field by providing a means to stabilize networks of any complexity. Our approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.
</details>
<details>
<summary>摘要</summary>
多种学习理论建议防止梯度差值的几何增长，以稳定和改进训练。通常，这些分析是对 fully-connected neural network 或 single-layer recurrent neural network 进行的，这些网络的数学性让其更易分析。然而，本研究表明，在网络太复杂，无法分析的情况下，先通过网络的本地稳定来初始化网络，可以取得良好的效果。此外，我们扩展了已知的稳定性理论，以覆盖更广泛的深度循环网络家族，这些网络的参数和数据分布假设最少。我们称之为本地稳定条件（LSC）。我们的调查发现，经典的 Glorot、He 和orthogonal 初始化方案满足 LSC 当应用于 fully-connected neural network。然而，对深度循环网络进行分析，我们发现了一种新的加法性梯度增长的问题，这种问题来自于在深度和时间方向上的 counting 梯度路径。我们提出了一种新的方法来解决这个问题，即将时间和深度方向的贡献权重设为 0.5，而不是经典的 1。我们的实验结果表明，在 feed-forward 和 recurrent 网络中，通过满足 LSC 来初始化网络，经常会导致最终性能的改进。这项研究对深度学习领域的稳定性做出了贡献，我们的方法可以作为训练之前的额外步骤，以及analytically 找到稳定初始化的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning"><a href="#Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning" class="headerlink" title="Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning"></a>Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12069">http://arxiv.org/abs/2308.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Dang, Tao Shi, Zengjie Zhang, Wanxin Jin, Marion Leibold, Martin Buss</li>
<li>for: 这篇论文旨在提出一种基于Maximum Entropy Inverse Reinforcement Learning（ME-IRL）方法来识别自动驾驶车辆（AV）的驾驶模式。</li>
<li>methods: 该方法使用了一系列加权特征来定义驾驶模式，并提出了一些新的反应式特征来捕捉AV对附近AV的反应。</li>
<li>results: 通过使用修改后的ME-IRL方法和新提出的特征，该研究成功地识别了从权重精算控制（SMPC）生成的示范轨迹中的驾驶模式。<details>
<summary>Abstract</summary>
The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV's trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV's reaction-aware characteristics. Then, we identify the driving styles from the demonstration trajectories generated by the Stochastic Model Predictive Control (SMPC) using a modified ME-IRL method with our newly proposed features. The proposed method is validated using MATLAB simulation and an off-the-shelf experiment.
</details>
<details>
<summary>摘要</summary>
自动驾驶车（AV）的驾驶方式指的是它在行驶过程中的行为和与其他AV的交互方式。在多辆自动驾驶车系统中，能够识别附近AV的驾驶方式的AV可以更可靠地评估碰撞风险并做出更合理的驾驶决策。然而，在文献中没有一致的定义自动驾驶车的驾驶方式，尽管人们认为驾驶方式在AV的轨迹中被编码，可以使用最大 entropy inverse reinforcement learning（ME-IRL）方法来作为成本函数来识别。然而，附近AV的反应不完全包含在前一代ME-IRL方法中的特征设计中。在这篇论文中，我们定义了自动驾驶车的驾驶方式为一系列权重的特征函数。然后，我们设计了新的反应意外特征，以更好地捕捉AV的反应特征。最后，我们使用修改后的ME-IRL方法和我们新提出的特征来识别示例轨迹，并从示例轨迹中提取驾驶方式。我们的方法在MATLAB simulate和一个简易实验中得到验证。
</details></li>
</ul>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
<li>for: 本研究旨在探讨大型语言模型如何通过限量高质量 instrucion-following 数据进行训练，以提高其对多模态任务的执行能力。</li>
<li>methods: 本研究使用了两个阶段的训练方法：首先在图像-文本对的集合上进行预训练，然后在超过200个例子的指令数据上进行精度训练。</li>
<li>results: 研究发现，使用少量但高质量的 instrucion-following 数据可以使大型语言模型实现更好的输出。InstructionGPT-4 在视觉问答、GPT-4 偏好等多种评价中表现出色，超过了原始 MiniGPT-4。<details>
<summary>Abstract</summary>
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.
</details>
<details>
<summary>摘要</summary>
多模式大语言模型通过两个阶段训练过程获得 instrucion-following 能力：先于敏感图文对应和精度检测数据进行预训练，然后在精度检测数据上进行微调。当前研究表明，大语言模型可以通过有限量高质量 instrucion-following 数据实现满意的结果。在本文中，我们介绍 InstructionGPT-4，它通过一个小数据集（约6%的对Alignment dataset）进行微调，并使用我们提出的多个评价指标来自动选择和筛选低质量视听数据。通过这种方法，InstructionGPT-4 在视觉问答和 GPT-4 偏好等评价中表现出色，超过原始 MiniGPT-4。总的来说，我们的发现表明，更少但高质量的 instrucion-following 准则可以使得多模式大语言模型生成更好的输出。
</details></li>
</ul>
<hr>
<h2 id="Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference"><a href="#Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference" class="headerlink" title="Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"></a>Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12066">http://arxiv.org/abs/2308.12066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang, Minsoo Rhu</li>
<li>for: 提高大型语言模型（LLM）的性能和可扩展性。</li>
<li>methods: 使用 Mixture-of-Experts（MoE）架构，并提出了一种新的预先阻塞函数来缓解 sparse expert 的动态活动问题，从而实现高性能和低内存占用。</li>
<li>results: 对比 conventional MoE 架构，提出的 Pre-gated MoE 系统能够提高性能、降低 GPU 内存占用，同时保持模型质量不变。这些特点使得 Pre-gated MoE 系统可以低成本地部署大规模 LLM，只需要一个 GPU 实现高性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.
</details>
<details>
<summary>摘要</summary>
Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs a novel pre-gating function that alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE can improve performance, reduce GPU memory consumption, and maintain the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.In simplified Chinese, the text would be:大型语言模型（LLMs）基于转换器的进步在最近几年很大，成功的原因是通过增加模型的大小来增加性能。然而，LLMs的高度计算和内存需求对应到前所未有的挑战。为了解决这些挑战，混合专家（MoE）架构被引入，它可以无需与计算需求成比例增加其模型大小来增加性能。然而，MoE的高内存需求和动态专家活动限制了它的实际应用。先前的解决方案，即将MoE的内存吃力强大的专家参数异步到CPU内存，不足以因为迁移到GPU的延迟会导致高性能开销。我们提出的预级MoE系统可以有效地解决计算和内存挑战，使用我们的算法-系统合作设计。预级MoE使用我们的新预级函数，解决了动态专家活动的问题，使我们的提议的系统可以面临大 Memory Footprint 的 MoE 问题，同时实现高性能。我们示出了预级MoE可以提高性能，减少GPU内存占用量，同时保持模型质量不变。这些特点使得我们的预级MoE系统可以效率地使用单个GPU进行大规模 LLMS 的部署，并且可以高性能。
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers"><a href="#Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers" class="headerlink" title="Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers"></a>Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12065">http://arxiv.org/abs/2308.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli</li>
<li>for: 提高机器学习模型的安全性，避免因误分类而导致的 kritical failures。</li>
<li>methods: 使用 ensemble of uncertainty measures 检测误分类，并在检测到误分类时阻止输出进行系统级别的处理。</li>
<li>results: SPROUT 能够准确地检测机器学习模型的误分类，并在检测到误分类时阻止输出进行系统级别的处理，从而提高机器学习模型的安全性。<details>
<summary>Abstract</summary>
Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of supervised classifiers, and it is able to detect all misclassifications in specific cases. SPROUT implementation contains pre-trained wrappers, it is publicly available and ready to be deployed with minimal effort.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）算法可能会预测错误的类别，导致错分。这是已知的事实，错分可能会对包含系统产生卷积效应，可能导致重要的失败。这篇论文提议了“SPROUT”，一个安全包装 Through ensembles of UncertainTy measures，它猜测错分的方式是计算输入和输出uncertainty measures的黑色框分类器。如果检测到了错分，SPROUT会阻止输出分类器的输出对包含系统的传递。这将导致安全性的改善，因为SPROUT将异常输出（错分）转化为数据损失失败，这可以轻松地在系统层面进行管理。SPROUT适用于二分和多分类фикация，包括图像和表格数据集。我们实验表明，SPROUT总能够检测出超级vised分类器中的大部分错分，而且在特定情况下，它能够检测所有错分。SPROUT实现包括预训练的包装，现在公开可用，只需要最小的努力就可以部署。
</details></li>
</ul>
<hr>
<h2 id="HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing"><a href="#HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing" class="headerlink" title="HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing"></a>HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12061">http://arxiv.org/abs/2308.12061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Xu, Amna Elmustafa, Liya Weldegebriel, Emnet Negash, Richard Lee, Chenlin Meng, Stefano Ermon, David Lobell</li>
<li>For: The paper aims to improve the accuracy of cropland mapping in smallholder farming regions, specifically in sub-Saharan Africa.* Methods: The authors use satellite images and expert knowledge to collect a dataset called HarvestNet, which includes 7,000 hand-labeled images and 2,000 ground-collected labels. They also benchmark several state-of-the-art models in remote sensing and compare their performance with a pre-existing coverage map.* Results: The authors achieve an accuracy of around 80% on hand-labeled data and 90%, 98% accuracy on ground truth data for Tigray and Amhara, respectively. They also detect an additional 56,621 hectares of cropland in Tigray using their method, which is not captured by the pre-existing coverage map.<details>
<summary>Abstract</summary>
Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data and 90%, 98% accuracy on ground truth data for Tigray, Amhara respectively. We also perform a visual comparison with a widely used pre-existing coverage map and show that our model detects an extra 56,621 hectares of cropland in Tigray. We conclude that remote sensing of harvest piles can contribute to more timely and accurate cropland assessments in food insecure region.
</details>
<details>
<summary>摘要</summary>
小规模农场在发展国家占较大的生产地面积。如在 SUB-SAHARAN AFRICA 地区，80% 的农场面积在 2 ha 以下， mapping 小holder 耕地是跟踪可持续发展的标准部署之一。然而，传统方法对小holder 耕地的映射受到 visually 多样和细节的限制。我们介绍了一种新的方法，基于耕地收割堆的检测，这种特征是许多小holder 系统中的共同特征。我们提供了 HarvestNet 数据集，用于在埃塞俄比亚地区的 Tigray 和 Amhara 地区在 2020-2023 年间的耕地映射。我们收集了 7000 个专家知识和卫星图像，以及 2000 个地面收集的标签。我们还对一些先进的远程感知模型进行了比较，我们的最佳模型在手动标注数据上达到了 80% 的分类性能，并在真实数据上达到了 90%、98% 的准确率。我们还进行了一个视觉比较，发现我们的模型可以检测到传统覆盖地图中缺失的 56,621 公顷耕地。我们结论认为，远程感知耕地收割堆可以为食 insecurities 地区提供更加准确和及时的耕地评估。
</details></li>
</ul>
<hr>
<h2 id="Manipulating-Embeddings-of-Stable-Diffusion-Prompts"><a href="#Manipulating-Embeddings-of-Stable-Diffusion-Prompts" class="headerlink" title="Manipulating Embeddings of Stable Diffusion Prompts"></a>Manipulating Embeddings of Stable Diffusion Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12059">http://arxiv.org/abs/2308.12059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/arxiv23-prompt-embedding-manipulation">https://github.com/webis-de/arxiv23-prompt-embedding-manipulation</a></li>
<li>paper_authors: Niklas Deckers, Julia Peters, Martin Potthast</li>
<li>for: 这个研究旨在提高文本描述到图像的转换精度，通过直接修改提示语 embedding，从而实现更细化和具有目标性的控制。</li>
<li>methods: 我们提出了一种将生成图像模型看作连续函数，通过图像空间和提示语 embedding 空间之间传递梯度的方法，以实现更精细和具有目标性的控制。</li>
<li>results: 我们的实验表明，这种方法可以实现更高精度的图像转换，并且可以帮助用户更好地完成创意任务。<details>
<summary>Abstract</summary>
Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative tasks by enabling them to navigate the image space along a selection of directions of "near" prompt embeddings. (3) Changing the embedding of the prompt to include information that the user has seen in a particular seed but finds difficult to describe in the prompt. Our experiments demonstrate the feasibility of the described methods.
</details>
<details>
<summary>摘要</summary>
“文本描述”を基于的生成图像模型，如稳定扩散，允许用户根据文本描述生成图像。但是，通过修改描述仍然是用户改变生成图像的主要方式。然而，通过修改描述来改变图像是一项困难的试验和错误过程，这导致了“提示工程”作为一种新的研究领域的出现。我们提出并分析了通过直接修改提示的embedding来改变图像的方法。这种方法允许更细化和targeted控制，考虑用户的意图。我们将生成文本到图像模型看作是连续函数，并在图像空间和提示 embedding 空间之间传递梯度。通过解决不同的用户互动问题，我们可以在以下三个场景中应用这个想法：1. 图像空间中定义的一个度量的优化，例如图像风格。2. 用户在创意任务中的帮助，通过让用户在“near”提示 embeddings 上导航图像空间来实现。3. 将提示 embedding 包含用户在种子中看到的信息，但是difficult to describe in the prompt。我们的实验表明这种方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks"><a href="#Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks" class="headerlink" title="Sample Complexity of Robust Learning against Evasion Attacks"></a>Sample Complexity of Robust Learning against Evasion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12054">http://arxiv.org/abs/2308.12054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascale Gourdeau</li>
<li>for: 本研究的目的是理解机器学习模型对攻击的抵触性。</li>
<li>methods: 本文使用了学习理论的角度，研究了在攻击下学习的可行性，并考虑了样本复杂度。</li>
<li>results: 本文显示了对于随机样本only的情况，需要 distributional assumptions 来保证机器学习模型的抵触性。此外，如果攻击者只能对输入数据进行 $O(\log n)$ 位的偏移，那么可以robustly学习 conjunctions 和 decision lists w.r.t. log-Lipschitz distributions。在learner具有更多权限的情况下，本文研究了local membership queries 和 local equivalence query oracle，并提供了可行性和Upper bound的结论。<details>
<summary>Abstract</summary>
It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibility of adversarially robust learning from the perspective of learning theory, considering sample complexity.   We first explore the setting where the learner has access to random examples only, and show that distributional assumptions are essential. We then focus on learning problems with distributions on the input data that satisfy a Lipschitz condition and show that robustly learning monotone conjunctions has sample complexity at least exponential in the adversary's budget (the maximum number of bits it can perturb on each input). However, if the adversary is restricted to perturbing $O(\log n)$ bits, then one can robustly learn conjunctions and decision lists w.r.t. log-Lipschitz distributions.   We then study learning models where the learner is given more power. We first consider local membership queries, where the learner can query the label of points near the training sample. We show that, under the uniform distribution, the exponential dependence on the adversary's budget to robustly learn conjunctions remains inevitable. We then introduce a local equivalence query oracle, which returns whether the hypothesis and target concept agree in a given region around a point in the training sample, and a counterexample if it exists. We show that if the query radius is equal to the adversary's budget, we can develop robust empirical risk minimization algorithms in the distribution-free setting. We give general query complexity upper and lower bounds, as well as for concrete concept classes.
</details>
<details>
<summary>摘要</summary>
“机器学习模型对于攻击性测试的漏洞日益增加的重要性。一个基本问题在于在恶意攻击下对于训练数据的需求量。在这些thesis中，我们运用精确在球体中的不可变量来测量机器学习模型的稳定性，并从学习理论的角度来研究抗攻击学习的可行性。我们首先考虑learner仅有随机样本的存在，并证明了需要分布方程的假设。接着，我们对受到点对点的分布的学习问题进行研究，并证明了对于log-Lipschitz分布，可以在 exponential 时间内对于攻击者的预算进行抗攻击学习。然后，我们研究learner具有更多权力的情况。我们首先考虑了本地会员询问，learner可以询问训练样本附近的标签。我们证明了，在 uniform 分布下，抗攻击学习 conjugation 和决策列在 exponential 时间内是不可避免的。然后，我们引入了本地相似询问 oracle，它可以返回训练样本附近的标签，以及在这个区域附近没有对应的 counterexample。我们证明了，如果询问半径等于攻击者的预算，则可以在分布自由设定下开发抗攻击 empirical risk minimization 算法。我们还给出了一般询问量上限和下限，以及具体的概念类别。”
</details></li>
</ul>
<hr>
<h2 id="Layer-wise-Feedback-Propagation"><a href="#Layer-wise-Feedback-Propagation" class="headerlink" title="Layer-wise Feedback Propagation"></a>Layer-wise Feedback Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12053">http://arxiv.org/abs/2308.12053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leander Weber, Jim Berend, Alexander Binder, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</li>
<li>for: 这paper是为了提出一种基于解释的神经网络训练方法，即层WISEFeedbackPropagation（LFP），用于评估神经网络模型中各个连接的贡献，并将其作为权重赋予各个连接。</li>
<li>methods: 这paper使用了层WISERelevancePropagation（LRP）来计算每个连接的权重，然后将这些权重分配给各个连接。这种方法不需要计算梯度，可以在模型的训练过程中分配权重。</li>
<li>results: 这paper提出了LFP的理论和实验研究，并证明了LFP可以在不同的模型和数据集上实现相同或更好的性能，而且可以超越传统的梯度下降法。此外，paper还 investigate了不同LRP规则的扩展和应用，如训练无意义梯度的神经网络模型，或者为转移学习而高效地利用现有知识。<details>
<summary>Abstract</summary>
In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how the different LRP-rules can be extended to LFP, what their effects are on training, as well as potential applications, such as training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs), or for transfer learning, to efficiently utilize existing knowledge.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出层wise Feedback Propagation（LFP），一种基于解释性的训练方法，使用层wise Relevance Propagation（LRP）来为各个连接分配奖励，以便解决给定任务。这与传统的梯度下降不同，梯度下降更新参数向估计损失最小点。LFP在模型中分配奖励信号，不需要梯度计算。它会强化接收正面奖励的结构，同时减弱接收负面奖励的结构。我们证明LFP的定理和实验均可以达到预期的性能，并在不同的模型和数据集上证明其效果。另外，LFP可以超越一些相关的梯度基于方法的限制，例如依赖于有意义的导数。我们还 investigate了不同的LRP规则如何扩展到LFP，以及它们在训练中的效果和应用，例如训练无意义导数的模型，如步函数激活的神经网络（SNN），或者用于过渡学习，以高效地利用现有的知识。
</details></li>
</ul>
<hr>
<h2 id="A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks"><a href="#A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks" class="headerlink" title="A multiobjective continuation method to compute the regularization path of deep neural networks"></a>A multiobjective continuation method to compute the regularization path of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12044">http://arxiv.org/abs/2308.12044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aamakor/continuation-method">https://github.com/aamakor/continuation-method</a></li>
<li>paper_authors: Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</li>
<li>for: 提高深度神经网络（DNNs）的数值效率、模型解释性和Robustness。</li>
<li>methods: 基于线性模型的机器学习方法，扩展了规则化路径的概念到DNNs，并通过处理经验损失和稀疏度（$\ell^1$ norm）为两个矛盾目标解决multiobjective optimization问题。</li>
<li>results: 提出了一种高效地近似Pareto前面的算法，并通过deterministic和随机梯度示例 validate了该算法的效果。此外，还证明了知道规则化路径可以为网络参数化提供好的泛化能力。<details>
<summary>Abstract</summary>
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）中的稀疏性是一个非常需要的特性，因为它保证了数学效率、提高模型解释性（由于更少的相关特征），并且提高了模型的稳定性。在基于线性模型的机器学习方法中，已经知道存在一个连接路径 между最稀疏的解决方案（按照$\ell^1$范数）和不Regularized解决方案，这个路径被称为Regularization路径。然而，在DNN中扩展Regularization路径的概念是非常困难，因为Empirical损失和稀疏性（$\ell^1$范数）是两个矛盾的目标。为了解决这个问题，我们提出了一种算法，可以高效地approximate整个Pareto前列。我们通过Deterministic和Stochastic梯度进行数值示例，并证明了知道Regularization路径可以获得一个很好地泛化网络参数化。
</details></li>
</ul>
<hr>
<h2 id="IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning"><a href="#IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"></a>IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12043">http://arxiv.org/abs/2308.12043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feiyuzhang98/increlora">https://github.com/feiyuzhang98/increlora</a></li>
<li>paper_authors: Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, Yiming Qian</li>
<li>for: 提高预训练语言模型（PLM）的 parameter efficiency，尤其是在多个下游任务时。</li>
<li>methods: 使用 Low-Rank Adaptation（LoRA）方法，并在每个目标模块中添加可学习的约数分解矩阵。</li>
<li>results: 在 GLUE 上进行了广泛的实验，显示我们的方法在低资源设置下表现更优，尤其是在 parameter efficiency 方面。<details>
<summary>Abstract</summary>
With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it is not limited by the initial number of training parameters, and each parameter matrix has a higher rank upper bound for the same training overhead. We conduct extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA. The results show that our method owns higher parameter efficiency, especially when under the low-resource settings where our method significantly outperforms the baselines. Our code is publicly available.
</details>
<details>
<summary>摘要</summary>
随着大型预训言语模型（PLM）的增加，精细调整模型中的所有参数不是高效的，特别是当有多个下游任务时，带来了训练和存储成本的增加。许多参数高效调整（PEFT）方法已经被提出，其中，低级别适应（LoRA）是一种代表性的方法，将适应矩阵注入到每个目标模块中。然而，LoRA忽略了模块中参数的重要性。为解决这个问题，许多工作已经被提出来剪裁LoRA中的参数。然而，在有限的训练条件下，剪裁后参数矩阵的最大级别仍然受到先前设置的值的限制。我们因此提出了IncreLoRA，一种逐步分配参数的方法，在训练过程中根据模块的重要性分配参数。这种方法与剪裁方法不同，不受初始训练参数的限制，每个参数矩阵的最大级别Upper bound也比剪裁方法高。我们在GLUE上进行了广泛的实验， demonstarted the effectiveness of IncreLoRA。结果显示，我们的方法在参数效率方面高于baseline，特别是在资源受限的情况下，我们的方法显著超过baseline。我们的代码公开可用。
</details></li>
</ul>
<hr>
<h2 id="CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures"><a href="#CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures" class="headerlink" title="CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures"></a>CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12031">http://arxiv.org/abs/2308.12031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Gherardini, Varun Ravi Varma, Karol Capala, Roger Woods, Jose Sousa</li>
<li>For: The paper is written for improving secure analytics using explainable artificial intelligence, specifically addressing the challenges of developing solutions with small data sets.* Methods: The paper presents a new tool called CACTUS, which employs explainable AI techniques to provide additional support for categorical attributes, optimize memory usage, and speed up computation through parallelization.* Results: The paper evaluates the performance of CACTUS on two data sets, Wisconsin diagnostic breast cancer and Thyroid0387, and shows that it can effectively rank attributes by their discriminative power and provide accurate classification results.Here’s the same information in Simplified Chinese text:* 为：本文是为了提高安全分析，使用可解释人工智能，特别是面临小数据集的挑战。* 方法：本文提出了一种新的工具called CACTUS，该工具使用可解释AI技术，为分类特征提供更多的支持，保持分类特征的原始含义，提高内存使用率，并通过并行化加速计算。* 结果：本文使用CACTUS工具对 Wisconcin诊断乳腺癌和 Thyroid0387 数据集进行评估，并显示了它可以准确地排序特征，提供高效的分类结果。<details>
<summary>Abstract</summary>
The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
</details>
<details>
<summary>摘要</summary>
大量数据的可用性对当前人工智能发展提供了动力。然而，对小数据集的解决方案存在实际和成本效益的挑战，主要是深度学习模型的透明度问题。本文提出了一种名为CACTUS的全面抽象和分类工具，用于提高安全分析。它可以有效地使用可解释人工智能，并且对分类属性进行了更好的支持，保持原始含义，优化内存使用和并行计算，以提高计算速度。它还可以为用户显示每个类别的属性频率，并将其排序为权重。本文通过应用于美国威斯康星诊断乳腺癌和 thyroid0387 数据集来评估其性能。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning"><a href="#Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning" class="headerlink" title="Prompt-Based Length Controlled Generation with Reinforcement Learning"></a>Prompt-Based Length Controlled Generation with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 这个论文旨在提出一种基于提示的Length Control方法，以实现控制长度的GPT-style语言模型生成。</li>
<li>methods: 该方法使用了 reward学习，通过给出trainable或规则型的奖励模型，对GPT-style语言模型的生成进行影响，以实现目标长度的控制。</li>
<li>results: 实验显示，该方法可以有效地提高CNNDM和NYT等 популяр的数据集上的提示基于长度控制精度。我们认为这种可控长度的能力可以为LLMs的未来带来更多的潜力。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.
</details>
<details>
<summary>摘要</summary>
近期，大型语言模型（LLM）如ChatGPT和GPT-4吸引了很大的注意，因其奇妙的进步和表现。控制LLM的Length emerges as an important topic，这也使得用户可以充分利用LLM的能力在更多的实际应用 scenario 中，如生成适当的答案或论文的 desired length。此外，LLM中的autoregressive generation extremely time-consuming，而控制这些生成的Length可以Randomly reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.
</details></li>
</ul>
<hr>
<h2 id="A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning"><a href="#A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning" class="headerlink" title="A Scale-Invariant Task Balancing Approach for Multi-Task Learning"></a>A Scale-Invariant Task Balancing Approach for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12029">http://arxiv.org/abs/2308.12029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu</li>
<li>for: This paper is written for people who are interested in multi-task learning (MTL) and want to learn about a new method called Scale-Invariant Multi-Task Learning (SI-MTL) that can alleviate the task-balancing problem.</li>
<li>methods: The paper proposes two methods to address the task-balancing problem in MTL: a logarithm transformation on all task losses to ensure scale-invariance at the loss level, and a gradient balancing method called SI-G that normalizes all task gradients to the same magnitude as the maximum gradient norm.</li>
<li>results: The paper reports extensive experimental results on several benchmark datasets, which consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.<details>
<summary>Abstract</summary>
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL），一种同时学习多个相关任务的学习方法，在不同领域都取得了很大成功。然而，任务均衡仍然是MTL中的主要挑战，由于任务损失/梯度的比例不同，经常会导致性能下降。在这篇论文中，我们提出了具有权重归一化和梯度归一化的缩减多任务学习方法（SI-MTL），以解决任务均衡问题从损失和梯度两个角度。具体来说，SI-MTL包括一种对所有任务损失进行对数变换，以保证损失水平上的归一化，以及一种梯度归一化方法SI-G，用于 норма化所有任务梯度，使其具有最大梯度 нор 的同样大小。我们在多个标准数据集上进行了广泛的实验，并 consistently demonstrates了SI-G的有效性和SI-MTL的状态的杰出性。
</details></li>
</ul>
<hr>
<h2 id="Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD"><a href="#Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD" class="headerlink" title="Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD"></a>Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12018">http://arxiv.org/abs/2308.12018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Knolle, Robert Dorfman, Alexander Ziller, Daniel Rueckert, Georgios Kaissis</li>
<li>for: 提高 differentially private SGD 的模型 Utility 和 Privacy 的融合</li>
<li>methods: 利用 per-sample gradient norms 和 private gradient oracle 的关系来减少 private gradient estimator bias</li>
<li>results: 在 CIFAR-10, CIFAR-100, 和 ImageNet-32  datasets 上提供了 empirical evidence, 显示 Bias-Aware Minimisation 不仅减少了 bias, 还有substantially improved privacy-utility trade-offs.<details>
<summary>Abstract</summary>
Differentially private SGD (DP-SGD) holds the promise of enabling the safe and responsible application of machine learning to sensitive datasets. However, DP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This renders optimisation steps less effective and limits model utility as a result. With this work, we show a connection between per-sample gradient norms and the estimation bias of the private gradient oracle used in DP-SGD. Here, we propose Bias-Aware Minimisation (BAM) that allows for the provable reduction of private gradient estimator bias. We show how to efficiently compute quantities needed for BAM to scale to large neural networks and highlight similarities to closely related methods such as Sharpness-Aware Minimisation. Finally, we provide empirical evidence that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32 datasets.
</details>
<details>
<summary>摘要</summary>
differentially private SGD (DP-SGD) 可以使机器学习应用于敏感数据集而不需担心隐私泄露。然而，DP-SGD只提供偏差、噪音的小批量梯度估计。这会导致优化步骤效果减退，模型实用性受限。在这项工作中，我们显示了每个样本梯度norm和私有梯度 Oracle 的估计偏差之间的连接。我们提出了偏差意识的最小化（BAM），允许降低私有梯度估计偏差。我们证明了如何有效地计算BAM所需的量，并将其扩展到大型神经网络。最后，我们提供了实验证明BAM不仅减少偏差，还substantially改善了隐私-实用性质量比在CIFAR-10、CIFAR-100和ImageNet-32数据集上。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Stochastic-Differential-Equations"><a href="#Graph-Neural-Stochastic-Differential-Equations" class="headerlink" title="Graph Neural Stochastic Differential Equations"></a>Graph Neural Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12316">http://arxiv.org/abs/2308.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Bergna, Felix Opolka, Pietro Liò, Jose Miguel Hernandez-Lobato</li>
<li>for: 这个论文旨在提出一种新的模型Graph Neural Stochastic Differential Equations (Graph Neural SDEs)，用于评估预测不确定性。</li>
<li>methods: 该模型基于Brownian Motion嵌入随机性，从而提高了现有模型缺失的预测不确定性评估。</li>
<li>results: 经验研究表明，Latent Graph Neural SDEs可以超过常见模型 like Graph Convolutional Networks和Graph Neural ODEs，特别是在信任预测中，能够更好地处理out-of-distribution检测。<details>
<summary>Abstract</summary>
We present a novel model Graph Neural Stochastic Differential Equations (Graph Neural SDEs). This technique enhances the Graph Neural Ordinary Differential Equations (Graph Neural ODEs) by embedding randomness into data representation using Brownian motion. This inclusion allows for the assessment of prediction uncertainty, a crucial aspect frequently missed in current models. In our framework, we spotlight the \textit{Latent Graph Neural SDE} variant, demonstrating its effectiveness. Through empirical studies, we find that Latent Graph Neural SDEs surpass conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making them superior in handling out-of-distribution detection across both static and spatio-temporal contexts.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的模型——图 neural 随机 дифференциаль方程（图 neural SDE）。这种技术在图 neural ordinary differential equation（图 neural ODE）中嵌入随机性，通过游戏摆动来表示数据的不确定性。这种包含允许我们评估预测不确定性，这是当前模型中常常缺失的一个重要方面。在我们的框架中，我们强调了《隐藏图 neural SDE》的变体，并证明其效果。通过实验研究，我们发现隐藏图 neural SDE 在信任预测方面表现出色，特别是在对于静态和空间时间上的 OUT-OF-DISTRIBUTION 检测中，与传统模型如图 convolutional networks 和图 neural ODEs 相比，它们更为稳定和可靠。
</details></li>
</ul>
<hr>
<h2 id="MKL-L-0-1-SVM"><a href="#MKL-L-0-1-SVM" class="headerlink" title="MKL-$L_{0&#x2F;1}$-SVM"></a>MKL-$L_{0&#x2F;1}$-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12016">http://arxiv.org/abs/2308.12016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxis1718/simplemkl">https://github.com/maxis1718/simplemkl</a></li>
<li>paper_authors: Bin Zhu, Yijie Shi</li>
<li>for: 这篇论文提出了一个基于多kernel学习（MKL）的支持向量机（SVM）模型，使用$(0, 1)$损失函数。</li>
<li>methods: 论文提供了一些首选条件，然后利用这些条件来开发一个快速的ADMM解决方案来处理非 convex 和非 гладhloss 优化问题。</li>
<li>results: 数据库实验表明，我们的MKL-$L_{0&#x2F;1}$-SVM表现与 SimpleMKL 比较相似，SimpleMKL 是 Rakotomamonjy 等人在 Journal of Machine Learning Research 上发表的一篇论文 [vol. 9, pp. 2491-2521, 2008] 。<details>
<summary>Abstract</summary>
This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种基于多kernel学习（简称MKL）的支持向量机器（SVM）$(0, 1)$损失函数的框架。文中给出了一些首要的优化条件，然后利用这些条件来开发一个快速的ADMM算法来解决非对称和不连续的优化问题。实验表明，我们的MKL-$L_{0/1}$-SVM的性能与2008年 Rakotomamonjy等在Journal of Machine Learning Research上发表的SimpleMKL方法相当。Here's the breakdown of the translation:* 这篇论文 (zhè běn zhōng zhì) - This paper* 提出了 (tī qù le) - proposes* 一种 (yī zhǒng) - a kind of* 基于多kernel学习 (jī yù duō jiān xué xí) - based on multiple kernel learning* SVM $(0, 1)$ 损失函数 (SVM $0, 1$ loss function) - SVM with the $(0, 1)$ loss function* 框架 (kōng zhì) - framework* 给出了 (gěi dòu le) - gives* 一些 (yī xiē) - some* 首要的 (shǒu yào de) - primary* 优化条件 (yòu yòu tiáo yì) - optimization conditions* 然后 (rán hái) - then* 利用 (lǐ yòng) - use* 这些条件 (zhè xiē tiáo yì) - these conditions* 开发 (kāi fā) - develop* 一个 (yī ge) - a* 快速的 (kuài sù de) - fast* ADMM算法 (ADMM suān gòu) - ADMM algorithm* 来解决 (laī jiě jué) - to solve* 非对称和不连续的 (fēi duì xiǎng yǔ bù lián zhí de) - nonconvex and nonsmooth* 优化问题 (yòu yòu wèn tí) - optimization problem* 实验 (shí yàn) - experiments* 表明 (biǎo mǐng) - show* 性能 (xìng néng) - performance* 与 (yǔ) - and* 2008年 Rakotomamonjy等 (2008 nián Rakotomamonjy déng) - Rakotomamonjy et al. in 2008* 在 (zài) - in* Journal of Machine Learning Research (Journal of Machine Learning Research)* 发表 (fā bèi) - published*  SimpleMKL (SimpleMKL) - SimpleMKL* 性能 (xìng néng) - performance* 相当 (xiāng dàng) - comparableI hope this helps!
</details></li>
</ul>
<hr>
<h2 id="Quantum-Noise-driven-Generative-Diffusion-Models"><a href="#Quantum-Noise-driven-Generative-Diffusion-Models" class="headerlink" title="Quantum-Noise-driven Generative Diffusion Models"></a>Quantum-Noise-driven Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12013">http://arxiv.org/abs/2308.12013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Parigi, Stefano Martina, Filippo Caruso</li>
<li>for: 这个论文的目的是提出和讨论量子扩散模型的量子扩散模型，用于生成复杂的数据分布。</li>
<li>methods: 这个论文使用了机器学习技术实现的生成模型，特别是使用量子随机过程来驱动扩散模型，以生成新的 sintetic 数据。</li>
<li>results: 这个论文预计可以通过利用量子随机过程的特点，例如干扰、Entanglement和噪声的非常规交互，超越传统的扩散模型在推断中的主要计算压力，从而实现更高效的数据生成和预测。<details>
<summary>Abstract</summary>
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a very remarkably beneficial key ingredient to generate much more complex probability distributions that would be difficult or even impossible to express classically, and from which a quantum processor might sample more efficiently than a classical one. Therefore, our results are expected to pave the way for new quantum-inspired or quantum-based generative diffusion algorithms addressing more powerfully classical tasks as data generation/prediction with widespread real-world applications ranging from climate forecasting to neuroscience, from traffic flow analysis to financial forecasting.
</details>
<details>
<summary>摘要</summary>
生成模型利用机器学习技术可以从 finite 数量的训练样本中推理出复杂和未知的数据分布，以生成新的 sintetic 数据。扩散模型是一种出现在的框架，最近已经超越了生成对抗网络在生成 sintetic 文本和高质量图像方面的性能。在这里，我们提出了量子扩散模型的量子扩散模型，可以在实际量子系统上进行实验。我们想利用量子特有的非rium特性，即减 coherence、Entanglement和噪声之间的非rivial交互，来超越类型 diffusion 模型的主要计算卷积。因此，我们建议利用量子噪声，不是探测和解决的问题，而是作为一个非常有利的元素，以生成更复杂的概率分布，这些分布可能是类型 diffusion 模型无法表达的，而且从量子处理器中采样可能更高效于类型处理器。因此，我们的结果预计将开拓出新的量子激发或量子基于的扩散算法，用于更有力的数据生成/预测，它们在广泛的实际应用中将扮演重要的角色，包括气候预测、神经科学、交通流量分析和金融预测等。
</details></li>
</ul>
<hr>
<h2 id="Neural-oscillators-for-magnetic-hysteresis-modeling"><a href="#Neural-oscillators-for-magnetic-hysteresis-modeling" class="headerlink" title="Neural oscillators for magnetic hysteresis modeling"></a>Neural oscillators for magnetic hysteresis modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12002">http://arxiv.org/abs/2308.12002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Chandra, Taniya Kapoor, Bram Daniels, Mitrofan Curti, Koen Tiels, Daniel M. Tartakovsky, Elena A. Lomonova</li>
<li>for: 模型和诊断普遍科学和工程中的普遍现象-干扰。</li>
<li>methods: 使用 ordinary differential equation-based recurrent neural network (RNN) 方法来模型和诊断干扰。</li>
<li>results: HystRNN 能够在未经训练的区域中适应并预测复杂的干扰模式，这是传统 RNN 方法所无法做到的。<details>
<summary>Abstract</summary>
Hysteresis is a ubiquitous phenomenon in science and engineering; its modeling and identification are crucial for understanding and optimizing the behavior of various systems. We develop an ordinary differential equation-based recurrent neural network (RNN) approach to model and quantify the hysteresis, which manifests itself in sequentiality and history-dependence. Our neural oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states. The performance of HystRNN is evaluated to predict generalized scenarios, involving first-order reversal curves and minor loops. The findings show the ability of HystRNN to generalize its behavior to previously untrained regions, an essential feature that hysteresis models must have. This research highlights the advantage of neural oscillators over the traditional RNN-based methods in capturing complex hysteresis patterns in magnetic materials, where traditional rate-dependent methods are inadequate to capture intrinsic nonlinearity.
</details>
<details>
<summary>摘要</summary>
《干支度量学习：一种基于偏微分方程的循环神经网络方法》Introduction:干支度量是科学和工程中的一种普遍现象，其模型化和识别是理解和优化系统的行为的关键。在本文中，我们提出了基于偏微分方程的循环神经网络方法（HystRNN），以模型和量化干支度量。HystRNN draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states.Methodology:我们的HystRNN方法基于Ordinary Differential Equation (ODE)，它可以模型干支度量的循环和历史依赖性。我们通过将循环神经网络中的每个节点更新为一个偏微分方程，来实现模型的循环和历史依赖性。这种方法可以更好地捕捉干支度量的复杂特征，比如折返曲线和小循环。Results:我们通过测试HystRNN的性能，发现它可以在未经训练的区域中预测折返曲线和小循环。这表明HystRNN具有普适性，是一种可以在不同的干支度量情况下预测行为的模型。此外，我们还发现HystRNN的性能比传统的RNN-based方法更好，这表明循环神经网络可以更好地捕捉干支度量的复杂特征。Conclusion:本文提出了一种基于偏微分方程的循环神经网络方法（HystRNN），用于模型和量化干支度量。HystRNN draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states.我们的实验表明，HystRNN具有普适性和更好的预测性，可以在不同的干支度量情况下预测行为。这些结果表明循环神经网络可以更好地捕捉干支度量的复杂特征，比如折返曲线和小循环。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Representation-Learning-Across-Domains"><a href="#Trustworthy-Representation-Learning-Across-Domains" class="headerlink" title="Trustworthy Representation Learning Across Domains"></a>Trustworthy Representation Learning Across Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12315">http://arxiv.org/abs/2308.12315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronghang Zhu, Dongliang Guo, Daiqing Qi, Zhixuan Chu, Xiang Yu, Sheng Li</li>
<li>for: 本研究旨在提供一个可靠的表征学习框架，以满足实际应用中的跨领域应用需求。</li>
<li>methods: 本研究使用了四个基本概念， namely 类别、隐私、公平和解释性，来建立一个包含多种方法的可靠表征学习框架。</li>
<li>results: 本研究提出了一个全面的文献综述，涵盖了从四个基本概念中的多种方法，以及它们在实际应用中的应用和发展前景。<details>
<summary>Abstract</summary>
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.
</details>
<details>
<summary>摘要</summary>
现在人工智能系统在我们日常生活和人类社会中广泛应用，人们不仅享受了这些技术的好处，也面临着由这些系统引起的许多社会问题。为了让人工智能系统足够可靠和可信，很多研究者在建立可靠人工智能系统的指南方面做出了很多努力。机器学习是人工智能系统中最重要的一部分，表示学习是机器学习的核心技术。为了让表示学习在实际应用中是可靠的，例如跨领域场景，是非常有价值和必需的。 inspirited by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.Here's the translation in Traditional Chinese:现在人工智能系统在我们日常生活和人类社会中广泛应用，人们不单享受了这些技术的好处，也面临由这些系统引起的许多社会问题。为了让人工智能系统足够可靠和可信，很多研究者在建立可靠人工智能系统的指南方面做出了很多努力。机器学习是人工智能系统中最重要的一部分，表示学习是机器学习的核心技术。为了让表示学习在实际应用中是可靠的，例如跨领域场景，是非常有价值和必需的。 inspirited by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.
</details></li>
</ul>
<hr>
<h2 id="On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget"><a href="#On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget" class="headerlink" title="On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget"></a>On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12000">http://arxiv.org/abs/2308.12000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-An Wang, Kaito Ariu, Alexandre Proutiere</li>
<li>For: 本研究考虑了固定预算下最佳臂标识问题，具体是随机两臂投注机制下的 Bernoulli 奖励。* Methods: 我们使用了一种自然的一臂投注算法，即所谓的{\it 均匀投注}算法，并证明了这种算法在所有情况下都是最佳的。此外，我们还引入了一种名为{\it 一臂投注}的自然的算法类，并证明了任何能够与{\it 均匀投注}算法相当的算法都必须属于这种类。* Results: 我们证明了，无论是在所有情况下还是在某些特定情况下，都无法找到一种能够超过{\it 均匀投注}算法的算法。具体来说，我们证明了任何能够与{\it 均匀投注}算法相当的算法都必须是一种{\it 一臂投注}算法。这个结论解决了在\cite{qin2022open}中提出的两个开放问题。<details>
<summary>Abstract</summary>
We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
</details>
<details>
<summary>摘要</summary>
我们研究了固定预算下最佳臂标识问题， Specifically, we study the problem of best-arm identification with a fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the uniform sampling algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm.To achieve this result, we introduce the natural class of consistent and stable algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.Here's the translation in Traditional Chinese:我们研究了固定预算下最佳臂标识问题， Specifically, we study the problem of best-arm identification with a fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the uniform sampling algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm.To achieve this result, we introduce the natural class of consistent and stable algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
</details></li>
</ul>
<hr>
<h2 id="Relational-Concept-Based-Models"><a href="#Relational-Concept-Based-Models" class="headerlink" title="Relational Concept Based Models"></a>Relational Concept Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11991">http://arxiv.org/abs/2308.11991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aghoreshwar/Awesome-Customer-Analytics">https://github.com/Aghoreshwar/Awesome-Customer-Analytics</a></li>
<li>paper_authors: Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra</li>
<li>for: 本研究的目的是解释深度学习模型在关系领域中的工作方式，以便提高模型的可解释性和可信度。</li>
<li>methods: 本研究提出了一种新的关系深度学习模型，即关系概念基模型（Relational Concept-Based Models，RCBMs），该模型结合了深度学习和概念分析技术，以提高模型的可解释性和可信度。</li>
<li>results: 实验结果表明，关系CBMs可以与现有的关系黑盒模型（relational black-box models）匹配的普适性和可解释性，同时支持生成量化的概念基模型解释。此外，关系CBMs还可以在各种具有挑战性的情况下表现出色，如out-of-distribution场景、有限的训练数据场景和罕见概念指导场景。<details>
<summary>Abstract</summary>
The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.
</details>
<details>
<summary>摘要</summary>
文本： Deep learning 模型在关系领域的设计呈现出一个开放的挑战：可读性深度学习方法，如概念基本模型（CBMs），不是设计来解决关系问题，而关系模型则不如可读性深度学习方法。为解决这个问题，我们提出了关系概念基本模型（Relational CBMs），这是一种可读性深度学习方法，可以在关系任务上提供可读性的任务预测。我们的实验，从图像分类到知识图表链接预测，表明了关系 CBMs 具有以下特点：(i) 与现有关系黑盒相比，可以达到相同的泛化性能; (ii) 可以生成量化的概念基本解释; (iii) 在测试时间干扰中能够有效回应; (iv) 在具有异常分布、有限训练数据和罕见概念监督的情况下，也能够坚持。翻译结果：文本：深度学习模型在关系领域的设计存在一个开放的挑战，可读性深度学习方法，如概念基本模型（CBMs），不是设计来解决关系问题，而关系模型则不如可读性深度学习方法。为解决这个问题，我们提出了关系概念基本模型（Relational CBMs），这是一种可读性深度学习方法，可以在关系任务上提供可读性的任务预测。我们的实验，从图像分类到知识图表链接预测，表明了关系 CBMs 具有以下特点：(i) 与现有关系黑盒相比，可以达到相同的泛化性能; (ii) 可以生成量化的概念基本解释; (iii) 在测试时间干扰中能够有效回应; (iv) 在具有异常分布、有限训练数据和罕见概念监督的情况下，也能够坚持。
</details></li>
</ul>
<hr>
<h2 id="Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks"><a href="#Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks" class="headerlink" title="Will More Expressive Graph Neural Networks do Better on Generative Tasks?"></a>Will More Expressive Graph Neural Networks do Better on Generative Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiandong Zou, Xiangyu Zhao, Pietro Liò, Yiren Zhao</li>
<li>For: The paper is focused on the task of graph generation, specifically in the context of molecular graph generation for de-novo drug and molecular design.* Methods: The paper investigates the expressiveness of different Graph Neural Network (GNN) architectures in two popular generative frameworks (GCPN and GraphAF) on six different molecular generative objectives using the ZINC-250k dataset.* Results: The paper demonstrates that advanced GNNs can improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Additionally, the paper shows that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results compared to 17 other non-GNN-based graph generative approaches on important metrics such as DRD2, Median1, and Median2.Here is the information in Simplified Chinese text:* 用途：研究 graph generation 任务，特别是在分子图生成中，以满足 de-novo 药物和分子设计。* 方法： investigate  Graph Neural Network (GNN)  Architecture 在 GCPN 和 GraphAF 中，并在 ZINC-250k 数据集上进行 six 个分子生成目标的测试。* 结果：显示 advanced GNNs 可以提高 GCPN 和 GraphAF 在分子生成任务中的性能，但 GNN 表达能力不是必需的条件。此外， paper 还证明 GCPN 和 GraphAF 可以使用 advanced GNNs 在 DRD2、Median1 和 Median2 等重要指标上达到 state-of-the-art 结果，并且比 17 种非 GNN-based graph generative approach 更好。<details>
<summary>Abstract</summary>
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZINC-250k dataset. Through our extensive experiments, we demonstrate that advanced GNNs can indeed improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results across 17 other non-GNN-based graph generative approaches, such as variational autoencoders and Bayesian optimisation models, on the proposed molecular generative objectives (DRD2, Median1, Median2), which are important metrics for de-novo molecular design.
</details>
<details>
<summary>摘要</summary>
Graph生成具有重要挑战，因为它需要预测具有多个节点和边的完整图基于单个标签。这个任务对实际应用中的许多应用，如新药和分子设计，具有基本重要性。在过去几年，有几种成功的方法在图生成领域出现。然而，这些方法受到两个主要缺点的影响：（1）被用的图神经网络（GNN）架构经常被忽视；和（2）这些方法通常只被评估在有限数量的指标上。为了填补这个空白，我们在图生成任务上investigate GNN的表达能力，并将GNN替换为更表达力强的GNN。 Specifically，我们分析了六种GNN在两个不同的生成框架（GCPN和GraphAF）中的表现，在ZINC-250k数据集上进行六种分子生成目标。通过我们的广泛实验，我们证明了高级GNN可以提高GCPN和GraphAF在分子生成任务中的表现，但GNN表达能力不是必要的condition for a good GNN-based generative model。此外，我们显示GCPN和GraphAF使用高级GNN可以在17种非GNN-based图生成方法（如变量自动编码器和抽象优化模型）上达到状态的最佳结果，在提案的分子生成目标（DRD2、Median1、Median2）上。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression"><a href="#Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression" class="headerlink" title="Approximating Score-based Explanation Techniques Using Conformal Regression"></a>Approximating Score-based Explanation Techniques Using Conformal Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11975">http://arxiv.org/abs/2308.11975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Alkhatib, Henrik Boström, Sofiane Ennadir, Ulf Johansson</li>
<li>for: 本文旨在提出和研究一种 computationally less costly 的 regression model，用于approximating score-based explanation techniques，如 SHAP 的输出。</li>
<li>methods: 本文使用了 inductive conformal prediction 框架，提供了有效性保证，以及多种 non-conformity measures，用于考虑 approximations 的困难性，同时保持计算成本低。</li>
<li>results: 本文通过大规模的实验研究，发现提案的方法可以significantly improve execution time compared to fast SHAP version，TreeSHAP。 results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. In addition, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.<details>
<summary>Abstract</summary>
Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time compared to the fast version of SHAP, TreeSHAP. The results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. Moreover, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.
</details>
<details>
<summary>摘要</summary>
黑盒模型的解释技术 often 使用 Score-based 的解释技术，但这些解释技术经常具有高计算成本，这限制了它们在时间紧张的上下文中的应用。因此，我们提出了和探索使用 computationally 较低成本的回归模型来近似黑盒模型的输出。此外，我们采用了 inductive  conformal prediction 框架提供了有效性保证。我们提出了一些非准确度度量，用于考虑近似解释的困难而保持计算成本低。我们在大规模的实验中提出了这些方法，并评估了这些方法的效率（间隔大小）。结果表明，我们的提议方法可以significantly 改善执行时间，相比于快速版本的 SHAP，TreeSHAP。结果还表明，我们的方法可以生成紧凑的间隔，同时提供有效性保证。此外，我们的方法允许比较不同的近似方法的解释，并选择一种基于解释是否具有紧凑的预测间隔的方法。
</details></li>
</ul>
<hr>
<h2 id="EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE"><a href="#EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE" class="headerlink" title="EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE"></a>EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, Dongyu Zhang<br>for:这篇论文旨在开探如何建立可扩展的视觉语言模型，以学习具有多 modal 数据的多模式资料。methods:论文提出了一个效率的视觉语言基础模型，名为EVE，它是一个统一的多模式Transformer预测器，具有适应器 Mixture-of-Experts（MoE）模组，可以选择性地转换到不同的专家。results:论文表明，EVE可以在训练时间和资源更少的情况下，实现更好的下游性能，并且在多种视觉语言下游任务上实现了州流的表现。<details>
<summary>Abstract</summary>
Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将视觉语言模型建立成可扩展的基础模型仍然是一个开放的挑战。在这篇论文中，我们介绍了一个高效的视觉语言基础模型，即EVE（高效视觉语言基础模型）。EVE是一个共享转换网络，其中视觉和语言都被编码在同一个网络中，并通过特性意识模块来捕捉不同类型的信息。这些模块可以选择性地切换到不同的专家，以捕捉不同类型的信息。为了统一视觉和语言预训练任务，EVE在图像和文本对中进行遮盲信号模型，即将图像像素和文本字符遮盲，然后使用可见信号来还原遮盲信号。这个简单而有效的预训练目标可以加速训练，比 tradicional Image-Text Contrastive和Image-Text Matching损失的3.5倍。由于EVE的整合的体系和预训练任务，它具有更好的下游性能，需要 fewer resources和更快的训练速度。尽管它的简单性，EVE仍然达到了视觉语言下游任务的州OF-THE-ART性能，包括视觉问答、视觉逻辑和图像-文本检索。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification"><a href="#Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification" class="headerlink" title="Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification"></a>Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11969">http://arxiv.org/abs/2308.11969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lambert, Pauline Roca, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>for: 本研究旨在提出一种自动化的肝脏和肿瘤分 segmentation 方法，以帮助治疗抑制肝癌的医疗决策。</li>
<li>methods: 本研究使用了两种不同的管道，即多类分类模型和二元分类模型，以实现肝脏和肿瘤的同时分 segmentation。</li>
<li>results: 研究结果显示了两种管道具有不同的优劣点，并提出了一种不确定性评估策略，以识别可能存在的假阳性肿瘤患者。<details>
<summary>Abstract</summary>
The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipelines exhibit different strengths and weaknesses. Moreover we propose an uncertainty quantification strategy allowing the identification of potential false positive tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge regarding liver and tumor segmentation.
</details>
<details>
<summary>摘要</summary>
liver tumor 的负担是非常重要的， ranking as the fourth leading cause of cancer mortality。在肝细胞癌（HCC）的 случа子中，通过对增强磁共振成像（CE-MRI）进行描述，以便引导治疗策略。然而，由于这个任务需要较高的专业知识和较长的时间，并且可能会受到观察者间的差异，因此有强需求于自动工具。然而，由于数据不足以及图像分辨率和MRI序列的高变化性，这些任务具有挑战性。在这项工作中，我们提出了两种不同的管道，基于不规则模型来实现肝脏和肿瘤的分割。第一个管道是基础多类模型，同时进行肝脏和肿瘤的同时分割。第二个管道是分别训练两个不同的二进制模型，一个用于肝脏的分割，另一个用于肿瘤的分割。我们的结果显示，这两种管道具有不同的优势和劣势。此外，我们还提出了一种不确定性评估策略，以便标识潜在的假阳性肿瘤患区。这两种解决方案都被提交到了MICCAI 2023 Atlas challenge，关于肝脏和肿瘤的分割。
</details></li>
</ul>
<hr>
<h2 id="Maintaining-Plasticity-via-Regenerative-Regularization"><a href="#Maintaining-Plasticity-via-Regenerative-Regularization" class="headerlink" title="Maintaining Plasticity via Regenerative Regularization"></a>Maintaining Plasticity via Regenerative Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Benjamin Van Roy</li>
<li>for: 维护 continual learning 中的材料塑性（plasticity），使 neural network 能够快速适应新信息。</li>
<li>methods: 提出了 L2 Init，一种简单的方法，通过在损失函数中添加 L2 正则化来维护初始参数的塑性。</li>
<li>results: 在不同类型的非站ARY数据流程上进行了简单的问题示例，证明了 L2 Init 能够有效地避免材料塑性损失。 另外，我们发现了这个正则化项可以减少参数的大小，并保持高效的特征级别。<details>
<summary>Abstract</summary>
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates plasticity loss. We additionally find that our regularization term reduces parameter magnitudes and maintains a high effective feature rank.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation"><a href="#When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation" class="headerlink" title="When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation"></a>When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11953">http://arxiv.org/abs/2308.11953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Geng Tian, Ming Tang<br>for:这个论文旨在解决 federated learning (FL) 中的客户端偏移问题，提出了 MiniBatch-SFL 算法，它将 MiniBatch SGD integrated into SFL 中。methods:这个论文使用了 SFL 分布式算法，并在客户端和服务器之间分割模型。客户端只需要训练部分模型，而服务器则负责训练服务器端模型。此外，这个论文还使用了 MiniBatch SGD 算法来优化客户端模型。results:这个论文的实验结果表明，MiniBatch-SFL 算法可以在非Identical Independent Distributions (non-IID) 数据上提高精度，并且可以与传统的 FL 和 SFL 相比，提高精度达到 24.1% 和 17.1%。此外，这个论文还发现，将 cut layer 放置在模型的末端可以降低客户端模型的均值梯度偏移。<details>
<summary>Abstract</summary>
Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-side updates do not depend on the non-IID degree of the clients' datasets and can potentially mitigate client drift. However, the client-side model relies on the non-IID degree and can be optimized by properly choosing the cut layer. Perhaps counter-intuitive, our empirical result shows that a latter position of the cut layer leads to a smaller average gradient divergence and a better algorithm performance. Moreover, numerical results show that MiniBatch-SFL achieves higher accuracy than conventional SFL and FL. The accuracy improvement can be up to 24.1\% and 17.1\% with highly non-IID data, respectively.
</details>
<details>
<summary>摘要</summary>
分布式学习（FL）允许分布式客户端（例如边缘设备）共同训练模型，无需分享原始数据。然而，FL可能具有 computationally expensive 的问题，因为客户端需要训练整个模型多次。SplitFed learning（SFL）是一种最近的分布式方法，它将模型在一个割层中分成两部分，客户端只需要训练一部分模型。然而，SFL仍然受到客户端数据非常不一致（non-IID）的问题困扰。为解决这个问题，我们提出了 MiniBatch-SFL。这个算法将 MiniBatch SGD  integrate into SFL，客户端在分布式方式上训练客户端模型，服务器则在服务器模型上进行类似的 MiniBatch SGD 训练。我们分析了 MiniBatch-SFL 的收敛性，并证明了 bound 的预期损失可以通过分析服务器和客户端模型更新的预期值来获得。服务器端更新不依赖于客户端数据的非一致程度，可能减轻客户端游弋问题。然而，客户端模型受到非一致度的影响，可以通过选择合适的割层来优化。 Surprisingly,我们的实验结果显示，在割层的位置越后，客户端模型的平均梯度差异越小，算法性能更好。此外，我们的数值结果显示，MiniBatch-SFL 可以达到更高的准确率，比 conventinal SFL 和 FL 高出 24.1% 和 17.1%。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting"><a href="#Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting"></a>Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11946">http://arxiv.org/abs/2308.11946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhang, Rui Wu, Sergiu M. Dascalu, Frederick C. Harris Jr</li>
<li>for: 这篇论文主要针对多重时间序列（MTS）预测任务进行探索，旨在模型时间序列之间的相互 dependencies。</li>
<li>methods: 本论文提出了一种维度不变的嵌入技术，可以将MTS数据转换为更高维度的空间，保持时间步骤和变数的维度，并且提出了一个多尺度 transformer  pyramid network（MTPNet），可以有效地捕捉时间序列之间的多个不同级数的相互dependencies。</li>
<li>results: 实验结果显示，提出的MTPNet方法在九个 benchmark 数据集上表现出色，较前一些现有的方法更好。<details>
<summary>Abstract</summary>
Multivariate Time Series (MTS) forecasting involves modeling temporal dependencies within historical records. Transformers have demonstrated remarkable performance in MTS forecasting due to their capability to capture long-term dependencies. However, prior work has been confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2). This limitation hinders their effectiveness in capturing diverse seasonalities, such as hourly and daily patterns. In this paper, we introduce a dimension invariant embedding technique that captures short-term temporal dependencies and projects MTS data into a higher-dimensional space, while preserving the dimensions of time steps and variables in MTS data. Furthermore, we present a novel Multi-scale Transformer Pyramid Network (MTPNet), specifically designed to effectively capture temporal dependencies at multiple unconstrained scales. The predictions are inferred from multi-scale latent representations obtained from transformers at various scales. Extensive experiments on nine benchmark datasets demonstrate that the proposed MTPNet outperforms recent state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）预测涉及到历史记录中的时间相关性模型化。transformers已经表现出了很好的性能在MTS预测中，因为它们可以捕捉长期相关性。然而，先前的工作受限于模型时间相关性的固定尺度或多个尺度，其中尺度递增级数(大多数为2)。这种限制使得它们难以捕捉多样化的季节性，如每小时和每天的模式。在这篇论文中，我们介绍了一种维度不变的嵌入技术，该技术可以捕捉短期时间相关性，并将MTS数据 проек到高维空间中，保持时间步骤和变量的维度。此外，我们提出了一种新的多尺度transformer piramid网络（MTPNet），该网络专门设计用于有效地捕捉多个不受限制的时间尺度的时间相关性。预测来自多个尺度的秘密表示，由transformers在不同尺度上获得。经验表明，我们提出的MTPNet在九个 benchmark datasets上表现出了较高的性能。
</details></li>
</ul>
<hr>
<h2 id="RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching"><a href="#RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching" class="headerlink" title="RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching"></a>RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11943">http://arxiv.org/abs/2308.11943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Vott, Adam M. Lehavi</li>
<li>for: 本研究探讨了应用最佳先进搜索算法和强化学习（RL）技术来找到特定的拉曼数字（Ramsey number）的对例。</li>
<li>methods: 本研究引入了图Vectorization和深度强化网络（DNN）基于的优化方法，用于评估图是否为对例，并且提出了一些算法优化来减少搜索时间的复杂度。</li>
<li>results: 本研究不目的是发现新的对例，而是提出了一种基于RL技术的对例探讨框架，可以应用于其他评估器。<details>
<summary>Abstract</summary>
The Ramsey number is the minimum number of nodes, $n = R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.
</details>
<details>
<summary>摘要</summary>
“拉姆馆数”是最少节点数量，$n = R(s, t)$, 使得所有无向简单图的顺序为$n$，都包含一个 clique 的顺序为$s$，或一个独立集的顺序为$t$。这篇论文探讨了使用最佳先搜索算法和强化学习（RL）技术来找到特定拉姆数字的对例。我们在先前搜索方法，如随机搜索，基础上进行了改进，通过引入图vector化和深度神经网络（DNN）基于的优化，来评估图是否为对例。这篇论文也提出了算法优化来限制搜索时间的多项式增长。本论文不是为提供新的对例，而是为探讨拉姆对例探索使用其他规则的框架。代码和方法通过PyPI包和GitHub存储库提供。
</details></li>
</ul>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
<li>for: 这个研究的目的是提高现有的文本到audio模型的可控性，以便在仅仅基于文本的情况下实现细化的声音生成。</li>
<li>methods: 该模型使用了一种新的方法，即在已经训练过的文本到audio模型基础上加入了内容（时间戳）和风格（折射和能量折射）等附加条件，以便控制生成的声音的时间顺序、折射和能量。</li>
<li>results: 实验结果表明，该模型成功实现了细化的声音生成，并且可以控制生成的声音在不同的时间戳、折射和能量上的表现。<details>
<summary>Abstract</summary>
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at https://conditionaudiogen.github.io/conditionaudiogen/
</details>
<details>
<summary>摘要</summary>
文本基于的音频生成模型受限，因为它们无法包含所有音频中的信息，导致仅仅基于文本的控制性不够。为解决这问题，我们提出了一种新的模型，它可以增强现有的预训练文本到音频模型的控制性，通过添加内容（时间戳）和风格（折射和能量折射）等条件。这种方法可以实现细致的控制音频的时间顺序、折射和能量。为保持生成的多样性，我们使用可训练的控制条件编码器，其中包括一个大型自然语言模型和可训练的拟合网络，以编码和融合其他条件，同时保持预训练文本到音频模型的重量冰结。由于缺乏适合的数据集和评价指标，我们将现有的数据集整合成一个新的数据集，包括音频和相应的条件，并使用一系列的评价指标来评价控制性性能。实验结果表明，我们的模型成功实现了细致的控制，以实现可控音频生成。生成的音频和数据集可以在https://conditionaudiogen.github.io/conditionaudiogen/上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series"><a href="#Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series" class="headerlink" title="Retail Demand Forecasting: A Comparative Study for Multivariate Time Series"></a>Retail Demand Forecasting: A Comparative Study for Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11939">http://arxiv.org/abs/2308.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah</li>
<li>for: 预测零售需求的精度预测是零售业的关键因素，对于公司的财务表现和供应链效率都是关键。</li>
<li>methods: 本研究使用了时间序列数据和 macro经济变量（如Consumer Price Index（CPI）、Index of Consumer Sentiment（ICS）和失业率）进行预测，并比较了不同的回归和机器学习模型以确定最佳预测模型。</li>
<li>results: 研究发现，通过添加macro经济变量，可以更好地预测零售需求，并且可以选择合适的预测模型以达到更高的预测精度。<details>
<summary>Abstract</summary>
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.
</details>
<details>
<summary>摘要</summary>
<<sys.language_model.translate(text="Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.", from_language="en", to_language="zh-CN")>>Here's the translation:精准预测销售需求是零售业的关键性质因素，直接影响营业性和供应链效率。随着全球市场变得越来越相连，企业正在寻找更高级的预测模型，以获得竞争优势。然而，现有的文献主要关注历史销售数据，忽视了消费者支出行为中的重要影响因素。本研究通过把 macro经济条件纳入销售需求时间序列数据中，使用 Consumer Price Index (CPI)、Index of Consumer Sentiment (ICS) 和失业率等macro经济变量，并利用这些全面的数据来开发和比较不同的回归和机器学习模型，以准确预测零售需求。
</details></li>
</ul>
<hr>
<h2 id="System-Identification-for-Continuous-time-Linear-Dynamical-Systems"><a href="#System-Identification-for-Continuous-time-Linear-Dynamical-Systems" class="headerlink" title="System Identification for Continuous-time Linear Dynamical Systems"></a>System Identification for Continuous-time Linear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11933">http://arxiv.org/abs/2308.11933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jonas-Nicodemus/phdmd">https://github.com/Jonas-Nicodemus/phdmd</a></li>
<li>paper_authors: Peter Halmos, Jonathan Pillow, David A. Knowles</li>
<li>for: 这篇论文旨在探讨kalman筛Filter的系统识别问题，具体来说是通过期望最大化（EM）程序来学习底层系统的参数。</li>
<li>methods: 这篇论文使用了一种新的两filter方法，其中一个是 posterior的分布，另一个是 bayesian derivation。这两个方法可以减少了前向传递的计算量。</li>
<li>results: 通过这种新的方法， authors 可以在不Regularly sampled measurements中进行系统识别，并且可以扩展kalman筛Filter的应用范围。他们还提出了一种扩展了learning的方法，可以在不Regularly sampled measurements中学习非线性系统。<details>
<summary>Abstract</summary>
The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurements. Generalizing the learning of latent linear dynamical systems (LDS) to continuous-time may extend the use of the hybrid Kalman filter to data which is not regularly sampled or has intermittent missing values, and can extend the power of non-linear system identification methods such as switching LDS (SLDS), which rely on EM for the linear discrete-time Kalman filter as a sub-unit for learning locally linearized behavior of a non-linear system. We apply the method by learning the parameters of a latent, multivariate Fokker-Planck SDE representing a toggle-switch genetic circuit using biologically realistic parameters, and compare the efficacy of learning relative to the discrete-time Kalman filter as the step-size irregularity and spectral-radius of the dynamics-matrix increases.
</details>
<details>
<summary>摘要</summary>
System identification for the Kalman filter, which relies on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has been largely studied assuming that observations are sampled at equally-spaced time points. However, in many applications, this assumption is unrealistic. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates that do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure that estimates the parameters of the SDE, naturally incorporating irregularly sampled measurements. Generalizing the learning of latent linear dynamical systems (LDS) to continuous-time may extend the use of the hybrid Kalman filter to data that is not regularly sampled or has intermittent missing values, and can extend the power of non-linear system identification methods such as switching LDS (SLDS), which rely on EM for the linear discrete-time Kalman filter as a sub-unit for learning locally linearized behavior of a non-linear system. We apply the method by learning the parameters of a latent, multivariate Fokker-Planck SDE representing a toggle-switch genetic circuit using biologically realistic parameters, and compare the efficacy of learning relative to the discrete-time Kalman filter as the step-size irregularity and spectral-radius of the dynamics-matrix increases.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas"><a href="#Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas" class="headerlink" title="Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas"></a>Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11929">http://arxiv.org/abs/2308.11929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cli-de/d_lsm">https://github.com/cli-de/d_lsm</a></li>
<li>paper_authors: Peifeng Ma, Li Chen, Chang Yu, Qing Zhu, Yulin Ding</li>
<li>for: 这项研究的目的是为了开发一种能够适应不同时间间隔的风险隐藏易受损地区风险评估方法，以便更好地预测滥覆风险。</li>
<li>methods: 这项研究使用了多种预测模型来实现年度风险评估，并使用了 SHAP 算法来解释每个模型的输入特征和预测结果。此外，研究还使用了 MT-InSAR 技术来增强和验证风险评估结果。</li>
<li>results: 研究结果显示，在香港大屿山岛的风险评估中，地形坡度和极端雨量是诱发滥覆的主要因素。此外，研究还发现，由于全球气候变化和香港政府实施的LPMitP计划，风险评估结果的变化归因于极端雨量事件。<details>
<summary>Abstract</summary>
Landslide susceptibility assessment (LSA) is of paramount importance in mitigating landslide risks. Recently, there has been a surge in the utilization of data-driven methods for predicting landslide susceptibility due to the growing availability of aerial and satellite data. Nonetheless, the rapid oscillations within the landslide-inducing environment (LIE), primarily due to significant changes in external triggers such as rainfall, pose difficulties for contemporary data-driven LSA methodologies to accommodate LIEs over diverse timespans. This study presents dynamic landslide susceptibility mapping that simply employs multiple predictive models for annual LSA. In practice, this will inevitably encounter small sample problems due to the limited number of landslide samples in certain years. Another concern arises owing to the majority of the existing LSA approaches train black-box models to fit distinct datasets, yet often failing in generalization and providing comprehensive explanations concerning the interactions between input features and predictions. Accordingly, we proposed to meta-learn representations with fast adaptation ability using a few samples and gradient updates; and apply SHAP for each model interpretation and landslide feature permutation. Additionally, we applied MT-InSAR for LSA result enhancement and validation. The chosen study area is Lantau Island, Hong Kong, where we conducted a comprehensive dynamic LSA spanning from 1992 to 2019. The model interpretation results demonstrate that the primary factors responsible for triggering landslides in Lantau Island are terrain slope and extreme rainfall. The results also indicate that the variation in landslide causes can be primarily attributed to extreme rainfall events, which result from global climate change, and the implementation of the Landslip Prevention and Mitigation Programme (LPMitP) by the Hong Kong government.
</details>
<details>
<summary>摘要</summary>
降坡风险评估 (LSA) 在减轻降坡风险方面具有重要的重要性。在最近几年，由于飞地和卫星数据的可用性的增加，数据驱动方法在预测降坡风险方面得到了广泛的应用。然而，降坡 inducing 环境 (LIE) 中的快速摆动，主要归因于外部触发因素的显著变化，如降水量，使得当今的数据驱动 LSA 方法困难于同时覆盖多年时间。本研究提出了动态降坡风险地图，使用多个预测模型来年度预测降坡风险。在实践中，这将不可避免小样本问题，因为降坡样本数在某些年仅有限制。另一个问题在于大多数现有 LSA 方法通常会训练黑盒模型适应特定数据集，而不能总结和提供降坡特征之间和预测之间的丰富解释。因此，我们提议使用元学习来学习表达能力快速适应，使用少量样本和梯度更新；并使用 SHAP 来对每个模型进行解释和降坡特征的排序。此外，我们还应用 MT-InSAR 来增强和验证 LSA 结果。选择的研究区为香港大屿山岛，我们在1992年至2019年之间进行了全面的动态 LSA。模型解释结果显示，降坡岛的主要触发降坡的因素是地形坡度和极端降水量。结果还表明，降坡的变化原因可以主要归因于全球气候变化和香港政府实施的降坡预防和控制Programme (LPMitP)。
</details></li>
</ul>
<hr>
<h2 id="Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks"><a href="#Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks" class="headerlink" title="Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks"></a>Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11925">http://arxiv.org/abs/2308.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangti Jin, Ramesh Sau, Luowei Yin, Zhi Zhou</li>
<li>for: 这篇论文是关于优化控制问题的数值解决方案，包括不包括箱constraint的情况。</li>
<li>methods: 该方法基于优化控制问题的第一阶最优系统，使用物理学 informed neural networks (PINNs) 解决coupled系统。</li>
<li>results: 论文提供了深度逻辑学习网络参数（例如深度、宽度、参数范围）和样本点数的$L^2(\Omega)$ 误差 bounds，并进行了误差分析。示例包括了比较现有三种方法。<details>
<summary>Abstract</summary>
In this work, we present and analyze a numerical solver for optimal control problems (without / with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种数值方法来解决优化控制问题（无框约束和框约束）的线性和半线性第二阶几何问题。我们基于优化控制问题的第一阶优化系统 derivated 一个联系系统，并使用物理学 Informed Neural Networks (PINNs) 解决这个联系系统。我们提供了数值方案的误差分析，并给出了 $L^2(\Omega)$ 误差 bound 在状态、控制和副状态上，这些 bound 取决于深度神经网络参数（例如深度、宽度和参数 bound）和采样点数量在领域和边界上。我们使用偏移Rademacher复杂度和领域和边界上的神经网络函数的稳定性和 lipschitz 连续性作为主要工具进行分析。我们在数据中提供了多个数值示例，以示出方法的应用和与三种现有方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe"><a href="#Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe" class="headerlink" title="Diverse Policies Converge in Reward-free Markov Decision Processe"></a>Diverse Policies Converge in Reward-free Markov Decision Processe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11924">http://arxiv.org/abs/2308.11924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrl-lab/diversepolicies">https://github.com/openrl-lab/diversepolicies</a></li>
<li>paper_authors: Fanqi Lin, Shiyu Huang, Weiwei Tu</li>
<li>for: 这篇论文的目的是提供一种统一的多种政策强化学习框架，并研究多种政策强化学习的训练是如何 converges 和效率如何。</li>
<li>methods: 这篇论文使用了一种提取多种政策的框架，并提出了一种可证明有效的多种政策强化学习算法。</li>
<li>results: 经过数学实验，论文证明了其方法的有效性和效率。<details>
<summary>Abstract</summary>
Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.
</details>
<details>
<summary>摘要</summary>
现在的束缚学习已经在许多决策任务中取得了很大的成功，但传统的束缚学习算法主要是为了获得单个优化解决方案。然而，最近的研究表明了多样化策略的重要性，使得这成为一个emerging研究话题。虽然多样化束缚学习算法的多种出现，但没有任何一个能够理论地回答束缚学习算法是如何收敛的和如何效率的问题。在这篇论文中，我们提供了一个统一的多样化束缚学习框架，并investigate束缚学习训练多样化策略的收敛性。根据这种框架，我们还提出了一种可证明高效的多样化束缚学习算法。最后，我们通过数学实验验证了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
<li>for: 这 paper 是为了解决音频描述中的 semantic difference 问题，提出了 Audio Difference Captioning (ADC) 任务。</li>
<li>methods: 该 paper 提出了一种 cross-attention-concentrated transformer encoder 和一种 similarity-discrepancy disentanglement 来提取差异。</li>
<li>results: 实验表明，提出的方法可以有效地解决 ADC 任务，并且可以提高 transformer encoder 中的 attention weights 来提取差异。<details>
<summary>Abstract</summary>
We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
</details>
<details>
<summary>摘要</summary>
我们提出了听音差异描述（ADC）作为audio描述中的新扩展任务，用于描述输入对的相似 yet slightly different 听音clip中的semantic差异。ADC解决了 convention audio描述 Sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach"><a href="#Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach" class="headerlink" title="Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach"></a>Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11912">http://arxiv.org/abs/2308.11912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/riiid/useraif">https://github.com/riiid/useraif</a></li>
<li>paper_authors: Soonwoo Kwon, Sojung Kim, Seunghyun Lee, Jin-Young Kim, Suyeong An, Kyuseok Kim</li>
<li>for: 这 paper 的目的是提出一种基于CAT服务中的应答数据的项目 profiling 方法，以提高CAT的效率和准确性。</li>
<li>methods: 这 paper 使用了一种叫做 user-wise aggregate influence function 方法，该方法可以纠正CAT中选择性的偏见问题，并提高CAT的性能。</li>
<li>results:  experiments 表明，使用了该方法可以减少CAT中的偏见问题，并提高CAT的准确性和效率。<details>
<summary>Abstract</summary>
Computerized Adaptive Testing (CAT) is a widely used, efficient test mode that adapts to the examinee's proficiency level in the test domain. CAT requires pre-trained item profiles, for CAT iteratively assesses the student real-time based on the registered items' profiles, and selects the next item to administer using candidate items' profiles. However, obtaining such item profiles is a costly process that involves gathering a large, dense item-response data, then training a diagnostic model on the collected data. In this paper, we explore the possibility of leveraging response data collected in the CAT service. We first show that this poses a unique challenge due to the inherent selection bias introduced by CAT, i.e., more proficient students will receive harder questions. Indeed, when naively training the diagnostic model using CAT response data, we observe that item profiles deviate significantly from the ground-truth. To tackle the selection bias issue, we propose the user-wise aggregate influence function method. Our intuition is to filter out users whose response data is heavily biased in an aggregate manner, as judged by how much perturbation the added data will introduce during parameter estimation. This way, we may enhance the performance of CAT while introducing minimal bias to the item profiles. We provide extensive experiments to demonstrate the superiority of our proposed method based on the three public datasets and one dataset that contains real-world CAT response data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models"><a href="#Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models" class="headerlink" title="Diagnosing Infeasible Optimization Problems Using Large Language Models"></a>Diagnosing Infeasible Optimization Problems Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12923">http://arxiv.org/abs/2308.12923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Gonzalo E. Constante-Flores, Can Li</li>
<li>for: 本研究旨在帮助决策问题中的优化模型实用化，通过自然语言对话系统OptiChat来帮助用户更好地理解和解释无法满足约束的优化模型。</li>
<li>methods: 本研究使用了GPT-4和一些自然语言处理技术，如几何学习、专家链思维、关键提取和情感提示，以提高OptiChat的可靠性。</li>
<li>results: 实验表明，OptiChat可以帮助both expert和非专家用户更好地理解优化模型，快速地发现约束不可遵循的来源。<details>
<summary>Abstract</summary>
Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optimization model itself, identify potential sources of infeasibility, and offer suggestions to make the model feasible. The implementation of OptiChat is built on GPT-4, which interfaces with an optimization solver to identify the minimal subset of constraints that render the entire optimization problem infeasible, also known as the Irreducible Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought, key-retrieve, and sentiment prompts to enhance OptiChat's reliability. Our experiments demonstrate that OptiChat assists both expert and non-expert users in improving their understanding of the optimization models, enabling them to quickly identify the sources of infeasibility.
</details>
<details>
<summary>摘要</summary>
决策问题可以表示为数学优化模型，在经济、工程和生产、运输和医疗等领域找到广泛应用。优化模型是决策问题的数学抽象，它的目的是找到满足一组要求或限制的最佳决策。但现有的优化模型诊断方法通常需要很多背景知识，特别是当模型无法满足所有限制时。在这篇论文中，我们介绍了一种名为OptiChat的自然语言基于系统，它通过自然语言描述优化模型、找到可能导致无法满足限制的来源、并提供改进模型的建议。OptiChat的实现基于GPT-4，它通过与优化解тил器集成来确定整个优化问题的不可能满足子集（IIS）。我们使用了少量学习、专家链条思维、关键检索和情感提示来增强OptiChat的可靠性。我们的实验表明，OptiChat可以帮助专家和非专家用户更好地理解优化模型，快速地找到无法满足限制的来源。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Admissible-Bounds-for-Heuristic-Learning"><a href="#Utilizing-Admissible-Bounds-for-Heuristic-Learning" class="headerlink" title="Utilizing Admissible Bounds for Heuristic Learning"></a>Utilizing Admissible Bounds for Heuristic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11905">http://arxiv.org/abs/2308.11905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Núñez-Molina, Masataro Asai</li>
<li>for: 这篇论文旨在解决超参论文中的问题，即使用现代机器学习技术学习前进搜索算法的准则，但是这些准则的选择和训练方法尚未得到充分的理论理解。</li>
<li>methods: 这篇论文使用了 truncated Gaussian distributions 作为参数，以便更紧的限制假设空间，从而更好地遵循最大 entropy 原则。</li>
<li>results: 论文的实验结果表明，使用 admissible heuristics 作为参数可以更准确地学习前进搜索算法的准则，并且在训练过程中更快地 converges。<details>
<summary>Abstract</summary>
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and empirically show that, as a result, it yields more accurate heuristics and converges faster during training.
</details>
<details>
<summary>摘要</summary>
“在最近几年中，使用现代机器学习技术学习前向搜索算法的启发函数Received increasing attention。然而，对于这些启发函数的学习还没有充分的理论理解，包括它们应该学习什么、如何训练它们以及为什么这样做。这导致了一些文献选择不优化的数据集（非最优化成本或非合法启发）和优化指标（例如平方差误差）。此外，由于启发函数的不合法性，对其学习过程中的合法性得 little attention。本文详细介绍了合法启发函数在监督式学习中的角色，并使用 truncated Gaussian distribution 作为参数，这会紧紧地限制假设空间，与普通 Gaussian distribution 相比。我们认为这种数学模型遵循最大Entropy原则，并且在实验中证明这会导致更加准确的启发函数和更快的学习速度。”Note: "Truncated Gaussian distribution" in the text refers to a type of probability distribution that is similar to a Gaussian distribution, but with a truncated range of values.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation"></a>Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11903">http://arxiv.org/abs/2308.11903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenzhao/dpms">https://github.com/zhenzhao/dpms</a></li>
<li>paper_authors: Zhen Zhao, Ye Liu, Meng Zhao, Di Yin, Yixuan Yuan, Luping Zhou<br>for: 本研究目的是提高 semi-supervised medical image segmentation（SSMIS）的性能。methods: 本研究提出了一种简单 yet effective的方法，称为DPMS，以提高 SSMIS 性能。DPMS 使用了一种 teacher-student 框架，并采用了数据杂化、模型稳定化和损失函数优化等策略来提高 SSMIS 性能。results: DPMS 在公共的 2D ACDC 和 3D LA 数据集上 across 多种 semi-supervised 设定中均获得了新的state-of-the-art 性能，比如在 ACDC 上使用 5% 标签时获得了22.62% 的提升。<details>
<summary>Abstract</summary>
Studies on semi-supervised medical image segmentation (SSMIS) have seen fast progress recently. Due to the limited labelled data, SSMIS methods mainly focus on effectively leveraging unlabeled data to enhance the segmentation performance. However, despite their promising performance, current state-of-the-art methods often prioritize integrating complex techniques and loss terms rather than addressing the core challenges of semi-supervised scenarios directly. We argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, we emphasize the crutiality of data perturbation and model stabilization in semi-supervised segmentation, and propose a simple yet effective approach to boost SSMIS performance significantly, dubbed DPMS. Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness. Based on these examinations, we then propose DPMS, which adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g. obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.
</details>
<details>
<summary>摘要</summary>
研究 semi-supervised medical image segmentation (SSMIS) 在最近几年内已经进步很快。由于受限于标注数据的有限性，SSMIS 方法主要关注使用无标注数据来提高 segmentation 性能。然而，现有的状态态� Armenian 方法经常强调 интегрирование复杂的技术和损失函数而不是直接面对半标注场景的核心挑战。我们认为 semi-supervised 的关键在于生成足够和合适的预测差异。为此，我们强调数据杂化和模型稳定在半标注 segmentation 中的重要性，并提出了一种简单 yet effective 的方法，称为 DPMS。我们从数据、模型和损失三个角度重新探讨 SSMIS，并进行了全面的研究相应的策略的效果。基于这些研究，我们提出了 DPMS，它采用了一种简单的教师-学生框架，并采用标准的supervised损失函数和无标注一致损失函数。为生成足够的预测差异，DPMS 通过强大的杂化来增加预测差异较大。另一方面，在强大杂化应用时，使用 EMA 教师并不一定提高性能。DPMS 还利用了前向 twice 和积分更新策略来normal化统计，以确保在无标注数据上的训练效果。尽管简单，DPMS 可以在各种 semi-supervised 设置中取得新的状态态� Armenian 性能，例如在 ACDC 和 LA 数据集上取得了22.62%的提高。
</details></li>
</ul>
<hr>
<h2 id="Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models"><a href="#Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models" class="headerlink" title="Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models"></a>Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11890">http://arxiv.org/abs/2308.11890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, Xia Ning</li>
<li>for:  Identifying novel drug candidates with desired 3D shapes that bind to protein target pockets.</li>
<li>methods: 使用ShapeMol，一种基于ShapeMol的均衡生成模型，将分子表面形状编码成 latent 表示，然后通过这些表示生成3D分子结构。</li>
<li>results: ShapeMol 可以生成 novel, diverse, drug-like 分子，其3D分子结构与给定形状相似。这些结果表明 ShapeMol 可能在设计抗体细胞膜的药物候选者中发挥作用。<details>
<summary>Abstract</summary>
Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
</details>
<details>
<summary>摘要</summary>
ligand-based drug design的目的是找到与已知活性分子相似的新药候选体。在这篇论文中，我们形ulated an in silico shape-conditioned molecule generation problem，以生成基于给定分子形状的3D分子结构。为解决这个问题，我们开发了一种具有转化和旋转对称的形态响应生成模型ShapeMol。ShapeMol包括一种对称形态编码器，该编码器将分子表面形状映射到缓存中的嵌入式，以及一种对称扩散模型，该模型基于这些嵌入生成3D分子。实验结果表明，ShapeMol可以生成新、多样、药理化的3D分子结构，这些结构与给定形状相似。这些结果表明ShapeMol在设计欲 Bind to protein target pocket的药物候选体中具有潜在的应用价值。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-Using-Feedback-Loops"><a href="#Adversarial-Training-Using-Feedback-Loops" class="headerlink" title="Adversarial Training Using Feedback Loops"></a>Adversarial Training Using Feedback Loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11881">http://arxiv.org/abs/2308.11881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Haisam Muhammad Rafid, Adrian Sandu</li>
<li>for: 防止深度神经网络受到攻击</li>
<li>methods: 使用控制理论和反馈控制网络</li>
<li>results: 比现状更有效地防止攻击<details>
<summary>Abstract</summary>
Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.   This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numerical results on standard test problems empirically show that our FLAT method is more effective than the state-of-the-art to guard against adversarial attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets"><a href="#SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets" class="headerlink" title="SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets"></a>SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11880">http://arxiv.org/abs/2308.11880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csimo005/summit">https://github.com/csimo005/summit</a></li>
<li>paper_authors: Cody Simons, Dripta S. Raychaudhuri, Sk Miraj Ahmed, Suya You, Konstantinos Karydis, Amit K. Roy-Chowdhury</li>
<li>for: 这 paper 的目的是解决多Modal 数据的Scene Understanding问题，以便在多种应用中实现自主导航等。</li>
<li>methods: 这 paper 使用了一种 switching 框架，通过自动选择 cross-modal pseudo-label 融合方法（agreement filtering 和 entropy weighting）来适应不同的目标领域。</li>
<li>results: 这 paper 的实验结果表明，该方法可以在七个复杂的适应场景中实现比较好的效果，与有源数据的方法相当或者超过。 Specifically, the improvement in mIoU was up to 12% compared to baseline methods.<details>
<summary>Abstract</summary>
Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. Our proposed approach solves this problem through a switching framework which automatically chooses between two complementary methods of cross-modal pseudo-label fusion -- agreement filtering and entropy weighting -- based on the estimated domain gap. We demonstrate our work on the semantic segmentation problem. Experiments across seven challenging adaptation scenarios verify the efficacy of our approach, achieving results comparable to, and in some cases outperforming, methods which assume access to source data. Our method achieves an improvement in mIoU of up to 12% over competing baselines. Our code is publicly available at https://github.com/csimo005/SUMMIT.
</details>
<details>
<summary>摘要</summary>
场景理解使用多Modal数据是非常重要的，例如自主导航。要在各种情况下实现这一点，现有的模型需要能够适应数据分布的变化，而无需辛苦地注释数据。现有的方法假设源数据在适应过程中可以获得，并且假设源数据是对应的多Modal数据。这两个假设可能会对许多应用程序带来问题。源数据可能因为隐私、安全或经济问题而不可用。假设存在对应的多Modal数据 для训练也会导致极大的数据收集成本，并且不利用广泛可用的免费分布的预训练单Modal模型。在这项工作中，我们松弛了这两个假设，解决了一个独立训练在单Modal数据上的模型，并在目标领域中使用无标记多Modal数据进行适应，无需访问源数据集。我们提出的方法通过自动选择两种补充方法——协调过滤和Entropy质量——根据估计的领域差来解决这个问题。我们在semantic segmentation问题中进行了实验，并在七个困难的适应场景中证明了我们的方法的有效性，与可以访问源数据的方法相比，我们的方法可以达到和在某些情况下超过相同的结果。我们的方法可以提高mIoU指标的改进率达到12%。我们的代码公开可用于https://github.com/csimo005/SUMMIT。
</details></li>
</ul>
<hr>
<h2 id="Cabrita-closing-the-gap-for-foreign-languages"><a href="#Cabrita-closing-the-gap-for-foreign-languages" class="headerlink" title="Cabrita: closing the gap for foreign languages"></a>Cabrita: closing the gap for foreign languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, Vinicius Caridá</li>
<li>for: 这个研究的目的是提高特定语言或领域上模型的性能，以及解决有效的tokenization问题。</li>
<li>methods: 这个研究使用了自scratch训练模型，并开发了一种名为Cabrita的方法，以解决性能和有效tokenization问题。</li>
<li>results: 研究表明，使用Cabrita方法可以在低成本下提高模型的性能，并且在少量学习任务中达到了与传统连续预训练方法和7B英语预训练模型相当的结果。<details>
<summary>Abstract</summary>
The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.   The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.   To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We believe that this methodology can be applied to any transformer-like architecture model. To validate the study, we conducted continuous pre-training exclusively using Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text. In our assessment, for few-shot learning tasks, we achieved similar results with this 3B model compared to a traditional continuous pre-training approach as well as to 7B models English pre-trained models.
</details>
<details>
<summary>摘要</summary>
这种训练模型从零开始在特定语言或领域中的策略有两个重要目的：一是提高特定语言或领域上表现，二是确保有效的分词。然而，这种方法的主要局限性在于相关的成本，可能达到六到七位数字的值，具体取决于模型大小和参数的数量。为了缓解这个挑战，我们可以依靠可用的预训练模型，尽管最近的进步，如LLaMA和LLaMA-2模型，仍然在特定领域问题上不够灵活，因为需要大量的 tokens 来表示文本。为了解决这个问题，我们提出了一种方法Named Cabrita，它成功地解决表现和有效的分词问题，并且具有可Affordable的成本。我们认为这种方法可以应用于任何 transformer-like 架构模型。为了验证这种方法的有效性，我们进行了不间断的预训练，只使用葡萄牙语文本，并使用一个3亿参数的模型，称为 OpenLLaMA，从而生成了一个名为 openCabrita 3B 的模型。openCabrita 3B 还有一个新的分词器，它可以减少表示文本所需的 tokens 的数量。在我们的评估中，我们在少量学习任务上达到了与传统的连续预训练方法和英语预训练模型相同的结果，而无需投入大量的时间和资源。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations"><a href="#Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations" class="headerlink" title="Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations"></a>Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11873">http://arxiv.org/abs/2308.11873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp1511unsw/dcc">https://github.com/comp1511unsw/dcc</a></li>
<li>paper_authors: Andrew Taylor, Alexandra Vassar, Jake Renzella, Hammond Pearce</li>
<li>for: 这个论文旨在使用大型自然语言模型（LLM）生成改进的编译器错误说明，以便为beginner programmer提供更好的学习体验。</li>
<li>methods: 这个论文使用了LLM生成错误说明，并在Debugging C Compiler（DCC）中实现了这种方法。</li>
<li>results: 经过专家评估，LLM生成的错误说明具有90%的编译时错误概率和75%的运行时错误概率的概念准确性。此外，DCC-help工具已经得到了学生的广泛采用，每周平均有1047次唯一运行，表明使用LLM complement编译器输出可以有助于初学者学习编程。<details>
<summary>Abstract</summary>
This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, demonstrating a promising initial assessment of using LLMs to complement compiler output to enhance programming education for beginners. We release our tool as open-source to the community.
</details>
<details>
<summary>摘要</summary>
Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, demonstrating a promising initial assessment of using LLMs to complement compiler output to enhance programming education for beginners. We release our tool as open-source to the community.
</details></li>
</ul>
<hr>
<h2 id="Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form"><a href="#Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form" class="headerlink" title="Fast Exact NPN Classification with Influence-aided Canonical Form"></a>Fast Exact NPN Classification with Influence-aided Canonical Form</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12311">http://arxiv.org/abs/2308.12311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhang, Liwei Ni, Jiaxi Zhang, Guojie Luo, Huawei Li, Shenggen Zheng</li>
<li>for: 本文关于NPN类别的研究，具有许多应用在数字电路的合成和验证中。</li>
<li>methods: 本文提出了一种新的 canonical form 和其算法，通过引入Boolean influence来加速NPN类别。</li>
<li>results: 实验结果表明，影响在计算 canonical form 中减少了转换枚举的数量，相比之前的算法在 ABC 中实现的最佳效果，本文的影响帮助 canoncal form 在 exact NPN 类别中获得了5.5倍的速度提升。<details>
<summary>Abstract</summary>
NPN classification has many applications in the synthesis and verification of digital circuits. The canonical-form-based method is the most common approach, designing a canonical form as representative for the NPN equivalence class first and then computing the transformation function according to the canonical form. Most works use variable symmetries and several signatures, mainly based on the cofactor, to simplify the canonical form construction and computation. This paper describes a novel canonical form and its computation algorithm by introducing Boolean influence to NPN classification, which is a basic concept in analysis of Boolean functions. We show that influence is input-negation-independent, input-permutation-dependent, and has other structural information than previous signatures for NPN classification. Therefore, it is a significant ingredient in speeding up NPN classification. Experimental results prove that influence plays an important role in reducing the transformation enumeration in computing the canonical form. Compared with the state-of-the-art algorithm implemented in ABC, our influence-aided canonical form for exact NPN classification gains up to 5.5x speedup.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
<li>for:  This paper aims to improve the robustness of Kinyarwanda speech recognition using self-supervised pre-training, a simple curriculum schedule, and semi-supervised learning.</li>
<li>methods: The approach uses public domain data only, including a new studio-quality speech dataset and a more diverse and noisy public dataset. The model is trained using a simple curriculum schedule and semi-supervised learning.</li>
<li>results: The final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on the Mozilla Common Voice benchmark, which is state-of-the-art to the best of the authors’ knowledge. Additionally, the authors find that syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.Here is the simplified Chinese text in the format you requested:</li>
<li>for: 这篇论文目的是提高基奈语音识别的稳定性，使用自动预训练、简单的学习级排程和半监督学习。</li>
<li>methods: 方法使用公共领域数据，包括一个新的录音室质量的语音数据集和一个更多样本和噪音的公共数据集。模型使用简单的学习级排程和半监督学习。</li>
<li>results: 最终模型在新数据集上达到3.2%词错率（WER）和Mozilla Common Voice标准测试数据上达到15.9% WER，这是作者们知道的最佳成绩。此外，作者们发现，使用 syllabic 而不是字符基本的分词可以提高基奈语音识别性能。<details>
<summary>Abstract</summary>
Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.
</details>
<details>
<summary>摘要</summary>
尽管最近有大量的采用Kinyarwanda语音训练数据，但实现Kinyarwanda语音识别仍然是一项挑战。在这项工作中，我们表明使用自我监督预训练、在练习阶段按照简单的课程表进行调整以及使用半监督学习来利用大量未标注的语音数据，可以显著提高Kinyarwanda语音识别性能。我们的方法仅使用公共领域数据。我们收集了一个新的studio质量语音数据集，然后使用这个数据集来训练一个清晰的基线模型。然后，我们使用这个基线模型来排序来自更加多样化和噪音公共数据集的示例，定义了一个简单的课程训练计划。最后，我们应用半监督学习来标注和学习大量未标注的数据，在四个成功的代代中进行四次生成。我们的最终模型在新数据集上达到3.2%的单词错误率（WER）和Mozilla Common Voicebenchmark上达到15.9%的WER，这是我们知道的状态之最。我们的实验还表明，使用音节基于的分词而不是字符基于的分词可以提高Kinyarwanda语音识别性能。
</details></li>
</ul>
<hr>
<h2 id="Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0"><a href="#Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0" class="headerlink" title="Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0"></a>Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11854">http://arxiv.org/abs/2308.11854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anmol Chaure, Ashok Kumar Behera, Sudip Bhattacharya</li>
<li>for: 该研究使用数据驱动机器学习模型作为气候侦测器，以帮助政策制定者做出了解的决策。</li>
<li>methods: 该研究使用机器学习 emulator 作为 computationally heavy GCM simulator 的代理，以降低时间和碳脚印。</li>
<li>results: 研究发现，尽管被视为基本的，回归模型在气候侦测方面具有许多优势。特别是通过内核技术，回归模型可以捕捉复杂的关系，提高预测能力。该研究比较了三种非线性回归模型，其中 Gaussian Process Regressor 表现最佳，但它在空间和时间复杂度方面具有较高的计算资源占用。相比之下，支持向量和 kernel ridge 模型也可以提供竞争力，但需要解决一些交易OFF。此外，我们正在活动调查 composite kernels 和变量推理技术的性能，以进一步提高回归模型的表现和有效地模型复杂非线性现象，包括降水现象。<details>
<summary>Abstract</summary>
Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computational resource hungry in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results and but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.
</details>
<details>
<summary>摘要</summary>
“气候预测使用数据驱动机器学模型作为模拟器，是当前气候政策决策支持的一个主要领域。使用机器学模型作为计算昂贵GCM模拟器的代理，可以降低时间和碳脚印。在这个方向上，气候Bench（1）是最近筛选的气候机器学模型评估 benchmark dataset。据报道，尽管被视为基本的，但是回归模型在气候模拟方面具有多种优势。具体来说，通过kernel trick，回归模型可以捕捉复杂的关系，提高预测能力。本研究将focus on evaluating non-linear regression models using the above dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computationally resource-intensive in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results, but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.”Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need further assistance.
</details></li>
</ul>
<hr>
<h2 id="A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data"><a href="#A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data" class="headerlink" title="A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data"></a>A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11849">http://arxiv.org/abs/2308.11849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Liu, Zhiyuan Lin, Judith Y. T. Wang, Hong Chen</li>
<li>for:  This paper aims to develop a real-time railway rescheduling approach that can automatically adjust the operation schedule in response to time-varying conditions, with a focus on a heavy-demand station upstream of a disrupted area.</li>
<li>methods:  The proposed approach uses mobile data (MD) to infer real-world passenger mobility and a deep reinforcement learning (DRL) framework to determine the optimal reschededuled timetable, route stops, and rolling stock allocation.</li>
<li>results:  The proposed approach can effectively handle challenges such as station overcrowding, rolling stock shortage, open-ended disruption duration, and delays due to detours, while ensuring real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.<details>
<summary>Abstract</summary>
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accumulation of overcrowded passengers at this station, to prevent additional accidents arising from overcrowding. This research addresses the challenges associated with this scenario, including the dynamics of arriving and leaving of passengers, station overcrowding, rolling stock shortage, open-ended disruption duration, integrated rescheduling on multiple routes, and delays due to detours. A deep reinforcement learning (DRL) framework is proposed to determine the optimal rescheduled timetable, route stops, and rolling stock allocation, while considering real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.
</details>
<details>
<summary>摘要</summary>
现实时铁路重新安排是一种时间可靠和灵活的技术，可以自动根据时间变化的条件修改运营时间表。现有研究缺乏基于实时旅客流动数据的数据驱动方法，多数依赖于 Origin-Destination（OD）数据和模型基于方法来估算列车需求。同时，长期紧急情况下的时间表更新原则忽视了时间分布的不均。为了填补这一空白，这篇论文提出了一种响应式的需求方法，通过抽象实际旅客流动数据来支持实时重新安排。与传统网络水平方法不同，这篇论文将重点放在一个受到紧急事件影响的重要客运站上，该站位于紧急事件的上游。目标是重新安排经过该站的所有列车，并且在严重紧急事件such as自然灾害发生时进行重新安排。特别是要避免在该站堆积过多的旅客，以避免由过度堆积而引起的额外事故。本研究解决了这种情况中的挑战，包括到站拥堵、列车短缺、开放式紧急事件持续时间、多路线重新安排、延迟due to detours。一种深度强化学习（DRL）框架被提出，以确定最佳重新安排时间表、站点停留、列车分配，同时考虑实时需求满足、站点拥堵、列车容量利用率和队列安全。
</details></li>
</ul>
<hr>
<h2 id="SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks"><a href="#SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks" class="headerlink" title="SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks"></a>SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11845">http://arxiv.org/abs/2308.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Gao, Ilia Shumailov, Kassem Fawaz</li>
<li>for: 防止机器学习系统面临黑盒攻击的攻击者 profiling 和攻击信息共享。</li>
<li>methods: 基于隐藏марков模型框架，对黑盒攻击进行攻击特征分析和攻击行为描述。</li>
<li>results: 能够准确地识别黑盒攻击，包括第二次攻击，并能够抗击 adaptive 策略和库特定漏洞攻击。<details>
<summary>Abstract</summary>
Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, there is a need for a more comprehensive approach to logging, analyzing, and sharing evidence of attacks. While classic security benefits from well-established forensics and intelligence sharing, Machine Learning is yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages the Hidden Markov Models framework to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on their second occurrence, and is robust to adaptive strategies designed to evade forensics analysis. Interestingly, SEA's explanations of the attack behavior allow us even to fingerprint specific minor implementation bugs in attack libraries. For example, we discover that the SignOPT and Square attacks implementation in ART v1.14 sends over 50% specific zero difference queries. We thoroughly evaluate SEA on a variety of settings and demonstrate that it can recognize the same attack's second occurrence with 90+% Top-1 and 95+% Top-3 accuracy.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）系统容易受到反面示例攻击，特别是来自查询基本黑盒攻击。尽管有各种努力检测和防止这些攻击，但是还是需要一种更全面的方法来记录、分析和分享攻击证据。 Classic security 受益于良好的遗产安全和知识共享，而 Machine Learning 尚未找到一种 profiling 攻击者并分享关于他们的信息的方法。因此，这篇论文介绍了 SEA，一种新的 ML 安全系统，用于 caracterize 黑盒攻击 ML 系统的攻击行为，以便进行审计和人类可解释的知识分享。SEA 基于隐藏markov模型框架，将观察到的查询序列归因于已知的攻击。因此，它可以理解攻击的进程，而不仅仅是关注最终的反面示例。我们的评估表明，SEA 可以有效地归因攻击，甚至在第二次出现时，并且具有鲁棒性，可以抵御适应攻击的策略。另外，SEA 的解释也可以让我们发现特定的小型实现漏洞。例如，我们发现ART v1.14中的SignOPT和方块攻击实现中发送了50%以上的特定零差查询。我们对 SEA 进行了多种设置的测试，并证明它可以在不同的设置下识别同一个攻击的第二次出现，Top-1 和 Top-3 准确率高于 90% 和 95%。
</details></li>
</ul>
<hr>
<h2 id="rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning"></a>${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11842">http://arxiv.org/abs/2308.11842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dchen48/e3ac">https://github.com/dchen48/e3ac</a></li>
<li>paper_authors: Dingyang Chen, Qi Zhang</li>
<li>for: 这篇论文的目的是解决协同多体学习（MARL）问题中的几何 symmetries问题，并通过嵌入几何约束来提高多体actor-critic方法的性能。</li>
<li>methods: 该论文使用了形式化 caracterizing a subclass of Markov games with a general notion of symmetries，并设计了具有几何约束的神经网络架构。</li>
<li>results: 该论文的实验结果表明，在各种协同MARL benchmark中，具有几何约束的神经网络架构可以获得更高的性能，并且具有很好的总结和传播学习能力。<details>
<summary>Abstract</summary>
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns. The code is available at: https://github.com/dchen48/E3AC.
</details>
<details>
<summary>摘要</summary>
“对自然世界中的对称图案进行识别和分析，对于不同科学领域的发现做出了重要贡献，例如物理学中的重力法则和化学结构的研究。在这篇文章中，我们专注于利用多客户多代理人学习（MARL）问题中的对称性，并将其应用到许多实际应用中。我们首先正式定义了一个泛化的Markov游戏，其中包含一般的对称性，并证明了存在对称优值和策略。这些对称性导致我们设计了具有对称约束的神经网络架构，并将其作为多代理人actor-critic方法的类别偏好。这种偏好导致了在多个合作MARL测试中的出色表现，包括零shot学习和转移学习在未见过的对称图案中。代码可以在以下GitHub页面上获取：https://github.com/dchen48/E3AC。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures"><a href="#A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures" class="headerlink" title="A Survey for Federated Learning Evaluations: Goals and Measures"></a>A Survey for Federated Learning Evaluations: Goals and Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11841">http://arxiv.org/abs/2308.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Chai, Leye Wang, Liu Yang, Junxue Zhang, Kai Chen, Qiang Yang</li>
<li>for: 评估 federated learning（FL）系统的目的是寻求一种系统atic approach to assessing how well a system achieves its intended purpose.</li>
<li>methods: 评估FL的方法包括评估utilities, efficiency,和security的多种指标。</li>
<li>results: 评估FL的结果包括提出了一种开源平台FedEval，以及描述了FL评估的一些挑战和未来研究方向。<details>
<summary>Abstract</summary>
Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.
</details>
<details>
<summary>摘要</summary>
评估是一种系统的方法，用于评估系统是否实现其意图的目的。联邦学习（FL）是一种新的隐私保护机器学习方法，允许多方共同训练模型而无需分享敏感数据。然而，评估FL具有多种目标和多学科性质，如有用性、效率和安全性。在这篇简介中，我们首先评审了现有研究中采用的主要评估目标，然后探讨每个目标的评估指标。我们还介绍了FedEval，一个开源的评估平台，为FL算法提供标准化和完整的评估框架，包括其有用性、效率和安全性。最后，我们讨论了一些挑战和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-Study-on-Calibration"><a href="#A-Benchmark-Study-on-Calibration" class="headerlink" title="A Benchmark Study on Calibration"></a>A Benchmark Study on Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11838">http://arxiv.org/abs/2308.11838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/history1">https://github.com/Aryia-Behroziuan/history1</a></li>
<li>paper_authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu</li>
<li>for: This paper aims to explore calibration properties within Neural Architecture Search (NAS) and provide a comprehensive dataset for studying calibration issues.</li>
<li>methods: The paper uses the NAS search space to create a model calibration dataset, which evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks.</li>
<li>results: The study aims to answer several longstanding questions in the field, including whether model calibration can be generalized across different tasks, whether robustness can be used as a calibration measurement, and how calibration interacts with accuracy. The paper also explores the impact of bin size on calibration measurement and which architectural designs are beneficial for calibration.<details>
<summary>Abstract</summary>
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS.
</details>
<details>
<summary>摘要</summary>
深度神经网络在不同机器学习任务中越来越广泛应用，然而随着模型复杂度的增加，它们经常面临调整问题，即使提高预测精度。许多研究已努力改进调整性能，使用数据预处理、特定损失函数和训练框架等方法。然而，调整性质的研究尚未得到充分的关注。我们的研究利用神经网络搜索（NAS）搜索空间，提供了详细的模型建筑空间，以便对调整性质进行全面的探索。我们专门创建了模型调整数据集，该数据集评估了90个分割值和12个附加调整测量，在117,702个Unique神经网络内进行了广泛的 NATS-Bench 搜索空间。我们的分析旨在回答以下长期问题：(i) 模型调整是否可以泛化到不同任务？(ii) 是否可以将Robustness作为调整测量？(iii) 准确性测试是如何可靠？(iv) 后期调整方法对所有模型是否具有同等影响？(v) 调整与准确度之间是否存在关系？(vi) 分割值如何影响调整测量？(vii) 哪些建筑设计有利于调整？我们的研究填补了现有的空白，通过调整在NAS中进行exploration。我们提供了这个数据集，以便进一步研究NAS调整性质。我们知道，我们的研究是首次对调整性质进行大规模的探索，也是首次在NAS中研究调整问题。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity"><a href="#Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity" class="headerlink" title="Characterizing normal perinatal development of the human brain structural connectivity"></a>Characterizing normal perinatal development of the human brain structural connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11836">http://arxiv.org/abs/2308.11836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Lana Vasung, Camilo Calixto, Ali Gholipour, Davood Karimi<br>for: 这项研究的目的是为了研究新生儿期的脑发育中的结构连接组织的发展趋势。methods: 这项研究使用了基于空间-时间平均值的计算框架，以确定正常发育阶段的结构连接指标。results: 研究发现，在新生儿期的33-44周岁期间，脑的结构连接显示了明显的增长趋势，全球和地方效率增加，特征路径长度减少，脑叶和脑半球之间的连接强化广泛。此外，研究还发现了一些差异征在不同的连接评价方法中的一致性。这些结果有助于评价新生儿期脑发育中的正常和异常发展。<details>
<summary>Abstract</summary>
Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.
</details>
<details>
<summary>摘要</summary>
early brain development Characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Therefore, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection"><a href="#Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection" class="headerlink" title="Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection"></a>Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11834">http://arxiv.org/abs/2308.11834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tosin Ige, Christopher Kiekintveld</li>
<li>for: 本研究旨在实现和比较每种 variant of Bayesian classifier（多omial、Bernoulli、Gaussian）在网络入侵异常检测中的性能，并investigate whether there is any association between each variant assumption and their performance.</li>
<li>methods: 本研究采用了不同 variant of Bayesian classifier，并对它们的性能进行了比较。</li>
<li>results: 实验结果表明，Bernoulli variant 的准确率为 69.9%（71% 训练数据），Multinomial variant 的准确率为 31.2%（31.2% 训练数据），而 Gaussian variant 的准确率为 81.69%（82.84% 训练数据）。进一步调查发现，每种 Naive Bayes variant 的性能和准确率主要归功于它们的假设。Gaussian classifier 的好性能主要归功于它假设特征遵循正态分布，而 Multinomial classifier 的差的性能主要归功于它假设特征遵循离散和多omial分布。<details>
<summary>Abstract</summary>
Bayesian classifiers perform well when each of the features is completely independent of the other which is not always valid in real world application. The aim of this study is to implement and compare the performances of each variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on anomaly detection in network intrusion, and to investigate whether there is any association between each variant assumption and their performance. Our investigation showed that each variant of Bayesian algorithm blindly follows its assumption regardless of feature property, and that the assumption is the single most important factor that influences their accuracy. Experimental results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69% test (82.84% train). Going deeper, we investigated and found that each Naive Bayes variants performances and accuracy is largely due to each classifier assumption, Gaussian classifier performed best on anomaly detection due to its assumption that features follow normal distributions which are continuous, while multinomial classifier have a dismal performance as it simply assumes discreet and multinomial distribution.
</details>
<details>
<summary>摘要</summary>
bayesian 分类器在实际应用中表现良好，只当每个特征独立无关其他特征时。本研究的目的是实现和比较bayesian分类器（多ategorical、bernoulli和gaussian）在网络侵入异常检测中的表现，以及每种variant assumption和其表现之间的关系。我们的调查表明，bayesian算法的每种变体都会遵循自己的假设，不管特征性质如何。实验结果显示，bernoulli的准确率为69.9%测试（71%训练），多ategorical的准确率为31.2%测试（31.2%训练），而gaussian的准确率为81.69%测试（82.84%训练）。进一步调查发现，每种naive bayes variant的表现和准确率主要归结于每个分类器假设，gaussian分类器在异常检测中表现最佳，因为它假设特征遵循正态分布，而多ategorical分类器表现很差，因为它只假设离散和多ategorical分布。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test"><a href="#Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test" class="headerlink" title="Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test"></a>Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Rahimi, Tucker Balch, Manuela Veloso</li>
<li>for: 提高 GPT 模型的问答能力，使其能够回答基于新的信息源的问题。</li>
<li>methods: 使用Context embedding、构建提问、使用 GPT 模型答题。</li>
<li>results: GPT-3 模型在一个控制测试场景中，使用加利福尼亚驾驶手册作为信息源，答题成功率为 96%，而无Context情况下答题成功率仅为 82%。但模型仍然无法正确回答一些问题，表明进一步改进是需要的。<details>
<summary>Abstract</summary>
Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to 82%. However, the model still fails to answer some questions correctly even with providing library of context, highlighting room for improvement. The research also examined the impact of prompt length and context format, on the model's performance. Overall, the study provides insights into the limitations and potential improvements for GPT models in question-answering tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks"><a href="#Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks" class="headerlink" title="Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks"></a>Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11825">http://arxiv.org/abs/2308.11825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiexi1990/iccad-accel-gnn">https://github.com/xiexi1990/iccad-accel-gnn</a></li>
<li>paper_authors: Xi Xie, Hongwu Peng, Amit Hasan, Shaoyi Huang, Jiahui Zhao, Haowen Fang, Wei Zhang, Tong Geng, Omer Khan, Caiwen Ding</li>
<li>for: 提高GCNs在主流GPU上的加速，解决工作负荷不均和内存访问不规则的挑战。</li>
<li>methods: 使用轻量级度排序、块级别分区策略和共享内存地方的合并战略来提高分布式内存和计算并行性。</li>
<li>results: 对18个 benchmark图进行评估，与cuSPARSE、GNNAdvisor和graph-BLAST相比，Accel-GCN可以提高GCN的计算效率，提高多级内存效率，并且可以提高内存带宽利用率。<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.   Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memory efficiency and optimizes memory bandwidth by exploiting memory coalescing and alignment. Evaluation of Accel-GCN across 18 benchmark graphs reveals that it outperforms cuSPARSE, GNNAdvisor, and graph-BLAST by factors of 1.17 times, 1.86 times, and 2.94 times respectively. The results underscore Accel-GCN as an effective solution for enhancing GCN computational efficiency.
</details>
<details>
<summary>摘要</summary>
图像卷积网络（GCNs）在各种领域中提取隐藏信息的缺省方法，但是它们在主流GPU上的加速受到工作负载不均和内存访问不规则的挑战。为了解决这些挑战，我们提出了Accel-GCN，一种为GCNs而设计的GPU加速架构。Accel-GCN的设计包括：（i）一种轻量级学位排序阶段，用于将节点按照学位相似度分组；（ii）一种基于块的分区策略，动态调整抗冲工作负载大小，提高共享内存本地性和工作负载均衡，并比GNNAdvisor等设计减少metadata开销；（iii）一种组合战略，用于改进内存启聚和计算并行性在纵向 dense 矩阵中。通过这些原则，我们设计了GCNs中的稀疏矩阵乘法（SpMM）的适用，并使用块级分区和组合战略。这种方法可以提高性能和多级内存效率，同时利用内存启聚和对齐来提高内存带宽。对Accel-GCN在18个标准图表上进行评估，发现它在cuSPARSE、GNNAdvisor和graph-BLAST等方法上提高了1.17倍、1.86倍和2.94倍的性能。结果证明Accel-GCN是GCN计算效率的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification"><a href="#PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification" class="headerlink" title="PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification"></a>PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11822">http://arxiv.org/abs/2308.11822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaiveryuan/patchbackdoor">https://github.com/xaiveryuan/patchbackdoor</a></li>
<li>paper_authors: Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu</li>
<li>for: 防止深度学习系统中的攻击，特别是在安全关键场景中。</li>
<li>methods: 提议使用一个特制的贴图（称为backdoor patch），将其放置在摄像头前面，并将其与输入图像一起传递给模型。</li>
<li>results: 在实验中，该攻击可以在93%到99%的场景中成功，并且在实际应用中也能够实现攻击。<details>
<summary>Abstract</summary>
Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and/or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate the backdoor patch and a digital-physical transformation modeling method to enhance the feasibility of the patch in real deployments. Extensive experiments show that PatchBackdoor can be applied to common deep learning models (VGG, MobileNet, ResNet) with an attack success rate of 93% to 99% on classification tasks. Moreover, we implement PatchBackdoor in real-world scenarios and show that the attack is still threatening.
</details>
<details>
<summary>摘要</summary>
迷宫攻击是深度学习系统的主要威胁，它目的是在攻击者控制的情况下让神经网络模型发生不正常的行为。然而，大多数迷宫攻击都需要修改神经网络模型通过调询毒品数据和/或直接模型修改，这导致了一个常见而又错误的信念，即迷宫攻击可以轻松避免通过正确保护模型。在这篇论文中，我们显示了迷宫攻击可以不需要修改模型。相反的，我们提议在前置摄像头前置一个特别设计的贴纸（称为迷宫贴纸），这个贴纸会在输入图像中扮演一个正常的角色，但当输入图像包含攻击者控制的触发物时，它会制造错误的预测。我们的主要技术包括生成迷宫贴纸的有效训练方法和将贴纸模型转换为实际应用中的实际实现方法。实验结果显示，PatchBackdoor可以在常用的深度学习模型（VGG、MobileNet、ResNet）上获得93%~99%的攻击成功率，而且我们在实际应用中实现了这个攻击。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder"><a href="#Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder" class="headerlink" title="Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder"></a>Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11819">http://arxiv.org/abs/2308.11819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Xiaohan Li, Philip Yu<br>for: The paper aims to address the fairness issue in clinical data modeling, particularly in Electronic Health Records (EHRs), by proposing a novel model called Fair Longitudinal Medical Deconfounder (FLMD).methods: FLMD employs a two-stage training process, which includes capturing unobserved confounders for each encounter and combining the learned latent representation with other relevant features to make predictions. The model incorporates appropriate fairness criteria, such as counterfactual fairness, to ensure high prediction accuracy while minimizing health disparities.results: The paper demonstrates the effectiveness of FLMD through comprehensive experiments on two real-world EHR datasets. The results show that FLMD outperforms baseline methods and FLMD variants in terms of fairness and accuracy, and provides valuable insights into its capabilities across different settings.<details>
<summary>Abstract</summary>
The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial for addressing the accuracy/fairness dilemma. In the second stage, FLMD combines the learned latent representation with other relevant features to make predictions. By incorporating appropriate fairness criteria, such as counterfactual fairness, FLMD ensures that it maintains high prediction accuracy while simultaneously minimizing health disparities. We conducted comprehensive experiments on two real-world EHR datasets to demonstrate the effectiveness of FLMD. Apart from the comparison of baseline methods and FLMD variants in terms of fairness and accuracy, we assessed the performance of all models on disturbed/imbalanced and synthetic datasets to showcase the superiority of FLMD across different settings and provide valuable insights into its capabilities.
</details>
<details>
<summary>摘要</summary>
“诊断资料模型中的公平问题，尤其是在电子健康记录（EHR）上，是非常重要的，因为EHR具有复杂的潜在结构和选择偏见。在实践中，通常需要实现健康差异化和模型绩效的平衡。然而，传统方法通常会面临精度和公平之间的贸易，因为它们无法捕捉潜在的因素。为了解决这个挑战，我们提出了一个新的模型，即公平长期医疗资料去掉条件（FLMD），旨在在长期EHR模型中同时维持精度和公平。参考了去掉条件理论，FLMD使用了两阶段训练过程。在第一阶段，FLMD捕捉了每次遇到的隐藏因素，这些隐藏因素代表了背后的医疗因素，例如患者基因和生活方式。这个隐藏因素是精度和公平问题的解决方案。在第二阶段，FLMD结合了学习的潜在表示和其他相关特征来进行预测。通过将应用公平问题的标准加入训练过程，FLMD确保了高精度和同时维持健康差异化。我们在两个真实世界EHR数据集上进行了充分的实验，以证明FLMD的有效性。除了与基eline方法和FLMD的变型进行公平和精度的比较外，我们还评估了所有模型在离散/不均衡和合成数据集上的表现，以显示FLMD在不同的设定下的优势和提供有价值的问题。”
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks"><a href="#Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks" class="headerlink" title="Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks"></a>Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11818">http://arxiv.org/abs/2308.11818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Archie J. Huang, Animesh Biswas, Shaurya Agarwal</li>
<li>for: 提高交通状态估算精度，提高交通管理策略效果。</li>
<li>methods: 基于非本地LWR模型的物理学习深度学习框架。</li>
<li>results: 比基eline方法具有更高的准确率和可靠性，能够更好地估算交通状态，提高交通管理策略的效果。<details>
<summary>Abstract</summary>
This research contributes to the advancement of traffic state estimation methods by leveraging the benefits of the nonlocal LWR model within a physics-informed deep learning framework. The classical LWR model, while useful, falls short of accurately representing real-world traffic flows. The nonlocal LWR model addresses this limitation by considering the speed as a weighted mean of the downstream traffic density. In this paper, we propose a novel PIDL framework that incorporates the nonlocal LWR model. We introduce both fixed-length and variable-length kernels and develop the required mathematics. The proposed PIDL framework undergoes a comprehensive evaluation, including various convolutional kernels and look-ahead windows, using data from the NGSIM and CitySim datasets. The results demonstrate improvements over the baseline PIDL approach using the local LWR model. The findings highlight the potential of the proposed approach to enhance the accuracy and reliability of traffic state estimation, enabling more effective traffic management strategies.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "physics-informed deep learning" is translated as "物理学推动的深度学习" (wùyì xué yùn dào)* "nonlocal" is translated as "非本地" (fēn běn dì)* "LWR" is translated as "流速模型" (liú sù módel)* "speed" is translated as "速度" (zhòu du)* "density" is translated as "密度" (mì dè)* "kernel" is translated as "核函数" (fāng xiàng)* "look-ahead windows" is translated as "预测窗口" (yù jì chuāng kōng)* "baseline" is translated as "基线" (jī xiào)* "accuracy" is translated as "准确性" (zhèng qiú xìng)* "reliability" is translated as "可靠性" (kě liào xìng)* "traffic management strategies" is translated as "交通管理策略" (tiáo tòng guǎn lí zhì lüè)
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting"><a href="#Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting" class="headerlink" title="Evaluation of Deep Neural Operator Models toward Ocean Forecasting"></a>Evaluation of Deep Neural Operator Models toward Ocean Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11814">http://arxiv.org/abs/2308.11814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellery Rajagopal, Anantha N. S. Babu, Tony Ryu, Patrick J. Haley Jr., Chris Mirabito, Pierre F. J. Lermusiaux</li>
<li>for:  investigate the possible effectiveness of deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics</li>
<li>methods:  trained on a simulated two-dimensional fluid flow past a cylinder, and applied to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay</li>
<li>results:  predicted idealized periodic eddy shedding, and showed some skill in predicting features of realistic ocean surface flows, with potential for future research and applications.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究探讨了深度学习运算符模型在复现和预测古典流体流和真实海洋动力学中的可能效果。</li>
<li>methods: 使用了一个模拟的两维流体流 past a cylinder 进行训练，并应用于forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay。</li>
<li>results: 训练后的深度学习运算符模型能够预测理想化的 periodic eddy shedding，并对真实海洋表面流体中的一些特征显示了一定的能力，具有未来研究和应用的潜在价值。<details>
<summary>Abstract</summary>
Data-driven, deep-learning modeling frameworks have been recently developed for forecasting time series data. Such machine learning models may be useful in multiple domains including the atmospheric and oceanic ones, and in general, the larger fluids community. The present work investigates the possible effectiveness of such deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics. We first briefly evaluate the capabilities of such deep neural operator models when trained on a simulated two-dimensional fluid flow past a cylinder. We then investigate their application to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay, learning from high-resolution data-assimilative simulations employed for real sea experiments. We confirm that trained deep neural operator models are capable of predicting idealized periodic eddy shedding. For realistic ocean surface flows and our preliminary study, they can predict several of the features and show some skill, providing potential for future research and applications.
</details>
<details>
<summary>摘要</summary>
“数据驱动、深度学习模型框架最近已经开发出来用于时间序列预测。这些机器学习模型可能在多个领域有用，包括大气和海洋领域，以及整体更大的液体社区。本工作研究了这些深度神经算法模型在复现和预测经典液体流和现实海洋动力学的能力。我们首先简要评估了这些深度神经算法模型在模拟的二维液体流 past 筒体上的能力。然后，我们研究了它们在中大西洋盆地和马萨诸塞湾 ocean surface 流动预测方面的应用，学习从高分辨率数据吸收式 simulations 中得到的实际海洋实验。我们发现，训练过的深度神经算法模型能够预测理想化 periodic eddy shedding。对于实际的海洋表面流动和我们的初步研究，它们可以预测一些特征，并且显示一定的能力，提供未来研究和应用的潜在可能。”
</details></li>
</ul>
<hr>
<h2 id="Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings"><a href="#Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings" class="headerlink" title="Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings"></a>Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11804">http://arxiv.org/abs/2308.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Bagdasaryan, Vitaly Shmatikov<br>for: 这篇论文主要针对的是多modal编码器的安全性问题，具体来说是对于图像、声音、文本、视频等多种模式的映射。methods: 该论文使用了多modal编码器，将不同模式的输入映射到单一的嵌入空间中，以实现模式之间的对应。results: 该论文发现，使用多modal编码器可能会导致”对抗幻觉”攻击，即对于任意输入模式，恶意攻击者可以将其嵌入空间中的映射与另一个模式的映射很近，从而实现模式之间的对应。这种攻击不依赖于特定任务的知识，因此可以影响多种下游任务。使用ImageBind embeddings，研究者示出了这种攻击的具体实现方式，并证明了它们可以在图像生成、文本生成和零参数分类等多种任务中引起混乱。<details>
<summary>Abstract</summary>
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.   Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
</details>
<details>
<summary>摘要</summary>
多modalEncoder将图像、声音、文本、视频等多种模式映射到单一的嵌入空间中，使模式之间的表示相似（例如，将一个狗图片与一个喊喊的声音相关联）。我们表明，多modal嵌入可能受到“攻击ILLUSION”的威胁。给定任意模式的输入，恶意者可以对其进行扰动，使其嵌入 Close to恶意者选择的另一种模式的输入。这些ILLUSION依据嵌入空间的 proximity，并不受下游任务的限制。使用ImageBind嵌入，我们示例了如何通过不知道特定下游任务的知识，使用恶意对齐的输入， Mislead图像生成、文本生成和零shot分类。
</details></li>
</ul>
<hr>
<h2 id="Variational-Density-Propagation-Continual-Learning"><a href="#Variational-Density-Propagation-Continual-Learning" class="headerlink" title="Variational Density Propagation Continual Learning"></a>Variational Density Propagation Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11801">http://arxiv.org/abs/2308.11801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Angelini, Nidhal Bouaynaya, Ghulam Rasool<br>for: 这篇论文的目的是提出一个框架来适应资料分布迁移，并使用 uncertainty quantification from Bayesian Inference 来减少严重遗传。methods: 这篇论文使用了 Continual Learning 的方法，并开发了一个基于 Bayesian Inference 的方法，以便获得更好的 uncertainty quantification。这个方法不需要 Monte Carlo 抽样，而是使用关键矩阵来实现预测分布的近似。results: 这篇论文的结果显示，使用这个方法可以对抗严重遗传，并且可以在多个 benchmark 数据集上进行适应资料分布迁移。此外，这个方法还可以在多个不同的任务序列长度下进行任务增量学习。总之，这篇论文的结果显示，这个方法可以实现一个具有最小化模型复杂度的 Continual Learning 框架。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) deployed to the real world are regularly subject to out-of-distribution (OoD) data, various types of noise, and shifting conceptual objectives. This paper proposes a framework for adapting to data distribution drift modeled by benchmark Continual Learning datasets. We develop and evaluate a method of Continual Learning that leverages uncertainty quantification from Bayesian Inference to mitigate catastrophic forgetting. We expand on previous approaches by removing the need for Monte Carlo sampling of the model weights to sample the predictive distribution. We optimize a closed-form Evidence Lower Bound (ELBO) objective approximating the predictive distribution by propagating the first two moments of a distribution, i.e. mean and covariance, through all network layers. Catastrophic forgetting is mitigated by using the closed-form ELBO to approximate the Minimum Description Length (MDL) Principle, inherently penalizing changes in the model likelihood by minimizing the KL Divergence between the variational posterior for the current task and the previous task's variational posterior acting as the prior. Leveraging the approximation of the MDL principle, we aim to initially learn a sparse variational posterior and then minimize additional model complexity learned for subsequent tasks. Our approach is evaluated for the task incremental learning scenario using density propagated versions of fully-connected and convolutional neural networks across multiple sequential benchmark datasets with varying task sequence lengths. Ultimately, this procedure produces a minimally complex network over a series of tasks mitigating catastrophic forgetting.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在实际世界中部署时常遇到不同类型的噪音、扰动目标和数据分布逐渐变化的问题。这篇论文提出了一种基于Continual Learning benchmarck datasets的数据分布演变适应框架。我们开发了一种基于uncertainty量化的Continual Learning方法，利用束缚推理来减少忘却现象。我们在之前的方法上进一步改进，即不需要蒙те Carlo样本来采样模型权重，以便采样预测分布。我们优化了一个关闭式证据下界（ELBO）目标，通过对所有神经网络层传递mean和covariance两个分布的首两个矩阵来近似预测分布。通过使用关闭式ELBO目标，我们可以近似地使用最小描述长度（MDL）原理，从而自然地减少模型概率变化的KL散度，以避免忘却现象。我们利用近似MDL原理，首先学习一个稀疏的变量 posterior，然后进一步减少后续任务学习的模型复杂度。我们的方法在完全链接神经网络和卷积神经网络上进行了多个顺序 benchmark 数据集上进行了评估，并最终生成了一个对多个任务具有最小复杂度的神经网络，以避免忘却现象。
</details></li>
</ul>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
<li>for: 防止声音掩伤和音频深伪检测</li>
<li>methods: 使用复лекс值神经网络处理CQT频域表示的输入音频，保留相位信息，并允许使用可解释AI方法</li>
<li>results: 比前方法在”In-the-Wild”反伪 dataset上表现出色，并可以通过可解释AI方法解释结果，精度研究表明模型已经学习使用相位信息检测声音掩伤<details>
<summary>Abstract</summary>
Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the "In-the-Wild" anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.
</details>
<details>
<summary>摘要</summary>
当前的反假识别和音频深度质模型使用 either 大小图表（如CQT或Melspectrograms）或Raw audio 经过卷积或填充层处理。两种方法都有缺点：大小图表抛弃相位信息，影响音频自然性，而Raw-feature-based 模型不能使用传统的可解释 AI 方法。这篇论文提议一种新的方法，通过使用复杂值神经网络处理输入音频的复杂值CQT频域表示。这种方法保留相位信息，并允许使用可解释 AI 方法。结果表明这种方法在 "In-the-Wild" 反假识别数据集上表现出色，并且可以通过可解释 AI 方法解释结果。剥离学研究表明，模型已经学会使用相位信息检测声音假设。
</details></li>
</ul>
<hr>
<h2 id="Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics"><a href="#Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics" class="headerlink" title="Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics"></a>Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11792">http://arxiv.org/abs/2308.11792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Scheinert, Philipp Wiesner, Thorsten Wittkopp, Lauritz Thamsen, Jonathan Will, Odej Kao</li>
<li>for: 这篇论文的目的是提出一种更有效的资源配置评估方法，以便更好地选择适合的机器和集群大小，并且能够同时优化多个目标。</li>
<li>methods: 这篇论文使用了一种名为“Karasu”的方法，它通过聚合多个用户的执行时间资讯，将其转换为轻量级的性能模型，以便更好地搜索适合的资源配置。</li>
<li>results: 根据论文的评估结果，Karasu方法能够与现有的方法相比，在性能、搜索时间和成本等方面获得显著提升，甚至在仅具有部分相似特征的比较轻量级 Profiling 运行中也能够获得提升。<details>
<summary>Abstract</summary>
Selecting the right resources for big data analytics jobs is hard because of the wide variety of configuration options like machine type and cluster size. As poor choices can have a significant impact on resource efficiency, cost, and energy usage, automated approaches are gaining popularity. Most existing methods rely on profiling recurring workloads to find near-optimal solutions over time. Due to the cold-start problem, this often leads to lengthy and costly profiling phases. However, big data analytics jobs across users can share many common properties: they often operate on similar infrastructure, using similar algorithms implemented in similar frameworks. The potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored.   We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information of collaborators and combines them into an ensemble method to exploit inherent knowledge of the configuration search space. Moreover, Karasu allows the optimization of multiple objectives simultaneously. Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job.
</details>
<details>
<summary>摘要</summary>
We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information of collaborators and combines them into an ensemble method to exploit inherent knowledge of the configuration search space. Moreover, Karasu allows the optimization of multiple objectives simultaneously. Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job. translate to Simplified Chinese:选择大数据分析任务的合适资源很Difficult，因为配置选项的多样性，如机器类型和集群大小。 incorrect choices can have a significant impact on resource efficiency, cost, and energy usage，所以自动化方法 becoming popular。most existing methods rely on profiling recurring workloads to find near-optimal solutions over time，but this often leads to lengthy and costly profiling phases due to the cold-start problem。 however，big data analytics jobs across users can share many common properties，such as operating on similar infrastructure，using similar algorithms implemented in similar frameworks。the potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored。我们提出了Karasu，一种更有效的资源配置 Profiling 方法，它推广用户工作在相似的基础设施，框架，算法或数据集之间的数据共享。Karasu 使用协作者的各种运行时信息 trains 轻量级性能模型，并将其组合成ensemble方法，以利用配置搜索空间的内在知识。此外，Karasu 允许同时优化多个目标。我们的评估基于公共云环境中的多种任务执行性能数据。我们表明，Karasu 能够 Significantly 提高现有方法的性能，搜索时间和成本，即使有 Only few comparable profiling runs are available that share only partial common characteristics with the target job。
</details></li>
</ul>
<hr>
<h2 id="HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials"><a href="#HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials" class="headerlink" title="HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials"></a>HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11787">http://arxiv.org/abs/2308.11787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoulatif Cisse, Xenophon Evangelopoulos, Sam Carruthers, Vladimir V. Gusev, Andrew I. Cooper</li>
<li>for: 本研究旨在使用人类专家知识来加速bayesian优化的搜索过程，以解决科学问题中的多变量问题。</li>
<li>methods: 本研究使用了 bayesian 优化方法，并利用专家理论来提供更好的初始样本，以优化搜索过程。</li>
<li>results: 研究结果表明，使用专家理论可以减少不重要的样本，并且可以更好地覆盖搜索空间，从而提高搜索效率。<details>
<summary>Abstract</summary>
Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling. This process continues in a global versus local search fashion, organized in a bilevel optimization framework. We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.
</details>
<details>
<summary>摘要</summary>
瑜珈和自动化技术可以为解决复杂多变量科学问题提供巨大的加速，但搜索空间可能会变得极其庞大。bayesian优化（BO）已经成为一种流行的高效搜索引擎，特别是在没有知道目标函数/属性的 analytic 表达的情况下。在这些任务中，我们利用专家人类知识来导向 bayesian 搜索更快速地访问有潜力的化学空间。先前的方法使用了基于现有实验测量的下面分布，这是对新、未探索的科学任务而言不可能的。此外，这些分布不能捕捉复杂的假设。我们提出的方法，即 HypBO，使用专家人类假设来生成改进的样本。不可能的样本会被排除，而有潜力的样本将被用来补充模型数据，从而实现更加有知识的搜索。这个过程会在全球与本地的搜索模式下进行，组织成两级优化框架。我们验证了我们的方法在一系列的synthetic函数上的性能，并在一个真实的化学设计任务中展示了它的实用性。在这个任务中，使用专家假设可以大幅度加速搜索过程。
</details></li>
</ul>
<hr>
<h2 id="Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers"><a href="#Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers" class="headerlink" title="Coarse-to-Fine Multi-Scene Pose Regression with Transformers"></a>Coarse-to-Fine Multi-Scene Pose Regression with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11783">http://arxiv.org/abs/2308.11783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yolish/c2f-ms-transformer">https://github.com/yolish/c2f-ms-transformer</a></li>
<li>paper_authors: Yoli Shavit, Ron Ferens, Yosi Keller</li>
<li>for: 这个论文目的是学习多场景绝对摄像头pose regression。</li>
<li>methods: 这个论文使用了变换器学习模型，包括激活Map的汇集和特征转换，以及多个场景编码。</li>
<li>results: 该方法在常见的室内和室外数据集上进行评估，并显示出过单场景和多场景绝对摄像头pose regressor的性能。<details>
<summary>Abstract</summary>
Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly benchmark indoor and outdoor datasets and has been shown to exceed both multi-scene and state-of-the-art single-scene absolute pose regressors.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>>绝对摄像头姿态回归器可以根据捕捉的图像估计摄像头的位置和方向。通常，一个卷积减少器（Convolutional Backbone）和多层感知器（Multi-layer Perceptron，MLP）头被训练使用图像和姿态标签来嵌入单个参考场景。在最近的研究中，这种方案被扩展以学习多个场景，通过取代MLP头而使用完全连接层。在这项工作中，我们提议使用变换器来学习多个场景绝对摄像头姿态回归，其中混合encoder和decoder被用来聚合活动地图和场景编码，并将其转换成姿态预测。这使得我们的模型能够专注于通用的特征，同时并行地嵌入多个场景。我们在前一项MS-Transformer方法（\cite{shavit2021learning}）的基础上增加了混合分类回归架构，以提高地点准确性。我们的方法在常见的室内和室外数据集上进行评估，并已经超过了多个场景和当前最佳单个场景绝对摄像头姿态回归器。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables"><a href="#Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables" class="headerlink" title="Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables"></a>Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11781">http://arxiv.org/abs/2308.11781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirban Mukherjee, Hannah H. Chang</li>
<li>for: 该论文旨在扩展量化模型，以便包含 качеitative数据进行 causal 估计。</li>
<li>methods: 该论文使用函数分析创建了一个更加灵活和弹性的框架，将观察到的类别嵌入到了一个Baire空间中，并通过一个连续线性映射将这些类别映射到一个 reproduce kernel Hilbert space（RKHS）中。</li>
<li>results: 该论文通过实验证明了其超越传统方法的性能，特别在qualitative信息复杂和细腻的场景下。<details>
<summary>Abstract</summary>
We propose a novel framework for incorporating qualitative data into quantitative models for causal estimation. Previous methods use categorical variables derived from qualitative data to build quantitative models. However, this approach can lead to data-sparse categories and yield inconsistent (asymptotically biased) and imprecise (finite sample biased) estimates if the qualitative information is dynamic and intricate. We use functional analysis to create a more nuanced and flexible framework. We embed the observed categories into a latent Baire space and introduce a continuous linear map -- a Hilbert space embedding -- from the Baire space of categories to a Reproducing Kernel Hilbert Space (RKHS) of representation functions. Through the Riesz representation theorem, we establish that the canonical treatment of categorical variables in causal models can be transformed into an identified structure in the RKHS. Transfer learning acts as a catalyst to streamline estimation -- embeddings from traditional models are paired with the kernel trick to form the Hilbert space embedding. We validate our model through comprehensive simulation evidence and demonstrate its relevance in a real-world study that contrasts theoretical predictions from economics and psychology in an e-commerce marketplace. The results confirm the superior performance of our model, particularly in scenarios where qualitative information is nuanced and complex.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的框架，用于将质量数据 incorporated 到量化模型中进行 causal 估计。先前的方法使用来自质量数据的分类变量来构建量化模型，但这种方法可能会导致数据缺乏和偏向（即不稳定和偏差）的估计结果，特别是当质量信息是动态且复杂时。我们使用函数分析来创建一个更加灵活和细腻的框架。我们将观察到的分类 embedding 到一个 latent Baire 空间中，并引入一个连续线性映射——一个 Reproducing Kernel Hilbert Space (RKHS) 的表示函数空间中的映射。通过 Riesz 表示定理，我们证明了在 causal 模型中对分类变量的 canonical 处理可以转化为一个唯一标识结构在 RKHS 中。trasnfer learning 作为一种 catalyst，我们可以通过对传统模型的 embeddings 与kernel trick 结合来形成一个 Hilbert 空间 embedding。我们通过了广泛的 simulations 证明和一个实际的例子，证明了我们的模型在复杂和细腻的质量信息场景下表现更加优秀。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning"><a href="#Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning" class="headerlink" title="Few-shot Anomaly Detection in Text with Deviation Learning"></a>Few-shot Anomaly Detection in Text with Deviation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11780">http://arxiv.org/abs/2308.11780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anindya Sundar Das, Aravind Ajay, Sriparna Saha, Monowar Bhuyan<br>for: 这种论文的目的是提出一种基于深度几个示例学习的方法，用于检测文本中的异常Example。methods: 该方法使用了几个示例学习的技术，包括异常示例的有限使用、直接学习异常分数和多头自我注意力层。results: 对多个 benchmark 数据集进行了广泛的实验，并达到了新的状态态的性能水平。<details>
<summary>Abstract</summary>
Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably deviate from the reference score in the upper tail of the prior. Additionally, our model is optimized to learn the distinct behavior of anomalies by utilizing a multi-head self-attention layer and multiple instance learning approaches. Comprehensive experiments on several benchmark datasets demonstrate that our proposed approach attains a new level of state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
现有的异常检测方法大多数都是基于无标注数据构建模型。这些模型假设没有异常示例，这限制了它们使用异常示例的先验知识，从而导致它们在许多实际应用中表现不佳。此外，这些模型更关注学习特征嵌入than直接优化异常分数，这可能会导致异常分数评估不准确和数据学习过程中的不efficient使用。在本文中，我们介绍了FATE，一种深度几个shot学习基于框架，它利用有限异常示例和直接学习异常分数的端到端方法。在这种方法中，正常示例的异常分数被调整，以便与先前分布中的参考分数相似。相反，异常示例被迫有异常分数，这些分数与参考分数在上 tail 的异常分布中异常大。此外，我们的模型利用多头自注意力层和多个实例学习方法来学习异常的特殊行为。我们在多个标准 benchmark 数据集上进行了广泛的实验，结果表明，我们的提议方法可以达到新的顶峰性能水平。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Hessian-Alignment-for-Domain-Generalization"><a href="#Understanding-Hessian-Alignment-for-Domain-Generalization" class="headerlink" title="Understanding Hessian Alignment for Domain Generalization"></a>Understanding Hessian Alignment for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11778">http://arxiv.org/abs/2308.11778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/federated-learning">https://github.com/huawei-noah/federated-learning</a></li>
<li>paper_authors: Sobhan Hemati, Guojun Zhang, Amir Estiri, Xi Chen<br>for: 这 paper 是关于 out-of-distribution (OOD) 泛化的研究，旨在提高深度学习模型在各种实际应用场景中的 OOD 泛化能力。methods: 这 paper 使用了 gradient-based 正则化技术来提高 OOD 泛化能力，并分析了 Hessian 和 gradient 在领域泛化中的角色。results: 这 paper 的结果表明，将分类器的头 Hessian 矩阵和梯度进行对齐可以提高 OOD 泛化能力，并且提出了两种简单 yet effective 的方法来实现对齐。这些方法在不同的 OOD 场景中都显示了出色的性能。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis explains the success of many regularizers like CORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier's head Hessian and/or gradient. Finally, we propose two simple yet effective methods to match the classifier's head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and without directly calculating Hessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transferability, severe correlation shift, label shift and diversity shift. Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.
</details>
<details>
<summary>摘要</summary>
外部分布（OOD）泛化是深度学习模型在实际应用场景中的重要能力之一，包括医疗和自动驾驶等。最近，不同的技术被提出来提高OOD泛化。其中，梯度基本的正则化方法表现出色，并且我们对域泛化中梯度和梯度对的角色的理解仍然受限。为了解决这个缺陷，我们通过最近的OOD理论来分析域泛化中梯度和梯度对的角色。我们 teorically 表明，在目标域与源域之间的传输度量上，梯度和梯度对的spectral norm是Upper bound。此外，我们还分析了梯度和梯度对相互对齐的所有特征。我们的分析解释了许多正则化器，如CORAL、IRM、V-REx、Fish、IGA和Fishr的成功，他们都在域泛化中对梯度和梯度对进行了正则化。最后，我们提出了两种简单 yet efficient的方法来匹配梯度和梯度对，基于梯度和梯度对的乘积（HGP）和欧几里得（Hutchinson）的方法，而不需直接计算梯度。我们 validate了我们提出的方法的OOD泛化能力在不同的场景中，包括传输性、严重相关转移、标签转移和多样性转移。我们的结果表明，梯度对齐方法在不同的OOD测试上表现出色。代码可以在 <https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment> 中找到。
</details></li>
</ul>
<hr>
<h2 id="3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network"><a href="#3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network" class="headerlink" title="3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network"></a>3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11771">http://arxiv.org/abs/2308.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao</li>
<li>for: 这 paper 是为了提出一种 sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) 模型，用于事件基于眼动跟踪，这种技术将被应用于下一代智能眼镜等设备中。</li>
<li>methods: 这 paper 使用了 retina-inspired event cameras，即具有低延迟响应和稀疏输出事件流的摄像头。 authors 还使用了 delta-encoded recurrent path，以提高活化稀疏性，从而降低计算量。</li>
<li>results: 这 paper 的 CB-ConvLSTM 架构可以高效地提取事件流中的 spatial-temporal 特征，用于眼动跟踪。 compared to conventional CNN 结构，CB-ConvLSTM 可以减少计算量约 4.7倍，不会失去准确性。 这种增强的效率使得其适用于实时眼动跟踪。<details>
<summary>Abstract</summary>
This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{https://github.com/qinche106/cb-convlstm-eyetracking}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data"><a href="#Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data" class="headerlink" title="Patient Clustering via Integrated Profiling of Clinical and Digital Data"></a>Patient Clustering via Integrated Profiling of Clinical and Digital Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11748">http://arxiv.org/abs/2308.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Choi, Andy Xiang, Ozgur Ozturk, Deep Shrestha, Barry Drake, Hamid Haidarian, Faizan Javed, Haesun Park</li>
<li>for: 这篇论文是为了开发一个基于patient profiling的健康照顾 clustering模型，以提高健康照顾中的患者分组和推荐能力。</li>
<li>methods: 这个模型使用一种基于受限制的低矩降降推导法，利用患者的临床数据和数位互动数据（包括搜寻和浏览），创建患者 profil。这 leads to nonnegative embedding vectors, which serve as a low-dimensional representation of the patients.</li>
<li>results: 评估过real-world patient data from a healthcare web portal， compared to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.<details>
<summary>Abstract</summary>
We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients' clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的基于 Profile 的患者划分模型，适用于医疗数据。我们的模型利用基于受限制的低级数据减少法，使用患者的临床数据和数字互动数据（包括浏览和搜索）构建患者profile。这种方法生成了非负嵌入 вектор，用于表示患者。我们的模型在使用真实世界患者数据from a healthcare web portal进行评估，并对划分和推荐能力进行全面评估。与其他基线相比，我们的方法在划分准确性和推荐准确性方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape"><a href="#Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape" class="headerlink" title="Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape"></a>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11737">http://arxiv.org/abs/2308.11737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</li>
<li>for: 研究动物3D姿势和形状估计，以解释动物行为，并可能帮助多个下游应用，如野生动物保育。</li>
<li>methods: 我们提出了 Animal3D 资料集，包括 3379 幅照片和 40 种哺乳类动物的高品质26个关键点标注。</li>
<li>results: 我们在 Animal3D 资料集上进行了代表性的形状和姿势估计模型评估，包括 supervised 学习、synthetic to real transfer 和 fine-tuning human pose 和形状估计模型。我们的实验结果显示，预测动物这些种类中的3D形状和姿势仍然是一个非常具有挑战性的任务，尽管人类姿势估计方法有了重要的进步。<details>
<summary>Abstract</summary>
Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estimation models. Our experimental results demonstrate that predicting the 3D shape and pose of animals across species remains a very challenging task, despite significant advances in human pose estimation. Our results further demonstrate that synthetic pre-training is a viable strategy to boost the model performance. Overall, Animal3D opens new directions for facilitating future research in animal 3D pose and shape estimation, and is publicly available.
</details>
<details>
<summary>摘要</summary>
正确估算动物3D姿态和形状是研究动物行为的重要步骤，可能对野生动物保护和其他下游应用具有巨大的应用前景。然而，这个领域的研究受到缺乏完整和多样化的3D姿态和形状标注数据的限制。在这篇论文中，我们提出了动物3D（Animal3D），第一个包括40种哺乳动物的完整数据集，以及高品质的26个关键点标注。这些标注都是透过多阶段的手动标注和确认程序来确保最高品质的结果。基于动物3D数据集，我们在：（1）仅使用动物3D数据进行监督学习，（2）从生成的 sintetic 图像进行转换到真实图像，以及（3）调整人体姿态和形状估算模型的表现进行比较。我们的实验结果显示，预测动物这些种类的3D姿态和形状仍然是一个非常困难的任务，尽管人体姿态估算领域已经取得了重大进步。我们的结果还显示，从 sintetic 预训成功地增强模型性能。总的来说，动物3D开启了新的研究方向，并且公开 disponibile。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering"><a href="#Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering" class="headerlink" title="Knowledge Graph Prompting for Multi-Document Question Answering"></a>Knowledge Graph Prompting for Multi-Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr</li>
<li>for: 这个研究旨在提高大语言模型（LLM）在多文档问答（MD-QA）中的表现，并 explore the “pre-train, prompt, predict”  paradigm in MD-QA.</li>
<li>methods: 这个研究提出了一种知识图提示（KGP）方法，包括一个知识图建构模块和一个知识图游走模块。知识图建构模块使用多个文档中的节点和边来表示文档之间的semantic和lexical相似性，而知识图游走模块使用LM响应来导航知识图，并收集支持答案的文档段落。</li>
<li>results: 实验结果表明，KGP方法可以提高LLM在MD-QA中的表现，并且可以减少检索时间。这种方法的实现可以在<a target="_blank" rel="noopener" href="https://github.com/YuWVandy/KG-LLM-MDQA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YuWVandy/KG-LLM-MDQA中找到。</a><details>
<summary>Abstract</summary>
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at https://github.com/YuWVandy/KG-LLM-MDQA.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的“预训练、提示、预测”模式在开放预测问答（OD-QA）中实现了很大的成功。然而，很少的研究探讨这种模式在多文档问答（MD-QA）中的应用， MD-QA 是需要对不同文档内容和结构的理解，以便更好地回答问题。为了填补这一重要的漏洞，我们提出了知识图 prompting（KGP）方法，用于在 LLM 中提示 MD-QA，该方法包括知识图构建模块和知识图游走模块。For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations.For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality.Exhaustive experiments demonstrate the effectiveness of KGP for MD-QA, indicating the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is available at <https://github.com/YuWVandy/KG-LLM-MDQA>.
</details></li>
</ul>
<hr>
<h2 id="When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making"><a href="#When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making" class="headerlink" title="When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making"></a>When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11721">http://arxiv.org/abs/2308.11721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kpdonahue/benefits_harms_joint_decision_making">https://github.com/kpdonahue/benefits_harms_joint_decision_making</a></li>
<li>paper_authors: Kate Donahue, Kostas Kollias, Sreenivas Gollapudi</li>
<li>for: 这种研究是为了优化人机算联合表现的最佳方式。</li>
<li>methods: 本研究使用了一种特定的人机算合作方式，其中算法有一组ITEMS，并将其中的一个subsetSize&#x3D;k个Item展示给人，人将选择最终的Item。这种方式可以应用于内容推荐、路径规划等任务。因为人和算法都有不准确的信息，因此关键问题是：哪个值得最大化最终选择最佳Item的概率？</li>
<li>results: 研究发现，在某些噪声模型下，最佳的$k$值在[2, n-1]之间，即在人机算合作下有着绝对的优势。然而，当人被anchor在算法提供的排序顺序时，联合系统的表现一定是差。此外，研究还发现在人机准确性水平不同时，存在一些情况下，一个更准确的代理会受益于与一个 menos准确的代理合作。<details>
<summary>Abstract</summary>
Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is reversed when the human is anchored on the algorithm's presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithm's accuracy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:历史上，许多机器学习研究都集中在算法性能上，但最近更多的关注集中在人机合作性能。在这种人机合作中，算法可以访问一个集合中的 $n$ 个项目，并将其中的一个 subset 大小为 $k$ 项显示给人类，人类将选择最终的项目。这种场景可以模型内容推荐、路径规划或任何类型的标签任务。由于人类和算法都有不准确、噪声的信息关于真实的项目顺序，关键问题是：哪个值的 $k$ 最大化人类最终选择的最佳项目的概率？ For $k=1$, 性能是由算法 acting alone 优化的，而 For $k=n$ 是由人类 acting alone 优化的。意外地，我们发现在多种噪声模型下，最佳的 $k $ 是 $[2, n-1]$ 的Interval -  то есть，在人类和算法都有等准确级别时，存在着协同的益处，即使人类和算法的准确率都是等于的。我们在 Mallows 模型和 Random Utilities 模型中证明这一结论，并通过实验证明这一结论。然而，我们发现在人类被算法的显示顺序固定时，人机合作系统总是有固定性下降的性能问题。我们扩展这些结论到人类和算法准确级别不同的情况下，并证明在某些情况下，更准确的代理人会受益于和更准确的算法合作。然而，这些情况是人类和算法准确级别之间的偏好的。
</details></li>
</ul>
<hr>
<h2 id="Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection"><a href="#Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection" class="headerlink" title="Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection"></a>Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12885">http://arxiv.org/abs/2308.12885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oana Inel, Tim Draws, Lora Aroyo</li>
<li>for: 本研究旨在提高人工智能（AI）数据收集过程中的质量和可靠性，以便提高AI模型的公平性和可靠性。</li>
<li>methods: 本研究提出了一种负责任AI（RAI）方法，用于系统地对数据收集过程中的因素进行深入分析，以便评估数据的内部可靠性和外部稳定性。</li>
<li>results: 研究人员对9个现有数据集和注释任务进行了验证，并在四种内容模式上进行了检验。结果表明，RAI方法可以帮助评估数据的稳定性和可靠性，并且可以处理公平和责任的方面在数据收集中的问题。<details>
<summary>Abstract</summary>
The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models' robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized and measured through a systematic set of appropriate metrics. In this paper, we propose a Responsible AI (RAI) methodology designed to guide the data collection with a set of metrics for an iterative in-depth analysis of the factors influencing the quality and reliability} of the generated data. We propose a granular set of measurements to inform on the internal reliability of a dataset and its external stability over time. We validate our approach across nine existing datasets and annotation tasks and four content modalities. This approach impacts the assessment of data robustness used for AI applied in the real world, where diversity of users and content is eminent. Furthermore, it deals with fairness and accountability aspects in data collection by providing systematic and transparent quality analysis for data collections.
</details>
<details>
<summary>摘要</summary>
machine learning 技术在我们日常生活和高度关键领域的快速进入需要透明度和检查其公平和可靠性。为了评估机器学习模型的可靠性，研究通常集中在部署之前的庞大数据集上，例如创建和维护这些数据集的文档，以便理解它们的起源、开发过程和伦理考虑。然而，AI数据收集仍然是一项一次性的做法，而且经常 reuse datasets 用于不同的问题或应用。此外，数据集的标注可能不具有时间的普适性，包含歧义或错误的标注，或者无法泛化到问题或领域。 latest research 表明这些做法可能会导致不公正、偏见或不准确的结果。我们认为AI数据收集应该进行负责任的方式，即在数据收集过程中评估和测量数据的质量，使用一套系统的精细度度量。在这篇论文中，我们提出了一种负责任AI（RAI）方法，用于指导数据收集，并提供了一系列适用于不同数据集和标注任务的度量。我们验证了我们的方法在九个现有数据集和标注任务中，以及四种内容模式中。这种方法对实际应用中的AI数据Robustness进行评估，并处理了公平和责任方面的问题。
</details></li>
</ul>
<hr>
<h2 id="SuperCalo-Calorimeter-shower-super-resolution"><a href="#SuperCalo-Calorimeter-shower-super-resolution" class="headerlink" title="SuperCalo: Calorimeter shower super-resolution"></a>SuperCalo: Calorimeter shower super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11700">http://arxiv.org/abs/2308.11700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ian-pang/supercalo">https://github.com/ian-pang/supercalo</a></li>
<li>paper_authors: Ian Pang, John Andrew Raine, David Shih</li>
<li>for:  overcome the challenge of calorimeter shower simulation in the Large Hadron Collider computational pipeline</li>
<li>methods:  employ deep-generative surrogate models, specifically a flow-based super-resolution model called SuperCalo</li>
<li>results:  high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers, reducing computational cost, memory requirements, and generation time<details>
<summary>Abstract</summary>
Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
</details>
<details>
<summary>摘要</summary>
喷泉计数器模拟是大型夸克粒子加速器计算管道中的主要瓶颈。近年来，有很多努力使用深度生成器模型来突破这个挑战。然而，许多最佳性能的模型培训和生成时间不能扩展到高维喷泉计数器。在这项工作中，我们介绍SuperCalo，一种流基的超分辨模型，并证明了高维细腔喷泉可以快速升sample自粗腔喷泉。这种新的方法可以减少计算成本、内存需求和生成时间相关于快速喷泉计数器模型。此外，我们表明了升sample后的喷泉具有高度的变化度。这意味着可以从少量粗腔喷泉中生成大量高维喷泉，具有高准确性，从而再次减少生成时间。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sumankrsh/Sentiment-Analysis.ipynb">https://github.com/sumankrsh/Sentiment-Analysis.ipynb</a></li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
<li>for: 本研究旨在解决语言模型评估中的效率问题，提出了一种智能减少语言模型评估计算成本的方法，以减少计算成本而不影响可靠性。</li>
<li>methods: 本研究使用了HELMbenchmark作为测试 caso，研究了不同的benchmark设计选择对计算vs可靠性的贸易OFF。提出了一种新的度量指标DIoR来评估决策对可靠性的影响。</li>
<li>results: 研究发现，现有的领导者在HELMbenchmark可以通过移除一些低排名的模型来改变排名，而且只需很少的例子即可获得正确的排名。同时，不同的HELM场景选择会导致排名差异很大。根据研究结果，提出了一些具体的建议，以减少计算成本并且保持可靠性，可以实现计算成本减少x100或更多。<details>
<summary>Abstract</summary>
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.
</details>
<details>
<summary>摘要</summary>
LM模型的多样化性带来了一新类的评价指标，这些指标涵盖了各种能力的广泛评估。然而，这些评价努力的效率方面几乎没有在文献中得到了讨论。在这项工作中，我们提出了一个问题，即如何智能减少LM评价的计算成本，无需妥协可靠性。使用HELM benchmark作为测试 caso，我们研究了不同的评价指标设计选择对计算与可靠性之间的负面影响。我们提出了一个新的度量器，即决策影响可靠性（DIoR），用于评估这些决策的可靠性。我们发现，例如，现有领先者在HELM上可能会改变，只需要从benchmark中移除一个低排名的模型即可。同时，我们发现一些不同的HELM场景可以导致评价排名差异极大。基于我们的发现，我们提出了一些具体的建议，包括更有效的评价设计和使用实践，可以实现计算成本减少100倍或更多，而且减少的成本幅度与可靠性损失之间的关系也可以得到精细控制。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Multi-Resolution-Communications"><a href="#Semantic-Multi-Resolution-Communications" class="headerlink" title="Semantic Multi-Resolution Communications"></a>Semantic Multi-Resolution Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11604">http://arxiv.org/abs/2308.11604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus</li>
<li>for: 这种深度学习基于JSCC的框架是为了提高数据重建的性能，特别是在finite block-length数据下，SSCC表现下降。此外，SSCC还无法在多用户和&#x2F;或多分辨率下进行数据重建，因为它只是为worst channel和&#x2F;或最高质量数据进行优化。</li>
<li>methods: 我们提出了一种基于MTL的深度学习多分辨率JSCC框架，通过层次结构来编码数据，并通过当前和过去层编码数据来进行解码。此外，这种框架还可以应用于semantic通信，其目标是保留特定semantic attribute。</li>
<li>results: 我们在MNIST和CIFAR10 dataset上进行实验，并证明了我们的提出的方法可以在不同分辨率下重建数据，并且可以在 successive layers中提高semantic classifier的准确性。这种能力特别有用于优先保留数据集中的关键semantic attribute。<details>
<summary>Abstract</summary>
Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC). This superiority arises from the suboptimality of SSCC when dealing with finite block-length data. Moreover, SSCC falls short in reconstructing data in a multi-user and/or multi-resolution fashion, as it only tries to satisfy the worst channel and/or the highest quality data. To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL). This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data. Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process. These semantic features could be crucial elements such as class labels, essential for classification tasks, or other key attributes that require preservation. Within this framework, each level of encoded data can be carefully designed to retain specific data semantics. As a result, the precision of a semantic classifier can be progressively enhanced across successive layers, emphasizing the preservation of targeted semantics throughout the encoding and decoding stages. We conduct experiments on MNIST and CIFAR10 dataset. The experiment with both datasets illustrates that our proposed method is capable of surpassing the SSCC method in reconstructing data with different resolutions, enabling the extraction of semantic features with heightened confidence in successive layers. This capability is particularly advantageous for prioritizing and preserving more crucial semantic features within the datasets.
</details>
<details>
<summary>摘要</summary>
深度学习基于联合源通道编码（JSCC）已经实现了数据重建的显著进步，比单独源通道编码（SSCC）更好。这种超越来自于finite block length数据下SSCC的优化不足。此外，SSCC无法在多用户和/或多分辨率上重建数据，因为它只是尝试满足最差通道和/或最高质量数据。为了超越这些限制，我们提议了一种基于多任务学习（MTL）的深度学习多分辨率JSCC框架。这个提议的框架通过层次结构来编码数据，并通过同当前和过去层编码数据来有效地解码。此外，这个框架具有潜在的semantic communication功能，其目标超出了数据重建，是保留特定semantic attribute的。在这个框架中，每个层的编码数据都可以被优化，以保留特定数据 semantics。因此，在successive层中，精度的semantic classifier可以进一步提高，强调在编码和解码过程中保留目标semantic attribute。我们在MNIST和CIFAR10 dataset上进行了实验，实验结果表明，我们的提议方法可以在不同的分辨率下重建数据，并且可以在successive层中提高semantic classifier的精度，这是特别有利于在数据中优先保留更重要的semantic attribute。
</details></li>
</ul>
<hr>
<h2 id="Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models"><a href="#Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models" class="headerlink" title="Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models"></a>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
<li>for: 这个研究是为了提出一个 контекст感知的路由系统， Tryage，以便将语言模型库中的专家模型选择依据输入提示的分析，以提高工作效率和适应性。</li>
<li>methods: 这个研究使用了语言模型路由器来预测下游模型的性能 на prompts，然后使用一个目标函数集成了性能预测、用户目标和约束来作出路由决策。</li>
<li>results: 在不同的数据集中，包括代码、文本、医疗资料和专利，Tryage框架在动态模型选择中比 Gorilla 和 GPT3.5 Turbo 高，实现了50.9% 的准确率，比 GPT3.5 Turbo 的23.6% 和 Gorilla 的10.8% 更高。<details>
<summary>Abstract</summary>
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.
</details>
<details>
<summary>摘要</summary>
Introduction of transformer architecture and self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200,000 models in the Hugging Face ecosystem, users struggle with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. We propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection, identifying the optimal model with an accuracy of 50.9%, compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.
</details></li>
</ul>
<hr>
<h2 id="Low-Tensor-Rank-Learning-of-Neural-Dynamics"><a href="#Low-Tensor-Rank-Learning-of-Neural-Dynamics" class="headerlink" title="Low Tensor Rank Learning of Neural Dynamics"></a>Low Tensor Rank Learning of Neural Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11567">http://arxiv.org/abs/2308.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Pellegrino, N Alex Cayco-Gajic, Angus Chadwick</li>
<li>for: 这个论文主要研究了 Recurrent Neural Networks (RNNs) 在学习过程中的 synaptic connectivity 的协调变化，以及这种变化如何影响学习的结果。</li>
<li>methods: 作者使用了 RNNs 的不同级别来适应不同的学习任务，并通过分析 weights 矩阵的低级别结构来研究学习过程中的 synaptic connectivity 的演化。</li>
<li>results: 作者发现，在学习过程中，RNNs 的 weights 矩阵具有低级别结构，并且这种低级别结构在整个学习过程中保持不变。此外，作者还 validate了这个发现，并提供了一些数学结果，证明在低维度任务上训练 RNNs 时，低级别 weights 自然地出现。<details>
<summary>Abstract</summary>
Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applied to the data faithfully recovers this low rank structure. Finally, we present a set of mathematical results bounding the matrix and tensor ranks of gradient descent learning dynamics which show that low-tensor-rank weights emerge naturally in RNNs trained to solve low-dimensional tasks. Taken together, our findings provide novel constraints on the evolution of population connectivity over learning in both biological and artificial neural networks, and enable reverse engineering of learning-induced changes in recurrent network dynamics from large-scale neural recordings.
</details>
<details>
<summary>摘要</summary>
学习 rely 于相协调的 synaptic 变化在 repeatedly 连接的 neuron  populations。因此，理解学习过程中 population 连接性的 collective 演化是 neuroscience 和 machine learning 中关键的挑战。特别是， latest 的研究表明，在任务训练后 RNN 的 weight matrix 通常具有低级数，但是这种低级数结构如何在学习过程中发展未知。为了解决这个问题，我们investigate  RNN 的 weight matrix 在学习过程中的级数。我们使用不同级数的 RNN 适应大规模的神经记录数据，并发现在整个学习过程中，推导出的 weights 都是 low-tensor-rank 的，因此在低维度的 subspace 中演化。我们验证了这一观察结果，并在 RNN 解决同一个任务时，直接对 ground truth  weights 进行 low-tensor-rank 分解，并证明了我们对数据进行的方法可以准确地恢复这种低级数结构。最后，我们提出了一些数学结果，证明在 gradient descent 学习动力学中，low-tensor-rank weights 会自然地出现在 RNN 解决低维度任务时。总之，我们的发现提供了对 population 连接性在学习过程中的新的约束，并允许从大规模神经记录数据中逆向工程学习-induced 变化的 recurrent network 动力学。
</details></li>
</ul>
<hr>
<h2 id="Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge"><a href="#Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge" class="headerlink" title="Practical Insights on Incremental Learning of New Human Physical Activity on the Edge"></a>Practical Insights on Incremental Learning of New Human Physical Activity on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11691">http://arxiv.org/abs/2308.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Arvanitakis, Jingwei Zuo, Mthandazo Ndhlovu, Hakim Hacid</li>
<li>for: 本研究探讨了Edge Machine Learning（Edge ML）中的一些独特挑战，包括受限的Edge设备存储空间、训练计算能力的有限性和学习类型的数量。</li>
<li>methods: 本研究使用了我们开发的MAGNETO系统，通过收集来自移动传感器的数据来学习人类活动。</li>
<li>results: 我们的实验结果显示，Edge ML在受限的 Edge 设备上进行学习时会面临一些独特的挑战，包括数据存储和计算能力的有限性。<details>
<summary>Abstract</summary>
Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.
</details>
<details>
<summary>摘要</summary>
《边缘机器学习（Edge ML）》，将计算智能从云端系统转移到边缘设备，吸引了广泛关注，因为它们的优点明显，包括降低延迟、提高数据隐私和减少连接依赖。然而，这些优点也引入了传统云端方法中缺失的挑战。本文介绍边缘学习的细节，探讨（i）边缘设备受限的数据存储（ii）训练计算能力的限制和（iii）学习类数量之间的互相关系。通过我们的MAGNETO系统的实验，关于通过移动感知器收集的数据来学习人类活动，我们高亮了这些挑战并提供了有价值的对边缘ML的视角。
</details></li>
</ul>
<hr>
<h2 id="Multi-event-Video-Text-Retrieval"><a href="#Multi-event-Video-Text-Retrieval" class="headerlink" title="Multi-event Video-Text Retrieval"></a>Multi-event Video-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11551">http://arxiv.org/abs/2308.11551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gengyuanmax/mevtr">https://github.com/gengyuanmax/mevtr</a></li>
<li>paper_authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</li>
<li>for: 这篇论文的目的是提出一种多事件视频文本检索任务（MeVTR），用于解决现实中视频内容通常包含多个事件，而文本查询或页面元数据通常与单个事件相关的问题。</li>
<li>methods: 这篇论文提出了一种简单的模型——Me-Retriever，它使用了关键事件视频表示和一种新的MeVTR损失函数来解决MeVTR任务。</li>
<li>results: 对于视频-to-文本和文本-to-视频任务，这种简单的框架超越了其他模型，并在MeVTR任务中建立了一个坚固的基础。<details>
<summary>Abstract</summary>
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR.
</details>
<details>
<summary>摘要</summary>
视频文本检索（VTR）是一个重要的多Modal任务，在互联网上巨量的视频文本数据时代而言。大量工作通过使用两气流视力语言模型建立一个共同表示视频文本对的方法来进行VTR任务。然而，这些模型假设视频内容和文本之间是一对一的对应关系，而忽略了实际应用中的多个事件场景。这种假设与实际应用之间存在一个差距，导致之前训练的目标与实际应用之间的差异，从而导致旧模型在推理过程中的性能下降。在这项研究中，我们引入多事件视频文本检索（MeVTR）任务，解决每个视频包含多个不同事件的场景，是传统VTR任务的一个 nichescenario。我们提出了一种简单的模型，Me-Retriever，该模型包括关键事件视频表示和一个新的MeVTR损失函数。我们进行了广泛的实验，并证明了这种简单的框架可以在视频到文本和文本到视频任务中高效地超越其他模型，并成为MeVTR任务的robust基线。我们认为这项工作将成为未来研究的坚实基础。代码可以在https://github.com/gengyuanmax/MeVTR中获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.LG_2023_08_23/" data-id="clmjn91mo007d0j887spaeaua" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/23/cs.CV_2023_08_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-08-23 21:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/23/cs.SD_2023_08_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-23 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
