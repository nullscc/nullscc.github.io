
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-23 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Analysis of XLS-R for Speech Quality Assessment paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12077 repo_url: https:&#x2F;&#x2F;github.com&#x2F;lcn-kul&#x2F;xls-r-analysis-sqa paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-23 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/23/cs.SD_2023_08_23/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Analysis of XLS-R for Speech Quality Assessment paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12077 repo_url: https:&#x2F;&#x2F;github.com&#x2F;lcn-kul&#x2F;xls-r-analysis-sqa paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-22T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:36.189Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.SD_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-23 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-XLS-R-for-Speech-Quality-Assessment"><a href="#Analysis-of-XLS-R-for-Speech-Quality-Assessment" class="headerlink" title="Analysis of XLS-R for Speech Quality Assessment"></a>Analysis of XLS-R for Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12077">http://arxiv.org/abs/2308.12077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcn-kul/xls-r-analysis-sqa">https://github.com/lcn-kul/xls-r-analysis-sqa</a></li>
<li>paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme</li>
<li>for: è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯å¯¹ speech quality assessment ä¸­ä½¿ç”¨ deep neural networks è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼Œä»¥æé«˜ç”¨æˆ·ä½“éªŒè´¨é‡ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨ pre-trained wav2vec-based XLS-R embeddingsï¼Œå¹¶è¿›è¡Œäº†å±‚æ•°æ®åˆ†æå’Œç‰¹å¾ç»„åˆçš„ç ”ç©¶ï¼Œä»¥ä¼˜åŒ– speech quality prediction çš„æ€§èƒ½ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œåœ¨ä¸åŒå±‚æ•°æ®ä¸­æå–ç‰¹å¾å¯ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”å¯¹ä¸åŒç±»å‹çš„å¹²æ‰°è¿›è¡Œäº†åˆ†æï¼Œå‘ç°ä½çº§ç‰¹å¾æ•æ‰å™ªéŸ³å’Œæˆ¿é—´å“åº”ç‰¹å¾ï¼Œé«˜çº§ç‰¹å¾åˆ™æ›´æ³¨é‡è¯­éŸ³å†…å®¹å’ŒæŠ—æ‚è°±æ€§ã€‚<details>
<summary>Abstract</summary>
In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨åœ¨çº¿ä¼šè®®åº”ç”¨ç¨‹åºä¸­ï¼Œä¼°è®¡è¯­éŸ³ä¿¡å·çš„æ„ŸçŸ¥è´¨é‡éå¸¸é‡è¦ï¼Œä»¥ç¡®ä¿ç”¨æˆ·çš„å“è´¨ä½“éªŒè¾¾åˆ°æœ€é«˜æ°´å¹³ã€‚äººç±»è¯„åˆ†æ˜¯æœ€å¯é çš„è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œä½†æ˜¯è¿™ç§æ–¹æ³•å—åˆ°åŠ³åŠ¨åŠ›çš„é™åˆ¶ï¼Œä¸é€‚åˆå¤§è§„æ¨¡åº”ç”¨ã€‚å› æ­¤ï¼Œç ”ç©¶è€…ä»¬çš„å…³æ³¨ç‚¹è½¬ç§»åˆ°äº†è‡ªåŠ¨åŒ–è¯­éŸ³è´¨é‡è¯„ä¼°ï¼Œé€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œçš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚æœ€æ–°çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„wav2vecåŸºäºXLS-Rçš„åµŒå…¥å¯ä»¥è¾¾åˆ°é¢„æµ‹è¯­éŸ³è´¨é‡çš„çŠ¶æ€ä¹‹ artsã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†åµŒå…¥çš„æ·±å…¥åˆ†æã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ†æäº†XLS-Rä¸­æ¯å±‚çš„åµŒå…¥è¡¨ç°ï¼Œä»¥åŠæ¯ä¸ªæ¨¡å‹çš„ä¸åŒå¤§å°ï¼ˆ300Mã€1Bã€2Bå‚æ•°ï¼‰ã€‚å¥‡æ€ªçš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªä¼˜åŒ–åŒºåŸŸï¼šä¸€ä¸ªåœ¨ä¸‹å±‚ç‰¹å¾ä¸­ï¼Œä¸€ä¸ªåœ¨é«˜å±‚ç‰¹å¾ä¸­ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è¿™ä¸¤ä¸ªä¼˜åŒ–åŒºåŸŸçš„åŸå› ã€‚æˆ‘ä»¬å‡è®¾ä¸‹å±‚ç‰¹å¾æ•æ‰äº†å™ªéŸ³å’Œæˆ¿é—´å“åº”çš„ç‰¹å¾ï¼Œè€Œé«˜å±‚ç‰¹å¾åˆ™ä¸“æ³¨äºè¯­éŸ³å†…å®¹å’Œç†è§£åº¦ã€‚ä¸ºäº†è¯æ˜è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒæ°´å¹³çš„å™ªéŸ³å’Œæˆ¿é—´å“åº”å¯¹MOSé¢„æµ‹çš„æ•æ„Ÿæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°è¯•å°†è¿™ä¸¤ä¸ªä¼˜åŒ–åŒºåŸŸèåˆï¼Œä»¥ç¡®å®šä»–ä»¬æ˜¯å¦åŒ…å«ç›¸äº’è¡¥å……çš„ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹ï¼Œå¹¶è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨æœªseenæ•°æ®ä¸Šçš„æ³›åŒ–æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning"><a href="#Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning" class="headerlink" title="Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning"></a>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11980">http://arxiv.org/abs/2308.11980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/hgrl">https://github.com/yuanbo2020/hgrl</a></li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren</li>
<li>for: This paper is written for the purpose of exploring the integration of objective audio events (AE) with subjective annoyance ratings (AR) of soundscape perceived by humans.</li>
<li>methods: The paper proposes a novel hierarchical graph representation learning (HGRL) approach to link AE with AR. The approach consists of fine-grained event (fAE) embeddings, coarse-grained event (cAE) embeddings, and AR embeddings.</li>
<li>results: The proposed HGRL approach successfully integrates AE with AR for audio event classification (AEC) and audio scene understanding (ARP) tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.<details>
<summary>Abstract</summary>
Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans' listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¬è§‰äº‹ä»¶æºå¸¦ç€ objective ä¸–ç•Œçš„ä¸°å¯Œä¿¡æ¯ã€‚å¬è§‰äº‹ä»¶çš„ç»„æˆä¼šå½±å“äººä»¬åœ¨å¬è§‰æ™¯è±¡ä¸­çš„å¿ƒç†çŠ¶æ€ã€‚å…ˆå‰çš„æ–¹æ³•é€šå¸¸åªæ˜¯å¯¹å¬è§‰äº‹ä»¶å’Œåœºæ™¯è¿›è¡Œåˆ†ç±»å’Œæ£€æµ‹ï¼Œå¯èƒ½å¿½ç•¥äº†è¿™äº›å¬è§‰äº‹ä»¶å¯¹äººä»¬å¬è§‰ç¯å¢ƒä¸­çš„ listening å¿ƒç†çŠ¶æ€çš„å½±å“ï¼Œä¾‹å¦‚åŒçƒ¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å±‚æ¬¡å›¾è¡¨å­¦ä¹ ï¼ˆHGRLï¼‰æ–¹æ³•ï¼Œå°† objective å¬è§‰äº‹ä»¶ï¼ˆAEï¼‰ä¸äººä»¬å¯¹å¬è§‰æ™¯è±¡çš„ä¸»è§‚åŒçƒ¦è¯„åˆ†ï¼ˆARï¼‰å…³è”èµ·æ¥ã€‚å±‚æ¬¡å›¾åŒ…æ‹¬ç»†åŒ–çš„äº‹ä»¶åµŒå…¥ï¼ˆfAEï¼‰ã€ä¸­ç»†åŒ–çš„äº‹ä»¶åµŒå…¥ï¼ˆcAEï¼‰å’Œ AR åµŒå…¥ã€‚å®éªŒæ˜¾ç¤ºï¼Œæå‡ºçš„ HGRL æ–¹æ³•æˆåŠŸåœ°ç»“åˆ AE ä¸ AR  Ğ´Ğ»Ñ AEC å’Œ ARP ä»»åŠ¡ï¼ŒåŒæ—¶åè°ƒ cAE å’Œ fAE ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶å°†ä¸¤ç§ä¸åŒçš„ AE ä¿¡æ¯ä¸ AR è¿›è¡Œå¯¹åº”ã€‚
</details></li>
</ul>
<hr>
<h2 id="CED-Consistent-ensemble-distillation-for-audio-tagging"><a href="#CED-Consistent-ensemble-distillation-for-audio-tagging" class="headerlink" title="CED: Consistent ensemble distillation for audio tagging"></a>CED: Consistent ensemble distillation for audio tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11957">http://arxiv.org/abs/2308.11957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richermans/ced">https://github.com/richermans/ced</a></li>
<li>paper_authors: Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Junbo Zhang, Yujun Wang</li>
<li>for: æé«˜éŸ³é¢‘åˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½å’Œå‡å°‘æ¨¡å‹å¤§å°</li>
<li>methods: ä½¿ç”¨æ‰©å±•å’ŒçŸ¥è¯†å¡«å……ï¼ˆKDï¼‰æŠ€æœ¯ï¼Œä»¥åŠä¸€ä¸ªç®€å•çš„è®­ç»ƒæ¡†æ¶ç§°ä¸ºå¸¸è§„æ•™å­¦ï¼ˆCEDï¼‰</li>
<li>results: ä½¿ç”¨CEDè®­ç»ƒå¤šç§åŸºäºå˜æ¢å™¨çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸€ä¸ª10Må‚æ•°æ¨¡å‹ï¼Œåœ¨Audiosetï¼ˆASï¼‰ä¸Šè¾¾åˆ°49.0çš„mean average precisionï¼ˆmAPï¼‰<details>
<summary>Abstract</summary>
Augmentation and knowledge distillation (KD) are well-established techniques employed in the realm of audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3\% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.
</details>
<details>
<summary>æ‘˜è¦</summary>
ğŸ‡¨ğŸ‡³ æ‰©å±•å’ŒçŸ¥è¯†å‚¨å­˜ï¼ˆKDï¼‰æ˜¯éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸­å¸¸ç”¨çš„æŠ€æœ¯ï¼Œå¯ä»¥æé«˜æ€§èƒ½å’Œå‡å°‘æ¨¡å‹å¤§å°ã€‚ although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distills student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model, requiring only 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.Here's the word-for-word translation of the text into Simplified Chinese:ğŸ‡¨ğŸ‡³ æ‰©å±•å’ŒçŸ¥è¯†å‚¨å­˜ï¼ˆKDï¼‰æ˜¯éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸­å¸¸ç”¨çš„æŠ€æœ¯ï¼Œå¯ä»¥æé«˜æ€§èƒ½å’Œå‡å°‘æ¨¡å‹å¤§å°ã€‚ although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distills student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model, requiring only 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.
</details></li>
</ul>
<hr>
<h2 id="Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation"><a href="#Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation" class="headerlink" title="Example-Based Framework for Perceptually Guided Audio Texture Generation"></a>Example-Based Framework for Perceptually Guided Audio Texture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11859">http://arxiv.org/abs/2308.11859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purnima Kamath, Chitralekha Gupta, Lonce Wyse, Suranga Nanayakkara</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æ§åˆ¶ç”Ÿæˆçš„éŸ³é¢‘TEXTUREï¼Œé€šè¿‡æ¡ä»¶ä½¿ç”¨æ ‡æ³¨æ•°æ®ï¼Œä½†æ˜¯è·å–æ ‡æ³¨æ•°æ®å¯èƒ½æ˜¯æ—¶é—´consumingå’Œå®¹æ˜“å‡ºé”™çš„ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¾‹å­çš„æ¡†æ¶ï¼Œé€šè¿‡ç”¨æˆ·å®šä¹‰çš„è¯­ä¹‰ç‰¹å¾æ¥å†³å®šç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ§åˆ¶å› ç´ ã€‚é€šè¿‡ç”Ÿæˆä¸€äº›ç¤ºä¾‹æ¥æŒ‡ç¤ºè¯­ä¹‰ç‰¹å¾çš„å­˜åœ¨æˆ–ç¼ºå¤±ï¼Œå¯ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ‰¾åˆ°æ§åˆ¶å› ç´ çš„æŒ‡å¯¼å‘é‡ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ‰¾åˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…·æœ‰è¯­ä¹‰ç‰¹å¾çš„æ½œåœ¨ç›¸å…³æ€§å’Œå†³å®šæ€§æŒ‡å¯¼å‘é‡ï¼Œå¹¶åº”ç”¨äºå…¶ä»–ä»»åŠ¡ï¼Œå¦‚é€‰æ‹©æ€§ semantic attribute transferã€‚<details>
<summary>Abstract</summary>
Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>ç”¨æŠ½è±¡æ¨¡å‹ç”Ÿæˆ audio æ–‡åŒ–æ—¶ï¼Œå¯ä»¥æ˜¾å¼ç¼–ç æ§åˆ¶æ€§ã€‚ä¸è¿‡ï¼Œå¯¹ audio æ–‡åŒ–çš„æ•°æ®è¿›è¡Œsemantic labelingæ˜¯costlyï¼Œtime-consumingï¼Œå’Œå®¹æ˜“å‡ºé”™ï¼Œå› ä¸ºäººå·¥æ ‡æ³¨è€…çš„ä¸»è§‚æ€§ã€‚å› æ­¤ï¼Œè¦æ§åˆ¶ç”Ÿæˆï¼Œéœ€è¦è‡ªåŠ¨ä»æ— æ ‡æ³¨ texture ä¸­æ¨æ–­ç”¨æˆ·å®šä¹‰çš„ Semantic attribute çš„å˜åŒ–å› ç´ ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¾‹å­çš„æ¡†æ¶ï¼Œç”¨äºç¡®å®š guide vectorï¼Œä»¥æ§åˆ¶ç”Ÿæˆä¸­çš„ Semantic attributeã€‚é€šè¿‡ç”Ÿæˆä¸€äº›synthetic exampleæ¥æŒ‡ç¤ºSemantic attributeçš„å­˜åœ¨æˆ–ç¼ºå¤±ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ¨æ–­guide vectorçš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ‰¾åˆ°å¯è§ relevance å’Œ deterministicçš„ guide vectorï¼Œä»¥ä¾¿åœ¨ç”Ÿæˆä¸­æ§åˆ¶ Semantic attributeã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†è¿™ç§æ–¹æ³•çš„åº”ç”¨äºå…¶ä»–ä»»åŠ¡ï¼Œå¦‚é€‰æ‹©æ€§ transferred attributeã€‚Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, BjÃ¶rn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
<li>for: The paper is written to investigate the use of natural language processing on social media to predict depression, with a focus on identifying specific speech topics that may indicate depression severity.</li>
<li>methods: The paper uses the Whisper tool and the BERTopic model to analyze 3919 smartphone-collected speech recordings from 265 participants, identifying 29 topics and finding that six of these topics are associated with higher depression severity.</li>
<li>results: The paper finds that specific speech topics can indicate depression severity, and that longitudinally monitoring language use can provide valuable insights into changes in depression severity over time. The study also demonstrates the practicality of using data-driven workflows to collect and analyze large-scale speech data from real-world settings for digital health research.Here are the three points in Simplified Chinese text:</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶ç¤¾äº¤åª’ä½“ä¸Šçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œä»¥é¢„æµ‹æŠ‘éƒï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„è¯­éŸ³è¯é¢˜æ¥è¯„ä¼°æŠ‘éƒä¸¥é‡ç¨‹åº¦ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨Whisperå·¥å…·å’ŒBERTopicæ¨¡å‹åˆ†æäº†3919ä¸ªæ‰‹æœºæ”¶é›†çš„è¯­éŸ³è®°å½•ï¼Œå¹¶å°†å…¶åˆ†ä¸º29ä¸ªè¯é¢˜ï¼Œå…¶ä¸­å…­ä¸ªè¯é¢˜ä¸æŠ‘éƒä¸¥é‡ç¨‹åº¦é«˜æœ‰å…³ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡å‘ç°ç‰¹å®šçš„è¯­éŸ³è¯é¢˜å¯ä»¥åæ˜ æŠ‘éƒä¸¥é‡ç¨‹åº¦ï¼Œå¹¶ä¸” longitudinal ç›‘æµ‹è¯­éŸ³ä½¿ç”¨å¯ä»¥ä¸ºæŠ‘éƒç ”ç©¶æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚ç ”ç©¶è¿˜è¯æ˜äº†ä½¿ç”¨æ•°æ®é©±åŠ¨çš„å·¥ä½œæµç¨‹æ”¶é›†å’Œåˆ†æå¤§è§„æ¨¡è¯­éŸ³æ•°æ®çš„å®ç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç ”ç©¶è¡¨æ˜è¯­è¨€ä½¿ç”¨ä¸æŠ‘éƒæœ‰ç›¸å…³æ€§ï¼Œä½†å¤§è§„æ¨¡éªŒè¯æ˜¯éœ€è¦çš„ã€‚ä¼ ç»Ÿçš„ä¸´åºŠç ”ç©¶æ˜¯æ˜‚è´µçš„ï¼Œå› æ­¤äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨ç¤¾äº¤åª’ä½“ä¸Šè¿›è¡Œè¯­è¨€é¢„æµ‹æ˜¯ä¸€ç§å¯èƒ½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¿˜å­˜åœ¨ä¸€äº›é™åˆ¶ï¼ŒåŒ…æ‹¬æ— æ•ˆçš„æ ‡ç­¾éªŒè¯ã€å—ä¼—æ ·æœ¬åè§å’Œç¼ºä¹ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡ä½¿ç”¨Whisperå·¥å…·å’ŒBERTopicæ¨¡å‹åˆ†æäº†3919ä¸ªæ‰‹æœºæ”¶é›†çš„è¯­éŸ³è®°å½•ï¼Œä»265åå‚ä¸è€…ä¸­æå–å‡º29ä¸ªè¯é¢˜ã€‚å…­ä¸ªè¯é¢˜çš„ä¸­å€¼PHQ-8å¤§äºæˆ–ç­‰äº10è¢«è§†ä¸ºæŠ‘éƒçš„é£é™©è¯é¢˜ï¼šæ— æœ›ã€ç¡çœ ã€å¿ƒç†æ²»ç–—ã€å‰ªå‘ã€å­¦ä¹ å’Œè¯¾ç¨‹ã€‚ä¸ºäº†è§£é‡Šè¯é¢˜å‡ºç°å’ŒæŠ‘éƒä¸¥é‡åº¦ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸åŒè¯é¢˜çš„è¡Œä¸ºç‰¹å¾ï¼ˆæ¥è‡ªä½©æˆ´å™¨ï¼‰å’Œè¯­è¨€ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜ investigateäº†è¯é¢˜å˜åŒ–å’ŒæŠ‘éƒä¸¥é‡åº¦å˜åŒ–çš„æ—¶é—´ç›¸å…³æ€§ï¼Œè¿™è¡¨æ˜äº†éœ€è¦é•¿æœŸç›‘æµ‹è¯­è¨€ä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜åœ¨ç›¸ä¼¼çš„å°æ ·æœ¬ä¸Šæµ‹è¯•äº†BERTopicæ¨¡å‹ï¼Œè·å¾—äº†ä¸€äº›ç›¸ä¼¼çš„ç»“æœã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„å‘ç°è¡¨æ˜ç‰¹å®šçš„è¯­éŸ³è¯é¢˜å¯èƒ½æŒ‡ç¤ºæŠ‘éƒä¸¥é‡åº¦ã€‚æˆ‘ä»¬æä¾›çš„æ•°æ®é©±åŠ¨çš„å·¥ä½œæµç¨‹ä¸ºæ•´ä½“å«ç”Ÿç ”ç©¶æä¾›äº†å®ç”¨çš„æ–¹æ³•ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.SD_2023_08_23/" data-id="cllurrpbc009jsw88d99mgscs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/23/cs.LG_2023_08_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-23 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/23/eess.AS_2023_08_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-08-23</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
