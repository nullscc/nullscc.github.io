
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-23 123:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Analysis of XLS-R for Speech Quality Assessment paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12077 repo_url: https:&#x2F;&#x2F;github.com&#x2F;lcn-kul&#x2F;xls-r-analysis-sqa paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-23 123:00:00">
<meta property="og:url" content="http://example.com/2023/08/23/cs.SD_2023_08_23/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Analysis of XLS-R for Speech Quality Assessment paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12077 repo_url: https:&#x2F;&#x2F;github.com&#x2F;lcn-kul&#x2F;xls-r-analysis-sqa paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-23T00:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:51.874Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.SD_2023_08_23/" class="article-date">
  <time datetime="2023-08-23T00:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-23 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-XLS-R-for-Speech-Quality-Assessment"><a href="#Analysis-of-XLS-R-for-Speech-Quality-Assessment" class="headerlink" title="Analysis of XLS-R for Speech Quality Assessment"></a>Analysis of XLS-R for Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12077">http://arxiv.org/abs/2308.12077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcn-kul/xls-r-analysis-sqa">https://github.com/lcn-kul/xls-r-analysis-sqa</a></li>
<li>paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme<br>for: 这个论文主要针对大规模应用中的语音质量评估问题，旨在提高用户体验质量。methods: 这篇论文使用了深度神经网络进行自动语音质量评估，并利用预训练的wav2vec-based XLS-R嵌入进行end-to-end训练。results: 研究发现，使用不同层次的XLS-R嵌入可以获得最佳性能，并且发现低层和高层嵌入之间存在较大的对比，这可能是因为低层嵌入捕捉了噪声和房间响应的特征，而高层嵌入则更关注语音内容和理解度。<details>
<summary>Abstract</summary>
In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.
</details>
<details>
<summary>摘要</summary>
在在线会议应用程序中，估算语音信号的感知质量非常重要，以确保用户的体验质量高。最可靠的方法是通过人类评分（MOS）指标来评估语音信号质量，但这种方法是人工劳动密集，不适合大规模应用。因此，研究者们的焦点已经转移到了使用深度神经网络进行自动化语音质量评估。最近，一些研究表明，利用预训练的wav2vec-基于XLS-R的嵌入可以达到领先的性能。在这篇论文中，我们进行了嵌入的深入分析。我们首先分析了XLS-R中每层提取的嵌入的性能，以及每个模型大小（300M、1B、2B参数）。 surprisingly，我们发现了两个优化区域：一个是在低级别特征，另一个是在高级别特征。接下来，我们研究了这两个优化区域的原因。我们假设低级别特征捕捉了噪声和房间响应的特征，而高级别特征则专注于语音内容和可理解性。为了证明这一点，我们分析了不同水平的噪声和房间响应对MOS预测的敏感性。之后，我们尝试将这两个优化区域的特征进行融合，以确定它们是否包含了相互补做的信息。最后，我们比较了我们提出的模型的性能，并评估模型在未看过的数据集上的普适性。
</details></li>
</ul>
<hr>
<h2 id="Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning"><a href="#Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning" class="headerlink" title="Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning"></a>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11980">http://arxiv.org/abs/2308.11980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/hgrl">https://github.com/yuanbo2020/hgrl</a></li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren</li>
<li>for: This paper aims to improve the perception of audio environments by linking objective audio events with subjective annoyance ratings.</li>
<li>methods: The paper proposes a novel hierarchical graph representation learning (HGRL) approach that integrates fine-grained event embeddings, coarse-grained event embeddings, and annoyance rating embeddings.</li>
<li>results: The proposed HGRL approach successfully integrates objective audio events with subjective annoyance ratings, coordinates the relations between different levels of event information, and aligns the two different grains of audio event information with the annoyance ratings.<details>
<summary>Abstract</summary>
Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans' listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.
</details>
<details>
<summary>摘要</summary>
日常生活中的听觉事件含有丰富的对象世界信息。这些听觉事件的组合会影响人们在听觉景中的情绪。大多数前期方法只是 классифика和检测听觉事件和场景，但可能忽略这些听觉事件的主观感受，例如厌烦。为此，这篇论文提出了一种新的层次图表学习（HGRL）方法，将对象听觉事件（AE）与人们对听觉景的主观厌烦评分（AR）联系起来。层次图包括细化事件（fAE）嵌入，单类事件 semantics，粗化事件（cAE）嵌入，多类事件 semantics，以及AR嵌入。实验显示，提议的HGRL成功地结合AE与AR进行AEC和ARP任务，同时协调粗化事件和细化事件之间的关系，并将两种不同的AE信息与AR进行对接。
</details></li>
</ul>
<hr>
<h2 id="CED-Consistent-ensemble-distillation-for-audio-tagging"><a href="#CED-Consistent-ensemble-distillation-for-audio-tagging" class="headerlink" title="CED: Consistent ensemble distillation for audio tagging"></a>CED: Consistent ensemble distillation for audio tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11957">http://arxiv.org/abs/2308.11957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richermans/ced">https://github.com/richermans/ced</a></li>
<li>paper_authors: Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Junbo Zhang, Yujun Wang</li>
<li>For: Enhancing performance and reducing model sizes on the Audioset (AS) benchmark for audio classification tasks.* Methods: Consistent teaching and knowledge distillation (KD) techniques.* Results: A scalable training framework called CED that efficiently stores logits and augmentation methods on disk, achieving a 49.0 mean average precision (mAP) on AS with a 10M parameter model, and available pretrained models and code at <a target="_blank" rel="noopener" href="https://github.com/RicherMans/CED">https://github.com/RicherMans/CED</a>.<details>
<summary>Abstract</summary>
Augmentation and knowledge distillation (KD) are well-established techniques employed in the realm of audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3\% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.
</details>
<details>
<summary>摘要</summary>
通常的扩展和知识塑造（KD）技术已经在音频分类任务中广泛应用，以提高性能并减少模型大小。虽然这两种技术都是有效的，但它们的结合使用，即一致教学，尚未得到探讨。本文提出了一种名为CED的简单的训练框架，它将教师模型中的大型 ensemble 降低到学生模型中。为实现这一点，CED efficiently stores logits以及扩展方法在磁盘上，使其可扩展到大规模数据集。中心的CED的有效性在于它的标签自由性，意味着只有存储的logits被用于学生模型的优化，需要额外磁盘空间为0.3%。研究使用了不同的转换器模型，包括一个10M参数模型，在AS上达到49.0的平均精度（mAP）。预训练模型和代码可以在https://github.com/RicherMans/CED上获取。
</details></li>
</ul>
<hr>
<h2 id="Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation"><a href="#Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation" class="headerlink" title="Example-Based Framework for Perceptually Guided Audio Texture Generation"></a>Example-Based Framework for Perceptually Guided Audio Texture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11859">http://arxiv.org/abs/2308.11859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purnima Kamath, Chitralekha Gupta, Lonce Wyse, Suranga Nanayakkara</li>
<li>for: 这篇论文的目的是控制生成AudioTexture的杂化特征，通过对标注数据进行conditioning来实现。</li>
<li>methods: 该论文提出了一种基于实例的框架，通过生成一些示例来指示用户定义的semantic attribute的存在或缺失，从而在生成过程中控制这些属性。</li>
<li>results: 该方法能够找到对杂化特征具有启发意义的指导向量，并且可以在生成过程中具有准确性和可控性。此外，该方法还可以应用于其他任务，如选择性semantic attribute transfer。<details>
<summary>Abstract</summary>
Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>生成模型用于 sintesizing 音频 текстуры显式地编码可控性，通过将模型 conditioning 于标注数据。 although 音频 текстуras 可以轻松地在实际中采集， semantically 标注它们是 expensive， time-consuming，和 prone to errors due to human annotator subjectivity。 therefore, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modeling unlabeled textures. in this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. by synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/23/cs.SD_2023_08_23/" data-id="cllt9prx2005kol885xbc90a3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/23/cs.LG_2023_08_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-23 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/23/eess.AS_2023_08_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-08-23</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
