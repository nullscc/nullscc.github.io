
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-23 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Tumor-Centered Patching for Enhanced Medical Image Segmentation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12168 repo_url: None paper_authors: Mutyyba Asghar, Ahmad Raza Shahid, Akhtar Jamil, Kiran Aftab, S">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-23 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/23/eess.IV_2023_08_23/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Tumor-Centered Patching for Enhanced Medical Image Segmentation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12168 repo_url: None paper_authors: Mutyyba Asghar, Ahmad Raza Shahid, Akhtar Jamil, Kiran Aftab, S">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-22T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:36.227Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eess.IV_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-23 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation"><a href="#Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Tumor-Centered Patching for Enhanced Medical Image Segmentation"></a>Tumor-Centered Patching for Enhanced Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12168">http://arxiv.org/abs/2308.12168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutyyba Asghar, Ahmad Raza Shahid, Akhtar Jamil, Kiran Aftab, Syed Ather Enam</li>
<li>for: 这篇论文旨在提高医疗影像诊断中的Computer-aided diagnosis和手术系统。</li>
<li>methods: 这篇论文使用了一种新的tumor-centered patching方法，将肿瘤作为分析区域，以改善分类不均和边界缺乏的问题。</li>
<li>results: 实验结果显示，这种方法可以改善分类不均， segmentation scores分别为0.78、0.76和0.71 для整体、核心和增强肿瘤。<details>
<summary>Abstract</summary>
The realm of medical image diagnosis has advanced significantly with the integration of computer-aided diagnosis and surgical systems. However, challenges persist, particularly in achieving precise image segmentation. While deep learning techniques show potential, obstacles like limited resources, slow convergence, and class imbalance impede their effectiveness. Traditional patch-based methods, though common, struggle to capture intricate tumor boundaries and often lead to redundant samples, compromising computational efficiency and feature quality. To tackle these issues, this research introduces an innovative approach centered on the tumor itself for patch-based image analysis. This novel tumor-centered patching method aims to address the class imbalance and boundary deficiencies, enabling focused and accurate tumor segmentation. By aligning patches with the tumor's anatomical context, this technique enhances feature extraction accuracy and reduces computational load. Experimental results demonstrate improved class imbalance, with segmentation scores of 0.78, 0.76, and 0.71 for whole, core, and enhancing tumors, respectively using a lightweight simple U-Net. This approach shows potential for enhancing medical image segmentation and improving computer-aided diagnosis systems.
</details>
<details>
<summary>摘要</summary>
医疗图像诊断领域已经得到了计算机支持的辅助诊断和手术系统的整合，但是还存在许多挑战，主要是精准图像分割的问题。深度学习技术表现出了潜在的潜力，但是有限的资源、慢速融合和分类不均等问题使其效果受限。传统的补丁方法，尽管广泛使用，但是它们往往难以捕捉复杂的肿瘤边界，导致重复的样本生成，从而降低计算效率和特征质量。为解决这些问题，本研究提出了一种新的方法，这种方法是基于肿瘤的补丁分析法。这种新的肿瘤中心的补丁方法希图解决分类不均和边界不足的问题，以提高精准的肿瘤分割。通过将补丁与肿瘤的 анатомиче上下文进行对齐，这种技术可以提高特征提取的准确性和降低计算负担。实验结果表明，使用了一种轻量级的简单U-Net，可以提高分类不均的问题， segmentation scores分别为0.78、0.76和0.71 для整体、核心和增强肿瘤。这种方法表现出了在医疗图像分割领域的潜力，并可能用于改进计算机支持的诊断系统。
</details></li>
</ul>
<hr>
<h2 id="DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning"><a href="#DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning" class="headerlink" title="DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning"></a>DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12084">http://arxiv.org/abs/2308.12084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann</li>
<li>for: 这个研究是为了提出一个可以同时进行超解析和降噪的深度学习模型，以扩展现有的超解析和降噪模型的能力。</li>
<li>methods: 这个模型使用了一个基于 residual-in-residual 的 generator，以及一个具有3D DWT和1x1卷积的 discriminator。</li>
<li>results: 这个模型可以同时进行高品质的超解析和降噪，并且可以在未见过的MRI数据上进行验证。<details>
<summary>Abstract</summary>
MRI super-resolution (SR) and denoising tasks are fundamental challenges in the field of deep learning, which have traditionally been treated as distinct tasks with separate paired training data. In this paper, we propose an innovative method that addresses both tasks simultaneously using a single deep learning model, eliminating the need for explicitly paired noisy and clean images during training. Our proposed model is primarily trained for SR, but also exhibits remarkable noise-cleaning capabilities in the super-resolved images. Instead of conventional approaches that introduce frequency-related operations into the generative process, our novel approach involves the use of a GAN model guided by a frequency-informed discriminator. To achieve this, we harness the power of the 3D Discrete Wavelet Transform (DWT) operation as a frequency constraint within the GAN framework for the SR task on magnetic resonance imaging (MRI) data. Specifically, our contributions include: 1) a 3D generator based on residual-in-residual connected blocks; 2) the integration of the 3D DWT with $1\times 1$ convolution into a DWT+conv unit within a 3D Unet for the discriminator; 3) the use of the trained model for high-quality image SR, accompanied by an intrinsic denoising process. We dub the model "Denoising Induced Super-resolution GAN (DISGAN)" due to its dual effects of SR image generation and simultaneous denoising. Departing from the traditional approach of training SR and denoising tasks as separate models, our proposed DISGAN is trained only on the SR task, but also achieves exceptional performance in denoising. The model is trained on 3D MRI data from dozens of subjects from the Human Connectome Project (HCP) and further evaluated on previously unseen MRI data from subjects with brain tumours and epilepsy to assess its denoising and SR performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>基于 residual-in-residual 的 3D generator，使用connected块来实现高质量的 SR 图像生成。2. 将 3D DWT 与 1x1  convolution 结合在一起，形成 DWT+conv 单元，并将其 integrate into 3D Unet 中的权重来实现高精度的 SR 预测。3. 使用训练好的模型进行高质量的图像 SR，同时实现了内在的噪声除除过程。我们称这种模型为 “Denoising Induced Super-resolution GAN”（DISGAN），因为它同时实现了 SR 图像生成和噪声除除。不同于传统的方法，我们的 DISGAN 只受 SR 任务培训，同时也可以在未看过的 MRI 数据上实现出色的噪声除除和 SR 性能。我们在 Human Connectome Project（HCP） 提供的3D MRI数据上进行了训练，并在患有脑肿和癫痫的患者的 MRI 数据上进行了评估，以评估其噪声除除和 SR 性能。</details></li>
</ol>
<hr>
<h2 id="StofNet-Super-resolution-Time-of-Flight-Network"><a href="#StofNet-Super-resolution-Time-of-Flight-Network" class="headerlink" title="StofNet: Super-resolution Time of Flight Network"></a>StofNet: Super-resolution Time of Flight Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12009">http://arxiv.org/abs/2308.12009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a></li>
<li>paper_authors: Christopher Hahne, Michel Hayoz, Raphael Sznitman</li>
<li>for: 该论文主要针对时间飞行（ToF）感知技术在 робо测试、医学成像和非 destruktive testing 等领域中的问题，即在复杂的 ambient condition 下，从简单的时间信息中进行逆模拟是不可能的。</li>
<li>methods: 该论文提出了一种现代超解像技术来学习困难 ambient condition，以提高ToF感知的可靠性和准确性。具体来说， authors 提出了一种结合超解像和高效减弱块的架构，以平衡细详信号的细节和大规模的上下文信息。</li>
<li>results: 该论文通过对六种现有方法进行比较，并使用两个公共可用的数据集进行测试，证明了提出的 StofNet 方法在精度、可靠性和模型复杂度三个方面具有显著的优势。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a> 上下载。<details>
<summary>Abstract</summary>
Time of Flight (ToF) is a prevalent depth sensing technology in the fields of robotics, medical imaging, and non-destructive testing. Yet, ToF sensing faces challenges from complex ambient conditions making an inverse modelling from the sparse temporal information intractable. This paper highlights the potential of modern super-resolution techniques to learn varying surroundings for a reliable and accurate ToF detection. Unlike existing models, we tailor an architecture for sub-sample precise semi-global signal localization by combining super-resolution with an efficient residual contraction block to balance between fine signal details and large scale contextual information. We consolidate research on ToF by conducting a benchmark comparison against six state-of-the-art methods for which we employ two publicly available datasets. This includes the release of our SToF-Chirp dataset captured by an airborne ultrasound transducer. Results showcase the superior performance of our proposed StofNet in terms of precision, reliability and model complexity. Our code is available at https://github.com/hahnec/stofnet.
</details>
<details>
<summary>摘要</summary>
时间飞行（ToF）是现代深度探测技术的重要应用领域，包括机器人、医学成像和非 destruktive testing。然而，ToF探测受到环境复杂性的影响，使得反向模型从稀疏的时间信息中做出准确的探测变得困难。本文提出了现代超分解技术的潜在作用，以提高ToF探测的可靠性和准确性。与现有模型不同，我们开发了一种结构，即StofNet，通过结合超分解和高效的剩余压缩块来平衡细信息和大规模的上下文信息。我们在六种state-of-the-art方法的基准比较中，使用了两个公共可用的数据集。这包括我们发布的SToF-Chirp数据集， capture by an airborne ultrasound transducer。结果表明我们提posed StofNet在精度、可靠性和模型复杂度方面表现出色。我们的代码可以在https://github.com/hahnec/stofnet中下载。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Autoencoder-to-Geometrical-Features-for-Vascular-Bifurcations-Identification"><a href="#Comparing-Autoencoder-to-Geometrical-Features-for-Vascular-Bifurcations-Identification" class="headerlink" title="Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification"></a>Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12314">http://arxiv.org/abs/2308.12314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibtissam Essadik, Anass Nouri, Raja Touahni, Florent Autrusseau</li>
<li>for: 这个论文的目的是提出两种基于自动Encoder和几何特征的新方法来识别血管分叉。</li>
<li>methods: 这两种方法分别使用自动Encoder和几何特征来提取特征和识别模式。</li>
<li>results: 经过评估，两种方法在使用医疗影像数据进行血管分叉分类中具有良好的性能和效果，其中自动Encoder方法的准确率和F1分数较高。<details>
<summary>Abstract</summary>
The cerebrovascular tree is a complex anatomical structure that plays a crucial role in the brain irrigation. A precise identification of the bifurcations in the vascular network is essential for understanding various cerebral pathologies. Traditional methods often require manual intervention and are sensitive to variations in data quality. In recent years, deep learning techniques, and particularly autoencoders, have shown promising performances for feature extraction and pattern recognition in a variety of domains. In this paper, we propose two novel approaches for vascular bifurcation identification based respectiveley on Autoencoder and geometrical features. The performance and effectiveness of each method in terms of classification of vascular bifurcations using medical imaging data is presented. The evaluation was performed on a sample database composed of 91 TOF-MRA, using various evaluation measures, including accuracy, F1 score and confusion matrix.
</details>
<details>
<summary>摘要</summary>
脑血管树是一种复杂的生物结构，对脑血液循环具有关键作用。正确地识别血管网络中的分枝是理解脑血液疾病的关键。传统方法经常需要手动干预，并且敏感于数据质量的变化。在最近几年，深度学习技术和特别是自动编码器在多种领域中表现出了扎实的功能。本文提出了两种基于自动编码器和几何特征的血管分枝识别方法。每种方法的性能和效果在使用医疗影像数据进行血管分枝分类中进行了评估，并使用了几种评价指标，包括准确率、F1分数和混淆矩阵。
</details></li>
</ul>
<hr>
<h2 id="Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies"><a href="#Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies" class="headerlink" title="Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies"></a>Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11927">http://arxiv.org/abs/2308.11927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Ye, Yuhang Wang, Hong Zhang, Yiqin Gao, Huan Wang, He Sun</li>
<li>for: 这研究旨在使用liquid-phase electron microscopy（liquid-phase EM）技术观察生物分子的动态变化。</li>
<li>methods: 该研究提出了TEMPOR算法，它是一种基于偶极神经网络（INR）和动态变量自适应器（DVAE）的时间序列分子结构重建方法。</li>
<li>results: 研究人员通过对两个 simulate数据集（7bcq和Cas9）进行测试，发现TEMPOR算法可以有效地回收不同的动态变化。这是首个直接从liquid-phase EM电影中回收动态变化的3D结构的研究，它为结构生物学提供了一个有前途的新方法。<details>
<summary>Abstract</summary>
The dynamics of biomolecules are crucial for our understanding of their functioning in living systems. However, current 3D imaging techniques, such as cryogenic electron microscopy (cryo-EM), require freezing the sample, which limits the observation of their conformational changes in real time. The innovative liquid-phase electron microscopy (liquid-phase EM) technique allows molecules to be placed in the native liquid environment, providing a unique opportunity to observe their dynamics. In this paper, we propose TEMPOR, a Temporal Electron MicroscoPy Object Reconstruction algorithm for liquid-phase EM that leverages an implicit neural representation (INR) and a dynamical variational auto-encoder (DVAE) to recover time series of molecular structures. We demonstrate its advantages in recovering different motion dynamics from two simulated datasets, 7bcq and Cas9. To our knowledge, our work is the first attempt to directly recover 3D structures of a temporally-varying particle from liquid-phase EM movies. It provides a promising new approach for studying molecules' 3D dynamics in structural biology.
</details>
<details>
<summary>摘要</summary>
生物分子动态是我们理解它们在生物系统中功能的关键。然而，现有的3D图像技术，如气化电子顾 microscopy (cryo-EM)，需要采样冻结，限制观察分子 conformational 变化的实时观察。新的液相电子顾 microscopy (liquid-phase EM) 技术可以将分子放在原生液态环境中，提供了观察分子动态的独特机会。在这篇论文中，我们提出了 TEMPOR，一种基于 implicit neural representation (INR) 和动态variational autoencoder (DVAE) 的 Temporal Electron MicroscoPy Object Reconstruction算法，可以从液相电子顾 movie 中回收时间序列的分子结构。我们在两个 simulated 数据集，7bcq 和 Cas9 中，证明了它的优势。到目前为止，我们的工作是直接从液相电子顾 movie 中回收变化的3D结构的第一次尝试。它提供了一种有前途的新方法，用于生物学结构中的分子3D动态研究。
</details></li>
</ul>
<hr>
<h2 id="Studying-the-Impact-of-Augmentations-on-Medical-Confidence-Calibration"><a href="#Studying-the-Impact-of-Augmentations-on-Medical-Confidence-Calibration" class="headerlink" title="Studying the Impact of Augmentations on Medical Confidence Calibration"></a>Studying the Impact of Augmentations on Medical Confidence Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11902">http://arxiv.org/abs/2308.11902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrit Rao, Joon-Young Lee, Oliver Aalami</li>
<li>for: 这 paper 的目的是evaluate the effects of three modern augmentation techniques on the calibration and performance of convolutional neural networks (CNNs) for medical tasks.</li>
<li>methods: 这 paper 使用了三种现代扩展技术：CutMix、MixUp 和 CutOut，以提高 CNNs 的准确率和准确性。</li>
<li>results: 研究发现，CutMix 最大程度地提高了 CNNs 的准确性，而 CutOut 有时会降低准确性。<details>
<summary>Abstract</summary>
The clinical explainability of convolutional neural networks (CNN) heavily relies on the joint interpretation of a model's predicted diagnostic label and associated confidence. A highly certain or uncertain model can significantly impact clinical decision-making. Thus, ensuring that confidence estimates reflect the true correctness likelihood for a prediction is essential. CNNs are often poorly calibrated and prone to overconfidence leading to improper measures of uncertainty. This creates the need for confidence calibration. However, accuracy and performance-based evaluations of CNNs are commonly used as the sole benchmark for medical tasks. Taking into consideration the risks associated with miscalibration is of high importance. In recent years, modern augmentation techniques, which cut, mix, and combine images, have been introduced. Such augmentations have benefited CNNs through regularization, robustness to adversarial samples, and calibration. Standard augmentations based on image scaling, rotating, and zooming, are widely leveraged in the medical domain to combat the scarcity of data. In this paper, we evaluate the effects of three modern augmentation techniques, CutMix, MixUp, and CutOut on the calibration and performance of CNNs for medical tasks. CutMix improved calibration the most while CutOut often lowered the level of calibration.
</details>
<details>
<summary>摘要</summary>
医学预测模型（Convolutional Neural Network，简称CNN）的解释性强调与预测结果和相关的信任度之间的共同解释。一个高度确定或不确定的模型可能会对临床决策产生重大影响。因此，确保模型的信任度反映预测的准确性 likelihood 是关键的。然而， CNNs  oftensuffer from poor calibration and overconfidence, leading to inappropriate measures of uncertainty. This creates the need for confidence calibration. However, accuracy and performance-based evaluations of CNNs are commonly used as the sole benchmark for medical tasks. Considering the risks associated with miscalibration is of high importance.Recently, modern augmentation techniques, such as CutMix, MixUp, and CutOut, have been introduced to improve the calibration and performance of CNNs. These techniques have been shown to benefit CNNs through regularization, robustness to adversarial samples, and calibration. Standard augmentations based on image scaling, rotating, and zooming are widely used in the medical domain to address the scarcity of data. In this paper, we evaluate the effects of these three modern augmentation techniques on the calibration and performance of CNNs for medical tasks. Our results show that CutMix improved calibration the most, while CutOut often lowered the level of calibration.
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Residual-SwinV2-Transformer-for-Learned-Image-Compression"><a href="#Enhanced-Residual-SwinV2-Transformer-for-Learned-Image-Compression" class="headerlink" title="Enhanced Residual SwinV2 Transformer for Learned Image Compression"></a>Enhanced Residual SwinV2 Transformer for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11864">http://arxiv.org/abs/2308.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Wang, Feng Liang, Haisheng Fu, Jie Liang, Haipeng Qin, Junzhe Liang</li>
<li>for: 提高图像压缩的率和质量之间的折衔，并且减少模型复杂度。</li>
<li>methods: 使用改进的差异Swinv2 transformer和特征增强模块，并在编码和超编码步骤中使用SwinV2 transformer-based attention机制。</li>
<li>results: 在Kodak和Tecnick数据集上实现了与一些最新的学习型图像压缩方法相当的性能，并且比一些传统的编码器更高。具体来说，我们的方法在同等性能下减少了56%的模型复杂度。<details>
<summary>Abstract</summary>
Recently, the deep learning technology has been successfully applied in the field of image compression, leading to superior rate-distortion performance. However, a challenge of many learning-based approaches is that they often achieve better performance via sacrificing complexity, which making practical deployment difficult. To alleviate this issue, in this paper, we propose an effective and efficient learned image compression framework based on an enhanced residual Swinv2 transformer. To enhance the nonlinear representation of images in our framework, we use a feature enhancement module that consists of three consecutive convolutional layers. In the subsequent coding and hyper coding steps, we utilize a SwinV2 transformer-based attention mechanism to process the input image. The SwinV2 model can help to reduce model complexity while maintaining high performance. Experimental results show that the proposed method achieves comparable performance compared to some recent learned image compression methods on Kodak and Tecnick datasets, and outperforms some traditional codecs including VVC. In particular, our method achieves comparable results while reducing model complexity by 56% compared to these recent methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-RF-Data-Normalization-for-Deep-Learning"><a href="#Robust-RF-Data-Normalization-for-Deep-Learning" class="headerlink" title="Robust RF Data Normalization for Deep Learning"></a>Robust RF Data Normalization for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11833">http://arxiv.org/abs/2308.11833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz</li>
<li>for: 用于深度神经网络的训练</li>
<li>methods: 使用个体标准化方法更好地利用RF数据</li>
<li>results: 提高深度神经网络的性能和通用性<details>
<summary>Abstract</summary>
Radio frequency (RF) data contain richer information compared to other data types, such as envelope or B-mode, and employing RF data for training deep neural networks has attracted growing interest in ultrasound image processing. However, RF data is highly fluctuating and additionally has a high dynamic range. Most previous studies in the literature have relied on conventional data normalization, which has been adopted within the computer vision community. We demonstrate the inadequacy of those techniques for normalizing RF data and propose that individual standardization of each image substantially enhances the performance of deep neural networks by utilizing the data more efficiently. We compare conventional and proposed normalizations in a phase aberration correction task and illustrate how the former enhances the generality of trained models.
</details>
<details>
<summary>摘要</summary>
radio frequency (RF) 数据含有更多信息，比如拥包或 B-模式数据类型，使用 RF 数据来训练深度神经网络已经吸引了各种各样的关注。然而，RF 数据具有很大的波动和动态范围。大多数先前的文献中的研究都采用了传统的数据Normalization技术。我们证明了这些技术不适用于 RF 数据Normalization，并提出了基于每个图像的个体标准化方法，可以更好地利用数据。我们在相位偏移 corrections 任务中比较了传统和我们提议的Normalization方法，并证明了后者可以提高训练的模型通用性。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Space-Prediction-Filtering-for-Phase-Aberration-Correction-in-Plane-Wave-Ultrasound"><a href="#Frequency-Space-Prediction-Filtering-for-Phase-Aberration-Correction-in-Plane-Wave-Ultrasound" class="headerlink" title="Frequency-Space Prediction Filtering for Phase Aberration Correction in Plane-Wave Ultrasound"></a>Frequency-Space Prediction Filtering for Phase Aberration Correction in Plane-Wave Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11830">http://arxiv.org/abs/2308.11830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz</li>
<li>For: 本研究旨在解决ultrasound imaging中的图像质量下降问题，具体来说是应对phas aberration的影响。* Methods: 本研究使用了frequency-space prediction filtering（FXPF）技术来缓解phas aberration的影响。FXPF假设存在一个自回归（AR）模型，用于描述接收器元素上的信号。* Results: 研究发现，在深度较浅的情况下，使用固定频率AR模型可能会导致图像重建的性能下降。为了解决这个问题，研究提出了一种自适应频率AR模型，并评估了其效果使用对比度和总对比度评价指标。<details>
<summary>Abstract</summary>
Ultrasound imaging often suffers from image degradation stemming from phase aberration, which represents a significant contributing factor to the overall image degradation in ultrasound imaging. Frequency-space prediction filtering or FXPF is a technique that has been applied within focused ultrasound imaging to alleviate the phase aberration effect. It presupposes the existence of an autoregressive (AR) model across the signals received at the transducer elements and removes any components that do not conform to the established model. In this study, we illustrate the challenge of applying this technique to plane-wave imaging, where, at shallower depths, signals from more distant elements lose relevance, and a fewer number of elements contribute to image reconstruction. While the number of contributing signals varies, adopting a fixed-order AR model across all depths, results in suboptimal performance. To address this challenge, we propose an AR model with an adaptive order and quantify its effectiveness using contrast and generalized contrast-to-noise ratio metrics.
</details>
<details>
<summary>摘要</summary>
ultrasound imaging经常受到阶段偏移引起的图像强化效应，这是ultrasound imaging中图像强化效应的重要贡献因素。frequency-space prediction filtering或FXPF是一种在高精度ultrasound imaging中应用的技术，以解决阶段偏移效应。它假设在传感器元素上接收的信号存在autoregressive（AR）模型，并从不符合该模型的组件中除掉噪声。在这种研究中，我们描述了应用FXPF技术到平面波形成像中的挑战，深度较浅的情况下，较远的传感器元素的信号失去了 relevance，而一些元素只能为图像重建做出贡献。尽管参与图像重建的信号数量变化，采用固定阶数AR模型在所有深度下的结果是不佳。为解决这个挑战，我们提议一种AR模型，其阶数随深度变化，并使用对比度和通用对比度-噪声比例度量来衡量其效果。
</details></li>
</ul>
<hr>
<h2 id="WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters"><a href="#WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters" class="headerlink" title="WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters"></a>WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Jack Noble</li>
<li>for: 这个研究旨在建立一个自我超级vised的深度和镜头积极定位系统，能够预测精确的深度地图和镜头积极。</li>
<li>methods: 本研究使用了一种基于成本量的超级vised方法，并通过一种自动生成的类比对照方法来提供辅助的超级vised。</li>
<li>results: 实验结果显示，提案的方法可以改善镜头积极、深度估计和镜头内 Parameters 的预测精度。<details>
<summary>Abstract</summary>
Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the system auxiliary supervision for camera parameters prediction. The experimental results showed that the proposed method improved the accuracy of estimated camera parameters, ego-motion, and depth estimation.
</details>
<details>
<summary>摘要</summary>
深度估计在手术视频中发挥重要作用，但创建深度图真实数据集在手术视频中具有许多挑战，包括手术场景中的不均匀亮度和噪声。因此，建立一个准确和可靠的自我超视导depth和摄像头自身运动估计系统在计算机视觉领域中受到更多的关注。虽然一些自我超视方法可以减少深度图和摄像头pose的需求，但它们仍需要已知的摄像头内参数，这些参数通常缺失或未记录。此外，现有的摄像头内参数预测方法仍然受到数据质量的限制。在这个工作中，我们目的是建立一个可以预测深度图、摄像头pose和摄像头内参数的自我超视depth和摄像头估计系统。我们提议一种基于cost volume的超视束来给系统 auxiliary supervision for camera parameters预测。实验结果表明，我们的方法可以改善摄像头参数、ego-动作和深度估计的准确性。
</details></li>
</ul>
<hr>
<h2 id="EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides"><a href="#EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides" class="headerlink" title="EndoNet: model for automatic calculation of H-score on histological slides"></a>EndoNet: model for automatic calculation of H-score on histological slides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11562">http://arxiv.org/abs/2308.11562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Ushakov, Anton Naumov, Vladislav Fomberg, Polina Vishnyakova, Aleksandra Asaturova, Alina Badlaeva, Anna Tregubova, Evgeny Karpulevich, Gennady Sukhikh, Timur Fatkhudinov</li>
<li>for: 这个论文主要是为了提高检验病理slide的效率和准确性，使用计算机支持的方法来自动计算H-score。</li>
<li>methods: 该论文提出了一种基于神经网络的模型EndoNet，它包括两个主要部分：首先是一个检测模型，用于预测核心点的位置；其次是一个H-score模块，用于根据预测的核心点的平均像素值来计算H-score。</li>
<li>results: 该模型在1780个注解的块中训练和验证，并在测试集上达到了0.77的mAP。此外，该模型可以根据特定的专家或实验室来调整H-score的计算方式，从而提高了模型的可靠性和可重复性。<details>
<summary>Abstract</summary>
H-score is a semi-quantitative method used to assess the presence and distribution of proteins in tissue samples by combining the intensity of staining and percentage of stained nuclei. It is widely used but time-consuming and can be limited in accuracy and precision. Computer-aided methods may help overcome these limitations and improve the efficiency of pathologists' workflows. In this work, we developed a model EndoNet for automatic calculation of H-score on histological slides. Our proposed method uses neural networks and consists of two main parts. The first is a detection model which predicts keypoints of centers of nuclei. The second is a H-score module which calculates the value of the H-score using mean pixel values of predicted keypoints. Our model was trained and validated on 1780 annotated tiles with a shape of 100x100 $\mu m$ and performed 0.77 mAP on a test dataset. Moreover, the model can be adjusted to a specific specialist or whole laboratory to reproduce the manner of calculating the H-score. Thus, EndoNet is effective and robust in the analysis of histology slides, which can improve and significantly accelerate the work of pathologists.
</details>
<details>
<summary>摘要</summary>
“H-score”是一种半量化方法，用于评估组织样本中蛋白质的存在和分布。它广泛使用，但时间费时且准确性和精度有限。计算机助け方法可以帮助解决这些限制，提高病理师的工作效率。在这项工作中，我们开发了一个名为“EndoNet”的自动计算H-score方法。我们的提案方法使用神经网络，包括两个主要部分。第一部分是一个检测模型，预测核心点的位置。第二部分是H-score模块，使用预测的核心点的平均像素值来计算H-score的值。我们的模型在1780个注解的块中训练和验证，在测试集上达到了0.77 mAP。此外，模型可以根据特定的专家或整个实验室来调整计算H-score的方式，因此EndoNet是有效和可靠的 histology 板块分析工具，可以提高和加速病理师的工作。
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Synthetic-Image-Source-Attribution"><a href="#Open-Set-Synthetic-Image-Source-Attribution" class="headerlink" title="Open Set Synthetic Image Source Attribution"></a>Open Set Synthetic Image Source Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11557">http://arxiv.org/abs/2308.11557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbang Fang, Tai D. Nguyen, Matthew C. Stamm</li>
<li>for: 本研究旨在开发一种基于度量学习的开放集成源归属分析方法，以检测和识别新未经见的图像生成器。</li>
<li>methods: 本研究使用度量学习来学习可转移的嵌入，以区分不同的图像生成器。首先将图像分配给候选生成器，然后根据图像与已知生成器学习的参考点的距离来判断是否来自新的生成器。</li>
<li>results: 经过一系列实验，本研究表明了该方法在开放集成源归属场景中能够准确地检测和识别新未经见的图像生成器。<details>
<summary>Abstract</summary>
AI-generated images have become increasingly realistic and have garnered significant public attention. While synthetic images are intriguing due to their realism, they also pose an important misinformation threat. To address this new threat, researchers have developed multiple algorithms to detect synthetic images and identify their source generators. However, most existing source attribution techniques are designed to operate in a closed-set scenario, i.e. they can only be used to discriminate between known image generators. By contrast, new image-generation techniques are rapidly emerging. To contend with this, there is a great need for open-set source attribution techniques that can identify when synthetic images have originated from new, unseen generators. To address this problem, we propose a new metric learning-based approach. Our technique works by learning transferrable embeddings capable of discriminating between generators, even when they are not seen during training. An image is first assigned to a candidate generator, then is accepted or rejected based on its distance in the embedding space from known generators' learned reference points. Importantly, we identify that initializing our source attribution embedding network by pretraining it on image camera identification can improve our embeddings' transferability. Through a series of experiments, we demonstrate our approach's ability to attribute the source of synthetic images in open-set scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/eess.IV_2023_08_23/" data-id="clluro5mr00eqq988fbzv0uus" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/23/eessp.SP_2023_08_23/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eessp.SP - 2023-08-23
        
      </div>
    </a>
  
  
    <a href="/2023/08/22/cs.AI_2023_08_22/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-08-22 20:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
