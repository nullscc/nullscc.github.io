
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-05 17:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10990 repo_url: None paper_authors: Jie Liu, Tao Zhang, Shuyu Sun for: 该研究旨在提出一种无像素的维">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-05 17:00:00">
<meta property="og:url" content="http://example.com/2023/08/05/eess.IV_2023_08_05/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10990 repo_url: None paper_authors: Jie Liu, Tao Zhang, Shuyu Sun for: 该研究旨在提出一种无像素的维">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-04T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:44.358Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/eess.IV_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-05 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flashlight-Search-Medial-Axis-A-Pixel-Free-Pore-Network-Extraction-Algorithm"><a href="#Flashlight-Search-Medial-Axis-A-Pixel-Free-Pore-Network-Extraction-Algorithm" class="headerlink" title="Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm"></a>Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10990">http://arxiv.org/abs/2308.10990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Liu, Tao Zhang, Shuyu Sun</li>
<li>for: 该研究旨在提出一种无像素的维持精度的脉径网络EXTRACTION方法，以便在大规模的含材质媒体中实现流体流动的研究。</li>
<li>methods: 该方法基于探灯搜索中心轴（FSMA）算法，在维持精度的情况下大幅降低计算复杂性，可以应用于多种不同的含材质媒体和流体流动情况。</li>
<li>results: 实验结果表明，FSMA算法可以准确地找到脉径网络，无论含材质媒体的结构如何，也无论脉径和喉径中心的位置如何。此外，该算法还可以检测死端脉径，这对于多相流动在含材质媒体中的研究非常重要。<details>
<summary>Abstract</summary>
Pore-network models (PNMs) have become an important tool in the study of fluid flow in porous media over the last few decades, and the accuracy of their results highly depends on the extraction of pore networks. Traditional methods of pore-network extraction are based on pixels and require images with high quality. Here, a pixel-free method called the flashlight search medial axis (FSMA) algorithm is proposed for pore-network extraction in a continuous space. The search domain in a two-dimensional space is a line, whereas a surface domain is searched in a three-dimensional scenario. Thus, the FSMA algorithm follows the dimensionality reduction idea; the medial axis can be identified using only a few points instead of calculating every point in the void space. In this way, computational complexity of this method is greatly reduced compared to that of traditional pixel-based extraction methods, thus enabling large-scale pore-network extraction. Based on cases featuring two- and three-dimensional porous media, the FSMA algorithm performs well regardless of the topological structure of the pore network or the positions of the pore and throat centers. This algorithm can also be used to examine both closed- and open-boundary cases. Finally, the FSMA algorithm can search dead-end pores, which is of great significance in the study of multiphase flow in porous media.
</details>
<details>
<summary>摘要</summary>
PORE-NETWORK MODELS (PNMs) 已经在过去几十年内成为论 fluid flow 在孔隙媒体的研究工具，而实际结果的准确性很大程度上取决于破孔网络的提取。传统的破孔网络提取方法基于像素，需要高质量的图像。在这里，一种没有像素的方法 called  flashlight search medial axis (FSMA) 算法是用于破孔网络提取的。在两维空间中，搜索域是一条直线，而在三维情况下，搜索域是一个表面。因此，FSMA 算法遵循缩放空间的想法，通过仅在缺空间中标识中点而不是计算所有点。这样，FSMA 算法的计算复杂性被大幅降低，从而实现大规模的破孔网络提取。无论破孔网络的结构是二维还是三维，FSMA 算法都能够正确地提取破孔网络。此外，该算法还可以处理封闭边界和开放边界两种情况。最后，FSMA 算法还可以搜索死绕孔，这对多相流在孔隙媒体的研究非常重要。
</details></li>
</ul>
<hr>
<h2 id="Landmark-Detection-using-Transformer-Toward-Robot-assisted-Nasal-Airway-Intubation"><a href="#Landmark-Detection-using-Transformer-Toward-Robot-assisted-Nasal-Airway-Intubation" class="headerlink" title="Landmark Detection using Transformer Toward Robot-assisted Nasal Airway Intubation"></a>Landmark Detection using Transformer Toward Robot-assisted Nasal Airway Intubation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02845">http://arxiv.org/abs/2308.02845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/conorlth/airway_intubation_landmarks_detection">https://github.com/conorlth/airway_intubation_landmarks_detection</a></li>
<li>paper_authors: Tianhang Liu, Hechen Li, Long Bai, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren</li>
<li>for: 该研究旨在提供一种高精度的自动标记检测方法，用于机器人协助的鼻腔插管。</li>
<li>methods: 该方法基于变换器（DeTR），并采用了可变DeTR和语义对齐匹配模块来检测鼻腔中的两个重要标记（鼻孔和肺膜）。</li>
<li>results: 实验结果表明，该方法可以具有竞争力的检测精度。<details>
<summary>Abstract</summary>
Robot-assisted airway intubation application needs high accuracy in locating targets and organs. Two vital landmarks, nostrils and glottis, can be detected during the intubation to accommodate the stages of nasal intubation. Automated landmark detection can provide accurate localization and quantitative evaluation. The Detection Transformer (DeTR) leads object detectors to a new paradigm with long-range dependence. However, current DeTR requires long iterations to converge, and does not perform well in detecting small objects. This paper proposes a transformer-based landmark detection solution with deformable DeTR and the semantic-aligned-matching module for detecting landmarks in robot-assisted intubation. The semantics aligner can effectively align the semantics of object queries and image features in the same embedding space using the most discriminative features. To evaluate the performance of our solution, we utilize a publicly accessible glottis dataset and automatically annotate a nostril detection dataset. The experimental results demonstrate our competitive performance in detection accuracy. Our code is publicly accessible.
</details>
<details>
<summary>摘要</summary>
机器人协助气管插管应用需要高精度在目标和器官的位置检测。气管插管过程中可以检测到两个重要的特征点，即鼻孔和软颈椎。自动化特征点检测可以提供高精度的位置定位和量化评估。 however， current DeTR requires long iterations to converge, and does not perform well in detecting small objects. This paper proposes a transformer-based landmark detection solution with deformable DeTR and the semantic-aligned-matching module for detecting landmarks in robot-assisted intubation. The semantics aligner can effectively align the semantics of object queries and image features in the same embedding space using the most discriminative features. To evaluate the performance of our solution, we utilize a publicly accessible glottis dataset and automatically annotate a nostril detection dataset. The experimental results demonstrate our competitive performance in detection accuracy. Our code is publicly accessible.Here's the translation in Traditional Chinese:机器人协助气管插管应用需要高精度在目标和器官的位置检测。气管插管过程中可以检测到两个重要的特征点，即鼻孔和软颈椎。自动化特征点检测可以提供高精度的位置定位和量化评估。然而， current DeTR需要长迭代才能融合，并不能够检测小型物体。这篇论文提出了一个基于 transformer 的特征点检测解决方案，该解决方案包括扭转 DeTR 和对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩�
</details></li>
</ul>
<hr>
<h2 id="Non-line-of-sight-reconstruction-via-structure-sparsity-regularization"><a href="#Non-line-of-sight-reconstruction-via-structure-sparsity-regularization" class="headerlink" title="Non-line-of-sight reconstruction via structure sparsity regularization"></a>Non-line-of-sight reconstruction via structure sparsity regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02782">http://arxiv.org/abs/2308.02782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duolan Huang, Quan Chen, Zhun Wei, Rui Chen</li>
<li>for: 本研究旨在提高非线视场（NLOS）成像质量，使其能够应用于自动驾驶、机器人视觉、医疗成像、安全监测等领域。</li>
<li>methods: 本研究使用了结构稀热（SS）正则化方法，通过利用方向光束变换（DLCT）模型中的核心矩阵来捕捉方向性的隐藏信息，从而提高NLOS成像的稀热性。</li>
<li>results: 经过实验和synthetic数据的评估，提出的方法可以在短时间和低SNR情况下提供高质量的NLOS成像，并且超过了现有的重建算法，特别是在封闭物体探测方面表现出色。<details>
<summary>Abstract</summary>
Non-line-of-sight (NLOS) imaging allows for the imaging of objects around a corner, which enables potential applications in various fields such as autonomous driving, robotic vision, medical imaging, security monitoring, etc. However, the quality of reconstruction is challenged by low signal-noise-ratio (SNR) measurements. In this study, we present a regularization method, referred to as structure sparsity (SS) regularization, for denoising in NLOS reconstruction. By exploiting the prior knowledge of structure sparseness, we incorporate nuclear norm penalization into the cost function of directional light-cone transform (DLCT) model for NLOS imaging system. This incorporation effectively integrates the neighborhood information associated with the directional albedo, thereby facilitating the denoising process. Subsequently, the reconstruction is achieved by optimizing a directional albedo model with SS regularization using fast iterative shrinkage-thresholding algorithm. Notably, the robust reconstruction of occluded objects is observed. Through comprehensive evaluations conducted on both synthetic and experimental datasets, we demonstrate that the proposed approach yields high-quality reconstructions, surpassing the state-of-the-art reconstruction algorithms, especially in scenarios involving short exposure and low SNR measurements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dual-Degradation-Inspired-Deep-Unfolding-Network-for-Low-Light-Image-Enhancement"><a href="#Dual-Degradation-Inspired-Deep-Unfolding-Network-for-Low-Light-Image-Enhancement" class="headerlink" title="Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement"></a>Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02776">http://arxiv.org/abs/2308.02776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huake Wang, Xingsong Hou, Xiaoyang Yan<br>for:这篇论文的目的是提出一种基于深度恢复模型的低光照图像改进方法，以解决现有的低光照图像恢复方法往往强调黑盒网络的增强性能，而忽略了图像恢复模型的物理意义。methods:该论文提出了一种基于双层降低模型的深度 unfolding 网络（DASUNet），其中包括了构建双层降低模型（DDM），以便显式地模拟低光照图像的劣化机制。DDM 学习了两个不同的图像假设，通过考虑颜色和灰度空间中的降低特点来进行适应。为使提议方案可行，我们设计了一种交叉优化解决方案，并将其拓展到一个具体的深度网络中，以形成 DASUNet。results:对多个流行的低光照图像dataset进行了广泛的实验，并证明了 DASUNet 比 canon 状态的低光照图像恢复方法更有效。我们将源代码和预训练模型公开发布。<details>
<summary>Abstract</summary>
Although low-light image enhancement has achieved great stride based on deep enhancement models, most of them mainly stress on enhancement performance via an elaborated black-box network and rarely explore the physical significance of enhancement models. Towards this issue, we propose a Dual degrAdation-inSpired deep Unfolding network, termed DASUNet, for low-light image enhancement. Specifically, we construct a dual degradation model (DDM) to explicitly simulate the deterioration mechanism of low-light images. It learns two distinct image priors via considering degradation specificity between luminance and chrominance spaces. To make the proposed scheme tractable, we design an alternating optimization solution to solve the proposed DDM. Further, the designed solution is unfolded into a specified deep network, imitating the iteration updating rules, to form DASUNet. Local and long-range information are obtained by prior modeling module (PMM), inheriting the advantages of convolution and Transformer, to enhance the representation capability of dual degradation priors. Additionally, a space aggregation module (SAM) is presented to boost the interaction of two degradation models. Extensive experiments on multiple popular low-light image datasets validate the effectiveness of DASUNet compared to canonical state-of-the-art low-light image enhancement methods. Our source code and pretrained model will be publicly available.
</details>
<details>
<summary>摘要</summary>
尽管深度修剪模型在低光照图像改善方面已经取得了很大的进步，但大多数模型都强调了修剪性能的提高，而很少探讨修剪模型的物理意义。为了解决这个问题，我们提出了一种名为DASUNet的深度 unfolding 网络，用于低光照图像改善。具体来说，我们构建了一个双层降低模型（DDM），以显式地模拟低光照图像的衰化机制。DDM 学习了两个不同的图像假设，通过考虑颜色和灰度空间之间的特定降低特征来进行适应。为了使我们的方案可行，我们设计了一种交叉优化解决方案，并将其 unfolding 成一个具体的深度网络，以形成DASUNet。PMM 模块（假设模型）在维护本地和长距离信息的同时，继承了 convolution 和 Transformer 的优点，以提高修剪两个假设的表示能力。此外，我们还提出了一种空间聚合模块（SAM），以提高两个降低模型之间的交互。我们在多个流行的低光照图像数据集上进行了广泛的实验，并证明了 DASUNet 的效果比 canonical 状态的低光照图像修剪方法更高。我们将源代码和预训练模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Incorporation-of-Eye-Tracking-and-Gaze-Feedback-to-Characterize-and-Improve-Radiologist-Search-Patterns-of-Chest-X-rays-A-Randomized-Controlled-Clinical-Trial"><a href="#Incorporation-of-Eye-Tracking-and-Gaze-Feedback-to-Characterize-and-Improve-Radiologist-Search-Patterns-of-Chest-X-rays-A-Randomized-Controlled-Clinical-Trial" class="headerlink" title="Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial"></a>Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06280">http://arxiv.org/abs/2308.06280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carolina Ramirez-Tamayo, Syed Hasib Akhter Faruqui, Stanford Martinez, Angel Brisco, Nicholas Czarnek, Adel Alaeddini, Jeffrey R. Mock, Edward J. Golob, Kal L. Clark</li>
<li>for:  This study aimed to improve the accuracy of radiologists in detecting suspicious pulmonary nodules.</li>
<li>methods: The study used eye-tracking technology to analyze radiologists’ search patterns and provided automated feedback to the intervention group.</li>
<li>results: The intervention group showed a 38.89% absolute improvement in detecting suspicious-for-cancer nodules compared to the control group, with improvement observed in all four training sessions.Here’s the text in Simplified Chinese:</li>
<li>for: 这项研究旨在提高胸部肿瘤检测中的医生准确率。</li>
<li>methods: 该研究使用眼动跟踪技术分析医生的搜寻模式，并对参与实验组提供自动反馈。</li>
<li>results: 参与实验组对可疑肿瘤的检测精度有38.89%的绝对提升，比控制组的改善（5.56%）显著（p值&#x3D;0.006），并在四个训练会议中持续改善（p值&#x3D;0.0001）。<details>
<summary>Abstract</summary>
Diagnostic errors in radiology often occur due to incomplete visual assessments by radiologists, despite their knowledge of predicting disease classes. This insufficiency is possibly linked to the absence of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.
</details>
<details>
<summary>摘要</summary>
radiologists  oftentimes make diagnostic errors due to inadequate visual assessments, despite their knowledge of predicting disease classes. This deficiency may be linked to the lack of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/05/eess.IV_2023_08_05/" data-id="cllsk9gs500959c887wcw5ldw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/05/cs.SD_2023_08_05/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-05 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/04/cs.LG_2023_08_04/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-04 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
