
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-05 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Edge of stability echo state networks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02902 repo_url: None paper_authors: Andrea Ceni, Claudio Gallicchio for: 这个论文提出了一种新的储存计算（Reservoir Computing，RC）架构，称为Edge of">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-05 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/05/cs.LG_2023_08_05/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Edge of stability echo state networks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02902 repo_url: None paper_authors: Andrea Ceni, Claudio Gallicchio for: 这个论文提出了一种新的储存计算（Reservoir Computing，RC）架构，称为Edge of">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-04T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:25.703Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/cs.LG_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-05 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Edge-of-stability-echo-state-networks"><a href="#Edge-of-stability-echo-state-networks" class="headerlink" title="Edge of stability echo state networks"></a>Edge of stability echo state networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02902">http://arxiv.org/abs/2308.02902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Ceni, Claudio Gallicchio</li>
<li>for: 这个论文提出了一种新的储存计算（Reservoir Computing，RC）架构，称为Edge of Stability Echo State Network（ES$^2$N）。</li>
<li>methods: 该模型基于定义储存层为非线性储存（如标准ESN）和线性储存（实现正交变换）的concat。 我们提供了整个数学分析，证明ES2N map的Jacobian的全谱特征在一个可控的圆锥形范围内，并利用这个性质来证明ES$^2$N的前向动力系统在设计的边缘混乱 режиobe动。</li>
<li>results: 我们的实验分析显示，新引入的储存模型可以达到理论上的最大短期记忆容量。同时，相比标准ESN，ES$^2$N具有更好的记忆和非线性之间的融合，以及在推理非线性模型方面的显著改进。<details>
<summary>Abstract</summary>
In this paper, we propose a new Reservoir Computing (RC) architecture, called the Edge of Stability Echo State Network (ES$^2$N). The introduced ES$^2$N model is based on defining the reservoir layer as a convex combination of a nonlinear reservoir (as in the standard ESN), and a linear reservoir that implements an orthogonal transformation. We provide a thorough mathematical analysis of the introduced model, proving that the whole eigenspectrum of the Jacobian of the ES2N map can be contained in an annular neighbourhood of a complex circle of controllable radius, and exploit this property to demonstrate that the ES$^2$N's forward dynamics evolves close to the edge-of-chaos regime by design. Remarkably, our experimental analysis shows that the newly introduced reservoir model is able to reach the theoretical maximum short-term memory capacity. At the same time, in comparison to standard ESN, ES$^2$N is shown to offer a favorable trade-off between memory and nonlinearity, as well as a significant improvement of performance in autoregressive nonlinear modeling.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的抽象计算（Reservoir Computing，RC）架构，称为边缘稳定声纳网络（ES$^2$N）。我们的ES$^2$N模型基于定义储存层为非线性储存（如标准ESN）和线性储存实现正交变换的混合体。我们对引入的模型进行了深入的数学分析，证明整个特征值谱可以在控制的圆盘内含义，并利用这个性质来证明ES$^2$N的前向动力系统在设计上靠近边缘混乱 режи宜。在实验中，我们发现新引入的储存模型能够达到理论上的最大短期记忆容量。同时，相比标准ESN，ES$^2$N表现出了更好的记忆与非线性之间的质量协议，以及在抽象非线性模型中显著的性能改善。
</details></li>
</ul>
<hr>
<h2 id="Textual-Data-Mining-for-Financial-Fraud-Detection-A-Deep-Learning-Approach"><a href="#Textual-Data-Mining-for-Financial-Fraud-Detection-A-Deep-Learning-Approach" class="headerlink" title="Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach"></a>Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03800">http://arxiv.org/abs/2308.03800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuru Li</li>
<li>for: 这个研究旨在使用深度学习方法进行自然语言处理（NLP）Binary分类任务，以分析金融诈骗文本。</li>
<li>methods: 我使用了多种神经网络模型，包括多层权重层、vanilla RNN、LSTM和GRU来进行文本分类任务。</li>
<li>results: 我的结果表明，使用这些多种神经网络模型可以提高金融诈骗检测的准确率，这些结果对于金融诈骗检测有着重要的意义，并为业界实践者、监管机构和研究人员提供有价值的洞察。<details>
<summary>Abstract</summary>
In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuable insights for industry practitioners, regulators, and researchers in the pursuit of more robust and effective fraud detection methodologies.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我使用深度学习方法来进行自然语言处理（以下简称 NLP）的二分类任务，以分析金融诈骗文本。首先，我从香港证券交易所新闻中搜索了规范公告和执行通知，以定义诈骗公司并提取其财务报告。然后，我将报告中的句子分为标签和时间排序。我的方法包括多层感知器、普通逻辑神经网络、长短期记忆网络（LSTM）和闭合逻辑Unit（GRU）等不同类型的神经网络模型，用于文本分类任务。通过利用这些多样化的模型，我希望能够对金融诈骗检测的准确率进行全面的比较。我的结果对金融诈骗检测具有重要的意义，这项工作将加入深度学习、NLP和金融之间的交叉领域的研究中，为业内专业人士、监管部门和研究人员提供价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Elucidate-Gender-Fairness-in-Singing-Voice-Transcription"><a href="#Elucidate-Gender-Fairness-in-Singing-Voice-Transcription" class="headerlink" title="Elucidate Gender Fairness in Singing Voice Transcription"></a>Elucidate Gender Fairness in Singing Voice Transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02898">http://arxiv.org/abs/2308.02898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guxm2021/svt_speechbrain">https://github.com/guxm2021/svt_speechbrain</a></li>
<li>paper_authors: Xiangming Gu, Wei Zeng, Ye Wang</li>
<li>for:  investigate the performance disparity in singing voice transcription (SVT) between males and females, and propose a method to address the gender bias.</li>
<li>methods: use an attribute predictor to predict gender labels and adversarially train the SVT system to enforce the gender-invariance of acoustic representations, conditionally align acoustic representations between demographic groups by feeding note events to the attribute predictor.</li>
<li>results: significant reduction of gender bias (up to more than 50%) with negligible degradation of overall SVT performance, on both in-domain and out-of-domain singing data, offering a better fairness-utility trade-off.<details>
<summary>Abstract</summary>
It is widely known that males and females typically possess different sound characteristics when singing, such as timbre and pitch, but it has never been explored whether these gender-based characteristics lead to a performance disparity in singing voice transcription (SVT), whose target includes pitch. Such a disparity could cause fairness issues and severely affect the user experience of downstream SVT applications. Motivated by this, we first demonstrate the female superiority of SVT systems, which is observed across different models and datasets. We find that different pitch distributions, rather than gender data imbalance, contribute to this disparity. To address this issue, we propose using an attribute predictor to predict gender labels and adversarially training the SVT system to enforce the gender-invariance of acoustic representations. Leveraging the prior knowledge that pitch distributions may contribute to the gender bias, we propose conditionally aligning acoustic representations between demographic groups by feeding note events to the attribute predictor. Empirical experiments on multiple benchmark SVT datasets show that our method significantly reduces gender bias (up to more than 50%) with negligible degradation of overall SVT performance, on both in-domain and out-of-domain singing data, thus offering a better fairness-utility trade-off.
</details>
<details>
<summary>摘要</summary>
广泛知道，男女在唱歌时通常具有不同的音色特征，如音 timbre 和音高，但这些 gender-based 特征是否会导致唱歌voice transcription（SVT）中的性别偏袋问题？如果存在这种偏袋问题，那么这将导致 fairness 问题并且严重地影响下游 SVT 应用程序的用户体验。为了解决这个问题，我们首先示出了女性 SVT 系统的优势，这种优势可以在不同的模型和数据集上被观察到。我们发现，不同的投射分布，而不是性别数据不均衡，是导致这种偏袋问题的主要原因。为了解决这个问题，我们提议使用一个 attribute predictor 来预测性别标签，并在 SVT 系统中进行对 gender-invariance 的 adversarial 训练。利用投射分布可能会导致性别偏袋的知识，我们提议通过将 note events 传递给 attribute predictor，来Conditional 地将音频表示同步。我们的方法在多个标准 SVT 数据集上进行了实验，结果显示，我们的方法可以减少性别偏袋（达到50%以上），同时不会影响 SVT 总性能，从而提供了更好的 fairness-utility 交易。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-Gaussian-process-model-for-Euler-Bernoulli-beam-elements"><a href="#Physics-informed-Gaussian-process-model-for-Euler-Bernoulli-beam-elements" class="headerlink" title="Physics-informed Gaussian process model for Euler-Bernoulli beam elements"></a>Physics-informed Gaussian process model for Euler-Bernoulli beam elements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02894">http://arxiv.org/abs/2308.02894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gledson Rodrigo Tondo, Sebastian Rau, Igor Kavrakov, Guido Morgenthal</li>
<li>for: 这个论文是用于开发一种基于物理学习的、多输出泛化过程的机器学习模型，用于估计结构的弯矩稳定性。</li>
<li>methods: 这个模型使用了欧拉-伯涅瓦梁式方程，并通过适当的数据集来训练。</li>
<li>results: 模型可以用来描述结构的弯矩稳定性，进行 interpolate 和 probabilistic 推断，并在结构健康监测中使用 Mahalanobis 距离来评估结构系统中可能的损害的位置和范围。<details>
<summary>Abstract</summary>
A physics-informed machine learning model, in the form of a multi-output Gaussian process, is formulated using the Euler-Bernoulli beam equation. Given appropriate datasets, the model can be used to regress the analytical value of the structure's bending stiffness, interpolate responses, and make probabilistic inferences on latent physical quantities. The developed model is applied on a numerically simulated cantilever beam, where the regressed bending stiffness is evaluated and the influence measurement noise on the prediction quality is investigated. Further, the regressed probabilistic stiffness distribution is used in a structural health monitoring context, where the Mahalanobis distance is employed to reason about the possible location and extent of damage in the structural system. To validate the developed framework, an experiment is conducted and measured heterogeneous datasets are used to update the assumed analytical structural model.
</details>
<details>
<summary>摘要</summary>
一种physics-informed机器学习模型，具体来说是一种多输出 Gaussian process，基于Euler-Bernoulli梁式方程。给定合适的数据集，该模型可以用来回归分析结构的弯矩稳定性， interpolate 响应，以及进行 probabilistic 推断 Physical quantity 的存在。该模型在 numerically simulated  cantilever beam 上进行应用，其中推断的弯矩稳定性被评估，并investigate 测量噪声对预测质量的影响。此外，推断的 probabilistic 弯矩分布被用于结构健康监测上，通过 Mahalanobis distance 来了解结构系统中可能的损害位置和范围。为验证开发的框架，进行了实验，并使用测量的 hetereogeneous 数据集来更新假设的分析结构模型。
</details></li>
</ul>
<hr>
<h2 id="Secure-Deep-JSCC-Against-Multiple-Eavesdroppers"><a href="#Secure-Deep-JSCC-Against-Multiple-Eavesdroppers" class="headerlink" title="Secure Deep-JSCC Against Multiple Eavesdroppers"></a>Secure Deep-JSCC Against Multiple Eavesdroppers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02892">http://arxiv.org/abs/2308.02892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyyed Amirhossein Ameli Kalkhoran, Mehdi Letafati, Ecenaz Erdemir, Babak Hossein Khalaj, Hamid Behroozi, Deniz Gündüz</li>
<li>for: 这个研究旨在提出一个基于深度学习的综合式通信安全方法，以保护传输过程中的私人资讯免受多名听者的窃取和探索。</li>
<li>methods: 这个方法使用深度学习的综合式模型，实现了一个数据驱动的安全通信方案，不需要对数据分布进行假设。</li>
<li>results: 实验结果显示，这个方法可以降低听者的攻击精度，对于不同的测试渠道（Rayleigh fading、Nakagami-m、AWGN）进行了评估。<details>
<summary>Abstract</summary>
In this paper, a generalization of deep learning-aided joint source channel coding (Deep-JSCC) approach to secure communications is studied. We propose an end-to-end (E2E) learning-based approach for secure communication against multiple eavesdroppers over complex-valued fading channels. Both scenarios of colluding and non-colluding eavesdroppers are studied. For the colluding strategy, eavesdroppers share their logits to collaboratively infer private attributes based on ensemble learning method, while for the non-colluding setup they act alone. The goal is to prevent eavesdroppers from inferring private (sensitive) information about the transmitted images, while delivering the images to a legitimate receiver with minimum distortion. By generalizing the ideas of privacy funnel and wiretap channel coding, the trade-off between the image recovery at the legitimate node and the information leakage to the eavesdroppers is characterized. To solve this secrecy funnel framework, we implement deep neural networks (DNNs) to realize a data-driven secure communication scheme, without relying on a specific data distribution. Simulations over CIFAR-10 dataset verifies the secrecy-utility trade-off. Adversarial accuracy of eavesdroppers are also studied over Rayleigh fading, Nakagami-m, and AWGN channels to verify the generalization of the proposed scheme. Our experiments show that employing the proposed secure neural encoding can decrease the adversarial accuracy by 28%.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了深度学习帮助的同时源渠道编码（Deep-JSCC）方法的扩展，以确保通信的安全性。我们提议一种终端到终端（E2E）学习基于的安全通信方法，用于对多个伪装者进行安全通信。我们研究了协作和不协作的情况下的伪装者。在协作情况下，伪装者共享其логиits来共同推理私有特征，而在不协作情况下，他们 acted alone。我们的目标是防止伪装者推理传输的图像中的私有（敏感）信息，同时将图像传输到合法接收器，并最小化干扰。通过扩展隐私管道和窃听渠道编码的想法，我们研究了图像恢复和伪装者信息泄露之间的质量负担。为解决这个隐私管道框架，我们使用深度神经网络（DNNs）来实现数据驱动的安全通信方案，不需要固定数据分布。我们的实验结果表明，通过使用我们的安全神经编码，可以降低伪装者的攻击精度，减少了28%。我们还对伪装者的攻击精度进行了随机抽样和AWGN渠道的研究，以验证我们的方案的普适性。
</details></li>
</ul>
<hr>
<h2 id="Private-Federated-Learning-with-Dynamic-Power-Control-via-Non-Coherent-Over-the-Air-Computation"><a href="#Private-Federated-Learning-with-Dynamic-Power-Control-via-Non-Coherent-Over-the-Air-Computation" class="headerlink" title="Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation"></a>Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02881">http://arxiv.org/abs/2308.02881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anbang Zhang, Shuaishuai Guo, Shuai Liu</li>
<li>for: 提高 Federated Learning（FL）中模型保护和性能提高的方法</li>
<li>methods: 基于动态功率控制的Over-the-Air Computation（AirComp）方案</li>
<li>results: 可以 Mitigate 时间同步错误、通道抑降和噪声的影响，并提供了理论上的整合证明。<details>
<summary>Abstract</summary>
To further preserve model weight privacy and improve model performance in Federated Learning (FL), FL via Over-the-Air Computation (AirComp) scheme based on dynamic power control is proposed. The edge devices (EDs) transmit the signs of local stochastic gradients by activating two adjacent orthogonal frequency division multi-plexing (OFDM) subcarriers, and majority votes (MVs) at the edge server (ES) are obtained by exploiting the energy accumulation on the subcarriers. Then, we propose a dynamic power control algorithm to further offset the biased aggregation of the MV aggregation values. We show that the whole scheme can mitigate the impact of the time synchronization error, channel fading and noise. The theoretical convergence proof of the scheme is re-derived.
</details>
<details>
<summary>摘要</summary>
为了进一步保护模型权重私钥和改进 Federated Learning（FL）的性能，我们提出了基于动态功率控制的 Federated Learning via Over-the-Air Computation（AirComp）方案。 Edge devices（ED）通过活动两个邻近的正交频分多普逊（OFDM）子频，将本地随机梯度签名发送到 Edge server（ES），然后通过利用频分的能量积累实现多数投票（MV）。然后，我们提出了一种动态功率控制算法，以更正偏好的MV汇聚值的偏好。我们证明了整个方案可以减轻时间同步错误、通道抑降和噪声的影响。我们重新证明了方案的理论收敛证明。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-in-healthcare-A-survey"><a href="#Meta-learning-in-healthcare-A-survey" class="headerlink" title="Meta-learning in healthcare: A survey"></a>Meta-learning in healthcare: A survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02877">http://arxiv.org/abs/2308.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Rafiei, Ronald Moore, Sina Jahromi, Farshid Hajati, Rishikesan Kamaleswaran</li>
<li>for: This paper aims to explore the applications of meta-learning in the healthcare domain and provide insights into how it can address critical healthcare challenges.</li>
<li>methods: The paper discusses the theoretical foundations and pivotal methods of meta-learning, including multi&#x2F;single-task learning and many&#x2F;few-shot learning.</li>
<li>results: The paper surveys various studies that have applied meta-learning in the healthcare domain and highlights the current challenges in meta-learning research, as well as potential solutions and future perspectives.<details>
<summary>Abstract</summary>
As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning and survey the studies. Finally, we highlight the current challenges in meta-learning research, discuss the potential solutions and provide future perspectives on meta-learning in healthcare.
</details>
<details>
<summary>摘要</summary>
为一种机器学习子领域，meta-learning，或学习学习，旨在提高模型的能力，通过使用先前知识和经验。meta-learning概念可以有效地解决传统学习方法的常见挑战，如样本不够、领域变化和泛化。这些特点使得meta-learning成为在医疗领域开发影响力强的解决方案的适用场景。本文首先介绍了meta-learning的理论基础和关键方法，然后将在医疗领域使用的meta-learning方法分为两个主要类别：多/单任务学习和多/少射学习，并对相关研究进行概述。最后，我们描述了当前meta-learning研究中的挑战，讨论了可能的解决方案，并提供了未来meta-learning在医疗领域的前景。
</details></li>
</ul>
<hr>
<h2 id="Data-Based-Design-of-Multi-Model-Inferential-Sensors"><a href="#Data-Based-Design-of-Multi-Model-Inferential-Sensors" class="headerlink" title="Data-Based Design of Multi-Model Inferential Sensors"></a>Data-Based Design of Multi-Model Inferential Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02872">http://arxiv.org/abs/2308.02872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Mojto, Karol Lubušký, Miroslav Fikar, Radoslav Paulen</li>
<li>for: 这篇论文关注软感知设计问题，旨在提高软感知仪器的预测性能。</li>
<li>methods: 本文提出了两种新的软感知设计方法，以提高软感知仪器的预测性能，并且可以维护其线性结构。</li>
<li>results: 对于一个实际的燃气油氢化单元，比较了多种单一模型软感知仪器和当前 referential 软感知仪器，结果表明了新方法的显著提高。<details>
<summary>Abstract</summary>
This paper deals with the problem of inferential (soft) sensor design. The nonlinear character of industrial processes is usually the main limitation to designing simple linear inferential sensors with sufficient accuracy. In order to increase the inferential sensor predictive performance and yet to maintain its linear structure, multi-model inferential sensors represent a straightforward option. In this contribution, we propose two novel approaches for the design of multi-model inferential sensors aiming to mitigate some drawbacks of the state-of-the-art approaches. For a demonstration of the developed techniques, we design inferential sensors for a Vacuum Gasoil Hydrogenation unit, which is a real-world petrochemical refinery unit. The performance of the multi-model inferential sensor is compared against various single-model inferential sensors and the current (referential) inferential sensor used in the refinery. The results show substantial improvements over the state-of-the-art design techniques for single-/multi-model inferential sensors.
</details>
<details>
<summary>摘要</summary>
In this study, we propose two new methods for designing multi-model inferential sensors. We demonstrate the effectiveness of these methods using a real-world petrochemical refinery unit, the vacuum gasoil hydrogenation unit. The performance of the multi-model inferential sensor is compared to various single-model inferential sensors and the current referential inferential sensor used in the refinery. The results show significant improvements over existing design techniques for single-/multi-model inferential sensors.The key contributions of this paper are:1. Two novel approaches for designing multi-model inferential sensors that mitigate some drawbacks of state-of-the-art techniques.2. A demonstration of the effectiveness of the proposed methods using a real-world petrochemical refinery unit.3. Comparison of the performance of the multi-model inferential sensor with various single-model inferential sensors and the current referential inferential sensor used in the refinery.The rest of the paper is organized as follows: Section 2 reviews the related work on soft sensors and multi-model inferential sensors. Section 3 describes the proposed methods for designing multi-model inferential sensors. Section 4 presents the case study of the vacuum gasoil hydrogenation unit. Section 5 compares the performance of the multi-model inferential sensor with other approaches. Finally, Section 6 concludes the paper and highlights future research directions.
</details></li>
</ul>
<hr>
<h2 id="NP-SemiSeg-When-Neural-Processes-meet-Semi-Supervised-Semantic-Segmentation"><a href="#NP-SemiSeg-When-Neural-Processes-meet-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation"></a>NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02866">http://arxiv.org/abs/2308.02866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianf-wang/np-semiseg">https://github.com/jianf-wang/np-semiseg</a></li>
<li>paper_authors: Jianfeng Wang, Daniela Massiceti, Xiaolin Hu, Vladimir Pavlovic, Thomas Lukasiewicz</li>
<li>for: 这种方法用于 assigning pixel-wise labels to unlabeled images at training time, 有助于减少标注成本和提高 segmentation 的准确率。</li>
<li>methods: 该方法使用 neural processes (NPs) for uncertainty quantification, and adapts NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg.</li>
<li>results: 实验结果表明，NP-SemiSeg 在 PASCAL VOC 2012 和 Cityscapes 上的公共benchmark上，在不同的训练设置下，具有显著的效果。<details>
<summary>Abstract</summary>
Semi-supervised semantic segmentation involves assigning pixel-wise labels to unlabeled images at training time. This is useful in a wide range of real-world applications where collecting pixel-wise labels is not feasible in time or cost. Current approaches to semi-supervised semantic segmentation work by predicting pseudo-labels for each pixel from a class-wise probability distribution output by a model. If the predicted probability distribution is incorrect, however, this leads to poor segmentation results, which can have knock-on consequences in safety critical systems, like medical images or self-driving cars. It is, therefore, important to understand what a model does not know, which is mainly achieved by uncertainty quantification. Recently, neural processes (NPs) have been explored in semi-supervised image classification, and they have been a computationally efficient and effective method for uncertainty quantification. In this work, we move one step forward by adapting NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg. We experimentally evaluated NP-SemiSeg on the public benchmarks PASCAL VOC 2012 and Cityscapes, with different training settings, and the results verify its effectiveness.
</details>
<details>
<summary>摘要</summary>
semi-supervised semantic segmentation是将不带标签的图像分类为不同类别的过程。这有很多实际应用场景，例如医疗图像或自动驾驶车辆，因为收集标签是不可能或者成本高昂。现有的方法是通过模型输出类别概率分布来预测每个像素的pseudo标签。如果预测结果不正确，则会导致 segmentation 结果差，这可能会对安全关键系统产生影响，例如医疗图像或自动驾驶车辆。因此，理解模型不知道的内容非常重要。最近，神经过程（NP）在半supervised图像分类中被探索，它们是一种计算效率高且有效的不确定量化方法。在这种工作中，我们将NP应用于半supervised semantic segmentation，得到了一种新的模型called NP-SemiSeg。我们对NP-SemiSeg进行了不同的训练设置，并在公共测试集PASCAL VOC 2012和Cityscapes上进行了实验，结果证明了它的有效性。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-for-Stain-Normalisation-in-Histopathology"><a href="#Generative-Adversarial-Networks-for-Stain-Normalisation-in-Histopathology" class="headerlink" title="Generative Adversarial Networks for Stain Normalisation in Histopathology"></a>Generative Adversarial Networks for Stain Normalisation in Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02851">http://arxiv.org/abs/2308.02851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Breen, Kieran Zucker, Katie Allen, Nishant Ravikumar, Nicolas M. Orsi</li>
<li>for: 提高临床诊断的准确率和效率，透过人工智能技术的发展。</li>
<li>methods: 主要是使用生成对抗网络（GANs）等技术进行染料标准化。</li>
<li>results: GAN-based methods typically outperform non-generative approaches, but the best method for stain normalization is still an ongoing field of study and depends on the specific scenario and performance metrics.<details>
<summary>Abstract</summary>
The rapid growth of digital pathology in recent years has provided an ideal opportunity for the development of artificial intelligence-based tools to improve the accuracy and efficiency of clinical diagnoses. One of the significant roadblocks to current research is the high level of visual variability across digital pathology images, causing models to generalise poorly to unseen data. Stain normalisation aims to standardise the visual profile of digital pathology images without changing the structural content of the images. In this chapter, we explore different techniques which have been used for stain normalisation in digital pathology, with a focus on approaches which utilise generative adversarial networks (GANs). Typically, GAN-based methods outperform non-generative approaches but at the cost of much greater computational requirements. However, it is not clear which method is best for stain normalisation in general, with different GAN and non-GAN approaches outperforming each other in different scenarios and according to different performance metrics. This is an ongoing field of study as researchers aim to identify a method which efficiently and effectively normalises pathology images to make AI models more robust and generalisable.
</details>
<details>
<summary>摘要</summary>
随着数字病理学的快速发展，提供了一个理想的机会，用于发展基于人工智能技术的工具，以提高临床诊断的准确性和效率。然而，一个 significante roadblock 是数字病理图像之间的视觉变化，导致模型很难泛化到未看过的数据。颜色标准化目的是标准化数字病理图像的视觉特征，而不改变图像的结构内容。在这章中，我们探讨了不同的技术，用于数字病理颜色标准化，特别是使用生成对抗网络（GAN）。通常，GAN基本方法在不同的场景下都会超越非生成方法，但是计算需求很高。然而，没有一个方法是最佳的，不同的 GAN 和非 GAN 方法在不同的场景和性能指标下都会出perform。这是一个持续的研究领域，研究人员希望能够找到一种能够有效地和效率地标准化病理图像的方法，以使AI模型更加可靠和泛化。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Positive-Homogeneous-Functions-with-Scale-Invariant-Neural-Networks"><a href="#Approximating-Positive-Homogeneous-Functions-with-Scale-Invariant-Neural-Networks" class="headerlink" title="Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks"></a>Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02836">http://arxiv.org/abs/2308.02836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Bamberger, Reinhard Heckel, Felix Krahmer</li>
<li>for:  investigated the possibility of solving linear inverse problems with $ReLu$ networks</li>
<li>methods:  used positive homogeneity and absence of bias terms in the network architecture</li>
<li>results:  showed that $ReLu$-networks with two hidden layers can achieve approximate recovery with arbitrary precision and sparsity level, and extended the results to a wider class of recovery problems and general positive homogeneous functions.<details>
<summary>Abstract</summary>
We investigate to what extent it is possible to solve linear inverse problems with $ReLu$ networks. Due to the scaling invariance arising from the linearity, an optimal reconstruction function $f$ for such a problem is positive homogeneous, i.e., satisfies $f(\lambda x) = \lambda f(x)$ for all non-negative $\lambda$. In a $ReLu$ network, this condition translates to considering networks without bias terms. We first consider recovery of sparse vectors from few linear measurements. We prove that $ReLu$- networks with only one hidden layer cannot even recover $1$-sparse vectors, not even approximately, and regardless of the width of the network. However, with two hidden layers, approximate recovery with arbitrary precision and arbitrary sparsity level $s$ is possible in a stable way. We then extend our results to a wider class of recovery problems including low-rank matrix recovery and phase retrieval. Furthermore, we also consider the approximation of general positive homogeneous functions with neural networks. Extending previous work, we establish new results explaining under which conditions such functions can be approximated with neural networks. Our results also shed some light on the seeming contradiction between previous works showing that neural networks for inverse problems typically have very large Lipschitz constants, but still perform very well also for adversarial noise. Namely, the error bounds in our expressivity results include a combination of a small constant term and a term that is linear in the noise level, indicating that robustness issues may occur only for very small noise levels.
</details>
<details>
<summary>摘要</summary>
我们研究可以使用 $ReLu$ 网络解决线性逆问题的可能性。由于线性的扩展对称性，一个好的复原函数 $f$ 的选择会是正Homogeneous，即满足 $f(\lambda x) = \lambda f(x)$ 的所有非负 $\lambda$。在 $ReLu$ 网络中，这个条件可以翻译为不包含偏好项。我们首先考虑从几个线性量测中回复簇节点。我们证明了 $ReLu$ 网络只有一个隐藏层时无法复原 $1$-簇节点，不管网络宽度如何。但是，具有两个隐藏层的 $ReLu$ 网络可以在稳定的方式下复原任意精度和簇节点数量 $s$。我们随后将结果扩展到更加广泛的复原问题，包括低维矩阵复原和相位回复。此外，我们还考虑了一般正Homogeneous函数的逼近，并建立了新的结果，说明在哪些情况下，这些函数可以透过神经网络逼近。我们的结果还照明了对于过去的研究所提出的对立之处，即神经网络 для反对数学问题通常具有非常大的Lipschitz常数，但是还是能够非常好地运行也在阶梯误差下。这意味着可能在非常小的误差水平下，发生了Robustness问题。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Financial-Index-Tracking"><a href="#Reinforcement-Learning-for-Financial-Index-Tracking" class="headerlink" title="Reinforcement Learning for Financial Index Tracking"></a>Reinforcement Learning for Financial Index Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02820">http://arxiv.org/abs/2308.02820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dppalomar/sparseindextracking">https://github.com/dppalomar/sparseindextracking</a></li>
<li>paper_authors: Xianhua Peng, Chenyin Gong, Xue Dong He</li>
<li>for: 本研究旨在提出一种精细时间架构的财务指数追踪问题解决方案，以满足返回基本追踪误差和价值基本追踪误差两种不同的追踪目标。</li>
<li>methods: 本研究使用了离散时间无限远景动态模型，并使用了Banach固定点迭代法解决端口重新平衡方程。此外，本研究还提出了一种基于深度强化学习（RL）方法的解决方案，以解决数据有限性问题。</li>
<li>results: 实验结果表明，提出的方法可以超过标准方法在追踪准确性和赚利率方面表现出色，并且可以通过具有策略的现金投入或抽取来实现额外的收益。<details>
<summary>Abstract</summary>
We propose the first discrete-time infinite-horizon dynamic formulation of the financial index tracking problem under both return-based tracking error and value-based tracking error. The formulation overcomes the limitations of existing models by incorporating the intertemporal dynamics of market information variables not limited to prices, allowing exact calculation of transaction costs, accounting for the tradeoff between overall tracking error and transaction costs, allowing effective use of data in a long time period, etc. The formulation also allows novel decision variables of cash injection or withdraw. We propose to solve the portfolio rebalancing equation using a Banach fixed point iteration, which allows to accurately calculate the transaction costs specified as nonlinear functions of trading volumes in practice. We propose an extension of deep reinforcement learning (RL) method to solve the dynamic formulation. Our RL method resolves the issue of data limitation resulting from the availability of a single sample path of financial data by a novel training scheme. A comprehensive empirical study based on a 17-year-long testing set demonstrates that the proposed method outperforms a benchmark method in terms of tracking accuracy and has the potential for earning extra profit through cash withdraw strategy.
</details>
<details>
<summary>摘要</summary>
我们提出了首个精细时间无限远景动态模型，用于跟踪金融指数问题，包括返点基本跟踪错误和价值基本跟踪错误。该模型超越了现有模型的局限性，因为它包含市场信息变量的时间动态，允许精确计算交易成本，考虑跟踪错误和交易成本之间的贸易做，使用长时间期间的数据，等等。该模型还允许新的决策变量：资金注入或撤回。我们提出使用巴нах固定点迭代法解决portfolio重新平衡方程，可以准确计算交易成本，实际上是非线性函数的交易量。我们还提出了RL方法的扩展，用于解决动态模型。我们的RL方法可以在单个财务数据样本路径上解决数据有限制的问题，通过一种新的训练方案。我们的实验表明，我们的方法在17年的测试集上表现出色，超过了参考方法的跟踪精度，并且有可能通过资金撤回策略获得额外利润。
</details></li>
</ul>
<hr>
<h2 id="A-generative-model-for-surrogates-of-spatial-temporal-wildfire-nowcasting"><a href="#A-generative-model-for-surrogates-of-spatial-temporal-wildfire-nowcasting" class="headerlink" title="A generative model for surrogates of spatial-temporal wildfire nowcasting"></a>A generative model for surrogates of spatial-temporal wildfire nowcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02810">http://arxiv.org/abs/2308.02810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sibo Cheng, Yike Guo, Rossella Arcucci</li>
<li>for: 预测野火发展 (predicting wildfire development)</li>
<li>methods: 使用三维vector-量化类似自适应器 (using three-dimensional Vector-Quantized Variational Autoencoders)</li>
<li>results: 成功生成了可见度和结构的野火燃烧区域 (successfully generated coherent and structured wildfire burned areas)<details>
<summary>Abstract</summary>
Recent increase in wildfires worldwide has led to the need for real-time fire nowcasting. Physics-driven models, such as cellular automata and computational fluid dynamics can provide high-fidelity fire spread simulations but they are computationally expensive and time-consuming. Much effort has been put into developing machine learning models for fire prediction. However, these models are often region-specific and require a substantial quantity of simulation data for training purpose. This results in a significant amount of computational effort for different ecoregions. In this work, a generative model is proposed using a three-dimensional Vector-Quantized Variational Autoencoders to generate spatial-temporal sequences of unseen wildfire burned areas in a given ecoregion. The model is tested in the ecoregion of a recent massive wildfire event in California, known as the Chimney fire. Numerical results show that the model succeed in generating coherent and structured fire scenarios, taking into account the impact from geophysical variables, such as vegetation and slope. Generated data are also used to train a surrogate model for predicting wildfire dissemination, which has been tested on both simulation data and the real Chimney fire event.
</details>
<details>
<summary>摘要</summary>
全球各地的野火增加的趋势，使得实时野火预测变得越来越重要。物理驱动的模型，如细胞自动机和计算流体力学，可以提供高精度的野火快速扩散模拟，但它们 Computationally expensive and time-consuming。大量的努力被投入到了机器学习模型的开发中，以预测野火。然而，这些模型通常是地域特定的，需要大量的模拟数据来训练。这会导致不同的生态区域需要大量的计算劳动。在这种情况下，本文提出了一种生成模型，使用三维向量量化自适应机制来生成未经见过的野火烧区Sequence。该模型在加利福尼亚州的一个大规模野火事件中进行了测试，称为奇尼火。numerical results show that the model successfully generated coherent and structured fire scenarios, taking into account the impact of geophysical variables such as vegetation and slope.Generated data were also used to train a surrogate model for predicting wildfire spread, which was tested on both simulation data and the real Chimney fire event.
</details></li>
</ul>
<hr>
<h2 id="MiAMix-Enhancing-Image-Classification-through-a-Multi-stage-Augmented-Mixed-Sample-Data-Augmentation-Method"><a href="#MiAMix-Enhancing-Image-Classification-through-a-Multi-stage-Augmented-Mixed-Sample-Data-Augmentation-Method" class="headerlink" title="MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixed Sample Data Augmentation Method"></a>MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixed Sample Data Augmentation Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02804">http://arxiv.org/abs/2308.02804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Liang, Youzhi Liang, Jianguo Jia</li>
<li>for: 提高深度学习模型的泛化性和性能，使其在多种计算机视觉任务中具有更好的普适性和稳定性。</li>
<li>methods: 提出了一种新的混合方法called MiAMix，它是基于混合框架的多阶段扩展混合方法，通过多种多样化的混合方法同时进行混合，并通过随机选择混合掩码的混合方法来提高混合方法。</li>
<li>results: 通过四个图像标准benchmark进行了全面的评估，并与当前状态的混合样本数据增强技术进行比较，demonstrated that MiAMix可以提高性能而无需增加计算负担。<details>
<summary>Abstract</summary>
Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pitting it against current state-of-the-art mixed sample data augmentation techniques to demonstrate that MIAMix improves performance without heavy computational overhead.
</details>
<details>
<summary>摘要</summary>
尽管深度学习领域已经取得了重大进步，但过拟合仍然是一个 kritical 挑战，而数据扩充被认为是一种有效的方法来提高模型的通用性。在不同的策略中，混合样本数据扩充（MSDA）被认为是提高模型性能和通用性的有效方法。我们介绍了一种新的mixup方法，称为MiAMix，它是多stage混合的augmentation方法。MiAMix将图像扩充integrated into the mixup框架，并同时使用多种多样化的混合方法，通过随机选择混合面积的混合方法来提高混合方法。现有的方法使用了saliency信息，而MiAMix是为计算效率而设计的，减少了额外的负担和提供了与现有训练管道的集成。我们对MiaMix进行了四个图像标准 benchMark 的完整评估，并与当前状态的混合样本数据扩充技术进行了比较，以示MiAMix可以提高性能而不带重大计算负担。
</details></li>
</ul>
<hr>
<h2 id="OBESEYE-Interpretable-Diet-Recommender-for-Obesity-Management-using-Machine-Learning-and-Explainable-AI"><a href="#OBESEYE-Interpretable-Diet-Recommender-for-Obesity-Management-using-Machine-Learning-and-Explainable-AI" class="headerlink" title="OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI"></a>OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02796">http://arxiv.org/abs/2308.02796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mrinmoy Roy, Srabonti Das, Anica Tasnim Protity</li>
<li>For: The paper aims to provide a novel machine learning-based system to predict the amount of nutrients an individual requires for being healthy, with a focus on patients with comorbidities.* Methods: The paper uses various machine learning algorithms, including linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, and LightGBM, to predict fluid, carbohydrate, protein, and fat consumption.* Results: The paper achieves high accuracy with low root mean square error (RMSE) using linear regression in fluid prediction, random forest in carbohydrate prediction, and LightGBM in protein and fat prediction. The system, called OBESEYE, is the only one of its kind to consider comorbidities and physical conditions when recommending diets.Here’s the simplified Chinese text for the three main points:* For: 这篇论文目的是提供一种基于机器学习的健康饮食计划，特别是为患有多种疾病的患者。* Methods: 论文使用了不同的机器学习算法，包括线性回归、支持向量机（SVM）、决策树、Random Forest、XGBoost和LightGBM等，来预测 fluid、碳水化合物、蛋白质和脂肪的消耗。* Results: 论文在 fluid 预测中使用线性回归得到了高精度低根平方差（RMSE），在碳水化合物预测中使用Random Forest 得到了高精度，在蛋白质和脂肪预测中使用LightGBM 得到了高精度。OBESEYE 系统是唯一考虑了患者的COMorbidities和物理状况的饮食建议系统。<details>
<summary>Abstract</summary>
Obesity, the leading cause of many non-communicable diseases, occurs mainly for eating more than our body requirements and lack of proper activity. So, being healthy requires heathy diet plans, especially for patients with comorbidities. But it is difficult to figure out the exact quantity of each nutrient because nutrients requirement varies based on physical and disease conditions. In our study we proposed a novel machine learning based system to predict the amount of nutrients one individual requires for being healthy. We applied different machine learning algorithms: linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, LightGBM on fluid and 3 other major micronutrients: carbohydrate, protein, fat consumption prediction. We achieved high accuracy with low root mean square error (RMSE) by using linear regression in fluid prediction, random forest in carbohydrate prediction and LightGBM in protein and fat prediction. We believe our diet recommender system, OBESEYE, is the only of its kind which recommends diet with the consideration of comorbidities and physical conditions and promote encouragement to get rid of obesity.
</details>
<details>
<summary>摘要</summary>
肥胖是多种非传染疾病的主要原因，主要由于食物摄入量超过身体需求，以及不足的适当活动。因此，保持健康需要健康的饮食计划，特别是 для患有多种疾病的患者。然而，确定每个营养素的准确量是困难的，因为营养素需求因 físical 和疾病状况而异。在我们的研究中，我们提出了一种基于机器学习的系统，可以预测个人需要的营养素量。我们应用了不同的机器学习算法：线性回归、支持向量机(SVM)、决策树、随机森林、XGBoost、LightGBM 等，对流体和三种主要微量粮类：碳水化合物、蛋白质、脂肪摄入预测。我们得到了高精度低根平方差(RMSE)的结果，通过线性回归在流体预测中，随机森林在碳水化合物预测中，LightGBM在蛋白质和脂肪预测中。我们认为我们的饮食建议系统“OBESEYE”是目前唯一一个考虑了慢性疾病和物理状况，并且激励人们做减肥的健康饮食建议系统。
</details></li>
</ul>
<hr>
<h2 id="OrcoDCS-An-IoT-Edge-Orchestrated-Online-Deep-Compressed-Sensing-Framework"><a href="#OrcoDCS-An-IoT-Edge-Orchestrated-Online-Deep-Compressed-Sensing-Framework" class="headerlink" title="OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework"></a>OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05757">http://arxiv.org/abs/2308.05757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Wei Ching, Chirag Gupta, Zi Huang, Liting Hu</li>
<li>for: 这个研究是为了提出一个可以灵活地适应不同感知任务和环境变化的协调式深度压缩数据聚合（CDA）框架，以提高这些应用的性能和可扩展性。</li>
<li>methods: 这个框架使用了专门设计的对称启发器，并与边缘设备进行协调，以实现在无线感知网络（WSN）上的线上执行和训练。</li>
<li>results: 这个研究显示，在训练时间和灵活性方面，OrcoDCS比前一代的深度压缩数据聚合（DCDA）要好，并且在进一步应用中实现了更高的性能。<details>
<summary>Abstract</summary>
Compressed data aggregation (CDA) over wireless sensor networks (WSNs) is task-specific and subject to environmental changes. However, the existing compressed data aggregation (CDA) frameworks (e.g., compressed sensing-based data aggregation, deep learning(DL)-based data aggregation) do not possess the flexibility and adaptivity required to handle distinct sensing tasks and environmental changes. Additionally, they do not consider the performance of follow-up IoT data-driven deep learning (DL)-based applications. To address these shortcomings, we propose OrcoDCS, an IoT-Edge orchestrated online deep compressed sensing framework that offers high flexibility and adaptability to distinct IoT device groups and their sensing tasks, as well as high performance for follow-up applications. The novelty of our work is the design and deployment of IoT-Edge orchestrated online training framework over WSNs by leveraging an specially-designed asymmetric autoencoder, which can largely reduce the encoding overhead and improve the reconstruction performance and robustness. We show analytically and empirically that OrcoDCS outperforms the state-of-the-art DCDA on training time, significantly improves flexibility and adaptability when distinct reconstruction tasks are given, and achieves higher performance for follow-up applications.
</details>
<details>
<summary>摘要</summary>
压缩数据聚合（CDA）在无线传感网络（WSN）上是任务特定和环境变化受限的。然而，现有的CDA框架（如扩lapsed sensing基于数据聚合、深度学习（DL）基于数据聚合）不具备适应性和灵活性，无法处理不同感知任务和环境变化。另外，它们不考虑ollow-up IoT数据驱动深度学习（DL）应用的性能。为了解决这些缺点，我们提议OrcoDCS，一个基于IoT-Edge协调在线深度压缩探测框架，具有高适应性和灵活性，以及高性能 дляollow-up应用。我们利用特制的非对称自动encoder，可以大幅减少编码负担，提高重建性能和稳定性。我们通过分析和实验证明，OrcoDCS在训练时间、适应性和灵活性方面都超过了现有的DCDA。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-Contrastive-Regression-for-Estimation-of-Eye-Gaze"><a href="#Semi-supervised-Contrastive-Regression-for-Estimation-of-Eye-Gaze" class="headerlink" title="Semi-supervised Contrastive Regression for Estimation of Eye Gaze"></a>Semi-supervised Contrastive Regression for Estimation of Eye Gaze</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02784">http://arxiv.org/abs/2308.02784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somsukla Maiti, Akshansh Gupta</li>
<li>for: 这篇论文的目的是发展一个 semi-supervised contrastive learning 框架，以便估算人们的 gaze 方向。</li>
<li>methods: 本论文使用 appearance-based deep learning 模型来进行 gaze 估算，并提出了一新的对称损失函数，从中对 similary 的图像进行对比，以提高对 gaze 方向的估算精度。</li>
<li>results: 本论文的实验结果显示，这个 semi-supervised contrastive learning 框架能够从小量 annotated gaze 数据中学习一个通用的 gaze 估算模型，并且与一些state-of-the-art contrastive learning 技术相比，表现更好。<details>
<summary>Abstract</summary>
With the escalated demand of human-machine interfaces for intelligent systems, development of gaze controlled system have become a necessity. Gaze, being the non-intrusive form of human interaction, is one of the best suited approach. Appearance based deep learning models are the most widely used for gaze estimation. But the performance of these models is entirely influenced by the size of labeled gaze dataset and in effect affects generalization in performance. This paper aims to develop a semi-supervised contrastive learning framework for estimation of gaze direction. With a small labeled gaze dataset, the framework is able to find a generalized solution even for unseen face images. In this paper, we have proposed a new contrastive loss paradigm that maximizes the similarity agreement between similar images and at the same time reduces the redundancy in embedding representations. Our contrastive regression framework shows good performance in comparison to several state of the art contrastive learning techniques used for gaze estimation.
</details>
<details>
<summary>摘要</summary>
“随着人机界面智能系统的需求增加，视线控制系统的开发已成为必填。视线是非侵入式人际互动的一种最佳方法。深度学习模型基于 appearances 是目前最受欢迎的视线估计方法。但是这些模型的性能受到labelled gaze dataset的大小影响，从而影响其一般化性能。本文提出了一个半监督对称学习框架，以便透过小量labelled gaze dataset来找到一个通用的解决方案，包括未见过的脸像。本文提出了一个新的对称损失函数，它将相似的图像 Similarity 提高，并同时将嵌入表现的统计复杂度降低。我们的对称回传框架在与多个现有的对称学习技术相比之下，表现良好。”
</details></li>
</ul>
<hr>
<h2 id="Dataopsy-Scalable-and-Fluid-Visual-Exploration-using-Aggregate-Query-Sculpting"><a href="#Dataopsy-Scalable-and-Fluid-Visual-Exploration-using-Aggregate-Query-Sculpting" class="headerlink" title="Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting"></a>Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02764">http://arxiv.org/abs/2308.02764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Naimul Hoque, Niklas Elmqvist<br>for:* 这个论文是为了描述一种faceted visual query技术，帮助用户在大规模多维数据中进行查询和浏览。methods:* 这种查询技术使用的方法包括PIVOT、PARTITION、PEEK、PILE、PROJECT和PRUNE等六个步骤，用于Progressive exploration of the dataset。results:* 通过使用这种查询技术，用户可以在大规模多维数据中进行有效的查询和浏览，并且可以逐步缩小数据集，以便更好地理解数据的结构和特征。<details>
<summary>Abstract</summary>
We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a "born scalable" query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as P6: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.
</details>
<details>
<summary>摘要</summary>
我们介绍了统计查询雕刻（AQS），一种适合大规模多维数据的faceted visual查询技术。作为一种“生成可扩展”的查询技术，AQS开始可视化的方法是透过单一的可视示表示数据集的总聚合。用户可以逐步探索数据集通过一系列操作缩写为P6：折冲（基于特征对资料聚合进行分割）、分区（在空间中排列分割）、侦错（查看子集使用聚合图表示）、堆叠（合并两个或更多的子集）、专案（将子集转换为新基板）、剪除（不再关注的聚合）。我们验证了AQS和Dataopsy，一个实现AQS的试验版本，在桌面和触控式移动设备上进行流过交互。我们透过两个案例和三个应用例子来示范AQS和Dataopsy。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-in-the-Intermediate-Hidden-Layers-of-Classification-Neural-Networks"><a href="#Neural-Collapse-in-the-Intermediate-Hidden-Layers-of-Classification-Neural-Networks" class="headerlink" title="Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks"></a>Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02760">http://arxiv.org/abs/2308.02760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Parker, Emre Onal, Anton Stengel, Jake Intrater</li>
<li>for: 这paper探讨了分类神经网络中间埋点层中的Neural Collapse（NC）现象的emergence。</li>
<li>methods: 这paper使用了多种网络架构、活化函数和数据集来研究NC现象在不同层次的emergence。</li>
<li>results: 研究发现，NC现象在大多数中间埋点层中出现，其度Of collapse与层次深度正相关。此外，研究还发现，大多数减少类内方差的改进发生在神经网络的浅层，而angular separation between class means随层次深度增加。<details>
<summary>Abstract</summary>
Neural Collapse (NC) gives a precise description of the representations of classes in the final hidden layer of classification neural networks. This description provides insights into how these networks learn features and generalize well when trained past zero training error. However, to date, (NC) has only been studied in the final layer of these networks. In the present paper, we provide the first comprehensive empirical analysis of the emergence of (NC) in the intermediate hidden layers of these classifiers. We examine a variety of network architectures, activations, and datasets, and demonstrate that some degree of (NC) emerges in most of the intermediate hidden layers of the network, where the degree of collapse in any given layer is typically positively correlated with the depth of that layer in the neural network. Moreover, we remark that: (1) almost all of the reduction in intra-class variance in the samples occurs in the shallower layers of the networks, (2) the angular separation between class means increases consistently with hidden layer depth, and (3) simple datasets require only the shallower layers of the networks to fully learn them, whereas more difficult ones require the entire network. Ultimately, these results provide granular insights into the structural propagation of features through classification neural networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="WeldMon-A-Cost-effective-Ultrasonic-Welding-Machine-Condition-Monitoring-System"><a href="#WeldMon-A-Cost-effective-Ultrasonic-Welding-Machine-Condition-Monitoring-System" class="headerlink" title="WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System"></a>WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05756">http://arxiv.org/abs/2308.05756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beitong Tian, Kuan-Chieh Lu, Ahmadreza Eslaminia, Yaohui Wang, Chenhui Shao, Klara Nahrstedt</li>
<li>for: 这个论文是为了提出一个可靠、高性能且Cost-effective的工具状态监控系统，以提高鋳接过程中的质量控制。</li>
<li>methods: 这个系统使用自定义的数据收集系统和资料分析管道，实现实时分析。标本检测算法结合自动生成的特征和手工设计的特征，在条件分类任务中达到了95.8%的预设准确率（相比前一代方法的92.5%）。数据增强方法可以减少概念变化问题，提高工具状态分类精度8.3%。所有算法都在本地运行，仅需385毫秒过程数据。</li>
<li>results: 我们部署了WeldMon和一个商业系统在实际的鋳接机上，进行了全面的比较。我们发现，WeldMon可以提供高性能、可靠且Cost-effective的工具状态监控系统。<details>
<summary>Abstract</summary>
Ultrasonic welding machines play a critical role in the lithium battery industry, facilitating the bonding of batteries with conductors. Ensuring high-quality welding is vital, making tool condition monitoring systems essential for early-stage quality control. However, existing monitoring methods face challenges in cost, downtime, and adaptability. In this paper, we present WeldMon, an affordable ultrasonic welding machine condition monitoring system that utilizes a custom data acquisition system and a data analysis pipeline designed for real-time analysis. Our classification algorithm combines auto-generated features and hand-crafted features, achieving superior cross-validation accuracy (95.8% on average over all testing tasks) compared to the state-of-the-art method (92.5%) in condition classification tasks. Our data augmentation approach alleviates the concept drift problem, enhancing tool condition classification accuracy by 8.3%. All algorithms run locally, requiring only 385 milliseconds to process data for each welding cycle. We deploy WeldMon and a commercial system on an actual ultrasonic welding machine, performing a comprehensive comparison. Our findings highlight the potential for developing cost-effective, high-performance, and reliable tool condition monitoring systems.
</details>
<details>
<summary>摘要</summary>
服务器焊接机在锂离子电池业中发挥关键作用，帮助焊接电池与导电器。保证高质量焊接是关键，因此工具状态监测系统成为了早期质量控制的必备工具。然而，现有监测方法存在成本高、下机时间长和适应性差的问题。本文提出了WeldMon，一种可靠、高性能、成本下降的焊接机状态监测系统。WeldMon使用自定义数据获取系统和实时分析管道，并将自动生成的特征和手动设计的特征结合使用，实现了95.8%的验证精度（相对于状态艺术法92.5%）。我们的数据扩展方法可以减轻概念飘移问题，提高工具状态分类精度8.3%。所有算法都运行在本地，仅需385毫秒处理数据每个焊接周期。我们将WeldMon和一个商业系统部署在实际的服务器焊接机上，进行了完整的比较。我们的发现表明，可以开发出成本下降、高性能、可靠的工具状态监测系统。
</details></li>
</ul>
<hr>
<h2 id="DaMSTF-Domain-Adversarial-Learning-Enhanced-Meta-Self-Training-for-Domain-Adaptation"><a href="#DaMSTF-Domain-Adversarial-Learning-Enhanced-Meta-Self-Training-for-Domain-Adaptation" class="headerlink" title="DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation"></a>DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02753">http://arxiv.org/abs/2308.02753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglong Lu, Zhen Huang, Yunxiang Zhao, Zhiliang Tian, Yang Liu, Dongsheng Li</li>
<li>for: 这篇论文的目的是提出一个新的自我训练框架 для领域适应（Domain Adaptation），以解决预测错误导致的标签噪音问题。</li>
<li>methods: 这篇论文使用了自我训练的方法，将模型的预测作为目标领域中的伪标签，通过这些伪标签进行自我训练。此外，论文还提出了一个名为Domain adversarial learning enhanced Self-Training Framework（DaMSTF）的新自我训练框架，具有以下三个特点：一、使用meta-learning估算伪标签的重要性，以降低标签噪音并保留困难的例子；二、设计了一个meta constructor来建立meta-验证集，以保证meta-learning模组的有效性；三、使用领域抗击学来初始化神经网络，以解决meta-learning模组的训练导向变差问题。</li>
<li>results: 论文通过理论和实验证明了DaMSTF的有效性。在跨领域情感分类任务上，DaMSTF比BERT提高了近4%的性能。<details>
<summary>Abstract</summary>
Self-training emerges as an important research line on domain adaptation. By taking the model's prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta-validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanishment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed DaMSTF. On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%.
</details>
<details>
<summary>摘要</summary>
自适应预测为域适应研究的重要线索。通过将模型的预测作为目标域无标签数据的 Pseudo 标签，自适应启动模型以 Pseudo 实例作为目标域中的训练数据。然而，预测错误（标签噪音）对自适应表现成为一个挑战。以前的方法仅使用可靠 Pseudo 实例来重新训练模型，即 Pseudo 实例具有高预测信任度。虽然这些策略有效减少标签噪音，但容易过损难例。在这篇论文中，我们提出了一种新的自适应框架 для域适应，即域适应学习强化自适应框架（DaMSTF）。首先，DaMSTF 通过元学习来估计每个 Pseudo 实例的重要性，以同时减少标签噪音并保留难例。其次，我们设计了元构建模块，用于构建元验证集，以保证元学习模块的有效性。最后，我们发现元学习模块受训练指导消失的问题，容易 converges 到一个差的优化点。为此，我们采用域适应学习作为一种启发函数初始化方法，可以帮助元学习模块 converge 到一个更好的优化点。理论和实验表明，我们提出的 DaMSTF 具有较高的效果。在跨域情感分类任务上，DaMSTF 可以提高 BERT 的性能，均提高约 4%。
</details></li>
</ul>
<hr>
<h2 id="NeRFs-The-Search-for-the-Best-3D-Representation"><a href="#NeRFs-The-Search-for-the-Best-3D-Representation" class="headerlink" title="NeRFs: The Search for the Best 3D Representation"></a>NeRFs: The Search for the Best 3D Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02751">http://arxiv.org/abs/2308.02751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravi Ramamoorthi</li>
<li>for: 这篇论文描述了NeRF表示法的应用和发展，以及三十年来寻找最佳视图合成和相关问题的3D表示方法的漫长历程。</li>
<li>methods: NeRF表示法使用神经网络来描述场景的连续体，包括视点依赖的光泽和体积密度。</li>
<li>results: NeRF表示法已成为计算机图形和视觉领域的标准表示方法，广泛应用于视图合成和图像基于渲染等问题，并且有 thousands of 篇论文进行扩展和建立。<details>
<summary>Abstract</summary>
Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe new developments in terms of NeRF representations and make some observations and insights regarding the future of 3D representations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>神经辐射场或NeRFs已成为视觉合成或基于图像渲染等问题的表示方法选择，以及计算机视觉领域中许多其他应用程序的首选方法。它们的核心在于描述了一种新的3D场景或3D几何表示方法。而不是mesh、投影图、多平面图像或者VOXEL网格，NeRF代表场景为一个连续的Volume，通过问题 neural network 获得了视觉依赖的辐射光照和体积密度。NeRF表示法已经广泛应用，每年有 thousands of 篇论文扩展或基于它，多个作者和网站提供了概述和评论，以及许多工业应用和创业公司。在这篇文章中，我们 briefly 评论了NeRF表示法，并描述了三十年来为视觉合成和相关问题寻找最佳3D表示方法的历程， culminating 在NeRF论文中。然后，我们描述了新的NeRF表示法和一些关于未来3D表示方法的见解和发现。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-On-chip-Heterogeneity-of-Versal-Architecture-for-GNN-Inference-Acceleration"><a href="#Exploiting-On-chip-Heterogeneity-of-Versal-Architecture-for-GNN-Inference-Acceleration" class="headerlink" title="Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration"></a>Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02749">http://arxiv.org/abs/2308.02749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Chen, Pavan Manjunath, Sasindu Wijeratne, Bingyi Zhang, Viktor Prasanna</li>
<li>for: 该 paper 是为了提高 Graph Neural Network (GNN) 的推理速度而设计的。</li>
<li>methods: 该 paper 使用 AMD Versal ACAP 架构的特性进行 GNN 推理加速。具体来说，它使用 Programmable Logic (PL) 和 AI Engine (AIE) 两种不同的计算模块来实现 sparse 和 dense 操作的快速执行。</li>
<li>results: 该 paper 的实现在 VCK5000 ACAP 平台上表现出了较好的性能，与 CPU、GPU、ACAP 和其他自定义 GNN 加速器相比，实现了显著的均值运行时速度提升（162.42x、17.01x、9.90x 和 27.23x）。此外，对于 Graph Convolutional Network (GCN) 推理，该approach 在同一 ACAP 设备上实现了3.9-96.7x的速度提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have revolutionized many Machine Learning (ML) applications, such as social network analysis, bioinformatics, etc. GNN inference can be accelerated by exploiting data sparsity in the input graph, vertex features, and intermediate data in GNN computations. For dynamic sparsity exploitation, we leverage the heterogeneous computing capabilities of AMD Versal ACAP architecture to accelerate GNN inference. We develop a custom hardware module that executes the sparse primitives of the computation kernel on the Programmable Logic (PL) and efficiently computes the dense primitives using the AI Engine (AIE). To exploit data sparsity during inference, we devise a runtime kernel mapping strategy that dynamically assigns computation tasks to the PL and AIE based on data sparsity. Our implementation on the VCK5000 ACAP platform leads to superior performance compared with the state-of-the-art implementations on CPU, GPU, ACAP, and other custom GNN accelerators. Compared with these implementations, we achieve significant average runtime speedup across various models and datasets of 162.42x, 17.01x, 9.90x, and 27.23x, respectively. Furthermore, for Graph Convolutional Network (GCN) inference, our approach leads to a speedup of 3.9-96.7x compared to designs using PL only on the same ACAP device.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）已经革命化了许多机器学习（ML）应用，如社交网络分析和生物信息学等。GNN推理可以通过利用输入图数据稀疏性来加速。为了实现动态稀疏性利用，我们利用AMD Versal ACAP架构的多样化计算能力来加速GNN推理。我们开发了一个自定义硬件模块，该模块在Programmable Logic（PL）上执行稀疏计算 primitives，并使用AI Engine（AIE）来高效计算 dense primitives。为了在推理过程中利用数据稀疏性，我们提出了一种运行时kernel映射策略，该策略在PL和AIE之间动态分配计算任务基于数据稀疏性。我们在VCK5000 ACAP平台上实现了比之前的状态泰器实现在CPU、GPU、ACAP和其他自定义GNN加速器上的性能。相比这些实现，我们实现了平均运行时速度提升值为162.42倍、17.01倍、9.90倍和27.23倍，分别。此外，对于图卷积网络（GCN）推理，我们的方法实现了PL只的设计相比3.9-96.7倍的加速。
</details></li>
</ul>
<hr>
<h2 id="SABRE-Robust-Bayesian-Peer-to-Peer-Federated-Learning"><a href="#SABRE-Robust-Bayesian-Peer-to-Peer-Federated-Learning" class="headerlink" title="SABRE: Robust Bayesian Peer-to-Peer Federated Learning"></a>SABRE: Robust Bayesian Peer-to-Peer Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02747">http://arxiv.org/abs/2308.02747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nasimeh Heydaribeni, Ruisi Zhang, Tara Javidi, Cristina Nita-Rotaru, Farinaz Koushanfar</li>
<li>for: 提出了一种新的 Federated Learning 框架，即 SABRE，以提高 robustness。</li>
<li>methods: 使用了一种新的权重聚合方法，可以在非标准分布Setting下工作，并且不需要benign节点多于恶意节点。</li>
<li>results: 在一些 benchmark 数据上进行了证明和评估，并证明了 SABRE 在各种恶意攻击下的Robustness。<details>
<summary>Abstract</summary>
We introduce SABRE, a novel framework for robust variational Bayesian peer-to-peer federated learning. We analyze the robustness of the known variational Bayesian peer-to-peer federated learning framework (BayP2PFL) against poisoning attacks and subsequently show that BayP2PFL is not robust against those attacks. The new SABRE aggregation methodology is then devised to overcome the limitations of the existing frameworks. SABRE works well in non-IID settings, does not require the majority of the benign nodes over the compromised ones, and even outperforms the baseline algorithm in benign settings. We theoretically prove the robustness of our algorithm against data / model poisoning attacks in a decentralized linear regression setting. Proof-of-Concept evaluations on benchmark data from image classification demonstrate the superiority of SABRE over the existing frameworks under various poisoning attacks.
</details>
<details>
<summary>摘要</summary>
我们介绍SABRE，一个新的 Federated Learning框架，可以防护Against poisoning攻击。我们分析了已知的Variational Bayesian Peer-to-Peer Federated Learning框架（BayP2PFL）对于毒素攻击的不敏感性，并证明BayP2PFL不具有抗毒素能力。我们随后设计了一个新的SABRE聚合方法，以解决现有框架的局限性。SABRE在非ID Setting下表现良好，不需要大多数善意节点 greater than 损坏节点，甚至在正常设定下超越了基准算法。我们对Decentralized Linear Regression Setting中的数据/模型毒素攻击进行了理论上的证明，并在benchmark数据上进行了Proof-of-Concept评估，证明SABRE在不同的毒素攻击下的superiority。
</details></li>
</ul>
<hr>
<h2 id="Meta-Tsallis-Entropy-Minimization-A-New-Self-Training-Approach-for-Domain-Adaptation-on-Text-Classification"><a href="#Meta-Tsallis-Entropy-Minimization-A-New-Self-Training-Approach-for-Domain-Adaptation-on-Text-Classification" class="headerlink" title="Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification"></a>Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02746">http://arxiv.org/abs/2308.02746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglong Lu, Zhen Huang, Zhiliang Tian, Yunxiang Zhao, Xuanyu Fei, Dongsheng Li</li>
<li>for: 本文旨在提高文本分类模型的适应性 across 多个频道，有广泛的应用。</li>
<li>methods: 本文提出了一种基于自适应学习的方法，即使用自适应学习生成pseudo例，并在源频道和目标频道中进行分类。</li>
<li>results: 实验表明， compared to traditional self-training methods, MTEM可以提高BERT模型的适应性，平均提高4%的精度。<details>
<summary>Abstract</summary>
Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the meta-learning algorithm in MTEM and analyze the effectiveness of MTEM in achieving domain adaptation. Experimentally, MTEM improves the adaptation performance of BERT with an average of 4 percent on the benchmark dataset.
</details>
<details>
<summary>摘要</summary>
文本分类是自然语言处理中的基本任务，并且在不同领域中适应文本分类模型有广泛的应用。自我训练会生成 pseudo-example 从模型预测中，并在这些 pseudo-example 上进行逐步训练，即在源领域中减少损失，并在目标领域中减少 Gibbs  entropy。然而，Gibbs entropy 受到预测错误的影响，因此自我训练在大量领域变换时常常失败。在这篇论文中，我们提出了 Meta-Tsallis Entropy 优化（MTEM），它使用元学习算法来优化目标领域中的实例适应 Tsallis  entropy。为了减少 MTEM 的计算成本，我们提出了一种近似技术来近似元学习中的第二阶导数。同时，我们还提出了一种缓和抽象采样机制，以便快速生成 pseudo 标签。理论上，我们证明了 MTEM 中元学习算法的收敛性，并分析了 MTEM 在适应性方面的效果。实验表明，MTEM 可以在标准数据集上提高 BERT 的适应性表现，平均提高了4%。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Schedule-in-Non-Stationary-Wireless-Networks-With-Unknown-Statistics"><a href="#Learning-to-Schedule-in-Non-Stationary-Wireless-Networks-With-Unknown-Statistics" class="headerlink" title="Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics"></a>Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02734">http://arxiv.org/abs/2308.02734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang Minh Nguyen, Eytan Modiano</li>
<li>for: 研究 wireless network 中的吞吐率最优化策略，满足通信系统中部分可见性和时间变化的假设。</li>
<li>methods: 提出了一种基于 Max-Weight 策略的 MW-UCB 算法，利用 Sliding-Window Upper-Confidence Bound 学习无线通信中的通道统计数据，并且可以在非站立性下实现throughput最优。</li>
<li>results: 经过 simulations 验证，MW-UCB 算法可以在具有部分可见性和时间变化的无线网络中实现高效的吞吐率最优化。<details>
<summary>Abstract</summary>
The emergence of large-scale wireless networks with partially-observable and time-varying dynamics has imposed new challenges on the design of optimal control policies. This paper studies efficient scheduling algorithms for wireless networks subject to generalized interference constraint, where mean arrival and mean service rates are unknown and non-stationary. This model exemplifies realistic edge devices' characteristics of wireless communication in modern networks. We propose a novel algorithm termed MW-UCB for generalized wireless network scheduling, which is based on the Max-Weight policy and leverages the Sliding-Window Upper-Confidence Bound to learn the channels' statistics under non-stationarity. MW-UCB is provably throughput-optimal under mild assumptions on the variability of mean service rates. Specifically, as long as the total variation in mean service rates over any time period grows sub-linearly in time, we show that MW-UCB can achieve the stability region arbitrarily close to the stability region of the class of policies with full knowledge of the channel statistics. Extensive simulations validate our theoretical results and demonstrate the favorable performance of MW-UCB.
</details>
<details>
<summary>摘要</summary>
现代无线网络中的大规模无线网络具有部分可见和时间变化的动态特性，对优化控制策略的设计带来了新的挑战。本文研究了基于通用干扰约束的无线网络占用策略的有效计划算法。这种模型体现了现代无线通信网络中Edge设备的准确特性。我们提出了一种名为MW-UCB的新算法，它基于Max-Weight策略，并利用Sliding-Window Upper-Confidence Bound来学习不确定的通道统计。MW-UCB可以在不确定的服务率的情况下保证通信吞吐量的最大化，并且可以在服务率的变化范围内保持稳定。我们证明MW-UCB在一定程度上是可以达到稳定区的最优策略，并且可以在服务率的变化范围内保持稳定。我们的实验结果 validate我们的理论结论，并证明MW-UCB在实际应用中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Personalization-of-Stress-Mobile-Sensing-using-Self-Supervised-Learning"><a href="#Personalization-of-Stress-Mobile-Sensing-using-Self-Supervised-Learning" class="headerlink" title="Personalization of Stress Mobile Sensing using Self-Supervised Learning"></a>Personalization of Stress Mobile Sensing using Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02731">http://arxiv.org/abs/2308.02731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Peter Washington</li>
<li>for: 这研究旨在开发一种个性化的压力预测方法，可以使用小数据量的标签来个性化学习。</li>
<li>methods: 研究人员使用了一种自动学习技术，即一维度的卷积神经网络（CNN），通过自动学习来学习每个用户的基线生物信号模式，从而实现个性化学习。</li>
<li>results: 研究人员发现，使用自动学习前training方法可以学习出高性能的压力预测模型，仅需使用少量的标签数据。这种个性化学习方法可以帮助实现精准健康系统，这些系统可以根据每个用户的特点进行个性化预测和提供推荐。<details>
<summary>Abstract</summary>
Stress is widely recognized as a major contributor to a variety of health issues. Stress prediction using biosignal data recorded by wearables is a key area of study in mobile sensing research because real-time stress prediction can enable digital interventions to immediately react at the onset of stress, helping to avoid many psychological and physiological symptoms such as heart rhythm irregularities. Electrodermal activity (EDA) is often used to measure stress. However, major challenges with the prediction of stress using machine learning include the subjectivity and sparseness of the labels, a large feature space, relatively few labels, and a complex nonlinear and subjective relationship between the features and outcomes. To tackle these issues, we examine the use of model personalization: training a separate stress prediction model for each user. To allow the neural network to learn the temporal dynamics of each individual's baseline biosignal patterns, thus enabling personalization with very few labels, we pre-train a 1-dimensional convolutional neural network (CNN) using self-supervised learning (SSL). We evaluate our method using the Wearable Stress and Affect prediction (WESAD) dataset. We fine-tune the pre-trained networks to the stress prediction task and compare against equivalent models without any self-supervised pre-training. We discover that embeddings learned using our pre-training method outperform supervised baselines with significantly fewer labeled data points: the models trained with SSL require less than 30% of the labels to reach equivalent performance without personalized SSL. This personalized learning method can enable precision health systems which are tailored to each subject and require few annotations by the end user, thus allowing for the mobile sensing of increasingly complex, heterogeneous, and subjective outcomes such as stress.
</details>
<details>
<summary>摘要</summary>
stress是广泛认可的健康问题的重要 contribuens。预测stress使用记录在佩戴式设备中的生物信号数据是移动感知研究中关键的预测领域，因为实时预测stress可以让数字干预出现在压力开始时，以避免心跳rhythm irregularities和许多心理和生理学症状。电导活动（EDA）经常用来测量压力。然而，机器学习预测压力的主要挑战包括标签的主观性和稀缺性，大的特征空间，相对少的标签，以及复杂的非线性和主观关系 между特征和结果。为解决这些问题，我们研究了个性化预测：为每个用户训练一个压力预测模型。为让神经网络学习每个人的基线生物信号模式的时间 dynamics，因此启用个性化预测，我们使用自动学习（SSL）预训练一个1维度卷积神经网络（CNN）。我们使用WESAD数据集进行评估我们的方法。我们精细调整预训练后的网络来完成压力预测任务，并与没有自动学习预训练的相同模型进行比较。我们发现，使用我们的预训练方法学习的嵌入超越了无预训练基线的表现，只需要少于30%的标签数据点来达到相同的性能。这种个性化学习方法可以启用手持式健康系统，这些系统可以根据每个用户而定制，并且只需要少量标签由用户提供，以便在移动感知中评估越来越复杂、多元和主观的结果。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Programmatic-Policies-with-Actor-Critic-Algorithms-and-ReLU-Networks"><a href="#Synthesizing-Programmatic-Policies-with-Actor-Critic-Algorithms-and-ReLU-Networks" class="headerlink" title="Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks"></a>Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02729">http://arxiv.org/abs/2308.02729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Orfanos, Levi H. S. Lelis</li>
<li>for: 本研究旨在探讨Programmatically Interpretable Reinforcement Learning（PIRL）算法是否需要专门的PIRL算法，以及如何使用actor-critic算法直接从 neural network 中提取政策。</li>
<li>methods: 本研究使用 actor-critic 算法，以及一种将ReLU神经网络翻译成 if-then-else 结构、线性变换和PID操作的方法，来将政策编码在程序中。</li>
<li>results: 实验结果表明，使用此翻译方法可以学习短而有效的政策，并且与PIRL算法 synthesize 的政策相比，译制后的政策在许多控制问题上表现至少兼容，有时甚至超越。<details>
<summary>Abstract</summary>
Programmatically Interpretable Reinforcement Learning (PIRL) encodes policies in human-readable computer programs. Novel algorithms were recently introduced with the goal of handling the lack of gradient signal to guide the search in the space of programmatic policies. Most of such PIRL algorithms first train a neural policy that is used as an oracle to guide the search in the programmatic space. In this paper, we show that such PIRL-specific algorithms are not needed, depending on the language used to encode the programmatic policies. This is because one can use actor-critic algorithms to directly obtain a programmatic policy. We use a connection between ReLU neural networks and oblique decision trees to translate the policy learned with actor-critic algorithms into programmatic policies. This translation from ReLU networks allows us to synthesize policies encoded in programs with if-then-else structures, linear transformations of the input values, and PID operations. Empirical results on several control problems show that this translation approach is capable of learning short and effective policies. Moreover, the translated policies are at least competitive and often far superior to the policies PIRL algorithms synthesize.
</details>
<details>
<summary>摘要</summary>
Programmatically Interpretable Reinforcement Learning (PIRL) 编码策略为人类可读性计算机程序。最近，为了解决策略搜索空间中缺失梯度信号的问题，新的算法得到了开发。大多数PIRL算法首先使用神经网络作为尝试导航搜索空间的oracle。在这篇论文中，我们表明PIRL特有的算法不是必要的，具体来说，取决于编码策略的语言。因为可以直接使用actor-critic算法获得程序编码策略。我们使用ReLU神经网络和倾斜决策树的连接将actor-critic算法学习的策略翻译成程序编码策略。这种翻译方法可以synthesize编码策略中的if-then-else结构、输入值线性变换和PID操作。我们在几个控制问题上进行了实验，结果表明这种翻译方法可以学习短而有效的策略。此外，翻译出来的策略与PIRL算法生成的策略相比，通常更加竞争力强。
</details></li>
</ul>
<hr>
<h2 id="Towards-Improving-Harmonic-Sensitivity-and-Prediction-Stability-for-Singing-Melody-Extraction"><a href="#Towards-Improving-Harmonic-Sensitivity-and-Prediction-Stability-for-Singing-Melody-Extraction" class="headerlink" title="Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction"></a>Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02723">http://arxiv.org/abs/2308.02723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smoothken/kknet">https://github.com/smoothken/kknet</a></li>
<li>paper_authors: Keren Shao, Ke Chen, Taylor Berg-Kirkpatrick, Shlomo Dubnov</li>
<li>for: 这 paper 是为了提高 singing melody extraction 的性能而写的。</li>
<li>methods: 这 paper 使用了 input feature modification 和 training objective modification 两种方法来提高模型的性能。</li>
<li>results: 实验结果表明，提posed modifications 对 singing melody extraction 是有 empirical effectiveness 的。Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 这 paper 是为了提高 singing melody extraction 的性能而写的。</li>
<li>methods: 这 paper 使用了 input feature modification 和 training objective modification 两种方法来提高模型的性能。</li>
<li>results: 实验结果表明，提posed modifications 对 singing melody extraction 是有 empirical effectiveness 的。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.
</details>
<details>
<summary>摘要</summary>
在深度学习研究中，许多旋律提取模型通过重新设计神经网络架构来提高性能。在这篇论文中，我们提出了输入特征修改和训练目标修改，基于两个假设。首先，音频数据中的干扰在频谱图中快速衰减。为了增强模型对尾部干扰的敏感性，我们使用离散ζ变换修改合并频率和周期性（CFP）表示。其次，歌唱和非歌唱段的时间非常短暂是非常罕见的。为保证更稳定的旋律轮廓，我们设计了可导的损失函数，避免模型预测这些段落。我们应用这些修改于多个模型，包括MSNet、FTANet以及我们新引入的PianoNet，这是基于钢琴谱写网络的修改。我们的实验结果表明，我们的修改是实际有效的对歌唱旋律提取。
</details></li>
</ul>
<hr>
<h2 id="Fluid-Property-Prediction-Leveraging-AI-and-Robotics"><a href="#Fluid-Property-Prediction-Leveraging-AI-and-Robotics" class="headerlink" title="Fluid Property Prediction Leveraging AI and Robotics"></a>Fluid Property Prediction Leveraging AI and Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02715">http://arxiv.org/abs/2308.02715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baratilab/vid2visc">https://github.com/baratilab/vid2visc</a></li>
<li>paper_authors: Jong Hoon Park, Gauri Pramod Dalwankar, Alison Bartsch, Abraham George, Amir Barati Farimani</li>
<li>for: 这篇论文是为了探讨如何使用视觉信息来推断流体的性质，以便在自动化流体处理系统中更好地控制流体的行为。</li>
<li>methods: 这篇论文使用了3D卷积自适应神经网络来学习不同的流体振荡模式在视频中的表征。这些表征然后被用来从视频中视觉地推断流体的动态粘度。</li>
<li>results: 研究发现，使用这种视觉方法可以准确地推断流体的动态粘度，并且比传统方法更快速和更加精准。<details>
<summary>Abstract</summary>
Inferring liquid properties from vision is a challenging task due to the complex nature of fluids, both in behavior and detection. Nevertheless, the ability to infer their properties directly from visual information is highly valuable for autonomous fluid handling systems, as cameras are readily available. Moreover, predicting fluid properties purely from vision can accelerate the process of fluid characterization saving considerable time and effort in various experimental environments. In this work, we present a purely vision-based approach to estimate viscosity, leveraging the fact that the behavior of the fluid oscillations is directly related to the viscosity. Specifically, we utilize a 3D convolutional autoencoder to learn latent representations of different fluid-oscillating patterns present in videos. We leverage this latent representation to visually infer the category of fluid or the dynamics viscosity of fluid from video.
</details>
<details>
<summary>摘要</summary>
<<SYS Translate="yes">推断流体属性从视觉是一项复杂的任务，主要因为流体的行为和检测都具有复杂的特性。然而，从视觉信息直接推断流体属性的能力具有高度的价值，因为摄像头是 readily available。此外，仅基于视觉预测流体属性可以大幅缩短各种实验室环境中的测试时间和努力。在这项工作中，我们提出了一种完全基于视觉的方法，使用3D卷积自适应神经网络来学习不同的流体振荡模式在视频中的含义。我们利用这种含义来从视频中可视化地推断流体的类别或流体的动态粘性。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Effect-of-Sparse-Recovery-on-the-Quality-of-Image-Superresolution"><a href="#Exploring-the-Effect-of-Sparse-Recovery-on-the-Quality-of-Image-Superresolution" class="headerlink" title="Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution"></a>Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02714">http://arxiv.org/abs/2308.02714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Castro</li>
<li>for: 该论文旨在研究用字典学习进行图像超解像，通过学习高分辨率和低分辨率图像对应的对应的patch对来学习一对联动的字典，以便使用这些字典来从低分辨率输入图像中恢复高分辨率图像patch。</li>
<li>methods: 该论文使用了字典学习技术，通过学习高分辨率和低分辨率图像对应的对应的patch对来学习一对联动的字典，并使用这些字典来从低分辨率输入图像中恢复高分辨率图像patch。</li>
<li>results: 该论文通过实验研究了不同的简单恢复算法对图像超解像质量的影响，并提出了最佳的简单恢复算法选择方法。<details>
<summary>Abstract</summary>
Dictionary learning can be used for image superresolution by learning a pair of coupled dictionaries of image patches from high-resolution and low-resolution image pairs such that the corresponding pairs share the same sparse vector when represented by the coupled dictionaries. These dictionaries then can be used to to reconstruct the corresponding high-resolution patches from low-resolution input images based on sparse recovery. The idea is to recover the shared sparse vector using the low-resolution dictionary and then multiply it by the high-resolution dictionary to recover the corresponding high-resolution image patch. In this work, we study the effect of the sparse recovery algorithm that we use on the quality of the reconstructed images. We offer empirical experiments to search for the best sparse recovery algorithm that can be used for this purpose.
</details>
<details>
<summary>摘要</summary>
《字典学习可以用于图像超分辨by学习一对相互关联的字典，这些字典分别表示高分辨率和低分辨率图像对应的图像 patches，以便在这些字典中共享相同的稀疏 вектор。这些字典可以用来从低分辨率输入图像中重建对应的高分辨率图像 patch，基于稀疏恢复。我们的研究将关注使用的稀疏恢复算法对重建图像质量的影响。我们将进行实验寻找最佳的稀疏恢复算法。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Scalable-Computation-of-Causal-Bounds"><a href="#Scalable-Computation-of-Causal-Bounds" class="headerlink" title="Scalable Computation of Causal Bounds"></a>Scalable Computation of Causal Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02709">http://arxiv.org/abs/2308.02709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madhumitha Shridharan, Garud Iyengar</li>
<li>for:  Computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold.</li>
<li>methods:  Significantly prune a linear programming (LP) formulation to compute bounds, allowing for larger causal inference problems compared to existing techniques. Extend the pruning methodology to fractional LPs for additional observations.</li>
<li>results:  Significant runtime improvement compared to benchmarks in experiments, and high-quality bounds produced by an efficient greedy heuristic that scales to larger problems.<details>
<summary>Abstract</summary>
We consider the problem of computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold. Existing non-parametric approaches for computing such bounds use linear programming (LP) formulations that quickly become intractable for existing solvers because the size of the LP grows exponentially in the number of edges in the causal graph. We show that this LP can be significantly pruned, allowing us to compute bounds for significantly larger causal inference problems compared to existing techniques. This pruning procedure allows us to compute bounds in closed form for a special class of problems, including a well-studied family of problems where multiple confounded treatments influence an outcome. We extend our pruning methodology to fractional LPs which compute bounds for causal queries which incorporate additional observations about the unit. We show that our methods provide significant runtime improvement compared to benchmarks in experiments and extend our results to the finite data setting. For causal inference without additional observations, we propose an efficient greedy heuristic that produces high quality bounds, and scales to problems that are several orders of magnitude larger than those for which the pruned LP can be solved.
</details>
<details>
<summary>摘要</summary>
我团队考虑了计算 causal 查询 bounds 的问题，这里有不观察到的假设变量和离散型观察变量。我们显示出现非参数方法的 LP 表示可以快速减少，使得我们可以计算 bounds  для更大的 causal inference 问题。我们扩展了我们的减少方法来计算 fractional LPs，这些 LPs 计算 bounds  для包含额外观察的 causal queries。我们在实验中显示了我们的方法可以提供重要的时间改进 compared to 参考值。此外，我们还提出了一种高质量 bounds 的生成策略，该策略可以扩展到许多个数量级更大的问题。Here's the translation of the text into Traditional Chinese:我团队考虑过 Computing causal queries bounds 的问题，这里有不观察到的假设变量和离散型观察变量。我们显示出现非参数方法的 LP 表示可以快速减少，使得我们可以计算 bounds  для更大的 causal inference 问题。我们扩展了我们的减少方法来计算 fractional LPs，这些 LPs 计算 bounds  для包含额外观察的 causal queries。我们在实验中显示了我们的方法可以提供重要的时间改进 compared to 参考值。此外，我们还提出了一种高质量 bounds 的生成策略，这策略可以扩展到许多个数量级更大的问题。
</details></li>
</ul>
<hr>
<h2 id="FPR-Estimation-for-Fraud-Detection-in-the-Presence-of-Class-Conditional-Label-Noise"><a href="#FPR-Estimation-for-Fraud-Detection-in-the-Presence-of-Class-Conditional-Label-Noise" class="headerlink" title="FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise"></a>FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02695">http://arxiv.org/abs/2308.02695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtittelfitz/fpr-estimation">https://github.com/jtittelfitz/fpr-estimation</a></li>
<li>paper_authors: Justin Tittelfitz</li>
<li>for: 本研究旨在估计二分类模型中存在标签噪声（label noise）时的假阳性率（FPR）和正确率（TPR）。</li>
<li>methods: 本研究使用了一种新的方法，即使用模型直接清洁验证数据，以避免因验证数据中的噪声而导致的假阳性率和正确率的估计偏差。</li>
<li>results: 研究发现，使用现有的方法可能会导致假阳性率和正确率的估计偏差，而使用新的方法可以更好地估计这两个指标。<details>
<summary>Abstract</summary>
We consider the problem of estimating the false-/ true-positive-rate (FPR/TPR) for a binary classification model when there are incorrect labels (label noise) in the validation set. Our motivating application is fraud prevention where accurate estimates of FPR are critical to preserving the experience for good customers, and where label noise is highly asymmetric. Existing methods seek to minimize the total error in the cleaning process - to avoid cleaning examples that are not noise, and to ensure cleaning of examples that are. This is an important measure of accuracy but insufficient to guarantee good estimates of the true FPR or TPR for a model, and we show that using the model to directly clean its own validation data leads to underestimates even if total error is low. This indicates a need for researchers to pursue methods that not only reduce total error but also seek to de-correlate cleaning error with model scores.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Explainable-Deep-Learning-based-Solar-Flare-Prediction-with-post-hoc-Attention-for-Operational-Forecasting"><a href="#Explainable-Deep-Learning-based-Solar-Flare-Prediction-with-post-hoc-Attention-for-Operational-Forecasting" class="headerlink" title="Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting"></a>Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02682">http://arxiv.org/abs/2308.02682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/gsudmlab/explainingfulldisk">https://bitbucket.org/gsudmlab/explainingfulldisk</a></li>
<li>paper_authors: Chetraj Pandey, Rafal A. Angryk, Manolis K. Georgoulis, Berkay Aydin</li>
<li>for: 预测日冕大地震（solar flare）的深度学习模型，以提高现有的预测方法的准确性和可靠性。</li>
<li>methods: 使用每小时的全盘线对视图磁图图像，采用二分类预测模式预测日冕大地震发生的可能性。采用自定义数据增强和样本权重来解决类偏置问题。使用真正的技能统计量和赫迪克技能分数作为评估指标。</li>
<li>results: 研究发现，全盘预测日冕大地震能够准确地 lokate和预测近日冕的大地震，这是操作预测中的关键特征。模型达到了平均的TSS&#x3D;0.51$\pm$0.05和HSS&#x3D;0.38$\pm$0.08水平，并且发现这些模型可以从全盘磁图中学习出明显的活跃区域特征。<details>
<summary>Abstract</summary>
This paper presents a post hoc analysis of a deep learning-based full-disk solar flare prediction model. We used hourly full-disk line-of-sight magnetogram images and selected binary prediction mode to predict the occurrence of $\geq$M1.0-class flares within 24 hours. We leveraged custom data augmentation and sample weighting to counter the inherent class-imbalance problem and used true skill statistic and Heidke skill score as evaluation metrics. Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods: (i) Guided Gradient-weighted Class Activation Mapping, (ii) Deep Shapley Additive Explanations, and (iii) Integrated Gradients. Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are: (1) We demonstrate that our full disk model can tangibly locate and predict near-limb solar flares, which is a critical feature for operational flare forecasting, (2) Our candidate model achieves an average TSS=0.51$\pm$0.05 and HSS=0.38$\pm$0.08, and (3) Our evaluation suggests that these models can learn conspicuous features corresponding to active regions from full-disk magnetograms.
</details>
<details>
<summary>摘要</summary>
Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods:1. Guided Gradient-weighted Class Activation Mapping: 使用梯度信号将决策担当分配到输入特征上。2. Deep Shapley Additive Explanations: 使用深度谱分解方法计算出每个特征对预测结果的贡献。3. Integrated Gradients: 使用梯度 интеграル方法计算出输入特征对预测结果的贡献。Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are:1. We demonstrate that our full disk model can tangibly locate and predict near-limb solar flares, which is a critical feature for operational flare forecasting.2. Our candidate model achieves an average TSS=0.51$\pm$0.05 and HSS=0.38$\pm$0.08.3. Our evaluation suggests that these models can learn conspicuous features corresponding to active regions from full-disk magnetograms.
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Change-of-Variable-Formulas-for-Generative-Modeling"><a href="#A-Review-of-Change-of-Variable-Formulas-for-Generative-Modeling" class="headerlink" title="A Review of Change of Variable Formulas for Generative Modeling"></a>A Review of Change of Variable Formulas for Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02652">http://arxiv.org/abs/2308.02652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ullrich Köthe</li>
<li>for: 本研究旨在总结Change-of-variables（CoV）方程的各种应用和 derivation，并从编码器&#x2F;解码器架构的视角出发，收集28种CoV方程，探讨各种方法之间的关系，强调文献中不一定够清楚地表达的重要区别，并发现未来研究中的潜在漏洞。</li>
<li>methods: 本研究使用了变量替换方程来简化复杂的概率分布，并通过学习的变换来实现 tractable Jacobian determinant。</li>
<li>results: 本研究收集了28种Change-of-variables方程，并通过对这些方程的系统性分析，探讨各种方法之间的关系，强调文献中不一定够清楚地表达的重要区别，并发现未来研究中的潜在漏洞。<details>
<summary>Abstract</summary>
Change-of-variables (CoV) formulas allow to reduce complicated probability densities to simpler ones by a learned transformation with tractable Jacobian determinant. They are thus powerful tools for maximum-likelihood learning, Bayesian inference, outlier detection, model selection, etc. CoV formulas have been derived for a large variety of model types, but this information is scattered over many separate works. We present a systematic treatment from the unifying perspective of encoder/decoder architectures, which collects 28 CoV formulas in a single place, reveals interesting relationships between seemingly diverse methods, emphasizes important distinctions that are not always clear in the literature, and identifies surprising gaps for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>变量更改（CoV）公式可以将复杂的概率密度降低到更简单的密度中，通过学习的变换，其Jacobian determinant是可追踪的。因此，它们是最大 likelihood 学习、 bayesian 推理、异常检测、模型选择等方面的强大工具。CoV 公式已经 derivated  для许多模型类型，但这些信息分散在多个不同的论文中。我们在encoder/decoder 架构的统一视角下提供了一个系统性的处理方法，收集了28种CoV 公式，在一个地方汇总了这些信息，揭示了文献中的感兴趣关系，强调了文献中不一定明确的重要区别，并发现了未来研究中的潜在空白。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="ReCLIP-Refine-Contrastive-Language-Image-Pre-Training-with-Source-Free-Domain-Adaptation"><a href="#ReCLIP-Refine-Contrastive-Language-Image-Pre-Training-with-Source-Free-Domain-Adaptation" class="headerlink" title="ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation"></a>ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03793">http://arxiv.org/abs/2308.03793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Hu, Ke Zhang, Lu Xia, Albert Chen, Jiajia Luo, Yuyin Sun, Ken Wang, Nan Qiao, Xiao Zeng, Min Sun, Cheng-Hao Kuo, Ram Nevatia</li>
<li>for: 提高CLIP模型在下游目标领域的性能，即使没有源数据或目标标注数据。</li>
<li>methods: 提出了一种源自由领域适应方法，通过减轻视文对象 embedding的偏差和使用杂模相关自动标注来实现。</li>
<li>results: 经过广泛的实验，ReCLIP方法可以将CLIP模型的平均错误率从30.17%降低至25.06%在22个图像分类 benchmark上。<details>
<summary>Abstract</summary>
Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, the first source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively. With extensive experiments, we demonstrate ReCLIP reduces the average error rate of CLIP from 30.17% to 25.06% on 22 image classification benchmarks.
</details>
<details>
<summary>摘要</summary>
大规模预训练视语模型，如CLIP，在零shot分类任务上表现出色，例如在ImageNet上取得76.3%的顶峰准确率无需见过任何示例，这可能导致多种任务 ohne 标注数据的潜在优势。然而，在应用CLIP到下游目标领域时，视图和文本频谱差和跨模态不一致可能会严重影响模型性能。为了解决这些挑战，我们提议ReCLIP，首先学习抑制视图和文本嵌入的投影空间，然后通过跨模态自适应学习，使用假标签，更新视图和文本编码器，细化标签，并逐步减少频谱差和不一致。经过广泛实验，我们表明ReCLIP可以将CLIP的平均错误率从30.17%降低至25.06%在22个图像分类任务上。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Topology-Cosmological-Parameter-Estimation-from-the-Large-scale-Structure"><a href="#Learning-from-Topology-Cosmological-Parameter-Estimation-from-the-Large-scale-Structure" class="headerlink" title="Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure"></a>Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02636">http://arxiv.org/abs/2308.02636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacky H. T. Yip, Adam Rouhiainen, Gary Shiu</li>
<li>for: 研究宇宙大规模结构的 cosmological 参数</li>
<li>methods: 使用神经网络模型从 persistency 图像中提取 cosmological 参数</li>
<li>results: 模型可以准确地估算 cosmological 参数，质量比传统 Bayesian 推理方法更高<details>
<summary>Abstract</summary>
The topology of the large-scale structure of the universe contains valuable information on the underlying cosmological parameters. While persistent homology can extract this topological information, the optimal method for parameter estimation from the tool remains an open question. To address this, we propose a neural network model to map persistence images to cosmological parameters. Through a parameter recovery test, we demonstrate that our model makes accurate and precise estimates, considerably outperforming conventional Bayesian inference approaches.
</details>
<details>
<summary>摘要</summary>
宇宙大规模结构的Topology含有价值的 cosmological参数信息。 persistent homology 可以提取这些 topological 信息，但是最佳的方法 для参数估计仍然是一个开放的问题。为解决这个问题，我们提议一种神经网络模型，将 persistency 图像映射到 cosmological 参数。通过参数恢复测试，我们证明我们的模型可以做出准确和精确的估计，明显超过了传统的 Bayesian 推理方法。Note: "persistent homology" is translated as " persistency" in Simplified Chinese, which is a common abbreviation used in the field of topological data analysis.
</details></li>
</ul>
<hr>
<h2 id="MM-Vet-Evaluating-Large-Multimodal-Models-for-Integrated-Capabilities"><a href="#MM-Vet-Evaluating-Large-Multimodal-Models-for-Integrated-Capabilities" class="headerlink" title="MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"></a>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02490">http://arxiv.org/abs/2308.02490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweihao/mm-vet">https://github.com/yuweihao/mm-vet</a></li>
<li>paper_authors: Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang</li>
<li>For: The paper proposes an evaluation benchmark for large multimodal models (LMMs) to evaluate their ability to integrate different core vision-language (VL) capabilities and solve complicated multimodal tasks.* Methods: The paper presents MM-Vet, a benchmark that defines 6 core VL capabilities and examines 16 integrations of interest derived from capability combination. The paper also proposes an LLM-based evaluator for open-ended outputs, which enables the evaluation across different question types and answer styles.* Results: The paper evaluates representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. The results show that the proposed evaluator can provide a unified scoring metric for open-ended outputs, and the MM-Vet benchmark can help identify the strengths and weaknesses of different LMM systems.<details>
<summary>Abstract</summary>
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. Code and data are available at https://github.com/yuweihao/MM-Vet.
</details>
<details>
<summary>摘要</summary>
我们提出了 MM-Vet，一个评估板准 benchmark，用于测试大型多Modal模型（LMM）在复杂多Modal任务上的能力。现代LMM在不同的任务上表现出了各种感人的能力，如解释黑板上的数学问题，理解新闻图片中的事件和名人，以及解释视觉笑话。由于模型的快速发展，评估板准的开发受到了挑战。问题包括：(1)如何系统地结构化和评估复杂多Modal任务；(2)如何设计评估指标，可以在问题和答案类型之间具有一致性；以及(3)如何为模型提供更多的材料，而不仅仅是一个简单的性能排名。为此，我们提出了 MM-Vet，基于了核心视语言（VL）能力的总结。MM-Vet定义了6个核心VL能力，并评估了这些能力之间的16种 интеграción。为评估指标，我们提议了基于LLM的评估器，可以对开放式输出进行评估，并且可以在不同的问题类型和答案风格之间具有一致性。我们对代表性的LMM进行了 MM-Vet的评估，提供了不同模型系统和模型的能力的见解。代码和数据可以在 https://github.com/yuweihao/MM-Vet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Generation-of-Realistic-Synthetic-Raw-Radar-Data-for-Automated-Driving-Applications-using-Generative-Adversarial-Networks"><a href="#Generation-of-Realistic-Synthetic-Raw-Radar-Data-for-Automated-Driving-Applications-using-Generative-Adversarial-Networks" class="headerlink" title="Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks"></a>Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02632">http://arxiv.org/abs/2308.02632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo C. Fidelis, Fabio Reway, Herick Y. S. Ribeiro, Pietro L. Campos, Werner Huber, Christian Icking, Lester A. Faria, Torsten Schön</li>
<li>for: 这个论文是为了模拟雷达数据的研究而写的。</li>
<li>methods: 这篇论文使用了生成对抗网络（GAN）来生成雷达数据。</li>
<li>results: 这个方法可以生成16个同时发射的雷达射频信号，并且可以用于进一步开发雷达数据处理算法（例如滤波和封装）。这可以增加数据的扩展和增强，例如生成不可能或安全关键的场景的数据，以便进行数据 augmentation。<details>
<summary>Abstract</summary>
The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the neural network. The synthetic generated radar chirps were evaluated using the Frechet Inception Distance (FID). Then, the Range-Azimuth (RA) map is calculated twice: first, based on synthetic data using this GAN and, second, based on real data. Based on these RA maps, an algorithm with adaptive threshold and edge detection is used for object detection. The results have shown that the data is realistic in terms of coherent radar reflections of the motorcycle and background noise based on the comparison of chirps, the RA maps and the object detection results. Thus, the proposed method in this work has shown to minimize the simulation-to-reality gap for the generation of radar data.
</details>
<details>
<summary>摘要</summary>
主要方法 для模拟FMCW雷达是基于射线追踪，通常是计算昂贵的并不考虑背景噪声。这项工作提出了一种更快的FMCW雷达模拟方法，可以生成基于生成对抗网络（GAN）的合成雷达数据。代码和预训练 весов在GitHub上公开可用。这种方法生成了16个同时发射的毫声信号，这使得生成的数据可以用于进一步开发雷达数据处理算法（过滤和归一）。这可以增加数据增强的潜在性，例如通过生成不可能或安全关键的场景中的数据，以便在实际生产中不可能重现。在这项工作中，GAN被训练使用雷达测量数据，用于生成雷达数据的 sintetic raw radar chirps。为生成这些数据，雷达车辆的距离和高斯噪声作为神经网络的输入。生成的雷达毫声信号被评估使用Frechet InceptionDistance（FID）。然后，雷达距离-方位（RA）图被计算两次：首先，基于生成数据使用这个GAN，然后，基于实际数据。基于这些RA图，一种适应阈值和边检测算法用于物体检测。结果表明，生成的数据具有准确的干扰雷达反射和背景噪声，基于毫声信号、RA图和物体检测结果的比较。因此，该提出的方法在本工作中已经成功地减小了模拟到实际的差距。
</details></li>
</ul>
<hr>
<h2 id="BlindSage-Label-Inference-Attacks-against-Node-level-Vertical-Federated-Graph-Neural-Networks"><a href="#BlindSage-Label-Inference-Attacks-against-Node-level-Vertical-Federated-Graph-Neural-Networks" class="headerlink" title="BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks"></a>BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02465">http://arxiv.org/abs/2308.02465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Arazzi, Mauro Conti, Stefanos Koffas, Marina Krcek, Antonino Nocera, Stjepan Picek, Jing Xu</li>
<li>for: 这篇论文的主要目的是研究 vertical federated learning（VFL）中的标签推理攻击，特别是在没有背景知识的情况下进行攻击。</li>
<li>methods: 本文使用了 zero-background knowledge 策略来进行攻击，具体是使用 Graph Neural Networks（GNNs）作为目标模型，并在 node classification 任务上进行了实验。</li>
<li>results: 本文的 proposed attack，BlindSage，在实验中实现了 nearly 100% 的准确率，甚至在攻击者没有任何背景知识的情况下仍能实现准确率高于 85%。此外，本文发现了一些常见的防御措施不能对抗本攻击，而且这些防御措施会对模型的表现造成负面影响。<details>
<summary>Abstract</summary>
Federated learning enables collaborative training of machine learning models by keeping the raw data of the involved workers private. One of its main objectives is to improve the models' privacy, security, and scalability. Vertical Federated Learning (VFL) offers an efficient cross-silo setting where a few parties collaboratively train a model without sharing the same features. In such a scenario, classification labels are commonly considered sensitive information held exclusively by one (active) party, while other (passive) parties use only their local information. Recent works have uncovered important flaws of VFL, leading to possible label inference attacks under the assumption that the attacker has some, even limited, background knowledge on the relation between labels and data. In this work, we are the first (to the best of our knowledge) to investigate label inference attacks on VFL using a zero-background knowledge strategy. To concretely formulate our proposal, we focus on Graph Neural Networks (GNNs) as a target model for the underlying VFL. In particular, we refer to node classification tasks, which are widely studied, and GNNs have shown promising results. Our proposed attack, BlindSage, provides impressive results in the experiments, achieving nearly 100% accuracy in most cases. Even when the attacker has no information about the used architecture or the number of classes, the accuracy remained above 85% in most instances. Finally, we observe that well-known defenses cannot mitigate our attack without affecting the model's performance on the main classification task.
</details>
<details>
<summary>摘要</summary>
合作学习（Federated Learning）可以保持参与者的原始数据加密，以提高模型的隐私、安全性和可扩展性。垂直联合学习（VFL）提供了跨存储设置，在不共享同一个特征的情况下，几个党合作训练模型。在这种场景下，分类标签通常被视为活动党拥有的敏感信息，而其他投降党仅使用本地信息。现有研究曝光了VFL的重要漏洞，可能导致标签推理攻击，假设攻击者具有一定的背景知识关于标签和数据之间的关系。在这种情况下，我们是第一个（到我们知道的最佳）investigate VFL中的标签推理攻击，使用零背景知识策略。为了具体实现我们的提议，我们将Graph Neural Networks（GNNs）作为目标模型，特别是节点分类任务，这是广泛研究的领域，GNNs在这些任务上表现出色。我们提出的攻击方法，BlindSage，在实验中提供了很好的结果，在大多数情况下达到了nearly 100%的准确率。即使攻击者没有关于使用的架构或分类数量的任何信息，我们的攻击仍然在大多数情况下保持了超过85%的准确率。最后，我们发现了一些常见的防御无法防止我们的攻击，而不会影响模型在主要分类任务上的性能。
</details></li>
</ul>
<hr>
<h2 id="Universal-Approximation-of-Linear-Time-Invariant-LTI-Systems-through-RNNs-Power-of-Randomness-in-Reservoir-Computing"><a href="#Universal-Approximation-of-Linear-Time-Invariant-LTI-Systems-through-RNNs-Power-of-Randomness-in-Reservoir-Computing" class="headerlink" title="Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing"></a>Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02464">http://arxiv.org/abs/2308.02464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu<br>for:RC is used to overcome the issues of vanishing and exploding gradients in standard RNN training, and it has demonstrated superior empirical performance in various fields. However, the theoretical grounding for this observed performance has not been fully developed.methods:The paper uses a signal processing interpretation of RC to universally approximate a general LTI system. The optimal probability distribution function for generating the recurrent weights of the underlying RNN of the RC is derived, and extensive numerical evaluations are provided to validate the optimality of the derived distribution.results:The paper shows that RC can universally approximate a general LTI system, and provides a clear signal processing-based model interpretability of RC. The derived optimal probability distribution function for the recurrent weights of the RC is found to be effective in simulating the LTI system. The work provides a complete analytical characterization of the untrained recurrent weights, which is important for explainable machine learning applications where training samples are limited.<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal processing interpretation of RC and utilize this understanding in the problem of simulating a generic LTI system through RC. Under this setup, we analytically characterize the optimal probability distribution function for generating the recurrent weights of the underlying RNN of the RC. We provide extensive numerical evaluations to validate the optimality of the derived optimum distribution of the recurrent weights of the RC for the LTI system simulation problem. Our work results in clear signal processing-based model interpretability of RC and provides theoretical explanation for the power of randomness in setting instead of training RC's recurrent weights. It further provides a complete optimum analytical characterization for the untrained recurrent weights, marking an important step towards explainable machine learning (XML) which is extremely important for applications where training samples are limited.
</details>
<details>
<summary>摘要</summary>
循环神经网络（RNN）是已知的通用函数近似器，可以处理时间信息。然而，标准的RNN训练过程中通常会出现消失和扩散梯度的问题。宽泛计算（RC）是一种特殊的RNN，其循环权重随机并未训练，可以解决这些问题，并在自然语言处理和无线通信等领域展现出优于标准RNN的实际表现。然而，对于RC的理论基础的发展并没有与实际表现相同的速度。在这个工作中，我们证明了RNN可以 universally approximate linear time-invariant（LTI）系统。具体来说，我们证明了RC可以 universally approximate任何LTI系统。我们提供了一个清晰的信号处理解释，用于理解RC的工作原理，并使用这种理解来解决一个通用LTI系统的模拟问题。在这个设置下，我们分析性地 caracterize了RC的优化梯度的概率分布。我们进行了广泛的数值评估，以验证我们所 derivated的RC的优化梯度概率分布的优化性。我们的工作具有以下优点：1. 我们提供了一个信号处理基于的RC模型解释，具有明确的数学基础。2. 我们提供了一个可靠的数值评估，以验证RC的优化梯度概率分布的优化性。3. 我们的结论可以用于解释Randomness在RC中的作用，并且提供了一个完整的优化梯度概率分布的分析。4. 我们的工作对于Explainable Machine Learning（XML）的发展具有重要意义，特别是在训练样本数量有限的情况下。总之，我们的工作提供了一个信号处理基于的RC模型解释，并且提供了一个可靠的数值评估，以验证RC的优化梯度概率分布的优化性。此外，我们的结论可以用于解释Randomness在RC中的作用，并且对于XML的发展具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Reduced-Order-Modeling-of-a-MOOSE-based-Additive-Manufacturing-Model-with-Operator-Learning"><a href="#Fast-and-Accurate-Reduced-Order-Modeling-of-a-MOOSE-based-Additive-Manufacturing-Model-with-Operator-Learning" class="headerlink" title="Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning"></a>Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02462">http://arxiv.org/abs/2308.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</li>
<li>for: 本研究目的是为了提高additive manufacturing（AM）过程中的物料特性，通过 manipulate 生产过程参数以实现特定的物料特性。</li>
<li>methods: 本研究使用了Operator Learning（OL）方法，通过学习减少了过程变量的 differential equation 家族，以建立快速和准确的减少模型（ROM）。</li>
<li>results: 研究发现，OL方法可以与传统的深度神经网络（DNN）相比，在预测scalar模型响应时提供了相似的性能，且在精度和泛化性方面甚至超越DNN。DNN基于的ROM具有最快的训练时间，但是所有的ROM都比原始的MOOSE模型快速，且仍提供了准确的预测。FNO在预测时间序数据方面具有较小的平均预测误差，但是DeepONet具有较大的变化。不同于DNN，FNO和DeepONet都可以无需维度减少技术来预测时间序数据。<details>
<summary>Abstract</summary>
One predominant challenge in additive manufacturing (AM) is to achieve specific material properties by manipulating manufacturing process parameters during the runtime. Such manipulation tends to increase the computational load imposed on existing simulation tools employed in AM. The goal of the present work is to construct a fast and accurate reduced-order model (ROM) for an AM model developed within the Multiphysics Object-Oriented Simulation Environment (MOOSE) framework, ultimately reducing the time/cost of AM control and optimization processes. Our adoption of the operator learning (OL) approach enabled us to learn a family of differential equations produced by altering process variables in the laser's Gaussian point heat source. More specifically, we used the Fourier neural operator (FNO) and deep operator network (DeepONet) to develop ROMs for time-dependent responses. Furthermore, we benchmarked the performance of these OL methods against a conventional deep neural network (DNN)-based ROM. Ultimately, we found that OL methods offer comparable performance and, in terms of accuracy and generalizability, even outperform DNN at predicting scalar model responses. The DNN-based ROM afforded the fastest training time. Furthermore, all the ROMs were faster than the original MOOSE model yet still provided accurate predictions. FNO had a smaller mean prediction error than DeepONet, with a larger variance for time-dependent responses. Unlike DNN, both FNO and DeepONet were able to simulate time series data without the need for dimensionality reduction techniques. The present work can help facilitate the AM optimization process by enabling faster execution of simulation tools while still preserving evaluation accuracy.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在添加制造（AM）是在运行时控制和优化过程中实现特定材料性能。这种控制通常会增加现有的Simulation工具在AM中的计算负担。目标是在MOOSE框架中开发一个快速和准确的减少维度模型（ROM），以降低AM控制和优化过程的时间/成本。我们采用了运算学（OL）方法，通过修改过程变量来学习激光的 Gaussian点热源生成的家族 diffeqential equations。我们使用了Fourier neural operator（FNO）和深度运算网络（DeepONet）来开发ROMs，并对这些OL方法与传统的深度神经网络（DNN）基于ROM进行比较。结果表明，OL方法可以与DNN相比，具有相同的性能和精度，并且在预测批量响应方面更高一点。DNN基于ROM培训时间最快，但所有ROM都比原始MOOSE模型更快，并且仍然提供了准确的预测。FNO的平均预测误差较小， DeepONet在时间相对应的响应中有较大的变化。不同于DNN，FNO和DeepONet都可以不需要维度减少技术来预测时间序列数据。现有的工作可以帮助加快AM优化过程中的Simulation工具执行，保持评估准确性。
</details></li>
</ul>
<hr>
<h2 id="Nonprehensile-Planar-Manipulation-through-Reinforcement-Learning-with-Multimodal-Categorical-Exploration"><a href="#Nonprehensile-Planar-Manipulation-through-Reinforcement-Learning-with-Multimodal-Categorical-Exploration" class="headerlink" title="Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration"></a>Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02459">http://arxiv.org/abs/2308.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar</li>
<li>for: 开发能够实现dexterous nonprehensile manipulation的 робот控制器，如pushing an object on a table。</li>
<li>methods: 使用Reinforcement Learning（RL）框架，并提出了多模态探索方法，以处理非线性和不确定性。</li>
<li>results: 实现了高精度、非平滑曲线和复杂运动的推动政策，并且可以承受外部干扰和观测噪声，同时也可以在多个推动器情况下进行缩放。<details>
<summary>Abstract</summary>
Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train planar pushing RL policies for arbitrary starting and target object poses, i.e. positions and orientations, and with improved accuracy. We show that the learned policies are robust to external disturbances and observation noise, and scale to tasks with multiple pushers. Furthermore, we validate the transferability of the learned policies, trained entirely in simulation, to a physical robot hardware using the KUKA iiwa robot arm. See our supplemental video: https://youtu.be/vTdva1mgrk4.
</details>
<details>
<summary>摘要</summary>
开发能够实现灵活无握 manipulate robot控制器，如表面上推pushing一个物体，是一项挑战。由于控制器的下降启动和混合动力学性质，以及由摩擦产生的不确定性，需要复杂的控制行为。 reinforcement learning (RL) 是一个强大的框架 для开发这类 robot控制器。然而，过去RL文献中对非握持推动任务的精度、非精炸曲线和简单运动（即不含旋转）很低。我们 conjecture  previous 使用单模态探索策略 failed  to capture 任务的内在混合动力学性质， arise  from 不同的接触交互方式 between the robot and the object， such as sticking, sliding, and separation.在这种工作中，我们提议使用多模态探索方法，通过 categorical distributions，可以训练平面推动 RL 政策，对于任意开始和目标物体姿态（位置和 orientations），并具有改进的精度。我们显示了学习的策略对于外部干扰和观察噪声具有Robustness，并可扩展到多个推动者任务。此外，我们验证了学习策略，完全在 simulator 中训练，在物理 Kuka iiwa 机械臂上运行。请参考我们的补充视频：https://youtu.be/vTdva1mgrk4.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-and-Propagation-in-Accelerated-MRI-Reconstruction"><a href="#Uncertainty-Estimation-and-Propagation-in-Accelerated-MRI-Reconstruction" class="headerlink" title="Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction"></a>Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02631">http://arxiv.org/abs/2308.02631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paulkogni/mr-recon-uq">https://github.com/paulkogni/mr-recon-uq</a></li>
<li>paper_authors: Paul Fischer, Thomas Küstner, Christian F. Baumgartner</li>
<li>for: 这篇论文是关于基于深度学习的MRI重建技术的研究，尤其是在高速 Settings下 achievable 的重建质量。</li>
<li>methods: 该论文提出了一种新的 probabilistic reconstruction technique (PHiRec)，基于conditional hierarchical variational autoencoders的想法。</li>
<li>results: 该方法可以生成高质量的重建结果，同时也可以提供substantially better calibrated的uncertainty quantification，以及可以协调到下游 segmentation 任务中的uncertainty estimate。<details>
<summary>Abstract</summary>
MRI reconstruction techniques based on deep learning have led to unprecedented reconstruction quality especially in highly accelerated settings. However, deep learning techniques are also known to fail unexpectedly and hallucinate structures. This is particularly problematic if reconstructions are directly used for downstream tasks such as real-time treatment guidance or automated extraction of clinical paramters (e.g. via segmentation). Well-calibrated uncertainty quantification will be a key ingredient for safe use of this technology in clinical practice. In this paper we propose a novel probabilistic reconstruction technique (PHiRec) building on the idea of conditional hierarchical variational autoencoders. We demonstrate that our proposed method produces high-quality reconstructions as well as uncertainty quantification that is substantially better calibrated than several strong baselines. We furthermore demonstrate how uncertainties arising in the MR econstruction can be propagated to a downstream segmentation task, and show that PHiRec also allows well-calibrated estimation of segmentation uncertainties that originated in the MR reconstruction process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Modelling-of-Levy-Area-for-High-Order-SDE-Simulation"><a href="#Generative-Modelling-of-Levy-Area-for-High-Order-SDE-Simulation" class="headerlink" title="Generative Modelling of Lévy Area for High Order SDE Simulation"></a>Generative Modelling of Lévy Area for High Order SDE Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02452">http://arxiv.org/abs/2308.02452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andraž Jelinčič, Jiajie Tao, William F. Turner, Thomas Cass, James Foster, Hao Ni</li>
<li>for: 这篇论文是为了提出一种基于深度学习的模型，用于生成精确的莱比零区 conditional on  Брау内幂增量。</li>
<li>methods: 这种模型使用了一种专门设计的 GNN-inspired 架构，以保证输出分布和 conditioning 变量之间的正确依赖关系。同时，使用了一种基于特征函数的数学原理的 discriminator。</li>
<li>results: 对于 4 维 Брау内幂，这种模型 exhibits state-of-the-art 性能 across 多个维度，并且在数学金融中的 log-Heston 模型中进行了一个数值实验，证明了高质量的 synthetic 莱比零区可以导致高阶弱 convergence 和 variance reduction when using multilevel Monte Carlo (MLMC)。<details>
<summary>Abstract</summary>
It is well known that, when numerically simulating solutions to SDEs, achieving a strong convergence rate better than O(\sqrt{h}) (where h is the step size) requires the use of certain iterated integrals of Brownian motion, commonly referred to as its "L\'{e}vy areas". However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature and for a d-dimensional Brownian motion with d > 2, no fast almost-exact sampling algorithm is known.   In this paper, we propose L\'{e}vyGAN, a deep-learning-based model for generating approximate samples of L\'{e}vy area conditional on a Brownian increment. Due to our "Bridge-flipping" operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored GNN-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function based discriminator. Lastly, we introduce a novel training mechanism termed "Chen-training", which circumvents the need for expensive-to-generate training data-sets. This new training procedure is underpinned by our two main theoretical results.   For 4-dimensional Brownian motion, we show that L\'{e}vyGAN exhibits state-of-the-art performance across several metrics which measure both the joint and marginal distributions. We conclude with a numerical experiment on the log-Heston model, a popular SDE in mathematical finance, demonstrating that high-quality synthetic L\'{e}vy area can lead to high order weak convergence and variance reduction when using multilevel Monte Carlo (MLMC).
</details>
<details>
<summary>摘要</summary>
它已经广泛知道，当数值实现解决涨落方程时，以较好的减法速率（即幂函数）为依据，需要使用某些迭代积分的布朗运动，通常称为其"Lévy区域"。然而，这些随机积分具有非高斯性质，而且为高维布朗运动（d > 2），没有快速准确样本生成算法。在这篇论文中，我们提出了LévyGAN，一种基于深度学习的模型，用于生成 conditional Lévy area 的相似样本。由于我们的 "桥跃" 操作，输出样本满足所有的共同偶极值和条件偶极值。我们的生成器采用了特制的 GNN-inspired 架构，以保证输出分布和条件变量之间的正确依赖关系。此外，我们采用了基于特征函数的数学原理的批量分类器。最后，我们介绍了一种新的训练机制，称为 "Chen-training"，它使得不需要生成昂贵的训练数据集。这种新的训练过程基于我们的两个主要理论结论。对于四维布朗运动，我们表明了LévyGAN在多个维度上的性能都达到了状态机器人的水平。我们结束于一个对柯本方程（一种流行的数学金融方程）的数字实验，展示了高质量的 synthetic Lévy area 可以导致高阶弱整合和变量减少，当使用多层 Monte Carlo（MLMC）时。
</details></li>
</ul>
<hr>
<h2 id="Pruning-a-neural-network-using-Bayesian-inference"><a href="#Pruning-a-neural-network-using-Bayesian-inference" class="headerlink" title="Pruning a neural network using Bayesian inference"></a>Pruning a neural network using Bayesian inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02451">http://arxiv.org/abs/2308.02451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunil Mathew, Daniel B. Rowe</li>
<li>for: 这个研究论文的目的是提出一种基于 Bayesian 推理的神经网络减少技术，以降低大神经网络的计算和存储占用。</li>
<li>methods: 该方法使用 Bayesian 推理，将神经网络的 posterior 概率分布计算到减少前和减少后，然后利用这些概率导向 iterative 减少。</li>
<li>results: 经过对多个 benchmarck 进行全面评估，我们表明了我们的方法可以实现 жела的稀有性，同时保持竞争性的准确率。<details>
<summary>Abstract</summary>
Neural network pruning is a highly effective technique aimed at reducing the computational and memory demands of large neural networks. In this research paper, we present a novel approach to pruning neural networks utilizing Bayesian inference, which can seamlessly integrate into the training procedure. Our proposed method leverages the posterior probabilities of the neural network prior to and following pruning, enabling the calculation of Bayes factors. The calculated Bayes factors guide the iterative pruning. Through comprehensive evaluations conducted on multiple benchmarks, we demonstrate that our method achieves desired levels of sparsity while maintaining competitive accuracy.
</details>
<details>
<summary>摘要</summary>
大脑网络剪辑是一种非常有效的技术，用于减少大脑网络的计算和内存占用。在这篇研究报告中，我们提出了一种基于 bayesian 推理的 neural network 剪辑方法，可以轻松地 integrate 到训练过程中。我们的提议方法利用 neural network 之前和之后剪辑 posterior 概率，以计算 bayes 因子。计算出的 bayes 因子导引了迭代剪辑。我们在多个 benchmark 上进行了广泛的评估，并证明了我们的方法可以达到所需的稀疏程度，同时保持竞争性的准确率。
</details></li>
</ul>
<hr>
<h2 id="From-Military-to-Healthcare-Adopting-and-Expanding-Ethical-Principles-for-Generative-Artificial-Intelligence"><a href="#From-Military-to-Healthcare-Adopting-and-Expanding-Ethical-Principles-for-Generative-Artificial-Intelligence" class="headerlink" title="From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence"></a>From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02448">http://arxiv.org/abs/2308.02448</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Oniani, Jordan Hilsman, Yifan Peng, COL, Ronald K. Poropatich, COL Jeremy C. Pamplin, LTC Gary L. Legault, Yanshan Wang</li>
<li>For: The paper aims to propose ethical principles for the use of generative AI in healthcare, with the goal of addressing ethical dilemmas and challenges in the integration of this technology in the medical field.* Methods: The paper uses a framework called GREAT PLEA, which stands for Governance, Reliability, Equity, Accountability, Traceability, Privacy, Lawfulness, Empathy, and Autonomy, to guide the development of ethical principles for generative AI in healthcare.* Results: The paper proposes a set of ethical principles for generative AI in healthcare, with the goal of proactively addressing the ethical dilemmas and challenges posed by the integration of this technology in the medical field. These principles include governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy.<details>
<summary>Abstract</summary>
In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about its application in healthcare, mainly due to concerns about transparency and related issues. Meanwhile, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. However, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI. In this paper, we propose GREAT PLEA ethical principles, encompassing governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to proactively address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.
</details>
<details>
<summary>摘要</summary>
在2020年，美国国防部官方公布了一组伦理原则，用于导引人工智能技术在未来战场上的使用。尽管在不同的情况下，军人和医疗人员之间有 stark 的不同，但是在面临生命改变的情况下，他们都需要快速做出决策。战场上的战士常常面临生命改变的情况，需要快速做出决策。医疗人员在医疗环境中也经常面临快速变化的情况，如在急诊室或在治疗生命危险的情况下。新兴的生成型人工智能技术在计算能力的提高和医疗数据的增加下，将健康卫生领域 revolutionized。 recent years, this technology has attracted significant attention from the research community, leading to debates about its application in healthcare, primarily due to concerns about transparency and related issues. However, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. Despite this, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI.在这篇论文中，我们提出了 GREAT PLEA 伦理原则，包括政府、可靠性、公平性、责任、可追溯性、隐私、法律合法性、 Empathy 和自主权，用于生成型人工智能技术在健康卫生领域中的应用。我们希望通过这些原则，active address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Preferential-Attached-kNN-Graph-with-Distribution-Awareness"><a href="#Adaptive-Preferential-Attached-kNN-Graph-with-Distribution-Awareness" class="headerlink" title="Adaptive Preferential Attached kNN Graph with Distribution-Awareness"></a>Adaptive Preferential Attached kNN Graph with Distribution-Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02442">http://arxiv.org/abs/2308.02442</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/4alexmin/knnsotas">https://github.com/4alexmin/knnsotas</a></li>
<li>paper_authors: Shaojie Min, Ji Liu</li>
<li>for: 提高机器学习任务中的总体性能和精度，特别是在具有复杂分布的实际数据上。</li>
<li>methods: 基于分布情况的 adaptive-k 技术，在建构图时采用分布信息作为一体。</li>
<li>results: 在多个实际数据集上进行了严格的评估，并证明了 paNNG 在不同场景下的适应性和效果优于现有算法。<details>
<summary>Abstract</summary>
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks due to their simplicity and effectiveness. However, as factual data often inherit complex distributions, the conventional kNN graph's reliance on a unified k-value can hinder its performance. A crucial factor behind this challenge is the presence of ambiguous samples along decision boundaries that are inevitably more prone to incorrect classifications. To address the situation, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which adopts distribution-aware adaptive-k into graph construction. By incorporating distribution information as a cohesive entity, paNNG can significantly improve performance on ambiguous samples by "pulling" them towards their original classes and hence enhance overall generalization capability. Through rigorous evaluations on diverse datasets, paNNG outperforms state-of-the-art algorithms, showcasing its adaptability and efficacy across various real-world scenarios.
</details>
<details>
<summary>摘要</summary>
基于图的kNN算法在机器学习任务中广泛受欢迎，因为它们简单易用而且有效。然而，由于实际数据经常具有复杂的分布，传统的kNN图中对一个固定的k值的依赖可能会降低其表现。这个挑战的关键原因在于具有抽象样本的异常高错误率，这些样本通常位于分类边界附近。为解决这个问题，我们提出了Preferential Attached k-Nearest Neighbors Graph（paNNG），该算法在图struc图构建中采用了分布情况的敏感性。通过将分布信息作为一个整体纳入图构建，paNNG可以在抽象样本上提高表现，“拖”这些样本向其原来的类别，从而提高总的泛化能力。经过严谨的评估，paNNG在多种实际场景中舒适地超越了当前的状态árt算法，示出了它的适应性和效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/05/cs.LG_2023_08_05/" data-id="cllurrpa2005ksw885n0xejrf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/06/eess.IV_2023_08_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-06 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/05/cs.SD_2023_08_05/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-05 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
