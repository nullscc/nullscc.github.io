
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-17 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Severity Classification of Parkinson’s Disease from Speech using Single Frequency Filtering-based Features paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.09042 repo_url: None paper_authors: Sudarsana Reddy Kadi">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-17 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/17/cs.SD_2023_08_17/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Severity Classification of Parkinson’s Disease from Speech using Single Frequency Filtering-based Features paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.09042 repo_url: None paper_authors: Sudarsana Reddy Kadi">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-16T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:32.373Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/17/cs.SD_2023_08_17/" class="article-date">
  <time datetime="2023-08-16T16:00:00.000Z" itemprop="datePublished">2023-08-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-17 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Severity-Classification-of-Parkinson’s-Disease-from-Speech-using-Single-Frequency-Filtering-based-Features"><a href="#Severity-Classification-of-Parkinson’s-Disease-from-Speech-using-Single-Frequency-Filtering-based-Features" class="headerlink" title="Severity Classification of Parkinson’s Disease from Speech using Single Frequency Filtering-based Features"></a>Severity Classification of Parkinson’s Disease from Speech using Single Frequency Filtering-based Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09042">http://arxiv.org/abs/2308.09042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Manila Kodali, Paavo Alku</li>
<li>for: 本研究旨在提出一种新的评估多动性 Parkinson 病（PD）严重程度的对象方法，以提高诊断和治疗的效果。</li>
<li>methods: 该研究使用了单频 filtering（SFF）方法 derive two sets of novel features：（1）SFF cepstral coefficients（SFFCC）和（2）MFCCs from SFF（MFCC-SFF），用于分类PD严重程度。SFF 方法可提供更高的spectro-temporal resolution，而且在多种应用中有着广泛的应用。</li>
<li>results: 实验使用 SVM 分类器，表明提案的特征在三种说话任务（元音、句子、文本读取）中都有出色的表现，与传统的 MFCC 特征相比，提案的 SFFCC 和 MFCC-SFF 特征在元音任务中提高了5.8%和2.3%，在句子任务中提高了7.0%和1.8%，在文本读取任务中提高了2.4%和1.1%。<details>
<summary>Abstract</summary>
Developing objective methods for assessing the severity of Parkinson's disease (PD) is crucial for improving the diagnosis and treatment. This study proposes two sets of novel features derived from the single frequency filtering (SFF) method: (1) SFF cepstral coefficients (SFFCC) and (2) MFCCs from the SFF (MFCC-SFF) for the severity classification of PD. Prior studies have demonstrated that SFF offers greater spectro-temporal resolution compared to the short-time Fourier transform. The study uses the PC-GITA database, which includes speech of PD patients and healthy controls produced in three speaking tasks (vowels, sentences, text reading). Experiments using the SVM classifier revealed that the proposed features outperformed the conventional MFCCs in all three speaking tasks. The proposed SFFCC and MFCC-SFF features gave a relative improvement of 5.8% and 2.3% for the vowel task, 7.0% & 1.8% for the sentence task, and 2.4% and 1.1% for the read text task, in comparison to MFCC features.
</details>
<details>
<summary>摘要</summary>
开发Objective方法评估parkinson病（PD）的严重程度是诊断和治疗的关键。本研究提出了两组新的特征：（1）单频 filtering（SFF）cepstral coefficient（SFFCC）和（2）MFCC from SFF（MFCC-SFF），用于PD严重分类。前研究表明，SFF提供了更高的spectro-temporal分辨率，比short-time Fourier transform。本研究使用PC-GITA数据库，包括PD患者和健康控制者在三种说话任务（vowel、 sentence、 text reading）中的speech。实验表明，提议的特征比普通的MFCC在所有三种说话任务中表现出色，相比MFCC特征，SFFCC和MFCC-SFF特征在vowel任务中提供了5.8%和2.3%的相对改进，在 sentence任务中提供了7.0%和1.8%的相对改进，在read text任务中提供了2.4%和1.1%的相对改进。
</details></li>
</ul>
<hr>
<h2 id="Home-monitoring-for-frailty-detection-through-sound-and-speaker-diarization-analysis"><a href="#Home-monitoring-for-frailty-detection-through-sound-and-speaker-diarization-analysis" class="headerlink" title="Home monitoring for frailty detection through sound and speaker diarization analysis"></a>Home monitoring for frailty detection through sound and speaker diarization analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08985">http://arxiv.org/abs/2308.08985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannis Tevissen, Dan Istrate, Vincent Zalc, Jérôme Boudy, Gérard Chollet, Frédéric Petitpont, Sami Boutamine</li>
<li>for: 这个研究是为了开发一个可靠、隐私保护的家庭监测系统，以预防衰老 population 中的衰退。</li>
<li>methods: 这个研究使用了最新的声音处理和speaker diarization技术，以提高现有的嵌入式系统的性能。</li>
<li>results: 研究表明，使用深度神经网络（DNN）的方法可以提高性能，比传统方法提高约100%。<details>
<summary>Abstract</summary>
As the French, European and worldwide populations are aging, there is a strong interest for new systems that guarantee a reliable and privacy preserving home monitoring for frailty prevention. This work is a part of a global environmental audio analysis system which aims to help identification of Activities of Daily Life (ADL) through human and everyday life sounds recognition, speech presence and number of speakers detection. The focus is made on the number of speakers detection. In this article, we present how recent advances in sound processing and speaker diarization can improve the existing embedded systems. We study the performances of two new methods and discuss the benefits of DNN based approaches which improve performances by about 100%.
</details>
<details>
<summary>摘要</summary>
“由于法国、欧洲和全球人口年龄增长，有强大的需求对于保证可靠且隐私保护的家居监控系统。这个工作是全球环境音频分析系统的一部分，旨在通过人类日常生活声音识别、语音存在和说话人数检测来帮助活动日常生活（ADL）的识别。我们在这篇文章中介绍了最新的音频处理和Speaker diarization技术的进步，并评估了这两种新方法的表现。我们发现这些方法可以提高现有的嵌入式系统表现，并且这些方法的表现提升约100%。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Explicit-Estimation-of-Magnitude-and-Phase-Spectra-in-Parallel-for-High-Quality-Speech-Enhancement"><a href="#Explicit-Estimation-of-Magnitude-and-Phase-Spectra-in-Parallel-for-High-Quality-Speech-Enhancement" class="headerlink" title="Explicit Estimation of Magnitude and Phase Spectra in Parallel for High-Quality Speech Enhancement"></a>Explicit Estimation of Magnitude and Phase Spectra in Parallel for High-Quality Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08926">http://arxiv.org/abs/2308.08926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye-Xin Lu, Yang Ai, Zhen-Hua Ling</li>
<li>for: 提高Speech perceived质量和可读性</li>
<li>methods: 提出了一种新的Speech Enhancement Network（MP-SENet），通过并行地提高了Magnitude和Phasespectra的表示</li>
<li>results: 实验结果表明，MP-SENet在多个任务中具有高质量的Speech增强，包括Speech denoising、dereverberation和频带扩展，并且成功避免了对Magnitude和Phase的相互赔偿效果，从而实现了更好的harmonic Restoration。特别是在Speech denoising任务上，MP-SENet实现了公共数据集VoiceBank+DEMAND上的最佳性能，PESQ为3.60。<details>
<summary>Abstract</summary>
Phase information has a significant impact on speech perceptual quality and intelligibility. However, existing speech enhancement methods encounter limitations in explicit phase estimation due to the non-structural nature and wrapping characteristics of the phase, leading to a bottleneck in enhanced speech quality. To overcome the above issue, in this paper, we proposed MP-SENet, a novel Speech Enhancement Network which explicitly enhances Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by time-frequency Transformers along both time and frequency dimensions. The encoder aims to encode time-frequency representations derived from the input distorted magnitude and phase spectra. The decoder comprises dual-stream magnitude and phase decoders, directly enhancing magnitude and wrapped phase spectra by incorporating a magnitude estimation architecture and a phase parallel estimation architecture, respectively. To train the MP-SENet model effectively, we define multi-level loss functions, including mean square error and perceptual metric loss of magnitude spectra, anti-wrapping loss of phase spectra, as well as mean square error and consistency loss of short-time complex spectra. Experimental results demonstrate that our proposed MP-SENet excels in high-quality speech enhancement across multiple tasks, including speech denoising, dereverberation, and bandwidth extension. Compared to existing phase-aware speech enhancement methods, it successfully avoids the bidirectional compensation effect between the magnitude and phase, leading to a better harmonic restoration. Notably, for the speech denoising task, the MP-SENet yields a state-of-the-art performance with a PESQ of 3.60 on the public VoiceBank+DEMAND dataset.
</details>
<details>
<summary>摘要</summary>
干扰信息对语音质量和可读性有着重要的影响。然而，现有的语音增强方法在显式阶段的阶段估计中遇到了限制，因为干扰信息的非结构性和包袋特性，导致增强语音质量的瓶颈。为解决上述问题，在这篇论文中，我们提出了MP-SENet，一种新的语音增强网络。MP-SENet在平行地增强了大小和频谱的谱 spectrum。提案的MP-SENet采用了codec架构，编码器和解码器通过时间频率变换器在时间和频率维度上相互连接。编码器的目标是将时间频率表示转化为输入损坏的大小和频谱 spectra。解码器包括两个同步的大小和包袋解码器，直接使用包含大小估计架构和包袋平行估计架构来增强损坏的大小和包袋 spectra。为了训练MP-SENet模型，我们定义了多级损失函数，包括平均平方误差和感知度 metric损失、反包袋损失、平均平方误差和一致性损失。实验结果表明，我们的提案的MP-SENet在多个任务中实现了高质量的语音增强，包括语音干扰、频率抑制和频谱扩展。与现有的阶段意识的语音增强方法相比，MP-SENet成功避免了对大小和频谱的双向赔率效应，从而实现了更好的干扰还原。特别是在语音干扰任务中，MP-SENet的PESQ为3.60，在公共的 VoiceBank+DEMAND 数据集上达到了状态机的性能。
</details></li>
</ul>
<hr>
<h2 id="Long-frame-shift-Neural-Speech-Phase-Prediction-with-Spectral-Continuity-Enhancement-and-Interpolation-Error-Compensation"><a href="#Long-frame-shift-Neural-Speech-Phase-Prediction-with-Spectral-Continuity-Enhancement-and-Interpolation-Error-Compensation" class="headerlink" title="Long-frame-shift Neural Speech Phase Prediction with Spectral Continuity Enhancement and Interpolation Error Compensation"></a>Long-frame-shift Neural Speech Phase Prediction with Spectral Continuity Enhancement and Interpolation Error Compensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08850">http://arxiv.org/abs/2308.08850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangai520/lfs-nspp">https://github.com/yangai520/lfs-nspp</a></li>
<li>paper_authors: Yang Ai, Ye-Xin Lu, Zhen-Hua Ling</li>
<li>for: 提高信号处理领域中语音频谱预测的精度，使其能够准确地预测长框框帧偏移的语音频谱。</li>
<li>methods: 提出了一种基于神经网络的长框帧偏移预测方法（LFS-NSPP），包括三个阶段： interpolate、predict和decimate。首先将长框帧频谱spectra interpolated到短框帧频谱spectra的频谱上，然后使用NSPP模型预测短框帧偏移 spectra，最后将长框帧偏移 spectra decimated into short-frame shift phase spectra。</li>
<li>results: 实验结果表明，提出的LFS-NSPP方法可以在预测长框帧偏移 phase spectra方面达到更高的质量，比原始NSPP模型和其他信号处理基于频谱估算法更好。<details>
<summary>Abstract</summary>
Speech phase prediction, which is a significant research focus in the field of signal processing, aims to recover speech phase spectra from amplitude-related features. However, existing speech phase prediction methods are constrained to recovering phase spectra with short frame shifts, which are considerably smaller than the theoretical upper bound required for exact waveform reconstruction of short-time Fourier transform (STFT). To tackle this issue, we present a novel long-frame-shift neural speech phase prediction (LFS-NSPP) method which enables precise prediction of long-frame-shift phase spectra from long-frame-shift log amplitude spectra. The proposed method consists of three stages: interpolation, prediction and decimation. The short-frame-shift log amplitude spectra are first constructed from long-frame-shift ones through frequency-by-frequency interpolation to enhance the spectral continuity, and then employed to predict short-frame-shift phase spectra using an NSPP model, thereby compensating for interpolation errors. Ultimately, the long-frame-shift phase spectra are obtained from short-frame-shift ones through frame-by-frame decimation. Experimental results show that the proposed LFS-NSPP method can yield superior quality in predicting long-frame-shift phase spectra than the original NSPP model and other signal-processing-based phase estimation algorithms.
</details>
<details>
<summary>摘要</summary>
干扰语音阶段预测（Speech phase prediction）是信号处理领域的一个重要研究方向，旨在从振荡功率相关特征中恢复语音阶段спектроgram。然而，现有的语音阶段预测方法都是固定的具有短框架偏移的phaspectra，这些偏移远小于理论最大允许的束缚波形重建短时域 Fourier transform（STFT）。为解决这个问题，我们提出了一种新的长框架偏移神经语音阶段预测（LFS-NSPP）方法，可以准确预测长框架偏移phaspectra从长框架偏移log amplitude spectra。该方法包括三个阶段： interpolate、predict和decimate。首先，从长框架偏移log amplitude spectra中提取出频率维度上的各个频率域的短框架偏移spectra，然后使用NSPP模型预测短框架偏移phaspectra，从而补偿插值错误。最后，通过frame-by-frame decimation，从短框架偏移phaspectra中提取出长框架偏移phaspectra。实验结果表明，提出的LFS-NSPP方法可以在预测长框架偏移phaspectra的质量上提高比原NSPP模型和其他信号处理基于阶段估计算法。
</details></li>
</ul>
<hr>
<h2 id="META-SELD-Meta-Learning-for-Fast-Adaptation-to-the-new-environment-in-Sound-Event-Localization-and-Detection"><a href="#META-SELD-Meta-Learning-for-Fast-Adaptation-to-the-new-environment-in-Sound-Event-Localization-and-Detection" class="headerlink" title="META-SELD: Meta-Learning for Fast Adaptation to the new environment in Sound Event Localization and Detection"></a>META-SELD: Meta-Learning for Fast Adaptation to the new environment in Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08847">http://arxiv.org/abs/2308.08847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbo Hu, Yin Cao, Ming Wu, Feiran Yang, Ziying Yu, Wenwu Wang, Mark D. Plumbley, Jun Yang</li>
<li>for: 这个研究是为了解决学习型 зву标定和探测（SELD）方法在不同的Acoustic环境下的性能差异问题。</li>
<li>methods: 这个研究使用了Meta-学习方法来实现快速适应新环境。基于Model Agnostic Meta-Learning（MAML）的Meta-SELD，将寻找适合新环境的初始化参数，并快速适应未见过的环境。</li>
<li>results: 实验结果显示，Meta-SELD在适应新环境方面效果非常好。<details>
<summary>Abstract</summary>
For learning-based sound event localization and detection (SELD) methods, different acoustic environments in the training and test sets may result in large performance differences in the validation and evaluation stages. Different environments, such as different sizes of rooms, different reverberation times, and different background noise, may be reasons for a learning-based system to fail. On the other hand, acquiring annotated spatial sound event samples, which include onset and offset time stamps, class types of sound events, and direction-of-arrival (DOA) of sound sources is very expensive. In addition, deploying a SELD system in a new environment often poses challenges due to time-consuming training and fine-tuning processes. To address these issues, we propose Meta-SELD, which applies meta-learning methods to achieve fast adaptation to new environments. More specifically, based on Model Agnostic Meta-Learning (MAML), the proposed Meta-SELD aims to find good meta-initialized parameters to adapt to new environments with only a small number of samples and parameter updating iterations. We can then quickly adapt the meta-trained SELD model to unseen environments. Our experiments compare fine-tuning methods from pre-trained SELD models with our Meta-SELD on the Sony-TAU Realistic Spatial Soundscapes 2023 (STARSSS23) dataset. The evaluation results demonstrate the effectiveness of Meta-SELD when adapting to new environments.
</details>
<details>
<summary>摘要</summary>
для学习基于声学Event localization and detection（SELD）方法，不同的声学环境在训练和测试集中可能会导致大幅度的性能差异在验证和评估阶段。不同的环境，如不同的房间大小、不同的延迟时间和不同的背景噪音，可能是学习基于系统失败的原因。同时，获取标注的空间声Event样本，包括启动和终止时间戳、声音事件类型和声音源的方向来达（DOA），非常昂贵。此外，在新环境中部署SELD系统经常会出现时间consuming的训练和精度调整问题。为解决这些问题，我们提出Meta-SELD，它应用meta学方法来实现快速适应新环境。更具体地说，基于Model Agnostic Meta-Learning（MAML），我们的Meta-SELD寻找适合新环境的好初始化参数，只需要一小数量的样本和参数更新迭代即可快速适应新环境。我们的实验比较了从预训练SELD模型的细化方法与我们的Meta-SELD在SONY-TAU Realistic Spatial Soundscapes 2023（STARSSS23）数据集上的性能。评估结果表明，Meta-SELD在适应新环境时非常有效。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Network-Backend-for-Speaker-Recognition"><a href="#Graph-Neural-Network-Backend-for-Speaker-Recognition" class="headerlink" title="Graph Neural Network Backend for Speaker Recognition"></a>Graph Neural Network Backend for Speaker Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08767">http://arxiv.org/abs/2308.08767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang He, Ruida Li, Mengqi Niu</li>
<li>for: 提高 speaker recognition 精度</li>
<li>methods: 使用图 neural network (GNN)  backend， Mine latent relationships among embeddings for classification</li>
<li>results: 在 NIST SRE14 i-vector challenging、VoxCeleb1-O、VoxCeleb1-E 和 VoxCeleb1-H 数据集上，与主流方法相比，提出的 GNN  backend 显示出了显著的提高。<details>
<summary>Abstract</summary>
Currently, most speaker recognition backends, such as cosine, linear discriminant analysis (LDA), or probabilistic linear discriminant analysis (PLDA), make decisions by calculating similarity or distance between enrollment and test embeddings which are already extracted from neural networks. However, for each embedding, the local structure of itself and its neighbor embeddings in the low-dimensional space is different, which may be helpful for the recognition but is often ignored. In order to take advantage of it, we propose a graph neural network (GNN) backend to mine latent relationships among embeddings for classification. We assume all the embeddings as nodes on a graph, and their edges are computed based on some similarity function, such as cosine, LDA+cosine, or LDA+PLDA. We study different graph settings and explore variants of GNN to find a better message passing and aggregation way to accomplish the recognition task. Experimental results on NIST SRE14 i-vector challenging, VoxCeleb1-O, VoxCeleb1-E, and VoxCeleb1-H datasets demonstrate that our proposed GNN backends significantly outperform current mainstream methods.
</details>
<details>
<summary>摘要</summary>
当前大多数说话识别后端，如cosine、线性混合分析（LDA）或概率线性混合分析（PLDA），做出决策时通常计算测试和托管模型之间的相似性或距离。然而，每个嵌入都有自己本地结构，与邻居嵌入在低维度空间中的结构不同，这可能对识别有帮助，但通常被忽略。为了利用这一点，我们提议使用图ael neural network（GNN）后端，挖掘嵌入之间的隐藏关系，用于分类。我们将所有嵌入视为图ael中的节点，其间的边是根据某种相似函数，如cosine、LDA+cosine或LDA+PLDA计算。我们研究不同的图ael设置和GNN变体，找到更好的消息传递和聚合方式，以完成识别任务。实验结果表明，我们提议的GNN后端在NIST SRE14 i-vector挑战、VoxCeleb1-O、VoxCeleb1-E和VoxCeleb1-H数据集上显著超越了当前主流方法。
</details></li>
</ul>
<hr>
<h2 id="The-DKU-MSXF-Speaker-Verification-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#The-DKU-MSXF-Speaker-Verification-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="The DKU-MSXF Speaker Verification System for the VoxCeleb Speaker Recognition Challenge 2023"></a>The DKU-MSXF Speaker Verification System for the VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08766">http://arxiv.org/abs/2308.08766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Li, Yuke Lin, Xiaoyi Qin, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 本文是DKU-MSXF系统的track1、track2和track3的VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）系统描述。</li>
<li>methods: 我们使用基于ResNet的网络结构进行训练，并通过构建跨年龄QMF训练集来实现显著提高系统性能。</li>
<li>results: 在track1中，我们通过混合训练方法和VOXBLINK-clean数据集来提高模型性能，相比track1，包含VOXBLINK-clean数据集的模型表现提高了 más de 10%。在track3中，我们采用了一种新的假标签方法，并通过三个阈值和子中心纯化来进行频率附加预测，最终提交得分为task1的mDCF0.1243、track2的mDCF0.1165和track3的EER4.952%。<details>
<summary>Abstract</summary>
This paper is the system description of the DKU-MSXF System for the track1, track2 and track3 of the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23). For Track 1, we utilize a network structure based on ResNet for training. By constructing a cross-age QMF training set, we achieve a substantial improvement in system performance. For Track 2, we inherite the pre-trained model from Track 1 and conducte mixed training by incorporating the VoxBlink-clean dataset. In comparison to Track 1, the models incorporating VoxBlink-clean data exhibit a performance improvement by more than 10% relatively. For Track3, the semi-supervised domain adaptation task, a novel pseudo-labeling method based on triple thresholds and sub-center purification is adopted to make domain adaptation. The final submission achieves mDCF of 0.1243 in task1, mDCF of 0.1165 in Track 2 and EER of 4.952% in Track 3.
</details>
<details>
<summary>摘要</summary>
这篇论文是DKU-MSXF系统的系统描述，用于VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）的track1、track2和track3。在track1中，我们采用基于ResNet的网络结构进行训练，通过构建跨年龄QMF训练集，实现了显著提高系统性能。在track2中，我们继承了track1中的预训练模型，并通过将VoxBlink-clean数据集integrated进行混合训练，相比track1，模型包含VoxBlink-clean数据显示了超过10%的性能提高。在track3中，我们采用了一种新的半监督领域适应方法，基于 triple thresholds和sub-center purification，实现了领域适应。最终提交的结果为task1中的mDCF为0.1243，track2中的mDCF为0.1165，以及track3中的EER为4.952%。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Emotions-A-comprehensive-Multilingual-Study-of-Speech-Models-for-Speech-Emotion-Recognition"><a href="#Decoding-Emotions-A-comprehensive-Multilingual-Study-of-Speech-Models-for-Speech-Emotion-Recognition" class="headerlink" title="Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition"></a>Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08713">http://arxiv.org/abs/2308.08713</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/95anantsingh/decoding-emotions">https://github.com/95anantsingh/decoding-emotions</a></li>
<li>paper_authors: Anant Singh, Akshat Gupta</li>
<li>for: 这paper主要是为了评估多种语言下的语音情感识别（SER）模型，以及这些模型的内部表示方法。</li>
<li>methods: 这paper使用了八种语音表示模型和六种语言进行比较，并通过 probing 实验来探索这些模型的内部工作方式。</li>
<li>results: 这paper得到了 average 的错误率下降32%，并在德语和波斯语中达到了状态的最佳 результаados。 probing 结果表明，语音模型的中间层 capture 最重要的情感信息。<details>
<summary>Abstract</summary>
Recent advancements in transformer-based speech representation models have greatly transformed speech processing. However, there has been limited research conducted on evaluating these models for speech emotion recognition (SER) across multiple languages and examining their internal representations. This article addresses these gaps by presenting a comprehensive benchmark for SER with eight speech representation models and six different languages. We conducted probing experiments to gain insights into inner workings of these models for SER. We find that using features from a single optimal layer of a speech model reduces the error rate by 32\% on average across seven datasets when compared to systems where features from all layers of speech models are used. We also achieve state-of-the-art results for German and Persian languages. Our probing results indicate that the middle layers of speech models capture the most important emotional information for speech emotion recognition.
</details>
<details>
<summary>摘要</summary>
近期的变换器基本模型在语音处理方面做出了重要进步，但是对于多语言语音情感识别（SER）的评估和内部表示却受到了有限的研究。本文填补这些差距，通过提供八种语音表示模型和六种不同语言的完整性评估。我们进行了探索实验，以了解这些模型内部的工作方式。我们发现，从单一最佳层的语音模型中提取特征可以降低错误率平均为32%，比较于使用所有层语音模型特征系统来说。我们还在德国语和波斯语方面达到了状态的最佳成绩。我们的探索结果表明，语音模型的中间层 capture最重要的情感信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/17/cs.SD_2023_08_17/" data-id="cllurrpb8009fsw88dkbv9is4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/17/cs.LG_2023_08_17/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-17 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/17/eess.IV_2023_08_17/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-17 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
