
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-17 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08974 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yilinliu610730&#x2F;eoe paper">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-17">
<meta property="og:url" content="https://nullscc.github.io/2023/08/17/eess.IV_2023_08_17/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08974 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yilinliu610730&#x2F;eoe paper">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-17T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:50:33.511Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/17/eess.IV_2023_08_17/" class="article-date">
  <time datetime="2023-08-17T09:00:00.000Z" itemprop="datePublished">2023-08-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-17
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Eosinophils-Instance-Object-Segmentation-on-Whole-Slide-Imaging-Using-Multi-label-Circle-Representation"><a href="#Eosinophils-Instance-Object-Segmentation-on-Whole-Slide-Imaging-Using-Multi-label-Circle-Representation" class="headerlink" title="Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation"></a>Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08974">http://arxiv.org/abs/2308.08974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilinliu610730/eoe">https://github.com/yilinliu610730/eoe</a></li>
<li>paper_authors: Yilin Liu, Ruining Deng, Juming Xiong, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo</li>
<li>for: 这个论文主要针对的是营养不良引起的食管炎的诊断和评估。</li>
<li>methods: 这篇论文提出了一种基于圆形图像分割的多标签圆形蛇形态模型，用于自动检测和分割食管中的凝血细胞。</li>
<li>results: 实验结果表明，这种多标签圆形蛇形态模型在食管炎细胞的标准化分割 tasks 中的平均准确率比传统的Mask R-CNN模型和DeepSnake模型高。<details>
<summary>Abstract</summary>
Eosinophilic esophagitis (EoE) is a chronic and relapsing disease characterized by esophageal inflammation. Symptoms of EoE include difficulty swallowing, food impaction, and chest pain which significantly impact the quality of life, resulting in nutritional impairments, social limitations, and psychological distress. The diagnosis of EoE is typically performed with a threshold (15 to 20) of eosinophils (Eos) per high-power field (HPF). Since the current counting process of Eos is a resource-intensive process for human pathologists, automatic methods are desired. Circle representation has been shown as a more precise, yet less complicated, representation for automatic instance cell segmentation such as CircleSnake approach. However, the CircleSnake was designed as a single-label model, which is not able to deal with multi-label scenarios. In this paper, we propose the multi-label CircleSnake model for instance segmentation on Eos. It extends the original CircleSnake model from a single-label design to a multi-label model, allowing segmentation of multiple object types. Experimental results illustrate the CircleSnake model's superiority over the traditional Mask R-CNN model and DeepSnake model in terms of average precision (AP) in identifying and segmenting eosinophils, thereby enabling enhanced characterization of EoE. This automated approach holds promise for streamlining the assessment process and improving diagnostic accuracy in EoE analysis. The source code has been made publicly available at https://github.com/yilinliu610730/EoE.
</details>
<details>
<summary>摘要</summary>
恶性肠炎（EoE）是一种慢性和复发的疾病， caracterized by esophageal inflammation.  Its symptoms include difficulty swallowing, food impaction, and chest pain, which significantly impact the quality of life, resulting in nutritional impairments, social limitations, and psychological distress. The diagnosis of EoE is typically performed with a threshold (15 to 20) of eosinophils (Eos) per high-power field (HPF). Since the current counting process of Eos is a resource-intensive process for human pathologists, automatic methods are desired. Circle representation has been shown as a more precise, yet less complicated, representation for automatic instance cell segmentation such as CircleSnake approach. However, the CircleSnake was designed as a single-label model, which is not able to deal with multi-label scenarios. In this paper, we propose the multi-label CircleSnake model for instance segmentation on Eos. It extends the original CircleSnake model from a single-label design to a multi-label model, allowing segmentation of multiple object types. Experimental results illustrate the CircleSnake model's superiority over the traditional Mask R-CNN model and DeepSnake model in terms of average precision (AP) in identifying and segmenting eosinophils, thereby enabling enhanced characterization of EoE. This automated approach holds promise for streamlining the assessment process and improving diagnostic accuracy in EoE analysis. The source code has been made publicly available at https://github.com/yilinliu610730/EoE.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="An-inexact-proximal-majorization-minimization-Algorithm-for-remote-sensing-image-stripe-noise-removal"><a href="#An-inexact-proximal-majorization-minimization-Algorithm-for-remote-sensing-image-stripe-noise-removal" class="headerlink" title="An inexact proximal majorization-minimization Algorithm for remote sensing image stripe noise removal"></a>An inexact proximal majorization-minimization Algorithm for remote sensing image stripe noise removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08866">http://arxiv.org/abs/2308.08866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjing Wang, Xile Zhao, Qingsong Wang, Zepei Ma, Peipei Tang</li>
<li>for: 提高remote sensing图像的视觉质量和数据分析精度，适应现有的扫描图像中的条带噪音。</li>
<li>methods: 提出一种非凸模型，使用DC函数（即差分凸函数）结构来除掉条带噪音。解决这个模型的方法是利用DC结构和不准确的 proximal 大antero-multipliers 算法，并设计了可实现的停止 criterion。</li>
<li>results: 对于numerical experiments表明，提出的模型和算法具有超越现有模型和算法的优势。<details>
<summary>Abstract</summary>
The stripe noise existing in remote sensing images badly degrades the visual quality and restricts the precision of data analysis. Therefore, many destriping models have been proposed in recent years. In contrast to these existing models, in this paper, we propose a nonconvex model with a DC function (i.e., the difference of convex functions) structure to remove the strip noise. To solve this model, we make use of the DC structure and apply an inexact proximal majorization-minimization algorithm with each inner subproblem solved by the alternating direction method of multipliers. It deserves mentioning that we design an implementable stopping criterion for the inner subproblem, while the convergence can still be guaranteed. Numerical experiments demonstrate the superiority of the proposed model and algorithm.
</details>
<details>
<summary>摘要</summary>
Remote sensing 图像中的条纹噪音会严重损害视觉质量和数据分析精度。因此，过去几年内，许多推 striping 模型已经被提出。与现有模型不同，在这篇论文中，我们提出了一种非凸模型，使用差分 convex 函数（i.e., 条纹函数的差）结构来除掉条纹噪音。为解这个模型，我们利用 DC 结构，并采用不准确的 proximal 大anterograde 方法，每个内部问题都由 alternate 方向多重因素方法解决。值得一提的是，我们设计了可实现的停止条件，而且可以保证 convergence。数字实验表明，我们提出的模型和算法具有superiority。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Alternating-Optimization-for-Real-World-Blind-Super-Resolution"><a href="#End-to-end-Alternating-Optimization-for-Real-World-Blind-Super-Resolution" class="headerlink" title="End-to-end Alternating Optimization for Real-World Blind Super Resolution"></a>End-to-end Alternating Optimization for Real-World Blind Super Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08816">http://arxiv.org/abs/2308.08816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/greatlog/realdan">https://github.com/greatlog/realdan</a></li>
<li>paper_authors: Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan</li>
<li>for: This paper proposes a new method for blind super-resolution (SR) of low-resolution (LR) images, which can estimate the degradation of the LR image and super-resolve it to its high-resolution (HR) counterpart in a single model.</li>
<li>methods: The proposed method uses an alternating optimization algorithm that consists of two convolutional neural modules: \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the estimated degradation, while \textit{Estimator} estimates the degradation with the help of the restored SR image.</li>
<li>results: The proposed method outperforms state-of-the-art methods in terms of both objective metrics and visual quality, and produces more visually favorable results. The codes are available at \url{<a target="_blank" rel="noopener" href="https://github.com/greatlog/RealDAN.git%7D">https://github.com/greatlog/RealDAN.git}</a>.<details>
<summary>Abstract</summary>
Blind Super-Resolution (SR) usually involves two sub-problems: 1) estimating the degradation of the given low-resolution (LR) image; 2) super-resolving the LR image to its high-resolution (HR) counterpart. Both problems are ill-posed due to the information loss in the degrading process. Most previous methods try to solve the two problems independently, but often fall into a dilemma: a good super-resolved HR result requires an accurate degradation estimation, which however, is difficult to be obtained without the help of original HR information. To address this issue, instead of considering these two problems independently, we adopt an alternating optimization algorithm, which can estimate the degradation and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the estimated degradation, and \textit{Estimator} estimates the degradation with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, both \textit{Restorer} and \textit{Estimator} could get benefited from the intermediate results of each other, and make each sub-problem easier. Moreover, \textit{Restorer} and \textit{Estimator} are optimized in an end-to-end manner, thus they could get more tolerant of the estimation deviations of each other and cooperate better to achieve more robust and accurate final results. Extensive experiments on both synthetic datasets and real-world images show that the proposed method can largely outperform state-of-the-art methods and produce more visually favorable results. The codes are rleased at \url{https://github.com/greatlog/RealDAN.git}.
</details>
<details>
<summary>摘要</summary>
干Resolution（SR）问题通常包含两个互相关联的优化问题：1）估计LR图像的劣化程度；2）SR图像的恢复。两个问题都是不定的，因为升级过程中会产生信息损失。大多数之前的方法是独立地解决这两个问题，但往往陷入一个困境：一个好的SRHR图像需要一个准确的劣化估计，但是没有原始HR图像的帮助，劣化估计很难以取得。为了解决这个问题，我们不是独立地考虑这两个问题，而是采用一种alternating optimization算法，可以同时估计劣化和SR图像的恢复。我们设计了两个卷积神经网络模块，namely \textit{Restorer}和\textit{Estimator}。\textit{Restorer}使用估计的劣化来恢复SR图像，而\textit{Estimator}使用恢复的SR图像来估计劣化。我们在这两个模块之间重复交换，并将这个过程膨胀成一个可训练的端到端网络。这样，\textit{Restorer}和\textit{Estimator}都可以通过对方的中间结果得到帮助，使每个优化问题更加容易。此外，\textit{Restorer}和\textit{Estimator}在端到端训练中被优化，因此它们可以更快地适应对方的估计偏差，并更好地合作以实现更加稳定和准确的最终结果。我们在Synthetic datasets和实际图像上进行了广泛的实验，结果显示，我们的方法可以大幅超越当前状态的方法，并生成更加视觉吸引人的结果。代码可以在 \url{https://github.com/greatlog/RealDAN.git} 中下载。
</details></li>
</ul>
<hr>
<h2 id="Recursive-Detection-and-Analysis-of-Nanoparticles-in-Scanning-Electron-Microscopy-Images"><a href="#Recursive-Detection-and-Analysis-of-Nanoparticles-in-Scanning-Electron-Microscopy-Images" class="headerlink" title="Recursive Detection and Analysis of Nanoparticles in Scanning Electron Microscopy Images"></a>Recursive Detection and Analysis of Nanoparticles in Scanning Electron Microscopy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08732">http://arxiv.org/abs/2308.08732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidan S. Wright, Nathaniel P. Youmans, Enrique F. Valderrama Araya</li>
<li>for: 准确地检测和分析扫描电子显微镜（SEM）图像中的粒子。</li>
<li>methods: 利用Python图像处理库，如OpenCV、SciPy和Scikit-Image，实现图像处理和提升。</li>
<li>results: 在五个不同的测试图像中，准确地检测粒子，具有97%的准确率，并能够识别杂乱的粒子布局和较弱的粒子信号。<details>
<summary>Abstract</summary>
In this study, we present a computational framework tailored for the precise detection and comprehensive analysis of nanoparticles within scanning electron microscopy (SEM) images. The primary objective of this framework revolves around the accurate localization of nanoparticle coordinates, accompanied by secondary objectives encompassing the extraction of pertinent morphological attributes including area, orientation, brightness, and length.   Constructed leveraging the robust image processing capabilities of Python, particularly harnessing libraries such as OpenCV, SciPy, and Scikit-Image, the framework employs an amalgamation of techniques, including thresholding, dilating, and eroding, to enhance the fidelity of image processing outcomes.   The ensuing nanoparticle data is seamlessly integrated into the RStudio environment to facilitate meticulous post-processing analysis. This encompasses a comprehensive evaluation of model accuracy, discernment of feature distribution patterns, and the identification of intricate particle arrangements. The finalized framework exhibits high nanoparticle identification within the primary sample image and boasts 97\% accuracy in detecting particles across five distinct test images drawn from a SEM nanoparticle dataset. Furthermore, the framework demonstrates the capability to discern nanoparticles of faint intensity, eluding manual labeling within the control group.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了一种计算框架，用于精确检测和全面分析射电显微镜像中的粒子。主要目标是准确地确定粒子坐标，并且包括次要目标，如粒子形态特征的提取，包括面积、方向、亮度和长度。这个框架基于Python的强大图像处理能力，特别是利用OpenCV、SciPy和Scikit-Image库，使用了一系列技术，如阈值处理、扩大和腐蚀，以提高图像处理结果的准确性。得到的粒子数据可以轻松地在RStudio环境中进行仔细的后处理分析，包括精确评估模型准确性、分析特征分布征特点，以及描述复杂的粒子排列。最终的框架在五个不同的射电显微镜像数据集中的主要样本图像中具有高粒子标识能力，并且在五个测试图像中达到97%的粒子检测精度。此外，框架还能够检测具有某些较弱亮度的粒子，而这些粒子在控制组中未能手动标注。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Kernel-Based-Adaptive-Spatial-Aggregation-for-Learned-Image-Compression"><a href="#Dynamic-Kernel-Based-Adaptive-Spatial-Aggregation-for-Learned-Image-Compression" class="headerlink" title="Dynamic Kernel-Based Adaptive Spatial Aggregation for Learned Image Compression"></a>Dynamic Kernel-Based Adaptive Spatial Aggregation for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08723">http://arxiv.org/abs/2308.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Huairui/DKIC">https://github.com/Huairui/DKIC</a></li>
<li>paper_authors: Huairui Wang, Nianxiang Fu, Zhenzhong Chen, Shan Liu</li>
<li>for: 提高图像压缩率和细节准确性。</li>
<li>methods: 使用动态kernel基于转换编码、适应汇集和共享权重机制，以及基于Entropy模型的自适应 globale context生成器。</li>
<li>results: 在三个标准测试集上比州前方学习基于方法 achieve superior rate-distortion performance。<details>
<summary>Abstract</summary>
Learned image compression methods have shown superior rate-distortion performance and remarkable potential compared to traditional compression methods. Most existing learned approaches use stacked convolution or window-based self-attention for transform coding, which aggregate spatial information in a fixed range. In this paper, we focus on extending spatial aggregation capability and propose a dynamic kernel-based transform coding. The proposed adaptive aggregation generates kernel offsets to capture valid information in the content-conditioned range to help transform. With the adaptive aggregation strategy and the sharing weights mechanism, our method can achieve promising transform capability with acceptable model complexity. Besides, according to the recent progress of entropy model, we define a generalized coarse-to-fine entropy model, considering the coarse global context, the channel-wise, and the spatial context. Based on it, we introduce dynamic kernel in hyper-prior to generate more expressive global context. Furthermore, we propose an asymmetric spatial-channel entropy model according to the investigation of the spatial characteristics of the grouped latents. The asymmetric entropy model aims to reduce statistical redundancy while maintaining coding efficiency. Experimental results demonstrate that our method achieves superior rate-distortion performance on three benchmarks compared to the state-of-the-art learning-based methods.
</details>
<details>
<summary>摘要</summary>
现有学习图像压缩方法已经显示出了更高的比特率-损均值性和强大的潜在性，比较传统压缩方法更出色。大多数现有的学习方法使用堆叠 convolution或窗口基于自注意力进行变换编码，这些方法会聚合空间信息在固定范围内。在这篇论文中，我们关注扩展空间聚合能力，并提议一种动态核心基于变换编码。我们的提案的自适应聚合生成核心偏移来捕捉有效信息在内容受限范围内，以帮助变换。通过适应聚合策略和共享权重机制，我们的方法可以实现有优的变换能力，同时保持可接受的模型复杂度。此外，根据最近的Entropy模型进展，我们定义一种通用粗化-细化Entropy模型，考虑了粗化全局上下文、通道级别和空间上下文。基于它，我们引入动态核心在超乘预先中生成更表达力的全局上下文。另外，我们提出一种偏 asymmetric spatial-channel Entropy模型，根据图像拼接特征的调查，以减少统计冗余，保持编码效率。实验结果表明，我们的方法在三个标准测试集上实现了与当前学习基于方法相比更高的比特率-损均值性。
</details></li>
</ul>
<hr>
<h2 id="Deployment-and-Analysis-of-Instance-Segmentation-Algorithm-for-In-field-Grade-Estimation-of-Sweetpotatoes"><a href="#Deployment-and-Analysis-of-Instance-Segmentation-Algorithm-for-In-field-Grade-Estimation-of-Sweetpotatoes" class="headerlink" title="Deployment and Analysis of Instance Segmentation Algorithm for In-field Grade Estimation of Sweetpotatoes"></a>Deployment and Analysis of Instance Segmentation Algorithm for In-field Grade Estimation of Sweetpotatoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08534">http://arxiv.org/abs/2308.08534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang M. Nguyen, Sydney Gyurek, Russell Mierop, Kenneth V. Pecota, Kylie LaGamba, Michael Boyette, G. Craig Yencho, Cranos M. Williams, Michael W. Kudenov</li>
<li>for: 这 paper 的目的是提出一种用于 directly 在田间进行 sweetpotato (SP) 存储根的检测和估算，以便更快速地获得 SP 的产量估算。</li>
<li>methods: 这 paper 使用的方法是使用 Detectron2 库中的深度学习对象检测算法来实现 Mask R-CNN 模型，以进行 SP 存储根的实例分割。</li>
<li>results: 研究结果表明，这个模型可以在不同的环境条件下（包括光照和土壤特性等）正确地检测 SP 存储根，并且与商业光学排分机相比，其误差值（RMSE）为0.66 cm、1.22 cm 和74.73 g，而Root 计数误差值（RMSE）为5.27根，与 r^2 为0.8。这种fenotyping 策略有望在没有高级和昂贵光学排分机的环境中实现快速的产量估算。<details>
<summary>Abstract</summary>
Shape estimation of sweetpotato (SP) storage roots is inherently challenging due to their varied size and shape characteristics. Even measuring "simple" metrics, such as length and width, requires significant time investments either directly in-field or afterward using automated graders. In this paper, we present the results of a model that can perform grading and provide yield estimates directly in the field quicker than manual measurements. Detectron2, a library consisting of deep-learning object detection algorithms, was used to implement Mask R-CNN, an instance segmentation model. This model was deployed for in-field grade estimation of SPs and evaluated against an optical sorter. Storage roots from various clones imaged with a cellphone during trials between 2019 and 2020, were used in the model's training and validation to fine-tune a model to detect SPs. Our results showed that the model could distinguish individual SPs in various environmental conditions including variations in lighting and soil characteristics. RMSE for length, width, and weight, from the model compared to a commercial optical sorter, were 0.66 cm, 1.22 cm, and 74.73 g, respectively, while the RMSE of root counts per plot was 5.27 roots, with r^2 = 0.8. This phenotyping strategy has the potential enable rapid yield estimates in the field without the need for sophisticated and costly optical sorters and may be more readily deployed in environments with limited access to these kinds of resources or facilities.
</details>
<details>
<summary>摘要</summary>
生长缘 estimation of sweetpotato (SP) storage roots 是一项自然而难的任务，主要因为它们的尺寸和形态特征具有很大的变化。甚至计算 "简单" 的指标，如长度和宽度，都需要投入很大的时间，可能是在场地上直接进行或使用自动化分级机器人。在这篇论文中，我们介绍了一种模型，可以在场地上进行排名和产量估算，比手动测量更快。使用 Detectron2 库中的深度学习对象检测算法，我们实现了面部分 Segmentation 模型。这个模型在场地上进行了 SP 的排名和产量估算，并与商业光学分级机器人进行比较。我们使用了不同的 CLONE 的存储根，在2019-2020 年的试验中拍摄了它们，并用于模型的训练和验证。我们的结果表明，模型可以在不同的环境条件下（包括光照和土壤特征）分辨准确的 SP。length、width 和 weight 的 RMSE 与商业光学分级机器人相比为 0.66 cm、1.22 cm 和 74.73 g，分类精度为 0.8。这种fenotyping策略有可能在不同的环境中快速地进行产量估算，不需要高昂的光学分级机器人，可能更容易在有限的资源和设施的环境中进行。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Distill-Global-Representation-for-Sparse-View-CT"><a href="#Learning-to-Distill-Global-Representation-for-Sparse-View-CT" class="headerlink" title="Learning to Distill Global Representation for Sparse-View CT"></a>Learning to Distill Global Representation for Sparse-View CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08463">http://arxiv.org/abs/2308.08463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilong Li, Chenglong Ma, Jie Chen, Junping Zhang, Hongming Shan<br>for:* 这篇论文的目的是为了提出一个新的实验方法，以减少X射影像资料的辐射剂量和资料收集时间，并提高影像资料的质量。methods:* 这篇论文使用了一种名为“GloRe”的全球表现框架，它是一种基于傅立叶数据的学习方法，可以将影像资料转换为更加简洁的表现，以减少影像资料的质量损失。results:* 实验结果显示，GloReDi方法比以往的方法更好地实现了实验中的目的，包括减少辐射剂量和资料收集时间，并提高影像资料的质量。此外，GloReDi方法还可以实现现场的实验操作和资料分析，实现了实时的影像资料处理和分析。<details>
<summary>Abstract</summary>
Sparse-view computed tomography (CT) -- using a small number of projections for tomographic reconstruction -- enables much lower radiation dose to patients and accelerated data acquisition. The reconstructed images, however, suffer from strong artifacts, greatly limiting their diagnostic value. Current trends for sparse-view CT turn to the raw data for better information recovery. The resultant dual-domain methods, nonetheless, suffer from secondary artifacts, especially in ultra-sparse view scenarios, and their generalization to other scanners/protocols is greatly limited. A crucial question arises: have the image post-processing methods reached the limit? Our answer is not yet. In this paper, we stick to image post-processing methods due to great flexibility and propose global representation (GloRe) distillation framework for sparse-view CT, termed GloReDi. First, we propose to learn GloRe with Fourier convolution, so each element in GloRe has an image-wide receptive field. Second, unlike methods that only use the full-view images for supervision, we propose to distill GloRe from intermediate-view reconstructed images that are readily available but not explored in previous literature. The success of GloRe distillation is attributed to two key components: representation directional distillation to align the GloRe directions, and band-pass-specific contrastive distillation to gain clinically important details. Extensive experiments demonstrate the superiority of the proposed GloReDi over the state-of-the-art methods, including dual-domain ones. The source code is available at https://github.com/longzilicart/GloReDi.
</details>
<details>
<summary>摘要</summary>
通用简化的 computed tomography (CT) -- 使用少量投影进行 Tomographic 重建 -- 可以减少病人的辐射剂量和数据收集的时间。然而，重建的图像却受到强烈的artifacts的限制，从而很大程度地降低了其诊断价值。现有的趋势是将着眼于原始数据，以便更好地回收信息。但是，结果的二元领域方法受到紧张视角场景中的次要artifacts的限制，并且其在其他扫描仪/协议上的普适性受到了严重的限制。问题是：图像后处理方法是否已经达到了限制？我们的答案是不是。在这篇论文中，我们坚持使用图像后处理方法，因为它具有很大的灵活性。我们提出了一种全局表示（GloRe）润色框架，称之为GloReDi。首先，我们提出了学习 GloRe 使用傅ри捷 convolution，使每个 GloRe 元素具有图像全面的感受野。其次，不同于以充满视图图像为监督的方法，我们提议使用中间视图重建图像，这些图像Ready availability，但在过去的文献中未经探讨。GloRe 润色的成功归功于两个关键组成部分：方向性润色 distillation 以确定 GloRe 方向，以及带通道特异的对比润色 distillation 以获取临床重要的细节。广泛的实验表明，提议的 GloReDi 超过了当前状态的方法，包括双Domain方法。源代码可以在 GitHub 上获取：https://github.com/longzilicart/GloReDi。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/17/eess.IV_2023_08_17/" data-id="cloh3sr5u0139h688c83h76c7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/17/cs.LG_2023_08_17/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-17
        
      </div>
    </a>
  
  
    <a href="/2023/08/16/cs.SD_2023_08_16/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-16</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
