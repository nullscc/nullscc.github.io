
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-17 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08974 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yilinliu610730&#x2F;eoe paper">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-17 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/17/eess.IV_2023_08_17/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08974 repo_url: https:&#x2F;&#x2F;github.com&#x2F;yilinliu610730&#x2F;eoe paper">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-16T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:32.455Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/17/eess.IV_2023_08_17/" class="article-date">
  <time datetime="2023-08-16T16:00:00.000Z" itemprop="datePublished">2023-08-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-17 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Eosinophils-Instance-Object-Segmentation-on-Whole-Slide-Imaging-Using-Multi-label-Circle-Representation"><a href="#Eosinophils-Instance-Object-Segmentation-on-Whole-Slide-Imaging-Using-Multi-label-Circle-Representation" class="headerlink" title="Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation"></a>Eosinophils Instance Object Segmentation on Whole Slide Imaging Using Multi-label Circle Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08974">http://arxiv.org/abs/2308.08974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilinliu610730/eoe">https://github.com/yilinliu610730/eoe</a></li>
<li>paper_authors: Yilin Liu, Ruining Deng, Juming Xiong, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo</li>
<li>for: 该研究旨在提高食管炎症诊断的精度和效率，并且提供一种自动化的诊断方法。</li>
<li>methods: 该研究使用了圆形表示法和圆形蛇形模型来实现自动化的实例 segmentation。</li>
<li>results: 对比传统的Mask R-CNN模型和DeepSnake模型，圆形蛇形模型在识别和分割嗜好蛋白质方面表现出了superiority，这可能地提高了EoE诊断的精度和效率。<details>
<summary>Abstract</summary>
Eosinophilic esophagitis (EoE) is a chronic and relapsing disease characterized by esophageal inflammation. Symptoms of EoE include difficulty swallowing, food impaction, and chest pain which significantly impact the quality of life, resulting in nutritional impairments, social limitations, and psychological distress. The diagnosis of EoE is typically performed with a threshold (15 to 20) of eosinophils (Eos) per high-power field (HPF). Since the current counting process of Eos is a resource-intensive process for human pathologists, automatic methods are desired. Circle representation has been shown as a more precise, yet less complicated, representation for automatic instance cell segmentation such as CircleSnake approach. However, the CircleSnake was designed as a single-label model, which is not able to deal with multi-label scenarios. In this paper, we propose the multi-label CircleSnake model for instance segmentation on Eos. It extends the original CircleSnake model from a single-label design to a multi-label model, allowing segmentation of multiple object types. Experimental results illustrate the CircleSnake model's superiority over the traditional Mask R-CNN model and DeepSnake model in terms of average precision (AP) in identifying and segmenting eosinophils, thereby enabling enhanced characterization of EoE. This automated approach holds promise for streamlining the assessment process and improving diagnostic accuracy in EoE analysis. The source code has been made publicly available at https://github.com/yilinliu610730/EoE.
</details>
<details>
<summary>摘要</summary>
《营养细胞损伤综合征（EoE）是一种慢性和再次发生的疾病，特征为食管内部的Inflammation。EoE的症状包括困难吞食、食物堵塞和胸痛，对生活质量产生重大影响，导致营养不良、社会限制和心理压力。EoE的诊断通常通过Esophageal高力场（HPF）中Eosinophils（Eos）的数量（15-20）进行。由于当前的Eos数计数过程需要人工Pathologist的劳动，因此自动方法被欢迎。圆形表示已被证明为更精准， yet less complicated的表示方法，但它是单标签模型，无法处理多标签场景。本文提出了基于圆形的多标签CircleSnake模型，用于实例分 segmentation。这个模型从单标签设计扩展到多标签模型，可以进行多种对象类型的分 segmentation。实验结果表明，CircleSnake模型在AP（准确率）方面与传统的Mask R-CNN模型和DeepSnake模型相比，在标识和分 segmentationEosinophils方面表现出了超过其他两个模型的优势。这种自动化方法可以提高EoE分析过程的效率和准确性，并且代码已经在https://github.com/yilinliu610730/EoE上公开发布。
</details></li>
</ul>
<hr>
<h2 id="An-inexact-proximal-majorization-minimization-Algorithm-for-remote-sensing-image-stripe-noise-removal"><a href="#An-inexact-proximal-majorization-minimization-Algorithm-for-remote-sensing-image-stripe-noise-removal" class="headerlink" title="An inexact proximal majorization-minimization Algorithm for remote sensing image stripe noise removal"></a>An inexact proximal majorization-minimization Algorithm for remote sensing image stripe noise removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08866">http://arxiv.org/abs/2308.08866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjing Wang, Xile Zhao, Qingsong Wang, Zepei Ma, Peipei Tang</li>
<li>for: 提高远程感知图像中的视觉质量和数据分析精度，抑制远程感知图像中的梭噪。</li>
<li>methods: 提出非凸模型，使用DC函数结构进行梭噪除除。解决方法利用DC结构和不准确的 proximal 大esten-multipliers 算法，并设计实现的停止条件。</li>
<li>results: 数值实验表明提出的模型和算法在梭噪除除方面具有superiority。<details>
<summary>Abstract</summary>
The stripe noise existing in remote sensing images badly degrades the visual quality and restricts the precision of data analysis. Therefore, many destriping models have been proposed in recent years. In contrast to these existing models, in this paper, we propose a nonconvex model with a DC function (i.e., the difference of convex functions) structure to remove the strip noise. To solve this model, we make use of the DC structure and apply an inexact proximal majorization-minimization algorithm with each inner subproblem solved by the alternating direction method of multipliers. It deserves mentioning that we design an implementable stopping criterion for the inner subproblem, while the convergence can still be guaranteed. Numerical experiments demonstrate the superiority of the proposed model and algorithm.
</details>
<details>
<summary>摘要</summary>
“远程感知图像中的条纹噪音会严重损害视觉质量和数据分析精度。因此，过去几年内，许多条纹除去模型已经被提出。与现有模型不同，在本文中，我们提出了一种非凸模型，其结构是基于差分 convex 函数（DC 函数）。为解决这个模型，我们利用 DC 结构，并采用不准确的 proximal 主要化-最小化算法，其中每个内部子问题通过 alternate direction method of multipliers 解决。值得一提的是，我们设计了可实施的停止条件，而且可以保证 converge。数值实验表明，提出的模型和算法具有优势。”Here's a word-for-word translation of the text into Simplified Chinese:“远程感知图像中的条纹噪音会严重损害视觉质量和数据分析精度。因此，过去几年内，许多条纹除去模型已经被提出。与现有模型不同，在本文中，我们提出了一种非凸模型，其结构是基于差分 convex 函数（DC 函数）。为解决这个模型，我们利用 DC 结构，并采用不准确的 proximal 主要化-最小化算法，其中每个内部子问题通过 alternate direction method of multipliers 解决。值得一提的是，我们设计了可实施的停止条件，而且可以保证 converge。数值实验表明，提出的模型和算法具有优势。”
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Alternating-Optimization-for-Real-World-Blind-Super-Resolution"><a href="#End-to-end-Alternating-Optimization-for-Real-World-Blind-Super-Resolution" class="headerlink" title="End-to-end Alternating Optimization for Real-World Blind Super Resolution"></a>End-to-end Alternating Optimization for Real-World Blind Super Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08816">http://arxiv.org/abs/2308.08816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/greatlog/realdan">https://github.com/greatlog/realdan</a></li>
<li>paper_authors: Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan<br>for: 这个论文的目的是提出一种基于 alternate optimization 算法的盲SR方法，以提高盲SR的精度和稳定性。methods: 该方法使用了两个卷积神经网络：Restorer 和 Estimator。 Restorer 用于还原 SR 图像，而 Estimator 用于估计质量损失。这两个模块在 alternate 的形式下进行循环训练，以便互相优化。results: 实验表明，提出的方法可以大幅超越当前state-of-the-art 方法，并生成更加可观的结果。<details>
<summary>Abstract</summary>
Blind Super-Resolution (SR) usually involves two sub-problems: 1) estimating the degradation of the given low-resolution (LR) image; 2) super-resolving the LR image to its high-resolution (HR) counterpart. Both problems are ill-posed due to the information loss in the degrading process. Most previous methods try to solve the two problems independently, but often fall into a dilemma: a good super-resolved HR result requires an accurate degradation estimation, which however, is difficult to be obtained without the help of original HR information. To address this issue, instead of considering these two problems independently, we adopt an alternating optimization algorithm, which can estimate the degradation and restore the SR image in a single model. Specifically, we design two convolutional neural modules, namely \textit{Restorer} and \textit{Estimator}. \textit{Restorer} restores the SR image based on the estimated degradation, and \textit{Estimator} estimates the degradation with the help of the restored SR image. We alternate these two modules repeatedly and unfold this process to form an end-to-end trainable network. In this way, both \textit{Restorer} and \textit{Estimator} could get benefited from the intermediate results of each other, and make each sub-problem easier. Moreover, \textit{Restorer} and \textit{Estimator} are optimized in an end-to-end manner, thus they could get more tolerant of the estimation deviations of each other and cooperate better to achieve more robust and accurate final results. Extensive experiments on both synthetic datasets and real-world images show that the proposed method can largely outperform state-of-the-art methods and produce more visually favorable results. The codes are rleased at \url{https://github.com/greatlog/RealDAN.git}.
</details>
<details>
<summary>摘要</summary>
干Resolution（SR）问题通常包含两个互相关联的互补问题：1）估计LR图像的劣化程度；2）LR图像的超Resolution（HR）图像。两个问题都是不定的，因为升级过程中的信息损失。大多数前一代方法会独立地解决这两个问题，但经常陷入一个困境：一个好的HR图像需要一个准确的劣化估计，但是不可以不使用原始HR图像来获得这个估计。为解决这个问题，我们采用了一种alternating optimization算法，可以同时估计劣化和SR图像。我们设计了两个卷积神经网络模块：Restorer和Estimator。Restorer使用估计的劣化来恢复SR图像，而Estimator使用恢复后的SR图像来估计劣化。我们重复地使用这两个模块，并将其拓展成一个端到端可训练的网络。这样，Restorer和Estimator都可以受益于对方的中间结果，使每个问题变得更加容易。此外，Restorer和Estimator在端到端上进行了结构优化，因此它们可以更快地适应对方的估计偏差，并更好地合作以实现更加稳定和准确的最终结果。我们在 synthetic datasets 和实际图像上进行了广泛的实验，结果显示，我们的方法可以与当前状态计算机技术相比，大幅提高SR图像的质量。代码可以在 \url{https://github.com/greatlog/RealDAN.git} 中下载。
</details></li>
</ul>
<hr>
<h2 id="Recursive-Detection-and-Analysis-of-Nanoparticles-in-Scanning-Electron-Microscopy-Images"><a href="#Recursive-Detection-and-Analysis-of-Nanoparticles-in-Scanning-Electron-Microscopy-Images" class="headerlink" title="Recursive Detection and Analysis of Nanoparticles in Scanning Electron Microscopy Images"></a>Recursive Detection and Analysis of Nanoparticles in Scanning Electron Microscopy Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08732">http://arxiv.org/abs/2308.08732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidan S. Wright, Nathaniel P. Youmans, Enrique F. Valderrama Araya</li>
<li>for: 这种 computational framework 的目的是为了精准检测和全面分析 SEM 图像中的粒子。</li>
<li>methods: 这个框架使用了 Python 图像处理库 OpenCV、SciPy 和 Scikit-Image，并结合了阈值处理、膨润和推杂等技术来提高图像处理结果的准确性。</li>
<li>results: 这个框架可以准确地确定粒子坐标，并提取粒子的相关形态特征，包括面积、方向、亮度和长度。 它在五个不同的测试图像中达到 97% 的粒子检测精度。<details>
<summary>Abstract</summary>
In this study, we present a computational framework tailored for the precise detection and comprehensive analysis of nanoparticles within scanning electron microscopy (SEM) images. The primary objective of this framework revolves around the accurate localization of nanoparticle coordinates, accompanied by secondary objectives encompassing the extraction of pertinent morphological attributes including area, orientation, brightness, and length.   Constructed leveraging the robust image processing capabilities of Python, particularly harnessing libraries such as OpenCV, SciPy, and Scikit-Image, the framework employs an amalgamation of techniques, including thresholding, dilating, and eroding, to enhance the fidelity of image processing outcomes.   The ensuing nanoparticle data is seamlessly integrated into the RStudio environment to facilitate meticulous post-processing analysis. This encompasses a comprehensive evaluation of model accuracy, discernment of feature distribution patterns, and the identification of intricate particle arrangements. The finalized framework exhibits high nanoparticle identification within the primary sample image and boasts 97\% accuracy in detecting particles across five distinct test images drawn from a SEM nanoparticle dataset. Furthermore, the framework demonstrates the capability to discern nanoparticles of faint intensity, eluding manual labeling within the control group.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一个计算框架，用于精确检测和全面分析射电镜像（SEM）中的粒子。主要目标是准确地确定粒子坐标，并且包括次要目标，如粒子形态特征的提取，包括面积、方向、亮度和长度。这个框架利用Python的强大图像处理能力，特别是OpenCV、SciPy和Scikit-Image库，结合多种技术，如阈值、扩展和膨润，以提高图像处理结果的准确性。获得的粒子数据可以轻松地 интегрирова到RStudio环境中，进行仔细的后处理分析。这包括完整评估模型准确性，分析特征分布图像，以及识别复杂的粒子排列。实验结果显示，该框架在主要样本图像中具有高精度的粒子检测，并在五个不同的测试图像中达到97%的检测精度。此外，框架还能够识别具有柔弱亮度的粒子，而控制组中的人工标注不能达到。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Kernel-Based-Adaptive-Spatial-Aggregation-for-Learned-Image-Compression"><a href="#Dynamic-Kernel-Based-Adaptive-Spatial-Aggregation-for-Learned-Image-Compression" class="headerlink" title="Dynamic Kernel-Based Adaptive Spatial Aggregation for Learned Image Compression"></a>Dynamic Kernel-Based Adaptive Spatial Aggregation for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08723">http://arxiv.org/abs/2308.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Huairui/DKIC">https://github.com/Huairui/DKIC</a></li>
<li>paper_authors: Huairui Wang, Nianxiang Fu, Zhenzhong Chen, Shan Liu</li>
<li>for: 提高图像压缩率和精度性能</li>
<li>methods: 使用动态核kernel基于转换编码，适应权重分享机制和自适应积集方法</li>
<li>results: 实验结果显示，当前方法在三个标准测试集上比现有学习基于方法具有更高的率压缩率和精度性能<details>
<summary>Abstract</summary>
Learned image compression methods have shown superior rate-distortion performance and remarkable potential compared to traditional compression methods. Most existing learned approaches use stacked convolution or window-based self-attention for transform coding, which aggregate spatial information in a fixed range. In this paper, we focus on extending spatial aggregation capability and propose a dynamic kernel-based transform coding. The proposed adaptive aggregation generates kernel offsets to capture valid information in the content-conditioned range to help transform. With the adaptive aggregation strategy and the sharing weights mechanism, our method can achieve promising transform capability with acceptable model complexity. Besides, according to the recent progress of entropy model, we define a generalized coarse-to-fine entropy model, considering the coarse global context, the channel-wise, and the spatial context. Based on it, we introduce dynamic kernel in hyper-prior to generate more expressive global context. Furthermore, we propose an asymmetric spatial-channel entropy model according to the investigation of the spatial characteristics of the grouped latents. The asymmetric entropy model aims to reduce statistical redundancy while maintaining coding efficiency. Experimental results demonstrate that our method achieves superior rate-distortion performance on three benchmarks compared to the state-of-the-art learning-based methods.
</details>
<details>
<summary>摘要</summary>
现有的学习型压缩方法已经显示出了Superior rate-distortion性能和吸引人的潜在性，相比传统压缩方法。大多数现有的学习方法使用堆叠 convolution或窗口基于自注意力 для变换编码，这些方法会汇集Fixed距离内的空间信息。在这篇论文中，我们关注到了扩展空间汇集能力，并提出了动态核心基于变换编码。我们的提案的适应汇集生成核心偏移来捕捉有效信息在内容受限的范围内，以帮助变换。通过适应汇集策略和共享权重机制，我们的方法可以实现可接受的变换能力，同时减少模型复杂度。此外，根据最近的Entropy模型进展，我们定义一个通用Coarse-to-fine Entropy模型，考虑Global上下文、通道级和空间上下文。基于它，我们引入动态核心在超乎 prior中生成更 expresive的全局上下文。另外，我们提出一种不对称的空间通道Entropy模型，根据Latent集的特点进行调整。这种不对称Entropy模型的目的是减少统计重复，保持编码效率。实验结果表明，我们的方法在三个标准底本上比State-of-the-art学习型方法 superior rate-distortion性能。
</details></li>
</ul>
<hr>
<h2 id="Deployment-and-Analysis-of-Instance-Segmentation-Algorithm-for-In-field-Grade-Estimation-of-Sweetpotatoes"><a href="#Deployment-and-Analysis-of-Instance-Segmentation-Algorithm-for-In-field-Grade-Estimation-of-Sweetpotatoes" class="headerlink" title="Deployment and Analysis of Instance Segmentation Algorithm for In-field Grade Estimation of Sweetpotatoes"></a>Deployment and Analysis of Instance Segmentation Algorithm for In-field Grade Estimation of Sweetpotatoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08534">http://arxiv.org/abs/2308.08534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoang M. Nguyen, Sydney Gyurek, Russell Mierop, Kenneth V. Pecota, Kylie LaGamba, Michael Boyette, G. Craig Yencho, Cranos M. Williams, Michael W. Kudenov<br>for:这个论文是为了提出一种直接在场地中进行 Storage roots 的检测和评估，以便更快速地获得 yields 的方法。methods:这个方法使用了 Detectron2 库中的深度学习对象检测算法，实现了 Mask R-CNN 模型，用于实时地在场地中识别 Storage roots。results:模型可以在不同的环境条件下（包括光照和土壤特性的变化）正确地识别 Storage roots，并且与商业化光学排分器的比较表明，模型的 RMSE 值为0.66 cm，1.22 cm，74.73 g 分别，而 root 数量的 RMSE 值为5.27根，r^2 值为0.8。这种phenotyping 策略有 potential 用于实时地在场地中获得 yields，而不需要高科技和昂贵的光学排分器。<details>
<summary>Abstract</summary>
Shape estimation of sweetpotato (SP) storage roots is inherently challenging due to their varied size and shape characteristics. Even measuring "simple" metrics, such as length and width, requires significant time investments either directly in-field or afterward using automated graders. In this paper, we present the results of a model that can perform grading and provide yield estimates directly in the field quicker than manual measurements. Detectron2, a library consisting of deep-learning object detection algorithms, was used to implement Mask R-CNN, an instance segmentation model. This model was deployed for in-field grade estimation of SPs and evaluated against an optical sorter. Storage roots from various clones imaged with a cellphone during trials between 2019 and 2020, were used in the model's training and validation to fine-tune a model to detect SPs. Our results showed that the model could distinguish individual SPs in various environmental conditions including variations in lighting and soil characteristics. RMSE for length, width, and weight, from the model compared to a commercial optical sorter, were 0.66 cm, 1.22 cm, and 74.73 g, respectively, while the RMSE of root counts per plot was 5.27 roots, with r^2 = 0.8. This phenotyping strategy has the potential enable rapid yield estimates in the field without the need for sophisticated and costly optical sorters and may be more readily deployed in environments with limited access to these kinds of resources or facilities.
</details>
<details>
<summary>摘要</summary>
sweetpotato (SP) 存储根的形状评估是一项自然的挑战，因为它们的形状和大小具有很大的变化。 même measuring "simple" metrics, such as length and width, requires a significant investment of time, either directly in the field or using automated graders. In this paper, we present the results of a model that can perform grading and provide yield estimates directly in the field faster than manual measurements. We used Detectron2, a library consisting of deep-learning object detection algorithms, to implement Mask R-CNN, an instance segmentation model. This model was deployed for in-field grade estimation of SPs and evaluated against a commercial optical sorter. Storage roots from various clones imaged with a cellphone during trials between 2019 and 2020 were used to fine-tune the model to detect SPs. Our results showed that the model could distinguish individual SPs in various environmental conditions, including variations in lighting and soil characteristics. The root mean squared error (RMSE) for length, width, and weight, from the model compared to a commercial optical sorter, were 0.66 cm, 1.22 cm, and 74.73 g, respectively, while the RMSE of root counts per plot was 5.27 roots, with r^2 = 0.8. This phenotyping strategy has the potential to enable rapid yield estimates in the field without the need for sophisticated and costly optical sorters, and may be more readily deployed in environments with limited access to these kinds of resources or facilities.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Distill-Global-Representation-for-Sparse-View-CT"><a href="#Learning-to-Distill-Global-Representation-for-Sparse-View-CT" class="headerlink" title="Learning to Distill Global Representation for Sparse-View CT"></a>Learning to Distill Global Representation for Sparse-View CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08463">http://arxiv.org/abs/2308.08463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilong Li, Chenglong Ma, Jie Chen, Junping Zhang, Hongming Shan</li>
<li>for: 这篇论文的目的是提出一种新的图像后处理方法，以提高稀疏视角计算Tomography（CT）图像的质量。</li>
<li>methods: 该方法使用了 globale representation（GloRe）核对应法，并通过对GloRe进行方向填充和频率特征填充来提高图像质量。</li>
<li>results: 对比于现有方法，该方法的 globale representation（GloRe）核对应法可以更好地提高稀疏视角CT图像的质量，并且可以更好地捕捉临床重要的诊断信息。<details>
<summary>Abstract</summary>
Sparse-view computed tomography (CT) -- using a small number of projections for tomographic reconstruction -- enables much lower radiation dose to patients and accelerated data acquisition. The reconstructed images, however, suffer from strong artifacts, greatly limiting their diagnostic value. Current trends for sparse-view CT turn to the raw data for better information recovery. The resultant dual-domain methods, nonetheless, suffer from secondary artifacts, especially in ultra-sparse view scenarios, and their generalization to other scanners/protocols is greatly limited. A crucial question arises: have the image post-processing methods reached the limit? Our answer is not yet. In this paper, we stick to image post-processing methods due to great flexibility and propose global representation (GloRe) distillation framework for sparse-view CT, termed GloReDi. First, we propose to learn GloRe with Fourier convolution, so each element in GloRe has an image-wide receptive field. Second, unlike methods that only use the full-view images for supervision, we propose to distill GloRe from intermediate-view reconstructed images that are readily available but not explored in previous literature. The success of GloRe distillation is attributed to two key components: representation directional distillation to align the GloRe directions, and band-pass-specific contrastive distillation to gain clinically important details. Extensive experiments demonstrate the superiority of the proposed GloReDi over the state-of-the-art methods, including dual-domain ones. The source code is available at https://github.com/longzilicart/GloReDi.
</details>
<details>
<summary>摘要</summary>
《简洁 computed tomography（CT）》——使用少量投射进行tomographic重建——可以大幅降低对病人的辐射剂量和数据获取的时间。然而，重建的图像却受到强烈的artefacts的限制，从而大大降低其诊断价值。当前的 sparse-view CT 趋势是转向原始数据，以便更好地回收信息。然而，结果性的 dual-domain 方法在 ultra-sparse 视图场景下受到次要artefact的影响，而且其在其他扫描器/协议上的普遍性受限。问题是：图像后处理方法是否已经达到了限制？我们的答案是不是。在这篇论文中，我们坚持使用图像后处理方法，因为它具有很大的灵活性。我们提出了 GloRe 整合框架（GloReDi），用于 sparse-view CT。首先，我们提出了学习 GloRe 使用 Fourier 杂化，使每个 GloRe 元素具有整个图像的广泛响应场。其次，不同于以前的方法，我们提出了使用 intermediate-view 重建图像进行监督，这些图像ready available，但在前期 литературе未被探讨。 GloRe 整合框架的成功归因于两个关键组成部分： representation directional distillation 用于对 GloRe 方向进行对齐，以及 band-pass-specific contrastive distillation 用于获取临床重要的细节。我们对 GloReDi 进行了广泛的实验，并证明其在state-of-the-art方法之上。源代码可以在 https://github.com/longzilicart/GloReDi 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/17/eess.IV_2023_08_17/" data-id="clltaagr100elr888fgxg3v24" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/17/cs.SD_2023_08_17/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-17 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/16/cs.LG_2023_08_16/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-16 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
