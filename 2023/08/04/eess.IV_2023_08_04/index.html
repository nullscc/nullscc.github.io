
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-04 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Frequency Disentangled Features in Neural Image Compression paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02620 repo_url: None paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Piyush Mehta, Mohammad Saeed Ebr">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-04 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/04/eess.IV_2023_08_04/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Frequency Disentangled Features in Neural Image Compression paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02620 repo_url: None paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Piyush Mehta, Mohammad Saeed Ebr">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-03T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:43.871Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/eess.IV_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-04 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Frequency-Disentangled-Features-in-Neural-Image-Compression"><a href="#Frequency-Disentangled-Features-in-Neural-Image-Compression" class="headerlink" title="Frequency Disentangled Features in Neural Image Compression"></a>Frequency Disentangled Features in Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02620">http://arxiv.org/abs/2308.02620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Piyush Mehta, Mohammad Saeed Ebrahimi Saadabadi, Mohammad Akyash, Nasser M. Nasrabadi</li>
<li>For: The paper proposes a neural image compression network that leverages feature-level frequency disentanglement and an augmented self-attention score calculation to improve the compression efficiency and de-correlate the image features.* Methods: The proposed network uses a rate-distortion variational autoencoder (R-D VAE) with relaxed scalar quantization, which is guided by a feature-level frequency disentanglement to capture the low-frequency texture of the image. The network also utilizes an augmented self-attention score calculation based on the Hadamard product during both encoding and decoding.* Results: The proposed network outperforms hand-engineered codecs and neural network-based codecs built on computation-heavy spatially autoregressive entropy models, demonstrating its effectiveness in image compression.<details>
<summary>Abstract</summary>
The design of a neural image compression network is governed by how well the entropy model matches the true distribution of the latent code. Apart from the model capacity, this ability is indirectly under the effect of how close the relaxed quantization is to the actual hard quantization. Optimizing the parameters of a rate-distortion variational autoencoder (R-D VAE) is ruled by this approximated quantization scheme. In this paper, we propose a feature-level frequency disentanglement to help the relaxed scalar quantization achieve lower bit rates by guiding the high entropy latent features to include most of the low-frequency texture of the image. In addition, to strengthen the de-correlating power of the transformer-based analysis/synthesis transform, an augmented self-attention score calculation based on the Hadamard product is utilized during both encoding and decoding. Channel-wise autoregressive entropy modeling takes advantage of the proposed frequency separation as it inherently directs high-informational low-frequency channels to the first chunks and conditions the future chunks on it. The proposed network not only outperforms hand-engineered codecs, but also neural network-based codecs built on computation-heavy spatially autoregressive entropy models.
</details>
<details>
<summary>摘要</summary>
neural 图像压缩网络的设计受到真实分布的熵模型匹配度的限制。 apart from 模型容量，这种能力受到实际硬化量化的距离影响。 在这篇论文中，我们提出了一种基于频谱分解的特征级频率分离，以帮助放松量化实现更低的比特率，使高熵特征映射到图像中的低频文本。 此外，我们还利用了在编码和解码过程中的扩展自我注意力计算，以增强trasformer 基于的分析/生成变换的分离能力。 通道 wise 自动化熵模型利用了我们提出的频谱分解，因为它直接引导高信息低频通道到首块，并将后续块condition 在它之上。 提议的网络不仅超过了手工编码器，还超过了基于计算昂贵的空间自动关联熵模型的神经网络编码器。
</details></li>
</ul>
<hr>
<h2 id="Brain-MRI-Segmentation-using-Template-Based-Training-and-Visual-Perception-Augmentation"><a href="#Brain-MRI-Segmentation-using-Template-Based-Training-and-Visual-Perception-Augmentation" class="headerlink" title="Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation"></a>Brain MRI Segmentation using Template-Based Training and Visual Perception Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02363">http://arxiv.org/abs/2308.02363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fang-Cheng Yeh</li>
<li>for: 用一个人类大脑MRI模板和其相关的分类标签来训练一个3D U-Net模型从头开始，不需要大量的训练数据。</li>
<li>methods: 使用模板基于的训练方法，并包括视觉感知增强以提高模型对各种图像输入的Robustness，以避免过拟合。</li>
<li>results: 通过这种方法，我们训练了mouse、rat、猴、猴和人类大脑MRI的3D U-Net模型，并实现了分割任务，如脑骨梁除、大脑分割和组织概率地图。这种工具有效地解决了深度学习应用图像分析中的数据有限问题，并为研究人员提供了一个统一的解决方案，只需要一个图像样本来训练深度神经网络。<details>
<summary>Abstract</summary>
Deep learning models usually require sufficient training data to achieve high accuracy, but obtaining labeled data can be time-consuming and labor-intensive. Here we introduce a template-based training method to train a 3D U-Net model from scratch using only one population-averaged brain MRI template and its associated segmentation label. The process incorporated visual perception augmentation to enhance the model's robustness in handling diverse image inputs and mitigating overfitting. Leveraging this approach, we trained 3D U-Net models for mouse, rat, marmoset, rhesus, and human brain MRI to achieve segmentation tasks such as skull-stripping, brain segmentation, and tissue probability mapping. This tool effectively addresses the limited availability of training data and holds significant potential for expanding deep learning applications in image analysis, providing researchers with a unified solution to train deep neural networks with only one image sample.
</details>
<details>
<summary>摘要</summary>
深度学习模型通常需要充足的训练数据来达到高精度，但获取标注数据可以是时间consuming和劳动 INTENSIVE。在这里，我们介绍了一种模板基于的训练方法，可以从scratch用一个人类大脑MRI模板和其关联的分割标注来训练3D U-Net模型。该过程包括视觉感知增强以提高模型对多种图像输入的抗衡能力和避免过拟合。通过这种方法，我们训练了3D U-Net模型用于鼠、老鼠、猴、人类大脑MRI的分割任务，如骨干取除、大脑分割和组织概率地图。这种工具有效地解决了训练数据的有限性问题，并具有扩展深度学习应用于图像分析的潜在 potential。
</details></li>
</ul>
<hr>
<h2 id="T-UNet-Triplet-UNet-for-Change-Detection-in-High-Resolution-Remote-Sensing-Images"><a href="#T-UNet-Triplet-UNet-for-Change-Detection-in-High-Resolution-Remote-Sensing-Images" class="headerlink" title="T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images"></a>T-UNet: Triplet UNet for Change Detection in High-Resolution Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02356">http://arxiv.org/abs/2308.02356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pl-2000/t-unet">https://github.com/pl-2000/t-unet</a></li>
<li>paper_authors: Huan Zhong, Chen Wu</li>
<li>for: 这篇论文旨在提出一个新的网络模型，用于远程感知图像变化检测，以提高检测精度和准确性。</li>
<li>methods: 这篇论文提出了一个三枝Encoder结构，并使用多枝空间特征聚合模组（MBSSCA）进行特征聚合和探索。在解码阶段，论文导入了通道注意力机制（CAM）和空间注意力机制（SAM），以实现完整地探索和融合详细的特征信息和 semantic 地域特征信息。</li>
<li>results: 这篇论文的实验结果显示，T-UNet 模型可以实现更高的准确率和精度，并且能够更好地探索和融合详细的特征信息和 semantic 地域特征信息。<details>
<summary>Abstract</summary>
Remote sensing image change detection aims to identify the differences between images acquired at different times in the same area. It is widely used in land management, environmental monitoring, disaster assessment and other fields. Currently, most change detection methods are based on Siamese network structure or early fusion structure. Siamese structure focuses on extracting object features at different times but lacks attention to change information, which leads to false alarms and missed detections. Early fusion (EF) structure focuses on extracting features after the fusion of images of different phases but ignores the significance of object features at different times for detecting change details, making it difficult to accurately discern the edges of changed objects. To address these issues and obtain more accurate results, we propose a novel network, Triplet UNet(T-UNet), based on a three-branch encoder, which is capable to simultaneously extract the object features and the change features between the pre- and post-time-phase images through triplet encoder. To effectively interact and fuse the features extracted from the three branches of triplet encoder, we propose a multi-branch spatial-spectral cross-attention module (MBSSCA). In the decoder stage, we introduce the channel attention mechanism (CAM) and spatial attention mechanism (SAM) to fully mine and integrate detailed textures information at the shallow layer and semantic localization information at the deep layer.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese remote sensing image change detection 图像变化检测是用于在同一区域内的不同时间检测到的图像差异。它广泛应用于土地管理、环境监测、灾害评估等领域。目前，大多数变化检测方法基于 Siamese 网络结构或 Early Fusion 结构。Siamese 结构注重在不同时间中EXTRACTING对象特征，但缺乏关注变化信息，导致假警报和漏报。 Early Fusion 结构注重在不同阶段图像归一化后EXTRACTING对象特征，但忽视对象特征在不同时间段中的变化细节，使其困难准确地分辨变化的边缘。为了解决这些问题并获得更高精度的结果，我们提出了一种新的网络模型，Triplet UNet（T-UNet），基于三个分支Encoder，能同时EXTRACT对象特征和不同时间段图像之间的变化特征。为了有效地交互和融合 triplet Encoder 中EXTRACT的特征，我们提出了多支分支空间特征跟踪模块（MBSSCA）。在解码阶段，我们引入了通道注意机制（CAM）和空间注意机制（SAM），以全面挖掘和融合图像的细节信息和semantic 本地化信息。
</details></li>
</ul>
<hr>
<h2 id="Generative-Image-Priors-for-MRI-Reconstruction-Trained-from-Magnitude-Only-Images"><a href="#Generative-Image-Priors-for-MRI-Reconstruction-Trained-from-Magnitude-Only-Images" class="headerlink" title="Generative Image Priors for MRI Reconstruction Trained from Magnitude-Only Images"></a>Generative Image Priors for MRI Reconstruction Trained from Magnitude-Only Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02340">http://arxiv.org/abs/2308.02340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrirecon/image-priors">https://github.com/mrirecon/image-priors</a></li>
<li>paper_authors: Guanxiong Luo, Xiaoqing Wang, Mortiz Blumenthal, Martin Schilling, Erik Hans Ulrich Rauf, Raviteja Kotikalapudi, Niels Focke, Martin Uecker</li>
<li>for: 这个研究旨在构建基于大数据集和阶段信息的通用和稳定的生成图像先验。这些先验可以用于恢复图像质量的正则化。</li>
<li>methods: 研究开始于准备用魔力只图像准备训练数据集，然后将这个数据集扩展到包括阶段信息，并用这个数据集训练生成图像先验。最后，研究人员使用不同的抽样方案进行了线性和非线性恢复的评估。</li>
<li>results: 实验结果表明，基于复杂图像的先验比只基于魔力图像的先验表现更好。此外，一个训练在更大的数据集上的先验也表现出更高的稳定性。最后，我们发现使用生成先验比L1-wavelet正则化更有利于扩展扫描成像。结论：这些发现表明，包括阶段信息和利用大数据集可以提高生成先验的性能和可靠性，并且这些先验可以用于提高MRI重建的质量。<details>
<summary>Abstract</summary>
Purpose: In this work, we present a workflow to construct generic and robust generative image priors from magnitude-only images. The priors can then be used for regularization in reconstruction to improve image quality. Methods: The workflow begins with the preparation of training datasets from magnitude-only MR images. This dataset is then augmented with phase information and used to train generative priors of complex images. Finally, trained priors are evaluated using both linear and nonlinear reconstruction for compressed sensing parallel imaging with various undersampling schemes. Results: The results of our experiments demonstrate that priors trained on complex images outperform priors trained only on magnitude images. Additionally, a prior trained on a larger dataset exhibits higher robustness. Finally, we show that the generative priors are superior to L1 -wavelet regularization for compressed sensing parallel imaging with high undersampling. Conclusion: These findings stress the importance of incorporating phase information and leveraging large datasets to raise the performance and reliability of the generative priors for MRI reconstruction. Phase augmentation makes it possible to use existing image databases for training.
</details>
<details>
<summary>摘要</summary>
目的：在这项工作中，我们提出了一个工作流程，用于从偏好度只图像中构建通用和稳定的生成图像先验。这些先验然后可以用于图像重建中的regularization，以提高图像质量。方法：工作流程开始于从偏好度只图像MR影像中准备训练集。这个集合然后被补充 phase信息，并用于训练复杂图像的生成先验。最后，我们使用线性和非线性重建进行测试，以评估训练后的先验表现。结果：我们的实验结果表明，基于复杂图像的先验比基于偏好度只图像的先验表现更好。此外，一个基于更大的数据集训练的先验表现更高稳定。最后，我们表明，生成先验比L1-wavelet regularization更有优势于高抽样率的并行扫描图像重建。结论：这些发现强调了在图像重建中包含相位信息和利用大数据集来提高生成先验的性能和可靠性。相位增强使得可以使用现有的图像数据库进行训练。
</details></li>
</ul>
<hr>
<h2 id="CT-Reconstruction-from-Few-Planar-X-rays-with-Application-towards-Low-resource-Radiotherapy"><a href="#CT-Reconstruction-from-Few-Planar-X-rays-with-Application-towards-Low-resource-Radiotherapy" class="headerlink" title="CT Reconstruction from Few Planar X-rays with Application towards Low-resource Radiotherapy"></a>CT Reconstruction from Few Planar X-rays with Application towards Low-resource Radiotherapy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02100">http://arxiv.org/abs/2308.02100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wanderinrain/xray2ct">https://github.com/wanderinrain/xray2ct</a></li>
<li>paper_authors: Yiran Sun, Tucker Netherton, Laurence Court, Ashok Veeraraghavan, Guha Balakrishnan</li>
<li>for: 这项研究的目的是使用几个平面X射图像生成CT卷积体，以便在低和中Resource Settings中提高肿瘤诊断和治疗的效率。</li>
<li>methods: 该研究使用了深度生成模型，基于神经隐式表示来合成volumetric CT扫描图像从几个平面X射图像的不同角度。同时，模型还可以在训练过程中使用 segmentation masks 来减少生成任务中的产生不必要的特征。</li>
<li>results: 研究发现，使用该方法生成的 thoracic CT 扫描图像和临床实际中获取的 CT 扫描图像之间的是ocenter 辐射剂量Error 小于1%。此外，该方法还比现有的稀疍CT重建基elines 高于标准像素和结构级别指标（PSNR、SSIM、Dice 分数）中的LIDC肺CT数据集。<details>
<summary>Abstract</summary>
CT scans are the standard-of-care for many clinical ailments, and are needed for treatments like external beam radiotherapy. Unfortunately, CT scanners are rare in low and mid-resource settings due to their costs. Planar X-ray radiography units, in comparison, are far more prevalent, but can only provide limited 2D observations of the 3D anatomy. In this work, we propose a method to generate CT volumes from few (<5) planar X-ray observations using a prior data distribution, and perform the first evaluation of such a reconstruction algorithm for a clinical application: radiotherapy planning. We propose a deep generative model, building on advances in neural implicit representations to synthesize volumetric CT scans from few input planar X-ray images at different angles. To focus the generation task on clinically-relevant features, our model can also leverage anatomical guidance during training (via segmentation masks). We generated 2-field opposed, palliative radiotherapy plans on thoracic CTs reconstructed by our method, and found that isocenter radiation dose on reconstructed scans have <1% error with respect to the dose calculated on clinically acquired CTs using <=4 X-ray views. In addition, our method is better than recent sparse CT reconstruction baselines in terms of standard pixel and structure-level metrics (PSNR, SSIM, Dice score) on the public LIDC lung CT dataset. Code is available at: https://github.com/wanderinrain/Xray2CT.
</details>
<details>
<summary>摘要</summary>
干扰X射线成像设备在低和中型资源设备中较为罕见，原因是它们的成本较高。相比之下，平面X射线成像设备更为普遍，但它们只能提供2D的观察结果，无法提供3D的解剖结构。在这项工作中，我们提出了一种方法，使用先前的数据分布来生成CT体积从少于5个平面X射线图像中，并在临床应用中进行了首次评估。我们提出了一种深度生成模型，基于神经隐式表示来生成3D的CT体积图像，并在训练过程中使用解剖指导来避免误差。我们使用了2个对称的反向X射线成像计划来规划肺部CT图像，并发现在重建的扫描图像上的穿透中心辐射剂量与临床获得的CT图像中的辐射剂量之间的差异小于1%。此外，我们的方法也比最近的稀疏CT重建基eline更好，根据标准像素和结构级度指标（PSNR、SSIM、Dice分数）来评估。代码可以在以下地址找到：https://github.com/wanderinrain/Xray2CT。
</details></li>
</ul>
<hr>
<h2 id="Motion-robust-free-running-cardiovascular-MRI"><a href="#Motion-robust-free-running-cardiovascular-MRI" class="headerlink" title="Motion-robust free-running cardiovascular MRI"></a>Motion-robust free-running cardiovascular MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02088">http://arxiv.org/abs/2308.02088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syedmurtazaarshad/motion-robust-CMR">https://github.com/syedmurtazaarshad/motion-robust-CMR</a></li>
<li>paper_authors: Syed M. Arshad, Lee C. Potter, Chong Chen, Yingmin Liu, Preethi Chandrasekaran, Christopher Crabtree, Yuchi Han, Rizwan Ahmad</li>
<li>For: 这种研究旨在提高自由运行cardiovascular MRI（CMR）的动力稳定性，以便在各种应用中减少运动artefacts。* Methods: 该方法模拟了异常值作为auxiliary变量，并对这个变量进行MR физи学引导的集成随机树减少（CORe）。通过迭代算法，同时对auxiliary变量和图像进行估算。* Results: 对于50个实现中的异常值处理方法，CORe在正常化平均方差（NMSE）和结构相似指标（SSIM）方面表现出色，并且在3D cinema图像中更好地抑制了artefacts，而无需减少图像锐度。在4D流动图像中，CORe得到了更一致的流动测量结果，特别是在运动压力下。<details>
<summary>Abstract</summary>
PURPOSE: To present and validate an outlier rejection method that makes free-running cardiovascular MRI (CMR) more motion robust.   METHODS: The proposed method, called compressive recovery with outlier rejection (CORe), models outliers as an auxiliary variable that is added to the measured data. We enforce MR physics-guided group-sparsity on the auxiliary variable and jointly estimate it along with the image using an iterative algorithm. For validation, CORe is first compared to traditional compressed sensing (CS), robust regression (RR), and another outlier rejection method using two simulation studies. Then, CORe is compared to CS using five 3D cine and ten rest and stress 4D flow imaging datasets.   RESULTS: Our simulation studies show that CORe outperforms CS, RR, and the outlier rejection method in terms of normalized mean squared error (NMSE) and structural similarity index (SSIM) across 50 different realizations. The expert reader evaluation of 3D cine images demonstrates that CORe is more effective in suppressing artifacts while maintaining or improving image sharpness. The flow consistency evaluation in 4D flow images show that CORe yields more consistent flow measurements, especially under exercise stress.   CONCLUSION: An outlier rejection method is presented and validated using simulated and measured data. This method can help suppress motion artifacts in a wide range of free-running CMR applications.   CODE: MATLAB implementation code is available on GitHub at https://github.com/syedmurtazaarshad/motion-robust-CMR
</details>
<details>
<summary>摘要</summary>
目的：提出和验证一种可以使自由运行征 Cardiovascular MRI (CMR) 更加鲁棒于运动 artifacts 的方法。方法：提出的方法，称为 compressive recovery with outlier rejection (CORe)，将异常值模型为 auxillary 变量，并在这个变量上强制施加 MR 物理指导的群 sparse  regularization。我们使用迭代算法来同时估算这个变量和图像。对比 CS、RR 和异常拒绝方法，我们使用两个 simulate 研究进行验证。然后，我们使用五个 3D cine 和十个 rest 和 stress 4D flow imaging 数据集进行验证。结果：我们的 simulate 研究表明，CORe 在 NMSE 和 SSIM 指标上都高于 CS、RR 和异常拒绝方法。专业读者评估 3D cine 图像时，CORe 更有效地抑制 artifacts，同时保持或改善图像的锐度。在 4D flow 图像中，CORe 产生的流量测量更加一致，特别是在运动压力下。结论：我们提出了一种可以鲁棒化自由运行 CMR 应用中的异常拒绝方法，可以帮助抑制运动 artifacts。代码：MATLAB 实现代码可以在 GitHub 上找到，https://github.com/syedmurtazaarshad/motion-robust-CMR
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Counterfactual-Generation-and-Anomaly-Detection-in-Brain-Images"><a href="#Diffusion-Models-for-Counterfactual-Generation-and-Anomaly-Detection-in-Brain-Images" class="headerlink" title="Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images"></a>Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02062">http://arxiv.org/abs/2308.02062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alessandro-f/dif-fuse">https://github.com/alessandro-f/dif-fuse</a></li>
<li>paper_authors: Alessandro Fontanella, Grant Mair, Joanna Wardlaw, Emanuele Trucco, Amos Storkey</li>
<li>for: This paper is written for the purpose of generating healthy counterfactuals of diseased images for medical applications such as brain tumor and stroke management.</li>
<li>methods: The paper proposes a weakly supervised method that uses a saliency map obtained with ACAT to generate a healthy version of a diseased image, followed by targeted modifications using a diffusion model trained on healthy samples. The method combines DDPM and DDIM at each step of the sampling process to ensure a seamless transition between edited and unedited parts.</li>
<li>results: The paper shows that the proposed method improves the DICE score of the best competing method from $0.6534$ to $0.7056$ on IST-3 for stroke lesion segmentation and on BraTS2021 for brain tumor segmentation.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文是为了生成疾病图像的健康对照样本而写的。</li>
<li>methods: 这篇论文提出了一种弱监督方法，使用ACAT获得的Saliency map来生成疾病图像的健康版本，然后进行targeted修改使用健康样本上训练的扩散模型。</li>
<li>results: 论文显示，提出的方法可以提高IST-3 stroke病变部分 segmentation的DICE分数从0.6534提高到0.7056，以及BraTS2021 brain tumor segmentation的DICE分数。<details>
<summary>Abstract</summary>
Segmentation masks of pathological areas are useful in many medical applications, such as brain tumour and stroke management. Moreover, healthy counterfactuals of diseased images can be used to enhance radiologists' training files and to improve the interpretability of segmentation models. In this work, we present a weakly supervised method to generate a healthy version of a diseased image and then use it to obtain a pixel-wise anomaly map. To do so, we start by considering a saliency map that approximately covers the pathological areas, obtained with ACAT. Then, we propose a technique that allows to perform targeted modifications to these regions, while preserving the rest of the image. In particular, we employ a diffusion model trained on healthy samples and combine Denoising Diffusion Probabilistic Model (DDPM) and Denoising Diffusion Implicit Model (DDIM) at each step of the sampling process. DDPM is used to modify the areas affected by a lesion within the saliency map, while DDIM guarantees reconstruction of the normal anatomy outside of it. The two parts are also fused at each timestep, to guarantee the generation of a sample with a coherent appearance and a seamless transition between edited and unedited parts. We verify that when our method is applied to healthy samples, the input images are reconstructed without significant modifications. We compare our approach with alternative weakly supervised methods on IST-3 for stroke lesion segmentation and on BraTS2021 for brain tumour segmentation, where we improve the DICE score of the best competing method from $0.6534$ to $0.7056$.
</details>
<details>
<summary>摘要</summary>
医学应用中的疾病区域分割mask是非常有用的，例如脑肿瘤和中风管理。此外，健康的对比样本可以用来提高放射学家的训练文件，并提高分割模型的解释性。在这种情况下，我们提出了一种弱相关的方法，可以生成一个疾病的健康版本，并使用其生成一个像素层级异常地图。我们的方法的核心思想是使用ACAT获取疾病区域的灵敏度地图，然后使用一种目标修改这些区域的技术，以保持图像的其他部分不受影响。我们使用训练于健康样本的扩散模型（DDPM）和扩散隐藏模型（DDIM），在每个步骤中进行修改和重建。DDPM用于修改疾病区域中的影响区域，而DDIM则 garantiz reconstruction of normal anatomy outside of it。这两个部分还 fusion at each timestep，以确保生成的样本具有一致的外观和无缝过渡。我们证明，当我们的方法应用于健康样本时，输入图像不会经受重要的修改。我们与其他弱相关方法进行比较，在IST-3上进行了肿瘤病变分割和在BraTS2021上进行了脑肿瘤分割，我们提高了最佳竞争方法的DICE分数从0.6534提高到0.7056。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Ki67-ER-PR-and-HER2-Statuses-from-H-E-stained-Breast-Cancer-Images"><a href="#Predicting-Ki67-ER-PR-and-HER2-Statuses-from-H-E-stained-Breast-Cancer-Images" class="headerlink" title="Predicting Ki67, ER, PR, and HER2 Statuses from H&amp;E-stained Breast Cancer Images"></a>Predicting Ki67, ER, PR, and HER2 Statuses from H&amp;E-stained Breast Cancer Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01982">http://arxiv.org/abs/2308.01982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Akbarnejad, Nilanjan Ray, Penny J. Barnes, Gilbert Bigras</li>
<li>for: This paper aims to investigate the accuracy of machine learning methods in predicting molecular information from histomorphology images.</li>
<li>methods: The authors built a large-scale dataset of histomorphology images with reliable measurements for Ki67, ER, PR, and HER2 statuses, and used a standard ViT-based pipeline to train classifiers for predicting these molecular markers.</li>
<li>results: The authors achieved prediction performances around 90% in terms of Area Under the Curve (AUC) when trained with a proper labeling protocol, and demonstrated the ability of the trained classifiers to localize relevant regions.Here is the simplified Chinese translation of the three key points:</li>
<li>for: 本研究旨在探讨机器学习方法是否可以准确地预测 histomorphology 图像中的分子信息。</li>
<li>methods: 作者们建立了一个大规模的 histomorphology 图像 dataset，并使用了标准的 ViT 基于管道来训练类ifiers 以预测 Ki67、ER、PR 和 HER2 状况。</li>
<li>results: 作者们在使用正确的标签协议训练的情况下，达到了约 90% 的预测性能（AUC），并证明了训练好的类ifiers 可以 correctly localize 相关区域。<details>
<summary>Abstract</summary>
Despite the advances in machine learning and digital pathology, it is not yet clear if machine learning methods can accurately predict molecular information merely from histomorphology. In a quest to answer this question, we built a large-scale dataset (185538 images) with reliable measurements for Ki67, ER, PR, and HER2 statuses. The dataset is composed of mirrored images of H\&E and corresponding images of immunohistochemistry (IHC) assays (Ki67, ER, PR, and HER2. These images are mirrored through registration. To increase reliability, individual pairs were inspected and discarded if artifacts were present (tissue folding, bubbles, etc). Measurements for Ki67, ER and PR were determined by calculating H-Score from image analysis. HER2 measurement is based on binary classification: 0 and 1+ (IHC scores representing a negative subset) vs 3+ (IHC score positive subset). Cases with IHC equivocal score (2+) were excluded. We show that a standard ViT-based pipeline can achieve prediction performances around 90% in terms of Area Under the Curve (AUC) when trained with a proper labeling protocol. Finally, we shed light on the ability of the trained classifiers to localize relevant regions, which encourages future work to improve the localizations. Our proposed dataset is publicly available: https://ihc4bc.github.io/
</details>
<details>
<summary>摘要</summary>
尽管机器学习和数字 PATHOLOGY 的进步，仍然没有确定机器学习方法可以准确地预测蛋白质信息仅基于组织结构。为了回答这个问题，我们建立了一个大规模数据集（185538张图像），其中包含可靠的测量结果 для Ki67、ER、PR 和 HER2 状况。这个数据集由 H\&E 和相关的免疫染色试验（Ki67、ER、PR 和 HER2）的图像组成，这些图像通过注册进行镜像。为了增强可靠性，我们检查了每个对并抛弃了包含artefacts（组织折叠、气泡等）的对。我们使用图像分析计算 H-Score 来确定 Ki67、ER 和 PR 的测量结果，而 HER2 的测量基于二分类：0 和 1+（IHC 分数表示负集）vs 3+（IHC 分数正集）。我们排除了 IHC 不确定分数（2+）的 случа。我们显示，使用标准 ViT-based 管道可以在训练 proper 标签协议下达到约 90% 的区域Under the Curve（AUC）的预测性能。最后，我们探讨了训练的分类器是否能够LOCALIZE relevant regions，这种能力鼓励未来的工作进一步提高本地化。我们的提出的数据集现在公开可用：https://ihc4bc.github.io/
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/04/eess.IV_2023_08_04/" data-id="cllta0ljl008xny88ab3r2c3j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/04/cs.SD_2023_08_04/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-04 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/03/cs.LG_2023_08_03/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-03 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
