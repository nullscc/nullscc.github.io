
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-04 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03887 repo_url: None paper_authors: Gergely Szabó, Paolo Bonaiuti, Andrea Ciliberto, András Ho">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-04 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/04/cs.LG_2023_08_04/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03887 repo_url: None paper_authors: Gergely Szabó, Paolo Bonaiuti, Andrea Ciliberto, András Ho">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-03T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:25.249Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/cs.LG_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-04 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Cell-Tracking-with-a-Time-Symmetric-Deep-Learning-Approach"><a href="#Enhancing-Cell-Tracking-with-a-Time-Symmetric-Deep-Learning-Approach" class="headerlink" title="Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach"></a>Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03887">http://arxiv.org/abs/2308.03887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gergely Szabó, Paolo Bonaiuti, Andrea Ciliberto, András Horváth</li>
<li>for: 生物学实验中跟踪细胞的动态行为</li>
<li>methods: 基于深度学习的方法，不受连续帧的限制，仅基于细胞的空间时间邻域</li>
<li>results: 能够处理大量视频帧，并且可以学习细胞的运动模式无需任何先前假设<details>
<summary>Abstract</summary>
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated through multiple biologically motivated validation strategies and compared against several state-of-the-art cell tracking methods.
</details>
<details>
<summary>摘要</summary>
live cells 的准确跟踪使用视频微scopy记录仍然是流行的state-of-the-art image processing基于对象跟踪方法中的挑战。在过去几年中，一些现有的和新的应用程序尝试 integrating deep learning基础框架来实现这项任务，但大多数它们仍然受限于连续帧基础或其他前提，这会阻碍总体学习。为解决这个问题，我们努力开发了一种新的深度学习基础的跟踪方法，这种方法假设Cells可以根据其空间-时间 neighborhood来跟踪，不需要 consecutive frame。这种方法还有一个利点，即 predictor 可以通过完全学习 cells 的运动模式而不需要任何先前假设，并且可以处理大量视频帧。我们通过多种生物学上驱动的验证方法来证明方法的效果，并与一些state-of-the-art cell tracking方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Learning-Optimal-Admission-Control-in-Partially-Observable-Queueing-Networks"><a href="#Learning-Optimal-Admission-Control-in-Partially-Observable-Queueing-Networks" class="headerlink" title="Learning Optimal Admission Control in Partially Observable Queueing Networks"></a>Learning Optimal Admission Control in Partially Observable Queueing Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02391">http://arxiv.org/abs/2308.02391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonatha Anselmi, Bruno Gaujal, Louis-Sébastien Rebuffi</li>
<li>for: 这个论文目的是学习一种有效的权限控制策略，以优化在部分可见的队列网络中的排队控制。</li>
<li>methods: 这个论文使用了一种基于POMDP的强化学习算法，并且使用了Norton的等价定理和生成-死亡过程的有效强化学习算法来实现。</li>
<li>results: 这个论文得到了一个只依赖于最大任务数 $S$ 的减少，而不是依赖于任务系统的径长，从而实现了高效的排队控制。<details>
<summary>Abstract</summary>
We present an efficient reinforcement learning algorithm that learns the optimal admission control policy in a partially observable queueing network. Specifically, only the arrival and departure times from the network are observable, and optimality refers to the average holding/rejection cost in infinite horizon.   While reinforcement learning in Partially Observable Markov Decision Processes (POMDP) is prohibitively expensive in general, we show that our algorithm has a regret that only depends sub-linearly on the maximal number of jobs in the network, $S$. In particular, in contrast with existing regret analyses, our regret bound does not depend on the diameter of the underlying Markov Decision Process (MDP), which in most queueing systems is at least exponential in $S$.   The novelty of our approach is to leverage Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death processes.
</details>
<details>
<summary>摘要</summary>
我们提出了一个高效的增强学习算法，用于在部分可观察queueing网络中找到最佳接受控制策略。具体来说，只有网络的到达和离开时间可观察，并且将 Optimal 定义为无限时间平均保持/拒绝成本。而在部分可观察Markov决策过程（POMDP）中，增强学习通常是不可能高效的，但我们显示我们的算法仅对最大作业数量($S$)有较低的干扰。具体来说，我们的干扰 bound 不过依赖 Underlying Markov Decision Process（MDP）的尺度，这在大多数队列系统中是至少对数尺度的 $S$。我们的新的方法是利用Norton的等效定理，以及高效的增强学习算法 для birth-and-death 过程结构的 MDP。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Survival-Analysis-in-Healthcare-with-Federated-Survival-Forests-A-Comparative-Study-on-Heart-Failure-and-Breast-Cancer-Genomics"><a href="#Scaling-Survival-Analysis-in-Healthcare-with-Federated-Survival-Forests-A-Comparative-Study-on-Heart-Failure-and-Breast-Cancer-Genomics" class="headerlink" title="Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics"></a>Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02382">http://arxiv.org/abs/2308.02382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Archetti, Francesca Ieva, Matteo Matteucci</li>
<li>for: This paper is written for the purpose of developing a federated learning algorithm for survival analysis, called FedSurF++, which can handle incomplete, censored, and distributed survival data while preserving user privacy.</li>
<li>methods: The FedSurF++ algorithm uses a federated ensemble method that constructs random survival forests in heterogeneous federations, and investigates several new tree sampling methods from client forests.</li>
<li>results: The paper shows that FedSurF++ achieves comparable performance to existing methods while requiring only a single communication round to complete, and presents results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private.<details>
<summary>Abstract</summary>
Survival analysis is a fundamental tool in medicine, modeling the time until an event of interest occurs in a population. However, in real-world applications, survival data are often incomplete, censored, distributed, and confidential, especially in healthcare settings where privacy is critical. The scarcity of data can severely limit the scalability of survival models to distributed applications that rely on large data pools. Federated learning is a promising technique that enables machine learning models to be trained on multiple datasets without compromising user privacy, making it particularly well-suited for addressing the challenges of survival data and large-scale survival applications. Despite significant developments in federated learning for classification and regression, many directions remain unexplored in the context of survival analysis. In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.
</details>
<details>
<summary>摘要</summary>
生存分析是医学中的基本工具，用于模型 populate 中的事件发生的时间。然而，在实际应用中，生存数据通常是不完整、审核、分布和保密的，特别是在医疗设置中，隐私是非常重要。数据的稀缺性可能会使生存模型在分布式应用中的扩展性受到严重限制。联邦学习是一种有 Promise 的技术，它允许机器学习模型在多个数据集上进行训练，而不需要违反用户隐私。因此，它在生存数据和大规模生存应用中具有潜在的优势。 despite ， many  direction  in the context of survival analysis remains unexplored in federated learning.In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Web-and-Knowledge-Graphs-for-Automated-Impact-Investing-Scoring"><a href="#Harnessing-the-Web-and-Knowledge-Graphs-for-Automated-Impact-Investing-Scoring" class="headerlink" title="Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring"></a>Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02622">http://arxiv.org/abs/2308.02622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingzhi Hu, Daniel Daza, Laurens Swinkels, Kristina Ūsaitė, Robbert-Jan ‘t Hoen, Paul Groth</li>
<li>for: 该研究旨在自动化SDG框架创建过程，提高了SDG批判和分析的效率和准确性。</li>
<li>methods: 该研究提出了一种数据驱动的方法，包括收集和筛选不同网络源和知识图文本数据，然后使用这些数据进行分类，预测公司与SDG的对应度。</li>
<li>results: 实验结果显示，该模型可以准确预测SDG分数，微平均F1分数为0.89，证明该方案的有效性。此外，该研究还提出了一种可以让人类使用的模型解释方法，以便更好地理解和使用模型预测结果。<details>
<summary>Abstract</summary>
The Sustainable Development Goals (SDGs) were introduced by the United Nations in order to encourage policies and activities that help guarantee human prosperity and sustainability. SDG frameworks produced in the finance industry are designed to provide scores that indicate how well a company aligns with each of the 17 SDGs. This scoring enables a consistent assessment of investments that have the potential of building an inclusive and sustainable economy. As a result of the high quality and reliability required by such frameworks, the process of creating and maintaining them is time-consuming and requires extensive domain expertise. In this work, we describe a data-driven system that seeks to automate the process of creating an SDG framework. First, we propose a novel method for collecting and filtering a dataset of texts from different web sources and a knowledge graph relevant to a set of companies. We then implement and deploy classifiers trained with this data for predicting scores of alignment with SDGs for a given company. Our results indicate that our best performing model can accurately predict SDG scores with a micro average F1 score of 0.89, demonstrating the effectiveness of the proposed solution. We further describe how the integration of the models for its use by humans can be facilitated by providing explanations in the form of data relevant to a predicted score. We find that our proposed solution enables access to a large amount of information that analysts would normally not be able to process, resulting in an accurate prediction of SDG scores at a fraction of the cost.
</details>
<details>
<summary>摘要</summary>
《可持续发展目标（SDG）》由联合国引入，以促进政策和活动，确保人类发展和可持续。 SDG 框架在金融业中生成，用于提供对每个 SDG 的分数，以衡量公司是否与它们相align。这些分数允许对投资进行一致的评估，以建立包容和可持续的经济。由于需要高质量和可靠性，创建和维护 SDG 框架的过程是时间consuming 和需要广泛领域专业知识。在这种情况下，我们描述了一个数据驱动的系统，用于自动化 SDG 框架的创建过程。我们首先提出了一种新的方法，收集和筛选来自不同网络源和知识图库相关的公司文本数据集。然后，我们实施和部署基于这些数据的分类器，以预测公司与 SDG 的Alignment 分数。我们的结果表明，我们的最佳表现模型可以准确预测 SDG 分数，μicro 平均 F1 分数为 0.89，证明了我们的解决方案的有效性。我们还描述了如何将模型与人类使用者集成，通过提供预测分数的数据可视化来提供解释。我们发现，我们的提议的解决方案可以访问大量信息， analysts  normally 不能处理，并在较低的成本下实现准确的 SDG 分数预测。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Method-for-Predicting-Traffic-Signal-Timing-from-Probe-Vehicle-Data"><a href="#A-Machine-Learning-Method-for-Predicting-Traffic-Signal-Timing-from-Probe-Vehicle-Data" class="headerlink" title="A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data"></a>A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02370">http://arxiv.org/abs/2308.02370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juliette Ugirumurera, Joseph Severino, Erik A. Bensen, Qichao Wang, Jane Macfarlane</li>
<li>for: 本研究使用机器学习技术来估算交通信号时间信息从车辆探测数据中。</li>
<li>methods: 我们使用极限梯度提升（XGBoost）模型来估算信号周期长度，并使用神经网络模型来确定每个阶段的红灯时间。</li>
<li>results: 我们的结果显示估算周期长度的错误在0.56秒之间，而红灯时间预测的平均错误为7.2秒。<details>
<summary>Abstract</summary>
Traffic signals play an important role in transportation by enabling traffic flow management, and ensuring safety at intersections. In addition, knowing the traffic signal phase and timing data can allow optimal vehicle routing for time and energy efficiency, eco-driving, and the accurate simulation of signalized road networks. In this paper, we present a machine learning (ML) method for estimating traffic signal timing information from vehicle probe data. To the authors best knowledge, very few works have presented ML techniques for determining traffic signal timing parameters from vehicle probe data. In this work, we develop an Extreme Gradient Boosting (XGBoost) model to estimate signal cycle lengths and a neural network model to determine the corresponding red times per phase from probe data. The green times are then be derived from the cycle length and red times. Our results show an error of less than 0.56 sec for cycle length, and red times predictions within 7.2 sec error on average.
</details>
<details>
<summary>摘要</summary>
交通信号机制环境中，交通信号控制对交通流控制和安全性具有重要作用。此外，了解交通信号阶段和时间数据可以帮助车辆进行优化的路径规划，以提高时间和能源效率，eco- driving，以及准确地模拟信号化道路网络。本文提出了一种机器学习（ML）方法，用于从车辆探测数据中估算交通信号时间信息。作者知道的研究 Works 中，很少有使用机器学习技术来确定交通信号时间参数从车辆探测数据。本文开发了极大幂boosting（XGBoost）模型来估算信号阶段长度，并使用神经网络模型来确定每个阶段的红灯时间。绿灯时间则可以从阶段长度和红灯时间中 derivation。我们的结果显示，ecycle length 的预测错误在0.56秒左右，而红灯时间的预测错误在7.2秒左右。
</details></li>
</ul>
<hr>
<h2 id="Color-Image-Recovery-Using-Generalized-Matrix-Completion-over-Higher-Order-Finite-Dimensional-Algebra"><a href="#Color-Image-Recovery-Using-Generalized-Matrix-Completion-over-Higher-Order-Finite-Dimensional-Algebra" class="headerlink" title="Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra"></a>Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02621">http://arxiv.org/abs/2308.02621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Liao, Zhuang Guo, Qi Gao, Yan Wang, Fajun Yu, Qifeng Zhao, Stephen Johh Maybank</li>
<li>for: 填充颜色图像中缺失数据的高精度图像完成</li>
<li>methods: 基于扩展的高阶矩阵模型，包括像素 neighborgood 扩展策略来描述地方像素约束</li>
<li>results: 对于各种算法进行了广泛的实验，并与公共可用的图像进行了比较，结果显示，我们的扩展矩阵完成模型和相应的算法与其低阶矩阵和传统矩阵对手相比，性能很高。<details>
<summary>Abstract</summary>
To improve the accuracy of color image completion with missing entries, we present a recovery method based on generalized higher-order scalars. We extend the traditional second-order matrix model to a more comprehensive higher-order matrix equivalent, called the "t-matrix" model, which incorporates a pixel neighborhood expansion strategy to characterize the local pixel constraints. This "t-matrix" model is then used to extend some commonly used matrix and tensor completion algorithms to their higher-order versions. We perform extensive experiments on various algorithms using simulated data and algorithms on simulated data and publicly available images and compare their performance. The results show that our generalized matrix completion model and the corresponding algorithm compare favorably with their lower-order tensor and conventional matrix counterparts.
</details>
<details>
<summary>摘要</summary>
为提高颜色图像完成缺失项的准确性，我们提出一种基于泛化高阶约束的恢复方法。我们将传统的第二阶矩阵模型扩展到更加全面的高阶矩阵等价物，称之为“t-矩阵”模型，该模型通过描述当地像素约束的像素邻域扩展策略。这个“t-矩阵”模型后来用于扩展一些通常使用的矩阵和张量完成算法到其高阶版本。我们在各种算法上进行了广泛的实验，使用模拟数据和公共可用的图像，并比较了其性能。结果显示，我们的泛化约束模型和相应的算法与其低阶张量和传统矩阵counterparts相比，表现良好。
</details></li>
</ul>
<hr>
<h2 id="Intensity-free-Integral-based-Learning-of-Marked-Temporal-Point-Processes"><a href="#Intensity-free-Integral-based-Learning-of-Marked-Temporal-Point-Processes" class="headerlink" title="Intensity-free Integral-based Learning of Marked Temporal Point Processes"></a>Intensity-free Integral-based Learning of Marked Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02360">http://arxiv.org/abs/2308.02360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stepinsilence/ifib">https://github.com/stepinsilence/ifib</a></li>
<li>paper_authors: Sishun Liu, Ke Deng, Xiuzhen Zhang, Yongli Ren</li>
<li>for: 这个论文的目的是为了开发一个高精度的数值点事件模型，以应对发生在多维数值空间中的数值事件。</li>
<li>methods: 这个论文使用了一个名为IFIB的解决方案，它是一种不使用强度函数的方法，它直接模型了条件共同PDF $p^{*}(m,t)$，并且可以实现高精度的数值点事件模型。</li>
<li>results: 这个论文的实验结果显示，IFIB可以实现高精度的数值点事件模型，并且在实际应用中具有较好的性能。另外，这个论文还提供了一个可用的代码库，供其他研究者使用。<details>
<summary>Abstract</summary>
In the marked temporal point processes (MTPP), a core problem is to parameterize the conditional joint PDF (probability distribution function) $p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history. The majority of existing studies predefine intensity functions. Their utility is challenged by specifying the intensity function's proper form, which is critical to balance expressiveness and processing efficiency. Recently, there are studies moving away from predefining the intensity function -- one models $p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point processes (TPPs), which do not consider marks. This study aims to develop high-fidelity $p^*(m,t)$ for discrete events where the event marks are either categorical or numeric in a multi-dimensional continuous space. We propose a solution framework IFIB (\underline{I}ntensity-\underline{f}ree \underline{I}ntegral-\underline{b}ased process) that models conditional joint PDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifies the process to compel the essential mathematical restrictions. We show the desired properties of IFIB and the superior experimental results of IFIB on real-world and synthetic datasets. The code is available at \url{https://github.com/StepinSilence/IFIB}.
</details>
<details>
<summary>摘要</summary>
在标记时间点过程中（MTPP），核心问题是参数化 conditional joint PDF（概率分布函数）$p^*(m,t)$， conditioned on the history，其中 $m$ 表示事件标记， $t$ 表示事件间隔时间。大多数现有研究都是先定义INTENSITY函数。然而，这些INTENSITY函数的合适形式是关键，需要平衡表达能力和处理效率。在最近几年，有一些研究尝试离开先定义INTENSITY函数，其中一种是将 $p^*(t)$ 和 $p^*(m)$ 分别模型，另一种是关注时间点过程（TPP），不考虑标记。本研究旨在开发高精度的 $p^*(m,t)$  для精确的事件时间点，其中事件标记可以是 categorical 或 numeric，并且在多维连续空间中。我们提出了一种解决方案框架 IFIB（INTENSITY-free INTEGRAL-based process），它直接模型 conditional joint PDF $p^*(m,t)$ 无需INTENSITY函数。这有效简化了过程，使得mathematical restrictions强制性地减少。我们显示了 IFIB 的愿望性质和实验结果，并提供了实验结果。代码可以在 <https://github.com/StepinSilence/IFIB> 上获取。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-for-GTFS-From-Words-to-Information"><a href="#ChatGPT-for-GTFS-From-Words-to-Information" class="headerlink" title="ChatGPT for GTFS: From Words to Information"></a>ChatGPT for GTFS: From Words to Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02618">http://arxiv.org/abs/2308.02618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/utel-uiuc/gtfs_llm">https://github.com/utel-uiuc/gtfs_llm</a></li>
<li>paper_authors: Saipraneeth Devunuri, Shirin Qiam, Lewis Lehe</li>
<li>For: The paper aims to explore the ability of large language models (LLMs) to retrieve information from the General Transit Feed Specification (GTFS) using natural language instructions.* Methods: The paper uses the ChatGPT model (GPT-3.5) to test its understanding of the GTFS specification and to perform information extraction from a filtered GTFS feed with 4 routes. The paper compares zero-shot and program synthesis methods for information retrieval.* Results: The paper finds that program synthesis achieves higher accuracy (~90% for simple questions and ~40% for complex questions) than zero-shot methods for information retrieval from GTFS using natural language instructions.<details>
<summary>Abstract</summary>
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
</details>
<details>
<summary>摘要</summary>
通用交通Feed规范（GTFS）是公共交通数据的发布标准，这种标准是表格数据，信息分布在不同文件中，因此需要专门的工具或包装来获取信息。同时，使用大型自然语言模型（LLM）来检索文本和信息的使用也在增长。本研究的想法是看看目前广泛采用的LLM（ChatGPT）能否使用自然语言指令来从GTFS中提取信息。我们首先测试了GPT-3.5是否理解GTFS规范。GPT-3.5回答了我们的多项选择题（MCQ）77% correctly。接下来，我们让LLM从过滤后的GTFS feed中提取信息。为了获取信息，我们比较了零shot和程序合成。程序合成更好，实现了~90%的简单问题的准确率和~40%的复杂问题的准确率。
</details></li>
</ul>
<hr>
<h2 id="Multi-attacks-Many-images-the-same-adversarial-attack-to-many-target-labels"><a href="#Multi-attacks-Many-images-the-same-adversarial-attack-to-many-target-labels" class="headerlink" title="Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels"></a>Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03792">http://arxiv.org/abs/2308.03792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanislavfort/multi-attacks">https://github.com/stanislavfort/multi-attacks</a></li>
<li>paper_authors: Stanislav Fort</li>
<li>for: 这个论文旨在描述一种可以同时对多个图像进行攻击的攻击方法。</li>
<li>methods: 这个论文使用了一种称为”多重攻击”的方法，可以对多个图像进行攻击，并且可以在不同的目标类上进行攻击。</li>
<li>results: 论文表明，使用这种多重攻击方法可以对数百个图像进行攻击，并且可以在不同的图像和目标类上进行攻击。此外，论文还发现了一些相关的结果，如图像的高信任区域数量是$\mathcal{O}(100)$以上，这会带来一些问题 для防御策略。<details>
<summary>Abstract</summary>
We show that we can easily design a single adversarial perturbation $P$ that changes the class of $n$ images $X_1,X_2,\dots,X_n$ from their original, unperturbed classes $c_1, c_2,\dots,c_n$ to desired (not necessarily all the same) classes $c^*_1,c^*_2,\dots,c^*_n$ for up to hundreds of images and target classes at once. We call these \textit{multi-attacks}. Characterizing the maximum $n$ we can achieve under different conditions such as image resolution, we estimate the number of regions of high class confidence around a particular image in the space of pixels to be around $10^{\mathcal{O}(100)}$, posing a significant problem for exhaustive defense strategies. We show several immediate consequences of this: adversarial attacks that change the resulting class based on their intensity, and scale-independent adversarial examples. To demonstrate the redundancy and richness of class decision boundaries in the pixel space, we look for its two-dimensional sections that trace images and spell words using particular classes. We also show that ensembling reduces susceptibility to multi-attacks, and that classifiers trained on random labels are more susceptible. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
我们显示出可以轻松设计一个单一敌对偏移$P$，使$n$个图像$X_1,X_2,\dots,X_n$的原始、未偏变的类别变更为Target类别$c_1,c_2,\dots,c_n$的欲要（可能不是所有的类别都是一样的）类别$c^*_1,c^*_2,\dots,c^*_n$，称之为“多元攻击”。我们估计在不同的图像分辨率下，可以达到大量的$n$，并且考虑到像素空间中高度信任类别的区域数量约为$10^{\mathcal{O}(100)}$，这会对对抗策略造成严重的问题。我们显示了一些立即的后果：对于图像的数量和Target类别的变化，以及对于图像的缩放和转换的类别攻击。为了证明像素空间中类别决策boundary的丰富和紧张，我们寻找了图像和字串之间的二维部分，并证明了折衣组合可以对抗多元攻击，而且随机 labels 训练的分类器更加易受到攻击。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Adapting-to-Change-Robust-Counterfactual-Explanations-in-Dynamic-Data-Landscapes"><a href="#Adapting-to-Change-Robust-Counterfactual-Explanations-in-Dynamic-Data-Landscapes" class="headerlink" title="Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes"></a>Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02353">http://arxiv.org/abs/2308.02353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bardhprenkaj/hansel">https://github.com/bardhprenkaj/hansel</a></li>
<li>paper_authors: Bardh Prenkaj, Mario Villaizan-Vallelado, Tobias Leemann, Gjergji Kasneci</li>
<li>for: This paper presents a novel semi-supervised method for counterfactual explanation generation, called Dynamic GRAph Counterfactual Explainer (DyGRACE).</li>
<li>methods: DyGRACE uses two graph autoencoders (GAEs) to learn the representation of each class in a binary classification scenario, and optimizes a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximizing the factual autoencoder’s reconstruction error.</li>
<li>results: DyGRACE is effective in identifying counterfactuals and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle’s predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery.<details>
<summary>Abstract</summary>
We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE) methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages initial knowledge about the data distribution to search for valid counterfactuals while avoiding using information from potentially outdated decision functions in subsequent time steps. Employing two graph autoencoders (GAEs), DyGRACE learns the representation of each class in a binary classification scenario. The GAEs minimise the reconstruction error between the original graph and its learned representation during training. The method involves (i) optimising a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximising the factual autoencoder's reconstruction error, (ii) minimising the counterfactual autoencoder's error, and (iii) maximising the similarity between the factual and counterfactual graphs. This semi-supervised approach is independent of an underlying black-box oracle. A logistic regression model is trained on a set of graph pairs to learn weights that aid in finding counterfactuals. At inference, for each unseen graph, the logistic regressor identifies the best counterfactual candidate using these learned weights, while the GAEs can be iteratively updated to represent the continual adaptation of the learned graph representation over iterations. DyGRACE is quite effective and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle's predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery. DyGRACE, with its capacity for contrastive learning and drift detection, will offer new avenues for semi-supervised learning and explanation generation.
</details>
<details>
<summary>摘要</summary>
我们介绍一种新的半监督式グラフカウンターファクタルエクスプレイナー（GCE）方法，即动态GRAPHカウンターファクタルエクスプレイナー（DyGRACE）。它利用初始知识来搜寻有效的假设，而不需要在后续时间步骤中使用可能已过时的决策函数。使用两个图自动生成器（GAE），DyGRACE学习图像中的每个类别表现。在训练过程中，GAE将图像和其学习的表现之间的差异降到最小。方法包括：(i) 优化一个 Parametric Density Function（实现为逻辑回归函数），以确定假设，最大化实际自动生成器的重建错误。(ii) 降低假设自动生成器的错误。(iii) 将实际和假设图像之间的相似度最大化。这个半监督式方法不需要背景黑盒模型，可以独立进行假设搜寻。在推断过程中，一个逻辑回归模型将被训练，以学习对图像对的权重，并且在每次推断过程中选择最佳的假设候选者。在迭代过程中，GAEs可以逐步更新，以反映适应学习的图像表现。DyGRACE能够实现对照学习和解释生成，并且可以检测分布迁移，根据不同的重建错误值进行推断。这些特点使得DyGRACE能够提高假设搜寻的效率，并且具有跨时间的内存和适应能力。
</details></li>
</ul>
<hr>
<h2 id="RobustMQ-Benchmarking-Robustness-of-Quantized-Models"><a href="#RobustMQ-Benchmarking-Robustness-of-Quantized-Models" class="headerlink" title="RobustMQ: Benchmarking Robustness of Quantized Models"></a>RobustMQ: Benchmarking Robustness of Quantized Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02350">http://arxiv.org/abs/2308.02350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, Xianglong Liu</li>
<li>for: 评估量化神经网络模型的可靠性和抗噪性。</li>
<li>methods: 使用了多种噪音（攻击性噪音、自然噪音和系统噪音）对量化神经网络模型进行了全面的评估。</li>
<li>results: 研究发现，量化模型对攻击性噪音具有更高的抗噪性，但对自然噪音和系统噪音更容易受损。增加量化比特宽度会导致对攻击性噪音的抗噪性下降，对自然噪音和系统噪音的抗噪性增加。等类型的噪音对量化模型的影响不同。<details>
<summary>Abstract</summary>
Quantization has emerged as an essential technique for deploying deep neural networks (DNNs) on devices with limited resources. However, quantized models exhibit vulnerabilities when exposed to various noises in real-world applications. Despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example: (1) quantized models exhibit higher adversarial robustness than their floating-point counterparts, but are more vulnerable to natural corruptions and systematic noises; (2) in general, increasing the quantization bit-width results in a decrease in adversarial robustness, an increase in natural robustness, and an increase in systematic robustness; (3) among corruption methods, \textit{impulse noise} and \textit{glass blur} are the most harmful to quantized models, while \textit{brightness} has the least impact; (4) among systematic noises, the \textit{nearest neighbor interpolation} has the highest impact, while bilinear interpolation, cubic interpolation, and area interpolation are the three least harmful. Our research contributes to advancing the robust quantization of models and their deployment in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
量化技术已成为深度神经网络（DNNs）部署在有限资源设备的重要手段。然而，量化模型在实际应用中受到各种噪声的影响，这些噪声包括攻击性噪声、自然损害和系统性噪声。despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example:1. 量化模型对攻击性噪声比浮点模型更高，但对自然损害和系统性噪声更容易受到影响。2. 在不同的量化比特宽度下，对攻击性噪声的影响随着量化比特宽度的增加而逐渐减少，对自然损害和系统性噪声的影响则随着量化比特宽度的增加而逐渐增加。3. 对量化模型的噪声方法，抖擦噪声和玻璃噪声是最有害的两种，而亮度噪声对量化模型的影响最小。4. 对系统性噪声，最近的邻居 interpolate 是最有害的一种，而 bilinear interpolate、cubic interpolate 和 area interpolate 是最弱的三种。我们的研究为深度神经网络的可靠量化和实际应用做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Vehicles-Control-Collision-Avoidance-using-Federated-Deep-Reinforcement-Learning"><a href="#Vehicles-Control-Collision-Avoidance-using-Federated-Deep-Reinforcement-Learning" class="headerlink" title="Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning"></a>Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02614">http://arxiv.org/abs/2308.02614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badr Ben Elallid, Amine Abouaomar, Nabil Benamar, Abdellatif Kobbane</li>
<li>for: 运输管理和安全性问题在都市化的人口增加和车辆量增加的情况下变得非常重要。这篇论文探讨了在碰撞避免方面使用智能控制系统的发展，并且利用联合深度循环学习（FDRL）技术。</li>
<li>methods: 这篇论文使用了两种模型：地方模型（DDPG）和联合模型（FDDPG），并进行了比较分析，以决定它们在碰撞避免方面的效果。</li>
<li>results: 结果显示，使用FDDPG算法可以更好地控制车辆，避免碰撞。尤其是，FDDPG-based algorithm在减少旅行延迟和提高平均速度方面表现出了明显的改善。<details>
<summary>Abstract</summary>
In the face of growing urban populations and the escalating number of vehicles on the roads, managing transportation efficiently and ensuring safety have become critical challenges. To tackle these issues, the development of intelligent control systems for vehicles is paramount. This paper presents a comprehensive study on vehicle control for collision avoidance, leveraging the power of Federated Deep Reinforcement Learning (FDRL) techniques. Our main goal is to minimize travel delays and enhance the average speed of vehicles while prioritizing safety and preserving data privacy. To accomplish this, we conducted a comparative analysis between the local model, Deep Deterministic Policy Gradient (DDPG), and the global model, Federated Deep Deterministic Policy Gradient (FDDPG), to determine their effectiveness in optimizing vehicle control for collision avoidance. The results obtained indicate that the FDDPG algorithm outperforms DDPG in terms of effectively controlling vehicles and preventing collisions. Significantly, the FDDPG-based algorithm demonstrates substantial reductions in travel delays and notable improvements in average speed compared to the DDPG algorithm.
</details>
<details>
<summary>摘要</summary>
面对城市人口增长和交通量不断增加的问题，有效地管理交通和确保安全已成为核心挑战。为此，开发智能控制系统 для车辆是极其重要的。本文通过详细的研究，探讨了采用联邦深度强化学习（FDRL）技术来控制车辆，以最小化旅行延迟和提高车辆的平均速度，同时保持数据隐私。为此，我们进行了本地模型（DDPG）和全球模型（FDDPG）的比较分析，以确定它们在避免碰撞方面的效果。结果表明，FDDPG算法在控制车辆和避免碰撞方面表现较好，并且在旅行延迟和车辆平均速度方面具有显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Neural-Networks-with-more-flexible-memory-better-predictions-than-rough-volatility"><a href="#Recurrent-Neural-Networks-with-more-flexible-memory-better-predictions-than-rough-volatility" class="headerlink" title="Recurrent Neural Networks with more flexible memory: better predictions than rough volatility"></a>Recurrent Neural Networks with more flexible memory: better predictions than rough volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08550">http://arxiv.org/abs/2308.08550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Damien Challet, Vincent Ragel</li>
<li>for: 这篇论文旨在扩展循环神经网络，以应对具有长 памя于或高度不均匀的时间步骤。</li>
<li>methods: 这篇论文使用了多个灵活的时间尺度，以提高循环神经网络的能力，并与标准LSTM进行比较。</li>
<li>results: 相比标准LSTM，扩展LSTM需要训练更少的epoch，并且预测资产波动性的模型体系性高于20%。<details>
<summary>Abstract</summary>
We extend recurrent neural networks to include several flexible timescales for each dimension of their output, which mechanically improves their abilities to account for processes with long memory or with highly disparate time scales. We compare the ability of vanilla and extended long short term memory networks (LSTMs) to predict asset price volatility, known to have a long memory. Generally, the number of epochs needed to train extended LSTMs is divided by two, while the variation of validation and test losses among models with the same hyperparameters is much smaller. We also show that the model with the smallest validation loss systemically outperforms rough volatility predictions by about 20% when trained and tested on a dataset with multiple time series.
</details>
<details>
<summary>摘要</summary>
我们扩展回传神经网络，以包括每个输出维度的多个灵活时间尺度，以机械提高其能力处理具有长期记忆或高度不同时间尺度的过程。我们比较了净体和扩展的长期快短时间记忆网络（LSTM）的能力预测资产波动性，知道具有长期记忆。通常，训练扩展LSTM需要的轮数比vanilla LSTM要少半，并且模型之间的验证和测试损失的变化相对较小。我们还显示，具有最小验证损失的模型系统地超过了使用多个时间序列的预测波动性预测值的20%。
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-Hypergraph-Collaborative-Networks"><a href="#Stability-and-Generalization-of-Hypergraph-Collaborative-Networks" class="headerlink" title="Stability and Generalization of Hypergraph Collaborative Networks"></a>Stability and Generalization of Hypergraph Collaborative Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02347">http://arxiv.org/abs/2308.02347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Ng, Hanrui Wu, Andy Yip</li>
<li>For: 本研究旨在确保hypergraph collaborative networks的核心层的算法稳定性，并提供一般化保证。* Methods: 本文使用hypergraph collaborative networks，并通过对它们的核心层进行分析，提供了一般化保证。* Results: 实验结果表明，通过合适地调整数据和hypergraph filters的缩放，可以实现uniform的学习过程稳定性。<details>
<summary>Abstract</summary>
Graph neural networks have been shown to be very effective in utilizing pairwise relationships across samples. Recently, there have been several successful proposals to generalize graph neural networks to hypergraph neural networks to exploit more complex relationships. In particular, the hypergraph collaborative networks yield superior results compared to other hypergraph neural networks for various semi-supervised learning tasks. The collaborative network can provide high quality vertex embeddings and hyperedge embeddings together by formulating them as a joint optimization problem and by using their consistency in reconstructing the given hypergraph. In this paper, we aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees. The analysis sheds light on the design of hypergraph filters in collaborative networks, for instance, how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process. Some experimental results on real-world datasets are presented to illustrate the theory.
</details>
<details>
<summary>摘要</summary>
graph neural networks 有 shown 能够很 effectively 利用 sample 对的 pairwise 关系。 最近， 有 several successful proposals 将 graph neural networks 扩展到 hypergraph neural networks，以利用更复杂的关系。特别是， hypergraph collaborative networks 在 various semi-supervised learning tasks 中 yield superior results compared to other hypergraph neural networks。 collaborative network 可以提供 high quality vertex embeddings 和 hyperedge embeddings，通过 formulating them as a joint optimization problem 和使用 their consistency in reconstructing the given hypergraph。在这篇 paper 中，我们 aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees。analysis  shed light on the design of hypergraph filters in collaborative networks, such as how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process。some experimental results on real-world datasets are presented to illustrate the theory。
</details></li>
</ul>
<hr>
<h2 id="Learning-Networks-from-Gaussian-Graphical-Models-and-Gaussian-Free-Fields"><a href="#Learning-Networks-from-Gaussian-Graphical-Models-and-Gaussian-Free-Fields" class="headerlink" title="Learning Networks from Gaussian Graphical Models and Gaussian Free Fields"></a>Learning Networks from Gaussian Graphical Models and Gaussian Free Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02344">http://arxiv.org/abs/2308.02344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhro Ghosh, Soumendu Sundar Mukherjee, Hoang-Son Tran, Ujan Gangopadhyay</li>
<li>for: 估计weighted网络的结构</li>
<li>methods: 基于重复测量Gaussian Graphical Model（GGM）的方法</li>
<li>results: 提出一种新的估计器，可以从GGM的复杂概率特性中提取有用信息，并且可以提供具体的回归保证和样本复杂度下界。特别是，在 Erdos-Renyi 随机网络上，我们证明了在样本大小 $n$ 足够大时，网络结构可以recovery With high probability.<details>
<summary>Abstract</summary>
We investigate the problem of estimating the structure of a weighted network from repeated measurements of a Gaussian Graphical Model (GGM) on the network. In this vein, we consider GGMs whose covariance structures align with the geometry of the weighted network on which they are based. Such GGMs have been of longstanding interest in statistical physics, and are referred to as the Gaussian Free Field (GFF). In recent years, they have attracted considerable interest in the machine learning and theoretical computer science. In this work, we propose a novel estimator for the weighted network (equivalently, its Laplacian) from repeated measurements of a GFF on the network, based on the Fourier analytic properties of the Gaussian distribution. In this pursuit, our approach exploits complex-valued statistics constructed from observed data, that are of interest on their own right. We demonstrate the effectiveness of our estimator with concrete recovery guarantees and bounds on the required sample complexity. In particular, we show that the proposed statistic achieves the parametric rate of estimation for fixed network size. In the setting of networks growing with sample size, our results show that for Erdos-Renyi random graphs $G(d,p)$ above the connectivity threshold, we demonstrate that network recovery takes place with high probability as soon as the sample size $n$ satisfies $n \gg d^4 \log d \cdot p^{-2}$.
</details>
<details>
<summary>摘要</summary>
我们研究如何从重复观测 Gaussian Graphical Model (GGM) 中 estimate 网络的结构。在这个意境下，我们考虑 GGM 的协调结构与网络的重量相对应。这些 GGM 在统计物理学中有很长的历史，通常被称为 Gaussian Free Field (GFF)。在最近几年中，它们在机器学习和理论计算机科学中受到了很大的关注。在这个工作中，我们提出了一个新的网络重量Estimator，基于网络上重复观测 GFF 的 Fourier分析特性。我们的方法利用观测数据中的复数统计，具有自己的科学价值。我们显示了这个统计的效果，并提供了具体的回溯保证和sample complexity bound。尤其是，我们证明了这个统计在固定网络大小下具有参数率的估计率。在探索网络规模 grow 于样本大小的情况下，我们的结果显示，当样本大小 $n$ 满足 $n \gg d^4 \log d \cdot p^{-2}$ 时，网络重建很有可能会在高可信度下发生。Note: "Simplified Chinese" is a romanization of Chinese that uses a simplified set of characters and grammar rules to represent the language. It is commonly used in mainland China and Singapore, and is one of the two official languages of the People's Republic of China.
</details></li>
</ul>
<hr>
<h2 id="RAHNet-Retrieval-Augmented-Hybrid-Network-for-Long-tailed-Graph-Classification"><a href="#RAHNet-Retrieval-Augmented-Hybrid-Network-for-Long-tailed-Graph-Classification" class="headerlink" title="RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification"></a>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02335">http://arxiv.org/abs/2308.02335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyang Mao, Wei Ju, Yifang Qin, Xiao Luo, Ming Zhang</li>
<li>for: 提高图像分类 tasks 中的泛化能力，尤其是在长尾类分布的real-world数据中。</li>
<li>methods: 提出了一种名为 Retrieval Augmented Hybrid Network (RAHNet) 的新框架，用于同时学习一个 Robust 特征提取器和一个不偏袋化的分类器，并在特征提取器和分类器之间进行分离学习。在特征提取器训练阶段，我们开发了一个图像检索模块，用于搜索与tail类相关的图像，以直接增强tail类之间的内部多样性。在分类器细化阶段，我们使用了两种重量规regularization技术，即Max-norm和weight decay，以均衡分类器的重量。</li>
<li>results: 在各种流行的benchmark上进行了实验，并证明了我们的方法在state-of-the-artapproaches中的优越性。<details>
<summary>Abstract</summary>
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant graphs that directly enrich the intra-class diversity for the tail classes. Moreover, we innovatively optimize a category-centered supervised contrastive loss to obtain discriminative representations, which is more suitable for long-tailed scenarios. In the classifier fine-tuning stage, we balance the classifier weights with two weight regularization techniques, i.e., Max-norm and weight decay. Experiments on various popular benchmarks verify the superiority of the proposed method against state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
“图像分类是现实世界多媒体应用中的关键任务，图像可以表示各种媒体数据类型，如图像、视频和社交网络。先前的尝试都是在平衡的情况下应用图像神经网络（GNNs），但实际数据通常会出现长尾分布，导致使用GNNs时对尾类的偏袋和有限的泛化能力。现有的方法主要集中在模型训练时重新平衡不同类别，但这会失去新知识的导入和头类的性能。为解决这些缺点，我们提出了一种新的框架，即Retrieval Augmented Hybrid Network（RAHNet），它可以同时学习一个强健的特征提取器和一个不偏袋的分类器。在特征提取器训练阶段，我们开发了一个图像检索模块，以找到适当的图像来增强尾类的内部多样性。此外，我们还创新地优化了一种类型中心的超级vised对比损失，以获得适合长尾情况的表示，”Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Interoperable-synthetic-health-data-with-SyntHIR-to-enable-the-development-of-CDSS-tools"><a href="#Interoperable-synthetic-health-data-with-SyntHIR-to-enable-the-development-of-CDSS-tools" class="headerlink" title="Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools"></a>Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02613">http://arxiv.org/abs/2308.02613</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/potter-coder89/synthir">https://github.com/potter-coder89/synthir</a></li>
<li>paper_authors: Pavitra Chauhan, Mohsen Gamal Saad Askar, Bjørn Fjukstad, Lars Ailo Bongo, Edvard Pedersen<br>for:这个论文旨在提出一种基于机器学习的临床决策支持系统（CDSS）的开发方法，使用高质量的患者日志和医疗注册来生成 synthetic EHR 数据，并在临床工作流程中实现 CDSS 工具的开发和测试。methods:这个论文使用的方法包括使用 FHIR 标准实现数据互操作性，使用 Gretel 框架生成 synthetic 数据，使用 Microsoft Azure FHIR 服务器作为基于 FHIR 的 EHR 系统，以及使用 SMART on FHIR 框架实现工具可重用性。results:论文通过开发一个基于机器学习的 CDSS 工具，使用 Norwegian Patient Register (NPR) 和 Norwegian Patient Prescriptions (NorPD) 数据进行开发，并在 SyntHIR 系统上测试和评估该工具。结果表明，SyntHIR 提供了一个通用的 CDSS 工具开发框架，可以使用 synthetic FHIR 数据进行测试和评估，并且可以在临床 setting 中实现。但是，synthetic 数据质量的问题还需要进一步改进。代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/potter-coder89/SyntHIR.git%E3%80%82">https://github.com/potter-coder89/SyntHIR.git。</a><details>
<summary>Abstract</summary>
There is a great opportunity to use high-quality patient journals and health registers to develop machine learning-based Clinical Decision Support Systems (CDSS). To implement a CDSS tool in a clinical workflow, there is a need to integrate, validate and test this tool on the Electronic Health Record (EHR) systems used to store and manage patient data. However, it is often not possible to get the necessary access to an EHR system due to legal compliance. We propose an architecture for generating and using synthetic EHR data for CDSS tool development. The architecture is implemented in a system called SyntHIR. The SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR) standards for data interoperability, the Gretel framework for generating synthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system and SMART on FHIR framework for tool transportability. We demonstrate the usefulness of SyntHIR by developing a machine learning-based CDSS tool using data from the Norwegian Patient Register (NPR) and Norwegian Patient Prescriptions (NorPD). We demonstrate the development of the tool on the SyntHIR system and then lift it to the Open DIPS environment. In conclusion, SyntHIR provides a generic architecture for CDSS tool development using synthetic FHIR data and a testing environment before implementing it in a clinical setting. However, there is scope for improvement in terms of the quality of the synthetic data generated. The code is open source and available at https://github.com/potter-coder89/SyntHIR.git.
</details>
<details>
<summary>摘要</summary>
“有一大机会使用高质量的患者日记和医疗注册来开发基于机器学习的临床决策支持系统（CDSS）。为实现CDSS工具在临床工作流程中的应用，需要将这个工具与电子医疗记录（EHR）系统集成、验证和测试。然而，由于法律合规的问题，通常无法获得EHR系统的必要访问权。我们提出了一种使用生成的Synthetic EHR数据来开发CDSS工具的建筑方案。该建筑方案在一个名为SyntHIR的系统中实现，该系统使用Fast Healthcare Interoperability Resources（FHIR）标准来实现数据互操作，使用Gretel框架生成synthetic数据，使用Microsoft Azure FHIR服务器作为FHIR基于EHR系统，并使用SMART on FHIR框架来提供工具可重用性。我们通过使用挪威患者注册（NPR）和挪威药品订单（NorPD）的数据开发了一个基于机器学习的CDSS工具，并在SyntHIR系统上测试了该工具。最后，我们将工具提取到Open DIPS环境中。总之，SyntHIR提供了一个通用的CDSS工具开发基于Synthetic FHIR数据的测试环境，但是可以进一步提高生成的synthetic数据质量。代码可以在https://github.com/potter-coder89/SyntHIR.git中获取。”
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-spike-detection-in-deep-brain-stimulation-surgery"><a href="#Deep-learning-for-spike-detection-in-deep-brain-stimulation-surgery" class="headerlink" title="Deep learning for spike detection in deep brain stimulation surgery"></a>Deep learning for spike detection in deep brain stimulation surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05755">http://arxiv.org/abs/2308.05755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arkadiusz Nowacki, Ewelina Kołpa, Mateusz Szychiewicz, Konrad Ciecierski</li>
<li>for: 这个论文是为了描述一种基于深度学习的神经活动记录分析方法，用于深 bran stimulation（DBS） neurosurgery 中的 neuronal activity 识别。</li>
<li>methods: 该方法使用了一种卷积神经网络（CNN）来分析神经活动记录，并在不同的时间窗口中进行识别。</li>
<li>results: 实验结果表明，该方法可以达到最高的准确率（98.98%）和受器操作特征曲线的面积（AUC）的最高值（0.9898），而无需进行数据预处理。<details>
<summary>Abstract</summary>
Deep brain stimulation (DBS) is a neurosurgical procedure successfully used to treat conditions such as Parkinson's disease. Electrostimulation, carried out by implanting electrodes into an identified focus in the brain, makes it possible to reduce the symptoms of the disease significantly. In this paper, a method for analyzing recordings of neuronal activity acquired during DBS neurosurgery using deep learning is presented. We tested using a convolutional neural network (CNN) for this purpose. Based on the time window, the classifier assesses whether neuronal activity (spike) is present. The maximum accuracy value for the classifier was 98.98%, and the area under the receiver operating characteristic curve (AUC) was 0.9898. The method made it possible to obtain a classification without using data preprocessing.
</details>
<details>
<summary>摘要</summary>
深度脑刺激（DBS）是一种 neurosurgical 程序，已经成功地治疗了 Parkinson's disease 等疾病。通过在脑中implanting 电极，可以减轻疾病的 симптом。在这篇论文中，我们提出了使用深度学习分析 DBS  neurosurgery 中记录的 neuronal 活动的方法。我们测试了 convolutional neural network（CNN）来完成这个任务。根据时间窗口，分类器评估 neuronal 活动（脉冲）是否存在。最大准确率值为 98.98%，准确率下接收操作特征曲线（AUC）值为 0.9898。这种方法可以不使用数据预处理来获得分类。
</details></li>
</ul>
<hr>
<h2 id="A-stochastic-optimization-approach-to-train-non-linear-neural-networks-with-a-higher-order-variation-regularization"><a href="#A-stochastic-optimization-approach-to-train-non-linear-neural-networks-with-a-higher-order-variation-regularization" class="headerlink" title="A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization"></a>A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02293">http://arxiv.org/abs/2308.02293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oknakfm/hovr">https://github.com/oknakfm/hovr</a></li>
<li>paper_authors: Akifumi Okuno</li>
<li>For: This paper aims to address the issue of overfitting in highly expressive parametric models, such as deep neural networks, by introducing a new regularization term called $(k,q)$th order variation regularization ($(k,q)$-VR).* Methods: The paper proposes a stochastic optimization algorithm that can efficiently train general models with the $(k,q)$-VR term without conducting explicit numerical integration. The algorithm is based on stochastic gradient descent and automatic differentiation, and can be applied to the training of deep neural networks with arbitrary structure.* Results: The paper demonstrates that the neural networks trained with the $(k,q)$-VR terms are more “resilient” than those with the conventional parameter regularization, and the proposed algorithm can also be extended to the physics-informed training of neural networks (PINNs).<details>
<summary>Abstract</summary>
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $(k,q)$th order variation regularization ($(k,q)$-VR), which is defined as the $q$th-powered integral of the absolute $k$th order derivative of the parametric models to be trained; penalizing the $(k,q)$-VR is expected to yield a smoother function, which is expected to avoid overfitting. Particularly, $(k,q)$-VR encompasses the conventional (general-order) total variation with $q=1$. While the $(k,q)$-VR terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $(k,q)$-VR without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradient descent algorithm and automatic differentiation. Our numerical experiments demonstrate that the neural networks trained with the $(k,q)$-VR terms are more ``resilient'' than those with the conventional parameter regularization. The proposed algorithm also can be extended to the physics-informed training of neural networks (PINNs).
</details>
<details>
<summary>摘要</summary>
“而高度表达力的 parametric 模型，如深度神经网络，具有模型复杂概念的优势。然而，训练这些非线性模型时存在高风险的过拟合。为解决这个问题，本研究考虑了 $(k,q)$ 项变化规则（$(k,q)$-VR），即将要训练的 parametric 模型的 $q$ 阶幂化积分 absolute $k$ 阶差分。penalizing $(k,q)$-VR 会导致更平滑的函数，以避免过拟合。特别是，$(k,q)$-VR 包括普通（总阶）变量的 $q=1$。而 $(k,q)$-VR 应用于普通 parametric 模型时 computationally intractable due to integration，本研究提供了一种可efficiently 训练通用模型的随机优化算法。这种方法可以应用于深度神经网络的训练，并且可以通过简单的随机梯度下降算法和自动导数来实现。我们的numerical experiments表明，使用 $(k,q)$-VR 训练的神经网络比使用传统参数正则化更为“坚固”。此外，这种算法还可以扩展到物理学信息训练神经网络（PINNs）。”
</details></li>
</ul>
<hr>
<h2 id="Frustratingly-Easy-Model-Generalization-by-Dummy-Risk-Minimization"><a href="#Frustratingly-Easy-Model-Generalization-by-Dummy-Risk-Minimization" class="headerlink" title="Frustratingly Easy Model Generalization by Dummy Risk Minimization"></a>Frustratingly Easy Model Generalization by Dummy Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02287">http://arxiv.org/abs/2308.02287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncheng Wang, Jindong Wang, Xixu Hu, Shujun Wang, Xing Xie</li>
<li>for: 提高机器学习模型的泛化能力</li>
<li>methods: 使用拟合风险最小化（Dummy Risk Minimization，DuRM）技术，即通过扩大输出логи特征来提高模型的泛化能力</li>
<li>results: DuRM可以在多个任务上提高表现，包括传统的分类、Semantic segmentation、out-of-distribution泛化、对抗训练和长尾识别等，并且可以与现有的泛化技术相结合使用。<details>
<summary>Abstract</summary>
Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the performance under all tasks with an almost free lunch manner. Furthermore, we show that DuRM is compatible with existing generalization techniques and we discuss possible limitations. We hope that DuRM could trigger new interest in the fundamental research on risk minimization.
</details>
<details>
<summary>摘要</summary>
empirical risk minimization (ERM) 是机器学习的一种基本思想。然而，其泛化能力在各种任务上有限。在这篇论文中，我们提出了干扰risk minimization（DuRM），一种极其简单和普遍适用的技术，以提高ERM的泛化能力。DuRM的实现非常简单：只需扩大输出logits的维度，然后使用标准的梯度下降优化。我们在理论和实验两方面 validate DuRM的有效性。在理论上，我们表明DuRM可以提高模型的泛化能力，通过观察更好的平坦的本地极小值。在实验上，我们对不同的数据集、模式和网络架构进行了多种任务的评估，包括传统的分类、semantic segmentation、out-of-distribution泛化、对抗训练和长尾识别。结果表明，DuRM可以在所有任务上提高性能，几乎没有免费的午餐。此外，我们还证明了DuRM与现有的泛化技术相容，并讨论了可能的限制。我们希望DuRM可以触发新的研究于风险最小化的基础。
</details></li>
</ul>
<hr>
<h2 id="DIVERSIFY-A-General-Framework-for-Time-Series-Out-of-distribution-Detection-and-Generalization"><a href="#DIVERSIFY-A-General-Framework-for-Time-Series-Out-of-distribution-Detection-and-Generalization" class="headerlink" title="DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization"></a>DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02282">http://arxiv.org/abs/2308.02282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, Xiangyang Ji, Qiang Yang, Xing Xie<br>for: This paper aims to address the challenges of out-of-distribution (OOD) detection and generalization on time series data, which is non-stationary and has dynamic distributions.methods: The proposed method, DIVERSIFY, is an iterative framework that uses adversarial training to obtain the “worst-case” latent distribution scenario, and then reduces the gap between these latent distributions. DIVERSIFY combines existing OOD detection methods with outputs of models for detection and utilizes outputs for classification.results: Extensive experiments on seven datasets with different OOD settings show that DIVERSIFY learns more generalized features and significantly outperforms other baselines. Theoretical insights also support the effectiveness of DIVERSIFY.<details>
<summary>Abstract</summary>
Time series remains one of the most challenging modalities in machine learning research. The out-of-distribution (OOD) detection and generalization on time series tend to suffer due to its non-stationary property, i.e., the distribution changes over time. The dynamic distributions inside time series pose great challenges to existing algorithms to identify invariant distributions since they mainly focus on the scenario where the domain information is given as prior knowledge. In this paper, we attempt to exploit subdomains within a whole dataset to counteract issues induced by non-stationary for generalized representation learning. We propose DIVERSIFY, a general framework, for OOD detection and generalization on dynamic distributions of time series. DIVERSIFY takes an iterative process: it first obtains the "worst-case" latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We implement DIVERSIFY via combining existing OOD detection methods according to either extracted features or outputs of models for detection while we also directly utilize outputs for classification. In addition, theoretical insights illustrate that DIVERSIFY is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY learns more generalized features and significantly outperforms other baselines.
</details>
<details>
<summary>摘要</summary>
时序序列仍然是机器学习研究中最为困难的模式之一。非站点性（OOD）检测和泛化在时序序列上通常受到非站点性的影响，即时序序列的分布随着时间的变化。时序序列中的动态分布对现有算法提供了很大挑战，因为它们主要假设有域信息作为先验知识。在这篇论文中，我们尝试利用时序序列中的子领域来缓解由非站点性引起的问题，以实现泛化学习。我们提出了DIVERSIFY，一种通用框架，用于OOD检测和泛化动态分布的时序序列。DIVERSIFY采用了迭代过程：首先通过对恶性学习获得“最差”的幂本分布场景，然后减少这些幂本分布之间的差距。我们通过结合现有OOD检测方法来实现DIVERSIFY，并直接利用模型输出进行分类。此外，理论分析表明DIVERSIFY是理论上支持的。我们对七个不同的数据集进行了广泛的实验，包括手势识别、语音命令识别、着装压力和情感识别以及基于传感器的人体活动识别。结果表明DIVERSIFY学习了更泛化的特征，并显著超过了其他基elines。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Proximal-Gradient-Method-for-Convex-Optimization"><a href="#Adaptive-Proximal-Gradient-Method-for-Convex-Optimization" class="headerlink" title="Adaptive Proximal Gradient Method for Convex Optimization"></a>Adaptive Proximal Gradient Method for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02261">http://arxiv.org/abs/2308.02261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yura Malitsky, Konstantin Mishchenko</li>
<li>for: 本文研究了两种基本的首阶算法在凸优化中，即梯度下降（GD）和 proximal梯度方法（ProxGD）。我们的注意点是使这两种算法完全适应тив，利用凸函数的地方几何信息。</li>
<li>methods: 我们提出了基于观察到的梯度差的自适应GD和ProxGD版本，无需额外计算成本。此外，我们证明了我们的方法在只假设本地 lipschitz 的梯度下 converges。</li>
<li>results: 我们的方法可以使用更大的步长 than those initially suggested in [MM20]。<details>
<summary>Abstract</summary>
In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了两种基本的首阶算法在凸优化中，即梯度下降（GD）和贝克斯 gradient 方法（ProxGD）。我们的关注点是使这些算法完全适应ive，利用当地凸函数的曲率信息。我们提出了基于观察到的梯度差的自适应GD和ProxGD版本，无需额外计算成本。此外，我们证明了我们的方法在本地lipchitz continuous的梯度下 converges。此外，我们的方法还允许更大的步长than those initially suggested in [MM20].
</details></li>
</ul>
<hr>
<h2 id="Finding-Tori-Self-supervised-Learning-for-Analyzing-Korean-Folk-Song"><a href="#Finding-Tori-Self-supervised-Learning-for-Analyzing-Korean-Folk-Song" class="headerlink" title="Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song"></a>Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02249">http://arxiv.org/abs/2308.02249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danbinaerinhan/finding-tori">https://github.com/danbinaerinhan/finding-tori</a></li>
<li>paper_authors: Danbinaerin Han, Rafael Caro Repetto, Dasaem Jeong</li>
<li>for: 这个论文是对韩国民族歌曲录音数据集进行计算分析的，该数据集包含约700小时的民歌，录制于1980-90年代。</li>
<li>methods: 作者使用自动超vision学习和卷积神经网络，通过抽象报表来解决录音中的挑战。</li>
<li>results: 实验结果表明，作者的方法可以更好地捕捉韩国民歌中的护卷特征，比传统的抑制历史更加精准。通过这种方法，作者可以对现有学术中的音乐讨论在实际录音中进行实质性的探讨。<details>
<summary>Abstract</summary>
In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种计算方法对韩国传统歌曲场记录数据集进行分析，该数据集约为700小时，录制于1980-90年代。由于大多数歌曲由非专业音乐家演唱，没有伴奏，因此该数据集具有许多挑战。为 Addressing this challenge, we utilized self-supervised learning with convolutional neural networks based on pitch contours, and analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. Our experimental results show that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.Here's the breakdown of the translation:* 韩国传统歌曲 (Korean traditional folk songs) -> 韩国传统歌曲 (Simplified Chinese)* 场记录数据集 (field recording dataset) -> 场记录数据集 (Simplified Chinese)* 约为700小时 (approximately 700 hours) -> 约为700小时 (Simplified Chinese)* 录制于1980-90年代 (recorded in the 1980s-1990s) -> 录制于1980-90年代 (Simplified Chinese)* 非专业音乐家 (non-expert musicians) -> 非专业音乐家 (Simplified Chinese)* 没有伴奏 (no accompaniment) -> 没有伴奏 (Simplified Chinese)* 计算方法 (computational method) -> 计算方法 (Simplified Chinese)* 自动学习 (self-supervised learning) -> 自动学习 (Simplified Chinese)* 基于折衣 (based on pitch contours) -> 基于折衣 (Simplified Chinese)* tori (a classification system) -> tori (Simplified Chinese)* 定义为特定的音阶、装饰音和idiomatic melodic contour -> 定义为特定的音阶、装饰音和idiomatic melodic contour (Simplified Chinese)* 使用我们的方法可以更好地捕捉折衣的特点 -> 使用我们的方法可以更好地捕捉折衣的特点 (Simplified Chinese)* 比传统折衣 histogram 更好 -> 比传统折衣 histogram 更好 (Simplified Chinese)* 使用我们的方法 -> 使用我们的方法 (Simplified Chinese)* 我们已经使用这些方法 -> 我们已经使用这些方法 (Simplified Chinese)* 对现有的音乐学讨论进行实际应用 -> 对现有的音乐学讨论进行实际应用 (Simplified Chinese)* 探讨了韩国传统歌曲中的音乐讨论 -> 探讨了韩国传统歌曲中的音乐讨论 (Simplified Chinese)
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-networks-from-the-perspective-of-ergodic-theory"><a href="#Deep-neural-networks-from-the-perspective-of-ergodic-theory" class="headerlink" title="Deep neural networks from the perspective of ergodic theory"></a>Deep neural networks from the perspective of ergodic theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03888">http://arxiv.org/abs/2308.03888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang</li>
<li>for: 这个论文旨在解释深度神经网络的设计是如何变成一种更加科学的过程，而不是一种艺术。</li>
<li>methods: 这个论文使用了时间演化观的思想，将神经网络看作是一个动力系统的时间演化，每层对应于一个时间实例。</li>
<li>results: 这个论文表明，一些可能看起来神秘的规则，可以被解释为启发。<details>
<summary>Abstract</summary>
The design of deep neural networks remains somewhat of an art rather than precise science. By tentatively adopting ergodic theory considerations on top of viewing the network as the time evolution of a dynamical system, with each layer corresponding to a temporal instance, we show that some rules of thumb, which might otherwise appear mysterious, can be attributed heuristics.
</details>
<details>
<summary>摘要</summary>
神经网络设计仍然很有创造性，更像是一种艺术而非精确科学。通过尝试将ergodic theory应用于视网膜上，视网膜为时间演化的动力系统，每层对应一个时间实例，我们显示了一些可能看起来神秘的规则，实际上可以归结为优化策略。
</details></li>
</ul>
<hr>
<h2 id="Self-Normalizing-Neural-Network-Enabling-One-Shot-Transfer-Learning-for-Modeling-EDFA-Wavelength-Dependent-Gain"><a href="#Self-Normalizing-Neural-Network-Enabling-One-Shot-Transfer-Learning-for-Modeling-EDFA-Wavelength-Dependent-Gain" class="headerlink" title="Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain"></a>Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02233">http://arxiv.org/abs/2308.02233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agastya Raj, Zehao Wang, Frank Slyne, Tingjun Chen, Dan Kilper, Marco Ruffini</li>
<li>for: 该论文旨在提出一种基于 semi-supervised, self-normalizing neural networks 的多芯片 EDFA 波长依赖性的模型化框架，以实现一次转移学习。</li>
<li>methods: 该论文使用 semi-supervised, self-normalizing neural networks 来模型多芯片 EDFA 的波长依赖性，并实现了一次转移学习。</li>
<li>results: 实验结果表明，该模型在 Open Ireland 和 COSMOS 测试平台上的 22 个 EDFA 中具有高精度的转移学习能力，即使操作在不同的芯片类型上。<details>
<summary>Abstract</summary>
We present a novel ML framework for modeling the wavelength-dependent gain of multiple EDFAs, based on semi-supervised, self-normalizing neural networks, enabling one-shot transfer learning. Our experiments on 22 EDFAs in Open Ireland and COSMOS testbeds show high-accuracy transfer-learning even when operated across different amplifier types.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的机器学习框架，用于模型多个电子发射激光扩展器（EDFA）的波长依赖性收益，基于半监督自适应神经网络。我们的实验表明，这种框架可以在不同类型的扩展器上实现高精度的传输学习，并且可以在22个EDFA上进行一次转移学习。
</details></li>
</ul>
<hr>
<h2 id="Likelihood-ratio-based-confidence-intervals-for-neural-networks"><a href="#Likelihood-ratio-based-confidence-intervals-for-neural-networks" class="headerlink" title="Likelihood-ratio-based confidence intervals for neural networks"></a>Likelihood-ratio-based confidence intervals for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02221">http://arxiv.org/abs/2308.02221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/laurenssluyterman/likelihood_ratio_intervals">https://github.com/laurenssluyterman/likelihood_ratio_intervals</a></li>
<li>paper_authors: Laurens Sluijterman, Eric Cator, Tom Heskes</li>
<li>for: 这个论文是为了建立一种基于likelihood ratio的方法来计算神经网络的信心 интерval。</li>
<li>methods: 这个方法使用了likelihood ratio的思想，可以建立不对称的信心 интерval，并且自动包含了训练时间、网络架构、训练技巧等因素。</li>
<li>results: 这个方法可以在对于医学预测或天文物理等领域，提供一个可靠的未知度估计，并且显示出这种方法在某些情况下可能已经有经济效益。<details>
<summary>Abstract</summary>
This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文介绍了一种基于likelihood-ratio的神经网络置信范围的首次实现方法，称为DeepLR。我们的方法具有许多优点：能够构建不均匀的置信范围，在数据有限的地方扩展，同时自动包含训练时间、网络架构和正则化技术等因素。虽然当前实现可能对许多深度学习应用程序来说过于昂贵，但在医学预测或天文物理等领域，准确地估计单个预测结果的不确定性可能已经被 justify。这篇文章探讨了基于likelihood-ratio的置信范围的可能性，并开启了未来研究的新途径。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Driven-Multi-Agent-Reinforcement-Learning-for-Computation-Offloading-in-Cybertwin-Enabled-Internet-of-Vehicles"><a href="#Knowledge-Driven-Multi-Agent-Reinforcement-Learning-for-Computation-Offloading-in-Cybertwin-Enabled-Internet-of-Vehicles" class="headerlink" title="Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles"></a>Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02603">http://arxiv.org/abs/2308.02603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijin Sun, Xiao Yang, Nan Cheng, Xiucheng Wang, Changle Li</li>
<li>for: 提高 cybertwin-enabled IoV 中任务卸载延迟</li>
<li>methods: 使用知识驱动多代理人学习（KMARL）方法，利用域知识加入图 neural networks，实现选择最佳卸载选项</li>
<li>results: 比较其他方法，KMARL 表现更高的奖励和更好的扩展性，受到域知识的整合帮助<details>
<summary>Abstract</summary>
By offloading computation-intensive tasks of vehicles to roadside units (RSUs), mobile edge computing (MEC) in the Internet of Vehicles (IoV) can relieve the onboard computation burden. However, existing model-based task offloading methods suffer from heavy computational complexity with the increase of vehicles and data-driven methods lack interpretability. To address these challenges, in this paper, we propose a knowledge-driven multi-agent reinforcement learning (KMARL) approach to reduce the latency of task offloading in cybertwin-enabled IoV. Specifically, in the considered scenario, the cybertwin serves as a communication agent for each vehicle to exchange information and make offloading decisions in the virtual space. To reduce the latency of task offloading, a KMARL approach is proposed to select the optimal offloading option for each vehicle, where graph neural networks are employed by leveraging domain knowledge concerning graph-structure communication topology and permutation invariance into neural networks. Numerical results show that our proposed KMARL yields higher rewards and demonstrates improved scalability compared with other methods, benefitting from the integration of domain knowledge.
</details>
<details>
<summary>摘要</summary>
通过异步计算任务转移到路边单元（RSU），移动边缘计算（MEC）在互联网机器人（IoV）中可以减轻车辆上计算负担。然而，现有的模型基于任务转移方法受到增加车辆和数据驱动方法的计算复杂性的影响。为解决这些挑战，在这篇论文中，我们提出了知识驱动多智能体强化学习（KMARL）方法，以减少异步任务转移的延迟。具体来说，在考虑的场景中， cybertwin 作为每辆车辆的通信代理，在虚拟空间中交换信息并做出转移决策。通过使用图神经网络，我们利用了域知识，包括图structure 通信topology和 permutation 不变性，从而提高了强化学习的精度和扩展性。 numerically 的结果表明，我们提出的 KMARL 方法可以获得更高的奖励，并且在其他方法相比，具有更好的扩展性，受益于域知识的集成。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Spanish-Clinical-Language-Models"><a href="#A-Survey-of-Spanish-Clinical-Language-Models" class="headerlink" title="A Survey of Spanish Clinical Language Models"></a>A Survey of Spanish Clinical Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02199">http://arxiv.org/abs/2308.02199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem García Subies, Álvaro Barbero Jiménez, Paloma Martínez Fernández</li>
<li>for: 这项研究专注于使用语言模型解决西班牙语医疗领域任务。</li>
<li>methods: 研究人员回顾了17个词库，主要集中在医疗任务上，然后列出了最有影响力的西班牙语语言模型和医疗语言模型。研究人员还对这些模型进行了严格的比较，用于找出最佳performing的模型，总共超过3000个模型进行了微调。</li>
<li>results: 研究人员对一些可访问的 corpora 进行了测试，并将结果公开发布，以便由独立团队重复或在未来对新的西班牙语医疗语言模型进行挑战。<details>
<summary>Abstract</summary>
This survey focuses in encoder Language Models for solving tasks in the clinical domain in the Spanish language. We review the contributions of 17 corpora focused mainly in clinical tasks, then list the most relevant Spanish Language Models and Spanish Clinical Language models. We perform a thorough comparison of these models by benchmarking them over a curated subset of the available corpora, in order to find the best-performing ones; in total more than 3000 models were fine-tuned for this study. All the tested corpora and the best models are made publically available in an accessible way, so that the results can be reproduced by independent teams or challenged in the future when new Spanish Clinical Language models are created.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" in Chinese.Please note that the translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="AutoML4ETC-Automated-Neural-Architecture-Search-for-Real-World-Encrypted-Traffic-Classification"><a href="#AutoML4ETC-Automated-Neural-Architecture-Search-for-Real-World-Encrypted-Traffic-Classification" class="headerlink" title="AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification"></a>AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02182">http://arxiv.org/abs/2308.02182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orangeuw/automl4etc">https://github.com/orangeuw/automl4etc</a></li>
<li>paper_authors: Navid Malekghaini, Elham Akbari, Mohammad A. Salahuddin, Noura Limam, Raouf Boutaba, Bertrand Mathieu, Stephanie Moteau, Stephane Tuffin</li>
<li>for: 这个研究是为了提出一个自动设计高性能的神经网络模型，用于实时隐私化网络流量分类。</li>
<li>methods: 这个研究使用了自动机器学习（AutoML）技术，定义了一个特定设计的搜寻空间，并运用不同的搜寻策略来寻找最佳的神经网络模型。</li>
<li>results: 研究发现，使用AutoML4ETC可以自动设计高性能的神经网络模型，并且比现有的隐私化网络流量分类模型更加精确和轻量级。<details>
<summary>Abstract</summary>
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark datasets and real-world TLS and QUIC traffic collected from the Orange mobile network. In addition to being more accurate, AutoML4ETC's architectures are significantly more efficient and lighter in terms of the number of parameters. Finally, we make AutoML4ETC publicly available for future research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-Clinical-Trial-Matching-Using-Large-Language-Models-A-Case-Study-in-Oncology"><a href="#Scaling-Clinical-Trial-Matching-Using-Large-Language-Models-A-Case-Study-in-Oncology" class="headerlink" title="Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology"></a>Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02180">http://arxiv.org/abs/2308.02180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cliff Wong, Sheng Zhang, Yu Gu, Christine Moung, Jacob Abel, Naoto Usuyama, Roshanthi Weerasinghe, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon</li>
<li>For: The paper is written for scaling clinical trial matching using large language models (LLMs) in the field of oncology.* Methods: The paper uses a systematic study approach with cutting-edge LLMs such as GPT-4 to structure eligibility criteria of clinical trials and extract complex matching logic.* Results: The initial findings show that LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop, but there are still areas for improvement such as context limitation and accuracy.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了扩大临床试验匹配使用大型自然语言模型（LLMs）的应用，主要是在肿瘤领域。* Methods: 该论文使用系统性的研究方法，使用最新的GPT-4等 LLMS来结构临床试验资格标准和提取复杂匹配逻辑。* Results: 初步发现结果表明，LLMs已经比前一代强大基elinesubstantially better，可能用作人工协作的初步解决方案，但还有一些需要进一步改进的方向，如上下文限制和准确性。<details>
<summary>Abstract</summary>
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.
</details>
<details>
<summary>摘要</summary>
临床试验匹配是医疗卫生系统中一个关键的过程，但在实践中却受到极多的不结构化数据和不可扩展的手动处理的困扰。在这篇论文中，我们进行了系统性的研究，使用大型自然语言模型（LLMs）来扩大临床试验匹配的规模。我们的研究基于一个目前在大型美国医疗网络中测试的临床试验匹配系统。初步的结果很有前途：直接使用最新的GPT-4等 cutting-edge LLMs，可以立即结构化临床试验报名标准和提取复杂的匹配逻辑（例如，嵌入 AND/OR/NOT 结构）。虽然还有一定的改进空间，但LLMs已经明显超过了先前的强基线，并可能作为人工干预的准备解决方案。我们的研究还揭示了应用LLMs到终端临床试验匹配中的一些重要成长点，如Context limitation和准确率，特别是从患者 longitudinal 医疗记录中提取patient信息。
</details></li>
</ul>
<hr>
<h2 id="High-Accuracy-Prediction-of-Metal-Insulator-Metal-Metasurface-with-Deep-Learning"><a href="#High-Accuracy-Prediction-of-Metal-Insulator-Metal-Metasurface-with-Deep-Learning" class="headerlink" title="High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning"></a>High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04450">http://arxiv.org/abs/2308.04450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaizhu Liu, Hsiang-Chen Chui, Changsen Sun, Xue Han</li>
<li>for: 本研究旨在提出一种基于深度学习的电磁软件计算结果预测方法，以提高计算效率和准确性。</li>
<li>methods: 本研究使用了ResNets-10模型进行预测плазмон喷流表 parameters的方法，并采用了k-fold cross-validation和小学习率的两个阶段训练。</li>
<li>results: 根据实验结果，对铝、金、银金属-隔体-铁的预测损失值分别为-48.45、-46.47和-35.54，表明提出的网络可以取代传统电磁计算方法，并且训练过程只需要少于1,100个迭代。<details>
<summary>Abstract</summary>
Deep learning prediction of electromagnetic software calculation results has been a widely discussed issue in recent years. But the prediction accuracy was still one of the challenges to be solved. In this work, we proposed that the ResNets-10 model was used for predicting plasmonic metasurface S11 parameters. The two-stage training was performed by the k-fold cross-validation and small learning rate. After the training was completed, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace the traditional electromagnetic computing method for calculation within a certain structural range. Besides, this network can finish the training process less than 1,100 epochs. This means that the network training process can effectively lower the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and biosensors, thereby reducing the time required for the calculation process. The ultralow error of the network indicates that this work contributes to the development of future artificial intelligence electromagnetic computing software.
</details>
<details>
<summary>摘要</summary>
Recently, deep learning prediction of electromagnetic software calculation results has been a widely discussed issue. However, prediction accuracy was still a challenge to be solved. In this work, we proposed using the ResNets-10 model to predict plasmonic metasurface S11 parameters. We performed two-stage training with k-fold cross-validation and small learning rate. After training, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace traditional electromagnetic computing methods for calculation within a certain structural range. Additionally, this network can complete the training process in less than 1,100 epochs, effectively lowering the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and biosensors, reducing the calculation process time. The ultralow error of the network indicates that this work contributes to the development of future artificial intelligence electromagnetic computing software.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-probabilistic-models-enhance-variational-autoencoder-for-crystal-structure-generative-modeling"><a href="#Diffusion-probabilistic-models-enhance-variational-autoencoder-for-crystal-structure-generative-modeling" class="headerlink" title="Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling"></a>Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02165">http://arxiv.org/abs/2308.02165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teerachote Pakornchote, Natthaphon Choomphon-anomakhun, Sorrjit Arrerut, Chayanon Atthapak, Sakarn Khamkaeo, Thiparat Chotibut, Thiti Bovornratanaraks</li>
<li>for: 生成真实的晶体结构，保持晶体对称性</li>
<li>methods: 使用新的扩散概率模型（DP模型）对原子坐标进行减噪，而不是采用标准的分数匹配方法</li>
<li>results: 能够生成和重建晶体结构，质量与原始CDVAE相似，而且与 relaxed 结构计算得到的碳结构更加接近ground state，能量差值平均为68.1 meV&#x2F;atom 下降，表明DP-CDVAE模型能够更好地代表晶体结构的ground state配置。<details>
<summary>Abstract</summary>
The crystal diffusion variational autoencoder (CDVAE) is a machine learning model that leverages score matching to generate realistic crystal structures that preserve crystal symmetry. In this study, we leverage novel diffusion probabilistic (DP) models to denoise atomic coordinates rather than adopting the standard score matching approach in CDVAE. Our proposed DP-CDVAE model can reconstruct and generate crystal structures whose qualities are statistically comparable to those of the original CDVAE. Furthermore, notably, when comparing the carbon structures generated by the DP-CDVAE model with relaxed structures obtained from density functional theory calculations, we find that the DP-CDVAE generated structures are remarkably closer to their respective ground states. The energy differences between these structures and the true ground states are, on average, 68.1 meV/atom lower than those generated by the original CDVAE. This significant improvement in the energy accuracy highlights the effectiveness of the DP-CDVAE model in generating crystal structures that better represent their ground-state configurations.
</details>
<details>
<summary>摘要</summary>
“单晶扩散条件自适应器”（CDVAE）是一种机器学习模型，利用得分匹配来生成具有实验室同调的晶体结构。在这个研究中，我们使用新的扩散概率模型（DP）来降噪原子坐标而不是采用CDVAE的标准得分匹配方法。我们称之为DP-CDVAE模型。这个模型可以重建和生成具有同等质量的晶体结构，并且在比较碳原子结构的情况下，DP-CDVAE模型生成的结构与 relaxation 计算得到的结构更加接近真实的基体状态。这些结构的能量差异与真实基体状态相比，平均降低了68.1 meV/atom。这显示DP-CDVAE模型具有更好的基体状态表现，并且能够更好地生成具有实验室同调的晶体结构。
</details></li>
</ul>
<hr>
<h2 id="Speaker-Diarization-of-Scripted-Audiovisual-Content"><a href="#Speaker-Diarization-of-Scripted-Audiovisual-Content" class="headerlink" title="Speaker Diarization of Scripted Audiovisual Content"></a>Speaker Diarization of Scripted Audiovisual Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02160">http://arxiv.org/abs/2308.02160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yogesh Virkar, Brian Thompson, Rohit Paturi, Sundararajan Srinivasan, Marcello Federico</li>
<li>for: 这篇论文主要是为了提高媒体本地化行业中的语音识别技术，具体来说是使用制作过程中使用的脚本来提高电视节目中的 speaker diarization 任务。</li>
<li>methods: 这篇论文提出了一种新的 semi-supervised 方法，通过使用制作过程中的脚本来提取 pseudo-labeled 数据，以提高 speaker diarization 任务的准确率。</li>
<li>results: 在测试集上，这种方法与两个无监督基线模型进行比较，实现了51.7% 的提升。<details>
<summary>Abstract</summary>
The media localization industry usually requires a verbatim script of the final film or TV production in order to create subtitles or dubbing scripts in a foreign language. In particular, the verbatim script (i.e. as-broadcast script) must be structured into a sequence of dialogue lines each including time codes, speaker name and transcript. Current speech recognition technology alleviates the transcription step. However, state-of-the-art speaker diarization models still fall short on TV shows for two main reasons: (i) their inability to track a large number of speakers, (ii) their low accuracy in detecting frequent speaker changes. To mitigate this problem, we present a novel approach to leverage production scripts used during the shooting process, to extract pseudo-labeled data for the speaker diarization task. We propose a novel semi-supervised approach and demonstrate improvements of 51.7% relative to two unsupervised baseline models on our metrics on a 66 show test set.
</details>
<details>
<summary>摘要</summary>
媒体地化业务通常需要最终电影或电视制作的字幕或配音脚本的 verbatim 脚本，以便在外语中创建字幕或配音脚本。特别是 verbatim 脚本（即播放版本）必须以时间码、说话人名和对话内容的结构组织。当前的语音识别技术使得转录步骤得以alleviates。然而，当前的话者分类模型仍然在电视节目中存在两个主要问题：（i）它们无法跟踪大量的说话人，（ii）它们在说话人变化频繁时的准确率低。为解决这个问题，我们提出了一种利用摄制过程中使用的制作脚本，提取 pseudo-labeled 数据来进行说话人分类任务。我们提出了一种新的半超vised方法，并在我们的测试集上实现了51.7%的相对提升，比两个无监督基线模型更高。
</details></li>
</ul>
<hr>
<h2 id="Improved-Order-Analysis-and-Design-of-Exponential-Integrator-for-Diffusion-Models-Sampling"><a href="#Improved-Order-Analysis-and-Design-of-Exponential-Integrator-for-Diffusion-Models-Sampling" class="headerlink" title="Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling"></a>Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02157">http://arxiv.org/abs/2308.02157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinsheng Zhang, Jiaming Song, Yongxin Chen</li>
<li>for: 提高 diffusion models (DMs) 的抽象速度，使其能够更快速地进行抽象。</li>
<li>methods: 利用高级别的减法积分器 (EI)，并通过重新设计高级别减法积分器来满足所有顺序条件，从而提高抽象质量和稳定性。</li>
<li>results: 通过 theoretically 和实际应用，提出了一种改进的减法积分器（RES），可以提高抽象质量和稳定性，并且在实际应用中可以减少数值缺陷和提高 FID 值。例如，在 ImageNet 扩散模型中，通过将单步 DPM-Solver++ 替换为 ORDER-satisfied RES solver，可以降低数值缺陷的比例为 25.2%，并提高 FID 值为 25.4%。<details>
<summary>Abstract</summary>
Efficient differential equation solvers have significantly reduced the sampling time of diffusion models (DMs) while retaining high sampling quality. Among these solvers, exponential integrators (EI) have gained prominence by demonstrating state-of-the-art performance. However, existing high-order EI-based sampling algorithms rely on degenerate EI solvers, resulting in inferior error bounds and reduced accuracy in contrast to the theoretically anticipated results under optimal settings. This situation makes the sampling quality extremely vulnerable to seemingly innocuous design choices such as timestep schedules. For example, an inefficient timestep scheduler might necessitate twice the number of steps to achieve a quality comparable to that obtained through carefully optimized timesteps. To address this issue, we reevaluate the design of high-order differential solvers for DMs. Through a thorough order analysis, we reveal that the degeneration of existing high-order EI solvers can be attributed to the absence of essential order conditions. By reformulating the differential equations in DMs and capitalizing on the theory of exponential integrators, we propose refined EI solvers that fulfill all the order conditions, which we designate as Refined Exponential Solver (RES). Utilizing these improved solvers, RES exhibits more favorable error bounds theoretically and achieves superior sampling efficiency and stability in practical applications. For instance, a simple switch from the single-step DPM-Solver++ to our order-satisfied RES solver when Number of Function Evaluations (NFE) $=9$, results in a reduction of numerical defects by $25.2\%$ and FID improvement of $25.4\%$ (16.77 vs 12.51) on a pre-trained ImageNet diffusion model.
</details>
<details>
<summary>摘要</summary>
高效的差分方程解析器在扩散模型（DM）中减少了采样时间，同时保持高质量的采样。其中，对数Integrators（EI）已经成为了状态之一，但现有的高阶EI基本样式依赖于弱化的EI解决方案，从而导致了较差的误差 bound和降低的准确性，与理论预期的结果不符。这种情况使得采样质量极易受到 seems innocuous的设计选择，如时间步骤调度。例如，使用不优化的时间步骤调度可能需要两倍的步骤数量以达到相同的质量。为解决这一问题，我们重新评估了高阶差分解析器的设计。通过系统的顺序分析，我们发现现有高阶EI解决方案的弱化可以归结于缺乏关键的顺序条件。我们根据扩散模型的差分方程和快速Integrators的理论，提出了改进的REFined Exponential Solver（RES）。我们的改进的解析器可以满足所有顺序条件，并且在实际应用中表现出较好的采样效率和稳定性。例如，将单步DPM-Solver++ switched to我们的顺序满足RES解析器，当Number of Function Evaluations（NFE）=9时，可以降低数值缺陷的比例为25.2%，并提高FID的改进率（16.77 vs 12.51）。
</details></li>
</ul>
<hr>
<h2 id="Optimization-on-Pareto-sets-On-a-theory-of-multi-objective-optimization"><a href="#Optimization-on-Pareto-sets-On-a-theory-of-multi-objective-optimization" class="headerlink" title="Optimization on Pareto sets: On a theory of multi-objective optimization"></a>Optimization on Pareto sets: On a theory of multi-objective optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02145">http://arxiv.org/abs/2308.02145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Roy, Geelon So, Yi-An Ma</li>
<li>for: 多目标优化中，一个单一决策 вектор需要寻找许多目标之间的最佳变数平衡。这些解答被称为Pareto优化解答，它们是对任何一个目标进行改善都需要在另一个目标上付出的决策 vector。但是，Pareto优化解答的集合可能很大，因此我们进一步考虑一个更实际 significanse的Pareto受限优化问题，其中的目标是将一个偏好函数对应到Pareto集。</li>
<li>methods: 我们调查了本地方法来解决这个受限优化问题，这个问题存在两个特点：（i）参数集是隐式定义的，（ii）通常是非凸非光滑的。我们定义了优化和稳定性的概念，并提供了一个Algorithm，其中的最后迭代速率为$O(K^{-1&#x2F;2})$，对于具有强式凹陷和Lipschitz光滑的目标而言。</li>
<li>results: 我们的研究表明，当目标是强式凹陷和Lipschitz光滑的时候，我们的方法具有最后迭代速率$O(K^{-1&#x2F;2})$，即在最后一迭代时，数据的变化速率为$O(K^{-1&#x2F;2})$。这表明我们的方法在解决Pareto受限优化问题时具有高效率和稳定性。<details>
<summary>Abstract</summary>
In multi-objective optimization, a single decision vector must balance the trade-offs between many objectives. Solutions achieving an optimal trade-off are said to be Pareto optimal: these are decision vectors for which improving any one objective must come at a cost to another. But as the set of Pareto optimal vectors can be very large, we further consider a more practically significant Pareto-constrained optimization problem, where the goal is to optimize a preference function constrained to the Pareto set.   We investigate local methods for solving this constrained optimization problem, which poses significant challenges because the constraint set is (i) implicitly defined, and (ii) generally non-convex and non-smooth, even when the objectives are. We define notions of optimality and stationarity, and provide an algorithm with a last-iterate convergence rate of $O(K^{-1/2})$ to stationarity when the objectives are strongly convex and Lipschitz smooth.
</details>
<details>
<summary>摘要</summary>
在多目标优化中，单个决策 вектор必须平衡多个目标之间的贸易offs。solutions达到优化的贸易offs是say Pareto优化的：这些决策 вектор在改进任何一个目标时，必须付出另一个目标的代价。但是Pareto优化集可能很大，因此我们进一步考虑一个更实际 significannot的 Pareto受限优化问题，其中的目标是通过对Pareto集进行优化。我们研究了本地方法来解决这个受限优化问题，这个问题具有以下两个特点：（i） constraint set是通过某种方式implcitly定义的，（ii）通常是非拥有凸形和光滑的。我们定义了优化和稳定性的概念，并提供了一个算法，其last-iterate convergence rate为 $O(K^{-1/2})$ 当目标函数是强转化和Lipschitz平滑的时候。
</details></li>
</ul>
<hr>
<h2 id="Event-based-Dynamic-Graph-Representation-Learning-for-Patent-Application-Trend-Prediction"><a href="#Event-based-Dynamic-Graph-Representation-Learning-for-Patent-Application-Trend-Prediction" class="headerlink" title="Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction"></a>Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09780">http://arxiv.org/abs/2308.09780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Zou, Le Yu, Leilei Sun, Bowen Du, Deqing Wang, Fuzhen Zhuang</li>
<li>for: 预测公司将在下一时期申请哪些专利，以估计其发展策略和找到前期伙伴或竞争对手。</li>
<li>methods: 我们提出了一种基于事件驱动图学习框架的专利申请趋势预测方法，利用公司和专利分类码的启动表示和历史记忆，以及 hierarchical message passing mechanism 来捕捉专利分类码的 semantic proximities。</li>
<li>results: 我们的方法在实际数据上进行了多种实验，并emonstrated 其效果 under various experimental conditions，并且探索了方法在学习分类码 semantics 和跟踪公司技术发展轨迹的能力。<details>
<summary>Abstract</summary>
Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic, and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic, and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies."中文翻译：准确预测公司将在下一时间段申请哪种专利，可以为其发展策略提供指导，并在前置的时间内发现可能的合作伙伴或竞争对手。虽然这个问题非常重要，但在前期研究中 rarely studied due to the challenges in modeling companies' continuously evolving preferences and capturing the semantic correlations of classification codes。为了填补这一空白，我们提出了一种基于事件的动态图学学习框架，用于预测专利申请趋势。具体来说，我们的方法基于公司和专利分类代码的记忆表示。当观察到新专利时，相关公司和专利分类代码的表示将根据历史记忆和当前编码的消息进行更新。此外，我们还提供了一种层次消息传递机制，以捕捉专利分类代码的semantic proximity。最后，我们通过 static、动态和层次视角的表示集成来预测专利申请趋势。实验结果表明，我们的方法在不同的实验条件下具有效果，并能够学习分类代码的 semantics和跟踪公司技术发展轨迹。
</details></li>
</ul>
<hr>
<h2 id="Learning-the-solution-operator-of-two-dimensional-incompressible-Navier-Stokes-equations-using-physics-aware-convolutional-neural-networks"><a href="#Learning-the-solution-operator-of-two-dimensional-incompressible-Navier-Stokes-equations-using-physics-aware-convolutional-neural-networks" class="headerlink" title="Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks"></a>Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02137">http://arxiv.org/abs/2308.02137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viktor Grimm, Alexander Heinlein, Axel Klawonn</li>
<li>for: 本研究旨在解决physics-inclusive机器学习技术中geometry的局限性问题，提出一种能够在不同的geometry中学习稳态 Navier-Stokes方程的解 approximate solutions的方法。</li>
<li>methods: 本研究使用了一种基于U-Net-like CNN和finite difference方法的combined方法，并与数据基于方法进行比较。</li>
<li>results: 研究结果表明，physics-aware CNN可以在不同的geometry中学习稳态 Navier-Stokes方程的解 approximate solutions，并且可以与数据基于方法相结合以提高性能。<details>
<summary>Abstract</summary>
In recent years, the concept of introducing physics to machine learning has become widely popular. Most physics-inclusive ML-techniques however are still limited to a single geometry or a set of parametrizable geometries. Thus, there remains the need to train a new model for a new geometry, even if it is only slightly modified. With this work we introduce a technique with which it is possible to learn approximate solutions to the steady-state Navier--Stokes equations in varying geometries without the need of parametrization. This technique is based on a combination of a U-Net-like CNN and well established discretization methods from the field of the finite difference method.The results of our physics-aware CNN are compared to a state-of-the-art data-based approach. Additionally, it is also shown how our approach performs when combined with the data-based approach.
</details>
<details>
<summary>摘要</summary>
近年来，将物理学引入机器学习的概念得到了广泛的推广。然而，大多数物理包含的机器学习技术仍然受限于单个几何或一组可 parametrize 的几何。因此，在新的几何上训练新的模型仍然是必要的。我们在这里介绍一种可以在不同几何中学习稳态奈特-斯托克方程的估计解的技术。这种技术基于一种组合了 U-Net 类 CNN 和已确立的精度方法的finite difference方法。我们对我们的物理意识 CNN 的结果进行了与当前最佳数据驱动方法的比较，同时还展示了我们的方法与数据驱动方法的组合效果。
</details></li>
</ul>
<hr>
<h2 id="Can-Attention-Be-Used-to-Explain-EHR-Based-Mortality-Prediction-Tasks-A-Case-Study-on-Hemorrhagic-Stroke"><a href="#Can-Attention-Be-Used-to-Explain-EHR-Based-Mortality-Prediction-Tasks-A-Case-Study-on-Hemorrhagic-Stroke" class="headerlink" title="Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke"></a>Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05110">http://arxiv.org/abs/2308.05110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhang Feng, Jiayi Yuan, Forhan Bin Emdad, Karim Hanna, Xia Hu, Zhe He</li>
<li>for: 预测中风死亡风险的早期预测</li>
<li>methods: 使用一种新的解释性听力基于变换器模型，以提高预测模型的准确性和可读性</li>
<li>results: 研究表明，这种解释性听力基于变换器模型可以提高预测模型的准确性和可读性，并且可以提供有用的特征重要性信息。<details>
<summary>Abstract</summary>
Stroke is a significant cause of mortality and morbidity, necessitating early predictive strategies to minimize risks. Traditional methods for evaluating patients, such as Acute Physiology and Chronic Health Evaluation (APACHE II, IV) and Simplified Acute Physiology Score III (SAPS III), have limited accuracy and interpretability. This paper proposes a novel approach: an interpretable, attention-based transformer model for early stroke mortality prediction. This model seeks to address the limitations of previous predictive models, providing both interpretability (providing clear, understandable explanations of the model) and fidelity (giving a truthful explanation of the model's dynamics from input to output). Furthermore, the study explores and compares fidelity and interpretability scores using Shapley values and attention-based scores to improve model explainability. The research objectives include designing an interpretable attention-based transformer model, evaluating its performance compared to existing models, and providing feature importance derived from the model.
</details>
<details>
<summary>摘要</summary>
stroke 是一个重要的死亡和残留症状的原因，需要早期预测方法来减少风险。传统的评估病人方法，如急性physiology和慢性健康评估（APACHE II、IV）和简化型急性 физиiology分数III（SAPS III），有限的准确性和可读性。这篇论文提出了一种新的方法：一种可解释的、注意力基本变换模型，用于早期stroke mortality预测。这个模型旨在解决之前的预测模型的局限性，提供了可解释性（提供明确、理解的解释）和诚实性（从输入到输出的模型动力学提供真实的解释）。此外，研究还研究了和比较了可解释性和诚实性分数使用Shapley值和注意力基本分数来提高模型解释性。研究的目标包括设计一种可解释的注意力基本变换模型，评估其性能与现有模型相比，并提供来自模型的特征重要性。
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Optimization-of-Wireless-Federated-Learning-with-Data-Heterogeneity"><a href="#Analysis-and-Optimization-of-Wireless-Federated-Learning-with-Data-Heterogeneity" class="headerlink" title="Analysis and Optimization of Wireless Federated Learning with Data Heterogeneity"></a>Analysis and Optimization of Wireless Federated Learning with Data Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03521">http://arxiv.org/abs/2308.03521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Han, Jun Li, Wen Chen, Zhen Mei, Kang Wei, Ming Ding, H. Vincent Poor</li>
<li>for: 本文旨在研究和优化无线 Federated Learning（FL）中的数据多样性和无线资源分配问题，以提高FL的性能和能效性。</li>
<li>methods: 本文使用closed-form表达式来计算FL损失函数的上界，并对Client Scheduling、资源分配和本地训练 epoch数进行优化。</li>
<li>results: 实验结果表明，提出的算法在实际数据集上比其他参考方法更高的学习精度和能 consumption。<details>
<summary>Abstract</summary>
With the rapid proliferation of smart mobile devices, federated learning (FL) has been widely considered for application in wireless networks for distributed model training. However, data heterogeneity, e.g., non-independently identically distributions and different sizes of training data among clients, poses major challenges to wireless FL. Limited communication resources complicate the implementation of fair scheduling which is required for training on heterogeneous data, and further deteriorate the overall performance. To address this issue, this paper focuses on performance analysis and optimization for wireless FL, considering data heterogeneity, combined with wireless resource allocation. Specifically, we first develop a closed-form expression for an upper bound on the FL loss function, with a particular emphasis on data heterogeneity described by a dataset size vector and a data divergence vector. Then we formulate the loss function minimization problem, under constraints on long-term energy consumption and latency, and jointly optimize client scheduling, resource allocation, and the number of local training epochs (CRE). Next, via the Lyapunov drift technique, we transform the CRE optimization problem into a series of tractable problems. Extensive experiments on real-world datasets demonstrate that the proposed algorithm outperforms other benchmarks in terms of the learning accuracy and energy consumption.
</details>
<details>
<summary>摘要</summary>
随着智能移动设备的普及，分布式学习（FL）在无线网络中得到了广泛的考虑，用于分布式模型训练。然而，数据不均衡，如非独立同分布和不同的训练数据大小 среди客户端，对无线FL的应用带来了主要挑战。限制通信资源使得实现公平调度变得更加困难，从而降低总性能。为解决这个问题，这篇论文关注无线FL的性能分析和优化，考虑到数据不均衡，并与无线资源分配相结合。首先，我们开发了一个关于FL损失函数上的上界，强调数据不均衡的特点，由一个数据大小向量和一个数据差异向量描述。然后，我们将损失函数最小化问题转化为一个具有长期能源占用和延迟的约束的优化问题。通过利用Lyapunov漂移技术，我们将CRE优化问题转化为一系列可解的问题。在实际数据上进行了广泛的实验，结果表明，我们的算法在学习精度和能源消耗方面比其他参考值更高。
</details></li>
</ul>
<hr>
<h2 id="Branched-Latent-Neural-Operators"><a href="#Branched-Latent-Neural-Operators" class="headerlink" title="Branched Latent Neural Operators"></a>Branched Latent Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02599">http://arxiv.org/abs/2308.02599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordcbcl/blno.jl">https://github.com/stanfordcbcl/blno.jl</a></li>
<li>paper_authors: Matteo Salvador, Alison Lesley Marsden</li>
<li>for:  This paper aims to develop a novel computational tool for building reliable and efficient reduced-order models for digital twinning in engineering applications.</li>
<li>methods: The paper proposes the use of Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. BLNOs are defined as simple and compact feedforward partially-connected neural networks that structurally disentangle inputs with different intrinsic roles.</li>
<li>results: The paper demonstrates the effectiveness of BLNOs in a challenging test case involving biophysically detailed electrophysiology simulations in a biventricular cardiac model of a pediatric patient with hypoplastic left heart syndrome. The model includes a purkinje network for fast conduction and a heart-torso geometry. The paper shows that BLNOs can retain just 7 hidden layers and 19 neurons per layer, and achieve a mean square error of $10^{-4}$ on an independent test dataset comprised of 50 additional electrophysiology simulations.<details>
<summary>Abstract</summary>
We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed electrophysiology simulations in a biventricular cardiac model of a pediatric patient with hypoplastic left heart syndrome. The model includes a purkinje network for fast conduction and a heart-torso geometry. Specifically, we trained BLNOs on 150 in silico generated 12-lead electrocardiograms (ECGs) while spanning 7 model parameters, covering cell-scale, organ-level and electrical dyssynchrony. Although the 12-lead ECGs manifest very fast dynamics with sharp gradients, after automatic hyperparameter tuning the optimal BLNO, trained in less than 3 hours on a single CPU, retains just 7 hidden layers and 19 neurons per layer. The mean square error is on the order of $10^{-4}$ on an independent test dataset comprised of 50 additional electrophysiology simulations. This paper provides a novel computational tool to build reliable and efficient reduced-order models for digital twinning in engineering applications.
</details>
<details>
<summary>摘要</summary>
我们引入分支隐藏神经操作符（BLNOs），以学习输入-输出对应器，模型复杂物理过程。 BLNO 是一个简单且紧凑的Feedforward 内部连接神经网络，它将输入变数分类为不同的内在角色，例如时间变数和模型参数，并将它们转换为一个通用的应用领域。 BLNO 利用可读性的隐藏输出增强学习过程，并突破维度给定问题的咒语，通过在训练阶段实现小训练集和短时间内的优秀一致性。此外，对于完全连接结构而言，部分连接可以对缩减可调 Parameters 数量。我们透过实际应用在一个儿童心脏病 hypoplastic left heart syndrome 的双心室心脏模型中，并在该模型中包含 Purkinje 网络和心脏-肋间 geometry。具体来说，我们将 BLNO 训练在 150 个silico生成的 12 项电击ogram (ECG) 上，涵盖 7 个模型参数，包括细胞层、器官层和电子 Dyssynchrony。虽然 12 项 ECG 呈现非常快的动态，但是通过自动优化参数后，最佳 BLNO 在仅三个小时内在单一 CPU 上训练，只有 7 个隐藏层和 19 个神经元 per 层。该模型的平方误差在统计上为 $10^{-4}$，在 50 个其他电生物频谱 simulations 的独立测试集中进行验证。本研究提供了一个新的 Computational 工具，可以建立可靠和高效的实际应用中的简化模型，以应用于工程应用中的数字双胞志。
</details></li>
</ul>
<hr>
<h2 id="Eva-A-General-Vectorized-Approximation-Framework-for-Second-order-Optimization"><a href="#Eva-A-General-Vectorized-Approximation-Framework-for-Second-order-Optimization" class="headerlink" title="Eva: A General Vectorized Approximation Framework for Second-order Optimization"></a>Eva: A General Vectorized Approximation Framework for Second-order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02123">http://arxiv.org/abs/2308.02123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Zhang, Shaohuai Shi, Bo Li</li>
<li>for: 这个研究旨在提高深度学习模型训练的效率，减少计算和记忆过程中的过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程<details>
<summary>Abstract</summary>
Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further extend Eva to a general vectorized approximation framework to improve the compute and memory efficiency of two existing second-order algorithms (FOOF and Shampoo) without affecting their convergence performance. Extensive experimental results on different models and datasets show that Eva reduces the end-to-end training time up to 2.05x and 2.42x compared to first-order SGD and second-order algorithms (K-FAC and Shampoo), respectively.
</details>
<details>
<summary>摘要</summary>
Second-order优化算法在训练深度学习模型时展现出极佳的收敛性质，但通常会导致计算和内存开销增加。这可能会导致训练效率低于首次优化算法 such as 随机梯度下降（SGD）。在这项工作中，我们提出了一种具有内存和时间效率的第二次优化算法名为Eva，并采用了两种新的技术：1）我们通过小批量训练数据的克ро内克分解来减少内存占用，2）我们 derivate了高效的更新公式，不需要直接计算矩阵的逆元。我们进一步扩展Eva到一个通用的向量化近似框架，以提高两个现有的第二次优化算法（FOOF和Shampoo）的计算和内存效率，无需影响其收敛性能。我们在不同的模型和数据集上进行了广泛的实验，结果显示，Eva可以比首次优化算法和第二次优化算法（K-FAC和Shampoo）减少综合训练时间，具体的比例为2.05倍和2.42倍。
</details></li>
</ul>
<hr>
<h2 id="Model-Provenance-via-Model-DNA"><a href="#Model-Provenance-via-Model-DNA" class="headerlink" title="Model Provenance via Model DNA"></a>Model Provenance via Model DNA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02121">http://arxiv.org/abs/2308.02121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Mu, Yu Wang, Yehong Zhang, Jiaqi Zhang, Hui Wang, Yang Xiang, Yue Yu</li>
<li>for: This paper focuses on the problem of Model Provenance (MP) in machine learning (ML), which aims to determine whether a source model serves as the provenance for a target model.</li>
<li>methods: The authors introduce a novel concept of Model DNA, which represents the unique characteristics of a machine learning model, and use a data-driven and model-driven representation learning method to encode the model’s training data and input-output information as a compact and comprehensive representation of the model.</li>
<li>results: The authors develop an efficient framework for model provenance identification, which enables them to accurately identify whether a source model is a pre-training model of a target model. They conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of their approach.<details>
<summary>Abstract</summary>
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model. We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance.
</details>
<details>
<summary>摘要</summary>
To address this gap, we introduce a novel concept called Model DNA, which represents the unique characteristics of a machine learning model. We use a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model.We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance. Our approach is efficient and can be applied to a wide range of ML models, providing a valuable tool for ensuring the security and intellectual property of ML models.
</details></li>
</ul>
<hr>
<h2 id="Designing-a-Deep-Learning-Driven-Resource-Efficient-Diagnostic-System-for-Metastatic-Breast-Cancer-Reducing-Long-Delays-of-Clinical-Diagnosis-and-Improving-Patient-Survival-in-Developing-Countries"><a href="#Designing-a-Deep-Learning-Driven-Resource-Efficient-Diagnostic-System-for-Metastatic-Breast-Cancer-Reducing-Long-Delays-of-Clinical-Diagnosis-and-Improving-Patient-Survival-in-Developing-Countries" class="headerlink" title="Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries"></a>Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02597">http://arxiv.org/abs/2308.02597</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Gao, Dayong Wang, Yi Huang</li>
<li>for: 这份研究旨在解决癌症病理诊断过程中的时间延迟问题，特别是癌症患者在发展中国家中的诊断过程中的延迟，以提高癌症患者的存活率。</li>
<li>methods: 这份研究使用了深度学习技术，开发了一个基于MobileNetV2的诊断模型，能够实现高精度的诊断和computational efficiency。</li>
<li>results: 根据评估结果，MobileNetV2基本模型在诊断精度、模型普遍性和模型训练效率等方面都超过了VGG16、ResNet50和ResNet101模型。此外，Visual比较表明，MobileNetV2诊断模型能够识别非常小的癌症细胞在大量正常细胞中，实现了人工影像分析的挑战。<details>
<summary>Abstract</summary>
Breast cancer is one of the leading causes of cancer mortality. Breast cancer patients in developing countries, especially sub-Saharan Africa, South Asia, and South America, suffer from the highest mortality rate in the world. One crucial factor contributing to the global disparity in mortality rate is long delay of diagnosis due to a severe shortage of trained pathologists, which consequently has led to a large proportion of late-stage presentation at diagnosis. The delay between the initial development of symptoms and the receipt of a diagnosis could stretch upwards 15 months. To tackle this critical healthcare disparity, this research has developed a deep learning-based diagnosis system for metastatic breast cancer that can achieve high diagnostic accuracy as well as computational efficiency. Based on our evaluation, the MobileNetV2-based diagnostic model outperformed the more complex VGG16, ResNet50 and ResNet101 models in diagnostic accuracy, model generalization, and model training efficiency. The visual comparisons between the model prediction and ground truth have demonstrated that the MobileNetV2 diagnostic models can identify very small cancerous nodes embedded in a large area of normal cells which is challenging for manual image analysis. Equally Important, the light weighted MobleNetV2 models were computationally efficient and ready for mobile devices or devices of low computational power. These advances empower the development of a resource-efficient and high performing AI-based metastatic breast cancer diagnostic system that can adapt to under-resourced healthcare facilities in developing countries. This research provides an innovative technological solution to address the long delays in metastatic breast cancer diagnosis and the consequent disparity in patient survival outcome in developing countries.
</details>
<details>
<summary>摘要</summary>
乳癌是全球最主要的癌症死亡原因之一，特别是在发展中国家，如非洲南部、南亚和南美， breast cancer 患者的死亡率最高。一个重要的因素导致全球的医疗差距是诊断延迟，因为缺乏培训的病理学家，导致许多患者在诊断时 already in 晚期。延迟从症状出现到诊断的时间可以达15个月。为了解决这个严重的医疗差距，这项研究开发了一个基于深度学习的乳癌诊断系统，可以实现高精度和计算效率。根据我们的评估，使用 MobileNetV2 模型的诊断模型在精度、通用性和训练效率三个方面都高于 VGG16、ResNet50 和 ResNet101 模型。视觉比较表明，MobileNetV2 模型可以准确地检测小型患者中的癌细胞，这是人工图像分析困难的。此外，MobileNetV2 模型的计算效率较低，适用于移动设备或低计算能力的设备。这些进步使得可以开发一个资源高效和高性能的人工智能基于乳癌诊断系统，适应发展中国家的医疗设施。这项研究提供了一种创新的科技解决方案，以Address the long delays in metastatic breast cancer diagnosis and the resulting disparity in patient survival outcomes in developing countries.
</details></li>
</ul>
<hr>
<h2 id="VQGraph-Graph-Vector-Quantization-for-Bridging-GNNs-and-MLPs"><a href="#VQGraph-Graph-Vector-Quantization-for-Bridging-GNNs-and-MLPs" class="headerlink" title="VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs"></a>VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02117">http://arxiv.org/abs/2308.02117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangling0818/vqgraph">https://github.com/yangling0818/vqgraph</a></li>
<li>paper_authors: Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec</li>
<li>for: 提高Graph Neural Networks (GNNs)的批处理能力和实时性，以便在具有延迟限制的实际应用中使用。</li>
<li>methods: 采用知识传承（KD）学习计算效率高的多层感知器（MLP），通过模仿GNN的输出来学习GNN的知识。同时，使用一种新的结构意识graph tokenizer，以及一种基于软标签分配的token-based distillation目标，以便充分传递GNN的结构知识到MLP中。</li>
<li>results: 实验和分析表明，VQGraph可以减少GNN的批处理时间，并且在七个图数据集上实现新的状态机器人性表现，包括在推导和泵化设置下的表现。VQGraph可以比GNN更快地进行推理，并且在实际应用中可以提高GNN的准确率。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propose a new token-based distillation objective based on soft token assignments to sufficiently transfer the structural knowledge from GNN to MLP. Extensive experiments and analyses demonstrate the strong performance of VQGraph, where we achieve new state-of-the-art performance on GNN-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 通过消息传递来更新节点表示，这会导致实际延迟应用中的可扩展性问题。为解决这个问题，现有方法采用知识传递（KD）来学习计算效率高的多层感知器（MLP），但是现有GNN表示空间可能不够表示图像下的多样化本地结构，这限制了GNN的知识传递。我们提出了一种新的框架VQGraph，用于学习图像表示空间，以bridging GNNs和MLPs。我们采用变体的vector-quantized variational autoencoder（VQ-VAE）的encoder作为结构意识图像tokenizer，该tokenizer可以明确表示不同本地结构中的节点，并组成一个有意义的代码库。利用学习的代码库，我们提出了一个新的符号分配目标，以便充分传递GNN中的结构知识到MLP。我们在七个图像 dataset 上进行了广泛的实验和分析，并证明了VQGraph的强大表现。我们在transductive和induction Setting中， achieved new state-of-the-art performance on GNN-MLP distillation，并且在GNN和独立MLP上的性能上提高了3.90%和28.05%。此外，我们还证明了VQGraph在GNN上进行更快的推理，比GNN的828倍。代码：https://github.com/YangLing0818/VQGraph。
</details></li>
</ul>
<hr>
<h2 id="Breast-Ultrasound-Tumor-Classification-Using-a-Hybrid-Multitask-CNN-Transformer-Network"><a href="#Breast-Ultrasound-Tumor-Classification-Using-a-Hybrid-Multitask-CNN-Transformer-Network" class="headerlink" title="Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network"></a>Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02101">http://arxiv.org/abs/2308.02101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryar Shareef, Min Xian, Aleksandar Vakanski, Haotian Wang</li>
<li>for: 这个研究旨在提出一个混合多任务深度学习网络（Hybrid-MT-ESTAN），用于肺肿瘤分类和分 segmentation。</li>
<li>methods: 这个方法使用了 CNN 和 Swin Transformer 两种不同的架构，以提高全球背景信息的捕捉和地方图像特征的维持。</li>
<li>results: 实验结果显示，Hybrid-MT-ESTAN 得到了最高的准确率（82.7%）、敏感度（86.4%）和 F1 分数（86.0%）。<details>
<summary>Abstract</summary>
Capturing global contextual information plays a critical role in breast ultrasound (BUS) image classification. Although convolutional neural networks (CNNs) have demonstrated reliable performance in tumor classification, they have inherent limitations for modeling global and long-range dependencies due to the localized nature of convolution operations. Vision Transformers have an improved capability of capturing global contextual information but may distort the local image patterns due to the tokenization operations. In this study, we proposed a hybrid multitask deep neural network called Hybrid-MT-ESTAN, designed to perform BUS tumor classification and segmentation using a hybrid architecture composed of CNNs and Swin Transformer components. The proposed approach was compared to nine BUS classification methods and evaluated using seven quantitative metrics on a dataset of 3,320 BUS images. The results indicate that Hybrid-MT-ESTAN achieved the highest accuracy, sensitivity, and F1 score of 82.7%, 86.4%, and 86.0%, respectively.
</details>
<details>
<summary>摘要</summary>
capture global contextual information 在乳腺超声图像分类中扮演着关键性的角色。尽管 convolutional neural networks (CNNs) 在肿瘤分类中表现出了可靠的性，但它们具有内置的局部化特性，因此可能导致模型长距离和全局依赖关系的模型化困难。 vision transformers 具有改善全局上下文信息捕捉的能力，但可能会因为 tokenization 操作而导致本地图像模式的扭曲。在这项研究中，我们提出了一种 hybrid multitask deep neural network called Hybrid-MT-ESTAN，用于实现乳腺超声图像分类和分割。我们的方法与 nine 种乳腺分类方法进行比较，并在一个包含 3,320 个乳腺超声图像的数据集上进行评估。结果表明，Hybrid-MT-ESTAN 达到了最高的准确率、敏感度和 F1 分数，即 82.7%、86.4% 和 86.0%  соответственно。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Model-Adaptation-for-Continual-Learning-at-the-Edge"><a href="#Efficient-Model-Adaptation-for-Continual-Learning-at-the-Edge" class="headerlink" title="Efficient Model Adaptation for Continual Learning at the Edge"></a>Efficient Model Adaptation for Continual Learning at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02084">http://arxiv.org/abs/2308.02084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, David Zhang</li>
<li>for: 这个研究旨在提供一个非站势自动机器学习（AutoML）框架，以便在资料分布随时变化时进行高效的连续学习。</li>
<li>methods: 这个框架使用固定的深度神经网（DNN）特征嵌入器，并训练浅层网络来处理新数据。它还使用了数维计算（HDC）和零 shot神经架搜索（ZS-NAS）来探测新数据是否为外部数据（OOD），并适当地调整模型以适应OOD数据。</li>
<li>results: 在多个域别数据集上进行评估，这个框架实现了优秀的性能，比如果探测OOD数据和几何shot NAS。<details>
<summary>Abstract</summary>
Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying low-parameter neural adaptors to adapt the model to the OOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizing catastrophic forgetting on previous tasks by progressively growing the neural architecture as needed and dynamically routing data through the appropriate adaptors and reconfigurators for handling domain-incremental and class-incremental continual learning. We systematically evaluate our approach on several benchmark datasets for domain adaptation and demonstrate strong performance compared to state-of-the-art algorithms for OOD detection and few-/zero-shot NAS.
</details>
<details>
<summary>摘要</summary>
大多数机器学习（ML）系统假设训练和部署时数据分布是静止的，这是一个不实际的假设。当 ML 模型在实际设备上部署时，数据分布经常会随着环境因素、传感器特性和任务 интерес而变化。虽然可以有人在Loop监控数据分布的变化并为此设计新的建筑，但这种设置不是可cost-effective的。而是需要不静止的自动机器学习（AutoML）模型。这篇论文提出了Encoder-Adaptor-Reconfigurator（EAR）框架，用于效率地进行适应域shift continual learning。EAR框架使用固定的深度神经网络（DNN）特征编码器，并在编码器之上训练浅层网络来处理新数据。EAR框架可以1）将新数据标记为out-of-distribution（OOD），通过将DNN与高维计算（HDC）结合使用，2）通过零 shot neural architecture search（ZS-NAS）来适应OOD数据，3）在前一个任务上避免忘记性衰变，通过逐渐增加神经建筑和动态路由数据通过适当的适应器和重配置器来处理域增量和类增量 continual learning。我们系统性地评估了我们的方法在域适应数据上的多个benchmark datasets，并demonstrated strong performance compared to state-of-the-art algorithms for OOD detection和few-/zero-shot NAS。
</details></li>
</ul>
<hr>
<h2 id="Target-specification-bias-counterfactual-prediction-and-algorithmic-fairness-in-healthcare"><a href="#Target-specification-bias-counterfactual-prediction-and-algorithmic-fairness-in-healthcare" class="headerlink" title="Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare"></a>Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02081">http://arxiv.org/abs/2308.02081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eran Tal</li>
<li>for: 这篇论文探讨了机器学习（ML）在医疗领域中的偏见问题，并提出了一种更普遍的偏见来源：目标规定偏见。</li>
<li>methods: 这篇论文使用了现有的数据和健康差异的研究，以及现有的机器学习算法和模型。然而，它发现了一种更加普遍的偏见来源：目标规定偏见。</li>
<li>results: 这篇论文发现了target specification bias可能会导致估计准确性过高，使用医疗资源不fficient，并导致伤害病人的决策。<details>
<summary>Abstract</summary>
Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology - the science of measurement - suggests ways of counteracting target specification bias and avoiding its harmful consequences.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在医疗领域中的偏见通常被归结于不完整或不代表性的数据，或者下面的健康差异。本文标识了更广泛的偏见来源，对临床实用性有影响的预测工具：目标规定偏见。目标规定偏见发生在运行化目标变量时与决策者定义的目标之间的匹配不匹配。这种匹配不匹配通常是柔和的，来自于决策者通常关心预测实际医疗情况下的结果，而不是实际情况。这种偏见不受数据限制和健康差异影响，并且不会被纠正。如果不纠正，它会导致预测精度的过高估计，医疗资源的不效利用，以及对病人伤害的不佳决策。近些年的metrology研究（量度科学）提供了对抗目标规定偏见的方法，避免其不良后果。
</details></li>
</ul>
<hr>
<h2 id="Causality-Guided-Disentanglement-for-Cross-Platform-Hate-Speech-Detection"><a href="#Causality-Guided-Disentanglement-for-Cross-Platform-Hate-Speech-Detection" class="headerlink" title="Causality Guided Disentanglement for Cross-Platform Hate Speech Detection"></a>Causality Guided Disentanglement for Cross-Platform Hate Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02080">http://arxiv.org/abs/2308.02080</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paras2612/catch">https://github.com/paras2612/catch</a></li>
<li>paper_authors: Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu</li>
<li>for: 寻找一种可以在多个不同平台上推断仇恨言语的 hate speech 检测模型。</li>
<li>methods: 我们使用了分离输入表示的方法，将输入特征分解成不同平台的特征和共同的特征，以便在不同平台上学习通用的 hate speech 检测模型。我们还学习了 causal 关系，以便更好地理解共同的表示。</li>
<li>results: 我们的模型在四个不同平台上进行了广泛的实验，结果显示我们的模型比现有的状态对方法更高效地检测通用 hate speech。<details>
<summary>Abstract</summary>
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causal relationships, which remain constant across diverse environments, can significantly aid in understanding invariant representations in hate speech. By disentangling input into platform-dependent features (useful for predicting hate targets) and platform-independent features (used to predict the presence of hate), we learn invariant representations resistant to distribution shifts. These features are then used to predict hate speech across unseen platforms. Our extensive experiments across four platforms highlight our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:社交媒体平台，尽管它们在促进开放对话方面具有价值，但它们经常被利用来传播危险内容。现有的深度学习和自然语言处理模型在检测这种危险内容方面过于依赖于域专门的术语，这会导致它们在检测普遍的谩骂言语方面减少其能力。另一个主要挑战是当 платформы缺乏高质量的标注数据 для训练时，导致需要跨平台模型，可以适应不同的分布Shift。我们的研究推出了一种可以在不同的平台上训练的跨平台谩骂言语检测模型。为了实现良好的泛化性 across platforms，我们可以分解输入表示为不变和平台特定的特征。我们还认为，学习不变的关系，可以在多种环境中保持相同的常量，可以大幅提高对不变表示的理解。通过将输入分解为平台特定的特征（有用于预测谩骂目标）和平台独立的特征（用于预测谩骂存在），我们学习了不变的表示，抗性于分布Shift。这些特征然后用于预测谩骂言语 across 未看到的平台。我们的广泛的实验 across four platforms  highlights our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.Translated into Traditional Chinese:社交媒体平台，不过它们在促进开放对话方面具有价值，但它们经常被利用来传播危险内容。现有的深度学习和自然语言处理模型在检测这种危险内容方面过度依赖域专门的术语，这会导致它们在检测普遍的谩驳言语方面减少其能力。另一个主要挑战是当平台缺乏高质量的标注数据 для训练时，导致需要跨平台模型，可以适应不同的分布Shift。我们的研究推出了一种可以在不同的平台上训练的跨平台谩驳言语检测模型。为了实现良好的泛化性 across platforms，我们可以分解输入表示为不变和平台特定的特征。我们还认为，学习不变的关系，可以在多种环境中保持相同的常量，可以大幅提高对不变表示的理解。通过将输入分解为平台特定的特征（有用于预测谩驳目标）和平台独立的特征（用于预测谩驳存在），我们学习了不变的表示，抗性于分布Shift。这些特征然后用于预测谩驳言语 across 未看到的平台。我们的广泛的实验 across four platforms  highlights our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</details></li>
</ul>
<hr>
<h2 id="Specious-Sites-Tracking-the-Spread-and-Sway-of-Spurious-News-Stories-at-Scale"><a href="#Specious-Sites-Tracking-the-Spread-and-Sway-of-Spurious-News-Stories-at-Scale" class="headerlink" title="Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale"></a>Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02068">http://arxiv.org/abs/2308.02068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hans W. A. Hanley, Deepak Kumar, Zakir Durumeric</li>
<li>for: 这篇论文旨在Automatically track and analyze online news narratives to identify misinformation and support fact-checking efforts.</li>
<li>methods: 该系统使用大型自然语言模型MPNet和DP-Means归一 clustering算法，每天抓取1,404家不可靠新闻网站，以分析在线社区中流行的新闻 narative。</li>
<li>results: 研究发现2022年最受欢迎的新闻 narative，并确定了传播这些新闻 narative的最有影响力的网站。系统还可以帮助 fact-checkers like Politifact, Reuters, AP News 更快地识别和推篱虚假新闻。<details>
<summary>Abstract</summary>
Misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. However, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. In this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model MPNet, and DP-Means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. Identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. Finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like Politifact, Reuters, and AP News in more quickly addressing misinformation stories.
</details>
<details>
<summary>摘要</summary>
互联网上充满谣言、宣传和谎言，一些媒体报道有危害公共健康、选举和个人安全的危险。然而，研究社区在 automatization 和 programmatic 方面对新闻媒体的跟踪仍然缺乏有效的方法。在这项工作中，我们利用每天抓取 1,404 个不可靠新闻网站的数据，大型自然语言模型 MPNet，以及 DP-Means 聚类算法，提出一个自动从在线生态系统中分离和分析新闻媒体的系统。我们分析了这些网站上的 55,301 个媒体报道，描述了在 2022 年最具影响力的新闻媒体，以及它们如何促进和强化新闻媒体。最后，我们示出了我们的系统可以帮助ifact-checkers like Politifact、Reuters 和 AP News 更快地处理谣言故事。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Task-Interference-in-Multi-Task-Learning-via-Explicit-Task-Routing-with-Non-Learnable-Primitives"><a href="#Mitigating-Task-Interference-in-Multi-Task-Learning-via-Explicit-Task-Routing-with-Non-Learnable-Primitives" class="headerlink" title="Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives"></a>Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02066">http://arxiv.org/abs/2308.02066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhichao-lu/etr-nlp-mtl">https://github.com/zhichao-lu/etr-nlp-mtl</a></li>
<li>paper_authors: Chuntao Ding, Zhichao Lu, Shangguang Wang, Ran Cheng, Vishnu Naresh Boddeti</li>
<li>for: 这个论文目的是提出一种基于非学习 primitives 和显式任务路由（ETR）的多任务学习（MTL）方法，以降低任务干扰。</li>
<li>methods: 该方法使用非学习 primitives 提取多个任务共同的特征，并将这些特征重新组合到共同分支和每个任务专门的分支中。它还使用显式任务路由来隔离学习参数，以便降低任务干扰。</li>
<li>results: 实验结果表明，ETR-NLP 在图像水平分类和像素粒度稠密预测多任务学习问题中具有显著优势，比基eline模型更高的性能，同时具有更少的学习参数和相似的计算量。代码可以在这里下载：<a target="_blank" rel="noopener" href="https://github.com/zhichao-lu/etr-nlp-mtl%E3%80%82">https://github.com/zhichao-lu/etr-nlp-mtl。</a><details>
<summary>Abstract</summary>
Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this \href{https://github.com/zhichao-lu/etr-nlp-mtl}.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）目的是学习一个模型来完成多个任务，利用任务之间的共享信息。现有的 MTL 模型却存在任务干扰的问题。减轻任务干扰的努力主要集中在损失/梯度均衡或隐式参数分割中，其中一些任务参数与其他任务参数之间存在部分重叠。在这篇论文中，我们提出了ETR-NLP，一种通过非学习性 primitives（NLPs）和显式任务路由（ETR）来减轻任务干扰的方法。我们的关键想法是使用非学习性 primitives 提取一组多样化的任务不受限制的特征，然后将其重新组合到一个共享的分支和每个任务的显式分支中。非学习性 primitives 和显式划分学习参数为共享和任务特定的一些允许我们适应性的灵活性，以最小化任务干扰。我们在图像级别的分类和像素级别的整合预测MTL问题中评估了ETR-NLP网络的效果。实验结果表明，ETR-NLP在所有数据集上都超越了当前的基eline，减少了学习参数数量和相同的FLOPs。代码可以在这里找到：https://github.com/zhichao-lu/etr-nlp-mtl。
</details></li>
</ul>
<hr>
<h2 id="On-the-Biometric-Capacity-of-Generative-Face-Models"><a href="#On-the-Biometric-Capacity-of-Generative-Face-Models" class="headerlink" title="On the Biometric Capacity of Generative Face Models"></a>On the Biometric Capacity of Generative Face Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02065">http://arxiv.org/abs/2308.02065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/human-analysis/capacity-generative-face-models">https://github.com/human-analysis/capacity-generative-face-models</a></li>
<li>paper_authors: Vishnu Naresh Boddeti, Gautam Sreekumar, Arun Ross</li>
<li>for: 本研究的目的是为了评估和比较不同的生成人脸模型，以及确定这些模型的扩展性上的最高限制。</li>
<li>methods: 本研究使用了一种统计方法来估算生成人脸图像在幂体特征空间中的生物学容量。</li>
<li>results: 研究发现，使用 ArcFace 表示法，在 false acceptance rate (FAR) 为 0.1% 时，StyleGAN3 和 DCFace 的生物学容量的最高限制分别为 $1.43\times10^6$ 和 $1.190\times10^4$。此外，随着 Desired FAR 的下降，生物学容量的估算值也降低了许多。 gender 和 age 的影响也被研究发现。<details>
<summary>Abstract</summary>
There has been tremendous progress in generating realistic faces with high fidelity over the past few years. Despite this progress, a crucial question remains unanswered: "Given a generative face model, how many unique identities can it generate?" In other words, what is the biometric capacity of the generative face model? A scientific basis for answering this question will benefit evaluating and comparing different generative face models and establish an upper bound on their scalability. This paper proposes a statistical approach to estimate the biometric capacity of generated face images in a hyperspherical feature space. We employ our approach on multiple generative models, including unconditional generators like StyleGAN, Latent Diffusion Model, and "Generated Photos," as well as DCFace, a class-conditional generator. We also estimate capacity w.r.t. demographic attributes such as gender and age. Our capacity estimates indicate that (a) under ArcFace representation at a false acceptance rate (FAR) of 0.1%, StyleGAN3 and DCFace have a capacity upper bound of $1.43\times10^6$ and $1.190\times10^4$, respectively; (b) the capacity reduces drastically as we lower the desired FAR with an estimate of $1.796\times10^4$ and $562$ at FAR of 1% and 10%, respectively, for StyleGAN3; (c) there is no discernible disparity in the capacity w.r.t gender; and (d) for some generative models, there is an appreciable disparity in the capacity w.r.t age. Code is available at https://github.com/human-analysis/capacity-generative-face-models.
</details>
<details>
<summary>摘要</summary>
在过去几年里，生成真实的脸部图像的进步很大。尽管如此，一个关键的问题仍然未得到答案：“给定一个生成脸部模型，它可以生成多少个唯一的标识？”或者说，生成脸部模型的生物 metric capacity 是多少？一个科学基础来回答这个问题将有助于评估和比较不同的生成脸部模型，并设置生成脸部模型的可扩展性的上限。本文提出了一种统计方法来估算生成脸部图像的生物 metric 容量，我们使用这种方法对多个生成模型进行了测试，包括 StyleGAN、Latent Diffusion Model 和 "Generated Photos" 等模型，以及 DCFace 等类别 conditional 生成模型。我们还估算了基于人口特征（如性别和年龄）的容量。我们的容量估算表明：（a）在 ArcFace 表示下，False Acceptance Rate (FAR) 为 0.1% 时，StyleGAN3 和 DCFace 的容量Upper Bound 分别为 $1.43\times10^6$ 和 $1.190\times10^4$；（b）随着 Desired FAR 降低，容量减少了极其剧烈， ArcFace 表示下，FAR 为 1% 和 10% 时，StyleGAN3 的容量估算为 $1.796\times10^4$ 和 $562$；（c）对于一些生成模型， gender 不存在显著的差异；（d）对于一些生成模型， age 存在可观的差异。相关代码可以在 GitHub 上找到：https://github.com/human-analysis/capacity-generative-face-models。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Neural-Network-Pruning-Requires-Rethinking-Sparse-Optimization"><a href="#Accurate-Neural-Network-Pruning-Requires-Rethinking-Sparse-Optimization" class="headerlink" title="Accurate Neural Network Pruning Requires Rethinking Sparse Optimization"></a>Accurate Neural Network Pruning Requires Rethinking Sparse Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02060">http://arxiv.org/abs/2308.02060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Kuznedelev, Eldar Kurtic, Eugenia Iofinova, Elias Frantar, Alexandra Peste, Dan Alistarh</li>
<li>for: 这个论文的目的是研究在使用标准的随机优化技术进行训练稀皮网络时，稀皮性如何影响模型训练。</li>
<li>methods: 作者使用了标准的计算机视觉和自然语言处理稀皮benchmark进行研究，并提供了新的方法来 Mitigate the issue of under-training in sparse training。</li>
<li>results: 研究发现，使用标准粗糙训练策略进行稀皮训练是不优化的，而使用新提出的方法可以在计算机视觉和自然语言处理领域中实现高精度和高稀皮性的模型训练。<details>
<summary>Abstract</summary>
Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art results in both settings in the high-sparsity regime, and providing detailed analyses for the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity, and should inspire further research into improving sparse model training, to reach higher accuracies under high sparsity, but also to do so efficiently.
</details>
<details>
<summary>摘要</summary>
In this work, we examine the impact of high sparsity on model training using standard computer vision and natural language processing sparsity benchmarks. We show that using standard dense training recipes for sparse training is suboptimal and results in under-training. We propose new approaches to mitigate this issue for both sparse pre-training of vision models (e.g., ResNet50/ImageNet) and sparse fine-tuning of language models (e.g., BERT/GLUE). Our approaches achieve state-of-the-art results in both settings in the high-sparsity regime and provide detailed analyses of the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity and should inspire further research into improving sparse model training to reach higher accuracies under high sparsity efficiently.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Recklessness-to-Collaborative-Filtering-based-Recommender-Systems"><a href="#Incorporating-Recklessness-to-Collaborative-Filtering-based-Recommender-Systems" class="headerlink" title="Incorporating Recklessness to Collaborative Filtering based Recommender Systems"></a>Incorporating Recklessness to Collaborative Filtering based Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02058">http://arxiv.org/abs/2308.02058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knodis-research-group/recklessness-regularization">https://github.com/knodis-research-group/recklessness-regularization</a></li>
<li>paper_authors: Diego Pérez-López, Fernando Ortega, Ángel González-Prieto, Jorge Dueñas-Lerín</li>
<li>for: 提高爆料系统的决策可靠性和创新性</li>
<li>methods: 引入一个新的学习过程中的recklessness项，用于控制决策时的风险水平</li>
<li>results: 实验结果表明，recklessness不仅能够进行风险规避，还可以提高爆料系统提供的预测量和质量。<details>
<summary>Abstract</summary>
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
</details>
<details>
<summary>摘要</summary>
建议系统，包括一些可靠度度量，往往会变得更加保守，因为它们需要保持可靠度。这会导致建议系统的覆盖率和新颖性下降。在这篇论文中，我们提议在矩阵分解基础的建议系统学习过程中添加一个新的参数，即不可靠度，以控制决策时的风险水平。实验结果表明，不可靠度不仅允许风险调节，还可以提高建议系统提供的预测量和质量。Note: "recklessness" is a term used in the original text, and it is not a word commonly used in Chinese. I translated it as "不可靠度" (bù kě yào dù), which means "unreliability" or "riskiness".
</details></li>
</ul>
<hr>
<h2 id="Seasonality-Based-Reranking-of-E-commerce-Autocomplete-Using-Natural-Language-Queries"><a href="#Seasonality-Based-Reranking-of-E-commerce-Autocomplete-Using-Natural-Language-Queries" class="headerlink" title="Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries"></a>Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02055">http://arxiv.org/abs/2308.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Verma, Shan Zhong, Xiaoyu Liu, Adithya Rajan</li>
<li>for: 提高搜索引擎的搜寻框架中的自动完成功能，使其能够适应季节性变化。</li>
<li>methods: 使用神经网络基本概念的自然语言处理算法，将季节性变化纳入搜寻框架中。</li>
<li>results: 提出一个终端评估模型，可以将季节性变化纳入搜寻框架中，提高自动完成的相关性和商业指标。<details>
<summary>Abstract</summary>
Query autocomplete (QAC) also known as typeahead, suggests list of complete queries as user types prefix in the search box. It is one of the key features of modern search engines specially in e-commerce. One of the goals of typeahead is to suggest relevant queries to users which are seasonally important. In this paper we propose a neural network based natural language processing (NLP) algorithm to incorporate seasonality as a signal and present end to end evaluation of the QAC ranking model. Incorporating seasonality into autocomplete ranking model can improve autocomplete relevance and business metric.
</details>
<details>
<summary>摘要</summary>
查询自动完成（QAC）也称为键盘提示，是现代搜索引擎中的一个重要功能，尤其在电商领域。QAC的一个目标是为用户提供相关的查询，以便在搜索框中输入搜索。在这篇论文中，我们提出了基于人工神经网络的自然语言处理（NLP）算法，以 incorporate 季节性作为信号，并进行了端到端评估QAC排名模型。在推入季节性到搜索框中的排名模型中，可以提高搜索结果的相关性和业务指标。
</details></li>
</ul>
<hr>
<h2 id="Robust-Independence-Tests-with-Finite-Sample-Guarantees-for-Synchronous-Stochastic-Linear-Systems"><a href="#Robust-Independence-Tests-with-Finite-Sample-Guarantees-for-Synchronous-Stochastic-Linear-Systems" class="headerlink" title="Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems"></a>Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02054">http://arxiv.org/abs/2308.02054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ambrus Tamás, Dániel Ágoston Bálint, Balázs Csanád Csáji</li>
<li>for: 这个论文是为了开发一种robust independence测试方法，可以 guaranteesignificance levels不受偏移的影响。</li>
<li>methods: 这个方法使用了信任区间估计和 permutation tests，以及一些总体依赖度测量方法，如希尔伯特-Ш密特独立性标准和距离协方差。</li>
<li>results: 这个方法可以检测非线性依赖关系，并且可以在各种不同的噪声模型下进行测试。我们还证明了这个假设测试方法的一致性下一些轻微的假设。<details>
<summary>Abstract</summary>
The paper introduces robust independence tests with non-asymptotically guaranteed significance levels for stochastic linear time-invariant systems, assuming that the observed outputs are synchronous, which means that the systems are driven by jointly i.i.d. noises. Our method provides bounds for the type I error probabilities that are distribution-free, i.e., the innovations can have arbitrary distributions. The algorithm combines confidence region estimates with permutation tests and general dependence measures, such as the Hilbert-Schmidt independence criterion and the distance covariance, to detect any nonlinear dependence between the observed systems. We also prove the consistency of our hypothesis tests under mild assumptions and demonstrate the ideas through the example of autoregressive systems.
</details>
<details>
<summary>摘要</summary>
文章介绍了一种robust独立性测试方法，可以 garantuee非对称性水平，对于随机线性时间不变系统。我们假设观测输出是同步的，即系统被共同的随机噪声驱动。我们的方法提供了不对归一化的类型I错误概率 bound，即噪声可以有任何分布。我们的算法结合信任区间估计与排序测试，以及通用的依赖度度量，如希尔伯特-尚瑟独立性 критерион和距离协方差，来检测观测系统中的非线性依赖关系。我们还证明了我们的假设检测下的假设是正确的，并通过拓扑系统的示例进行了证明。
</details></li>
</ul>
<hr>
<h2 id="A-Graphical-Approach-to-Document-Layout-Analysis"><a href="#A-Graphical-Approach-to-Document-Layout-Analysis" class="headerlink" title="A Graphical Approach to Document Layout Analysis"></a>A Graphical Approach to Document Layout Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02051">http://arxiv.org/abs/2308.02051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jilin Wang, Michael Krumdick, Baojia Tong, Hamima Halim, Maxim Sokolov, Vadym Barda, Delphine Vendryes, Chris Tanner</li>
<li>for: This paper focuses on document layout analysis (DLA) and proposes a lightweight graph neural network called GLAM to improve the efficiency of DLA models.</li>
<li>methods: The GLAM model represents each PDF page as a structured graph and frames the DLA problem as a graph segmentation and classification problem.</li>
<li>results: The GLAM model achieves competitive performance with state-of-the-art (SOTA) models on two challenging DLA datasets, with an order of magnitude fewer parameters. A simple ensemble of GLAM and a leading computer vision-based model achieves a new state-of-the-art on DocLayNet, with an increase in mean average precision (mAP) from 76.8 to 80.8.<details>
<summary>Abstract</summary>
Document layout analysis (DLA) is the task of detecting the distinct, semantic content within a document and correctly classifying these items into an appropriate category (e.g., text, title, figure). DLA pipelines enable users to convert documents into structured machine-readable formats that can then be used for many useful downstream tasks. Most existing state-of-the-art (SOTA) DLA models represent documents as images, discarding the rich metadata available in electronically generated PDFs. Directly leveraging this metadata, we represent each PDF page as a structured graph and frame the DLA problem as a graph segmentation and classification problem. We introduce the Graph-based Layout Analysis Model (GLAM), a lightweight graph neural network competitive with SOTA models on two challenging DLA datasets - while being an order of magnitude smaller than existing models. In particular, the 4-million parameter GLAM model outperforms the leading 140M+ parameter computer vision-based model on 5 of the 11 classes on the DocLayNet dataset. A simple ensemble of these two models achieves a new state-of-the-art on DocLayNet, increasing mAP from 76.8 to 80.8. Overall, GLAM is over 5 times more efficient than SOTA models, making GLAM a favorable engineering choice for DLA tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SMARLA-A-Safety-Monitoring-Approach-for-Deep-Reinforcement-Learning-Agents"><a href="#SMARLA-A-Safety-Monitoring-Approach-for-Deep-Reinforcement-Learning-Agents" class="headerlink" title="SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents"></a>SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02594">http://arxiv.org/abs/2308.02594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Zolfagharian, Manel Abdellatif, Lionel C. Briand, Ramesh S</li>
<li>for: 这篇论文旨在提出一种基于机器学习的安全监测方法，用于保障深度优化学习（DRL）Agent的安全性。</li>
<li>methods: 该方法基于黑盒（不需要访问代理的内部），利用状态抽象减少状态空间，从而使得学习安全违反预测模型的可能性更高。</li>
<li>results: 验证结果表明，SMARLA可以准确预测安全违反，false positive率低，可以在代理执行前半部分预测安全违反。<details>
<summary>Abstract</summary>
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before violations occur.
</details>
<details>
<summary>摘要</summary>
深度强化学习算法（DRL）在安全关键系统中日益被使用。保证DRL代理的安全是这些上下文中的关键问题。然而，仅仅通过测试不能保证安全，因为它不提供保证。建立安全监控器是一个解决方案，以降低这个挑战。这篇论文提出了基于机器学习的安全监控方法SMARLA，专门为DRL代理设计。由于实际原因，SMARLA采用黑盒设计（不需要代理的内部访问权限），并利用状态抽象来减少状态空间，从而使得学习代理违规预测模型从代理的状态中更加容易。我们对两个常见RL案例进行了验证。实验分析表明，SMARLA可以准确预测违规行为，false positive率较低，能够在代理执行前一半预测违规行为。
</details></li>
</ul>
<hr>
<h2 id="FuNToM-Functional-Modeling-of-RF-Circuits-Using-a-Neural-Network-Assisted-Two-Port-Analysis-Method"><a href="#FuNToM-Functional-Modeling-of-RF-Circuits-Using-a-Neural-Network-Assisted-Two-Port-Analysis-Method" class="headerlink" title="FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method"></a>FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02050">http://arxiv.org/abs/2308.02050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morteza Fayazi, Morteza Tavakoli Taba, Amirata Tabatabavakili, Ehsan Afshari, Ronald Dreslinski<br>for:这个论文主要目的是提出一种功能模型化方法，以提高RLC电路的自动化设计效率。methods:该方法使用人工智能技术，并利用两个Port分析方法，可以模型多种架构，并且仅需要一个主要数据集和多个小数据集。results:该方法可以与现有方法匹配精度，但需要训练数据的数量则被降低了2.8倍至10.9倍，并且在后期设计阶段的训练集收集时间则被降低了176.8倍至188.6倍。<details>
<summary>Abstract</summary>
Automatic synthesis of analog and Radio Frequency (RF) circuits is a trending approach that requires an efficient circuit modeling method. This is due to the expensive cost of running a large number of simulations at each synthesis cycle. Artificial intelligence methods are promising approaches for circuit modeling due to their speed and relative accuracy. However, existing approaches require a large amount of training data, which is still collected using simulation runs. In addition, such approaches collect a whole separate dataset for each circuit topology even if a single element is added or removed. These matters are only exacerbated by the need for post-layout modeling simulations, which take even longer. To alleviate these drawbacks, in this paper, we present FuNToM, a functional modeling method for RF circuits. FuNToM leverages the two-port analysis method for modeling multiple topologies using a single main dataset and multiple small datasets. It also leverages neural networks which have shown promising results in predicting the behavior of circuits. Our results show that for multiple RF circuits, in comparison to the state-of-the-art works, while maintaining the same accuracy, the required training data is reduced by 2.8x - 10.9x. In addition, FuNToM needs 176.8x - 188.6x less time for collecting the training set in post-layout modeling.
</details>
<details>
<summary>摘要</summary>
《自动化分析和设计 analog和 radio frequency（RF）电路的方法是一种流行的趋势，因为在每一个合理化周期中运行大量的 simulate 实际上是昂贵的。人工智能方法是电路模型的承诺之一，因为它们具有速度和相对准确性。然而，现有的方法需要大量的训练数据，这些数据通常通过 simulate 实际来采集。此外，这些方法每个电路结构都需要采集一个分开的数据集，即使只是添加或删除一个元素。这些问题由 Layout 模拟所加剧，它们需要更长的时间。为了解决这些问题，我们在这篇论文中提出了 FuNToM，一种功能模型方法 для RF 电路。FuNToM 利用了两个端口分析方法，可以模型多种 topology 使用单个主数据集和多个小数据集。它还利用了人工神经网络，这些神经网络在预测电路行为方面表现出色。我们的结果表明，对多个 RF 电路，相比之前的状态艺术作品，在保持同样的准确性下，需要的训练数据被减少了 2.8x - 10.9x。此外，FuNToM 在 post-layout 模拟中收集训练集的时间需要 176.8x - 188.6x  menos。
</details></li>
</ul>
<hr>
<h2 id="Deep-Maxout-Network-based-Feature-Fusion-and-Political-Tangent-Search-Optimizer-enabled-Transfer-Learning-for-Thalassemia-Detection"><a href="#Deep-Maxout-Network-based-Feature-Fusion-and-Political-Tangent-Search-Optimizer-enabled-Transfer-Learning-for-Thalassemia-Detection" class="headerlink" title="Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection"></a>Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02029">http://arxiv.org/abs/2308.02029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hemn Barzan Abdalla, Awder Ahmed, Guoquan Li, Nasser Mustafa, Abdur Rashid Sangi</li>
<li>for: 本研究旨在探讨一种基于政治向量搜索优化的深度学习方法（PTSO_TL）用于抑制遗传贫血病诊断。</li>
<li>methods: 本研究使用的方法包括数据normalization、特征融合、数据增强和深度学习模型。</li>
<li>results: 根据实验结果，PTSO_TL方法在识别遗传贫血病方面达到了最高的精度（94.3%）、回归率（96.1%）和相关度（95.2%）。<details>
<summary>Abstract</summary>
Thalassemia is a heritable blood disorder which is the outcome of a genetic defect causing lack of production of hemoglobin polypeptide chains. However, there is less understanding of the precise frequency as well as sharing in these areas. Knowing about the frequency of thalassemia occurrence and dependable mutations is thus a significant step in preventing, controlling, and treatment planning. Here, Political Tangent Search Optimizer based Transfer Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input data obtained from a particular dataset is normalized in the data normalization stage. Quantile normalization is utilized in the data normalization stage, and the data are then passed to the feature fusion phase, in which Weighted Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data augmentation is performed using the oversampling method to increase data dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a convolutional neural network (CNN) is utilized with hyperparameters from a trained model such as Xception. TL is tuned by PTSO, and the training algorithm PTSO is presented by merging of Political Optimizer (PO) and Tangent Search Algorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and f-measure values of about 94.3%, 96.1%, and 95.2%, respectively.
</details>
<details>
<summary>摘要</summary>
贝壳血症是一种遗传血液疾病，由于遗传错误导致血液中不够生成含铁蛋白链。然而，贝壳血症的具体发生频率以及传递的精准性仍未得到充分理解。了解贝壳血症发生频率和可靠的突变是一项重要的步骤，以便预防、控制和治疗规划。在这里，我们引入政治弧搜索优化器基于传输学习（PTSO_TL）以检测贝壳血症。首先，输入数据从特定数据集被normalized，并使用量谱normalization进行数据归一化。然后，数据被传递到特征融合阶段，在这里使用Weighted Euclidean Distance with Deep Maxout Network（DMN）。接着，数据进行了增强处理，使用扩充方法增加数据维度。最后，贝壳血症检测由TL进行，其中使用一个具有训练模型的 convolutional neural network（CNN），并将 hyperparameters 从已训练模型 such as Xception。TL 被PTSO 调整，并且PTSO 是由政治优化器（PO）和 Tangent Search Algorithm（TSA）的 merge 所presentation。此外，PTSO_TL 在评价指标中获得了最高的准确率、回归率和准确度值，它们分别为 approximately 94.3%, 96.1%, and 95.2%。
</details></li>
</ul>
<hr>
<h2 id="Federated-Representation-Learning-for-Automatic-Speech-Recognition"><a href="#Federated-Representation-Learning-for-Automatic-Speech-Recognition" class="headerlink" title="Federated Representation Learning for Automatic Speech Recognition"></a>Federated Representation Learning for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02013">http://arxiv.org/abs/2308.02013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guruprasad V Ramesh, Gopinath Chennupati, Milind Rao, Anit Kumar Sahu, Ariya Rastrow, Jasha Droppo</li>
<li>for: 这篇论文是为了探讨 Federated Learning (FL) 和 Self-supervised Learning (SSL) 的结合，以学习 Automatic Speech Recognition (ASR) 模型，保持数据隐私。</li>
<li>methods: 这篇论文使用了 Libri-Light 语音 dataset，使用了 Speaker 和 Chapter 信息来模拟非Identical Independent Distributions (non-IID) 的数据分布，采用了 Contrastive Predictive Coding 框架和 FedSGD 进行训练。</li>
<li>results: 研究发现，使用 Federated Learning 预训练 ASR 模型，可以达到中心预训练模型的性能水平，并且在新语言 French 中进行适应，可以提高 WER 表达误差率 by 20%。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种隐私保护的方法论，允许边缘设备共同学习无需分享数据。边缘设备如 Alexa 和 Siri 是可能的无标语音数据的来源，可以用于学习 Robust 语音表示。在这项工作中，我们将 Self-supervised Learning (SSL) 和 FL 结合来学习 Automatic Speech Recognition (ASR) 的表示，尊重数据隐私约束。我们使用 Libri-Light 无标语音集中的 Speaker 和章节信息来模拟非Identical Independent Distribution (IID) 的Speaker-siloed 数据分布，并在 FedSGD 框架下预训练一个 LSTM 编码器。我们显示预训练的 ASR 编码器在 FL 中表现与中央预训练模型一样好，并且对无预训练情况下提高了12-15% (WER)。我们进一步适应了联邦预训练模型到一种新语言法语，并显示对无预训练情况下提高了20% (WER)。
</details></li>
</ul>
<hr>
<h2 id="Memory-capacity-of-two-layer-neural-networks-with-smooth-activations"><a href="#Memory-capacity-of-two-layer-neural-networks-with-smooth-activations" class="headerlink" title="Memory capacity of two layer neural networks with smooth activations"></a>Memory capacity of two layer neural networks with smooth activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02001">http://arxiv.org/abs/2308.02001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Madden, Christos Thrampoulidis</li>
<li>for: 这篇论文探讨了两层神经网络的内存容量，即最大化一般数据集的网络大小。</li>
<li>methods: 作者使用了非多项式实数Activation函数，如sigmoid和smoothed ReLU，并使用Jacobian的秩来分析网络的内存容量。</li>
<li>results: 作者发现，对于非多项式实数Activation函数，网络的内存容量至少为md&#x2F;2，并且可以达到约2倍的优化。这些结果比前一些研究更加广泛，并且可以推广到更深的模型和其他架构。<details>
<summary>Abstract</summary>
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.
</details>
<details>
<summary>摘要</summary>
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters) is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.Here's the translation in Traditional Chinese:决定两层神经网络中隐藏层 neuron 数目为 m，输入维度为 d（即 md+m 总可训练参数）的记忆容量是机器学习中的基本问题。对于非多项演算 activation functions，例如 sigmoid 和 smoothed rectified linear units (smoothed ReLUs)，我们设置了 md/2 的下界和约2的优化因子。这些结果与 preceded 的 results 相似，但是过去的结果仅适用于 Heaviside 和 ReLU 激活函数，而且这些激活函数的结果受到了 logarithmic 因子的影响，并且需要随机数据。从构成记忆容量的角度来看，我们查看了神经网络的雅可比安的排名，通过计算包含 Hadamard powers 和 Khati-Rao 产品的矩阵的排名。我们的计算扩展了 класиical 的线性代数实验，关于 Hadamard powers 的排名。整体而言，我们的方法与之前的工作不同，并且保持可以扩展到更深的模型和其他架构。
</details></li>
</ul>
<hr>
<h2 id="On-the-Transition-from-Neural-Representation-to-Symbolic-Knowledge"><a href="#On-the-Transition-from-Neural-Representation-to-Symbolic-Knowledge" class="headerlink" title="On the Transition from Neural Representation to Symbolic Knowledge"></a>On the Transition from Neural Representation to Symbolic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02000">http://arxiv.org/abs/2308.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyan Cheng, Peter Chin</li>
<li>for: 本研究旨在bridge neural和Symbolic Representation之间的巨大差距，以便将Symbolic Thinking incorporated into neural networks的核心。</li>
<li>methods: 我们提出了一个Neural-Symbolic Transitional Dictionary Learning（TDL）框架，使用EM算法学习数据的转换表示，压缩输入数据的高维信息到一组tensor作为神经变量，自然地发现数据中隐藏的 predicate 结构。我们在 diffusion model 中对输入的分解视为合作游戏，然后通过prototype clustering来学习预测。此外，我们还使用RLEnabled by diffusion models来进一步调整学习的got prototype。</li>
<li>results: 我们在3个抽象compositional visual objects dataset上进行了广泛的实验，这些dataset需要模型可以对输入进行部分 segmentation，不含任何视觉特征，例如 texture、颜色或阴影。我们的learned representation可以带来可 interpret的 decompositions of visual input，并且在下游任务中进行了smooth的适应。这些下游任务包括神经&#x2F;Symbolic downstream tasks。<details>
<summary>Abstract</summary>
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.
</details>
<details>
<summary>摘要</summary>
bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. we propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. we implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. we additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.Here's a word-for-word translation of the text into Simplified Chinese:bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. we propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. we implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. we additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.
</details></li>
</ul>
<hr>
<h2 id="Explainable-unsupervised-multi-modal-image-registration-using-deep-networks"><a href="#Explainable-unsupervised-multi-modal-image-registration-using-deep-networks" class="headerlink" title="Explainable unsupervised multi-modal image registration using deep networks"></a>Explainable unsupervised multi-modal image registration using deep networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01994">http://arxiv.org/abs/2308.01994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjia Wang, Giorgos Papanastasiou</li>
<li>for: 这个论文是用于描述一种基于深度学习的多模态MRI图像匹配方法，用于临床决策。</li>
<li>methods: 该方法使用了多种MRI序列（定义为’模态’），并使用了 Grad-CAM基于解释框架来解释模型和数据之间的关系。</li>
<li>results: 该研究表明，通过 incorporating Grad-CAM解释框架，该方法可以实现高性能和可解释的多模态MRI图像匹配。<details>
<summary>Abstract</summary>
Clinical decision making from magnetic resonance imaging (MRI) combines complementary information from multiple MRI sequences (defined as 'modalities'). MRI image registration aims to geometrically 'pair' diagnoses from different modalities, time points and slices. Both intra- and inter-modality MRI registration are essential components in clinical MRI settings. Further, an MRI image processing pipeline that can address both afine and non-rigid registration is critical, as both types of deformations may be occuring in real MRI data scenarios. Unlike image classification, explainability is not commonly addressed in image registration deep learning (DL) methods, as it is challenging to interpet model-data behaviours against transformation fields. To properly address this, we incorporate Grad-CAM-based explainability frameworks in each major component of our unsupervised multi-modal and multi-organ image registration DL methodology. We previously demonstrated that we were able to reach superior performance (against the current standard Syn method). In this work, we show that our DL model becomes fully explainable, setting the framework to generalise our approach on further medical imaging data.
</details>
<details>
<summary>摘要</summary>
临床决策从核磁共振成像（MRI）结合多种MRI序列（定义为“模态”）的信息。MRI图像匹配目标是在不同模态、时间点和切片之间进行几何匹配诊断。Intra-和inter-模态MRI匹配都是临床MRI设置中的重要组件。此外，一个能够处理both afine和non-rigid匹配的MRI图像处理管道是关键，因为这两种类型的变形都可能发生在实际MRI数据场景中。不同于图像分类，explainability不是通常在图像匹配深度学习（DL）方法中被考虑的，因为它是困难 interpret模型-数据行为对于转换场景。为了正确地Address这个问题，我们在每个主要组件中都 incorporate Grad-CAM基于的解释框架。在我们之前的研究中，我们已经能够达到superior performance（相比于当前标准Syn方法）。在这项工作中，我们显示了我们的DL模型已经变得完全可解释，设置了框架可以通过更多的医疗影像数据进行普适化。
</details></li>
</ul>
<hr>
<h2 id="CartiMorph-a-framework-for-automated-knee-articular-cartilage-morphometrics"><a href="#CartiMorph-a-framework-for-automated-knee-articular-cartilage-morphometrics" class="headerlink" title="CartiMorph: a framework for automated knee articular cartilage morphometrics"></a>CartiMorph: a framework for automated knee articular cartilage morphometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01981">http://arxiv.org/abs/2308.01981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongchengyao/cartimorph">https://github.com/yongchengyao/cartimorph</a></li>
<li>paper_authors: Yongcheng Yao, Junru Zhong, Liping Zhang, Sheheryar Khan, Weitian Chen</li>
<li>for: 这个研究的目的是发展一个准确地量化膝盖韧带组织的自动化方法，以便发现膝盖韧带组织的问题。</li>
<li>methods: 这个研究使用了深度学习模型来表现图像特征，并使用了标本建立和图像注册等方法来自动化膝盖韧带组织的量化。</li>
<li>results: 这个研究获得了膝盖韧带组织的量化结果，包括全厚度膝盖韧带损伤率（FCL）、平均厚度、表面积和体积等多个量化指标。这些量化结果显示了膝盖韧带组织的问题，并且与手动量化结果之间存在强相关。<details>
<summary>Abstract</summary>
We introduce CartiMorph, a framework for automated knee articular cartilage morphometrics. It takes an image as input and generates quantitative metrics for cartilage subregions, including the percentage of full-thickness cartilage loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the power of deep learning models for hierarchical image feature representation. Deep learning models were trained and validated for tissue segmentation, template construction, and template-to-image registration. We established methods for surface-normal-based cartilage thickness mapping, FCL estimation, and rule-based cartilage parcellation. Our cartilage thickness map showed less error in thin and peripheral regions. We evaluated the effectiveness of the adopted segmentation model by comparing the quantitative metrics obtained from model segmentation and those from manual segmentation. The root-mean-squared deviation of the FCL measurements was less than 8%, and strong correlations were observed for the mean thickness (Pearson's correlation coefficient $\rho \in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in [0.89,0.98]$) measurements. We compared our FCL measurements with those from a previous study and found that our measurements deviated less from the ground truths. We observed superior performance of the proposed rule-based cartilage parcellation method compared with the atlas-based approach. CartiMorph has the potential to promote imaging biomarkers discovery for knee osteoarthritis.
</details>
<details>
<summary>摘要</summary>
我们介绍CartiMorph，一个框架用于自动诊断膝关节软骨质量量表。它可以从图像中提取量表膝关节软骨质量量表，包括软骨质量量表的全厚度损伤率（FCL）、平均厚度、表面积和体积。CartiMorph利用深度学习模型来实现层次图像特征表示。我们在识别、构建模板和模板与图像匹配中使用深度学习模型。我们实现了基于表面法向的软骨厚度映射、FCL估计和规则基于的软骨分割。我们的软骨厚度图表示在薄和边缘区域中具有较低的错误。我们通过比较我们采用的分 segmentation模型与手动分 segmentation结果所得到的量表metric来评估模型的效果。我们发现root-mean-squared deviation of FCL measurements是less than 8%，并且在mean thickness、surface area和volume measurement中observation了强相关性（Pearson's correlation coefficient $\rho \in [0.82,0.97]$、[0.82,0.98]$和[0.89,0.98]$）。我们对我们的FCL测量与之前的研究中的参照值进行比较，发现我们的测量偏差较少。我们发现了规则基于的软骨分割方法的优越性，比Atlas-based方法更好。CartiMorph具有推动膝关节风湿病影像生物标志物的潜力。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Parkinson’s-Disease-with-Smile-An-AI-enabled-Screening-Framework"><a href="#Unmasking-Parkinson’s-Disease-with-Smile-An-AI-enabled-Screening-Framework" class="headerlink" title="Unmasking Parkinson’s Disease with Smile: An AI-enabled Screening Framework"></a>Unmasking Parkinson’s Disease with Smile: An AI-enabled Screening Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02588">http://arxiv.org/abs/2308.02588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Adnan, Md Saiful Islam, Wasifur Rahman, Sangwu Lee, Sutapa Dey Tithi, Kazi Noshin, Imran Sarker, M Saifur Rahman, Ehsan Hoque</li>
<li>for: 预测帕金森病（PD）的诊断具有挑战性，因为没有可靠的生物标志物和有限的临床护理资源。本研究通过分析最大的视频数据集，检测PD的微表情。</li>
<li>methods: 我们使用了人脸特征点和动作单元，提取与低表情相关的特征。我们将这些特征用于一个 ensemble 模型，实现了89.7%的准确率和89.3%的接受分布函数点（AUROC）。</li>
<li>results: 我们发现，只使用笑脸视频中的特征，可以达到相似的性能，甚至在两个外部测试集上，模型没有在训练过程中看到的数据上进行了分类，这表明了PD风险评估可能通过笑脸自拍视频进行。<details>
<summary>Abstract</summary>
Parkinson's disease (PD) diagnosis remains challenging due to lacking a reliable biomarker and limited access to clinical care. In this study, we present an analysis of the largest video dataset containing micro-expressions to screen for PD. We collected 3,871 videos from 1,059 unique participants, including 256 self-reported PD patients. The recordings are from diverse sources encompassing participants' homes across multiple countries, a clinic, and a PD care facility in the US. Leveraging facial landmarks and action units, we extracted features relevant to Hypomimia, a prominent symptom of PD characterized by reduced facial expressions. An ensemble of AI models trained on these features achieved an accuracy of 89.7% and an Area Under the Receiver Operating Characteristic (AUROC) of 89.3% while being free from detectable bias across population subgroups based on sex and ethnicity on held-out data. Further analysis reveals that features from the smiling videos alone lead to comparable performance, even on two external test sets the model has never seen during training, suggesting the potential for PD risk assessment from smiling selfie videos.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Domain-specificity-and-data-efficiency-in-typo-tolerant-spell-checkers-the-case-of-search-in-online-marketplaces"><a href="#Domain-specificity-and-data-efficiency-in-typo-tolerant-spell-checkers-the-case-of-search-in-online-marketplaces" class="headerlink" title="Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces"></a>Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01976">http://arxiv.org/abs/2308.01976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayananda Ubrangala, Juhi Sharma, Ravi Prasad Kondapalli, Kiran R, Amit Agarwala, Laurent Boué</li>
<li>for: 提高在线市场场所上的拼写错误检测精度</li>
<li>methods: 使用数据扩充方法生成域限定特定的隐藏表示，并使用回归神经网络进行训练</li>
<li>results: 实现了在实时推荐API中的 typo 检测，提高了搜索效果<details>
<summary>Abstract</summary>
Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.
</details>
<details>
<summary>摘要</summary>
typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool, especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Synthesising-Rare-Cataract-Surgery-Samples-with-Guided-Diffusion-Models"><a href="#Synthesising-Rare-Cataract-Surgery-Samples-with-Guided-Diffusion-Models" class="headerlink" title="Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models"></a>Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02587">http://arxiv.org/abs/2308.02587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meclabtuda/catasynth">https://github.com/meclabtuda/catasynth</a></li>
<li>paper_authors: Yannik Frisch, Moritz Fuchs, Antoine Sanner, Felix Anton Ucar, Marius Frenzel, Joana Wasielica-Poslednik, Adrian Gericke, Felix Mathias Wagner, Thomas Dratsch, Anirban Mukhopadhyay</li>
<li>for: 提高Automated Cataract Surgery Assistance System的发展，提供可靠的人工合成数据。</li>
<li>methods: 使用Denosing Diffusion Implicit Models（DDIM）和Classifier-Free Guidance（CFG）Conditional Generative Model Synthesize complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools.</li>
<li>results: 通过生成不同、高质量的示例，提高downstream工具分类器的性能，最高提高10%。<details>
<summary>Abstract</summary>
Cataract surgery is a frequently performed procedure that demands automation and advanced assistance systems. However, gathering and annotating data for training such systems is resource intensive. The publicly available data also comprises severe imbalances inherent to the surgical process. Motivated by this, we analyse cataract surgery video data for the worst-performing phases of a pre-trained downstream tool classifier. The analysis demonstrates that imbalances deteriorate the classifier's performance on underrepresented cases. To address this challenge, we utilise a conditional generative model based on Denoising Diffusion Implicit Models (DDIM) and Classifier-Free Guidance (CFG). Our model can synthesise diverse, high-quality examples based on complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools. We affirm that the synthesised samples display tools that the classifier recognises. These samples are hard to differentiate from real images, even for clinical experts with more than five years of experience. Further, our synthetically extended data can improve the data sparsity problem for the downstream task of tool classification. The evaluations demonstrate that the model can generate valuable unseen examples, allowing the tool classifier to improve by up to 10% for rare cases. Overall, our approach can facilitate the development of automated assistance systems for cataract surgery by providing a reliable source of realistic synthetic data, which we make available for everyone.
</details>
<details>
<summary>摘要</summary>
喉痒手术是一种常见的手术过程，需要自动化和高级帮助系统。然而，收集和标注数据 для训练这些系统是资源占用的。公共可用数据也包含了手术过程中的严重偏见。为了解决这个挑战，我们分析了喉痒手术视频数据，找到最差表现的阶段。分析结果表明，偏见会使下游工具分类器的表现在不足表现的案例下下降。为了解决这个问题，我们使用基于减噪扩散模型（DDIM）和无类标注指南（CFG）的 conditional generative model。我们的模型可以生成多样化、高质量的示例，基于复杂的多类多标签条件，如手术阶段和手术工具的组合。我们证明了生成的样本中的工具，可以由分类器识别。这些样本与真实图像很难分辨，甚至对有 более чем五年的临床经验的专业人员来说。此外，我们通过增加的数据可以改善下游任务中的数据稀缺问题。评估结果表明，我们的模型可以生成有价值的未看到的示例，使工具分类器提高至10%。总的来说，我们的方法可以促进喉痒手术自动化的发展，提供一个可靠的真实Synthetic数据源，我们将其公开给 everyone。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Agent-Policy-with-Externalities-Reward-Design-via-Bilevel-RL"><a href="#Aligning-Agent-Policy-with-Externalities-Reward-Design-via-Bilevel-RL" class="headerlink" title="Aligning Agent Policy with Externalities: Reward Design via Bilevel RL"></a>Aligning Agent Policy with Externalities: Reward Design via Bilevel RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02585">http://arxiv.org/abs/2308.02585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Dinesh Manocha, Huazheng Wang, Furong Huang, Mengdi Wang</li>
<li>for: 本研究旨在批处RL政策优化问题中的奖励函数假设，以及RL政策优化过程中的状态空间覆盖和安全性考虑。</li>
<li>methods: 本研究提出了一种级联优化问题，将主体（principal）定义为系统的更广泛目标和约束，而代理（agent）则解决Markov决策过程（MDP）。</li>
<li>results: 研究提出了主体驱动政策对应性via级联RL（PPA-BRL），该方法可有效地将代理的政策与主体的目标相吻合。研究还证明了PPA-BRL的收敛性，并通过多个示例验证了该方法的优点，包括能效地实现能源充足的操作任务、社会福利基础的税制设计以及成本效益的机器人导航。<details>
<summary>Abstract</summary>
In reinforcement learning (RL), a reward function is often assumed at the outset of a policy optimization procedure. Learning in such a fixed reward paradigm in RL can neglect important policy optimization considerations, such as state space coverage and safety. Moreover, it can fail to encompass broader impacts in terms of social welfare, sustainability, or market stability, potentially leading to undesirable emergent behavior and potentially misaligned policy. To mathematically encapsulate the problem of aligning RL policy optimization with such externalities, we consider a bilevel optimization problem and connect it to a principal-agent framework, where the principal specifies the broader goals and constraints of the system at the upper level and the agent solves a Markov Decision Process (MDP) at the lower level. The upper-level deals with learning a suitable reward parametrization corresponding to the broader goals and the lower-level deals with learning the policy for the agent. We propose Principal driven Policy Alignment via Bilevel RL (PPA-BRL), which efficiently aligns the policy of the agent with the principal's goals. We explicitly analyzed the dependence of the principal's trajectory on the lower-level policy, prove the convergence of PPA-BRL to the stationary point of the problem. We illuminate the merits of this framework in view of alignment with several examples spanning energy-efficient manipulation tasks, social welfare-based tax design, and cost-effective robotic navigation.
</details>
<details>
<summary>摘要</summary>
在增强学习（RL）中，常常假设一个奖金函数，用于policy优化过程的开始。这种固定奖金的假设可能忽略了重要的策略优化考虑因素，如状态空间覆盖率和安全性。此外，它可能无法涵盖更广泛的影响，如社会福利、可持续发展和市场稳定性，可能导致不жела的潜在行为和不一致策略。为了数学地表述RL策略优化与外部影响的问题，我们考虑了一个双层优化问题，并将其连接到一个主体-代理模型，其中主体规定系统的更广泛目标和约束，而代理在下层解决一个Markov决策过程（MDP）。上层学习一个适当的奖金参数化，与下层学习代理的策略。我们提出了主体驱动策略对齐（PPA-BRL），它高效地将代理的策略与主体的目标相对应。我们证明了PPA-BRL在站点点问题中的收敛性。我们通过一些示例，如能效的机器人 Navigation，社会福利基于税制的设计，以及成本效果的机器人 Navigation， illustrate the advantages of this framework。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-in-Large-Language-Models-Through-Symbolic-Math-Word-Problems"><a href="#Reasoning-in-Large-Language-Models-Through-Symbolic-Math-Word-Problems" class="headerlink" title="Reasoning in Large Language Models Through Symbolic Math Word Problems"></a>Reasoning in Large Language Models Through Symbolic Math Word Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01906">http://arxiv.org/abs/2308.01906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedant Gaur, Nikunj Saunshi</li>
<li>for: 这篇论文探讨了自然语言处理（NLP）领域中大语言模型（LLM）的理解能力。</li>
<li>methods: 该论文使用了符号版本的数学Word问题（MWP）来研究LLM的理解能力，并创建了一个符号版本的SVAMP数据集。</li>
<li>results: 研究发现，使用自我提示approach可以使LLM的符号理解更加准确，并且自动提取出符号答案和数学答案之间的对应关系，从而使LLM的理解更加明确。此外，自我提示还能够提高符号准确率，超过 numeric 和 symbolic 准确率，从而实现了一种ensemble效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be released for future research on symbolic math problems.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经革命化NLG，解决了少量或无标签数据下的下游任务。 despite their 多元能力，大型问题的理解仍然不够了解。 本文研究 math word problems（MWPs）的推理，通过研究 symbolic versions of the numeric problems，因为一个 symbolic expression 是一个 "简洁解释" 的 numeric answer。 我们创建了一个 symbolic version of the SVAMP dataset，并发现 GPT-3 的 davinci-002 模型在 symbolic MWPs 上也有良好的 zero-shot accuracy。 为了评估模型的 faithfulness，我们不仅评估了模型的准确性，还进一步评估了模型输出的推理与答案的对齐度，这与 numeric 和 symbolic 答案对应。 我们还探索了自我提示的方法，以便将 symbolic reasoning 与 numeric answer 相互适应，从而让 LLM 具备提供简洁且可靠的推理，并使其更易理解。  surprisingly，自我提示也使 symbolic 准确性高于 numeric 和 symbolic 准确性，提供了一个 ensemble 效果。 我们将 SVAMP_Sym dataset 发布给未来的研究人员对于符号数学问题进行研究。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Deformable-Convolution-for-Depth-Completion"><a href="#Revisiting-Deformable-Convolution-for-Depth-Completion" class="headerlink" title="Revisiting Deformable Convolution for Depth Completion"></a>Revisiting Deformable Convolution for Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01905">http://arxiv.org/abs/2308.01905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinglong Sun, Jean Ponce, Yu-Xiong Wang</li>
<li>for: 这篇论文旨在提高深度地图的质量，具体来说是从粗糙的深度地图中生成高质量的稠密深度地图。</li>
<li>methods: 该论文提出了一种使用可变核函数卷积来单 passes地进行改进，从而解决了迭代循环的缺点，并且通过系统地调查了多种表现方法，以更好地理解可变核函数的作用和如何利用其进行深度 completion。</li>
<li>results: 研究人员通过对大规模的 KITTI 数据集进行评估，发现他们的模型在准确率和执行速度两个方面均达到了领先水平。<details>
<summary>Abstract</summary>
Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convolution needs to be applied on an estimated depth map with a relatively high density for better performance. We evaluate our model on the large-scale KITTI dataset and achieve state-of-the-art level performance in both accuracy and inference speed. Our code is available at https://github.com/AlexSunNik/ReDC.
</details>
<details>
<summary>摘要</summary>
深度完成，目标是生成高质量的稠密深度地图从稀疏深度地图中，在最近几年内吸引了越来越多的注意力。先前的工作通常使用RGB图像作为引导，并通过迭代的空间卷积来精细化估计的粗略深度地图。然而，大多数卷积修充方法需要多个迭代和固定的接受范围，可能包含无关和无用的信息，尤其是与稀疏输入相比。在这篇论文中，我们 simultanously解决了这两个挑战，通过再次探讨可变核 convolution的想法。我们提议一种有效的架构，利用可变核 convolution作为单pass精细化模块，并经验证其超越性。为了更好地理解可变核 convolution的功能和利用其进行深度完成，我们进一步系统地调查了一些代表性的策略。我们的研究表明，与先前工作不同，可变核 convolution需要在估计的深度地图中的相对较高的密度来获得更好的性能。我们在大规模的KITTI dataset上评估了我们的模型，并在准确率和推理速度两个指标上达到了当前领域的状态码水平。我们的代码可以在https://github.com/AlexSunNik/ReDC中找到。
</details></li>
</ul>
<hr>
<h2 id="How-many-preprints-have-actually-been-printed-and-why-a-case-study-of-computer-science-preprints-on-arXiv"><a href="#How-many-preprints-have-actually-been-printed-and-why-a-case-study-of-computer-science-preprints-on-arXiv" class="headerlink" title="How many preprints have actually been printed and why: a case study of computer science preprints on arXiv"></a>How many preprints have actually been printed and why: a case study of computer science preprints on arXiv</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01899">http://arxiv.org/abs/2308.01899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialiang Lin, Yao Yu, Yu Zhou, Zhiyang Zhou, Xiaodong Shi</li>
<li>For: This paper aims to quantify the number of preprints that are eventually published in peer-reviewed venues, and to investigate the characteristics of published preprints in the field of computer science.* Methods: The authors use a case study of computer science preprints submitted to arXiv from 2008 to 2017, and employ a semantics-based mapping method using BERT to match preprints with their published versions.* Results: The authors find that 66% of all sampled preprints are published under unchanged titles and 11% are published under different titles and with other modifications. They also identify several characteristics that are associated with published preprints, including adequate revisions, multiple authorship, detailed abstract and introduction, extensive and authoritative references, and available source code.<details>
<summary>Abstract</summary>
Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sources, we find that 66% of all sampled preprints are published under unchanged titles and 11% are published under different titles and with other modifications. A further analysis was then performed to investigate why these preprints but not others were accepted for publication. Our comparison reveals that in the field of computer science, published preprints feature adequate revisions, multiple authorship, detailed abstract and introduction, extensive and authoritative references and available source code.
</details>
<details>
<summary>摘要</summary>
Preprints 在学术社区中发挥越来越重要的作用。有很多原因使研究人员将文稿上传到 précis servers 之前，而不是正式提交到期刊或会议，但使用 preprints 也引起了较大的争议，特别是在优先权方面。在这篇论文中，我们对计算机科学 preprints 在 arXiv 上从 2008 年到 2017 年的 submissions 进行了案例研究，以计算这些 manuscripts 最终被 print 在 peer-reviewed venue 中的数量。其中一些已经被 published 的文稿，有些在 preprints 上没有更新，这些 manuscripts  traditional fuzzy matching 方法无法映射 preprints 到最终发表的版本。为解决这个问题，我们引入 semantics-based mapping 方法，使用 Bidirectional Encoder Representations from Transformers (BERT)。与传统方法不同的是，我们使用多种数据源，并发现了以下结果：66% 的样本 preprints 被发表不变的标题，11% 的样本 preprints 被发表并有其他修改。然后，我们进行了进一步的分析，以 investigating 为什么这些 preprints 而不是其他的被accepted  для发表。我们的比较发现，在计算机科学领域中，发表的 preprints 具有充分的修改、多个作者、详细的摘要和引言、详细的参考文献和可用的源代码。
</details></li>
</ul>
<hr>
<h2 id="Improving-Replay-Sample-Selection-and-Storage-for-Less-Forgetting-in-Continual-Learning"><a href="#Improving-Replay-Sample-Selection-and-Storage-for-Less-Forgetting-in-Continual-Learning" class="headerlink" title="Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning"></a>Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01895">http://arxiv.org/abs/2308.01895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Brignac, Niels Lobo, Abhijit Mahalanobis</li>
<li>for: 该研究旨在解决深度学习模型在进行连续学习时免受前任务卷积失忆的问题。</li>
<li>methods: 该研究使用了一种新的比较方法，与常见的储存样本方法进行对比，并提供了一种细致的分析方法来找到最佳储存样本的数量。</li>
<li>results: 该研究结果表明，使用该新的比较方法和细致的分析方法可以更好地选择最有价值的样本进行储存，从而提高连续学习的性能。<details>
<summary>Abstract</summary>
Continual learning seeks to enable deep learners to train on a series of tasks of unknown length without suffering from the catastrophic forgetting of previous tasks. One effective solution is replay, which involves storing few previous experiences in memory and replaying them when learning the current task. However, there is still room for improvement when it comes to selecting the most informative samples for storage and determining the optimal number of samples to be stored. This study aims to address these issues with a novel comparison of the commonly used reservoir sampling to various alternative population strategies and providing a novel detailed analysis of how to find the optimal number of stored samples.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Continual learning 目标是帮助深度学习者学习一系列任务的长度未知而不受前任务忘记的影响。一种有效的解决方案是 reuse，即将前一些经验存储在内存中，并在学习当前任务时重新播放。然而，还有很多可以提高的空间，包括选择存储的最有用样本和确定存储样本的优化数量。这项研究目标是通过对通用的队列抽样与其他人口策略进行比较，并提供一种新的详细分析，以寻找最佳存储样本的数量。
</details></li>
</ul>
<hr>
<h2 id="Exact-identification-of-nonlinear-dynamical-systems-by-Trimmed-Lasso"><a href="#Exact-identification-of-nonlinear-dynamical-systems-by-Trimmed-Lasso" class="headerlink" title="Exact identification of nonlinear dynamical systems by Trimmed Lasso"></a>Exact identification of nonlinear dynamical systems by Trimmed Lasso</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01891">http://arxiv.org/abs/2308.01891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn L. Kiser, Mikhail Guskov, Marc Rébillat, Nicolas Ranc</li>
<li>for: 本研究旨在提出一种可以在实际数据中进行非线性动力系统标定的方法，能够处理有限长度和噪声的实际数据。</li>
<li>methods: 本研究使用了SINDy算法，以及其多种扩展，如E-SINDy和TRIM。这些方法都是基于梯度最小化的方法，但是TRIM方法可以提供更加精准的结果，并且可以在更加严重的噪声和有限数据情况下进行标定。</li>
<li>results: 本研究对三个不同的非线性动力系统进行了实验，结果表明，TRIM方法可以在有限长度和噪声的实际数据中提供更加精准的标定结果，而E-SINDy方法则可能会出现残差。此外，TRIM方法的计算成本与STLS算法相同，可以使用可 convex 的解决方法进行优化。<details>
<summary>Abstract</summary>
Identification of nonlinear dynamical systems has been popularized by sparse identification of the nonlinear dynamics (SINDy) via the sequentially thresholded least squares (STLS) algorithm. Many extensions SINDy have emerged in the literature to deal with experimental data which are finite in length and noisy. Recently, the computationally intensive method of ensembling bootstrapped SINDy models (E-SINDy) was proposed for model identification, handling finite, highly noisy data. While the extensions of SINDy are numerous, their sparsity-promoting estimators occasionally provide sparse approximations of the dynamics as opposed to exact recovery. Furthermore, these estimators suffer under multicollinearity, e.g. the irrepresentable condition for the Lasso. In this paper, we demonstrate that the Trimmed Lasso for robust identification of models (TRIM) can provide exact recovery under more severe noise, finite data, and multicollinearity as opposed to E-SINDy. Additionally, the computational cost of TRIM is asymptotically equal to STLS since the sparsity parameter of the TRIM can be solved efficiently by convex solvers. We compare these methodologies on challenging nonlinear systems, specifically the Lorenz 63 system, the Bouc Wen oscillator from the nonlinear dynamics benchmark of No\"el and Schoukens, 2016, and a time delay system describing tool cutting dynamics. This study emphasizes the comparisons between STLS, reweighted $\ell_1$ minimization, and Trimmed Lasso in identification with respect to problems faced by practitioners: the problem of finite and noisy data, the performance of the sparse regression of when the library grows in dimension (multicollinearity), and automatic methods for choice of regularization parameters.
</details>
<details>
<summary>摘要</summary>
非线性动力系统的识别已经得到了广泛的应用，通过非线性动力系统简化的逻辑（SINDy）via 随机阈值最小二乘（STLS）算法。在文献中，许多基于SINDy的扩展出现了，以处理实际数据的限定长度和噪声。最近，为了模型识别，提出了 computationally intensive的 ensemble bootstrapped SINDy模型（E-SINDy）方法。虽然扩展SINDy多种，但它们的稀疏采样器 occasionally提供稀疏的动力简化，而不是精确的回归。此外，这些采样器在多icollinearity情况下会受到影响，例如Lasso中的不可 reprehender condition。在这篇论文中，我们表明了 Trimmed Lasso 可以在更严重的噪声、有限数据和多icollinearity情况下提供精确的回归，而不是E-SINDy。此外，TRIM的计算成本是 STLS 的 asymptotic 等价，因为TRIM 的稀疏参数可以由 convex 解决器有效地解决。我们将这些方法在非线性系统中进行比较，包括 Lorenz 63 系统、Bouc Wen 振荡器和时延系统，以及2016年 No\"el 和 Schoukens 非线性动力系统比赛中的非线性动力系统 benchmark。这一研究强调了 STLS、重量 $\ell_1$ 最小化和 Trimmed Lasso 在面临实际问题时的比较：有限和噪声数据、稀疏回归在库存 grows 时的性能，以及自动选择正则化参数的问题。
</details></li>
</ul>
<hr>
<h2 id="DualCoOp-Fast-and-Effective-Adaptation-to-Multi-Label-Recognition-with-Limited-Annotations"><a href="#DualCoOp-Fast-and-Effective-Adaptation-to-Multi-Label-Recognition-with-Limited-Annotations" class="headerlink" title="DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations"></a>DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01890">http://arxiv.org/abs/2308.01890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Hu, Ximeng Sun, Stan Sclaroff, Kate Saenko</li>
<li>for: 这个研究的目的是提高多 Label 图像识别 tasks 的准确性，特别是在低标签情况下。</li>
<li>methods: 这个研究使用了一个名为 Evidence-guided Dual Context Optimization (DualCoOp++) 的框架，这是一个统一的方法来解决 partial-label 和 zero-shot multi-label 识别 задачі。DualCoOp++ 使用了不同的文本内容来分类目标类别，并且将这些内容转换为 Parametric 组件。</li>
<li>results: 实验结果显示，DualCoOp++ 在两个低标签情况下的标准多 Label 识别Benchmark上表现出色，较以前的方法更好。<details>
<summary>Abstract</summary>
Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive and negative contexts from the spatial domain of the image, enabling better distinguishment between similar categories. Additionally, we introduce a Winner-Take-All module that promotes inter-class interaction during training, while avoiding the need for extra parameters and costs. As DualCoOp++ imposes minimal additional learnable overhead on the pretrained vision-language framework, it enables rapid adaptation to multi-label recognition tasks with limited annotations and even unseen classes. Experiments on standard multi-label recognition benchmarks across two challenging low-label settings demonstrate the superior performance of our approach compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多 Label 图像识别在低标签 режиме是一项具有挑战性和实际意义的任务。先前的研究通过学习文本和视觉空间之间的对应关系来资料缺乏多 Label 图像标注，但可能会受到质量不佳多 Label 图像标注的影响。在这项研究中，我们利用了强大的文本和视觉特征之间的对应关系，它们在 Millionen 个 auxiliary 图像-文本对中预训练。我们提出了一种高效可靠的框架，即 Evidence-guided Dual Context Optimization（DualCoOp++），它作为多 Label 图像识别中的一种统一方法。在 DualCoOp++ 中，我们分别编码目标类的证据、积极和消极上下文为参数化的文本输入（即提示）中的Parametric 组件。证据上下文的目的是找到目标类相关的所有视觉内容，并作为指导将空间领域中的积极和消极上下文聚合，以更好地区分相似类别。此外，我们还引入了一个 Winner-Take-All 模块，它在训练中促进类之间的交互，而不需要额外的参数和成本。由于 DualCoOp++ 对预训练的视觉语言框架做出了最小的额外学习负担，因此它可以快速适应多 Label 图像识别任务，即使具有有限的标注和未看到的类。实验表明，我们的方法在标准多 Label 图像识别标准 benchmark 上表现出优于状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Cream-Skimming-the-Underground-Identifying-Relevant-Information-Points-from-Online-Forums"><a href="#Cream-Skimming-the-Underground-Identifying-Relevant-Information-Points-from-Online-Forums" class="headerlink" title="Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums"></a>Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02581">http://arxiv.org/abs/2308.02581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe Moreno-Vera, Mateus Nogueira, Cainã Figueiredo, Daniel Sadoc Menasché, Miguel Bicudo, Ashton Woiwood, Enrico Lovat, Anton Kocheturov, Leandro Pfleger de Aguiar</li>
<li>for: 本研究提出一种基于机器学习的方法，用于在野外抓取漏洞利用情况。随着在线上讨论漏洞利用的帖子和帖子数量不断增加，需要一种自动化处理这些帖子和帖子的方法，以触发警报 Depending on their content.</li>
<li>methods: 我们使用了CrimeBB数据集，该数据集包含多个下面forum中的数据，并开发了一个监督式机器学习模型，可以过滤引用CVEs的帖子，并将其分为Proof-of-Concept、Weaponization和利用三个类别。使用Random Forest算法，我们表明可以在分类任务中达到0.99以上的准确率、精度和准确率。</li>
<li>results: 我们发现，在 weaponization和利用之间存在差异，例如解释决定树的输出，并分析了黑客社区的利益和其他相关方面。总的来说，我们的工作提供了野外漏洞利用情况的研究，可以用于提供额外的真实数据，以便更好地评估模型如EPSS和Expected Exploitability。<details>
<summary>Abstract</summary>
This paper proposes a machine learning-based approach for detecting the exploitation of vulnerabilities in the wild by monitoring underground hacking forums. The increasing volume of posts discussing exploitation in the wild calls for an automatic approach to process threads and posts that will eventually trigger alarms depending on their content. To illustrate the proposed system, we use the CrimeBB dataset, which contains data scraped from multiple underground forums, and develop a supervised machine learning model that can filter threads citing CVEs and label them as Proof-of-Concept, Weaponization, or Exploitation. Leveraging random forests, we indicate that accuracy, precision and recall above 0.99 are attainable for the classification task. Additionally, we provide insights into the difference in nature between weaponization and exploitation, e.g., interpreting the output of a decision tree, and analyze the profits and other aspects related to the hacking communities. Overall, our work sheds insight into the exploitation of vulnerabilities in the wild and can be used to provide additional ground truth to models such as EPSS and Expected Exploitability.
</details>
<details>
<summary>摘要</summary>
Note: "EPSS" stands for "Expected Potential Security Score" and "Expected Exploitability" is a metric used to measure the severity of a vulnerability.
</details></li>
</ul>
<hr>
<h2 id="Statistical-Estimation-Under-Distribution-Shift-Wasserstein-Perturbations-and-Minimax-Theory"><a href="#Statistical-Estimation-Under-Distribution-Shift-Wasserstein-Perturbations-and-Minimax-Theory" class="headerlink" title="Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory"></a>Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01853">http://arxiv.org/abs/2308.01853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrickrchao/dist_shift_exp">https://github.com/patrickrchao/dist_shift_exp</a></li>
<li>paper_authors: Patrick Chao, Edgar Dobriban</li>
<li>for: 本文研究了现代统计学中的分布Shift问题，即数据点的perturbation可能会系统地改变数据的性质。</li>
<li>methods: 本文使用 Wasserstein distribution shift，研究了每个数据点可能会受到轻微改动的情况，而不是Huber contamination模型中的一部分观察值是异常值。本文还研究了各种重要的统计问题，包括位置估计、线性回归和非 Parametric density estimation。</li>
<li>results: 本文发现，在平方损函数下的mean估计和线性回归预测错误中， sample mean和least squares estimator是相对最佳的。这些优点在独立分布shift和共同分布shift下都存在，但最差的perturbation和最大风险不同。其他问题中，提供了近似最佳的估计器和精确的finite-sample bound。本文还介绍了一些用于下界最大风险的工具，如location家族的缓和技术，以及classical工具的扩展，如最差序列的 prior、modulus of continuity、Le Cam的、Fano的和Assouad的方法。<details>
<summary>Abstract</summary>
Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For other problems, we provide nearly optimal estimators and precise finite-sample bounds. We also introduce several tools for bounding the minimax risk under distribution shift, such as a smoothing technique for location families, and generalizations of classical tools including least favorable sequences of priors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.
</details>
<details>
<summary>摘要</summary>
现代统计学中的分布转移是一个严重的问题，因为它可能会系统性地改变数据的性质，从真实的情况偏离。我们关注 Wasserstein 分布转移，其中每个数据点都可能会经历一些微的扰动，而不是 Huber 污染模型，其中一部分观测值是异常值。我们提出并研究了分布转移的不同类型，包括共同扰动分布转移。我们分析了一些重要的统计问题，包括位置估计、线性回归和非 Parametric 密度估计。在平方损失下，我们发现了最小最大风险、最不利的扰动和 sample 均值和最小二乘估计器是相应优化的。这些优化存在独立和共同转移下都是正确的，但最不利的扰动和最大风险不同。对于其他问题，我们提供了近似优化的估计器和精确的 finite-sample 上限。我们还引入了一些用于下界最大风险的工具，包括分布转移后的平滑技术、类 least favorable 序列假设、模ulus 稳定性、Le Cam 、Fano 和 Assouad 的方法。
</details></li>
</ul>
<hr>
<h2 id="Curricular-Transfer-Learning-for-Sentence-Encoded-Tasks"><a href="#Curricular-Transfer-Learning-for-Sentence-Encoded-Tasks" class="headerlink" title="Curricular Transfer Learning for Sentence Encoded Tasks"></a>Curricular Transfer Learning for Sentence Encoded Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01849">http://arxiv.org/abs/2308.01849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jader Martins Camboim de Sá, Matheus Ferraroni Sanches, Rafael Roque de Souza, Júlio Cesar dos Reis, Leandro Aparecido Villas</li>
<li>for: 提高NLU任务中模型的表现，尤其是在数据分布变化时。</li>
<li>methods: 提出了一种逐步适应（curriculum）策略，通过数据黑客和语法分析导航进行适应。</li>
<li>results: 在我们的实验中，我们的方法比其他已知预训练方法在多语言对话任务（MultiWoZ）中获得了显著提高。<details>
<summary>Abstract</summary>
Fine-tuning language models in a downstream task is the standard approach for many state-of-the-art methodologies in the field of NLP. However, when the distribution between the source task and target task drifts, \textit{e.g.}, conversational environments, these gains tend to be diminished. This article proposes a sequence of pre-training steps (a curriculum) guided by "data hacking" and grammar analysis that allows further gradual adaptation between pre-training distributions. In our experiments, we acquire a considerable improvement from our method compared to other known pre-training approaches for the MultiWoZ task.
</details>
<details>
<summary>摘要</summary>
通常的方法是在下游任务中细化语言模型，以获得许多状态OF-THE-ART的成果。但是，当源任务和目标任务的分布发生变化，例如对话环境，这些改进往往减少。这篇文章提出了一系列的预训练步骤（课程），通过“数据黑客”和语法分析引导，以进一步适应预训练分布的变化。在我们的实验中，我们获得了与其他已知预训练方法相比较大的改进，用于多语言对话任务。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Deep-Supervision-Network-A-Noise-Resilient-Approach-for-QoS-Prediction"><a href="#Probabilistic-Deep-Supervision-Network-A-Noise-Resilient-Approach-for-QoS-Prediction" class="headerlink" title="Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction"></a>Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02580">http://arxiv.org/abs/2308.02580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hotfrom/pds-net">https://github.com/hotfrom/pds-net</a></li>
<li>paper_authors: Ziliang Wang, Xiaohong Zhang, Sheng Huang, Wei Zhang, Dan Yang, Meng Yan</li>
<li>for: 提高用户满意度，准确预测 unknown QoS 值</li>
<li>methods: 提出了 Probabilistic Deep Supervision Network (PDS-Net) 框架，利用 Gaussian 型概率空间进行中间层级supervision，学习known features和真实标签的概率空间</li>
<li>results: 在两个实际 QoS 数据集上进行实验评估，比对 estado-of-the-art 基elines， validate 我们的方法的有效性<details>
<summary>Abstract</summary>
Quality of Service (QoS) prediction is an essential task in recommendation systems, where accurately predicting unknown QoS values can improve user satisfaction. However, existing QoS prediction techniques may perform poorly in the presence of noise data, such as fake location information or virtual gateways. In this paper, we propose the Probabilistic Deep Supervision Network (PDS-Net), a novel framework for QoS prediction that addresses this issue. PDS-Net utilizes a Gaussian-based probabilistic space to supervise intermediate layers and learns probability spaces for both known features and true labels. Moreover, PDS-Net employs a condition-based multitasking loss function to identify objects with noise data and applies supervision directly to deep features sampled from the probability space by optimizing the Kullback-Leibler distance between the probability space of these objects and the real-label probability space. Thus, PDS-Net effectively reduces errors resulting from the propagation of corrupted data, leading to more accurate QoS predictions. Experimental evaluations on two real-world QoS datasets demonstrate that the proposed PDS-Net outperforms state-of-the-art baselines, validating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
服务质量（QoS）预测是推荐系统中的一项重要任务，可以提高用户满意度。然而，现有的QoS预测技术可能在噪声数据存在时表现不佳。在这篇论文中，我们提出了可靠性深度监督网络（PDS-Net），一种解决这个问题的新框架。PDS-Net使用 Gaussian 型概率空间来监督中间层，并学习概率空间 для已知特征和真实标签。此外，PDS-Net 使用基于条件的多任务损失函数来识别具有噪声数据的对象，并直接将深度特征从概率空间中抽取到真实标签的概率空间中进行监督。因此，PDS-Net 可以减少噪声数据的传播错误，从而提高 QoS 预测的准确性。实验评估在两个真实 QoS 数据集上表明，提出的 PDS-Net 已经超越了状态艺术基eline。
</details></li>
</ul>
<hr>
<h2 id="URET-Universal-Robustness-Evaluation-Toolkit-for-Evasion"><a href="#URET-Universal-Robustness-Evaluation-Toolkit-for-Evasion" class="headerlink" title="URET: Universal Robustness Evaluation Toolkit (for Evasion)"></a>URET: Universal Robustness Evaluation Toolkit (for Evasion)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01840">http://arxiv.org/abs/2308.01840</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/uret">https://github.com/ibm/uret</a></li>
<li>paper_authors: Kevin Eykholt, Taesung Lee, Douglas Schales, Jiyong Jang, Ian Molloy, Masha Zorin</li>
<li>for: 本研究旨在提高机器学习模型的安全和可靠性，通过生成可逃脱攻击的输入，以帮助确保AI任务的正确性和可靠性。</li>
<li>methods: 本研究提出了一种新的框架，可以生成不同输入类型和任务领域的攻击输入。该框架使用给定的输入变换集合，找到一个符合semantic和功能要求的攻击输入序列。</li>
<li>results: 本研究在多种不同的机器学习任务和输入表示中展示了框架的通用性。此外，研究还表明了生成攻击示例的重要性，以便应用防御技术。<details>
<summary>Abstract</summary>
Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples as they enable the deployment of mitigation techniques.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a new framework to generate adversarial inputs regardless of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples, as they enable the deployment of mitigation techniques.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/04/cs.LG_2023_08_04/" data-id="clly3dvze00600988bvyo0jbp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/05/eess.IV_2023_08_05/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-05 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/04/cs.SD_2023_08_04/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-04 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
