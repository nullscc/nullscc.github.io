
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-04 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Efficient Monaural Speech Enhancement using Spectrum Attention Fusion paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02263 repo_url: None paper_authors: Jinyu Long, Jetic Gū, Binhao Bai, Zhibo Yang, Ping Wei, J">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-04 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/04/cs.SD_2023_08_04/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Efficient Monaural Speech Enhancement using Spectrum Attention Fusion paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02263 repo_url: None paper_authors: Jinyu Long, Jetic Gū, Binhao Bai, Zhibo Yang, Ping Wei, J">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-03T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:25.258Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/cs.SD_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-04 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Monaural-Speech-Enhancement-using-Spectrum-Attention-Fusion"><a href="#Efficient-Monaural-Speech-Enhancement-using-Spectrum-Attention-Fusion" class="headerlink" title="Efficient Monaural Speech Enhancement using Spectrum Attention Fusion"></a>Efficient Monaural Speech Enhancement using Spectrum Attention Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02263">http://arxiv.org/abs/2308.02263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Long, Jetic Gū, Binhao Bai, Zhibo Yang, Ping Wei, Junli Li</li>
<li>for: 提高自动 speech 处理管道中的speech减噪性能，以提高干扰 speech 的分离效果。</li>
<li>methods: 提出了一种 Spectrum Attention Fusion 技术，用于将自我注意力 fusion 与 spectral 特征 fusion 结合，以提高模型的表达能力和效率。</li>
<li>results: 在 Voice Bank + DEMAND 数据集上，与 state-of-the-art 模型比较，提出的模型能够达到相当或更好的结果，同时具有较少的参数（0.58M）。<details>
<summary>Abstract</summary>
Speech enhancement is a demanding task in automated speech processing pipelines, focusing on separating clean speech from noisy channels. Transformer based models have recently bested RNN and CNN models in speech enhancement, however at the same time they are much more computationally expensive and require much more high quality training data, which is always hard to come by. In this paper, we present an improvement for speech enhancement models that maintains the expressiveness of self-attention while significantly reducing model complexity, which we have termed Spectrum Attention Fusion. We carefully construct a convolutional module to replace several self-attention layers in a speech Transformer, allowing the model to more efficiently fuse spectral features. Our proposed model is able to achieve comparable or better results against SOTA models but with significantly smaller parameters (0.58M) on the Voice Bank + DEMAND dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Speech enhancement 是自动化语音处理流程中的一个要求，旨在分离含杂的语音和清晰的语音。基于Transformer的模型在最近的Speech enhancement中表现出色，但同时它们也更加计算昂贵，需要更多高质量的训练数据，这并不容易获得。在这篇论文中，我们提出了一种改进 speech enhancement 模型，保持了自注意的表达力，同时显著减少模型的复杂度，我们称之为 Spectrum Attention Fusion。我们在一个 convolutional 模块中代替了一些自注意层，让模型更有效地融合频谱特征。我们的提议模型在 Voice Bank + DEMAND 数据集上可以达到与顶峰模型相当或更好的结果，但具有远小于参数（0.58M）。
</details></li>
</ul>
<hr>
<h2 id="Emo-DNA-Emotion-Decoupling-and-Alignment-Learning-for-Cross-Corpus-Speech-Emotion-Recognition"><a href="#Emo-DNA-Emotion-Decoupling-and-Alignment-Learning-for-Cross-Corpus-Speech-Emotion-Recognition" class="headerlink" title="Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition"></a>Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02190">http://arxiv.org/abs/2308.02190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaxin-ye/emo-dna">https://github.com/jiaxin-ye/emo-dna</a></li>
<li>paper_authors: Jiaxin Ye, Yujie Wei, Xin-Cheng Wen, Chenglong Ma, Zhizhong Huang, Kunhong Liu, Hongming Shan</li>
<li>for: 这个研究的目的是将拥有不同 Corpora 的语音情感识别系统进行整合，以提高其在不同 Corpora 上的表现。</li>
<li>methods: 这个研究提出了一个名为 Emotion Decoupling aNd Alignment 的新框架，它使用了对照分离和双层情感对齐来学习语音情感识别系统。</li>
<li>results: 实验结果显示，这个新框架在多个跨 Corpora 的情感识别任务中表现更好，比起现有的方法。<details>
<summary>Abstract</summary>
Cross-corpus speech emotion recognition (SER) seeks to generalize the ability of inferring speech emotion from a well-labeled corpus to an unlabeled one, which is a rather challenging task due to the significant discrepancy between two corpora. Existing methods, typically based on unsupervised domain adaptation (UDA), struggle to learn corpus-invariant features by global distribution alignment, but unfortunately, the resulting features are mixed with corpus-specific features or not class-discriminative. To tackle these challenges, we propose a novel Emotion Decoupling aNd Alignment learning framework (EMO-DNA) for cross-corpus SER, a novel UDA method to learn emotion-relevant corpus-invariant features. The novelties of EMO-DNA are two-fold: contrastive emotion decoupling and dual-level emotion alignment. On one hand, our contrastive emotion decoupling achieves decoupling learning via a contrastive decoupling loss to strengthen the separability of emotion-relevant features from corpus-specific ones. On the other hand, our dual-level emotion alignment introduces an adaptive threshold pseudo-labeling to select confident target samples for class-level alignment, and performs corpus-level alignment to jointly guide model for learning class-discriminative corpus-invariant features across corpora. Extensive experimental results demonstrate the superior performance of EMO-DNA over the state-of-the-art methods in several cross-corpus scenarios. Source code is available at https://github.com/Jiaxin-Ye/Emo-DNA.
</details>
<details>
<summary>摘要</summary>
cross-corpus speech emotion recognition (SER) 提高了推断语音情绪的能力，从一个很好地标注的 corpora 扩展到另一个没有标注的 corpora，这是一项非常具有挑战性的任务，因为两个 corpora 之间存在很大的差异。现有的方法通常基于无监督领域适应 (UDA)，尝试通过全局分布对齐来学习 corpora  invariant 特征，但是 unfortunately，得到的特征都是混合 corpora 特定特征或不是类别特征。为了解决这些挑战，我们提出了一个新的 Emotion Decoupling and Alignment learning framework (EMO-DNA)  для cross-corpus SER，一种新的 UDA 方法来学习情绪相关的 corpora  invariant 特征。EMO-DNA 的两大创新是：对比情绪分离和双级情绪对接。一方面，我们的对比情绪分离通过对比分离损失来强化情绪相关特征与 corpora 特定特征之间的分离性。另一方面，我们的双级情绪对接引入了一个 adaptive 阈值 pseudo-labeling，选择 confidence 的目标样本进行类别对接，并在 corpora 级别对接以协助模型学习类别特征的 cross-corpus 普适性。我们的实验结果表明，EMO-DNA 在多个 cross-corpus 场景中表现出了与当前状态OF 法的超越性。代码可以在 <https://github.com/Jiaxin-Ye/Emo-DNA> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Spectral-and-Long-term-Contextual-Information-for-Speech-Emotion-Recognition-Using-Deep-Learning-Techniques"><a href="#Capturing-Spectral-and-Long-term-Contextual-Information-for-Speech-Emotion-Recognition-Using-Deep-Learning-Techniques" class="headerlink" title="Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques"></a>Capturing Spectral and Long-term Contextual Information for Speech Emotion Recognition Using Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04517">http://arxiv.org/abs/2308.04517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samiul Islam, Md. Maksudul Haque, Abu Jobayer Md. Sadat</li>
<li>for: 本研究旨在超越传统的speech emotion recognition方法（如LSTM、CNN、RNN、SVM、MLP），这些方法具有难以捕捉长期依赖关系、捕捉时间动态和捕捉复杂模式关系等缺陷。</li>
<li>methods: 本研究提出了一个 ensemble 模型，该模型将文本数据处理GCN（图 convolutional networks）和音频信号分析 HuBERT trasformer 相结合。GCN 可以利用文本的图形表示，捕捉文本中的长期Contextual 依赖关系和 semantics 关系，而 HuBERT 通过自我注意机制，可以捕捉音频信号中的长期依赖关系，捕捉时间动态。</li>
<li>results: 结果表明，将 GCN 和 HuBERT 相结合，可以充分利用这两种方法的优势，同时分析多Modal 数据，并将这些模式相互融合，从而提高情绪识别系统的准确性。<details>
<summary>Abstract</summary>
Traditional approaches in speech emotion recognition, such as LSTM, CNN, RNN, SVM, and MLP, have limitations such as difficulty capturing long-term dependencies in sequential data, capturing the temporal dynamics, and struggling to capture complex patterns and relationships in multimodal data. This research addresses these shortcomings by proposing an ensemble model that combines Graph Convolutional Networks (GCN) for processing textual data and the HuBERT transformer for analyzing audio signals. We found that GCNs excel at capturing Long-term contextual dependencies and relationships within textual data by leveraging graph-based representations of text and thus detecting the contextual meaning and semantic relationships between words. On the other hand, HuBERT utilizes self-attention mechanisms to capture long-range dependencies, enabling the modeling of temporal dynamics present in speech and capturing subtle nuances and variations that contribute to emotion recognition. By combining GCN and HuBERT, our ensemble model can leverage the strengths of both approaches. This allows for the simultaneous analysis of multimodal data, and the fusion of these modalities enables the extraction of complementary information, enhancing the discriminative power of the emotion recognition system. The results indicate that the combined model can overcome the limitations of traditional methods, leading to enhanced accuracy in recognizing emotions from speech.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="N-gram-Boosting-Improving-Contextual-Biasing-with-Normalized-N-gram-Targets"><a href="#N-gram-Boosting-Improving-Contextual-Biasing-with-Normalized-N-gram-Targets" class="headerlink" title="N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets"></a>N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02092">http://arxiv.org/abs/2308.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Yau Li, Shreekantha Nadig, Karol Chang, Zafarullah Mahmood, Riqiang Wang, Simon Vandieken, Jonas Robertson, Fred Mailhot</li>
<li>for: 提高 keywords 识别率</li>
<li>methods: 使用 two-step keyword boosting mechanism，Normalize unigrams 和 n-grams，避免 missing hits 和 over-boosting multi-token keywords</li>
<li>results: 提高 keyword recognition rate by 26% Relative on proprietary in-domain dataset，和 2% on LibriSpeechI hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Accurate transcription of proper names and technical terms is particularly important in speech-to-text applications for business conversations. These words, which are essential to understanding the conversation, are often rare and therefore likely to be under-represented in text and audio training data, creating a significant challenge in this domain. We present a two-step keyword boosting mechanism that successfully works on normalized unigrams and n-grams rather than just single tokens, which eliminates missing hits issues with boosting raw targets. In addition, we show how adjusting the boosting weight logic avoids over-boosting multi-token keywords. This improves our keyword recognition rate by 26% relative on our proprietary in-domain dataset and 2% on LibriSpeech. This method is particularly useful on targets that involve non-alphabetic characters or have non-standard pronunciations.
</details>
<details>
<summary>摘要</summary>
精准转写特有名称和技术术语 particualrly important in speech-to-text应用程序中，这些词语是理解对话的关键，但它们通常是罕见的，因此在文本和音频训练数据中受到抑制。我们提出了一种两步关键词强化机制，该机制可以在 норма化单个字和n-gram中工作，而不是只是单个token，这将消除 raw 目标中的缺失命中问题。此外，我们还证明了如何调整强化权重逻辑，以避免多token关键被过度强化。这将提高我们的关键识别率达26%，相对于我们的自有领域数据集，并且2% 在 LibriSpeech 上。这种方法特别有用于targets 中包含非字母字符或非标准发音。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/04/cs.SD_2023_08_04/" data-id="clluro5ks008kq988ckoe86rg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/04/cs.LG_2023_08_04/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-04 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/04/eess.IV_2023_08_04/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-04 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
