
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-14 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07145 repo_url: https:&#x2F;&#x2F;github.com&#x2F;w-wu&#x2F;steer paper_authors: Wen">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-14 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/14/cs.SD_2023_08_14/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07145 repo_url: https:&#x2F;&#x2F;github.com&#x2F;w-wu&#x2F;steer paper_authors: Wen">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-13T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:30.348Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.SD_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-14 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations"><a href="#Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations" class="headerlink" title="Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations"></a>Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07145">http://arxiv.org/abs/2308.07145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/w-wu/steer">https://github.com/w-wu/steer</a></li>
<li>paper_authors: Wen Wu, Chao Zhang, Philip C. Woodland</li>
<li>for: 提高自动情感识别（AER）的精度和效果，使其能够在对话系统中应用。</li>
<li>methods:  integrate AER with automatic speech recognition（ASR）和speaker diarisation（SD），共同训练一个系统，并使用分布式编码器建立不同的输出层。</li>
<li>results: 在IEMOCAP dataset上进行测试，提议的系统与两个基准系统相比，在AER、ASR和SD三个任务中均表现出色，并且在时间权重 emotions 和 speaker classification 错误上采用了两种评价指标。<details>
<summary>Abstract</summary>
Although automatic emotion recognition (AER) has recently drawn significant research interest, most current AER studies use manually segmented utterances, which are usually unavailable for dialogue systems. This paper proposes integrating AER with automatic speech recognition (ASR) and speaker diarisation (SD) in a jointly-trained system. Distinct output layers are built for four sub-tasks including AER, ASR, voice activity detection and speaker classification based on a shared encoder. Taking the audio of a conversation as input, the integrated system finds all speech segments and transcribes the corresponding emotion classes, word sequences, and speaker identities. Two metrics are proposed to evaluate AER performance with automatic segmentation based on time-weighted emotion and speaker classification errors. Results on the IEMOCAP dataset show that the proposed system consistently outperforms two baselines with separately trained single-task systems on AER, ASR and SD.
</details>
<details>
<summary>摘要</summary>
尽管自动情感识别（AER）在最近几年内受到了广泛的研究兴趣，但大多数当前AER研究使用手动分割的语音，这些语音通常不可用于对话系统。这篇论文提议将AER、自动语音识别（ASR）和 speaker分类（SD）集成为一个集成系统。该系统使用共享Encoder生成了四个子任务的特征输出层，包括AER、ASR、语音活动检测和 speaker分类。将对话的音频作为输入，该集成系统可以找到所有的语音段落，并将对应的情感类别、词序列和Speaker标识转化为文本。为评估AER性能，提出了两种指标，即基于时间权重的情感错误和Speaker错误。results表明，提议的系统在IEMOCAP dataset上比基eline两个独立的单任务系统在AER、ASR和SD领域具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera"><a href="#VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera" class="headerlink" title="VoxBlink: X-Large Speaker Verification Dataset on Camera"></a>VoxBlink: X-Large Speaker Verification Dataset on Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07056">http://arxiv.org/abs/2308.07056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuke Lin, Xiaoyi Qin, Ming Cheng, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 本研究做出了一个新的和广泛的语音认可数据集，包括噪音38k个标识&#x2F;1.45M次语音（VoxBlink）和相对干净的18k个标识&#x2F;1.02M次语音（VoxBlink-Clean） для训练。</li>
<li>methods: 我们首先建立了一个自动化和可扩展的数据提取管道，从YouTube上下载了60,000个用户的短视频，并从这些视频中自动提取了相关的语音和视频段落。</li>
<li>results: 我们的实验结果表明，将VoxBlink-Clean数据集用于训练，可以提高语音认可性能，比如13%-30%的提升，不同的后向架构之间。这个数据集即将公开发布。<details>
<summary>Abstract</summary>
In this paper, we contribute a novel and extensive dataset for speaker verification, which contains noisy 38k identities/1.45M utterances (VoxBlink) and relatively cleaned 18k identities/1.02M (VoxBlink-Clean) utterances for training. Firstly, we accumulate a 60K+ users' list with their avatars and download their short videos on YouTube. We then established an automatic and scalable pipeline to extract relevant speech and video segments from these videos. To our knowledge, the VoxBlink dataset is one of the largest speaker recognition datasets available. Secondly, we conduct a series of experiments based on different backbones trained on a mix of the VoxCeleb2 and the VoxBlink-Clean. Our findings highlight a notable performance improvement, ranging from 13% to 30%, across different backbone architectures upon integrating our dataset for training. The dataset will be made publicly available shortly.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一个新的和广泛的说话人验证数据集，包括噪音38k个人/1.45万个语音（VoxBlink）和相对清晰的18k个人/1.02万个语音（VoxBlink-Clean） для训练。首先，我们积累了60,000个用户的名单和他们的aviator，然后下载了YouTube上的短视频。我们然后建立了一个自动化和可扩展的管道，以提取视频和语音段落。根据我们所知，VoxBlink数据集是目前最大的说话人识别数据集之一。其次，我们进行了基于不同的后准据体系的实验，发现在将我们的数据集用于训练时，其性能提升范围为13%到30%。这些数据将在不久的将来公开。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder"><a href="#Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder" class="headerlink" title="Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder"></a>Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08488">http://arxiv.org/abs/2308.08488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mispchallenge/misp-icme-avsr">https://github.com/mispchallenge/misp-icme-avsr</a></li>
<li>paper_authors: Yusheng Dai, Hang Chen, Jun Du, Xiaofei Ding, Ning Ding, Feijun Jiang, Chin-Hui Lee</li>
<li>for: 本研究旨在提高自动语音识别系统的音视频联合识别系统（AVSR）性能，并在预训练和精度调整框架下实现这一目标。</li>
<li>methods: 本研究提出了两种新技术来提高AVSR的性能，包括利用叙述形态学生 lip shapes 和 syllable-level subword units 的相关性来确定准确的帧级句子界限，以及使用主要训练参数进行多个跨Modal的注意力层来充分利用多Modal的共轭性。</li>
<li>results: 实验结果表明，使用这两种技术可以提高AVSR系统的性能，并在MISP2021-AVSR数据集上达到比 estado-of-the-art 系统更高的性能水平，使用的训练数据量也相对较少。<details>
<summary>Abstract</summary>
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the MISP2021-AVSR data set show the effectiveness of the two proposed techniques. Together, using only a relatively small amount of training data, the final system achieves better performances than state-of-the-art systems with more complex front-ends and back-ends.
</details>
<details>
<summary>摘要</summary>
近期研究发现，自动语音识别系统到Audio-Visual语音识别系统在端到端框架下有轻微的性能提升，但是存在不匹配的协调速率和特殊的输入表示之间的问题。在这篇论文中，我们提出了两种新的技巧来提高Audio-Visual语音识别（AVSR）在预训练和精度调整训练框架下。首先，我们探索了拼音和字节水平的叙述单元之间的相关性，以确定良好的帧级叙述边界。这使得视频和音频流之间的对齐变得精准，从而提高了视频和音频流之间的混合。其次，我们提出了一种受主要训练参数 guideline的Audio-Visual混合抽象Encoder（CMFE）神经网络，以便在多个跨模态扩散层中使用主要训练参数，以便充分利用多模态的共轭性。实验表明，使用这两种技巧可以提高系统的性能，并且只需使用相对较少的训练数据。最终系统可以在与更复杂的前端和后端的系统相比，达到更好的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06981">http://arxiv.org/abs/2308.06981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Uhlich, Giorgio Fabbro, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji</li>
<li>for: 这篇论文描述了2023年 зву隔离挑战（SDX’23）的电影幂分融合（CDX）轨迹。</li>
<li>methods: 论文详细介绍了比赛的结构和使用的数据集，特别是新构建的CDXDB23隐藏数据集，以及参与者所采用的最成功的方法。</li>
<li>results: 相比干杯餐 fork基线，专门在 simulated Divide and Remaster（DnR）数据集上训练的系统得到了1.8dB的SDR提升，而开放排行榜上的最佳系统则看到了5.7dB的显著提升。<details>
<summary>Abstract</summary>
This paper summarizes the cinematic demixing (CDX) track of the Sound Demixing Challenge 2023 (SDX'23). We provide a comprehensive summary of the challenge setup, detailing the structure of the competition and the datasets used. Especially, we detail CDXDB23, a new hidden dataset constructed from real movies that was used to rank the submissions. The paper also offers insights into the most successful approaches employed by participants. Compared to the cocktail-fork baseline, the best-performing system trained exclusively on the simulated Divide and Remaster (DnR) dataset achieved an improvement of 1.8dB in SDR whereas the top performing system on the open leaderboard, where any data could be used for training, saw a significant improvement of 5.7dB.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了2023年 зву频分离挑战（SDX'23）的电影式分离（CDX）轨迹。我们提供了竞赛设置的完整摘要，包括竞赛结构和使用的数据集。特别是，我们详细介绍了CDXDB23，一个新的隐藏数据集，从真实电影中构建而成，用于评估参赛系统的表现。文章还提供了参与者采用的最成功方法的折衔。相比干杯叉基线，专门在 simulate 的 Divide and Remaster（DnR）数据集上训练的系统得到了1.8dB的SDR提升，而在开放排行榜上，任何数据可以用于训练的系统则得到了显著的5.7dB的提升。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06979">http://arxiv.org/abs/2308.06979</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfturbo/mvsep-mdx23-music-separation-model">https://github.com/zfturbo/mvsep-mdx23-music-separation-model</a></li>
<li>paper_authors: Giorgio Fabbro, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stöter, Alexandre Défossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji</li>
<li>for: 这篇论文描述了Sound Demixing Challenge（SDX’23）的音乐分离（MDX）轨迹。</li>
<li>methods: 论文介绍了MDX系统在训练数据中出现错误的情况下的训练方法，并提出了一种对MDX系统训练数据设计的错误形式化。</li>
<li>results: 论文描述了SDXDB23_LabelNoise和SDXDB23_Bleeding1两个新的数据集，以及在SDX’23中获得最高分的方法。此外，论文还对上一届音乐分离比赛（Music Demixing Challenge 2021）的赛果进行了直接比较，发现当用MDXDB21进行评估时，最佳实现在标准MSS形式下获得了1.6dB的信号至噪声比提高。此外，论文还进行了基于人类听觉评价的听测，并对系统的感知质量进行了报告。最后，论文提供了比赛组织方式的反思和未来版本的展望。<details>
<summary>Abstract</summary>
This paper summarizes the music demixing (MDX) track of the Sound Demixing Challenge (SDX'23). We provide a summary of the challenge setup and introduce the task of robust music source separation (MSS), i.e., training MSS models in the presence of errors in the training data. We propose a formalization of the errors that can occur in the design of a training dataset for MSS systems and introduce two new datasets that simulate such errors: SDXDB23_LabelNoise and SDXDB23_Bleeding1. We describe the methods that achieved the highest scores in the competition. Moreover, we present a direct comparison with the previous edition of the challenge (the Music Demixing Challenge 2021): the best performing system under the standard MSS formulation achieved an improvement of over 1.6dB in signal-to-distortion ratio over the winner of the previous competition, when evaluated on MDXDB21. Besides relying on the signal-to-distortion ratio as objective metric, we also performed a listening test with renowned producers/musicians to study the perceptual quality of the systems and report here the results. Finally, we provide our insights into the organization of the competition and our prospects for future editions.
</details>
<details>
<summary>摘要</summary>
In Simplified Chinese:这篇文章介绍了Sound Demixing Challenge（SDX'23）的音乐分离（MDX）轨迹，包括音乐来源分离（MSS）的Robust Training数据集的设计和两个新的数据集：SDXDB23_LabelNoise和SDXDB23_Bleeding1。文章介绍了在比赛中得分最高的方法，并对上一届音乐分离挑战（Music Demixing Challenge 2021）的赛果进行比较。结果显示，使用标准MSS形式化的最佳系统在MDXDB21上的信号至噪比高于上一届赛事的冠军的赛果。此外，文章还执行了由知名的制作人/音乐人组织的听力测试，以研究系统的主观质量。最后，文章提供了比赛组织和未来版本的前景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.SD_2023_08_14/" data-id="cllurrpb40091sw882901dk8u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/14/cs.LG_2023_08_14/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-14 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/14/eess.IV_2023_08_14/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-14 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
