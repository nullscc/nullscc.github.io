
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-14 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07214 repo_url: None paper_authors: Chiranjeewe">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-14 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/14/eess.IV_2023_08_14/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07214 repo_url: None paper_authors: Chiranjeewe">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-13T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:30.425Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/eess.IV_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-14 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data"></a>Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07214">http://arxiv.org/abs/2308.07214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiranjeewee Prasad Koirala, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这篇论文旨在利用深度学习对多Modalities MRI数据进行脑肿瘤精准分割，以优化在 SUB-SAHARAN AFRICA 患者群体中的诊断和治疗。</li>
<li>methods: 这篇论文提出了一种ensemble方法，包括eleven个不同的变种，基于三种核心架构：UNet3D、ONet3D 和 SphereNet3D，以及修改的损失函数。</li>
<li>results: 研究发现， ensemble方法可以在多Modalities MRI数据上提高脑肿瘤分割精度，特别是在 age-和 population-based 分割模型方面。 Results表明， ensemble方法的 dice分数为 0.82、0.82 和 0.87 分别用于提高脑肿瘤、脑肿瘤核心和全脑肿瘤标签。<details>
<summary>Abstract</summary>
Brain tumors, particularly glioblastoma, continue to challenge medical diagnostics and treatments globally. This paper explores the application of deep learning to multi-modality magnetic resonance imaging (MRI) data for enhanced brain tumor segmentation precision in the Sub-Saharan Africa patient population. We introduce an ensemble method that comprises eleven unique variations based on three core architectures: UNet3D, ONet3D, SphereNet3D and modified loss functions. The study emphasizes the need for both age- and population-based segmentation models, to fully account for the complexities in the brain. Our findings reveal that the ensemble approach, combining different architectures, outperforms single models, leading to improved evaluation metrics. Specifically, the results exhibit Dice scores of 0.82, 0.82, and 0.87 for enhancing tumor, tumor core, and whole tumor labels respectively. These results underline the potential of tailored deep learning techniques in precisely segmenting brain tumors and lay groundwork for future work to fine-tune models and assess performance across different brain regions.
</details>
<details>
<summary>摘要</summary>
脑肿，特别是 glioblastoma，仍然在全球医疗领域面临挑战。这篇论文探讨了深度学习在多Modal magnetic resonance imaging（MRI）数据上进行脑肿分 segmentation的精度提高。我们引入了一个ensemble方法，包括11个独特的变种，基于三个核心体系：UNet3D、ONet3D和SphereNet3D，以及修改的损失函数。该研究强调了需要根据年龄和人口进行分 segmentation模型，以全面考虑脑肿的复杂性。我们的发现表明， ensemble方法，将不同的体系结合起来，表现出了提高评价指标的效果。特别是，结果显示 dice分数为0.82、0.82和0.87，用于加强肿体、肿体核心和整个肿体标签。这些结果高亮了深度学习技术在精度地分 segmentation脑肿的潜在优势，并为未来细化模型和评价不同脑区的表现提供了基础。
</details></li>
</ul>
<hr>
<h2 id="SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation"><a href="#SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation" class="headerlink" title="SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation"></a>SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07156">http://arxiv.org/abs/2308.07156</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren</li>
<li>for: 本研究探讨了Segment Anything Model（SAM）在 роботиче外科中的 robustness和零shot泛化能力。</li>
<li>methods: 本研究使用了SAM模型，并对其进行了多种场景探讨，包括提示和无提示的情况，以及不同的提示方法。</li>
<li>results: 研究发现，SAM模型在提示情况下表现出了很好的零shot泛化能力，但在无提示情况下或者 Instrument部分重叠时，模型很难正确地分类Instrument。此外，模型在复杂的外科手术场景下也表现不佳，尤其是在血液、反射、模糊和阴影等情况下。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) serves as a fundamental model for semantic segmentation and demonstrates remarkable generalization capabilities across a wide range of downstream scenarios. In this empirical study, we examine SAM's robustness and zero-shot generalizability in the field of robotic surgery. We comprehensively explore different scenarios, including prompted and unprompted situations, bounding box and points-based prompt approaches, as well as the ability to generalize under corruptions and perturbations at five severity levels. Additionally, we compare the performance of SAM with state-of-the-art supervised models. We conduct all the experiments with two well-known robotic instrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results reveal that although SAM shows remarkable zero-shot generalization ability with bounding box prompts, it struggles to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrate that the model either failed to predict certain parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as wrong classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, SAM struggles to identify instruments in complex surgical scenarios characterized by the presence of blood, reflection, blur, and shade. Additionally, SAM is insufficiently robust to maintain high performance when subjected to various forms of data corruption. We also attempt to fine-tune SAM using Low-rank Adaptation (LoRA) and propose SurgicalSAM, which shows the capability in class-wise mask prediction without prompt. Therefore, we can argue that, without further domain-specific fine-tuning, SAM is not ready for downstream surgical tasks.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 是一种基本模型 для semantics segmentation，它在各种下游场景中表现出了很好的普适性。在这个实验性研究中，我们研究了SAM在 робо学手术场景中的 robustness和零shot普适性。我们全面探讨了不同的场景，包括提示和无提示的情况，以及 bounding box 和点based提示方法。此外，我们还评估了 SAM 与当前顶尖指导学习模型的性能比较。我们在 MICCAI EndoVis 2017 和 2018 挑战中获得的两个 robotic instrument segmentation 数据集进行了所有的实验。我们的广泛的评估结果表明，虽然 SAM 在 bounding box 提示下显示出了remarkable零shot普适性，但在点based提示和无提示情况下，它很难正确地分类整个工具。此外，我们的资深图示表明，当工具在同一个 bounding box 内或者点based提示情况下，模型会预测错误的部分或者完全错过certain parts of the instrument mask（例如，钩子、臂部）。实际上，SAM 在复杂的外科手术场景中，即血肉泛滥、反射、模糊和抑吸的情况下，也很难分类工具。此外，SAM 对数据损害不具备充分的Robustness，无法保持高性能。为了解决这些问题，我们尝试使用 LoRA 进行微调，并提出了 SurgicalSAM，它可以在无提示情况下进行类别 маска预测。因此，我们可以 argue ，无需进一步领域特定的微调，SAM 不够准备于下游外科任务。
</details></li>
</ul>
<hr>
<h2 id="FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving"><a href="#FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving" class="headerlink" title="FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving"></a>FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07104">http://arxiv.org/abs/2308.07104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhonghuayi/focusflow_official">https://github.com/zhonghuayi/focusflow_official</a></li>
<li>paper_authors: Zhonghua Yi, Hao Shi, Kailun Yang, Qi Jiang, Yaozu Ye, Ze Wang, Kaiwei Wang</li>
<li>for: 提高数据驱动的光流估算精度，特别是关键点方面。</li>
<li>methods: 提出点 clouds模型，并使用混合损失函数和特定点控制损失函数进行多个点精度的监督。 Condition Control Encoder (CCE) 将经典特征编码器替换为 Condition Feature Encoder (CFE)，并将帧特征编码器 (FFE) 与 CFE 进行控制相互传输。</li>
<li>results: 与普通的数据驱动光流估算方法相比，FocusFlow 在关键点方面提高了精度，并且可以与普通的特征编码器进行比较，在整个帧上也能达到类似或更高的性能。<details>
<summary>Abstract</summary>
Key-point-based scene understanding is fundamental for autonomous driving applications. At the same time, optical flow plays an important role in many vision tasks. However, due to the implicit bias of equal attention on all points, classic data-driven optical flow estimation methods yield less satisfactory performance on key points, limiting their implementations in key-point-critical safety-relevant scenarios. To address these issues, we introduce a points-based modeling method that requires the model to learn key-point-related priors explicitly. Based on the modeling method, we present FocusFlow, a framework consisting of 1) a mix loss function combined with a classic photometric loss function and our proposed Conditional Point Control Loss (CPCL) function for diverse point-wise supervision; 2) a conditioned controlling model which substitutes the conventional feature encoder by our proposed Condition Control Encoder (CCE). CCE incorporates a Frame Feature Encoder (FFE) that extracts features from frames, a Condition Feature Encoder (CFE) that learns to control the feature extraction behavior of FFE from input masks containing information of key points, and fusion modules that transfer the controlling information between FFE and CFE. Our FocusFlow framework shows outstanding performance with up to +44.5% precision improvement on various key points such as ORB, SIFT, and even learning-based SiLK, along with exceptional scalability for most existing data-driven optical flow methods like PWC-Net, RAFT, and FlowFormer. Notably, FocusFlow yields competitive or superior performances rivaling the original models on the whole frame. The source code will be available at https://github.com/ZhonghuaYi/FocusFlow_official.
</details>
<details>
<summary>摘要</summary>
“键点基本Scene理解是自动驾驶应用的基础。同时，光流扮演了许多视觉任务中重要的角色。然而，由于预设所有点都受到同等的注意力， класи型数据驱动的光流估计方法对键点的表现不如预期，从而限制它们在键点敏感的安全相关enario中的实现。为解决这些问题，我们介绍了一个点 cloud Modeling 方法，让模型Explicitly learn键点相关的先验知识。基于这个方法，我们发表了FocusFlow框架，包括以下几个部分：1) 一个mix损失函数和类别摄影损失函数以及我们提出的Conditional Point Control Loss (CPCL)函数 для多点精确指导; 2) 一个受控制的模型，将传统的Feature Encoder取代为我们提出的Condition Control Encoder (CCE)。CCE包括Frame Feature Encoder (FFE)、Condition Feature Encoder (CFE) 和融合模块，从输入mask中学习控制FFE的特征提取行为，并将控制信息转移到FFE和CFE之间。我们的FocusFlow框架在不同的键点上显示出惊人的表现，包括ORB、SIFT 和甚至学习式SiLK，并且具有卓越的扩展性，可以与现有的大多数数据驱动的光流方法相容。尤其是，FocusFlow在整幅图上表现竞争或超越原始模型。代码将在https://github.com/ZhonghuaYi/FocusFlow_official中公开。”
</details></li>
</ul>
<hr>
<h2 id="When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation"><a href="#When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation" class="headerlink" title="When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation"></a>When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07093">http://arxiv.org/abs/2308.07093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Zhiyong Wang, Yulin Huang, Junjie Wu, Haiguang Yang, Jianyu Yang</li>
<li>for: 本研究旨在提出一种基于多任务学习的Synthetic Aperture Radar（SAR）自动目标识别（ATR）方法，以实现精准的目标类别和精确的目标形态同时识别。</li>
<li>methods: 该方法基于深度学习理论，提出了一种新的多任务学习框架，包括两个主要结构：编码器和解码器。编码器用于抽取不同缩放级别的图像特征，而解码器则是一个任务特有的结构，通过使用这些抽取的特征进行适应性和优化地满足不同识别和分割任务的特征需求。</li>
<li>results: 基于Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集的实验结果表明，提出的方法在识别和分割任务中具有优越性。<details>
<summary>Abstract</summary>
With the recent advances of deep learning, automatic target recognition (ATR) of synthetic aperture radar (SAR) has achieved superior performance. By not being limited to the target category, the SAR ATR system could benefit from the simultaneous extraction of multifarious target attributes. In this paper, we propose a new multi-task learning approach for SAR ATR, which could obtain the accurate category and precise shape of the targets simultaneously. By introducing deep learning theory into multi-task learning, we first propose a novel multi-task deep learning framework with two main structures: encoder and decoder. The encoder is constructed to extract sufficient image features in different scales for the decoder, while the decoder is a tasks-specific structure which employs these extracted features adaptively and optimally to meet the different feature demands of the recognition and segmentation. Therefore, the proposed framework has the ability to achieve superior recognition and segmentation performance. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, experimental results show the superiority of the proposed framework in terms of recognition and segmentation.
</details>
<details>
<summary>摘要</summary>
Our approach is based on a deep learning framework with two main structures: an encoder and a decoder. The encoder is designed to extract comprehensive image features at multiple scales, while the decoder is a task-specific structure that adaptively and optimally utilizes these features to meet the diverse demands of recognition and segmentation. This allows our framework to achieve superior performance in both recognition and segmentation.Experimental results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset demonstrate the superiority of our proposed framework. With the ability to accurately recognize and precisely segment targets, our approach offers a significant improvement over traditional SAR ATR methods.
</details></li>
</ul>
<hr>
<h2 id="Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks"><a href="#Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks" class="headerlink" title="Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks"></a>Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07003">http://arxiv.org/abs/2308.07003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Fisch, Stefan Zumdick, Carlotta Barkhau, Daniel Emden, Jan Ernsting, Ramona Leenings, Kelvin Sarink, Nils R. Winter, Benjamin Risse, Udo Dannlowski, Tim Hahn</li>
<li>for: 这个论文主要是为了提出一个高精度、快速的Magnetic Resonance Imaging（MRI）数据中的脑部分 segmentation工具，以取代传统的脑部分分类方法。</li>
<li>methods: 这个论文使用了现代的深度学习方法，包括LinkNet的现代UNet架构，在两个阶段预测过程中进行预测。这将提高了脑部分分类的性能，在测验中得到了一个新的州OF-THE-ART性能， median Dice score（DSC）为99.0%，比现有的模型高出2.2%和1.9%。</li>
<li>results: 这个论文的模型可以实现高精度的脑部分分类，Dice score（DSC）高于96.9%，并且更敏感于噪音。此外，这个模型可以将脑部分分类的时间加速到了约10倍，可以在低级硬件上处理一个数据仅需2秒钟。<details>
<summary>Abstract</summary>
Brain extraction in magnetic resonance imaging (MRI) data is an important segmentation step in many neuroimaging preprocessing pipelines. Image segmentation is one of the research fields in which deep learning had the biggest impact in recent years enabling high precision segmentation with minimal compute. Consequently, traditional brain extraction methods are now being replaced by deep learning-based methods. Here, we used a unique dataset comprising 568 T1-weighted (T1w) MR images from 191 different studies in combination with cutting edge deep learning methods to build a fast, high-precision brain extraction tool called deepbet. deepbet uses LinkNet, a modern UNet architecture, in a two stage prediction process. This increases its segmentation performance, setting a novel state-of-the-art performance during cross-validation with a median Dice score (DSC) of 99.0% on unseen datasets, outperforming current state of the art models (DSC = 97.8% and DSC = 97.9%). While current methods are more sensitive to outliers, resulting in Dice scores as low as 76.5%, deepbet manages to achieve a Dice score of > 96.9% for all samples. Finally, our model accelerates brain extraction by a factor of ~10 compared to current methods, enabling the processing of one image in ~2 seconds on low level hardware.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 数据中的脑部EXTRACTION是许多神经成像预处理管道中重要的 segmentation 步骤。图像 segmentation 是深度学习在过去几年中对神经成像领域产生了最大的影响，使得传统的脑部EXTRACTION 方法被深度学习基于的方法所取代。在本文中，我们使用了568张T1-weighted (T1w) MRI图像和 cutting-edge deep learning 方法建立了一个快速、高精度的脑部EXTRACTION 工具 called deepbet。deepbet 使用了 LinkNet，一种现代的 U-Net 架构，在两个阶段预测过程中。这使得它的 segmentation 性能得到了提高，在跨验证中 median Dice 分数 (DSC) 为 99.0%，超过当前的状态对照模型 (DSC = 97.8%和DSC = 97.9%)。而现有方法更敏感于异常值，导致 Dice 分数只有 76.5%，而 deepbet 则能够达到 > 96.9% 的 Dice 分数 для所有样本。最后，我们的模型将脑部EXTRACTION 加速了约10倍，使得一个图像只需要 ~2秒钟的处理时间。
</details></li>
</ul>
<hr>
<h2 id="How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation"><a href="#How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation" class="headerlink" title="How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation"></a>How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06964">http://arxiv.org/abs/2308.06964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parinaz Roshanzamir, Hassan Rivaz, Joshua Ahn, Hamza Mirza, Neda Naghdi, Meagan Anstruther, Michele C. Battié, Maryse Fortin, Yiming Xiao</li>
<li>for: This paper aims to explore the relationship between inter-rater variability and uncertainties in deep learning models for medical image segmentation, and to compare the performance of different label fusion strategies and DL models.</li>
<li>methods: The paper uses test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties, and compares the performance of UNet and TransUNet with two label fusion strategies.</li>
<li>results: The study reveals the interplay between inter-rater variability and uncertainties, and shows that choices of label fusion strategies and DL models can affect the resulting segmentation performance.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是探讨医学影像分割任务中间质量标注人员之间的差异对深度学习模型的不确定性的关系，以及不同的标签汇集策略和深度学习模型的性能比较。</li>
<li>methods: 该论文使用测试时数据增强（TTA）、测试时dropout（TTD）和深度ensemble来测量 aleatoric 和 epistemic 不确定性，并比较 UNet 和 TransUNet 的性能。</li>
<li>results: 研究发现，医学影像分割任务中间质量标注人员之间的差异会影响深度学习模型的不确定性，并且选择不同的标签汇集策略和深度学习模型可以affect segmentation性能。<details>
<summary>Abstract</summary>
Recent developments in deep learning (DL) techniques have led to great performance improvement in medical image segmentation tasks, especially with the latest Transformer model and its variants. While labels from fusing multi-rater manual segmentations are often employed as ideal ground truths in DL model training, inter-rater variability due to factors such as training bias, image noise, and extreme anatomical variability can still affect the performance and uncertainty of the resulting algorithms. Knowledge regarding how inter-rater variability affects the reliability of the resulting DL algorithms, a key element in clinical deployment, can help inform better training data construction and DL models, but has not been explored extensively. In this paper, we measure aleatoric and epistemic uncertainties using test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to explore their relationship with inter-rater variability. Furthermore, we compare UNet and TransUNet to study the impacts of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, affected by choices of label fusion strategies and DL models.
</details>
<details>
<summary>摘要</summary>
In this paper, we use test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties and explore their relationship with inter-rater variability. We also compare UNet and TransUNet to study the impact of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, which is influenced by choices of label fusion strategies and DL models.
</details></li>
</ul>
<hr>
<h2 id="Robustness-Stress-Testing-in-Medical-Image-Classification"><a href="#Robustness-Stress-Testing-in-Medical-Image-Classification" class="headerlink" title="Robustness Stress Testing in Medical Image Classification"></a>Robustness Stress Testing in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06889">http://arxiv.org/abs/2308.06889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobarakol/robustness_stress_testing">https://github.com/mobarakol/robustness_stress_testing</a></li>
<li>paper_authors: Mobarakol Islam, Zeju Li, Ben Glocker</li>
<li>For: 评估医学图像疾病检测算法的 клиниче验证性能。* Methods: 使用进行挑战测试来评估模型的可靠性和不同类型和地区的表现差异。* Results: 表明了一些模型在不同的图像挑战测试中的Robustness和公平性。还发现预训练特征对下游的可靠性产生了重要的影响。<details>
<summary>Abstract</summary>
Deep neural networks have shown impressive performance for image-based disease detection. Performance is commonly evaluated through clinical validation on independent test sets to demonstrate clinically acceptable accuracy. Reporting good performance metrics on test sets, however, is not always a sufficient indication of the generalizability and robustness of an algorithm. In particular, when the test data is drawn from the same distribution as the training data, the iid test set performance can be an unreliable estimate of the accuracy on new data. In this paper, we employ stress testing to assess model robustness and subgroup performance disparities in disease detection models. We design progressive stress testing using five different bidirectional and unidirectional image perturbations with six different severity levels. As a use case, we apply stress tests to measure the robustness of disease detection models for chest X-ray and skin lesion images, and demonstrate the importance of studying class and domain-specific model behaviour. Our experiments indicate that some models may yield more robust and equitable performance than others. We also find that pretraining characteristics play an important role in downstream robustness. We conclude that progressive stress testing is a viable and important tool and should become standard practice in the clinical validation of image-based disease detection models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/eess.IV_2023_08_14/" data-id="cllurrpck00dxsw8859yl5gu9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/14/cs.SD_2023_08_14/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-14 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/13/cs.LG_2023_08_13/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-13 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
