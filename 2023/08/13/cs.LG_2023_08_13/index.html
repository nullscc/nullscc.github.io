
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-13 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Faithful to Whom? Questioning Interpretability Measures in NLP paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06795 repo_url: None paper_authors: Evan Crothers, Herna Viktor, Nathalie Japkowicz for: 这篇论文主要是为了量化">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-13 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/13/cs.LG_2023_08_13/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Faithful to Whom? Questioning Interpretability Measures in NLP paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06795 repo_url: None paper_authors: Evan Crothers, Herna Viktor, Nathalie Japkowicz for: 这篇论文主要是为了量化">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-12T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:29.809Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/cs.LG_2023_08_13/" class="article-date">
  <time datetime="2023-08-12T16:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-13 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP"><a href="#Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP" class="headerlink" title="Faithful to Whom? Questioning Interpretability Measures in NLP"></a>Faithful to Whom? Questioning Interpretability Measures in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06795">http://arxiv.org/abs/2308.06795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Crothers, Herna Viktor, Nathalie Japkowicz</li>
<li>for: 这篇论文主要是为了量化模型解释性的方法。</li>
<li>methods: 这篇论文使用了基于循环屏蔽输入Token的方法来计算 faithfulness 度量。</li>
<li>results: 论文发现现有的 faithfulness 度量不适合比较不同的神经网络文本分类器的解释性，因为屏蔽输入的样本频繁出现在训练时未看到的分布外。<details>
<summary>Abstract</summary>
A common approach to quantifying model interpretability is to calculate faithfulness metrics based on iteratively masking input tokens and measuring how much the predicted label changes as a result. However, we show that such metrics are generally not suitable for comparing the interpretability of different neural text classifiers as the response to masked inputs is highly model-specific. We demonstrate that iterative masking can produce large variation in faithfulness scores between comparable models, and show that masked samples are frequently outside the distribution seen during training. We further investigate the impact of adversarial attacks and adversarial training on faithfulness scores, and demonstrate the relevance of faithfulness measures for analyzing feature salience in text adversarial attacks. Our findings provide new insights into the limitations of current faithfulness metrics and key considerations to utilize them appropriately.
</details>
<details>
<summary>摘要</summary>
一种常见的方法量化模型解释性是计算基于 iteratively 掩码输入Token 的 faithfulness 度量。然而，我们显示这些度量不适合比较不同的神经网络文本分类器的解释性，因为掩码输入的响应是高度特定的。我们示出了 iterative 掩码可以导致大量的 faithfulness 分数变化，并且masked 样本 часто处于训练过程中未见过的分布之外。我们进一步研究了对 faithfulness 度量的影响，以及对文本 adversarial 攻击的分析。我们的发现为现有的 faithfulness 度量带来新的认识和关键考虑因素。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-at-a-Fraction-with-Pruned-Quaternions"><a href="#Neural-Networks-at-a-Fraction-with-Pruned-Quaternions" class="headerlink" title="Neural Networks at a Fraction with Pruned Quaternions"></a>Neural Networks at a Fraction with Pruned Quaternions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06780">http://arxiv.org/abs/2308.06780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smlab-niser/quartLT22">https://github.com/smlab-niser/quartLT22</a></li>
<li>paper_authors: Sahel Mohammad Iqbal, Subhankar Mishra</li>
<li>for: 这个研究旨在测试适用于具有限制性的处理器的现代神经网络，以及使用高维度数据嵌入（如复数或四元数）来实现更好的优化。</li>
<li>methods: 这个研究使用了剪除来简化神经网络中的 Parameters，并在分类任务上进行了不同架构和数据集的实验。</li>
<li>results: 研究发现，在某些架构和任务上，将神经网络转换为复数值的版本可以在具有很高简化水平的情况下提供更高的准确性。例如，在CIFAR-10中使用Conv-4架构，将 Parameters 剪除至3%以下，复数值版本可以与原始模型相比提高超过10%的准确性。<details>
<summary>Abstract</summary>
Contemporary state-of-the-art neural networks have increasingly large numbers of parameters, which prevents their deployment on devices with limited computational power. Pruning is one technique to remove unnecessary weights and reduce resource requirements for training and inference. In addition, for ML tasks where the input data is multi-dimensional, using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy. In this work, we conduct pruning on real and quaternion-valued implementations of different architectures on classification tasks. We find that for some architectures, at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts. For example, at the task of image classification on CIFAR-10 using Conv-4, at $3\%$ of the number of parameters as the original model, the pruned quaternion version outperforms the pruned real by more than $10\%$. Experiments on various network architectures and datasets show that for deployment in extremely resource-constrained environments, a sparse quaternion network might be a better candidate than a real sparse model of similar architecture.
</details>
<details>
<summary>摘要</summary>
现代神经网络的参数数量逐渐增加，这使得具有有限计算能力的设备上进行训练和推理变得困难。剪枝技术可以将不必要的权重从神经网络中移除，以降低训练和推理的资源需求。此外，在多维输入数据的机器学习任务上，使用高维数域嵌入，如复数或四元数，可以降低参数数量而保持准确性。在这种情况下，我们对不同的架构和数据集进行了剪枝和权重融合的实验。我们发现，在某些架构上，在非常高的精灵度水平上，使用四元数模型可以提高准确性，比如在CIFAR-10上使用Conv-4，在3%的参数数量下，剪枝后的四元数模型的准确性高于剪枝后的实数模型的10%以上。各种网络架构和数据集的实验表明，在极其有限的资源环境下，一个稀疏的四元数网络可能比同样的架构的实数稀疏网络更适合进行部署。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations"><a href="#A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations" class="headerlink" title="A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations"></a>A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06767">http://arxiv.org/abs/2308.06767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrcheng1066/awesome-pruning">https://github.com/hrcheng1066/awesome-pruning</a></li>
<li>paper_authors: Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi</li>
<li>for: 本文提供了一份涵盖现有研究工作的深度神经网络减少报告，以便更好地理解现有的方法和技术。</li>
<li>methods: 本文分类了现有的减少方法，包括一般&#x2F;特定速度减少、 WHEN TO PRUNE、HOW TO PRUNE 和减少与其他压缩技术的融合。</li>
<li>results: 本文进行了七对对照设定的比较分析，探讨了不同级别的监督和不同应用场景，以便更好地了解现有方法的共同点和区别。<details>
<summary>Abstract</summary>
Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络，特别是最近的大型语言模型，具有庞大的计算和存储资源需求。为实现资源有限环境中部署现代模型和加速推理时间，研究人员已经不断探索剪裁技术作为神经网络压缩的流行研究方向。然而，目前的相关评论综述缺乏。为解决这问题，在这篇评论中，我们提供了一份全面的评论综述，分为以下四个方面：1) 通用/特定加速，2) 何时剪裁，3) 如何剪裁，和4) 剪裁与其他压缩技术的融合。然后，我们对七对对比设定进行了仔细的比较分析（例如，无结构/结构），并探讨了emerging topics，如后处理剪裁、不同水平的监督剪裁和更广泛的应用（例如，防御性鲁棒性），以抛光现有方法的相似和不同，并为未来的研究铺垫基础。为便于未来的研究，我们建立了一个 curaated 的数据集、网络和评估集。最后，我们提供了一些有价值的建议，包括选择剪裁方法和未来研究方向。我们在 GitHub 上建立了一个存储库，请参考 <https://github.com/hrcheng1066/awesome-pruning>。
</details></li>
</ul>
<hr>
<h2 id="Conic-Descent-Redux-for-Memory-Efficient-Optimization"><a href="#Conic-Descent-Redux-for-Memory-Efficient-Optimization" class="headerlink" title="Conic Descent Redux for Memory-Efficient Optimization"></a>Conic Descent Redux for Memory-Efficient Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07343">http://arxiv.org/abs/2308.07343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingcong Li, Georgios B. Giannakis</li>
<li>for: 该论文旨在提高首项壳programming的效率和精度，并应用于信号处理和机器学习领域。</li>
<li>methods: 该论文提出了三个方面的改进：一是具有直观的几何解释，二是基于对偶问题的理论基础，三是一种新的批处理算法。</li>
<li>results: 研究发现，首项壳programming可以通过增加矩阵的权重来加速对偶解释，并且可以通过采用偏好的初始值来加速搜索过程。<details>
<summary>Abstract</summary>
Conic programming has well-documented merits in a gamut of signal processing and machine learning tasks. This contribution revisits a recently developed first-order conic descent (CD) solver, and advances it in three aspects: intuition, theory, and algorithmic implementation. It is found that CD can afford an intuitive geometric derivation that originates from the dual problem. This opens the door to novel algorithmic designs, with a momentum variant of CD, momentum conic descent (MOCO) exemplified. Diving deeper into the dual behavior CD and MOCO reveals: i) an analytically justified stopping criterion; and, ii) the potential to design preconditioners to speed up dual convergence. Lastly, to scale semidefinite programming (SDP) especially for low-rank solutions, a memory efficient MOCO variant is developed and numerically validated.
</details>
<details>
<summary>摘要</summary>
带有较好的记录的圆形编程在信号处理和机器学习任务中具有良好的优点。本贡献将最近开发的首览圆形下降（CD）解决方案进行三个方面的提高：直观、理论和算法实现。研究发现CD可以从对准问题的 dual 问题中得到直观的几何 derivation，这打开了新的算法设计的门户，例如旋转圆形下降（MOCO）。钻 deeper into CD和MOCO的双重行为，发现：i) 可以分析正确的停止标准；ii) 可以设计加速对准速度的预conditioners。最后，为优化semidefinite程序（SDP），尤其是低维解决方案，我们开发了内存高效的MOCO变体并 NUMERICALLY 验证了其正确性。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Class-incremental-Learning-A-Survey"><a href="#Few-shot-Class-incremental-Learning-A-Survey" class="headerlink" title="Few-shot Class-incremental Learning: A Survey"></a>Few-shot Class-incremental Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06764">http://arxiv.org/abs/2308.06764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghua Zhang, Li Liu, Olli Silven, Matti Pietikäinen, Dewen Hu</li>
<li>for: 这篇论文的目的是为了提供对几拟学习（Few-shot Class-Incremental Learning，FSCIL）的系统性和全面的综述。</li>
<li>methods: 这篇论文使用了多种方法来探讨FSCIL，包括问题定义、主要挑战的不可靠的实验准确风险和稳定性-多样性矛盾、通用方案和相关的增量学习和几拟学习方法。</li>
<li>results: 这篇论文提供了各种FSCIL中的分类方法和对象检测方法，包括数据基于、结构基于和优化基于的方法，以及 anchor-free和 anchor-based的对象检测方法。<details>
<summary>Abstract</summary>
Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in machine learning, as it necessitates the continuous learning of new classes from sparse labeled training samples without forgetting previous knowledge. While this field has seen recent progress, it remains an active area of exploration. This paper aims to provide a comprehensive and systematic review of FSCIL. In our in-depth examination, we delve into various facets of FSCIL, encompassing the problem definition, the discussion of primary challenges of unreliable empirical risk minimization and the stability-plasticity dilemma, general schemes, and relevant problems of incremental learning and few-shot learning. Besides, we offer an overview of benchmark datasets and evaluation metrics. Furthermore, we introduce the classification methods in FSCIL from data-based, structure-based, and optimization-based approaches and the object detection methods in FSCIL from anchor-free and anchor-based approaches. Beyond these, we illuminate several promising research directions within FSCIL that merit further investigation.
</details>
<details>
<summary>摘要</summary>
《几个shot类增长学习（FSCIL）》是机器学习中的一个特殊挑战，它需要不断学习新的类型从罕见的标签训练样本中学习，而不会忘记之前的知识。尽管这个领域已经有了一定的进步，但仍然是一个活跃的研究领域。本文的目的是提供了FSCIL的全面和系统性的综述。在我们的深入检查中，我们探讨了FSCIL的各个方面，包括问题定义、不可靠的实际风险最小化和稳定-柔软之间的矛盾、通用方案和相关的增量学习和几个shot学习问题。此外，我们介绍了FSCIL中的数据集和评价指标。此外，我们还介绍了FSCIL中的分类方法，包括数据基于、结构基于和优化基于的方法，以及对象检测方法，包括固定和无固定的方法。此外，我们还释明了FSCIL中的一些有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining"><a href="#Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining" class="headerlink" title="Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining"></a>Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06763">http://arxiv.org/abs/2308.06763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Dehghani, Zahra Yazdanparast, Mobin Mohammadi</li>
<li>for: 本研究旨在利用关联规则挖掘技术对 COVID-19 患者症状进行分析，以提供临床医生管理疾病的有用信息。</li>
<li>methods: 本研究使用 Apriori 算法进行关联规则挖掘，分析 COVID-19 患者2875条病例记录，并发现最常见的症状为呼吸困难（72%）、咳嗽（64%）、发热（59%）、衰弱（18%）、 мышьяк（14.5%）和喉咙痛（12%）。</li>
<li>results: 本研究发现，Apriori 算法可以帮助临床医生更好地理解 COVID-19 疾病的表现形式，并提供有价值的信息来帮助他们更好地诊断和治疗疾病。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has a devastating impact globally, claiming millions of lives and causing significant social and economic disruptions. In order to optimize decision-making and allocate limited resources, it is essential to identify COVID-19 symptoms and determine the severity of each case. Machine learning algorithms offer a potent tool in the medical field, particularly in mining clinical datasets for useful information and guiding scientific decisions. Association rule mining is a machine learning technique for extracting hidden patterns from data. This paper presents an application of association rule mining based Apriori algorithm to discover symptom patterns from COVID-19 patients. The study, using 2875 records of patient, identified the most common symptoms as apnea (72%), cough (64%), fever (59%), weakness (18%), myalgia (14.5%), and sore throat (12%). The proposed method provides clinicians with valuable insight into disease that can assist them in managing and treating it effectively.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对全球造成了毁灭性的影响，负死亡人数和社会经济秩序受到了重大的影响。为了优化决策和分配有限的资源，必须能够识别 COVID-19 的症状和每个患者的严重程度。机器学习算法在医疗领域提供了一种极其有用的工具，特别是在挖掘医疗数据中找到有用信息并导引科学决策。在本文中，我们使用 Apriori 算法来应用关联规则挖掘技术，从 COVID-19 患者的记录中挖掘症状模式。研究使用了 2875 个病人记录，并发现最常见的症状为呼吸停止（72%）、咳嗽（64%）、发热（59%）、衰竭（18%）、肌肉疼痛（14.5%）和喉咙痛（12%）。该方法为临床医生提供了有价值的病理知识，可以帮助他们更好地诊断和治疗疾病。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization"><a href="#Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization" class="headerlink" title="Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization"></a>Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06741">http://arxiv.org/abs/2308.06741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mehdi Nasiri, Mansoor Rezghi</li>
<li>for: 这个论文旨在解决多Agent Reinforcement Learning（MARL）中协同努力的挑战，其中Agent具有不同能力和个人策略。</li>
<li>methods: 该论文提出了一种基于镜像下降法的多Agent策略优化算法（HAMDPO），利用多Agent优化减少问题中的策略更新，保证总体性能提高。</li>
<li>results: 该论文通过在Multi-Agent MuJoCo和StarCraftII任务上评估HAMDPO算法，并证明其在相比之前的状态静态算法（HATRPO和HAPPO）的稳定性和性能提高。<details>
<summary>Abstract</summary>
This paper presents an extension of the Mirror Descent method to overcome challenges in cooperative Multi-Agent Reinforcement Learning (MARL) settings, where agents have varying abilities and individual policies. The proposed Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm utilizes the multi-agent advantage decomposition lemma to enable efficient policy updates for each agent while ensuring overall performance improvements. By iteratively updating agent policies through an approximate solution of the trust-region problem, HAMDPO guarantees stability and improves performance. Moreover, the HAMDPO algorithm is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. We evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks, demonstrating its superiority over state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文提出了一种新的方法 called Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO)，用于解决合作多代理演算学习（MARL）中的挑战，其中代理有不同的能力和个人策略。HAMDPO算法使用多代理优势分解 Lemma 来有效地更新代理策略，并保证总性的性能提高。通过迭代更新代理策略的近似解决方案，HAMDPO算法保证稳定性和性能提高。此外，该算法可以处理不同类型的动作空间，包括连续和离散动作空间，并在多种 MARL 问题中进行应用。作者们在 Multi-Agent MuJoCo 和 StarCraftII 任务上评估了 HAMDPO 算法，并证明其在当前状态的算法中具有优势，例如 HATRPO 和 HAPPO 等算法。这些结果表明，HAMDPO 是一种有前途的方法，可以解决合作 MARL 问题，并可能扩展到其他难题。
</details></li>
</ul>
<hr>
<h2 id="Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection"><a href="#Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection" class="headerlink" title="Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection"></a>Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06740">http://arxiv.org/abs/2308.06740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenwenmin/wspls">https://github.com/wenwenmin/wspls</a></li>
<li>paper_authors: Wenwen Min, Taosheng Xu, Chris Ding</li>
<li>For: The paper proposes a method for joint sample and feature selection in data fusion using sparse partial least squares (sPLS) with $\ell_\infty&#x2F;\ell_0$-norm constrained weighted sparse PLS (wsPLS) and extends it to multi-view data fusion.* Methods: The proposed method uses the $\ell_\infty&#x2F;\ell_0$-norm constrains to select a subset of samples and the Kurdyka-\L{ojasiewicz}~property to ensure global convergence of the algorithm.* Results: The proposed method is demonstrated to be efficient through numerical and biomedical data experiments, and is shown to outperform traditional sPLS methods in terms of computational efficiency and accuracy.<details>
<summary>Abstract</summary>
Sparse Partial Least Squares (sPLS) is a common dimensionality reduction technique for data fusion, which projects data samples from two views by seeking linear combinations with a small number of variables with the maximum variance. However, sPLS extracts the combinations between two data sets with all data samples so that it cannot detect latent subsets of samples. To extend the application of sPLS by identifying a specific subset of samples and remove outliers, we propose an $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS ($\ell_\infty/\ell_0$-wsPLS) method for joint sample and feature selection, where the $\ell_\infty/\ell_0$-norm constrains are used to select a subset of samples. We prove that the $\ell_\infty/\ell_0$-norm constrains have the Kurdyka-\L{ojasiewicz}~property so that a globally convergent algorithm is developed to solve it. Moreover, multi-view data with a same set of samples can be available in various real problems. To this end, we extend the $\ell_\infty/\ell_0$-wsPLS model and propose two multi-view wsPLS models for multi-view data fusion. We develop an efficient iterative algorithm for each multi-view wsPLS model and show its convergence property. As well as numerical and biomedical data experiments demonstrate the efficiency of the proposed methods.
</details>
<details>
<summary>摘要</summary>
“稀疏部分最小倍数（sPLS）是一种常见的维度减少技术 для数据融合，它通过寻找两个视图中数据样本之间的线性组合来降低维度。然而，sPLS会捕捉两个数据集中所有数据样本的组合，因此无法检测隐藏的样本subset。为了扩展sPLS的应用，我们提出了一种使用 $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS（$\ell_\infty/\ell_0$-wsPLS）方法，该方法可以同时进行样本选择和特征选择。我们证明了 $\ell_\infty/\ell_0$-norm constrains possess Kurdyka-\L{ojasiewicz} 性质，因此可以开发一个全球收敛的算法来解决它。此外，多视图数据中可以有同一个样本集。为此，我们扩展了 $\ell_\infty/\ell_0$-wsPLS 模型，并提出了两种多视图 wsPLS 模型 для多视图数据融合。我们开发了一个高效的迭代算法，并证明其收敛性。numerical和生物医学数据实验表明提出的方法的效率。”Note: Simplified Chinese is a simplified version of Chinese that is used in mainland China and is different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data"><a href="#Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data" class="headerlink" title="Probabilistic Imputation for Time-series Classification with Missing Data"></a>Probabilistic Imputation for Time-series Classification with Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06738">http://arxiv.org/abs/2308.06738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout">https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout</a></li>
<li>paper_authors: SeungHyun Kim, Hyunsu Kim, EungGu Yun, Hwangrae Lee, Jaehun Lee, Juho Lee</li>
<li>for: 用于处理多变量时间序列数据中的缺失值</li>
<li>methods: 使用深度生成模型进行缺失值填充，并使用分类器进行信号分类</li>
<li>results: 通过实验表明，该方法可以有效地处理多变量时间序列数据中的缺失值，并且可以提高信号分类的准确率<details>
<summary>Abstract</summary>
Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that na\"ively combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
多变量时间序列数据在实际应用中通常含有较大的缺失值。现有的主流方法为这种缺失值是单纯地假设缺失值为零、平均值或相邻时间步的值。然而，这些简单策略并不考虑数据生成过程，更重要的是，它们不能有效地捕捉预测中的不确定性，因为缺失值有多个可能性。在这篇论文中，我们提出了一种新的概率 frameworks for classification with multivariate time series data with missing values。我们的模型包括两部分：深度生成模型和分类器。 extending the existing deep generative models to better capture the structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation。分类器部分接受时间序列数据和假设的缺失值，并将时间序列数据分类，并且是通过捕捉多个可能性的假设来捕捉预测中的不确定性。然而，我们发现在直接组合生成模型和分类器时，可能会导致生成模型不生成有用的假设值，从而影响分类的准确性。为了解决这问题，我们提出了一种新的正则化技术，可以推动模型生成有用的假设值，以便于分类。通过对实际时间序列数据进行广泛的实验，我们证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Precipitation-nowcasting-with-generative-diffusion-models"><a href="#Precipitation-nowcasting-with-generative-diffusion-models" class="headerlink" title="Precipitation nowcasting with generative diffusion models"></a>Precipitation nowcasting with generative diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06733">http://arxiv.org/abs/2308.06733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models">https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models</a></li>
<li>paper_authors: Andrea Asperti, Fabio Merizzi, Alberto Paparella, Giorgio Pedrazzi, Matteo Angelinelli, Stefano Colamonaco</li>
<li>for: 本研究使用数字天气预测方法进行气象预测，旨在提高预测的准确性和可靠性。</li>
<li>methods: 本研究使用了泛化激发模型（Diffusion Model）来处理气象预测任务，并通过将多个激发模型 ensemble起来，使用post处理网络来组合可能的天气场景，以获得最终的可能性预测结果。</li>
<li>results: 对于中欧2016-2021年的 hourly 数据，与已有的U-Net模型相比，泛化激发模型在气象预测任务中表现出了明显的优势，并且可以substantially 提高总体性能。<details>
<summary>Abstract</summary>
In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely prediction.   In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data pertaining to Central Europe from the years 2016 to 2021. Within this context, we examine the efficacy of diffusion models in handling the task of precipitation nowcasting. Our work is conducted in comparison to the performance of well-established U-Net models, as documented in the existing literature. Our proposed approach of Generative Ensemble Diffusion (GED) utilizes a diffusion model to generate a set of possible weather scenarios which are then amalgamated into a probable prediction via the use of a post-processing network. This approach, in comparison to recent deep learning models, substantially outperformed them in terms of overall performance.
</details>
<details>
<summary>摘要</summary>
近年来，传统的数学方法 для准确的天气预测遭受了深度学习方法的挑战。历史数据库用于短距离和中距离天气预测通常按照一定的正方形格结构进行组织。这种设置与图像非常相似，每个天气变量都可以视为地图或者在考虑时间轴时视为视频。许多类型的生成模型，包括生成对抗网络、自适应变换器和最近的干扰扩散模型，在下一帧预测问题上都有广泛的应用，因此在天气预测标准benchmark上进行测试是自然的。干扩散模型在这个上特别有吸引力，因为天气预测的本质是 probabilistic 的：我们真正 interesseted 的是天气指标的概率分布，而这个分布的期望值是最有可能的预测。在我们的研究中，我们关注了ERA-5数据集的一个特定子集，包括2016年至2021年中欧每小时的数据。在这个上下文中，我们研究了干扩散模型在降水预测中的效果。我们的方法与文献中已有的U-Net模型相比，并使用生成ensemble扩散（GED）模型，该模型使用干扩散模型生成一系列可能的天气场景，然后通过使用后处理网络将这些场景融合成一个可能的预测。与最新的深度学习模型相比，我们的方法在总性表现上substantially outperform了它们。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables"><a href="#Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables" class="headerlink" title="Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables"></a>Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06718">http://arxiv.org/abs/2308.06718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Xie, Biwei Huang, Zhengming Chen, Ruichu Cai, Clark Glymour, Zhi Geng, Kun Zhang</li>
<li>for: 学习 causal structure 中存在 latent variables 的挑战任务 (learning causal structure with latent variables)</li>
<li>methods: 提出 Generalized Independent Noise (GIN) 条件，用于 Linear non-Gaussian acyclic causal models 中包含 latent variables (propose a GIN condition for linear non-Gaussian acyclic causal models with latent variables)</li>
<li>results: 提供 necessary and sufficient graphical criteria of the GIN condition, 并可以快速 estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs) (provide necessary and sufficient graphical criteria of the GIN condition and can efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models)<details>
<summary>Abstract</summary>
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\mathcal{S}$ relative to the parent set of $\mathbf{Y}$ (w.r.t. the causal ordering), such that $\mathcal{S}$ d-separates $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们研究一个复杂的任务：在含有隐变量的情况下学习 causal structure。包括找到隐变量的位置和它们的数量，以及确定隐变量和观测变量之间的 causal 关系。为解决这个问题，我们提议一种 Generalized Independent Noise (GIN) 条件，该条件在 linear non-Gaussian acyclic causal models 中包含隐变量，并且确定了某些观测变量的线性组合和其他观测变量的独立性。具体来说，对两个观测变量 $\mathbf{Y}$ 和 $\mathbf{Z}$，GIN 条件成立只要 $\omega^\intercal \mathbf{Y}$ 和 $\mathbf{Z}$ 独立，其中 $\omega$ 是由 $\mathbf{Y}$ 和 $\mathbf{Z}$ 之间的叠加协方差确定的非零参数向量。然后，我们给出了 linear non-Gaussian acyclic causal models 中 GIN 条件的必要和 suficient 图形 критери产。总之，GIN 条件等价于隐变量集 $\mathcal{S}$ 对于 $\mathbf{Y}$ 的父集（根据 causal 顺序）是独立的，并且 $\mathcal{S}$  separates $\mathbf{Y}$ 和 $\mathbf{Z}$。具体来说，如果没有干扰者，则 causal 关系可以看作一种特殊情况。我们还发现，GIN 条件和隐变量的 causal 结构之间存在着直接的关系。因此，我们可以通过 GIN 条件和一种合理的搜索过程来效率地估计 Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs)，其中隐变量可能也有 causal 关系，并且可能会 suivre 一种层次结构。我们证明了 LiNGLaHs 的下面 causal 结构是可Identifiable的，只要 GIN 条件 在轻量级的假设下成立。实验结果表明了我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards"><a href="#Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards" class="headerlink" title="Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards"></a>Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06717">http://arxiv.org/abs/2308.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilgin Dogan, Zuo-Jun Max Shen, Anil Aswani</li>
<li>For: This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal, with the goal of addressing the challenges of information asymmetry and incentive design in real-life scenarios.* Methods: The paper introduces a novel estimator that uses the history of principal’s incentives and agent’s choices to estimate the agent’s unknown rewards, and proposes a data-driven incentive policy within a multi-armed bandit (MAB) framework.* Results: The paper proves the finite-sample consistency of the estimator and a rigorous regret bound for the principal, and provides simulations to demonstrate the applicability of the framework to green energy aggregator contracts.<details>
<summary>Abstract</summary>
In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-off between consistently estimating the agent's unknown rewards and maximizing their own utility by offering adaptive incentives to lead the agent. For a non-parametric model, we introduce an estimator whose only input is the history of principal's incentives and agent's choices. We unite this estimator with a proposed data-driven incentive policy within a MAB framework. Without restricting the type of the agent's algorithm, we prove finite-sample consistency of the estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent. Lastly, our theoretical results are reinforced by simulations justifying applicability of our framework to green energy aggregator contracts.
</details>
<details>
<summary>摘要</summary>
在实践中，奖励提供者（即主体）通常无法观察奖励的实现，这与许多主体-代理模型不同。这种信息不对称性使得主体在 solely 观察代理人的决策后难以一直估计代理人的未知奖励。这种复杂的设定在各种实际场景中出现，如可再生能源存储合同和个性化奖励。因此，它不仅提出了有趣的理论问题，还具有广泛的实际 relevance。本文研究了一个 repeat 的反对选择游戏，在这个游戏中，一个自私的学习代理人和一个学习主体之间发生对抗。代理人面临一个多重武器bandit（MAB）问题，以最大化他们的预期奖励加上奖励。主体则在代理人学习的同时，训练一个并行的算法，面临着在估计代理人的未知奖励和自己的利用之间的负担。为了不假设代理人的算法类型，我们引入了一个仅基于奖励提供者历史和代理人选择的估计器。我们将这个估计器与一个基于 MAB 框架的数据驱动奖励策略结合。我们证明了该估计器在非 Parametric 模型下的finite-sample consistency和主体的正确偏误 bound。最后，我们通过实验证明了我们的框架在绿色能源聚合合同中的适用性。
</details></li>
</ul>
<hr>
<h2 id="CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation"><a href="#CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation" class="headerlink" title="CDR: Conservative Doubly Robust Learning for Debiased Recommendation"></a>CDR: Conservative Doubly Robust Learning for Debiased Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08461">http://arxiv.org/abs/2308.08461</a></li>
<li>repo_url: None</li>
<li>paper_authors: ZiJie Song, JiaWei Chen, Sheng Zhou, QiHao Shi, Yan Feng, Chun Chen, Can Wang</li>
<li>For: 本研究旨在解决推荐系统中的偏见问题，提高推荐系统的准确性和可靠性。* Methods: 本研究使用了双重Robust学习策略（DR），并提出了一种名为保守双重Robust策略（CDR）来解决偏见问题。* Results: 实验结果表明，CDR可以显著提高推荐系统的性能，同时减少偏见的频率。<details>
<summary>Abstract</summary>
In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.   To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
</details>
<details>
<summary>摘要</summary>
在推荐系统（RS）中，用户行为数据是观察性的而不是实验性的，导致数据中存在广泛的偏见。因此，解决偏见问题成为了推荐系统领域的主要挑战。近些年来，双重稳健学习（DR）已经受到了广泛关注，因为它在性能和稳健性方面表现出色。然而，我们的实验结果表明，现有的DR方法受到了叫做“毒补假设”的问题的影响，即假设的插入数据与事实有很大差异，导致计算结果变得对抗性很强。为解决这个问题，本文提出了保守的双重稳健策略（CDR），通过检验插入数据的均值和方差来筛选掉不符合事实的插入数据。理论分析表明，CDR可以降低方差并提高尾部上限。此外，我们的实验调查表明，CDR可以显著提高性能，并且可以减少毒补假设的频率。
</details></li>
</ul>
<hr>
<h2 id="Learning-on-Graphs-with-Out-of-Distribution-Nodes"><a href="#Learning-on-Graphs-with-Out-of-Distribution-Nodes" class="headerlink" title="Learning on Graphs with Out-of-Distribution Nodes"></a>Learning on Graphs with Out-of-Distribution Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06714">http://arxiv.org/abs/2308.06714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songyyyy/kdd22-oodgat">https://github.com/songyyyy/kdd22-oodgat</a></li>
<li>paper_authors: Yu Song, Donglin Wang</li>
<li>for: 本文旨在解决图像学中存在非标准节点的问题，特别是检测图像中的异常节点和分类其他节点为已知类别。</li>
<li>methods: 本文提出了一种基于图像注意力网络的异常检测方法，称为Out-of-Distribution Graph Attention Network (OODGAT)，它通过显式地模型不同类型节点之间的交互来分离异常节点和正常节点。</li>
<li>results: EXTENSIVE EXPERIMENTS表明，OODGAT在异常检测方面比现有方法大幅提高，而且与已知类别分类中的性能相当或更好。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT), a novel GNN model which explicitly models the interaction between different kinds of nodes and separate inliers from outliers during feature propagation. Extensive experiments show that OODGAT outperforms existing outlier detection methods by a large margin, while being better or comparable in terms of in-distribution classification.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 是现代图学习模型中的状态机器。而现有的 GNN 模型在许多图学习任务中表现出色，但是对于图中存在异常节点（OOD）的情况却很少受到关注。基于 CV 和 NLP 中的概念，我们定义 OOD 节点为训练集中未见过的标签。由于现实世界中的图 oftentimes 是自动生成的，因此真实的图可能会包含来自未知分布的节点。在这工作中，我们定义了图学习中的 OOD 节点问题。特别是，我们希望完成两个任务：1）检测图中不属于已知分布的节点，2）将剩下的节点分类为已知类别之一。我们发现图中的连接模式是有用的异常检测信息，并提出了 Out-of-Distribution Graph Attention Network (OODGAT)，一种新的 GNN 模型，可以在特征传播过程中分离异常节点和正常节点。我们的实验表明，OODGAT 在异常检测方面超过现有的方法，而且在已知分布下的分类性能也不丢下。
</details></li>
</ul>
<hr>
<h2 id="The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems"><a href="#The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems" class="headerlink" title="The Hard-Constraint PINNs for Interface Optimal Control Problems"></a>The Hard-Constraint PINNs for Interface Optimal Control Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06709">http://arxiv.org/abs/2308.06709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tianyouzeng/pinns-interface-optimal-control">https://github.com/tianyouzeng/pinns-interface-optimal-control</a></li>
<li>paper_authors: Ming-Chih Lai, Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng</li>
<li>for: 解决部分泛函方程（PDEs）中的优化控制问题，包括界面和一些控制约束。</li>
<li>methods: 使用物理学 informed neural networks（PINNs）和最近开发的缺陷捕捉神经网络，解决 mesh-free 和可扩展的 PDEs 优化控制问题。</li>
<li>results: 提出了一种具有硬约束的 PINNs 方法，可以 garantuee 精确满足界面和边界条件，并且可以分离学习 PDEs 和约束。其效率在一些椭球和Parabolic 界面优化控制问题中得到了承诺。<details>
<summary>Abstract</summary>
We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a novel neural network architecture. The resulting hard-constraint PINNs approach guarantees that both the boundary and interface conditions can be satisfied exactly and they are decoupled from the learning of the PDEs. Its efficiency is promisingly validated by some elliptic and parabolic interface optimal control problems.
</details>
<details>
<summary>摘要</summary>
我们证明，用物理知识整合的神经网络（PINNs），可以与最近开发的阶段错误捕捉神经网络（DCNNs）一起解决受到部分数据方程式（PDEs）的最佳控制问题。这个算法是不含网点的和可扩展到不同的PDEs，并且确保控制限制。由于边界和界面条件，以及PDEs，都是转化为转量的loss函数中的软类似，因此需要同时学习它们。但是，这会导致调整这些类似的变量的问题和训练神经网络的问题。为了解决这些问题和保证数据精度，我们提议将边界和界面条件转化为硬类似的PINNs架构。这个方法可以保证边界和界面条件能够精确地满足，并且与PDEs的学习分开。我们在一些elliptic和parabolic interface最佳控制问题中调查了这个方法的效率，结果显示了承认的效果。
</details></li>
</ul>
<hr>
<h2 id="Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model"><a href="#Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model" class="headerlink" title="Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model"></a>Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06708">http://arxiv.org/abs/2308.06708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yasahi-hpc/generative-enkf">https://github.com/yasahi-hpc/generative-enkf</a></li>
<li>paper_authors: Yuuichi Asahi, Yuta Hasegawa, Naoyuki Onodera, Takashi Shimokawabe, Hayato Shiba, Yasuhiro Idomura</li>
<li>for: 本文提出了一种 ensemble data assimilation 方法，使用 pseudo ensemble 生成的推 diffusion  probabilistic model。</li>
<li>methods: 本方法使用 trained 模型生成不寻常的 ensemble，通过 variance 在 ensemble 中的差异来提高数据融合的性能。</li>
<li>results: 对比传统 ensemble data assimilation 方法，本方法在 simulation model 不完美的情况下显示更好的性能。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper presents an ensemble data assimilation method using the pseudo ensembles generated by denoising diffusion probabilistic model. Since the model is trained against noisy and sparse observation data, this model can produce divergent ensembles close to observations. Thanks to the variance in generated ensembles, our proposed method displays better performance than the well-established ensemble data assimilation method when the simulation model is imperfect.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种ensemble数据融合方法，使用 Pseudo Ensemble 生成的随机模型。由于模型在噪声和缺失观测数据上训练，因此可以生成具有较高差异的 ensemble，使得我们的提议方法在模型不完美的情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods"><a href="#Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods" class="headerlink" title="Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods"></a>Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06703">http://arxiv.org/abs/2308.06703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avery Ma, Yangchen Pan, Amir-massoud Farahmand</li>
<li>For: 这个论文的目的是研究权重迭代法和适应性权重迭代法在训练深度神经网络时的表现。* Methods: 这个论文使用了随机梯度下降（SGD）和适应性梯度下降（Adam、RMSProp）等方法来训练深度神经网络。* Results: 研究发现，使用SGD训练的神经网络比使用适应性梯度下降方法更具robustness，并且在输入变化时表现更好。此外，研究还发现了自然 dataset 中的无关频率，并证明了使用适应性梯度下降方法可能会导致模型对这些变化敏感。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risks close to zero but vary in their adversarial risks. Our result shows that linear models' robustness to $\ell_2$-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness. In the context of deep learning, our experiments show that SGD-trained neural networks show smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods.
</details>
<details>
<summary>摘要</summary>
Stochastic gradient descent (SGD) 和 adaptive gradient 方法，如 Adam 和 RMSProp，在深度神经网络训练中广泛应用。我们实验显示，虽然这些方法的标准化预测性表现相似，但SGD 训练的模型在输入扰动下的Robustness 表现强度远胜 adaptive 方法。尤其是，我们的调查发现自然数据集中存在无关频率，其变化不会影响模型的预测性表现。然而，使用 adaptive 方法训练的模型对这些变化具有敏感性，表明它们使用无关频率可能会导致敏感解决方案。为了更好地理解这种差异，我们研究了梯度 descend (GD) 和签名梯度 descend (signGD) 在人工数据集上的学习动态。在三维输入空间中，使用 GD 和 signGD 优化的模型具有标准风险几乎为零，但它们在 $\ell_2$-norm 约束的变化下的风险异常大。我们的结果表明，线性模型对 $\ell_2$-norm 约束的变化的Robustness 与模型参数的权重 нор呈反比关系：小权重 нор呈指示更好的Robustness。在深度学习中，我们的实验表明，SGD 训练的神经网络显示更小的 Lipschitz 常数，解释它们在输入扰动下的更好的Robustness 比 adaptive 方法训练的模型。
</details></li>
</ul>
<hr>
<h2 id="Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection"><a href="#Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection" class="headerlink" title="Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection"></a>Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06701">http://arxiv.org/abs/2308.06701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haichao Zhang, Can Qin, Yu Yin, Yun Fu</li>
<li>for: 提高深度学习模型检测掩蔽物的能力</li>
<li>methods: 使用生成模型生成真实的掩蔽图像，用于训练现有的物体检测模型</li>
<li>results: 在三个数据集（COD10k、CAMO和CHAMELEON）上表现出色，超过当前状态的方法表现，证明了该方法的有效性。<details>
<summary>Abstract</summary>
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>隐形对象在自然场景中摒除成为深度学习模型检测和生成的挑战。隐形对象检测是计算机视觉中重要的应用领域之一，但这一研究领域受到有限的数据可用性的限制。我们提出了一种框架，用于增强自然场景中隐形对象的检测。我们的方法使用生成模型生成真实的隐形图像，这些图像可以用来训练现有的对象检测模型。我们的框架在三个数据集（COD10k、CAMO和CHAMELEON）上表现出比前一个状态的方法更高的性能，这说明了我们的方法的有效性。这种方法可以作为现有隐形对象检测任务的数据生成和扩展模块，并提供一种新的多样性和分布的引入方式，以提高当前的隐形图像数据集。
</details></li>
</ul>
<hr>
<h2 id="SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency"><a href="#SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency" class="headerlink" title="SimMatchV2: Semi-Supervised Learning with Graph Consistency"></a>SimMatchV2: Semi-Supervised Learning with Graph Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06692">http://arxiv.org/abs/2308.06692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingkai-zheng/simmatchv2">https://github.com/mingkai-zheng/simmatchv2</a></li>
<li>paper_authors: Mingkai Zheng, Shan You, Lang Huang, Chen Luo, Fei Wang, Chen Qian, Chang Xu</li>
<li>for: This paper is written for the purpose of proposing a new semi-supervised learning algorithm called SimMatchV2, which can improve the performance of image classification tasks with limited labeled data.</li>
<li>methods: The SimMatchV2 algorithm uses a graph-based approach to formulate various consistencies between labeled and unlabeled data, and utilizes a message passing mechanism to improve the performance of the algorithm.</li>
<li>results: The paper reports that SimMatchV2 achieves state-of-the-art performance on multiple semi-supervised learning benchmarks, with Top-1 Accuracy of 71.9% and 76.2% on ImageNet using 1% and 10% labeled examples, respectively.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍一种新的半监督学习算法SimMatchV2，可以在有限的标注数据的情况下提高图像分类任务的性能。</li>
<li>methods: SimMatchV2算法使用图表的方式来定义各种半监督数据之间的一致性，并利用消息传递机制来提高算法的性能。</li>
<li>results: 论文报告SimMatchV2在多个半监督学习benchmark上达到了状态的最佳性能，ImageNet上使用1%和10%标注样本时，Top-1准确率分别达到71.9%和76.2%。<details>
<summary>Abstract</summary>
Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2. Our SimMatchV2 has been validated on multiple semi-supervised learning benchmarks. Notably, with ResNet-50 as our backbone and 300 epochs of training, SimMatchV2 achieves 71.9\% and 76.2\% Top-1 Accuracy with 1\% and 10\% labeled examples on ImageNet, which significantly outperforms the previous methods and achieves state-of-the-art performance. Code and pre-trained models are available at \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2}.
</details>
<details>
<summary>摘要</summary>
“半支持学习图像分类是计算机视觉领域中最基本的问题之一，它可以大幅减少人工劳动。在这篇论文中，我们介绍了一种新的半支持学习算法——SimMatchV2，它通过图形视角来形式化各种一致性 regularization。在SimMatchV2中，我们将每个样本的扩展视图视为一个节点，该节点包含标签和其对应的表示。不同的节点之间连接起来，这些连接由节点表示的相似度来度量。受图形理论中的消息传递和节点分类的启发，我们提出了四种一致性，namely 1) 节点-节点一致性、2) 节点-边一致性、3) 边-边一致性、4) 边-节点一致性。我们还发现，一个简单的特征 нормализа可以大幅减少不同扩展视图之间的特征 нор值差距，从而显著提高 SimMatchV2 的性能。我们的 SimMatchV2 在多个半支持学习 benchmark 上进行验证，与 ResNet-50 作为 backing 和 300  epoch 训练，SimMatchV2 在 ImageNet 上 achiev 71.9% 和 76.2% Top-1 Accuracy with 1% 和 10% 标注样本，在前一些方法中显著超越，实现了状态的杰出性。代码和预训练模型可以在 \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2} 上获取。”
</details></li>
</ul>
<hr>
<h2 id="MDB-Interactively-Querying-Datasets-and-Models"><a href="#MDB-Interactively-Querying-Datasets-and-Models" class="headerlink" title="MDB: Interactively Querying Datasets and Models"></a>MDB: Interactively Querying Datasets and Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06686">http://arxiv.org/abs/2308.06686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaditya Naik, Adam Stein, Yinjun Wu, Eric Wong, Mayur Naik</li>
<li>for: 本文为了帮助开发人员系统地调试机器学习模型中出现的错误。</li>
<li>methods: 本文使用了函数编程和关系代数来构建对数据集和模型预测数据进行表达的查询框架。</li>
<li>results: 我们的实验表明，使用MDB可以比其他基elines快速40%短 queries，并且开发人员可以成功地构建复杂的查询来描述机器学习模型的错误。<details>
<summary>Abstract</summary>
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
</details>
<details>
<summary>摘要</summary>
developers 需要可以系统地调试机器学习管道中出现的错误。我们提出了MDB，一个用于交互式查询数据集和模型的调试框架。MDB将函数编程与关系代数结合起来，以建立表达性的查询数据集和模型预测中的问题。查询可重用和容易修改，允许调试者快速 iterate和细化查询，以描述和Characterize错误和模型行为。我们在对自动驾驶视频、大语言模型和医疗记录进行对象检测、偏见探测、图像分类和数据填充任务中进行了实验，结果显示MDB可以提高查询速度和查询长度，相比于其他基elines。在用户研究中，我们发现开发者可以成功地构建复杂的查询，以描述机器学习模型的错误。
</details></li>
</ul>
<hr>
<h2 id="Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations"><a href="#Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations" class="headerlink" title="Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations"></a>Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06679">http://arxiv.org/abs/2308.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Xing, Jianqiao Sun<br>for: 用于 tri-variate function approximations 和 complex geometry 函数近似methods: 使用 Separable Gaussian Neural Network (SGNN)，利用 Gaussian 函数的分离性，将输入数据拆分成多列并在平行层中逐步 feeding them into uni-variate Gaussian functionsresults: 与 GRBFNN 相比，SGNN 可以实现100倍减少计算时间，同时保持相同精度水平，并且在approximating 函数 with complex geometry 方面可以达到三个数量级更高的精度。同时，SGNN 也比 DNNs with RuLU 和 Sigmoid 函数更易于调试和优化。<details>
<summary>Abstract</summary>
The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GRBFNN on tri-variate function approximations. The SGNN also has better trainability and is more tuning-friendly than DNNs with RuLU and Sigmoid functions. For approximating functions with complex geometry, SGNN can lead to three orders of magnitude more accurate results than a RuLU-DNN with twice the number of layers and the number of neurons per layer.
</details>
<details>
<summary>摘要</summary>
Gaussian-radial-basis function neural network (GRBFNN) 是一种广泛使用的插值和分类方法。然而，当输入向量维度高时，GRBFNN 的计算复杂性会增加很多。为了解决这个问题，我们提出了一个新的前向网络 - Separable Gaussian Neural Network (SGNN)，通过利用 Gaussian 函数的分离性，将输入数据分成多列，然后将其顺序地输入到由单变量 Gaussian 函数所组成的平行层中。这样的结构可以将 GRBFNN 的neuron 数量由 O(N^d) 降至 O(dN)，从而将 computacional speed 加速到 exponentially ，并且让 SGNN 在输入维度增加时阶段性地提高。此外，SGNN 可以保留 GRBFNN 的主对角线 Hessian 矩阵的主对角线，使得在梯度下降训练中可以达到相似的精度水准。实验表明，SGNN 可以在 tri-variate 函数插值中实现 100 倍的速度提升，并且保持相似的精度水准。此外，SGNN 的训练性和适配性比 DNNs  WITH RuLU 和 Sigmoid 函数更好。当插值函数具有复杂的几何结构时，SGNN 可以实现三倍的精度提升。
</details></li>
</ul>
<hr>
<h2 id="A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks"><a href="#A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks" class="headerlink" title="A deep learning framework for multi-scale models based on physics-informed neural networks"></a>A deep learning framework for multi-scale models based on physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06672">http://arxiv.org/abs/2308.06672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Wang, Yanzhong Yao, Jiawei Guo, Zhiming Gao</li>
<li>for: 解决多Scale问题</li>
<li>methods: 修改损失函数，对不同级别的损失项应用不同数量的Power运算，使损失函数中各个损失项的级别相近</li>
<li>results: 能同时优化不同级别的损失项，扩展PINN的应用范围<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdomains. The proposed method enables loss terms with different magnitudes to be optimized simultaneously, and it advances the application of PINN for multi-scale problems.
</details>
<details>
<summary>摘要</summary>
物理学 informed neural networks (PINN) combine deep neural networks 与解决 partial differential equations (PDEs) 的解，创造了一个新的研究领域，用于数值解决 PDEs。面临多个尺度问题，其中loss function中的损失项有不同的量级，标准 PINN 方法难以获得可靠预测。在这篇论文中，我们提出了一种新的多尺度问题解决框架。该框架基于标准 PINN 方法，并对不同量级的损失项进行不同数量的幂运算，使得各个损失项组成的损失函数具有相似的量级。此外，我们还提出了一种分组常数化策略，可以有效地处理不同子区域中变化很大的问题。该方法可以同时优化不同量级的损失项，并推动 PINN 在多尺度问题上的应用。
</details></li>
</ul>
<hr>
<h2 id="Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent"><a href="#Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent" class="headerlink" title="Law of Balance and Stationary Distribution of Stochastic Gradient Descent"></a>Law of Balance and Stationary Distribution of Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06671">http://arxiv.org/abs/2308.06671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Ziyin, Hongchao Li, Masahito Ueda</li>
<li>for: 本文研究了Stochastic Gradient Descent（SGD）算法如何训练神经网络，特别是SGD在神经网络的高维和潜在不稳定的损失函数空间中如何导航。</li>
<li>methods: 本文使用了Symmetry的概念来研究SGD的训练过程，并证明了SGD在损失函数包含Symmetry时可以减轻损失函数的不稳定性。</li>
<li>results: 本文研究发现，SGD在深度和宽度具有某些特定的Symmetry时可以导致神经网络的站点分布具有复杂非线性现象，如相转化、破碎Ergodicity和强制转换。这些现象只存在于深度具有某些特定Symmetry的神经网络中，这表明了深度和浅度模型之间的基本区别。<details>
<summary>Abstract</summary>
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details>
<details>
<summary>摘要</summary>
SGD算法是我们用来训练神经网络的算法，但是它在神经网络的高度不对称和缺失散射的损失函数空间中 Navigation remains poorly understood. In this work, we prove that SGD的小批量噪声规范化解决方案带有批处理的散射过程，当损失函数具有扩大Symmetry时。由于在对称性存在时，SGD动力学与批处理的差异最大，我们的理论表明损失函数的对称性是SGD工作的重要检验。我们 then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges"><a href="#Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges" class="headerlink" title="Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges"></a>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06668">http://arxiv.org/abs/2308.06668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajiali04/agriculture-foundation-models">https://github.com/jiajiali04/agriculture-foundation-models</a></li>
<li>paper_authors: Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhaojian Li</li>
<li>for: 这项研究旨在探索基于 Machine Learning 和 Deep Learning 的智能农业领域中的Foundation Models（基础模型）的潜力。</li>
<li>methods: 我们首先回顾了最新的基础模型在通用计算机科学领域，并将它们分为四类：语言基础模型、视觉基础模型、多modal基础模型以及强化学习基础模型。然后，我们详细介绍了在农业领域开发基础模型的过程，以及它们在智能农业中的潜在应用。</li>
<li>results: 通过本研究，我们对智能农业领域中基础模型的应用提出了新的研究方向，并提供了一个概念工具和技术背景来促进理解问题空间和探索新的研究方向。此外，我们还讨论了在开发基础模型时存在的独特挑战，包括模型训练、验证和部署。通过这项研究，我们对农业 AI 系统的发展作出了贡献，并介绍了基础模型作为一种可能地减少大量标注数据的潜在解决方案。<details>
<summary>Abstract</summary>
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, we first review recent FMs in the general computer science domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, we outline the process of developing agriculture FMs and discuss their potential applications in smart agriculture. We also discuss the unique challenges associated with developing AFMs, including model training, validation, and deployment. Through this study, we contribute to the advancement of AI in agriculture by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.
</details>
<details>
<summary>摘要</summary>
过去一代，机器学习（ML）和深度学习（DL）方法在农业系统中得到了迅速发展，其中很多成果在各种农业应用中得到了证明。然而，传统的ML/DL模型具有一些局限性：它们需要大量、昂贵的标注数据进行训练，需要专业的技术人员进行开发和维护，而且主要针对特定任务，缺乏普适性。最近，基础模型（Foundation Models，FMs）在语言和视觉任务中获得了非常成功的结果，它们在多个领域和模式上训练，并且可以通过微调和少量任务特定的标注数据来完成多种任务。Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field.首先，我们回顾了最近的FMs在通用计算机科学领域中的发展，并将它们分为四类：语言FMs、视觉FMs、多模式FMs和强化学习FMs。然后，我们详细介绍了农业FMs的开发过程，并讨论了它们在智能农业中的潜在应用。我们还讨论了开发农业FMs的独特挑战，包括模型训练、验证和部署。通过本研究，我们对农业AI的发展做出了贡献，通过引入AFMs作为一种可靠的替代方案，以减少对广泛标注数据的依赖，提高农业AI系统的效率、有效性和普适性。
</details></li>
</ul>
<hr>
<h2 id="ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN"><a href="#ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN" class="headerlink" title="ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN"></a>ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06663">http://arxiv.org/abs/2308.06663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abul Bashar, Richi Nayak</li>
<li>for: 这篇论文目的是提出一个新的生成对抗网络模型（ALGAN），用于不监控的时间序列资料中的异常检测。</li>
<li>methods: 这篇论文使用的方法是基于LSTM网络的对抗网络（GAN）模型，并且对输出进行调整以提高异常检测精度。</li>
<li>results: 根据实验结果，ALGAN在46个真实世界单 Variate时间序列数据集和多个领域的大量多 Variate时间序列数据集上的异常检测精度较高，比较传统、神经网络基于的和其他GAN型方法更好。<details>
<summary>Abstract</summary>
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
</details>
<details>
<summary>摘要</summary>
《时序数据异常检测使用生成对抗网络》Introduction:时序数据异常检测是各个领域的常见问题，如制造、医疗影像和网络安全等。在这些领域中，检测时序数据中异常点的异常行为是非常重要的。Recently, Generative Adversarial Networks (GANs) have been shown to be effective in detecting anomalies in time series data. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting.Methodology:我们的方法包括以下几个部分：1. 生成对抗网络模型（GAN）的概述2. 基于LSTM网络的异常检测模型（ALGAN）的提出3. 实验设计和结果分析Results:我们对46个实际时序数据集进行了实验，并对多个领域的大量多变量时序数据进行了分析。结果表明，ALGAN在无监督的情况下，对时序数据中异常点的检测性能有显著提高。在单变量和多变量时序数据中，ALGAN都能够准确地检测异常点。Conclusion:本文提出了一种基于GAN的新方法，可以在无监督的情况下，提高时序数据中异常点的检测性能。我们的实验结果表明，ALGAN在多个领域中都能够准确地检测异常点。这种方法可以广泛应用于各个领域中的时序数据异常检测问题。
</details></li>
</ul>
<hr>
<h2 id="Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features"><a href="#Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features" class="headerlink" title="Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features"></a>Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08482">http://arxiv.org/abs/2308.08482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiiizhang/shortcutDebiasing">https://github.com/yiiizhang/shortcutDebiasing</a></li>
<li>paper_authors: Yi Zhang, Jitao Sang, Junyang Wang, Dongmei Jiang, Yaowei Wang</li>
<li>for: 降低机器学习模型中的偏见风险，特别是在社会应用中，如雇用、银行和刑事司法等领域。</li>
<li>methods: 我们提出了一种新的短路减震方法（Shortcut Debiasing），通过在训练阶段将偏见特征转换为短路特征，然后使用 causal intervention 来消除短路特征 durante la inferencia。</li>
<li>results: 我们在多个 benchmark 数据集上应用了短路减震方法，并实现了与状态前的减震方法相比的显著改善 both accuracy 和 fairness。<details>
<summary>Abstract</summary>
Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate shortcut features during inference. The key idea of \emph{Shortcut Debiasing} is to design controllable shortcut features to on one hand replace bias features in contributing to the target task during the training stage, and on the other hand be easily removed by intervention during the inference stage. This guarantees the learning of the target task does not hinder the elimination of bias features. We apply \emph{Shortcut Debiasing} to several benchmark datasets, and achieve significant improvements over the state-of-the-art debiasing methods in both accuracy and fairness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks"><a href="#Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks" class="headerlink" title="Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks"></a>Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06654">http://arxiv.org/abs/2308.06654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 预测步行者的轨迹是自动驾驶汽车安全导航中的关键能力，特别是在与步行者共享空间时。步行者在共享空间中的运动受到汽车和其他步行者的影响，因此，可以准确地模拟步行者-汽车和步行者-步行者的互动，可以提高步行者轨迹预测模型的准确性。</li>
<li>methods: 我们提出了一种基于启发的方法，通过计算碰撞风险来选择互动对象。我们将关注与目标步行者之间的碰撞风险，并使用时间到碰撞和两个对象的接近方向角来编码互动效果。我们还提出了一种新的极天球碰撞网格图，以便更好地表示互动效果。</li>
<li>results: 我们的结果表明，使用我们提出的方法可以比基eline方法更加准确地预测步行者的轨迹，特别是在HBS数据集上。<details>
<summary>Abstract</summary>
Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.
</details>
<details>
<summary>摘要</summary>
预测行人轨迹是自动驾驶车辆安全导航中的关键能力，特别是在与行人共享空间时。行人运动在共享空间中受到车辆和其他行人的影响。因此，可以准确模拟行人与其他行人和车辆之间的互动，以提高行人轨迹预测模型的准确性。尽管有庞大的文献关于使用深度学习模型来编码互动对行人预测轨迹的影响，但是有限的努力被投入到有效选择互动者方面。在大多数情况下，互动特征主要基于相对距离，而忽略了互动形式中速度和接近方向的效果。在这篇文章中，我们提出了一种基于碰撞风险计算的互动者选择规则。我们关注了可能碰撞的两个代理人之间的时间到碰撞和接近方向角的互动效果。我们通过引入一种新的极地碰撞格图来实现这一点。我们的结果显示，与基eline方法相比，我们的方法在HBS数据集上预测轨迹更加准确。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation"><a href="#Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation" class="headerlink" title="Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation"></a>Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06644">http://arxiv.org/abs/2308.06644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation">https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation</a></li>
<li>paper_authors: Junwei Huang, Zhiqing Sun, Yiming Yang</li>
<li>for: 提高NP-完全 combinatorial优化问题的解决速度</li>
<li>methods: 使用进步压缩来减少推理步骤数</li>
<li>results: 在TSP-50 dataset上，提高推理速度16倍，性能下降0.019%<details>
<summary>Abstract</summary>
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
</details>
<details>
<summary>摘要</summary>
GRAPH-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition"><a href="#Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition" class="headerlink" title="Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition"></a>Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11633">http://arxiv.org/abs/2308.11633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Sheffield, Frank E. Bobe III, Bradley Marchand, Matthew S. Emigh</li>
<li>for: 本研究提出了一种基于自助学习的SAS数据处理方法，以解决SAS数据处理中缺乏标注数据的问题。</li>
<li>methods: 本研究使用了MoCo-SAS方法，即基于自助学习的SAS数据处理、分类和模式识别方法。</li>
<li>results: 实验结果表明，MoCo-SAS方法在SAS数据处理中显著超过了传统的指导学习方法，并且在F1分数方面得到了显著改善。这些发现提出了使用自助学习进行SAS数据处理的潜在可能性，并且对水下对象检测和分类提供了新的思路。<details>
<summary>Abstract</summary>
Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.
</details>
<details>
<summary>摘要</summary>
这篇研究论文提出了一个名为MoCo-SAS的自动学习方法，用于对水下探索中的Synthetic Aperture Sonar（SAS）数据进行处理、分类和图像识别。这种方法利用自动学习的自我指导学习（SSL）技术，以提高SAS数据处理的精度和效率。实验结果显示，MoCo-SAS方法与传统的超级vised learning方法相比，有着明显的改善，特别是在F1分数上。这些结果显示出SSL在SAS数据处理中的应用潜力，并开启了更进一步的水下物体探测和分类技术的可能性。
</details></li>
</ul>
<hr>
<h2 id="ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss"><a href="#ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss" class="headerlink" title="ADRMX: Additive Disentanglement of Domain Features with Remix Loss"></a>ADRMX: Additive Disentanglement of Domain Features with Remix Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06624">http://arxiv.org/abs/2308.06624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berkerdemirel/ADRMX">https://github.com/berkerdemirel/ADRMX</a></li>
<li>paper_authors: Berker Demirel, Erchan Aptoula, Huseyin Ozkan</li>
<li>for: 这个论文的目的是解决多个源领域中模型在新未经见过的领域中的泛化问题。</li>
<li>methods: 这个论文提出了一种新的架构 named Additive Disentanglement of Domain Features with Remix Loss (ADRMX)，它使用了添加式分解策略将域特征与域 invariants 相结合，以提高模型的泛化能力。</li>
<li>results: 经过广泛的实验，ADRMX 在 DomainBed 上实现了最佳性能。<details>
<summary>Abstract</summary>
The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original additive disentanglement strategy. Moreover, a new data augmentation technique is introduced to further support the generalization capacity of ADRMX, where samples from different domains are mixed within the latent space. Through extensive experiments conducted on DomainBed under fair conditions, ADRMX is shown to achieve state-of-the-art performance. Code will be made available at GitHub after the revision process.
</details>
<details>
<summary>摘要</summary>
通常假设训练集和测试集遵循类似的分布是在部署场景下不成立。给定多个源领域，领域泛化目标是创建抗辐射的模型，以便在新未看过的领域中进行泛化。然而，现有的研究通常是通过找到源领域中共同的特征来减轻交领域分布变化的影响。这可能会限制模型的泛化能力，因为它仅仅依据源领域中共同的特征来泛化。这些研究忽略了可能存在的领域特有特征，这些特征可能在一些领域中占据主导地位，并且可能包含有价值信息。在这项工作中，一种新的架构名为加法解决方案（ADRMX）被提出，它解决了这一限制，通过将领域特征和领域不变特征结合在一起使用一种原始的加法解决方案。此外，一种新的数据增强技术也被引入，以支持ADRMX的泛化能力，其中各个领域的样本在离散空间中混合。经过对DomainBed进行了广泛的实验，ADRMX被证明可以在公正的条件下实现状态的泛化性能。代码将在GitHub上公布 после修订过程。
</details></li>
</ul>
<hr>
<h2 id="Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks"><a href="#Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks" class="headerlink" title="Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?"></a>Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06619">http://arxiv.org/abs/2308.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Liao, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione</li>
<li>for: 降低深度神经网络的大小 while maintaining performance</li>
<li>methods: 引入Entropy Guided Pruning（EGP）算法，优先采用含有低 entropy 的连接进行剪除</li>
<li>results: 实验结果表明，EGP可以有效地剪除深度神经网络，同时保持竞争性性能水平<details>
<summary>Abstract</summary>
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.
</details>
<details>
<summary>摘要</summary>
《剪除技术在深度神经网络中减小大小而保持性能的应用广泛。然而，这种技术，即使能够压缩深度模型，几乎不能完全从模型中移除整层（即使结构化）：是这个任务可解决吗？在这项研究中，我们介绍了EGP算法，一种基于熵指导的剪除算法，用于减小深度神经网络大小，保持性能水平。EGP的关键焦点是优先剪除层次熵低的连接，从而导致其完全移除。经过广泛的实验，我们发现EGP能够有效地减小深度神经网络，同时保持竞争性能水平。我们的结果不仅揭示了剪除技术的优势，还探讨了剪除、熵和深度学习性能之间的复杂关系。EGP算法和其理解拥有推动深度网络压缩和优化领域的前景。EGP算法的源代码已经公开发布。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness"><a href="#On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness" class="headerlink" title="On the Interplay of Convolutional Padding and Adversarial Robustness"></a>On the Interplay of Convolutional Padding and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06612">http://arxiv.org/abs/2308.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 本文旨在研究 padding 对 adversarial attack 的影响，以及不同 padding 模式对 adversarial robustness 的影响。</li>
<li>methods: 本文使用了 convolutional neural network (CNN) 和 adversarial attack 的方法，并进行了 padding 的分析和对比。</li>
<li>results: 本文发现了 perturbation anomalies 在图像边缘区域，这些区域是 padding 的应用区域。此外，本文还发现了不同 padding 模式对 adversarial robustness 的影响。<details>
<summary>Abstract</summary>
It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
</details>
<details>
<summary>摘要</summary>
通常情况下，在卷积神经网络（CNN）中，会将padding应用于特征地图以保持其分辨率。虽然有很多替代方案，通常是通过添加边界上的零值来实现。在这项工作中，我们发现了对抗攻击通常会在图像边界上产生异常的扰动，这是padding使用的区域。因此，我们想进行对padding和对抗攻击之间的交互分析，并查找不同的padding模式（或其缺失）对对抗鲁棒性在不同的场景中的影响。
</details></li>
</ul>
<hr>
<h2 id="LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net"><a href="#LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net" class="headerlink" title="LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net"></a>LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06603">http://arxiv.org/abs/2308.06603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ach-1914/ladlenet">https://github.com/ach-1914/ladlenet</a></li>
<li>paper_authors: Tonghui Zou</li>
<li>for: 这篇论文的目的是为了将抛光热成像（TIR）图像转换成可见光成像（VI）图像，这个问题在各个领域都有广泛的应用，例如TIR-VI图像匹配和融合。</li>
<li>methods: 这篇论文提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net concatenation结构，并添加了跳过连接和精细特征聚合技术，从而提高了模型性能。</li>
<li>results: 在KAIST数据集上测试和分析了LadleNet和LadleNet+两种方法，结果显示，LadleNet+在图像清晰度和感知质量方面达到了当前最佳性能。<details>
<summary>Abstract</summary>
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibility by allowing the substitution of its network architecture with semantic segmentation networks, thereby establishing more abstract semantic spaces to bolster model performance. Consequently, we propose LadleNet+, which replaces LadleNet's Handle module with the pre-trained DeepLabv3+ network, thereby endowing the model with enhanced semantic space construction capabilities. The proposed method is evaluated and tested on the KAIST dataset, accompanied by quantitative and qualitative analyses. Compared to existing methodologies, our approach achieves state-of-the-art performance in terms of image clarity and perceptual quality. The source code will be made available at https://github.com/Ach-1914/LadleNet/tree/main/.
</details>
<details>
<summary>摘要</summary>
《热传 инфра红（TIR）图像到可见光（VI）图像的翻译 задача具有各种应用领域的潜在投入，如TIR-VI图像匹配和融合。利用TIR图像转换得到的补充信息可以显著提高模型性能和泛化性。然而，现有的问题包括低效图像准确性和限制模型可扩展性。在本文中，我们提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net叠加结构，并添加了跳跃连接和细化特征聚合技术，从而实现了模型性能的明显提高。LadleNet由“托”和“碗”模块组成，其中“托”模块建立了一个抽象 semantic space，而“碗”模块将这个 semantic space 转换为生成的VI图像。“托”模块具有可扩展性，可以将其网络架构替换为semantic segmentation网络，从而建立更加抽象的semantic space，进一步提高模型性能。因此，我们提出了LadleNet+，其将LadleNet的“托”模块替换为预训练的DeepLabv3+网络，从而增强模型的semantic space建构能力。我们的方法在KAIST数据集上进行评估和测试，并通过量化和质量分析进行比较。与现有方法相比，我们的方法在图像清晰度和感知质量上达到了状态 искусственный的性能。代码将在https://github.com/Ach-1914/LadleNet/tree/main/中提供。》
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/cs.LG_2023_08_13/" data-id="clluro5jx005tq9889s5r6u0e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/14/eess.IV_2023_08_14/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-14 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/13/eess.IV_2023_08_13/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-13 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
