
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-13 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06781 repo_url: None paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Al">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-13">
<meta property="og:url" content="https://nullscc.github.io/2023/08/13/eess.IV_2023_08_13/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06781 repo_url: None paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Al">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-13T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:48:36.686Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/eess.IV_2023_08_13/" class="article-date">
  <time datetime="2023-08-13T09:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-13
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature"><a href="#Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature" class="headerlink" title="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature"></a>Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06781">http://arxiv.org/abs/2308.06781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>for: 了解脑血管系统中圆形封闭（Circle of Willis，CoW）的多样性和配置，以提高脑血管疾病研究和临床 intervención的精度。</li>
<li>methods: 使用条件潜在扩散模型（conditional latent diffusion model），包括形态和解剖指导，生成真实的3D CoW分割结果，包括不同的现象型变化。</li>
<li>results: 比较 conditional variants of 3D GAN和3D VAE的模型，发现我们的模型能够更好地保持血管连续性，并且生成的CoW变化更加真实，FID分数比最佳performing GAN-based model高53%。<details>
<summary>Abstract</summary>
The Circle of Willis (CoW) is the part of cerebral vasculature responsible for delivering blood to the brain. Understanding the diverse anatomical variations and configurations of the CoW is paramount to advance research on cerebrovascular diseases and refine clinical interventions. However, comprehensive investigation of less prevalent CoW variations remains challenging because of the dominance of a few commonly occurring configurations. We propose a novel generative approach utilising a conditional latent diffusion model with shape and anatomical guidance to generate realistic 3D CoW segmentations, including different phenotypical variations. Our conditional latent diffusion model incorporates shape guidance to better preserve vessel continuity and demonstrates superior performance when compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE. We observed that our model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches with an FID score 53\% better than the best-performing GAN-based model.
</details>
<details>
<summary>摘要</summary>
圆形维利斯（CoW）是脑血管系统的一部分，负责将血液传递到脑中。了解不同的静脉维利斯变化和配置是研究脑血管疾病的前进和精细化临床 intervención的关键。然而，对于较少seen CoW变化的全面调查仍然是挑战，因为一些常见的配置占据了主导地位。我们提出了一种新的生成方法，使用conditioned latent diffusion模型，包含形态指导，以生成真实的3D CoW分割，包括不同的现象变化。我们的conditioned latent diffusion模型能够更好地保持血管连续性，并与其他生成模型相比，如3D GAN和3D VAE的conditioned变种，显示出更高的性能。我们发现，我们的模型生成的CoW变化比competing approach更真实，Visual fidelity高于53%。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches"><a href="#Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches" class="headerlink" title="Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches"></a>Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06776">http://arxiv.org/abs/2308.06776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxin0/scpgabnet">https://github.com/linxin0/scpgabnet</a></li>
<li>paper_authors: Xin Lin, Chao Ren, Xiao Liu, Jie Huang, Yinjie Lei</li>
<li>for: 提高无监督图像净化的性能，不需要大量的对称数据。</li>
<li>methods: 基于生成敌对网络的Unsupervised Approach， iteratively replace previous less powerful denoiser with current powerful denoiser， generate better synthetic clean-noisy image pairs。</li>
<li>results: 比 state-of-the-art unsupervised方法有更好的性能。<details>
<summary>Abstract</summary>
Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.
</details>
<details>
<summary>摘要</summary>
深度学习方法在图像噪声除除表现出了惊人的表现，特别是在大规模对应数据集上训练的情况下。然而，在真实世界场景中获得对应数据集的获得是一项重要挑战。 Although 无监督方法基于生成对抗网络提供了一种噪声除除无需对应数据集的解决方案，但它们在不改变现有结构或提高噪声除除器的计算复杂度下难以超越传统GAN基于无监督框架的性能限制。为解决这个问题，我们提议了SC策略 для多个噪声除除器。这种策略可以在不增加GAN基于噪声除除框架的推理复杂度下实现显著性能提高。其基本思想是在滤波器引导噪声提取模块中，逐次将前一个较弱的噪声除除器 replaced 为当前更强的噪声除除器。这个过程生成了更好的人工干扰净损像对，导致更强的噪声除除器。这个基准保证了训练网络的稳定性和效果。实验结果表明，我们的方法在无监督方法中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes"><a href="#Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes" class="headerlink" title="Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes"></a>Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06762">http://arxiv.org/abs/2308.06762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Huang, Xukun Zhang, Zhiming Cui, He Zhang, Geng Chen, Dinggang Shen</li>
<li>for: 这个研究的目的是提高胎儿脑MR扫描中的组织分类精度，以便重建iso类型脑MR扫描 volume 和评估胎儿脑发展。</li>
<li>methods: 这个研究使用了域 adaptation 技术，将高品质的iso类型脑MR扫描 volume 作为指导，对厚层扫描进行组织分类。</li>
<li>results: 实验结果显示，这个方法可以对胎儿脑MR扫描中的组织分类进行高精度的调整，并且与现有的方法相比，表现更加出色。<details>
<summary>Abstract</summary>
Accurate tissue segmentation of thick-slice fetal brain magnetic resonance (MR) scans is crucial for both reconstruction of isotropic brain MR volumes and the quantification of fetal brain development. However, this task is challenging due to the use of thick-slice scans in clinically-acquired fetal brain data. To address this issue, we propose to leverage high-quality isotropic fetal brain MR volumes (and also their corresponding annotations) as guidance for segmentation of thick-slice scans. Due to existence of significant domain gap between high-quality isotropic volume (i.e., source data) and thick-slice scans (i.e., target data), we employ a domain adaptation technique to achieve the associated knowledge transfer (from high-quality <source> volumes to thick-slice <target> scans). Specifically, we first register the available high-quality isotropic fetal brain MR volumes across different gestational weeks to construct longitudinally-complete source data. To capture domain-invariant information, we then perform Fourier decomposition to extract image content and style codes. Finally, we propose a novel Cycle-Consistent Domain Adaptation Network (C2DA-Net) to efficiently transfer the knowledge learned from high-quality isotropic volumes for accurate tissue segmentation of thick-slice scans. Our C2DA-Net can fully utilize a small set of annotated isotropic volumes to guide tissue segmentation on unannotated thick-slice scans. Extensive experiments on a large-scale dataset of 372 clinically acquired thick-slice MR scans demonstrate that our C2DA-Net achieves much better performance than cutting-edge methods quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
幼虫脑magnetic resonance（MR）扫描的粗层扫描是重要的，因为它们可以提供高级别的脑部MR影像Volume，并且可以量化胎儿脑部发展的进度。然而，这个任务具有挑战性，因为在临床中获取的胎儿脑部MR扫描通常使用粗层扫描。为解决这个问题，我们提议使用高质量的ISO分布式胎儿脑MR影像（以及其相应的标注）作为指导，以实现粗层扫描的准确分割。由于源数据和目标数据之间存在很大的领域差异，我们采用领域适应技术来实现相关的知识传递。具体来说，我们首先将可用的高质量ISO分布式胎儿脑MR影像长itudinally完整地注册，以构建不同 gestational weeks的源数据。然后，我们使用快速 Fourier 分解来提取图像内容和风格代码。最后，我们提出了一种名为C2DA-Net的循环一致领域适应网络，以高效地传递从高质量ISO分布式胎儿脑MR影像中学习的知识，以便在无标注的粗层扫描中进行准确的组织分割。我们的C2DA-Net可以全面利用一小组标注的ISO分布式胎儿脑MR影像来导导粗层扫描中的组织分割。我们在372个临床获取的粗层扫描中进行了广泛的实验，并证明了我们的C2DA-Net可以在量化和质量上superior于当前的方法。
</details></li>
</ul>
<hr>
<h2 id="FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table"><a href="#FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table" class="headerlink" title="FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table"></a>FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06749">http://arxiv.org/abs/2308.06749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenhao-li-777/fastllve">https://github.com/wenhao-li-777/fastllve</a></li>
<li>paper_authors: Wenhao Li, Guangyang Wu, Wenyi Wang, Peiran Ren, Xiaohong Liu</li>
<li>for: 提高低光照视频质量</li>
<li>methods: 利用Look-Up-Table（LUT）技术维护 между帧亮度一致性，并设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块进行自适应增强。</li>
<li>results: 实验结果表明，我们的方法可以在标准数据集上达到最新状态的性能，同时在帧速和计算复杂度方面也具有优势。 Code available at <a target="_blank" rel="noopener" href="https://github.com/Wenhao-Li-777/FastLLVE">https://github.com/Wenhao-Li-777/FastLLVE</a>.<details>
<summary>Abstract</summary>
Low-Light Video Enhancement (LLVE) has received considerable attention in recent years. One of the critical requirements of LLVE is inter-frame brightness consistency, which is essential for maintaining the temporal coherence of the enhanced video. However, most existing single-image-based methods fail to address this issue, resulting in flickering effect that degrades the overall quality after enhancement. Moreover, 3D Convolution Neural Network (CNN)-based methods, which are designed for video to maintain inter-frame consistency, are computationally expensive, making them impractical for real-time applications. To address these issues, we propose an efficient pipeline named FastLLVE that leverages the Look-Up-Table (LUT) technique to maintain inter-frame brightness consistency effectively. Specifically, we design a learnable Intensity-Aware LUT (IA-LUT) module for adaptive enhancement, which addresses the low-dynamic problem in low-light scenarios. This enables FastLLVE to perform low-latency and low-complexity enhancement operations while maintaining high-quality results. Experimental results on benchmark datasets demonstrate that our method achieves the State-Of-The-Art (SOTA) performance in terms of both image quality and inter-frame brightness consistency. More importantly, our FastLLVE can process 1,080p videos at $\mathit{50+}$ Frames Per Second (FPS), which is $\mathit{2 \times}$ faster than SOTA CNN-based methods in inference time, making it a promising solution for real-time applications. The code is available at https://github.com/Wenhao-Li-777/FastLLVE.
</details>
<details>
<summary>摘要</summary>
低光照视频增强（LLVE）在最近几年内受到了广泛关注。一个关键的要求是 между帧亮度一致性，以保持视频增强后的时间一致性。然而，大多数现有的单张图像基的方法无法解决这个问题，导致幻灯效应，从而降低了整体质量。此外，基于3D卷积神经网络（CNN）的方法，它们是为视频维护 между帧一致性而设计的，但是计算成本高，使其不适用于实时应用。为解决这些问题，我们提出了高效的渠道名为快速LLVE，利用Look-Up-Table（LUT）技术来维护between帧亮度一致性。我们特制了可学习的Intensity-Aware LUT（IA-LUT）模块，用于适应增强，解决低动态问题在低光照场景中。这使得快速LLVE可以在低延迟和低复杂度下进行增强操作，同时维护高质量结果。实验结果表明，我们的方法在标准数据集上达到了状态之作（SOTA）的性能， both图像质量和between帧亮度一致性方面。此外，我们的快速LLVE可以处理1080P视频，在50+帧每秒（FPS）处理速度，高于SOTA CNN基于方法的两倍即2倍的执行速度，这使得它在实时应用中成为一个有前途的解决方案。代码可以在https://github.com/Wenhao-Li-777/FastLLVE中找到。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising"><a href="#Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising" class="headerlink" title="Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising"></a>Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06746">http://arxiv.org/abs/2308.06746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyuan01/self-supervised-noise2noise-for-ldct">https://github.com/xyuan01/self-supervised-noise2noise-for-ldct</a></li>
<li>paper_authors: Yuting Zhu, Qiang He, Yudong Yao, Yueyang Teng</li>
<li>for: 这个研究旨在提出一种基于单束 Computed Tomography (CT) 影像的自我监督噪声降低方法，不需要对CT影像进行训练。</li>
<li>methods: 本研究使用了一种组合自我监督噪声模型和降低噪声的方法，首先将LDCT影像添加了两种相似的噪声，然后使用这些降低噪声的影像进行训练。</li>
<li>results: 实验结果显示，提出的方法比前一代深度学习方法更有效地进行LDCT影像降低噪声。<details>
<summary>Abstract</summary>
Deep learning is a very promising technique for low-dose computed tomography (LDCT) image denoising. However, traditional deep learning methods require paired noisy and clean datasets, which are often difficult to obtain. This paper proposes a new method for performing LDCT image denoising with only LDCT data, which means that normal-dose CT (NDCT) is not needed. We adopt a combination including the self-supervised noise2noise model and the noisy-as-clean strategy. First, we add a second yet similar type of noise to LDCT images multiple times. Note that we use LDCT images based on the noisy-as-clean strategy for corruption instead of NDCT images. Then, the noise2noise model is executed with only the secondary corrupted images for training. We select a modular U-Net structure from several candidates with shared parameters to perform the task, which increases the receptive field without increasing the parameter size. The experimental results obtained on the Mayo LDCT dataset show the effectiveness of the proposed method compared with that of state-of-the-art deep learning methods. The developed code is available at https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT.
</details>
<details>
<summary>摘要</summary>
深度学习是LDCT图像减噪的非常有前途的技术。然而，传统的深度学习方法通常需要配备零噪和干净的数据集，这些数据集往往很难获得。这篇论文提出了一种只使用LDCT数据进行LDCT图像减噪的新方法。我们采用了一种组合，包括自我监督的噪声2噪模型和噪声作为干净策略。首先，我们将LDCT图像添加了多次相似的噪声。注意我们使用LDCT图像来代替NDCT图像进行损害。然后，我们在噪声2噪模型中进行训练，只使用第二次损害的图像。我们选择了一个模块化U-Net结构，从多个候选者中选择了共享参数来完成任务，这样可以增加感知范围而不是增加参数大小。实验结果表明，提出的方法在Mayo LDCT数据集上比州前的深度学习方法更有效。代码可以在https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT上下载。
</details></li>
</ul>
<hr>
<h2 id="Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation"><a href="#Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation" class="headerlink" title="Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?"></a>Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06623">http://arxiv.org/abs/2308.06623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RisabBiswas/Polyp-SAM-PlusPlus">https://github.com/RisabBiswas/Polyp-SAM-PlusPlus</a></li>
<li>paper_authors: Risab Biswas</li>
<li>for: 这个论文的目的是提高肿瘤 segmentation 的精度和稳定性，并通过文本提示来使用 SAM 模型进行肿瘤 segmentation。</li>
<li>methods: 这个论文使用的方法是使用文本提示来提高 SAM 模型的精度和稳定性，并在 benchmark 数据集上进行评估。</li>
<li>results: 研究发现，使用文本提示可以提高 SAM 模型的肿瘤 segmentation 精度和稳定性，并且比不使用文本提示的情况下更好。<details>
<summary>Abstract</summary>
Meta recently released SAM (Segment Anything Model) which is a general-purpose segmentation model. SAM has shown promising results in a wide variety of segmentation tasks including medical image segmentation. In the field of medical image segmentation, polyp segmentation holds a position of high importance, thus creating a model which is robust and precise is quite challenging. Polyp segmentation is a fundamental task to ensure better diagnosis and cure of colorectal cancer. As such in this study, we will see how Polyp-SAM++, a text prompt-aided SAM, can better utilize a SAM using text prompting for robust and more precise polyp segmentation. We will evaluate the performance of a text-guided SAM on the polyp segmentation task on benchmark datasets. We will also compare the results of text-guided SAM vs unprompted SAM. With this study, we hope to advance the field of polyp segmentation and inspire more, intriguing research. The code and other details will be made publically available soon at https://github.com/RisabBiswas/Polyp-SAM++.
</details>
<details>
<summary>摘要</summary>
meta 最近发布了 SAM（分割任务模型），这是一种通用的分割模型。SAM 在多种分割任务中表现出色，包括医疗影像分割。在医疗影像分割领域，肿瘤分割具有非常高的重要性，因此创建一个精度高和可靠的模型是非常挑战性的。肿瘤分割是诊断和治疗抑郁癌的基本任务。在本研究中，我们将看到 Polyp-SAM++，一种使用文本提示的 SAM，如何更好地利用 SAM 进行肿瘤分割。我们将对 Polyp-SAM++ 在标准数据集上进行评估，并与不提示 SAM 进行比较。我们希望通过这项研究，推动肿瘤分割领域的进步，并鼓励更多的感人研究。代码和其他细节将在https://github.com/RisabBiswas/Polyp-SAM++ 上公开。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/eess.IV_2023_08_13/" data-id="clopawo0r014nag88583gah1c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/13/cs.LG_2023_08_13/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-13
        
      </div>
    </a>
  
  
    <a href="/2023/08/12/cs.SD_2023_08_12/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-12</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
