
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-13 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06781 repo_url: None paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Al">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-13 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/13/eess.IV_2023_08_13/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06781 repo_url: None paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Al">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-12T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:29.841Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/eess.IV_2023_08_13/" class="article-date">
  <time datetime="2023-08-12T16:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-13 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature"><a href="#Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature" class="headerlink" title="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature"></a>Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06781">http://arxiv.org/abs/2308.06781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>For: The paper aims to generate realistic 3D segmentations of the Circle of Willis (CoW) using a conditional latent diffusion model with shape and anatomical guidance, in order to advance research on cerebrovascular diseases and refine clinical interventions.* Methods: The authors propose a novel generative approach using a conditional latent diffusion model, which incorporates shape guidance to better preserve vessel continuity and demonstrate superior performance compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE.* Results: The authors observed that their model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches, with an FID score 53% better than the best-performing GAN-based model.Here are the three points in Simplified Chinese text:* For: 本研究旨在使用 conditional latent diffusion model 生成真实的 3D CoW 分割，以提高脑血管疾病研究和临床 interven 的技术水平。* Methods: 作者们提出了一种新的生成方法，使用 conditional latent diffusion model，该模型具有形态指导，以更好地保持血管连续性。* Results: 作者们发现，他们的模型可以生成更加真实的 CoW 变体，并且与其他方法相比，有 53% 更高的视觉质量。<details>
<summary>Abstract</summary>
The Circle of Willis (CoW) is the part of cerebral vasculature responsible for delivering blood to the brain. Understanding the diverse anatomical variations and configurations of the CoW is paramount to advance research on cerebrovascular diseases and refine clinical interventions. However, comprehensive investigation of less prevalent CoW variations remains challenging because of the dominance of a few commonly occurring configurations. We propose a novel generative approach utilising a conditional latent diffusion model with shape and anatomical guidance to generate realistic 3D CoW segmentations, including different phenotypical variations. Our conditional latent diffusion model incorporates shape guidance to better preserve vessel continuity and demonstrates superior performance when compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE. We observed that our model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches with an FID score 53\% better than the best-performing GAN-based model.
</details>
<details>
<summary>摘要</summary>
圆形的威廉圈（CoW）是脑血管系统中带来脑部血液的部分。了解各种不同的CoW结构和配置是研究脑血管疾病的进步和优化临床 intervención的关键。然而，对于较少seen CoW变化的全面调查仍然是一个挑战，因为一些常见的配置占据了主导地位。我们提出了一种新的生成方法，利用决定性液态扩散模型，包括形态指导来生成真实的3D CoW分割结果，包括不同的fenotipical变化。我们的决定性液态扩散模型包含形态指导，以更好地保持血管连续性，并且与其他生成模型相比，包括 conditional GAN和3D VAE的 conditional变种，显示出更高的性能。我们观察到，我们的模型生成的CoW变化更加真实，与竞争方法的FID分数比53%高。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches"><a href="#Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches" class="headerlink" title="Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches"></a>Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06776">http://arxiv.org/abs/2308.06776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxin0/scpgabnet">https://github.com/linxin0/scpgabnet</a></li>
<li>paper_authors: Xin Lin, Chao Ren, Xiao Liu, Jie Huang, Yinjie Lei</li>
<li>for: 提高无监督图像净化的性能，不需要大规模的对照数据集。</li>
<li>methods: 基于生成对抗网络的不监督方法，通过多个denoiser的级联使用，逐渐提高净化性能。</li>
<li>results: 与现有无监督方法相比，提出了新的SC策略，可以减少对GAN-based净化框架的计算复杂性，同时提高图像净化性能。实验结果表明，该方法可以在无监督下实现更高的净化性能。<details>
<summary>Abstract</summary>
Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.
</details>
<details>
<summary>摘要</summary>
深度学习方法在图像除噪方面表现了非常出色，特别是在大规模对应数据集上训练的情况下。然而，在实际场景中获得这些对应数据集是一项非常困难的任务。尽管使用生成对抗网络来实现无监督的推荐方法可以解决这个问题，但是这些方法往往难以超越传统的GAN基础架构下的性能限制，而且不需要明显地修改现有结构或增加推荐器的计算复杂度。为解决这个问题，我们提出了一种SC策略。这种策略可以在GAN基础架构下实现明显的性能提升，而无需增加推荐器的推理复杂度。其基本思想是在滤波器导向噪音提取模块中，iteratively替换之前的较弱推荐器，使得当前的强大推荐器可以在下一轮中使用。这个过程生成了更好的干净清噪图像对，从而导致更强大的推荐器。这个基准保证了训练网络的稳定性和效果。实验结果表明，我们的方法在无监督方法中表现出了superiority。
</details></li>
</ul>
<hr>
<h2 id="Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes"><a href="#Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes" class="headerlink" title="Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes"></a>Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06762">http://arxiv.org/abs/2308.06762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Huang, Xukun Zhang, Zhiming Cui, He Zhang, Geng Chen, Dinggang Shen</li>
<li>for: 这个研究旨在提高胎儿脑 MR 影像中的组织分类精度，以便重建高精度的胎儿脑 MR 影像量和评估胎儿脑发展。</li>
<li>methods: 这个研究使用了领域适应技术，将高品质的胎儿脑 MR 影像作为指导，对厚层胎儿脑 MR 影像进行分类。</li>
<li>results: 研究结果显示，这个方法可以很好地改善胎儿脑 MR 影像中的组织分类精度，与现有的方法相比，表现更加出色。<details>
<summary>Abstract</summary>
Accurate tissue segmentation of thick-slice fetal brain magnetic resonance (MR) scans is crucial for both reconstruction of isotropic brain MR volumes and the quantification of fetal brain development. However, this task is challenging due to the use of thick-slice scans in clinically-acquired fetal brain data. To address this issue, we propose to leverage high-quality isotropic fetal brain MR volumes (and also their corresponding annotations) as guidance for segmentation of thick-slice scans. Due to existence of significant domain gap between high-quality isotropic volume (i.e., source data) and thick-slice scans (i.e., target data), we employ a domain adaptation technique to achieve the associated knowledge transfer (from high-quality <source> volumes to thick-slice <target> scans). Specifically, we first register the available high-quality isotropic fetal brain MR volumes across different gestational weeks to construct longitudinally-complete source data. To capture domain-invariant information, we then perform Fourier decomposition to extract image content and style codes. Finally, we propose a novel Cycle-Consistent Domain Adaptation Network (C2DA-Net) to efficiently transfer the knowledge learned from high-quality isotropic volumes for accurate tissue segmentation of thick-slice scans. Our C2DA-Net can fully utilize a small set of annotated isotropic volumes to guide tissue segmentation on unannotated thick-slice scans. Extensive experiments on a large-scale dataset of 372 clinically acquired thick-slice MR scans demonstrate that our C2DA-Net achieves much better performance than cutting-edge methods quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
通过借助高质量的ISO体积脑MR图像（以及其相应的标注），我们提议利用这些图像作为厚层扫描图像的指导。由于高质量ISO体积图像和厚层扫描图像之间存在域之间的差距，我们采用域适应技术来实现相关的知识传递。特别是，我们首先将可用的高质量ISO体积脑MR图像进行了 longitudinally-complete的注册，以构建不同 gestational 周的完整的源数据。然后，我们通过Fourier分解来提取图像内容和风格代码。最后，我们提出了一种名为C2DA-Net的循环相互适应域适应网络，以高效地传递高质量ISO体积图像中学习的知识来进行厚层扫描图像的精准组织分割。我们的C2DA-Net可以充分利用一小组标注的ISO体积图像来导引厚层扫描图像的组织分割。我们对372个临床获取的厚层扫描MR扫描图像进行了广泛的实验，并证明了我们的C2DA-Net在量和质量上都与当今的方法相比较为出色。
</details></li>
</ul>
<hr>
<h2 id="FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table"><a href="#FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table" class="headerlink" title="FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table"></a>FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06749">http://arxiv.org/abs/2308.06749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenhao-li-777/fastllve">https://github.com/wenhao-li-777/fastllve</a></li>
<li>paper_authors: Wenhao Li, Guangyang Wu, Wenyi Wang, Peiran Ren, Xiaohong Liu</li>
<li>for: 提高低光照视频质量，保持视频的时间协调性。</li>
<li>methods: 利用Look-Up-Table（LUT）技术，实现高效的低光照视频提高。设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，以适应低动态范围问题。</li>
<li>results: 实验结果表明，我们的方法在质量和时间协调性两个方面均达到了领先水平。与现有的单帧图像基于方法相比，我们的方法可以在1080p视频中实现50+帧&#x2F;秒的处理速度，并且可以保持高质量结果。<details>
<summary>Abstract</summary>
Low-Light Video Enhancement (LLVE) has received considerable attention in recent years. One of the critical requirements of LLVE is inter-frame brightness consistency, which is essential for maintaining the temporal coherence of the enhanced video. However, most existing single-image-based methods fail to address this issue, resulting in flickering effect that degrades the overall quality after enhancement. Moreover, 3D Convolution Neural Network (CNN)-based methods, which are designed for video to maintain inter-frame consistency, are computationally expensive, making them impractical for real-time applications. To address these issues, we propose an efficient pipeline named FastLLVE that leverages the Look-Up-Table (LUT) technique to maintain inter-frame brightness consistency effectively. Specifically, we design a learnable Intensity-Aware LUT (IA-LUT) module for adaptive enhancement, which addresses the low-dynamic problem in low-light scenarios. This enables FastLLVE to perform low-latency and low-complexity enhancement operations while maintaining high-quality results. Experimental results on benchmark datasets demonstrate that our method achieves the State-Of-The-Art (SOTA) performance in terms of both image quality and inter-frame brightness consistency. More importantly, our FastLLVE can process 1,080p videos at $\mathit{50+}$ Frames Per Second (FPS), which is $\mathit{2 \times}$ faster than SOTA CNN-based methods in inference time, making it a promising solution for real-time applications. The code is available at https://github.com/Wenhao-Li-777/FastLLVE.
</details>
<details>
<summary>摘要</summary>
低光照视频增强（LLVE）在过去几年内得到了广泛关注。一个重要的需求是 между帧亮度一致性，以保持视频增强后的时间一致性。然而，大多数单张图像基本方法无法解决这个问题，导致干扰效应，从而降低总质量。此外，基于3D convolutional neural network（CNN）的方法，它们是为视频维护间帧一致性而设计的，但它们计算成本高，使其在实时应用中不实际。为解决这些问题，我们提出了一个高效的排序管道，即快速LLVE，该管道利用Look-Up-Table（LUT）技术保持间帧亮度一致性。特别是，我们设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，用于自适应增强，解决低动态问题在低光照场景中。这使得快速LLVE可以在低延迟和低复杂度下进行增强操作，同时保持高质量结果。实验结果表明，我们的方法在标准测试集上达到了状态方法（SOTA）的性能，并且在帧率和亮度一致性两个指标上均有显著提高。此外，我们的快速LLVE可以处理1080P视频，并在50+帧/秒的速度下进行增强，这比SOTA CNN基本方法在推理时间上快两倍，使其成为实时应用的优秀解决方案。代码可以在https://github.com/Wenhao-Li-777/FastLLVE上获取。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising"><a href="#Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising" class="headerlink" title="Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising"></a>Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06746">http://arxiv.org/abs/2308.06746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyuan01/self-supervised-noise2noise-for-ldct">https://github.com/xyuan01/self-supervised-noise2noise-for-ldct</a></li>
<li>paper_authors: Yuting Zhu, Qiang He, Yudong Yao, Yueyang Teng</li>
<li>for: 这篇论文旨在提出一种基于单静电 Tomatoes CT（LDCT）数据的自我监督噪声降低方法，并不需要对比的噪音和清洁数据。</li>
<li>methods: 本研究使用了一种组合自我监督噪声模型和降低噪声策略，包括在LDCT图像中添加多次相似的噪音，并使用这些次生噪音做为训练数据。</li>
<li>results: 实验结果显示，提案的方法在Mayo LDCT数据集上比前一些深度学习方法更有效率。<details>
<summary>Abstract</summary>
Deep learning is a very promising technique for low-dose computed tomography (LDCT) image denoising. However, traditional deep learning methods require paired noisy and clean datasets, which are often difficult to obtain. This paper proposes a new method for performing LDCT image denoising with only LDCT data, which means that normal-dose CT (NDCT) is not needed. We adopt a combination including the self-supervised noise2noise model and the noisy-as-clean strategy. First, we add a second yet similar type of noise to LDCT images multiple times. Note that we use LDCT images based on the noisy-as-clean strategy for corruption instead of NDCT images. Then, the noise2noise model is executed with only the secondary corrupted images for training. We select a modular U-Net structure from several candidates with shared parameters to perform the task, which increases the receptive field without increasing the parameter size. The experimental results obtained on the Mayo LDCT dataset show the effectiveness of the proposed method compared with that of state-of-the-art deep learning methods. The developed code is available at https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT.
</details>
<details>
<summary>摘要</summary>
深度学习是低剂量 computed tomography（LDCT）图像减噪的非常有前途的技术。然而，传统的深度学习方法通常需要对噪声和清晰图像的对应对进行预处理，这可能具有困难。这篇论文提出了一种使用仅LDCT数据进行LDCT图像减噪的新方法。我们采用了一种组合，包括自我超级vised noise2noise模型和噪声作为清晰Strategy。首先，我们在LDCT图像中添加了多个类似的噪声。请注意，我们使用LDCT图像来实现损害代替NDCT图像。然后，我们在噪声模型中进行训练，使用只有次噪声图像。我们选择了一种模块化U-Net结构，其中共享参数来完成任务，这将增加了感知场景而不会增加参数的大小。我们在Mayo LDCT数据集上进行实验，并证明了提议方法的有效性，比对已有的深度学习方法更高。开发的代码可以在https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT中找到。
</details></li>
</ul>
<hr>
<h2 id="Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation"><a href="#Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation" class="headerlink" title="Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?"></a>Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06623">http://arxiv.org/abs/2308.06623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RisabBiswas/Polyp-SAM-PlusPlus">https://github.com/RisabBiswas/Polyp-SAM-PlusPlus</a></li>
<li>paper_authors: Risab Biswas</li>
<li>for: 本研究旨在提高肠Rectal cancer的诊断和治疗，通过使用文本提示来改进SAM模型，以提高肠脏膜膜蛋白分 segmentation的精度和稳定性。</li>
<li>methods: 本研究使用的是Segment Anything Model (SAM)，并通过文本提示来改进SAM模型，提高其对肠脏膜膜蛋白分 segmentation的能力。</li>
<li>results: 研究表明，使用文本提示的SAM模型可以提高肠脏膜膜蛋白分 segmentation的精度和稳定性，并且比未使用文本提示的SAM模型更好地处理不同的肠脏膜膜蛋白分样本。<details>
<summary>Abstract</summary>
Meta recently released SAM (Segment Anything Model) which is a general-purpose segmentation model. SAM has shown promising results in a wide variety of segmentation tasks including medical image segmentation. In the field of medical image segmentation, polyp segmentation holds a position of high importance, thus creating a model which is robust and precise is quite challenging. Polyp segmentation is a fundamental task to ensure better diagnosis and cure of colorectal cancer. As such in this study, we will see how Polyp-SAM++, a text prompt-aided SAM, can better utilize a SAM using text prompting for robust and more precise polyp segmentation. We will evaluate the performance of a text-guided SAM on the polyp segmentation task on benchmark datasets. We will also compare the results of text-guided SAM vs unprompted SAM. With this study, we hope to advance the field of polyp segmentation and inspire more, intriguing research. The code and other details will be made publically available soon at https://github.com/RisabBiswas/Polyp-SAM++.
</details>
<details>
<summary>摘要</summary>
Meta 最近发布了 SAM（Segment Anything Model），这是一种通用分割模型。SAM 在各种分割任务中表现出了扎实的成果，包括医学影像分割。在医学影像分割领域，肿瘤分割具有非常高的重要性，因此创建一个精准和Robust的模型是非常挑战性的。肿瘤分割是检测和治疗抗Rectal cancer的基本任务之一。在这项研究中，我们将看到Polyp-SAM++ 是如何使用文本提示来更好地利用 SAM 进行肿瘤分割。我们将对 Polyp-SAM++ 在标准数据集上进行评估，并与不提示 SAM 进行比较。我们希望通过这项研究，推动肿瘤分割领域的进步，并鼓励更多的有趣的研究。代码和其他细节将于 https://github.com/RisabBiswas/Polyp-SAM++ 上公开。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/eess.IV_2023_08_13/" data-id="clmjn91qj00gp0j8844l4clln" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/13/cs.LG_2023_08_13/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-13 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/12/cs.LG_2023_08_12/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-12 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
