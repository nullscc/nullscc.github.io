
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-21 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Extraction of Text from Optic Nerve Optical Coherence Tomography Reports paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10790 repo_url: None paper_authors: Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-21 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/21/eess.IV_2023_08_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Extraction of Text from Optic Nerve Optical Coherence Tomography Reports paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10790 repo_url: None paper_authors: Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-20T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:34.738Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/21/eess.IV_2023_08_21/" class="article-date">
  <time datetime="2023-08-20T16:00:00.000Z" itemprop="datePublished">2023-08-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-21 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Extraction-of-Text-from-Optic-Nerve-Optical-Coherence-Tomography-Reports"><a href="#Extraction-of-Text-from-Optic-Nerve-Optical-Coherence-Tomography-Reports" class="headerlink" title="Extraction of Text from Optic Nerve Optical Coherence Tomography Reports"></a>Extraction of Text from Optic Nerve Optical Coherence Tomography Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10790">http://arxiv.org/abs/2308.10790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia Y. Wang</li>
<li>for: This study aimed to develop and evaluate rule-based algorithms for extracting text data, including RNFL values and GCC data, from Zeiss Cirrus OCT scan reports.</li>
<li>methods: The study used DICOM files with encapsulated PDF reports, converted them into image files, and used the PaddleOCR Python package for optical character recognition. Rule-based algorithms were designed and optimized for improved performance in extracting RNFL and GCC data.</li>
<li>results: The developed algorithms demonstrated high precision in extracting data from both RNFL and GCC scans, with slightly better precision for the right eye in RNFL extraction and for the left eye in GCC extraction. However, some values presented more challenges in extraction, such as clock hours 5 and 6 for RNFL thickness and signal strength for GCC.Here’s the information in Simplified Chinese text:</li>
<li>for: 这项研究的目的是开发和评估基于规则的算法，以提高从Zeiss Cirrus光子共振成像扫描报告中提取文本数据，包括肾脉层（RNFL）值和其他膝肾细胞计数（GCC）数据。</li>
<li>methods: 该研究使用DICOM文件中嵌入的PDF报告，并将其转换为图像文件，使用Python中的PaddleOCR包进行光学字符识别。研究人员设计了和优化了基于规则的算法，以提高对RNFL和GCC扫描报告中的数据提取精度。</li>
<li>results: 开发的算法在提取RNFL和GCC扫描报告中的数据时显示了高精度。对于右眼，RNFL提取精度较高（OD: 0.9803 vs. OS: 0.9046），对于左眼，GCC提取精度较高（OD: 0.9567 vs. OS: 0.9677）。然而，一些值在提取时存在更大的挑战，如RNFL厚度的时钟小时5和6，以及GCC的信号强度。<details>
<summary>Abstract</summary>
Purpose: The purpose of this study was to develop and evaluate rule-based algorithms to enhance the extraction of text data, including retinal nerve fiber layer (RNFL) values and other ganglion cell count (GCC) data, from Zeiss Cirrus optical coherence tomography (OCT) scan reports. Methods: DICOM files that contained encapsulated PDF reports with RNFL or Ganglion Cell in their document titles were identified from a clinical imaging repository at a single academic ophthalmic center. PDF reports were then converted into image files and processed using the PaddleOCR Python package for optical character recognition. Rule-based algorithms were designed and iteratively optimized for improved performance in extracting RNFL and GCC data. Evaluation of the algorithms was conducted through manual review of a set of RNFL and GCC reports. Results: The developed algorithms demonstrated high precision in extracting data from both RNFL and GCC scans. Precision was slightly better for the right eye in RNFL extraction (OD: 0.9803 vs. OS: 0.9046), and for the left eye in GCC extraction (OD: 0.9567 vs. OS: 0.9677). Some values presented more challenges in extraction, particularly clock hours 5 and 6 for RNFL thickness, and signal strength for GCC. Conclusions: A customized optical character recognition algorithm can identify numeric results from optical coherence scan reports with high precision. Automated processing of PDF reports can greatly reduce the time to extract OCT results on a large scale.
</details>
<details>
<summary>摘要</summary>
目的：本研究的目的是开发和评估基于规则的算法，以提高从Zeiss Cirrus光合成 Tomatoes(OCT)扫描报告中提取文本数据的精度，包括胁肤神经层(RNFL)值和神经细胞计数(GCC)数据。方法：从一所学术眼科中心的临床扫描存储系统中标记为包含DICOM文档的PDF报告，并将PDF报告转换为图像文件，然后使用Python包PaddleOCR进行光学字符识别。基于规则的算法被设计并优化，以提高提取RNFL和GCC数据的精度。评估算法的效果通过手动复审一组RNFL和GCC报告进行评估。结果：开发的算法在RNFL和GCC扫描报告中提取数据的精度很高，OD和OS的精度分别为0.9803和0.9046，以及0.9567和0.9677。但是，某些值在提取中存在更大的挑战，例如RNFL厚度的时钟小时5和6，以及GCC的信号强度。结论：可以使用自定义的光学字符识别算法来从OCT扫描报告中提取数据，并且自动处理PDF报告可以大幅减少大规模提取OCT结果的时间。
</details></li>
</ul>
<hr>
<h2 id="Dense-Error-Map-Estimation-for-MRI-Ultrasound-Registration-in-Brain-Tumor-Surgery-Using-Swin-UNETR"><a href="#Dense-Error-Map-Estimation-for-MRI-Ultrasound-Registration-in-Brain-Tumor-Surgery-Using-Swin-UNETR" class="headerlink" title="Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR"></a>Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10784">http://arxiv.org/abs/2308.10784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soorena Salari, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao</li>
<li>for: 降低脑肿瘤手术死亡率的早期手术治疗</li>
<li>methods: 使用投射照像（iUS）跟踪脑组织变形，并使用高精度的MRI-iUS匹配技术更新先前的手术计划，以提高手术安全性和效果</li>
<li>results: 提出了一种基于深度学习（DL）的框架，可以自动评估MRI-iUS匹配结果的质量，并在实际临床数据上显示其性能。<details>
<summary>Abstract</summary>
Early surgical treatment of brain tumors is crucial in reducing patient mortality rates. However, brain tissue deformation (called brain shift) occurs during the surgery, rendering pre-operative images invalid. As a cost-effective and portable tool, intra-operative ultrasound (iUS) can track brain shift, and accurate MRI-iUS registration techniques can update pre-surgical plans and facilitate the interpretation of iUS. This can boost surgical safety and outcomes by maximizing tumor removal while avoiding eloquent regions. However, manual assessment of MRI-iUS registration results in real-time is difficult and prone to errors due to the 3D nature of the data. Automatic algorithms that can quantify the quality of inter-modal medical image registration outcomes can be highly beneficial. Therefore, we propose a novel deep-learning (DL) based framework with the Swin UNETR to automatically assess 3D-patch-wise dense error maps for MRI-iUS registration in iUS-guided brain tumor resection and show its performance with real clinical data for the first time.
</details>
<details>
<summary>摘要</summary>
早期手术治疗脑肿的时间点对病人死亡率有重要影响。然而，手术过程中脑组织变形（称为脑Shift）会使先前的图像无效。作为一种cost-effective和可搬式工具，在手术过程中的ultrasound（iUS）可以跟踪脑Shift，并且精准的MRI-iUS注册技术可以更新先前的 планы和促进iUS的解释。这可以提高手术安全性和效果，最大化肿瘤除除而避免感知区域。然而，手动评估MRI-iUS注册结果的实时性具有困难和错误的可能性，因为数据的3D性。自动的算法可以评估多Modal医疗图像注册结果的质量。因此，我们提出了一个基于深度学习（DL）的框架，使用Swin UNITER来自动评估3D-patch-wise稠密错误地图进行MRI-iUS注册的性能，并在实际临床数据上展示其性能。
</details></li>
</ul>
<hr>
<h2 id="Automated-Identification-of-Failure-Cases-in-Organ-at-Risk-Segmentation-Using-Distance-Metrics-A-Study-on-CT-Data"><a href="#Automated-Identification-of-Failure-Cases-in-Organ-at-Risk-Segmentation-Using-Distance-Metrics-A-Study-on-CT-Data" class="headerlink" title="Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data"></a>Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10636">http://arxiv.org/abs/2308.10636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Honarmandi Shandiz, Attila Rádics, Rajesh Tamada, Makk Árpád, Karolina Glowacka, Lehel Ferenczi, Sandeep Dutta, Michael Fanariotis</li>
<li>for: 提高自动生成的肿瘤分割精度，以便更好地规划辐射治疗</li>
<li>methods: 使用维度距离和 Hausdorff 距离的组合来自动标识失败案例，从而更快地修复失败案例</li>
<li>results: 通过设置维度距离和 Hausdorff 距离的阈值，能够快速地自动标识失败案例，并且可以对12个不同的失败案例进行视觉评估<details>
<summary>Abstract</summary>
Automated organ at risk (OAR) segmentation is crucial for radiation therapy planning in CT scans, but the generated contours by automated models can be inaccurate, potentially leading to treatment planning issues. The reasons for these inaccuracies could be varied, such as unclear organ boundaries or inaccurate ground truth due to annotation errors. To improve the model's performance, it is necessary to identify these failure cases during the training process and to correct them with some potential post-processing techniques. However, this process can be time-consuming, as traditionally it requires manual inspection of the predicted output. This paper proposes a method to automatically identify failure cases by setting a threshold for the combination of Dice and Hausdorff distances. This approach reduces the time-consuming task of visually inspecting predicted outputs, allowing for faster identification of failure case candidates. The method was evaluated on 20 cases of six different organs in CT images from clinical expert curated datasets. By setting the thresholds for the Dice and Hausdorff distances, the study was able to differentiate between various states of failure cases and evaluate over 12 cases visually. This thresholding approach could be extended to other organs, leading to faster identification of failure cases and thereby improving the quality of radiation therapy planning.
</details>
<details>
<summary>摘要</summary>
自动化器官风险（OAR）分割是辐射疗法规划CT扫描图中的关键，但自动生成的边界可能存在误差，可能导致治疗规划问题。这些误差的原因可能是不清晰的器官边界或者实际数据错误，导致标注错误。为了提高模型的性能，需要在训练过程中识别这些失败案例，并使用一些可能的后处理技术来更正。然而，这个过程可能占用很多时间，因为传统上需要手动检查预测输出。这篇论文提出了一种方法，通过设置Dice和 Hausdorff距离的组合阈值，自动地识别失败案例。这种方法可以减少手动检查预测输出的时间占用，并允许更快地识别失败案例候选者。该方法在20个不同器官的CT图像中进行了20个案例的评估，通过设置阈值，能够分辨出不同类型的失败案例，并评估12个案例。这种阈值设置方法可以扩展到其他器官，从而更快地识别失败案例，提高辐射疗法规划的质量。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Medical-Image-Segmentation-Optimizing-Cross-Entropy-Weights-and-Post-Processing-with-Autoencoders"><a href="#Enhancing-Medical-Image-Segmentation-Optimizing-Cross-Entropy-Weights-and-Post-Processing-with-Autoencoders" class="headerlink" title="Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders"></a>Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10488">http://arxiv.org/abs/2308.10488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Singh, Luoyao Chen, Mei Chen, Jinqian Pan, Raviteja Chukkapalli, Shravan Chaudhari, Jacopo Cirrone</li>
<li>for: 本研究旨在提高医学图像分割的精度和效率，特别是在抗体性疾病如皮肤粘液病中分割细胞和炎症的过程中。</li>
<li>methods: 本研究采用深度学习技术，开发了一种适应性能高的医学图像分割方法，并对捷克网络（U-Net）和U-Net++进行了比较。</li>
<li>results: 实验结果表明，本研究的方法在皮肤粘液病图像分割任务上比现有技术高效率12.26%和12.04%。此外，我们还对loss函数权重的优化和三个医学图像分割任务进行了比较。<details>
<summary>Abstract</summary>
The task of medical image segmentation presents unique challenges, necessitating both localized and holistic semantic understanding to accurately delineate areas of interest, such as critical tissues or aberrant features. This complexity is heightened in medical image segmentation due to the high degree of inter-class similarities, intra-class variations, and possible image obfuscation. The segmentation task further diversifies when considering the study of histopathology slides for autoimmune diseases like dermatomyositis. The analysis of cell inflammation and interaction in these cases has been less studied due to constraints in data acquisition pipelines. Despite the progressive strides in medical science, we lack a comprehensive collection of autoimmune diseases. As autoimmune diseases globally escalate in prevalence and exhibit associations with COVID-19, their study becomes increasingly essential. While there is existing research that integrates artificial intelligence in the analysis of various autoimmune diseases, the exploration of dermatomyositis remains relatively underrepresented. In this paper, we present a deep-learning approach tailored for Medical image segmentation. Our proposed method outperforms the current state-of-the-art techniques by an average of 12.26% for U-Net and 12.04% for U-Net++ across the ResNet family of encoders on the dermatomyositis dataset. Furthermore, we probe the importance of optimizing loss function weights and benchmark our methodology on three challenging medical image segmentation tasks
</details>
<details>
<summary>摘要</summary>
医疗图像分割任务具有独特的挑战，需要同时具备本地化和整体 semantics 的理解，以准确地分割关键区域，如病理性组织或异常特征。这种复杂性在医疗图像分割中受到高度的类间相似性、类内变化和可能的图像掩蔽的影响。医疗图像分割任务进一步复杂化，当考虑到研究 histopathology 板块 для自遗护疾病如dermatomyositis时。对于这些病例，分割和检测细胞Inflammation和互动的分析尚未得到了充分的研究，这主要是因为数据获取管道的限制。尽管医学科技在进步的同时，我们缺乏一个完整的自遗护疾病集合。自遗护疾病在全球范围内的发病率不断增长，并与 COVID-19 相关，因此其研究变得越来越重要。虽然现有的研究已经将人工智能 integrate 到了不同的自遗护疾病的分析中，但是dermatomyositis 的研究仍然相对落后。在这篇文章中，我们提出了一种适用于医疗图像分割的深度学习方法。我们的提议方法在 ResNet 家族Encoder 上的 U-Net 和 U-Net++ 中超过了平均提高12.26%和12.04%。此外，我们还评估了优化损失函数的重要性，并在三个困难的医疗图像分割任务上进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Pneumonia-and-COVID-19-Using-Deep-Neural-Networks"><a href="#Prediction-of-Pneumonia-and-COVID-19-Using-Deep-Neural-Networks" class="headerlink" title="Prediction of Pneumonia and COVID-19 Using Deep Neural Networks"></a>Prediction of Pneumonia and COVID-19 Using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10368">http://arxiv.org/abs/2308.10368</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. S. Haque, M. S. Taluckder, S. B. Shawkat, M. A. Shahriyar, M. A. Sayed, C. Modak</li>
<li>for: 这个研究旨在探讨医疗图像分析是否可以帮助早期识别感染病毒和细菌所致的肺炎，以便减少其传播。</li>
<li>methods: 这个研究使用机器学习技术来预测肺炎，并评估不同的机器学习模型在肺炎患者的颈部X射线图像上的表现。</li>
<li>results: 研究发现，使用DenseNet121模型可以实现肺炎的准确预测，其准确率为99.58%。这项研究显示了机器学习技术在精确肺炎诊断中的重要性，并提供了这方面的技术传承。<details>
<summary>Abstract</summary>
Pneumonia, caused by bacteria and viruses, is a rapidly spreading viral infection with global implications. Prompt identification of infected individuals is crucial for containing its transmission. This study explores the potential of medical image analysis to address this challenge. We propose machine-learning techniques for predicting Pneumonia from chest X-ray images. Chest X-ray imaging is vital for Pneumonia diagnosis due to its accessibility and cost-effectiveness. However, interpreting X-rays for Pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions. We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients. Performance measures and confusion matrices are employed to assess and compare the models. The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%. This study underscores the significance of machine learning in the accurate detection of Pneumonia, leveraging chest X-ray images. Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics.
</details>
<details>
<summary>摘要</summary>
《肺炎，由病毒和 бактерий引起的，是一种迅速传播的感染病种，具有全球化的意义。》Prompt identification of infected individuals is crucial for containing the transmission of pneumonia. This study explores the potential of medical image analysis to address this challenge. We propose machine-learning techniques for predicting pneumonia from chest X-ray images. Chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness. However, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions. We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients. Performance measures and confusion matrices are employed to assess and compare the models. The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%. This study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images. Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics.Here's the text with some notes on the translation:* "肺炎" (pneumonia) is a noun, and it is translated as "肺炎" (pneumonia) in Simplified Chinese.* "由病毒和 бактерий引起" (caused by bacteria and viruses) is a prepositional phrase, and it is translated as "由病毒和 бактерий引起" (caused by bacteria and viruses) in Simplified Chinese.* "是一种迅速传播的感染病种" (a rapidly spreading viral infection) is a sentence, and it is translated as "是一种迅速传播的感染病种" (a rapidly spreading viral infection) in Simplified Chinese.* "Prompt identification of infected individuals is crucial for containing the transmission of pneumonia" is a sentence, and it is translated as "Prompt identification of infected individuals is crucial for containing the transmission of pneumonia" in Simplified Chinese.* "This study explores the potential of medical image analysis to address this challenge" is a sentence, and it is translated as "这种研究探讨了医疗图像分析如何解决这一挑战" (this study explores the potential of medical image analysis to address this challenge) in Simplified Chinese.* "We propose machine-learning techniques for predicting pneumonia from chest X-ray images" is a sentence, and it is translated as "我们提议使用机器学习技术预测肺炎从胸部X射图像" (we propose machine-learning techniques for predicting pneumonia from chest X-ray images) in Simplified Chinese.* "Chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness" is a sentence, and it is translated as "胸部X射图像诊断肺炎具有可行性和成本效益" (chest X-ray imaging is vital for pneumonia diagnosis due to its accessibility and cost-effectiveness) in Simplified Chinese.* "However, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions" is a sentence, and it is translated as "然而，从X射图像中诊断肺炎可能会具有复杂性，因为肺炎的Radiographic特征可能与其他呼吸道疾病重叠" (however, interpreting X-rays for pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions) in Simplified Chinese.* "We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients" is a sentence, and it is translated as "我们使用不同的机器学习模型，包括DenseNet121、Inception Resnet-v2、Inception Resnet-v3、Resnet50和Xception，使用肺炎患者的胸部X射图像进行评估" (we evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients) in Simplified Chinese.* "Performance measures and confusion matrices are employed to assess and compare the models" is a sentence, and it is translated as "我们使用性能指标和混淆矩阵来评估和比较不同模型的表现" (performance measures and confusion matrices are employed to assess and compare the models) in Simplified Chinese.* "The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%" is a sentence, and it is translated as "发现结果表明，DenseNet121模型在识别肺炎方面的准确率为99.58%" (the findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%) in Simplified Chinese.* "This study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images" is a sentence, and it is translated as "这种研究强调了机器学习在肺炎检测中的重要性，利用胸部X射图像" (this study underscores the significance of machine learning in the accurate detection of pneumonia, leveraging chest X-ray images) in Simplified Chinese.* "Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics" is a sentence, and it is translated as "我们的研究提供了有关技术在防止肺炎传播的精准诊断方面的洞察" (our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics) in Simplified Chinese.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/21/eess.IV_2023_08_21/" data-id="clm0t8e2w00fiv7882x3vgk1e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/21/cs.SD_2023_08_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-21 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/20/cs.LG_2023_08_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-20 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
