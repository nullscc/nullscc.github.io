
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-21 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Extraction of Text from Optic Nerve Optical Coherence Tomography Reports paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10790 repo_url: None paper_authors: Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-21">
<meta property="og:url" content="https://nullscc.github.io/2023/08/21/eess.IV_2023_08_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Extraction of Text from Optic Nerve Optical Coherence Tomography Reports paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10790 repo_url: None paper_authors: Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-21T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:52:08.193Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/21/eess.IV_2023_08_21/" class="article-date">
  <time datetime="2023-08-21T09:00:00.000Z" itemprop="datePublished">2023-08-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-21
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Extraction-of-Text-from-Optic-Nerve-Optical-Coherence-Tomography-Reports"><a href="#Extraction-of-Text-from-Optic-Nerve-Optical-Coherence-Tomography-Reports" class="headerlink" title="Extraction of Text from Optic Nerve Optical Coherence Tomography Reports"></a>Extraction of Text from Optic Nerve Optical Coherence Tomography Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10790">http://arxiv.org/abs/2308.10790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iyad Majid, Youchen Victor Zhang, Robert Chang, Sophia Y. Wang</li>
<li>for: 这项研究的目的是开发和评估基于规则的算法，以提高光学同步 Tomatoes OCT扫描报告中的文本数据提取，包括肾脚层（RNFL）值和其他神经细胞计数（GCC）数据。</li>
<li>methods: 这项研究使用了DICOM文件，其中包含PDF报告，并使用PaddleOCR Python包进行光学字符识别。基于规则的算法被设计和优化，以提高RNFL和GCC数据提取的精度。</li>
<li>results: 研究显示，开发的算法可以准确地从RNFL和GCC扫描报告中提取数据，其中OD和OS之间的准确率有所不同。这些值可以大幅提高大规模的OCT结果提取速度。<details>
<summary>Abstract</summary>
Purpose: The purpose of this study was to develop and evaluate rule-based algorithms to enhance the extraction of text data, including retinal nerve fiber layer (RNFL) values and other ganglion cell count (GCC) data, from Zeiss Cirrus optical coherence tomography (OCT) scan reports. Methods: DICOM files that contained encapsulated PDF reports with RNFL or Ganglion Cell in their document titles were identified from a clinical imaging repository at a single academic ophthalmic center. PDF reports were then converted into image files and processed using the PaddleOCR Python package for optical character recognition. Rule-based algorithms were designed and iteratively optimized for improved performance in extracting RNFL and GCC data. Evaluation of the algorithms was conducted through manual review of a set of RNFL and GCC reports. Results: The developed algorithms demonstrated high precision in extracting data from both RNFL and GCC scans. Precision was slightly better for the right eye in RNFL extraction (OD: 0.9803 vs. OS: 0.9046), and for the left eye in GCC extraction (OD: 0.9567 vs. OS: 0.9677). Some values presented more challenges in extraction, particularly clock hours 5 and 6 for RNFL thickness, and signal strength for GCC. Conclusions: A customized optical character recognition algorithm can identify numeric results from optical coherence scan reports with high precision. Automated processing of PDF reports can greatly reduce the time to extract OCT results on a large scale.
</details>
<details>
<summary>摘要</summary>
目的：本研究旨在开发和评估基于规则的算法，以提高从Zeiss Cirrus光学同步图像（OCT）扫描报告中提取文本数据的精度，包括视网膜神经层（RNFL）值和其他神经细胞计数（GCC）数据。方法：从一所学术眼科中心的临床扫描存储库中提取符合DICOM文件格式的PDF报告，其中文档标题包含“RNFL”或“GCC”。PDF报告被转换为图像文件，并使用Python中的PaddleOCR包进行光学字符识别。为了提高数据提取的精度，我们设计了规则基本的算法，并在多个迭代优化过程中进行优化。结果：开发的算法在RNFL和GCC扫描报告中的数据提取中具有高精度。对于右眼RNFL提取，精度为0.9803，对于左眼GCC提取，精度为0.9567。然而，某些值在提取时存在更大的挑战，包括时钟小时5和6的RNFL厚度，以及GCC的信号强度。结论：可以使用自定义的光学字符识别算法来从OCT扫描报告中提取数据，并且自动处理PDF报告可以大幅降低在大规模上提取OCT结果的时间。
</details></li>
</ul>
<hr>
<h2 id="Dense-Error-Map-Estimation-for-MRI-Ultrasound-Registration-in-Brain-Tumor-Surgery-Using-Swin-UNETR"><a href="#Dense-Error-Map-Estimation-for-MRI-Ultrasound-Registration-in-Brain-Tumor-Surgery-Using-Swin-UNETR" class="headerlink" title="Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR"></a>Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10784">http://arxiv.org/abs/2308.10784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soorena Salari, Amirhossein Rasoulian, Hassan Rivaz, Yiming Xiao</li>
<li>for: 降低手术死亡率的早期脑肿瘤手术是关键，但是手术期间脑组织变形（脑Shift）会使得先前的图像无效。因此，成本低廉，可携带的内部手术超声（iUS）可以跟踪脑Shift，并准确地将MRI-iUS注射技术与先前的图像进行匹配。这可以提高手术安全性和效果，最大化肿瘤除除 while avoiding critical regions.</li>
<li>methods: 我们提出了一种基于深度学习（DL）的框架，使用Swin UNITER来自动评估MRI-iUS注射结果的3D密集错误地图，并在实际临床数据上显示其性能。</li>
<li>results: 我们的方法可以准确地评估MRI-iUS注射结果的质量，并且可以在实际临床情况下提高手术安全性和效果。<details>
<summary>Abstract</summary>
Early surgical treatment of brain tumors is crucial in reducing patient mortality rates. However, brain tissue deformation (called brain shift) occurs during the surgery, rendering pre-operative images invalid. As a cost-effective and portable tool, intra-operative ultrasound (iUS) can track brain shift, and accurate MRI-iUS registration techniques can update pre-surgical plans and facilitate the interpretation of iUS. This can boost surgical safety and outcomes by maximizing tumor removal while avoiding eloquent regions. However, manual assessment of MRI-iUS registration results in real-time is difficult and prone to errors due to the 3D nature of the data. Automatic algorithms that can quantify the quality of inter-modal medical image registration outcomes can be highly beneficial. Therefore, we propose a novel deep-learning (DL) based framework with the Swin UNETR to automatically assess 3D-patch-wise dense error maps for MRI-iUS registration in iUS-guided brain tumor resection and show its performance with real clinical data for the first time.
</details>
<details>
<summary>摘要</summary>
早期脑肿手术治疗是减少病人死亡率的关键。然而，手术期间脑组织变形（称为脑移动）会使先前的成像无效。作为一种可靠且可搬运的工具，实时ultrasound（iUS）可以跟踪脑移动，并且精准的MRI-iUS注册技术可以更新先前的计划，并且可以帮助解释iUS。这可以提高手术安全性和效果，最大化肿瘤除除而避免重要区域。然而，手动评估MRI-iUS注册结果的实时性困难，并且容易出错，因为数据的3D性。自动的算法可以评估多Modal医疗影像注册结果的质量。因此，我们提出了一种基于深度学习（DL）的框架，使用Swin UNITER来自动评估3D-patch-wise稠密错误地图 дляMRI-iUS注册，并且在实际临床数据上展示其性能。
</details></li>
</ul>
<hr>
<h2 id="Automated-Identification-of-Failure-Cases-in-Organ-at-Risk-Segmentation-Using-Distance-Metrics-A-Study-on-CT-Data"><a href="#Automated-Identification-of-Failure-Cases-in-Organ-at-Risk-Segmentation-Using-Distance-Metrics-A-Study-on-CT-Data" class="headerlink" title="Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data"></a>Automated Identification of Failure Cases in Organ at Risk Segmentation Using Distance Metrics: A Study on CT Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10636">http://arxiv.org/abs/2308.10636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Honarmandi Shandiz, Attila Rádics, Rajesh Tamada, Makk Árpád, Karolina Glowacka, Lehel Ferenczi, Sandeep Dutta, Michael Fanariotis</li>
<li>for: 提高自动组织器的肿瘤识别精度，以避免治疗规划错误。</li>
<li>methods: 使用自动模型生成的肿瘤边界概率分布，并使用Dice和 Hausdorff距离的组合来自动标识失败案例。</li>
<li>results: 通过设置Dice和 Hausdorff距离的阈值，可以快速自动标识失败案例，并且可以 diferenciate 不同类型的失败案例。<details>
<summary>Abstract</summary>
Automated organ at risk (OAR) segmentation is crucial for radiation therapy planning in CT scans, but the generated contours by automated models can be inaccurate, potentially leading to treatment planning issues. The reasons for these inaccuracies could be varied, such as unclear organ boundaries or inaccurate ground truth due to annotation errors. To improve the model's performance, it is necessary to identify these failure cases during the training process and to correct them with some potential post-processing techniques. However, this process can be time-consuming, as traditionally it requires manual inspection of the predicted output. This paper proposes a method to automatically identify failure cases by setting a threshold for the combination of Dice and Hausdorff distances. This approach reduces the time-consuming task of visually inspecting predicted outputs, allowing for faster identification of failure case candidates. The method was evaluated on 20 cases of six different organs in CT images from clinical expert curated datasets. By setting the thresholds for the Dice and Hausdorff distances, the study was able to differentiate between various states of failure cases and evaluate over 12 cases visually. This thresholding approach could be extended to other organs, leading to faster identification of failure cases and thereby improving the quality of radiation therapy planning.
</details>
<details>
<summary>摘要</summary>
自动化的器官在险 (OAR) 分 segmentation 是辐射疗程规划 CT 扫描中的关键，但自动生成的界限可能不准确，可能导致治疗规划问题。这些不准确的原因可能是不清晰的器官边界或不准确的真实数据 due to 注释错误。要提高模型的表现，需要在训练过程中确认这些失败案例，并使用一些可能的后处理技术来更正它们。但是，这个过程可能会占用时间，因为传统上需要手动检查预测的输出。这篇论文提出了一种方法，通过设置 Dice 和 Hausdorff 距离的组合来自动确定失败案例。这种方法可以减少手动检查预测输出的时间消耗，使得更快地确定失败案例候选者。本研究在 20 个不同的器官中进行了 CT 图像的评估，并通过设置 Dice 和 Hausdorff 距离的阈值，可以区分不同的失败案例状态，并评估了 12 个案例。这种阈值设定方法可以扩展到其他器官，从而更快地确定失败案例，提高辐射疗程规划的质量。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Medical-Image-Segmentation-Optimizing-Cross-Entropy-Weights-and-Post-Processing-with-Autoencoders"><a href="#Enhancing-Medical-Image-Segmentation-Optimizing-Cross-Entropy-Weights-and-Post-Processing-with-Autoencoders" class="headerlink" title="Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders"></a>Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10488">http://arxiv.org/abs/2308.10488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Singh, Luoyao Chen, Mei Chen, Jinqian Pan, Raviteja Chukkapalli, Shravan Chaudhari, Jacopo Cirrone<br>for:This paper is written for the task of medical image segmentation, specifically focusing on the analysis of cell inflammation and interaction in autoimmune diseases like dermatomyositis.methods:The proposed method uses a deep-learning approach tailored for medical image segmentation, which includes the use of U-Net and U-Net++ architectures with optimized loss function weights.results:The proposed method outperforms the current state-of-the-art techniques by an average of 12.26% for U-Net and 12.04% for U-Net++ across the ResNet family of encoders on the dermatomyositis dataset.<details>
<summary>Abstract</summary>
The task of medical image segmentation presents unique challenges, necessitating both localized and holistic semantic understanding to accurately delineate areas of interest, such as critical tissues or aberrant features. This complexity is heightened in medical image segmentation due to the high degree of inter-class similarities, intra-class variations, and possible image obfuscation. The segmentation task further diversifies when considering the study of histopathology slides for autoimmune diseases like dermatomyositis. The analysis of cell inflammation and interaction in these cases has been less studied due to constraints in data acquisition pipelines. Despite the progressive strides in medical science, we lack a comprehensive collection of autoimmune diseases. As autoimmune diseases globally escalate in prevalence and exhibit associations with COVID-19, their study becomes increasingly essential. While there is existing research that integrates artificial intelligence in the analysis of various autoimmune diseases, the exploration of dermatomyositis remains relatively underrepresented. In this paper, we present a deep-learning approach tailored for Medical image segmentation. Our proposed method outperforms the current state-of-the-art techniques by an average of 12.26% for U-Net and 12.04% for U-Net++ across the ResNet family of encoders on the dermatomyositis dataset. Furthermore, we probe the importance of optimizing loss function weights and benchmark our methodology on three challenging medical image segmentation tasks
</details>
<details>
<summary>摘要</summary>
医学影像分割任务存在独特的挑战，需要同时具备地方化和整体的semantic理解，以准确地界定关键组织或异常特征。这种复杂性由医学影像分割中高度的间类相似性、内类变化和可能存在的图像掩蔽所增加。在医学影像分割任务中，考虑 histopathology 扫描片的涂抹病例，如dermatomyositis，分割任务会更加多样化。由于医学科学的进步，我们缺乏一个全面的自适应病种集成。随着自适应病种的全球流行病例和COVID-19的相关性，其研究变得越来越重要。虽然现有的研究已经将人工智能Integrated into the analysis of various autoimmune diseases，但dermatomyositis的研究仍然相对落后。在这篇文章中，我们提出了一种适应医学影像分割的深度学习方法。我们的提议方法在ResNet家族encoder上的dermatomyositis数据集上比现状前景技术平均提高12.26%和12.04%。此外，我们还探索了优化损失函数的权重和对三个困难的医学影像分割任务进行比较。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Pneumonia-and-COVID-19-Using-Deep-Neural-Networks"><a href="#Prediction-of-Pneumonia-and-COVID-19-Using-Deep-Neural-Networks" class="headerlink" title="Prediction of Pneumonia and COVID-19 Using Deep Neural Networks"></a>Prediction of Pneumonia and COVID-19 Using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10368">http://arxiv.org/abs/2308.10368</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. S. Haque, M. S. Taluckder, S. B. Shawkat, M. A. Shahriyar, M. A. Sayed, C. Modak</li>
<li>for: 这个研究旨在采用医疗图像分析技术来检测肺炎。</li>
<li>methods: 该研究使用了多种机器学习模型，包括 DenseNet121、Inception Resnet-v2、Inception Resnet-v3、Resnet50 和 Xception，以分析肺 X 光图像。</li>
<li>results: 研究发现，DenseNet121 模型在肺炎检测中表现最佳，准确率达到 99.58%。<details>
<summary>Abstract</summary>
Pneumonia, caused by bacteria and viruses, is a rapidly spreading viral infection with global implications. Prompt identification of infected individuals is crucial for containing its transmission. This study explores the potential of medical image analysis to address this challenge. We propose machine-learning techniques for predicting Pneumonia from chest X-ray images. Chest X-ray imaging is vital for Pneumonia diagnosis due to its accessibility and cost-effectiveness. However, interpreting X-rays for Pneumonia detection can be complex, as radiographic features can overlap with other respiratory conditions. We evaluate the performance of different machine learning models, including DenseNet121, Inception Resnet-v2, Inception Resnet-v3, Resnet50, and Xception, using chest X-ray images of pneumonia patients. Performance measures and confusion matrices are employed to assess and compare the models. The findings reveal that DenseNet121 outperforms other models, achieving an accuracy rate of 99.58%. This study underscores the significance of machine learning in the accurate detection of Pneumonia, leveraging chest X-ray images. Our study offers insights into the potential of technology to mitigate the spread of pneumonia through precise diagnostics.
</details>
<details>
<summary>摘要</summary>
“肺炎，由病毒和 бактеリум引起，是一种快速传播的病毒感染，具有全球化的意义。 Prompt 识别感染者是防止传播的关键。这项研究探讨医疗图像分析可以解决这个挑战。我们提出运用机器学习技术预测肺炎的方法。颈部X线图像是肺炎诊断的重要工具，因为它具有访问性和成本效益。但是，解释颈部X线可能会与其他呼吸道疾病相似，因此需要进一步的分析。我们评估了不同的机器学习模型，包括DenseNet121、Inception Resnet-v2、Inception Resnet-v3、Resnet50和Xception，使用颈部X线图像。我们使用性能指标和混淆矩阵来评估和比较这些模型。研究结果显示DenseNet121最高的准确率为99.58%。这项研究认为机器学习可以帮助精确诊断肺炎，利用颈部X线图像。我们的研究给出了技术可以减少肺炎的传播的可能性。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/21/eess.IV_2023_08_21/" data-id="clp9qz8dr019aok8895xdckba" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/21/cs.LG_2023_08_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-21
        
      </div>
    </a>
  
  
    <a href="/2023/08/20/cs.SD_2023_08_20/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-20</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
