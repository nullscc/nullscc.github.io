
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-21 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Graph Neural Bandits paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10808 repo_url: https:&#x2F;&#x2F;github.com&#x2F;lasgroup&#x2F;GNNBO paper_authors: Yunzhe Qi, Yikun Ban, Jingrui He for: 这篇论文是为了提出一种基于图神经网络的推荐算法框架，以便利用用户之间的协同作用">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-21 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/21/cs.LG_2023_08_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Graph Neural Bandits paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10808 repo_url: https:&#x2F;&#x2F;github.com&#x2F;lasgroup&#x2F;GNNBO paper_authors: Yunzhe Qi, Yikun Ban, Jingrui He for: 这篇论文是为了提出一种基于图神经网络的推荐算法框架，以便利用用户之间的协同作用">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-20T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:34.642Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/21/cs.LG_2023_08_21/" class="article-date">
  <time datetime="2023-08-20T16:00:00.000Z" itemprop="datePublished">2023-08-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-21 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Graph-Neural-Bandits"><a href="#Graph-Neural-Bandits" class="headerlink" title="Graph Neural Bandits"></a>Graph Neural Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10808">http://arxiv.org/abs/2308.10808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lasgroup/GNNBO">https://github.com/lasgroup/GNNBO</a></li>
<li>paper_authors: Yunzhe Qi, Yikun Ban, Jingrui He</li>
<li>for: 这篇论文是为了提出一种基于图神经网络的推荐算法框架，以便利用用户之间的协同作用来提高推荐效果。</li>
<li>methods: 该论文使用了图神经网络来建模用户之间的协同关系，并分别使用GNN-based模型来进行优化的推荐策略。</li>
<li>results: 该论文通过对多个实际数据集进行比较，证明了其提出的推荐算法框架的有效性。<details>
<summary>Abstract</summary>
Contextual bandits algorithms aim to choose the optimal arm with the highest reward out of a set of candidates based on the contextual information. Various bandit algorithms have been applied to real-world applications due to their ability of tackling the exploitation-exploration dilemma. Motivated by online recommendation scenarios, in this paper, we propose a framework named Graph Neural Bandits (GNB) to leverage the collaborative nature among users empowered by graph neural networks (GNNs). Instead of estimating rigid user clusters as in existing works, we model the "fine-grained" collaborative effects through estimated user graphs in terms of exploitation and exploration respectively. Then, to refine the recommendation strategy, we utilize separate GNN-based models on estimated user graphs for exploitation and adaptive exploration. Theoretical analysis and experimental results on multiple real data sets in comparison with state-of-the-art baselines are provided to demonstrate the effectiveness of our proposed framework.
</details>
<details>
<summary>摘要</summary>
Contextual bandits算法目的是选择最高奖励的武器（arm）中的集合基于Contextual信息。多种bandit算法在实际应用中使用，因为它们可以解决探索和利用之间的矛盾。在这篇论文中，我们提出了一个名为图 neural bandits（GNB）的框架，利用用户之间的协同作用，激活了图神经网络（GNNs）。而不是在现有的工作中估计固定的用户群集，我们通过估计用户图来模型“细化”的协同效果，分别用于探索和适应探索。然后，我们使用分开的GNN-based模型来修改推荐策略，并对多个实际数据集进行了 teoretic 分析和实验研究，以证明我们的提出的框架的效果。
</details></li>
</ul>
<hr>
<h2 id="DynED-Dynamic-Ensemble-Diversification-in-Data-Stream-Classification"><a href="#DynED-Dynamic-Ensemble-Diversification-in-Data-Stream-Classification" class="headerlink" title="DynED: Dynamic Ensemble Diversification in Data Stream Classification"></a>DynED: Dynamic Ensemble Diversification in Data Stream Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10807">http://arxiv.org/abs/2308.10807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soheilabadifard/dyned">https://github.com/soheilabadifard/dyned</a></li>
<li>paper_authors: Soheil Abadifard, Sepehr Bakhshi, Sanaz Gheibuni, Fazli Can</li>
<li>for: 这篇论文是为了提高在数据流中的类别准确率，因为数据流中的变化可能会导致模型的性能下降。</li>
<li>methods: 这篇论文使用了一种新的集成方法，叫做DynED，它可以在数据流中维持集成模型的性能。DynED使用了MMR（最大margin relevance）来选择最佳的集成元件，以确保集成模型的各个元件都能够做出贡献。</li>
<li>results: 根据实验结果，DynED在11个 sintetic dataset和4个真实 dataset上的平均准确率高于5个现有的基eline。<details>
<summary>Abstract</summary>
Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
ensemble方法在分类 task 中广泛应用，因其表现出色。在数据流环境中达到高精度是一项具有挑战性的任务，因为数据分布会不断变化，也就是说 concept drift。更多的ensemble组件可以提高预测精度。 despite the diversity of components within an ensemble, not all contribute as expected to its overall performance。这种情况需要一种方法来选择表现好的和多样的组件。我们提出了一种基于 MMR（最大边际相关）的新集成建立和维护方法，可以在构建集成时动态组合多样性和预测精度。实验结果表明，我们的方法（DynED）在14个实验数据集上比基eline5种状态 искусственный顶峰表现出更高的平均含义精度。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Frank-Wolfe-Optimization-Layer"><a href="#Differentiable-Frank-Wolfe-Optimization-Layer" class="headerlink" title="Differentiable Frank-Wolfe Optimization Layer"></a>Differentiable Frank-Wolfe Optimization Layer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10806">http://arxiv.org/abs/2308.10806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Liu, Liu Liu, Xueqian Wang, Peilin Zhao</li>
<li>for: 提出了一种高效的梯度下降优化层（DFWLayer），用于解决具有约束的机器学习问题。</li>
<li>methods: 基于Frank-Wolfe算法，DFWLayer不需要计算投影和希格曼矩阵，从而实现了高效的大规模问题解决方法。</li>
<li>results: 在实验中，DFWLayer不仅实现了竞争性的准确率和梯度，还可靠地遵守约束。此外，它在前向和反向计算速度方面也超过了基elines。<details>
<summary>Abstract</summary>
Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks. The existing methods leverages the optimality conditions and implicit function theorem to obtain the Jacobian matrix of the output, which increases the computational cost and limits the application of differentiable optimization. In addition, some non-differentiable constraints lead to more challenges when using prior differentiable optimization layers. This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimization algorithm which can solve constrained optimization problems without projections and Hessian matrix computations, thus leading to a efficient way of dealing with large-scale problems. Theoretically, we establish a bound on the suboptimality gap of the DFWLayer in the context of l1-norm constraints. Experimental assessments demonstrate that the DFWLayer not only attains competitive accuracy in solutions and gradients but also consistently adheres to constraints. Moreover, it surpasses the baselines in both forward and backward computational speeds.
</details>
<details>
<summary>摘要</summary>
differentiable 优化在机器学习领域中得到了广泛的关注，因为它在神经网络上构建的机器学习模型中扮演了基础性的角色。现有的方法利用优化性条件和隐函数定理来获取输出的雅可比矩阵，这会增加计算成本并限制 differentiable 优化的应用。此外，一些非 differentiable 约束会使得使用先前的 differentiable 优化层更加困难。这篇论文提出了一种名为差分可 differentiable Frank-Wolfe 层（DFWLayer）的层，通过折衔 Frank-Wolfe 算法，该算法可以解决具有约束的优化问题，而不需要计算投影和资料矩阵，因此可以方便地处理大规模问题。理论上，我们提出了在 l1-norm 约束下的优化误差幅度的下界。实验评估表明，DFWLayer 不仅在解决方案和梯度方面达到了竞争性的准确性，而且一致地遵循约束。此外，它在前向和后向计算速度方面也超过了基eline。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-Unsupervised-Environment-Design-with-a-Learned-Adversary"><a href="#Stabilizing-Unsupervised-Environment-Design-with-a-Learned-Adversary" class="headerlink" title="Stabilizing Unsupervised Environment Design with a Learned Adversary"></a>Stabilizing Unsupervised Environment Design with a Learned Adversary</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10797">http://arxiv.org/abs/2308.10797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/dcd">https://github.com/facebookresearch/dcd</a></li>
<li>paper_authors: Ishita Mediratta, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, Tim Rocktäschel</li>
<li>for: 本研究旨在提高普通能力Agent的训练，通过设计适应环境变化的训练任务来促进广泛的普通化和稳定性。</li>
<li>methods: 本研究使用了强化学习（RL）来训练教师策略，从 scratch generate任务，使得可以直接生成适应当前机器人能力的任务。</li>
<li>results: 本研究在多个已知的难度 Navigation 和赛车环境中实现了与当前状态的比较或超越，生成了可靠的普通化Agent。<details>
<summary>Abstract</summary>
A key challenge in training generally-capable agents is the design of training tasks that facilitate broad generalization and robustness to environment variations. This challenge motivates the problem setting of Unsupervised Environment Design (UED), whereby a student agent trains on an adaptive distribution of tasks proposed by a teacher agent. A pioneering approach for UED is PAIRED, which uses reinforcement learning (RL) to train a teacher policy to design tasks from scratch, making it possible to directly generate tasks that are adapted to the agent's current capabilities. Despite its strong theoretical backing, PAIRED suffers from a variety of challenges that hinder its practical performance. Thus, state-of-the-art methods currently rely on curation and mutation rather than generation of new tasks. In this work, we investigate several key shortcomings of PAIRED and propose solutions for each shortcoming. As a result, we make it possible for PAIRED to match or exceed state-of-the-art methods, producing robust agents in several established challenging procedurally-generated environments, including a partially-observed maze navigation task and a continuous-control car racing environment. We believe this work motivates a renewed emphasis on UED methods based on learned models that directly generate challenging environments, potentially unlocking more open-ended RL training and, as a result, more general agents.
</details>
<details>
<summary>摘要</summary>
training 通常遇到的一个挑战是设计训练任务，以便在环境变化时能够广泛通用和Robust。这个挑战激发了无监督环境设计（UED）的问题，其中学生机器人通过一个教师机器人提供的 adaptive 任务分布进行训练。一种开拓性的方法是 PAIRED，它使用强化学习（RL）来训练一个教师策略，从scratch生成任务，使得可以直接生成适应机器人当前能力的任务。然而，PAIRED 受到多种挑战，这些挑战使得实际性受到影响。因此，当前的state-of-the-art方法通常采用审核和变化而不是生成新任务。在这种情况下，我们调查了 PAIRED 的一些关键缺陷，并提出了解决方案。结果是，我们使得 PAIRED 能够与当前的state-of-the-art方法匹配或超越，在一些已知的复杂生成环境中训练 robust 的机器人，包括部分观察 Maze 导航任务和连续控制汽车竞赛环境。我们认为这项工作将激励更多的 UED 方法基于学习的模型，直接生成挑战任务，可能解锁更多的 open-ended RL 训练和更一般的机器人。
</details></li>
</ul>
<hr>
<h2 id="MGMAE-Motion-Guided-Masking-for-Video-Masked-Autoencoding"><a href="#MGMAE-Motion-Guided-Masking-for-Video-Masked-Autoencoding" class="headerlink" title="MGMAE: Motion Guided Masking for Video Masked Autoencoding"></a>MGMAE: Motion Guided Masking for Video Masked Autoencoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10794">http://arxiv.org/abs/2308.10794</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingkun Huang, Zhiyu Zhao, Guozhen Zhang, Yu Qiao, Limin Wang</li>
<li>For: The paper is focused on improving the performance of video masked autoencoding (MMAE) by incorporating motion information into the masking strategy.* Methods: The paper introduces a motion guided masking strategy that explicitly incorporates motion information to build temporal consistent masking volume. The authors also use an online efficient optical flow estimator and backward masking map warping strategy to implement their method.* Results: The authors demonstrate superior performance of their motion guided MMAE (MGMAE) compared to the original VideoMAE on the datasets of Something-Something V2 and Kinetics-400. They also provide visualization analysis to illustrate that their method can sample temporal consistent cubes in a motion-adaptive manner for more effective video pre-training.Here is the simplified Chinese text for the three key points:* 为：本文主要关注提高视频Masked Autoencoding（MMAE）性能，通过 incorporating 运动信息到Masking策略中。* 方法：本文引入运动指导Masking策略，其中Explicitly incorporates 运动信息构建 temporal consistent 的Masking volume。作者还使用了在线高效的Optical flow estimator和backward Masking map warping策略来实现方法。* 结果：作者在Something-Something V2和Kinetics-400 datasets上展示了MGMAE的超越VideoMAE性能。他们还提供了可视化分析，以 illustrate MGMAE可以在运动响应的方式采样时间一致的cubes，以便更有效地预训练视频。<details>
<summary>Abstract</summary>
Masked autoencoding has shown excellent performance on self-supervised video representation learning. Temporal redundancy has led to a high masking ratio and customized masking strategy in VideoMAE. In this paper, we aim to further improve the performance of video masked autoencoding by introducing a motion guided masking strategy. Our key insight is that motion is a general and unique prior in video, which should be taken into account during masked pre-training. Our motion guided masking explicitly incorporates motion information to build temporal consistent masking volume. Based on this masking volume, we can track the unmasked tokens in time and sample a set of temporal consistent cubes from videos. These temporal aligned unmasked tokens will further relieve the information leakage issue in time and encourage the MGMAE to learn more useful structure information. We implement our MGMAE with an online efficient optical flow estimator and backward masking map warping strategy. We perform experiments on the datasets of Something-Something V2 and Kinetics-400, demonstrating the superior performance of our MGMAE to the original VideoMAE. In addition, we provide the visualization analysis to illustrate that our MGMAE can sample temporal consistent cubes in a motion-adaptive manner for more effective video pre-training.
</details>
<details>
<summary>摘要</summary>
卷积自编码（Masked Autoencoding）在自我监督视频表示学习中表现出色。视频中的时间重复性导致了高的面罩率和自定义面罩策略，在这篇论文中，我们想要进一步提高视频卷积自编码的性能，通过引入运动指导的面罩策略。我们的关键发现是，运动是视频中一个通用和特殊的先验，应该在面罩预训练中考虑。我们的运动指导面罩 explictly incorporates 运动信息，建立了时间一致的面罩量。基于这个面罩量，我们可以在时间上跟踪不面罩的 токен，并从视频中采样一组时间一致的立方体。这些时间一致的不面罩的 токен将进一步减轻时间泄露问题，促使MGMAE学习更有用的结构信息。我们实现了我们的MGMAE，使用了在线高效的滤波器估计器和反向面罩地图抽象策略。我们在Something-Something V2和Kinetics-400 datasets上进行了实验，并证明了我们的MGMAE在原始VideoMAE的基础上具有更高的性能。此外，我们还提供了视觉分析，以 Illustrate 我们的MGMAE可以在运动适应的方式下采样时间一致的立方体，以更有效地进行视频预训练。
</details></li>
</ul>
<hr>
<h2 id="Instruction-Tuning-for-Large-Language-Models-A-Survey"><a href="#Instruction-Tuning-for-Large-Language-Models-A-Survey" class="headerlink" title="Instruction Tuning for Large Language Models: A Survey"></a>Instruction Tuning for Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10792">http://arxiv.org/abs/2308.10792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang</li>
<li>for: 本文审查了大语言模型（LLM）的指令调整（IT）研究，以提高LLM的能力和可控性。</li>
<li>methods: 本文使用系统性的文献综述方法，包括IT的总方法、IT数据集的构建、IT模型的训练、不同Modalities、领域和应用程序中的应用，以及影响IT结果的因素的分析（如生成指令输出、指令数据集的大小等）。</li>
<li>results: 本文对IT的总体结果进行了分析，包括IT的应用、潜在的坏处和批评，以及现有策略的不足和未来研究的可能性。<details>
<summary>Abstract</summary>
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.
</details>
<details>
<summary>摘要</summary>
The paper provides a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains, and applications. The review also analyzes factors that influence the outcome of IT, such as the generation of instruction outputs and the size of the instruction dataset.Furthermore, the paper discusses potential pitfalls of IT and criticism against it, as well as efforts to address current deficiencies in existing strategies and suggests avenues for fruitful research. Overall, the paper provides a comprehensive overview of the current state of IT research and its potential applications in various domains.Here is the translation in Simplified Chinese:这篇论文检讨了大语言模型（LLM）的指令调整（IT）研究，这是一种能够提高LLM的能力和可控性的重要技术。IT通过在指令集合（instruction, output）的supervised模式下进行进一步训练LLM， bridge了LLM的下一个词预测目标和用户的目标，即使LLM遵循人类的指令。论文提供了系统性的文献综述，包括IT的总方法、IT数据集的建构、IT模型的训练、以及不同的modalities、domains和应用程序中的应用。文献还分析了IT的结果受到的因素，如生成指令输出和指令集合的大小。此外，论文还讨论了IT的潜在弱点和批评，以及现有策略的不足和改进的可能性。文献还提出了一些有优势的研究方向。总的来说，论文提供了大语言模型研究的现状和IT研究的潜在应用领域。
</details></li>
</ul>
<hr>
<h2 id="Zero-and-Few-Shot-Prompting-with-LLMs-A-Comparative-Study-with-Fine-tuned-Models-for-Bangla-Sentiment-Analysis"><a href="#Zero-and-Few-Shot-Prompting-with-LLMs-A-Comparative-Study-with-Fine-tuned-Models-for-Bangla-Sentiment-Analysis" class="headerlink" title="Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis"></a>Zero- and Few-Shot Prompting with LLMs: A Comparative Study with Fine-tuned Models for Bangla Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10783">http://arxiv.org/abs/2308.10783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum, Avijit Sarker, Sheak Rashed Haider Noori</li>
<li>for: 本研究旨在提供大量手动标注的孟加拉语新闻微博和Facebook评论数据集，以及对多种语言模型进行零、几shot在场景学习的研究。</li>
<li>methods: 本研究使用了零、几shot在场景学习的方法，包括Flan-T5、GPT-4和Bloomz等语言模型，并进行了对比研究。</li>
<li>results: 研究发现，单语言变换器模型在零、几shot场景下表现出了强劲的性能，超过其他模型。<details>
<summary>Abstract</summary>
The rapid expansion of the digital world has propelled sentiment analysis into a critical tool across diverse sectors such as marketing, politics, customer service, and healthcare. While there have been significant advancements in sentiment analysis for widely spoken languages, low-resource languages, such as Bangla, remain largely under-researched due to resource constraints. Furthermore, the recent unprecedented performance of Large Language Models (LLMs) in various applications highlights the need to evaluate them in the context of low-resource languages. In this study, we present a sizeable manually annotated dataset encompassing 33,605 Bangla news tweets and Facebook comments. We also investigate zero- and few-shot in-context learning with several language models, including Flan-T5, GPT-4, and Bloomz, offering a comparative analysis against fine-tuned models. Our findings suggest that monolingual transformer-based models consistently outperform other models, even in zero and few-shot scenarios. To foster continued exploration, we intend to make this dataset and our research tools publicly available to the broader research community. In the spirit of further research, we plan to make this dataset and our experimental resources publicly accessible to the wider research community.
</details>
<details>
<summary>摘要</summary>
随着数字世界的快速扩张，情感分析已成为多个领域的关键工具，包括市场营销、政治、客户服务和医疗等。虽然普通话和其他语言的情感分析得到了 significiant 进步，但低资源语言，如孟加拉语，仍然受到资源限制，因此它们尚未得到了足够的研究。此外，最近的不同应用中大型自然语言处理器（LLMs）的表现也提出了对低资源语言进行评估的需求。在本研究中，我们提供了一个大量手动标注的孟加拉语新闻微博和Facebook评论数据集，并进行了零、几个适应学习和翻译模型的研究。我们的发现表明，单语言转换器基于模型在零和几个适应场景中一直表现出色，even in zero and few-shot scenarios。为了促进进一步的探索，我们计划在未来公开这个数据集和我们的研究工具，以便更广泛的研究者社区可以进行更多的研究。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Linear-Concept-Discovery-Models"><a href="#Sparse-Linear-Concept-Discovery-Models" class="headerlink" title="Sparse Linear Concept Discovery Models"></a>Sparse Linear Concept Discovery Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10782">http://arxiv.org/abs/2308.10782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/konpanousis/conceptdiscoverymodels">https://github.com/konpanousis/conceptdiscoverymodels</a></li>
<li>paper_authors: Konstantinos P. Panousis, Dino Ienco, Diego Marcos</li>
<li>for: This paper aims to improve the interpretability of Deep Neural Networks (DNNs) by proposing a simple and intuitive framework based on Contrastive Language Image models and a single sparse linear layer.</li>
<li>methods: The proposed framework uses a data-driven Bernoulli distribution to infer concept presence and achieve sparsity in the model, which is a novel approach compared to previous CBM methods.</li>
<li>results: The proposed framework outperforms recent CBM approaches in accuracy and achieves high per-example concept sparsity, making it easier to investigate the emerging concepts.<details>
<summary>Abstract</summary>
The recent mass adoption of DNNs, even in safety-critical scenarios, has shifted the focus of the research community towards the creation of inherently intrepretable models. Concept Bottleneck Models (CBMs) constitute a popular approach where hidden layers are tied to human understandable concepts allowing for investigation and correction of the network's decisions. However, CBMs usually suffer from: (i) performance degradation and (ii) lower interpretability than intended due to the sheer amount of concepts contributing to each decision. In this work, we propose a simple yet highly intuitive interpretable framework based on Contrastive Language Image models and a single sparse linear layer. In stark contrast to related approaches, the sparsity in our framework is achieved via principled Bayesian arguments by inferring concept presence via a data-driven Bernoulli distribution. As we experimentally show, our framework not only outperforms recent CBM approaches accuracy-wise, but it also yields high per example concept sparsity, facilitating the individual investigation of the emerging concepts.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a simple and highly intuitive interpretable framework based on Contrastive Language Image models and a single sparse linear layer. Unlike other approaches, our framework achieves sparsity through principled Bayesian arguments by inferring concept presence via a data-driven Bernoulli distribution. Our experiments show that our framework not only outperforms recent CBM approaches in terms of accuracy, but it also yields high per-example concept sparsity, making it easier to investigate the emerging concepts.Here is the text in Simplified Chinese:近些时间，深度神经网络（DNNs）在安全关键场景中广泛应用，导致研究人员强调创建可解释性强的模型。概念瓶颈模型（CBMs）是一种受欢迎的方法，它将隐藏层与人理解的概念绑定在一起，以便调查和修正网络决策的过程。然而，CBMs 通常会受到以下两个问题的影响：（i）性能下降，和（ii）解释性比预期更低，这是因为每个决策中的概念的数量过多。在这项工作中，我们提出了一种简单、易于理解的框架，基于对比语言图像模型和单个稀疏线性层。与相关方法不同，我们的框架中的稀疏性是通过原则性的极 bayesian 理由来实现的，通过数据驱动的 Берну利分布来判断概念存在。我们的实验表明，我们的框架不仅在准确性上超越了最近的 CBM 方法，而且每个例子的概念稀疏性也很高，使得可以轻松地调查出现的概念。
</details></li>
</ul>
<hr>
<h2 id="Mixed-Integer-Projections-for-Automated-Data-Correction-of-EMRs-Improve-Predictions-of-Sepsis-among-Hospitalized-Patients"><a href="#Mixed-Integer-Projections-for-Automated-Data-Correction-of-EMRs-Improve-Predictions-of-Sepsis-among-Hospitalized-Patients" class="headerlink" title="Mixed-Integer Projections for Automated Data Correction of EMRs Improve Predictions of Sepsis among Hospitalized Patients"></a>Mixed-Integer Projections for Automated Data Correction of EMRs Improve Predictions of Sepsis among Hospitalized Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10781">http://arxiv.org/abs/2308.10781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehak Arora, Hassan Mortagy, Nathan Dwarshius, Swati Gupta, Andre L. Holder, Rishikesan Kamaleswaran</li>
<li>for: 这个研究目的是为了提高机器学习模型在医疗内容中自动化诊断决策的精度。</li>
<li>methods: 这篇研究使用了一种创新的投影方法，将诊断领域的专业知识融入机器学习工作流程中，生成重要的元数据，并通过高维混合整数程式来修正患者数据，以确保数据的正确性。</li>
<li>results: 研究结果显示，使用这种投影方法可以提高机器学习分类器在实际医疗 Setting中的性能，特别是在检测 septic shock 的早期检测中，AUROC 为 0.865，精度为 0.922，较无投影的机器学习模型更高。<details>
<summary>Abstract</summary>
Machine learning (ML) models are increasingly pivotal in automating clinical decisions. Yet, a glaring oversight in prior research has been the lack of proper processing of Electronic Medical Record (EMR) data in the clinical context for errors and outliers. Addressing this oversight, we introduce an innovative projections-based method that seamlessly integrates clinical expertise as domain constraints, generating important meta-data that can be used in ML workflows. In particular, by using high-dimensional mixed-integer programs that capture physiological and biological constraints on patient vitals and lab values, we can harness the power of mathematical "projections" for the EMR data to correct patient data. Consequently, we measure the distance of corrected data from the constraints defining a healthy range of patient data, resulting in a unique predictive metric we term as "trust-scores". These scores provide insight into the patient's health status and significantly boost the performance of ML classifiers in real-life clinical settings. We validate the impact of our framework in the context of early detection of sepsis using ML. We show an AUROC of 0.865 and a precision of 0.922, that surpasses conventional ML models without such projections.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型在医疗决策自动化中日益重要。然而，在先前的研究中，缺乏对电子医疗记录（EMR）数据在临床上的正确处理是一大缺点。我们解决这一问题，引入了一种创新的投影方法，它可以融合临床专业知识作为领域约束，生成重要的元数据，可以在机器学习工作流中使用。具体来说，我们使用高维混合整数程序，捕捉了生物和生物学约束，以 Correct patient数据。然后，我们测量了对约束定义的健康范围内的 corrected data 的距离，得到了一个唯一的预测指标，我们称之为 "信任分数"。这些分数可以提供患者的健康状况信息，并在实际临床设置中显著提高机器学习分类器的性能。我们验证了我们的框架在早期识别 septic shock 中的效果，我们显示了 AUC 为 0.865，精度为 0.922，这些结果超过了不含投影的 ML 模型。
</details></li>
</ul>
<hr>
<h2 id="Spear-and-Shield-Adversarial-Attacks-and-Defense-Methods-for-Model-Based-Link-Prediction-on-Continuous-Time-Dynamic-Graphs"><a href="#Spear-and-Shield-Adversarial-Attacks-and-Defense-Methods-for-Model-Based-Link-Prediction-on-Continuous-Time-Dynamic-Graphs" class="headerlink" title="Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs"></a>Spear and Shield: Adversarial Attacks and Defense Methods for Model-Based Link Prediction on Continuous-Time Dynamic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10779">http://arxiv.org/abs/2308.10779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Lee, Juho Lee, Kijung Shin</li>
<li>for: 这 paper 的目的是调查 Temporal Graph Neural Networks (TGNNs) 在静态图模型中的敏感性，并提出了一种简单而有效的针对链接预测任务的对抗攻击方法。</li>
<li>methods: 作者提出了一种名为 T-SPEAR 的对抗攻击方法，通过在预测过程之前对数据进行边扰动，使得预测模型失效。同时，作者还提出了一种名为 T-SHIELD 的Robust Training Approach，通过边筛选和时间稳定性来提高预测模型的Robustness。</li>
<li>results: 实验表明，T-SPEAR 可以很好地降低预测模型的链接预测性能，而且这些攻击可以转移到其他 TGNNs 上，即使它们与预测模型不同。此外，T-SHIELD 可以有效地筛选出对预测模型不良的边，并在对抗攻击时表现出更高的Robustness。<details>
<summary>Abstract</summary>
Real-world graphs are dynamic, constantly evolving with new interactions, such as financial transactions in financial networks. Temporal Graph Neural Networks (TGNNs) have been developed to effectively capture the evolving patterns in dynamic graphs. While these models have demonstrated their superiority, being widely adopted in various important fields, their vulnerabilities against adversarial attacks remain largely unexplored. In this paper, we propose T-SPEAR, a simple and effective adversarial attack method for link prediction on continuous-time dynamic graphs, focusing on investigating the vulnerabilities of TGNNs. Specifically, before the training procedure of a victim model, which is a TGNN for link prediction, we inject edge perturbations to the data that are unnoticeable in terms of the four constraints we propose, and yet effective enough to cause malfunction of the victim model. Moreover, we propose a robust training approach T-SHIELD to mitigate the impact of adversarial attacks. By using edge filtering and enforcing temporal smoothness to node embeddings, we enhance the robustness of the victim model. Our experimental study shows that T-SPEAR significantly degrades the victim model's performance on link prediction tasks, and even more, our attacks are transferable to other TGNNs, which differ from the victim model assumed by the attacker. Moreover, we demonstrate that T-SHIELD effectively filters out adversarial edges and exhibits robustness against adversarial attacks, surpassing the link prediction performance of the naive TGNN by up to 11.2% under T-SPEAR.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Modular-and-Adaptive-System-for-Business-Email-Compromise-Detection"><a href="#A-Modular-and-Adaptive-System-for-Business-Email-Compromise-Detection" class="headerlink" title="A Modular and Adaptive System for Business Email Compromise Detection"></a>A Modular and Adaptive System for Business Email Compromise Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10776">http://arxiv.org/abs/2308.10776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Brabec, Filip Šrajer, Radek Starosta, Tomáš Sixta, Marc Dupont, Miloš Lenoch, Jiří Menšík, Florian Becker, Jakub Boros, Tomáš Pop, Pavel Novák</li>
<li>for: 防止商业电子邮件骗财（BEC）和特攻钓鱼攻击，帮助组织nings worldwide  Addressing the growing sophistication of Business Email Compromise (BEC) and spear phishing attacks, which pose significant challenges to organizations worldwide.</li>
<li>methods:  combining multiple machine learning approaches, including Natural Language Understanding (NLU), to detect BEC-related behaviors across various email modalities such as text, images, metadata, and the email’s communication context. </li>
<li>results:  CAPE, a comprehensive and efficient system for BEC detection that has been proven in a production environment for over two years, with naturally explainable verdicts. The system combines independent ML models and algorithms to detect BEC-related behaviors and adapts continuously through a Bayesian approach that combines limited data with domain knowledge.<details>
<summary>Abstract</summary>
The growing sophistication of Business Email Compromise (BEC) and spear phishing attacks poses significant challenges to organizations worldwide. The techniques featured in traditional spam and phishing detection are insufficient due to the tailored nature of modern BEC attacks as they often blend in with the regular benign traffic. Recent advances in machine learning, particularly in Natural Language Understanding (NLU), offer a promising avenue for combating such attacks but in a practical system, due to limitations such as data availability, operational costs, verdict explainability requirements or a need to robustly evolve the system, it is essential to combine multiple approaches together. We present CAPE, a comprehensive and efficient system for BEC detection that has been proven in a production environment for a period of over two years. Rather than being a single model, CAPE is a system that combines independent ML models and algorithms detecting BEC-related behaviors across various email modalities such as text, images, metadata and the email's communication context. This decomposition makes CAPE's verdicts naturally explainable. In the paper, we describe the design principles and constraints behind its architecture, as well as the challenges of model design, evaluation and adapting the system continuously through a Bayesian approach that combines limited data with domain knowledge. Furthermore, we elaborate on several specific behavioral detectors, such as those based on Transformer neural architectures.
</details>
<details>
<summary>摘要</summary>
企业电子邮件攻击（BEC）和攻击者针对邮件的攻击（spear phishing）的复杂度不断增加，对全球企业造成了严重的挑战。传统的防御策略，如防火墙和杀毒软件，已经无法满足现代BEC攻击的需求，因为这些攻击往往与正常的邮件交互混合在一起。现代机器学习技术，特别是自然语言理解（NLU），在抗击BEC攻击方面提供了一个有希望的方向。然而，在实践中，由于数据可用性、运营成本、评估透明度和持续性提高等因素，需要结合多种方法来实现。我们提出了CAPE，一个涵盖和高效的BEC检测系统，在生产环境中证明了超过两年的有效性。CAPE不是单一的模型，而是一个结合独立的机器学习模型和算法，检测电子邮件中的BEC相关行为，包括文本、图像、元数据和邮件交互Context。这种分解使得CAPE的评决具有自然的透明度。在这篇论文中，我们介绍了CAPE的设计原则和限制，以及模型设计、评估和持续性改进的挑战。此外，我们还详细介绍了一些特定的行为检测器，如基于Transformer нейロ网络架构的检测器。
</details></li>
</ul>
<hr>
<h2 id="GBM-based-Bregman-Proximal-Algorithms-for-Constrained-Learning"><a href="#GBM-based-Bregman-Proximal-Algorithms-for-Constrained-Learning" class="headerlink" title="GBM-based Bregman Proximal Algorithms for Constrained Learning"></a>GBM-based Bregman Proximal Algorithms for Constrained Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10767">http://arxiv.org/abs/2308.10767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenweilin/constrainedgbm">https://github.com/zhenweilin/constrainedgbm</a></li>
<li>paper_authors: Zhenwei Lin, Qi Deng<br>for:* The paper is focused on developing a new algorithm for constrained learning tasks in machine learning, specifically for Neyman-Pearson classification and fairness classification.methods:* The authors adapt the gradient boosting machine (GBM) algorithm for constrained learning tasks within the framework of Bregman proximal algorithms.* They introduce a new Bregman primal-dual method with a global optimality guarantee for convex learning objectives and constraint functions.results:* The authors provide substantial experimental evidence to showcase the effectiveness of the Bregman algorithm framework for NPC and fairness ML, and demonstrate its potential for a broader range of constrained learning applications.<details>
<summary>Abstract</summary>
As the complexity of learning tasks surges, modern machine learning encounters a new constrained learning paradigm characterized by more intricate and data-driven function constraints. Prominent applications include Neyman-Pearson classification (NPC) and fairness classification, which entail specific risk constraints that render standard projection-based training algorithms unsuitable. Gradient boosting machines (GBMs) are among the most popular algorithms for supervised learning; however, they are generally limited to unconstrained settings. In this paper, we adapt the GBM for constrained learning tasks within the framework of Bregman proximal algorithms. We introduce a new Bregman primal-dual method with a global optimality guarantee when the learning objective and constraint functions are convex. In cases of nonconvex functions, we demonstrate how our algorithm remains effective under a Bregman proximal point framework. Distinct from existing constrained learning algorithms, ours possess a unique advantage in their ability to seamlessly integrate with publicly available GBM implementations such as XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017), exclusively relying on their public interfaces. We provide substantial experimental evidence to showcase the effectiveness of the Bregman algorithm framework. While our primary focus is on NPC and fairness ML, our framework holds significant potential for a broader range of constrained learning applications. The source code is currently freely available at https://github.com/zhenweilin/ConstrainedGBM}{https://github.com/zhenweilin/ConstrainedGBM.
</details>
<details>
<summary>摘要</summary>
As the complexity of learning tasks increases, modern machine learning encounters a new constrained learning paradigm with more intricate and data-driven function constraints. Prominent applications include Neyman-Pearson classification (NPC) and fairness classification, which involve specific risk constraints that render standard projection-based training algorithms unsuitable. Gradient boosting machines (GBMs) are among the most popular algorithms for supervised learning; however, they are generally limited to unconstrained settings. In this paper, we adapt the GBM for constrained learning tasks within the framework of Bregman proximal algorithms. We introduce a new Bregman primal-dual method with a global optimality guarantee when the learning objective and constraint functions are convex. In cases of nonconvex functions, we demonstrate how our algorithm remains effective under a Bregman proximal point framework. Distinct from existing constrained learning algorithms, ours possess a unique advantage in their ability to seamlessly integrate with publicly available GBM implementations such as XGBoost (Chen and Guestrin, 2016) and LightGBM (Ke et al., 2017), exclusively relying on their public interfaces. We provide substantial experimental evidence to showcase the effectiveness of the Bregman algorithm framework. While our primary focus is on NPC and fairness ML, our framework holds significant potential for a broader range of constrained learning applications. The source code is currently freely available at <https://github.com/zhenweilin/ConstrainedGBM>.
</details></li>
</ul>
<hr>
<h2 id="To-Whom-are-You-Talking-A-Deep-Learning-Model-to-Endow-Social-Robots-with-Addressee-Estimation-Skills"><a href="#To-Whom-are-You-Talking-A-Deep-Learning-Model-to-Endow-Social-Robots-with-Addressee-Estimation-Skills" class="headerlink" title="To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills"></a>To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10757">http://arxiv.org/abs/2308.10757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlo Mazzola, Marta Romeo, Francesco Rea, Alessandra Sciutti, Angelo Cangelosi</li>
<li>for: 本研究旨在理解人类对话中的 адресат估计问题，以便在人机交互中使用机器人。</li>
<li>methods: 本研究使用了一种hybrid深度学习模型，组合了卷积层和LSTM细胞，将说话者的面部表情和姿势vector作为输入。</li>
<li>results: 研究表明，该模型能够在 robot egocentric 视角下解决 addressee 的本地化问题。<details>
<summary>Abstract</summary>
Communicating shapes our social word. For a robot to be considered social and being consequently integrated in our social environment it is fundamental to understand some of the dynamics that rule human-human communication. In this work, we tackle the problem of Addressee Estimation, the ability to understand an utterance's addressee, by interpreting and exploiting non-verbal bodily cues from the speaker. We do so by implementing an hybrid deep learning model composed of convolutional layers and LSTM cells taking as input images portraying the face of the speaker and 2D vectors of the speaker's body posture. Our implementation choices were guided by the aim to develop a model that could be deployed on social robots and be efficient in ecological scenarios. We demonstrate that our model is able to solve the Addressee Estimation problem in terms of addressee localisation in space, from a robot ego-centric point of view.
</details>
<details>
<summary>摘要</summary>
人际交流 shapes our 社会语言。为了让机器人被视为社交的并被顺利地整合到我们的社交环境中，我们必须理解一些人类对人交流的dinamics。在这项工作中，我们面临了Addresssee Estimation问题，即理解说话者的utterance的接收人，通过解释和利用说话者的非语言性肢体姿势。我们通过实施一种hybrid深度学习模型，组合 convolutional层和LSTM细胞，将说话者的脸部图像和说话者的身姿 вектор作为输入。我们的实现选择是根据目标开发一种可部署于社交机器人上的模型，并能在生态环境中高效地解决问题。我们示示了我们的模型能够解决Addresssee Estimation问题，从机器人自身的视角来看，对话者的位置在空间中。
</details></li>
</ul>
<hr>
<h2 id="On-the-Adversarial-Robustness-of-Multi-Modal-Foundation-Models"><a href="#On-the-Adversarial-Robustness-of-Multi-Modal-Foundation-Models" class="headerlink" title="On the Adversarial Robustness of Multi-Modal Foundation Models"></a>On the Adversarial Robustness of Multi-Modal Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10741">http://arxiv.org/abs/2308.10741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Schlarmann, Matthias Hein</li>
<li>for: 保护用户免受恶意内容的误导和害害</li>
<li>methods: 使用隐形攻击破坏图像，改变多模态基础模型的描述输出</li>
<li>results: 显示了恶意内容提供者可以使用隐形攻击诱导善良用户访问恶势力网站或播报假信息，需要对多模态基础模型进行防御性减噪措施<details>
<summary>Abstract</summary>
Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model.
</details>
<details>
<summary>摘要</summary>
多modal基础模型，如FLAMINGO或GPT-4，在最近受到了巨大的关注。对基础模型的Alignment用于防止模型提供有害或肤浅的输出。然而，有人试图使用恶意内容来攻击基础模型，这也是一个非常重要的问题。在这篇论文中，我们展示了如何使用图像攻击来改变多modal基础模型的描述输出，从而导致善意用户被诱导到有害网站或接受假信息。这表明，在部署多modal基础模型时应该使用防御性攻击的countermeasure。
</details></li>
</ul>
<hr>
<h2 id="We-Don’t-Need-No-Adam-All-We-Need-Is-EVE-On-The-Variance-of-Dual-Learning-Rate-And-Beyond"><a href="#We-Don’t-Need-No-Adam-All-We-Need-Is-EVE-On-The-Variance-of-Dual-Learning-Rate-And-Beyond" class="headerlink" title="We Don’t Need No Adam, All We Need Is EVE: On The Variance of Dual Learning Rate And Beyond"></a>We Don’t Need No Adam, All We Need Is EVE: On The Variance of Dual Learning Rate And Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10740">http://arxiv.org/abs/2308.10740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akhadangi/EVE">https://github.com/akhadangi/EVE</a></li>
<li>paper_authors: Afshin Khadangi</li>
<li>for: 这篇论文是为了优化深度学习模型而写的。</li>
<li>methods: 这篇论文提出了一种新的方法，即增强速度估计（EVE），它利用不同的学习率来区分不同的组件。</li>
<li>results: 实验表明，EVE方法可以快速地 converge 并且在不同的数据集和架构上表现出色，与现有的优化技术相比有所提高。<details>
<summary>Abstract</summary>
In the rapidly advancing field of deep learning, optimising deep neural networks is paramount. This paper introduces a novel method, Enhanced Velocity Estimation (EVE), which innovatively applies different learning rates to distinct components of the gradients. By bifurcating the learning rate, EVE enables more nuanced control and faster convergence, addressing the challenges associated with traditional single learning rate approaches. Utilising a momentum term that adapts to the learning landscape, the method achieves a more efficient navigation of the complex loss surface, resulting in enhanced performance and stability. Extensive experiments demonstrate that EVE significantly outperforms existing optimisation techniques across various benchmark datasets and architectures.
</details>
<details>
<summary>摘要</summary>
在深度学习领域的快速发展中，优化深度神经网络非常重要。本文介绍了一种新方法，即增强速度估计（EVE），它创新地将不同的学习率应用到不同的梯度组件。通过分化学习率，EVE允许更细化的控制和更快的 converges，解决传统单学习率方法所遇到的挑战。利用适应学习地面的旋转矩阵，方法实现了更高效地导航复杂的损失函数表面，从而提高性能和稳定性。广泛的实验表明，EVEsignificantly exceeds现有优化技术在各种 benchmark 数据集和架构上。
</details></li>
</ul>
<hr>
<h2 id="UGSL-A-Unified-Framework-for-Benchmarking-Graph-Structure-Learning"><a href="#UGSL-A-Unified-Framework-for-Benchmarking-Graph-Structure-Learning" class="headerlink" title="UGSL: A Unified Framework for Benchmarking Graph Structure Learning"></a>UGSL: A Unified Framework for Benchmarking Graph Structure Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10737">http://arxiv.org/abs/2308.10737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Bahare Fatemi, Sami Abu-El-Haija, Anton Tsitsulin, Mehran Kazemi, Dustin Zelle, Neslihan Bulut, Jonathan Halcrow, Bryan Perozzi</li>
<li>for: 本研究旨在提供一个统一的框架，用于评估图structured learning模型的效果。</li>
<li>methods: 本研究使用了多种现有的图structured learning模型，并对这些模型进行了广泛的分析和比较。</li>
<li>results: 研究结果表明，不同的模型在不同的情况下具有不同的优劣点，并提供了一个清晰和简洁的理解于这些模型的效果。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) demonstrate outstanding performance in a broad range of applications. While the majority of GNN applications assume that a graph structure is given, some recent methods substantially expanded the applicability of GNNs by showing that they may be effective even when no graph structure is explicitly provided. The GNN parameters and a graph structure are jointly learned. Previous studies adopt different experimentation setups, making it difficult to compare their merits. In this paper, we propose a benchmarking strategy for graph structure learning using a unified framework. Our framework, called Unified Graph Structure Learning (UGSL), reformulates existing models into a single model. We implement a wide range of existing models in our framework and conduct extensive analyses of the effectiveness of different components in the framework. Our results provide a clear and concise understanding of the different methods in this area as well as their strengths and weaknesses. The benchmark code is available at https://github.com/google-research/google-research/tree/master/ugsl.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在广泛的应用领域中表现出色。大多数 GNN 应用假设给定的图结构，但一些最近的方法已经大幅扩展了 GNN 的可应用范围，显示它们可以在没有明确提供图结构的情况下也是有效的。在这些方法中，GNN 参数和图结构都是共同学习的。先前的研究采用了不同的实验设置，使得比较他们的优劣很Difficult。本文提出了一种图结构学习的 benchmarking 策略，称为 Unified Graph Structure Learning (UGSL)。我们将现有的模型重新表述为单一的模型，并在这个框架中实现了广泛的现有模型。我们进行了广泛的现有模型的分析，并对不同的组件的效果进行了广泛的分析。我们的结果为这一领域的不同方法、优劣点提供了清晰和简洁的理解。代码可以在 https://github.com/google-research/google-research/tree/master/ugsl 中下载。
</details></li>
</ul>
<hr>
<h2 id="Artificial-intelligence-driven-antimicrobial-peptide-discovery"><a href="#Artificial-intelligence-driven-antimicrobial-peptide-discovery" class="headerlink" title="Artificial intelligence-driven antimicrobial peptide discovery"></a>Artificial intelligence-driven antimicrobial peptide discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10921">http://arxiv.org/abs/2308.10921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulina Szymczak, Ewa Szczurek</li>
<li>for: 抗微生物蛋白（AMPs）作为替代性抗生素，提供了一种新的抗菌抗性材料。</li>
<li>methods: 人工智能（AI）在AMP发现中扮演了关键角色，通过对优秀候选者的预测和材料的生成。</li>
<li>results: AI在AMP发现中取得了 significiant advances，包括预测活性和药理性，以及生成新的AMP候选者。<details>
<summary>Abstract</summary>
Antimicrobial peptides (AMPs) emerge as promising agents against antimicrobial resistance, providing an alternative to conventional antibiotics. Artificial intelligence (AI) revolutionized AMP discovery through both discrimination and generation approaches. The discriminators aid the identification of promising candidates by predicting key peptide properties such as activity and toxicity, while the generators learn the distribution over peptides and enable sampling novel AMP candidates, either de novo, or as analogues of a prototype peptide. Moreover, the controlled generation of AMPs with desired properties is achieved by discriminator-guided filtering, positive-only learning, latent space sampling, as well as conditional and optimized generation. Here we review recent achievements in AI-driven AMP discovery, highlighting the most exciting directions.
</details>
<details>
<summary>摘要</summary>
antimicrobial peptides (AMPs) emerge as promising agents against antimicrobial resistance, providing an alternative to conventional antibiotics. artificial intelligence (AI) revolutionized AMP discovery through both discrimination and generation approaches. the discriminators aid the identification of promising candidates by predicting key peptide properties such as activity and toxicity, while the generators learn the distribution over peptides and enable sampling novel AMP candidates, either de novo, or as analogues of a prototype peptide. Moreover, the controlled generation of AMPs with desired properties is achieved by discriminator-guided filtering, positive-only learning, latent space sampling, as well as conditional and optimized generation. here we review recent achievements in AI-driven AMP discovery, highlighting the most exciting directions.Note: Simplified Chinese is used here as the text is intended for a general audience and not for academic or technical purposes. If you need a more formal or precise translation, please let me know and I can provide it in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="What’s-Race-Got-to-do-with-it-Predicting-Youth-Depression-Across-Racial-Groups-Using-Machine-and-Deep-Learning"><a href="#What’s-Race-Got-to-do-with-it-Predicting-Youth-Depression-Across-Racial-Groups-Using-Machine-and-Deep-Learning" class="headerlink" title="What’s Race Got to do with it? Predicting Youth Depression Across Racial Groups Using Machine and Deep Learning"></a>What’s Race Got to do with it? Predicting Youth Depression Across Racial Groups Using Machine and Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11591">http://arxiv.org/abs/2308.11591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Zhong, Nikhil Yadav</li>
<li>for: 本研究旨在透过机器学习（ML）和人工神经网络（ANN）模型，识别高中生中的抑郁症状。</li>
<li>methods: 本研究使用了全国青少年风险行为调查系统（YRBSS）调查数据，并对不同的种族 subgroup 进行了不同的分类。</li>
<li>results: 研究发现，不同的种族 subgroup 有不同的参量，这些参量对于识别抑郁症状具有不同的重要性。ANN模型在整个数据集上得到了82.90%的F1分数，而最佳表现的机器学习模型支持向量机器（SVM）则得到了81.90%的F1分数。<details>
<summary>Abstract</summary>
Depression is a common yet serious mental disorder that affects millions of U.S. high schoolers every year. Still, accurate diagnosis and early detection remain significant challenges. In the field of public health, research shows that neural networks produce promising results in identifying other diseases such as cancer and HIV. This study proposes a similar approach, utilizing machine learning (ML) and artificial neural network (ANN) models to classify depression in a student. Additionally, the study highlights the differences in relevant factors for race subgroups and advocates the need for more extensive and diverse datasets. The models train on nationwide Youth Risk Behavior Surveillance System (YRBSS) survey data, in which the most relevant factors of depression are found with statistical analysis. The survey data is a structured dataset with 15000 entries including three race subsets each consisting of 900 entries. For classification, the research problem is modeled as a supervised learning binary classification problem. Factors relevant to depression for different racial subgroups are also identified. The ML and ANN models are trained on the entire dataset followed by different race subsets to classify whether an individual has depression. The ANN model achieves the highest F1 score of 82.90% while the best-performing machine learning model, support vector machines (SVM), achieves a score of 81.90%. This study reveals that different parameters are more valuable for modeling depression across diverse racial groups and furthers research regarding American youth depression.
</details>
<details>
<summary>摘要</summary>
每年数百万美国高中生都会患上抑郁症，但确定诊断和早期发现仍然是一项大型挑战。在公共卫生领域，研究表明，神经网络生成出了对其他疾病，如癌症和HIV，的识别promising results。这个研究提议了类似的方法，使用机器学习（ML）和人工神经网络（ANN）模型来识别高中生中的抑郁。此外，研究还 highlights the differences in relevant factors for different racial subgroups and advocates the need for more extensive and diverse datasets.研究使用全国青年风险行为监测系统（YRBSS）调查数据进行训练，该数据包括3个种族 subsets，每个 subsets包含900个数据。为了进行分类，研究问题被模型为一个指导学习binary classification问题。对于不同的种族 subgroup，研究还 indentified the factors relevant to depression.使用整个数据集和不同种族 subsets进行训练，ANN模型 achieve最高的F1分数为82.90%，而最佳performing机器学习模型，support vector machines（SVM）， achieve分数为81.90%。这个研究表明，不同种族 subgroup的参数在模型抑郁有所不同，并且推动了美国青年抑郁的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Test-time-augmentation-based-active-learning-and-self-training-for-label-efficient-segmentation"><a href="#Test-time-augmentation-based-active-learning-and-self-training-for-label-efficient-segmentation" class="headerlink" title="Test-time augmentation-based active learning and self-training for label-efficient segmentation"></a>Test-time augmentation-based active learning and self-training for label-efficient segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10727">http://arxiv.org/abs/2308.10727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bella Specktor-Fadida, Anna Levchakov, Dana Schonberger, Liat Ben-Sira, Dafna Ben-Bashat, Leo Joskowicz</li>
<li>for: 这个论文目的是提出一种 combining 自动学习（AL）和自我教学（ST）方法，以减轻数据注释的卷积 neural network 的Annotation burden。</li>
<li>methods: 这个论文使用的方法包括 Test-Time Augmentations（TTA）、自动学习（AL）和自我教学（ST）。</li>
<li>results: 研究结果显示，ST 是对 MRI 胚胎体和 Placenta 分割任务的非常有效的，可以提高 ID 和 OOD 数据的性能。但是，对于单序列胚胎体分割任务，搅拌自动学习可以提高性能，而对于多序列 Placenta 分割任务，搅拌自我教学可以提高性能。此外，AL 在高变化 Placenta 数据上是有帮助的，但是对于单序列胚胎体数据，AL 不能提高性能。<details>
<summary>Abstract</summary>
Deep learning techniques depend on large datasets whose annotation is time-consuming. To reduce annotation burden, the self-training (ST) and active-learning (AL) methods have been developed as well as methods that combine them in an iterative fashion. However, it remains unclear when each method is the most useful, and when it is advantageous to combine them. In this paper, we propose a new method that combines ST with AL using Test-Time Augmentations (TTA). First, TTA is performed on an initial teacher network. Then, cases for annotation are selected based on the lowest estimated Dice score. Cases with high estimated scores are used as soft pseudo-labels for ST. The selected annotated cases are trained with existing annotated cases and ST cases with border slices annotations. We demonstrate the method on MRI fetal body and placenta segmentation tasks with different data variability characteristics. Our results indicate that ST is highly effective for both tasks, boosting performance for in-distribution (ID) and out-of-distribution (OOD) data. However, while self-training improved the performance of single-sequence fetal body segmentation when combined with AL, it slightly deteriorated performance of multi-sequence placenta segmentation on ID data. AL was helpful for the high variability placenta data, but did not improve upon random selection for the single-sequence body data. For fetal body segmentation sequence transfer, combining AL with ST following ST iteration yielded a Dice of 0.961 with only 6 original scans and 2 new sequence scans. Results using only 15 high-variability placenta cases were similar to those using 50 cases. Code is available at: https://github.com/Bella31/TTA-quality-estimation-ST-AL
</details>
<details>
<summary>摘要</summary>
深度学习技术需要大量的数据集，其标注是时间consuming的。为了减轻标注压力，自适应学习（ST）和活动学习（AL）方法已经被开发出来，同时也有将其结合在迭代方式下的方法。然而，尚未确定哪些情况下使用哪一种方法是最有用，并且何时结合它们是有利的。在这篇论文中，我们提出了一种新的方法，即将ST与AL结合使用测试时间扩展（TTA）。首先，TTA被应用于初始教师网络。然后，根据最低估计的 dice 分数选择标注案例。高估分数的案例用作软件 Pseudo-标注，并与现有标注案例和ST案例进行训练。我们在MRI胎体和胎盘分割任务上进行了实验，并通过不同数据特征来评估我们的方法。我们的结果表明，ST具有高效性，对于ID和OOD数据都有提高性。然而，当与AL结合使用时，单个序列胎体分割的性能有所下降，而多个序列胎盘分割的性能则没有得到提高。AL对高变化胎盘数据是有帮助的，但对单个序列胎体数据没有提高。在胎体分割序列传输中，将AL与ST结合使用，并在ST迭代后进行AL，可以达到0.961的Dice值，只需要6个原始扫描和2个新序列扫描。在50个高变化胎盘案例中，结果与使用50个案例相同。相关代码可以在GitHub上找到：https://github.com/Bella31/TTA-quality-estimation-ST-AL。
</details></li>
</ul>
<hr>
<h2 id="Clustered-Linear-Contextual-Bandits-with-Knapsacks"><a href="#Clustered-Linear-Contextual-Bandits-with-Knapsacks" class="headerlink" title="Clustered Linear Contextual Bandits with Knapsacks"></a>Clustered Linear Contextual Bandits with Knapsacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10722">http://arxiv.org/abs/2308.10722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Michalis Mamakos, Zhao Song</li>
<li>for: 本研究studies clustered contextual bandits, where rewards and resource consumption are the outcomes of cluster-specific linear models.</li>
<li>methods: 该算法使用了 clustering 技术，并 combining techniques from the literature of econometrics and of bandits with constraints.</li>
<li>results: 该算法可以 achieve regret sublinear in the number of time periods, without requiring access to all of the arms.<details>
<summary>Abstract</summary>
In this work, we study clustered contextual bandits where rewards and resource consumption are the outcomes of cluster-specific linear models. The arms are divided in clusters, with the cluster memberships being unknown to an algorithm. Pulling an arm in a time period results in a reward and in consumption for each one of multiple resources, and with the total consumption of any resource exceeding a constraint implying the termination of the algorithm. Thus, maximizing the total reward requires learning not only models about the reward and the resource consumption, but also cluster memberships. We provide an algorithm that achieves regret sublinear in the number of time periods, without requiring access to all of the arms. In particular, we show that it suffices to perform clustering only once to a randomly selected subset of the arms. To achieve this result, we provide a sophisticated combination of techniques from the literature of econometrics and of bandits with constraints.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究集中的上下文抽奖问题，其中奖励和资源消耗是集中的线性模型的结果。武器被分成集群，集群成员身份未知于算法。在一个时间段内抽一把武器会得到奖励和每种多种资源的消耗，而某些资源的总消耗超过限制，则算法将被终止。因此，最大化总奖励需要学习不仅奖励和资源消耗，还需要集群成员身份。我们提供一种算法，其 regret 是时间段数量的下界，不需要所有武器的访问。具体来说，我们表明只需要对随机选择的一 subset of 武器进行分 clustering 即可。为 достичь这一结果，我们 combinated 了文献中的 econometrics 和抽奖问题的技术。
</details></li>
</ul>
<hr>
<h2 id="CoMIX-A-Multi-agent-Reinforcement-Learning-Training-Architecture-for-Efficient-Decentralized-Coordination-and-Independent-Decision-Making"><a href="#CoMIX-A-Multi-agent-Reinforcement-Learning-Training-Architecture-for-Efficient-Decentralized-Coordination-and-Independent-Decision-Making" class="headerlink" title="CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision Making"></a>CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10721">http://arxiv.org/abs/2308.10721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Minelli, Mirco Musolesi</li>
<li>for: 这篇论文的目的是提出一种基于协调的多代理人训练框架，以便在共享环境中协同完成共同目标，同时保持每个代理人的独立决策能力。</li>
<li>methods: 这篇论文提出了一种名为Coordinated QMIX（CoMIX）的训练框架，该框架基于协调的策略，允许每个代理人在决策过程中独立地做出决定，同时能够适应不同的情况，保持独立和协同的平衡。</li>
<li>results: 实验结果表明，CoMIX在协同任务上表现出色，比基线方法高效。这 validate了 authors 的增量策略方法是一种有效的协调提高多代理人系统的技术。<details>
<summary>Abstract</summary>
Robust coordination skills enable agents to operate cohesively in shared environments, together towards a common goal and, ideally, individually without hindering each other's progress. To this end, this paper presents Coordinated QMIX (CoMIX), a novel training framework for decentralized agents that enables emergent coordination through flexible policies, allowing at the same time independent decision-making at individual level. CoMIX models selfish and collaborative behavior as incremental steps in each agent's decision process. This allows agents to dynamically adapt their behavior to different situations balancing independence and collaboration. Experiments using a variety of simulation environments demonstrate that CoMIX outperforms baselines on collaborative tasks. The results validate our incremental policy approach as effective technique for improving coordination in multi-agent systems.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable text into Simplified Chinese.<</SYS>> Robust coordination skills allow agents to work together in shared environments, working towards a common goal and, ideally, making decisions independently without hindering each other's progress. To achieve this, this paper presents Coordinated QMIX (CoMIX), a novel training framework for decentralized agents that enables emergent coordination through flexible policies, allowing for independent decision-making at the individual level. CoMIX models selfish and collaborative behavior as incremental steps in each agent's decision process, allowing agents to adapt their behavior dynamically to different situations, balancing independence and collaboration. Experimental results using a variety of simulation environments show that CoMIX outperforms baselines on collaborative tasks, validating our incremental policy approach as an effective technique for improving coordination in multi-agent systems.
</details></li>
</ul>
<hr>
<h2 id="Relax-and-penalize-a-new-bilevel-approach-to-mixed-binary-hyperparameter-optimization"><a href="#Relax-and-penalize-a-new-bilevel-approach-to-mixed-binary-hyperparameter-optimization" class="headerlink" title="Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization"></a>Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10711">http://arxiv.org/abs/2308.10711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marianna de Santis, Jordan Frecon, Francesco Rinaldi, Saverio Salzo, Martin Schmidt</li>
<li>for: 这篇论文是为了提出一种能够有效地优化高维度机器学习模型的混合二级方法的研究。</li>
<li>methods: 该论文使用了一种基于罚项的等值 continous bilevel reformulation 方法，以解决现有的二级参数问题。</li>
<li>results: 该论文的实验结果表明，使用该方法可以对 regression 问题中的组 sparse 结构进行优化，并且比现有的 relaxation 和 rounding 方法表现更好。<details>
<summary>Abstract</summary>
In recent years, bilevel approaches have become very popular to efficiently estimate high-dimensional hyperparameters of machine learning models. However, to date, binary parameters are handled by continuous relaxation and rounding strategies, which could lead to inconsistent solutions. In this context, we tackle the challenging optimization of mixed-binary hyperparameters by resorting to an equivalent continuous bilevel reformulation based on an appropriate penalty term. We propose an algorithmic framework that, under suitable assumptions, is guaranteed to provide mixed-binary solutions. Moreover, the generality of the method allows to safely use existing continuous bilevel solvers within the proposed framework. We evaluate the performance of our approach for a specific machine learning problem, i.e., the estimation of the group-sparsity structure in regression problems. Reported results clearly show that our method outperforms state-of-the-art approaches based on relaxation and rounding
</details>
<details>
<summary>摘要</summary>
近年来，二级方法已经非常受欢迎地用于高维度机器学习模型参数的效率估算。然而，至今为止，Binary参数都是通过连续弹簧和圆拟法来处理，这可能会导致不一致的解决方案。在这个上下文中，我们解决了高维度混合 Binary 参数的困难优化问题，通过适当的罚项来转化为连续二级 reformulation。我们提出了一个算法框架，在适当的假设下，可以保证提供混合 Binary 解决方案。此外，我们的方法总体来说具有一致性和可靠性，可以安全地使用现有的连续二级解决方案。我们对一个特定的机器学习问题，即回归问题中的集群稀疏结构估算进行评估。报告的结果显示，我们的方法明显超过了当前的 relaxation 和圆拟法所能达到的性能。
</details></li>
</ul>
<hr>
<h2 id="Measuring-the-Effect-of-Causal-Disentanglement-on-the-Adversarial-Robustness-of-Neural-Network-Models"><a href="#Measuring-the-Effect-of-Causal-Disentanglement-on-the-Adversarial-Robustness-of-Neural-Network-Models" class="headerlink" title="Measuring the Effect of Causal Disentanglement on the Adversarial Robustness of Neural Network Models"></a>Measuring the Effect of Causal Disentanglement on the Adversarial Robustness of Neural Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10708">http://arxiv.org/abs/2308.10708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prebenness/causal_disentanglement_robustness">https://github.com/prebenness/causal_disentanglement_robustness</a></li>
<li>paper_authors: Preben M. Ness, Dusica Marijan, Sunanda Bose</li>
<li>for: 这些论文主要用于研究 causal Neural Network 模型在针对性攻击方面的Robustness和通用性能。</li>
<li>methods: 这些模型使用了 causal disentanglement 来提高其 Robustness和通用性能，并使用了 Computer Vision 领域的内容&#x2F;风格分离指标来衡量 causal disentanglement。</li>
<li>results: 研究发现， causal Neural Network 模型在针对性攻击方面的 Robustness 与模型 decorrelate causal 和干扰信号的程度有很强的相关性（r&#x3D;0.820， p&#x3D;0.001），同时也发现隐藏信号的像素级信息含量与针对性攻击的Robustness有负相关性（r&#x3D;-0.597， p&#x3D;0.040）。<details>
<summary>Abstract</summary>
Causal Neural Network models have shown high levels of robustness to adversarial attacks as well as an increased capacity for generalisation tasks such as few-shot learning and rare-context classification compared to traditional Neural Networks. This robustness is argued to stem from the disentanglement of causal and confounder input signals. However, no quantitative study has yet measured the level of disentanglement achieved by these types of causal models or assessed how this relates to their adversarial robustness.   Existing causal disentanglement metrics are not applicable to deterministic models trained on real-world datasets. We, therefore, utilise metrics of content/style disentanglement from the field of Computer Vision to measure different aspects of the causal disentanglement for four state-of-the-art causal Neural Network models. By re-implementing these models with a common ResNet18 architecture we are able to fairly measure their adversarial robustness on three standard image classification benchmarking datasets under seven common white-box attacks. We find a strong association (r=0.820, p=0.001) between the degree to which models decorrelate causal and confounder signals and their adversarial robustness. Additionally, we find a moderate negative association between the pixel-level information content of the confounder signal and adversarial robustness (r=-0.597, p=0.040).
</details>
<details>
<summary>摘要</summary>
causal neural network 模型在针对敌意攻击方面表现出了高水平的 robustness，以及在几个类型的泛化任务中提高了能力，如少量学习和罕见情况分类。这种 robustness 被认为是由 causal 和干扰输入信号的分离所带来的。然而，现在没有任何量化研究 measure 了这些类型的 causal 模型中的分离水平，或者如何这与其针对敌意攻击的 robustness 相关。现有的 causal 分离度量不适用于 deterministic 模型在实际数据集上的训练。我们因此利用计算机视觉领域中的内容/风格分离度量来测量不同 causal 模型的不同方面的分离度。通过对四种 state-of-the-art causal neural network 模型重新实现，并使用共同的 ResNet18 架构，我们可以公平地测量它们在三个标准图像分类 benchmark 数据集上的针对敌意攻击的 robustness。我们发现 causal 和干扰信号的分离度与模型的针对敌意攻击的 robustness 之间存在强相关性（r = 0.820， p = 0.001）。此外，我们发现干扰信号的像素级信息 contenido 和针对敌意攻击的 robustness 之间存在负相关性（r = -0.597， p = 0.040）。
</details></li>
</ul>
<hr>
<h2 id="Sampling-From-Autoencoders’-Latent-Space-via-Quantization-And-Probability-Mass-Function-Concepts"><a href="#Sampling-From-Autoencoders’-Latent-Space-via-Quantization-And-Probability-Mass-Function-Concepts" class="headerlink" title="Sampling From Autoencoders’ Latent Space via Quantization And Probability Mass Function Concepts"></a>Sampling From Autoencoders’ Latent Space via Quantization And Probability Mass Function Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10704">http://arxiv.org/abs/2308.10704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aymene Mohammed Bouayed, Adrian Iaccovelli, David Naccache</li>
<li>for: 这篇论文主要关注于从生成模型中的伪扩散空间抽样，以确保生成的数据是生活化的图像。</li>
<li>methods: 我们提出了一个新的后训练抽样算法，基于几率质量函数的概念，并与量化过程相结合。我们的算法在每个伪扩散 вектор的输入数据中定义了一个范围，然后从这些定义的邻域中抽样数据。这种策略可以确保抽样的伪扩散 вектор主要 inhabit 高几率区域，并且可以实际地转换为真实世界的图像。</li>
<li>results: 我们的抽样算法在多种模型和数据集上展现出了统计上的优势，特别是在MNIST Benchmark dataset上，我们的方法与GMM抽样比较之下，增加了0.89的FID值。此外，在生成面孔和眼睛图像时，我们的方法也显示了明显的改善，FID值分别提高了1.69和0.87。<details>
<summary>Abstract</summary>
In this study, we focus on sampling from the latent space of generative models built upon autoencoders so as the reconstructed samples are lifelike images. To do to, we introduce a novel post-training sampling algorithm rooted in the concept of probability mass functions, coupled with a quantization process. Our proposed algorithm establishes a vicinity around each latent vector from the input data and then proceeds to draw samples from these defined neighborhoods. This strategic approach ensures that the sampled latent vectors predominantly inhabit high-probability regions, which, in turn, can be effectively transformed into authentic real-world images. A noteworthy point of comparison for our sampling algorithm is the sampling technique based on Gaussian mixture models (GMM), owing to its inherent capability to represent clusters. Remarkably, we manage to improve the time complexity from the previous $\mathcal{O}(n\times d \times k \times i)$ associated with GMM sampling to a much more streamlined $\mathcal{O}(n\times d)$, thereby resulting in substantial speedup during runtime. Moreover, our experimental results, gauged through the Fr\'echet inception distance (FID) for image generation, underscore the superior performance of our sampling algorithm across a diverse range of models and datasets. On the MNIST benchmark dataset, our approach outperforms GMM sampling by yielding a noteworthy improvement of up to $0.89$ in FID value. Furthermore, when it comes to generating images of faces and ocular images, our approach showcases substantial enhancements with FID improvements of $1.69$ and $0.87$ respectively, as compared to GMM sampling, as evidenced on the CelebA and MOBIUS datasets. Lastly, we substantiate our methodology's efficacy in estimating latent space distributions in contrast to GMM sampling, particularly through the lens of the Wasserstein distance.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们关注生成模型基于autoencoder的 latent space采样，以便生成真实的图像。为此，我们提出了一种新的 posterior 采样算法，基于概率质量函数，并与量化过程结合。我们的算法在每个输入数据的 latent vector周围建立一个定义的邻域，然后从这些定义的邻域中采样。这种策略使得采样的 latent vector主要居住在高概率区域中，从而可以有效地转换为真实的世界图像。与基于 Gaussian mixture models (GMM) 的采样技术相比，我们的采样算法具有更高的时间复杂度优化，从 $\mathcal{O}(n\times d\times k\times i)$ 降低到 $\mathcal{O}(n\times d)$，从而在运行时间中获得了显著的加速。此外，我们通过 Fréchet inception distance (FID) 测试，发现我们的采样算法在不同的模型和数据集上显示出了显著的性能优势。在 MNIST 测试集上，我们的方法与 GMM 采样相比，提高了 FID 值的不可或缺的提升，达到 $0.89$。此外，在生成人脸和眼部图像时，我们的方法也表现出了显著的改善，FID 提升 $1.69$ 和 $0.87$ 分别，与 GMM 采样相比。最后，我们通过 Wasserstein distance 测试，证明我们的方法在估计 latent space 分布方面的效果比 GMM 采样更好。
</details></li>
</ul>
<hr>
<h2 id="Refashioning-Emotion-Recognition-Modelling-The-Advent-of-Generalised-Large-Models"><a href="#Refashioning-Emotion-Recognition-Modelling-The-Advent-of-Generalised-Large-Models" class="headerlink" title="Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models"></a>Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11578">http://arxiv.org/abs/2308.11578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixing Zhang, Liyizhe Peng, Tao Pang, Jing Han, Huan Zhao, Bjorn W. Schuller</li>
<li>for: This paper aims to investigate the performance of large language models (LLMs) in emotion recognition, specifically looking at in-context learning, few-shot learning, accuracy, generalization, and explanation.</li>
<li>methods: The paper uses LLMs, such as ChatGPT, to perform emotion recognition tasks, and examines their performance in various aspects.</li>
<li>results: The paper provides insights into the performance of LLMs in emotion recognition, including their ability to learn with few examples, adapt to new contexts, and provide explanations for their predictions.<details>
<summary>Abstract</summary>
After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, accuracy, generalisation, and explanation. Moreover, we offer some insights and pose other potential challenges, hoping to ignite broader discussions about enhancing emotion recognition in the new era of advanced and generalised large models.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: послеinception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, accuracy, generalisation, and explanation. Moreover, we offer some insights and pose other potential challenges, hoping to ignite broader discussions about enhancing emotion recognition in the new era of advanced and generalised large models.Translated into Traditional Chinese:在内心 recognition of emotion or affective computing 的启动以来，它已经成为一个活跃的研究领域，因为它的应用非常广泛。过去几十年，情感识别模型在从 statistically shallow 模型演化到基于神经网络的深度模型，可以对情感识别模型的性能提高，并在不同的 benchmark 上获得最好的结果。因此，在最近的年份，深度模型一直被视为情感识别的首选。然而，大型语言模型（LLMs），如 ChatGPT，在世界上发生了惊人的出现，因为它们的发展出了过去深度模型 nunca 显示的一些能力，包括零/几架学习、上下文学习、链接思维等。在 presente 纸上，我们全面调查了 LLMs 在情感识别方面的表现，包括上下文学习、几架学习、准确性、一致性和解释等方面。此外，我们还提供了一些问题和潜在的挑战，希望能够发球更广泛的讨论，以推动情感识别在新的时代的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="An-engine-to-simulate-insurance-fraud-network-data"><a href="#An-engine-to-simulate-insurance-fraud-network-data" class="headerlink" title="An engine to simulate insurance fraud network data"></a>An engine to simulate insurance fraud network data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11659">http://arxiv.org/abs/2308.11659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bavo D. C. Campo, Katrien Antonio</li>
<li>for: 本研究旨在开发高效、准确的欺诈检测模型，以便更好地检测保险欺诈laims。</li>
<li>methods: 本研究使用社交网络中充满特征的数据进行学习，并可以控制数据生成机制以便模拟不同的情况。</li>
<li>results: 通过使用这些synthetic数据，研究人员和实践者可以测试不同的检测模型，并评估其预测性能。<details>
<summary>Abstract</summary>
Traditionally, the detection of fraudulent insurance claims relies on business rules and expert judgement which makes it a time-consuming and expensive process (\'Oskarsd\'ottir et al., 2022). Consequently, researchers have been examining ways to develop efficient and accurate analytic strategies to flag suspicious claims. Feeding learning methods with features engineered from the social network of parties involved in a claim is a particularly promising strategy (see for example Van Vlasselaer et al. (2016); Tumminello et al. (2023)). When developing a fraud detection model, however, we are confronted with several challenges. The uncommon nature of fraud, for example, creates a high class imbalance which complicates the development of well performing analytic classification models. In addition, only a small number of claims are investigated and get a label, which results in a large corpus of unlabeled data. Yet another challenge is the lack of publicly available data. This hinders not only the development of new methods, but also the validation of existing techniques. We therefore design a simulation machine that is engineered to create synthetic data with a network structure and available covariates similar to the real life insurance fraud data set analyzed in \'Oskarsd\'ottir et al. (2022). Further, the user has control over several data-generating mechanisms. We can specify the total number of policyholders and parties, the desired level of imbalance and the (effect size of the) features in the fraud generating model. As such, the simulation engine enables researchers and practitioners to examine several methodological challenges as well as to test their (development strategy of) insurance fraud detection models in a range of different settings. Moreover, large synthetic data sets can be generated to evaluate the predictive performance of (advanced) machine learning techniques.
</details>
<details>
<summary>摘要</summary>
传统上，探测欺诈保险索赔靠业务规则和专家判断，这使得过程时间consuming和成本高（'Oskarsd\'ottir等，2022）。因此，研究人员在尝试开发高效和准确的分析策略来标识可疑索赔。从社交网络中提取和Engineered特征来帮助学习方法是一种非常有前途的策略（如Van Vlasselaer等，2016；Tumminello等，2023）。在开发欺诈检测模型时，我们面临了多个挑战。欺诈的罕见性导致分类问题的吸引性异常高，这使得开发高性能分类模型的发展具有挑战性。此外，只有一小部分索赔被调查并获得标签，导致大量的无标签数据。此外，公共数据的缺乏也阻碍了新方法的开发和现有技术的验证。为了解决这些问题，我们设计了一台模拟机器，可以生成具有网络结构和实际欺诈数据集中可用特征的 sintetic数据。用户可以控制数据生成机制，包括总体保险持有人和相关人数、欺诈水平和特征效果等。因此，模拟机器可以帮助研究人员和实践者在不同的设定下测试开发策略和检测模型，并生成大量的 sintetic数据来评估高级机器学习技术的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Cost-Efficient-Online-Decision-Making-A-Combinatorial-Multi-Armed-Bandit-Approach"><a href="#Cost-Efficient-Online-Decision-Making-A-Combinatorial-Multi-Armed-Bandit-Approach" class="headerlink" title="Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach"></a>Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10699">http://arxiv.org/abs/2308.10699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arman Rahbar, Niklas Åkerblom, Morteza Haghir Chehreghani</li>
<li>for: 这篇论文是关于在多种应用场景中进行在线决策的研究，具体来说是在收到新数据点时进行测试序列的决策。</li>
<li>methods: 本文提出了一种基于 combinatorial multi-armed bandits 的新的在线决策问题的形式ulation，并使用 posterior sampling 或 BayesUCB 进行探索。</li>
<li>results: 本文提供了一种新的成本效益的在线决策框架，并进行了严格的理论分析和多种实验 validate 其可用性。<details>
<summary>Abstract</summary>
Online decision making plays a crucial role in numerous real-world applications. In many scenarios, the decision is made based on performing a sequence of tests on the incoming data points. However, performing all tests can be expensive and is not always possible. In this paper, we provide a novel formulation of the online decision making problem based on combinatorial multi-armed bandits and take the cost of performing tests into account. Based on this formulation, we provide a new framework for cost-efficient online decision making which can utilize posterior sampling or BayesUCB for exploration. We provide a rigorous theoretical analysis for our framework and present various experimental results that demonstrate its applicability to real-world problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Beyond-expectations-Residual-Dynamic-Mode-Decomposition-and-Variance-for-Stochastic-Dynamical-Systems"><a href="#Beyond-expectations-Residual-Dynamic-Mode-Decomposition-and-Variance-for-Stochastic-Dynamical-Systems" class="headerlink" title="Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems"></a>Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10697">http://arxiv.org/abs/2308.10697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew J. Colbrook, Qin Li, Ryan V. Raut, Alex Townsend</li>
<li>For: 这个论文的目的是为了解决非线性动力系统中的库曼操作符的特征，以及其spectral information的计算方法。* Methods: 这篇论文使用了Dynamic Mode Decomposition（DMD）方法，以及一个额外的DMD-type矩阵，来 aproximate the sum of a squared residual and a variance term，从而控制 projection error。* Results: 这篇论文的结果表明，通过包含差异的库曼框架，可以 Addressing challenges such as spurious modes, essential spectra, and the verification of Koopman mode decompositions for stochastic systems。 In addition, the authors introduce the concept of variance-pseudospectra to gauge statistical coherence, and present a suite of convergence results for the spectral quantities of stochastic Koopman operators. Finally, the study demonstrates practical applications using both simulated and experimental data, and reveals physiologically significant information unavailable to standard expectation-based dynamical models.<details>
<summary>Abstract</summary>
Koopman operators linearize nonlinear dynamical systems, making their spectral information of crucial interest. Numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. Although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses various challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods tailored for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and a variance term, each of which can be approximated individually using batched snapshot data. This allows verified computation of the spectral properties of stochastic Koopman operators, controlling the projection error. We also introduce the concept of variance-pseudospectra to gauge statistical coherency. Finally, we present a suite of convergence results for the spectral quantities of stochastic Koopman operators. Our study concludes with practical applications using both simulated and experimental data. In neural recordings from awake mice, we demonstrate how variance-pseudospectra can reveal physiologically significant information unavailable to standard expectation-based dynamical models.
</details>
<details>
<summary>摘要</summary>
黑曼操作符 linearizes nonlinear dynamical systems, making its spectral information of crucial interest. numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. Although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses various challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods tailored for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and a variance term, each of which can be approximated individually using batched snapshot data. This allows verified computation of the spectral properties of stochastic Koopman operators, controlling the projection error. We also introduce the concept of variance-pseudospectra to gauge statistical coherency. Finally, we present a suite of convergence results for the spectral quantities of stochastic Koopman operators. Our study concludes with practical applications using both simulated and experimental data. In neural recordings from awake mice, we demonstrate how variance-pseudospectra can reveal physiologically significant information unavailable to standard expectation-based dynamical models.Here's the translation in Traditional Chinese:科普曼操作符可以线性化非线性动力学系统，使其特征对应的spectral information得到了核心地位。numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses various challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods tailored for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and a variance term, each of which can be approximated individually using batched snapshot data. This allows verified computation of the spectral properties of stochastic Koopman operators, controlling the projection error. We also introduce the concept of variance-pseudospectra to gauge statistical coherency. Finally, we present a suite of convergence results for the spectral quantities of stochastic Koopman operators. Our study concludes with practical applications using both simulated and experimental data. In neural recordings from awake mice, we demonstrate how variance-pseudospectra can reveal physiologically significant information unavailable to standard expectation-based dynamical models.
</details></li>
</ul>
<hr>
<h2 id="An-Improved-Best-of-both-worlds-Algorithm-for-Bandits-with-Delayed-Feedback"><a href="#An-Improved-Best-of-both-worlds-Algorithm-for-Bandits-with-Delayed-Feedback" class="headerlink" title="An Improved Best-of-both-worlds Algorithm for Bandits with Delayed Feedback"></a>An Improved Best-of-both-worlds Algorithm for Bandits with Delayed Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10675">http://arxiv.org/abs/2308.10675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Masoudian, Julian Zimmert, Yevgeny Seldin</li>
<li>for: 本文提出了一种新的best-of-both-worlds算法，用于带延迟反馈的bandit问题。该算法改进了先前的 Masoudian et al. 的工作，不需要先知道最大延迟 $d_{\max}$，并提供了更紧的 regret bounds。</li>
<li>methods: 本文使用了 counts of outstanding observations（在动作时间被观察到的尚未完成的观察数）而不是延迟或最大延迟（只在反馈 arrive时被观察到）来实现。一个重要贡献是控制分布漂移，基于偏损失估计器和延迟过大的观察被跳过。</li>
<li>results: 本文的 regret bounds 是基于 counts of outstanding observations after skipping of observations with excessively large delays，而不是延迟或最大延迟。这一结论表明了best-of-both-worlds bandit with delayed feedback的复杂性是由出standing observations的总计数所定义，而不是延迟或最大延迟。<details>
<summary>Abstract</summary>
We propose a new best-of-both-worlds algorithm for bandits with variably delayed feedback. The algorithm improves on prior work by Masoudian et al. [2022] by eliminating the need in prior knowledge of the maximal delay $d_{\mathrm{max}}$ and providing tighter regret bounds in both regimes. The algorithm and its regret bounds are based on counts of outstanding observations (a quantity that is observed at action time) rather than delays or the maximal delay (quantities that are only observed when feedback arrives). One major contribution is a novel control of distribution drift, which is based on biased loss estimators and skipping of observations with excessively large delays. Another major contribution is demonstrating that the complexity of best-of-both-worlds bandits with delayed feedback is characterized by the cumulative count of outstanding observations after skipping of observations with excessively large delays, rather than the delays or the maximal delay.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的最佳之both-worlds算法，用于带有变化延迟反馈的带宽问题。该算法超越了先前的 Masoudian 等人（2022）的工作，不需要先知道最大延迟 $d_{\max}$，并提供了更紧的 regret bound 在两个 режиме下。该算法和其 regret bound 基于当时行动时观察到的待机数（一个观察到的量）而不是延迟或最大延迟（只有在反馈来到时才能观察到的量）。我们的一个重要贡献是一种新的分布漂移控制，基于偏置损失估计器和延迟过长的观察抛弃。另一个重要贡献是表明了最佳之both-worlds 带宽问题的复杂度被定义为在延迟抛弃后的累积待机数后，而不是延迟或最大延迟。
</details></li>
</ul>
<hr>
<h2 id="A-Safe-Deep-Reinforcement-Learning-Approach-for-Energy-Efficient-Federated-Learning-in-Wireless-Communication-Networks"><a href="#A-Safe-Deep-Reinforcement-Learning-Approach-for-Energy-Efficient-Federated-Learning-in-Wireless-Communication-Networks" class="headerlink" title="A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks"></a>A Safe Deep Reinforcement Learning Approach for Energy Efficient Federated Learning in Wireless Communication Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10664">http://arxiv.org/abs/2308.10664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolaos Koursioumpas, Lina Magoula, Nikolaos Petropouleas, Alexandros-Ioannis Thanopoulos, Theodora Panagea, Nancy Alonistioti, M. A. Gutierrez-Estevez, Ramin Khalili</li>
<li>for: 提高 Federated Learning（FL）过程中的能源占用率，并保证模型的性能。</li>
<li>methods: 提出一种 Soft Actor Critic Deep Reinforcement Learning（DRL）解决方案，并在训练过程中引入罚函数，以避免环境约束的违反。</li>
<li>results: 与四个州OF-the-art基eline解决方案进行比较，在静态和动态环境中达到了94%的能源占用减少。<details>
<summary>Abstract</summary>
Progressing towards a new era of Artificial Intelligence (AI) - enabled wireless networks, concerns regarding the environmental impact of AI have been raised both in industry and academia. Federated Learning (FL) has emerged as a key privacy preserving decentralized AI technique. Despite efforts currently being made in FL, its environmental impact is still an open problem. Targeting the minimization of the overall energy consumption of an FL process, we propose the orchestration of computational and communication resources of the involved devices to minimize the total energy required, while guaranteeing a certain performance of the model. To this end, we propose a Soft Actor Critic Deep Reinforcement Learning (DRL) solution, where a penalty function is introduced during training, penalizing the strategies that violate the constraints of the environment, and ensuring a safe RL process. A device level synchronization method, along with a computationally cost effective FL environment are proposed, with the goal of further reducing the energy consumption and communication overhead. Evaluation results show the effectiveness of the proposed scheme compared to four state-of-the-art baseline solutions in both static and dynamic environments, achieving a decrease of up to 94% in the total energy consumption.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Translation Notes:* "Federated Learning" is translated as "联合学习" (Liánhégòngxué)* "Soft Actor Critic" is translated as "软 actor 批评" (Ruǎn yuǎn jīng zhì)* "Deep Reinforcement Learning" is translated as "深度强化学习" (Shēngrán jīnghòu xuéxí)* "Device level synchronization" is translated as "设备层同步" (Jījī kuàng yì)* "Computationally cost-effective" is translated as "计算成本低" (Jìsuàn jīngbèi dī)
</details></li>
</ul>
<hr>
<h2 id="Practical-Parallel-Algorithms-for-Non-Monotone-Submodular-Maximization"><a href="#Practical-Parallel-Algorithms-for-Non-Monotone-Submodular-Maximization" class="headerlink" title="Practical Parallel Algorithms for Non-Monotone Submodular Maximization"></a>Practical Parallel Algorithms for Non-Monotone Submodular Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10656">http://arxiv.org/abs/2308.10656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuang Cui, Kai Han, Jing Tang, He Huang, Xueying Li, Aakas Zhiyuli, Hanxiao Li</li>
<li>for: 这paper focuses on developing efficient and parallelizable algorithms for submodular maximization in various domains of artificial intelligence, such as machine learning, computer vision, and natural language processing.</li>
<li>methods: 该paper proposes two algorithms for non-monotone submodular maximization subject to a knapsack constraint and a $k$-system constraint, respectively. The first algorithm achieves an $(8+\epsilon)$-approximation under $\mathcal{O}(\log n)$ adaptive complexity, which is optimal up to a factor of $\mathcal{O}(\log\log n)$. The second algorithm has both provable approximation ratio and sublinear adaptive complexity.</li>
<li>results: 该paper achieves performance bounds comparable with those of state-of-the-art algorithms on the special case of submodular maximization subject to a cardinality constraint. Extensive experiments on real-world applications demonstrate the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Submodular maximization has found extensive applications in various domains within the field of artificial intelligence, including but not limited to machine learning, computer vision, and natural language processing. With the increasing size of datasets in these domains, there is a pressing need to develop efficient and parallelizable algorithms for submodular maximization. One measure of the parallelizability of a submodular maximization algorithm is its adaptive complexity, which indicates the number of sequential rounds where a polynomial number of queries to the objective function can be executed in parallel. In this paper, we study the problem of non-monotone submodular maximization subject to a knapsack constraint, and propose the first combinatorial algorithm achieving an $(8+\epsilon)$-approximation under $\mathcal{O}(\log n)$ adaptive complexity, which is \textit{optimal} up to a factor of $\mathcal{O}(\log\log n)$. Moreover, we also propose the first algorithm with both provable approximation ratio and sublinear adaptive complexity for the problem of non-monotone submodular maximization subject to a $k$-system constraint. As a by-product, we show that our two algorithms can also be applied to the special case of submodular maximization subject to a cardinality constraint, and achieve performance bounds comparable with those of state-of-the-art algorithms. Finally, the effectiveness of our approach is demonstrated by extensive experiments on real-world applications.
</details>
<details>
<summary>摘要</summary>
“对于人工智能领域内的不同领域，例如机器学习、computer vision和自然语言处理，submodular maximization已经获得了广泛的应用。随着这些领域的数据规模的增加，发展高效且可并行化的submodular maximization算法成为了一个紧迫的需求。一个measure of the parallelizability of a submodular maximization algorithm是其adaptive complexity，它指出在执行多个并行查询时，可以在执行多个sequential round中 Execute a polynomial number of queries to the objective function。在这篇论文中，我们研究了非单调submodular maximization subject to a knapsack constraint，并提出了首次的 combinatorial algorithm，可以在 $\mathcal{O}(\log n)$ adaptive complexity下实现 $(8+\epsilon)$-approximation，这是optimal up to a factor of $\mathcal{O}(\log\log n)$。此外，我们还提出了首次的一个Algorithm with both provable approximation ratio and sublinear adaptive complexity for the problem of non-monotone submodular maximization subject to a $k$-system constraint。在特殊情况下，我们的两个算法可以应用到submodular maximization subject to a cardinality constraint，并实现与现有算法相当的性能。最后，我们通过实验证明了我们的方法的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Deep-Evidential-Learning-for-Bayesian-Quantile-Regression"><a href="#Deep-Evidential-Learning-for-Bayesian-Quantile-Regression" class="headerlink" title="Deep Evidential Learning for Bayesian Quantile Regression"></a>Deep Evidential Learning for Bayesian Quantile Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10650">http://arxiv.org/abs/2308.10650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederik Boe Hüttel, Filipe Rodrigues, Francisco Câmara Pereira</li>
<li>for: 这篇论文旨在提出一种深度 bayesian 量化回归模型，能够无需 Gaussian 假设来Estimate 连续目标分布的Quantiles。</li>
<li>methods: 该方法基于 evidential learning，通过单个推理过程来捕捉 aleatoric 和 epistemic uncertainty。</li>
<li>results: 对于非 Gaussian 分布，该方法可以实现准确的 uncertainty estimation，并且可以分解 aleatoric 和 epistemic uncertainty，以及对于异常样本的 Robustness。<details>
<summary>Abstract</summary>
It is desirable to have accurate uncertainty estimation from a single deterministic forward-pass model, as traditional methods for uncertainty quantification are computationally expensive. However, this is difficult because single forward-pass models do not sample weights during inference and often make assumptions about the target distribution, such as assuming it is Gaussian. This can be restrictive in regression tasks, where the mean and standard deviation are inadequate to model the target distribution accurately. This paper proposes a deep Bayesian quantile regression model that can estimate the quantiles of a continuous target distribution without the Gaussian assumption. The proposed method is based on evidential learning, which allows the model to capture aleatoric and epistemic uncertainty with a single deterministic forward-pass model. This makes the method efficient and scalable to large models and datasets. We demonstrate that the proposed method achieves calibrated uncertainties on non-Gaussian distributions, disentanglement of aleatoric and epistemic uncertainty, and robustness to out-of-distribution samples.
</details>
<details>
<summary>摘要</summary>
希望通过单个决定性前进模型获得准确的不确定性估计，然而这是困难的因为单个前进模型在推理过程中不会采样权重。传统方法的不确定性量化 computationally expensive。这些方法经常假设目标分布是 Gaussian，这可能是回归任务中的限制。这篇文章提出了深度 bayesian 量化回归模型，可以无需 Gaussian 假设来估计连续目标分布的quantiles。提出的方法基于证据学习，使得模型能够捕捉 aleatoric 和 epistemic 不确定性。这使得方法高效可扩展。我们示示了该方法可以在非 Gaussian 分布上获得准确的不确定性估计，分离 aleatoric 和 epistemic 不确定性，并对异常样本具有鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-Based-Sensor-Optimization-for-Bio-markers"><a href="#Reinforcement-Learning-Based-Sensor-Optimization-for-Bio-markers" class="headerlink" title="Reinforcement Learning Based Sensor Optimization for Bio-markers"></a>Reinforcement Learning Based Sensor Optimization for Bio-markers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10649">http://arxiv.org/abs/2308.10649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajal Khandelwal, Pawan Kumar, Syed Azeemuddin</li>
<li>for: 这个论文旨在提高基于电子板的射频生物感测器的敏感度。</li>
<li>methods: 这个论文使用了一种新的强化学习基于二进制蜂群优化（RLBPSO）算法来优化感测器的设计参数，并与其他当前领域的方法进行比较。</li>
<li>results: 研究发现，RLBPSO方法可以在不同频率范围内提高感测器的敏感度，并且在不同的电极设计和脚宽参数下显示出最佳性能。<details>
<summary>Abstract</summary>
Radio frequency (RF) biosensors, in particular those based on inter-digitated capacitors (IDCs), are pivotal in areas like biomedical diagnosis, remote sensing, and wireless communication. Despite their advantages of low cost and easy fabrication, their sensitivity can be hindered by design imperfections, environmental factors, and circuit noise. This paper investigates enhancing the sensitivity of IDC-based RF sensors using novel reinforcement learning based Binary Particle Swarm Optimization (RLBPSO), and it is compared to Ant Colony Optimization (ACO), and other state-of-the-art methods. By focusing on optimizing design parameters like electrode design and finger width, the proposed study found notable improvements in sensor sensitivity. The proposed RLBPSO method shows best optimized design for various frequency ranges when compared to current state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Radio frequency (RF) 感测器，尤其是基于交错电极（IDC）的感测器，在生物医学诊断、远程探测和无线通信等领域具有重要地位。尽管它们具有低成本和易于制造的优点，但是设计瑕疵、环境因素和电路噪声可能会减少它们的敏感度。这篇论文探讨了使用新的强化学习基于二进制群集优化（RLBPSO）方法来提高IDC基于RF感测器的敏感度，并与抗群优化（ACO）和其他当前最佳方法进行比较。通过对设计参数如电极设计和脚宽进行优化，该研究发现了显著提高感测器敏感度的可能性。RLBPSO方法在不同频率范围内对感测器设计优化表现出了最佳的效果。
</details></li>
</ul>
<hr>
<h2 id="Faster-Training-of-Neural-ODEs-Using-Gaus-Legendre-Quadrature"><a href="#Faster-Training-of-Neural-ODEs-Using-Gaus-Legendre-Quadrature" class="headerlink" title="Faster Training of Neural ODEs Using Gauß-Legendre Quadrature"></a>Faster Training of Neural ODEs Using Gauß-Legendre Quadrature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10644">http://arxiv.org/abs/2308.10644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a-norcliffe/torch_gq_adjoint">https://github.com/a-norcliffe/torch_gq_adjoint</a></li>
<li>paper_authors: Alexander Norcliffe, Marc Peter Deisenroth</li>
<li>for: 提高神经泛化方法的训练速度，尤其是大型神经网络。</li>
<li>methods: 使用Gau{\ss}-Legendre quadrature来更快地解决 интегралы，而不会影响神经网络的表达能力。</li>
<li>results: 提出了一种新的方法来快速训练神经泛化方法，并且可以应用于大型神经网络和SDE-based模型的训练。<details>
<summary>Abstract</summary>
Neural ODEs demonstrate strong performance in generative and time-series modelling. However, training them via the adjoint method is slow compared to discrete models due to the requirement of numerically solving ODEs. To speed neural ODEs up, a common approach is to regularise the solutions. However, this approach may affect the expressivity of the model; when the trajectory itself matters, this is particularly important. In this paper, we propose an alternative way to speed up the training of neural ODEs. The key idea is to speed up the adjoint method by using Gau{\ss}-Legendre quadrature to solve integrals faster than ODE-based methods while remaining memory efficient. We also extend the idea to training SDEs using the Wong-Zakai theorem, by training a corresponding ODE and transferring the parameters. Our approach leads to faster training of neural ODEs, especially for large models. It also presents a new way to train SDE-based models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SCULPT-Shape-Conditioned-Unpaired-Learning-of-Pose-dependent-Clothed-and-Textured-Human-Meshes"><a href="#SCULPT-Shape-Conditioned-Unpaired-Learning-of-Pose-dependent-Clothed-and-Textured-Human-Meshes" class="headerlink" title="SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes"></a>SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10638">http://arxiv.org/abs/2308.10638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soubhik Sanyal, Partha Ghosh, Jinlong Yang, Michael J. Black, Justus Thies, Timo Bolkart</li>
<li>for: 这个论文的目的是提出一种基于深度学习的3D生成模型，用于生成披覆的人体3D模型。</li>
<li>methods: 这个论文使用的方法包括：（1）利用CAPE数据集和大规模的2D图像数据集来隐式学习 clothed human body的几何和外观分布；（2）提出一种不需要对数据集进行对应的学习方法，可以从2D图像数据集中学习 pose-dependent clothed human body的几何和外观特征；（3）使用 attribute labels 来分离 cloth 类型和 pose 之间的杂糅关系，以及 cloth 颜色和 pose 之间的杂糅关系。</li>
<li>results: 这个论文的结果显示，使用这种方法可以生成高质量的 clothed human body 3D模型，并且比之前的方法更加灵活和可控。<details>
<summary>Abstract</summary>
We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns to represent the geometry and appearance distribution of clothed human bodies. Training such a model is challenging, as datasets of textured 3D meshes for humans are limited in size and accessibility. Our key observation is that there exist medium-sized 3D scan datasets like CAPE, as well as large-scale 2D image datasets of clothed humans and multiple appearances can be mapped to a single geometry. To effectively learn from the two data modalities, we propose an unpaired learning procedure for pose-dependent clothed and textured human meshes. Specifically, we learn a pose-dependent geometry space from 3D scan data. We represent this as per vertex displacements w.r.t. the SMPL model. Next, we train a geometry conditioned texture generator in an unsupervised way using the 2D image data. We use intermediate activations of the learned geometry model to condition our texture generator. To alleviate entanglement between pose and clothing type, and pose and clothing appearance, we condition both the texture and geometry generators with attribute labels such as clothing types for the geometry, and clothing colors for the texture generator. We automatically generated these conditioning labels for the 2D images based on the visual question answering model BLIP and CLIP. We validate our method on the SCULPT dataset, and compare to state-of-the-art 3D generative models for clothed human bodies. We will release the codebase for research purposes.
</details>
<details>
<summary>摘要</summary>
我们介绍SCULPT，一种新的3D生成模型，用于 clothed和textured 3D雷达图像的人体。我们设计了一个深度神经网络，用于表示人体的几何和外观分布。由于人体3D扫描数据的限制，训练这种模型是一个挑战。我们的关键观察是，存在中等大小的3D扫描数据集，如CAPE，以及大规模的2D图像数据集，包含不同的服装和人体姿势。为了有效地利用这两种数据模式，我们提出了一种无关的学习过程，用于pose-dependent clothed和textured人体3D雷达图像。我们从3D扫描数据中学习一个pose-dependent的几何空间，并将其表示为每个顶点的位差相对于SMPL模型。然后，我们在无监督的情况下使用2D图像数据来训练一个几何条件的文本生成器。我们使用学习的几何模型的中间活动来condition我们的文本生成器。为了避免姿势和服装类型之间的杂谱和姿势和服装外观之间的杂谱，我们将condition both texture和geometry生成器使用属性标签，例如服装类型 для几何和服装颜色 для文本生成器。我们自动生成了这些conditioning标签基于视觉问答模型BLIP和CLIP。我们验证了我们的方法SCULPT数据集上，并与状态之前的3D生成模型进行比较。我们将发布代码库用于研究目的。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-oriented-Robustness-Robust-Image-Model-Evaluation-with-Pretrained-Models"><a href="#Foundation-Model-oriented-Robustness-Robust-Image-Model-Evaluation-with-Pretrained-Models" class="headerlink" title="Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models"></a>Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10632">http://arxiv.org/abs/2308.10632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyan Zhang, Haoyang Liu, Chaozhuo Li, Xing Xie, Sunghun Kim, Haohan Wang</li>
<li>for: 这篇论文主要关注于评估图像分类模型的 robustness 性能，以及开发一种新的评估方法以外测试模型的性能。</li>
<li>methods: 本论文提出了一种新的评估方法，即与基本模型（oracle）进行比较，以评估图像分类模型的性能。此外， authors还提出了一种使用新的样本生成方法，以增加测试数据的多样性。</li>
<li>results: 本论文的实验结果表明，使用该新的评估方法可以准确地评估图像分类模型的 robustness 性能，并且可以免除 fix benchmarks 的限制。此外， authors 还通过对模型的行为进行分析，了解模型在不同情况下的行为。<details>
<summary>Abstract</summary>
Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model's performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle (e.g., the human users), thus a good evaluation protocol is probably to evaluate the models' behaviors in comparison to the oracle. In this paper, we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model). Besides, we design a simple method that can accomplish the evaluation beyond the scope of the benchmarks. Our method extends the image datasets with new samples that are sufficiently perturbed to be distinct from the ones in the original sets, but are still bounded within the same image-label structure the original test image represents, constrained by a foundation model pretrained with a large amount of samples. As a result, our new method will offer us a new way to evaluate the models' robustness performance, free of limitations of fixed benchmarks or constrained perturbations, although scoped by the power of the oracle. In addition to the evaluation results, we also leverage our generated data to understand the behaviors of the model and our new evaluation strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Homogenization-Approach-for-Gradient-Dominated-Stochastic-Optimization"><a href="#A-Homogenization-Approach-for-Gradient-Dominated-Stochastic-Optimization" class="headerlink" title="A Homogenization Approach for Gradient-Dominated Stochastic Optimization"></a>A Homogenization Approach for Gradient-Dominated Stochastic Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10630">http://arxiv.org/abs/2308.10630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyuan Tan, Chenyu Xue, Chuwen Zhang, Qi Deng, Dongdong Ge, Yinyu Ye</li>
<li>for: 这篇论文是关于非对称优化的研究，尤其是gradient-dominated optimization，并使用了homogenization approach来解决问题。</li>
<li>methods: 本文使用了stochastic homogeneous second-order descent method (SHSODM)，并提出了一种具有variance reduction技术的SHSODM，具体来说是使用extremal eigenvector problems来解决问题。</li>
<li>results: 本文的结果显示SHSODM可以在非对称优化中实现global convergence，并且可以比其他off-the-shelf方法更高效。具体来说，本文的结果显示SHSODM可以在$\alpha \in [1, 2]$的情况下实现sample complexity bound of $O(\epsilon^{-7&#x2F;(2 \alpha) +1})$和$\tilde{O}(\epsilon^{-2&#x2F;\alpha})$。此外，本文还提出了一种具有variance reduction技术的SHSODM，其sample complexity bound为$O( \epsilon ^{-( 7-3\alpha ) &#x2F;( 2\alpha )})$。<details>
<summary>Abstract</summary>
Gradient dominance property is a condition weaker than strong convexity, yet it sufficiently ensures global convergence for first-order methods even in non-convex optimization. This property finds application in various machine learning domains, including matrix decomposition, linear neural networks, and policy-based reinforcement learning (RL). In this paper, we study the stochastic homogeneous second-order descent method (SHSODM) for gradient-dominated optimization with $\alpha \in [1, 2]$ based on a recently proposed homogenization approach. Theoretically, we show that SHSODM achieves a sample complexity of $O(\epsilon^{-7/(2 \alpha) +1})$ for $\alpha \in [1, 3/2)$ and $\tilde{O}(\epsilon^{-2/\alpha})$ for $\alpha \in [3/2, 2]$. We further provide a SHSODM with a variance reduction technique enjoying an improved sample complexity of $O( \epsilon ^{-( 7-3\alpha ) /( 2\alpha )})$ for $\alpha \in [1,3/2)$. Our results match the state-of-the-art sample complexity bounds for stochastic gradient-dominated optimization without \emph{cubic regularization}. Since the homogenization approach only relies on solving extremal eigenvector problems instead of Newton-type systems, our methods gain the advantage of cheaper iterations and robustness in ill-conditioned problems. Numerical experiments on several RL tasks demonstrate the efficiency of SHSODM compared to other off-the-shelf methods.
</details>
<details>
<summary>摘要</summary>
“Gradient占主性质是一种弱于强Converter的条件，但它足够保证全局收敛性 для首频方法，即使在非 convex 优化中。这种性质在机器学习中有很多应用，包括矩阵分解、线性神经网络和Policy-based 强化学习（RL）。在这篇论文中，我们研究了随机同质二阶 descend 方法（SHSODM）在 Gradient-dominated 优化中，基于最近提出的同质化方法。我们证明了 SHSODM 的样本复杂度为 $O(\epsilon^{-7/(2\alpha) +1})$ for $\alpha \in [1, 3/2)$ 和 $\tilde{O}(\epsilon^{-2/\alpha})$ for $\alpha \in [3/2, 2]$。我们还提供了一种 SHSODM 使用减少噪声的技术，其样本复杂度为 $O( \epsilon ^{-( 7-3\alpha ) /( 2\alpha )})$ for $\alpha \in [1,3/2)$。我们的结果与不含 кубиック regularization 的 Stochastic gradient-dominated 优化的状态艺术性样本复杂度匹配。由于同质化方法只需解决极值 eigenvector 问题而不需要 Newton-type 系统，我们的方法具有更便宜的迭代和稳定性。 numerical experiments on several RL tasks 表明 SHSODM 比其他 off-the-shelf 方法更高效。”
</details></li>
</ul>
<hr>
<h2 id="GaitPT-Skeletons-Are-All-You-Need-For-Gait-Recognition"><a href="#GaitPT-Skeletons-Are-All-You-Need-For-Gait-Recognition" class="headerlink" title="GaitPT: Skeletons Are All You Need For Gait Recognition"></a>GaitPT: Skeletons Are All You Need For Gait Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10623">http://arxiv.org/abs/2308.10623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andy Catruna, Adrian Cosma, Emilian Radoi</li>
<li>for: 人脸识别（人脸认知）</li>
<li>methods: pose estimation skeletons + hierarchical transformer architecture</li>
<li>results: 比其他skeleton-based gait recognition工作高出6%的平均准确率（82.6%），以及52.16%的排名第1精度（Rank-1）。<details>
<summary>Abstract</summary>
The analysis of patterns of walking is an important area of research that has numerous applications in security, healthcare, sports and human-computer interaction. Lately, walking patterns have been regarded as a unique fingerprinting method for automatic person identification at a distance. In this work, we propose a novel gait recognition architecture called Gait Pyramid Transformer (GaitPT) that leverages pose estimation skeletons to capture unique walking patterns, without relying on appearance information. GaitPT adopts a hierarchical transformer architecture that effectively extracts both spatial and temporal features of movement in an anatomically consistent manner, guided by the structure of the human skeleton. Our results show that GaitPT achieves state-of-the-art performance compared to other skeleton-based gait recognition works, in both controlled and in-the-wild scenarios. GaitPT obtains 82.6% average accuracy on CASIA-B, surpassing other works by a margin of 6%. Moreover, it obtains 52.16% Rank-1 accuracy on GREW, outperforming both skeleton-based and appearance-based approaches.
</details>
<details>
<summary>摘要</summary>
研究人行姿势的分析是一个重要的领域，它在安全、医疗、运动和人机交互等领域都有广泛的应用。最近，人行姿势被视为一种唯一的指纹方法，用于自动识别人员。在这项工作中，我们提出了一种新的步态识别架构，称为步态 pyramid transformer（GaitPT），它利用 pose estimation skeleton 来捕捉唯一的人行姿势特征，不依赖于外观信息。GaitPT 采用了层次 transformer 架构，能够有效地提取人体运动的空间和时间特征，并且以人体骨架为引导，以遵循人体解剖结构。我们的结果表明，GaitPT 在 CASIA-B 上取得了82.6% 的平均准确率，比其他skeleton-based gait recognition 工作高出6%。此外，它在 GREW 上取得了52.16% 的排名第一准确率，超过了skeleton-based和 appearance-based Approaches。
</details></li>
</ul>
<hr>
<h2 id="Weighting-by-Tying-A-New-Approach-to-Weighted-Rank-Correlation"><a href="#Weighting-by-Tying-A-New-Approach-to-Weighted-Rank-Correlation" class="headerlink" title="Weighting by Tying: A New Approach to Weighted Rank Correlation"></a>Weighting by Tying: A New Approach to Weighted Rank Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10622">http://arxiv.org/abs/2308.10622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sascha Henzgen, Eyke Hüllermeier</li>
<li>for: 这篇论文是关于 Statistics 中的排名相关性度量的研究，旨在捕捉两个排名的相同集合中的项目之间的协调程度。</li>
<li>methods: 该论文使用了基于杂态顺序关系的尺度的权重版本，具有可 Parametrized 的权重分配方式，以及一种灵活的权重分配方法。</li>
<li>results: 研究人员通过提出了一种基于杂态顺序关系的尺度的权重版本，具有sound的形式基础和灵活的权重分配方式，实现了在排名相关性度量中增加权重。<details>
<summary>Abstract</summary>
Measures of rank correlation are commonly used in statistics to capture the degree of concordance between two orderings of the same set of items. Standard measures like Kendall's tau and Spearman's rho coefficient put equal emphasis on each position of a ranking. Yet, motivated by applications in which some of the positions (typically those on the top) are more important than others, a few weighted variants of these measures have been proposed. Most of these generalizations fail to meet desirable formal properties, however. Besides, they are often quite inflexible in the sense of committing to a fixed weighing scheme. In this paper, we propose a weighted rank correlation measure on the basis of fuzzy order relations. Our measure, called scaled gamma, is related to Goodman and Kruskal's gamma rank correlation. It is parametrized by a fuzzy equivalence relation on the rank positions, which in turn is specified conveniently by a so-called scaling function. This approach combines soundness with flexibility: it has a sound formal foundation and allows for weighing rank positions in a flexible way.
</details>
<details>
<summary>摘要</summary>
ranking 关系度量通常用于统计学中，用于捕捉两个顺序化的同一组项目之间的协调度。标准度量 LIKE Kendall tau 和 Spearman ρ 系数都强调每个排名位置的等效性。然而，基于应用场景中一些排名位置（通常是排名的顶部）更重要 than others，一些加权变体的度量已经被提议。然而，大多数这些扩展都不具备 Desirable 的正式性质，而且它们通常具有固定的加权方案。在这篇论文中，我们提出了基于杂元顺序关系的加权排名相关度量。我们的度量，叫做 scaled gamma，与 Goodman 和 Kruskal 的 gamma 排名相关度量相关。它是通过一个杂元等同关系来规定排名位置，这个关系再次是通过一个called scaling function来指定。这种方法结合了准确性和灵活性：它具有准确的形式基础，同时允许对排名位置进行灵活的归一化。
</details></li>
</ul>
<hr>
<h2 id="centroIDA-Cross-Domain-Class-Discrepancy-Minimization-Based-on-Accumulative-Class-Centroids-for-Imbalanced-Domain-Adaptation"><a href="#centroIDA-Cross-Domain-Class-Discrepancy-Minimization-Based-on-Accumulative-Class-Centroids-for-Imbalanced-Domain-Adaptation" class="headerlink" title="centroIDA: Cross-Domain Class Discrepancy Minimization Based on Accumulative Class-Centroids for Imbalanced Domain Adaptation"></a>centroIDA: Cross-Domain Class Discrepancy Minimization Based on Accumulative Class-Centroids for Imbalanced Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10619">http://arxiv.org/abs/2308.10619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaona Sun, Zhenyu Wu, Yichen Liu, Saier Hu, Zhiqiang Zhan, Yang Ji</li>
<li>for: 本研究targets the imbalanced domain adaptation (IDA) problem, which involves both covariate and long-tailed label shifts across domains.</li>
<li>methods: The proposed method, called centroIDA, uses a cross-domain class discrepancy minimization approach based on accumulative class-centroids alignment. This method includes class-based re-sampling, accumulative class-centroids alignment, and class-wise feature alignment.</li>
<li>results: The proposed method outperforms other state-of-the-art (SOTA) methods on the IDA problem, especially when the degree of label shift increases.<details>
<summary>Abstract</summary>
Unsupervised Domain Adaptation (UDA) approaches address the covariate shift problem by minimizing the distribution discrepancy between the source and target domains, assuming that the label distribution is invariant across domains. However, in the imbalanced domain adaptation (IDA) scenario, covariate and long-tailed label shifts both exist across domains. To tackle the IDA problem, some current research focus on minimizing the distribution discrepancies of each corresponding class between source and target domains. Such methods rely much on the reliable pseudo labels' selection and the feature distributions estimation for target domain, and the minority classes with limited numbers makes the estimations more uncertainty, which influences the model's performance. In this paper, we propose a cross-domain class discrepancy minimization method based on accumulative class-centroids for IDA (centroIDA). Firstly, class-based re-sampling strategy is used to obtain an unbiased classifier on source domain. Secondly, the accumulative class-centroids alignment loss is proposed for iterative class-centroids alignment across domains. Finally, class-wise feature alignment loss is used to optimize the feature representation for a robust classification boundary. A series of experiments have proved that our method outperforms other SOTA methods on IDA problem, especially with the increasing degree of label shift.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation (UDA)方法解决 covariate shift 问题，即在源频率和目标频率之间的分布差异假设 label 分布是各频率之间不变。然而，在不平衡频率适应 (IDA) 场景中，covariate 和长尾标签各自存在在不同频率之间的差异。为解决 IDA 问题，一些当前研究强调在每个匹配的类之间分布差异的最小化。这些方法需要可靠的 pseudo label 选择和目标频率的特征分布估计，但是少数类的数量有限，这会使估计更加不确定，影响模型的性能。在这篇论文中，我们提出了一种基于积累类中心的 cross-domain 类差异最小化方法（centroIDA）。首先，使用 class-based 重采样策略来在源频率上获得不偏向的类ifier。其次，为了迭代类中心的对齐，我们提出了积累类中心对齐损失。最后，使用类别特征对齐损失来优化特征表示，以实现一个可靠的分类边界。经过一系列实验证明，我们的方法在 IDA 问题上表现出优于现有 SOTA 方法，特别是随着标签差异的增加。
</details></li>
</ul>
<hr>
<h2 id="ST-RAP-A-Spatio-Temporal-Framework-for-Real-Estate-Appraisal"><a href="#ST-RAP-A-Spatio-Temporal-Framework-for-Real-Estate-Appraisal" class="headerlink" title="ST-RAP: A Spatio-Temporal Framework for Real Estate Appraisal"></a>ST-RAP: A Spatio-Temporal Framework for Real Estate Appraisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10609">http://arxiv.org/abs/2308.10609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dojeon-ai/strap">https://github.com/dojeon-ai/strap</a></li>
<li>paper_authors: Hojoon Lee, Hawon Jeong, Byungkun Lee, Kyungyup Lee, Jaegul Choo</li>
<li>for: 这个论文是为了提出一种新的空间 temporal 框架，用于房地产评估。</li>
<li>methods: 这个框架使用层次结构和不同类型的图神经网络来同时捕捉空间和时间方面的关系和动态。</li>
<li>results: 通过对大规模房地产数据集进行广泛的实验，ST-RAP 表明了在房地产评估中同时考虑空间和时间方面的综合效果。Here’s the translation in English:</li>
<li>for: This paper proposes a novel Spatio-Temporal framework for Real estate APpraisal.</li>
<li>methods: The framework employs a hierarchical architecture with a heterogeneous graph neural network to simultaneously capture temporal dynamics and spatial relationships.</li>
<li>results: Through comprehensive experiments on a large-scale real estate dataset, ST-RAP outperforms previous methods, demonstrating the significant benefits of integrating spatial and temporal aspects in real estate appraisal.<details>
<summary>Abstract</summary>
In this paper, we introduce ST-RAP, a novel Spatio-Temporal framework for Real estate APpraisal. ST-RAP employs a hierarchical architecture with a heterogeneous graph neural network to encapsulate temporal dynamics and spatial relationships simultaneously. Through comprehensive experiments on a large-scale real estate dataset, ST-RAP outperforms previous methods, demonstrating the significant benefits of integrating spatial and temporal aspects in real estate appraisal. Our code and dataset are available at https://github.com/dojeon-ai/STRAP.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了ST-RAP，一种新的空间-时间框架，用于房地产评估。ST-RAP使用层次架构和不同类型的图 neural network来同时捕捉时间动态和空间关系。通过对大规模房地产数据进行全面的实验，ST-RAP表明了将空间和时间方面综合考虑在房地产评估中的重要性，并且超过了之前的方法。我们的代码和数据可以在https://github.com/dojeon-ai/STRAP上获取。
</details></li>
</ul>
<hr>
<h2 id="FocalDreamer-Text-driven-3D-Editing-via-Focal-fusion-Assembly"><a href="#FocalDreamer-Text-driven-3D-Editing-via-Focal-fusion-Assembly" class="headerlink" title="FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly"></a>FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10608">http://arxiv.org/abs/2308.10608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, Bingbing Ni</li>
<li>for: 该论文旨在提供一种基于文本指示的高级3D编辑方法，以便在特定区域内进行精细编辑。</li>
<li>methods: 该方法基于文本蒸馈采样的技术，并使用geometry union和双轨渲染技术将独立的3D部件组合成完整的物体。此外，该方法还提出了geometry focal loss和样式一致准则，以促进焦点融合和整体外观的一致性。</li>
<li>results: 实验结果表明，FocalDreamer方法可以在量化和质量上提供高级3D编辑功能，并且在多种图形引擎下生成高质量的几何学和PBRTexture。<details>
<summary>Abstract</summary>
While text-3D editing has made significant strides in leveraging score distillation sampling, emerging approaches still fall short in delivering separable, precise and consistent outcomes that are vital to content creation. In response, we introduce FocalDreamer, a framework that merges base shape with editable parts according to text prompts for fine-grained editing within desired regions. Specifically, equipped with geometry union and dual-path rendering, FocalDreamer assembles independent 3D parts into a complete object, tailored for convenient instance reuse and part-wise control. We propose geometric focal loss and style consistency regularization, which encourage focal fusion and congruent overall appearance. Furthermore, FocalDreamer generates high-fidelity geometry and PBR textures which are compatible with widely-used graphics engines. Extensive experiments have highlighted the superior editing capabilities of FocalDreamer in both quantitative and qualitative evaluations.
</details>
<details>
<summary>摘要</summary>
“文本3D编辑在利用分数浸泡样本方面已经做出了重要进步，但现有方法仍然缺乏提供分解、精准和一致的结果，这些结果对内容创建至关重要。为此，我们介绍了FOCAL Dreamer框架，该框架将基本形状与可编辑部分相结合，根据文本提示进行细致的编辑，并在 Desired 区域内进行精细控制。Specifically, FOCAL Dreamer 使用geometry union和双路渲染技术，将独立的3D部件组合成完整的 объек，且适用于便捷的实例重用和部件控制。我们还提出了 геометрической专注损失和风格一致 regularization，这两种正则化激励核心融合和一致的整体外观。此外，FOCAL Dreamer 生成高质量的几何学和PBR文本ure，与广泛使用的图形引擎相容。我们的实验表明，FOCAL Dreamer 在量化和质量两个方面的评价都具有明显的优势。”
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Complex-Systems-with-Cascades-Using-Continuous-Time-Bayesian-Networks"><a href="#Analyzing-Complex-Systems-with-Cascades-Using-Continuous-Time-Bayesian-Networks" class="headerlink" title="Analyzing Complex Systems with Cascades Using Continuous-Time Bayesian Networks"></a>Analyzing Complex Systems with Cascades Using Continuous-Time Bayesian Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10606">http://arxiv.org/abs/2308.10606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Bregoli, Karin Rathsman, Marco Scutari, Fabio Stella, Søren Wengel Mogensen</li>
<li>for: 本研究旨在分析复杂系统中的协同事件链的冲击行为，以便更好地理解系统中哪些状态会触发冲击行为。</li>
<li>methods: 本研究使用连续时间权重网络（CTBN）模型来分析复杂系统中的协同事件链，并提出了一种新的知识提取方法来从CTBN中提取有用的信息。</li>
<li>results: 研究人员通过应用CTBN模型和新的知识提取方法，成功地找到了可能导致冲击行为的系统状态，并获得了可读的输出结果。<details>
<summary>Abstract</summary>
Interacting systems of events may exhibit cascading behavior where events tend to be temporally clustered. While the cascades themselves may be obvious from the data, it is important to understand which states of the system trigger them. For this purpose, we propose a modeling framework based on continuous-time Bayesian networks (CTBNs) to analyze cascading behavior in complex systems. This framework allows us to describe how events propagate through the system and to identify likely sentry states, that is, system states that may lead to imminent cascading behavior. Moreover, CTBNs have a simple graphical representation and provide interpretable outputs, both of which are important when communicating with domain experts. We also develop new methods for knowledge extraction from CTBNs and we apply the proposed methodology to a data set of alarms in a large industrial system.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：复杂系统中的事件互动可能会展现堆叠行为，其中事件倾向于在时间上叠加。虽然堆叠本身可能从数据中容易看到，但是要理解触发它们的系统状态非常重要。为了解决这个问题，我们提出了基于连续时间概率网络（CTBN）的模型化框架，用于分析复杂系统中的堆叠行为。这个框架可以描述事件如何在系统中传播，并识别可能导致堆叠行为的系统状态。此外，CTBN具有简单的图形表示和可解释的输出，这两点都是与领域专家交流时非常重要。我们还开发了新的知识提取方法，并将该方法应用于一个大型工业系统的数据集。
</details></li>
</ul>
<hr>
<h2 id="BackTrack-Robust-template-update-via-Backward-Tracking-of-candidate-template"><a href="#BackTrack-Robust-template-update-via-Backward-Tracking-of-candidate-template" class="headerlink" title="BackTrack: Robust template update via Backward Tracking of candidate template"></a>BackTrack: Robust template update via Backward Tracking of candidate template</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10604">http://arxiv.org/abs/2308.10604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongwook Lee, Wonjun Choi, Seohyung Lee, ByungIn Yoo, Eunho Yang, Seongju Hwang</li>
<li>for: 提高视觉对象跟踪性能，解决模板更新引起的模型漂移问题</li>
<li>methods: 提出BackTrack方法，通过倒计时追踪候选模板，计算候选模板的可靠程度，选择可靠的候选模板更新模板</li>
<li>results: 对多个跟踪 benchmark 进行了广泛的实验，证明BackTrack方法可以提高模板更新精度，达到最新的状态角度表现In English, this translates to:</li>
<li>for: Improving visual object tracking performance, solving the template drift problem caused by template updates</li>
<li>methods: Propose BackTrack method, which backward tracks candidates on past frames to calculate their reliability, and selects reliable candidates to update the template</li>
<li>results: Extensive experiments on various tracking benchmarks demonstrate that BackTrack method can improve template update accuracy, achieving state-of-the-art performance.<details>
<summary>Abstract</summary>
Variations of target appearance such as deformations, illumination variance, occlusion, etc., are the major challenges of visual object tracking that negatively impact the performance of a tracker. An effective method to tackle these challenges is template update, which updates the template to reflect the change of appearance in the target object during tracking. However, with template updates, inadequate quality of new templates or inappropriate timing of updates may induce a model drift problem, which severely degrades the tracking performance. Here, we propose BackTrack, a robust and reliable method to quantify the confidence of the candidate template by backward tracking it on the past frames. Based on the confidence score of candidates from BackTrack, we can update the template with a reliable candidate at the right time while rejecting unreliable candidates. BackTrack is a generic template update scheme and is applicable to any template-based trackers. Extensive experiments on various tracking benchmarks verify the effectiveness of BackTrack over existing template update algorithms, as it achieves SOTA performance on various tracking benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduction de texte en chinois simplifiéVariations de l'apparence cible telles que les déformations, les variations d'éclairage, les occlusions, etc., sont les principales difficultés de la suivi d'objets visuels qui négativement affectent la performance d'un traqueur. Une méthode efficace pour relever ces défis est l'actualisation du modèle, qui met à jour le modèle pour refléter les changements d'apparence dans l'objet cible pendant la traque. Cependant, avec des mises à jour de modèle, la qualité inadéquate des nouveaux modèles ou la mise à jour inopportune peuvent entraîner un problème de drift de modèle, qui altère gravement la performance de la traque. Voici ce que nous proposons : BackTrack, un méthode robuste et fiable pour estimer la confiance du modèle de candidature en backward tracking les frames passées. En fonction de la note de confiance des candidats de BackTrack, nous pouvons mettre à jour le modèle avec un candidat fiable à temps opportun, tout en rejetant les candidats non fiables. BackTrack est un schéma d'actualisation de modèle générique et est applicable à tous les trackers de modèle. Les expériences extensives sur divers bancs d'essai de suivi ont verifié l'efficacité de BackTrack par rapport aux algorithmes d'actualisation de modèle existants, car elle atteint des performances SOTA sur divers bancs d'essai de suivi.
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Transferability-of-Adversarial-Examples-with-Arbitrary-Style-Transfer"><a href="#Improving-the-Transferability-of-Adversarial-Examples-with-Arbitrary-Style-Transfer" class="headerlink" title="Improving the Transferability of Adversarial Examples with Arbitrary Style Transfer"></a>Improving the Transferability of Adversarial Examples with Arbitrary Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10601">http://arxiv.org/abs/2308.10601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhijin-ge/stm">https://github.com/zhijin-ge/stm</a></li>
<li>paper_authors: Zhijin Ge, Fanhua Shang, Hongying Liu, Yuanyuan Liu, Liang Wan, Wei Feng, Xiaosen Wang</li>
<li>for: 本研究旨在提高黑盒 Setting 下 adversarial example 的转移性，通过将具有不同领域的资料作为增强资料。</li>
<li>methods: 我们提出了一种名为 Style Transfer Method (STM) 的新攻击方法，它利用一个提案的自由式类别转换网络将图像转换为不同的领域。为了确保涂抹后的图像仍保持 semantics 信息，我们在涂抹过程中进行了精确的调整和混合。</li>
<li>results: 我们在 ImageNet-compatible 数据集上进行了广泛的实验，结果显示，我们的提案的 STM 方法可以在 normally 训练的模型或 adversarially 训练的模型上提高 adversarial transferability，较前者的 state-of-the-art input transformation-based 攻击方法。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Zhijin-Ge/STM%E3%80%82">https://github.com/Zhijin-Ge/STM。</a><details>
<summary>Abstract</summary>
Deep neural networks are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on clean inputs. Although many attack methods can achieve high success rates in the white-box setting, they also exhibit weak transferability in the black-box setting. Recently, various methods have been proposed to improve adversarial transferability, in which the input transformation is one of the most effective methods. In this work, we notice that existing input transformation-based works mainly adopt the transformed data in the same domain for augmentation. Inspired by domain generalization, we aim to further improve the transferability using the data augmented from different domains. Specifically, a style transfer network can alter the distribution of low-level visual features in an image while preserving semantic content for humans. Hence, we propose a novel attack method named Style Transfer Method (STM) that utilizes a proposed arbitrary style transfer network to transform the images into different domains. To avoid inconsistent semantic information of stylized images for the classification network, we fine-tune the style transfer network and mix up the generated images added by random noise with the original images to maintain semantic consistency and boost input diversity. Extensive experimental results on the ImageNet-compatible dataset show that our proposed method can significantly improve the adversarial transferability on either normally trained models or adversarially trained models than state-of-the-art input transformation-based attacks. Code is available at: https://github.com/Zhijin-Ge/STM.
</details>
<details>
<summary>摘要</summary>
Extensive experimental results on the ImageNet-compatible dataset show that our proposed method can significantly improve the adversarial transferability on either normally trained models or adversarially trained models than state-of-the-art input transformation-based attacks. Code is available at: https://github.com/Zhijin-Ge/STM.Translated into Simplified Chinese:深度神经网络受到人类不可见的攻击例子的威胁，尽管许多攻击方法在白盒设定下可以达到高成功率，但它们也表现出了软输送性。在黑盒设定下，许多方法已经被提出来改进攻击传输性，其中输入转换是最有效的方法。在这项工作中，我们注意到现有的输入转换基于工作主要采用转换后的数据进行增强。受到域泛化的启发，我们想要进一步提高传输性，使用不同域的数据进行增强。Specifically, a style transfer network can alter the distribution of low-level visual features in an image while preserving semantic content for humans. Hence, we propose a novel attack method named Style Transfer Method (STM) that utilizes a proposed arbitrary style transfer network to transform the images into different domains. To avoid inconsistent semantic information of stylized images for the classification network, we fine-tune the style transfer network and mix up the generated images added by random noise with the original images to maintain semantic consistency and boost input diversity. 广泛的实验结果表明，我们提出的方法可以在 ImageNet  compatible 数据集上对 normally 训练的模型和 adversarially 训练的模型进行较好的攻击传输性，比采用 state-of-the-art 的输入转换基于攻击方法高。代码可以在：https://github.com/Zhijin-Ge/STM 中找到。
</details></li>
</ul>
<hr>
<h2 id="Image-free-Classifier-Injection-for-Zero-Shot-Classification"><a href="#Image-free-Classifier-Injection-for-Zero-Shot-Classification" class="headerlink" title="Image-free Classifier Injection for Zero-Shot Classification"></a>Image-free Classifier Injection for Zero-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10599">http://arxiv.org/abs/2308.10599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/explainableml/imagefreezsl">https://github.com/explainableml/imagefreezsl</a></li>
<li>paper_authors: Anders Christensen, Massimiliano Mancini, A. Sophia Koepke, Ole Winther, Zeynep Akata</li>
<li>for: 这个研究目的是为了将预训模型转换为零shot学习模型，不需要训练图像数据。</li>
<li>methods: 我们提出了一种名为Image-free Classifier Injection with Semantics（ICIS）的方法，可以在预训模型上附加零shot学习能力，不需要任何图像数据。ICIS使用两个encoder-decoder网络，将描述器转换为分类器的重量，并且运用(cross-)重建和偏角损失来调整解oding过程。</li>
<li>results: 实验结果显示，ICIS可以在 benchmark ZSL 数据集上生成出高性能的零shot分类器。code可以在<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/ImageFreeZSL">https://github.com/ExplainableML/ImageFreeZSL</a> 上获取。<details>
<summary>Abstract</summary>
Zero-shot learning models achieve remarkable results on image classification for samples from classes that were not seen during training. However, such models must be trained from scratch with specialised methods: therefore, access to a training dataset is required when the need for zero-shot classification arises. In this paper, we aim to equip pre-trained models with zero-shot classification capabilities without the use of image data. We achieve this with our proposed Image-free Classifier Injection with Semantics (ICIS) that injects classifiers for new, unseen classes into pre-trained classification models in a post-hoc fashion without relying on image data. Instead, the existing classifier weights and simple class-wise descriptors, such as class names or attributes, are used. ICIS has two encoder-decoder networks that learn to reconstruct classifier weights from descriptors (and vice versa), exploiting (cross-)reconstruction and cosine losses to regularise the decoding process. Notably, ICIS can be cheaply trained and applied directly on top of pre-trained classification models. Experiments on benchmark ZSL datasets show that ICIS produces unseen classifier weights that achieve strong (generalised) zero-shot classification performance. Code is available at https://github.com/ExplainableML/ImageFreeZSL .
</details>
<details>
<summary>摘要</summary>
Zero-shot learning模型在图像分类任务中达到了非常出色的结果，但是这些模型必须通过特殊的方法进行训练，因此需要训练集的存在。在这篇论文中，我们想要让预训练模型具有零批学习的分类能力，而无需使用图像数据。我们通过我们提出的图像自由分类插入（ICIS）技术来实现这一目标。ICIS使用两个Encoder-Decoder网络来学习从描述符（包括类名或属性）中重建分类器权重，不需要使用图像数据。我们通过（cross-）重建和归一化损失来规范解码过程。吸引人的是，ICIS可以便宜地训练和应用于预训练分类模型之上。我们在 benchmark ZSL 数据集上进行了实验，并证明了 ICIS 可以生成出高性能的零批分类器。代码可以在 GitHub 上找到：https://github.com/ExplainableML/ImageFreeZSL。
</details></li>
</ul>
<hr>
<h2 id="RADIANCE-Radio-Frequency-Adversarial-Deep-learning-Inference-for-Automated-Network-Coverage-Estimation"><a href="#RADIANCE-Radio-Frequency-Adversarial-Deep-learning-Inference-for-Automated-Network-Coverage-Estimation" class="headerlink" title="RADIANCE: Radio-Frequency Adversarial Deep-learning Inference for Automated Network Coverage Estimation"></a>RADIANCE: Radio-Frequency Adversarial Deep-learning Inference for Automated Network Coverage Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10584">http://arxiv.org/abs/2308.10584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sopan Sarkar, Mohammad Hossein Manshaei, Marwan Krunz</li>
<li>for: 这个研究是为了自动生成无线电网络的覆盖地图（RF maps），以便于无线网络的计划、Access Point和基站的位置设置、地点定位和覆盖估计。</li>
<li>methods: 这个研究使用了一个对抗式深度学习推断（GAN）基础的方法，叫做 radio-frequency adversarial deep-learning inference for automated network coverage estimation（RADIANCE），它使用了一个 semantic map，一个高级的内部环境表示，来编码空间关系和内部环境中物体的特征。这个方法还使用了一个新的梯度基于的损失函数，它计算了从环境中的点到接收信号强度（RSS）值的变化的大小和方向。</li>
<li>results: 这个研究的结果显示，使用 RADIANCE 可以实现覆盖地图的自动生成，并且与射线追踪模拟相比，RADIANCE 可以得到较佳的结果，其中的 mean average error（MAE）为 0.09，root-mean-squared error（RMSE）为 0.29，peak signal-to-noise ratio（PSNR）为 10.78，multi-scale structural similarity index（MS-SSIM）为 0.80。<details>
<summary>Abstract</summary>
Radio-frequency coverage maps (RF maps) are extensively utilized in wireless networks for capacity planning, placement of access points and base stations, localization, and coverage estimation. Conducting site surveys to obtain RF maps is labor-intensive and sometimes not feasible. In this paper, we propose radio-frequency adversarial deep-learning inference for automated network coverage estimation (RADIANCE), a generative adversarial network (GAN) based approach for synthesizing RF maps in indoor scenarios. RADIANCE utilizes a semantic map, a high-level representation of the indoor environment to encode spatial relationships and attributes of objects within the environment and guide the RF map generation process. We introduce a new gradient-based loss function that computes the magnitude and direction of change in received signal strength (RSS) values from a point within the environment. RADIANCE incorporates this loss function along with the antenna pattern to capture signal propagation within a given indoor configuration and generate new patterns under new configuration, antenna (beam) pattern, and center frequency. Extensive simulations are conducted to compare RADIANCE with ray-tracing simulations of RF maps. Our results show that RADIANCE achieves a mean average error (MAE) of 0.09, root-mean-squared error (RMSE) of 0.29, peak signal-to-noise ratio (PSNR) of 10.78, and multi-scale structural similarity index (MS-SSIM) of 0.80.
</details>
<details>
<summary>摘要</summary>
射频覆盖地图（RF 地图）在无线网络中广泛使用，用于容量规划、Access Point和基站的布局、地理位置和覆盖估计。进行场景调查以获取RF地图是劳动密集且不可靠。在这篇论文中，我们提出了射频对抗深度学习推断（RADIANCE），一种基于生成对抗网络（GAN）的方法，用于自动生成室内场景中的射频地图。RADIANCE利用了 semantic map，一个高级的室内环境表示，以编码空间关系和环境中对象的特征。我们引入了一个新的梯度基于损失函数，计算从环境中的点的受信号强度（RSS）值的变化大小和方向。RADIANCE将这个损失函数与天线 patrón 混合，以捕捉室内配置下的信号传播特性，并生成新的射频地图。我们对RADIANCE与射频投影 simulations进行了广泛的比较。结果显示，RADIANCE的 mean average error（MAE）为0.09，root-mean-squared error（RMSE）为0.29，受信号强度比（PSNR）为10.78，和多尺度结构相似度（MS-SSIM）为0.80。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-online-framework-for-BCI-evaluation-A-MOABB-perspective"><a href="#Pseudo-online-framework-for-BCI-evaluation-A-MOABB-perspective" class="headerlink" title="Pseudo-online framework for BCI evaluation: A MOABB perspective"></a>Pseudo-online framework for BCI evaluation: A MOABB perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11656">http://arxiv.org/abs/2308.11656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Igor Carrara, Théodore Papadopoulo</li>
<li>for: 本研究旨在扩展现有的MOABB框架，以便在 Pseudo-online 模式下比较不同算法的性能。</li>
<li>methods: 本研究使用了 overlaping sliding windows 技术，并引入了停靠状态事件，以考虑所有不同的思维方式。</li>
<li>results: 研究分析了过去 15 年的州际顶点算法，并对多个 Motor Imagery 数据集进行了分析，显示了两种方法之间的统计上的差异。<details>
<summary>Abstract</summary>
Objective: BCI (Brain-Computer Interface) technology operates in three modes: online, offline, and pseudo-online. In the online mode, real-time EEG data is constantly analyzed. In offline mode, the signal is acquired and processed afterwards. The pseudo-online mode processes collected data as if they were received in real-time. The main difference is that the offline mode often analyzes the whole data, while the online and pseudo-online modes only analyze data in short time windows. Offline analysis is usually done with asynchronous BCIs, which restricts analysis to predefined time windows. Asynchronous BCI, compatible with online and pseudo-online modes, allows flexible mental activity duration. Offline processing tends to be more accurate, while online analysis is better for therapeutic applications. Pseudo-online implementation approximates online processing without real-time constraints. Many BCI studies being offline introduce biases compared to real-life scenarios, impacting classification algorithm performance. Approach: The objective of this research paper is therefore to extend the current MOABB framework, operating in offline mode, so as to allow a comparison of different algorithms in a pseudo-online setting with the use of a technology based on overlapping sliding windows. To do this will require the introduction of a idle state event in the dataset that takes into account all different possibilities that are not task thinking. To validate the performance of the algorithms we will use the normalized Matthews Correlation Coefficient (nMCC) and the Information Transfer Rate (ITR). Main results: We analyzed the state-of-the-art algorithms of the last 15 years over several Motor Imagery (MI) datasets composed by several subjects, showing the differences between the two approaches from a statistical point of view. Significance: The ability to analyze the performance of different algorithms in offline and pseudo-online modes will allow the BCI community to obtain more accurate and comprehensive reports regarding the performance of classification algorithms.
</details>
<details>
<summary>摘要</summary>
目标：BCI（脑computer接口）技术运行在三种模式之一：在线、离线和假在线模式。在在线模式下，实时EEG数据被不断分析。在离线模式下，信号被获取并处理后进行分析。假在线模式处理收集的数据，如果收集的数据是在实时进行处理的话。主要的区别在于离线模式通常分析整个数据，而在线和假在线模式只分析短时间窗口中的数据。离线分析通常更加准确，而在线分析更适合治疗应用。假在线实现方式模拟在线处理，不受实时限制。许多BCI研究都是离线进行，这会引入比实际情况更多的偏见，影响分类算法的性能。方法：为了扩展当前的MOABB框架，它在离线模式下运行，以便对不同算法的比较在假在线设置下进行。为此，需要引入一个假静止状态事件，考虑所有不同的可能性，不是任务思考。为了验证算法的性能，我们使用 норма化的玛特维克相互相关系数（nMCC）和信息传输率（ITR）。结果：我们对过去15年的State-of-the-art算法进行分析，并在多个motor imagery（MI）数据集上进行了比较。显示了两种方法的 statistically 的差异。重要性：能够在离线和假在线模式下分析不同算法的性能，将BCI社区获得更加准确和全面的报告关于分类算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Overconfidence-for-Active-Learning"><a href="#Overcoming-Overconfidence-for-Active-Learning" class="headerlink" title="Overcoming Overconfidence for Active Learning"></a>Overcoming Overconfidence for Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10571">http://arxiv.org/abs/2308.10571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujin Hwang, Won Jo, Juyoung Hong, Yukyung Choi</li>
<li>for: 这篇论文主要关注于解决人工智能学习中的数据选择问题，以提高模型的准确性和可靠性。</li>
<li>methods: 本论文提出了两种解决过去误差问题的方法：一是将数据扩展为多个不同的数据分布，以帮助模型更好地调整；二是根据预测margin的排名，选择数据进行调整，以避免选择过度自信的数据。</li>
<li>results: 经过各种实验和分析， authors发现了两种方法可以有效地缓和过度自信的问题，并且这些方法可以轻松地应用。<details>
<summary>Abstract</summary>
It is not an exaggeration to say that the recent progress in artificial intelligence technology depends on large-scale and high-quality data. Simultaneously, a prevalent issue exists everywhere: the budget for data labeling is constrained. Active learning is a prominent approach for addressing this issue, where valuable data for labeling is selected through a model and utilized to iteratively adjust the model. However, due to the limited amount of data in each iteration, the model is vulnerable to bias; thus, it is more likely to yield overconfident predictions. In this paper, we present two novel methods to address the problem of overconfidence that arises in the active learning scenario. The first is an augmentation strategy named Cross-Mix-and-Mix (CMaM), which aims to calibrate the model by expanding the limited training distribution. The second is a selection strategy named Ranked Margin Sampling (RankedMS), which prevents choosing data that leads to overly confident predictions. Through various experiments and analyses, we are able to demonstrate that our proposals facilitate efficient data selection by alleviating overconfidence, even though they are readily applicable.
</details>
<details>
<summary>摘要</summary>
不是订正说，现代人工智能技术的进步几乎完全取决于大规模高质量数据。然而，一个普遍存在的问题是 everywhere：数据标注预算受限。活动学习是一种主要的方法来解决这个问题，其中通过模型选择有价值的数据进行标注，并使用这些数据来逐次调整模型。然而，由于每次迭代中的数据量有限，模型容易受到偏见，因此更可能产生过optimistic预测。在这篇论文中，我们提出了两种解决活动学习场景中出现的过optimistic问题的方法。首先是一种扩展训练分布的扩展策略名为 Cross-Mix-and-Mix (CMaM)，其目的是使模型更加准确。其次是一种名为 Ranked Margin Sampling (RankedMS)的选择策略，它避免选择导致过optimistic预测的数据。通过多种实验和分析，我们能够证明我们的建议可以有效地选择数据，减少过optimistic预测，即使它们可以应用。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Riemannian-Conjugate-Gradient-Method-on-the-Stiefel-Manifold"><a href="#Decentralized-Riemannian-Conjugate-Gradient-Method-on-the-Stiefel-Manifold" class="headerlink" title="Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold"></a>Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10547">http://arxiv.org/abs/2308.10547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Chen, Haishan Ye, Mengmeng Wang, Tianxin Huang, Guang Dai, Ivor W. Tsang, Yong Liu</li>
<li>for: 这个论文 targets at solving optimization problems on the Stiefel manifold, a non-convex set, using a decentralized Riemannian conjugate gradient descent (DRCGD) method.</li>
<li>methods: 该方法使用了一种分布式的agent网络，每个agent负责一个本地函数，并且通过无向连接图进行交互。DRCGD方法不需要进行Expensive的里曼几何运算，从而减少了每个agent的计算复杂性。</li>
<li>results: 该论文提出了一种首次在分布式里曼空间中实现全球收敛的DRCGD方法，并且证明了该方法的 globally convergence。<details>
<summary>Abstract</summary>
The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than the second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there has little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from expensive Riemannian geometric operations such as retractions, exponential maps, and vector transports, thereby reducing the computational complexity required by each agent. To the best of our knowledge, DRCGD is the first decentralized Riemannian conjugate gradient algorithm to achieve global convergence over the Stiefel manifold.
</details>
<details>
<summary>摘要</summary>
“ conjugate gradient 方法是一种重要的首尔顺化方法，通常比坡度下降法快速 converges，且计算成本较低于第二项方法。然而，在分布式场景中，各种 conjugate gradient 方法已经在欧几何空间和里曼尼投影上进行了许多研究，但对于分布式场景的研究却有很少。这篇论文提出了一种分布式里曼尼 conjugate gradient descent（DRCGD）方法，旨在全局函数的最小化，这个函数是在Stiefel manifold上的一个全部函数。实际上，这个问题是分布式在一个无向网络中的一个问题，每个代理人都有一个本地函数，并且在各个代理人之间进行了无向网络上的通信。由于Stiefel manifold是一个非凸集，因此全球函数是一个可能非凸（但是光滑的）的全部函数。提案的方法不需要 expensive Riemannian geometric operations，例如投影、对映图和向量运输，因此每个代理人的计算复杂度很低。到目前为止，DRCGD 是首个在 Stiefel manifold 上实现全球均衡的分布式里曼尼 conjugate gradient 算法。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Accelerated-Model-Training-via-Bayesian-Data-Selection"><a href="#Towards-Accelerated-Model-Training-via-Bayesian-Data-Selection" class="headerlink" title="Towards Accelerated Model Training via Bayesian Data Selection"></a>Towards Accelerated Model Training via Bayesian Data Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10544">http://arxiv.org/abs/2308.10544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijie Deng, Peng Cui, Jun Zhu</li>
<li>for: This paper is written to address the problem of mislabeled, duplicated, or biased data in real-world scenarios, which can hinder model convergence and prolong training.</li>
<li>methods: The paper proposes a more reasonable data selection principle by examining the data’s impact on the model’s generalization loss, and incorporates off-the-shelf zero-shot predictors built on large-scale pre-trained models.</li>
<li>results: The paper performs extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observes superior training efficiency over competitive baselines. Specifically, the method achieves similar predictive performance with significantly fewer training iterations than leading data selection methods on the challenging WebVision benchmark.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解决现实世界中的杂乱数据问题，包括杂乱标签、重复数据和偏见数据，这些问题会延长模型训练和抑制模型的准确性。</li>
<li>methods: 论文提出了一种更加合理的数据选择原则，即通过考虑数据对模型通用损失的影响来选择数据。此外，它还 incorporates off-the-shelf zero-shot predictors built on large-scale pre-trained models。</li>
<li>results: 论文在实际中进行了严格的实验研究，包括在具有较大数据噪和数据不均衡的online批处理场景中进行了训练效率的比较。研究结果显示，该方法在比较难的WebVision数据集上可以在相对较少的训练迭代数下达到类似的预测性能，与主流数据选择方法相比。<details>
<summary>Abstract</summary>
Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional clean holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy-to-implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can achieve similar predictive performance with significantly fewer training iterations than leading data selection methods.
</details>
<details>
<summary>摘要</summary>
错分、重复或偏袋数据在实际场景中可能导致模型训练延长，甚至阻碍模型平衡。传统解决方案会优先级化易于或困难的样本，缺乏适应性来处理多种样本同时。现有研究提出了一种更合理的数据选择原则，通过评估模型通用损失来评估数据的影响。然而，其实践几乎依赖于不原则的近似和额外的干净保留数据。这种方法解决了这些问题，通过利用轻量级权重抽象和大规模预训练模型建立的零shot预测器。该算法是高效和易于实现。我们在具有较大数据噪音和不均衡的在线批处理场景中进行了广泛的实验研究，并观察到了与竞争对手基eline的高效训练性能。尤其是在WebVision标准园 benchmark上，我们的方法可以在与其他数据选择方法相比较同等预测性能的情况下，通过significantly fewer training iterations获得更高的训练效率。
</details></li>
</ul>
<hr>
<h2 id="Learning-Weakly-Convex-Regularizers-for-Convergent-Image-Reconstruction-Algorithms"><a href="#Learning-Weakly-Convex-Regularizers-for-Convergent-Image-Reconstruction-Algorithms" class="headerlink" title="Learning Weakly Convex Regularizers for Convergent Image-Reconstruction Algorithms"></a>Learning Weakly Convex Regularizers for Convergent Image-Reconstruction Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10542">http://arxiv.org/abs/2308.10542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexis Goujon, Sebastian Neumayer, Michael Unser</li>
<li>for: 本研究旨在学习非凸正则化方法，并将其的弱凸性模块规定为上限。这些正则化方法可以用来实现变分降噪器，并且具有少量参数（ fewer than 15,000）和可解释性。</li>
<li>methods: 本研究使用了数字实验来证明这些变分降噪器可以超越凸正则化方法和BM3D降噪器的性能。此外，学习的正则化方法还可以用于解决反射问题，并且可以提供可靠的分布式迭代方法。</li>
<li>results: 数据驱动的实验结果表明，使用学习的非凸正则化方法可以在CT和MRI重建中提供优秀的平衡点，并且在性能、参数数量、保证和可解释性等方面表现出色，在比较其他数据驱动方法时表现更好。<details>
<summary>Abstract</summary>
We propose to learn non-convex regularizers with a prescribed upper bound on their weak-convexity modulus. Such regularizers give rise to variational denoisers that minimize a convex energy. They rely on few parameters (less than 15,000) and offer a signal-processing interpretation as they mimic handcrafted sparsity-promoting regularizers. Through numerical experiments, we show that such denoisers outperform convex-regularization methods as well as the popular BM3D denoiser. Additionally, the learned regularizer can be deployed to solve inverse problems with iterative schemes that provably converge. For both CT and MRI reconstruction, the regularizer generalizes well and offers an excellent tradeoff between performance, number of parameters, guarantees, and interpretability when compared to other data-driven approaches.
</details>
<details>
<summary>摘要</summary>
我们提议使用具有固定上界的弱矩形变数的非凸调教器。这些调教器将实现一个内部平面上的内附式数据过滤，并且具有少于15,000个参数。它们可以视为手工设计的稀疏化调教器，并且通过数据实验显示，这些调教器可以超过对称调教器和BM3D调教器的性能。此外，学习的调教器可以用迭代方案来解决反射问题，并且可以提供可靠的收敛保证。在CT和MRI重建中，调教器具有良好的泛化性和优秀的贡献比例、保证和可读性，在与其他数据驱动方法比较之下表现出色。
</details></li>
</ul>
<hr>
<h2 id="KGrEaT-A-Framework-to-Evaluate-Knowledge-Graphs-via-Downstream-Tasks"><a href="#KGrEaT-A-Framework-to-Evaluate-Knowledge-Graphs-via-Downstream-Tasks" class="headerlink" title="KGrEaT: A Framework to Evaluate Knowledge Graphs via Downstream Tasks"></a>KGrEaT: A Framework to Evaluate Knowledge Graphs via Downstream Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10537">http://arxiv.org/abs/2308.10537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Heist, Sven Hertling, Heiko Paulheim</li>
<li>for: 这种研究旨在创建、扩展或完善知识图，以便创建更大、更正确或更多样化的知识图。</li>
<li>methods: 这些研究通常使用创建或扩展知识图的方法，而不是评估下游任务的性能。</li>
<li>results: KGrEaT 框架可以评估知识图的质量，并对不同的知识图进行比较，以确定它们在实际任务上的表现。<details>
<summary>Abstract</summary>
In recent years, countless research papers have addressed the topics of knowledge graph creation, extension, or completion in order to create knowledge graphs that are larger, more correct, or more diverse. This research is typically motivated by the argumentation that using such enhanced knowledge graphs to solve downstream tasks will improve performance. Nonetheless, this is hardly ever evaluated. Instead, the predominant evaluation metrics - aiming at correctness and completeness - are undoubtedly valuable but fail to capture the complete picture, i.e., how useful the created or enhanced knowledge graph actually is. Further, the accessibility of such a knowledge graph is rarely considered (e.g., whether it contains expressive labels, descriptions, and sufficient context information to link textual mentions to the entities of the knowledge graph). To better judge how well knowledge graphs perform on actual tasks, we present KGrEaT - a framework to estimate the quality of knowledge graphs via actual downstream tasks like classification, clustering, or recommendation. Instead of comparing different methods of processing knowledge graphs with respect to a single task, the purpose of KGrEaT is to compare various knowledge graphs as such by evaluating them on a fixed task setup. The framework takes a knowledge graph as input, automatically maps it to the datasets to be evaluated on, and computes performance metrics for the defined tasks. It is built in a modular way to be easily extendable with additional tasks and datasets.
</details>
<details>
<summary>摘要</summary>
To better evaluate the performance of knowledge graphs on actual tasks, we propose KGrEaT, a framework for estimating the quality of knowledge graphs through actual downstream tasks such as classification, clustering, or recommendation. Instead of comparing different methods of processing knowledge graphs for a single task, KGrEaT compares various knowledge graphs by evaluating them on a fixed task setup. The framework takes a knowledge graph as input, automatically maps it to the datasets to be evaluated on, and computes performance metrics for the defined tasks. It is designed to be easily extendable with additional tasks and datasets.
</details></li>
</ul>
<hr>
<h2 id="DPAN-Dynamic-Preference-based-and-Attribute-aware-Network-for-Relevant-Recommendations"><a href="#DPAN-Dynamic-Preference-based-and-Attribute-aware-Network-for-Relevant-Recommendations" class="headerlink" title="DPAN: Dynamic Preference-based and Attribute-aware Network for Relevant Recommendations"></a>DPAN: Dynamic Preference-based and Attribute-aware Network for Relevant Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10527">http://arxiv.org/abs/2308.10527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Dai, Yingmin Su, Xiaofeng Pan</li>
<li>for: 提高电商平台的相关推荐的Click-Through Rate (CTR)</li>
<li>methods: 提出了一种名为动态偏好和特征意识网络（DPAN）的新方法，通过Attribute-aware Activation Values Generation (AAVG)、Bi-dimensional Compression-based Re-expression (BCR)和Shallow and Deep Union-based Fusion (SDUF)等技术来学习用户的动态偏好和物品信息的细致表示，并Capture users’ dynamic preferences for the diverse degree of recommendation results according to various conditions。</li>
<li>results: 经过大量的Offline experiment和Online A&#x2F;B testing，DPAN已经 demonstrably improved CTR by 7.62%。现在DPAN已经成功部署到了我们的电商平台，并为主要交通的相关推荐提供服务。代码已经公开发布。<details>
<summary>Abstract</summary>
In e-commerce platforms, the relevant recommendation is a unique scenario providing related items for a trigger item that users are interested in. However, users' preferences for the similarity and diversity of recommendation results are dynamic and vary under different conditions. Moreover, individual item-level diversity is too coarse-grained since all recommended items are related to the trigger item. Thus, the two main challenges are to learn fine-grained representations of similarity and diversity and capture users' dynamic preferences for them under different conditions. To address these challenges, we propose a novel method called the Dynamic Preference-based and Attribute-aware Network (DPAN) for predicting Click-Through Rate (CTR) in relevant recommendations. Specifically, based on Attribute-aware Activation Values Generation (AAVG), Bi-dimensional Compression-based Re-expression (BCR) is designed to obtain similarity and diversity representations of user interests and item information. Then Shallow and Deep Union-based Fusion (SDUF) is proposed to capture users' dynamic preferences for the diverse degree of recommendation results according to various conditions. DPAN has demonstrated its effectiveness through extensive offline experiments and online A/B testing, resulting in a significant 7.62% improvement in CTR. Currently, DPAN has been successfully deployed on our e-commerce platform serving the primary traffic for relevant recommendations. The code of DPAN has been made publicly available.
</details>
<details>
<summary>摘要</summary>
在电商平台上，相关推荐的情况是一种特殊的enario，提供用户感兴趣的相关物品的相关物品。然而，用户对相似性和多样性的偏好是动态的，并在不同情况下发生变化。此外，单个物品级别的多样性是太粗糙，所有推荐的物品都与触发项目有关。因此，两大挑战是学习细化的相似性和多样性表示，以及在不同情况下捕捉用户的动态偏好。为解决这些挑战，我们提出了一种新的方法 called Dynamic Preference-based and Attribute-aware Network (DPAN)，用于预测相关推荐的Click-Through Rate (CTR)。具体来说，基于Attribute-aware Activation Values Generation (AAVG)，我们设计了Bi-dimensional Compression-based Re-expression (BCR)，以获取用户兴趣的相似性和多样性表示。然后，我们提出了Shallow and Deep Union-based Fusion (SDUF)，以捕捉用户在不同情况下对多样度推荐结果的动态偏好。DPAN在广泛的Offline实验和Online A/B测试中表现出色，对相关推荐的Click-Through Rate (CTR)进行了7.62%的提升。现在，DPAN已经成功地部署在我们的电商平台上，负责主要的相关推荐任务。我们已经公开发布DPAN的代码。
</details></li>
</ul>
<hr>
<h2 id="Information-Theory-Guided-Heuristic-Progressive-Multi-View-Coding"><a href="#Information-Theory-Guided-Heuristic-Progressive-Multi-View-Coding" class="headerlink" title="Information Theory-Guided Heuristic Progressive Multi-View Coding"></a>Information Theory-Guided Heuristic Progressive Multi-View Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10522">http://arxiv.org/abs/2308.10522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangmeng Li, Hang Gao, Wenwen Qiang, Changwen Zheng</li>
<li>for: 本研究旨在提出一种基于信息理论的多视图学习方法，以捕捉多个视图共享的全面信息。</li>
<li>methods: 该方法基于对各视图之间的对比学习，并采用三级进行可加进程设计：分布层、集合层和实例层。在分布层中，IPMC方法对各视图的分布进行协调，以减少视图特定的噪声。在集合层中，IPMC方法建立自适应对比池，并通过视图筛选器进行适应修改。在实例层中，我们采用设计的统一损失函数来学习表示和减少梯度干扰。</li>
<li>results: 理论和实验研究表明，IPMC方法在比较 estado-of-the-art 方法时具有superiority。<details>
<summary>Abstract</summary>
Multi-view representation learning aims to capture comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning to different views in a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; evenly measuring the similarities between terms might interfere with optimization. Importantly, few works study the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the perspective of information theory and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive architecture, namely Information theory-guided hierarchical Progressive Multi-view Coding (IPMC). In the distribution-tier, IPMC aligns the distribution between views to reduce view-specific noise. In the set-tier, IPMC constructs self-adjusted contrasting pools, which are adaptively modified by a view filter. Lastly, in the instance-tier, we adopt a designed unified loss to learn representations and reduce the gradient interference. Theoretically and empirically, we demonstrate the superiority of IPMC over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>View-specific noise is not filtered in learning view-shared representations.2. The fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated.3. Evenly measuring the similarities between terms might interfere with optimization.To address these limitations, we propose a novel information theoretical framework for generalized multi-view learning. Guided by this framework, we develop a multi-view coding method with a three-tier progressive architecture, called Information theory-guided hierarchical Progressive Multi-view Coding (IPMC).The IPMC method consists of three tiers:1. Distribution-tier: IPMC aligns the distribution between views to reduce view-specific noise.2. Set-tier: IPMC constructs self-adjusted contrasting pools, which are adaptively modified by a view filter.3. Instance-tier: We adopt a designed unified loss to learn representations and reduce the gradient interference.Theoretically and empirically, we demonstrate the superiority of IPMC over state-of-the-art methods.</details></li>
</ol>
<hr>
<h2 id="Performance-Enhancement-Leveraging-Mask-RCNN-on-Bengali-Document-Layout-Analysis"><a href="#Performance-Enhancement-Leveraging-Mask-RCNN-on-Bengali-Document-Layout-Analysis" class="headerlink" title="Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis"></a>Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10511">http://arxiv.org/abs/2308.10511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shrestha Datta, Md Adith Mollah, Raisa Fairooz, Tariful Islam Fahim</li>
<li>for: 这个论文的目的是提高机器理解报告文档，尤其是历史报告文档。</li>
<li>methods: 这个论文使用了文档格式分析（Document Layout Analysis，DLA）技术，将报告文档分解成不同的部分，如段落、图片和表格。此外，它还使用了一种特殊的模型 called Mask R-CNN 来帮助机器理解这些文档。</li>
<li>results: 这个论文在DL Sprint 2.0比赛中达到了一个好的 dice 分数（0.889），但并不是所有情况都是如此。它发现使用英文文档的模型并不适用于孟加拉文档，这说明了每种语言都有其独特的挑战。<details>
<summary>Abstract</summary>
Understanding digital documents is like solving a puzzle, especially historical ones. Document Layout Analysis (DLA) helps with this puzzle by dividing documents into sections like paragraphs, images, and tables. This is crucial for machines to read and understand these documents. In the DL Sprint 2.0 competition, we worked on understanding Bangla documents. We used a dataset called BaDLAD with lots of examples. We trained a special model called Mask R-CNN to help with this understanding. We made this model better by step-by-step hyperparameter tuning, and we achieved a good dice score of 0.889. However, not everything went perfectly. We tried using a model trained for English documents, but it didn't fit well with Bangla. This showed us that each language has its own challenges. Our solution for the DL Sprint 2.0 is publicly available at https://www.kaggle.com/competitions/dlsprint2/discussion/432201 along with notebooks, weights, and inference notebook.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Clustering-Algorithm-to-Organize-Satellite-Hotspot-Data-for-the-Purpose-of-Tracking-Bushfires-Remotely"><a href="#A-Clustering-Algorithm-to-Organize-Satellite-Hotspot-Data-for-the-Purpose-of-Tracking-Bushfires-Remotely" class="headerlink" title="A Clustering Algorithm to Organize Satellite Hotspot Data for the Purpose of Tracking Bushfires Remotely"></a>A Clustering Algorithm to Organize Satellite Hotspot Data for the Purpose of Tracking Bushfires Remotely</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10505">http://arxiv.org/abs/2308.10505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tengmcing/hotspots-clustering-algorithm">https://github.com/tengmcing/hotspots-clustering-algorithm</a></li>
<li>paper_authors: Weihao Li, Emily Dodwell, Dianne Cook</li>
<li>for: 这篇论文旨在提出一种空间时间划分算法和其在R包spotoroo中的实现，以应对澳大利亚2019-2020年夏季的恶劣森林大火。</li>
<li>methods: 该算法受两种现有的空间时间划分算法的影响，并在每个时间间隔期间将点云 spatially划分，同时考虑点云的运动。它还允许根据不同的地点和卫星数据源进行参数调整。</li>
<li>results: 使用澳大利亚维多利亚省的 bushfire 数据示例，该算法可以准确划分空间时间点云，并且可以根据不同的参数进行调整。<details>
<summary>Abstract</summary>
This paper proposes a spatiotemporal clustering algorithm and its implementation in the R package spotoroo. This work is motivated by the catastrophic bushfires in Australia throughout the summer of 2019-2020 and made possible by the availability of satellite hotspot data. The algorithm is inspired by two existing spatiotemporal clustering algorithms but makes enhancements to cluster points spatially in conjunction with their movement across consecutive time periods. It also allows for the adjustment of key parameters, if required, for different locations and satellite data sources. Bushfire data from Victoria, Australia, is used to illustrate the algorithm and its use within the package.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adaptive-Thresholding-Heuristic-for-KPI-Anomaly-Detection"><a href="#Adaptive-Thresholding-Heuristic-for-KPI-Anomaly-Detection" class="headerlink" title="Adaptive Thresholding Heuristic for KPI Anomaly Detection"></a>Adaptive Thresholding Heuristic for KPI Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10504">http://arxiv.org/abs/2308.10504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ebenezer R. H. P. Isaac, Akshat Sharma</li>
<li>For: The paper is written for the purpose of proposing an Adaptive Thresholding Heuristic (ATH) for anomaly detection in time series Key Performance Indicators (KPIs).* Methods: The paper uses a combination of seasonality decomposition and outlier detection techniques to dynamically adjust the detection threshold based on the local properties of the data distribution and adapt to changes in time series patterns.* Results: The paper validates the effectiveness of ATH using experimental results on a labeled KPI anomaly dataset produced by Ericsson, showing that ATH is computationally efficient and flexible with multiple forecasters and outlier detectors.Here’s the simplified Chinese version of the three key points:* For: 这篇论文是为了提出一种适应阈值规则（ATH），用于时间序列指标预测异常检测。* Methods: 论文使用了时间序列分解和异常检测技术，以适应数据分布的本地特性和时间序列模式的变化，动态调整检测阈值。* Results: 论文使用实验结果 validate ATH 的有效性，并表明它可以扩展到多个预测器和异常检测技术，并且可以实现实时异常检测。<details>
<summary>Abstract</summary>
A plethora of outlier detectors have been explored in the time series domain, however, in a business sense, not all outliers are anomalies of interest. Existing anomaly detection solutions are confined to certain outlier detectors limiting their applicability to broader anomaly detection use cases. Network KPIs (Key Performance Indicators) tend to exhibit stochastic behaviour producing statistical outliers, most of which do not adversely affect business operations. Thus, a heuristic is required to capture the business definition of an anomaly for time series KPI. This article proposes an Adaptive Thresholding Heuristic (ATH) to dynamically adjust the detection threshold based on the local properties of the data distribution and adapt to changes in time series patterns. The heuristic derives the threshold based on the expected periodicity and the observed proportion of anomalies minimizing false positives and addressing concept drift. ATH can be used in conjunction with any underlying seasonality decomposition method and an outlier detector that yields an outlier score. This method has been tested on EON1-Cell-U, a labeled KPI anomaly dataset produced by Ericsson, to validate our hypothesis. Experimental results show that ATH is computationally efficient making it scalable for near real time anomaly detection and flexible with multiple forecasters and outlier detectors.
</details>
<details>
<summary>摘要</summary>
“有许多异常检测器在时间序列领域被探索，但在业务上，不 все异常都是产生关注的异常。现有的异常检测解决方案受到一定的限制，因此无法应用于更广泛的异常检测用例。网络 KPI（关键性能指标）通常会表现出统计学异常，大多数这些异常不会影响业务运行。因此，需要一个决策规则来捕捉业务定义的异常。这篇文章提出了一种适应阈值调整规则（ATH），可以基于数据分布的本地特性和时间序列模式来动态调整检测阈值，以避免假阳性和概念退化。ATH可以与任何基础seasonality分解方法和异常检测器结合使用，并且可以在实时异常检测中进行批处理。我们在Ericsson提供的EON1-Cell-U标注异常数据集上进行了试验，实验结果表明，ATH具有计算效率和灵活性，可以在实时异常检测中执行。”
</details></li>
</ul>
<hr>
<h2 id="GradientCoin-A-Peer-to-Peer-Decentralized-Large-Language-Models"><a href="#GradientCoin-A-Peer-to-Peer-Decentralized-Large-Language-Models" class="headerlink" title="GradientCoin: A Peer-to-Peer Decentralized Large Language Models"></a>GradientCoin: A Peer-to-Peer Decentralized Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10502">http://arxiv.org/abs/2308.10502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeqi Gao, Zhao Song, Junze Yin</li>
<li>For: The paper proposes a decentralized large language model (LLM) that operates similarly to a Bitcoin cash system, but it is unlikely to perform better than the standard Bitcoin system in economics.* Methods: The paper uses a purely theoretical design of a decentralized LLM, but implementing such a system might encounter various practical difficulties.* Results: The paper suggests that only two types of people may be interested in setting up a practical system for the decentralized LLM: those who prefer to use a decentralized ChatGPT-like software, and those who believe that the purpose of carbon-based life is to create silicon-based life.<details>
<summary>Abstract</summary>
Since 2008, after the proposal of a Bitcoin electronic cash system, Bitcoin has fundamentally changed the economic system over the last decade. Since 2022, large language models (LLMs) such as GPT have outperformed humans in many real-life tasks. However, these large language models have several practical issues. For example, the model is centralized and controlled by a specific unit. One weakness is that if that unit decides to shut down the model, it cannot be used anymore. The second weakness is the lack of guaranteed discrepancy behind this model, as certain dishonest units may design their own models and feed them unhealthy training data.   In this work, we propose a purely theoretical design of a decentralized LLM that operates similarly to a Bitcoin cash system. However, implementing such a system might encounter various practical difficulties. Furthermore, this new system is unlikely to perform better than the standard Bitcoin system in economics. Therefore, the motivation for designing such a system is limited. It is likely that only two types of people would be interested in setting up a practical system for it:   $\bullet$ Those who prefer to use a decentralized ChatGPT-like software.   $\bullet$ Those who believe that the purpose of carbon-based life is to create silicon-based life, such as Optimus Prime in Transformers.   The reason the second type of people may be interested is that it is possible that one day an AI system like this will awaken and become the next level of intelligence on this planet.
</details>
<details>
<summary>摘要</summary>
自2008年提出比特币电子币系统以来，比特币已经在过去的一个十年内对经济系统进行了深刻的改变。自2022年以来，大型自然语言模型（LLM）如GPT已经在许多实际任务上超越了人类。然而，这些大型语言模型存在多个实用问题。例如，模型是由特定单位控制和中央化的，如果这个单位停止使用模型，它就不能再使用了。第二个弱点是模型的 garantizado desviación缺失，因为恶意单位可能会设计自己的模型并Feed it unhealthy training data。在这种情况下，我们提出了一种理论上的减中心化LLM设计，其运作方式类似于比特币现金系统。然而，实施这种系统可能会遇到各种实际困难。此外，这新的系统不太可能超越标准比特币系统在经济方面的性能。因此，设计这种系统的动机相对有限。只有两种人可能会尝试实现这种系统：‧ Those who prefer to use a decentralized ChatGPT-like software.‧ Those who believe that the purpose of carbon-based life is to create silicon-based life, such as Optimus Prime in Transformers.这种第二种人可能有兴趣的原因是，可能有一天AI系统像这样会醒来，成为地球上下一个水平的智慧。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-of-Delay-Compensated-Backstepping-for-Reaction-Diffusion-PDEs"><a href="#Deep-Learning-of-Delay-Compensated-Backstepping-for-Reaction-Diffusion-PDEs" class="headerlink" title="Deep Learning of Delay-Compensated Backstepping for Reaction-Diffusion PDEs"></a>Deep Learning of Delay-Compensated Backstepping for Reaction-Diffusion PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10501">http://arxiv.org/abs/2308.10501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Wang, Mamadou Diagne, Miroslav Krstić</li>
<li>for: 这篇论文是用于描述深度神经网络（DeepONet）如何用于近似非线性函数-to-函数映射（operator）的研究。</li>
<li>methods: 该论文使用的方法包括深度神经网络（DeepONet）以及backstepping控制方法。</li>
<li>results: 该论文的研究结果表明，使用深度神经网络对多个非线性运算（cascade&#x2F;composition of operators）的近似可以实现稳定控制。<details>
<summary>Abstract</summary>
Deep neural networks that approximate nonlinear function-to-function mappings, i.e., operators, which are called DeepONet, have been demonstrated in recent articles to be capable of encoding entire PDE control methodologies, such as backstepping, so that, for each new functional coefficient of a PDE plant, the backstepping gains are obtained through a simple function evaluation. These initial results have been limited to single PDEs from a given class, approximating the solutions of only single-PDE operators for the gain kernels. In this paper we expand this framework to the approximation of multiple (cascaded) nonlinear operators. Multiple operators arise in the control of PDE systems from distinct PDE classes, such as the system in this paper: a reaction-diffusion plant, which is a parabolic PDE, with input delay, which is a hyperbolic PDE. The DeepONet-approximated nonlinear operator is a cascade/composition of the operators defined by one hyperbolic PDE of the Goursat form and one parabolic PDE on a rectangle, both of which are bilinear in their input functions and not explicitly solvable. For the delay-compensated PDE backstepping controller, which employs the learned control operator, namely, the approximated gain kernel, we guarantee exponential stability in the $L^2$ norm of the plant state and the $H^1$ norm of the input delay state. Simulations illustrate the contributed theory.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DeepONet），用于表示非线性函数-函数映射（operator），已经在 latest articles 中展示了能够编码整个PDE控制方法ologies，例如backstepping，以便为每个新的函数系数的PDE植物获得简单的函数评估。这些初始结果受限于单个PDE的扩展。在这篇文章中，我们将扩展这个框架，以便多个（堆叠）非线性运算的 aproximation。多个运算出现在PDE系统的控制中，例如在这篇文章中所描述的反应扩散植物，这是一个parabolic PDE，带有输入延迟，这是一个拥有Goursat形式的hyperbolic PDE。 DeepONet-approximated nonlinear operator是一个堆叠/组合的操作，其中一个hyperbolic PDE和一个parabolic PDE在一个矩形上都是 bilinear 的输入函数，并不能直接解决。为延迟补偿PDE backstepping控制器，我们提供了对learn control operator，即approximated gain kernel的 guarantee 的稳定性。我们 guarantee 在植物状态的 $L^2$  нор和输入延迟状态的 $H^1$  нор中的稳定性。实验证明了我们的理论。
</details></li>
</ul>
<hr>
<h2 id="Using-Autoencoders-and-AutoDiff-to-Reconstruct-Missing-Variables-in-a-Set-of-Time-Series"><a href="#Using-Autoencoders-and-AutoDiff-to-Reconstruct-Missing-Variables-in-a-Set-of-Time-Series" class="headerlink" title="Using Autoencoders and AutoDiff to Reconstruct Missing Variables in a Set of Time Series"></a>Using Autoencoders and AutoDiff to Reconstruct Missing Variables in a Set of Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10496">http://arxiv.org/abs/2308.10496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Philipp Roche, Oliver Niggemann, Jens Friebe</li>
<li>for: 本研究旨在提供一种能够重构缺失变量的黑盒模型方法，以解决现有黑盒模型方法中的固定输入和输出特征组合限制。</li>
<li>methods: 本研究使用自适应神经网络模型（autoencoder），通过定义缺失变量并对其进行优化，实现不同的输入和输出特征组合。</li>
<li>results: 实验结果表明，当一个变量缺失时，使用本方法可以很好地重构缺失变量，而且可以处理多个缺失变量。<details>
<summary>Abstract</summary>
Existing black box modeling approaches in machine learning suffer from a fixed input and output feature combination. In this paper, a new approach to reconstruct missing variables in a set of time series is presented. An autoencoder is trained as usual with every feature on both sides and the neural network parameters are fixed after this training. Then, the searched variables are defined as missing variables at the autoencoder input and optimized via automatic differentiation. This optimization is performed with respect to the available features loss calculation. With this method, different input and output feature combinations of the trained model can be realized by defining the searched variables as missing variables and reconstructing them. The combination can be changed without training the autoencoder again. The approach is evaluated on the base of a strongly nonlinear electrical component. It is working well for one of four variables missing and generally even for multiple missing variables.
</details>
<details>
<summary>摘要</summary>
现有的黑盒模型方法在机器学习中受到固定输入和输出特征组合的限制。本文提出了一种新的方法来重建时间序列中缺失的变量。在这种方法中，首先训练一个自适应神经网络，然后定义搜索变量为神经网络输入中缺失的变量，通过自动微分优化。这种优化基于可用特征损失计算。通过这种方法，可以实现不同的输入和输出特征组合，而不需要再训练自适应神经网络。这种方法在一种强不连续电子元件上进行了评估，并且在一个变量缺失的情况下工作良好，甚至可以处理多个缺失变量。
</details></li>
</ul>
<hr>
<h2 id="Deciphering-Raw-Data-in-Neuro-Symbolic-Learning-with-Provable-Guarantees"><a href="#Deciphering-Raw-Data-in-Neuro-Symbolic-Learning-with-Provable-Guarantees" class="headerlink" title="Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees"></a>Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10487">http://arxiv.org/abs/2308.10487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lue Tao, Yu-Xuan Huang, Wang-Zhou Dai, Yuan Jiang</li>
<li>for: 这篇论文旨在探讨neuromorphic hybrid系统的学习可能性，以整合机器学习和符号逻辑的优点。</li>
<li>methods: 这篇论文使用了一种新的方法来描述知识库的指导信号，并提出了一个条件来评估知识库是否能够有效地帮助学习。</li>
<li>results: 这篇论文的实验结果显示，许多知识库满足了这个条件，因此能够有效地帮助学习，而其他知识库则无法满足这个条件，这表示可能会出现学习失败的情况。<details>
<summary>Abstract</summary>
Neuro-symbolic hybrid systems are promising for integrating machine learning and symbolic reasoning, where perception models are facilitated with information inferred from a symbolic knowledge base through logical reasoning. Despite empirical evidence showing the ability of hybrid systems to learn accurate perception models, the theoretical understanding of learnability is still lacking. Hence, it remains unclear why a hybrid system succeeds for a specific task and when it may fail given a different knowledge base. In this paper, we introduce a novel way of characterising supervision signals from a knowledge base, and establish a criterion for determining the knowledge's efficacy in facilitating successful learning. This, for the first time, allows us to address the two questions above by inspecting the knowledge base under investigation. Our analysis suggests that many knowledge bases satisfy the criterion, thus enabling effective learning, while some fail to satisfy it, indicating potential failures. Comprehensive experiments confirm the utility of our criterion on benchmark tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Metric-Loss-for-Multimodal-Learning"><a href="#Deep-Metric-Loss-for-Multimodal-Learning" class="headerlink" title="Deep Metric Loss for Multimodal Learning"></a>Deep Metric Loss for Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10486">http://arxiv.org/abs/2308.10486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sehwan Moon, Hyunju Lee</li>
<li>for:  This paper is written for researchers and practitioners in the field of multimodal learning, particularly those interested in developing more effective and efficient multimodal models.</li>
<li>methods:  The paper introduces a novel loss function called \text{MultiModal} loss, which subgroups instances according to their unimodal contributions. This loss function is designed to prevent inefficient learning caused by overfitting and to efficiently optimize multimodal models.</li>
<li>results:  The paper demonstrates improved classification performance on synthetic data and four real multimodal datasets using the proposed \text{MultiModal} loss. Ablation studies verify the effectiveness of the loss, and the paper shows that the loss generates a reliable prediction score for each modality, which is essential for subgrouping.<details>
<summary>Abstract</summary>
Multimodal learning often outperforms its unimodal counterparts by exploiting unimodal contributions and cross-modal interactions. However, focusing only on integrating multimodal features into a unified comprehensive representation overlooks the unimodal characteristics. In real data, the contributions of modalities can vary from instance to instance, and they often reinforce or conflict with each other. In this study, we introduce a novel \text{MultiModal} loss paradigm for multimodal learning, which subgroups instances according to their unimodal contributions. \text{MultiModal} loss can prevent inefficient learning caused by overfitting and efficiently optimize multimodal models. On synthetic data, \text{MultiModal} loss demonstrates improved classification performance by subgrouping difficult instances within certain modalities. On four real multimodal datasets, our loss is empirically shown to improve the performance of recent models. Ablation studies verify the effectiveness of our loss. Additionally, we show that our loss generates a reliable prediction score for each modality, which is essential for subgrouping. Our \text{MultiModal} loss is a novel loss function to subgroup instances according to the contribution of modalities in multimodal learning and is applicable to a variety of multimodal models with unimodal decisions. Our code is available at https://github.com/SehwanMoon/MultiModalLoss.
</details>
<details>
<summary>摘要</summary>
多模态学习通常超越单模态对手，因为它可以利用单模态贡献和跨模态交互。然而，只专注于将多模态特征集成到一个总体表示中，就会忽视单模态特征。在实际数据中，不同模态的贡献可能会从实例到实例不同，而且经常增强或冲突。在这项研究中，我们提出了一种新的多模态损失函数（MultiModal loss），可以将实例 subgrouped 根据它们的单模态贡献。MultiModal损失可以避免过拟合并有效地优化多模态模型。在 sintetic 数据上，我们证明了MultiModal损失可以提高分类性能，并在四个实际多模态数据集上证明了我们的损失。我们的损失函数可以 subgrouping 实例根据不同模态的贡献，并且可以应用于多种多模态模型。我们的代码可以在 <https://github.com/SehwanMoon/MultiModalLoss> 中找到。
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Method-using-Phrase-Mechanism-in-Neural-Machine-Translation"><a href="#An-Effective-Method-using-Phrase-Mechanism-in-Neural-Machine-Translation" class="headerlink" title="An Effective Method using Phrase Mechanism in Neural Machine Translation"></a>An Effective Method using Phrase Mechanism in Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10482">http://arxiv.org/abs/2308.10482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phuongnm94/PhraseTransformer">https://github.com/phuongnm94/PhraseTransformer</a></li>
<li>paper_authors: Phuong Minh Nguyen, Le Minh Nguyen</li>
<li>for: 本研究的目的是提高基eline模型Transformer在构建并行文本翻译系统（NMT）中的性能。</li>
<li>methods: 本研究使用了一种句子机制，PhraseTransformer，来改进基eline模型Transformer。</li>
<li>results: 我们在VLSP 2022大赛的MT数据集上进行了实验，得到了 Vietnamese to Chinese 的 BLEU 分数为 35.3，以及 Chinese to Vietnamese 的 BLEU 分数为 33.2。<details>
<summary>Abstract</summary>
Machine Translation is one of the essential tasks in Natural Language Processing (NLP), which has massive applications in real life as well as contributing to other tasks in the NLP research community. Recently, Transformer -based methods have attracted numerous researchers in this domain and achieved state-of-the-art results in most of the pair languages. In this paper, we report an effective method using a phrase mechanism, PhraseTransformer, to improve the strong baseline model Transformer in constructing a Neural Machine Translation (NMT) system for parallel corpora Vietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022 competition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2 BLEU scores on Chinese to Vietnamese data. Our code is available at https://github.com/phuongnm94/PhraseTransformer.
</details>
<details>
<summary>摘要</summary>
机器翻译是自然语言处理（NLP）中的一项重要任务，它在实际生活中有很大的应用，同时也对其他NLP研究领域的任务产生了重要的贡献。近些年，基于Transformer算法的方法在这个领域中吸引了大量研究人员，并在大多数对应语言的情况下实现了状态的报表结果。在这篇论文中，我们报告了一种使用短语机制，PhraseTransformer，以改进基eline模型Transformer在建立并行 corpora Vietnamese-Chinese 的神经机器翻译（NMT）系统。我们在 VLSP 2022 比赛的MT数据集上进行了实验，实现了对越南语到中文的 BLEU 分数为 35.3，以及对中文到越南语的 BLEU 分数为 33.2。我们的代码可以在 GitHub 上找到：https://github.com/phuongnm94/PhraseTransformer。
</details></li>
</ul>
<hr>
<h2 id="Deep-Semi-supervised-Anomaly-Detection-with-Metapath-based-Context-Knowledge"><a href="#Deep-Semi-supervised-Anomaly-Detection-with-Metapath-based-Context-Knowledge" class="headerlink" title="Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge"></a>Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10918">http://arxiv.org/abs/2308.10918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hwan Kim, Junghoon Kim, Byung Suk Lee, Sungsu Lim</li>
<li>for: 本研究旨在提出一种基于metapath semi-supervised learning的图像异常检测方法，以解决现有方法的局限性。</li>
<li>methods: 本方法基于GCN层，在编码器和解码器中都使用GCN层，以高效地传播Context信息 между异常和正常节点。 métapath基于的上下文信息和特定的异常社区，可以增强学习结构和属性的差异， both globally and locally。</li>
<li>results: 通过对7个真实网络进行了全面的实验，本研究证明了MSAD方法在比 estado-of-the-art技术的比较优于。 这些成果铺垫了未来的研究，关注metapath模式的优化和分析，以进一步提高异常检测的效果在特征化网络上。<details>
<summary>Abstract</summary>
Graph anomaly detection has attracted considerable attention in recent years. This paper introduces a novel approach that leverages metapath-based semi-supervised learning, addressing the limitations of previous methods. We present a new framework, Metapath-based Semi-supervised Anomaly Detection (MSAD), incorporating GCN layers in both the encoder and decoder to efficiently propagate context information between abnormal and normal nodes. The design of metapath-based context information and a specifically crafted anomaly community enhance the process of learning differences in structures and attributes, both globally and locally. Through a comprehensive set of experiments conducted on seven real-world networks, this paper demonstrates the superiority of the MSAD method compared to state-of-the-art techniques. The promising results of this study pave the way for future investigations, focusing on the optimization and analysis of metapath patterns to further enhance the effectiveness of anomaly detection on attributed networks.
</details>
<details>
<summary>摘要</summary>
《图像异常检测在最近几年内吸引了广泛关注。本文介绍一种新的方法，即基于мета路 semi-supervised learning的图像异常检测方法（MSAD），解决前方法的局限性。我们提出了一新框架，包括GCN层在编码器和解码器中，以高效地传播图像异常和正常节点之间的上下文信息。基于ме타路上的上下文信息设计和特制的异常社区，可以增强学习结构和属性之间的差异， both globally and locally。经过对7个真实网络的广泛实验，本文证明MSAD方法比现有技术更高效。这些成果开创了未来的研究，关注于优化和分析méta路模式，以进一步提高图像异常检测的效iveness。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Parameter-Efficient-Fine-Tuning-Techniques-for-Code-Generation-with-Large-Language-Models"><a href="#Exploring-Parameter-Efficient-Fine-Tuning-Techniques-for-Code-Generation-with-Large-Language-Models" class="headerlink" title="Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models"></a>Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10462">http://arxiv.org/abs/2308.10462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, Houari Sahraoui</li>
<li>for: 这 paper 的目的是研究 Parameter-Efficient Fine-Tuning (PEFT) 技术在自动代码生成场景下的影响，以提高 Large Language Models (LLMs) 的表现和可扩展性。</li>
<li>methods: 这 paper 使用了多种 Parameter-Efficient Fine-Tuning (PEFT) 技术，包括 Weight-Decay Fine-Tuning (WDFT)、Adversarial Training (AT) 和 Multi-Task Learning (MTL)，以提高 LLMs 的表现和可扩展性。</li>
<li>results: 这 paper 的实验结果表明，使用 PEFT 技术可以有效地降低 LLMs 的计算成本和提高其表现，特别是在资源受限的情况下。 In-Context Learning (ICL) 方法在某种程度上受到了 PEFT 技术的替代，但 PEFT 技术可以在执行时间上具有更高的灵活性和可扩展性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) possess impressive capabilities to generate meaningful code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. In the perspective of unleashing their full potential, prior work has demonstrated the benefits of fine-tuning the models to task-specific data. However, fine-tuning process demands heavy computational costs and is intractable when resources are scarce, especially for models with billions of parameters. In light of these challenges, previous studies explored In-Context Learning (ICL) as an effective strategy to generate contextually appropriate code without fine-tuning. However, it operates at inference time and does not involve learning task-specific parameters, potentially limiting the model's performance on downstream tasks. In this context, we foresee that Parameter-Efficient Fine-Tuning (PEFT) techniques carry a high potential for efficiently specializing LLMs to task-specific data. In this paper, we deliver a comprehensive study of LLMs with the impact of PEFT techniques under the automated code generation scenario. Our experimental results reveal the superiority and potential of such techniques over ICL on a wide range of LLMs in reducing the computational burden and improving performance. Therefore, the study opens opportunities for broader applications of PEFT in software engineering scenarios.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有吸引人的能力，可以生成 relevante 的代码几何 Natural Language Intent 的零shot 环境下，无需特定的精细调整。在探索这些模型的全面潜力方面，先前的研究显示了对任务特定数据的特定调整可以提供多项优点。然而，调整过程需要巨大的计算成本，尤其是当模型具有十亿个参数时，这使得调整成为不可能的。为了解决这个问题，先前的研究探访了内部学习（ICL）作为一种可能的策略，可以在没有调整的情况下生成适当的代码。然而，ICL 在推论时进行操作，不会学习任务特定的参数，这可能会限制模型在下游任务中的表现。在这个情况下，我们认为 Paramter-Efficient Fine-Tuning（PEFT）技术具有可能的高效特化能力。在这篇文章中，我们进行了 LLMS 的全面研究，以评估 PEFT 技术在自动代码生成 scenarios 中的影响。我们的实验结果显示，PEFT 技术可以对多种 LLMS 提供更高效的特化和改善表现。因此，这些研究开启了软件工程enario 中 PEFT 技术的更广泛应用的可能性。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Local-Steps-Federated-Learning-with-Differential-Privacy-Driven-by-Convergence-Analysis"><a href="#Adaptive-Local-Steps-Federated-Learning-with-Differential-Privacy-Driven-by-Convergence-Analysis" class="headerlink" title="Adaptive Local Steps Federated Learning with Differential Privacy Driven by Convergence Analysis"></a>Adaptive Local Steps Federated Learning with Differential Privacy Driven by Convergence Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10457">http://arxiv.org/abs/2308.10457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinpeng Ling, Jie Fu, Zhili Chen</li>
<li>for: 这个研究是为了研究在资源限制的情况下，如何实现隐私保护和联合学习。</li>
<li>methods: 这个研究使用了分散式机器学习技术和隐私保护技术，具体的是使用了差异攻击和隐私保护加密。</li>
<li>results: 这个研究的实验结果显示，在资源限制的情况下，使用ALS-DPFL算法可以实现隐私保护和联合学习，并且与之前的研究相比，其性能几乎相同。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations without sharing data. However, while FL ensures that the raw data is not directly accessible to external adversaries, adversaries can still obtain some statistical information about the data through differential attacks. Differential Privacy (DP) has been proposed, which adds noise to the model or gradients to prevent adversaries from inferring private information from the transmitted parameters. We reconsider the framework of differential privacy federated learning in resource-constrained scenarios (privacy budget and communication resources). We analyze the convergence of federated learning with differential privacy (DPFL) on resource-constrained scenarios and propose an Adaptive Local Steps Differential Privacy Federated Learning (ALS-DPFL) algorithm. We experiment our algorithm on the FashionMNIST and Cifar-10 datasets and achieve quite good performance relative to previous work.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习技术，允许多个设备或组织共同训练模型，无需共享数据。然而，FL 确保外部敌对者无法直接访问原始数据，但敌对者可以通过差异攻击获取数据的一些统计信息。差异隐私 (DP) 被提出，它在传输参数或梯度中添加噪声，以防止敌对者从 Parameters 中推断私人信息。我们在有限的隐私预算和通信资源的情况下重新考虑了差异隐私联合学习 (DPFL) 框架。我们分析了在有限隐私预算和通信资源的情况下 DPFL 的整合和稳定性，并提出了适应性的本地步骤差异隐私联合学习 (ALS-DPFL) 算法。我们在 FashionMNIST 和 Cifar-10 数据集上实验了我们的算法，并与之前的工作相比获得了很好的性能。
</details></li>
</ul>
<hr>
<h2 id="DOMINO-Domain-aware-Loss-Regularization-for-Deep-Learning-Generalizability"><a href="#DOMINO-Domain-aware-Loss-Regularization-for-Deep-Learning-Generalizability" class="headerlink" title="DOMINO++: Domain-aware Loss Regularization for Deep Learning Generalizability"></a>DOMINO++: Domain-aware Loss Regularization for Deep Learning Generalizability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10453">http://arxiv.org/abs/2308.10453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skylar E. Stolte, Kyle Volle, Aprinda Indahlastari, Alejandro Albizu, Adam J. Woods, Kevin Brink, Matthew Hale, Ruogu Fang</li>
<li>for: 这篇论文主要应用于提高深度学习（DL）模型在不同测试数据上的外部数据（OOD）纵贯性。</li>
<li>methods: 这篇论文提出了DOMINO++方法，它是一种两种引导和动态领域相关调整的损失调整方法，旨在提高DL模型的OOD纵贯性。</li>
<li>results: 实验结果显示，DOMINO++方法在不同类型的OOD数据上具有较高的纵贯性，并且比基eline模型和DOMINO方法更具有可靠性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization poses a serious challenge for modern deep learning (DL). OOD data consists of test data that is significantly different from the model's training data. DL models that perform well on in-domain test data could struggle on OOD data. Overcoming this discrepancy is essential to the reliable deployment of DL. Proper model calibration decreases the number of spurious connections that are made between model features and class outputs. Hence, calibrated DL can improve OOD generalization by only learning features that are truly indicative of the respective classes. Previous work proposed domain-aware model calibration (DOMINO) to improve DL calibration, but it lacks designs for model generalizability to OOD data. In this work, we propose DOMINO++, a dual-guidance and dynamic domain-aware loss regularization focused on OOD generalizability. DOMINO++ integrates expert-guided and data-guided knowledge in its regularization. Unlike DOMINO which imposed a fixed scaling and regularization rate, DOMINO++ designs a dynamic scaling factor and an adaptive regularization rate. Comprehensive evaluations compare DOMINO++ with DOMINO and the baseline model for head tissue segmentation from magnetic resonance images (MRIs) on OOD data. The OOD data consists of synthetic noisy and rotated datasets, as well as real data using a different MRI scanner from a separate site. DOMINO++'s superior performance demonstrates its potential to improve the trustworthy deployment of DL on real clinical data.
</details>
<details>
<summary>摘要</summary>
现代深度学习（DL）面临了对外部数据（OOD）泛化的严重挑战。OOD数据包括测试数据，与DL模型训练数据异常不同。DL模型在域内测试数据上表现良好，但在OOD数据上可能表现不佳。解决这种差异是DL模型的可靠部署的关键。正确地调整DL模型可以减少模型中的偶极连接，从而提高OOD泛化。先前的工作提出了领域相关的模型准确（DOMINO）以提高DL准确性，但它缺乏关注OOD数据的设计。在这种工作中，我们提出了DOMINO++，一种双引导和动态领域相关损失规范，专注于OOD泛化。DOMINO++结合了专家指导和数据指导的知识在其规范中。与DOMINO不同，DOMINO++不同的是动态缩放因子和自适应规则率。我们对DOMINO++与DOMINO和基eline模型进行了广泛的评估，用于头部组织 segmentation from magnetic resonance imaging（MRI）数据上的OOD数据。OOD数据包括synthetic noisy和旋转数据集，以及实际数据使用不同的MRI扫描仪from separate site。DOMINO++的优秀表现表明它在实际临床数据上的可靠部署的潜力。
</details></li>
</ul>
<hr>
<h2 id="PACS-Prediction-and-analysis-of-cancer-subtypes-from-multi-omics-data-based-on-a-multi-head-attention-mechanism-model"><a href="#PACS-Prediction-and-analysis-of-cancer-subtypes-from-multi-omics-data-based-on-a-multi-head-attention-mechanism-model" class="headerlink" title="PACS: Prediction and analysis of cancer subtypes from multi-omics data based on a multi-head attention mechanism model"></a>PACS: Prediction and analysis of cancer subtypes from multi-omics data based on a multi-head attention mechanism model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10917">http://arxiv.org/abs/2308.10917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangrui Pan, Dazheng Liu, Zhichao Feng, Wenjuan Liu, Shaoliang Peng</li>
<li>for: 这个研究旨在精确分类不同抑衰癌种，帮助医生选择最适合的治疗选择，提高治疗结果，并提供更准确的病人存活预测。</li>
<li>methods: 本研究提出了一个监督式多头注意力机制模型（SMA），它可以成功地学习多种资料的全球和地方特征信息。其第二，通过深度融合多头注意力嵌入式的融合模块，扩大模型的参数。</li>
<li>results: 根据广泛的实验验证，SMA模型在实验、单细胞和癌多资料集中的准确分类率最高，比AE、CNN和GNN-based模型高。因此，我们对多资料分析中的注意力方法做出了贡献。<details>
<summary>Abstract</summary>
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omic data and clinical characteristics among different cancer subtypes. Therefore, accurate classification of cancer subtypes can help doctors choose the most appropriate treatment options, improve treatment outcomes, and provide more accurate patient survival predictions. In this study, we propose a supervised multi-head attention mechanism model (SMA) to classify cancer subtypes successfully. The attention mechanism and feature sharing module of the SMA model can successfully learn the global and local feature information of multi-omics data. Second, it enriches the parameters of the model by deeply fusing multi-head attention encoders from Siamese through the fusion module. Validated by extensive experiments, the SMA model achieves the highest accuracy, F1 macroscopic, F1 weighted, and accurate classification of cancer subtypes in simulated, single-cell, and cancer multiomics datasets compared to AE, CNN, and GNN-based models. Therefore, we contribute to future research on multiomics data using our attention-based approach.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由于肿瘤多样性和临床特征之间的高度不同，肿瘤分型中存在显著的差异。因此，正确地分类肿瘤分型可以帮助医生选择最适合的治疗方案，提高治疗效果，并为患者生存预测提供更加准确的信息。在本研究中，我们提出了一种supervised多头注意机制模型（SMA），用于成功地分类肿瘤分型。SMA模型的注意机制和特征共享模块可以成功地学习多Omics数据的全局和本地特征信息。其次，它通过深度融合多头注意Encoder从Siamese中深度融合多头注意Encoder来增强模型的参数。经验证ified by extensive experiments, SMA模型在simulated、单细群和肿瘤多Omics数据集中的准确率、F1宽泛、F1权重和肿瘤分型准确率上达到了AE、CNN和GNN-based模型的最高水平。因此，我们对多Omics数据进行未来研究的贡献。
</details></li>
</ul>
<hr>
<h2 id="CVFC-Attention-Based-Cross-View-Feature-Consistency-for-Weakly-Supervised-Semantic-Segmentation-of-Pathology-Images"><a href="#CVFC-Attention-Based-Cross-View-Feature-Consistency-for-Weakly-Supervised-Semantic-Segmentation-of-Pathology-Images" class="headerlink" title="CVFC: Attention-Based Cross-View Feature Consistency for Weakly Supervised Semantic Segmentation of Pathology Images"></a>CVFC: Attention-Based Cross-View Feature Consistency for Weakly Supervised Semantic Segmentation of Pathology Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10449">http://arxiv.org/abs/2308.10449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangrui Pan, Lian Wang, Zhichao Feng, Liwen Xu, Shaoliang Peng</li>
<li>for:  histopathology image segmentation for cancer diagnosis and prognosis</li>
<li>methods:  attention-based cross-view feature consistency end-to-end pseudo-mask generation framework (CVFC) using Resnet38 and Resnet50, with multi-scale integrated feature map and class activation map (CAM)</li>
<li>results:  outperformed HistoSegNet, SEAM, C-CAM, WSSS-Tissue, and OEEM on WSSS4LUAD dataset, with IoU of 0.7122 and fwIoU of 0.7018<details>
<summary>Abstract</summary>
Histopathology image segmentation is the gold standard for diagnosing cancer, and can indicate cancer prognosis. However, histopathology image segmentation requires high-quality masks, so many studies now use imagelevel labels to achieve pixel-level segmentation to reduce the need for fine-grained annotation. To solve this problem, we propose an attention-based cross-view feature consistency end-to-end pseudo-mask generation framework named CVFC based on the attention mechanism. Specifically, CVFC is a three-branch joint framework composed of two Resnet38 and one Resnet50, and the independent branch multi-scale integrated feature map to generate a class activation map (CAM); in each branch, through down-sampling and The expansion method adjusts the size of the CAM; the middle branch projects the feature matrix to the query and key feature spaces, and generates a feature space perception matrix through the connection layer and inner product to adjust and refine the CAM of each branch; finally, through the feature consistency loss and feature cross loss to optimize the parameters of CVFC in co-training mode. After a large number of experiments, An IoU of 0.7122 and a fwIoU of 0.7018 are obtained on the WSSS4LUAD dataset, which outperforms HistoSegNet, SEAM, C-CAM, WSSS-Tissue, and OEEM, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DySuse-Susceptibility-Estimation-in-Dynamic-Social-Networks"><a href="#DySuse-Susceptibility-Estimation-in-Dynamic-Social-Networks" class="headerlink" title="DySuse: Susceptibility Estimation in Dynamic Social Networks"></a>DySuse: Susceptibility Estimation in Dynamic Social Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10442">http://arxiv.org/abs/2308.10442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingdan Shi, Jingya Zhou, Congcong Zhang</li>
<li>for: 预测社交网络中 influencer 的总影响范围，并且更具吸引力和实用价值的是预测每个用户是否被影响。</li>
<li>methods: 基于动态网络特性和启发性的 embeddings 技术，提出了一种名为 DySuse 的框架，包括结构特征模块、进步机制和自注意力块。</li>
<li>results: 对多种影响扩散模型进行了实验，得到了比现有动态图像模型更高的预测性能。<details>
<summary>Abstract</summary>
Influence estimation aims to predict the total influence spread in social networks and has received surged attention in recent years. Most current studies focus on estimating the total number of influenced users in a social network, and neglect susceptibility estimation that aims to predict the probability of each user being influenced from the individual perspective. As a more fine-grained estimation task, susceptibility estimation is full of attractiveness and practical value. Based on the significance of susceptibility estimation and dynamic properties of social networks, we propose a task, called susceptibility estimation in dynamic social networks, which is even more realistic and valuable in real-world applications. Susceptibility estimation in dynamic networks has yet to be explored so far and is computationally intractable to naively adopt Monte Carlo simulation to obtain the results. To this end, we propose a novel end-to-end framework DySuse based on dynamic graph embedding technology. Specifically, we leverage a structural feature module to independently capture the structural information of influence diffusion on each single graph snapshot. Besides, {we propose the progressive mechanism according to the property of influence diffusion,} to couple the structural and temporal information during diffusion tightly. Moreover, a self-attention block {is designed to} further capture temporal dependency by flexibly weighting historical timestamps. Experimental results show that our framework is superior to the existing dynamic graph embedding models and has satisfactory prediction performance in multiple influence diffusion models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>>社交网络的影响估计目标是预测社交网络中总的影响扩散，在最近几年内受到了广泛关注。大多数当前的研究都是专注于预测社交网络中总的感染用户数量，而忽略了各个用户的感染可能性，即每个用户是否会被感染。作为一个更细化的估计任务，感染可能性估计具有吸引力和实际价值。基于社交网络的动态性和感染的特性，我们提出了一项任务，即动态社交网络中的感染可能性估计，这项任务在实际应用中更加真实和有价值。动态网络中的感染可能性估计还未被探索，而且直接使用 Monte Carlo  simulations 来获取结果是计算上不可能的。为此，我们提出了一个新的框架，即 DySuse，基于动态图像技术。具体来说，我们利用一个结构特征模块来独立地捕捉影响扩散在每个单Graph快照中的结构信息。此外，我们还提出了一种进程机制，根据感染的特性，将结构和时间信息紧密地连接起来。此外，我们还设计了一个自注意机制，以捕捉 diffusion 过程中的时间相关性，并通过灵活地赋予历史时间权重来进行权重补做。实验结果表明，我们的框架在多种感染模型下具有优于现有的动态图像嵌入模型，并且在多种实际应用中具有满意的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Approximately-Equivariant-Graph-Networks"><a href="#Approximately-Equivariant-Graph-Networks" class="headerlink" title="Approximately Equivariant Graph Networks"></a>Approximately Equivariant Graph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10436">http://arxiv.org/abs/2308.10436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nhuang37/approx_equivariant_graph_nets">https://github.com/nhuang37/approx_equivariant_graph_nets</a></li>
<li>paper_authors: Ningyuan Huang, Ron Levie, Soledad Villar</li>
<li>for: 这种研究是为了解决图像填充、交通流量预测和人体姿态估计等问题，通过选择合适的同质群来提高模型的泛化性和稳定性。</li>
<li>methods: 这篇论文使用了图 neural network (GNN) 和自Symmetry group 来解决这些问题，并提出了一种基于同质群的权重补做法来提高模型的泛化性。</li>
<li>results: 实验结果表明，通过选择合适的同质群可以提高模型的泛化性和稳定性，并且可以在不同的问题上达到最佳的泛化性和精度。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance symmetry of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to symmetries of the fixed domain acting on the image signal (sometimes known as active symmetries), whereas in GNNs any permutation acts on both the graph signals and the graph domain (sometimes described as passive symmetries). In this work, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We present a bias-variance formula that quantifies the tradeoff between the loss in expressivity and the gain in the regularity of the learned estimator, depending on the chosen symmetry group. To illustrate our approach, we conduct extensive experiments on image inpainting, traffic flow prediction, and human pose estimation with different choices of symmetries. We show theoretically and empirically that the best generalization performance can be achieved by choosing a suitably larger group than the graph automorphism group, but smaller than the full permutation group.
</details>
<details>
<summary>摘要</summary>
Graph neural networks (GNNs) 通常被描述为对节点重新分配 permutation 对称的。这种 GNNs 的对称性和图像抽象卷积神经网络 (CNNs) 中的翻译对称性有所不同：图像抽象卷积的对称性对应于图像信号中的活动对称性 (sometimes known as active symmetries),而 GNNs 中任意 permutation 都会影响图像信号和图像domain (sometimes described as passive symmetries)。在这个工作中，我们关注 GNNs 中的活动对称性，通过考虑固定图像上支持的信号来进行学习设定。在这种情况下，GNNs 的自然对称性是图像的自动omorphism。由于实际图像往往偏 asymmetric，我们放宽了对称性的定义，通过图像缩放来形式化 Approximate symmetries。我们提出了一个 bias-variance 公式，它量化了在选择的对称群时，损失表达能力和学习器的规范性之间的交易。为了证明我们的方法，我们在图像填充、交通流量预测和人体姿态估计中进行了广泛的实验，并证明了在不同的对称群选择情况下，可以达到最佳的总体化性能。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-Robust-to-Byzantine-Attacks-Achieving-Zero-Optimality-Gap"><a href="#Federated-Learning-Robust-to-Byzantine-Attacks-Achieving-Zero-Optimality-Gap" class="headerlink" title="Federated Learning Robust to Byzantine Attacks: Achieving Zero Optimality Gap"></a>Federated Learning Robust to Byzantine Attacks: Achieving Zero Optimality Gap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10427">http://arxiv.org/abs/2308.10427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Zuo, Rongfei Fan, Han Hu, Ning Zhang, Shimin Gong</li>
<li>for: 这个研究旨在提出一种适应 federated learning (FL) 中抗衡邪恶拜仁攻击的强健聚合方法，能够有效地解决这种攻击。</li>
<li>methods: 在每个用户端，首先更新模型参数通过多个步骤，这些步骤可调整到迭代过程中，然后直接将更新后的模型参数发送到聚合中心。这减少了聚合中心和用户端之间的互动次数，让每个用户可以自主设定训练parameter，并减少了与已有的方法相比的计算负担。在聚合中心，使用几何 médian 合并接收到的各个用户的模型参数。</li>
<li>results: 这个方法可以在邪恶攻击者占 Fraction 不超过半的情况下，实现零优化差和线性参数。 numéríques résultats 显示了这个方法的有效性。<details>
<summary>Abstract</summary>
In this paper, we propose a robust aggregation method for federated learning (FL) that can effectively tackle malicious Byzantine attacks. At each user, model parameter is firstly updated by multiple steps, which is adjustable over iterations, and then pushed to the aggregation center directly. This decreases the number of interactions between the aggregation center and users, allows each user to set training parameter in a flexible way, and reduces computation burden compared with existing works that need to combine multiple historical model parameters. At the aggregation center, geometric median is leveraged to combine the received model parameters from each user. Rigorous proof shows that zero optimality gap is achieved by our proposed method with linear convergence, as long as the fraction of Byzantine attackers is below half. Numerical results verify the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种鲁棒的聚合方法 для联邦学习（FL），可以有效地对恶意的比宗安攻击进行防御。每个用户的模型参数首先通过多个步骤进行更新，可以在迭代中调整，然后直接将更新后的参数发送到聚合中心。这会减少聚合中心和用户之间的交互次数，让每个用户可以自由地设置训练参数，并且相比现有的方法，减少计算卷积的负担。在聚合中心，使用几何中心来合并来自每个用户的接收到的模型参数。严格的证明显示，我们的提议的方法可以在拥有比宗安攻击者少于半数的情况下，实现零优化差和线性快速收敛。numerical Results表明我们的提议的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Adaptive-Embedding-Makes-Vanilla-Transformer-SOTA-for-Traffic-Forecasting"><a href="#Spatio-Temporal-Adaptive-Embedding-Makes-Vanilla-Transformer-SOTA-for-Traffic-Forecasting" class="headerlink" title="Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting"></a>Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10425">http://arxiv.org/abs/2308.10425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xdzhelheim/staeformer">https://github.com/xdzhelheim/staeformer</a></li>
<li>paper_authors: Hangchen Liu, Zheng Dong, Renhe Jiang, Jiewen Deng, Jinliang Deng, Quanjun Chen, Xuan Song</li>
<li>for: 预测交通流量，即使在复杂的交通环境下。</li>
<li>methods: 使用vanilla transformer和spatio-temporal adaptive embedding技术。</li>
<li>results: 在五个真实的交通预测数据集上达到了状态之最的表现。<details>
<summary>Abstract</summary>
With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
</details>
<details>
<summary>摘要</summary>
随着智能交通系统（ITS）的快速发展，准确的交通预测已成为一项关键挑战。关键瓶颈在于捕捉复杂的空间-时间交通模式。在过去几年，许多基于神经网络的复杂架构的方法已经被提出来解决这个问题。然而，网络架构的提高带来的性能提升减少。在本研究中，我们提出了一种新的组件 called spatio-temporal adaptive embedding（STAEformer），它可以在vanilla transformers中实现出色的效果。我们的提posed Spatio-Temporal Adaptive Embedding transformer（STAEformer）在五个真实世界交通预测数据集上实现了状态的最佳性能。进一步的实验表明，spatio-temporal adaptive embedding在交通预测中扮演着关键的作用，可以有效地捕捉交通时序序中的内在空间-时间关系和时间序列信息。
</details></li>
</ul>
<hr>
<h2 id="TokenSplit-Using-Discrete-Speech-Representations-for-Direct-Refined-and-Transcript-Conditioned-Speech-Separation-and-Recognition"><a href="#TokenSplit-Using-Discrete-Speech-Representations-for-Direct-Refined-and-Transcript-Conditioned-Speech-Separation-and-Recognition" class="headerlink" title="TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition"></a>TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10415">http://arxiv.org/abs/2308.10415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hakan Erdogan, Scott Wisdom, Xuankai Chang, Zalán Borsos, Marco Tagliasacchi, Neil Zeghidour, John R. Hershey</li>
<li>for: 这个论文是为了提出一种基于字符序列的语音分离模型，用于分离多个语音源。</li>
<li>methods: 该模型是一种基于Transformer架构的序列-到-序列编码器-解码器模型，通过masking输入来实现多任务同时训练。</li>
<li>results: 使用对象指标和主观MUSHRA听测， authors表明该模型在语音分离方面表现出色，并且可以在不使用文本条件下也 достичь好的效果。  Additionally, the authors also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of their model.<details>
<summary>Abstract</summary>
We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a "refinement" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.
</details>
<details>
<summary>摘要</summary>
我们介绍了TokenSplit，一种基于字符串的语音分离模型。该模型同时进行多个任务的训练：分离每个语音来源，并将语音转化为文本。模型操作于讲解和音频TokenSequence上，通过masking输入来实现多个任务。该模型采用Transformer架构，是一种sequence-to-sequence编码器-解码器模型。我们还介绍了一个"精度"版本的模型，该模型使用传统分离模型生成优化的音频Token。通过对象指标和主观MUSHRA听测，我们证明了我们的模型在分离方面具有出色的表现，无论是否使用讲解conditioning。我们还测量了自动语音识别（ASR）性能，并提供了语音合成示例，以示出该模型的其他应用价值。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Connected-and-Automated-Vehicles-A-Survey-of-Existing-Approaches-and-Challenges"><a href="#Federated-Learning-for-Connected-and-Automated-Vehicles-A-Survey-of-Existing-Approaches-and-Challenges" class="headerlink" title="Federated Learning for Connected and Automated Vehicles: A Survey of Existing Approaches and Challenges"></a>Federated Learning for Connected and Automated Vehicles: A Survey of Existing Approaches and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10407">http://arxiv.org/abs/2308.10407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishnu Pandi Chellapandi, Liangqi Yuan, Christopher G. Brinton, Stanislaw H Zak, Ziran Wang</li>
<li>for: 本文主要针对Connected and Automated Vehicles (CAV) 领域内的机器学习 (ML) 技术，即使在各种驾驶环境中进行学习和控制。</li>
<li>methods: 本文使用了联邦学习 (Federated Learning, FL) 方法，允许多辆车辆协同发展模型，从而拓宽学习环境，提高总性能，同时保护每辆车辆的本地数据隐私和安全。</li>
<li>results: 本文对 FL 在 CAV 领域的应用进行了评论，包括分析中心化和分布式架构的 FL 方法，评估不同数据源、模型和数据安全技术的重要性，以及各种特定应用中的基模型和数据集使用情况。<details>
<summary>Abstract</summary>
Machine learning (ML) is widely used for key tasks in Connected and Automated Vehicles (CAV), including perception, planning, and control. However, its reliance on vehicular data for model training presents significant challenges related to in-vehicle user privacy and communication overhead generated by massive data volumes. Federated learning (FL) is a decentralized ML approach that enables multiple vehicles to collaboratively develop models, broadening learning from various driving environments, enhancing overall performance, and simultaneously securing local vehicle data privacy and security. This survey paper presents a review of the advancements made in the application of FL for CAV (FL4CAV). First, centralized and decentralized frameworks of FL are analyzed, highlighting their key characteristics and methodologies. Second, diverse data sources, models, and data security techniques relevant to FL in CAVs are reviewed, emphasizing their significance in ensuring privacy and confidentiality. Third, specific and important applications of FL are explored, providing insight into the base models and datasets employed for each application. Finally, existing challenges for FL4CAV are listed and potential directions for future work are discussed to further enhance the effectiveness and efficiency of FL in the context of CAV.
</details>
<details>
<summary>摘要</summary>
本文对 FL4CAV 的应用进行了评论，包括中央化和分布式框架的分析，以及相关的数据源、模型和数据安全技术。此外，文章还探讨了 FL4CAV 在不同应用中的特点和挑战。最后，文章列出了现有的挑战和未来工作的可能性，以便进一步提高 FL4CAV 的效率和效果。中央化和分布式框架的分析：中央化框架是一种传统的机器学习方法，它需要所有的数据集中集成到一个中央服务器上，然后进行训练和推理。分布式框架则是一种分布式机器学习方法，它允许多个车辆共同开发模型，从而扩大学习到不同的驾驶环境。数据源、模型和数据安全技术的评论：FL4CAV 的数据源包括车辆的传感器数据、GPS 数据、路况数据等。模型包括深度学习模型、支持向量机模型等。数据安全技术包括数据加密、数据隐私保护等。特定应用的探讨：FL4CAV 可以应用于许多不同的领域，包括自动驾驶、路况预测、车辆追踪等。每个应用都有其特点和挑战。现有的挑战和未来工作的可能性：FL4CAV 面临着许多挑战，包括数据安全性、通信开销、模型质量等。未来工作可能包括提高 FL4CAV 的效率和效果，开发更加智能的车辆系统，以及解决数据隐私和安全性问题等。
</details></li>
</ul>
<hr>
<h2 id="Label-Selection-Approach-to-Learning-from-Crowds"><a href="#Label-Selection-Approach-to-Learning-from-Crowds" class="headerlink" title="Label Selection Approach to Learning from Crowds"></a>Label Selection Approach to Learning from Crowds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10396">http://arxiv.org/abs/2308.10396</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssatsuki/label-selection-layer">https://github.com/ssatsuki/label-selection-layer</a></li>
<li>paper_authors: Kosuke Yoshimura, Hisashi Kashima</li>
<li>for:  This paper is written for the purpose of improving the performance of supervised deep learning models by using crowdsourced labeled data, which is often noisy and contains label noise.</li>
<li>methods: The proposed method, called Label Selection Layer, uses a selector network to determine whether to use a worker’s label for training, and can be applied to almost all variants of supervised learning problems by simply adding a selector network and changing the objective function for existing models.</li>
<li>results: The experimental results show that the performance of the proposed method is almost equivalent to or better than the Crowd Layer, which is one of the state-of-the-art methods for Deep Learning from Crowds, except for the regression problem case.<details>
<summary>Abstract</summary>
Supervised learning, especially supervised deep learning, requires large amounts of labeled data. One approach to collect large amounts of labeled data is by using a crowdsourcing platform where numerous workers perform the annotation tasks. However, the annotation results often contain label noise, as the annotation skills vary depending on the crowd workers and their ability to complete the task correctly. Learning from Crowds is a framework which directly trains the models using noisy labeled data from crowd workers. In this study, we propose a novel Learning from Crowds model, inspired by SelectiveNet proposed for the selective prediction problem. The proposed method called Label Selection Layer trains a prediction model by automatically determining whether to use a worker's label for training using a selector network. A major advantage of the proposed method is that it can be applied to almost all variants of supervised learning problems by simply adding a selector network and changing the objective function for existing models, without explicitly assuming a model of the noise in crowd annotations. The experimental results show that the performance of the proposed method is almost equivalent to or better than the Crowd Layer, which is one of the state-of-the-art methods for Deep Learning from Crowds, except for the regression problem case.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate english text into simplified chinese文本：Supervised learning, especially supervised deep learning, requires large amounts of labeled data. One approach to collect large amounts of labeled data is by using a crowdsourcing platform where numerous workers perform the annotation tasks. However, the annotation results often contain label noise, as the annotation skills vary depending on the crowd workers and their ability to complete the task correctly. Learning from Crowds is a framework which directly trains the models using noisy labeled data from crowd workers. In this study, we propose a novel Learning from Crowds model, inspired by SelectiveNet proposed for the selective prediction problem. The proposed method called Label Selection Layer trains a prediction model by automatically determining whether to use a worker's label for training using a selector network. A major advantage of the proposed method is that it can be applied to almost all variants of supervised learning problems by simply adding a selector network and changing the objective function for existing models, without explicitly assuming a model of the noise in crowd annotations. The experimental results show that the performance of the proposed method is almost equivalent to or better than the Crowd Layer, which is one of the state-of-the-art methods for Deep Learning from Crowds, except for the regression problem case.翻译结果：超级vised learning，特别是深度学习，需要大量标注数据。一种收集大量标注数据的方法是使用一个人工智能平台，让多名工作者完成标注任务。然而，标注结果经常包含标签噪音，因为 annotator的技能因人而异，完成任务正确性不一致。我们提出了一种 Learning from Crowds 框架，直接使用群体标注数据来训练模型。这种方法称为 Label Selection Layer，通过一个选择器网络来自动决定使用工作者的标注来训练预测模型。我们的方法具有一个优势，可以适用于大多数supervised learning问题，只需要添加一个选择器网络，修改现有模型的目标函数，不需要直接假设群体标注中的噪音模型。实验结果表明，我们的方法与 State-of-the-art 方法 Crowd Layer 相当或更好，除了回归问题例外。
</details></li>
</ul>
<hr>
<h2 id="DiffPrep-Differentiable-Data-Preprocessing-Pipeline-Search-for-Learning-over-Tabular-Data"><a href="#DiffPrep-Differentiable-Data-Preprocessing-Pipeline-Search-for-Learning-over-Tabular-Data" class="headerlink" title="DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data"></a>DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10915">http://arxiv.org/abs/2308.10915</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chu-data-lab/diffprep">https://github.com/chu-data-lab/diffprep</a></li>
<li>paper_authors: Peng Li, Zhiyi Chen, Xu Chu, Kexin Rong</li>
<li>for: 提高机器学习模型的性能，自动搜索数据预处理管道。</li>
<li>methods: 使用梯度下降法和一次训练的机器学习模型，将数据预处理管道的搜索转化为连续和可导的问题，以提高搜索效率。</li>
<li>results: 在15个实际世界数据集中，DiffPrep得到了最佳测试精度，并在一些数据集上提高了机器学习模型的测试精度达6.6%。<details>
<summary>Abstract</summary>
Data preprocessing is a crucial step in the machine learning process that transforms raw data into a more usable format for downstream ML models. However, it can be costly and time-consuming, often requiring the expertise of domain experts. Existing automated machine learning (AutoML) frameworks claim to automate data preprocessing. However, they often use a restricted search space of data preprocessing pipelines which limits the potential performance gains, and they are often too slow as they require training the ML model multiple times. In this paper, we propose DiffPrep, a method that can automatically and efficiently search for a data preprocessing pipeline for a given tabular dataset and a differentiable ML model such that the performance of the ML model is maximized. We formalize the problem of data preprocessing pipeline search as a bi-level optimization problem. To solve this problem efficiently, we transform and relax the discrete, non-differential search space into a continuous and differentiable one, which allows us to perform the pipeline search using gradient descent with training the ML model only once. Our experiments show that DiffPrep achieves the best test accuracy on 15 out of the 18 real-world datasets evaluated and improves the model's test accuracy by up to 6.6 percentage points.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换 raw 数据 into 更usable 格式，以便下游机器学习模型使用。然而，这可能是 costly 和时间consuming的，并且frequently 需要域专家的帮助。现有的自动机器学习（AutoML）框架声称可以自动化数据预处理。然而，它们通常使用一个限制的搜索空间，这限制了性能提高的可能性，并且它们经常是slow，因为它们需要训练 ML 模型多次。在这篇论文中，我们提出了 DiffPrep，一种可以自动和高效地搜索一个 tabular 数据集和一个可导 ML 模型的数据预处理管道，以便maximize 模型的性能。我们将数据预处理管道搜索问题形式化为二级优化问题。为了解决这个问题高效，我们将离散、不准确的搜索空间转换为连续和可导的一个，这allowed us 使用梯度下降来搜索管道，只需要训练 ML 模型一次。我们的实验表明，DiffPrep 在 18 个实际世界数据集上测试精度最高，提高模型的测试精度 by up to 6.6 个百分点。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Opinion-Aggregation-–-A-Statistical-Perspective"><a href="#Unsupervised-Opinion-Aggregation-–-A-Statistical-Perspective" class="headerlink" title="Unsupervised Opinion Aggregation – A Statistical Perspective"></a>Unsupervised Opinion Aggregation – A Statistical Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10386">http://arxiv.org/abs/2308.10386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noyan C. Sevuktekin, Andrew C. Singer</li>
<li>for: 这篇论文旨在探讨一种无需知道真实状况的情况下，基于专家意见来评估专家准确性的统计方法。</li>
<li>methods: 该论文提出了一种基于专家意见的准确性评估方法，即measure the competence of each expert by their likeliness to agree with their peers。</li>
<li>results: 论文表明，更可靠的专家更有可能与其他专家一致，并提出了一种无监督的朴素贝叶斯分类器，可以在较大的问题空间达到最优性。<details>
<summary>Abstract</summary>
Complex decision-making systems rarely have direct access to the current state of the world and they instead rely on opinions to form an understanding of what the ground truth could be. Even in problems where experts provide opinions without any intention to manipulate the decision maker, it is challenging to decide which expert's opinion is more reliable -- a challenge that is further amplified when decision-maker has limited, delayed, or no access to the ground truth after the fact. This paper explores a statistical approach to infer the competence of each expert based on their opinions without any need for the ground truth. Echoing the logic behind what is commonly referred to as \textit{the wisdom of crowds}, we propose measuring the competence of each expert by their likeliness to agree with their peers. We further show that the more reliable an expert is the more likely it is that they agree with their peers. We leverage this fact to propose a completely unsupervised version of the na\"{i}ve Bayes classifier and show that the proposed technique is asymptotically optimal for a large class of problems. In addition to aggregating a large block of opinions, we further apply our technique for online opinion aggregation and for decision-making based on a limited the number of opinions.
</details>
<details>
<summary>摘要</summary>
复杂的决策系统通常没有直接访问当前世界的现状，而是基于意见来形成决策者的理解。即使在专家提供意见无恶意欺诈决策者的情况下，决策者难以判断哪个专家的意见更可靠，这个问题在决策者没有或延迟了现实后的真实信息时变得更加复杂。本文探讨一种统计方法来评估每位专家的能力基于他们的意见，不需要真实信息。按照群体智慧的逻辑，我们提出测量每位专家的能力的方法是根据他们与同行的相互一致程度。我们还证明了更可靠的专家更likely会与同行一致。我们利用这一点来提出一种完全不需要监督的na\"{i}ve Bayes分类器，并证明该技术在一类问题上是 asymptotically 优化的。除了聚合大量意见外，我们还应用该技术于在线意见聚合和基于有限数量的意见做出决策。
</details></li>
</ul>
<hr>
<h2 id="Automated-mapping-of-virtual-environments-with-visual-predictive-coding"><a href="#Automated-mapping-of-virtual-environments-with-visual-predictive-coding" class="headerlink" title="Automated mapping of virtual environments with visual predictive coding"></a>Automated mapping of virtual environments with visual predictive coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10913">http://arxiv.org/abs/2308.10913</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Gornet, Matthew Thomson</li>
<li>for: 这个论文的目的是探索人类大脑中的内在地图构建方式，以及如何使用预测编码来实现这一目的。</li>
<li>methods: 这个论文使用了预测编码网络，该网络使用自我注意力来学习从视觉数据中预测下一幅图像。在学习过程中，网络自动地构建了一个内部表示环境的空间地图，该地图可以让Agent在只有视觉信息的情况下准确地确定自己的位置。</li>
<li>results: 研究结果表明，预测编码可以作为一种自然的和通用的神经网络算法，用于构建大脑中的内在地图。这种地图可以自动地扩展到听觉、感觉和语言输入。<details>
<summary>Abstract</summary>
Humans construct internal cognitive maps of their environment directly from sensory inputs without access to a system of explicit coordinates or distance measurements. While machine learning algorithms like SLAM utilize specialized visual inference procedures to identify visual features and construct spatial maps from visual and odometry data, the general nature of cognitive maps in the brain suggests a unified mapping algorithmic strategy that can generalize to auditory, tactile, and linguistic inputs. Here, we demonstrate that predictive coding provides a natural and versatile neural network algorithm for constructing spatial maps using sensory data. We introduce a framework in which an agent navigates a virtual environment while engaging in visual predictive coding using a self-attention-equipped convolutional neural network. While learning a next image prediction task, the agent automatically constructs an internal representation of the environment that quantitatively reflects distances. The internal map enables the agent to pinpoint its location relative to landmarks using only visual information.The predictive coding network generates a vectorized encoding of the environment that supports vector navigation where individual latent space units delineate localized, overlapping neighborhoods in the environment. Broadly, our work introduces predictive coding as a unified algorithmic framework for constructing cognitive maps that can naturally extend to the mapping of auditory, sensorimotor, and linguistic inputs.
</details>
<details>
<summary>摘要</summary>
人类直接从感知输入中构建内部的认知地图，而不需要访问专门的坐标系或距离测量。而机器学习算法如SLAM则利用专门的视觉推理过程来识别视觉特征并从视觉和运动数据中构建空间地图。然而，大脑内部的认知地图的通用性 suggets a unified mapping algorithmic strategy that can generalize to auditory, tactile, and linguistic inputs。在这里，我们展示了预测编码提供了一种自然和灵活的神经网络算法，可以使用感知数据来构建空间地图。我们介绍了一个框架，在该框架中，一个代理人在虚拟环境中导航，同时使用自我注意力抽象 convolutional neural network 进行视觉预测任务。通过学习预测图像任务，代理人自动构建了内部表示环境，该表示环境可以量化地表示距离。内部地图使得代理人可以使用仅视觉信息来定位自己的位置相对于标志。预测编码网络生成了一个vector化的环境编码，该编码支持vector Navigation，其中个别的latent space单位界定了环境中的局部、重叠的区域。总之，我们的工作将预测编码作为一种统一的算法框架，可以自然扩展到对听频、感知动作和语言输入的映射。
</details></li>
</ul>
<hr>
<h2 id="HoSNN-Adversarially-Robust-Homeostatic-Spiking-Neural-Networks-with-Adaptive-Firing-Thresholds"><a href="#HoSNN-Adversarially-Robust-Homeostatic-Spiking-Neural-Networks-with-Adaptive-Firing-Thresholds" class="headerlink" title="HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds"></a>HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10373">http://arxiv.org/abs/2308.10373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hejia Geng, Peng Li<br>for: 这个研究旨在开发一种具有防御性的神经网络模型，以抵抗神经网络对于攻击性输入的脆弱性。methods: 这个研究使用了一种叫做阀值自适应的 neuron 模型，具有自适应的阀值动态调整机制，以减少攻击性输入的传播和保护神经网络的稳定性。results: 研究发现，这种具有防御性的神经网络模型能够在 CIFAR-10 测试集上实现高度的抗攻击性和稳定性，并且在不需要Explicit adversarial training的情况下，具有较高的抗攻击性和稳定性。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) offer promise for efficient and powerful neurally inspired computation. Common to other types of neural networks, however, SNNs face the severe issue of vulnerability to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to develop a bio-inspired solution that counters the susceptibilities of SNNs to adversarial onslaughts. At the heart of our approach is a novel threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model, which we adopt to construct the proposed adversarially robust homeostatic SNN (HoSNN). Distinct from traditional LIF models, our TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, curtailing adversarial noise propagation and safeguarding the robustness of HoSNNs in an unsupervised manner. Theoretical analysis is presented to shed light on the stability and convergence properties of the TA-LIF neurons, underscoring their superior dynamic robustness under input distributional shifts over traditional LIF neurons. Remarkably, without explicit adversarial training, our HoSNNs demonstrate inherent robustness on CIFAR-10, with accuracy improvements to 72.6% and 54.19% against FGSM and PGD attacks, up from 20.97% and 0.6%, respectively. Furthermore, with minimal FGSM adversarial training, our HoSNNs surpass previous models by 29.99% under FGSM and 47.83% under PGD attacks on CIFAR-10. Our findings offer a new perspective on harnessing biological principles for bolstering SNNs adversarial robustness and defense, paving the way to more resilient neuromorphic computing.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）具有高效和强大的神经元灵感计算的承诺。然而，SNN也面临严重的攻击性风险攻击的问题。我们的研究是首先借鉴神经自适应性来开发一种生物发生的解决方案，以抵御SNN对攻击的抵触性。我们的方法的核心是一种新的阈值调整泄漏 integrate-and-fire（TA-LIF）神经元模型，我们采用这种模型来构建我们的提案的对抗性 robust homeostatic SNN（HoSNN）。与传统的LIF模型不同，我们的TA-LIF模型包含一种自我稳定的动态阈值调整机制， thereby curtailing adversarial noise propagation and safeguarding the robustness of HoSNNs in an unsupervised manner. 我们的理论分析表明，TA-LIF neurons具有更高的稳定性和对输入分布的适应性，而不需要显式的对抗训练。在CIFAR-10上，我们的HoSNNs没有接受过对抗训练，具有72.6%和54.19%的准确率提升，分别对抗FGSM和PGD攻击。此外，通过最小化FGSM对抗训练，我们的HoSNNs超越了先前的模型，在FGSM和PGD攻击下的CIFAR-10上具有29.99%和47.83%的提升。我们的发现开 up a new perspective on harnessing biological principles for bolstering SNNs adversarial robustness and defense, paving the way to more resilient neuromorphic computing.
</details></li>
</ul>
<hr>
<h2 id="Developing-a-Machine-Learning-Based-Clinical-Decision-Support-Tool-for-Uterine-Tumor-Imaging"><a href="#Developing-a-Machine-Learning-Based-Clinical-Decision-Support-Tool-for-Uterine-Tumor-Imaging" class="headerlink" title="Developing a Machine Learning-Based Clinical Decision Support Tool for Uterine Tumor Imaging"></a>Developing a Machine Learning-Based Clinical Decision Support Tool for Uterine Tumor Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10372">http://arxiv.org/abs/2308.10372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darryl E. Wright, Adriana V. Gregory, Deema Anaam, Sepideh Yadollahi, Sumana Ramanathan, Kafayat A. Oyemade, Reem Alsibai, Heather Holmes, Harrison Gottlich, Cherie-Akilah G. Browne, Sarah L. Cohen Rassier, Isabel Green, Elizabeth A. Stewart, Hiroaki Takahashi, Bohyun Kim, Shannon Laughlin-Tommaso, Timothy L. Kline</li>
<li>for: 这个研究是为了开发一个自动化的方法来分类uterus和uterine tumor（UT），以及在不同的肿瘤型别之间进行分类。</li>
<li>methods: 这个研究使用了nnU-Net来自动分类uterus和UT，并评估了不同训练集大小的影响。</li>
<li>results: 研究获得了在分类degenerated leiomyoma（LM）和uterine leiomyosarcoma（LMS）之间的F1分数为0.80，并在分类不同的肿瘤型别时获得了F1分数在0.53-0.80之间。<details>
<summary>Abstract</summary>
Uterine leiomyosarcoma (LMS) is a rare but aggressive malignancy. On imaging, it is difficult to differentiate LMS from, for example, degenerated leiomyoma (LM), a prevalent but benign condition. We curated a data set of 115 axial T2-weighted MRI images from 110 patients (mean [range] age=45 [17-81] years) with UTs that included five different tumor types. These data were randomly split stratifying on tumor volume into training (n=85) and test sets (n=30). An independent second reader (reader 2) provided manual segmentations for all test set images. To automate segmentation, we applied nnU-Net and explored the effect of training set size on performance by randomly generating subsets with 25, 45, 65 and 85 training set images. We evaluated the ability of radiomic features to distinguish between types of UT individually and when combined through feature selection and machine learning. Using the entire training set the mean [95% CI] fibroid DSC was measured as 0.87 [0.59-1.00] and the agreement between the two readers was 0.89 [0.77-1.0] on the test set. When classifying degenerated LM from LMS we achieve a test set F1-score of 0.80. Classifying UTs based on radiomic features we identify classifiers achieving F1-scores of 0.53 [0.45, 0.61] and 0.80 [0.80, 0.80] on the test set for the benign versus malignant, and degenerated LM versus LMS tasks. We show that it is possible to develop an automated method for 3D segmentation of the uterus and UT that is close to human-level performance with fewer than 150 annotated images. For distinguishing UT types, while we train models that merit further investigation with additional data, reliable automatic differentiation of UTs remains a challenge.
</details>
<details>
<summary>摘要</summary>
uterine leiomyosarcoma (LMS) 是一种罕见 yet aggressive 的肿瘤。在医学影像上，它与、例如，退化的子宫omyoma (LM) 相似，这是一种常见 yet benign 的疾病。我们筛选了115个轴向T2束重MRI图像，来自110名患者（年龄的 mean [range] = 45 [17-81] 岁），包括五种不同的肿瘤类型。这些数据被随机分割，按照肿瘤体积进行训练（n=85）和测试集（n=30）。一名独立的第二读者（读者2）提供了测试集图像的手动分割。为了自动分割，我们应用了nnU-Net，并通过 randomly generating subsets with 25, 45, 65 and 85 训练集图像来调查训练集大小对性能的影响。我们评估了用于分类不同类型的uterine tumor 的 радиомiros特征的能力，并通过特征选择和机器学习来组合这些特征。使用整个训练集，我们测算了 fibroid DSC 的 mean [95% CI] 为 0.87 [0.59-1.00]，并且测试集上的两个读者之间的一致性为 0.89 [0.77-1.0]。在分类退化LM from LMS 时，我们在测试集上获得了 F1 分数为 0.80。基于 радиомiros特征，我们identified classifiers achieving F1 分数为 0.53 [0.45, 0.61] 和 0.80 [0.80, 0.80] 在测试集上，用于分类 benign versus malignant 和 degenerated LM versus LMS 任务。我们显示，可以通过使用 fewer than 150 annotated images 来开发一种可以与人类水平的自动 segmentation of the uterus and UT 的方法。然而，在分类不同类型的 UT 时，尽管我们训练了一些值得进一步调查的模型，但可靠的自动分类 UT 仍然是一个挑战。
</details></li>
</ul>
<hr>
<h2 id="SE-3-Equivariant-Augmented-Coupling-Flows"><a href="#SE-3-Equivariant-Augmented-Coupling-Flows" class="headerlink" title="SE(3) Equivariant Augmented Coupling Flows"></a>SE(3) Equivariant Augmented Coupling Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10364">http://arxiv.org/abs/2308.10364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lollcat/se3-augmented-coupling-flows">https://github.com/lollcat/se3-augmented-coupling-flows</a></li>
<li>paper_authors: Laurence I. Midgley, Vincent Stimper, Javier Antorán, Emile Mathieu, Bernhard Schölkopf, José Miguel Hernández-Lobato</li>
<li>for: 本文为了提出一种可以保持SE(3)和 permutation equivariant的coupling流程，以便在物理系统中进行 probabilistic modeling。</li>
<li>methods: 本文使用了coordinate splits along additional augmented dimensions，将原始坐标映射到学习的SE(3) invarianton bases中，然后应用标准流转换。</li>
<li>results: 本文的coupling流程可以保持fast sampling和density evaluation，并且可以生成无偏见的期望值。在训练DW4、LJ13和QM9-positional数据集时，本文的coupling流程与equivarian continuous normalizing flows相比，可以快速样本两个数量级。此外，本文还是第一个通过只模型分子的Cartesian坐标位置来学习全 Boltzmann 分布的 Alanine dipeptide。最后，本文示出了其可以使用只有能量函数来 aproximately 样本 DW4 和 LJ13 粒子系统的Boltzmann分布。<details>
<summary>Abstract</summary>
Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13 and QM9-positional datasets, our flow is competitive with equivariant continuous normalizing flows, while allowing sampling two orders of magnitude faster. Moreover, to the best of our knowledge, we are the first to learn the full Boltzmann distribution of alanine dipeptide by only modeling the Cartesian positions of its atoms. Lastly, we demonstrate that our flow can be trained to approximately sample from the Boltzmann distribution of the DW4 and LJ13 particle systems using only their energy functions.
</details>
<details>
<summary>摘要</summary>
通过卷积整合流可以快速采样和评估概率分布，使其成为物理系统的概率模型工具。然而，标准 coupling 架构禁止将 Cartesian 坐标系中的 atoms 映射到 SE(3) 和 permutation 协变的物理系统中。这项工作提议一种 coupling 流，可以保持 SE(3) 和 permutation 协变，通过在附加的扩展维度上进行坐标拆分。在每层中，流量将 atoms 的位置映射到学习的 SE(3) 不变基底中，然后应用标准 flow 变换，如 monotonic rational-quadratic splines，然后返回到原基底。关键的是，我们的流量保持快速采样和概率评估，并可以生成偏向采样来计算对 Target 分布的预期值。当我们在 DW4、LJ13 和 QM9-positional 数据集上训练我们的流量时，它与 equivariant continuous normalizing flows 相当竞争，而且可以快速采样两个数量级。此外，我们是第一个通过只模型 Cartesian 坐标系中 atoms 的概率分布来学习 full Boltzmann 分布的 alanine dipeptide。最后，我们示示了我们的流量可以使用 DW4 和 LJ13 粒子系统的能量函数来约 approximate 其 Boltzmann 分布。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Find-And-Fix-Vulnerable-Software"><a href="#Can-Large-Language-Models-Find-And-Fix-Vulnerable-Software" class="headerlink" title="Can Large Language Models Find And Fix Vulnerable Software?"></a>Can Large Language Models Find And Fix Vulnerable Software?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10345">http://arxiv.org/abs/2308.10345</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Noever</li>
<li>for: 这个研究是用于评估大语言模型（LLMs）在检测软件漏洞方面的能力，特别是OpenAI的GPT-4，并与传统的静态代码分析工具如Snyk和Fortify进行比较。</li>
<li>methods: 这个研究使用了许多Repository，包括NASA和美国国防部的Repository，并使用了129个代码示例 across eight programming languages来测试GPT-4的性能。</li>
<li>results: GPT-4可以检测到 aproximately four times 的漏洞，并且可以提供可行的修复方案，false positive rate很低。Code corrections可以减少漏洞的数量by 90%，但需要增加代码行数by 11%。LLMs还能够自我审查，并提供修复漏洞的建议。<details>
<summary>Abstract</summary>
In this study, we evaluated the capability of Large Language Models (LLMs), particularly OpenAI's GPT-4, in detecting software vulnerabilities, comparing their performance against traditional static code analyzers like Snyk and Fortify. Our analysis covered numerous repositories, including those from NASA and the Department of Defense. GPT-4 identified approximately four times the vulnerabilities than its counterparts. Furthermore, it provided viable fixes for each vulnerability, demonstrating a low rate of false positives. Our tests encompassed 129 code samples across eight programming languages, revealing the highest vulnerabilities in PHP and JavaScript. GPT-4's code corrections led to a 90% reduction in vulnerabilities, requiring only an 11% increase in code lines. A critical insight was LLMs' ability to self-audit, suggesting fixes for their identified vulnerabilities and underscoring their precision. Future research should explore system-level vulnerabilities and integrate multiple static code analyzers for a holistic perspective on LLMs' potential.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们评估了大型自然语言模型（LLM），尤其是OpenAI的GPT-4，在检测软件漏洞方面的能力，并与传统的静态代码分析工具如Snyk和Fortify进行比较。我们的分析覆盖了多个Repository，包括NASA和国防部的Repository。GPT-4在检测漏洞方面表现出了约四倍的能力，并提供了每个漏洞的可行的修复方案，表明了它的假阳性率非常低。我们的测试包括129个代码样本 across eight种编程语言，发现PHP和JavaScript中的漏洞最高。GPT-4的代码更正引起了漏洞的减少率达90%，仅需要11%的代码行数增加。LLMs的自我审查能力为我们提供了一个关键的发现，它们可以自动提供修复方案，并证明了它们的精度。未来的研究应该探索系统级别的漏洞和将多个静态代码分析工具集成到LLMs的潜在力量中。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Empirical-Evaluation-on-Online-Continual-Learning"><a href="#A-Comprehensive-Empirical-Evaluation-on-Online-Continual-Learning" class="headerlink" title="A Comprehensive Empirical Evaluation on Online Continual Learning"></a>A Comprehensive Empirical Evaluation on Online Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10328">http://arxiv.org/abs/2308.10328</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albinsou/ocl_survey">https://github.com/albinsou/ocl_survey</a></li>
<li>paper_authors: Albin Soutif–Cormerais, Antonio Carta, Andrea Cossu, Julio Hurtado, Vincenzo Lomonaco, Joost Van de Weijer, Hamed Hemati</li>
<li>for: 本研究旨在评估在流动数据上进行连续学习的不同方法，以实现更加接近实际学习经验。</li>
<li>methods: 本研究考虑了Literature中的多种方法，包括分类增量学习。</li>
<li>results: 研究发现大多数方法受到稳定性和过拟合问题的影响，但是学习表示的质量与同样计算预算下的随机批处理相当。<details>
<summary>Abstract</summary>
Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the results and basic experience replay, when properly tuned and implemented, is a very strong baseline. We release our modular and extensible codebase at https://github.com/AlbinSou/ocl_survey based on the avalanche framework to reproduce our results and encourage future research.
</details>
<details>
<summary>摘要</summary>
在线连续学习目标是通过直接学习流动数据的方式，与时间变化的分布进行融合，并将最少量的数据存储在流动中。在这个实验性评估中，我们评估了文献中的多种在线连续学习方法。更specifically，我们在图像分类任务中强调类增量设置，learner需要从数据流中逐渐学习新类。我们在Split-CIFAR100和Split-TinyImagenet bencmarks上对这些方法进行了平均精度、忘记、稳定性和表示质量的评估，以评估不同方法的各个方面。我们发现大多数方法受到稳定性和下降问题的影响。然而，学习的表示比i.i.d.培育下相同计算预算下的表示相当。没有一个明确的赢家，基本的经验回快在适度调整和实现下成为了强大的基准。我们在https://github.com/AlbinSou/ocl_survey中发布了我们的模块化和可扩展的代码基础，以便重现我们的结果并促进未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Quantum-State-Tomography-using-Quantum-Machine-Learning"><a href="#Quantum-State-Tomography-using-Quantum-Machine-Learning" class="headerlink" title="Quantum State Tomography using Quantum Machine Learning"></a>Quantum State Tomography using Quantum Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10327">http://arxiv.org/abs/2308.10327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hongyehu/Machine_Learning_Quantum_State_Tomography">https://github.com/hongyehu/Machine_Learning_Quantum_State_Tomography</a></li>
<li>paper_authors: Nouhaila Innan, Owais Ishtiaq Siddiqui, Shivang Arora, Tamojit Ghosh, Yasemin Poyraz Koçak, Dominic Paragas, Abdullah Al Omar Galib, Muhammad Al-Zafar Khan, Mohamed Bennai</li>
<li>for: 这个论文的目的是提出一种基于量子机器学习（QML）技术的量子状态探测（QST）方法，以提高QST的效率。</li>
<li>methods: 这个论文使用了多种类ссиkal和量子方法来进行QST，并评估了不同的QML方法的效果。</li>
<li>results: 研究结果表明，使用QML技术可以significantly reduce the number of measurements required for QST, 并且可以达到高准确率（98%）。<details>
<summary>Abstract</summary>
Quantum State Tomography (QST) is a fundamental technique in Quantum Information Processing (QIP) for reconstructing unknown quantum states. However, the conventional QST methods are limited by the number of measurements required, which makes them impractical for large-scale quantum systems. To overcome this challenge, we propose the integration of Quantum Machine Learning (QML) techniques to enhance the efficiency of QST. In this paper, we conduct a comprehensive investigation into various approaches for QST, encompassing both classical and quantum methodologies; We also implement different QML approaches for QST and demonstrate their effectiveness on various simulated and experimental quantum systems, including multi-qubit networks. Our results show that our QML-based QST approach can achieve high fidelity (98%) with significantly fewer measurements than conventional methods, making it a promising tool for practical QIP applications.
</details>
<details>
<summary>摘要</summary>
量子状态测测（QST）是Quantum Information Processing（QIP）中重要的技术，用于重建未知量子状态。然而，现有的QST方法受限于测量数量的限制，使其对大规模量子系统不实际。为解决这个挑战，我们提议通过量子机器学习（QML）技术来增强QST的效率。在这篇论文中，我们进行了各种QST方法的全面调查，包括класси方法和量子方法；我们还实施了不同的QML方法，并在多个模拟和实验量子系统上进行了测试，包括多元比特网络。我们的结果表明，我们的QML基于QST方法可以达到高准确率（98%），并且使用了许多 fewer measurements than conventional methods，这使其成为实际QIP应用中的一个有前途的工具。
</details></li>
</ul>
<hr>
<h2 id="Homogenising-SoHO-EIT-and-SDO-AIA-171A-Images-A-Deep-Learning-Approach"><a href="#Homogenising-SoHO-EIT-and-SDO-AIA-171A-Images-A-Deep-Learning-Approach" class="headerlink" title="Homogenising SoHO&#x2F;EIT and SDO&#x2F;AIA 171Å$~$ Images: A Deep Learning Approach"></a>Homogenising SoHO&#x2F;EIT and SDO&#x2F;AIA 171Å$~$ Images: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10322">http://arxiv.org/abs/2308.10322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhamoy Chatterjee, Andrés Muñoz-Jaramillo, Maher Dayeh, Hazel M. Bain, Kimberly Moreland</li>
<li>for: 这个研究旨在创建一个具有同一个特征的 EUV 图像数据集，以便进行太阳气候预测任务。</li>
<li>methods: 这个研究使用了 SoHO&#x2F;EIT 和 SDO&#x2F;AIA 171 ~\AA ~的探测数据，通过 ensemble learning 技术将多个探测数据集合并，并使用 Approximate Bayesian Ensembling 方法来生成一个 ensemble 模型，以估计对于不同气候预测任务的 uncertainty。</li>
<li>results: 研究发现， ensemble 模型的 uncertainty 随着训练集大小的增加而降低，而且 ensemble 模型在测试数据中表现出较高的 uncertainty，表明它可以准确地预测不同的气候预测任务。<details>
<summary>Abstract</summary>
Extreme Ultraviolet images of the Sun are becoming an integral part of space weather prediction tasks. However, having different surveys requires the development of instrument-specific prediction algorithms. As an alternative, it is possible to combine multiple surveys to create a homogeneous dataset. In this study, we utilize the temporal overlap of SoHO/EIT and SDO/AIA 171~\AA ~surveys to train an ensemble of deep learning models for creating a single homogeneous survey of EUV images for 2 solar cycles. Prior applications of deep learning have focused on validating the homogeneity of the output while overlooking the systematic estimation of uncertainty. We use an approach called `Approximate Bayesian Ensembling' to generate an ensemble of models whose uncertainty mimics that of a fully Bayesian neural network at a fraction of the cost. We find that ensemble uncertainty goes down as the training set size increases. Additionally, we show that the model ensemble adds immense value to the prediction by showing higher uncertainty in test data that are not well represented in the training data.
</details>
<details>
<summary>摘要</summary>
Extreme Ultraviolet 图像的太阳是天气预测任务中的一个重要组成部分。然而，不同的调查需要开发专门的仪器预测算法。作为一种 alternatives，我们可以将多个调查结合起来创建一个一致的数据集。在这项研究中，我们利用SOHO/EIT和SDO/AIA 171 Å 调查的时间重叠来训练一个深度学习模型的 ensemble，以生成一个包含2个太阳周期的homogeneous EUV图像数据集。先前的深度学习应用都是验证输出的一致性，而忽略了系统性的估计不确定性。我们使用一种名为“ Approximate Bayesian Ensembling”的方法，生成一个ensemble的模型，其不确定性与完全 Bayesian 神经网络相似，但是比其便宜。我们发现，ensemble 不确定性随着训练集大小的增加而下降。此外，我们还显示，模型ensemble 对预测具有很大的价值，可以在测试数据中显示更高的不确定性，这些数据不良 represent 在训练数据中。
</details></li>
</ul>
<hr>
<h2 id="Towards-Sustainable-Development-A-Novel-Integrated-Machine-Learning-Model-for-Holistic-Environmental-Health-Monitoring"><a href="#Towards-Sustainable-Development-A-Novel-Integrated-Machine-Learning-Model-for-Holistic-Environmental-Health-Monitoring" class="headerlink" title="Towards Sustainable Development: A Novel Integrated Machine Learning Model for Holistic Environmental Health Monitoring"></a>Towards Sustainable Development: A Novel Integrated Machine Learning Model for Holistic Environmental Health Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10317">http://arxiv.org/abs/2308.10317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Mazumder, Sarthak Engala, Aditya Nallaparaju</li>
<li>for: 这个研究旨在帮助政府确定 intervención点，改善规划和保护努力，以促进可持续发展。</li>
<li>methods: 这个研究使用机器学习技术来识别环境状况的预测特征，并使用污染物水平和 particulate matter 作为环境状况的指标。</li>
<li>results: 研究发现了连接不良环境 condition 的特征，帮助政府更好地识别 intervención点，并且可以促进可持续发展。<details>
<summary>Abstract</summary>
Urbanization enables economic growth but also harms the environment through degradation. Traditional methods of detecting environmental issues have proven inefficient. Machine learning has emerged as a promising tool for tracking environmental deterioration by identifying key predictive features. Recent research focused on developing a predictive model using pollutant levels and particulate matter as indicators of environmental state in order to outline challenges. Machine learning was employed to identify patterns linking areas with worse conditions. This research aims to assist governments in identifying intervention points, improving planning and conservation efforts, and ultimately contributing to sustainable development.
</details>
<details>
<summary>摘要</summary>
城市化促进经济增长，但也导致环境退化。传统的环境问题检测方法已经证明不够有效。机器学习技术在跟踪环境衰退方面表现出了替代性，通过确定关键预测特征来开发预测模型。最近的研究集中于开发基于污染物水平和悬尘物含量的预测模型，以描述环境状况的挑战。机器学习技术被用来找出与环境状况相关的模式，以帮助政府确定 intervención点，改善规划和保护努力，最终贡献到可持续发展。
</details></li>
</ul>
<hr>
<h2 id="Demystifying-the-Performance-of-Data-Transfers-in-High-Performance-Research-Networks"><a href="#Demystifying-the-Performance-of-Data-Transfers-in-High-Performance-Research-Networks" class="headerlink" title="Demystifying the Performance of Data Transfers in High-Performance Research Networks"></a>Demystifying the Performance of Data Transfers in High-Performance Research Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10312">http://arxiv.org/abs/2308.10312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Saeedizade, Bing Zhang, Engin Arslan</li>
<li>for: 本文旨在提供一个可扩展的监控框架，以便监测高速研究网络中数据传输的性能问题。</li>
<li>methods: 本文使用了一种可扩展的监控框架，可以收集和存储关键性能指标，以便了解数据传输的性能问题。</li>
<li>results: 评估结果显示，提案的框架可以同时监测400个传输并收集一秒精度的性能统计数据，并且可以自动处理收集到的性能指标，并在87-98%的准确率下识别性能异常。<details>
<summary>Abstract</summary>
High-speed research networks are built to meet the ever-increasing needs of data-intensive distributed workflows. However, data transfers in these networks often fail to attain the promised transfer rates for several reasons, including I/O and network interference, server misconfigurations, and network anomalies. Although understanding the root causes of performance issues is critical to mitigating them and increasing the utilization of expensive network infrastructures, there is currently no available mechanism to monitor data transfers in these networks. In this paper, we present a scalable, end-to-end monitoring framework to gather and store key performance metrics for file transfers to shed light on the performance of transfers. The evaluation results show that the proposed framework can monitor up to 400 transfers per host and more than 40, 000 transfers in total while collecting performance statistics at one-second precision. We also introduce a heuristic method to automatically process the gathered performance metrics and identify the root causes of performance anomalies with an F-score of 87 - 98%.
</details>
<details>
<summary>摘要</summary>
高速研究网络是为满足数据敏感分布式工作流程的需求而建立的。然而，在这些网络中的数据传输经常无法实现承诺的传输速率，这可能是因为输入输出和网络干扰、服务器配置错误以及网络异常。虽然了解性能问题的根本原因是解决性能问题的关键，但目前没有可用的监控数据传输的机制。在本文中，我们提出了一个可扩展的终端到终端监控框架，以收集和存储关键性能指标。评估结果显示，我们的框架可以同时监控400个传输任务和总共超过40,000个传输任务，并在1秒精度收集性能统计数据。我们还提出了一种冒险法来自动处理收集到的性能指标，并使用F-score在87-98%的情况下自动确定性能异常的根本原因。
</details></li>
</ul>
<hr>
<h2 id="I-O-Burst-Prediction-for-HPC-Clusters-using-Darshan-Logs"><a href="#I-O-Burst-Prediction-for-HPC-Clusters-using-Darshan-Logs" class="headerlink" title="I&#x2F;O Burst Prediction for HPC Clusters using Darshan Logs"></a>I&#x2F;O Burst Prediction for HPC Clusters using Darshan Logs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10311">http://arxiv.org/abs/2308.10311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Saeedizade, Roya Taheri, Engin Arslan<br>for: 这篇论文的目的是分析大规模高性能计算（HPC）集群内的各种I&#x2F;O模式，以便最小化I&#x2F;O干扰的发生和影响。methods: 论文使用Darshan报告来抽出系统水平的读写I&#x2F;O率，并使用机器学习模型预测系统层级I&#x2F;O填充的发生。results: 结果显示在五分钟间隔内，读写I&#x2F;O率会有明显的颠簸（超过100倍），并且使用机器学习模型可以预测I&#x2F;O填充的发生，精度高于90%（F-1分数）。此外，研究还显示了一个可以根据I&#x2F;O填充预测的内存管理策略，可以降低应用程序的执行时间。<details>
<summary>Abstract</summary>
Understanding cluster-wide I/O patterns of large-scale HPC clusters is essential to minimize the occurrence and impact of I/O interference. Yet, most previous work in this area focused on monitoring and predicting task and node-level I/O burst events. This paper analyzes Darshan reports from three supercomputers to extract system-level read and write I/O rates in five minutes intervals. We observe significant (over 100x) fluctuations in read and write I/O rates in all three clusters. We then train machine learning models to estimate the occurrence of system-level I/O bursts 5 - 120 minutes ahead. Evaluation results show that we can predict I/O bursts with more than 90% accuracy (F-1 score) five minutes ahead and more than 87% accuracy two hours ahead. We also show that the ML models attain more than 70% accuracy when estimating the degree of the I/O burst. We believe that high-accuracy predictions of I/O bursts can be used in multiple ways, such as postponing delay-tolerant I/O operations (e.g., checkpointing), pausing nonessential applications (e.g., file system scrubbers), and devising I/O-aware job scheduling methods. To validate this claim, we simulated a burst-aware job scheduler that can postpone the start time of applications to avoid I/O bursts. We show that the burst-aware job scheduling can lead to an up to 5x decrease in application runtime.
</details>
<details>
<summary>摘要</summary>
理解大规模HPC集群的总体I/O模式是必要的，以避免I/O干扰的发生和影响。然而，大多数前一些研究都集中于监测和预测任务和节点级I/O异常事件。这篇论文使用Darshan报告数据分析了三个超级计算机的系统级读写I/O速率，发现了所有三个集群中的读写I/O速率存在明显的异常波动（大于100倍）。然后，我们使用机器学习模型来预测系统级I/O异常事件的发生5-120分钟前。评估结果表明，我们可以在5分钟前预测I/O异常事件的发生率高于90%（F-1分数），并在2小时前预测精度高于87%。此外，我们发现ML模型在预测I/O异常事件的严重程度时的准确率超过70%。我们认为高精度的I/O异常预测可以用于多种方式，如延迟快速I/O操作（例如检查点）、挂起非关键应用程序（例如文件系统扫描器），并开发I/O意识的任务调度策略。为验证这一点，我们 simulate了一种基于I/O异常预测的任务调度策略，并证明该策略可以减少应用程序执行时间的最大幅度达5倍。
</details></li>
</ul>
<hr>
<h2 id="Co-Evolution-of-Pose-and-Mesh-for-3D-Human-Body-Estimation-from-Video"><a href="#Co-Evolution-of-Pose-and-Mesh-for-3D-Human-Body-Estimation-from-Video" class="headerlink" title="Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video"></a>Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10305">http://arxiv.org/abs/2308.10305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kasvii/pmce">https://github.com/kasvii/pmce</a></li>
<li>paper_authors: Yingxuan You, Hong Liu, Ti Wang, Wenhao Li, Runwei Ding, Xia Li</li>
<li>for: 用于重建3D人体动作 from video</li>
<li>methods: 使用 Pose and Mesh Co-Evolution network (PMCE)，分为两个部分：1) 视频基于3D人体pose estimation，2) 从估计 pose 和时间图像特征中进行预测 mesh  vertices</li>
<li>results: 在三个基准数据集上（3DPW、Human3.6M 和 MPI-INF-3DHP）实现了以前没有达到的高精度和temporal consistency，并且超越了之前的状态 искусственный智能方法。<details>
<summary>Abstract</summary>
Despite significant progress in single image-based 3D human mesh recovery, accurately and smoothly recovering 3D human motion from a video remains challenging. Existing video-based methods generally recover human mesh by estimating the complex pose and shape parameters from coupled image features, whose high complexity and low representation ability often result in inconsistent pose motion and limited shape patterns. To alleviate this issue, we introduce 3D pose as the intermediary and propose a Pose and Mesh Co-Evolution network (PMCE) that decouples this task into two parts: 1) video-based 3D human pose estimation and 2) mesh vertices regression from the estimated 3D pose and temporal image feature. Specifically, we propose a two-stream encoder that estimates mid-frame 3D pose and extracts a temporal image feature from the input image sequence. In addition, we design a co-evolution decoder that performs pose and mesh interactions with the image-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the human body shape. Extensive experiments demonstrate that the proposed PMCE outperforms previous state-of-the-art methods in terms of both per-frame accuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M, and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.
</details>
<details>
<summary>摘要</summary>
尽管单个图像基于的3D人体模lesh recovering已经取得了 significativ progress,但从视频中准确地和平滑地recover 3D人体运动仍然是一个挑战。现有的视频基于方法通常通过估算复杂的姿势和形状参数从相关的图像特征来recover人体模lesh, whose high complexity and low representation ability often result in inconsistent pose motion and limited shape patterns. To address this issue, we introduce 3D pose as the intermediary and propose a Pose and Mesh Co-Evolution network (PMCE) that decouples this task into two parts: 1) video-based 3D human pose estimation and 2) mesh vertices regression from the estimated 3D pose and temporal image feature. Specifically, we propose a two-stream encoder that estimates mid-frame 3D pose and extracts a temporal image feature from the input image sequence. In addition, we design a co-evolution decoder that performs pose and mesh interactions with the image-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit the human body shape. Extensive experiments demonstrate that the proposed PMCE outperforms previous state-of-the-art methods in terms of both per-frame accuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M, and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/21/cs.LG_2023_08_21/" data-id="clluro5k7006dq988ah3p88fx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/22/eess.IV_2023_08_22/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-22 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/21/cs.SD_2023_08_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-21 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
