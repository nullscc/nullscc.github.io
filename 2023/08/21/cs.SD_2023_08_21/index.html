
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-21 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="LibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10682 repo_url: None paper_authors: Joerg Schmalenst">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-21 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/21/cs.SD_2023_08_21/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="LibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10682 repo_url: None paper_authors: Joerg Schmalenst">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-20T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:34.695Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/21/cs.SD_2023_08_21/" class="article-date">
  <time datetime="2023-08-20T16:00:00.000Z" itemprop="datePublished">2023-08-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-21 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="LibriWASN-A-Data-Set-for-Meeting-Separation-Diarization-and-Recognition-with-Asynchronous-Recording-Devices"><a href="#LibriWASN-A-Data-Set-for-Meeting-Separation-Diarization-and-Recognition-with-Asynchronous-Recording-Devices" class="headerlink" title="LibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices"></a>LibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10682">http://arxiv.org/abs/2308.10682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joerg Schmalenstroeer, Tobias Gburrek, Reinhold Haeb-Umbach</li>
<li>for: 这个论文是为测试随机位置的无线听写系统而设计的数据集。</li>
<li>methods: 该数据集使用了九种不同的设备，包括五个智能手机和四个麦克风阵列，共记录了29个通道的数据。</li>
<li>results: 该数据集包含了与LibriCSS相似的LibriSpeech句子播放和会议 overlap的情况，可以用于测试时钟同步算法、会议分离、转录和讲话系统。<details>
<summary>Abstract</summary>
We present LibriWASN, a data set whose design follows closely the LibriCSS meeting recognition data set, with the marked difference that the data is recorded with devices that are randomly positioned on a meeting table and whose sampling clocks are not synchronized. Nine different devices, five smartphones with a single recording channel and four microphone arrays, are used to record a total of 29 channels. Other than that, the data set follows closely the LibriCSS design: the same LibriSpeech sentences are played back from eight loudspeakers arranged around a meeting table and the data is organized in subsets with different percentages of speech overlap. LibriWASN is meant as a test set for clock synchronization algorithms, meeting separation, diarization and transcription systems on ad-hoc wireless acoustic sensor networks. Due to its similarity to LibriCSS, meeting transcription systems developed for the former can readily be tested on LibriWASN. The data set is recorded in two different rooms and is complemented with ground-truth diarization information of who speaks when.
</details>
<details>
<summary>摘要</summary>
我们介绍LibriWASN数据集，其设计与LibriCSS会议认知数据集相似，但唯一不同之处是数据记录设备随机分布在会议表前，并且采样时钟不同步。数据集使用了五款智能手机和四个麦克风组合，共记录29个通道。除此之外，数据集的设计与LibriCSS相同：同样使用LibriSpeech句子在会议表周围的八个扬声器播放，并将数据分成不同的发言重叠百分比。LibriWASN是为无线听取感知网络的时钟同步算法、会议分离、笔记录和转录系统进行测试而设计的。由于与LibriCSS的相似性，可以将为LibriCSS开发的会议转录系统直接应用到LibriWASN上。数据集在两个不同的房间中录制，并且配备了会议发言人员时间ground truth的 диари化信息。
</details></li>
</ul>
<hr>
<h2 id="An-Anchor-Point-Based-Image-Model-for-Room-Impulse-Response-Simulation-with-Directional-Source-Radiation-and-Sensor-Directivity-Patterns"><a href="#An-Anchor-Point-Based-Image-Model-for-Room-Impulse-Response-Simulation-with-Directional-Source-Radiation-and-Sensor-Directivity-Patterns" class="headerlink" title="An Anchor-Point Based Image-Model for Room Impulse Response Simulation with Directional Source Radiation and Sensor Directivity Patterns"></a>An Anchor-Point Based Image-Model for Room Impulse Response Simulation with Directional Source Radiation and Sensor Directivity Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10543">http://arxiv.org/abs/2308.10543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Pan, Lei Zhang, Yilong Lu, Jilu Jin, Lin Qiu, Jingdong Chen, Jacob Benesty</li>
<li>for: 这篇论文的目的是扩展图像模型方法，以便在不同应用中使用。</li>
<li>methods: 该方法使用了 anchor point image model (APIM) 方法，包括源辐射和感知器直达性特征。</li>
<li>results: 该方法可以根据方向函数、分解时延和计算复杂性生成房间响应。可用于多种声学问题，以便模拟房间响应和评估处理算法。<details>
<summary>Abstract</summary>
The image model method has been widely used to simulate room impulse responses and the endeavor to adapt this method to different applications has also piqued great interest over the last few decades. This paper attempts to extend the image model method and develops an anchor-point-image-model (APIM) approach as a solution for simulating impulse responses by including both the source radiation and sensor directivity patterns. To determine the orientations of all the virtual sources, anchor points are introduced to real sources, which subsequently lead to the determination of the orientations of the virtual sources. An algorithm is developed to generate room impulse responses with APIM by taking into account the directional pattern functions, factional time delays, as well as the computational complexity. The developed model and algorithms can be used in various acoustic problems to simulate room acoustics and improve and evaluate processing algorithms.
</details>
<details>
<summary>摘要</summary>
“图像模型方法在过去几十年内已经广泛使用来模拟房间快速响应，而将这方法应用于不同的应用场景也引起了很大的兴趣。本文尝试将图像模型方法扩展，开发出一个图像模型（APIM）方法来模拟快速响应，这个方法包括了源辐射和感应器直径图形的考虑。为了决定所有虚拟源的方向，本文引入了紧缩点，这些紧缩点将导致虚拟源的方向的决定。具有考虑irectional pattern functions、分割时延和计算复杂度的算法是为了生成房间快速响应的APIM模型和算法。这个模型和算法可以在各种音响问题中模拟房间音响，提高和评估处理算法。”
</details></li>
</ul>
<hr>
<h2 id="Implicit-Self-supervised-Language-Representation-for-Spoken-Language-Diarization"><a href="#Implicit-Self-supervised-Language-Representation-for-Spoken-Language-Diarization" class="headerlink" title="Implicit Self-supervised Language Representation for Spoken Language Diarization"></a>Implicit Self-supervised Language Representation for Spoken Language Diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10470">http://arxiv.org/abs/2308.10470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jagabandhu Mishra, S. R. Mahadeva Prasanna<br>for: 这个论文的目的是提出一种基于自适应框架的语音话语分类方法，以便在低资源语言上进行语音分类。methods: 这个论文使用了三种不同的框架，分别是固定分 segmentation、变点基本分 segmentation 和 E2E 框架，并使用 x-vector 作为隐式语言表示。results: 该论文的实验结果表明，使用 x-vector 作为隐式语言表示可以达到同样的表达效果，而且使用 E2E 框架可以达到最佳的隐式语言分类性能（JER &#x3D; 6.38）。然而，在使用实际的 Microsoft CS（MSCS）数据集时，隐式语言分类性能下降至 60.4，主要是因为 MSCS 数据集中次语言的分段持续时间的分布不同于 TTSF-LD 数据集。此外，使用小值 N 可以避免分段膨润，但是同时 x-vector 表示不能够捕捉到语言差异，因为同一个 speaker 在两种语言中说话。因此，该研究提出了一种自适应隐式语言表示，并与 x-vector 表示进行比较。<details>
<summary>Abstract</summary>
In a code-switched (CS) scenario, the use of spoken language diarization (LD) as a pre-possessing system is essential. Further, the use of implicit frameworks is preferable over the explicit framework, as it can be easily adapted to deal with low/zero resource languages. Inspired by speaker diarization (SD) literature, three frameworks based on (1) fixed segmentation, (2) change point-based segmentation and (3) E2E are proposed to perform LD. The initial exploration with synthetic TTSF-LD dataset shows, using x-vector as implicit language representation with appropriate analysis window length ($N$) can able to achieve at per performance with explicit LD. The best implicit LD performance of $6.38$ in terms of Jaccard error rate (JER) is achieved by using the E2E framework. However, considering the E2E framework the performance of implicit LD degrades to $60.4$ while using with practical Microsoft CS (MSCS) dataset. The difference in performance is mostly due to the distributional difference between the monolingual segment duration of secondary language in the MSCS and TTSF-LD datasets. Moreover, to avoid segment smoothing, the smaller duration of the monolingual segment suggests the use of a small value of $N$. At the same time with small $N$, the x-vector representation is unable to capture the required language discrimination due to the acoustic similarity, as the same speaker is speaking both languages. Therefore, to resolve the issue a self-supervised implicit language representation is proposed in this study. In comparison with the x-vector representation, the proposed representation provides a relative improvement of $63.9\%$ and achieved a JER of $21.8$ using the E2E framework.
</details>
<details>
<summary>摘要</summary>
在码换（CS）场景中，使用口语语音分类（LD）作为预处理系统是必备的。此外，使用隐式框架更有利于处理低/零资源语言，因此根据说话者分类（SD）文献，提出了三种基于（1）固定分 segmentation、（2）变点分 segmentation和（3）E2E的框架来实现LD。使用xvector作为隐式语言表示，并选择合适的分析窗口长度($N$)，可以达到与Explicit LD相同的性能。最佳隐式LD性能为6.38个Jaccard错误率（JER）， achievable by using E2E框架。然而，在使用实际的微软CS（MSCS） dataset时，隐式LD性能下降到60.4个JER，主要是由于MSCS dataset中次语言的单语言分段时间的分布不同于TTSF-LD dataset。此外，避免分段平滑的问题，需要选择小的分段时间。同时，使用小的$N$值，xvector表示无法捕捉到必要的语言划分，因为同一个说话者同时说两种语言，导致问题。因此，本研究提出了一种自动学习的隐式语言表示，与xvector表示相比，提供了63.9%的相对改进，并使用E2E框架达到JER21.8。
</details></li>
</ul>
<hr>
<h2 id="Multi-GradSpeech-Towards-Diffusion-based-Multi-Speaker-Text-to-speech-Using-Consistent-Diffusion-Models"><a href="#Multi-GradSpeech-Towards-Diffusion-based-Multi-Speaker-Text-to-speech-Using-Consistent-Diffusion-Models" class="headerlink" title="Multi-GradSpeech: Towards Diffusion-based Multi-Speaker Text-to-speech Using Consistent Diffusion Models"></a>Multi-GradSpeech: Towards Diffusion-based Multi-Speaker Text-to-speech Using Consistent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10428">http://arxiv.org/abs/2308.10428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heyang Xue, Shuai Guo, Pengcheng Zhu, Mengxiao Bi</li>
<li>for: 提高多 speaker Text-to-Speech（TTS）性能</li>
<li>methods: 引入 Consistent Diffusion Model（CDM）生成模型，在训练过程中保证 CDM 的一致性，以避免抽取难点问题</li>
<li>results: 对多 speaker TTS 表现有显著提高，甚至超过了细化调整方法的表现In English:</li>
<li>for: Improving the performance of multi-speaker Text-to-Speech (TTS)</li>
<li>methods: Introducing the Consistent Diffusion Model (CDM) as a generative modeling approach, ensuring the consistency property during training to alleviate the sampling drift problem in the inference stage</li>
<li>results: Significant improvements in multi-speaker TTS performance, outperforming the fine-tuning approach and available audio samples at <a target="_blank" rel="noopener" href="https://welkinyang.github.io/multi-gradspeech/">https://welkinyang.github.io/multi-gradspeech/</a><details>
<summary>Abstract</summary>
Recent advancements in diffusion-based acoustic models have revolutionized data-sufficient single-speaker Text-to-Speech (TTS) approaches, with Grad-TTS being a prime example. However, diffusion models suffer from drift in training and sampling distributions due to imperfect score-matching. The sampling drift problem leads to these approaches struggling in multi-speaker scenarios in practice. In this paper, we present Multi-GradSpeech, a multi-speaker diffusion-based acoustic models which introduces the Consistent Diffusion Model (CDM) as a generative modeling approach. We enforce the consistency property of CDM during the training process to alleviate the sampling drift problem in the inference stage, resulting in significant improvements in multi-speaker TTS performance. Our experimental results corroborate that our proposed approach can improve the performance of different speakers involved in multi-speaker TTS compared to Grad-TTS, even outperforming the fine-tuning approach. Audio samples are available at https://welkinyang.github.io/multi-gradspeech/
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Architectures-Learning-Fourier-Transforms-Signal-Processing-and-Much-More…"><a href="#Neural-Architectures-Learning-Fourier-Transforms-Signal-Processing-and-Much-More…" class="headerlink" title="Neural Architectures Learning Fourier Transforms, Signal Processing and Much More…."></a>Neural Architectures Learning Fourier Transforms, Signal Processing and Much More….</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10388">http://arxiv.org/abs/2308.10388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Verma</li>
<li>For: The paper explores the use of neural architectures for learning kernels in Fourier Transform, specifically for audio signal processing applications.* Methods: The paper uses a neural network to learn sinusoidal kernel shapes and discovers various signal-processing properties such as windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. The neural architecture has a comb filter-like structure on top of the learned kernels.* Results: The paper shows that the neural architecture not only learns sinusoidal kernel shapes but also discovers all kinds of incredible signal-processing properties, such as windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. The learned kernels can be used for a variety of signal processing tasks, and the content of the kernels can be made adaptive to different inputs.<details>
<summary>Abstract</summary>
This report will explore and answer fundamental questions about taking Fourier Transforms and tying it with recent advances in AI and neural architecture. One interpretation of the Fourier Transform is decomposing a signal into its constituent components by projecting them onto complex exponentials. Variants exist, such as discrete cosine transform that does not operate on the complex domain and projects an input signal to only cosine functions oscillating at different frequencies. However, this is a fundamental limitation, and it needs to be more suboptimal. The first one is that all kernels are sinusoidal: What if we could have some kernels adapted or learned according to the problem? What if we can use neural architectures for this? We show how one can learn these kernels from scratch for audio signal processing applications. We find that the neural architecture not only learns sinusoidal kernel shapes but discovers all kinds of incredible signal-processing properties. E.g., windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. Further, upon analysis of the filters, we find that the neural architecture has a comb filter-like structure on top of the learned kernels. Comb filters that allow harmonic frequencies to pass through are one of the core building blocks/types of filters similar to high-pass, low-pass, and band-pass filters of various traditional signal processing algorithms. Further, we can also use the convolution operation with a signal to be learned from scratch, and we will explore papers in the literature that uses this with that robust Transformer architectures. Further, we would also explore making the learned kernel's content adaptive, i.e., learning different kernels for different inputs.
</details>
<details>
<summary>摘要</summary>
traducción al chino simplificado:这份报告将探索并回答关于使用傅立叶变换和最新的人工智能和神经网络架构之间的关系的基本问题。傅立叶变换的一种解释是将信号分解成其成分部分，并将它们 proyect onto 复杂的指数函数。 variants exist， such as the discrete cosine transform that does not operate on the complex domain and projects an input signal to only cosine functions oscillating at different frequencies. However, this is a fundamental limitation, and it needs to be more suboptimal. The first one is that all kernels are sinusoidal: What if we could have some kernels adapted or learned according to the problem? What if we can use neural architectures for this? We show how one can learn these kernels from scratch for audio signal processing applications. We find that the neural architecture not only learns sinusoidal kernel shapes but discovers all kinds of incredible signal-processing properties. E.g., windowing functions, onset detectors, high pass filters, low pass filters, modulations, etc. Further, upon analysis of the filters, we find that the neural architecture has a comb filter-like structure on top of the learned kernels. Comb filters that allow harmonic frequencies to pass through are one of the core building blocks/types of filters similar to high-pass, low-pass, and band-pass filters of various traditional signal processing algorithms. Further, we can also use the convolution operation with a signal to be learned from scratch, and we will explore papers in the literature that uses this with that robust Transformer architectures. Further, we would also explore making the learned kernel's content adaptive, i.e., learning different kernels for different inputs.
</details></li>
</ul>
<hr>
<h2 id="Local-Periodicity-Based-Beat-Tracking-for-Expressive-Classical-Piano-Music"><a href="#Local-Periodicity-Based-Beat-Tracking-for-Expressive-Classical-Piano-Music" class="headerlink" title="Local Periodicity-Based Beat Tracking for Expressive Classical Piano Music"></a>Local Periodicity-Based Beat Tracking for Expressive Classical Piano Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10355">http://arxiv.org/abs/2308.10355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunnycyc/plpdp4beat">https://github.com/sunnycyc/plpdp4beat</a></li>
<li>paper_authors: Ching-Yu Chiu, Meinard Müller, Matthew E. P. Davies, Alvin Wen-Yu Su, Yi-Hsuan Yang</li>
<li>for: 这个论文是为了探讨现代拍谱系统如何模型拍子的征性，以及如何在西 classical 钢琴音乐中提高拍谱的准确性。</li>
<li>methods: 这个论文使用了两个大的西 classical 钢琴音乐数据集， namely Aligned Scores and Performances (ASAP) 数据集和 Chopin 的 Mazurkas (Maz-5) 数据集，并通过实验表明了现有的拍谱系统在地方拍子变化时的缺陷，因此需要新的方法。这个论文提出了一种新的本地征性基于的拍谱系统，即 predominant local pulse-based dynamic programming (PLPDP) 跟踪方法，它可以更好地考虑地方的拍子变化。</li>
<li>results: 相比现有的拍谱系统，PLPDP 方法可以提高 ASAP 数据集中的 F1-score 从 0.473 提高到 0.493，并在 Maz-5 数据集中提高 F1-score 从 0.595 提高到 0.838。<details>
<summary>Abstract</summary>
To model the periodicity of beats, state-of-the-art beat tracking systems use "post-processing trackers" (PPTs) that rely on several empirically determined global assumptions for tempo transition, which work well for music with a steady tempo. For expressive classical music, however, these assumptions can be too rigid. With two large datasets of Western classical piano music, namely the Aligned Scores and Performances (ASAP) dataset and a dataset of Chopin's Mazurkas (Maz-5), we report on experiments showing the failure of existing PPTs to cope with local tempo changes, thus calling for new methods. In this paper, we propose a new local periodicity-based PPT, called predominant local pulse-based dynamic programming (PLPDP) tracking, that allows for more flexible tempo transitions. Specifically, the new PPT incorporates a method called "predominant local pulses" (PLP) in combination with a dynamic programming (DP) component to jointly consider the locally detected periodicity and beat activation strength at each time instant. Accordingly, PLPDP accounts for the local periodicity, rather than relying on a global tempo assumption. Compared to existing PPTs, PLPDP particularly enhances the recall values at the cost of a lower precision, resulting in an overall improvement of F1-score for beat tracking in ASAP (from 0.473 to 0.493) and Maz-5 (from 0.595 to 0.838).
</details>
<details>
<summary>摘要</summary>
<<SYS>>请将以下文本翻译成简化中文。<</SYS>>现代拍谱系统使用“后处理跟踪器”（PPT）来模拟拍征的周期性，这些PPTs基于一些实际所确定的全球假设，以适应音乐中的稳定拍 tempo。然而，对于表达力强的古典音乐来说，这些假设可能是太僵化了。基于西方古典钢琴音乐的两大数据集， namely Aligned Scores and Performances（ASAP）数据集和钢琴 Mazurkas（Maz-5）数据集，我们报告了现有PPTs在本地拍征变化时的失败，从而需要新的方法。在这篇论文中，我们提出了一种新的本地周期性基于的PPT，即主导性本地拍（PLP）基于的动态规划（DP）跟踪器。PLPDP通过同时考虑检测到的本地周期性和拍动强度来联合考虑本地拍征和全球拍征。因此，PLPDP不依赖于全球拍假设，而是根据本地周期性来跟踪拍征。相比现有PPTs，PLPDP尤其提高了ASAP和Maz-5中的回归值（从0.473提高到0.493）和总体的F1分数（从0.595提高到0.838）。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/21/cs.SD_2023_08_21/" data-id="clluro5l0009gq9880fk37a3r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/21/cs.LG_2023_08_21/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-21 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/21/eess.IV_2023_08_21/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-21 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
