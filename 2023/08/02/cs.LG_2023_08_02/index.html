
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-02 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Careful Whisper – leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.01327 repo_url: None paper_author">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-02 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/02/cs.LG_2023_08_02/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Careful Whisper – leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.01327 repo_url: None paper_author">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-01T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:24.284Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/02/cs.LG_2023_08_02/" class="article-date">
  <time datetime="2023-08-01T16:00:00.000Z" itemprop="datePublished">2023-08-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-02 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Careful-Whisper-–-leveraging-advances-in-automatic-speech-recognition-for-robust-and-interpretable-aphasia-subtype-classification"><a href="#Careful-Whisper-–-leveraging-advances-in-automatic-speech-recognition-for-robust-and-interpretable-aphasia-subtype-classification" class="headerlink" title="Careful Whisper – leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification"></a>Careful Whisper – leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01327">http://arxiv.org/abs/2308.01327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurin Wagner, Mario Zusag, Theresa Bloder</li>
<li>for: 这篇论文旨在自动从语音录音中识别speech anomalies，以便评估语音障碍。</li>
<li>methods: 该方法结合了Connectionist Temporal Classification（CTC）和encoder-decoder型自动语音识别模型，生成了丰富的听音和清晰的字幕。然后，通过应用多种自然语言处理方法，提取了字幕中的特征，生成了健康语音的原型。</li>
<li>results: 通过使用这些原型的基本距离度量，可以通过标准机器学习分类器实现人类水平的准确率，并能够正确地分类出常见的语音障碍类型。此外，该管道可以直接应用于其他疾病和语言，表现出抽象找出语音生物标志的可能性。<details>
<summary>Abstract</summary>
This paper presents a fully automated approach for identifying speech anomalies from voice recordings to aid in the assessment of speech impairments. By combining Connectionist Temporal Classification (CTC) and encoder-decoder-based automatic speech recognition models, we generate rich acoustic and clean transcripts. We then apply several natural language processing methods to extract features from these transcripts to produce prototypes of healthy speech. Basic distance measures from these prototypes serve as input features for standard machine learning classifiers, yielding human-level accuracy for the distinction between recordings of people with aphasia and a healthy control group. Furthermore, the most frequently occurring aphasia types can be distinguished with 90% accuracy. The pipeline is directly applicable to other diseases and languages, showing promise for robustly extracting diagnostic speech biomarkers.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种完全自动化的方法，用于从语音录音中识别语音异常，以 помочь评估语音障碍。通过结合连接主义时间分类（CTC）和编解码器基于自动语音识别模型，我们生成了丰富的声学特征和清晰的字幕。然后，我们应用了一些自然语言处理方法，从这些字幕中提取特征，生成健康语音的原型。这些原型的基本距离测量作为输入特征，用标准机器学习分类器进行识别，达到了人类水平的准确率，用于分别记录了人类Language和健康控制组的录音。此外，最常出现的语言障碍类型也可以达到90%的准确率。这个管道直接适用于其他疾病和语言，表现出抗颤异常抽取 диагностических语音标记的可能性。
</details></li>
</ul>
<hr>
<h2 id="Do-Multilingual-Language-Models-Think-Better-in-English"><a href="#Do-Multilingual-Language-Models-Think-Better-in-English" class="headerlink" title="Do Multilingual Language Models Think Better in English?"></a>Do Multilingual Language Models Think Better in English?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01223">http://arxiv.org/abs/2308.01223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juletx/self-translate">https://github.com/juletx/self-translate</a></li>
<li>paper_authors: Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de Lacalle, Mikel Artetxe</li>
<li>for: 提高多语言模型的性能</li>
<li>methods: 利用多语言模型内置的几个shot翻译能力</li>
<li>results: 自翻译方法比直接推理更高效，证明语言模型在非英语语言下的潜在多语言能力未能得到充分利用。<details>
<summary>Abstract</summary>
Translate-test is a popular technique to improve the performance of multilingual language models. This approach works by translating the input into English using an external machine translation system, and running inference over the translated input. However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model. In this work, we introduce a new approach called self-translate, which overcomes the need of an external translation system by leveraging the few-shot translation capabilities of multilingual language models. Experiments over 5 tasks show that self-translate consistently outperforms direct inference, demonstrating that language models are unable to leverage their full multilingual potential when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.
</details>
<details>
<summary>摘要</summary>
自 traducción-test 是一种受欢迎的技术，用于提高多语言语音模型的性能。这种方法工作如下：将输入翻译成英语使用外部机器翻译系统，然后运行推理过程。然而，这些改进可以归因于使用外部翻译系统，这些系统通常是基于大量并行数据，这些数据不是语言模型所见的。在这种工作中，我们介绍了一种新的方法，即自翻译，可以超越外部翻译系统的需求。我们的实验结果表明，自翻译在 5 个任务上一直表现出色，常常超过直接推理，这表明了语言模型在非英语语言下的推理能力尚未得到完全利用。我们的代码可以在 GitHub 上找到：https://github.com/juletx/self-translate。
</details></li>
</ul>
<hr>
<h2 id="Calibration-in-Deep-Learning-A-Survey-of-the-State-of-the-Art"><a href="#Calibration-in-Deep-Learning-A-Survey-of-the-State-of-the-Art" class="headerlink" title="Calibration in Deep Learning: A Survey of the State-of-the-Art"></a>Calibration in Deep Learning: A Survey of the State-of-the-Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01222">http://arxiv.org/abs/2308.01222</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wang</li>
<li>for: 这篇论文主要探讨了深度神经网络模型的准确性和可靠性问题，以及如何使用不同机制来进行模型准确性的调整。</li>
<li>methods: 本文主要介绍了现有的模型准确性调整方法，可以分为四个类别：后期准确化、regularization方法、uncertainty估计和组合方法。</li>
<li>results: 本文提供了一个系统性的对现有准确性调整方法的介绍，并讨论了一些最新的进展和挑战在大型模型（特别是大语言模型）的准确性调整方面。<details>
<summary>Abstract</summary>
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibration methods that we roughly classified into four categories: post-hoc calibration, regularization methods, uncertainty estimation, and composition methods. We also covered some recent advancements in calibrating large models, particularly large language models (LLMs). Finally, we discuss some open issues, challenges, and potential directions.
</details>
<details>
<summary>摘要</summary>
<<SYS>Translate into Simplified Chinese.</SYS>>深度神经网络的准确性评估在安全关键应用中扮演着重要的角色。现代神经网络具有高度预测能力，但是它们的准确性受到诸如模型误差等因素的影响。深度学习模型在不同的benchmark上实现了卓越的表现，但是对模型准确性和可靠性的研究还是相对不充分。理想的深度模型应该具有高度预测性能和准确性。在这篇评论中，我们将对当前领域的state-of-the-art准确方法进行回顾，并提供这些方法的原理的理解。首先，我们将定义模型准确性的定义，并解释模型误差的根本原因。然后我们会介绍用于评估这个方面的关键度量。接着，我们将概括一些准确方法，分为四类：后期准确、正则化方法、uncertainty估计和组合方法。我们还讲解了一些在准确大模型中的最新进展，特别是大语言模型（LLMs）。最后，我们讨论了一些开放的问题、挑战和可能的方向。
</details></li>
</ul>
<hr>
<h2 id="Using-ScrutinAI-for-Visual-Inspection-of-DNN-Performance-in-a-Medical-Use-Case"><a href="#Using-ScrutinAI-for-Visual-Inspection-of-DNN-Performance-in-a-Medical-Use-Case" class="headerlink" title="Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case"></a>Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01220">http://arxiv.org/abs/2308.01220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebekka Görge, Elena Haedecke, Michael Mock</li>
<li>for: 这篇论文旨在提高人工分析者通过可视化工具ScrutinAI来调查模型性能和数据集。</li>
<li>methods: 该论文使用了ScrutinAI工具来分析不同专家对数据集的标签变化对模型性能的影响。</li>
<li>results: 该研究发现了模型性能受标签质量变化的影响，并通过Root Cause Analysis分析了DNN模型中真正的弱点。<details>
<summary>Abstract</summary>
Our Visual Analytics (VA) tool ScrutinAI supports human analysts to investigate interactively model performanceand data sets. Model performance depends on labeling quality to a large extent. In particular in medical settings, generation of high quality labels requires in depth expert knowledge and is very costly. Often, data sets are labeled by collecting opinions of groups of experts. We use our VA tool to analyse the influence of label variations between different experts on the model performance. ScrutinAI facilitates to perform a root cause analysis that distinguishes weaknesses of deep neural network (DNN) models caused by varying or missing labeling quality from true weaknesses. We scrutinize the overall detection of intracranial hemorrhages and the more subtle differentiation between subtypes in a publicly available data set.
</details>
<details>
<summary>摘要</summary>
我们的视觉分析工具ScrutinAI可以帮助人类分析员 Investigate model性能和数据集的交互方式。模型性能受标签质量影响很大。尤其在医疗设置下，生成高质量标签需要深厚的专家知识和很昂贵。经常，数据集被由多个专家的意见收集标签。我们使用ScrutinAI来分析标签变化 между不同专家对模型性能的影响。ScrutinAI可以进行根本原因分析，从而分化true weaknesses和变化或缺失标签质量导致的深度神经网络模型的弱点。我们在一个公共可用数据集中分析了脑内出血的总检测和更加细致的类型差异。
</details></li>
</ul>
<hr>
<h2 id="Global-Hierarchical-Neural-Networks-using-Hierarchical-Softmax"><a href="#Global-Hierarchical-Neural-Networks-using-Hierarchical-Softmax" class="headerlink" title="Global Hierarchical Neural Networks using Hierarchical Softmax"></a>Global Hierarchical Neural Networks using Hierarchical Softmax</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01210">http://arxiv.org/abs/2308.01210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jschuurmans/hsoftmax">https://github.com/jschuurmans/hsoftmax</a></li>
<li>paper_authors: Jetze Schuurmans, Flavius Frasincar</li>
<li>for: 这 paper 提出了一种基于层次softmax的全球分类器框架，适用于任何具有自然层次结构的分类任务。</li>
<li>methods: 该方法使用层次softmax创建全球分类器，并在四个文本分类数据集上进行了实验，并在所有数据集中超过了常见的平铺softmax在macro-F1和macro-recall上的表现。</li>
<li>results: 在三个数据集中，层次softmax达到了更高的微精度和macro-精度。<details>
<summary>Abstract</summary>
This paper presents a framework in which hierarchical softmax is used to create a global hierarchical classifier. The approach is applicable for any classification task where there is a natural hierarchy among classes. We show empirical results on four text classification datasets. In all datasets the hierarchical softmax improved on the regular softmax used in a flat classifier in terms of macro-F1 and macro-recall. In three out of four datasets hierarchical softmax achieved a higher micro-accuracy and macro-precision.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Noisy-Label-Learning-by-Implicit-Dicriminative-Approximation-with-Partial-Label-Prior"><a href="#Generative-Noisy-Label-Learning-by-Implicit-Dicriminative-Approximation-with-Partial-Label-Prior" class="headerlink" title="Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior"></a>Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01184">http://arxiv.org/abs/2308.01184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengbei Liu, Yuanhong Chen, Chong Wang, Yuyuan Liu, Gustavo Carneiro</li>
<li>for: 本 paper 是Addressing Learning with Noisy Labels 的一种新方法，它提供了一种新的生成模型优化方法，以及一种新的有用的标签假设。</li>
<li>methods: 本 paper 使用了一种新的生成模型，其中使用了一种基于推理的权重学习方法，以及一种基于partial label learning的标签假设。</li>
<li>results: 本 paper 的实验结果表明，使用该新方法可以在不同的噪音标签 benchmark 上达到state-of-the-art的结果，而且与传统的推理模型相比，该方法具有类似的计算复杂度。<details>
<summary>Abstract</summary>
The learning with noisy labels has been addressed with both discriminative and generative models. Although discriminative models have dominated the field due to their simpler modeling and more efficient computational training processes, generative models offer a more effective means of disentangling clean and noisy labels and improving the estimation of the label transition matrix. However, generative approaches maximize the joint likelihood of noisy labels and data using a complex formulation that only indirectly optimizes the model of interest associating data and clean labels. Additionally, these approaches rely on generative models that are challenging to train and tend to use uninformative clean label priors. In this paper, we propose a new generative noisy-label learning approach that addresses these three issues. First, we propose a new model optimisation that directly associates data and clean labels. Second, the generative model is implicitly estimated using a discriminative model, eliminating the inefficient training of a generative model. Third, we propose a new informative label prior inspired by partial label learning as supervision signal for noisy label learning. Extensive experiments on several noisy-label benchmarks demonstrate that our generative model provides state-of-the-art results while maintaining a similar computational complexity as discriminative models.
</details>
<details>
<summary>摘要</summary>
学习含杂标签问题已经由权重分配和生成模型来解决。虽然权重分配模型在场景中占据主导地位，因为它们的模型化更加简单，并且在训练过程中更加高效，但生成模型可以更好地分离干净标签和含杂标签，并改善标签过渡矩阵的估计。然而，生成方法需要使用复杂的形式来最大化含杂标签和数据的联合概率函数，这会间接地优化模型的关注点。此外，这些方法通常需要训练困难的生成模型，并使用无用的干净标签辅助信号。在这篇论文中，我们提出了一种新的生成含杂标签学习方法，解决了这三个问题。首先，我们提出了一种直接关联数据和干净标签的模型优化方法。其次，我们使用权重分配模型来隐式地估计生成模型，从而消除了生成模型的不必要培训。最后，我们提出了一种基于部分标签学习的新备注标签监督信号。我们在多个含杂标签 benchmark 进行了广泛的实验，结果显示，我们的生成模型可以在计算复杂性方面与权重分配模型相当，同时提供了最佳的结果。
</details></li>
</ul>
<hr>
<h2 id="Direct-Gradient-Temporal-Difference-Learning"><a href="#Direct-Gradient-Temporal-Difference-Learning" class="headerlink" title="Direct Gradient Temporal Difference Learning"></a>Direct Gradient Temporal Difference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01170">http://arxiv.org/abs/2308.01170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochi Qian, Shangtong Zhang</li>
<li>for: 解决强化学习（RL）中的稳定性问题，即“三难kill”问题，使RL agent可以对不被执行的策略进行反思。</li>
<li>methods: 提出了一种直接使用两个样本在马可夫链数据流中进行解决双样本问题的方法，从而解决了GTD的额外Weight问题，而不需要额外的负担。</li>
<li>results: 提供了对数据流中的Markov链进行分析，并证明了该算法与 canonical on-policy  temporal difference learning的吞吐量相同，但是具有更高的计算效率和更低的内存占用。<details>
<summary>Abstract</summary>
Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the convergence rate is on-par with the canonical on-policy temporal difference learning. Key to our analysis is a novel refined discretization of limiting ODEs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>掌握偏离策略的学习（RL）Agent可以对不执行的策略进行反思，是RL最重要的一个想法。然而，它可能会导致不稳定性，当与函数approximation和启动器（bootstrapping）相结合时，这些都是大规模RL中不可或缺的两个重要成分。这被称为“死亡三角形”。梯度时间差（GTD）是一种强大的解决方案，它通过解决对折 sampling 问题来 indirectly 使用重复或Fenchel duality。在这篇文章中，我们则提出了直接解决对折 sampling 问题的方法，通过在Markovian数据流中使用两个样本，并随着时间的增加，间隔逐渐增加。这种算法的计算效率与GTD相同，但是没有额外的 weights。我们只需要付出一个对时间的增长有限的logarithmic 的内存占用。我们提供了对数学分析和finite sample分析，其 converge 速率与标准的在政策上的 temporal difference 学习相同。关键在于我们的分析中使用了一种新的精细的限制ODEs的拟合。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Based-Diabetes-Detection-Using-Photoplethysmography-Signal-Features"><a href="#Machine-Learning-Based-Diabetes-Detection-Using-Photoplethysmography-Signal-Features" class="headerlink" title="Machine Learning-Based Diabetes Detection Using Photoplethysmography Signal Features"></a>Machine Learning-Based Diabetes Detection Using Photoplethysmography Signal Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01930">http://arxiv.org/abs/2308.01930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filipe A. C. Oliveira, Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Douglas A. Almeida, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 这个研究旨在开发一种非侵入性的光学脉冲测量技术，以检测糖尿病。</li>
<li>methods: 本研究使用了非侵入性的光学脉冲测量技术，并使用了条件调教的类别学习算法，包括Logistic Regression和eXtreme Gradient Boosting。</li>
<li>results: 研究获得了一个F1-Score和AUC的值为58.8±20.0%和79.2±15.0%，表明这种非侵入性的光学脉冲测量技术可以实现糖尿病的检测和预防。<details>
<summary>Abstract</summary>
Diabetes is a prevalent chronic condition that compromises the health of millions of people worldwide. Minimally invasive methods are needed to prevent and control diabetes but most devices for measuring glucose levels are invasive and not amenable for continuous monitoring. Here, we present an alternative method to overcome these shortcomings based on non-invasive optical photoplethysmography (PPG) for detecting diabetes. We classify non-Diabetic and Diabetic patients using the PPG signal and metadata for training Logistic Regression (LR) and eXtreme Gradient Boosting (XGBoost) algorithms. We used PPG signals from a publicly available dataset. To prevent overfitting, we divided the data into five folds for cross-validation. By ensuring that patients in the training set are not in the testing set, the model's performance can be evaluated on unseen subjects' data, providing a more accurate assessment of its generalization. Our model achieved an F1-Score and AUC of $58.8\pm20.0\%$ and $79.2\pm15.0\%$ for LR and $51.7\pm16.5\%$ and $73.6\pm17.0\%$ for XGBoost, respectively. Feature analysis suggested that PPG morphological features contains diabetes-related information alongside metadata. Our findings are within the same range reported in the literature, indicating that machine learning methods are promising for developing remote, non-invasive, and continuous measurement devices for detecting and preventing diabetes.
</details>
<details>
<summary>摘要</summary>
diabetes 是一种流行的慢性疾病，影响全球数百万人的健康。 however， most devices for measuring glucose levels are invasive and not suitable for continuous monitoring. 在这种情况下，我们提出了一种 alternatively method， based on non-invasive optical photoplethysmography (PPG) for detecting diabetes. we use PPG signal and metadata to train logistic regression (LR) and eXtreme gradient boosting (XGBoost) algorithms to classify non-diabetic and diabetic patients.we used PPG signals from a publicly available dataset and divided the data into five folds for cross-validation to prevent overfitting. our model achieved an F1-score and AUC of $58.8\pm20.0\%$ and $79.2\pm15.0\%$ for LR and $51.7\pm16.5\%$ and $73.6\pm17.0\%$ for XGBoost, respectively. feature analysis suggested that PPG morphological features contain diabetes-related information alongside metadata. our findings are within the same range reported in the literature, indicating that machine learning methods are promising for developing remote, non-invasive, and continuous measurement devices for detecting and preventing diabetes.
</details></li>
</ul>
<hr>
<h2 id="LLMs-Understand-Glass-Box-Models-Discover-Surprises-and-Suggest-Repairs"><a href="#LLMs-Understand-Glass-Box-Models-Discover-Surprises-and-Suggest-Repairs" class="headerlink" title="LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs"></a>LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01157">http://arxiv.org/abs/2308.01157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/interpretml/talktoebm">https://github.com/interpretml/talktoebm</a></li>
<li>paper_authors: Benjamin J. Lengerich, Sebastian Bordt, Harsha Nori, Mark E. Nunnally, Yin Aphinyanaphongs, Manolis Kellis, Rich Caruana</li>
<li>for: 这篇论文主要是为了探讨大语言模型（LLMs）如何与可解释模型（ interpretable models）结合使用，以实现数据科学中一些常见任务的自动化。</li>
<li>methods: 论文使用了一种层次 reasoning 方法，使得 LLMS 可以根据自己的背景知识来自动完成数据科学中的一些任务，如检测异常、描述异常的原因和修复异常。</li>
<li>results: 论文通过多个医疗实例示例，展示了 LLMS 在使用 GAMs 时的有用性，并提供了一个开源的 LLM-GAM 接口 $\texttt{TalkToEBM}$。<details>
<summary>Abstract</summary>
We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as an open-source LLM-GAM interface.
</details>
<details>
<summary>摘要</summary>
我们证明大型语言模型（LLM）具有极其出色的可解释性，可以将复杂的结果拆分为单一图表示的分成部分。通过预设的层级方法来理解，LLM可以提供完整的模型水平摘要，不需要整个模型适应上下文。这种方法允许LLM将它们广泛的背景知识应用到自动化资料科学中的常见任务，例如检测过去知识背景下的偏差、描述偏差的可能原因，以及修复偏差所需的修补措施。我们使用了多个预𫓹例子，尤其是通用加性模型（GAMs），以示出这些新的LLM功能的用用。最后，我们提出了一个名为$\texttt{TalkToEBM}$的开源LLM-GAM界面。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Prediction-Method-for-Depth-of-Anesthesia-During-Target-controlled-Infusion-of-Propofol-and-Remifentanil"><a href="#A-Transformer-based-Prediction-Method-for-Depth-of-Anesthesia-During-Target-controlled-Infusion-of-Propofol-and-Remifentanil" class="headerlink" title="A Transformer-based Prediction Method for Depth of Anesthesia During Target-controlled Infusion of Propofol and Remifentanil"></a>A Transformer-based Prediction Method for Depth of Anesthesia During Target-controlled Infusion of Propofol and Remifentanil</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01929">http://arxiv.org/abs/2308.01929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heeeyk/transformer-doa-prediction">https://github.com/heeeyk/transformer-doa-prediction</a></li>
<li>paper_authors: Yongkang He, Siyuan Peng, Mingjin Chen, Zhijing Yang, Yuanhui Chen</li>
<li>for: 预测麻醉效果的准确性是脊梁控制输液系统的关键。传统的PK-PD模型 для Bispectral index（BIS）预测需要手动选择模型参数，这可能在临床设置中具有挑战性。最近提出的深度学习方法可能只能捕捉总趋势，而不能预测急变的BIS。</li>
<li>methods: 我们提议使用 transformer 网络来预测麻醉深度（DOA），使用 propofol 和 remifentanil 的药液输液。我们的方法使用 LSTM 和 GRN 网络来提高特征融合的效率，并应用了注意力机制来探索药物之间的互动。我们还使用标签分布平滑和重新分配损失来解决数据不均衡问题。</li>
<li>results: 我们的提议方法比传统的PK-PD模型和先前的深度学习方法更高效，能够有效地预测麻醉深度在急变和深度麻醉条件下。<details>
<summary>Abstract</summary>
Accurately predicting anesthetic effects is essential for target-controlled infusion systems. The traditional (PK-PD) models for Bispectral index (BIS) prediction require manual selection of model parameters, which can be challenging in clinical settings. Recently proposed deep learning methods can only capture general trends and may not predict abrupt changes in BIS. To address these issues, we propose a transformer-based method for predicting the depth of anesthesia (DOA) using drug infusions of propofol and remifentanil. Our method employs long short-term memory (LSTM) and gate residual network (GRN) networks to improve the efficiency of feature fusion and applies an attention mechanism to discover the interactions between the drugs. We also use label distribution smoothing and reweighting losses to address data imbalance. Experimental results show that our proposed method outperforms traditional PK-PD models and previous deep learning methods, effectively predicting anesthetic depth under sudden and deep anesthesia conditions.
</details>
<details>
<summary>摘要</summary>
优先预测麻醉效果是控制性Infusion系统中的关键。传统的PK-PD模型需要手动选择参数，这可能是临床设置中的挑战。最近提出的深度学习方法可能只能捕捉总趋势，并不能预测突然变化的BIS。为解决这些问题，我们提议一种基于变换器的方法，用于预测麻醉深度（DOA），使用propofol和remifentanil的药物注射。我们的方法使用长期短期记忆（LSTM）和门控异常网络（GRN）来提高特征融合的效率，并使用注意力机制来揭示药物之间的交互。此外，我们还使用标签分布平滑和重新评估损失来Address数据不均衡。实验结果表明，我们的提议方法比传统PK-PD模型和前一些深度学习方法更高效，可以有效地预测麻醉深度在突然和深度麻醉条件下。
</details></li>
</ul>
<hr>
<h2 id="DySTreSS-Dynamically-Scaled-Temperature-in-Self-Supervised-Contrastive-Learning"><a href="#DySTreSS-Dynamically-Scaled-Temperature-in-Self-Supervised-Contrastive-Learning" class="headerlink" title="DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning"></a>DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01140">http://arxiv.org/abs/2308.01140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siladittya Manna, Soumitri Chattopadhyay, Rakesh Dey, Saumik Bhattacharya, Umapada Pal</li>
<li>for: 提高自然语言处理（NLP）领域中的自我监督学习（SSL）性能，尤其是对于SimCLR和MoCo等 contemporaneous自编程对比算法。</li>
<li>methods: 研究InfoNCE损失函数中温度超参数的影响，并提出一种基于cosinus相似性的温度扩散函数来优化样本分布在特征空间中。</li>
<li>results: 经验证明，提出的方法可以超过或与对比损失函数基于SSL算法相当。此外，我们还对温度的变化情况进行了全面的分析，并证明了在特征空间中的本地和全局结构的变化。<details>
<summary>Abstract</summary>
In contemporary self-supervised contrastive algorithms like SimCLR, MoCo, etc., the task of balancing attraction between two semantically similar samples and repulsion between two samples from different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyper-parameter is the key to regulating the penalties and the trade-off between uniformity and tolerance. In this work, we focus our attention to improve the performance of InfoNCE loss in SSL by studying the effect of temperature hyper-parameter values. We propose a cosine similarity-dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We further analyze the uniformity and tolerance metrics to investigate the optimal regions in the cosine similarity space for better optimization. Additionally, we offer a comprehensive examination of the behavior of local and global structures in the feature space throughout the pre-training phase, as the temperature varies. Experimental evidence shows that the proposed framework outperforms or is at par with the contrastive loss-based SSL algorithms. We believe our work (DySTreSS) on temperature scaling in SSL provides a foundation for future research in contrastive learning.
</details>
<details>
<summary>摘要</summary>
现代自我监督对势学习算法如SimCLR、MoCo等，任务是在两个semantically similar sample之间吸引，并在不同类型的sample之间强制分化。而这种任务受到强度负样本的存在影响最大。InfoNCE损失已经显示出对强度做出了罚款，但温度超参数是控制这些罚款和吸引vs抵抗之间的平衡。在这个工作中，我们关注了InfoNCE损失在SSL中的性能优化，研究温度超参数的影响。我们提议一种cosine similarity-dependent温度扩大函数，可以有效地优化样本在特征空间的分布。我们进一步分析了uniformity和tolerance度量来探索最佳的cosine similarity空间区域，以便更好地优化。此外，我们还对批量和全局结构在特征空间中的行为进行了详细的分析，从温度变化的过程中。实验证明，我们提出的框架（DySTreSS）在SSL中表现出色，与对势损失基于SSL算法相当。我们认为我们的工作在温度扩大中的SSL提供了一个基础，以便未来对对势学习进行进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Privacy-Allocation-for-Locally-Differentially-Private-Federated-Learning-with-Composite-Objectives"><a href="#Dynamic-Privacy-Allocation-for-Locally-Differentially-Private-Federated-Learning-with-Composite-Objectives" class="headerlink" title="Dynamic Privacy Allocation for Locally Differentially Private Federated Learning with Composite Objectives"></a>Dynamic Privacy Allocation for Locally Differentially Private Federated Learning with Composite Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01139">http://arxiv.org/abs/2308.01139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaojiao Zhang, Dominik Fay, Mikael Johansson</li>
<li>for: 该文章提出了一种基于地域ifferentially private federated学习算法，用于强 converges但可能是非流�emetric问题的保护工作者 gradient  против善良但怀疑的服务器。</li>
<li>methods: 提议的算法在共享信息中添加人工噪音来保护隐私，并动态分配时变的噪音 variance来最小化优化错误的上限，以满足先定的隐私预算限制。</li>
<li>results: 数值结果表明提议的算法在比较方法之上具有优势，可以在一个邻居的优解中实现隐私保护和实用性。<details>
<summary>Abstract</summary>
This paper proposes a locally differentially private federated learning algorithm for strongly convex but possibly nonsmooth problems that protects the gradients of each worker against an honest but curious server. The proposed algorithm adds artificial noise to the shared information to ensure privacy and dynamically allocates the time-varying noise variance to minimize an upper bound of the optimization error subject to a predefined privacy budget constraint. This allows for an arbitrarily large but finite number of iterations to achieve both privacy protection and utility up to a neighborhood of the optimal solution, removing the need for tuning the number of iterations. Numerical results show the superiority of the proposed algorithm over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种基于地域ifferentially private的联合学习算法，用于强 convex但可能非滑的问题，保护每个工作者的梯度 против一个诚实但感兴趣的服务器。提议的算法添加了随机噪声到共享信息中，以保障隐私，并动态分配时间变化的噪声方差，以最小化优化错误的Upper bound，subject to预定的隐私预算限制。这允许进行无限多次迭代，以达到隐私保护和实用性的权限，取消了迭代次数的调整。numerical results show that the proposed algorithm outperforms existing methods.
</details></li>
</ul>
<hr>
<h2 id="Can-We-Transfer-Noise-Patterns-A-Multi-environment-Spectrum-Analysis-Model-Using-Generated-Cases"><a href="#Can-We-Transfer-Noise-Patterns-A-Multi-environment-Spectrum-Analysis-Model-Using-Generated-Cases" class="headerlink" title="Can We Transfer Noise Patterns? A Multi-environment Spectrum Analysis Model Using Generated Cases"></a>Can We Transfer Noise Patterns? A Multi-environment Spectrum Analysis Model Using Generated Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01138">http://arxiv.org/abs/2308.01138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/magnomic/cnst">https://github.com/magnomic/cnst</a></li>
<li>paper_authors: Haiwen Du, Zheng Ju, Yu An, Honghui Du, Dongjie Zhu, Zhaoshuo Tian, Aonghus Lawlor, Ruihai Dong</li>
<li>for: 这个论文的目的是提高在线水质测试中的谱分析系统的精度和可靠性，使regulatory agencies能够更快地回应污染事件。</li>
<li>methods: 该论文提出了一种采用 transferred noise pattern 模型，可以在不同环境中提高谱分析系统的鲁棒性和可靠性。这种模型通过学习不同环境中标准水样的谱spectrum的差异，以 Transfer learning 的方式将陌生样本的噪声转移到标准样本中，从而提高谱分析系统的性能。</li>
<li>results: 实验表明，提出的方法可以减少谱分析系统中的噪声影响，提高系统的鲁棒性和可靠性。与基eline系统相比，该方法在不同背景噪声下的实验结果表明，它可以更好地适应不同环境中的噪声。<details>
<summary>Abstract</summary>
Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on spectral data with different background noises demonstrate the good noise-transferring ability of the proposed method against baseline systems ranging from wavelet denoising, deep neural networks, and generative models. From this research, we posit that our method can enhance the performance of DL models by generating high-quality cases. The source code is made publicly available online at https://github.com/Magnomic/CNST.
</details>
<details>
<summary>摘要</summary>
Spectrum analysis systems in online water quality testing are designed to detect types and concentrations of pollutants and enable regulatory agencies to respond promptly to pollution incidents. However, spectral data-based testing devices suffer from complex noise patterns when deployed in non-laboratory environments. To make the analysis model applicable to more environments, we propose a noise patterns transferring model, which takes the spectrum of standard water samples in different environments as cases and learns the differences in their noise patterns, thus enabling noise patterns to transfer to unknown samples. Unfortunately, the inevitable sample-level baseline noise makes the model unable to obtain the paired data that only differ in dataset-level environmental noise. To address the problem, we generate a sample-to-sample case-base to exclude the interference of sample-level noise on dataset-level noise learning, enhancing the system's learning performance. Experiments on spectral data with different background noises demonstrate the good noise-transferring ability of the proposed method against baseline systems ranging from wavelet denoising, deep neural networks, and generative models. From this research, we posit that our method can enhance the performance of DL models by generating high-quality cases. The source code is made publicly available online at <https://github.com/Magnomic/CNST>.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Multi-task-learning-for-classification-segmentation-reconstruction-and-detection-on-chest-CT-scans"><a href="#Multi-task-learning-for-classification-segmentation-reconstruction-and-detection-on-chest-CT-scans" class="headerlink" title="Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans"></a>Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01137">http://arxiv.org/abs/2308.01137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weronika Hryniewska-Guzik, Maria Kędzierska, Przemysław Biecek</li>
<li>for: 针对肺癌和covid-19两种疾病，提出了一种多任务学习框架，用于分类、分割、重建和检测。</li>
<li>methods: 使用多任务学习方法，抽取医学数据中重要的特征，如肿瘤，从小量数据中提取出来。</li>
<li>results: 提出了一种新的多任务框架，并在分类、分割、重建和检测任务中进行了实验，并取得了优秀的结果。此外，还研究了使用不同的后向和损失函数在分割任务中的可能性。<details>
<summary>Abstract</summary>
Lung cancer and covid-19 have one of the highest morbidity and mortality rates in the world. For physicians, the identification of lesions is difficult in the early stages of the disease and time-consuming. Therefore, multi-task learning is an approach to extracting important features, such as lesions, from small amounts of medical data because it learns to generalize better. We propose a novel multi-task framework for classification, segmentation, reconstruction, and detection. To the best of our knowledge, we are the first ones who added detection to the multi-task solution. Additionally, we checked the possibility of using two different backbones and different loss functions in the segmentation task.
</details>
<details>
<summary>摘要</summary>
肺癌和 COVID-19 在全球病例和死亡率中排名非常高。为医生来说，在疾病的早期阶段 identification of lesions 是困难和耗时的。因此，多任务学习是一种提取重要特征的方法，如肿瘤，从小量医疗数据中提取出来。我们提出了一种新的多任务框架，用于分类、分割、重建和检测。根据我们所知，我们是第一个在多任务解决方案中添加检测的人。此外，我们还检查了使用两种不同的背bone 和不同的损失函数在 segmentation 任务中的可能性。
</details></li>
</ul>
<hr>
<h2 id="Unlearning-Spurious-Correlations-in-Chest-X-ray-Classification"><a href="#Unlearning-Spurious-Correlations-in-Chest-X-ray-Classification" class="headerlink" title="Unlearning Spurious Correlations in Chest X-ray Classification"></a>Unlearning Spurious Correlations in Chest X-ray Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01119">http://arxiv.org/abs/2308.01119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Misgina Tsighe Hagos, Kathleen M. Curran, Brian Mac Namee</li>
<li>For: The paper aims to address the issue of spurious correlations in medical image classification, particularly in musculoskeletal image classification, due to unintended confounding factors such as skeletal maturation-induced bone growth.* Methods: The authors use a deep learning approach called eXplanation Based Learning (XBL) that incorporates interactive user feedback, specifically feature annotations, to unlearn spurious correlations and construct robust models.* Results: The study demonstrates the effectiveness of XBL in eliminating spurious correlations and improving model accuracy, even in the presence of confounding factors.Here is the same information in Simplified Chinese text:* For: 本研究旨在解决医学图像分类中的假相关性问题，特别是骨骼成熔融引起的骨骼成熔融相关性问题。* Methods: 作者使用了一种深度学习方法——解释基于学习（XBL），该方法通过获取用户交互式反馈，特别是特征标注，来消除假相关性和构建可靠的模型。* Results: 研究表明，XBL 可以有效消除假相关性，提高模型准确性，即使在骨骼成熔融等干扰因素存在时。<details>
<summary>Abstract</summary>
Medical image classification models are frequently trained using training datasets derived from multiple data sources. While leveraging multiple data sources is crucial for achieving model generalization, it is important to acknowledge that the diverse nature of these sources inherently introduces unintended confounders and other challenges that can impact both model accuracy and transparency. A notable confounding factor in medical image classification, particularly in musculoskeletal image classification, is skeletal maturation-induced bone growth observed during adolescence. We train a deep learning model using a Covid-19 chest X-ray dataset and we showcase how this dataset can lead to spurious correlations due to unintended confounding regions. eXplanation Based Learning (XBL) is a deep learning approach that goes beyond interpretability by utilizing model explanations to interactively unlearn spurious correlations. This is achieved by integrating interactive user feedback, specifically feature annotations. In our study, we employed two non-demanding manual feedback mechanisms to implement an XBL-based approach for effectively eliminating these spurious correlations. Our results underscore the promising potential of XBL in constructing robust models even in the presence of confounding factors.
</details>
<details>
<summary>摘要</summary>
医疗图像分类模型frequently使用多种数据源进行训练。虽然利用多种数据源对模型泛化起到关键作用，但是需要注意这些来源的多样性自然而来会引入无意义的干扰因素和其他挑战，这些因素可能会影响模型的准确性和透明度。在医疗图像分类中，特别是在肢体成像分类中，肿瘤成熔是一个强大的干扰因素。我们使用COVID-19肺X射线图像集进行训练深度学习模型，并表明这些数据集可能会导致偶极相关性，因为无意义的干扰区域。eXplanation Based Learning（XBL）是一种深度学习方法，它不仅提供了解释，还可以通过交互式解释来卸除偶极相关性。我们使用了两种简单的手动反馈机制来实现XBL中的解释。我们的结果表明XBL在存在干扰因素的情况下可以构建Robust的模型。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Popularity-Bias-in-Recommender-Systems"><a href="#A-Survey-on-Popularity-Bias-in-Recommender-Systems" class="headerlink" title="A Survey on Popularity Bias in Recommender Systems"></a>A Survey on Popularity Bias in Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01118">http://arxiv.org/abs/2308.01118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasiia Klimashevskaia, Dietmar Jannach, Mehdi Elahi, Christoph Trattner</li>
<li>for: The paper aims to discuss the popularity bias in recommender systems and explore ways to detect, quantify, and mitigate it.</li>
<li>methods: The paper reviews existing approaches to reduce popularity bias in recommender systems, including computational metrics and technical approaches.</li>
<li>results: The paper discusses the potential reasons for popularity bias and critically examines the current literature on the topic, which is mostly based on computational experiments and assumptions about the practical effects of including long-tail items in recommendations.<details>
<summary>Abstract</summary>
Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discuss today's literature, where we observe that the research is almost entirely based on computational experiments and on certain assumptions regarding the practical effects of including long-tail items in the recommendations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spatio-Temporal-Branching-for-Motion-Prediction-using-Motion-Increments"><a href="#Spatio-Temporal-Branching-for-Motion-Prediction-using-Motion-Increments" class="headerlink" title="Spatio-Temporal Branching for Motion Prediction using Motion Increments"></a>Spatio-Temporal Branching for Motion Prediction using Motion Increments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01097">http://arxiv.org/abs/2308.01097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jasonwang959/stpmp">https://github.com/jasonwang959/stpmp</a></li>
<li>paper_authors: Jiexin Wang, Yujie Zhou, Wenwen Qiang, Ying Ba, Bing Su, Ji-Rong Wen</li>
<li>for: 人体动作预测（HMP）是一个广泛应用的研究领域，但是它仍然是一个具有较高难度的任务，因为未来姿势的概率分布是随机和不规则的。</li>
<li>methods: 我们提出了一种新的分支网络方法，它将时间频率预测和空间结构预测解耦开，提取更多的动作信息，并通过知识储存来实现跨频率知识学习。</li>
<li>results: 我们在标准HMP bencmark上评估了我们的方法，并与当前的方法相比，在预测精度方面表现出色。<details>
<summary>Abstract</summary>
Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and achieves complementary cross-domain knowledge learning through knowledge distillation. Our approach effectively reduces noise interference and provides more expressive information for characterizing motion by separately extracting temporal and spatial features. We evaluate our approach on standard HMP benchmarks and outperform state-of-the-art methods in terms of prediction accuracy.
</details>
<details>
<summary>摘要</summary>
人体运动预测（HMP）已成为一个流行的研究主题，它的应用场景非常广泛，但是预测人体运动的任务仍然具有挑战性，因为未来的姿势是随机和不规则的。传统的方法通常使用手工设计的特征和机器学习技术来实现预测，但这些方法通常无法模型人体运动的复杂动力学。现代深度学习基于的方法在学习运动的空间时间表示方面取得成功，但是这些模型经常忽视运动数据的可靠性。此外，运动数据中的时间和空间关系不同。时间关系捕捉运动信息的时间方面，而空间关系描述人体结构和不同节点之间的关系。在本文中，我们提出了一种新的空间时间分支网络使用增量信息进行HMP，这种方法可以分离学习时间频率和空间频率特征，提取更多的运动信息，并通过知识填充来实现跨频率知识学习。我们的方法可以减少噪声扰动并为运动特征提供更加精细的描述。我们在标准HMP测试benchmark上评估了我们的方法，并与当前的状态艺技术相比，在预测精度方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Multi-variable-Hard-Physical-Constraints-for-Climate-Model-Downscaling"><a href="#Multi-variable-Hard-Physical-Constraints-for-Climate-Model-Downscaling" class="headerlink" title="Multi-variable Hard Physical Constraints for Climate Model Downscaling"></a>Multi-variable Hard Physical Constraints for Climate Model Downscaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01868">http://arxiv.org/abs/2308.01868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jose González-Abad, Álex Hernández-García, Paula Harder, David Rolnick, José Manuel Gutiérrez</li>
<li>for:  GCMs 全球气候模型是主要用于 simulate 气候演变和评估气候变化的工具，但它们通常在尺度较粗的尺度下运行，导致它们无法准确地复制当地规模的地理现象。</li>
<li>methods: 统计下降方法利用深度学习可以解决这个问题，通过将粗略变量转换为本地气候场，以便在区域GCM预测中使用。然而，通常会独立地下降不同变量的气候场，导致物理性关系 между 不同变量的下降场被违反。</li>
<li>results: 本研究探讨了这个问题的范围，并通过对温度变量进行应用，建立了一个满足物理关系 между 多个下降场的框架。<details>
<summary>Abstract</summary>
Global Climate Models (GCMs) are the primary tool to simulate climate evolution and assess the impacts of climate change. However, they often operate at a coarse spatial resolution that limits their accuracy in reproducing local-scale phenomena. Statistical downscaling methods leveraging deep learning offer a solution to this problem by approximating local-scale climate fields from coarse variables, thus enabling regional GCM projections. Typically, climate fields of different variables of interest are downscaled independently, resulting in violations of fundamental physical properties across interconnected variables. This study investigates the scope of this problem and, through an application on temperature, lays the foundation for a framework introducing multi-variable hard constraints that guarantees physical relationships between groups of downscaled climate variables.
</details>
<details>
<summary>摘要</summary>
全球气候模型（GCM）是气候进化和气候变化影响的主要工具。然而，它们通常在粗略的空间分辨率下运行，限制其在本地规模现象的准确还原。统计下降方法，利用深度学习解决这个问题，通过将粗略变量下降到本地气候场，以便地区GCM预测。然而，通常会在不同变量 интересов的气候场下降独立，导致物理关系的违反，这种情况下，这种研究探讨范围和应用于温度下降，为多变量硬件约束的框架奠基。
</details></li>
</ul>
<hr>
<h2 id="Homography-Estimation-in-Complex-Topological-Scenes"><a href="#Homography-Estimation-in-Complex-Topological-Scenes" class="headerlink" title="Homography Estimation in Complex Topological Scenes"></a>Homography Estimation in Complex Topological Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01086">http://arxiv.org/abs/2308.01086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giacomo D’Amicantonio, Egor Bondarau, Peter H. N. De With</li>
<li>for: 用于视觉监测应用，如交通分析和犯罪检测</li>
<li>methods: 使用自定义的字典方法，不需要先知 camera 设置</li>
<li>results: 提高 IoU 指标，比对state-of-the-art模型在五个synthetic dataset和2014年世界杯 dataset 的提高为12%<details>
<summary>Abstract</summary>
Surveillance videos and images are used for a broad set of applications, ranging from traffic analysis to crime detection. Extrinsic camera calibration data is important for most analysis applications. However, security cameras are susceptible to environmental conditions and small camera movements, resulting in a need for an automated re-calibration method that can account for these varying conditions. In this paper, we present an automated camera-calibration process leveraging a dictionary-based approach that does not require prior knowledge on any camera settings. The method consists of a custom implementation of a Spatial Transformer Network (STN) and a novel topological loss function. Experiments reveal that the proposed method improves the IoU metric by up to 12% w.r.t. a state-of-the-art model across five synthetic datasets and the World Cup 2014 dataset.
</details>
<details>
<summary>摘要</summary>
视频监测和图像被用于广泛的应用领域，从交通分析到犯罪检测。外部摄像头准确性是重要的。然而，安全摄像头容易受到环境因素和小型摄像头运动的影响，需要一种自动重新准确方法，可以考虑这些不同的条件。本文提出了一种基于词典方法的自动摄像头准确化 процесса，不需要任何摄像头设置的先前知识。该方法包括一种自定义的空间变换网络（STN）和一种新的拓扑损失函数。实验表明，提议的方法可以在五个 sintetic 数据集和2014年世界杯数据集中提高 IoU 指标至最多 12% 相比于现有模型。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Identification-of-Quadratic-Symplectic-Representations-of-Nonlinear-Hamiltonian-Systems"><a href="#Data-Driven-Identification-of-Quadratic-Symplectic-Representations-of-Nonlinear-Hamiltonian-Systems" class="headerlink" title="Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems"></a>Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01084">http://arxiv.org/abs/2308.01084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Süleyman Yildiz, Pawan Goyal, Thomas Bendokat, Peter Benner</li>
<li>for: 学习哈密顿系统使用数据</li>
<li>methods: 使用提升假设，将非线性哈密顿系统写作非线性系统，并使用协调器抽象器保持哈密顿结构</li>
<li>results: 实现了使用数据学习哈密顿系统，并保持系统的长期稳定性，同时使用三次哈密顿函数提供较低的模型复杂性<details>
<summary>Abstract</summary>
We present a framework for learning Hamiltonian systems using data. This work is based on the lifting hypothesis, which posits that nonlinear Hamiltonian systems can be written as nonlinear systems with cubic Hamiltonians. By leveraging this, we obtain quadratic dynamics that are Hamiltonian in a transformed coordinate system. To that end, for given generalized position and momentum data, we propose a methodology to learn quadratic dynamical systems, enforcing the Hamiltonian structure in combination with a symplectic auto-encoder. The enforced Hamiltonian structure exhibits long-term stability of the system, while the cubic Hamiltonian function provides relatively low model complexity. For low-dimensional data, we determine a higher-order transformed coordinate system, whereas, for high-dimensional data, we find a lower-order coordinate system with the desired properties. We demonstrate the proposed methodology by means of both low-dimensional and high-dimensional nonlinear Hamiltonian systems.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于数据学习哈密顿系统的框架。这项工作基于升高假设，即非线性哈密顿系统可以写作非线性系统的立方 Hamiltonians。通过这种方式，我们得到了各次幂动力学，并且在一种变换坐标系中保持哈密顿结构。为此，我们提出了一种使用泛化位势和动量数据学习 quadratic dynamical systems的方法，并在合理的坐标系中强制实施哈密顿结构。这种方法可以保证系统的长期稳定性，而且立方 Hamiltonians 的模型复杂度相对较低。对于低维数据，我们可以找到一个更高阶的变换坐标系，而对于高维数据，我们可以找到一个更低阶的坐标系，满足所需的性质。我们通过低维和高维非线性哈密顿系统的示例来证明我们的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Practical-Deep-Learning-Based-Acoustic-Side-Channel-Attack-on-Keyboards"><a href="#A-Practical-Deep-Learning-Based-Acoustic-Side-Channel-Attack-on-Keyboards" class="headerlink" title="A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards"></a>A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01074">http://arxiv.org/abs/2308.01074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JBFH-Dev/Keystroke-Datasets">https://github.com/JBFH-Dev/Keystroke-Datasets</a></li>
<li>paper_authors: Joshua Harrison, Ehsan Toreini, Maryam Mehrnezhad</li>
<li>for: 防止键盘Side channel攻击</li>
<li>methods: 使用深度学习模型类型 laptop键盘输入</li>
<li>results:  achieved an accuracy of 95% and 93% when trained on keystrokes recorded by a nearby phone and Zoom video-conferencing software respectively, proving the practicality of these side channel attacks via off-the-shelf equipment and algorithms.<details>
<summary>Abstract</summary>
With recent developments in deep learning, the ubiquity of micro-phones and the rise in online services via personal devices, acoustic side channel attacks present a greater threat to keyboards than ever. This paper presents a practical implementation of a state-of-the-art deep learning model in order to classify laptop keystrokes, using a smartphone integrated microphone. When trained on keystrokes recorded by a nearby phone, the classifier achieved an accuracy of 95%, the highest accuracy seen without the use of a language model. When trained on keystrokes recorded using the video-conferencing software Zoom, an accuracy of 93% was achieved, a new best for the medium. Our results prove the practicality of these side channel attacks via off-the-shelf equipment and algorithms. We discuss a series of mitigation methods to protect users against these series of attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Automatic-Feature-Engineering-for-Time-Series-Classification-Evaluation-and-Discussion"><a href="#Automatic-Feature-Engineering-for-Time-Series-Classification-Evaluation-and-Discussion" class="headerlink" title="Automatic Feature Engineering for Time Series Classification: Evaluation and Discussion"></a>Automatic Feature Engineering for Time Series Classification: Evaluation and Discussion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01071">http://arxiv.org/abs/2308.01071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aurélien Renault, Alexis Bondu, Vincent Lemaire, Dominique Gay</li>
<li>for: 本文主要针对时间序列分类（TSC）问题，尤其是在数据科学和知识工程领域。</li>
<li>methods: 本文使用了多种方法，包括相似度量、间隔、Shapelets、字典等，以及深度学习方法和混合ensemble方法。</li>
<li>results: 本文的实验结果表明，使用feature工程工具可以获得高度可预测性的时间序列分类结果，与当前状态艺术TSC算法相当。<details>
<summary>Abstract</summary>
Time Series Classification (TSC) has received much attention in the past two decades and is still a crucial and challenging problem in data science and knowledge engineering. Indeed, along with the increasing availability of time series data, many TSC algorithms have been suggested by the research community in the literature. Besides state-of-the-art methods based on similarity measures, intervals, shapelets, dictionaries, deep learning methods or hybrid ensemble methods, several tools for extracting unsupervised informative summary statistics, aka features, from time series have been designed in the recent years. Originally designed for descriptive analysis and visualization of time series with informative and interpretable features, very few of these feature engineering tools have been benchmarked for TSC problems and compared with state-of-the-art TSC algorithms in terms of predictive performance. In this article, we aim at filling this gap and propose a simple TSC process to evaluate the potential predictive performance of the feature sets obtained with existing feature engineering tools. Thus, we present an empirical study of 11 feature engineering tools branched with 9 supervised classifiers over 112 time series data sets. The analysis of the results of more than 10000 learning experiments indicate that feature-based methods perform as accurately as current state-of-the-art TSC algorithms, and thus should rightfully be considered further in the TSC literature.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-Analytic-Calculus-Cracks-AdaBoost-Code"><a href="#When-Analytic-Calculus-Cracks-AdaBoost-Code" class="headerlink" title="When Analytic Calculus Cracks AdaBoost Code"></a>When Analytic Calculus Cracks AdaBoost Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01070">http://arxiv.org/abs/2308.01070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean-Marc Brossier, Olivier Lafitte, Lenny Réthoré</li>
<li>for: 本研究探讨了支持学习中的加强原理，特别是 AdaBoost 算法是否真的是一种优化算法。</li>
<li>methods: 本研究使用了一个二分类问题作为例子，并通过三个二进制分类器的组合来显示 AdaBoost 算法可以由一个真表示表示出来。</li>
<li>results: 研究结果表明，AdaBoost 算法不是一种优化算法，而是一种可以由真表示表示出来的算法。与 scikit-learn 库中的 AdaBoost 算法实现相比，本研究的结果也提供了一些对 AdaBoost 算法的比较。<details>
<summary>Abstract</summary>
The principle of boosting in supervised learning involves combining multiple weak classifiers to obtain a stronger classifier. AdaBoost has the reputation to be a perfect example of this approach. We have previously shown that AdaBoost is not truly an optimization algorithm. This paper shows that AdaBoost is an algorithm in name only, as the resulting combination of weak classifiers can be explicitly calculated using a truth table. This study is carried out by considering a problem with two classes and is illustrated by the particular case of three binary classifiers and presents results in comparison with those from the implementation of AdaBoost algorithm of the Python library scikit-learn.
</details>
<details>
<summary>摘要</summary>
“boosting”在超级vised学习中的原则是将多个弱分类器组合而成一个更强大的分类器。阿达Boost被认为是这种方法的完美示例。我们之前已经证明了阿达Boost不是一个优化算法。这篇论文表明，阿达Boost并不是一个真正的算法，因为将弱分类器组合的结果可以通过真值表进行直观计算。本研究通过考虑两个类别问题，并通过三个二进制分类器的特殊情况进行示例，并与scikit-learn库中的阿达Boost实现相比较。
</details></li>
</ul>
<hr>
<h2 id="Graph-Anomaly-Detection-at-Group-Level-A-Topology-Pattern-Enhanced-Unsupervised-Approach"><a href="#Graph-Anomaly-Detection-at-Group-Level-A-Topology-Pattern-Enhanced-Unsupervised-Approach" class="headerlink" title="Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach"></a>Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01063">http://arxiv.org/abs/2308.01063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xing Ai, Jialong Zhou, Yulin Zhu, Gaolei Li, Tomasz P. Michalak, Xiapu Luo, Kai Zhou</li>
<li>for: 本研究旨在提出一种新的无监督框架，用于检测图像中异常的群体（Group-level Graph Anomaly Detection，Gr-GAD）。</li>
<li>methods: 该框架首先使用变体的图自动编码器（Graph AutoEncoder，GAE）来找到可能的异常群体的anchor节点，然后使用群体采样来选择候选组，并使用图像特征来生成每个候选组的嵌入。</li>
<li>results: 在实际世界和synthetic数据集上的实验结果显示，提出的框架能够有效地识别和地图异常群体，这表明该框架是一种有前途的解决方案。<details>
<summary>Abstract</summary>
Graph anomaly detection (GAD) has achieved success and has been widely applied in various domains, such as fraud detection, cybersecurity, finance security, and biochemistry. However, existing graph anomaly detection algorithms focus on distinguishing individual entities (nodes or graphs) and overlook the possibility of anomalous groups within the graph. To address this limitation, this paper introduces a novel unsupervised framework for a new task called Group-level Graph Anomaly Detection (Gr-GAD). The proposed framework first employs a variant of Graph AutoEncoder (GAE) to locate anchor nodes that belong to potential anomaly groups by capturing long-range inconsistencies. Subsequently, group sampling is employed to sample candidate groups, which are then fed into the proposed Topology Pattern-based Graph Contrastive Learning (TPGCL) method. TPGCL utilizes the topology patterns of groups as clues to generate embeddings for each candidate group and thus distinct anomaly groups. The experimental results on both real-world and synthetic datasets demonstrate that the proposed framework shows superior performance in identifying and localizing anomaly groups, highlighting it as a promising solution for Gr-GAD. Datasets and codes of the proposed framework are at the github repository https://anonymous.4open.science/r/Topology-Pattern-Enhanced-Unsupervised-Group-level-Graph-Anomaly-Detection.
</details>
<details>
<summary>摘要</summary>
graph anomaly detection (GAD) 已经取得成功并广泛应用于不同领域，如诈骗检测、网络安全、金融安全和生物化学。然而，现有的图像异常检测算法都是专注于区分个体节点或图像，忽略图像中可能存在异常群集的可能性。为了解决这种局限性，本文提出了一种新的无监督框架，即Group-level Graph Anomaly Detection (Gr-GAD)。该框架首先使用变体的图自动编码器（GAE）来找到可能异常群集的anchor节点，并通过捕捉长范围的不一致性来捕捉异常群集的特征。然后，群体采样被用来采样候选群集，并将其传递给提出的Topology Pattern-based Graph Contrastive Learning（TPGCL）方法。TPGCL利用群体的topology特征作为准确的准确度，生成每个候选群集的嵌入。实验结果表明，提出的框架在真实世界和 sintetic 数据集上具有优秀的异常群集识别和定位能力，从而证明了该框架的潜在价值。数据集和代码可以在 GitHub 上获取：https://anonymous.4open.science/r/Topology-Pattern-Enhanced-Unsupervised-Group-level-Graph-Anomaly-Detection。
</details></li>
</ul>
<hr>
<h2 id="Simulation-based-inference-using-surjective-sequential-neural-likelihood-estimation"><a href="#Simulation-based-inference-using-surjective-sequential-neural-likelihood-estimation" class="headerlink" title="Simulation-based inference using surjective sequential neural likelihood estimation"></a>Simulation-based inference using surjective sequential neural likelihood estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01054">http://arxiv.org/abs/2308.01054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dirmeier/ssnl">https://github.com/dirmeier/ssnl</a></li>
<li>paper_authors: Simon Dirmeier, Carlo Albert, Fernando Perez-Cruz</li>
<li>for: 这篇论文的目的是提出一种基于模拟的推理方法，用于处理不可分解的几何函数模型，只有生成 synthetic 数据的 simulator 可用。</li>
<li>methods: 该方法使用一种维度减少的射影正常化流形模型，作为几何函数模型的代理，以便使用普通的极限渐近法或变量极限法进行推理。</li>
<li>results: 作者在多种实验中证明了 SSNL 比当今的同类方法在高维数据集上表现更佳，包括一个具有实际意义的astrophysical例子，用于模拟太阳magnetic field strength。<details>
<summary>Abstract</summary>
We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel method for simulation-based inference in models where the evaluation of the likelihood function is not tractable and only a simulator that can generate synthetic data is available. SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function which allows for conventional Bayesian inference using either Markov chain Monte Carlo methods or variational inference. By embedding the data in a low-dimensional space, SSNL solves several issues previous likelihood-based methods had when applied to high-dimensional data sets that, for instance, contain non-informative data dimensions or lie along a lower-dimensional manifold. We evaluate SSNL on a wide variety of experiments and show that it generally outperforms contemporary methods used in simulation-based inference, for instance, on a challenging real-world example from astrophysics which models the magnetic field strength of the sun using a solar dynamo model.
</details>
<details>
<summary>摘要</summary>
我们提出了Sequential Neural Likelihood（SSNL）估计方法，用于基于模拟的推理，其中评估likelihood函数不可求解，只有一个可以生成假数据的 simulator。SSNL使用一个维度减少的射影正常分布模型，并将其作为媒介函数使用，这使得可以使用 conventinal Bayesian推理方法，如Markov chain Monte Carlo方法或variational推理。通过嵌入数据到低维空间中，SSNL解决了之前基于likelihood方法在高维数据集上遇到的多种问题，如非有用数据维度或数据集 lying在一个低维度扁平面上。我们在各种实验中评估了SSNL，并显示它通常超过了当今使用在基于模拟的推理中的方法，如在一个具有实际意义的例子中，模拟太阳magnetic field strength using solar dynamo模型。
</details></li>
</ul>
<hr>
<h2 id="A-Counterfactual-Safety-Margin-Perspective-on-the-Scoring-of-Autonomous-Vehicles’-Riskiness"><a href="#A-Counterfactual-Safety-Margin-Perspective-on-the-Scoring-of-Autonomous-Vehicles’-Riskiness" class="headerlink" title="A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles’ Riskiness"></a>A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles’ Riskiness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01050">http://arxiv.org/abs/2308.01050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Zanardi, Andrea Censi, Margherita Atzei, Luigi Di Lillo, Emilio Frazzoli</li>
<li>for: This paper aims to provide a data-driven framework for comparing the risk of different autonomous vehicles (AVs) in various operational design domains (ODDs).</li>
<li>methods: The paper uses counterfactual simulations of “misbehaving” road users to assess the risk of AVs. The proposed methodology is applicable even when the AV’s behavioral policy is unknown, making it useful for external third-party risk assessors.</li>
<li>results: The experimental results demonstrate a correlation between the safety margin, the driving policy quality, and the ODD, shedding light on the relative risk associated with different AV providers. The paper contributes to AV safety assessment and addresses legislative and insurance concerns surrounding the technology.<details>
<summary>Abstract</summary>
Autonomous Vehicles (AVs) have the potential to provide numerous societal benefits, such as decreased road accidents and increased overall transportation efficiency. However, quantifying the risk associated with AVs is challenging due to the lack of historical data and the rapidly evolving technology. This paper presents a data-driven framework for comparing the risk of different AVs' behaviors in various operational design domains (ODDs), based on counterfactual simulations of "misbehaving" road users. We introduce the concept of counterfactual safety margin, which represents the minimum deviation from normal behavior that could lead to a collision. This concept helps to find the most critical scenarios but also to assess the frequency and severity of risk of AVs. We show that the proposed methodology is applicable even when the AV's behavioral policy is unknown -- through worst- and best-case analyses -- making the method useful also to external third-party risk assessors. Our experimental results demonstrate the correlation between the safety margin, the driving policy quality, and the ODD shedding light on the relative risk associated with different AV providers. This work contributes to AV safety assessment and aids in addressing legislative and insurance concerns surrounding this emerging technology.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Are-Easy-Data-Easy-for-K-Means"><a href="#Are-Easy-Data-Easy-for-K-Means" class="headerlink" title="Are Easy Data Easy (for K-Means)"></a>Are Easy Data Easy (for K-Means)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01926">http://arxiv.org/abs/2308.01926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Mieczysław A. Kłopotek</li>
<li>for:  investigate the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm.</li>
<li>methods:  uses the $k$-means algorithm and derives conditions for well-separated clusters, and proposes a new algorithm that is a variation of $k$-means++ via repeated subsampling.</li>
<li>results:  the new algorithm outperforms four other algorithms from the $k$-means family on the task of discovering well-separated clusters.<details>
<summary>Abstract</summary>
This paper investigates the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm. The concept of well-separatedness used here is derived directly from the common definition of clusters, which imposes an interplay between the requirements of within-cluster-homogenicity and between-clusters-diversity. Conditions are derived for a special case of well-separated clusters such that the global minimum of $k$-means cost function coincides with the well-separatedness. An experimental investigation is performed to find out whether or no various brands of $k$-means are actually capable of discovering well separated clusters. It turns out that they are not. A new algorithm is proposed that is a variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The new algorithm outperforms four other algorithms from $k$-means family on the task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluation-of-network-guided-random-forest-for-disease-gene-discovery"><a href="#Evaluation-of-network-guided-random-forest-for-disease-gene-discovery" class="headerlink" title="Evaluation of network-guided random forest for disease gene discovery"></a>Evaluation of network-guided random forest for disease gene discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01323">http://arxiv.org/abs/2308.01323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianchang Hu, Silke Szymczak</li>
<li>For: The paper aims to investigate the performance of a network-guided random forest (RF) algorithm for gene expression data analysis, specifically for disease module and pathway identification.* Methods: The paper uses a network-guided RF approach that summarizes network information into a sampling probability of predictor variables, which is then used in the construction of the RF.* Results: The paper finds that network-guided RF does not provide better disease prediction than the standard RF, but it can identify disease genes more accurately when disease status is independent from genes in the given network. Additionally, the paper demonstrates that network-guided RF can identify genes from PGR-related pathways, leading to a better connected module of identified genes.Here is the same information in Simplified Chinese text:* For: 本研究用于 investigate random forest（RF）算法在基因表达数据分析中的性能，特别是疾病模块和路径标识。* Methods: 本研究使用一种基于网络信息的网络导向RF方法，将网络信息总结为预测变量的抽样概率，然后在RF构建中使用。* Results: 研究发现，网络导向RF不比标准RF提供更好的疾病预测。但是，当疾病状态与网络中的基因独立时，网络导向RF可以更准确地预测疾病基因。此外，研究还发现，使用网络信息时，特别是对于核心基因，可能会出现偶极选择的问题。<details>
<summary>Abstract</summary>
Gene network information is believed to be beneficial for disease module and pathway identification, but has not been explicitly utilized in the standard random forest (RF) algorithm for gene expression data analysis. We investigate the performance of a network-guided RF where the network information is summarized into a sampling probability of predictor variables which is further used in the construction of the RF. Our results suggest that network-guided RF does not provide better disease prediction than the standard RF. In terms of disease gene discovery, if disease genes form module(s), network-guided RF identifies them more accurately. In addition, when disease status is independent from genes in the given network, spurious gene selection results can occur when using network information, especially on hub genes. Our empirical analysis on two balanced microarray and RNA-Seq breast cancer datasets from The Cancer Genome Atlas (TCGA) for classification of progesterone receptor (PR) status also demonstrates that network-guided RF can identify genes from PGR-related pathways, which leads to a better connected module of identified genes.
</details>
<details>
<summary>摘要</summary>
GENE网络信息被认为对疾病模块和路径标识有利，但尚未直接应用于标准随机森林（RF）算法中。我们调查了基于网络信息的网络指导RF的性能。我们的结果表明，网络指导RF不比标准RF提供更好的疾病预测。在疾病基因发现方面，如果疾病基因组成模块， тогда网络指导RF能够更准确地确定它们。此外，当疾病状态与网络中的基因独立时，使用网络信息时，特别是对于枢纽基因，可能会出现假阳性基因选择结果。我们对TCGA breast cancer数据集进行了二分类分析，并证明了网络指导RF可以从PGR相关的路径中选择基因，从而构建更好地连接的基因模块。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-Distance-between-unbalanced-Distributions-–-The-flat-Metric"><a href="#Computing-the-Distance-between-unbalanced-Distributions-–-The-flat-Metric" class="headerlink" title="Computing the Distance between unbalanced Distributions – The flat Metric"></a>Computing the Distance between unbalanced Distributions – The flat Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01039">http://arxiv.org/abs/2308.01039</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hs42/flat_metric">https://github.com/hs42/flat_metric</a></li>
<li>paper_authors: Henri Schmidt, Christian Düll</li>
<li>for: 本研究是为了计算任意维度中的平凡度量，以扩展 Wasserstein 距离 W1 的概念，并应用于不均衡优化运输任务以及数据分布的分析。</li>
<li>methods: 本研究使用神经网络来确定两个给定概率分布之间的距离。特别是通过独立训练多个网络来实现对Distance的比较性。</li>
<li>results: 经过多个实验和 simulate 数据测试，研究发现该方法可以准确地计算平凡度量，并且可以在不同的维度下实现相似的比较性。<details>
<summary>Abstract</summary>
We provide an implementation to compute the flat metric in any dimension. The flat metric, also called dual bounded Lipschitz distance, generalizes the well-known Wasserstein distance W1 to the case that the distributions are of unequal total mass. This is of particular interest for unbalanced optimal transport tasks and for the analysis of data distributions where the sample size is important or normalization is not possible. The core of the method is based on a neural network to determine on optimal test function realizing the distance between two given measures. Special focus was put on achieving comparability of pairwise computed distances from independently trained networks. We tested the quality of the output in several experiments where ground truth was available as well as with simulated data.
</details>
<details>
<summary>摘要</summary>
我们提供了一个实现方法来计算任何维度的扁平度量。扁平度量，也被称为双重约束点距离，将水星拓扑距离W1推广到分布是不均匀的情况下。这对于不均匀优化运输任务和资料分布分析中是非常有兴趣的。我们的核心方法是使用神经网络来决定两个给定的概率分布之间的距离。我们特别强调了独立训练的网络之间的比较可靠性。我们在许多实验中评估了结果，包括可用的基准值和实验数据。
</details></li>
</ul>
<hr>
<h2 id="Three-Factors-to-Improve-Out-of-Distribution-Detection"><a href="#Three-Factors-to-Improve-Out-of-Distribution-Detection" class="headerlink" title="Three Factors to Improve Out-of-Distribution Detection"></a>Three Factors to Improve Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01030">http://arxiv.org/abs/2308.01030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunjun Choi, JaeHo Chung, Hawook Jeong, Jin Young Choi</li>
<li>for: 提高 OOD 检测性能和分类精度的方法</li>
<li>methods: 使用自知力塑化损失、半硬外围样本训练和超vised contrastive learning</li>
<li>results: 提高 OOD 检测性能和分类精度，缓解分类和 OOD 检测之间的负面关系<details>
<summary>Abstract</summary>
In the problem of out-of-distribution (OOD) detection, the usage of auxiliary data as outlier data for fine-tuning has demonstrated encouraging performance. However, previous methods have suffered from a trade-off between classification accuracy (ACC) and OOD detection performance (AUROC, FPR, AUPR). To improve this trade-off, we make three contributions: (i) Incorporating a self-knowledge distillation loss can enhance the accuracy of the network; (ii) Sampling semi-hard outlier data for training can improve OOD detection performance with minimal impact on accuracy; (iii) The introduction of our novel supervised contrastive learning can simultaneously improve OOD detection performance and the accuracy of the network. By incorporating all three factors, our approach enhances both accuracy and OOD detection performance by addressing the trade-off between classification and OOD detection. Our method achieves improvements over previous approaches in both performance metrics.
</details>
<details>
<summary>摘要</summary>
在 OUT-OF-DISTRIBUTION（OOD）检测问题中，使用辅助数据作为精度数据进行微调得到了鼓舞人的成绩。然而，先前的方法受到了准确率（ACC）和OOD检测性能（AUROC、FPR、AUPR）的负面交互。为了改进这种交互，我们提出了三项贡献：（i）将自我知识热释损失纳入网络准确性;（ii）在训练中采用半硬度异常数据抽样可以提高OOD检测性能，无需影响准确率;（iii）我们提出的新的监督对比学习可以同时提高OOD检测性能和网络准确性。通过结合这三个因素，我们的方法可以同时提高准确率和OOD检测性能，解决准确率和OOD检测之间的负面交互。我们的方法在性能指标上超越先前的方法。
</details></li>
</ul>
<hr>
<h2 id="Maximizing-Success-Rate-of-Payment-Routing-using-Non-stationary-Bandits"><a href="#Maximizing-Success-Rate-of-Payment-Routing-using-Non-stationary-Bandits" class="headerlink" title="Maximizing Success Rate of Payment Routing using Non-stationary Bandits"></a>Maximizing Success Rate of Payment Routing using Non-stationary Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01028">http://arxiv.org/abs/2308.01028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aayush Chaudhary, Abhinav Rai, Abhishek Gupta</li>
<li>For: This paper discusses the design and deployment of a non-stationary multi-armed bandit approach to determine a near-optimal payment routing policy based on recent transaction history.* Methods: The proposed Routing Service architecture uses a novel Ray-based implementation to optimize payment routing for over 10,000 transactions per second, while adhering to PCI DSS system design requirements and ecosystem constraints.* Results: The authors evaluate the effectiveness of multiple bandit-based payment routing algorithms on a custom simulator and demonstrate consistent improvement in transaction success rate (0.92%) compared to traditional rule-based methods over one month in live experiments on a fantasy sports platform.<details>
<summary>Abstract</summary>
This paper discusses the system architecture design and deployment of non-stationary multi-armed bandit approaches to determine a near-optimal payment routing policy based on the recent history of transactions. We propose a Routing Service architecture using a novel Ray-based implementation for optimally scaling bandit-based payment routing to over 10000 transactions per second, adhering to the system design requirements and ecosystem constraints with Payment Card Industry Data Security Standard (PCI DSS). We first evaluate the effectiveness of multiple bandit-based payment routing algorithms on a custom simulator to benchmark multiple non-stationary bandit approaches and identify the best hyperparameters. We then conducted live experiments on the payment transaction system on a fantasy sports platform Dream11. In the live experiments, we demonstrated that our non-stationary bandit-based algorithm consistently improves the success rate of transactions by 0.92\% compared to the traditional rule-based methods over one month.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Representation-Learning-for-Periodic-Time-Series-with-Floss-A-Frequency-Domain-Regularization-Approach"><a href="#Enhancing-Representation-Learning-for-Periodic-Time-Series-with-Floss-A-Frequency-Domain-Regularization-Approach" class="headerlink" title="Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach"></a>Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01011">http://arxiv.org/abs/2308.01011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agustdd/floss">https://github.com/agustdd/floss</a></li>
<li>paper_authors: Chunwei Yang, Xiaoxu Chen, Lijun Sun, Hongyu Yang, Yuankai Wu</li>
<li>for: 本研究旨在提出一种自适应 periodic regularization 方法，以提高深度学习模型对时间序列数据的表征。</li>
<li>methods: 该方法首先自动检测时间序列中主要的周期性，然后使用周期偏移和频谱浓度相似度度量来学习具有周期性的表征。</li>
<li>results: 通过对常见时间序列分类、预测和异常检测任务进行广泛的实验，我们证明了 Floss 方法可以自动捕捉时间序列中的周期性，并在深度学习模型中提高表征性能。<details>
<summary>Abstract</summary>
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, forecasting, and anomaly detection tasks to demonstrate the effectiveness of Floss. We incorporate Floss into several representative deep learning solutions to justify our design choices and demonstrate that it is capable of automatically discovering periodic dynamics and improving state-of-the-art deep learning models.
</details>
<details>
<summary>摘要</summary>
时间序列分析是各个应用领域的基本任务，深度学习方法在这个领域表现出了惊人的表现。然而，许多实际时间序列数据具有重要的周期或半周期动力学特性，这些特性经常由现有的深度学习基于解决方案不充分捕捉。这会导致时间序列中的下一步动力学行为的捕捉不准确。为解决这个问题，我们提出了一种不supervised方法called Floss，它可以自动在频域中规范学习的结果。Floss方法首先自动检测时间序列中的主要周期性。然后，它使用周期偏移和频谱密度相似度度量来学习具有周期一致性的有意义表示。此外，Floss可以轻松地被 incorporated 到supervised、semi-supervised和Unsupervised learning框架中。我们在常见的时间序列分类、预测和异常检测任务中进行了广泛的实验，以示Floss的有效性。我们将Floss与一些代表性的深度学习解决方案结合，以证明我们的设计选择和证明Floss可以自动找到周期动力学和提高现有的深度学习模型。
</details></li>
</ul>
<hr>
<h2 id="MDT3D-Multi-Dataset-Training-for-LiDAR-3D-Object-Detection-Generalization"><a href="#MDT3D-Multi-Dataset-Training-for-LiDAR-3D-Object-Detection-Generalization" class="headerlink" title="MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization"></a>MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01000">http://arxiv.org/abs/2308.01000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Soum-Fontez, Jean-Emmanuel Deschaud, François Goulette</li>
<li>for: 这个研究旨在提高3D物体检测模型在新环境中的Robustness，使其能够更好地适应不同的感测器配置和数据来源。</li>
<li>methods: 这个研究使用了Multi-Dataset Training for 3D Object Detection（MDT3D）方法，利用多个标注的源数据集来增强3D物体检测模型的适应能力。另外，这个方法还使用了一新的标签映射技术来填充标签之间的差异，以及一种新的跨数据集增强技术：跨数据集物体注入。</li>
<li>results: 这个研究发现，这个MDT3D方法可以对不同类型的3D物体检测模型进行改进，并且可以增强这些模型在新环境中的适应能力。<details>
<summary>Abstract</summary>
Supervised 3D Object Detection models have been displaying increasingly better performance in single-domain cases where the training data comes from the same environment and sensor as the testing data. However, in real-world scenarios data from the target domain may not be available for finetuning or for domain adaptation methods. Indeed, 3D object detection models trained on a source dataset with a specific point distribution have shown difficulties in generalizing to unseen datasets. Therefore, we decided to leverage the information available from several annotated source datasets with our Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the robustness of 3D object detection models when tested in a new environment with a different sensor configuration. To tackle the labelling gap between datasets, we used a new label mapping based on coarse labels. Furthermore, we show how we managed the mix of datasets during training and finally introduce a new cross-dataset augmentation method: cross-dataset object injection. We demonstrate that this training paradigm shows improvements for different types of 3D object detection models. The source code and additional results for this research project will be publicly available on GitHub for interested parties to access and utilize: https://github.com/LouisSF/MDT3D
</details>
<details>
<summary>摘要</summary>
超级vised 3D对象检测模型在单个领域情况下显示出了不断提高的性能，其中训练数据和测试数据来自同一个环境和感知器。然而，在实际应用场景中，目标领域的数据可能无法用于训练或适应方法。实际上，3D对象检测模型在特定点分布的训练集上显示了难以泛化到未看过的集合。因此，我们决定利用多个注释源集合的信息来提高3D对象检测模型在新环境中测试时的Robustness。为了 bridge label gap между集合，我们使用了新的标签映射基于粗略标签。此外，我们详细介绍了在训练中管理多个集合的方法，并引入了一种新的交叉集 augmentation 方法：交叉集对象注入。我们表明这种训练方法在不同类型的3D对象检测模型上显示出了改进。详细的源代码和更多结果将在 GitHub 上公开，欢迎有兴趣的朋友们来到 GitHub 上获取和利用：https://github.com/LouisSF/MDT3D。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Synthetic-Data-for-Data-Imbalance-Problems-Baselines-from-a-Data-Perspective"><a href="#Exploiting-Synthetic-Data-for-Data-Imbalance-Problems-Baselines-from-a-Data-Perspective" class="headerlink" title="Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective"></a>Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00994">http://arxiv.org/abs/2308.00994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, Nayeong Kim, Suha Kwak, Tae-Hyun Oh</li>
<li>for: addressing the challenges of data imbalance in deep neural networks</li>
<li>methods: utilizes synthetic data as a preliminary step before employing task-specific algorithms</li>
<li>results: impressive performance on various datasets, surpassing existing task-specific methods<details>
<summary>Abstract</summary>
We live in a vast ocean of data, and deep neural networks are no exception to this. However, this data exhibits an inherent phenomenon of imbalance. This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social consequences. To address these challenges, we believe that the use of generative models is a promising approach for comprehending tasks, given the remarkable advancements demonstrated by recent diffusion models in generating high-quality images. In this work, we propose a simple yet effective baseline, SYNAuG, that utilizes synthetic data as a preliminary step before employing task-specific algorithms to address data imbalance problems. This straightforward approach yields impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbird, surpassing the performance of existing task-specific methods. While we do not claim that our approach serves as a complete solution to the problem of data imbalance, we argue that supplementing the existing data with synthetic data proves to be an effective and crucial preliminary step in addressing data imbalance concerns.
</details>
<details>
<summary>摘要</summary>
我们生活在一个庞大的数据海洋中，深度神经网络也不例外。然而，这些数据具有内在的不均衡现象，这可能导致深度神经网络预测结果受到偏见的影响，从而导致严重的伦理和社会问题。为解决这些挑战，我们认为使用生成模型是一种有前途的方法，因为最近的扩散模型在生成高质量图像方面已经达到了很高的水平。在这个工作中，我们提出了一个简单 yet有效的基线方案，即SYNAuG，它利用生成的数据作为先决步骤，然后使用任务特定的算法来解决数据不均衡问题。这种简单的方法在CIFAR100-LT、ImageNet100-LT、UTKFace和Waterbird等数据集上达到了较高的性能，超过了现有的任务特定方法的表现。虽然我们不能声称我们的方法是完全解决数据不均衡问题的解决方案，但我们认为在补充现有数据时使用生成的数据是一种有效和重要的先决步骤。
</details></li>
</ul>
<hr>
<h2 id="Wasserstein-Diversity-Enriched-Regularizer-for-Hierarchical-Reinforcement-Learning"><a href="#Wasserstein-Diversity-Enriched-Regularizer-for-Hierarchical-Reinforcement-Learning" class="headerlink" title="Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning"></a>Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00989">http://arxiv.org/abs/2308.00989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haorui Li, Jiaqi Liang, Linjing Li, Daniel Zeng</li>
<li>for: 这个论文旨在提出一种新的自动发现策略，用于解决现有的升降问题，以提高复杂任务的执行性。</li>
<li>methods: 该论文使用了 Hierarchical Reinforcement Learning（层次优化学习）和 Automated Subpolicies Discovery（自动发现策略）等方法，并提出了一种新的任务无关的正则化器 called Wasserstein Diversity-Enriched Regularizer（WDER），用于提高策略的多样性。</li>
<li>results: 实验结果表明，使用提出的WDER可以更好地提高性能和样本效率，而无需修改超参数，这表明了WDER的可应用性和稳定性。<details>
<summary>Abstract</summary>
Hierarchical reinforcement learning composites subpolicies in different hierarchies to accomplish complex tasks.Automated subpolicies discovery, which does not depend on domain knowledge, is a promising approach to generating subpolicies.However, the degradation problem is a challenge that existing methods can hardly deal with due to the lack of consideration of diversity or the employment of weak regularizers. In this paper, we propose a novel task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer (WDER), which enlarges the diversity of subpolicies by maximizing the Wasserstein distances among action distributions. The proposed WDER can be easily incorporated into the loss function of existing methods to boost their performance further.Experimental results demonstrate that our WDER improves performance and sample efficiency in comparison with prior work without modifying hyperparameters, which indicates the applicability and robustness of the WDER.
</details>
<details>
<summary>摘要</summary>
hierarchical reinforcement learning 将不同层次的 subpolicies  compose 以完成复杂任务。自动发现 subpolicies 的方法，不виси于Domain Knowledge，是一种有前途的approach。然而，降低问题是现有方法难以处理的挑战，这是因为现有方法缺乏多样性或使用弱正则化。在这篇论文中，我们提出了一种新的任务无关的正则化方法，called Wasserstein Diversity-Enriched Regularizer (WDER)，它通过最大化 Wasserstein 距离来增大 subpolicies 的多样性。我们的 WDER 可以轻松地与现有方法的损失函数结合使用，以提高其性能。实验结果表明，我们的 WDER 可以提高性能和样本效率，而无需修改超参数，这表明了我们的 WDER 的可行性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Regionalization-within-a-Differentiable-High-Resolution-Hydrological-Model-using-Accurate-Spatial-Cost-Gradients"><a href="#Learning-Regionalization-within-a-Differentiable-High-Resolution-Hydrological-Model-using-Accurate-Spatial-Cost-Gradients" class="headerlink" title="Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients"></a>Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02040">http://arxiv.org/abs/2308.02040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngo Nghi Truyen Huynh, Pierre-André Garambois, François Colleoni, Benjamin Renard, Hélène Roux, Julie Demargne, Pierre Javelle<br>for:This paper is written to address the challenging problem of estimating spatially distributed hydrological parameters in ungauged catchments, and to propose a new approach called Hybrid Data Assimilation and Parameter Regionalization (HDA-PR) that incorporates learnable regionalization mappings.methods:The HDA-PR approach uses a differentiable hydrological model and incorporates multivariate regressions or neural networks to regionalize the model parameters. The approach also uses adjoint-based gradients to account for information from multiple observation sites and to tackle the inverse problem of calibrating the model parameters.results:The HDA-PR approach was tested on two flash-flood-prone areas in the South of France, and the results showed a strong regionalization performance, with median Nash-Sutcliffe efficiency (NSE) scores ranging from 0.52 to 0.78 at pseudo-ungauged sites over calibration and validation periods. The approach improved the NSE by up to 0.57 compared to the baseline model calibrated with lumped parameters, and achieved a performance comparable to the reference solution obtained with local uniform calibration.<details>
<summary>Abstract</summary>
Estimating spatially distributed hydrological parameters in ungauged catchments poses a challenging regionalization problem and requires imposing spatial constraints given the sparsity of discharge data. A possible approach is to search for a transfer function that quantitatively relates physical descriptors to conceptual model parameters. This paper introduces a Hybrid Data Assimilation and Parameter Regionalization (HDA-PR) approach incorporating learnable regionalization mappings, based on either multivariate regressions or neural networks, into a differentiable hydrological model. It enables the exploitation of heterogeneous datasets across extensive spatio-temporal computational domains within a high-dimensional regionalization context, using accurate adjoint-based gradients. The inverse problem is tackled with a multi-gauge calibration cost function accounting for information from multiple observation sites. HDA-PR was tested on high-resolution, hourly and kilometric regional modeling of two flash-flood-prone areas located in the South of France. In both study areas, the median Nash-Sutcliffe efficiency (NSE) scores ranged from 0.52 to 0.78 at pseudo-ungauged sites over calibration and validation periods. These results highlight a strong regionalization performance of HDA-PR, improving NSE by up to 0.57 compared to the baseline model calibrated with lumped parameters, and achieving a performance comparable to the reference solution obtained with local uniform calibration (median NSE from 0.59 to 0.79). Multiple evaluation metrics based on flood-oriented hydrological signatures are also employed to assess the accuracy and robustness of the approach. The regionalization method is amenable to state-parameter correction from multi-source data over a range of time scales needed for operational data assimilation, and it is adaptable to other differentiable geophysical models.
</details>
<details>
<summary>摘要</summary>
估算分布式ydrological参数在无测站catchments中存在一个挑战性的区域化问题，需要在缺乏流量数据的情况下强制实施空间约束。本文提出了一种Hybrid Data Assimilation and Parameter Regionalization（HDA-PR）方法，该方法将学习的区域化映射纳入可微分ydrological模型中，使得在广泛的空间和时间域内对高维区域化问题进行可靠的逻辑拟合。该方法可以利用多种多样的数据来源，包括多个观测站，并且可以在不同的时间尺度上进行数据适应。在高分辨率、小时间间隔和kilometric级别上对两个暴雨洪水频发区域进行了实验，结果表明HDA-PR方法在 pseudo-无测站 sites 上的 median Nash-Sutcliffe efficiency（NSE）分布在0.52-0.78之间，比基线模型强制Parameter regionalization 提高了0.57个NSE分数。这些结果表明HDA-PR方法在区域化性能方面具有强大的表现，与参照解（median NSE从0.59到0.79）相当。此外，多种流水学量的评估指标也被利用，以评估方法的准确性和稳定性。该区域化方法可以在多个数据源上进行状态-参数修正，并且适用于其他可微分地球物理模型。
</details></li>
</ul>
<hr>
<h2 id="Certified-Multi-Fidelity-Zeroth-Order-Optimization"><a href="#Certified-Multi-Fidelity-Zeroth-Order-Optimization" class="headerlink" title="Certified Multi-Fidelity Zeroth-Order Optimization"></a>Certified Multi-Fidelity Zeroth-Order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00978">http://arxiv.org/abs/2308.00978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Étienne de Montbrun, Sébastien Gerchinovitz</li>
<li>for: 这个论文主要针对的问题是多级别质量零次优化问题，即可以评估函数 f 在不同级别的 aproximation 水平（具有不同的成本），并且目标是使用最低成本来优化 f。</li>
<li>methods: 这个论文提出了证明的算法，即在证明算法和评估环境之间的游戏中，对函数 f 进行优化。这种证明算法的成本复杂度被证明为对于任何 lipschitz 函数 f，是near-optimal的。</li>
<li>results: 这个论文提出了一种证明算法，并且证明了这种算法的成本复杂度在对于任何 lipschitz 函数 f 时是near-optimal的。此外，论文还证明了这种算法在噪声评估环境下的性能。<details>
<summary>Abstract</summary>
We consider the problem of multi-fidelity zeroth-order optimization, where one can evaluate a function $f$ at various approximation levels (of varying costs), and the goal is to optimize $f$ with the cheapest evaluations possible. In this paper, we study \emph{certified} algorithms, which are additionally required to output a data-driven upper bound on the optimization error. We first formalize the problem in terms of a min-max game between an algorithm and an evaluation environment. We then propose a certified variant of the MFDOO algorithm and derive a bound on its cost complexity for any Lipschitz function $f$. We also prove an $f$-dependent lower bound showing that this algorithm has a near-optimal cost complexity. We close the paper by addressing the special case of noisy (stochastic) evaluations as a direct example.
</details>
<details>
<summary>摘要</summary>
我们考虑多项调度零项优化问题，其中可以评估函数 $f$ 在不同的近似水平（具有不同的成本）上，并且目标是将 $f$ 优化到最低成本为可能。在这篇论文中，我们研究认证算法，它们还需要输出一个基于数据的上限 bounds 估计优化误差。我们首先将问题正式化为一个算法和评估环境之间的最小最大游戏。然后，我们提出了认证版本的 MFDOO 算法，并derive了这个算法对任何 Lipschitz 函数 $f$ 的成本复杂度上限。此外，我们证明了一个 $f$-dependent 下界，证明这个算法具有近乎最佳的成本复杂度。最后，我们处理了随机（测度）评估的特殊情况作为直接例子。
</details></li>
</ul>
<hr>
<h2 id="A-new-approach-for-evaluating-internal-cluster-validation-indices"><a href="#A-new-approach-for-evaluating-internal-cluster-validation-indices" class="headerlink" title="A new approach for evaluating internal cluster validation indices"></a>A new approach for evaluating internal cluster validation indices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03894">http://arxiv.org/abs/2308.03894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zoltán Botta-Dukát</li>
<li>for: 本研究旨在提供一种内部验证指标来选择最佳的无监督分类算法和参数设置。</li>
<li>methods: 本研究评论了一些内部验证指标的方法，包括使用known cluster structure的数据集进行评估。</li>
<li>results: 本研究提出了一种新的验证方法，并评估了其优劣点。<details>
<summary>Abstract</summary>
A vast number of different methods are available for unsupervised classification. Since no algorithm and parameter setting performs best in all types of data, there is a need for cluster validation to select the actually best-performing algorithm. Several indices were proposed for this purpose without using any additional (external) information. These internal validation indices can be evaluated by applying them to classifications of datasets with a known cluster structure. Evaluation approaches differ in how they use the information on the ground-truth classification. This paper reviews these approaches, considering their advantages and disadvantages, and then suggests a new approach.
</details>
<details>
<summary>摘要</summary>
很多不同的方法可以用于无监督分类。由于不同的算法和参数设置在不同的数据上不一定都表现最佳，因此需要使用集群验证来选择实际最佳表现的算法。许多内部验证指标已经被提议，但这些指标不使用外部信息。这些验证指标可以通过应用于知道的分类结构的数据来评估。本文将评论这些方法，包括其优缺点，然后建议一种新的方法。Here's the translation of the text with some additional information about the Simplified Chinese translation:Simplified Chinese is a romanization of Mandarin Chinese that uses a simplified set of characters and grammar rules to represent the language. It is widely used in mainland China and other countries where Mandarin is spoken.In the translation, I have used the Simplified Chinese characters and grammar to represent the text. However, please note that the translation may not be exact, as there are some nuances and subtleties in the original text that may not be fully captured by the Simplified Chinese translation.
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Daily-News-Sentiment-on-Stock-Price-Forecasting"><a href="#Effects-of-Daily-News-Sentiment-on-Stock-Price-Forecasting" class="headerlink" title="Effects of Daily News Sentiment on Stock Price Forecasting"></a>Effects of Daily News Sentiment on Stock Price Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08549">http://arxiv.org/abs/2308.08549</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Srinivas, R. Gadela, R. Sabu, A. Das, G. Nath, V. Datla</li>
<li>for: 这篇论文的目的是设计一个高效的新闻听说系统，以捕捉NITY50股票的新闻听说，并研究这些新闻听说对股票价格的影响。</li>
<li>methods: 这篇论文使用了一个Robust数据采集和处理框架，创建了一个新闻数据库，包含了约半百万篇新闻文章，以及相应的股票价格信息。其中，使用了不同的情感库计算不同部分的情感分数。</li>
<li>results: 这篇论文通过使用不同的LSTM模型，对股票价格进行预测，并 Compares their performances with and without using sentiment scores as features.<details>
<summary>Abstract</summary>
Predicting future prices of a stock is an arduous task to perform. However, incorporating additional elements can significantly improve our predictions, rather than relying solely on a stock's historical price data to forecast its future price. Studies have demonstrated that investor sentiment, which is impacted by daily news about the company, can have a significant impact on stock price swings. There are numerous sources from which we can get this information, but they are cluttered with a lot of noise, making it difficult to accurately extract the sentiments from them. Hence the focus of our research is to design an efficient system to capture the sentiments from the news about the NITY50 stocks and investigate how much the financial news sentiment of these stocks are affecting their prices over a period of time. This paper presents a robust data collection and preprocessing framework to create a news database for a timeline of around 3.7 years, consisting of almost half a million news articles. We also capture the stock price information for this timeline and create multiple time series data, that include the sentiment scores from various sections of the article, calculated using different sentiment libraries. Based on this, we fit several LSTM models to forecast the stock prices, with and without using the sentiment scores as features and compare their performances.
</details>
<details>
<summary>摘要</summary>
Our research focuses on designing an efficient system to capture sentiments from news about NITY50 stocks and investigating how much financial news sentiment affects their prices over time. We present a robust data collection and preprocessing framework to create a news database spanning 3.7 years, consisting of nearly half a million news articles. We also collect stock price information for this timeline and create multiple time series data, including sentiment scores from various sections of the article calculated using different sentiment libraries.We fit several LSTM models to forecast stock prices, with and without using sentiment scores as features, and compare their performances. This paper provides a comprehensive framework for incorporating investor sentiment into stock price predictions, and demonstrates the effectiveness of our approach using a large and diverse dataset.
</details></li>
</ul>
<hr>
<h2 id="Integrating-Homomorphic-Encryption-and-Trusted-Execution-Technology-for-Autonomous-and-Confidential-Model-Refining-in-Cloud"><a href="#Integrating-Homomorphic-Encryption-and-Trusted-Execution-Technology-for-Autonomous-and-Confidential-Model-Refining-in-Cloud" class="headerlink" title="Integrating Homomorphic Encryption and Trusted Execution Technology for Autonomous and Confidential Model Refining in Cloud"></a>Integrating Homomorphic Encryption and Trusted Execution Technology for Autonomous and Confidential Model Refining in Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00963">http://arxiv.org/abs/2308.00963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pinglan Liu, Wensheng Zhang</li>
<li>For: This paper aims to design a scheme for autonomous and confidential model refining in cloud computing, to address security and privacy concerns while continuously improving model accuracy.* Methods: The proposed scheme utilizes homomorphic encryption and trusted execution environment technology to protect confidentiality during autonomous computation, and integrates these two techniques to enhance the scheme’s feasibility and efficiency.* Results: The proposed scheme allows the cloud server to autonomously refine an encrypted model with newly provided encrypted training data, continuously improving its accuracy. However, the efficiency is still lower than the baseline scheme, and can be improved by fully utilizing the higher level of parallelism and computational power of GPU at the cloud server.Here is the Chinese version of the three key points:* For: 本文提出了一种用于云计算中自动化和保密模型优化的方案，以解决安全和隐私问题，同时不断提高模型精度。* Methods: 该方案利用了同知加密和可信执行环境技术来保护数据和模型的Confidentiality，并将这两种技术相互衔接以提高方案的可行性和效率。* Results: 该方案使得云服务器可以自动地使用新提供的加密训练数据来加密模型，不断提高其精度。然而，效率仍然远低于基线方案，可以通过全面利用云服务器的高级并行和GPU的计算能力来进一步提高效率。<details>
<summary>Abstract</summary>
With the popularity of cloud computing and machine learning, it has been a trend to outsource machine learning processes (including model training and model-based inference) to cloud. By the outsourcing, other than utilizing the extensive and scalable resource offered by the cloud service provider, it will also be attractive to users if the cloud servers can manage the machine learning processes autonomously on behalf of the users. Such a feature will be especially salient when the machine learning is expected to be a long-term continuous process and the users are not always available to participate. Due to security and privacy concerns, it is also desired that the autonomous learning preserves the confidentiality of users' data and models involved. Hence, in this paper, we aim to design a scheme that enables autonomous and confidential model refining in cloud. Homomorphic encryption and trusted execution environment technology can protect confidentiality for autonomous computation, but each of them has their limitations respectively and they are complementary to each other. Therefore, we further propose to integrate these two techniques in the design of the model refining scheme. Through implementation and experiments, we evaluate the feasibility of our proposed scheme. The results indicate that, with our proposed scheme the cloud server can autonomously refine an encrypted model with newly provided encrypted training data to continuously improve its accuracy. Though the efficiency is still significantly lower than the baseline scheme that refines plaintext-model with plaintext-data, we expect that it can be improved by fully utilizing the higher level of parallelism and the computational power of GPU at the cloud server.
</details>
<details>
<summary>摘要</summary>
With the popularity of cloud computing and machine learning, it has become a trend to outsource machine learning processes (including model training and model-based inference) to the cloud. By outsourcing, users can not only utilize the extensive and scalable resources offered by the cloud service provider but also enjoy the convenience of autonomous management of machine learning processes by the cloud servers. This feature is especially important when machine learning is expected to be a long-term continuous process and users are not always available to participate. However, due to security and privacy concerns, it is essential to ensure that the autonomous learning preserves the confidentiality of users' data and models involved. Therefore, in this paper, we aim to design a scheme that enables autonomous and confidential model refining in the cloud. Homomorphic encryption and trusted execution environment technology can protect the confidentiality of autonomous computation, but each of them has its limitations, respectively. Therefore, we propose to integrate these two techniques in the design of the model refining scheme. Through implementation and experiments, we evaluate the feasibility of our proposed scheme. The results show that the cloud server can autonomously refine an encrypted model with newly provided encrypted training data to continuously improve its accuracy. Although the efficiency is still significantly lower than the baseline scheme that refines plaintext-model with plaintext-data, we expect that it can be improved by fully utilizing the higher level of parallelism and the computational power of GPU at the cloud server.Here's the translation in Traditional Chinese as well: With the popularity of cloud computing and machine learning, it has become a trend to outsource machine learning processes (including model training and model-based inference) to the cloud. By outsourcing, users can not only utilize the extensive and scalable resources offered by the cloud service provider but also enjoy the convenience of autonomous management of machine learning processes by the cloud servers. This feature is especially important when machine learning is expected to be a long-term continuous process and users are not always available to participate. However, due to security and privacy concerns, it is essential to ensure that the autonomous learning preserves the confidentiality of users' data and models involved. Therefore, in this paper, we aim to design a scheme that enables autonomous and confidential model refining in the cloud. Homomorphic encryption and trusted execution environment technology can protect the confidentiality of autonomous computation, but each of them has its limitations, respectively. Therefore, we propose to integrate these two techniques in the design of the model refining scheme. Through implementation and experiments, we evaluate the feasibility of our proposed scheme. The results show that the cloud server can autonomously refine an encrypted model with newly provided encrypted training data to continuously improve its accuracy. Although the efficiency is still significantly lower than the baseline scheme that refines plaintext-model with plaintext-data, we expect that it can be improved by fully utilizing the higher level of parallelism and the computational power of GPU at the cloud server.
</details></li>
</ul>
<hr>
<h2 id="Causal-Inference-with-Differentially-Private-Clustered-Outcomes"><a href="#Causal-Inference-with-Differentially-Private-Clustered-Outcomes" class="headerlink" title="Causal Inference with Differentially Private (Clustered) Outcomes"></a>Causal Inference with Differentially Private (Clustered) Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00957">http://arxiv.org/abs/2308.00957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Javanmard, Vahab Mirrokni, Jean Pouget-Abadie</li>
<li>For: The paper focuses on developing a new differential privacy mechanism, “Cluster-DP”, to improve the variance of causal effect estimation while maintaining strong privacy guarantees.* Methods: The proposed method leverages any given cluster structure of the data to reduce the variance loss and improve the privacy guarantees. The algorithm uses a novel privacy-variance trade-off to achieve a balance between privacy and accuracy.* Results: The paper shows that the proposed “Cluster-DP” algorithm outperforms its unclustered version and a more extreme uniform-prior version in terms of variance loss, while maintaining the same privacy guarantees. The results demonstrate the effectiveness of the proposed method in achieving lower variance for stronger privacy guarantees.<details>
<summary>Abstract</summary>
Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm's privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, "Cluster-DP", which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending on an intuitive measure of cluster quality, we can improve the variance loss while maintaining our privacy guarantees. We compare its performance, theoretically and empirically, to that of its unclustered version and a more extreme uniform-prior version which does not use any of the original response distribution, both of which are special cases of the "Cluster-DP" algorithm.
</details>
<details>
<summary>摘要</summary>
估计 causal effect from randomized experiments 只能实现 if participants agree to reveal their potentially sensitive responses. 保护 participants' privacy 的多种方法之一是 label differential privacy, 这种方法可能会鼓励 participants 分享 responses  без running the risk of de-anonymization. 多种 differentially private mechanisms 会把 noise 注入到原始数据集中以实现这种 privacy guarantee, 这会增加 most statistical estimators 的变量和 makes the precise measurement of causal effects difficult. 在进行 causal analyses from differentially private data 时, there exists a fundamental privacy-variance trade-off.为了实现 lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, "Cluster-DP", which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. we show that, depending on an intuitive measure of cluster quality, we can improve the variance loss while maintaining our privacy guarantees. we compare its performance, theoretically and empirically, to that of its unclustered version and a more extreme uniform-prior version which does not use any of the original response distribution, both of which are special cases of the "Cluster-DP" algorithm.
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Guided-Domain-Adaptation-in-the-Dark"><a href="#Curriculum-Guided-Domain-Adaptation-in-the-Dark" class="headerlink" title="Curriculum Guided Domain Adaptation in the Dark"></a>Curriculum Guided Domain Adaptation in the Dark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00956">http://arxiv.org/abs/2308.00956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chowdhury Sadman Jahan, Andreas Savakis</li>
<li>for: 这个研究的目的是为了解决隐私和安全性的问题，将黑盒模型训练到目标领域中而不需要任何源数据或源模型参数。</li>
<li>methods: 这个研究使用了curriculum guided adaptation approach，首先在目标数据上训练预设值很高的clean标签，然后在这些标签上训练另一个分支网络以抑制错误的累累。此外，研究使用了Jensen-Shannon数值来分类清洁和杂音标签，并且不需要任何额外的调整阶段。</li>
<li>results: 研究结果显示，CABB比现有的黑盒领域域整合模型优化性能更好，并且与白盒域整合模型优化性能相似。<details>
<summary>Abstract</summary>
Addressing the rising concerns of privacy and security, domain adaptation in the dark aims to adapt a black-box source trained model to an unlabeled target domain without access to any source data or source model parameters. The need for domain adaptation of black-box predictors becomes even more pronounced to protect intellectual property as deep learning based solutions are becoming increasingly commercialized. Current methods distill noisy predictions on the target data obtained from the source model to the target model, and/or separate clean/noisy target samples before adapting using traditional noisy label learning algorithms. However, these methods do not utilize the easy-to-hard learning nature of the clean/noisy data splits. Also, none of the existing methods are end-to-end, and require a separate fine-tuning stage and an initial warmup stage. In this work, we present Curriculum Adaptation for Black-Box (CABB) which provides a curriculum guided adaptation approach to gradually train the target model, first on target data with high confidence (clean) labels, and later on target data with noisy labels. CABB utilizes Jensen-Shannon divergence as a better criterion for clean-noisy sample separation, compared to the traditional criterion of cross entropy loss. Our method utilizes co-training of a dual-branch network to suppress error accumulation resulting from confirmation bias. The proposed approach is end-to-end trainable and does not require any extra finetuning stage, unlike existing methods. Empirical results on standard domain adaptation datasets show that CABB outperforms existing state-of-the-art black-box DA models and is comparable to white-box domain adaptation models.
</details>
<details>
<summary>摘要</summary>
In this work, we present Curriculum Adaptation for Black-Box (CABB), which provides a curriculum-guided adaptation approach to gradually train the target model. First, it trains the target model on target data with high confidence (clean) labels, and later on target data with noisy labels. CABB utilizes Jensen-Shannon divergence as a better criterion for clean-noisy sample separation compared to the traditional criterion of cross-entropy loss. Our method utilizes co-training of a dual-branch network to suppress error accumulation resulting from confirmation bias. The proposed approach is end-to-end trainable and does not require any extra fine-tuning stage, unlike existing methods.Empirical results on standard domain adaptation datasets show that CABB outperforms existing state-of-the-art black-box DA models and is comparable to white-box domain adaptation models.
</details></li>
</ul>
<hr>
<h2 id="From-Sparse-to-Soft-Mixtures-of-Experts"><a href="#From-Sparse-to-Soft-Mixtures-of-Experts" class="headerlink" title="From Sparse to Soft Mixtures of Experts"></a>From Sparse to Soft Mixtures of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00951">http://arxiv.org/abs/2308.00951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/vmoe">https://github.com/google-research/vmoe</a></li>
<li>paper_authors: Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Neil Houlsby</li>
<li>for: 这 paper 的目的是提出一种可 diferenciable 的异步 Mixture of Experts（Soft MoE）模型，以解决 MoE 模型在训练和推理过程中的稳定性和效果问题。</li>
<li>methods: Soft MoE 使用了一种隐式软分配方法，通过将不同权重的输入 токен传递给每个专家，实现了隐藏状态的共享和模型的扩展。</li>
<li>results: 在视觉识别任务中，Soft MoE 与标准 Transformer 和 популяр的 MoE 变体（Token Choice 和 Experts Choice）进行了比较，并取得了substantially better的性能。例如，Soft MoE-Base&#x2F;16 只需要10.5倍的推理成本（5.7倍的wall-clock时间）和 ViT-Huge&#x2F;14 相同的训练效果。此外，Soft MoE 还可以扩展到大型模型，Soft MoE Huge&#x2F;14  WITH 128 专家在 16 个 MoE 层中，与 ViT Huge&#x2F;14 的参数数量相比，只有2% 的推理时间成本增加。<details>
<summary>Abstract</summary>
Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40x more parameters than ViT Huge/14, while inference time cost grows by only 2%, and it performs substantially better.
</details>
<details>
<summary>摘要</summary>
稀疏混合专家架构（MoE）可以增加模型容量而无需大幅提高训练或执行成本。尽管它们取得了成功，但MoE受到一些问题的限制：训练不稳定、减少токен、不能扩展专家数量、或者训练不 efective。在这项工作中，我们提出了软MoE，一种完全可微分的稀疏变换器，解决了这些挑战，同时保留MoE的优点。软MoE通过不同权重的Weighted combinations passing所有输入 токен给每个专家进行隐式软分配。与其他MoE工作一样，专家在Soft MoE中只处理一 subset of (combined)  токен，使得模型容量可以在更低的执行成本下增加。在视觉认知上，软MoE大幅超越标准转换器（ViTs）和流行的MoE变种（Tokens Choice和Experts Choice）。例如，Soft MoE-Base/16需要10.5倍更低的执行成本（5.7倍更低的墙 clock时间），与ViT-Huge/14匹配其性能，同时需要相似的训练。软MoE还可扩展：Soft MoE Huge/14 WITH 128专家在16个MoE层中有40倍更多的参数，而执行成本上增加的成本只增加2%，并且表现出substantially better。
</details></li>
</ul>
<hr>
<h2 id="Decomposing-and-Coupling-Saliency-Map-for-Lesion-Segmentation-in-Ultrasound-Images"><a href="#Decomposing-and-Coupling-Saliency-Map-for-Lesion-Segmentation-in-Ultrasound-Images" class="headerlink" title="Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images"></a>Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00947">http://arxiv.org/abs/2308.00947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyuan Ning, Yixiao Mao, Qianjin Feng, Shengzhou Zhong, Yu Zhang</li>
<li>for: Accurate lesion segmentation in ultrasound images, particularly in the presence of complex background textures.</li>
<li>methods: Decomposition-coupling network (DC-Net) that disentangles foreground and background saliency maps, followed by fusion of regional features, relation-aware representation fusion, and dependency-aware prior incorporation.</li>
<li>results: Remarkable performance improvement over existing state-of-the-art methods on two ultrasound lesion segmentation tasks.<details>
<summary>Abstract</summary>
Complex scenario of ultrasound image, in which adjacent tissues (i.e., background) share similar intensity with and even contain richer texture patterns than lesion region (i.e., foreground), brings a unique challenge for accurate lesion segmentation. This work presents a decomposition-coupling network, called DC-Net, to deal with this challenge in a (foreground-background) saliency map disentanglement-fusion manner. The DC-Net consists of decomposition and coupling subnets, and the former preliminarily disentangles original image into foreground and background saliency maps, followed by the latter for accurate segmentation under the assistance of saliency prior fusion. The coupling subnet involves three aspects of fusion strategies, including: 1) regional feature aggregation (via differentiable context pooling operator in the encoder) to adaptively preserve local contextual details with the larger receptive field during dimension reduction; 2) relation-aware representation fusion (via cross-correlation fusion module in the decoder) to efficiently fuse low-level visual characteristics and high-level semantic features during resolution restoration; 3) dependency-aware prior incorporation (via coupler) to reinforce foreground-salient representation with the complementary information derived from background representation. Furthermore, a harmonic loss function is introduced to encourage the network to focus more attention on low-confidence and hard samples. The proposed method is evaluated on two ultrasound lesion segmentation tasks, which demonstrates the remarkable performance improvement over existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
复杂的ultrasound图像场景下，邻近组织（即背景）与病变区域（即前景）的INTENSITY具有相似的特征，甚至含有更加丰富的тексту征，这带来了精确病变部细分的搜索挑战。本文提出了一种 decomposition-coupling网络（DC-Net），通过对原始图像进行前期分解和聚合来解决这个挑战。DC-Net包括分解和聚合子网络，前者首先分解原始图像为病变和背景的saliency map，然后后者利用这些saliency map进行准确的病变部细分。聚合子网络包括三种融合策略：1）地域特征聚合（通过可微上下文抽取器在encoder中），以适应大尺寸场景下的本地Contextual details; 2）相关性认知表示融合（通过cross-correlation融合模块在decoder中），以高效地融合低级视觉特征和高级 semantics特征; 3）依赖关系的先天约束（通过coupler），以强制前景病变表示与背景表示之间的依赖关系。此外，我们还引入了一种和谐损失函数，以促进网络对低信任和困难样本的更多注意。提议的方法在两个ultrasound病变部细分任务上进行了评估，并显示出与现有状态 искус家技术的remarkable性能提升。
</details></li>
</ul>
<hr>
<h2 id="On-the-use-of-deep-learning-for-phase-recovery"><a href="#On-the-use-of-deep-learning-for-phase-recovery" class="headerlink" title="On the use of deep learning for phase recovery"></a>On the use of deep learning for phase recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00942">http://arxiv.org/abs/2308.00942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiqiang Wang, Li Song, Chutian Wang, Zhenbo Ren, Guangyuan Zhao, Jiazhen Dou, Jianglei Di, George Barbastathis, Renjie Zhou, Jianlin Zhao, Edmund Y. Lam</li>
<li>for: 这篇论文主要是为了探讨深度学习在phas recovery（PR）方面的应用和支持。</li>
<li>methods: 论文首先简要介绍了传统的PR方法，然后详细介绍了深度学习在PR的三个阶段：先processing、进程中处理和后processing中的应用。</li>
<li>results: 论文则 Summarize了在DL中为PR工作的进展，并提供了一个live-updating资源（<a target="_blank" rel="noopener" href="https://github.com/kqwang/phase-recovery%EF%BC%89%EF%BC%8C%E4%BB%A5%E4%BE%BF%E8%AF%BB%E8%80%85%E6%9B%B4%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3PR%E3%80%82">https://github.com/kqwang/phase-recovery），以便读者更深入了解PR。</a><details>
<summary>Abstract</summary>
Phase recovery (PR) refers to calculating the phase of the light field from its intensity measurements. As exemplified from quantitative phase imaging and coherent diffraction imaging to adaptive optics, PR is essential for reconstructing the refractive index distribution or topography of an object and correcting the aberration of an imaging system. In recent years, deep learning (DL), often implemented through deep neural networks, has provided unprecedented support for computational imaging, leading to more efficient solutions for various PR problems. In this review, we first briefly introduce conventional methods for PR. Then, we review how DL provides support for PR from the following three stages, namely, pre-processing, in-processing, and post-processing. We also review how DL is used in phase image processing. Finally, we summarize the work in DL for PR and outlook on how to better use DL to improve the reliability and efficiency in PR. Furthermore, we present a live-updating resource (https://github.com/kqwang/phase-recovery) for readers to learn more about PR.
</details>
<details>
<summary>摘要</summary>
phase recovery (PR) 指的是从光场强度测量中计算光场的阶段。例如从量化阶段影像和相干折射影像到自适应optics，PR 是重要的 для重建对象的反射指数分布或地图，并对投影系统的偏差进行修复。在最近几年，深度学习（DL），通常通过深度神经网络实现，对计算成像提供了前所未有的支持，导致了许多PR问题的更高效的解决方案。在这篇文章中，我们首先简要介绍了传统的PR方法。然后，我们查看了DL 如何在三个阶段提供支持，即预处理、实时处理和后处理。我们还查看了DL 在阶段图像处理中的应用。最后，我们summarize了DL 在PR 方面的工作，并讨论了如何更好地使用DL 提高PR 的可靠性和效率。此外，我们提供了一个Live-updating资源（https://github.com/kqwang/phase-recovery），让读者了解更多关于PR。
</details></li>
</ul>
<hr>
<h2 id="QUANT-A-Minimalist-Interval-Method-for-Time-Series-Classification"><a href="#QUANT-A-Minimalist-Interval-Method-for-Time-Series-Classification" class="headerlink" title="QUANT: A Minimalist Interval Method for Time Series Classification"></a>QUANT: A Minimalist Interval Method for Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00928">http://arxiv.org/abs/2308.00928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angus924/quant">https://github.com/angus924/quant</a></li>
<li>paper_authors: Angus Dempster, Daniel F. Schmidt, Geoffrey I. Webb</li>
<li>for: 这个论文是为了提出一种快速准确的时间序列分类方法。</li>
<li>methods: 这个方法使用了一种单种特征（量iles）、固定间隔和 ‘Off-the-shelf’ 分类器。</li>
<li>results: 这个方法可以在一个标准的测试集上达到与最高精度 interval 方法相同的准确率，并且在142个 UCR 数据集上达到了状态机器的准确率，用于训练和推断的总计算时间仅为15分钟左右，使用单个 CPU 核心。<details>
<summary>Abstract</summary>
We show that it is possible to achieve the same accuracy, on average, as the most accurate existing interval methods for time series classification on a standard set of benchmark datasets using a single type of feature (quantiles), fixed intervals, and an 'off the shelf' classifier. This distillation of interval-based approaches represents a fast and accurate method for time series classification, achieving state-of-the-art accuracy on the expanded set of 142 datasets in the UCR archive with a total compute time (training and inference) of less than 15 minutes using a single CPU core.
</details>
<details>
<summary>摘要</summary>
我们证明可以在标准的测试集上实现同等准确率，而且这种方法可以使用单种特征（quantiles）、固定间隔和 'off the shelf' 分类器实现时间序列分类。这种简化的间隔方法可以快速和准确地进行时间序列分类，在扩展的142个测试集上达到状态的最佳准确率，用单个CPU核心并在训练和推理之间共计使用了 menos than 15分钟的计算时间。
</details></li>
</ul>
<hr>
<h2 id="Continual-Domain-Adaptation-on-Aerial-Images-under-Gradually-Degrading-Weather"><a href="#Continual-Domain-Adaptation-on-Aerial-Images-under-Gradually-Degrading-Weather" class="headerlink" title="Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather"></a>Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00924">http://arxiv.org/abs/2308.00924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sadman-jahan/aid-ucm-degradingweather">https://github.com/sadman-jahan/aid-ucm-degradingweather</a></li>
<li>paper_authors: Chowdhury Sadman Jahan, Andreas Savakis<br>for:*  Mitigate the domain gap between the source domain and the target domain in aerial deployment.methods:*  Synthesize two gradually worsening weather conditions on real images from two existing aerial imagery datasets.*  Evaluate three DA models (baseline standard DA model and two continual DA models) under the continual, or test-time adaptation setting.results:*  Discover stability issues during adaptation for existing buffer-fed continual DA methods.*  Offer gradient normalization as a simple solution to curb training instability.<details>
<summary>Abstract</summary>
Domain adaptation (DA) strives to mitigate the domain gap between the source domain where a model is trained, and the target domain where the model is deployed. When a deep learning model is deployed on an aerial platform, it may face gradually degrading weather conditions during operation, leading to widening domain gaps between the training data and the encountered evaluation data. We synthesize two such gradually worsening weather conditions on real images from two existing aerial imagery datasets, generating a total of four benchmark datasets. Under the continual, or test-time adaptation setting, we evaluate three DA models on our datasets: a baseline standard DA model and two continual DA models. In such setting, the models can access only one small portion, or one batch of the target data at a time, and adaptation takes place continually, and over only one epoch of the data. The combination of the constraints of continual adaptation, and gradually deteriorating weather conditions provide the practical DA scenario for aerial deployment. Among the evaluated models, we consider both convolutional and transformer architectures for comparison. We discover stability issues during adaptation for existing buffer-fed continual DA methods, and offer gradient normalization as a simple solution to curb training instability.
</details>
<details>
<summary>摘要</summary>
域 adaptation (DA) 目标是减少源域和目标域之间的域度差。当深度学习模型在空中平台上部署时，它可能会遇到逐渐恶化的天气条件，导致模型在训练数据和评估数据之间的域度差加大。我们使用两个现有的空中影像集合拼接了两种逐渐恶化的天气条件，生成了四个benchmark集合。在 continual 或 test-time adaptation  Setting，我们评估了三种 DA 模型：基线标准 DA 模型和两种 continual DA 模型。在这种设定下，模型只能访问一小部分或一批目标数据一次，并且适应发生在一个epoch内。combine continual adaptation 和 gradually deteriorating 天气条件提供了实际的 DA enario for aerial deployment。我们考虑了 convolutional 和 transformer 架构进行比较。我们发现了 continual adaptation 时的稳定性问题，并提供了 Gradient normalization 的简单解决方案来缓解训练不稳定。
</details></li>
</ul>
<hr>
<h2 id="Survey-on-Computer-Vision-Techniques-for-Internet-of-Things-Devices"><a href="#Survey-on-Computer-Vision-Techniques-for-Internet-of-Things-Devices" class="headerlink" title="Survey on Computer Vision Techniques for Internet-of-Things Devices"></a>Survey on Computer Vision Techniques for Internet-of-Things Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02553">http://arxiv.org/abs/2308.02553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishmeet Kaur, Adwaita Janardhan Jadhav</li>
<li>for: 本研究旨在提高深度神经网络（DNNs）在低功耗设备上的部署，以提高公共安全。</li>
<li>methods: 本研究使用了低功耗和能效的DNN实现技术，包括神经网络压缩、网络架构搜索和设计、编译器和图优化等。</li>
<li>results: 本研究总结了低功耗DNN实现技术的优缺点和未来研究方向。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are state-of-the-art techniques for solving most computer vision problems. DNNs require billions of parameters and operations to achieve state-of-the-art results. This requirement makes DNNs extremely compute, memory, and energy-hungry, and consequently difficult to deploy on small battery-powered Internet-of-Things (IoT) devices with limited computing resources. Deployment of DNNs on Internet-of-Things devices, such as traffic cameras, can improve public safety by enabling applications such as automatic accident detection and emergency response.Through this paper, we survey the recent advances in low-power and energy-efficient DNN implementations that improve the deployability of DNNs without significantly sacrificing accuracy. In general, these techniques either reduce the memory requirements, the number of arithmetic operations, or both. The techniques can be divided into three major categories: neural network compression, network architecture search and design, and compiler and graph optimizations. In this paper, we survey both low-power techniques for both convolutional and transformer DNNs, and summarize the advantages, disadvantages, and open research problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Virtual-histological-staining-of-unlabeled-autopsy-tissue"><a href="#Virtual-histological-staining-of-unlabeled-autopsy-tissue" class="headerlink" title="Virtual histological staining of unlabeled autopsy tissue"></a>Virtual histological staining of unlabeled autopsy tissue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00920">http://arxiv.org/abs/2308.00920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhu Li, Nir Pillar, Jingxi Li, Tairan Liu, Di Wu, Songyu Sun, Guangdong Ma, Kevin de Haan, Luzhe Huang, Sepehr Hamidi, Anatoly Urisman, Tal Keidar Haran, William Dean Wallace, Jonathan E. Zuckerman, Aydogan Ozcan</li>
<li>for: 这个研究旨在解决传统压潮化方法面临的多种挑战，包括延迟了尸体组织的固定，以及大量的化学染料和大量的劳动力成本。</li>
<li>methods: 这个研究使用了训练 neural network 将自动染料图像转换成快速呈现的明亮场景图像，从而消除传统压潮化方法中的严重染料 artifacts。</li>
<li>results: 这个研究显示，虽然样本经历了严重的自杀和蛋白损害，但虚拟压潮化技术仍能快速和高效地生成 artifact-free H&amp;E 染料，并且可以减少劳动力成本和基础设施需求。<details>
<summary>Abstract</summary>
Histological examination is a crucial step in an autopsy; however, the traditional histochemical staining of post-mortem samples faces multiple challenges, including the inferior staining quality due to autolysis caused by delayed fixation of cadaver tissue, as well as the resource-intensive nature of chemical staining procedures covering large tissue areas, which demand substantial labor, cost, and time. These challenges can become more pronounced during global health crises when the availability of histopathology services is limited, resulting in further delays in tissue fixation and more severe staining artifacts. Here, we report the first demonstration of virtual staining of autopsy tissue and show that a trained neural network can rapidly transform autofluorescence images of label-free autopsy tissue sections into brightfield equivalent images that match hematoxylin and eosin (H&E) stained versions of the same samples, eliminating autolysis-induced severe staining artifacts inherent in traditional histochemical staining of autopsied tissue. Our virtual H&E model was trained using >0.7 TB of image data and a data-efficient collaboration scheme that integrates the virtual staining network with an image registration network. The trained model effectively accentuated nuclear, cytoplasmic and extracellular features in new autopsy tissue samples that experienced severe autolysis, such as COVID-19 samples never seen before, where the traditional histochemical staining failed to provide consistent staining quality. This virtual autopsy staining technique can also be extended to necrotic tissue, and can rapidly and cost-effectively generate artifact-free H&E stains despite severe autolysis and cell death, also reducing labor, cost and infrastructure requirements associated with the standard histochemical staining.
</details>
<details>
<summary>摘要</summary>
histological examination是検死试中的关键步骤，但传统的透明化学染色方法面临多种挑战，包括由尸体组织自逐浸导致的自逐浸破坏，以及涉及大量化学染料、劳动、成本和时间的资源占用。在全球卫生危机期间， histopathology服务的可用性受限，导致组织涂染的延迟和更严重的染料artefacts。在这篇报告中，我们展示了虚拟染色技术的首次应用，使得一种已经训练的神经网络可以快速地将自动染色图像转换成和染料染色版本相同的明亮场景图像，从而消除自逐浸破坏导致的严重染料artefacts。我们的虚拟H&E模型在 >0.7 TB的图像数据和数据效率协作方案的支持下进行训练。训练后，模型能够强调组织涂染中的核、细胞膜和 extracellular 特征，并在新的急死组织样本中表现出优秀的性能，包括COVID-19样本，这些样本在传统的透明化学染色方法下无法保持一致的染料质量。此虚拟染色技术可以扩展到肿瘤组织，并可以快速、成本低、设备简单地生成 artefact-free H&E染料，无论是严重的自逐浸破坏或细胞死亡。
</details></li>
</ul>
<hr>
<h2 id="VLUCI-Variational-Learning-of-Unobserved-Confounders-for-Counterfactual-Inference"><a href="#VLUCI-Variational-Learning-of-Unobserved-Confounders-for-Counterfactual-Inference" class="headerlink" title="VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference"></a>VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00904">http://arxiv.org/abs/2308.00904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Qiang Huang, Siwei Wu, Yun Peng, Huiyan Sun</li>
<li>For: 本研究旨在解决在观察数据中存在未知干扰因素的 causal inference 问题，提高 counterfactual outcome 的准确性。* Methods: 本文提出了一种新的变分学习模型，可以学习未知干扰因素的 posterior distribution，从而改进 counterfactual inference 的准确性。* Results: 对于 synthetic 和 semi-synthetic 数据集，变分学习模型在推断未知干扰因素方面表现出色，与现有的 counterfactual inference 方法相比，具有更高的准确性。  Additionally, the paper provides confidence intervals for counterfactual outcomes, which can aid decision-making in risk-sensitive domains.<details>
<summary>Abstract</summary>
Causal inference plays a vital role in diverse domains like epidemiology, healthcare, and economics. De-confounding and counterfactual prediction in observational data has emerged as a prominent concern in causal inference research. While existing models tackle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and impacting counterfactual outcome accuracy. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes. Extensive experiments on synthetic and semi-synthetic datasets demonstrate VLUCI's superior performance in inferring unobserved confounders. It is compatible with state-of-the-art counterfactual inference models, significantly improving inference accuracy at both group and individual levels. Additionally, VLUCI provides confidence intervals for counterfactual outcomes, aiding decision-making in risk-sensitive domains. We further clarify the considerations when applying VLUCI to cases where unobserved confounders don't strictly conform to our model assumptions using the public IHDP dataset as an example, highlighting the practical advantages of VLUCI.
</details>
<details>
<summary>摘要</summary>
causal inference在多个领域中扮演着重要的角色，如epidemiology、医疗和经济。在观察数据中，去掉和预测Counterfactual outcome Has become a prominent concern in causal inference research. Although existing models can handle observed confounders, the presence of unobserved confounders remains a significant challenge, distorting causal inference and affecting the accuracy of counterfactual outcomes. To address this, we propose a novel variational learning model of unobserved confounders for counterfactual inference (VLUCI), which generates the posterior distribution of unobserved confounders. VLUCI relaxes the unconfoundedness assumption often overlooked by most causal inference methods. By disentangling observed and unobserved confounders, VLUCI constructs a doubly variational inference model to approximate the distribution of unobserved confounders, which are used for inferring more accurate counterfactual outcomes.在实验中，VLUCI在 synthetic 和 semi-synthetic 数据上显示出了更高的 counterfactual inference 精度。它与当前的 state-of-the-art counterfactual inference models 相容，并在 group 和 individual 水平上提高了推断精度。此外，VLUCI还提供了对 counterfactual outcomes 的信任间隔，帮助在风险敏感的领域做出决策。我们还讨论了在 VLUCI 应用时需要考虑的一些因素，使用公共的 IHDP 数据集作为例子， highlighting the practical advantages of VLUCI。
</details></li>
</ul>
<hr>
<h2 id="User-Controllable-Recommendation-via-Counterfactual-Retrospective-and-Prospective-Explanations"><a href="#User-Controllable-Recommendation-via-Counterfactual-Retrospective-and-Prospective-Explanations" class="headerlink" title="User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations"></a>User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00894">http://arxiv.org/abs/2308.00894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisjtan/ucr">https://github.com/chrisjtan/ucr</a></li>
<li>paper_authors: Juntao Tan, Yingqiang Ge, Yan Zhu, Yinglong Xia, Jiebo Luo, Jianchao Ji, Yongfeng Zhang</li>
<li>for: 提高用户满意度和信任度，通过增加用户可控性</li>
<li>methods: 利用解释能力和可控性，在一个统一框架中呈现retrospective和prospective解释，让用户自定义控制系统</li>
<li>results: 在MovieLens和Yelp数据集上进行实验评估，并证明了提posed方案的有效性，同时发现给用户提供控制选项可能会提高未来的推荐精度<details>
<summary>Abstract</summary>
Modern recommender systems utilize users' historical behaviors to generate personalized recommendations. However, these systems often lack user controllability, leading to diminished user satisfaction and trust in the systems. Acknowledging the recent advancements in explainable recommender systems that enhance users' understanding of recommendation mechanisms, we propose leveraging these advancements to improve user controllability. In this paper, we present a user-controllable recommender system that seamlessly integrates explainability and controllability within a unified framework. By providing both retrospective and prospective explanations through counterfactual reasoning, users can customize their control over the system by interacting with these explanations.   Furthermore, we introduce and assess two attributes of controllability in recommendation systems: the complexity of controllability and the accuracy of controllability. Experimental evaluations on MovieLens and Yelp datasets substantiate the effectiveness of our proposed framework. Additionally, our experiments demonstrate that offering users control options can potentially enhance recommendation accuracy in the future. Source code and data are available at \url{https://github.com/chrisjtan/ucr}.
</details>
<details>
<summary>摘要</summary>
现代推荐系统通常利用用户历史行为生成个性化推荐，但这些系统经常缺乏用户可控性，导致用户满意度和信任度减降。考虑到 latest advancements in explainable recommender systems 可以提高用户理解推荐机制的程度，我们提议利用这些进步来提高用户可控性。在这篇论文中，我们提出了一个可控的推荐系统，该系统可以具有一体化的解释和可控性特性。通过对解释进行反思和评估，用户可以自定义他们对系统的控制。此外，我们还引入了两个推荐系统可控性的属性：复杂性和准确性。我们在 MovieLens 和 Yelp 数据集上进行了实验评估，并证明了我们提出的框架的效果。此外，我们的实验还表明，向用户提供控制选项可能会提高未来的推荐准确性。用户可以通过访问 GitHub 上的 <https://github.com/chrisjtan/ucr> 获取源代码和数据。
</details></li>
</ul>
<hr>
<h2 id="Tango-rethinking-quantization-for-graph-neural-network-training-on-GPUs"><a href="#Tango-rethinking-quantization-for-graph-neural-network-training-on-GPUs" class="headerlink" title="Tango: rethinking quantization for graph neural network training on GPUs"></a>Tango: rethinking quantization for graph neural network training on GPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00890">http://arxiv.org/abs/2308.00890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyang Chen, Da Zheng, Caiwen Ding, Chengying Huan, Yuede Ji, Hang Liu</li>
<li>for: 这篇论文主要是为了提高图神经网络（GNNs）的训练效率和精度。</li>
<li>methods: 这篇论文提出了一种名为Tango的新方法，包括有效的精度维护策略、量化意识的 primitives 和间接优化，以加速GNN训练。</li>
<li>results: 根据实验结果，Tango在各种GNN模型和数据集上表现出了superior的性能，比现有方法更快和更精度。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are becoming increasingly popular due to their superior performance in critical graph-related tasks. While quantization is widely used to accelerate GNN computation, quantized training faces unprecedented challenges. Current quantized GNN training systems often have longer training times than their full-precision counterparts for two reasons: (i) addressing the accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quantization is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations that can speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its superior performance over state-of-the-art approaches on various GNN models and datasets.
</details>
<details>
<summary>摘要</summary>
Graph Neural Networks (GNNs) 是越来越受欢迎，因为它们在图相关任务中表现出色。虽然量化是广泛使用来加速 GNN 计算，但量化训练面临了无 precedent 的挑战。现有的量化 GNN 训练系统经常比其全精度对应者 longer 的训练时间，因为：（i）保持精度需要增加过程，（ii）量化 expose 优化潜力并未得到有效利用。本文介绍 Tango，它重新思考量化挑战和机遇，并在 GPU 上进行 GNN 训练。Tango 具有以下三个贡献：首先，我们提出了高效的规则，以保持量化 GNN 训练中的精度。其次，我们设计并实现了量化感知的基本对象和间接优化，可以加速 GNN 训练。最后，我们将 Tango 集成到了流行的 Deep Graph Library (DGL) 系统中，并在不同的 GNN 模型和数据集上示出了与现有方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Factor-Graph-Neural-Networks"><a href="#Factor-Graph-Neural-Networks" class="headerlink" title="Factor Graph Neural Networks"></a>Factor Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00887">http://arxiv.org/abs/2308.00887</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Zhen Zhang, Mohammed Haroon Dupty, Fan Wu, Javen Qinfeng Shi, Wee Sun Lee</li>
<li>for: 本研究的目的是提出一种高效的图神经网络（FGNN），以便更好地捕捉高阶关系。</li>
<li>methods: 该研究使用了一种有效的权重估计方法，以及一种基于图神经网络（GNN）的新的消息传递方案。</li>
<li>results: 实验结果表明，该模型在 sintetic 数据集和实际数据集上具有潜在的应用前景。<details>
<summary>Abstract</summary>
In recent years, we have witnessed a surge of Graph Neural Networks (GNNs), most of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They have resemblance to Probabilistic Graphical Models (PGMs), but break free from some limitations of PGMs. By aiming to provide expressive methods for representation learning instead of computing marginals or most likely configurations, GNNs provide flexibility in the choice of information flowing rules while maintaining good performance. Despite their success and inspirations, they lack efficient ways to represent and learn higher-order relations among variables/nodes. More expressive higher-order GNNs which operate on k-tuples of nodes need increased computational resources in order to process higher-order tensors. We propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning. To do so, we first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module by allowing richer representations of the message update rules; this facilitates both efficient inference and powerful end-to-end learning. We further show that with a suitable choice of message aggregation operators, our FGNN is also able to represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a surge of Graph Neural Networks (GNNs), many of which can learn powerful representations in an end-to-end fashion with great success in many real-world applications. They share some similarities with Probabilistic Graphical Models (PGMs), but have broken free from some of PGMs' limitations. GNNs aim to provide expressive methods for representation learning, rather than computing marginals or most likely configurations. However, they lack efficient ways to represent and learn higher-order relations among variables/nodes. To address this, we propose Factor Graph Neural Networks (FGNNs) to effectively capture higher-order relations for inference and learning.We first derive an efficient approximate Sum-Product loopy belief propagation inference algorithm for discrete higher-order PGMs. We then neuralize the novel message passing scheme into a Factor Graph Neural Network (FGNN) module, allowing richer representations of the message update rules. This facilitates both efficient inference and powerful end-to-end learning. Moreover, with a suitable choice of message aggregation operators, our FGNN can also represent Max-Product belief propagation, providing a single family of architecture that can represent both Max and Sum-Product loopy belief propagation. Our extensive experimental evaluation on synthetic as well as real datasets demonstrates the potential of the proposed model.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Machine-Learning-Performance-with-Continuous-In-Session-Ground-Truth-Scores-Pilot-Study-on-Objective-Skeletal-Muscle-Pain-Intensity-Prediction"><a href="#Enhancing-Machine-Learning-Performance-with-Continuous-In-Session-Ground-Truth-Scores-Pilot-Study-on-Objective-Skeletal-Muscle-Pain-Intensity-Prediction" class="headerlink" title="Enhancing Machine Learning Performance with Continuous In-Session Ground Truth Scores: Pilot Study on Objective Skeletal Muscle Pain Intensity Prediction"></a>Enhancing Machine Learning Performance with Continuous In-Session Ground Truth Scores: Pilot Study on Objective Skeletal Muscle Pain Intensity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00886">http://arxiv.org/abs/2308.00886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boluwatife E. Faremi, Jonathon Stavres, Nuno Oliveira, Zhaoxian Zhou, Andrew H. Sung</li>
<li>for: 这项研究旨在开发两种设备，用于实时采集痛苦分数和血液征动活动评估。</li>
<li>methods: 该研究使用了多层感知神经网络（MLP）和随机森林（RF）机器学习模型，并将对象血液征动活动特征与实时痛苦分数相结合。</li>
<li>results: 研究发现，使用实时痛苦分数可以有效地提高机器学习模型在痛苦Intensity characterization中的性能，比使用后期痛苦分数更高。<details>
<summary>Abstract</summary>
Machine learning (ML) models trained on subjective self-report scores struggle to objectively classify pain accurately due to the significant variance between real-time pain experiences and recorded scores afterwards. This study developed two devices for acquisition of real-time, continuous in-session pain scores and gathering of ANS-modulated endodermal activity (EDA).The experiment recruited N = 24 subjects who underwent a post-exercise circulatory occlusion (PECO) with stretch, inducing discomfort. Subject data were stored in a custom pain platform, facilitating extraction of time-domain EDA features and in-session ground truth scores. Moreover, post-experiment visual analog scale (VAS) scores were collected from each subject. Machine learning models, namely Multi-layer Perceptron (MLP) and Random Forest (RF), were trained using corresponding objective EDA features combined with in-session scores and post-session scores, respectively. Over a 10-fold cross-validation, the macro-averaged geometric mean score revealed MLP and RF models trained with objective EDA features and in-session scores achieved superior performance (75.9% and 78.3%) compared to models trained with post-session scores (70.3% and 74.6%) respectively. This pioneering study demonstrates that using continuous in-session ground truth scores significantly enhances ML performance in pain intensity characterization, overcoming ground truth sparsity-related issues, data imbalance, and high variance. This study informs future objective-based ML pain system training.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型在主观自报分数上训练时，很难准确地分类疼痛，因为实际时间疼痛经验和记录下来的分数之间存在了很大的变化。这项研究开发了两种设备用于实时、连续式疼痛分数的获取和抽取 autonomic nervous system 模ulated endodermal activity（EDA）。实验采用 N = 24 个参与者，经过 POST-EXERCISE circulatory occlusion（PECO）后，induced discomfort。参与者数据被存储在自定义疼痛平台上，方便提取时间域 EDA 特征和实时真实分数。此外，每个参与者也提供了POST-EXPERIMENT visual analog scale（VAS）分数。机器学习模型，包括多层感知网络（MLP）和随机森林（RF），在对对应的对象 EDA 特征和实时分数、POST-SESSION 分数进行训练时，在十个横向验证中，macro-averaged geometric mean score 表明 MLP 和 RF 模型在对象 EDA 特征和实时分数上训练时， achievement 的性能（75.9% 和 78.3%）高于在 POST-SESSION 分数上训练时的性能（70.3% 和 74.6%）。这项先驱的研究表明，使用连续实时真实分数可以准确地强化 ML 在疼痛intensity  characterization 中，超越真实分数稀缺、数据不均衡和高方差问题。这项研究提供了未来对象基于 ML 疼痛系统训练的指导。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Wireless-Networks-with-Federated-Learning-A-Comprehensive-Review"><a href="#Revolutionizing-Wireless-Networks-with-Federated-Learning-A-Comprehensive-Review" class="headerlink" title="Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review"></a>Revolutionizing Wireless Networks with Federated Learning: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04404">http://arxiv.org/abs/2308.04404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajjad Emdadi Mahdimahalleh</li>
<li>for: The paper is written for discussing the significance of Machine Learning in wireless communication and introducing Federated Learning (FL) as a novel approach for future mobile networks, particularly 6G and beyond.</li>
<li>methods: The paper uses Federated Learning (FL) as a machine learning model that separates data acquisition and computation at the central unit, and discusses the challenges of implementing FL in a wireless edge network with limited and unreliable communication resources.</li>
<li>results: The paper highlights the potential of Federated Learning (FL) to play a vital role in future mobile networks, particularly 6G and beyond, by enabling the separation of data acquisition and computation at the central unit and addressing the challenges of limited and unreliable communication resources.<details>
<summary>Abstract</summary>
These days with the rising computational capabilities of wireless user equipment such as smart phones, tablets, and vehicles, along with growing concerns about sharing private data, a novel machine learning model called federated learning (FL) has emerged. FL enables the separation of data acquisition and computation at the central unit, which is different from centralized learning that occurs in a data center. FL is typically used in a wireless edge network where communication resources are limited and unreliable. Bandwidth constraints necessitate scheduling only a subset of UEs for updates in each iteration, and because the wireless medium is shared, transmissions are susceptible to interference and are not assured. The article discusses the significance of Machine Learning in wireless communication and highlights Federated Learning (FL) as a novel approach that could play a vital role in future mobile networks, particularly 6G and beyond.
</details>
<details>
<summary>摘要</summary>
Note: "UE" stands for "user equipment" in the context of wireless communication.
</details></li>
</ul>
<hr>
<h2 id="PeRP-Personalized-Residual-Policies-For-Congestion-Mitigation-Through-Co-operative-Advisory-Systems"><a href="#PeRP-Personalized-Residual-Policies-For-Congestion-Mitigation-Through-Co-operative-Advisory-Systems" class="headerlink" title="PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems"></a>PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00864">http://arxiv.org/abs/2308.00864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aamir Hasan, Neeloy Chakraborty, Haonan Chen, Jung-Hoon Cho, Cathy Wu, Katherine Driggs-Campbell</li>
<li>for: 提高交通效率和油耗成本</li>
<li>methods: 基于 Piecewise Constant (PC) 策略和变量自动编码器实现个性化驾驶建议</li>
<li>results: 在模拟驾驶中，我们的方法可以成功地减轻交通堵塞，并适应不同驾驶者行为，比基本方案提高4-22%的平均速度。<details>
<summary>Abstract</summary>
Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned on the inferred trait adapts the action of the PC policy to provide the driver with a personalized recommendation. Our system is trained in simulation with novel driver modeling of instruction adherence. We show that our approach successfully mitigates congestion while adapting to different driver behaviors, with 4 to 22% improvement in average speed over baselines.
</details>
<details>
<summary>摘要</summary>
智能驾驶系统可以减轻堵塞，从而改善许多社会经济因素，如通勤时间和油费成本。然而，这些系统假设自动驾驶车辆队伍的精确控制，因此在实践中有限，因为它们无法考虑人类行为的不确定性。 Piecewise Constant（PC）策略可以解决这些问题，通过结构化模型人类驾驶行为，以减少拥堵的潜在风险。然而，PC策略假设所有 drivers 的行为相似。为此，我们开发了一种合作建议系统，基于 PC 策略和一种新的个性化剩余策略（PeRP）。PeRP 会提供适应不同 Driver 行为的个性化建议，以减少拥堵。我们首先通过一种异构自动编码器来无监督地推断 Driver 的内在特征，然后基于推断的特征来 condition 策略，以提供个性化的建议。我们的系统在 simulator 中进行了训练，并在新的 Driver 模型中模拟了 Driver 的指令遵从程度。我们的方法可以成功地减少拥堵，同时适应不同 Driver 行为，相比基eline 提高了4%-22%的平均速度。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Activation-Patterns-in-Artificial-Neural-Networks-by-Exploring-Stochastic-Processes"><a href="#Understanding-Activation-Patterns-in-Artificial-Neural-Networks-by-Exploring-Stochastic-Processes" class="headerlink" title="Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes"></a>Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00858">http://arxiv.org/abs/2308.00858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephan Johann Lehmler, Muhammad Saif-ur-Rehman, Tobias Glasmachers, Ioannis Iossifidis</li>
<li>for: 这paper的目的是强调使用 Stochastic Processes 模型来描述人工神经网络的行为和学习 dinamics。</li>
<li>methods: 该 paper 使用 Stochastic Processes 模型来模拟人工神经网络的动作 Patterns，并使用 neuroscience 技术来描述实际神经元的冲击 Train。</li>
<li>results: 该 paper 通过对不同的人工神经网络图像识别任务数据进行分析，发现了不同的建筑和训练集之间的差异，并计算了 Mean Firing Rate、Mean Fano Factor 和 Variances 等指标，以描述人工神经网络的吞吐量和学习行为。<details>
<summary>Abstract</summary>
To gain a deeper understanding of the behavior and learning dynamics of (deep) artificial neural networks, it is valuable to employ mathematical abstractions and models. These tools provide a simplified perspective on network performance and facilitate systematic investigations through simulations. In this paper, we propose utilizing the framework of stochastic processes, which has been underutilized thus far.   Our approach models activation patterns of thresholded nodes in (deep) artificial neural networks as stochastic processes. We focus solely on activation frequency, leveraging neuroscience techniques used for real neuron spike trains. During a classification task, we extract spiking activity and use an arrival process following the Poisson distribution.   We examine observed data from various artificial neural networks in image recognition tasks, fitting the proposed model's assumptions. Through this, we derive parameters describing activation patterns in each network. Our analysis covers randomly initialized, generalizing, and memorizing networks, revealing consistent differences across architectures and training sets.   Calculating Mean Firing Rate, Mean Fano Factor, and Variances, we find stable indicators of memorization during learning, providing valuable insights into network behavior. The proposed model shows promise in describing activation patterns and could serve as a general framework for future investigations. It has potential applications in theoretical simulations, pruning, and transfer learning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:使用数学准确模型来深入理解人工神经网络（深度人工神经网络）的行为和学习动态。这些工具可以简化网络性能的视角，并且通过仿真来进行系统性的调查。在这篇论文中，我们提出使用 Stochastic Processes 框架，这种框架在人工神经网络中的应用仍然很少。  我们的方法是将人工神经网络中的激活模式模型为 Stochastic Processes。我们仅ocus on 激活频率，利用了 neuroscience 中用于真正神经元脉冲的技术。在图像识别任务中，我们从人工神经网络中提取激活活动，并使用随机过程遵循 Poisson 分布。  我们对不同的人工神经网络数据进行分析，包括随机初始化、泛化和记忆化网络。我们发现这些网络在学习过程中的激活模式存在差异，这些差异随着训练集的不同而变化。  通过计算 Mean Firing Rate、Mean Fano Factor 和 Variances，我们发现在学习过程中存在稳定的记忆指标，这些指标提供了人工神经网络行为的重要信息。我们的模型显示 promise 在描述激活模式方面，并且可能在理论仿真、剪枝和转移学习等领域有应用。
</details></li>
</ul>
<hr>
<h2 id="Differential-Privacy-for-Adaptive-Weight-Aggregation-in-Federated-Tumor-Segmentation"><a href="#Differential-Privacy-for-Adaptive-Weight-Aggregation-in-Federated-Tumor-Segmentation" class="headerlink" title="Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation"></a>Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00856">http://arxiv.org/abs/2308.00856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Irfan Khan, Esa Alhoniemi, Elina Kontio, Suleiman A. Khan, Mojtaba Jafaritadi</li>
<li>For: This paper focuses on developing a differentially private federated deep learning framework for brain tumor segmentation in multi-modal magnetic resonance imaging (MRI). The goal is to enhance model segmentation capabilities while preserving the privacy of client data.* Methods: The proposed method, called DP-SimAgg, is a differentially private similarity-weighted aggregation algorithm that extends the existing SimAgg method to ensure privacy preservation. The algorithm uses a secure multi-party computation (MPC) protocol to enable private similarity computation and weight aggregation.* Results: The proposed method is evaluated extensively, with a focus on computational performance, and demonstrates accurate and robust brain tumor segmentation while minimizing communication costs during model training. The results show that the DP-SimAgg method provides an additional layer of privacy preservation without compromising segmentation model efficacy.<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning approach that safeguards privacy by creating an impartial global model while respecting the privacy of individual client data. However, the conventional FL method can introduce security risks when dealing with diverse client data, potentially compromising privacy and data integrity. To address these challenges, we present a differential privacy (DP) federated deep learning framework in medical image segmentation. In this paper, we extend our similarity weight aggregation (SimAgg) method to DP-SimAgg algorithm, a differentially private similarity-weighted aggregation algorithm for brain tumor segmentation in multi-modal magnetic resonance imaging (MRI). Our DP-SimAgg method not only enhances model segmentation capabilities but also provides an additional layer of privacy preservation. Extensive benchmarking and evaluation of our framework, with computational performance as a key consideration, demonstrate that DP-SimAgg enables accurate and robust brain tumor segmentation while minimizing communication costs during model training. This advancement is crucial for preserving the privacy of medical image data and safeguarding sensitive information. In conclusion, adding a differential privacy layer in the global weight aggregation phase of the federated brain tumor segmentation provides a promising solution to privacy concerns without compromising segmentation model efficacy. By leveraging DP, we ensure the protection of client data against adversarial attacks and malicious participants.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式机器学习方法，它保障隐私性的同时创造一个公正的全球模型，对各个客户端数据进行尊重。然而，传统的 FL 方法可能会在处理多样化的客户端数据时引入安全风险，可能会侵犯隐私性和数据完整性。为解决这些挑战，我们在医学图像分割中提出了一种基于分布式深度学习的幂等隐私（DP）框架。在这篇论文中，我们扩展了我们的相似权聚合（SimAgg）方法，并将其转化为 differentially private similarity-weighted aggregation algorithm（DP-SimAgg），用于多Modal磁共振成像（MRI）中的脑肿瘤分割。我们的 DP-SimAgg 方法不仅提高了模型的分割能力，还提供了一层额外的隐私保护。我们对我们的框架进行了广泛的 benchmarking 和评估，并且计算性能作为关键考虑因素，得到了DP-SimAgg 能够实现精准而Robust的脑肿瘤分割，同时尽量降低模型训练期间的通信成本。这种进步是关键的，因为它使得医学图像数据的隐私得到了保障，并防止了恶意攻击和黑客参与者。In conclusion, adding a differential privacy layer in the global weight aggregation phase of the federated brain tumor segmentation provides a promising solution to privacy concerns without compromising segmentation model efficacy. By leveraging DP, we ensure the protection of client data against adversarial attacks and malicious participants.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-Groundbreaking-Machine-Learning-Research-Analyzing-Highly-Cited-and-Impactful-Publications-across-Six-Decades"><a href="#A-Comprehensive-Study-of-Groundbreaking-Machine-Learning-Research-Analyzing-Highly-Cited-and-Impactful-Publications-across-Six-Decades" class="headerlink" title="A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades"></a>A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00855">http://arxiv.org/abs/2308.00855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Absalom E. Ezugwu, Japie Greeff, Yuh-Shan Ho</li>
<li>for: 本研究旨在探讨机器学习（ML）领域最具影响力的论文，以了解领域的发展趋势、关键人物和贡献。</li>
<li>methods: 本研究采用了多种 bibliometric 技术，包括引用分析、合作关系分析、关键词分析和发表趋势分析，以分析数据。</li>
<li>results: 研究发现了机器学习领域最具影响力的论文、作者和合作网络，并揭示出了不同年代的发表趋势和热点研究主题。<details>
<summary>Abstract</summary>
Machine learning (ML) has emerged as a prominent field of research in computer science and other related fields, thereby driving advancements in other domains of interest. As the field continues to evolve, it is crucial to understand the landscape of highly cited publications to identify key trends, influential authors, and significant contributions made thus far. In this paper, we present a comprehensive bibliometric analysis of highly cited ML publications. We collected a dataset consisting of the top-cited papers from reputable ML conferences and journals, covering a period of several years from 1959 to 2022. We employed various bibliometric techniques to analyze the data, including citation analysis, co-authorship analysis, keyword analysis, and publication trends. Our findings reveal the most influential papers, highly cited authors, and collaborative networks within the machine learning community. We identify popular research themes and uncover emerging topics that have recently gained significant attention. Furthermore, we examine the geographical distribution of highly cited publications, highlighting the dominance of certain countries in ML research. By shedding light on the landscape of highly cited ML publications, our study provides valuable insights for researchers, policymakers, and practitioners seeking to understand the key developments and trends in this rapidly evolving field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CASSINI-Network-Aware-Job-Scheduling-in-Machine-Learning-Clusters"><a href="#CASSINI-Network-Aware-Job-Scheduling-in-Machine-Learning-Clusters" class="headerlink" title="CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters"></a>CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00852">http://arxiv.org/abs/2308.00852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsanan Rajasekaran, Manya Ghobadi, Aditya Akella</li>
<li>for: 提高机器学习（ML）集群的任务调度效率和网络带宽利用率。</li>
<li>methods: 使用一个novel的 геометрического抽象来考虑不同任务之间的通信模式，并使用一个相互关联的时间排序来调整任务的通信阶段。</li>
<li>results: 在24个服务器的测试环境中，与状态时艺ML调度器相比，CASSINI可以提高任务的平均和尾部完成时间 by up to 1.6x和2.5x，同时还可以减少集群中的ECN标记包数量 by up to 33x。<details>
<summary>Abstract</summary>
We present CASSINI, a network-aware job scheduler for machine learning (ML) clusters. CASSINI introduces a novel geometric abstraction to consider the communication pattern of different jobs while placing them on network links. To do so, CASSINI uses an affinity graph that finds a series of time-shift values to adjust the communication phases of a subset of jobs, such that the communication patterns of jobs sharing the same network link are interleaved with each other. Experiments with 13 common ML models on a 24-server testbed demonstrate that compared to the state-of-the-art ML schedulers, CASSINI improves the average and tail completion time of jobs by up to 1.6x and 2.5x, respectively. Moreover, we show that CASSINI reduces the number of ECN marked packets in the cluster by up to 33x.
</details>
<details>
<summary>摘要</summary>
我们介绍了CASSINI，一个基于网络的机器学习（ML）集群中的任务调度器。CASSINI引入了一种新的几何封装，以考虑不同任务之间的通信模式。为此，CASSINI使用一个相互关系图来找到一系列时间偏移值，以调整与共享同一个网络链接的任务的通信阶段。实验结果表明，相比现有的ML调度器，CASSINI可以提高平均和尾部完成时间的任务执行时间，最多提高1.6倍和2.5倍。此外，我们还发现CASSINI可以减少集群中ECN标记包的数量，最多减少33倍。
</details></li>
</ul>
<hr>
<h2 id="An-Exact-Kernel-Equivalence-for-Finite-Classification-Models"><a href="#An-Exact-Kernel-Equivalence-for-Finite-Classification-Models" class="headerlink" title="An Exact Kernel Equivalence for Finite Classification Models"></a>An Exact Kernel Equivalence for Finite Classification Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00824">http://arxiv.org/abs/2308.00824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Bell, Michael Geyer, David Glickenstein, Amanda Fernandez, Juston Moore</li>
<li>for: 本研究探讨神经网络和核函数方法之间的等价性，并derive了任何 finite-size parametric classification model通过梯度下降训练的第一个精确表示。</li>
<li>methods: 本研究使用梯度下降训练来 derive the exact representation of any finite-size parametric classification model, and compare it with well-known Neural Tangent Kernel (NTK) and other non-exact path kernel formulations.</li>
<li>results: 实验表明，可以用这个精确kernel来计算现实网络中的预测结果，并且这个kernel可以提供有用的推理方法，尤其是在神经网络的泛化性方面。<details>
<summary>Abstract</summary>
We explore the equivalence between neural networks and kernel methods by deriving the first exact representation of any finite-size parametric classification model trained with gradient descent as a kernel machine. We compare our exact representation to the well-known Neural Tangent Kernel (NTK) and discuss approximation error relative to the NTK and other non-exact path kernel formulations. We experimentally demonstrate that the kernel can be computed for realistic networks up to machine precision. We use this exact kernel to show that our theoretical contribution can provide useful insights into the predictions made by neural networks, particularly the way in which they generalize.
</details>
<details>
<summary>摘要</summary>
我们研究神经网络和核函数方法之间的等价性，通过将任何具有Gradient Descent训练的finite-size parametric类别模型转换为核机制。我们与Well-known Neural Tangent Kernel（NTK）进行比较，并讨论这些非正确的路径核函数表示的误差。我们还详细地验证了这个核函数可以在现实的神经网络上计算到机器精度。我们使用这个精确的核函数，以示我们的理论贡献可以对神经网络的预测提供有用的洞察。
</details></li>
</ul>
<hr>
<h2 id="An-Introduction-to-Bi-level-Optimization-Foundations-and-Applications-in-Signal-Processing-and-Machine-Learning"><a href="#An-Introduction-to-Bi-level-Optimization-Foundations-and-Applications-in-Signal-Processing-and-Machine-Learning" class="headerlink" title="An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning"></a>An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00788">http://arxiv.org/abs/2308.00788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihua Zhang, Prashant Khanduri, Ioannis Tsaknakis, Yuguang Yao, Mingyi Hong, Sijia Liu</li>
<li>for: 这篇论文主要是为了探讨bi-level optimization（BLO）在信号处理（SP）和机器学习（ML）领域的应用，以及BLO在这些领域的潜在应用前景。</li>
<li>methods: 本文主要介绍了一类可解释的BLO问题，包括其优化条件、标准算法（包括其优化原理和实践）以及如何使其在SP和ML应用中 дости得状态最优结果。</li>
<li>results: 本文详细介绍了一些BLO问题的基本概念，包括其优化条件、标准算法和实践，以及如何使其在SP和ML应用中实现状态最优结果。此外，本文还讨论了BLO理论的最新进展、其对应用的影响以及未来研究的重要性。<details>
<summary>Abstract</summary>
Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be leveraged to obtain state-of-the-art results for a number of key SP and ML applications. Further, we discuss some recent advances in BLO theory, its implications for applications, and point out some limitations of the state-of-the-art that require significant future research efforts. Overall, we hope that this article can serve to accelerate the adoption of BLO as a generic tool to model, analyze, and innovate on a wide array of emerging SP and ML applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-Spiking-Neural-Network-On-Neuromorphic-Platform-For-Human-Activity-Recognition"><a href="#Evaluating-Spiking-Neural-Network-On-Neuromorphic-Platform-For-Human-Activity-Recognition" class="headerlink" title="Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition"></a>Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00787">http://arxiv.org/abs/2308.00787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sizhen Bian, Michele Magno<br>for: This paper is written for the purpose of evaluating the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications.methods: The paper uses a multi-threshold delta modulation approach to encode input sensor data into spike trains, and a spiking neural network with direct-event training. The trained model is deployed on the research neuromorphic platform from Intel, Loihi.results: The spike-based workouts recognition system achieves a comparable accuracy (87.5%) to a popular milliwatt RISC-V bases multi-core processor GAP8 with a traditional neural network (88.1%) while achieving two times better energy-delay product (0.66 \si{\micro\joule\second} vs. 1.32 \si{\micro\joule\second}).<details>
<summary>Abstract</summary>
Energy efficiency and low latency are crucial requirements for designing wearable AI-empowered human activity recognition systems, due to the hard constraints of battery operations and closed-loop feedback. While neural network models have been extensively compressed to match the stringent edge requirements, spiking neural networks and event-based sensing are recently emerging as promising solutions to further improve performance due to their inherent energy efficiency and capacity to process spatiotemporal data in very low latency. This work aims to evaluate the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications. The case of workout recognition with wrist-worn wearable motion sensors is used as a study. A multi-threshold delta modulation approach is utilized for encoding the input sensor data into spike trains to move the pipeline into the event-based approach. The spikes trains are then fed to a spiking neural network with direct-event training, and the trained model is deployed on the research neuromorphic platform from Intel, Loihi, to evaluate energy and latency efficiency. Test results show that the spike-based workouts recognition system can achieve a comparable accuracy (87.5\%) comparable to the popular milliwatt RISC-V bases multi-core processor GAP8 with a traditional neural network ( 88.1\%) while achieving two times better energy-delay product (0.66 \si{\micro\joule\second} vs. 1.32 \si{\micro\joule\second}).
</details>
<details>
<summary>摘要</summary>
“能量效率和延迟时间是设计智能盔牌人活动识别系统的关键要求，因为电池操作和关闭反馈带来的硬件限制。尽管神经网络模型已经广泛压缩以满足边缘的强制要求，但使用快速进程和事件检测是最近几年出现的优秀解决方案，因为它们的内生能量效率和可以在非常低延迟时间处理空间时间数据。本工作的目的是评估使用神经元处理器在人活动识别方面的快速进程神经网络的有效性。使用腕上穿戴式运动传感器进行运动识别的情况作为研究。将输入传感器数据编码为脉冲列表，然后将脉冲列表传递给快速进程神经网络进行训练，并使用直接事件训练。训练后的模型被部署在智能研究平台Intel Loihi上，以评估能量和延迟效率。测试结果显示，使用脉冲基于的运动识别系统可以达到与流行的百万瓦RISC-V基于多核心处理器GAP8的传统神经网络（88.1%）相同的准确率（87.5%），同时在能量延迟产品上提供两倍的提升（0.66 \si{\micro\joule\second} vs. 1.32 \si{\micro\joule\second）。”
</details></li>
</ul>
<hr>
<h2 id="DYMOND-DYnamic-MOtif-NoDes-Network-Generative-Model"><a href="#DYMOND-DYnamic-MOtif-NoDes-Network-Generative-Model" class="headerlink" title="DYMOND: DYnamic MOtif-NoDes Network Generative Model"></a>DYMOND: DYnamic MOtif-NoDes Network Generative Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00770">http://arxiv.org/abs/2308.00770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeno129/dymond">https://github.com/zeno129/dymond</a></li>
<li>paper_authors: Giselle Zeno, Timothy La Fond, Jennifer Neville</li>
<li>for: 这个论文旨在提出一种基于动态网络结构的生成模型，以capture longer-range correlations in connections and activity。</li>
<li>methods: 该模型使用 temporal motif activity 和节点的角色来生成动态网络结构，并提出了一种新的评价方法来评估网络的动态特性。</li>
<li>results: 与基eline模型进行比较后，该模型能够更好地生成网络结构和节点的行为，同时提供了一种新的评价方法来评估网络的动态特性。<details>
<summary>Abstract</summary>
Motifs, which have been established as building blocks for network structure, move beyond pair-wise connections to capture longer-range correlations in connections and activity. In spite of this, there are few generative graph models that consider higher-order network structures and even fewer that focus on using motifs in models of dynamic graphs. Most existing generative models for temporal graphs strictly grow the networks via edge addition, and the models are evaluated using static graph structure metrics -- which do not adequately capture the temporal behavior of the network. To address these issues, in this work we propose DYnamic MOtif-NoDes (DYMOND) -- a generative model that considers (i) the dynamic changes in overall graph structure using temporal motif activity and (ii) the roles nodes play in motifs (e.g., one node plays the hub role in a wedge, while the remaining two act as spokes). We compare DYMOND to three dynamic graph generative model baselines on real-world networks and show that DYMOND performs better at generating graph structure and node behavior similar to the observed network. We also propose a new methodology to adapt graph structure metrics to better evaluate the temporal aspect of the network. These metrics take into account the changes in overall graph structure and the individual nodes' behavior over time.
</details>
<details>
<summary>摘要</summary>
网络结构中的模式，作为建筑块，超越了对连接的对应关系，以捕捉更长范围的连接和活动的相关性。尽管如此，目前的生成图模型很少考虑高阶网络结构，而且更少的模型将注重使用模式来生成动态图。大多数现有的生成模型都是通过边加入来增长网络，并且这些模型被评估使用静止图结构指标，这些指标不够 capture 网络的时间行为。为解决这些问题，在这项工作中，我们提出了动态模式无核（DYMOND）生成模型，它考虑了（i）动态变化的总图结构使用时间模式活动，以及（ii）节点在模式中的角色（例如，一个节点在wedgel中扮演核角色，而剩下两个节点扮演螺旋的角色）。我们比较了DYMOND与三种动态图生成模型基eline 的性能，并发现DYMOND在生成图结构和节点行为方面表现出色，能够更好地生成与观察网络相似的图结构和节点行为。此外，我们还提出了一种新的方法来适应图结构指标的改进，这些指标考虑了网络的时间变化和个体节点的行为过程中的变化。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Contrastive-BERT-Fine-tuning-for-Fusion-based-Reviewed-Item-Retrieval"><a href="#Self-Supervised-Contrastive-BERT-Fine-tuning-for-Fusion-based-Reviewed-Item-Retrieval" class="headerlink" title="Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval"></a>Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00762">http://arxiv.org/abs/2308.00762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/d3mlab/rir_data">https://github.com/d3mlab/rir_data</a></li>
<li>paper_authors: Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Armin Toroghi, Anton Korikov, Ali Pesaranghader, Touqir Sajed, Manasa Bharadwaj, Borislav Mavrin, Scott Sanner<br>for: This paper focuses on the task of Reviewed-Item Retrieval (RIR), which involves aggregating query-review scores to generate item-level scores for ranking.methods: The authors extend Neural Information Retrieval (IR) methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. They explore different techniques for selecting positive and negative samples, including anchor sub-sampling and augmenting with meta-data.results: The authors show that Late Fusion contrastive learning for Neural RIR outperforms all other contrastive IR configurations, Neural IR, and sparse retrieval baselines. This demonstrates the effectiveness of exploiting the two-level structure in Neural RIR approaches and the importance of preserving the nuance of individual review content via Late Fusion methods.<details>
<summary>Abstract</summary>
As natural language interfaces enable users to express increasingly complex natural language queries, there is a parallel explosion of user review content that can allow users to better find items such as restaurants, books, or movies that match these expressive queries. While Neural Information Retrieval (IR) methods have provided state-of-the-art results for matching queries to documents, they have not been extended to the task of Reviewed-Item Retrieval (RIR), where query-review scores must be aggregated (or fused) into item-level scores for ranking. In the absence of labeled RIR datasets, we extend Neural IR methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. Specifically, contrastive learning requires a choice of positive and negative samples, where the unique two-level structure of our item-review data combined with meta-data affords us a rich structure for the selection of these samples. For contrastive learning in a Late Fusion scenario, we investigate the use of positive review samples from the same item and/or with the same rating, selection of hard positive samples by choosing the least similar reviews from the same anchor item, and selection of hard negative samples by choosing the most similar reviews from different items. We also explore anchor sub-sampling and augmenting with meta-data. For a more end-to-end Early Fusion approach, we introduce contrastive item embedding learning to fuse reviews into single item embeddings. Experimental results show that Late Fusion contrastive learning for Neural RIR outperforms all other contrastive IR configurations, Neural IR, and sparse retrieval baselines, thus demonstrating the power of exploiting the two-level structure in Neural RIR approaches as well as the importance of preserving the nuance of individual review content via Late Fusion methods.
</details>
<details>
<summary>摘要</summary>
随着自然语言界面的出现，用户可以提出更加复杂的自然语言查询，这也导致了用户评论内容的激增，可以帮助用户更好地找到符合其查询的餐厅、书籍或电影等。尽管神经信息检索（IR）方法已经提供了状态机器的结果，但它们没有被扩展到评论遍历（RIR）任务中，其中查询评论得分需要聚合（或融合）到物品级别的分数。由于没有标注的 RIR 数据集，我们扩展了神经 IR 方法到 RIR，通过自我超vision 方法来学习 BERT 表示的对比学习。具体来说，对比学习需要选择正例和负例样本，我们利用 item-review 数据的两级结构，以及元数据，选择正例和负例样本。在晚期融合方案中，我们 investigate 使用同一个物品的正例评论，或者同一个评级的负例评论作为正例，选择最不相似的 anchor item 中的负例评论，以及选择最相似的 anchor item 中的正例评论作为负例。我们还 explore anchor 抽样和元数据的扩展。在更加端到端的早期融合方案中，我们引入对比项表示学习，将评论融合到单个物品表示中。实验结果显示，晚期融合对比学习在神经 RIR 中表现出色，超过了其他对比 IR 配置、神经 IR 和稀缺检索基准。这也证明了在神经 RIR 方法中利用两级结构的优势，以及在保留评论内容的细节方面，晚期融合方法的重要性。
</details></li>
</ul>
<hr>
<h2 id="The-Bias-Amplification-Paradox-in-Text-to-Image-Generation"><a href="#The-Bias-Amplification-Paradox-in-Text-to-Image-Generation" class="headerlink" title="The Bias Amplification Paradox in Text-to-Image Generation"></a>The Bias Amplification Paradox in Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00755">http://arxiv.org/abs/2308.00755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/preethiseshadri518/bias-amplification-paradox">https://github.com/preethiseshadri518/bias-amplification-paradox</a></li>
<li>paper_authors: Preethi Seshadri, Sameer Singh, Yanai Elazar</li>
<li>for: 研究文章探讨了模型中的偏见增强现象，具体来说是在文本到图像领域中使用稳定扩散来研究。</li>
<li>methods: 作者使用了稳定扩散来研究模型中的偏见增强现象，并比较了训练和生成图像中的性别比例。</li>
<li>results: 研究发现，模型在训练和生成图像中增强了 gender-occupation 偏见，但是这种增强可以大量归因于训练数据中的 gender 信息和生成图像中的 prompts 之间的差异。当考虑到不同的文本使用于训练和生成时，偏见增强的减少了许多。这些发现告诉我们在比较模型和训练数据中的偏见时需要考虑各种因素，以及这些因素如何影响偏见增强。<details>
<summary>Abstract</summary>
Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding factors that contribute to bias amplification.
</details>
<details>
<summary>摘要</summary>
“偏调增强”是一种现象，模型在训练数据中增加了偏调。在这篇论文中，我们研究了偏调增强在文本到图像领域中，使用稳定扩散法比较训练和生成图像中的性别比例。我们发现，模型会增加训练数据中的性别职业偏调。但是，我们发现这种增强可以大多归因于训练描述和模型说明之间的不同。例如，训练数据中的描述通常包含直接表达性别信息，而模型说明则不包含这种信息，这会导致分布shift和影响偏调测量。一旦我们考虑到不同的分布特征，我们发现增强降低了许多。我们的发现显示了比较模型和它们训练数据中的偏调是具有挑战性的，并高亮了干扰因素对偏调增强的影响。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Hypervectors-A-Survey-on-Hypervector-Encoding"><a href="#Learning-from-Hypervectors-A-Survey-on-Hypervector-Encoding" class="headerlink" title="Learning from Hypervectors: A Survey on Hypervector Encoding"></a>Learning from Hypervectors: A Survey on Hypervector Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00685">http://arxiv.org/abs/2308.00685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sercan Aygun, Mehran Shoushtari Moghadam, M. Hassan Najafi, Mohsen Imani</li>
<li>for: 本研究旨在探讨高维计算（HDC）系统输入和超对数据编码过程中的hypervector生成方法，以及不同应用场景下的编码类型和其限制、挑战和优势。</li>
<li>methods: 本研究将各种来自不同研究的超对数据生成方法融合探讨，包括Orthogonal Vector Generation（OVG）、Correlated Vector Generation（CVG）等。</li>
<li>results: 本研究可以帮助读者更深入理解HDC系统输入和超对数据编码过程中的hypervector生成方法，并了解不同应用场景下的编码类型和其限制、挑战和优势。<details>
<summary>Abstract</summary>
Hyperdimensional computing (HDC) is an emerging computing paradigm that imitates the brain's structure to offer a powerful and efficient processing and learning model. In HDC, the data are encoded with long vectors, called hypervectors, typically with a length of 1K to 10K. The literature provides several encoding techniques to generate orthogonal or correlated hypervectors, depending on the intended application. The existing surveys in the literature often focus on the overall aspects of HDC systems, including system inputs, primary computations, and final outputs. However, this study takes a more specific approach. It zeroes in on the HDC system input and the generation of hypervectors, directly influencing the hypervector encoding process. This survey brings together various methods for hypervector generation from different studies and explores the limitations, challenges, and potential benefits they entail. Through a comprehensive exploration of this survey, readers will acquire a profound understanding of various encoding types in HDC and gain insights into the intricate process of hypervector generation for diverse applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CodeBPE-Investigating-Subtokenization-Options-for-Large-Language-Model-Pretraining-on-Source-Code"><a href="#CodeBPE-Investigating-Subtokenization-Options-for-Large-Language-Model-Pretraining-on-Source-Code" class="headerlink" title="CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code"></a>CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00683">http://arxiv.org/abs/2308.00683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadezhda Chirkova, Sergey Troshin</li>
<li>for:  investigate another important aspect of large language model pretraining for source code, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations.</li>
<li>methods: propose subtokenization that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.</li>
<li>results: identify most effective and length-efficient subtokenizations, taking into account code specifics, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.<details>
<summary>Abstract</summary>
Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tool-Documentation-Enables-Zero-Shot-Tool-Usage-with-Large-Language-Models"><a href="#Tool-Documentation-Enables-Zero-Shot-Tool-Usage-with-Large-Language-Models" class="headerlink" title="Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"></a>Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00675">http://arxiv.org/abs/2308.00675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, Tomas Pfister</li>
<li>for: 这个论文的目的是为了提出一种新的方法，即使用工具文档来取代示例来教育大型自然语言模型（LLM）使用新工具。</li>
<li>methods: 这个论文使用了六个任务，分别在视觉和语言领域进行了测试，并使用了多种工具API来检验其效果。</li>
<li>results: 研究发现，只使用工具文档，LLM可以很好地使用新工具，并且在一些任务上甚至可以达到与几个示例的性能。此外，研究还发现，使用工具文档可以在很多情况下超过示例的效果，并且可以自动激活新的应用程序。<details>
<summary>Abstract</summary>
Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation. Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Some words and phrases may be written differently in Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Mapping-Computer-Science-Research-Trends-Influences-and-Predictions"><a href="#Mapping-Computer-Science-Research-Trends-Influences-and-Predictions" class="headerlink" title="Mapping Computer Science Research: Trends, Influences, and Predictions"></a>Mapping Computer Science Research: Trends, Influences, and Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00733">http://arxiv.org/abs/2308.00733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Almutairi, Ozioma Collins Oguine</li>
<li>for: 这篇论文探讨了计算机科学领域当前流行的研究领域，并研究了这些领域的出现的因素。</li>
<li>methods: 该论文使用了高级机器学习技术，包括决策树和逻辑回归模型，以预测流行的研究领域。</li>
<li>results: 研究发现，参考论文的数量（Reference Count）是当前流行研究领域的关键因素，使得参考论文数量成为计算机科学领域中最重要的驱动因素。此外，NSF拨款和专利在流行话题中的影响也在不断增长。逻辑回归模型比决策树模型更高精度、精度、回归率和F1分数，表明逻辑回归模型在预测流行研究领域的表现更佳。该数据驱动的方法比随机猜测基准更高精度和有效性，为研究人员和机构提供了数据驱动的基础 для决策和未来研究方向。<details>
<summary>Abstract</summary>
This paper explores the current trending research areas in the field of Computer Science (CS) and investigates the factors contributing to their emergence. Leveraging a comprehensive dataset comprising papers, citations, and funding information, we employ advanced machine learning techniques, including Decision Tree and Logistic Regression models, to predict trending research areas. Our analysis reveals that the number of references cited in research papers (Reference Count) plays a pivotal role in determining trending research areas making reference counts the most relevant factor that drives trend in the CS field. Additionally, the influence of NSF grants and patents on trending topics has increased over time. The Logistic Regression model outperforms the Decision Tree model in predicting trends, exhibiting higher accuracy, precision, recall, and F1 score. By surpassing a random guess baseline, our data-driven approach demonstrates higher accuracy and efficacy in identifying trending research areas. The results offer valuable insights into the trending research areas, providing researchers and institutions with a data-driven foundation for decision-making and future research direction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/02/cs.LG_2023_08_02/" data-id="clly36043005rdd88bqyk7ndn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/03/eess.IV_2023_08_03/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-03 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/02/cs.SD_2023_08_02/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-02 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
