
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-02 17:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.01239 repo_url: https:&#x2F;&#x2F;github.com&#x2F;FengheTan9&#x2F;Medical-Image-Segmenta">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-02 17:00:00">
<meta property="og:url" content="http://example.com/2023/08/02/eess.IV_2023_08_02/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.01239 repo_url: https:&#x2F;&#x2F;github.com&#x2F;FengheTan9&#x2F;Medical-Image-Segmenta">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-01T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:42.765Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/02/eess.IV_2023_08_02/" class="article-date">
  <time datetime="2023-08-01T16:00:00.000Z" itemprop="datePublished">2023-08-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-02 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CMUNeXt-An-Efficient-Medical-Image-Segmentation-Network-based-on-Large-Kernel-and-Skip-Fusion"><a href="#CMUNeXt-An-Efficient-Medical-Image-Segmentation-Network-based-on-Large-Kernel-and-Skip-Fusion" class="headerlink" title="CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion"></a>CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01239">http://arxiv.org/abs/2308.01239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/Medical-Image-Segmentation-Benchmarks">https://github.com/FengheTan9/Medical-Image-Segmentation-Benchmarks</a></li>
<li>paper_authors: Fenghe Tang, Jianrui Ding, Lingtao Wang, Chunping Ning, S. Kevin Zhou</li>
<li>for: 这个研究旨在提出一个高效、轻量级的医疗影像分类网络，以应对现场医疗 scenarios 中的病变探测和诊断。</li>
<li>methods: 本研究使用了 U-shaped 架构，并将 CNN 和 Transformer 混合使用，以获得更好的全局 контекст资讯和精确的地方化探测。</li>
<li>results: 实验结果显示，CMUNeXt 可以在多个医疗影像数据集上实现更好的分类性能，并且比其他现有的重量级和轻量级医疗影像分类网络更快速、更轻量级、并且降低了计算成本。<details>
<summary>Abstract</summary>
The U-shaped architecture has emerged as a crucial paradigm in the design of medical image segmentation networks. However, due to the inherent local limitations of convolution, a fully convolutional segmentation network with U-shaped architecture struggles to effectively extract global context information, which is vital for the precise localization of lesions. While hybrid architectures combining CNNs and Transformers can address these issues, their application in real medical scenarios is limited due to the computational resource constraints imposed by the environment and edge devices. In addition, the convolutional inductive bias in lightweight networks adeptly fits the scarce medical data, which is lacking in the Transformer based network. In order to extract global context information while taking advantage of the inductive bias, we propose CMUNeXt, an efficient fully convolutional lightweight medical image segmentation network, which enables fast and accurate auxiliary diagnosis in real scene scenarios. CMUNeXt leverages large kernel and inverted bottleneck design to thoroughly mix distant spatial and location information, efficiently extracting global context information. We also introduce the Skip-Fusion block, designed to enable smooth skip-connections and ensure ample feature fusion. Experimental results on multiple medical image datasets demonstrate that CMUNeXt outperforms existing heavyweight and lightweight medical image segmentation networks in terms of segmentation performance, while offering a faster inference speed, lighter weights, and a reduced computational cost. The code is available at https://github.com/FengheTan9/CMUNeXt.
</details>
<details>
<summary>摘要</summary>
医疗图像分 segmentation 网络中，U字型架构已成为重要的设计方法。然而，由于 convolution 的本质性限制，一个完全 convolutional 分 segmentation 网络具有 U字型架构很难准确地提取全局上下文信息，这是精确地定位疾病所必需的。尽管混合 Architecture 组合 CNN 和 Transformer 可以解决这些问题，但在实际医疗应用中受到环境和边缘设备的计算资源限制。此外， convolutional 的学习偏好适合疾病数据的缺乏，Transformer 基于的网络具有更好的表现。为了提取全局上下文信息而不丢失 convolutional 的学习偏好，我们提出了 CMUNeXt，一种高效的全 convolutional 轻量级医疗图像分 segmentation 网络。CMUNeXt 利用大kernel和翻转瓶颈设计，fficiently 混合远程空间和位置信息，提取全局上下文信息。我们还引入了 Skip-Fusion 块，以便在 skip-connection 中实现细腻的融合。实验结果表明，CMUNeXt 在多个医疗图像数据集上的 segmentation 性能超过现有的重量级和轻量级医疗图像分 segmentation 网络，同时具有快速的推理速度、轻量级的权重和减少的计算成本。代码可以在 <https://github.com/FengheTan9/CMUNeXt> 上获取。
</details></li>
</ul>
<hr>
<h2 id="High-efficient-deep-learning-based-DTI-reconstruction-with-flexible-diffusion-gradient-encoding-scheme"><a href="#High-efficient-deep-learning-based-DTI-reconstruction-with-flexible-diffusion-gradient-encoding-scheme" class="headerlink" title="High-efficient deep learning-based DTI reconstruction with flexible diffusion gradient encoding scheme"></a>High-efficient deep learning-based DTI reconstruction with flexible diffusion gradient encoding scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01173">http://arxiv.org/abs/2308.01173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zejun Wu, Jiechao Wang, Zunquan Chen, Qinqin Yang, Shuhui Cai, Zhong Chen, Congbo Cai</li>
<li>for: 本研究旨在开发和评估一种基于动态核函数的新方法，称为FlexDTI，以实现高效的Diffusion Tensor Imaging（DTI）重建。</li>
<li>methods: FlexDTI方法使用动态核函数来嵌入扩散梯度方向信息到相应的扩散信号特征图中，以实现高质量DTI参数地图的重建。此外，我们的方法实现了扩散梯度方向的通用化，通过设置最大输入通道数量来实现 flexible number of diffusion gradient directions。</li>
<li>results: 与其他高级tensor参数估计方法相比，FlexDTI成功地实现了高质量扩散tensor-derived变量，即使扩散梯度方向和数量是变量的。它提高了 peak signal-to-noise ratio（PSNR）约10 dB，相对于基于深度学习的状态对抗方法。<details>
<summary>Abstract</summary>
Purpose: To develop and evaluate a novel dynamic-convolution-based method called FlexDTI for high-efficient diffusion tensor reconstruction with flexible diffusion encoding gradient schemes. Methods: FlexDTI was developed to achieve high-quality DTI parametric mapping with flexible number and directions of diffusion encoding gradients. The proposed method used dynamic convolution kernels to embed diffusion gradient direction information into feature maps of the corresponding diffusion signal. Besides, our method realized the generalization of a flexible number of diffusion gradient directions by setting the maximum number of input channels of the network. The network was trained and tested using data sets from the Human Connectome Project and a local hospital. Results from FlexDTI and other advanced tensor parameter estimation methods were compared. Results: Compared to other methods, FlexDTI successfully achieves high-quality diffusion tensor-derived variables even if the number and directions of diffusion encoding gradients are variable. It increases peak signal-to-noise ratio (PSNR) by about 10 dB on Fractional Anisotropy (FA) and Mean Diffusivity (MD), compared with the state-of-the-art deep learning method with flexible diffusion encoding gradient schemes. Conclusion: FlexDTI can well learn diffusion gradient direction information to achieve generalized DTI reconstruction with flexible diffusion gradient schemes. Both flexibility and reconstruction quality can be taken into account in this network.
</details>
<details>
<summary>摘要</summary>
目的：开发和评估一种基于动态核函数的新方法called FlexDTI，用于高效的扩散矢量重建with flexible扩散编码 Gradient schemes。方法：FlexDTI方法实现了高质量的DTI参数地图with flexible扩散编码方向数和扩散编码方向。提案的方法通过动态核函数嵌入扩散Gradient方向信息到相应的扩散信号特征图中。此外，我们的方法实现了一般化扩散编码方向的数目的扩展，通过设置网络的最大输入通道数来实现。网络通过使用数据集from the Human Connectome Project和当地医院进行训练和测试。与其他先进的矢量参数估计方法相比，FlexDTI成功地实现了高质量的扩散矢量变量，即使扩散编码方向数和方向不固定。结果：相比其他方法，FlexDTI可以在不同的扩散编码方向下实现高质量的扩散矢量重建。它提高了平均扩散矢量(FA)和扩散率(MD)的峰峰信号强度比(PSNR)约10dB，相比先进的深度学习方法with flexible扩散编码方向。结论：FlexDTI可以很好地学习扩散Gradient方向信息，实现通用的DTI重建with flexible扩散编码方向。这种网络可以同时考虑扩散Gradient方向信息和重建质量。
</details></li>
</ul>
<hr>
<h2 id="Contrast-augmented-Diffusion-Model-with-Fine-grained-Sequence-Alignment-for-Markup-to-Image-Generation"><a href="#Contrast-augmented-Diffusion-Model-with-Fine-grained-Sequence-Alignment-for-Markup-to-Image-Generation" class="headerlink" title="Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment for Markup-to-Image Generation"></a>Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment for Markup-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01147">http://arxiv.org/abs/2308.01147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zgj77/fsacdm">https://github.com/zgj77/fsacdm</a></li>
<li>paper_authors: Guojin Zhong, Jin Yuan, Pan Wang, Kailun Yang, Weili Guan, Zhiyong Li</li>
<li>for: 本研究旨在提高markup-to-image生成中的性能，特别是对于 markup language 中的Error tolerance和序列相关性具有更高的要求。</li>
<li>methods: 本paper提出了一种名为“增强对比扩散模型with fine-grained sequence alignment”（FSA-CDM）的新模型，其中引入了对比正例&#x2F;负例的样本，以提高markup-to-image生成的性能。技术上，我们设计了一种细致的交叉模式对应模块，以便充分利用两个模式之间的序列相似性，从而学习更加稳定的特征表示。</li>
<li>results: 在四个不同领域的benchmark dataset上，我们进行了广泛的实验，并证明了提出的组件在FSA-CDM中具有显著的效果，与当前最佳性能差异为2%-12% DTW。<details>
<summary>Abstract</summary>
The recently rising markup-to-image generation poses greater challenges as compared to natural image generation, due to its low tolerance for errors as well as the complex sequence and context correlations between markup and rendered image. This paper proposes a novel model named "Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment" (FSA-CDM), which introduces contrastive positive/negative samples into the diffusion model to boost performance for markup-to-image generation. Technically, we design a fine-grained cross-modal alignment module to well explore the sequence similarity between the two modalities for learning robust feature representations. To improve the generalization ability, we propose a contrast-augmented diffusion model to explicitly explore positive and negative samples by maximizing a novel contrastive variational objective, which is mathematically inferred to provide a tighter bound for the model's optimization. Moreover, the context-aware cross attention module is developed to capture the contextual information within markup language during the denoising process, yielding better noise prediction results. Extensive experiments are conducted on four benchmark datasets from different domains, and the experimental results demonstrate the effectiveness of the proposed components in FSA-CDM, significantly exceeding state-of-the-art performance by about 2%-12% DTW improvements. The code will be released at https://github.com/zgj77/FSACDM.
</details>
<details>
<summary>摘要</summary>
最近升起的markup-to-图像生成问题，相比于自然图像生成，具有较低的容忍度和复杂的序列和上下文相关性。这篇论文提出了一种新的模型，即“对比增强扩散模型with细致过程对齐”（FSA-CDM），该模型在markup-to-图像生成中提高性能。技术上，我们设计了细致的过程对齐模块，以便充分探索两个模式之间的序列相似性，从而学习强健的特征表示。为提高泛化能力，我们提出了对比增强扩散模型，通过最大化一种新的对比算法，以提高模型的优化。此外，我们还开发了上下文意识卷积模块，以捕捉markup语言中的上下文信息，从而实现更好的噪声预测结果。我们在四个不同领域的四个benchmark dataset上进行了广泛的实验，并证明了提案的组件在FSA-CDM中的效果，与状态艺ircle的表现提高约2%-12% DTW。代码将在https://github.com/zgj77/FSACDM中发布。
</details></li>
</ul>
<hr>
<h2 id="UCDFormer-Unsupervised-Change-Detection-Using-a-Transformer-driven-Image-Translation"><a href="#UCDFormer-Unsupervised-Change-Detection-Using-a-Transformer-driven-Image-Translation" class="headerlink" title="UCDFormer: Unsupervised Change Detection Using a Transformer-driven Image Translation"></a>UCDFormer: Unsupervised Change Detection Using a Transformer-driven Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01146">http://arxiv.org/abs/2308.01146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhu-xlab/ucdformer">https://github.com/zhu-xlab/ucdformer</a></li>
<li>paper_authors: Qingsong Xu, Yilei Shi, Jianhua Guo, Chaojun Ouyang, Xiao Xiang Zhu<br>for: 这个论文主要针对的是如何使用不需要标注的变化信息进行远程感知中的变化检测（CD） task，特别是考虑到多个时间图像中的季节和风格差异。methods: 该论文提出了一种基于域shift的CD方法，使用了一种轻量级的变换器（UCDFormer）来减轻域shift问题，并提出了一种新的无监督CD方法。这种方法首先使用了一种变换器驱动的图像翻译模型，然后使用了域specific affinity weight来调整图像之间的域shift。最后，该方法使用了一种可靠的像素提取模块来选择changed和unchanged像素，并使用了一种二分类器来获得最终的变化地图。results: 该论文的实验结果表明，Compared with several other related methods, UCDFormer improves performance on the Kappa coefficient by more than 12%. In addition, UCDFormer achieves excellent performance for earthquake-induced landslide detection when considering large-scale applications.<details>
<summary>Abstract</summary>
Change detection (CD) by comparing two bi-temporal images is a crucial task in remote sensing. With the advantages of requiring no cumbersome labeled change information, unsupervised CD has attracted extensive attention in the community. However, existing unsupervised CD approaches rarely consider the seasonal and style differences incurred by the illumination and atmospheric conditions in multi-temporal images. To this end, we propose a change detection with domain shift setting for remote sensing images. Furthermore, we present a novel unsupervised CD method using a light-weight transformer, called UCDFormer. Specifically, a transformer-driven image translation composed of a light-weight transformer and a domain-specific affinity weight is first proposed to mitigate domain shift between two images with real-time efficiency. After image translation, we can generate the difference map between the translated before-event image and the original after-event image. Then, a novel reliable pixel extraction module is proposed to select significantly changed/unchanged pixel positions by fusing the pseudo change maps of fuzzy c-means clustering and adaptive threshold. Finally, a binary change map is obtained based on these selected pixel pairs and a binary classifier. Experimental results on different unsupervised CD tasks with seasonal and style changes demonstrate the effectiveness of the proposed UCDFormer. For example, compared with several other related methods, UCDFormer improves performance on the Kappa coefficient by more than 12\%. In addition, UCDFormer achieves excellent performance for earthquake-induced landslide detection when considering large-scale applications. The code is available at \url{https://github.com/zhu-xlab/UCDFormer}
</details>
<details>
<summary>摘要</summary>
优化变更检测（CD）方法是远程感知领域中的关键任务。无监督CD方法受到了社区的广泛关注，因为它们不需要繁琐的标注更改信息。然而，现有的无监督CD方法很少考虑远程图像中的季节和风格差异，这些差异可能由照明和大气条件引起。为此，我们提出了基于频率域差分的CD方法。此外，我们还提出了一种基于小型转换器的新无监督CD方法，称为UCDFormer。具体来说，我们首先使用一种小型转换器和域特定相似权重来将两个图像进行图像翻译。然后，我们可以生成原始事件后图像和事件前图像之间的差分图。接着，我们提出了一种新的可靠像检测模块，可以通过将各种瑞利变换和适应阈值融合来选择有优势的更改/不更改像素位置。最后，我们根据这些选择的像素对生成一个二进制更改地图，并使用一个二进制分类器。实验结果表明，在不同的无监督CD任务中，UCDFormer的性能明显优于其他相关方法。例如，相比其他方法，UCDFormer在卡平差 coefficient 上提高了更多于12%。此外，UCDFormer在考虑大规模应用时对地球quake-induced landslide检测表现出色。代码可以在 \url{https://github.com/zhu-xlab/UCDFormer} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-Fourier-Constrained-Diffusion-Bridges-for-MRI-Reconstruction"><a href="#Learning-Fourier-Constrained-Diffusion-Bridges-for-MRI-Reconstruction" class="headerlink" title="Learning Fourier-Constrained Diffusion Bridges for MRI Reconstruction"></a>Learning Fourier-Constrained Diffusion Bridges for MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01096">http://arxiv.org/abs/2308.01096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/icon-lab/fdb">https://github.com/icon-lab/fdb</a></li>
<li>paper_authors: Muhammad U. Mirza, Onat Dalmaz, Hasan A. Bedel, Gokberk Elmas, Yilmaz Korkmaz, Alper Gungor, Salman UH Dar, Tolga Çukur</li>
<li>for: 这篇论文的目的是提出一种新的泵diffusion bridge（FDB）来加速MRI重建。</li>
<li>methods: FDB使用了一种通过噪音添加和随机频率除去的泵进程来将受测样本转换为全样本样本。</li>
<li>results: 实验结果显示，FDB可以比州前方的重建方法，包括传统的泵假设。<details>
<summary>Abstract</summary>
Recent years have witnessed a surge in deep generative models for accelerated MRI reconstruction. Diffusion priors in particular have gained traction with their superior representational fidelity and diversity. Instead of the target transformation from undersampled to fully-sampled data, common diffusion priors are trained to learn a multi-step transformation from Gaussian noise onto fully-sampled data. During inference, data-fidelity projections are injected in between reverse diffusion steps to reach a compromise solution within the span of both the diffusion prior and the imaging operator. Unfortunately, suboptimal solutions can arise as the normality assumption of the diffusion prior causes divergence between learned and target transformations. To address this limitation, here we introduce the first diffusion bridge for accelerated MRI reconstruction. The proposed Fourier-constrained diffusion bridge (FDB) leverages a generalized process to transform between undersampled and fully-sampled data via random noise addition and random frequency removal as degradation operators. Unlike common diffusion priors that use an asymptotic endpoint based on Gaussian noise, FDB captures a transformation between finite endpoints where the initial endpoint is based on moderate degradation of fully-sampled data. Demonstrations on brain MRI indicate that FDB outperforms state-of-the-art reconstruction methods including conventional diffusion priors.
</details>
<details>
<summary>摘要</summary>
近年来，深度生成模型在加速MRI重建方面得到了广泛应用。尤其是Diffusion priors在其superior representational fidelity和多样性方面受到了广泛关注。而不是target transformation从 undersampled 数据到fully-sampled数据，通常的Diffusion priors是通过学习一个多步转换从 Gaussian noise 到 fully-sampled 数据。在推理过程中，数据准确性投影会在反卷 diffusion 步骤之间插入，以达到一个compromise solution在 diffusion prior 和 imaging operator 之间。然而，可能会出现优化解决方案的局限性，因为normality assumption of diffusion prior 会导致learned 和 target 转换之间的分歧。为了解决这个限制，我们在这里引入了首个Fourier-constrained diffusion bridge (FDB)。该提案利用一种通用过程来将 undersampled 和 fully-sampled 数据之间进行转换，通过随机噪声添加和随机频率除法作为降低操作符。与常见的Diffusion priors不同，FDB capture一个转换 междуfinite endpoints，其初始endpoint基于moderate degradation of fully-sampled data。在脑MRI示例中，FDB超过了当前重建方法，包括conventional Diffusion priors。
</details></li>
</ul>
<hr>
<h2 id="Push-the-Boundary-of-SAM-A-Pseudo-label-Correction-Framework-for-Medical-Segmentation"><a href="#Push-the-Boundary-of-SAM-A-Pseudo-label-Correction-Framework-for-Medical-Segmentation" class="headerlink" title="Push the Boundary of SAM: A Pseudo-label Correction Framework for Medical Segmentation"></a>Push the Boundary of SAM: A Pseudo-label Correction Framework for Medical Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00883">http://arxiv.org/abs/2308.00883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Huang, Hongshan Liu, Haofeng Zhang, Fuyong Xing, Andrew Laine, Elsa Angelini, Christine Hendon, Yu Gan</li>
<li>for: 这个研究旨在提高静止学习segmentation的精度，特别是在医疗影像segmentation中，因为这里的标签是劳动密集且需要专业知识。</li>
<li>methods: 本研究使用Segment anything model (SAM)，并导入了一个新的标签损坏模型来提高SAM-based segmentation的精度。这个模型可以区分清洁标签和噪标签，并使用一个自我修复模组来更正噪标签。</li>
<li>results: 研究结果显示，使用提案的模型可以提高segmentation的精度，并在X-ray和肺CT数据集上超越基准方法。<details>
<summary>Abstract</summary>
Segment anything model (SAM) has emerged as the leading approach for zero-shot learning in segmentation, offering the advantage of avoiding pixel-wise annotation. It is particularly appealing in medical image segmentation where annotation is laborious and expertise-demanding. However, the direct application of SAM often yields inferior results compared to conventional fully supervised segmentation networks. While using SAM generated pseudo label could also benefit the training of fully supervised segmentation, the performance is limited by the quality of pseudo labels. In this paper, we propose a novel label corruption to push the boundary of SAM-based segmentation. Our model utilizes a novel noise detection module to distinguish between noisy labels from clean labels. This enables us to correct the noisy labels using an uncertainty-based self-correction module, thereby enriching the clean training set. Finally, we retrain the network with updated labels to optimize its weights for future predictions. One key advantage of our model is its ability to train deep networks using SAM-generated pseudo labels without relying on a subset of expert-level annotations. We demonstrate the effectiveness of our proposed model on both X-ray and lung CT datasets, indicating its ability to improve segmentation accuracy and outperform baseline methods in label correction.
</details>
<details>
<summary>摘要</summary>
对于零条件学习分类，对于医疗影像分类而言，模型有了新的进步——分割任意模型（SAM）。SAM可以避免像素级标注，尤其是在医疗影像分类中，标注是劳动密集且需要专家知识。然而，直接应用SAM often yields inferior results compared to conventional fully supervised segmentation networks。使用SAM生成的伪标签也可以帮助完全指定分类网络的训练，但是性能受到伪标签质量的限制。在这篇论文中，我们提出了一个新的标签损害方法，我们的模型具有一个新的错误检测模组，可以区别于噪标签和清洁标签。这使我们可以使用一个不certainty-based自修正模组来更正噪标签，从而增加清洁训练集。最后，我们重新训练网络使用更新的标签，以便在未来预测中优化其权重。我们的模型有一个关键优势，即可以透过SAM生成的伪标签进行深度网络的训练，不需要一subset of expert-level标注。我们在X-ray和肺CT dataset上显示了我们的提案的效果，显示了它可以提高分类精度和超越基eline方法。
</details></li>
</ul>
<hr>
<h2 id="Decomposition-Ascribed-Synergistic-Learning-for-Unified-Image-Restoration"><a href="#Decomposition-Ascribed-Synergistic-Learning-for-Unified-Image-Restoration" class="headerlink" title="Decomposition Ascribed Synergistic Learning for Unified Image Restoration"></a>Decomposition Ascribed Synergistic Learning for Unified Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00759">http://arxiv.org/abs/2308.00759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghao Zhang, Jie Huang, Man Zhou, Chongyi Li, Feng Zhao</li>
<li>for: 这个论文主要针对的问题是如何在单一模型中学习多种图像干扰的纠正。</li>
<li>methods: 该论文提出了一种基于Singular Value Decomposition（SVD）的Decomposition Ascribed Synergistic Learning（DASL）方法，包括两种有效的运算器：Singular VEctor Operator（SVEO）和Singular VAlue Operator（SVAO），以便更好地利用多种纠正任务之间的关系。</li>
<li>results: 在五种混合图像纠正任务上进行了广泛的实验，证明了该方法的有效性，包括雨彩纠正、霜尘纠正、噪声纠正、抖润纠正和低光照图像增强。<details>
<summary>Abstract</summary>
Learning to restore multiple image degradations within a single model is quite beneficial for real-world applications. Nevertheless, existing works typically concentrate on regarding each degradation independently, while their relationship has been less exploited to ensure the synergistic learning. To this end, we revisit the diverse degradations through the lens of singular value decomposition, with the observation that the decomposed singular vectors and singular values naturally undertake the different types of degradation information, dividing various restoration tasks into two groups,\ie, singular vector dominated and singular value dominated. The above analysis renders a more unified perspective to ascribe the diverse degradations, compared to previous task-level independent learning. The dedicated optimization of degraded singular vectors and singular values inherently utilizes the potential relationship among diverse restoration tasks, attributing to the Decomposition Ascribed Synergistic Learning (DASL). Specifically, DASL comprises two effective operators, namely, Singular VEctor Operator (SVEO) and Singular VAlue Operator (SVAO), to favor the decomposed optimization, which can be lightly integrated into existing convolutional image restoration backbone. Moreover, the congruous decomposition loss has been devised for auxiliary. Extensive experiments on blended five image restoration tasks demonstrate the effectiveness of our method, including image deraining, image dehazing, image denoising, image deblurring, and low-light image enhancement.
</details>
<details>
<summary>摘要</summary>
To optimize the degraded singular vectors and singular values, we propose the Decomposition Ascribed Synergistic Learning (DASL) method. DASL consists of two effective operators: the Singular VEctor Operator (SVEO) and the Singular VAlue Operator (SVAO). These operators favor the decomposed optimization and can be easily integrated into existing convolutional image restoration backbones. Additionally, we have developed a congruous decomposition loss for auxiliary purposes.Extensive experiments on five blended image restoration tasks (image deraining, image dehazing, image denoising, image deblurring, and low-light image enhancement) demonstrate the effectiveness of our method. Our approach is able to effectively restore images and outperform existing methods in terms of both objective metrics and visual quality.
</details></li>
</ul>
<hr>
<h2 id="Phase-Diverse-Phase-Retrieval-for-Microscopy-Comparison-of-Gaussian-and-Poisson-Approaches"><a href="#Phase-Diverse-Phase-Retrieval-for-Microscopy-Comparison-of-Gaussian-and-Poisson-Approaches" class="headerlink" title="Phase Diverse Phase Retrieval for Microscopy: Comparison of Gaussian and Poisson Approaches"></a>Phase Diverse Phase Retrieval for Microscopy: Comparison of Gaussian and Poisson Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00734">http://arxiv.org/abs/2308.00734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikolajreiser/poissonphasediversity">https://github.com/nikolajreiser/poissonphasediversity</a></li>
<li>paper_authors: Nikolaj Reiser, Min Guo, Hari Shroff, Patrick J. La Riviere</li>
<li>for: 这篇论文旨在研究phas diversity是一种广泛偏振 Aberration correction方法，用于 Correcting aberrations in widefield microscopy.</li>
<li>methods: 论文使用了多个图像来估算折射率在干扰系统的 pupil plane，通过解决优化问题来实现折射率的估算。</li>
<li>results: 实验结果表明，Poisson模型比 Gaussian 模型在各种情况下匹配或超越 Gaussian 模型，并且在较低的照明条件下表现更好。Poisson 算法也更加鲁棒于空间不连续的偏振和相位噪声的影响。最后，论文对重新获取受到偏振 corrections 和使用偏振点阵的杂化比较。<details>
<summary>Abstract</summary>
Phase diversity is a widefield aberration correction method that uses multiple images to estimate the phase aberration at the pupil plane of an imaging system by solving an optimization problem. This estimated aberration can then be used to deconvolve the aberrated image or to reacquire it with aberration corrections applied to a deformable mirror. The optimization problem for aberration estimation has been formulated for both Gaussian and Poisson noise models but the Poisson model has never been studied in microscopy nor compared with the Gaussian model. Here, the Gaussian- and Poisson-based estimation algorithms are implemented and compared for widefield microscopy in simulation. The Poisson algorithm is found to match or outperform the Gaussian algorithm in a variety of situations, and converges in a similar or decreased amount of time. The Gaussian algorithm does perform better in low-light regimes when image noise is dominated by additive Gaussian noise. The Poisson algorithm is also found to be more robust to the effects of spatially variant aberration and phase noise. Finally, the relative advantages of re-acquisition with aberration correction and deconvolution with aberrated point spread functions are compared.
</details>
<details>
<summary>摘要</summary>
“phase多样性是一种广角偏差 corrections方法，使用多个图像来估计图像系统的焦点平面上的相位偏差，并通过解决优化问题来应用偏差 corrections。这个估计的偏差可以用来deconvolve受偏差影响的图像，或者使用可变曲面镜来重新获取受偏差 corrections的图像。在微scopic中，Gaussian和Poisson noise模型中的优化问题已经被形式化，但是Poisson模型从来没有在微scopic中被研究过。在这里，Gaussian和Poisson基本的优化算法被实现并比较，在广角微scopic中的 simulate中表现出了不同的情况。Poisson算法在许多情况下与Gaussian算法匹配或超越Gaussian算法，并且在相同或更短的时间内 converges。Gaussian算法在低光量条件下，当图像噪声主要是加itive Gaussian噪声时，表现较好。Poisson算法也较Gaussian算法更加Robust，抗性于空间variant aberration和相位噪声的影响。最后，将受偏差 corrections应用于重新获取图像和受偏差点扩展函数deconvolution的相对优势被比较。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/02/eess.IV_2023_08_02/" data-id="cllsjvzef008qf588grh740dn" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/02/cs.SD_2023_08_02/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-02 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/01/cs.LG_2023_08_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-01 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
