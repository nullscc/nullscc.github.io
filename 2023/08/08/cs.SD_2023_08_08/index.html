
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-08 123:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Towards an AI to Win Ghana’s National Science and Maths Quiz paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04333 repo_url: https:&#x2F;&#x2F;github.com&#x2F;nsmq-ai&#x2F;nsmqai paper_authors: George Boateng, Jonathan Abrefah Mens">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-08 123:00:00">
<meta property="og:url" content="http://example.com/2023/08/08/cs.SD_2023_08_08/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Towards an AI to Win Ghana’s National Science and Maths Quiz paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04333 repo_url: https:&#x2F;&#x2F;github.com&#x2F;nsmq-ai&#x2F;nsmqai paper_authors: George Boateng, Jonathan Abrefah Mens">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-07T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:45.968Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/cs.SD_2023_08_08/" class="article-date">
  <time datetime="2023-08-07T16:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-08 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-an-AI-to-Win-Ghana’s-National-Science-and-Maths-Quiz"><a href="#Towards-an-AI-to-Win-Ghana’s-National-Science-and-Maths-Quiz" class="headerlink" title="Towards an AI to Win Ghana’s National Science and Maths Quiz"></a>Towards an AI to Win Ghana’s National Science and Maths Quiz</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04333">http://arxiv.org/abs/2308.04333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nsmq-ai/nsmqai">https://github.com/nsmq-ai/nsmqai</a></li>
<li>paper_authors: George Boateng, Jonathan Abrefah Mensah, Kevin Takyi Yeboah, William Edor, Andrew Kojo Mensah-Onumah, Naafi Dasana Ibrahim, Nana Sam Yeboah</li>
<li>For: The paper aims to build an AI system to compete in Ghana’s National Science and Maths Quiz (NSMQ) and win.* Methods: The project uses open-source technology and involves building AI systems to answer questions across biology, chemistry, physics, and math.* Results: The AI system is being developed and tested, with progress made thus far and the next steps planned for a launch in October 2023.Here is the information in Simplified Chinese text:* For: 这个论文的目的是建立一个AI系统，参加加纳国家科学和数学竞赛（NSMQ），并赢得奖。* Methods: 这个项目使用开源技术，建立AI系统，以回答生物、化学、物理和数学等领域的问题。* Results: AI系统正在开发和测试中，已经完成了一些进度，下一步计划在2023年10月发布。<details>
<summary>Abstract</summary>
Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the question we seek to answer in the NSMQ AI project, an open-source project that is building AI to compete live in the NSMQ and win. The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. The NSMQ is an exciting live quiz competition with interesting technical challenges across speech-to-text, text-to-speech, question-answering, and human-computer interaction. In this ongoing work that began in January 2023, we give an overview of the project, describe each of the teams, progress made thus far, and the next steps toward our planned launch and debut of the AI in October for NSMQ 2023. An AI that conquers this grand challenge can have real-world impact on education such as enabling millions of students across Africa to have one-on-one learning support from this AI.
</details>
<details>
<summary>摘要</summary>
可以AI赢得加纳的国家科学与数学竞赛（NSMQ）呢？我们正在NSMQ AI项目中努力解决这个问题，这是一个开源项目，用AI参加NSMQ并赢得奖。NSMQ是每年举行的生活科学和数学竞赛，参赛者是加纳高中二年级学生，他们需要在5轮5个阶段中回答生物、化学、物理和数学等领域的问题。这是一项有趣的现场竞赛，涉及到语音识别、文本识别、问题回答和人机交互等技术挑战。在我们自2023年1月开始的工作中，我们将介绍项目的概况，描述各个团队，已经成就的进度以及下一步的计划，以准在10月份的NSMQ 2023上发布和使用AI。如果AI成功完成这项挑战，可以对教育产生实际的影响，例如，使得非洲的数百万学生得到AI一对一的学习支持。
</details></li>
</ul>
<hr>
<h2 id="Auditory-Attention-Decoding-with-Task-Related-Multi-View-Contrastive-Learning"><a href="#Auditory-Attention-Decoding-with-Task-Related-Multi-View-Contrastive-Learning" class="headerlink" title="Auditory Attention Decoding with Task-Related Multi-View Contrastive Learning"></a>Auditory Attention Decoding with Task-Related Multi-View Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04244">http://arxiv.org/abs/2308.04244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Chen, Changde Du, Qiongyi Zhou, Huiguang He</li>
<li>For: 本研究旨在解决现有深度学习方法困难充分利用不同视角（即注意力和EEG数据）的问题，并提取有利的表示。* Methods: 我们提出了基于多视角VAE的听觉注意力解码方法（AAD），并使用任务相关多视角对比（TMC）学习来融合不同视角的知识。* Results: 我们在两个常用AAD数据集上进行了实验，并证明了我们的方法在比较当前状态方法时表现出了超越。<details>
<summary>Abstract</summary>
The human brain can easily focus on one speaker and suppress others in scenarios such as a cocktail party. Recently, researchers found that auditory attention can be decoded from the electroencephalogram (EEG) data. However, most existing deep learning methods are difficult to use prior knowledge of different views (that is attended speech and EEG are task-related views) and extract an unsatisfactory representation. Inspired by Broadbent's filter model, we decode auditory attention in a multi-view paradigm and extract the most relevant and important information utilizing the missing view. Specifically, we propose an auditory attention decoding (AAD) method based on multi-view VAE with task-related multi-view contrastive (TMC) learning. Employing TMC learning in multi-view VAE can utilize the missing view to accumulate prior knowledge of different views into the fusion of representation, and extract the approximate task-related representation. We examine our method on two popular AAD datasets, and demonstrate the superiority of our method by comparing it to the state-of-the-art method.
</details>
<details>
<summary>摘要</summary>
人脑可以轻松地专注一个说话者，而抑制其他说话者的场景，如cocktail party。最近，研究人员发现了基于电enzephalogram（EEG）数据的听力注意力可以被解码。然而，现有的深度学习方法很难以利用不同视图（即注意力和EEG数据是关联任务的视图）的先前知识，提取不满足的表示。受布洛满特的筛子模型启发，我们在多视图 paradigm 中解码听力注意力，并使用缺失视图来汇集不同视图中的先前知识，提取关键和重要的信息。我们提出了基于多视图VAE的听力注意力解码方法（AAD），并使用任务相关多视图强制学习（TMC）来学习。通过TMC学习在多视图VAE中，可以利用缺失视图来汇集不同视图中的先前知识，提取相似任务的表示。我们在两个流行的AAD数据集上测试了我们的方法，并证明了我们的方法在比较顶尖方法的基础上具有优势。
</details></li>
</ul>
<hr>
<h2 id="Evil-Operation-Breaking-Speaker-Recognition-with-PaddingBack"><a href="#Evil-Operation-Breaking-Speaker-Recognition-with-PaddingBack" class="headerlink" title="Evil Operation: Breaking Speaker Recognition with PaddingBack"></a>Evil Operation: Breaking Speaker Recognition with PaddingBack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04179">http://arxiv.org/abs/2308.04179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Ye, Diqun Yan, Li Dong, Kailai Shen</li>
<li>for: 这篇论文旨在攻击语音识别系统，实现隐蔽的攻击方式。</li>
<li>methods: 论文使用了padding操作来制造恶意样本，并且通过滥聚积极的方式来降低人工干预的可见性。</li>
<li>results: 实验结果表明，提案的方法可以达到高度的攻击成功率，同时保持高度的正常准确率。此外，方法还能够抵抗防御方法并继续保持隐蔽性。<details>
<summary>Abstract</summary>
Machine Learning as a Service (MLaaS) has gained popularity due to advancements in machine learning. However, untrusted third-party platforms have raised concerns about AI security, particularly in backdoor attacks. Recent research has shown that speech backdoors can utilize transformations as triggers, similar to image backdoors. However, human ears easily detect these transformations, leading to suspicion. In this paper, we introduce PaddingBack, an inaudible backdoor attack that utilizes malicious operations to make poisoned samples indistinguishable from clean ones. Instead of using external perturbations as triggers, we exploit the widely used speech signal operation, padding, to break speaker recognition systems. Our experimental results demonstrate the effectiveness of the proposed approach, achieving a significantly high attack success rate while maintaining a high rate of benign accuracy. Furthermore, PaddingBack demonstrates the ability to resist defense methods while maintaining its stealthiness against human perception. The results of the stealthiness experiment have been made available at https://nbufabio25.github.io/paddingback/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MSAC-Multiple-Speech-Attribute-Control-Method-for-Speech-Emotion-Recognition"><a href="#MSAC-Multiple-Speech-Attribute-Control-Method-for-Speech-Emotion-Recognition" class="headerlink" title="MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition"></a>MSAC: Multiple Speech Attribute Control Method for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04025">http://arxiv.org/abs/2308.04025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Pan</li>
<li>for: 本研究旨在提高speech emotion recognition（SER）的可靠性和泛化能力，探讨如何从数据分布角度模型speech emotion。</li>
<li>methods: 本研究提出了一种基于CNN的SER模型，采用了添加marginsoftmax损失函数以提高类别间特征之间的距离，以提高分类的准确性。此外，还提出了一种多个speech attribute控制方法（MSAC），可以控制speech attribute，使模型更加具有感情相关特征。</li>
<li>results: 实验结果表明，提出的SER工作流程在单个和跨库SER场景中具有优秀的认知、泛化和可靠性性能。单个库SER场景中，提出的SER工作流程在IEMOCAP数据集上达到了72.97%的WR和71.76%的UAR。<details>
<summary>Abstract</summary>
Despite significant progress, speech emotion recognition (SER) remains challenging due to inherent complexity and ambiguity of the emotion attribute, particularly in wild world. Whereas current studies primarily focus on recognition and generalization capabilities, this work pioneers an exploration into the reliability of SER methods and investigates how to model the speech emotion from the aspect of data distribution across various speech attributes. Specifically, we first build a novel CNN-based SER model which adopts additive margin softmax loss to expand the distance between features of different classes, thereby enhancing their discrimination. Second, a novel multiple speech attribute control method MSAC is proposed to explicitly control speech attributes, enabling the model to be less affected by emotion-agnostic attributes and capture more fine-grained emotion-related features. Third, we make a first attempt to test and analyze the reliability of the proposed SER workflow using the out-of-distribution detection method. Extensive experiments on both single and cross-corpus SER scenarios show that our proposed unified SER workflow consistently outperforms the baseline in terms of recognition, generalization, and reliability performance. Besides, in single-corpus SER, the proposed SER workflow achieves superior recognition results with a WAR of 72.97\% and a UAR of 71.76\% on the IEMOCAP corpus.
</details>
<details>
<summary>摘要</summary>
尽管已经做出了 significiant 进步，speech emotion recognition（SER）仍然具有挑战性，尤其在野外环境中。当前的研究主要关注recognition和泛化能力，而这个工作则倡导一种探索SER方法的可靠性，并研究如何从数据分布角度来模型speech emotion。具体来说，我们首先构建了一个基于CNN的SER模型，采用添加式margin softmax损失函数，以增强不同类别之间的距离，从而提高 их分辨率。其次，我们提出了一种多个speech attribute控制方法（MSAC），以控制speech attribute，使模型免受情感无关的 attribute的影响，捕捉更细腻的情感相关特征。最后，我们对提议的SER工作流进行了首次可靠性测试和分析，并通过out-of-distribution检测方法进行评估。广泛的实验表明，我们的提议的SER工作流在recognition、泛化和可靠性性能方面均具有优异表现。此外，在单 Corporpus SER 情况下，我们的提议SER工作流的识别率为72.97%和71.76%，在IEMOCAP corpora上。
</details></li>
</ul>
<hr>
<h2 id="Target-Speech-Extraction-with-Conditional-Diffusion-Model"><a href="#Target-Speech-Extraction-with-Conditional-Diffusion-Model" class="headerlink" title="Target Speech Extraction with Conditional Diffusion Model"></a>Target Speech Extraction with Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03987">http://arxiv.org/abs/2308.03987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoyuki Kamo, Marc Delcroix, Tomohiro Nakatani</li>
<li>for: 这篇论文旨在提出一种基于分散模型的目标语音提取方法，用于在多话者杂音场景中提取目标话者的干净语音信号。</li>
<li>methods: 该方法基于一种基于决定器的分散模型，通过条件式地使用决定器来识别目标话者，然后使用分散模型来提取目标话者的语音信号。此外，该方法还使用ensemble推理来降低可能的提取错误。</li>
<li>results: 在Libri2mix数据集上进行实验，提出的分散模型基于TSE方法，结合ensemble推理，与相对的分类式TSE系统相比，显示更高的性能。<details>
<summary>Abstract</summary>
Diffusion model-based speech enhancement has received increased attention since it can generate very natural enhanced signals and generalizes well to unseen conditions. Diffusion models have been explored for several sub-tasks of speech enhancement, such as speech denoising, dereverberation, and source separation. In this paper, we investigate their use for target speech extraction (TSE), which consists of estimating the clean speech signal of a target speaker in a mixture of multi-talkers. TSE is realized by conditioning the extraction process on a clue identifying the target speaker. We show we can realize TSE using a conditional diffusion model conditioned on the clue. Besides, we introduce ensemble inference to reduce potential extraction errors caused by the diffusion process. In experiments on Libri2mix corpus, we show that the proposed diffusion model-based TSE combined with ensemble inference outperforms a comparable TSE system trained discriminatively.
</details>
<details>
<summary>摘要</summary>
模式：简化中文报告：听话提升技术基于分散模型已经受到更多关注，因为它可以生成非常自然的提升信号，并且适用于未经见过的情况。分散模型在各种听话提升任务中被探索，如听话噪声除除、听话反射消除和源分离。在这篇论文中，我们研究了它们在目标听话提取（TSE）任务中的使用，该任务的目标是估计一个混合多个人的听话信号中的清晰听话信号。TSE通过将提取过程conditioning于特征标识target speaker来实现。我们表明可以使用conditioned diffusion model来实现TSE。此外，我们引入ensemble inference来降低扩散过程可能导致的抽取错误。在对Libri2mix数据集进行实验的结果表明，我们提出的扩散模型基于TSE，并与ensemble inference结合，可以与相似的TSE系统进行比较，并且表现更好。
</details></li>
</ul>
<hr>
<h2 id="Universal-Automatic-Phonetic-Transcription-into-the-International-Phonetic-Alphabet"><a href="#Universal-Automatic-Phonetic-Transcription-into-the-International-Phonetic-Alphabet" class="headerlink" title="Universal Automatic Phonetic Transcription into the International Phonetic Alphabet"></a>Universal Automatic Phonetic Transcription into the International Phonetic Alphabet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03917">http://arxiv.org/abs/2308.03917</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ctaguchi/multipa">https://github.com/ctaguchi/multipa</a></li>
<li>paper_authors: Chihiro Taguchi, Yusuke Sakai, Parisa Haghani, David Chiang</li>
<li>for: 这个研究旨在开发一种能够将任何语言的语音转换为国际音响字母表（IPA）的模型。</li>
<li>methods: 这个模型基于wav2vec 2.0，并在听音输入上进行了微调，以预测IPA。</li>
<li>results: 该模型可以准确地将语音转换为IPA，并且与人工标注的质量相似。<details>
<summary>Abstract</summary>
This paper presents a state-of-the-art model for transcribing speech in any language into the International Phonetic Alphabet (IPA). Transcription of spoken languages into IPA is an essential yet time-consuming process in language documentation, and even partially automating this process has the potential to drastically speed up the documentation of endangered languages. Like the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is based on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use training data from seven languages from CommonVoice 11.0, transcribed into IPA semi-automatically. Although this training dataset is much smaller than Wav2Vec2Phoneme's, its higher quality lets our model achieve comparable or better results. Furthermore, we show that the quality of our universal speech-to-IPA models is close to that of human annotators.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种最新的语音转写模型，可以将任何语言的语音转写为国际音域字母（IPA）。将口语语言转写为IPA是语言记录的重要步骤，但是这个过程占用了大量时间。即使只是部分自动化这个过程，也可以快速化语言记录的过程，特别是对于受威胁的语言。我们的模型基于wav2vec 2.0，并在音频输入上进行了微调，以预测IPA。我们使用了CommonVoice 11.0中的七种语言的训练数据，这些数据被 semi-automatically 转写为IPA。虽然这个训练集比前一个最佳语音到IPA模型（Wav2Vec2Phoneme）更小，但是它的质量更高，使我们的模型在比较或更好的结果。此外，我们还证明了我们的通用语音到IPA模型的质量与人工标注几乎相同。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/08/cs.SD_2023_08_08/" data-id="cllt6z4js005bq38851shdm2v" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/08/cs.LG_2023_08_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-08 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/08/eess.AS_2023_08_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-08-08 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
