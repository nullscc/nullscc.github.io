
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-08 18:00:00 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10843 repo_url: None paper_authors: Mireille Fares, Catherine Pelachaud, N">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-08 18:00:00">
<meta property="og:url" content="http://example.com/2023/08/08/cs.LG_2023_08_08/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.10843 repo_url: None paper_authors: Mireille Fares, Catherine Pelachaud, N">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-07T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:45.921Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/cs.LG_2023_08_08/" class="article-date">
  <time datetime="2023-08-07T16:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-08 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TranSTYLer-Multimodal-Behavioral-Style-Transfer-for-Facial-and-Body-Gestures-Generation"><a href="#TranSTYLer-Multimodal-Behavioral-Style-Transfer-for-Facial-and-Body-Gestures-Generation" class="headerlink" title="TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation"></a>TranSTYLer: Multimodal Behavioral Style Transfer for Facial and Body Gestures Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10843">http://arxiv.org/abs/2308.10843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mireille Fares, Catherine Pelachaud, Nicolas Obin</li>
<li>for: 本文解决了让虚拟代表人物的行为特征风格转移到另一个代表人物中，保持行为的形态不变的挑战。</li>
<li>methods: 我们提出了一种基于多模态trasformer模型的TranSTYLer模型，可以将多模态讲话者的行为风格转移到目标讲话者中，同时保持行为的意思表达。</li>
<li>results: 我们的模型在PATS数据集上进行了训练，并在对seen和unseen风格进行了目标和主观评价，结果显示我们的模型在样式转移中超过了现有模型的性能。<details>
<summary>Abstract</summary>
This paper addresses the challenge of transferring the behavior expressivity style of a virtual agent to another one while preserving behaviors shape as they carry communicative meaning. Behavior expressivity style is viewed here as the qualitative properties of behaviors. We propose TranSTYLer, a multimodal transformer based model that synthesizes the multimodal behaviors of a source speaker with the style of a target speaker. We assume that behavior expressivity style is encoded across various modalities of communication, including text, speech, body gestures, and facial expressions. The model employs a style and content disentanglement schema to ensure that the transferred style does not interfere with the meaning conveyed by the source behaviors. Our approach eliminates the need for style labels and allows the generalization to styles that have not been seen during the training phase. We train our model on the PATS corpus, which we extended to include dialog acts and 2D facial landmarks. Objective and subjective evaluations show that our model outperforms state of the art models in style transfer for both seen and unseen styles during training. To tackle the issues of style and content leakage that may arise, we propose a methodology to assess the degree to which behavior and gestures associated with the target style are successfully transferred, while ensuring the preservation of the ones related to the source content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accurate-Explainable-and-Private-Models-Providing-Recourse-While-Minimizing-Training-Data-Leakage"><a href="#Accurate-Explainable-and-Private-Models-Providing-Recourse-While-Minimizing-Training-Data-Leakage" class="headerlink" title="Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage"></a>Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04341">http://arxiv.org/abs/2308.04341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catherine Huang, Chelse Swoopes, Christina Xiao, Jiaqi Ma, Himabindu Lakkaraju</li>
<li>for: 防止机器学习模型中的隐私泄露</li>
<li>methods: 使用差异隐私模型（DPM）和拉普拉斯回退（LR）两种方法来生成差异隐私的回退</li>
<li>results: 使用логистиック回归类ifier和实际世界和生成的数据集，发现DPM和LR方法可以减少对敌人的攻击，尤其是在低 False Positive Rate 下。当训练集大小充分时，我们发现LR方法能够很好地防止隐私泄露，同时保持模型和回退的准确率。<details>
<summary>Abstract</summary>
Machine learning models are increasingly utilized across impactful domains to predict individual outcomes. As such, many models provide algorithmic recourse to individuals who receive negative outcomes. However, recourse can be leveraged by adversaries to disclose private information. This work presents the first attempt at mitigating such attacks. We present two novel methods to generate differentially private recourse: Differentially Private Model (DPM) and Laplace Recourse (LR). Using logistic regression classifiers and real world and synthetic datasets, we find that DPM and LR perform well in reducing what an adversary can infer, especially at low FPR. When training dataset size is large enough, we find particular success in preventing privacy leakage while maintaining model and recourse accuracy with our novel LR method.
</details>
<details>
<summary>摘要</summary>
机器学习模型在影响各界预测个人结果的应用越来越普遍。因此，许多模型提供了算法性的纠正机制，以便对不良结果进行纠正。然而，这些纠正机制可能会被敌对者利用，泄露private信息。这项工作首次介绍了纠正攻击的防范方法。我们提出了两种新的敏感度保护纠正方法：敏感度保护模型（DPM）和拉普拉斯纠正（LR）。使用Logistic回归分类器和实际世界和 sintetic数据集，我们发现，DPM和LR在低False Positive Rate（FP）下具有良好的隐私保护性，特别是在训练数据集规模充分时。我们的LR方法在防止隐私泄露的同时保持模型和纠正精度的情况下表现出特别的成功。
</details></li>
</ul>
<hr>
<h2 id="RLHF-Blender-A-Configurable-Interactive-Interface-for-Learning-from-Diverse-Human-Feedback"><a href="#RLHF-Blender-A-Configurable-Interactive-Interface-for-Learning-from-Diverse-Human-Feedback" class="headerlink" title="RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback"></a>RLHF-Blender: A Configurable Interactive Interface for Learning from Diverse Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04332">http://arxiv.org/abs/2308.04332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannick Metz, David Lindner, Raphaël Baur, Daniel Keim, Mennatallah El-Assady</li>
<li>for: 这篇论文旨在应用人工增强学习（RL）在实际应用中，并且从多种人类反馈中学习奖励模型，同时考虑人类因素的影响。</li>
<li>methods: 这篇论文提出了RLHF-Blender，一个可配置的交互式界面，用于从人类反馈中学习奖励模型。RLHF-Blender提供了一个模块化的实验框架和实现，使研究人员可以系统地研究人类反馈的性质和质量。</li>
<li>results: RLHF-Blender可以帮助研究人员系统地探索不同类型的人类反馈，包括示例、排名、比较和自然语言指令，以及考虑人类因素的影响。RLHF-Blender还提供了一些具体的研究机会，详细信息请参阅<a target="_blank" rel="noopener" href="https://rlhfblender.info/">https://rlhfblender.info/</a>.<details>
<summary>Abstract</summary>
To use reinforcement learning from human feedback (RLHF) in practical applications, it is crucial to learn reward models from diverse sources of human feedback and to consider human factors involved in providing feedback of different types. However, the systematic study of learning from diverse types of feedback is held back by limited standardized tooling available to researchers. To bridge this gap, we propose RLHF-Blender, a configurable, interactive interface for learning from human feedback. RLHF-Blender provides a modular experimentation framework and implementation that enables researchers to systematically investigate the properties and qualities of human feedback for reward learning. The system facilitates the exploration of various feedback types, including demonstrations, rankings, comparisons, and natural language instructions, as well as studies considering the impact of human factors on their effectiveness. We discuss a set of concrete research opportunities enabled by RLHF-Blender. More information is available at https://rlhfblender.info/.
</details>
<details>
<summary>摘要</summary>
为了在实际应用中使用人类反馈学习（RLHF），必须从多种人类反馈来学习奖励模型，并考虑人类提供反馈的因素。然而，实际研究从多种反馈类型中学习的系统atic study 受到了研究人员的限制。为了bridging这个差距，我们提议RLHF-Blender，一个可配置的交互式界面，用于学习人类反馈。RLHF-Blender提供了一个可组合的实验框架和实现，帮助研究人员系统地 investigate人类反馈的性质和质量，以及人类因素对其效果的影响。系统支持 exploration多种反馈类型，包括示例、排名、比较和自然语言指令，以及考虑人类因素对其效果的研究。我们介绍了RLHF-Blender启发的具体研究机会。更多信息请访问https://rlhfblender.info/.
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Multi-agent-Bandits-Distributed-Algorithms-with-Optimal-Individual-Regret-and-Constant-Communication-Costs"><a href="#Cooperative-Multi-agent-Bandits-Distributed-Algorithms-with-Optimal-Individual-Regret-and-Constant-Communication-Costs" class="headerlink" title="Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs"></a>Cooperative Multi-agent Bandits: Distributed Algorithms with Optimal Individual Regret and Constant Communication Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04314">http://arxiv.org/abs/2308.04314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Xuchuang Wang, Mohammad Hajiesmaili, Lijun Zhang, John C. S. Lui, Don Towsley</li>
<li>for: 这个研究旨在开发多代理多臂枪击游戏中的合作多代理算法，以实现最佳的集体和个人后悔，并且将通信成本降到最低。</li>
<li>methods: 这篇论文使用了两种方法：领导者-追随者和完全分布式算法。这些方法都能够实现集体后悔的最佳后悔，但是领导者-追随者算法对个人后悔不够优秀，而完全分布式算法则对通信成本不够优秀。</li>
<li>results: 这篇论文提出了一个简单 yet有效的通信策略，并将其整合到一个学习算法中，以实现最佳的个人后悔和常规通信成本。<details>
<summary>Abstract</summary>
Recently, there has been extensive study of cooperative multi-agent multi-armed bandits where a set of distributed agents cooperatively play the same multi-armed bandit game. The goal is to develop bandit algorithms with the optimal group and individual regrets and low communication between agents. The prior work tackled this problem using two paradigms: leader-follower and fully distributed algorithms. Prior algorithms in both paradigms achieve the optimal group regret. The leader-follower algorithms achieve constant communication costs but fail to achieve optimal individual regrets. The state-of-the-art fully distributed algorithms achieve optimal individual regrets but fail to achieve constant communication costs. This paper presents a simple yet effective communication policy and integrates it into a learning algorithm for cooperative bandits. Our algorithm achieves the best of both paradigms: optimal individual regret and constant communication costs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Model-Inversion-Eavesdropping-Attack-in-Semantic-Communication-Systems"><a href="#The-Model-Inversion-Eavesdropping-Attack-in-Semantic-Communication-Systems" class="headerlink" title="The Model Inversion Eavesdropping Attack in Semantic Communication Systems"></a>The Model Inversion Eavesdropping Attack in Semantic Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04304">http://arxiv.org/abs/2308.04304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Chen, Qianqian Yang, Zhiguo Shi, Jiming Chen</li>
<li>for: 本研究探讨了 semantic communication 系统中的隐私泄露问题，并提出了一种基于 random permutation 和 substitution 的防御方法来解决这问题。</li>
<li>methods: 本研究使用了 model inversion eavesdropping attack (MIEA) 来攻击 semantic communication 系统，并考虑了 white-box 和 black-box 两种设定。</li>
<li>results: 实验结果表明，提出的防御方法可以有效防止 MIEA，并且在不同的 channel conditions 下可以 obtaint 高质量的重建结果。<details>
<summary>Abstract</summary>
In recent years, semantic communication has been a popular research topic for its superiority in communication efficiency. As semantic communication relies on deep learning to extract meaning from raw messages, it is vulnerable to attacks targeting deep learning models. In this paper, we introduce the model inversion eavesdropping attack (MIEA) to reveal the risk of privacy leaks in the semantic communication system. In MIEA, the attacker first eavesdrops the signal being transmitted by the semantic communication system and then performs model inversion attack to reconstruct the raw message, where both the white-box and black-box settings are considered. Evaluation results show that MIEA can successfully reconstruct the raw message with good quality under different channel conditions. We then propose a defense method based on random permutation and substitution to defend against MIEA in order to achieve secure semantic communication. Our experimental results demonstrate the effectiveness of the proposed defense method in preventing MIEA.
</details>
<details>
<summary>摘要</summary>
现在的几年，semantic communication在通信效率方面的研究非常流行，因为它可以通过深度学习提取消息的意义来提高通信效率。然而，由于semantic communication依赖于深度学习模型，因此它是攻击目标。在这篇论文中，我们介绍了模型反向窃听攻击（MIEA），以探索 semantic communication系统中隐私泄露的风险。在MIEA中，攻击者首先监听 semantic communication系统传输的信号，然后通过模型反向攻击来重construct原始消息，包括白盒和黑盒两种设置。我们的评估结果表明，MIEA可以在不同的通信频率下成功重construct原始消息，并且可以在不同的通信条件下保持高质量。我们then propose了一种基于随机排序和替换的防御方法，以防止MIEA。我们的实验结果表明，我们的防御方法可以有效防止MIEA，以实现安全的semantic communication。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-the-wav2vec-2-0-Feature-Extractor"><a href="#Comparative-Analysis-of-the-wav2vec-2-0-Feature-Extractor" class="headerlink" title="Comparative Analysis of the wav2vec 2.0 Feature Extractor"></a>Comparative Analysis of the wav2vec 2.0 Feature Extractor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04286">http://arxiv.org/abs/2308.04286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Vieting, Ralf Schlüter, Hermann Ney</li>
<li>for:  This paper aims to evaluate the capability of neural raw waveform feature extractors (FEs) in replacing standard feature extraction methods in a connectionist temporal classification (CTC) automatic speech recognition (ASR) model, and to compare it with an alternative neural FE.</li>
<li>methods:  The paper uses a convolutional FE, which operates directly on the speech waveform, and a set of bandpass filters to analyze the learned filters and the most important information for the ASR system.</li>
<li>results:  The paper shows that both the neural raw waveform FE and the alternative neural FE are competitive with traditional FEs on the LibriSpeech benchmark, and analyzes the effect of the individual components. The paper also shows that the most important information for the ASR system is obtained by a set of bandpass filters.<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) systems typically use handcrafted feature extraction pipelines. To avoid their inherent information loss and to achieve more consistent modeling from speech to transcribed text, neural raw waveform feature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model, which has recently gained large popularity, uses a convolutional FE which operates directly on the speech waveform. However, it is not yet studied extensively in the literature. In this work, we study its capability to replace the standard feature extraction methods in a connectionist temporal classification (CTC) ASR model and compare it to an alternative neural FE. We show that both are competitive with traditional FEs on the LibriSpeech benchmark and analyze the effect of the individual components. Furthermore, we analyze the learned filters and show that the most important information for the ASR system is obtained by a set of bandpass filters.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统通常使用手工设计的特征提取管道。以避免它们的内在信息损失并实现更一致的模型化从语音到转录文本，神经原始波形特征提取器（FE）是一种吸引人的方法。另外，最近广受欢迎的wav2vec 2.0模型使用了一种卷积 convolutional FE，该模型直接操作语音波形。然而，它在文献中还没有得到广泛的研究。在这个工作中，我们研究了它是否可以取代标准特征提取方法在一个连接主义时间分类（CTC） ASR 模型中，并与一个 alternativa neural FE 进行比较。我们发现两者都能够与传统的特征提取方法竞争在 LibriSpeech benchmark 上，并分析了各个组件的效果。此外，我们还分析了学习的滤波器，发现最重要的信息对 ASR 系统来说是由一组带通频滤波器提取出来的。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Alignment-Chat-with-Vanilla-Language-Models-Before-Fine-Tuning"><a href="#In-Context-Alignment-Chat-with-Vanilla-Language-Models-Before-Fine-Tuning" class="headerlink" title="In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning"></a>In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04275">http://arxiv.org/abs/2308.04275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xhan77/in-context-alignment">https://github.com/xhan77/in-context-alignment</a></li>
<li>paper_authors: Xiaochuang Han</li>
<li>for: 这个论文是关于在运行时进行匹配的研究，即使没有任何细化调教。</li>
<li>methods: 作者使用了一个名为Llama-2的预训练语言模型，并通过在对话式指令下提取示例来进行匹配。</li>
<li>results: 结果显示，在没有改变模型参数的情况下，通过在上下文中学习进行匹配，可以使得 vanilla 语言模型与 OpenAI 提供的 text-davinci-003 模型相当，并且比直接提示的方法提高了7倍的赢利率。<details>
<summary>Abstract</summary>
In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
</details>
<details>
<summary>摘要</summary>
在这份笔记中，我们探讨了在上下文学习中进行推理时的对齐。我们考虑了未经任何精度调整的语言模型Llama-2，并在模型被提示按照带ialog式指令时获取了9个示例对齐例。与直接提示相比，无需更改模型参数的上下文对齐导致了与文本-达梦系统OpenAI的模型003相比的7倍增加赢利率，使得vanilla语言模型与对齐调整相当。
</details></li>
</ul>
<hr>
<h2 id="Teacher-Student-Architecture-for-Knowledge-Distillation-A-Survey"><a href="#Teacher-Student-Architecture-for-Knowledge-Distillation-A-Survey" class="headerlink" title="Teacher-Student Architecture for Knowledge Distillation: A Survey"></a>Teacher-Student Architecture for Knowledge Distillation: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04268">http://arxiv.org/abs/2308.04268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengming Hu, Xuan Li, Dan Liu, Haolun Wu, Xi Chen, Ju Wang, Xue Liu</li>
<li>for: 这篇论文的目的是对于知识传递（Knowledge Distillation，KD）领域的研究和发展，特别是关于多个知识传递目标的教师生成架构。</li>
<li>methods: 这篇论文使用了教师生成架构，包括知识传递、知识增强、知识适应和知识压缩等多种知识传递目标。它还使用了各种学习算法和有效的传递方案。</li>
<li>results: 这篇论文总结了现有的教师生成架构和学习算法，并评估了它们在多个知识传递目标上的性能。它还概述了现有的应用案例，包括分类、识别、生成、排名和回归等多种应用。<details>
<summary>Abstract</summary>
Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. With the help of Teacher-Student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. Different from existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various knowledge representations and their corresponding optimization objectives. Additionally, we provide a systematic overview of Teacher-Student architectures with representative learning algorithms and effective distillation schemes. This survey also summarizes recent applications of Teacher-Student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. Lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. Through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying Teacher-Student architectures on various distillation objectives.
</details>
<details>
<summary>摘要</summary>
although deep neural networks (DNNs) have shown strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters. to tackle this issue, teacher-student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. recently, teacher-student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives, including knowledge compression, knowledge expansion, knowledge adaptation, and knowledge enhancement. with the help of teacher-student architectures, current studies are able to achieve multiple distillation objectives through lightweight and generalized student networks. different from existing KD surveys that primarily focus on knowledge compression, this survey first explores teacher-student architectures across multiple distillation objectives. this survey presents an introduction to various knowledge representations and their corresponding optimization objectives. additionally, we provide a systematic overview of teacher-student architectures with representative learning algorithms and effective distillation schemes. this survey also summarizes recent applications of teacher-student architectures across multiple purposes, including classification, recognition, generation, ranking, and regression. lastly, potential research directions in KD are investigated, focusing on architecture design, knowledge quality, and theoretical studies of regression-based learning, respectively. through this comprehensive survey, industry practitioners and the academic community can gain valuable insights and guidelines for effectively designing, learning, and applying teacher-student architectures on various distillation objectives.
</details></li>
</ul>
<hr>
<h2 id="BarlowRL-Barlow-Twins-for-Data-Efficient-Reinforcement-Learning"><a href="#BarlowRL-Barlow-Twins-for-Data-Efficient-Reinforcement-Learning" class="headerlink" title="BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning"></a>BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04263">http://arxiv.org/abs/2308.04263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omer Veysel Cagatan</li>
<li>for: 这个论文是为了提出一种数据效果的强化学习代理人BarlowRL，该代理人结合Barlow Twins自动学习框架和DER数据效果雨bow算法。</li>
<li>methods: BarlowRL使用了Barlow Twins自动学习框架和DER算法，以提高数据效果和避免维度塌陷。</li>
<li>results: BarlowRL在Atari 100k测试 benchmark 上表现出色，超过了DER和其对应的对比算法CURL。 BarlowRL能够充分利用均匀分布的状态表示，从而达到了很高的性能。<details>
<summary>Abstract</summary>
This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了BarlowRL，一种数据效率的 reinforcement learning代理人，它将Barlow Twins自我超vised学习框架和DER（数据效率雨bow）算法相结合。BarlowRL在Atari 100k测试 benchmark 上表现出色，比DER和其它对比算法 CURL 更高。BarlowRL通过保证信息在全空间传播来避免维度坍缩，从而使RL算法可以利用 uniformly 分布的状态表示，最终导致了非常出色的性能。将Barlow Twins与DER相结合可以提高数据效率，并在RL任务中实现优秀表现。BarlowRL表明了在RL算法中 интеGRating自我超vised学习技术的潜力。
</details></li>
</ul>
<hr>
<h2 id="SDLFormer-A-Sparse-and-Dense-Locality-enhanced-Transformer-for-Accelerated-MR-Image-Reconstruction"><a href="#SDLFormer-A-Sparse-and-Dense-Locality-enhanced-Transformer-for-Accelerated-MR-Image-Reconstruction" class="headerlink" title="SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction"></a>SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04262">http://arxiv.org/abs/2308.04262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rahul-gs-16/sdlformer">https://github.com/rahul-gs-16/sdlformer</a></li>
<li>paper_authors: Rahul G. S., Sriprabha Ramnarayanan, Mohammad Al Fahim, Keerthi Ram, Preejith S. P, Mohanasankar Sivaprakasam</li>
<li>for: 本研究旨在提出一种基于窗口变换器的快速MRI图像重建方法，以提高MRI图像重建的计算效率。</li>
<li>methods: 该方法使用了窗口变换器网络，并将它与增强 distant neighborhood pixel relationship 的层 dilated attention 机制和depth-wise convolutions 结合在一起。</li>
<li>results: 对多栅积MRI加速的多栅积PD、PDFS和T2对照图进行了广泛的实验，并与其他重建架构和平行领域自我超vised学习基准进行了比较。结果显示，提出的方法在PSNR和SSIM两个指标上具有1.40dB和0.028的提升差。<details>
<summary>Abstract</summary>
Transformers have emerged as viable alternatives to convolutional neural networks owing to their ability to learn non-local region relationships in the spatial domain. The self-attention mechanism of the transformer enables transformers to capture long-range dependencies in the images, which might be desirable for accelerated MRI image reconstruction as the effect of undersampling is non-local in the image domain. Despite its computational efficiency, the window-based transformers suffer from restricted receptive fields as the dependencies are limited to within the scope of the image windows. We propose a window-based transformer network that integrates dilated attention mechanism and convolution for accelerated MRI image reconstruction. The proposed network consists of dilated and dense neighborhood attention transformers to enhance the distant neighborhood pixel relationship and introduce depth-wise convolutions within the transformer module to learn low-level translation invariant features for accelerated MRI image reconstruction. The proposed model is trained in a self-supervised manner. We perform extensive experiments for multi-coil MRI acceleration for coronal PD, coronal PDFS and axial T2 contrasts with 4x and 5x under-sampling in self-supervised learning based on k-space splitting. We compare our method against other reconstruction architectures and the parallel domain self-supervised learning baseline. Results show that the proposed model exhibits improvement margins of (i) around 1.40 dB in PSNR and around 0.028 in SSIM on average over other architectures (ii) around 1.44 dB in PSNR and around 0.029 in SSIM over parallel domain self-supervised learning. The code is available at https://github.com/rahul-gs-16/sdlformer.git
</details>
<details>
<summary>摘要</summary>
transformers 已经成为了卷积神经网络的可行 альтернатив，因为它们可以学习图像空间中的非本地区关系。 transformers 的归一化机制使得它们可以捕捉图像中的长距离依赖关系，这可能是加速 MRI 图像重建的潜在优点。 despite its 计算效率，窗口基于的 transformers 受限于图像窗口范围内的依赖关系。 we propose a window-based transformer network that integrates dilated attention mechanism and convolution for accelerated MRI image reconstruction. the proposed network consists of dilated and dense neighborhood attention transformers to enhance the distant neighborhood pixel relationship and introduce depth-wise convolutions within the transformer module to learn low-level translation invariant features for accelerated MRI image reconstruction. the proposed model is trained in a self-supervised manner. we perform extensive experiments for multi-coil MRI acceleration for coronal PD, coronal PDFS and axial T2 contrasts with 4x and 5x under-sampling in self-supervised learning based on k-space splitting. we compare our method against other reconstruction architectures and the parallel domain self-supervised learning baseline. results show that the proposed model exhibits improvement margins of (i) around 1.40 dB in PSNR and around 0.028 in SSIM on average over other architectures (ii) around 1.44 dB in PSNR and around 0.029 in SSIM over parallel domain self-supervised learning. the code is available at https://github.com/rahul-gs-16/sdlformer.git.
</details></li>
</ul>
<hr>
<h2 id="Advancing-Natural-Language-Based-Audio-Retrieval-with-PaSST-and-Large-Audio-Caption-Data-Sets"><a href="#Advancing-Natural-Language-Based-Audio-Retrieval-with-PaSST-and-Large-Audio-Caption-Data-Sets" class="headerlink" title="Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets"></a>Advancing Natural-Language Based Audio Retrieval with PaSST and Large Audio-Caption Data Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04258">http://arxiv.org/abs/2308.04258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optimusprimus/dcase2023_task6b">https://github.com/optimusprimus/dcase2023_task6b</a></li>
<li>paper_authors: Paul Primus, Khaled Koutini, Gerhard Widmer</li>
<li>for: 这篇论文旨在提出一种基于预训练文本和 спектogram transformer 的文本至声音回 recuperation 系统。</li>
<li>methods: 该方法将录音和文本描述 proyect 到一个共享的声音-caption 空间中，使相关的不同模式的示例在空间上相互靠近。通过系统atic分析，我们查看每个系统组件对回 recuperation性能的影响。</li>
<li>results: 我们发现两个关键组件对回 recuperation性能具有关键作用：基于自我注意力的声音编码器为声音嵌入，以及在预训练阶段使用additional human-generated和 sintetic 数据集。我们进一步尝试了在 ClothoV2 描述中添加可用的关键词，但这只导致了微妙的改进。我们的系统在 2023 年 DCASE 挑战中 Ranking 第一，并在 ClothoV2  bencmark 上比当前状态的艺术高5.6 pp. mAP@10。<details>
<summary>Abstract</summary>
This work presents a text-to-audio-retrieval system based on pre-trained text and spectrogram transformers. Our method projects recordings and textual descriptions into a shared audio-caption space in which related examples from different modalities are close. Through a systematic analysis, we examine how each component of the system influences retrieval performance. As a result, we identify two key components that play a crucial role in driving performance: the self-attention-based audio encoder for audio embedding and the utilization of additional human-generated and synthetic data sets during pre-training. We further experimented with augmenting ClothoV2 captions with available keywords to increase their variety; however, this only led to marginal improvements. Our system ranked first in the 2023's DCASE Challenge, and it outperforms the current state of the art on the ClothoV2 benchmark by 5.6 pp. mAP@10.
</details>
<details>
<summary>摘要</summary>
Note:* "pre-trained text and spectrogram transformers" refers to the use of pre-trained language models and audio processing models to improve the performance of the text-to-audio retrieval system.* "self-attention-based audio encoder" refers to a specific type of audio encoding method that uses self-attention mechanisms to extract relevant features from the audio data.* "additional human-generated and synthetic data sets" refers to the use of additional data sets that are generated by humans or by algorithms to improve the performance of the system.* "ClothoV2 captions" refers to a specific dataset of textual descriptions that are used to evaluate the performance of the text-to-audio retrieval system.* "mAP@10" refers to the mean average precision at the 10th position, which is a common evaluation metric for text-to-audio retrieval systems.
</details></li>
</ul>
<hr>
<h2 id="Federated-Inference-with-Reliable-Uncertainty-Quantification-over-Wireless-Channels-via-Conformal-Prediction"><a href="#Federated-Inference-with-Reliable-Uncertainty-Quantification-over-Wireless-Channels-via-Conformal-Prediction" class="headerlink" title="Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction"></a>Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04237">http://arxiv.org/abs/2308.04237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meiyi Zhu, Matteo Zecchin, Sangwoo Park, Caili Guo, Chunyan Feng, Osvaldo Simeone</li>
<li>for: 这个论文主要探讨了一种名为联邦兼容预测（Federated Conformal Prediction，简称FCP）的技术，该技术利用设备到服务器的通信来提高服务器的决策质量。</li>
<li>methods: 这个论文提出了一种新的协议，名为无线联邦兼容预测（WFCP），它基于类型基本多访问（TBMA）和一种新的量化更正策略。WFCP提供了正式的可靠性保证，包括预测集的覆盖率。</li>
<li>results: 数据分析表明，WFCP在有限通信资源和大量设备情况下具有显著优势，特别是在对比数字实现的现有联邦CP方案时。<details>
<summary>Abstract</summary>
Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a single real number to the server. In this paper, we study for the first time federated CP in a wireless setting. We introduce a novel protocol, termed wireless federated conformal prediction (WFCP), which builds on type-based multiple access (TBMA) and on a novel quantile correction strategy. WFCP is proved to provide formal reliability guarantees in terms of coverage of the predicted set produced by the server. Using numerical results, we demonstrate the significant advantages of WFCP against digital implementations of existing federated CP schemes, especially in regimes with limited communication resources and/or large number of devices.
</details>
<details>
<summary>摘要</summary>
假设设备和服务器共享预训练模型。服务器想要对新输入进行推断。设备有访问未使用过训练数据的权限，可以与服务器通过共享的无线通信chnnel进行通信。如果设备没有访问新输入，是否可以通过设备到服务器的交流提高服务器的推断决策质量？latest work introduce federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a single real number to the server. In this paper, we study for the first time federated CP in a wireless setting. We introduce a novel protocol, termed wireless federated conformal prediction (WFCP), which builds on type-based multiple access (TBMA) and on a novel quantile correction strategy. WFCP is proved to provide formal reliability guarantees in terms of coverage of the predicted set produced by the server. Using numerical results, we demonstrate the significant advantages of WFCP against digital implementations of existing federated CP schemes, especially in regimes with limited communication resources and/or large number of devices.
</details></li>
</ul>
<hr>
<h2 id="OpinionConv-Conversational-Product-Search-with-Grounded-Opinions"><a href="#OpinionConv-Conversational-Product-Search-with-Grounded-Opinions" class="headerlink" title="OpinionConv: Conversational Product Search with Grounded Opinions"></a>OpinionConv: Conversational Product Search with Grounded Opinions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04226">http://arxiv.org/abs/2308.04226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vahid Sadiri Javadi, Martin Potthast, Lucie Flek</li>
<li>for: 本研究旨在帮助语言模型在对话中更加真实地表达用户的主观意见，以便更好地帮助用户做出决策。</li>
<li>methods: 本研究使用产品评论作为实际用户的主观意见的来源，并通过一种新的对话模型来 simulate sales conversations。</li>
<li>results: 研究发现，使用产品评论来生成对话可以提供更加真实的用户意见，并且用户认为这些意见具有决策价值。<details>
<summary>Abstract</summary>
When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an AI for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational AI in true subjective narratives. With OpinionConv, we develop the first conversational AI for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision-making.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在寻找产品时，其他人的意见具有重要的指导作用，可以提供有价值的信息。这也是销售对话中的事实，客户和销售助手交换产品的信息和意见。然而，用语言模型训练销售对话是复杂的，因为语言模型缺乏真实的世界经验。我们解决这个问题，利用产品评论作为产品意见的丰富源，将对话AI根据真实的主观故事进行定制。通过我们的 OpinionConv，我们开发了首个用于模拟销售对话的对话AI。为验证生成的对话，我们进行了多个用户研究，显示生成的意见被视为真实。我们的评估人也证实了意见作为决策基础的重要性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Interpretation-and-Validation-of-Graph-Attention-based-Explanations-for-GNN-Models"><a href="#Semantic-Interpretation-and-Validation-of-Graph-Attention-based-Explanations-for-GNN-Models" class="headerlink" title="Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models"></a>Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04220">http://arxiv.org/abs/2308.04220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Efimia Panagiotaki, Daniele De Martini, Lars Kunze</li>
<li>for: 这 paper aims to enhance the explainability of Graph Neural Network (GNN)-based models by applying semantic attention and establishing a correlation between predicted feature-importance weights and model accuracy.</li>
<li>methods: 该 paper introduces semantically-informed perturbations and utilizes attention mechanisms to provide feature-based explanations for GNN predictions. It extends existing attention-based graph-explainability methods by analyzing the behavior of predicted attention-weights distribution in correlation with model accuracy.</li>
<li>results: 通过应用这些方法， paper successfully identifies key semantic classes that contribute to enhanced performance and generates reliable post-hoc semantic explanations for the GNN model.<details>
<summary>Abstract</summary>
In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use of attention weights as importance indicators of semantically sorted feature sets. Through analysing the behaviour of predicted attention-weights distribution in correlation with model accuracy, we gain valuable insights into feature importance with respect to the behaviour of the GNN model. We apply our methodology to a lidar pointcloud estimation model successfully identifying key semantic classes that contribute to enhanced performance effectively generating reliable post-hoc semantic explanations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种方法来使用semantic attention提高graph neural network（GNN）模型的解释性，通过引入semantically-informed的干扰和建立predicted feature-importance权重和模型精度之间的相关性。 Graph Deep Learning（GDL）已经成为一个有前途的领域，用于场景理解等任务，利用flexible graph结构 concisely describe复杂的特征和关系。 Traditional explainability方法在XAI中无法直接应用于这些结构，因此需要graph-specific approach。 Attention机制已经在深度学习模型中证明其能力 estimating输入特征的重要性，因此在GNN预测中也被使用来提供基于特征的解释。 在这些基础上，我们进一步扩展了现有的注意力基本graph-explainability方法， investigate使用注意力权重作为semantic sorted feature set的重要性指标。 通过分析预测注意力权重分布与模型精度之间的相关性，我们获得了对feature importance的重要性见解，并且可以有效地生成post-hoc semantic explanation。 我们应用了我们的方法ology lidar pointcloud estimation模型，成功地标识了改进性的 semantic classes，从而生成了可靠的后期semantic explanation。
</details></li>
</ul>
<hr>
<h2 id="Varying-coefficients-for-regional-quantile-via-KNN-based-LASSO-with-applications-to-health-outcome-study"><a href="#Varying-coefficients-for-regional-quantile-via-KNN-based-LASSO-with-applications-to-health-outcome-study" class="headerlink" title="Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study"></a>Varying-coefficients for regional quantile via KNN-based LASSO with applications to health outcome study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04212">http://arxiv.org/abs/2308.04212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/younghhk/software">https://github.com/younghhk/software</a></li>
<li>paper_authors: Seyoung Park, Eun Ryung Lee, Hyokyoung G. Hong</li>
<li>for: 本研究旨在建立一种动态模型，描述健康结果和风险因素之间的关系，并考虑年龄的时间变化。</li>
<li>methods: 本研究使用 varying-coefficients (VC) 区域量化回归，结合 K-nearest neighbors (KNN) 融合lasso，以捕捉年龄的时间变化效应。</li>
<li>results: 研究结果表明，提议的方法能够准确捕捉健康结果和风险因素之间的复杂年龄dependent关系。<details>
<summary>Abstract</summary>
Health outcomes, such as body mass index and cholesterol levels, are known to be dependent on age and exhibit varying effects with their associated risk factors. In this paper, we propose a novel framework for dynamic modeling of the associations between health outcomes and risk factors using varying-coefficients (VC) regional quantile regression via K-nearest neighbors (KNN) fused Lasso, which captures the time-varying effects of age. The proposed method has strong theoretical properties, including a tight estimation error bound and the ability to detect exact clustered patterns under certain regularity conditions. To efficiently solve the resulting optimization problem, we develop an alternating direction method of multipliers (ADMM) algorithm. Our empirical results demonstrate the efficacy of the proposed method in capturing the complex age-dependent associations between health outcomes and their risk factors.
</details>
<details>
<summary>摘要</summary>
健康结果，如体重指数和胆固酶水平，与年龄存在相互关系，其影响因素的效果随着年龄的变化而变化。在这篇论文中，我们提出了一种新的动态模型，使用不同系数（VC）地域量规 regression via K-最近邻（KNN）混合lasso，以捕捉年龄的时间变化效应。我们的提议方法具有强制实现的理论属性，包括紧张估计误差 bound和在某些常数条件下检测精确的征性征 Patterns。为解决相应的优化问题，我们开发了一种分解方法 OF multipliers（ADMM）算法。我们的实验结果表明，我们的提议方法能够准确捕捉健康结果和其风险因素之间的复杂年龄相关性。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Sketching-for-Secure-Coded-Regression"><a href="#Iterative-Sketching-for-Secure-Coded-Regression" class="headerlink" title="Iterative Sketching for Secure Coded Regression"></a>Iterative Sketching for Secure Coded Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04185">http://arxiv.org/abs/2308.04185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neophytos Charalambides, Hessam Mahdavifar, Mert Pilanci, Alfred O. Hero III</li>
<li>for: 提高线性回归的速度，保证安全性。</li>
<li>methods: 利用随机抽取技术，改进异步系统中的延迟性。Specifically, 应用随机正交矩阵，并对块进行抽样，同时保护信息和降低回归问题的维度。</li>
<li>results: 实现了一种分布式iterative sketching方法，用于生成 $\ell_2$-子空间嵌入。此外，还对特殊情况下的随机化哈达姆变换进行推广，并讨论如何保护数据。<details>
<summary>Abstract</summary>
In this work, we propose methods for speeding up linear regression distributively, while ensuring security. We leverage randomized sketching techniques, and improve straggler resilience in asynchronous systems. Specifically, we apply a random orthonormal matrix and then subsample \textit{blocks}, to simultaneously secure the information and reduce the dimension of the regression problem. In our setup, the transformation corresponds to an encoded encryption in an \textit{approximate gradient coding scheme}, and the subsampling corresponds to the responses of the non-straggling workers; in a centralized coded computing network. This results in a distributive \textit{iterative sketching} approach for an $\ell_2$-subspace embedding, \textit{i.e.} a new sketch is considered at each iteration. We also focus on the special case of the \textit{Subsampled Randomized Hadamard Transform}, which we generalize to block sampling; and discuss how it can be modified in order to secure the data.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了加速线性回归的分布式方法，同时保证安全性。我们利用随机抽象技术，并改进异步系统中的延迟鲁棒性。具体来说，我们首先应用随机正交矩阵，然后对块进行抽样，以同时保护信息和减少回归问题的维度。在我们的设置中，这种变换对应于一种编码加密在一个 Approximate Gradient Coding Scheme 中，而抽样对应于非延迟工作者的响应；在中央coded computing网络中。这 führt zu a distributed iterative sketching approach for an $\ell_2$-subspace embedding, i.e., a new sketch is considered at each iteration.我们还专注于特殊情况下的 Subsampled Randomized Hadamard Transform，我们扩展了块抽样，并讨论了如何修改以保护数据。
</details></li>
</ul>
<hr>
<h2 id="Studying-Socially-Unacceptable-Discourse-Classification-SUD-through-different-eyes-“Are-we-on-the-same-page-”"><a href="#Studying-Socially-Unacceptable-Discourse-Classification-SUD-through-different-eyes-“Are-we-on-the-same-page-”" class="headerlink" title="Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: “Are we on the same page ?”"></a>Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: “Are we on the same page ?”</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04180">http://arxiv.org/abs/2308.04180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlinardicyu/sud_study_different_eyes">https://github.com/mlinardicyu/sud_study_different_eyes</a></li>
<li>paper_authors: Bruno Machado Carneiro, Michele Linardi, Julien Longhi</li>
<li>for: 本研究探讨了在在线文本中分类和检测社会不CCE接受的语言（SUD）。</li>
<li>methods: 我们首先构建了一个包含多种不同在线来源的手动标注文本的新集合，以便测试现有的机器学习（ML）SUD检测解决方案中的通用性。</li>
<li>results: 我们发现，不同的注解模式可能会影响SUD学习，并提出了一些开放的挑战和研究方向。  Additionally, we provide several data insights that can support domain experts in the annotation task.<details>
<summary>Abstract</summary>
We study Socially Unacceptable Discourse (SUD) characterization and detection in online text. We first build and present a novel corpus that contains a large variety of manually annotated texts from different online sources used so far in state-of-the-art Machine learning (ML) SUD detection solutions. This global context allows us to test the generalization ability of SUD classifiers that acquire knowledge around the same SUD categories, but from different contexts. From this perspective, we can analyze how (possibly) different annotation modalities influence SUD learning by discussing open challenges and open research directions. We also provide several data insights which can support domain experts in the annotation task.
</details>
<details>
<summary>摘要</summary>
我们研究社会不可接受的语言（SUD）Characterization和检测在在线文本中。我们首先建立了一个新的 corpus，包含不同在线来源的手动标注的文本，这些文本在前一个 Machine learning（ML） SUD检测解决方案中使用过。这种全球背景允许我们测试SUD分类器在不同上下文中 acquire 知识的泛化能力。从这个角度来看，我们可以分析不同标注模式对 SUD 学习的影响，并讨论开放的挑战和未来研究方向。我们还提供了一些数据视角，可以支持领域专家在标注任务中。Note: "SUD" stands for "Socially Unacceptable Discourse" in English.
</details></li>
</ul>
<hr>
<h2 id="Dual-input-neural-networks-for-positional-sound-source-localization"><a href="#Dual-input-neural-networks-for-positional-sound-source-localization" class="headerlink" title="Dual input neural networks for positional sound source localization"></a>Dual input neural networks for positional sound source localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04169">http://arxiv.org/abs/2308.04169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/egrinstein/di_nn">https://github.com/egrinstein/di_nn</a></li>
<li>paper_authors: Eric Grinstein, Vincent W. Neo, Patrick A. Naylor</li>
<li>for: 本研究用于提高音频信号处理中Metadata的使用，以优化输出。</li>
<li>methods: 本研究提出了 dual input neural networks (DI-NNs) 作为一种简单 yet effective的方法，将多个感知器的 audio 信号和场景特性信息结合在一起，以优化音源位置估计。</li>
<li>results: 对于不同的难度和真实性水平的场景，DI-NNs 与基准方法（Least-Squares 方法和 Convolutional Recurrent Neural Network）进行比较，结果显示 DI-NNs 在实际录制数据集中表现出五倍于基准方法的Localization error 下降，并且与 CRNN 相比，有两倍的性能提升。<details>
<summary>Abstract</summary>
In many signal processing applications, metadata may be advantageously used in conjunction with a high dimensional signal to produce a desired output. In the case of classical Sound Source Localization (SSL) algorithms, information from a high dimensional, multichannel audio signals received by many distributed microphones is combined with information describing acoustic properties of the scene, such as the microphones' coordinates in space, to estimate the position of a sound source. We introduce Dual Input Neural Networks (DI-NNs) as a simple and effective way to model these two data types in a neural network. We train and evaluate our proposed DI-NN on scenarios of varying difficulty and realism and compare it against an alternative architecture, a classical Least-Squares (LS) method as well as a classical Convolutional Recurrent Neural Network (CRNN). Our results show that the DI-NN significantly outperforms the baselines, achieving a five times lower localization error than the LS method and two times lower than the CRNN in a test dataset of real recordings.
</details>
<details>
<summary>摘要</summary>
在许多信号处理应用中，元数据可以与高维信号结合使用，以生成所需的输出。在传统的Sound Source Localization（SSL）算法中，来自多个分布式麦克风的高维多通道音频信号和场景的听音性特征，如麦克风的空间坐标，被组合以估算声源的位置。我们介绍了双输入神经网络（DI-NN）作为一种简单而有效的方法，将这两种数据类型模型在神经网络中。我们在不同的难度和真实性水平上训练和评估我们的提议DI-NN，并与基准 Architecture，即经典的最小二乘法（LS）方法和经典的卷积回归神经网络（CRNN）进行比较。我们的结果表明，DI-NN在测试数据集中与LS方法和CRNN相比，显著地下降了5倍的地标错误，达到了2倍的CRNN水平。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Assessment-of-the-Performance-of-Deep-Learning-Classifiers-Reveals-a-Surprising-Lack-of-Robustness"><a href="#Comprehensive-Assessment-of-the-Performance-of-Deep-Learning-Classifiers-Reveals-a-Surprising-Lack-of-Robustness" class="headerlink" title="Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness"></a>Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04137">http://arxiv.org/abs/2308.04137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael W. Spratling</li>
<li>for: The paper aims to evaluate the robustness of machine learning models, particularly deep neural networks, and to develop a more comprehensive benchmarking protocol for assessing their performance.</li>
<li>methods: The paper proposes using a wide range of different types of data to benchmark the performance of machine learning models, and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance.</li>
<li>results: The paper finds that current deep neural networks are extremely vulnerable to making mistakes on certain types of data, and that they are insecure as they can easily be fooled into making the wrong decisions. The results suggest that more comprehensive testing methods are needed to develop more robust machine learning models in the future.<details>
<summary>Abstract</summary>
Reliable and robust evaluation methods are a necessary first step towards developing machine learning models that are themselves robust and reliable. Unfortunately, current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others. For example, using the standard test data fails to evaluate the predictions made by the classifier to samples from classes it was not trained on. On the other hand, testing with data containing samples from unknown classes fails to evaluate how well the classifier can predict the labels for known classes. This article advocates bench-marking performance using a wide range of different types of data and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance. Using such a benchmark it is found that current deep neural networks, including those trained with methods that are believed to produce state-of-the-art robustness, are extremely vulnerable to making mistakes on certain types of data. This means that such models will be unreliable in real-world scenarios where they may encounter data from many different domains, and that they are insecure as they can easily be fooled into making the wrong decisions. It is hoped that these results will motivate the wider adoption of more comprehensive testing methods that will, in turn, lead to the development of more robust machine learning methods in the future.   Code is available at: \url{https://codeberg.org/mwspratling/RobustnessEvaluation}
</details>
<details>
<summary>摘要</summary>
可靠且可靠的评估方法是机器学习模型的发展的必要首步。然而，当前的评估协议通常只使用有限种的测试数据来评估性能，而忽略其他类型的数据。例如，使用标准测试数据将不能评估机器学习器对未知类型的样本进行预测。相反，使用包含未知类型样本的数据进行测试将不能评估机器学习器对已知类型样本的预测。这篇文章提议使用多种不同类型的数据进行比较，并使用一个可以应用于所有数据类型的单一指标来生成一致的评估性能。使用这种标准可以发现，当前的深度神经网络，包括通过认为会生成状态的最佳Robustness训练方法，在某些类型的数据上表现出极高的敏感性。这意味着这些模型在实际世界中将是不可靠的，因为它们可能会遇到来自多个领域的数据，并且它们容易被骗到作出错误的决策。希望这些结果能够激励更广泛的测试方法的采用，以便在未来发展更加可靠的机器学习方法。Code可以在以下链接中找到：https://codeberg.org/mwspratling/RobustnessEvaluation
</details></li>
</ul>
<hr>
<h2 id="D-Score-A-Synapse-Inspired-Approach-for-Filter-Pruning"><a href="#D-Score-A-Synapse-Inspired-Approach-for-Filter-Pruning" class="headerlink" title="D-Score: A Synapse-Inspired Approach for Filter Pruning"></a>D-Score: A Synapse-Inspired Approach for Filter Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04470">http://arxiv.org/abs/2308.04470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doyoung Park, Jinsoo Kim, Jina Nam, Jooyoung Chang, Sang Min Park</li>
<li>for: 本研究旨在提出一种基于神经元系统的筛选方法，用于降低卷积神经网络（CNN）中不重要的筛选器的计算量。</li>
<li>methods: 该方法基于神经科学的视角，提出了一种名为动态分数（D-Score）的筛选器分数分析方法，该方法分析了每个筛选器中独立重要的正向和负向权重，并将其分配分数。</li>
<li>results: 实验结果表明，使用该方法可以在CIFAR-10和ImageNet datasets上减少显著的计算量和参数，而无需导致准确率下降。<details>
<summary>Abstract</summary>
This paper introduces a new aspect for determining the rank of the unimportant filters for filter pruning on convolutional neural networks (CNNs). In the human synaptic system, there are two important channels known as excitatory and inhibitory neurotransmitters that transmit a signal from a neuron to a cell. Adopting the neuroscientific perspective, we propose a synapse-inspired filter pruning method, namely Dynamic Score (D-Score). D-Score analyzes the independent importance of positive and negative weights in the filters and ranks the independent importance by assigning scores. Filters having low overall scores, and thus low impact on the accuracy of neural networks are pruned. The experimental results on CIFAR-10 and ImageNet datasets demonstrate the effectiveness of our proposed method by reducing notable amounts of FLOPs and Params without significant Acc. Drop.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OmniDataComposer-A-Unified-Data-Structure-for-Multimodal-Data-Fusion-and-Infinite-Data-Generation"><a href="#OmniDataComposer-A-Unified-Data-Structure-for-Multimodal-Data-Fusion-and-Infinite-Data-Generation" class="headerlink" title="OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation"></a>OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04126">http://arxiv.org/abs/2308.04126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Shihao Wang, Yuan Fang, Wangpeng An</li>
<li>for: 这篇论文旨在提出一种新的多模态数据融合和无限数据生成方法，以协调和简化不同数据模式之间的交互。</li>
<li>methods: 该方法使用了多种先进算法，包括视频&#x2F;图像描述EXTRACTION、稠密描述EXTRACTION、自动语音识别（ASR）、光学字符识别（OCR）、Recognize Anything Model（RAM）和对象跟踪。</li>
<li>results: 该方法可以识别6400种类型的对象，显著扩大了视觉信息的谱度。它将多种模式融合起来，促进不同模式之间的互助和cross-模式数据校正。最终输出将每个视频输入转化为详细的时间序列文档，使得视频更容易被大语言模型处理。<details>
<summary>Abstract</summary>
This paper presents OmniDataComposer, an innovative approach for multimodal data fusion and unlimited data generation with an intent to refine and uncomplicate interplay among diverse data modalities. Coming to the core breakthrough, it introduces a cohesive data structure proficient in processing and merging multimodal data inputs, which include video, audio, and text.   Our crafted algorithm leverages advancements across multiple operations such as video/image caption extraction, dense caption extraction, Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything Model(RAM), and object tracking. OmniDataComposer is capable of identifying over 6400 categories of objects, substantially broadening the spectrum of visual information. It amalgamates these diverse modalities, promoting reciprocal enhancement among modalities and facilitating cross-modal data correction. \textbf{The final output metamorphoses each video input into an elaborate sequential document}, virtually transmuting videos into thorough narratives, making them easier to be processed by large language models.   Future prospects include optimizing datasets for each modality to encourage unlimited data generation. This robust base will offer priceless insights to models like ChatGPT, enabling them to create higher quality datasets for video captioning and easing question-answering tasks based on video content. OmniDataComposer inaugurates a new stage in multimodal learning, imparting enormous potential for augmenting AI's understanding and generation of complex, real-world data.
</details>
<details>
<summary>摘要</summary>
The final output transforms each video input into an elaborate sequential document, virtually transmuting videos into thorough narratives that can be easily processed by large language models. Future prospects include optimizing datasets for each modality to encourage unlimited data generation, which will offer valuable insights to models like ChatGPT and ease question-answering tasks based on video content.OmniDataComposer opens a new stage in multimodal learning, possessing enormous potential for augmenting AI's understanding and generation of complex, real-world data. With the ability to identify over 6400 categories of objects, the algorithm significantly broadens the spectrum of visual information. By merging diverse modalities and facilitating cross-modal data correction, OmniDataComposer promotes reciprocal enhancement among modalities, leading to more accurate and comprehensive data analysis.
</details></li>
</ul>
<hr>
<h2 id="Constructing-Custom-Thermodynamics-Using-Deep-Learning"><a href="#Constructing-Custom-Thermodynamics-Using-Deep-Learning" class="headerlink" title="Constructing Custom Thermodynamics Using Deep Learning"></a>Constructing Custom Thermodynamics Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04119">http://arxiv.org/abs/2308.04119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoli Chen, Beatrice W. Soh, Zi-En Ooi, Eleonore Vissol-Gaudin, Haijun Yu, Kostya S. Novoselov, Kedar Hippalgaonkar, Qianxiao Li</li>
<li>for: 这项研究旨在开发一种基于总体热动力学原理的自动化科学发现平台，用于研究复杂动态系统，以提高科学家对这些系统的理解和预测能力。</li>
<li>methods: 该平台基于一种总体热动力学原理来学习微观轨迹的 макро观动力学描述，并同时构建了减少的热动力学坐标系和解释动力学。</li>
<li>results: 通过对长聚合物链的延伸行为进行理论和实验研究，该方法学习了三个可解释的热动力学坐标，并构建了聚合物延伸的动力学景观，包括稳定和过渡态的标识以及延伸速率的控制。此外，该方法还应用于了不同领域的另一个问题——构建空间传染疫苗的macroscopic动力学，展示了该方法的广泛科学和技术应用前景。<details>
<summary>Abstract</summary>
One of the most exciting applications of AI is automated scientific discovery based on previously amassed data, coupled with restrictions provided by the known physical principles, including symmetries and conservation laws. Such automated hypothesis creation and verification can assist scientists in studying complex phenomena, where traditional physical intuition may fail. Of particular importance are complex dynamic systems where their time evolution is strongly influenced by varying external parameters. In this paper we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes render complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by simultaneously constructing reduced thermodynamic coordinates and interpreting the dynamics on these coordinates. We demonstrate our method by studying theoretically and validating experimentally, the stretching of long polymer chains in an externally applied field. Specifically, we learn three interpretable thermodynamic coordinates and build a dynamical landscape of polymer stretching, including (1) the identification of stable and transition states and (2) the control of the stretching rate. We further demonstrate the universality of our approach by applying it to an unrelated problem in a different domain: constructing macroscopic dynamics for spatial epidemics, showing that our method addresses wide scientific and technological applications.
</details>
<details>
<summary>摘要</summary>
In this paper, we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes make complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by simultaneously constructing reduced thermodynamic coordinates and interpreting the dynamics on these coordinates.We demonstrate our method by studying theoretically and validating experimentally the stretching of long polymer chains in an externally applied field. Specifically, we learn three interpretable thermodynamic coordinates and build a dynamical landscape of polymer stretching, including the identification of stable and transition states and the control of the stretching rate. We further demonstrate the universality of our approach by applying it to an unrelated problem in a different domain: constructing macroscopic dynamics for spatial epidemics, showing that our method addresses wide scientific and technological applications.
</details></li>
</ul>
<hr>
<h2 id="PTransIPs-Identification-of-phosphorylation-sites-based-on-protein-pretrained-language-model-and-Transformer"><a href="#PTransIPs-Identification-of-phosphorylation-sites-based-on-protein-pretrained-language-model-and-Transformer" class="headerlink" title="PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer"></a>PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05115">http://arxiv.org/abs/2308.05115</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/statxzy7/ptransips">https://github.com/statxzy7/ptransips</a></li>
<li>paper_authors: Ziyang Xu, Haitian Zhong<br>for:这个研究旨在开发一个新的深度学习模型，用于识别蛋白质中的磷酸化位点。methods:这个模型使用了深度学习的卷积神经网络和传统的Transformer模型，并且使用了大量预训练的蛋白质模型嵌入。results:实验结果显示，这个模型比现有的State-of-the-art方法高效，具有AUROC值为0.9232和0.9660 для识别磷酸化S&#x2F;T和Y位点。<details>
<summary>Abstract</summary>
Phosphorylation is central to numerous fundamental cellular processes, influencing the onset and progression of a variety of diseases. The correct identification of these phosphorylation sites is of great importance to unravel the intricate molecular mechanisms within cells and during viral infections, potentially leading to the discovery of new therapeutic targets. In this study, we introduce PTransIPs, a novel deep learning model for the identification of phosphorylation sites. PTransIPs treat amino acids within protein sequences as words, extracting unique encodings based on their type and sequential position. The model also incorporates embeddings from large pretrained protein models as additional data inputs. PTransIPS is further trained on a combination model of convolutional neural network with residual connections and Transformer model equipped with multi-head attention mechanisms. At last, the model outputs classification results through a fully connected layer. The results of independent testing reveal that PTransIPs outperforms existing state-of-the-art(SOTA) methods, achieving AUROCs of 0.9232 and 0.9660 for identifying phosphorylated S/T and Y sites respectively. In addition, ablation studies prove that pretrained model embeddings contribute to the performance of PTransIPs. Furthermore, PTransIPs has interpretable amino acid preference, visible training process and shows generalizability on other bioactivity classification tasks. To facilitate usage, our code and data are publicly accessible at \url{https://github.com/StatXzy7/PTransIPs}.
</details>
<details>
<summary>摘要</summary>
“磷酸化是细胞内多种基本生物过程的中心，影响疾病的发生和进程。正确识别这些磷酸化位点非常重要，以解释细胞内分子机制，并在病毒感染时发挥作用，可能导致新的药 targets 的发现。在这项研究中，我们介绍 PTransIPs，一种新的深度学习模型，用于磷酸化位点的识别。 PTransIPs 将蛋白质序列中的氨基酸看作为单词，提取唯一的编码，基于其类型和顺序位置。模型还使用大型预训练蛋白质模型的嵌入。 PTransIPS 通过将 convolutional neural network 与 residual connections 和 transformer 模型组合，并在其中添加多头注意机制。最后，模型输出分类结果通过完全连接层。独立测试结果显示，PTransIPs 在识别磷酸化 S/T 和 Y 位点方面的 AUROC 分别达到 0.9232 和 0.9660。此外，归因研究表明，预训练模型嵌入对 PTransIPs 的表现有助。此外，PTransIPs 具有可读性的氨基酸偏好、可见的训练过程和普适性。为便于使用，我们在 GitHub 上公开了代码和数据，请参考 \url{https://github.com/StatXzy7/PTransIPs}.”
</details></li>
</ul>
<hr>
<h2 id="Correlating-Medi-Claim-Service-by-Deep-Learning-Neural-Networks"><a href="#Correlating-Medi-Claim-Service-by-Deep-Learning-Neural-Networks" class="headerlink" title="Correlating Medi-Claim Service by Deep Learning Neural Networks"></a>Correlating Medi-Claim Service by Deep Learning Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04469">http://arxiv.org/abs/2308.04469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanthi Vajiram, Negha Senthil, Nean Adhith. P</li>
<li>for: 医疗保险诈骗案件与患者、医生、诊断中心和保险公司之间的关系，需要不断监测。这种诈骗活动会对保险人和医疗保险公司的财务发展产生影响。</li>
<li>methods: 这篇论文使用卷积神经网络架构来检测诈骗申请，通过对不同提供者的申请进行相关性研究，帮助检测洗钱活动。用于检测诈骗和非诈骗申请的有supervised和Unsupervised分类器。</li>
<li>results: 该论文的结果表明，使用卷积神经网络架构和相关性研究可以帮助检测医疗保险诈骗案件，并且可以提高检测精度。<details>
<summary>Abstract</summary>
Medical insurance claims are of organized crimes related to patients, physicians, diagnostic centers, and insurance providers, forming a chain reaction that must be monitored constantly. These kinds of frauds affect the financial growth of both insured people and health insurance companies. The Convolution Neural Network architecture is used to detect fraudulent claims through a correlation study of regression models, which helps to detect money laundering on different claims given by different providers. Supervised and unsupervised classifiers are used to detect fraud and non-fraud claims.
</details>
<details>
<summary>摘要</summary>
医疗保险laims有组织犯罪关系到patients、医生、诊断中心和保险公司，形成一个排序链 reaction，需要不断监测。这种骗局会影响保险人和健康保险公司的财务增长。通过对不同提供者的laims进行相关性研究，涂抹神经网络架构可以检测针对不同提供者的骗局。使用supervised和Unsupervised分类器可以检测骗局和非骗局laims。
</details></li>
</ul>
<hr>
<h2 id="Explainable-machine-learning-to-enable-high-throughput-electrical-conductivity-optimization-of-doped-conjugated-polymers"><a href="#Explainable-machine-learning-to-enable-high-throughput-electrical-conductivity-optimization-of-doped-conjugated-polymers" class="headerlink" title="Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers"></a>Explainable machine learning to enable high-throughput electrical conductivity optimization of doped conjugated polymers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04103">http://arxiv.org/abs/2308.04103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Wei Yoon, Adithya Kumar, Pawan Kumar, Kedar Hippalgaonkar, J Senthilnath, Vijila Chellappan</li>
<li>for: 加速材料发现和优化</li>
<li>methods: 使用机器学习（ML）技术，利用易于测量的吸收спектроскопия数据，加速测量电性征的工作流程</li>
<li>results: 提高测量电性征的效率89%，并且可以解释ML模型的具体影响因素，从而获得了有用的科学见解。<details>
<summary>Abstract</summary>
The combination of high-throughput experimentation techniques and machine learning (ML) has recently ushered in a new era of accelerated material discovery, enabling the identification of materials with cutting-edge properties. However, the measurement of certain physical quantities remains challenging to automate. Specifically, meticulous process control, experimentation and laborious measurements are required to achieve optimal electrical conductivity in doped polymer materials. We propose a ML approach, which relies on readily measured absorbance spectra, to accelerate the workflow associated with measuring electrical conductivity. The first ML model (classification model), accurately classifies samples with a conductivity >~25 to 100 S/cm, achieving a maximum of 100% accuracy rate. For the subset of highly conductive samples, we employed a second ML model (regression model), to predict their conductivities, yielding an impressive test R2 value of 0.984. To validate the approach, we showed that the models, neither trained on the samples with the two highest conductivities of 498 and 506 S/cm, were able to, in an extrapolative manner, correctly classify and predict them at satisfactory levels of errors. The proposed ML workflow results in an improvement in the efficiency of the conductivity measurements by 89% of the maximum achievable using our experimental techniques. Furthermore, our approach addressed the common challenge of the lack of explainability in ML models by exploiting bespoke mathematical properties of the descriptors and ML model, allowing us to gain corroborated insights into the spectral influences on conductivity. Through this study, we offer an accelerated pathway for optimizing the properties of doped polymer materials while showcasing the valuable insights that can be derived from purposeful utilization of ML in experimental science.
</details>
<details>
<summary>摘要</summary>
高通过率实验技术和机器学习（ML）的结合，已经 ushered in一个新的时代，用于加速材料发现，并使得可以通过高级特性来标识材料。然而，一些物理量的测量仍然具有挑战。Specifically, 需要精心控制过程，实验和劳动ioso measurements 以实现高电导率填充 polymer 材料中的优化。我们提议一种 ML 方法，基于Ready-to-measure absorbance spectrum，以加速测量电导率的工作流程。首先，我们使用了一种分类模型（第一种 ML 模型），准确地将样本分类为电导率 > ~25 to 100 S/cm，实现最高的准确率（100%）。对于高电导率样本 subset，我们使用了一种 regression 模型（第二种 ML 模型），来预测他们的电导率，得到了惊人的 test R2 值为 0.984。为了验证方法，我们表明，无论在填充 polymer 材料中的两个最高电导率样本（498和506 S/cm）的情况下，模型都能够，在扩展性的方式下，正确地分类和预测它们，并且得到了满意的误差水平。我们的 ML 工作流程对于测量电导率的效率提高了89%的最大可达水平。此外，我们的方法解决了通用机器学习模型的普遍问题，即无法解释性。我们通过抽象数学属性和 ML 模型的特性，获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠的干涉因素，从而获得了可靠
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Evolution-of-Deep-Neural-Network-Architectures"><a href="#Asynchronous-Evolution-of-Deep-Neural-Network-Architectures" class="headerlink" title="Asynchronous Evolution of Deep Neural Network Architectures"></a>Asynchronous Evolution of Deep Neural Network Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04102">http://arxiv.org/abs/2308.04102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Liang, Hormoz Shahrzad, Risto Miikkulainen</li>
<li>for: 提高 ENAS 的并发评估速度</li>
<li>methods: 使用异步评估策略 (AES)，维护最多 $K$ 个个体Ready to be sent to the workers for evaluation，并在 $M&lt;&lt;K$ 个个体已经被评估的情况下提交下一代</li>
<li>results: 在 11-bit 多路分配和图像描述任务中观察到多倍性和开放式优化任务中的性能提高，表明 AES 是一种有效的并发评估策略，适用于复杂系统的进化中长度和变化很大的评估时间。<details>
<summary>Abstract</summary>
Many evolutionary algorithms (EAs) take advantage of parallel evaluation of candidates. However, if evaluation times vary significantly, many worker nodes (i.e.,\ compute clients) are idle much of the time, waiting for the next generation to be created. Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks, is particularly vulnerable to this issue. This paper proposes a generic asynchronous evaluation strategy (AES) that is then adapted to work with ENAS. AES increases throughput by maintaining a queue of upto $K$ individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as $M<<K$ individuals have been evaluated by the workers. A suitable value for $M$ is determined experimentally, balancing diversity and efficiency. To showcase the generality and power of AES, it was first evaluated in 11-bit multiplexer design (a single-population verifiable discovery task) and then scaled up to ENAS for image captioning (a multi-population open-ended-optimization task). In both problems, a multifold performance improvement was observed, suggesting that AES is a promising method for parallelizing the evolution of complex systems with long and variable evaluation times, such as those in ENAS.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)许多进化算法（EA）利用并发评估候选者。然而，如果评估时间异常长， THEN 多个工作节点（即计算客户端）会大部分时间浪费于等待下一代被创建。进化神经网络架构搜索（ENAS），一类进化算法，特别容易受到这种问题的影响。这篇论文提出了一种通用异步评估策略（AES），然后将其应用于 ENAS。AES 通过维护一个最多 $K$ 个个体ready to be sent to the workers for evaluation，并在工作者评估 $M\ll K$ 个个体后进行下一代的创建，提高了 durchput。一个合适的 $M$ 值由实验确定，平衡多样性和效率。为了证明 AES 的普适性和能力，它首先在 11-bit 多路复用器设计（一个单种人口可验证发现任务）中进行了测试，然后扩展到 ENAS  для图像描述（一个多种人口开放结束优化任务）。在这两个问题中，都观察到了多倍性的性能改进，表明 AES 是一种有前途的方法 для并发进化复杂系统的评估，如 ENAS 中的长变化评估时间。
</details></li>
</ul>
<hr>
<h2 id="Why-Data-Science-Projects-Fail"><a href="#Why-Data-Science-Projects-Fail" class="headerlink" title="Why Data Science Projects Fail"></a>Why Data Science Projects Fail</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04896">http://arxiv.org/abs/2308.04896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xLaszlo/datascience-fails">https://github.com/xLaszlo/datascience-fails</a></li>
<li>paper_authors: Balaram Panda</li>
</ul>
<details>
<summary>Abstract</summary>
Data Science is a modern Data Intelligence practice, which is the core of many businesses and helps businesses build smart strategies around to deal with businesses challenges more efficiently. Data Science practice also helps in automating business processes using the algorithm, and it has several other benefits, which also deliver in a non-profitable framework. In regards to data science, three key components primarily influence the effective outcome of a data science project. Those are 1.Availability of Data 2.Algorithm 3.Processing power or infrastructure
</details>
<details>
<summary>摘要</summary>
现代数据智能实践---数据科学，是许多企业的核心，帮助企业建立更智能的策略，更有效地面对企业挑战。数据科学实践还可以自动化商业过程，并具有许多其他利点，如非营利框架。在关于数据科学的问题上，三个关键组件主要影响项目的效果。那是1.数据可用性2.算法3.处理能力或基础设施。
</details>


<hr>
<h2 id="Application-Oriented-Benchmarking-of-Quantum-Generative-Learning-Using-QUARK"><a href="#Application-Oriented-Benchmarking-of-Quantum-Generative-Learning-Using-QUARK" class="headerlink" title="Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK"></a>Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04082">http://arxiv.org/abs/2308.04082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian J. Kiwit, Marwa Marso, Philipp Ross, Carlos A. Riofrío, Johannes Klepsch, Andre Luckow</li>
<li>for: 本研究旨在提供一个标准化和简化量子机器学习（QML）算法评估的框架，以便更好地评估量子 Computing 应用程序的性能。</li>
<li>methods: 本研究使用了更新后的 QUantum computing Application benchmaRK（QUARK）框架，以便评估量子生成模型的训练和部署。</li>
<li>results: 本研究通过训练不同的量子生成模型，使用不同的图体设计、数据集和转换，并评估模型的普遍性和可靠性。<details>
<summary>Abstract</summary>
Benchmarking of quantum machine learning (QML) algorithms is challenging due to the complexity and variability of QML systems, e.g., regarding model ansatzes, data sets, training techniques, and hyper-parameters selection. The QUantum computing Application benchmaRK (QUARK) framework simplifies and standardizes benchmarking studies for quantum computing applications. Here, we propose several extensions of QUARK to include the ability to evaluate the training and deployment of quantum generative models. We describe the updated software architecture and illustrate its flexibility through several example applications: (1) We trained different quantum generative models using several circuit ansatzes, data sets, and data transformations. (2) We evaluated our models on GPU and real quantum hardware. (3) We assessed the generalization capabilities of our generative models using a broad set of metrics that capture, e.g., the novelty and validity of the generated data.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将量子机器学习（QML）算法进行测试和评估是具有复杂性和变化性的，例如模型 ansatzes、数据集、训练技术和超参数选择等。QUantum computing Application benchmaRK（QUARK）框架可以简化和标准化量子计算应用程序的测试和评估研究。我们提议在QUARK框架中添加了评估量子生成模型的训练和部署功能。我们描述了更新后的软件架构，并通过多个示例应用 illustrate its flexibility：(1) 我们使用不同的量子生成模型circuit ansatzes、数据集和数据变换来训练不同的量子生成模型。(2) 我们在GPU和真正量子硬件上评估了我们的模型。(3) 我们使用了一系列的metric来评估我们的生成模型的通用性，例如生成数据的新鲜性和有效性。Note: "quantum generative models" in the text refers to quantum neural networks or quantum machine learning models that can generate new data samples.
</details></li>
</ul>
<hr>
<h2 id="Federated-Zeroth-Order-Optimization-using-Trajectory-Informed-Surrogate-Gradients"><a href="#Federated-Zeroth-Order-Optimization-using-Trajectory-Informed-Surrogate-Gradients" class="headerlink" title="Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients"></a>Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04077">http://arxiv.org/abs/2308.04077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Shu, Xiaoqiang Lin, Zhongxiang Dai, Bryan Kian Hsiang Low</li>
<li>for: 这个研究是为了提出一种能够在无法读取本地数据的情况下进行联合优化的方法，并且可以在各种实际应用中实现更好的效率和积极性。</li>
<li>methods: 这个研究使用了联合优化的背景下的零次项目优化（ZOO）方法，并且提出了一种基于轨迹信息的渐进幂函数估计（Trajectory-Informed Gradient Surrogates，TIGS）以及一种适应性的渐进幂函数调整（Adaptive Gradient Correction，AGC）技术，以提高查询和通信效率。</li>
<li>results: 这个研究透过实际实验（包括联合黑洞攻击和联合非对称度量优化），证明了FZooS方法的理论上的改进，并且在实际应用中实现了更好的效率和积极性。<details>
<summary>Abstract</summary>
Federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimation, and (b) develop the technique of adaptive gradient correction using these gradient surrogates to mitigate the aforementioned disparity. Based on these, we propose the federated zeroth-order optimization using trajectory-informed surrogate gradients (FZooS) algorithm for query- and communication-efficient federated ZOO. Our FZooS achieves theoretical improvements over the existing approaches, which is supported by our real-world experiments such as federated black-box adversarial attack and federated non-differentiable metric optimization.
</details>
<details>
<summary>摘要</summary>
federated optimization, an emerging paradigm which finds wide real-world applications such as federated learning, enables multiple clients (e.g., edge devices) to collaboratively optimize a global function. The clients do not share their local datasets and typically only share their local gradients. However, the gradient information is not available in many applications of federated optimization, which hence gives rise to the paradigm of federated zeroth-order optimization (ZOO). Existing federated ZOO algorithms suffer from the limitations of query and communication inefficiency, which can be attributed to (a) their reliance on a substantial number of function queries for gradient estimation and (b) the significant disparity between their realized local updates and the intended global updates. To this end, we (a) introduce trajectory-informed gradient surrogates which is able to use the history of function queries during optimization for accurate and query-efficient gradient estimation, and (b) develop the technique of adaptive gradient correction using these gradient surrogates to mitigate the aforementioned disparity. Based on these, we propose the federated zeroth-order optimization using trajectory-informed surrogate gradients (FZooS) algorithm for query- and communication-efficient federated ZOO. Our FZooS achieves theoretical improvements over the existing approaches, which is supported by our real-world experiments such as federated black-box adversarial attack and federated non-differentiable metric optimization.Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Learning-Specialized-Activation-Functions-for-Physics-informed-Neural-Networks"><a href="#Learning-Specialized-Activation-Functions-for-Physics-informed-Neural-Networks" class="headerlink" title="Learning Specialized Activation Functions for Physics-informed Neural Networks"></a>Learning Specialized Activation Functions for Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04073">http://arxiv.org/abs/2308.04073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leaplabthu/adaafforpinns">https://github.com/leaplabthu/adaafforpinns</a></li>
<li>paper_authors: Honghui Wang, Lu Lu, Shiji Song, Gao Huang</li>
<li>for: 解决Physics-informed neural networks (PINNs)中的优化困难问题。</li>
<li>methods: 提出了一种基于活动函数的自适应搜索方法，并对不同的问题进行比较和讨论。</li>
<li>results: 通过对多个 benchmark 进行测试，证明了该方法的效果和可INTERPRETABLE性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) are known to suffer from optimization difficulty. In this work, we reveal the connection between the optimization difficulty of PINNs and activation functions. Specifically, we show that PINNs exhibit high sensitivity to activation functions when solving PDEs with distinct properties. Existing works usually choose activation functions by inefficient trial-and-error. To avoid the inefficient manual selection and to alleviate the optimization difficulty of PINNs, we introduce adaptive activation functions to search for the optimal function when solving different problems. We compare different adaptive activation functions and discuss their limitations in the context of PINNs. Furthermore, we propose to tailor the idea of learning combinations of candidate activation functions to the PINNs optimization, which has a higher requirement for the smoothness and diversity on learned functions. This is achieved by removing activation functions which cannot provide higher-order derivatives from the candidate set and incorporating elementary functions with different properties according to our prior knowledge about the PDE at hand. We further enhance the search space with adaptive slopes. The proposed adaptive activation function can be used to solve different PDE systems in an interpretable way. Its effectiveness is demonstrated on a series of benchmarks. Code is available at https://github.com/LeapLabTHU/AdaAFforPINNs.
</details>
<details>
<summary>摘要</summary>
物理学信息感知神经网络（PINNs）经常遇到优化困难。在这项工作中，我们揭示了PINNs在解决不同类型的偏微分方程（PDEs）时的优化困难和活动函数之间的关系。特别是，我们发现PINNs在解决不同类型的PDEs时会具有高敏感性，而且现有的活动函数通常是通过不必要的试错来选择的。为了避免不必要的手动选择和提高PINNs的优化效果，我们引入了适应性的活动函数来搜索解决不同问题的最佳函数。我们比较了不同的适应性活动函数，并讨论了它们在PINNs中的限制。此外，我们提议在PINNs优化中应用学习组合候选活动函数的思想，以提高学习的稳定性和多样性。我们通过从候选函数中移除无法提供高阶导数的活动函数，并根据我们对PDE的先验知识添加不同性质的元素函数来实现这一点。最终，我们还使用自适应坡度来增强搜索空间。我们提出的适应性活动函数可以在可解释的方式下解决不同PDE系统。我们在一系列的benchmark中证明了它的效果。代码可以在https://github.com/LeapLabTHU/AdaAFforPINNs上获取。
</details></li>
</ul>
<hr>
<h2 id="Path-Signatures-for-Diversity-in-Probabilistic-Trajectory-Optimisation"><a href="#Path-Signatures-for-Diversity-in-Probabilistic-Trajectory-Optimisation" class="headerlink" title="Path Signatures for Diversity in Probabilistic Trajectory Optimisation"></a>Path Signatures for Diversity in Probabilistic Trajectory Optimisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04071">http://arxiv.org/abs/2308.04071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Barcelos, Tin Lai, Rafael Oliveira, Paulo Borges, Fabio Ramos</li>
<li>for: 这篇论文主要应用于精确的机器人动作规划，尤其是在具有复杂障碍物和复杂 geometrical 的环境下。</li>
<li>methods: 这篇论文使用了当代 computation 硬件的平行优化方法，通过将多个解 initialize 自不同的起始点，同时解决问题。然而，如果没有一个策略来避免解彼此灰度，则可能会出现模式崩溃，从而降低了方法的效率和寻找全局解的可能性。这篇论文利用了最近的条件paths 理论，设计了一个避免模式崩溃的算法，并且使用了path signatures 和 Hilbert space representation of trajectories。</li>
<li>results: 这篇论文的实验结果显示，该算法可以在许多问题上取得更低的平均成本，比如2D navigation 和 robotic manipulators 在填充环境下的运作。<details>
<summary>Abstract</summary>
Motion planning can be cast as a trajectory optimisation problem where a cost is minimised as a function of the trajectory being generated. In complex environments with several obstacles and complicated geometry, this optimisation problem is usually difficult to solve and prone to local minima. However, recent advancements in computing hardware allow for parallel trajectory optimisation where multiple solutions are obtained simultaneously, each initialised from a different starting point. Unfortunately, without a strategy preventing two solutions to collapse on each other, naive parallel optimisation can suffer from mode collapse diminishing the efficiency of the approach and the likelihood of finding a global solution. In this paper we leverage on recent advances in the theory of rough paths to devise an algorithm for parallel trajectory optimisation that promotes diversity over the range of solutions, therefore avoiding mode collapses and achieving better global properties. Our approach builds on path signatures and Hilbert space representations of trajectories, and connects parallel variational inference for trajectory estimation with diversity promoting kernels. We empirically demonstrate that this strategy achieves lower average costs than competing alternatives on a range of problems, from 2D navigation to robotic manipulators operating in cluttered environments.
</details>
<details>
<summary>摘要</summary>
moviment planning 可以被看作为一个轨迹优化问题，其中要尽可能地降低轨迹的成本。在复杂的环境中，充满障碍物和复杂的几何结构时，这个优化问题通常很难解，容易陷入地方最优点。然而，当前的计算硬件技术允许同时进行多个解的优化，每个解从不同的初始点开始。然而，如果没有避免两个解归并在一起的策略，纯粹的平行优化可能会受到模式归并的影响，从而降低方法的效率和找到全局解的可能性。在这篇论文中，我们利用了最近的粗路理论进行平行轨迹优化算法，以便在轨迹优化过程中促进多元性，因此避免模式归并，并实现更好的全局性质。我们的方法基于路径签名和希尔伯特空间表示轨迹，并将平行变分推理与多样性激活函数连接起来。我们在几个问题上进行了实验，证明了这种策略在相对轨迹成本方面具有更好的性能。
</details></li>
</ul>
<hr>
<h2 id="ConDistFL-Conditional-Distillation-for-Federated-Learning-from-Partially-Annotated-Data"><a href="#ConDistFL-Conditional-Distillation-for-Federated-Learning-from-Partially-Annotated-Data" class="headerlink" title="ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data"></a>ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04070">http://arxiv.org/abs/2308.04070</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvidia/nvflare">https://github.com/nvidia/nvflare</a></li>
<li>paper_authors: Pochuan Wang, Chen Shen, Weichung Wang, Masahiro Oda, Chiou-Shann Fuh, Kensaku Mori, Holger R. Roth</li>
<li>for: 提出了一种总结多器官多疾病的整合分割模型，使用联邦学习（FL）技术实现共同开发模型而无需交换训练数据。</li>
<li>methods: 提出了一种名为ConDistFL的框架，将FL与知识塑造相结合，以便从部分标注数据中提取本地模型中的器官和肿瘤知识。</li>
<li>results: 在四个不同的部分标注腹部CT数据集上进行验证，结果表明提出的框架在FedAvg和FedOpt基elines之上显著提高了性能，并且在外部测试数据集上表现出了更高的普适性。<details>
<summary>Abstract</summary>
Developing a generalized segmentation model capable of simultaneously delineating multiple organs and diseases is highly desirable. Federated learning (FL) is a key technology enabling the collaborative development of a model without exchanging training data. However, the limited access to fully annotated training data poses a major challenge to training generalizable models. We propose "ConDistFL", a framework to solve this problem by combining FL with knowledge distillation. Local models can extract the knowledge of unlabeled organs and tumors from partially annotated data from the global model with an adequately designed conditional probability representation. We validate our framework on four distinct partially annotated abdominal CT datasets from the MSD and KiTS19 challenges. The experimental results show that the proposed framework significantly outperforms FedAvg and FedOpt baselines. Moreover, the performance on an external test dataset demonstrates superior generalizability compared to models trained on each dataset separately. Our ablation study suggests that ConDistFL can perform well without frequent aggregation, reducing the communication cost of FL. Our implementation will be available at https://github.com/NVIDIA/NVFlare/tree/dev/research/condist-fl.
</details>
<details>
<summary>摘要</summary>
开发一个通用分割模型，可同时分割多个器官和疾病，是非常感兴趣的。联邦学习（FL）是一种关键技术，它允许参与者共同开发模型，无需交换训练数据。然而，受到完全注释训练数据的限制，很难训练通用的模型。我们提议一种名为“ConDistFL”的框架，通过结合FL和知识储存来解决这个问题。本地模型可以从全球模型中提取不完全注释的器官和肿瘤的知识，并使用适当的条件概率表示。我们在四个不同的部分注释的腹部CT数据集上验证了我们的框架。实验结果表明，我们的框架在FedAvg和FedOpt基elines之上显著提高了性能。此外，对于外部测试数据集的性能表明，我们的模型具有更高的普适性比于每个数据集 separately 训练的模型。我们的剖析研究表明，ConDistFL可以在不经常的聚合下perform well， thereby reducing the communication cost of FL。我们的实现将在https://github.com/NVIDIA/NVFlare/tree/dev/research/condist-fl中提供。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Adversarial-Robustness-in-Low-Label-Regime-via-Adaptively-Weighted-Regularization-and-Knowledge-Distillation"><a href="#Enhancing-Adversarial-Robustness-in-Low-Label-Regime-via-Adaptively-Weighted-Regularization-and-Knowledge-Distillation" class="headerlink" title="Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation"></a>Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04061">http://arxiv.org/abs/2308.04061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoon Yang, Insung Kong, Yongdai Kim</li>
<li>for: 本研究强调 semi-supervised adversarial training，即在标注数据稀缺的情况下进行鲁棒化训练。</li>
<li>methods: 本文提出了两个Upper bound的robust risk，并提出了一个基于这两个Upper bound的Regularization term。此外，本文还提出了一种基于知识传播的semi-supervised teacher的方法。</li>
<li>results: 实验表明，我们的提议的算法可以 дости得 estado-of-the-art的性能，与现有算法相比，它在标注数据稀缺的情况下表现更优。例如，我们的算法只使用8%的标注数据时，与全量标注数据使用的超参数比较，其性能与标准准确率和鲁棒准确率在CIFAR-10上几乎相同。<details>
<summary>Abstract</summary>
Adversarial robustness is a research area that has recently received a lot of attention in the quest for trustworthy artificial intelligence. However, recent works on adversarial robustness have focused on supervised learning where it is assumed that labeled data is plentiful. In this paper, we investigate semi-supervised adversarial training where labeled data is scarce. We derive two upper bounds for the robust risk and propose a regularization term for unlabeled data motivated by these two upper bounds. Then, we develop a semi-supervised adversarial training algorithm that combines the proposed regularization term with knowledge distillation using a semi-supervised teacher (i.e., a teacher model trained using a semi-supervised learning algorithm). Our experiments show that our proposed algorithm achieves state-of-the-art performance with significant margins compared to existing algorithms. In particular, compared to supervised learning algorithms, performance of our proposed algorithm is not much worse even when the amount of labeled data is very small. For example, our algorithm with only 8\% labeled data is comparable to supervised adversarial training algorithms that use all labeled data, both in terms of standard and robust accuracies on CIFAR-10.
</details>
<details>
<summary>摘要</summary>
“反抗学习”是一个在人工智能领域受到非常多关注的研究领域，最近几年来它已经受到了很多关注。然而，当前的研究主要集中在超级vised学习中，假设有充足的标注数据。在这篇论文中，我们研究了半supervised adversarial training，即在标注数据匮乏的情况下进行反抗训练。我们 derive了两个Upper bound的 robust risk，并提出了一个基于这两个Upper bound的Regularization term。然后，我们开发了一种半supervised adversarial training算法，该算法将该Regularization term与知识塑化（即使用半supervised学习算法训练的教师模型）结合使用。我们的实验表明，我们的提出的算法可以达到现有算法的状态之巅性表现，并且与supervised学习算法相比，它在很小量标注数据下的性能不会太差。例如，我们的算法只使用8%的标注数据时，与supervised adversarial training算法相比，其性能在标准和Robust度上都不会太差，尤其是在CIFAR-10上。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Federated-Learning-by-Poisoning-Backdoor-Critical-Layers"><a href="#Backdoor-Federated-Learning-by-Poisoning-Backdoor-Critical-Layers" class="headerlink" title="Backdoor Federated Learning by Poisoning Backdoor-Critical Layers"></a>Backdoor Federated Learning by Poisoning Backdoor-Critical Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04466">http://arxiv.org/abs/2308.04466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan</li>
<li>for: 本研究旨在探讨 Federated Learning（FL）中的后门攻击和防御策略，特别是关注小部分层次（Backdoor-Critical，BC）的攻击和防御。</li>
<li>methods: 本研究提出了一种基于实际攻击者视角的BC层自适应后门攻击方法，通过细化地攻击BC层来实现较好的攻击效果和隐蔽性。</li>
<li>results: 实验结果显示，对于七种SOTA防御策略，我们的BC层自适应后门攻击可以成功攻击FL模型，仅需10%的坏客户端，并且超过最新的后门攻击方法。<details>
<summary>Abstract</summary>
Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense methodologies typically focus on the whole model. None of them recognizes the existence of backdoor-critical (BC) layers-a small subset of layers that dominate the model vulnerabilities. Attacking the BC layers achieves equivalent effects as attacking the whole model but at a far smaller chance of being detected by state-of-the-art (SOTA) defenses. This paper proposes a general in-situ approach that identifies and verifies BC layers from the perspective of attackers. Based on the identified BC layers, we carefully craft a new backdoor attack methodology that adaptively seeks a fundamental balance between attacking effects and stealthiness under various defense strategies. Extensive experiments show that our BC layer-aware backdoor attacks can successfully backdoor FL under seven SOTA defenses with only 10% malicious clients and outperform the latest backdoor attack methods.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 已经广泛应用于在分布式设备上进行敏感数据的机器学习训练。然而，分布式学习 paradigma 和 FL 中的不同设备之间的差异，使得攻击表面变得更加扩大。现有的 FL 攻击和防御方法ologies 通常集中在整个模型上。其中没有一个认可 BC 层（backdoor-critical layers）——一个小型层的 subsets 控制模型的漏洞。对 BC 层进行攻击可以达到攻击整个模型的同样效果，但是在现有的防御策略下可以让攻击更加隐蔽。本文提出了一种通用的即场方法，可以从攻击者的角度进行 BC 层的识别和验证。根据识别的 BC 层，我们针对这些层进行了一种新的后门攻击方法，可以在不同的防御策略下寻求一种基本的平衡，以确保攻击的效果和隐蔽性。经过广泛的实验，我们发现，我们的 BC 层认可后门攻击可以在七种 SOTA 防御策略下成功后门 FL，且比最新的后门攻击方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Toward-Improving-Predictive-Risk-Modelling-for-New-Zealand’s-Child-Welfare-System-Using-Clustering-Methods"><a href="#Toward-Improving-Predictive-Risk-Modelling-for-New-Zealand’s-Child-Welfare-System-Using-Clustering-Methods" class="headerlink" title="Toward Improving Predictive Risk Modelling for New Zealand’s Child Welfare System Using Clustering Methods"></a>Toward Improving Predictive Risk Modelling for New Zealand’s Child Welfare System Using Clustering Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04060">http://arxiv.org/abs/2308.04060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahar Barmomanesh, Victor Miranda-Soberanis</li>
<li>For: 这个论文旨在帮助社工更好地识别受到虐待儿童的风险，并决定当局是否应该 intervene。* Methods: 这个论文使用了行政数据和机器学习算法，以及主成分分析和K-Means聚类分析方法，以identify特定的儿童特征，并分析这些特征如何影响现有的风险模型的性能。* Results: 研究发现，使用不同的聚类方法可以分解出不同的儿童群体，并且这些群体之间存在显著的差异。LASSO逻辑回归模型在不同的聚类中都表现了较好的性能，但是对于年龄较小的儿童，模型表现更好。这些结果表明，可能需要为不同的年龄组分开发不同的风险模型，以提高模型的准确性。<details>
<summary>Abstract</summary>
The combination of clinical judgement and predictive risk models crucially assist social workers to segregate children at risk of maltreatment and decide when authorities should intervene. Predictive risk modelling to address this matter has been initiated by several governmental welfare authorities worldwide involving administrative data and machine learning algorithms. While previous studies have investigated risk factors relating to child maltreatment, several gaps remain as to understanding how such risk factors interact and whether predictive risk models perform differently for children with different features. By integrating Principal Component Analysis and K-Means clustering, this paper presents initial findings of our work on the identification of such features as well as their potential effect on current risk modelling frameworks. This approach allows examining existent, unidentified yet, clusters of New Zealand (NZ) children reported with care and protection concerns, as well as to analyse their inner structure, and evaluate the performance of prediction models trained cluster wise. We aim to discover the extent of clustering degree required as an early step in the development of predictive risk models for child maltreatment and so enhance the accuracy of such models intended for use by child protection authorities. The results from testing LASSO logistic regression models trained on identified clusters revealed no significant difference in their performance. The models, however, performed slightly better for two clusters including younger children. our results suggest that separate models might need to be developed for children of certain age to gain additional control over the error rates and to improve model accuracy. While results are promising, more evidence is needed to draw definitive conclusions, and further investigation is necessary.
</details>
<details>
<summary>摘要</summary>
“职业判断和预测风险模型在社工为了识别受到虐待风险的儿童和应否干预时作出决策非常重要。世界各地政府的儿童福利机构已经开始使用行政数据和机器学习算法进行预测风险模型。 although previous studies have investigated child maltreatment risk factors, there are still gaps in understanding how these risk factors interact and whether predictive risk models perform differently for children with different features. 本研究通过统计分析和K-Means聚类技术，初步发现了儿童特有的特征，并评估了这些特征对现有的风险模型的影响。我们的目标是发现这些群集的弹性度合理的程度，以便在开发预测风险模型时提高模型的准确性。我们发现，将模型训练分为各群集而训练，不会有 statistically significant difference in their performance. however, the models performed slightly better for two clusters including younger children. our results suggest that separate models might need to be developed for children of certain age to gain additional control over the error rates and to improve model accuracy. although the results are promising, more evidence is needed to draw definitive conclusions, and further investigation is necessary.”
</details></li>
</ul>
<hr>
<h2 id="The-Five-Dollar-Model-Generating-Game-Maps-and-Sprites-from-Sentence-Embeddings"><a href="#The-Five-Dollar-Model-Generating-Game-Maps-and-Sprites-from-Sentence-Embeddings" class="headerlink" title="The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings"></a>The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04052">http://arxiv.org/abs/2308.04052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TimMerino1710/five-dollar-model">https://github.com/TimMerino1710/five-dollar-model</a></li>
<li>paper_authors: Timothy Merino, Roman Negri, Dipika Rajesh, M Charity, Julian Togelius</li>
<li>for: 用于生成低维度图像，维护文本提示中的含义 semantics</li>
<li>methods: 使用五元模型，应用新的扩展策略来提高模型在有限数据集上的性能</li>
<li>results: 生成的图像保持文本提示中的含义，并且在三个小数据集上达到了良好的性能Here’s the same information in Traditional Chinese:</li>
<li>for: 用于生成低维度图像，维护文本提示中的含义 semantics</li>
<li>methods: 使用五元模型，应用新的扩展策略来提高模型在有限数据集上的性能</li>
<li>results: 生成的图像保持文本提示中的含义，并且在三个小数据集上达到了良好的性能<details>
<summary>Abstract</summary>
The five-dollar model is a lightweight text-to-image generative architecture that generates low dimensional images from an encoded text prompt. This model can successfully generate accurate and aesthetically pleasing content in low dimensional domains, with limited amounts of training data. Despite the small size of both the model and datasets, the generated images are still able to maintain the encoded semantic meaning of the textual prompt. We apply this model to three small datasets: pixel art video game maps, video game sprite images, and down-scaled emoji images and apply novel augmentation strategies to improve the performance of our model on these limited datasets. We evaluate our models performance using cosine similarity score between text-image pairs generated by the CLIP VIT-B/32 model.
</details>
<details>
<summary>摘要</summary>
五元模型是一种轻量级的文本到图像生成架构，可以将编码的文本提示转换成低维度的图像。这种模型可以在有限的训练数据量下生成准确和美观的内容，并且保持文本提示中的编码含义。我们对三个小 datasets（像素艺术视频游戏地图、视频游戏 sprite 图像和缩小的表情图像）进行应用，并使用新的扩展策略来提高我们的模型在这些有限的 datasets 上的性能。我们使用 CLIP VIT-B/32 模型计算文本-图像对的cosine相似性分数来评估我们的模型表现。
</details></li>
</ul>
<hr>
<h2 id="Generative-Models-for-Anomaly-Detection-and-Design-Space-Dimensionality-Reduction-in-Shape-Optimization"><a href="#Generative-Models-for-Anomaly-Detection-and-Design-Space-Dimensionality-Reduction-in-Shape-Optimization" class="headerlink" title="Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization"></a>Generative Models for Anomaly Detection and Design-Space Dimensionality Reduction in Shape Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04051">http://arxiv.org/abs/2308.04051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danny D’Agostino</li>
<li>for: 提高全球优化算法的效率，同时促进优化过程中高质量设计的生成。</li>
<li>methods: 减少原始设计变量，定义一个新的减少子空间，在这个子空间中最大化几何差异，并使用概率线性隐藏变量模型，如因子分析和概率主成分分析。</li>
<li>results: 通过测试DTMB 5415模型的壳体优化问题，示出了新框架可以提高全球优化算法的收敛，同时仅生成高质量几何特征的设计。<details>
<summary>Abstract</summary>
Our work presents a novel approach to shape optimization, that has the twofold objective to improve the efficiency of global optimization algorithms while promoting the generation of high-quality designs during the optimization process free of geometrical anomalies. This is accomplished by reducing the number of the original design variables defining a new reduced subspace where the geometrical variance is maximized and modeling the underlying generative process of the data via probabilistic linear latent variable models such as Factor Analysis and Probabilistic Principal Component Analysis. We show that the data follows approximately a Gaussian distribution when the shape modification method is linear and the design variables are sampled uniformly at random, due to the direct application of the central limit theorem. The model uncertainty is measured in terms of Mahalanobis distance, and the paper demonstrates that anomalous designs tend to exhibit a high value of this metric. This enables the definition of a new optimization model where anomalous geometries are penalized and consequently avoided during the optimization loop. The procedure is demonstrated for hull shape optimization of the DTMB 5415 model, extensively used as an international benchmark for shape optimization problems. The global optimization routine is carried out using Bayesian Optimization and the DIRECT algorithm. From the numerical results, the new framework improves the convergence of global optimization algorithms, while only designs with high-quality geometrical features are generated through the optimization routine thereby avoiding the wastage of precious computationally expensive simulations.
</details>
<details>
<summary>摘要</summary>
我们的工作提出了一种新的形优化方法，旨在提高全球优化算法的效率，同时在优化过程中生成高质量的设计，免受几何缺陷。这是通过减少原始设计变量数，定义一个新的减少子空间，在这个子空间中最大化几何差异，并通过泛化线性latent variable模型，如因子分析和概率主成分分析来模型数据。我们证明，当shape modification方法是线性的，并且设计变量是随机选择的时候，数据 approximate Gaussian distribution。模型不确定性是通过Mahalanobis距离测量的。我们发现，异常设计通常具有高值的这个指标。这使得我们可以定义一个新的优化模型，惩罚异常几何，并在优化过程中避免几何缺陷。我们使用 bayesian优化和DIRECT算法进行全球优化。从数值结果来看，新的框架可以提高全球优化算法的收敛速度，同时只有高质量的几何特征被生成，从而避免了 computationally expensive simulations的浪费。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-on-TF-IDF-feature-Weighting-Method-and-its-Analysis-using-Unstructured-Dataset"><a href="#A-Comparative-Study-on-TF-IDF-feature-Weighting-Method-and-its-Analysis-using-Unstructured-Dataset" class="headerlink" title="A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset"></a>A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04037">http://arxiv.org/abs/2308.04037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mamata Das, Selvakumar K., P. J. A. Alphonse<br>for: 本研究旨在提高文本分类的精度，并 investigate了文本特征权重方法的影响。methods: 本研究使用了两种特征提取方法：N-Grams和TF-IDF，并使用了多种State-of-the-art分类器进行验证，包括支持向量机(SVM)、概率Logistic Regression、多omial Naive Bayes(Multinomial NB)、Random Forest、决策树和k-nearest neighbors(KNN)。results: 结果显示，基于TF-IDF特征而不是基于N-Grams特征时，文本特征权重方法可以获得显著提高的性能，TF-IDF得到了最高的准确率(93.81%),精度(94.20%),回归率(93.81%)和F1-score(91.99%)值，在Random Forest分类器中。<details>
<summary>Abstract</summary>
Text Classification is the process of categorizing text into the relevant categories and its algorithms are at the core of many Natural Language Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP are the most highly used information retrieval methods in text classification. We have investigated and analyzed the feature weighting method for text classification on unstructured data. The proposed model considered two features N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset for sentiment analysis. Then we have used the state-of-the-art classifier to validate the method i.e., Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and k-nearest neighbors (KNN). From those two feature extractions, a significant increase in feature extraction with TF-IDF features rather than based on N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall (93.81%), and F1-score (91.99%) value in Random Forest classifier.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Top-K-Relevant-Passage-Retrieval-for-Biomedical-Question-Answering"><a href="#Top-K-Relevant-Passage-Retrieval-for-Biomedical-Question-Answering" class="headerlink" title="Top K Relevant Passage Retrieval for Biomedical Question Answering"></a>Top K Relevant Passage Retrieval for Biomedical Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04028">http://arxiv.org/abs/2308.04028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shashank140195/Biomedical_QA_Model">https://github.com/shashank140195/Biomedical_QA_Model</a></li>
<li>paper_authors: Shashank Gupta</li>
<li>for:  answers medical questions in the biomedical domain</li>
<li>methods:  uses the existing Dense Passage Retrieval framework and retrieves answers from Pubmed articles</li>
<li>results:  achieved a 0.81 F1 score on the BioASQ QA dataset<details>
<summary>Abstract</summary>
Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia Articles. In this work, we work on the existing DPR framework for the biomedical domain and retrieve answers from the Pubmed articles which is a reliable source to answer medical questions. When evaluated on a BioASQ QA dataset, our fine-tuned dense retriever results in a 0.81 F1 score.
</details>
<details>
<summary>摘要</summary>
问答任务是回答用户问题，使用大量文档库。其目的是在自然语言中提供精确的答案。问答需要高效的段落检索，以选择可能的上下文，传统的稀疏 вектор空间模型，如TF-IDF或BM25，是现行的方法。在互联网上，没有单独的文章可以提供用户问题的所有可能的答案。现有的 dense passage retrieval 模型在2018年12月20日的Wikipedia dump上进行训练，用作答案的来源文档。问答（QA）在开放领域和机器理解领域得到了 significanthuge strides，但在医学领域，这个问题还很未得到了探索。根据多份调查，生物医学问题无法从Wikipedia文章中正确地回答。在这项工作中，我们在现有的 DPR 框架上进行了修改，并从可靠的 Pubmed 文章中检索答案。当 evaluated 在 BioASQ QA 数据集上时，我们的精制 dense retriever 得到了0.81 F1 分数。
</details></li>
</ul>
<hr>
<h2 id="Scope-Loss-for-Imbalanced-Classification-and-RL-Exploration"><a href="#Scope-Loss-for-Imbalanced-Classification-and-RL-Exploration" class="headerlink" title="Scope Loss for Imbalanced Classification and RL Exploration"></a>Scope Loss for Imbalanced Classification and RL Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04024">http://arxiv.org/abs/2308.04024</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hasham Burhani, Xiao Qi Shi, Jonathan Jaegerman, Daniel Balicki</li>
<li>for: This paper is written for researchers and practitioners in the field of reinforcement learning and supervised classification.</li>
<li>methods: The paper uses a novel loss function called Scope Loss, which adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances.</li>
<li>results: The paper shows that Scope Loss outperforms state-of-the-art loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了针对机器学习领域的奖励学习和监督学习而写的。</li>
<li>methods: 这篇论文使用了一种新的损失函数called Scope Loss，它调整 gradients来避免过度利用和数据不均衡导致的性能损失。</li>
<li>results: 这篇论文显示，Scope Loss 在一个笔记式的奖励学习任务和一个偏见的分类 dataset 上比 State-of-the-art 损失函数表现出色。<details>
<summary>Abstract</summary>
We demonstrate equivalence between the reinforcement learning problem and the supervised classification problem. We consequently equate the exploration exploitation trade-off in reinforcement learning to the dataset imbalance problem in supervised classification, and find similarities in how they are addressed. From our analysis of the aforementioned problems we derive a novel loss function for reinforcement learning and supervised classification. Scope Loss, our new loss function, adjusts gradients to prevent performance losses from over-exploitation and dataset imbalances, without the need for any tuning. We test Scope Loss against SOTA loss functions over a basket of benchmark reinforcement learning tasks and a skewed classification dataset, and show that Scope Loss outperforms other loss functions.
</details>
<details>
<summary>摘要</summary>
我们证明了增强学习问题和分类问题之间的等价性。我们遂视探索优化和数据集不均势问题在增强学习和分类中的相似性，并从这些问题的分析中获得了一个新的损失函数。我们称之为Scope Loss。Scope Loss可以调整Gradient以避免过度探索和数据集不均势导致的性能损失，无需任何调整。我们对一签benchmark增强学习任务和一个偏斜的分类 dataset进行测试，与现有的损失函数进行比较，发现Scope Loss在这些任务中表现更好。
</details></li>
</ul>
<hr>
<h2 id="Improving-Performance-of-Semi-Supervised-Learning-by-Adversarial-Attacks"><a href="#Improving-Performance-of-Semi-Supervised-Learning-by-Adversarial-Attacks" class="headerlink" title="Improving Performance of Semi-Supervised Learning by Adversarial Attacks"></a>Improving Performance of Semi-Supervised Learning by Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04018">http://arxiv.org/abs/2308.04018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyoon Yang, Kunwoong Kim, Yongdai Kim</li>
<li>for: 提高现代半监督学习算法的性能</li>
<li>methods: 使用对预训练模型进行 adversarial 攻击，选择高自信度无标样本进行标注</li>
<li>results: 在 CIFAR10 上，与 SCAR 结合的三种现代半监督学习算法显著提高图像分类性能<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) algorithm is a setup built upon a realistic assumption that access to a large amount of labeled data is tough. In this study, we present a generalized framework, named SCAR, standing for Selecting Clean samples with Adversarial Robustness, for improving the performance of recent SSL algorithms. By adversarially attacking pre-trained models with semi-supervision, our framework shows substantial advances in classifying images. We introduce how adversarial attacks successfully select high-confident unlabeled data to be labeled with current predictions. On CIFAR10, three recent SSL algorithms with SCAR result in significantly improved image classification.
</details>
<details>
<summary>摘要</summary>
半supervised learning（SSL）算法是基于现实的假设，即获得大量标注数据困难。在这项研究中，我们提出了一个通用框架，名为SCAR，意为选择干净样本并具有对抗性强化，以提高最近的SSL算法性能。我们通过对预训练模型进行对抗攻击，成功地选择高度自信的未标注样本进行标注。在CIFAR10上，我们使用SCAR结果与三种最近的SSL算法显著提高图像分类。
</details></li>
</ul>
<hr>
<h2 id="Continual-Pre-Training-of-Large-Language-Models-How-to-re-warm-your-model"><a href="#Continual-Pre-Training-of-Large-Language-Models-How-to-re-warm-your-model" class="headerlink" title="Continual Pre-Training of Large Language Models: How to (re)warm your model?"></a>Continual Pre-Training of Large Language Models: How to (re)warm your model?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04014">http://arxiv.org/abs/2308.04014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, Timothée Lesort</li>
<li>for: 这个论文的目的是探讨 kontinual pre-training 的可行性，即在新数据上更新先前训练的模型，而不是从scratch重新训练。</li>
<li>methods: 作者们使用了不同的暖身策略来研究模型在新数据上的表现。他们的假设是，在训练新数据时，学习率需要重新增加以提高计算效率。</li>
<li>results: 研究结果显示，虽然在训练新数据时模型的损失 initially increases，但在长期运行时，它们的下游数据表现得更好，even outperforming scratch-trained models。<details>
<summary>Abstract</summary>
Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity. We experiment with different pre-training checkpoints, various maximum learning rates, and various warmup lengths. Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance, outperforming models trained from scratch$\unicode{x2013}$even for a large downstream dataset.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常在数十亿个字符上进行预训练，然后重新开始预训练过程。然而，这样的方法可能是比较昂贵和不fficient的。因此，我们提出了一种更有效的方法，即在新数据出现后不断更新已经预训练的模型，而不是从scratch重新训练模型。然而，新数据引入的分布变化通常会导致过去的数据表现下降。为了减少这种问题，在这项工作中，我们研究了不同的暖身策略的效果。我们假设，在训练新数据集时，学习率应该进行适度的增加，以提高计算效率。我们在Pile（上游数据集，300亿个字符）和SlimPajama（下游数据集，297亿个字符）中继续预训练Pythia 410M语言模型 architecture，并采用线性暖身和cosine衰减调度。我们在预训练检查点、最大学习率和暖身长度等方面进行了多种实验。我们的结果表明，虽然在upstream和downstream数据集上，首先使模型暖身会导致损失增加，但在更长时间后，它会提高下游表现，并在大下游数据集上超越从scratch训练的模型，即使使用大型数据集。
</details></li>
</ul>
<hr>
<h2 id="Generalization-bound-for-estimating-causal-effects-from-observational-network-data"><a href="#Generalization-bound-for-estimating-causal-effects-from-observational-network-data" class="headerlink" title="Generalization bound for estimating causal effects from observational network data"></a>Generalization bound for estimating causal effects from observational network data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04011">http://arxiv.org/abs/2308.04011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruichu Cai, Zeqin Yang, Weilin Chen, Yuguang Yan, Zhifeng Hao</li>
<li>for: 这篇论文旨在适用于 causal effect estimation 问题，具体来说是在 observational network 数据上进行 causal effect 估计。</li>
<li>methods: 该论文使用了 joint propensity score 重新分配 schema 和 Integral Probability Metric (IPM) 表示学习 schema， derive 了一个通用的 generalization bound  для causal effect estimation。</li>
<li>results: 实验研究表明，该算法在两个实际网络上的 semi-synthetic 数据上具有效果。<details>
<summary>Abstract</summary>
Estimating causal effects from observational network data is a significant but challenging problem. Existing works in causal inference for observational network data lack an analysis of the generalization bound, which can theoretically provide support for alleviating the complex confounding bias and practically guide the design of learning objectives in a principled manner. To fill this gap, we derive a generalization bound for causal effect estimation in network scenarios by exploiting 1) the reweighting schema based on joint propensity score and 2) the representation learning schema based on Integral Probability Metric (IPM). We provide two perspectives on the generalization bound in terms of reweighting and representation learning, respectively. Motivated by the analysis of the bound, we propose a weighting regression method based on the joint propensity score augmented with representation learning. Extensive experimental studies on two real-world networks with semi-synthetic data demonstrate the effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: estimating causal effects from observational network data is a significant but challenging problem. existing works in causal inference for observational network data lack an analysis of the generalization bound, which can theoretically provide support for alleviating the complex confounding bias and practically guide the design of learning objectives in a principled manner. to fill this gap, we derive a generalization bound for causal effect estimation in network scenarios by exploiting 1) the reweighting schema based on joint propensity score and 2) the representation learning schema based on integral probability metric (ipm). we provide two perspectives on the generalization bound in terms of reweighting and representation learning, respectively. motivated by the analysis of the bound, we propose a weighting regression method based on the joint propensity score augmented with representation learning. extensive experimental studies on two real-world networks with semi-synthetic data demonstrate the effectiveness of our algorithm.
</details></li>
</ul>
<hr>
<h2 id="Understanding-CNN-Hidden-Neuron-Activations-Using-Structured-Background-Knowledge-and-Deductive-Reasoning"><a href="#Understanding-CNN-Hidden-Neuron-Activations-Using-Structured-Background-Knowledge-and-Deductive-Reasoning" class="headerlink" title="Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning"></a>Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03999">http://arxiv.org/abs/2308.03999</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abhilekha-dalal/xai-using-wikidataAndEcii">https://github.com/abhilekha-dalal/xai-using-wikidataAndEcii</a></li>
<li>paper_authors: Abhilekha Dalal, Md Kamruzzaman Sarker, Adrita Barua, Eugene Vasserman, Pascal Hitzler</li>
<li>for: This paper aims to provide a systematic and automated method for interpreting the activations of hidden neurons in deep learning systems, specifically Convolutional Neural Networks (CNNs).</li>
<li>methods: The proposed method uses large-scale background knowledge and a symbolic reasoning approach called Concept Induction based on description logics to attach meaningful labels from the background knowledge to individual neurons in the dense layer of a CNN.</li>
<li>results: The paper demonstrates that the proposed method provides meaningful interpretations of hidden neuron activations, allowing for a better understanding of what the CNN has internally detected as relevant on the input.<details>
<summary>Abstract</summary>
A major challenge in Explainable AI is in correctly interpreting activations of hidden neurons: accurate interpretations would provide insights into the question of what a deep learning system has internally detected as relevant on the input, demystifying the otherwise black-box character of deep learning systems. The state of the art indicates that hidden node activations can, in some cases, be interpretable in a way that makes sense to humans, but systematic automated methods that would be able to hypothesize and verify interpretations of hidden neuron activations are underexplored. In this paper, we provide such a method and demonstrate that it provides meaningful interpretations. Our approach is based on using large-scale background knowledge approximately 2 million classes curated from the Wikipedia concept hierarchy together with a symbolic reasoning approach called Concept Induction based on description logics, originally developed for applications in the Semantic Web field. Our results show that we can automatically attach meaningful labels from the background knowledge to individual neurons in the dense layer of a Convolutional Neural Network through a hypothesis and verification process.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在可解释AI是正确解释隐藏节点的活动：正确的解释可以提供关于深度学习系统内部检测到的输入相关信息的深入了解，从而使深度学习系统从黑盒模型变为 clearer。现状的最佳实践表明，隐藏节点的活动可以在某些情况下被解释为人类可理解的方式，但是系统化的自动方法，能够对隐藏节点的活动进行假设和验证，尚未得到充分的探索。在这篇论文中，我们提供了一种这样的方法，并证明了它可以提供有意义的解释。我们的方法基于使用大规模的背景知识，约200万个类型从Wikipedia概念层次结构中精心筛选出来，并使用符号逻辑方法called Concept Induction，原本是为Semantic Web领域开发的。我们的结果表明，我们可以通过一种假设和验证过程，自动将 background knowledge 中的意义 labels 附加到 convolutional Neural Network 的稠密层中的各个神经元。
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Multi-Type-Multi-Agent-Deep-Reinforcement-Learning-for-Resource-Management-in-Space-Air-Ground-Integrated-Networks"><a href="#Cooperative-Multi-Type-Multi-Agent-Deep-Reinforcement-Learning-for-Resource-Management-in-Space-Air-Ground-Integrated-Networks" class="headerlink" title="Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks"></a>Cooperative Multi-Type Multi-Agent Deep Reinforcement Learning for Resource Management in Space-Air-Ground Integrated Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03995">http://arxiv.org/abs/2308.03995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengxi Zhang, Huaze Tang, Wenbo Ding, Xiao-Ping Zhang</li>
<li>for: 提高智能城市应用的潜在价值和可行性</li>
<li>methods: 提出了一种涵盖五个不同通信链的完整SAGIN系统，并提出了一种高效的多类多代理深度学习方法来解决资源管理问题</li>
<li>results: 实验结果表明提议的CMT-MARL方法能够提高总传输率和传输成功率，这些结果证明SAGIN的可能性和实现性<details>
<summary>Abstract</summary>
The Space-Air-Ground Integrated Network (SAGIN), integrating heterogeneous devices including low earth orbit (LEO) satellites, unmanned aerial vehicles (UAVs), and ground users (GUs), holds significant promise for advancing smart city applications. However, resource management of the SAGIN is a challenge requiring urgent study in that inappropriate resource management will cause poor data transmission, and hence affect the services in smart cities. In this paper, we develop a comprehensive SAGIN system that encompasses five distinct communication links and propose an efficient cooperative multi-type multi-agent deep reinforcement learning (CMT-MARL) method to address the resource management issue. The experimental results highlight the efficacy of the proposed CMT-MARL, as evidenced by key performance indicators such as the overall transmission rate and transmission success rate. These results underscore the potential value and feasibility of future implementation of the SAGIN.
</details>
<details>
<summary>摘要</summary>
space-air-ground 集成网络（SAGIN），包括低地球轨道卫星（LEO）、无人飞行器（UAV）和地面用户（GU）等多种设备，具有推动智能城市应用的潜在优势。然而，SAGIN资源管理却是一项需要紧迫研究的挑战，因为不当的资源管理可能导致数据传输差，影响智能城市服务的质量。在本文中，我们提出了一个全面的 SAGIN 系统，包括五种不同的通信链接，并提出了一种高效的合作多类多代理深度学习（CMT-MARL）方法来解决资源管理问题。实验结果表明，提议的 CMT-MARL 方法具有优秀的性能指标，如总传输率和传输成功率。这些结果证明了 SAGIN 的可能价值和可行性，并且为未来实施 SAGIN 提供了重要的参考。
</details></li>
</ul>
<hr>
<h2 id="Fourier-neural-operator-for-real-time-simulation-of-3D-dynamic-urban-microclimate"><a href="#Fourier-neural-operator-for-real-time-simulation-of-3D-dynamic-urban-microclimate" class="headerlink" title="Fourier neural operator for real-time simulation of 3D dynamic urban microclimate"></a>Fourier neural operator for real-time simulation of 3D dynamic urban microclimate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03985">http://arxiv.org/abs/2308.03985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhui Peng, Shaoxiang Qin, Senwen Yang, Jianchun Wang, Xue Liu, Liangzhu, Wang</li>
<li>for: 这篇论文主要关注于城市微气候的研究，以提高城市舒适性、健康性和建筑物能效性。</li>
<li>methods: 该论文使用了深度学习技术来加速复杂非线性交互的模型化。特别是使用了快速傅里叶 ней网络（FNO）来实时三维城市风场 simulate。</li>
<li>results: 实验结果表明，FNO模型可以准确重建快速更新的三维城市风场。此外，FNO方法可以在不同风向下进行泛化，并且在图形处理器上进行实时预测，使得城市微气候的实时模拟成为可能。<details>
<summary>Abstract</summary>
Global urbanization has underscored the significance of urban microclimates for human comfort, health, and building/urban energy efficiency. They profoundly influence building design and urban planning as major environmental impacts. Understanding local microclimates is essential for cities to prepare for climate change and effectively implement resilience measures. However, analyzing urban microclimates requires considering a complex array of outdoor parameters within computational domains at the city scale over a longer period than indoors. As a result, numerical methods like Computational Fluid Dynamics (CFD) become computationally expensive when evaluating the impact of urban microclimates. The rise of deep learning techniques has opened new opportunities for accelerating the modeling of complex non-linear interactions and system dynamics. Recently, the Fourier Neural Operator (FNO) has been shown to be very promising in accelerating solving the Partial Differential Equations (PDEs) and modeling fluid dynamic systems. In this work, we apply the FNO network for real-time three-dimensional (3D) urban wind field simulation. The training and testing data are generated from CFD simulation of the urban area, based on the semi-Lagrangian approach and fractional stepping method to simulate urban microclimate features for modeling large-scale urban problems. Numerical experiments show that the FNO model can accurately reconstruct the instantaneous spatial velocity field. We further evaluate the trained FNO model on unseen data with different wind directions, and the results show that the FNO model can generalize well on different wind directions. More importantly, the FNO approach can make predictions within milliseconds on the graphics processing unit, making real-time simulation of 3D dynamic urban microclimate possible.
</details>
<details>
<summary>摘要</summary>
全球城市化强调了城市微气候对人类舒适、健康和建筑/城市能效的重要性。它们对城市规划和建筑设计产生了深远的影响，是主要的环境因素。了解当地微气候非常重要，以便城市在气候变化面前做好准备，有效地实施抗逆性措施。然而，分析城市微气候需要考虑一系列的外部参数，包括城市规模内的计算领域和时间长达于室内。这使得计算流体动力学（CFD）方法成为计算成本较高的方法。随着深度学习技术的发展，新的机会在于加速复杂非线性交互和系统动力学模型化。在这种情况下，我们使用了整流神经网络（FNO）来实现实时三维城市风场 simulate。我们的实验表明，FNO网络可以准确重construct三维风场的快速变化。此外，我们还评估了在不同风向下测试的FNO模型，结果表明FNO模型可以在不同风向下 generale well。更重要的是，FNO方法可以在毫秒级别内进行预测，使实时三维动态城市微气候的模拟成为可能。
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-Human-Balance-through-a-Reinforcement-Learning-based-Muscle-Controller"><a href="#Characterization-of-Human-Balance-through-a-Reinforcement-Learning-based-Muscle-Controller" class="headerlink" title="Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller"></a>Characterization of Human Balance through a Reinforcement Learning-based Muscle Controller</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04462">http://arxiv.org/abs/2308.04462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kübra Akbaş, Carlotta Mummolo, Xianlian Zhou</li>
<li>for: This paper aims to provide a new approach for objectively assessing balance capability in humans by using a musculoskeletal model integrated with a balance controller trained through reinforcement learning.</li>
<li>methods: The paper employs a musculoskeletal model, reinforcement learning, and Proximal Policy Optimization to train a balance controller and investigate balancing capabilities.</li>
<li>results: The study shows that the approach can provide a promising new method for establishing balance recovery limits and objectively assessing balance capability in bipedal systems, particularly in humans, and reveals the effects of muscle weakness and neural excitation delay on balance recovery.Here is the simplified Chinese translation of the three key points:</li>
<li>for: 这篇论文目的是提供一种新的方法来对人类的平衡能力进行 объектив评估，使用一种 integrate 了 musculoskeletal 模型和平衡控制器的 reinforcement learning 方法。</li>
<li>methods: 这篇论文使用了 musculoskeletal 模型、reinforcement learning 和 Proximal Policy Optimization 等方法来训练平衡控制器并调查平衡能力。</li>
<li>results: 这个研究表明这种方法可以提供一种有前途的新方法来确定平衡恢复限制和对人类的平衡能力进行 объектив评估，并揭示了肌肉衰竭和神经延迟对平衡恢复的影响。<details>
<summary>Abstract</summary>
Balance assessment during physical rehabilitation often relies on rubric-oriented battery tests to score a patient's physical capabilities, leading to subjectivity. While some objective balance assessments exist, they are often limited to tracking the center of pressure (COP), which does not fully capture the whole-body postural stability. This study explores the use of the center of mass (COM) state space and presents a promising avenue for monitoring the balance capabilities in humans. We employ a musculoskeletal model integrated with a balance controller, trained through reinforcement learning (RL), to investigate balancing capabilities. The RL framework consists of two interconnected neural networks governing balance recovery and muscle coordination respectively, trained using Proximal Policy Optimization (PPO) with reference state initialization, early termination, and multiple training strategies. By exploring recovery from random initial COM states (position and velocity) space for a trained controller, we obtain the final BR enclosing successful balance recovery trajectories. Comparing the BRs with analytical postural stability limits from a linear inverted pendulum model, we observe a similar trend in successful COM states but more limited ranges in the recoverable areas. We further investigate the effect of muscle weakness and neural excitation delay on the BRs, revealing reduced balancing capability in different regions. Overall, our approach of learning muscular balance controllers presents a promising new method for establishing balance recovery limits and objectively assessing balance capability in bipedal systems, particularly in humans.
</details>
<details>
<summary>摘要</summary>
评估身体重建期间的平衡能力frequently使用测试 battery Rubric-oriented 评估病人的身体能力，导致主观性。 Although some objective balance assessments exist, they are often limited to tracking the center of pressure (COP), which does not fully capture the whole-body postural stability. This study explores the use of the center of mass (COM) state space and presents a promising avenue for monitoring balance capabilities in humans. We employ a musculoskeletal model integrated with a balance controller, trained through reinforcement learning (RL), to investigate balancing capabilities. The RL framework consists of two interconnected neural networks governing balance recovery and muscle coordination, respectively, trained using Proximal Policy Optimization (PPO) with reference state initialization, early termination, and multiple training strategies. By exploring recovery from random initial COM states (position and velocity) space for a trained controller, we obtain the final balance recovery (BR) enclosing successful balance recovery trajectories. Comparing the BRs with analytical postural stability limits from a linear inverted pendulum model, we observe a similar trend in successful COM states but more limited ranges in the recoverable areas. We further investigate the effect of muscle weakness and neural excitation delay on the BRs, revealing reduced balancing capability in different regions. Overall, our approach of learning muscular balance controllers presents a promising new method for establishing balance recovery limits and objectively assessing balance capability in bipedal systems, particularly in humans.
</details></li>
</ul>
<hr>
<h2 id="PUG-Photorealistic-and-Semantically-Controllable-Synthetic-Data-for-Representation-Learning"><a href="#PUG-Photorealistic-and-Semantically-Controllable-Synthetic-Data-for-Representation-Learning" class="headerlink" title="PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning"></a>PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03977">http://arxiv.org/abs/2308.03977</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/pug">https://github.com/facebookresearch/pug</a></li>
<li>paper_authors: Florian Bordes, Shashank Shekhar, Mark Ibrahim, Diane Bouchacourt, Pascal Vincent, Ari S. Morcos</li>
<li>for: 这项研究旨在推广使用真实准确的 sintetic 图像数据，以便更好地设计和评估深度神经网络。</li>
<li>methods: 这项研究使用了 Unreal Engine 游戏引擎，生成了 Photorealistic Unreal Graphics（PUG）环境和数据集，以便进行表示学习研究。</li>
<li>results: 这项研究示出了 PUG 环境和数据集可以帮助进行更加正式的视觉模型评估。<details>
<summary>Abstract</summary>
Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. Despite such promise, the use of synthetic image data is still limited -- and often played down -- mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and realism. We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning. In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的实验室数据集有以下几个缺点：首先，它们的数据量有限，使得模型的训练和评估受到限制。其次，实验室数据集通常是从互联网上抓取的，可能受到隐私、偏见和版权问题的影响。最后，实验室数据集的分布shift可能会导致模型在测试中表现不佳。在这种情况下，使用合成图像数据集成为一个有利的选择。合成图像数据集可以提供大量的数据样本，并且可以准确地控制每个场景和获得精细的标签和描述。在本研究中，我们提出了一种使用Unreal Engine游戏引擎生成PUG（实际图像）环境和数据集，以便进行表征学学习的研究。我们使用Unreal Engine来生成PUG环境，以提供更加准确和有利的图像数据。在本篇论文中，我们展示了PUG环境的潜在力量，可以帮助更好地评估视觉模型。>>
</details></li>
</ul>
<hr>
<h2 id="Amortized-Global-Search-for-Efficient-Preliminary-Trajectory-Design-with-Deep-Generative-Models"><a href="#Amortized-Global-Search-for-Efficient-Preliminary-Trajectory-Design-with-Deep-Generative-Models" class="headerlink" title="Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models"></a>Amortized Global Search for Efficient Preliminary Trajectory Design with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03960">http://arxiv.org/abs/2308.03960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjian Li, Amlan Sinha, Ryne Beeson</li>
<li>for: 本 paper 的目的是提出一种基于归一化搜索的抽象轨迹设计方法，以解决高维度和非几何的轨迹优化问题。</li>
<li>methods: 本 paper 使用深度生成模型来预测 trajectory 解的结构，并使用这些结构来加速对未经过seen的参数值进行全球搜索。</li>
<li>results: 本 paper 通过使用 deep generative models 预测 trajectory 解的结构，提高了轨迹优化问题的解决效率。<details>
<summary>Abstract</summary>
Preliminary trajectory design is a global search problem that seeks multiple qualitatively different solutions to a trajectory optimization problem. Due to its high dimensionality and non-convexity, and the frequent adjustment of problem parameters, the global search becomes computationally demanding. In this paper, we exploit the clustering structure in the solutions and propose an amortized global search (AmorGS) framework. We use deep generative models to predict trajectory solutions that share similar structures with previously solved problems, which accelerates the global search for unseen parameter values. Our method is evaluated using De Jong's 5th function and a low-thrust circular restricted three-body problem.
</details>
<details>
<summary>摘要</summary>
先期轨迹设计是一个全球搜索问题，旨在找到多个 качеitatively 不同的轨迹优化问题的解。由于其高维度和非拟合性，以及问题参数的频繁调整，全球搜索变得计算挑战。在这篇论文中，我们利用聚类结构在解决方案中，并提出了一个总体搜索（AmorGS）框架。我们使用深度生成模型预测轨迹解决方案，这些解决方案与之前解决过的问题有相似的结构，从而加速全球搜索未见参数值。我们的方法在De Jong的第五函数和一个低推力圆形三体问题中进行评估。
</details></li>
</ul>
<hr>
<h2 id="Fixed-Inter-Neuron-Covariability-Induces-Adversarial-Robustness"><a href="#Fixed-Inter-Neuron-Covariability-Induces-Adversarial-Robustness" class="headerlink" title="Fixed Inter-Neuron Covariability Induces Adversarial Robustness"></a>Fixed Inter-Neuron Covariability Induces Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03956">http://arxiv.org/abs/2308.03956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ahmed Shah, Bhiksha Raj</li>
<li>for: 提高深度神经网络（DNN）对 adversarial perturbation 的Robustness，以确保其在实际应用中的可靠性。</li>
<li>methods: 开发了 Self-Consistent Activation（SCA）层，该层包含 neuron 的活动是相互协调的，以满足一定的 covariability 模式。</li>
<li>results: 在图像和声音识别任务中，使用 SCA 层的模型可以达到高精度，并对 state-of-the-art Auto-PGD adversarial attack 显示出更高的Robustness，无需在 adversarially perturbed data 上进行训练。<details>
<summary>Abstract</summary>
The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \textit{without being trained on adversarially perturbed data
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PMU-measurements-based-short-term-voltage-stability-assessment-of-power-systems-via-deep-transfer-learning"><a href="#PMU-measurements-based-short-term-voltage-stability-assessment-of-power-systems-via-deep-transfer-learning" class="headerlink" title="PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning"></a>PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03953">http://arxiv.org/abs/2308.03953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SuperBruceJia/Power-Systems-Stability-Transfer-Learning">https://github.com/SuperBruceJia/Power-Systems-Stability-Transfer-Learning</a></li>
<li>paper_authors: Yang Li, Shitu Zhang, Yuanzheng Li, Jiting Cao, Shuyue Jia</li>
<li>for: 本研究旨在提出一种基于PMU测量数据的短期电压稳定评估方法，以解决现有深度学习方法对于电网结构变化、标签采集和小样本处理的局限性。</li>
<li>methods: 本方法使用深度传输学习，利用PMU测量数据创建初始数据集，采用时间拼接标注法和LSGAN数据增强技术，实现深度学习在小样本上的有效采用。另外，该方法还具有适应电网结构变化的能力，通过探索不同缺陷之间的连接关系。</li>
<li>results: 对于IEEE 39-bus试验系统，提出的方法可以通过转移学习提高评估精度约20%，并且具有强大的适应电网结构变化能力。与浅学习方法和其他深度学习基于方法相比，该方法具有显著的优势，特别是通过使用Transformer模型中的自注意机制。<details>
<summary>Abstract</summary>
Deep learning has emerged as an effective solution for addressing the challenges of short-term voltage stability assessment (STVSA) in power systems. However, existing deep learning-based STVSA approaches face limitations in adapting to topological changes, sample labeling, and handling small datasets. To overcome these challenges, this paper proposes a novel phasor measurement unit (PMU) measurements-based STVSA method by using deep transfer learning. The method leverages the real-time dynamic information captured by PMUs to create an initial dataset. It employs temporal ensembling for sample labeling and utilizes least squares generative adversarial networks (LSGAN) for data augmentation, enabling effective deep learning on small-scale datasets. Additionally, the method enhances adaptability to topological changes by exploring connections between different faults. Experimental results on the IEEE 39-bus test system demonstrate that the proposed method improves model evaluation accuracy by approximately 20% through transfer learning, exhibiting strong adaptability to topological changes. Leveraging the self-attention mechanism of the Transformer model, this approach offers significant advantages over shallow learning methods and other deep learning-based approaches.
</details>
<details>
<summary>摘要</summary>
深度学习已成为电力系统短期电压稳定评估（STVSA）的有效解决方案。然而，现有的深度学习基于STVSA方法存在适应性改变和小样本标注的限制。为了突破这些挑战，本文提出了一种基于phasor measurement unit（PMU）测量的新型STVSA方法。该方法利用PMU在实时动态信息中捕捉到的实际数据来创建初始数据集。它采用时间ensemble для样本标注，并使用最小二乘生成整形网络（LSGAN）进行数据扩展，以便在小规模数据集上进行深度学习。此外，该方法增强了 topological change 的适应性，通过探索不同的缺陷之间的连接。实验结果表明，该方法在IEEE 39-bus测试系统上提高了模型评估精度约20%，并且具有强大的适应性。基于Transformer模型的自注意机制，这种方法在对比浅学习方法和其他深度学习基于方法上表现出了显著优势。
</details></li>
</ul>
<hr>
<h2 id="The-Prospect-of-Enhancing-Large-Scale-Heterogeneous-Federated-Learning-with-Transformers"><a href="#The-Prospect-of-Enhancing-Large-Scale-Heterogeneous-Federated-Learning-with-Transformers" class="headerlink" title="The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers"></a>The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03945">http://arxiv.org/abs/2308.03945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yulan Gao, Zhaoxiang Hou, Chengyi Yang, Zengxiang Li, Han Yu</li>
<li>for: This paper investigates the use of Transformer-based federated learning (FL) models for achieving generalization and personalization in large-scale, heterogeneous FL tasks.</li>
<li>methods: The paper compares the performance of Transformer-based FL models with other deep neural network-based approaches, including ResNet and personalized ResNet-based FL, under various scenarios with varying numbers of data owners.</li>
<li>results: The paper shows that Transformer-based FL models outperform other approaches in large-scale, heterogeneous FL tasks, and provides insight into the reasons behind their superior performance through analysis of the Centered Kernel Alignment (CKA) representation similarity across different layers and FL models.<details>
<summary>Abstract</summary>
Federated learning (FL) addresses data privacy concerns by enabling collaborative training of AI models across distributed data owners. Wide adoption of FL faces the fundamental challenges of data heterogeneity and the large scale of data owners involved. In this paper, we investigate the prospect of Transformer-based FL models for achieving generalization and personalization in this setting. We conduct extensive comparative experiments involving FL with Transformers, ResNet, and personalized ResNet-based FL approaches under various scenarios. These experiments consider varying numbers of data owners to demonstrate Transformers' advantages over deep neural networks in large-scale heterogeneous FL tasks. In addition, we analyze the superior performance of Transformers by comparing the Centered Kernel Alignment (CKA) representation similarity across different layers and FL models to gain insight into the reasons behind their promising capabilities.
</details>
<details>
<summary>摘要</summary>
合作学习（FL）解决了数据隐私问题，通过在分布式数据所有者之间进行AI模型的共同训练。随着FL的广泛应用，面临着数据不一致和数据所有者的大规模挑战。本文 investigate了使用Transformer-based FL模型来实现泛化和个性化在这种情况下。我们进行了对FL与Transformers、ResNet和个性化ResNet-based FL方法的比较性实验，包括不同数据所有者的场景。这些实验演示了Transformers在大规模不一致FL任务中的优势。此外，我们还分析了CKA表示相似性的中心kernel对FL模型的性能提高的原因。
</details></li>
</ul>
<hr>
<h2 id="GraPhSyM-Graph-Physical-Synthesis-Model"><a href="#GraPhSyM-Graph-Physical-Synthesis-Model" class="headerlink" title="GraPhSyM: Graph Physical Synthesis Model"></a>GraPhSyM: Graph Physical Synthesis Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03944">http://arxiv.org/abs/2308.03944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Agiza, Rajarshi Roy, Teodor Dumitru Ene, Saad Godil, Sherief Reda, Bryan Catanzaro</li>
<li>For: 预计算机化电路延迟和面积指标的快速和准确估算。* Methods: 使用图structure、连接性和电学性特征来预测物理合成变换的影响，并通过图注意力网络模型（GATv2）进行预测。* Results: 在6000个预测器添加器设计中训练GraPhSyM模型，可以快速和准确地预测未看过的添加器延迟和面积指标（98.3%和96.1%），并且可以在不同的延迟目标下精确预测添加器延迟和面积指标。<details>
<summary>Abstract</summary>
In this work, we introduce GraPhSyM, a Graph Attention Network (GATv2) model for fast and accurate estimation of post-physical synthesis circuit delay and area metrics from pre-physical synthesis circuit netlists. Once trained, GraPhSyM provides accurate visibility of final design metrics to early EDA stages, such as logic synthesis, without running the slow physical synthesis flow, enabling global co-optimization across stages. Additionally, the swift and precise feedback provided by GraPhSym is instrumental for machine-learning-based EDA optimization frameworks. Given a gate-level netlist of a circuit represented as a graph, GraPhSyM utilizes graph structure, connectivity, and electrical property features to predict the impact of physical synthesis transformations such as buffer insertion and gate sizing. When trained on a dataset of 6000 prefix adder designs synthesized at an aggressive delay target, GraPhSyM can accurately predict the post-synthesis delay (98.3%) and area (96.1%) metrics of unseen adders with a fast 0.22s inference time. Furthermore, we illustrate the compositionality of GraPhSyM by employing the model trained on a fixed delay target to accurately anticipate post-synthesis metrics at a variety of unseen delay targets. Lastly, we report promising generalization capabilities of the GraPhSyM model when it is evaluated on circuits different from the adders it was exclusively trained on. The results show the potential for GraPhSyM to serve as a powerful tool for advanced optimization techniques and as an oracle for EDA machine learning frameworks.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们引入了GraPhSyM模型，它是基于图注意力网络（GATv2）的一种快速和准确地计算逻辑电路延迟和面积指标的模型。一旦训练完成，GraPhSyM可以在普通逻辑合成阶段提供准确的最终设计指标视图，不需要执行慢速物理合成流程，从而实现全球协调。此外，GraPhSyM提供了快速和准确的反馈，这对机器学习基于EDA优化框架是非常有利的。给定一个逻辑电路的门级网络表示，GraPhSyM利用图结构、连接性和电学性特征来预测物理合成转换（如缓冲插入和ゲート大小调整）后的影响。当训练在6000个逻辑加法器的逻辑合成过程中，GraPhSyM可以准确预测未看过的加法器延迟（98.3%）和面积（96.1%）指标，并且具有0.22秒的快速推理时间。此外，我们证明了GraPhSyM模型的可组合性，可以使用已训练的固定延迟目标来准确预测未看过的延迟目标。最后，我们报告了GraPhSyM模型在不同于它专门训练的逻辑加法器上的良好泛化能力。结果表明，GraPhSyM可能成为高级优化技术和EDA机器学习框架的强大工具。
</details></li>
</ul>
<hr>
<h2 id="The-Compatibility-between-the-Pangu-Weather-Forecasting-Model-and-Meteorological-Operational-Data"><a href="#The-Compatibility-between-the-Pangu-Weather-Forecasting-Model-and-Meteorological-Operational-Data" class="headerlink" title="The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data"></a>The Compatibility between the Pangu Weather Forecasting Model and Meteorological Operational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04460">http://arxiv.org/abs/2308.04460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wencong Cheng, Yan Yan, Jiangjiang Xia, Qi Liu, Chang Qu, Zhigang Wang</li>
<li>for: 这 paper 是为了评估 Pangu-Weather 模型与各种常用的 NWP 操作分析相容性。</li>
<li>methods: 这 paper 使用了 Pangu-Weather 模型，并通过 caso studies 评估了模型与不同 NWP 系统的操作分析之间的 compatibilty。</li>
<li>results: 结果表明，Pangu-Weather 模型与不同的操作分析相容，并且可以提高初始条件质量以提高预测性能。<details>
<summary>Abstract</summary>
Recently, multiple data-driven models based on machine learning for weather forecasting have emerged. These models are highly competitive in terms of accuracy compared to traditional numerical weather prediction (NWP) systems. In particular, the Pangu-Weather model, which is open source for non-commercial use, has been validated for its forecasting performance by the European Centre for Medium-Range Weather Forecasts (ECMWF) and has recently been published in the journal "Nature". In this paper, we evaluate the compatibility of the Pangu-Weather model with several commonly used NWP operational analyses through case studies. The results indicate that the Pangu-Weather model is compatible with different operational analyses from various NWP systems as the model initial conditions, and it exhibits a relatively stable forecasting capability. Furthermore, we have verified that improving the quality of global or local initial conditions significantly contributes to enhancing the forecasting performance of the Pangu-Weather model.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:近期，基于机器学习的多种气象预报模型已经出现，与传统的数值天气预测系统（NWP）相比，它们的准确性非常高。特别是Pangu-Weather模型，该模型为非商业用途开源，在ECMWF（欧洲中期天气预测中心）的验证下，最近发表在《自然》杂志上。在这篇论文中，我们通过实例研究了Pangu-Weather模型与各种常用的NWP操作分析相容性。结果表明，Pangu-Weather模型可以与不同的NWP系统的操作分析相容，并且在初始条件质量提高后，预报性能有所提高。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-the-switching-operation-in-monoclonal-antibody-production-Economic-MPC-and-reinforcement-learning"><a href="#Optimizing-the-switching-operation-in-monoclonal-antibody-production-Economic-MPC-and-reinforcement-learning" class="headerlink" title="Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning"></a>Optimizing the switching operation in monoclonal antibody production: Economic MPC and reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03928">http://arxiv.org/abs/2308.03928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra A. Obiri, Song Bo, Bernard T. Agyeman, Benjamin Decardi-Nelson, Jinfeng Liu</li>
<li>for: 这paper的目的是提出了一种可靠的、高效的continuous manufacturing process для大规模生产monoclonal antibodies (mAbs)。</li>
<li>methods: 这paper使用了经济模控算法（EMPC）和深度强化学习（DRL）来优化连续生产过程中的换 columns操作。</li>
<li>results: 这paper的实验结果表明，使用sigmoid function approximation approach和ReLU approximation approach可以提高连续生产过程的效率和质量，而传统的 switching approach based on 1% product breakthrough rule则不能达到这样的效果。<details>
<summary>Abstract</summary>
Monoclonal antibodies (mAbs) have emerged as indispensable assets in medicine, and are currently at the forefront of biopharmaceutical product development. However, the growing market demand and the substantial doses required for mAb clinical treatments necessitate significant progress in its large-scale production. Most of the processes for industrial mAb production rely on batch operations, which result in significant downtime. The shift towards a fully continuous and integrated manufacturing process holds the potential to boost product yield and quality, while eliminating the extra expenses associated with storing intermediate products. The integrated continuous mAb production process can be divided into the upstream and downstream processes. One crucial aspect that ensures the continuity of the integrated process is the switching of the capture columns, which are typically chromatography columns operated in a fed-batch manner downstream. Due to the discrete nature of the switching operation, advanced process control algorithms such as economic MPC (EMPC) are computationally difficult to implement. This is because an integer nonlinear program (INLP) needs to be solved online at each sampling time. This paper introduces two computationally-efficient approaches for EMPC implementation, namely, a sigmoid function approximation approach and a rectified linear unit (ReLU) approximation approach. It also explores the application of deep reinforcement learning (DRL). These three methods are compared to the traditional switching approach which is based on a 1% product breakthrough rule and which involves no optimization.
</details>
<details>
<summary>摘要</summary>
单核球抗体（mAb）在医疗中已经成为不可或缺的资产，目前在生物制药产品开发中扮演了前列的角色。然而，增长的市场需求和大量的药品临床使用需要大幅提高大规模生产的效率。现有的大规模生产过程大多数采用批量操作，它们导致了很大的下时间。将生产过程推向完全连续和整合的制程可以提高产品的收益和质量，同时消除储存中间产品的额外成本。生产过程中的连续过程可以分为上游和下游过程。一个重要的确保连续过程的因素是捕捉Column的转换，它们通常是在下游过程中运行的牛顿批量方式。由于这个离散的转换操作，高级的调控算法如经济多项式控制（EMPC）在computationally具有问题。这是因为在每个样本时间点上需要解决一个数学问题。这篇文章介绍了三种 computationally-efficient的EMPC实现方法，namely，σ函数近似方法和Rectified Linear Unit（ReLU）近似方法。它还探讨了深度征兆学习（DRL）的应用。这三种方法与传统的转换方法，基于1%产品破坏规则，并不含优化。
</details></li>
</ul>
<hr>
<h2 id="Spellburst-A-Node-based-Interface-for-Exploratory-Creative-Coding-with-Natural-Language-Prompts"><a href="#Spellburst-A-Node-based-Interface-for-Exploratory-Creative-Coding-with-Natural-Language-Prompts" class="headerlink" title="Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts"></a>Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03921">http://arxiv.org/abs/2308.03921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Angert, Miroslav Ivan Suzara, Jenny Han, Christopher Lawrence Pondoc, Hariharan Subramonyam</li>
<li>for: This paper aims to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.</li>
<li>methods: The paper introduces Spellburst, a large language model (LLM) powered creative-coding environment that provides a node-based interface, expressive prompt-based interactions, and dynamic prompt-driven interfaces and direct code editing.</li>
<li>results: The paper evaluates Spellburst with artists and demonstrates its potential to enhance creative coding practices and inform the design of computational creativity tools.<details>
<summary>Abstract</summary>
Creative coding tasks are often exploratory in nature. When producing digital artwork, artists usually begin with a high-level semantic construct such as a "stained glass filter" and programmatically implement it by varying code parameters such as shape, color, lines, and opacity to produce visually appealing results. Based on interviews with artists, it can be effortful to translate semantic constructs to program syntax, and current programming tools don't lend well to rapid creative exploration. To address these challenges, we introduce Spellburst, a large language model (LLM) powered creative-coding environment. Spellburst provides (1) a node-based interface that allows artists to create generative art and explore variations through branching and merging operations, (2) expressive prompt-based interactions to engage in semantic programming, and (3) dynamic prompt-driven interfaces and direct code editing to seamlessly switch between semantic and syntactic exploration. Our evaluation with artists demonstrates Spellburst's potential to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.
</details>
<details>
<summary>摘要</summary>
创造性编程任务经常具有探索性质。当生成数字艺术作品时，艺术家通常从高水平semantic construct开始，如镜面纹理滤波器，然后通过代码参数的变化，如形状、颜色、线条和透明度，来生成visually appealing的结果。根据艺术家的采访，将semantic constructs翻译成编程语法可能会困难，现有的编程工具也不适合快速的创造性探索。为解决这些挑战，我们介绍Spellburst，一个基于大语言模型（LLM）的创造性编程环境。Spellburst提供以下特性：1. 节点基本的界面，允许艺术家通过分支和合并操作来生成生成艺术作品并探索不同的变化。2. 表达式基本的交互方式，让艺术家通过自然语言提示来参与semantic programming。3. dinamic prompt-driven界面和直接代码编辑，允许艺术家轻松地在semantic和syntactic空间之间转换。我们的评估表明，Spellburst可以提高创造性编程实践，并为计算创ativity工具的设计提供指导。
</details></li>
</ul>
<hr>
<h2 id="Predicting-and-explaining-nonlinear-material-response-using-deep-Physically-Guided-Neural-Networks-with-Internal-Variables"><a href="#Predicting-and-explaining-nonlinear-material-response-using-deep-Physically-Guided-Neural-Networks-with-Internal-Variables" class="headerlink" title="Predicting and explaining nonlinear material response using deep Physically Guided Neural Networks with Internal Variables"></a>Predicting and explaining nonlinear material response using deep Physically Guided Neural Networks with Internal Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03915">http://arxiv.org/abs/2308.03915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Orera-Echeverria, Jacobo Ayensa-Jiménez, Manuel Doblare</li>
<li>for: 这篇论文是为了探讨非线性材料模型化的问题，尤其是使用 físicamente guided neural networks with internal variables (PGNNIV) 方法来找出 constitutive laws。</li>
<li>methods: 这篇论文使用了PGNNIV方法，这是一种基于物理定义的神经网络方法，通过对压缩数据进行训练，可以预测 external 和 internal 变量，无需内部变量数据。</li>
<li>results: 研究发现，PGNNIV 方法可以预测不同类型材料（线性、硬化、软化）的 external 和 internal 变量，并且可以解释材料的 constitutive law，属于 Explainable Artificial Intelligence (XAI) 领域。<details>
<summary>Abstract</summary>
Nonlinear materials are often difficult to model with classical state model theory because they have a complex and sometimes inaccurate physical and mathematical description or we simply do not know how to describe such materials in terms of relations between external and internal variables. In many disciplines, Neural Network methods have arisen as powerful tools to identify very complex and non-linear correlations. In this work, we use the very recently developed concept of Physically Guided Neural Networks with Internal Variables (PGNNIV) to discover constitutive laws using a model-free approach and training solely with measured force-displacement data. PGNNIVs make a particular use of the physics of the problem to enforce constraints on specific hidden layers and are able to make predictions without internal variable data. We demonstrate that PGNNIVs are capable of predicting both internal and external variables under unseen load scenarios, regardless of the nature of the material considered (linear, with hardening or softening behavior and hyperelastic), unravelling the constitutive law of the material hence explaining its nature altogether, placing the method in what is known as eXplainable Artificial Intelligence (XAI).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ViLP-Knowledge-Exploration-using-Vision-Language-and-Pose-Embeddings-for-Video-Action-Recognition"><a href="#ViLP-Knowledge-Exploration-using-Vision-Language-and-Pose-Embeddings-for-Video-Action-Recognition" class="headerlink" title="ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition"></a>ViLP: Knowledge Exploration using Vision, Language, and Pose Embeddings for Video Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03908">http://arxiv.org/abs/2308.03908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumyabrata Chaudhuri, Saumik Bhattacharya</li>
<li>for: 这个论文旨在提出一种基于多modal学习的人体动作识别方法，以提高人体动作识别的准确率。</li>
<li>methods: 该方法使用了视觉信息（RGB模式）和人体姿态信息（2D skeleton或pose模式），并将这两种模式与文本特征相结合，以提高人体动作识别的精度。</li>
<li>results: 该方法在两个人体动作识别 benchmark dataset（UCF-101和HMDB-51）上达到了92.81%和73.02%的准确率，而无需视频数据预训练，并且 после kinetics 预训练，准确率提高至96.11%和75.75%。<details>
<summary>Abstract</summary>
Video Action Recognition (VAR) is a challenging task due to its inherent complexities. Though different approaches have been explored in the literature, designing a unified framework to recognize a large number of human actions is still a challenging problem. Recently, Multi-Modal Learning (MML) has demonstrated promising results in this domain. In literature, 2D skeleton or pose modality has often been used for this task, either independently or in conjunction with the visual information (RGB modality) present in videos. However, the combination of pose, visual information, and text attributes has not been explored yet, though text and pose attributes independently have been proven to be effective in numerous computer vision tasks. In this paper, we present the first pose augmented Vision-language model (VLM) for VAR. Notably, our scheme achieves an accuracy of 92.81% and 73.02% on two popular human video action recognition benchmark datasets, UCF-101 and HMDB-51, respectively, even without any video data pre-training, and an accuracy of 96.11% and 75.75% after kinetics pre-training.
</details>
<details>
<summary>摘要</summary>
视频动作识别（VAR）是一项复杂的任务，具有内在的复杂性。不同的方法在文献中已经被探讨，但设计一个统一的框架来识别大量人类动作仍然是一个挑战。近年来，多模态学习（MML）已经在这个领域展现出了有前途的成绩。在文献中，2D骨骼或姿势特征经常用于这项任务，可以独立或与视觉信息（RGB模式）一起使用。然而，将姿势、视觉信息和文本特征结合使用还没有被探讨，尽管姿势特征和文本特征独立地已经在计算机视觉任务中证明有效。在本文中，我们首次提出了一种姿势增强的视觉语言模型（VLM），并实现了92.81%和73.02%的准确率在两个流行的人类视频动作识别标准 dataset（UCF-101和HMDB-51）中，而无需任何视频数据预训练，并且在预训练后达到96.11%和75.75%的准确率。
</details></li>
</ul>
<hr>
<h2 id="Advancements-In-Crowd-Monitoring-System-A-Comprehensive-Analysis-of-Systematic-Approaches-and-Automation-Algorithms-State-of-The-Art"><a href="#Advancements-In-Crowd-Monitoring-System-A-Comprehensive-Analysis-of-Systematic-Approaches-and-Automation-Algorithms-State-of-The-Art" class="headerlink" title="Advancements In Crowd-Monitoring System: A Comprehensive Analysis of Systematic Approaches and Automation Algorithms: State-of-The-Art"></a>Advancements In Crowd-Monitoring System: A Comprehensive Analysis of Systematic Approaches and Automation Algorithms: State-of-The-Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03907">http://arxiv.org/abs/2308.03907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Ameen, Richard Stone</li>
<li>for: 这篇论文主要关注于如何提供可靠和安全的人群监测系统，以满足现在全球各地政府和安全机构对公共安全的担忧。</li>
<li>methods: 这篇论文将研究两种方法：视觉基础的技术和非视觉基础的技术，并进行深入的分析，以了解它们在不同的环境和时间上的效果。</li>
<li>results: 这篇论文将强调现代应用程序和人工智能算法在自动化系统中的应用，以及它们在不同情况下的效果。<details>
<summary>Abstract</summary>
Growing apprehensions surrounding public safety have captured the attention of numerous governments and security agencies across the globe. These entities are increasingly acknowledging the imperative need for reliable and secure crowd-monitoring systems to address these concerns. Effectively managing human gatherings necessitates proactive measures to prevent unforeseen events or complications, ensuring a safe and well-coordinated environment. The scarcity of research focusing on crowd monitoring systems and their security implications has given rise to a burgeoning area of investigation, exploring potential approaches to safeguard human congregations effectively. Crowd monitoring systems depend on a bifurcated approach, encompassing vision-based and non-vision-based technologies. An in-depth analysis of these two methodologies will be conducted in this research. The efficacy of these approaches is contingent upon the specific environment and temporal context in which they are deployed, as they each offer distinct advantages. This paper endeavors to present an in-depth analysis of the recent incorporation of artificial intelligence (AI) algorithms and models into automated systems, emphasizing their contemporary applications and effectiveness in various contexts.
</details>
<details>
<summary>摘要</summary>
全球各地政府和安全机构都有增长的担忧，即使是公众安全的问题。这些机构认为，有效地管理人群聚集需要采取积极措施，以防止意外事件或复杂的情况，确保安全和有效的环境。由于关于人群监测系统的研究不够，这个领域在过去几年中得到了快速发展。人群监测系统通常采用分两部分的方法：视觉基本的和非视觉基本的技术。本研究将进行深入的分析这两种方法，并评估它们在不同的环境和时间上的效果。此外，本文还将评估人群监测系统中的人工智能（AI）算法和模型的应用，包括它们在不同情况下的当代应用和效果。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Assistant-Language-Understanding-On-Device"><a href="#Intelligent-Assistant-Language-Understanding-On-Device" class="headerlink" title="Intelligent Assistant Language Understanding On Device"></a>Intelligent Assistant Language Understanding On Device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03905">http://arxiv.org/abs/2308.03905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Cecilia Aas, Hisham Abdelsalam, Irina Belousova, Shruti Bhargava, Jianpeng Cheng, Robert Daland, Joris Driesen, Federico Flego, Tristan Guigue, Anders Johannsen, Partha Lal, Jiarui Lu, Joel Ruben Antony Moniz, Nathan Perkins, Dhivya Piraviperumal, Stephen Pulman, Diarmuid Ó Séaghdha, David Q. Sun, John Torr, Marco Del Vecchio, Jay Wacker, Jason D. Williams, Hong Yu</li>
<li>for: 这篇论文目标是描述一种运行于个人设备上的自然语言理解系统的设计。</li>
<li>methods: 该系统采用了一些选择的建筑和技术，例如在对话系统文献中一些方法可能在部署环境中困难维护。</li>
<li>results: 该系统比服务器端助手更加私钥、可靠、快速、表达力强、准确。<details>
<summary>Abstract</summary>
It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
</details>
<details>
<summary>摘要</summary>
现在可以在手机和其他个人设备上运行个人数字助手。在这篇论文中，我们描述了一种运行于设备上的自然语言理解系统的设计。相比服务器基于的助手，这种系统更加私钥、可靠、快速、表达力 stronger和更准确。我们介绍了一些关键的架构和技术选择的原因。例如，一些对话系统文献中的方法在部署环境中具有困难维护的特点。我们希望通过分享我们的实践经验，可以对未来的研究工作产生影响。
</details></li>
</ul>
<hr>
<h2 id="On-genuine-invariance-learning-without-weight-tying"><a href="#On-genuine-invariance-learning-without-weight-tying" class="headerlink" title="On genuine invariance learning without weight-tying"></a>On genuine invariance learning without weight-tying</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03904">http://arxiv.org/abs/2308.03904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amoskalev/ginvariance">https://github.com/amoskalev/ginvariance</a></li>
<li>paper_authors: Artem Moskalev, Anna Sepliarskaia, Erik J. Bekkers, Arnold Smeulders</li>
<li>for: 本研究探讨神经网络学习的不变性和其限制，并与固定权重绑定的真正不变性进行比较。</li>
<li>methods: 作者采用群理论的视角分析神经网络的不变性学习，并未受权重绑定的限制。他们发现，即使神经网络能够正确地分类样本，但是下面的决策不具有真正的不变性。</li>
<li>results: 作者提出了几种度量学习不变性的指标，包括预测分布不变性、对数不变性和响应不变性相似性。他们发现，通过在训练过程中加权error regularization来引导学习不变性，可以使得神经网络学习的不变性与固定权重绑定模型的不变性相似。<details>
<summary>Abstract</summary>
In this paper, we investigate properties and limitations of invariance learned by neural networks from the data compared to the genuine invariance achieved through invariant weight-tying. To do so, we adopt a group theoretical perspective and analyze invariance learning in neural networks without weight-tying constraints. We demonstrate that even when a network learns to correctly classify samples on a group orbit, the underlying decision-making in such a model does not attain genuine invariance. Instead, learned invariance is strongly conditioned on the input data, rendering it unreliable if the input distribution shifts. We next demonstrate how to guide invariance learning toward genuine invariance by regularizing the invariance of a model at the training. To this end, we propose several metrics to quantify learned invariance: (i) predictive distribution invariance, (ii) logit invariance, and (iii) saliency invariance similarity. We show that the invariance learned with the invariance error regularization closely reassembles the genuine invariance of weight-tying models and reliably holds even under a severe input distribution shift. Closer analysis of the learned invariance also reveals the spectral decay phenomenon, when a network chooses to achieve the invariance to a specific transformation group by reducing the sensitivity to any input perturbation.
</details>
<details>
<summary>摘要</summary>
本文研究神经网络学习的不变性和其限制，并与真正的不变性相比较。为此，我们采用群理论的视角，分析神经网络没有重量约束时的不变性学习。我们发现，即使神经网络能正确地分类样本，其下面的决策过程并不具有真正的不变性。相反，学习的不变性受到输入数据的强烈条件，因此在输入分布变化时变得不可靠。我们后来提出了一些度量学习的不变性的指标，包括预测分布不变性、Logit不变性和吸引力不变性相似性。我们发现，通过不变性错误规范来帮助学习不变性可以很好地寄存真正的不变性，并且可以在输入分布变化时保持稳定。进一步分析学习的不变性还发现，神经网络会选择通过减少输入变化的敏感度来实现不变性。
</details></li>
</ul>
<hr>
<h2 id="FLIPS-Federated-Learning-using-Intelligent-Participant-Selection"><a href="#FLIPS-Federated-Learning-using-Intelligent-Participant-Selection" class="headerlink" title="FLIPS: Federated Learning using Intelligent Participant Selection"></a>FLIPS: Federated Learning using Intelligent Participant Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03901">http://arxiv.org/abs/2308.03901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Atul Bhope, K. R. Jayaram, Nalini Venkatasubramanian, Ashish Verma, Gegi Thomas</li>
<li>for: 这篇论文旨在设计和实现一个基于中间件的聚合系统，用于管理数据和参与者多样性在联合学习训练任务中。具体来说，文章研究了在联合学习训练中使用标签分布划分的参与者选择方法的效果。</li>
<li>methods: 文章使用了标签分布划分来 clustering 参与者，并在联合学习训练过程中确保每个群组都具有相对的代表性。文章还 incorporates 一种异常管理机制来处理分布式环境中的资源变化。</li>
<li>results: 文章的实验比较了 FLIPS 与随机选择、Oort 和梯度划分等三种 “聪明” 选择方法，并在两个真实世界数据集和三种常见的联合学习算法（FedYogi、FedProx 和 FedAvg）上进行了广泛的 empirical evaluation。结果表明，FLIPS 可以提高收敛率，并在具有延迟参与者的情况下保持高度的准确率，这些优点在不同的分布式环境和资源条件下也能够持续。<details>
<summary>Abstract</summary>
This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random participant selection, as well as two other "smart" selection mechanisms - Oort and gradient clustering using two real-world datasets, two different non-IID distributions and three common FL algorithms (FedYogi, FedProx and FedAvg). We demonstrate that FLIPS significantly improves convergence, achieving higher accuracy by 17 - 20 % with 20 - 60 % lower communication costs, and these benefits endure in the presence of straggler participants.
</details>
<details>
<summary>摘要</summary>
Empirical evaluation compares FLIPS with random participant selection, as well as two other "smart" selection mechanisms - Oort and gradient clustering. The results show that FLIPS significantly improves convergence, achieving higher accuracy by 17-20% with 20-60% lower communication costs. These benefits persist even in the presence of straggler participants. The paper uses two real-world datasets, two different non-IID distributions, and three common FL algorithms (FedYogi, FedProx, and FedAvg) to demonstrate the effectiveness of FLIPS.
</details></li>
</ul>
<hr>
<h2 id="Scalable-and-Equitable-Math-Problem-Solving-Strategy-Prediction-in-Big-Educational-Data"><a href="#Scalable-and-Equitable-Math-Problem-Solving-Strategy-Prediction-in-Big-Educational-Data" class="headerlink" title="Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data"></a>Scalable and Equitable Math Problem Solving Strategy Prediction in Big Educational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03892">http://arxiv.org/abs/2308.03892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anupshakya07/attn-scaling">https://github.com/anupshakya07/attn-scaling</a></li>
<li>paper_authors: Anup Shakya, Vasile Rus, Deepak Venugopal</li>
<li>for: 这项研究的目的是提高学生数学学习的效果，使用智能教育系统（ITS）和适应教学系统（AIS）。</li>
<li>methods: 该研究使用机器学习和人工智能技术，开发了一种名为MVec的嵌入，以学习学生的掌握度表示。然后使用非 Parametric 聚类方法，将这些嵌入分成不同的群集。最后，使用Transformers和Node2Vec进行学习mastery embedding，并使用LSTM进行策略预测。</li>
<li>results: 该研究可以在大规模学生互动数据集上实现高精度的策略预测，并且具有预测平等性，即可以平等地预测学生的策略水平。<details>
<summary>Abstract</summary>
Understanding a student's problem-solving strategy can have a significant impact on effective math learning using Intelligent Tutoring Systems (ITSs) and Adaptive Instructional Systems (AISs). For instance, the ITS/AIS can better personalize itself to correct specific misconceptions that are indicated by incorrect strategies, specific problems can be designed to improve strategies and frustration can be minimized by adapting to a student's natural way of thinking rather than trying to fit a standard strategy for all. While it may be possible for human experts to identify strategies manually in classroom settings with sufficient student interaction, it is not possible to scale this up to big data. Therefore, we leverage advances in Machine Learning and AI methods to perform scalable strategy prediction that is also fair to students at all skill levels. Specifically, we develop an embedding called MVec where we learn a representation based on the mastery of students. We then cluster these embeddings with a non-parametric clustering method where we progressively learn clusters such that we group together instances that have approximately symmetrical strategies. The strategy prediction model is trained on instances sampled from these clusters. This ensures that we train the model over diverse strategies and also that strategies from a particular group do not bias the DNN model, thus allowing it to optimize its parameters over all groups. Using real world large-scale student interaction datasets from MATHia, we implement our approach using transformers and Node2Vec for learning the mastery embeddings and LSTMs for predicting strategies. We show that our approach can scale up to achieve high accuracy by training on a small sample of a large dataset and also has predictive equality, i.e., it can predict strategies equally well for learners at diverse skill levels.
</details>
<details>
<summary>摘要</summary>
理解学生的问题解决策略可以对智能教学系统（ITS）和适应教学系统（AIS）的有效学习产生重要影响。例如，ITS/AIS可以更好地个性化自己，根据学生提交的错误策略来更正特定的误解，设计特定问题以提高策略，并降低学生的沮丧情绪，而不是强制学生遵循标准策略。虽然在教室 SETTINGS 中，人工专家可能可以手动确定策略，但不可能扩大到大数据。因此，我们利用机器学习和人工智能技术进行可扩展的策略预测，并保证该方法对所有技能水平的学生是公平的。我们开发了一个名为 MVec 的嵌入，其中我们学习基于学生的尝试情况的表示。然后，我们使用非Parametric 分 clustering方法，将这些嵌入进行分组，以便将相似策略的实例相互分组。我们的策略预测模型是基于这些分组的实例进行训练的。这种方法可以扩大到达到高准确率，并且有Predictive Equality 性，即它可以平等地预测学生的策略，无论他们的技能水平如何。使用实际世界的大规模学生互动数据集，我们使用 transformers 和 Node2Vec 来学习尝试情况的尝试情况，并使用 LSTM 来预测策略。我们的方法可以扩大到达到高准确率，并且有Predictive Equality 性。
</details></li>
</ul>
<hr>
<h2 id="Generative-Benchmark-Creation-for-Table-Union-Search"><a href="#Generative-Benchmark-Creation-for-Table-Union-Search" class="headerlink" title="Generative Benchmark Creation for Table Union Search"></a>Generative Benchmark Creation for Table Union Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03883">http://arxiv.org/abs/2308.03883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/northeastern-datalab/alt-gen">https://github.com/northeastern-datalab/alt-gen</a></li>
<li>paper_authors: Koyena Pal, Aamod Khatiwada, Roee Shraga, Renée J. Miller<br>for: tables union search benchmarkmethods: generative AI models large language modelsresults: new benchmark more challenging for existing methods top-performing method achieves lower Mean Average Precision on new benchmark compared to existing manually created benchmarks detailed analysis of methods possible with new benchmark<details>
<summary>Abstract</summary>
Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method for using generative models to create tables with specified properties. Using this method, we create a new benchmark containing pairs of tables that are both unionable and non-unionable but related. We thoroughly evaluate recent existing table union search methods over existing benchmarks and our new benchmark. We also present and evaluate a new table search methods based on recent large language models over all benchmarks. We show that the new benchmark is more challenging for all methods than hand-curated benchmarks, specifically, the top-performing method achieves a Mean Average Precision of around 60%, over 30% less than its performance on existing manually created benchmarks. We examine why this is the case and show that the new benchmark permits more detailed analysis of methods, including a study of both false positives and false negatives that were not possible with existing benchmarks.
</details>
<details>
<summary>摘要</summary>
传统上，数据管理受到人工生成的数据生成器的限制，如TPC集成，以控制数据大小和分布的重要参数。这些标准 benchmark 对数据库管理系统的采用和普及做出了重要贡献。然而，随着数据管理问题的 semanticization，人工生成的数据不能满足需求。我们的当前方法是通过手动筛选和标注实际数据来创建 benchmark。这些方法不具有可靠性和扩展性，而且无法确定创建的标准 benchmark 的可靠性。我们提议使用生成 AI 模型创建结构化数据 benchmark。我们介绍了一种使用生成模型创建表 avec 指定属性的方法。使用这种方法，我们创建了一个新的标准 benchmark，其中包含可 union 和不可 union 的表对。我们进行了现有benchmark和我们新创建的benchmark上现有方法的严格评估。我们还介绍了基于最新大语言模型的新表搜索方法，并对所有benchmark进行评估。我们发现新benchmark比手动创建的benchmark更加具有挑战性，特别是最高级performing方法在新benchmark上的 Mean Average Precision 约为 60%，相比手动创建 benchmark 上的表现下降了30%。我们分析了这种情况，并证明新benchmark允许更加细致的方法分析，包括对方法的false positives和false negatives进行研究，这些研究不可能通过现有benchmark进行。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Generalization-in-Offline-Reinforcement-Learning-via-Unseen-State-Augmentations"><a href="#Exploiting-Generalization-in-Offline-Reinforcement-Learning-via-Unseen-State-Augmentations" class="headerlink" title="Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations"></a>Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03882">http://arxiv.org/abs/2308.03882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirbhay Modhe, Qiaozi Gao, Ashwin Kalyan, Dhruv Batra, Govind Thattai, Gaurav Sukhatme</li>
<li>for: 这篇论文是关于线上强化学习（RL）方法的研究，它们可以寻找未经见过的状态和动作。</li>
<li>methods: 这些方法使用保守的价值估计，对未经见过的状态和动作进行惩罚，以保证在探索和利用之间寻找平衡。</li>
<li>results: 研究人员通过提出一种新的未经见过状态扩展策略，使得RL方法能够更好地找到未经见过的状态，并且可以更好地适应不同的任务。此外，他们还发现，使用这种扩展策略可以降低平均数据集Q估计的值，即更保守的估计。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to seen data). We observe improved performance in several offline RL tasks and find that our augmentation strategy consistently leads to overall lower average dataset Q-value estimates i.e. more conservative Q-value estimates than a baseline.
</details>
<details>
<summary>摘要</summary>
在线RL方法寻求 между探索和占用的平衡，通过保守的价值估计---对未见的状态和动作进行惩罚。无模型方法对所有未见动作进行惩罚，而模型基于方法可以通过模型执行来进一步利用未见状态。然而，这些方法在找到未见状态的远程处理方面受到两个因素的限制：1. 在模型中的很短执行 horizon，由于链式模型错误而导致。2. 模型执行仅从看到的Offline数据中的状态开始。我们relax这个第二个假设，并提出一种新的未见状态扩充策略，以便在已学习的模型和价值估计中进行扩充。我们的策略通过值指导的扰动seen状态，然后过滤出epistemicuncertainty度量过高（高错误）或过低（太相似于seen数据）的状态。我们发现在多个OfflineRL任务中表现出色，并发现我们的扩充策略通常比基线靠前。我们的augmentation策略通过在seen状态中进行值指导的扰动，以及过滤出高错误或太相似于seen数据的状态来实现。这使得我们的价值估计更加保守，并且在多个OfflineRL任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-and-Explaining-Large-Language-Models-for-Code-Using-Syntactic-Structures"><a href="#Evaluating-and-Explaining-Large-Language-Models-for-Code-Using-Syntactic-Structures" class="headerlink" title="Evaluating and Explaining Large Language Models for Code Using Syntactic Structures"></a>Evaluating and Explaining Large Language Models for Code Using Syntactic Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03873">http://arxiv.org/abs/2308.03873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wm-semeru/codesyntaxconcept">https://github.com/wm-semeru/codesyntaxconcept</a></li>
<li>paper_authors: David N Palacio, Alejandro Velasco, Daniel Rodriguez-Cardenas, Kevin Moran, Denys Poshyvanyk</li>
<li>for: This paper aims to provide a new method for explaining the predictions of large language models (LLMs) for code, called ASTxplainer, which can be used to evaluate the effectiveness of these models and help end-users understand their predictions.</li>
<li>methods: The paper proposes a novel approach called ASTxplainer, which aligns token predictions with abstract syntax trees (ASTs) to provide a fine-grained understanding of LLM predictions. The method extracts and aggregates normalized model logits within AST structures to provide insights into model behavior.</li>
<li>results: The paper presents the results of an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. The results show that ASTxplainer can provide valuable insights into LLM effectiveness and aid end-users in understanding predictions. Additionally, a user study found that the visualization of model predictions provided by ASTxplainer was useful for enabling end-users to explain predictions.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions.   To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enables both new methods for LLM evaluation and visualizations of LLM predictions that aid end-users in understanding model predictions. At its core, ASTxplainer provides an automated method for aligning token predictions with AST nodes, by extracting and aggregating normalized model logits within AST structures. To demonstrate the practical benefit of ASTxplainer, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. Additionally, we perform a user study examining the usefulness of an ASTxplainer-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for ASTxplainer to provide insights into LLM effectiveness, and aid end-users in understanding predictions.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM） для代码是一家高参数、转换器基于神经网络的家族，预训练在庞大的自然语言和编程语言数据集上。这些模型在商业AI基于开发者工具中使用，如GitHub CoPilot。然而，评估和解释LLMs的效果在编程任务上是一个复杂的问题，因为它们的大小和复杂性。LLMs的评估和解释方法紧密相关，即要可靠地将模型预测映射到细腻可理解的概念上。一旦这种映射实现，新的详细模型评估方法就可能实现。然而，现有的解释技术和评估标准主要关注模型稳定性或个别任务性能，而不是解释模型预测。为此，本文介绍了ASTxplainer，一种特有的解释方法，用于LLMs for code。ASTxplainer提供了一种自动将字符预测映射到AST结构中的方法，通过提取和聚合 норциали化模型强度的技术。为证明ASTxplainer的实用性，我们对12种流行的LLMs for code进行了一系列实验，并使用一个优选的GitHub项目数据集进行了一个详细的评估。此外，我们还进行了一次用户研究，以确定ASTxplainer derivated的可视化是否有用于解释模型预测。研究结果表明，ASTxplainer有可能为LLM效果提供新的视角，并帮助用户理解预测。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Equivalence-of-e-Commerce-Queries"><a href="#Semantic-Equivalence-of-e-Commerce-Queries" class="headerlink" title="Semantic Equivalence of e-Commerce Queries"></a>Semantic Equivalence of e-Commerce Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03869">http://arxiv.org/abs/2308.03869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritra Mandal, Daniel Tunkelang, Zhe Wu</li>
<li>for: 本研究旨在提高电商搜索中的搜索结果和商业效果，通过认可和利用查询Equivalence来解决搜索查询的问题。</li>
<li>methods: 本研究提出了一种框架，包括将查询映射到搜索意图的 вектор表示，并使用surface similarity和行为相似性来确定查询Equivalence。</li>
<li>results: 实验结果表明，提出的方法可以高效地认可和利用查询Equivalence，并在 Popular sentence transformer模型之上出performanced。结果还 highlights the potential of  leveraging历史行为数据和训练模型来提高电商搜索的用户体验和商业效果。<details>
<summary>Abstract</summary>
Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen queries. Experimental evaluations demonstrate the effectiveness of the proposed approach, outperforming popular sentence transformer models and achieving a Pearson correlation of 0.85 for query similarity. The results highlight the potential of leveraging historical behavior data and training models to recognize and utilize query equivalence in e-commerce search, leading to improved user experiences and business outcomes. Further advancements and benchmark datasets are encouraged to facilitate the development of solutions for this critical problem in the e-commerce domain.
</details>
<details>
<summary>摘要</summary>
<SYS>    输入文本翻译为简化字符串。</SYS>搜索查询的变化 pose 电商搜索中的挑战，因为相同的搜索意图可以通过不同的查询语句表达。本文介绍了一种框架，用于认可和利用查询等价性，以提高搜索者和商业目标的结果。该框架解决了三个关键问题：将查询映射到搜索意图的 вектор表示，标识Equivalent或相似的搜索意图的查询，以及优化用户或商业目标。该框架利用了表面相似性和行为相似性来确定查询等价性。表面相似性包括根据词形变化、词序排序、复合词和噪音词进行 canonicalization。行为相似性利用历史搜索行为生成搜索意图的 вектор表示。在线上进行训练的offline进程用于培训句子相似性模型，而在线上的最近邻近approach支持处理未经看过的查询。实验证明了提案的方法的有效性，比 популяр的句子转换模型更好，并达到了0.85的Pearson相关系数 для查询相似性。结果表明，通过利用历史行为数据和训练模型，可以认可和利用查询等价性，从而提高用户体验和商业结果。进一步的进步和标准 datasets 鼓励开发者们在电商领域开发解决这个关键问题的解决方案。
</details></li>
</ul>
<hr>
<h2 id="AI-Text-to-Behavior-A-Study-In-Steerability"><a href="#AI-Text-to-Behavior-A-Study-In-Steerability" class="headerlink" title="AI Text-to-Behavior: A Study In Steerability"></a>AI Text-to-Behavior: A Study In Steerability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07326">http://arxiv.org/abs/2308.07326</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Noever, Sam Hyams</li>
<li>for: 这项研究探讨了大语言模型（LLM）的可控性，尤其是OpenAI的ChatGPT迭代。</li>
<li>methods: 我们使用了行为心理学框架OCEAN（开放性、聪明性、外向性、合作性、情绪性），量测模型对特定提示的回应。</li>
<li>results: 我们发现，“开放性”存在语言含义模糊，而“聪明性”和“情绪性”在OCEAN框架中明确表现出来，而“外向性”和“合作性”则显示出了明确的分化。这些结果表明GPT的多样性和适应能力，但也指出了LLM的快速进步和一些训练技术的权限性。<details>
<summary>Abstract</summary>
The research explores the steerability of Large Language Models (LLMs), particularly OpenAI's ChatGPT iterations. By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts. When asked to generate text mimicking an extroverted personality, OCEAN scored the language alignment to that behavioral trait. In our analysis, while "openness" presented linguistic ambiguity, "conscientiousness" and "neuroticism" were distinctly evoked in the OCEAN framework, with "extroversion" and "agreeableness" showcasing a notable overlap yet distinct separation from other traits. Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions. Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles. However, the rapid advancements in LLM capabilities and the opaque nature of some training techniques make metric proposals degrade rapidly. Our research emphasizes a quantitative role to describe steerability in LLMs, presenting both its promise and areas for further refinement in aligning its progress to human intentions.
</details>
<details>
<summary>摘要</summary>
研究探讨大语言模型（LLM）的可控性，特别是OpenAI的ChatGPT迭代。通过使用行为心理学框架 called OCEAN（开放性、亲和力、EXTROVERSION、善于合作性、神经性），我们量化了模型对特定提示的应对性。当请求生成 simulate extroverted personality 的文本时，OCEAN 评分语言对该行为特征的Alignment。在我们的分析中，“开放性”表现出语言的ambiguity，而“谨慎性”和“神经性”在OCEAN框架中得到了明确的识别，而“EXTROVERSION”和“善于合作性”则显示了明显的重叠 yet distinct separation from other traits。我们的发现推动GPT的多样性和能力，并且可以根据人类的INTENTIONS来定制和适应。然而，LLM的快速进步和一些训练技术的不透明性使得度量建议迅速衰退。我们的研究强调了量化描述 LLM 的可控性的作用，并提出了该领域的进一步完善和人类INTENTIONS的Alignment。
</details></li>
</ul>
<hr>
<h2 id="MCTS-guided-Genetic-Algorithm-for-optimization-of-neural-network-weights"><a href="#MCTS-guided-Genetic-Algorithm-for-optimization-of-neural-network-weights" class="headerlink" title="MCTS guided Genetic Algorithm for optimization of neural network weights"></a>MCTS guided Genetic Algorithm for optimization of neural network weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04459">http://arxiv.org/abs/2308.04459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AkshayHebbar/MCTS-GA">https://github.com/AkshayHebbar/MCTS-GA</a></li>
<li>paper_authors: Akshay Hebbar</li>
<li>for: 本研究探讨了如何使用搜寻策略应用于遗传算法，以搜寻整个遗传树结构中的优化解决方案。</li>
<li>methods: 本研究使用了许多搜寻策略，包括宽度优先、深度优先和迭代法，但这些方法通常需要大量计算时间。在搜寻过程中，我们采用了反对抗技术，以快速获得最优解。</li>
<li>results: 本研究结果表明，将遗传算法和蒙特卡洛tree搜寻策略结合使用，可以快速找到遗传算法优化解决方案。<details>
<summary>Abstract</summary>
In this research, we investigate the possibility of applying a search strategy to genetic algorithms to explore the entire genetic tree structure. Several methods aid in performing tree searches; however, simpler algorithms such as breadth-first, depth-first, and iterative techniques are computation-heavy and often result in a long execution time. Adversarial techniques are often the preferred mechanism when performing a probabilistic search, yielding optimal results more quickly. The problem we are trying to tackle in this paper is the optimization of neural networks using genetic algorithms. Genetic algorithms (GA) form a tree of possible states and provide a mechanism for rewards via the fitness function. Monte Carlo Tree Search (MCTS) has proven to be an effective tree search strategy given states and rewards; therefore, we will combine these approaches to optimally search for the best result generated with genetic algorithms.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了将搜索策略应用于生物学算法，以探索整个遗传树结构。许多方法可以进行树搜索，但是简单的算法如广度优先、深度优先和迭代方法通常需要较长的计算时间。对于 probabilistic 搜索，反击技术通常是首选的机制，可以快速获得优化结果。我们在这篇论文中面临的问题是使用生物学算法优化神经网络。生物学算法形成一棵可能状态的树，并提供了回归函数来计算奖励。蒙特卡洛树搜索（MCTS）已经证明是在给定状态和奖励时效果的搜索策略，因此我们将这些方法结合使用，以优化使用生物学算法生成的最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Prompt-Engineering-via-Declarative-Crowdsourcing"><a href="#Revisiting-Prompt-Engineering-via-Declarative-Crowdsourcing" class="headerlink" title="Revisiting Prompt Engineering via Declarative Crowdsourcing"></a>Revisiting Prompt Engineering via Declarative Crowdsourcing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03854">http://arxiv.org/abs/2308.03854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya G. Parameswaran, Shreya Shankar, Parth Asawa, Naman Jain, Yujie Wang</li>
<li>for: 提高 LLM 数据处理工作流程的质量和效率，以及提供一种更原则化的描述引擎方法。</li>
<li>methods: 利用多种描述策略、保证内部一致性，以及混合 LLM 和非 LLM 方法来实现更原则化的描述引擎。</li>
<li>results: 在排序、实体解析和填充等应用中，采用该方法可以提高 LLM 的性能和可靠性，并且可以自动化和机器化描述引擎过程。<details>
<summary>Abstract</summary>
Large language models (LLMs) are incredibly powerful at comprehending and generating data in the form of text, but are brittle and error-prone. There has been an advent of toolkits and recipes centered around so-called prompt engineering-the process of asking an LLM to do something via a series of prompts. However, for LLM-powered data processing workflows, in particular, optimizing for quality, while keeping cost bounded, is a tedious, manual process. We put forth a vision for declarative prompt engineering. We view LLMs like crowd workers and leverage ideas from the declarative crowdsourcing literature-including leveraging multiple prompting strategies, ensuring internal consistency, and exploring hybrid-LLM-non-LLM approaches-to make prompt engineering a more principled process. Preliminary case studies on sorting, entity resolution, and imputation demonstrate the promise of our approach
</details>
<details>
<summary>摘要</summary>
巨型语言模型（LLM）拥有强大的文本理解和生成能力，但是容易受到影响和出错。随着召集工具和热门趋势的出现，有关提问工程（prompt engineering）的研究得到了更多的关注，即通过一系列提问请求LLM进行某种任务。然而，为LLM数据处理工作流程而优化质量，同时保持成本在可控范围内，是一个繁琐、手动的过程。我们提出了声明式提问工程的视野，将LLM视为众生力量，并借鉴了声明式人员召集文献中的一些想法，包括多种提问策略、保证内部一致性，以及混合LLM非LLM方法。这些案例研究显示了排序、实体匹配和填充等应用场景的搅麻潜力。
</details></li>
</ul>
<hr>
<h2 id="Search-Engine-and-Recommendation-System-for-the-Music-Industry-built-with-JinaAI"><a href="#Search-Engine-and-Recommendation-System-for-the-Music-Industry-built-with-JinaAI" class="headerlink" title="Search Engine and Recommendation System for the Music Industry built with JinaAI"></a>Search Engine and Recommendation System for the Music Industry built with JinaAI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03842">http://arxiv.org/abs/2308.03842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishita Gopalakrishnan, Sanjjushri Varshini R, Ponshriharini V</li>
<li>for: 这篇论文的目的是为了开发一个基于 Machine Learning 的音乐产业搜索引擎和推荐系统。</li>
<li>methods: 这篇论文使用了 Jina AI 框架，一个基于 MLOps 的搜索引擎架构，以提高搜索引擎的速度、精度和搜寻结果的质量。</li>
<li>results: 这篇论文的结果显示，使用 Jina AI 框架可以实现更好的搜寻结果和推荐系统，帮助用户更加方便地搜寻音乐。<details>
<summary>Abstract</summary>
One of the most intriguing debates regarding a novel task is the development of search engines and recommendation-based systems in the music industry. Studies have shown a drastic depression in the search engine fields, due to concerning factors such as speed, accuracy and the format of data given for querying. Often people face difficulty in searching for a song solely based on the title, hence a solution is proposed to complete a search analysis through a single query input and is matched with the lyrics of the songs present in the database. Hence it is essential to incorporate cutting-edge technology tools for developing a user-friendly search engine. Jina AI is an MLOps framework for building neural search engines that are utilized, in order for the user to obtain accurate results. Jina AI effectively helps to maintain and enhance the quality of performance for the search engine for the query given. An effective search engine and a recommendation system for the music industry, built with JinaAI.
</details>
<details>
<summary>摘要</summary>
一个非常吸引人的问题在音乐产业中是搜索引擎和推荐系统的开发。研究表明，搜索引擎领域受到了严重的萧瑟，主要原因包括速度、准确性和数据格式的问题。人们经常遇到查找歌曲 solely based on the title 是困难的，因此一种解决方案是通过单个查询输入完成搜索分析，并将数据库中的歌曲 lyrics 与输入进行匹配。因此，搜索引擎的开发需要采用 cutting-edge 技术工具，以建立用户友好的搜索引擎。Jina AI 是一个 MLOps 框架，用于构建基于神经网络的搜索引擎，可以帮助用户 obtian 精准的结果。Jina AI 有效地帮助维护和提高搜索引擎的性能质量。一个基于 JinaAI 的高效搜索引擎和推荐系统，可以为音乐产业提供更好的用户体验。
</details></li>
</ul>
<hr>
<h2 id="The-Copycat-Perceptron-Smashing-Barriers-Through-Collective-Learning"><a href="#The-Copycat-Perceptron-Smashing-Barriers-Through-Collective-Learning" class="headerlink" title="The Copycat Perceptron: Smashing Barriers Through Collective Learning"></a>The Copycat Perceptron: Smashing Barriers Through Collective Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03743">http://arxiv.org/abs/2308.03743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Catania, Aurélien Decelle, Beatriz Seoane</li>
<li>for: 研究一种 teacher-student enario中的 $y$ coupled binary perceptrons 的平衡性质。</li>
<li>methods: 使用一种适当的学习规则，并对学生模型的权重进行显式杂化，使其与教师模型的权重具有哈明顿距离的关系。</li>
<li>results: 在存在温度噪声的情况下，研究发现，在教师模型的指导下，学生模型的学习表现得到改进，而且随着学生模型的数量增加，学习表现得到进一步改进。<details>
<summary>Abstract</summary>
We characterize the equilibrium properties of a model of $y$ coupled binary perceptrons in the teacher-student scenario, subject to a suitable learning rule, with an explicit ferromagnetic coupling proportional to the Hamming distance between the students' weights. In contrast to recent works, we analyze a more general setting in which a thermal noise is present that affects the generalization performance of each student. Specifically, in the presence of a nonzero temperature, which assigns nonzero probability to configurations that misclassify samples with respect to the teacher's prescription, we find that the coupling of replicas leads to a shift of the phase diagram to smaller values of $\alpha$: This suggests that the free energy landscape gets smoother around the solution with good generalization (i.e., the teacher) at a fixed fraction of reviewed examples, which allows local update algorithms such as Simulated Annealing to reach the solution before the dynamics gets frozen. Finally, from a learning perspective, these results suggest that more students (in this case, with the same amount of data) are able to learn the same rule when coupled together with a smaller amount of data.
</details>
<details>
<summary>摘要</summary>
我们描述一个 teacher-student 模型中的 $y$ coupled binary perceptron 的平衡性能，采用一种合适的学习规则，带有明确的 ferromagnetic 相互作用，其比例与学生们的权重之间的汉明距离相关。与先前的研究不同，我们分析了一个更通用的设置，在其中每个学生都受到一定温度的影响，这使得每个学生的泛化性能受到影响。 Specifically, 在非零温度下，权重的配置可能会错误地分类样本，从而导致学生的泛化性能下降。我们发现，在这种情况下，同学生之间的相互作用会使得解析的相对稳定点变小，这意味着解析的自由能面积变得更加平滑，使得本地更新算法如模拟热化可以更容易地到达解析。最后，从学习角度来看，这些结果表明，当学生们相互couple时，可以通过更小的数据量来学习同样的规则，这意味着更多的学生可以在同样的数据量下学习。
</details></li>
</ul>
<hr>
<h2 id="Randomized-algorithms-for-precise-measurement-of-differentially-private-personalized-recommendations"><a href="#Randomized-algorithms-for-precise-measurement-of-differentially-private-personalized-recommendations" class="headerlink" title="Randomized algorithms for precise measurement of differentially-private, personalized recommendations"></a>Randomized algorithms for precise measurement of differentially-private, personalized recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03735">http://arxiv.org/abs/2308.03735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-dprecs">https://github.com/apple/ml-dprecs</a></li>
<li>paper_authors: Allegra Laro, Yanqing Chen, Hao He, Babak Aghazadeh</li>
<li>for: 提出一种隐私保护的个性化推荐算法，以便在个性化推荐中保护用户隐私。</li>
<li>methods: 提出的算法使用幂等函数和均值分布来保证隐私，并通过实验证明其可以保持 preciseness 和隐私性。</li>
<li>results: 对于广告应用场景，该算法可以提高用户体验、广告商价值和平台收入，同时保持隐私性。<details>
<summary>Abstract</summary>
Personalized recommendations form an important part of today's internet ecosystem, helping artists and creators to reach interested users, and helping users to discover new and engaging content. However, many users today are skeptical of platforms that personalize recommendations, in part due to historically careless treatment of personal data and data privacy. Now, businesses that rely on personalized recommendations are entering a new paradigm, where many of their systems must be overhauled to be privacy-first. In this article, we propose an algorithm for personalized recommendations that facilitates both precise and differentially-private measurement. We consider advertising as an example application, and conduct offline experiments to quantify how the proposed privacy-preserving algorithm affects key metrics related to user experience, advertiser value, and platform revenue compared to the extremes of both (private) non-personalized and non-private, personalized implementations.
</details>
<details>
<summary>摘要</summary>
个人化推荐作为今天互联网生态系统中的一部分，帮助艺术家和创作者达到有趣用户，并帮助用户发现新的有趣内容。然而，许多用户今天对个人化推荐平台表示怀疑，其中一部分是由于历史上不谨慎处理个人数据和隐私。现在，企业们正在进入一个新的平台，其中许多系统需要重新设计，以保持隐私First。在这篇文章中，我们提出一种用于个人化推荐的算法，以实现精准和隐私保护。我们使用广告作为应用例子，并在线上实验来衡量该提议的隐私保护算法对用户体验、广告商价值和平台收入的影响，与非个人化和非隐私的个人化实现相比。
</details></li>
</ul>
<hr>
<h2 id="SurvBeX-An-explanation-method-of-the-machine-learning-survival-models-based-on-the-Beran-estimator"><a href="#SurvBeX-An-explanation-method-of-the-machine-learning-survival-models-based-on-the-Beran-estimator" class="headerlink" title="SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator"></a>SurvBeX: An explanation method of the machine learning survival models based on the Beran estimator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03730">http://arxiv.org/abs/2308.03730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danilaeremenko/survbex">https://github.com/danilaeremenko/survbex</a></li>
<li>paper_authors: Lev V. Utkin, Danila Y. Eremenko, Andrei V. Konstantinov</li>
<li>For: The paper proposes a new method called SurvBeX to explain the predictions of machine learning survival black-box models.* Methods: The method uses a modified Beran estimator as a surrogate explanation model, and generates many points in a local area around an example of interest to compute the survival function of the black-box model and the Beran estimator.* Results: The paper demonstrates the efficiency of SurvBeX through numerical experiments with synthetic and real survival data, and compares the method with the well-known method SurvLIME and SurvSHAP. The code implementing SurvBeX is available online.Here are the three points in Simplified Chinese text:* For: 这篇论文提出了一种新的方法SurvBeX，用于解释机器学习生存黑盒模型的预测结果。* Methods: SurvBeX使用修改后的Beran估计器作为准确解释模型，并在 интерес示例附近生成多个点，以计算黑盒模型和Beran估计器的生存函数。* Results: 论文通过synthetic和实际生存数据的numerical实验，证明SurvBeX的效果，并与SurvLIME和SurvSHAP进行比较。代码实现SurvBeX可以在<a target="_blank" rel="noopener" href="https://github.com/DanilaEremenko/SurvBeX%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/DanilaEremenko/SurvBeX上获取。</a><details>
<summary>Abstract</summary>
An explanation method called SurvBeX is proposed to interpret predictions of the machine learning survival black-box models. The main idea behind the method is to use the modified Beran estimator as the surrogate explanation model. Coefficients, incorporated into Beran estimator, can be regarded as values of the feature impacts on the black-box model prediction. Following the well-known LIME method, many points are generated in a local area around an example of interest. For every generated example, the survival function of the black-box model is computed, and the survival function of the surrogate model (the Beran estimator) is constructed as a function of the explanation coefficients. In order to find the explanation coefficients, it is proposed to minimize the mean distance between the survival functions of the black-box model and the Beran estimator produced by the generated examples. Many numerical experiments with synthetic and real survival data demonstrate the SurvBeX efficiency and compare the method with the well-known method SurvLIME. The method is also compared with the method SurvSHAP. The code implementing SurvBeX is available at: https://github.com/DanilaEremenko/SurvBeX
</details>
<details>
<summary>摘要</summary>
提出一种名为SurvBeX的解释方法，用于解释机器学习预测模型的逝去黑盒模型。该方法的主要想法是使用修改后的Beran估计器作为解释模型。 incorporated into Beran estimator的系数可以 viewed as预测模型中特征影响值。采用LIME方法的思路，在对 interessant example的local区域附近生成多个例子。每个生成的例子中，预测模型的生存函数被计算，并将生存函数转化为解释系数的函数。以 minimize the mean distance between the survival functions of the black-box model and the Beran estimator produced by the generated examples。 numerically experiments with synthetic and real survival data demonstrate SurvBeX efficiency and compare the method with well-known method SurvLIME. The method is also compared with SurvSHAP method. SurvBeX的代码可以在以下链接获取：https://github.com/DanilaEremenko/SurvBeX。
</details></li>
</ul>
<hr>
<h2 id="Dimensionality-Reduction-for-Improving-Out-of-Distribution-Detection-in-Medical-Image-Segmentation"><a href="#Dimensionality-Reduction-for-Improving-Out-of-Distribution-Detection-in-Medical-Image-Segmentation" class="headerlink" title="Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation"></a>Dimensionality Reduction for Improving Out-of-Distribution Detection in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03723">http://arxiv.org/abs/2308.03723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mckellwoodland/dimen_reduce_mahal">https://github.com/mckellwoodland/dimen_reduce_mahal</a></li>
<li>paper_authors: McKell Woodland, Nihil Patel, Mais Al Taie, Joshua P. Yung, Tucker J. Netherton, Ankit B. Patel, Kristy K. Brock</li>
<li>for: 这个研究是为了检测MRI肿瘤影像分类模型的外部分布情况，以避免自动偏见。</li>
<li>methods: 这个研究使用了Mahalanobis距离后置法，将Swin UNITER模型的瓶颈特征使用主成分分析，实现高性能的外部分布检测。</li>
<li>results: 研究发现，这种方法可以实现高度的外部分布检测，并且具有较少的计算负载。<details>
<summary>Abstract</summary>
Clinically deployed segmentation models are known to fail on data outside of their training distribution. As these models perform well on most cases, it is imperative to detect out-of-distribution (OOD) images at inference to protect against automation bias. This work applies the Mahalanobis distance post hoc to the bottleneck features of a Swin UNETR model that segments the liver on T1-weighted magnetic resonance imaging. By reducing the dimensions of the bottleneck features with principal component analysis, OOD images were detected with high performance and minimal computational load.
</details>
<details>
<summary>摘要</summary>
临床部署的分割模型经常会在训练数据外部失败。由于这些模型在大多数情况下表现良好，因此在推断时检测出对外部数据的自动偏见是必要的。这项工作使用Swin UNITER模型的瓶颈特征进行 Mahalanobis 距离后处理，以检测T1核磁共振成像上的肝脏分割图像。通过减少瓶颈特征的维度使用主成分分析，可以高效地检测到对外部数据的图像。
</details></li>
</ul>
<hr>
<h2 id="“Do-Anything-Now”-Characterizing-and-Evaluating-In-The-Wild-Jailbreak-Prompts-on-Large-Language-Models"><a href="#“Do-Anything-Now”-Characterizing-and-Evaluating-In-The-Wild-Jailbreak-Prompts-on-Large-Language-Models" class="headerlink" title="“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"></a>“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03825">http://arxiv.org/abs/2308.03825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/verazuo/jailbreak_llms">https://github.com/verazuo/jailbreak_llms</a></li>
<li>paper_authors: Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang</li>
<li>for: This paper aims to study the emergence and evolution of jailbreak prompts in the wild, and to assess the potential harm caused by these prompts on large language models (LLMs).</li>
<li>methods: The authors use natural language processing technologies and graph-based community detection methods to collect and analyze 6,387 jailbreak prompts from four platforms over six months. They also create a question set comprising 46,800 samples across 13 forbidden scenarios to evaluate the effectiveness of current LLMs and safeguards in defending against jailbreak prompts.</li>
<li>results: The authors find unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. They also observe that jailbreak prompts are increasingly shifting from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. Additionally, they identify two highly effective jailbreak prompts that achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and have persisted online for over 100 days.<details>
<summary>Abstract</summary>
The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）的不当使用已经引起了广泛的关注，并且有努力以寻求对human values和合法用途进行Alignment。然而，一种特殊的恶意提示，称为监狱提示，已经出现并不断演化以绕过安全措施并征引出危险内容。在这篇论文中，我们进行了第一次在野外中对监狱提示的量化研究，收集了6,387个提示从四个平台上经过六个月。通过自然语言处理技术和图形基本社区探测方法，我们发现了监狱提示的独特特征和主要攻击策略，如提示注入和特权提升。我们还发现，监狱提示逐渐从公共平台迁移到私有平台，这对LLM供应商提出了新的检测挑战。为了评估监狱提示的可能伤害，我们创建了46,800个问题组合，覆盖13种禁止enario。我们的实验表明，当前的LLM和安全措施无法在所有情况下有效防御监狱提示。特别是，我们标识出了两个非常有效的监狱提示，在ChatGPT（GPT-3.5）和GPT-4上具有0.99攻击成功率，并且它们在线上延续了超过100天。我们的工作照明了监狱提示的严重和演化的威胁风险。我们希望我们的研究可以促进研究社区和LLM供应商在推广安全和有序的LLM方面做出更多的努力。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Framework-for-Distributed-Image-Semantic-Wireless-Transmission"><a href="#Communication-Efficient-Framework-for-Distributed-Image-Semantic-Wireless-Transmission" class="headerlink" title="Communication-Efficient Framework for Distributed Image Semantic Wireless Transmission"></a>Communication-Efficient Framework for Distributed Image Semantic Wireless Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03713">http://arxiv.org/abs/2308.03713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingyan Xie, Yongpeng Wu, Yuxuan Shi, Derrick Wing Kwan Ng, Wenjun Zhang</li>
<li>for: 这篇论文主要targets the problem of communication-efficient distributed data transmission in Internet-of-Things (IoT) scenarios with multiple devices, and proposes a federated learning-based semantic communication (FLSC) framework for multi-task distributed image transmission.</li>
<li>methods: The proposed FLSC framework uses a hierarchical vision transformer (HVT)-based extractor and a task-adaptive translator for coarse-to-fine semantic extraction and meaning translation, and a channel state information-based multiple-input multiple-output transmission module to combat channel fading and noise.</li>
<li>results: Simulation results show that the coarse semantic information can deal with a range of image-level tasks, and the FLSC framework outperforms traditional schemes in low signal-to-noise ratio and channel bandwidth ratio regimes, with a gain of around 10 peak signal-to-noise ratio in the 3 dB channel condition.<details>
<summary>Abstract</summary>
Multi-node communication, which refers to the interaction among multiple devices, has attracted lots of attention in many Internet-of-Things (IoT) scenarios. However, its huge amounts of data flows and inflexibility for task extension have triggered the urgent requirement of communication-efficient distributed data transmission frameworks. In this paper, inspired by the great superiorities on bandwidth reduction and task adaptation of semantic communications, we propose a federated learning-based semantic communication (FLSC) framework for multi-task distributed image transmission with IoT devices. Federated learning enables the design of independent semantic communication link of each user while further improves the semantic extraction and task performance through global aggregation. Each link in FLSC is composed of a hierarchical vision transformer (HVT)-based extractor and a task-adaptive translator for coarse-to-fine semantic extraction and meaning translation according to specific tasks. In order to extend the FLSC into more realistic conditions, we design a channel state information-based multiple-input multiple-output transmission module to combat channel fading and noise. Simulation results show that the coarse semantic information can deal with a range of image-level tasks. Moreover, especially in low signal-to-noise ratio and channel bandwidth ratio regimes, FLSC evidently outperforms the traditional scheme, e.g. about 10 peak signal-to-noise ratio gain in the 3 dB channel condition.
</details>
<details>
<summary>摘要</summary>
多节点通信，指的是多个设备之间的交互，在互联网关键设备（IoT）场景中吸引了很多关注。然而，它的巨量数据流和任务扩展不灵活性已经触发了高效通信分布数据传输框架的紧迫需求。本文提出了基于联合学习的 semantic communication（FLSC）框架，用于多任务分布式图像传输。联合学习使得每个用户独立的semantic communication链接可以进一步提高semantic抽取和任务性能通过全球汇总。每个FLSC链接都包括层次视transformer（HVT）基于抽取器和任务适应译码器，用于层次semantic抽取和意义翻译。为了扩展FLSC到更加实际的情况，我们设计了基于 kanal 状态信息的多输入多输出传输模块，以对抗通道抖动和噪声。实验结果显示，粗粒度semantic信息可以处理多种图像级任务。此外，特别在低信号噪声比和通道宽bandwidth比例 regime下，FLSC明显超过传统方案，例如在3dB通道条件下约10带宽比例的增强。
</details></li>
</ul>
<hr>
<h2 id="Scaling-may-be-all-you-need-for-achieving-human-level-object-recognition-capacity-with-human-like-visual-experience"><a href="#Scaling-may-be-all-you-need-for-achieving-human-level-object-recognition-capacity-with-human-like-visual-experience" class="headerlink" title="Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience"></a>Scaling may be all you need for achieving human-level object recognition capacity with human-like visual experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03712">http://arxiv.org/abs/2308.03712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eminorhan/humanlike-vits">https://github.com/eminorhan/humanlike-vits</a></li>
<li>paper_authors: A. Emin Orhan</li>
<li>for: 这 paper  investigate whether current self-supervised learning methods can reach human-level visual object recognition capabilities with the same type and amount of visual experience as humans.</li>
<li>methods: 这 paper 使用 vision transformers 和 masked autoencoders (MAEs) 进行自然语言处理，并进行了数据大小、模型大小和图像分辨率的同时缩放实验。</li>
<li>results: 结果表明，可以通过同时缩放数据大小、模型大小和图像分辨率来达到人类水平的 объекRecognition 能力，而无需使用专门的 inductive biases。例如，一个 2.5B 参数的 ViT 模型，通过使用 20K 小时的人类样式视频数据和 952x952 像素的空间分辨率，应该可以达到 ImageNet 上的人类水平准确率。<details>
<summary>Abstract</summary>
This paper asks whether current self-supervised learning methods, if sufficiently scaled up, would be able to reach human-level visual object recognition capabilities with the same type and amount of visual experience humans learn from. Previous work on this question only considered the scaling of data size. Here, we consider the simultaneous scaling of data size, model size, and image resolution. We perform a scaling experiment with vision transformers up to 633M parameters in size (ViT-H/14) trained with up to 5K hours of human-like video data (long, continuous, mostly egocentric videos) with image resolutions of up to 476x476 pixels. The efficiency of masked autoencoders (MAEs) as a self-supervised learning algorithm makes it possible to run this scaling experiment on an unassuming academic budget. We find that it is feasible to reach human-level object recognition capacity at sub-human scales of model size, data size, and image size, if these factors are scaled up simultaneously. To give a concrete example, we estimate that a 2.5B parameter ViT model trained with 20K hours (2.3 years) of human-like video data with a spatial resolution of 952x952 pixels should be able to reach roughly human-level accuracy on ImageNet. Human-level competence is thus achievable for a fundamental perceptual capability from human-like perceptual experience (human-like in both amount and type) with extremely generic learning algorithms and architectures and without any substantive inductive biases.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeRisk-An-Effective-Deep-Learning-Framework-for-Credit-Risk-Prediction-over-Real-World-Financial-Data"><a href="#DeRisk-An-Effective-Deep-Learning-Framework-for-Credit-Risk-Prediction-over-Real-World-Financial-Data" class="headerlink" title="DeRisk: An Effective Deep Learning Framework for Credit Risk Prediction over Real-World Financial Data"></a>DeRisk: An Effective Deep Learning Framework for Credit Risk Prediction over Real-World Financial Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03704">http://arxiv.org/abs/2308.03704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yancheng Liang, Jiajie Zhang, Hui Li, Xiaochen Liu, Yi Hu, Yong Wu, Jinyao Zhang, Yongyan Liu, Yi Wu</li>
<li>for: 预测信用风险（credit risk prediction）在真实世界金融数据上。</li>
<li>methods: 提出了一种深度学习风险预测框架（DeRisk），该框架可以在真实世界金融数据上高效地预测信用风险。</li>
<li>results: DeRisk 已经在实际生产环境中超过了统计学学习方法的表现，并进行了广泛的缺失研究以证明模型的成功因素。<details>
<summary>Abstract</summary>
Despite the tremendous advances achieved over the past years by deep learning techniques, the latest risk prediction models for industrial applications still rely on highly handtuned stage-wised statistical learning tools, such as gradient boosting and random forest methods. Different from images or languages, real-world financial data are high-dimensional, sparse, noisy and extremely imbalanced, which makes deep neural network models particularly challenging to train and fragile in practice. In this work, we propose DeRisk, an effective deep learning risk prediction framework for credit risk prediction on real-world financial data. DeRisk is the first deep risk prediction model that outperforms statistical learning approaches deployed in our company's production system. We also perform extensive ablation studies on our method to present the most critical factors for the empirical success of DeRisk.
</details>
<details>
<summary>摘要</summary>
尽管深度学习技术过去几年取得了很大的进步，现在最新的风险预测模型仍然基于高度手动调整的阶段性统计学学习工具，如梯度批量和随机森林方法。与图像或语言不同，实际世界金融数据具有高维、稀疏、噪音和极其不均衡的特点，这使得深度神经网络模型在实践中特别困难并脆弱。在这项工作中，我们提出了DeRisk，一种高效的深度学习风险预测框架，用于实际世界金融数据上的信用风险预测。DeRisk是我们公司生产系统中部署的统计学学习方法的首个深度风险预测模型，我们还进行了广泛的减少研究，以阐明DeRisk的成功的重要因素。
</details></li>
</ul>
<hr>
<h2 id="AgentBench-Evaluating-LLMs-as-Agents"><a href="#AgentBench-Evaluating-LLMs-as-Agents" class="headerlink" title="AgentBench: Evaluating LLMs as Agents"></a>AgentBench: Evaluating LLMs as Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03688">http://arxiv.org/abs/2308.03688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/agentbench">https://github.com/thudm/agentbench</a></li>
<li>paper_authors: Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang</li>
<li>for: 评估大型自然语言处理器（LLM）在实际世界中的智能和自主能力。</li>
<li>methods: 使用多维度演化的 benchmark 来评估 LLM 作为代理的判断和决策能力。</li>
<li>results: 测试了 25 个 LLM（包括 API 和开源模型），发现商业 LLM 表现出色，但是开源竞争对手的表现差异较大。Note: “LLM” stands for “Large Language Models” in English.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Almost-sure-convergence-of-iterates-and-multipliers-in-stochastic-sequential-quadratic-optimization"><a href="#Almost-sure-convergence-of-iterates-and-multipliers-in-stochastic-sequential-quadratic-optimization" class="headerlink" title="Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization"></a>Almost-sure convergence of iterates and multipliers in stochastic sequential quadratic optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03687">http://arxiv.org/abs/2308.03687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank E. Curtis, Xin Jiang, Qi Wang</li>
<li>for:  solves large-scale data-fitting problems subject to nonconvex constraints</li>
<li>methods:  stochastic-gradient methodology from the unconstrained setting</li>
<li>results:  new almost-sure convergence guarantees for the primal iterates, Lagrange multipliers, and stationarity measures<details>
<summary>Abstract</summary>
Stochastic sequential quadratic optimization (SQP) methods for solving continuous optimization problems with nonlinear equality constraints have attracted attention recently, such as for solving large-scale data-fitting problems subject to nonconvex constraints. However, for a recently proposed subclass of such methods that is built on the popular stochastic-gradient methodology from the unconstrained setting, convergence guarantees have been limited to the asymptotic convergence of the expected value of a stationarity measure to zero. This is in contrast to the unconstrained setting in which almost-sure convergence guarantees (of the gradient of the objective to zero) can be proved for stochastic-gradient-based methods. In this paper, new almost-sure convergence guarantees for the primal iterates, Lagrange multipliers, and stationarity measures generated by a stochastic SQP algorithm in this subclass of methods are proved. It is shown that the error in the Lagrange multipliers can be bounded by the distance of the primal iterate to a primal stationary point plus the error in the latest stochastic gradient estimate. It is further shown that, subject to certain assumptions, this latter error can be made to vanish by employing a running average of the Lagrange multipliers that are computed during the run of the algorithm. The results of numerical experiments are provided to demonstrate the proved theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
This paper presents new almost-sure convergence guarantees for the primal iterates, Lagrange multipliers, and stationarity measures generated by a stochastic SQP algorithm in this subclass of methods. The error in the Lagrange multipliers can be bounded by the distance of the primal iterate to a primal stationary point plus the error in the latest stochastic gradient estimate. Furthermore, it is shown that this latter error can be made to vanish by employing a running average of the Lagrange multipliers computed during the run of the algorithm, subject to certain assumptions.Numerical experiments are provided to demonstrate the proved theoretical guarantees. These results demonstrate the effectiveness of the proposed method in solving continuous optimization problems with nonlinear equality constraints.
</details></li>
</ul>
<hr>
<h2 id="Linear-Convergence-Bounds-for-Diffusion-Models-via-Stochastic-Localization"><a href="#Linear-Convergence-Bounds-for-Diffusion-Models-via-Stochastic-Localization" class="headerlink" title="Linear Convergence Bounds for Diffusion Models via Stochastic Localization"></a>Linear Convergence Bounds for Diffusion Models via Stochastic Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03686">http://arxiv.org/abs/2308.03686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joe Benton, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis</li>
<li>for: 这个论文的目的是提供高维数据分布采样的精度模型，并且提供了对这些模型的证明。</li>
<li>methods: 这个论文使用了扩散模型，并且使用了$L^2$-精度分布估计器。</li>
<li>results: 这个论文提供了高维数据分布采样的 linear  bounds，即在数据维度上具有 $\tilde O(\frac{d \log^2(1&#x2F;\delta)}{\varepsilon^2})$ 步骤可以将数据分布 approximate 到 Within $\varepsilon^2$ 的 Kullback–Leibler 差分。<details>
<summary>Abstract</summary>
Diffusion models are a powerful method for generating approximate samples from high-dimensional data distributions. Several recent results have provided polynomial bounds on the convergence rate of such models, assuming $L^2$-accurate score estimators. However, up until now the best known such bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde O(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary data distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in Kullback--Leibler divergence. Our proof builds on the Girsanov-based methods of previous works. We introduce a refined treatment of the error arising from the discretization of the reverse SDE, which is based on tools from stochastic localization.
</details>
<details>
<summary>摘要</summary>
传播模型是一种强大的方法来生成高维数据分布的近似样本。一些最近的结果提供了多项关于传播模型的数值精度，假设$L^2$-精确的分配 estimator。然而，直到现在，最好的知识是 either 超过线性的数据维度或需要强大的平滑假设。我们提供了首个线性于数据维度（以logarithmic factor）的传播模型数值精度 bound，只需要对数据分布的第二 moment finite。我们证明了传播模型可以在 $\mathbb{R}^d$ 上静态数据分布上静态数据分布 corrupted with Gaussian noise of variance $\delta$ 到 Within $\varepsilon^2$ 库拉克-莱比勒分布 divergence 以内 $\tilde O\left(\frac{d \log^2(1/\delta)}{\varepsilon^2}\right)$ 步骤。我们的证明基于先前的 Girsanov-based 方法，并 introducing 对 reverse SDE 的精度误差的更加精确的处理，基于 Stochastic localization 的工具。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/08/cs.LG_2023_08_08/" data-id="cllt6z4it003aq388dnzga170" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/22/eess.IV_2023_08_22/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-22 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/08/cs.SD_2023_08_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-08 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
