
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-08 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Blur aware metric depth estimation with multi-focus plenoptic cameras paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04252 repo_url: https:&#x2F;&#x2F;github.com&#x2F;comsee-research&#x2F;blade paper_authors: Mathieu Labussière, C">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-08 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/08/eess.IV_2023_08_08/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Blur aware metric depth estimation with multi-focus plenoptic cameras paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04252 repo_url: https:&#x2F;&#x2F;github.com&#x2F;comsee-research&#x2F;blade paper_authors: Mathieu Labussière, C">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-07T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:27.024Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_08" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/08/eess.IV_2023_08_08/" class="article-date">
  <time datetime="2023-08-07T16:00:00.000Z" itemprop="datePublished">2023-08-08</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-08 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Blur-aware-metric-depth-estimation-with-multi-focus-plenoptic-cameras"><a href="#Blur-aware-metric-depth-estimation-with-multi-focus-plenoptic-cameras" class="headerlink" title="Blur aware metric depth estimation with multi-focus plenoptic cameras"></a>Blur aware metric depth estimation with multi-focus plenoptic cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04252">http://arxiv.org/abs/2308.04252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comsee-research/blade">https://github.com/comsee-research/blade</a></li>
<li>paper_authors: Mathieu Labussière, Céline Teulière, Omar Ait-Aider</li>
<li>for: 这篇论文的目的是提出一种基于raw图像的多Focus plenoptic摄像机的metric depth estimation算法，以提高对不同距离物体的depth estimation。</li>
<li>methods: 该算法使用了 correspondence和defocus规则，并利用了模糊信息来提高depth estimation。具体来说， authors derivied一个 inverse projection模型，包括了defocus模糊，并提出了一种Calibration方法来实现精确的depth scaling。</li>
<li>results: 论文的实验结果表明，通过引入模糊信息，可以提高depth estimation的准确性。 authors还进行了实验，并证明了该算法在实际场景中的效果。<details>
<summary>Abstract</summary>
While a traditional camera only captures one point of view of a scene, a plenoptic or light-field camera, is able to capture spatial and angular information in a single snapshot, enabling depth estimation from a single acquisition. In this paper, we present a new metric depth estimation algorithm using only raw images from a multi-focus plenoptic camera. The proposed approach is especially suited for the multi-focus configuration where several micro-lenses with different focal lengths are used. The main goal of our blur aware depth estimation (BLADE) approach is to improve disparity estimation for defocus stereo images by integrating both correspondence and defocus cues. We thus leverage blur information where it was previously considered a drawback. We explicitly derive an inverse projection model including the defocus blur providing depth estimates up to a scale factor. A method to calibrate the inverse model is then proposed. We thus take into account depth scaling to achieve precise and accurate metric depth estimates. Our results show that introducing defocus cues improves the depth estimation. We demonstrate the effectiveness of our framework and depth scaling calibration on relative depth estimation setups and on real-world 3D complex scenes with ground truth acquired with a 3D lidar scanner.
</details>
<details>
<summary>摘要</summary>
while a traditional camera only captures one point of view of a scene, a plenoptic or light-field camera can capture spatial and angular information in a single snapshot, enabling depth estimation from a single acquisition. in this paper, we present a new metric depth estimation algorithm using only raw images from a multi-focus plenoptic camera. the proposed approach is especially suited for the multi-focus configuration where several micro-lenses with different focal lengths are used. the main goal of our blur-aware depth estimation (blade) approach is to improve disparity estimation for defocus stereo images by integrating both correspondence and defocus cues. we thus leverage blur information where it was previously considered a drawback. we explicitly derive an inverse projection model including the defocus blur providing depth estimates up to a scale factor. a method to calibrate the inverse model is then proposed. we thus take into account depth scaling to achieve precise and accurate metric depth estimates. our results show that introducing defocus cues improves the depth estimation. we demonstrate the effectiveness of our framework and depth scaling calibration on relative depth estimation setups and on real-world 3d complex scenes with ground truth acquired with a 3d lidar scanner.
</details></li>
</ul>
<hr>
<h2 id="Under-Display-Camera-Image-Restoration-with-Scattering-Effect"><a href="#Under-Display-Camera-Image-Restoration-with-Scattering-Effect" class="headerlink" title="Under-Display Camera Image Restoration with Scattering Effect"></a>Under-Display Camera Image Restoration with Scattering Effect</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04163">http://arxiv.org/abs/2308.04163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/namecantbenull/srudc">https://github.com/namecantbenull/srudc</a></li>
<li>paper_authors: Binbin Song, Xiangyu Chen, Shuning Xu, Jiantao Zhou</li>
<li>for: 提供全屏视频经验而不受缺口或孔洞的干扰，但是半透明显示器引入了严重的图像干扰。</li>
<li>methods: 使用物理干扰模型来模拟显示器的散射效应，并改进图像synthesis的形成管道来构建真实的UDC数据集。</li>
<li>results: 提出了一种基于自注意力的两极分支网络，其中散射分支利用通道级自注意力来估算散射效应的参数，而图像分支利用CNN来恢复清晰的场景图像。对实验和 sinthezied 数据进行了广泛的测试，并证明了该方法在UDC图像恢复方面的优越性。<details>
<summary>Abstract</summary>
The under-display camera (UDC) provides consumers with a full-screen visual experience without any obstruction due to notches or punched holes. However, the semi-transparent nature of the display inevitably introduces the severe degradation into UDC images. In this work, we address the UDC image restoration problem with the specific consideration of the scattering effect caused by the display. We explicitly model the scattering effect by treating the display as a piece of homogeneous scattering medium. With the physical model of the scattering effect, we improve the image formation pipeline for the image synthesis to construct a realistic UDC dataset with ground truths. To suppress the scattering effect for the eventual UDC image recovery, a two-branch restoration network is designed. More specifically, the scattering branch leverages global modeling capabilities of the channel-wise self-attention to estimate parameters of the scattering effect from degraded images. While the image branch exploits the local representation advantage of CNN to recover clear scenes, implicitly guided by the scattering branch. Extensive experiments are conducted on both real-world and synthesized data, demonstrating the superiority of the proposed method over the state-of-the-art UDC restoration techniques. The source code and dataset are available at \url{https://github.com/NamecantbeNULL/SRUDC}.
</details>
<details>
<summary>摘要</summary>
“display under-camera（UDC）允许消耗者欣赏全屏视觉体验，不受萤幕上的不透明或孔径所阻碍。然而，半透明的萤幕将导致UDC图像受到严重的损坏。在这个工作中，我们解决UDC图像修复问题，特别是考虑到类推效应。我们明确地模型类推效应，将类推效应视为一个 homogeneous 散射媒体。使用物理模型，我们改善图像形成管道，以建立一个实际的 UDC 数据集，并提供真实的参考数据。为了抑制类推效应，我们设计了两条分支网络。更 specifically，类推分支利用通道wise self-attention 的全局模型能力，从受损图像中估计类推效应的参数。另一方面，图像分支利用 CNN 的地方表现优势，复原清晰的景象，协力地 guid 由类推分支。我们对真实和 sinthe 的数据进行了广泛的实验，展示了我们的方法与现有的 UDC 修复技术相比，具有superiority。请参考 \url{https://github.com/NamecantbeNULL/SRUDC} 以下载取源代码和数据。”
</details></li>
</ul>
<hr>
<h2 id="Towards-Top-Down-Stereoscopic-Image-Quality-Assessment-via-Stereo-Attention"><a href="#Towards-Top-Down-Stereoscopic-Image-Quality-Assessment-via-Stereo-Attention" class="headerlink" title="Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention"></a>Towards Top-Down Stereoscopic Image Quality Assessment via Stereo Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04156">http://arxiv.org/abs/2308.04156</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanning-zhang/satnet">https://github.com/fanning-zhang/satnet</a></li>
<li>paper_authors: Huilin Zhang, Sumei Li, Yongli Chang</li>
<li>for: This paper proposes a novel network for stereoscopic image quality assessment (SIQA) that utilizes a top-down approach to guide the quality assessment process.</li>
<li>methods: The proposed method employs a stereo attention mechanism that fuses high-level binocular signals with low-level monocular signals, and utilizes an energy coefficient to adaptively tune the magnitude of binocular responses. The method also utilizes a dual-pooling strategy to extract the most discriminative quality information from the two branches of monocular features.</li>
<li>results: Experimental results show that the proposed top-down method outperforms existing bottom-up methods in simulating the property of visual perception and advancing the state-of-the-art in the SIQA field.<details>
<summary>Abstract</summary>
Stereoscopic image quality assessment (SIQA) plays a crucial role in evaluating and improving the visual experience of 3D content. Existing binocular properties and attention-based methods for SIQA have achieved promising performance. However, these bottom-up approaches are inadequate in exploiting the inherent characteristics of the human visual system (HVS). This paper presents a novel network for SIQA via stereo attention, employing a top-down perspective to guide the quality assessment process. Our proposed method realizes the guidance from high-level binocular signals down to low-level monocular signals, while the binocular and monocular information can be calibrated progressively throughout the processing pipeline. We design a generalized Stereo AttenTion (SAT) block to implement the top-down philosophy in stereo perception. This block utilizes the fusion-generated attention map as a high-level binocular modulator, influencing the representation of two low-level monocular features. Additionally, we introduce an Energy Coefficient (EC) to account for recent findings indicating that binocular responses in the primate primary visual cortex are less than the sum of monocular responses. The adaptive EC can tune the magnitude of binocular response flexibly, thus enhancing the formation of robust binocular features within our framework. To extract the most discriminative quality information from the summation and subtraction of the two branches of monocular features, we utilize a dual-pooling strategy that applies min-pooling and max-pooling operations to the respective branches. Experimental results highlight the superiority of our top-down method in simulating the property of visual perception and advancing the state-of-the-art in the SIQA field. The code of this work is available at https://github.com/Fanning-Zhang/SATNet.
</details>
<details>
<summary>摘要</summary>
三维内容的视觉体验评估（SIQA）在评估和改进三维内容的视觉体验方面发挥关键性的作用。现有的异常性和注意力基于的方法已经实现了有前途的表现。然而，这些底层方法无法充分利用人类视觉系统（HVS）的内在特征。本文提出了一种新的网络 для SIQA，通过双目注意力来引导评估过程。我们的提议方法可以从高级双目信号下降到低级单目信号，而双目和单目信息可以在处理管道中进行满足进行进度性规整。我们设计了一种通用的双目注意力块（SAT）来实现上述哲学。这个块使用生成的注意力地图作为高级双目调制器，影响两个低级单目特征表示。此外，我们引入了能量系数（EC），以考虑实验发现，双目响应在人类初级视觉层中小于单目响应的现象。可以根据实际情况灵活调整EC的大小，从而提高在我们框架中形成的可靠双目特征。为了从两个分支中提取最有价值的质量信息，我们采用了双pooling策略，将各个分支的最小和最大池化操作应用于相应的分支。实验结果表明，我们的底层方法在模拟视觉启发和提高SIQA领域的状态而且取得了更好的表现。代码可以在https://github.com/Fanning-Zhang/SATNet上获取。
</details></li>
</ul>
<hr>
<h2 id="Physics-driven-universal-twin-image-removal-network-for-digital-in-line-holographic-microscopy"><a href="#Physics-driven-universal-twin-image-removal-network-for-digital-in-line-holographic-microscopy" class="headerlink" title="Physics-driven universal twin-image removal network for digital in-line holographic microscopy"></a>Physics-driven universal twin-image removal network for digital in-line holographic microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04471">http://arxiv.org/abs/2308.04471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Rogalski, Piotr Arcab, Luiza Stanaszek, Vicente Micó, Chao Zuo, Maciej Trusiak</li>
<li>for: 这项研究旨在开发一种能够高效、成本低的数字探针式干涉 Microscopy（DIHM）中的量子阶段成像技术，以便研究细胞运动、迁徙和生物微流体动力学。</li>
<li>methods: 该研究使用深度学习解决方案UTIRnet，可以快速、稳定、universally applicable地纹理双像干涉，并且通过数字生成的数据进行训练。</li>
<li>results: 实验证明，UTIRnet可以快速、稳定地抑制双像干涉，并且保持输入干涉图像的一致性，从而提高了计算量相对于传统深度学习方法的可靠性。此外，UTIRnet在live neural glial cell culture migration感知中得到了实验证明。<details>
<summary>Abstract</summary>
Digital in-line holographic microscopy (DIHM) enables efficient and cost-effective computational quantitative phase imaging with a large field of view, making it valuable for studying cell motility, migration, and bio-microfluidics. However, the quality of DIHM reconstructions is compromised by twin-image noise, posing a significant challenge. Conventional methods for mitigating this noise involve complex hardware setups or time-consuming algorithms with often limited effectiveness. In this work, we propose UTIRnet, a deep learning solution for fast, robust, and universally applicable twin-image suppression, trained exclusively on numerically generated datasets. The availability of open-source UTIRnet codes facilitates its implementation in various DIHM systems without the need for extensive experimental training data. Notably, our network ensures the consistency of reconstruction results with input holograms, imparting a physics-based foundation and enhancing reliability compared to conventional deep learning approaches. Experimental verification was conducted among others on live neural glial cell culture migration sensing, which is crucial for neurodegenerative disease research.
</details>
<details>
<summary>摘要</summary>
数字内线投射微镜（DIHM）可提供高效且成本下降的计算量相图像，大幅提高了生物学研究中cell motility、迁徙和生物微流体等领域的研究价值。然而，DIHM重建的质量受到双像噪声的限制，这成为一大挑战。传统的方法对这种噪声进行缓解通常包括复杂的硬件设置或时间消耗的算法，效果不一定。在这种工作中，我们提出了UTIRnet，一种深度学习解决方案，能够快速、稳定、universally aplicable的双像抑制。我们的网络具有数字生成的数据集进行培训，不需要大量的实验室训练数据。特别是，我们的网络保证重建结果与输入投射图像之间的一致性，从而增强了physics-based的基础和可靠性，与传统的深度学习方法相比。实验证明包括live neural glial cell culture migration感知等，这些研究对 neuroscience disease 有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Single-shot-experimental-numerical-twin-image-removal-in-lensless-digital-holographic-microscopy"><a href="#Single-shot-experimental-numerical-twin-image-removal-in-lensless-digital-holographic-microscopy" class="headerlink" title="Single-shot experimental-numerical twin-image removal in lensless digital holographic microscopy"></a>Single-shot experimental-numerical twin-image removal in lensless digital holographic microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04131">http://arxiv.org/abs/2308.04131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piotr Arcab, Mikolaj Rogalski, Maciej Trusiak</li>
<li>for: 这篇论文旨在提出一种新的单shot实验数字折射镜微scopic技术，用于解决折射镜微scopic图像中的双像问题。</li>
<li>methods: 该技术基于两个源的偏心折射agram记录，使用简单的纤维分 splitting器实现。此外，该技术还提出了一种专门为获取的HOLOGRAMS数据进行 phase retrieval numerical algorithm，以提供无双像的重建。</li>
<li>results: 作者通过对频率测试目标和唾液细胞样本进行质量和量化验证，证明了该技术可以提供低成本、外围实验室的LDHM成像，并且可以提高图像的精度。这些结果开创了新的可靠性和生物医学成像应用领域，特别是在成本效益和可搬运性是关键的场景下。<details>
<summary>Abstract</summary>
Lensless digital holographic microscopy (LDHM) offers very large field-of-view label-free imaging crucial, e.g., in high-throughput particle tracking and biomedical examination of cells and tissues. Compact layouts promote point-of-case and out-of-laboratory applications. The LDHM, based on the Gabor in-line holographic principle, is inherently spoiled by the twin-image effect, which complicates the quantitative analysis of reconstructed phase and amplitude maps. Popular family of solutions consists of numerical methods, which tend to minimize twin-image upon iterative process based on data redundancy. Additional hologram recordings are needed, and final results heavily depend on the algorithmic parameters, however. In this contribution we present a novel single-shot experimental-numerical twin-image removal technique for LDHM. It leverages two-source off-axis hologram recording deploying simple fiber splitter. Additionally, we introduce a novel phase retrieval numerical algorithm specifically tailored to the acquired holograms, that provides twin-image-free reconstruction without compromising the resolution. We quantitatively and qualitatively verify proposed method employing phase test target and cheek cells biosample. The results demonstrate that the proposed technique enables low-cost, out-of-laboratory LDHM imaging with enhanced precision, achieved through the elimination of twin-image errors. This advancement opens new avenues for more accurate technical and biomedical imaging applications using LDHM, particularly in scenarios where cost-effective and portable imaging solutions are desired.
</details>
<details>
<summary>摘要</summary>
凡�жен数字投射icroscopy (LDHM) 提供了非常大的视场label-free 图像，对于高通过率粒子跟踪和生物医学Cells和组织的检查非常重要。 紧凑的设计使得点检测和出 laboratory 应用更加容易。基于Gabor直线投射原理的 LDHM 由于双像效应而受到质量分析重建phasemap和ampliute map中的干扰。通过数字方法来减少双像效应，但需要多个捕获图像，并且算法参数会影响最终结果。在这篇论文中，我们提出了一种新的单shot实验numerical twin-image removal技术，利用两个偏移 angle 的折分纤维Splitter来记录两个源的偏移 angle 折分图像。此外，我们还提出了一种专门为获取的ho lograms而设计的phaserecovery数字算法，可以在无需多个捕获图像的情况下提供无双像效应的重建结果，而且不会减少分辨率。我们使用phasetest target和唾液细胞样本进行量化和质量检测，结果表明，我们的方法可以提供低成本、出 laboratory的LDHM映像，并且提高了精度。这一进展打开了更多的技术和生物医学应用的可能性，特别是在成本效益和可搬式设备的情况下。
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Electric-Load-Monitoring-Approach-Based-on-Current-Feature-Visualization-for-Smart-Energy-Management"><a href="#Non-Intrusive-Electric-Load-Monitoring-Approach-Based-on-Current-Feature-Visualization-for-Smart-Energy-Management" class="headerlink" title="Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management"></a>Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11627">http://arxiv.org/abs/2308.11627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Xu, Dengfeng Liu, Liangtao Huang, Zhiquan Lin, Tiesong Zhao, Sam Kwong</li>
<li>for: 这篇论文的目的是为智能城市的电力系统进行经济可效的能源管理。特别是监控和分析所有用户的电力负载。</li>
<li>methods: 这篇论文使用了人工智能的受欢迎计算机视觉技术，设计了一种非侵入式负载监控方法。首先，使用信号变换（包括波лет变换和离散傅立叶变换）和gramianangular场（GAF）方法将一维现在信号映射到二维色彩特征图像。其次，提出了使用U型深度神经网络，具有多尺度特征提取和注意机制，从色彩特征图像中识别所有的电力负载。</li>
<li>results: 实验结果显示，该方法在公共和私人数据集上均 achieves superior performance，并支持大规模互联网的非侵入式监控。<details>
<summary>Abstract</summary>
The state-of-the-art smart city has been calling for an economic but efficient energy management over large-scale network, especially for the electric power system. It is a critical issue to monitor, analyze and control electric loads of all users in system. In this paper, we employ the popular computer vision techniques of AI to design a non-invasive load monitoring method for smart electric energy management. First of all, we utilize both signal transforms (including wavelet transform and discrete Fourier transform) and Gramian Angular Field (GAF) methods to map one-dimensional current signals onto two-dimensional color feature images. Second, we propose to recognize all electric loads from color feature images using a U-shape deep neural network with multi-scale feature extraction and attention mechanism. Third, we design our method as a cloud-based, non-invasive monitoring of all users, thereby saving energy cost during electric power system control. Experimental results on both public and our private datasets have demonstrated our method achieves superior performances than its peers, and thus supports efficient energy management over large-scale Internet of Things (IoT).
</details>
<details>
<summary>摘要</summary>
现代智能城市需要一种经济高效的能源管理方法，特别是电力系统。监测、分析和控制所有用户的电压信号是一个关键问题。在这篇论文中，我们使用了人工智能的流行计算机视觉技术，设计了一种不侵入式的负荷监测方法。首先，我们使用了声波变换（包括wavelet transform和Discrete Fourier Transform）和 Gramian Angular Field（GAF）方法将一维电流信号映射到二维颜色特征图像上。其次，我们提议使用U型深度神经网络，并提取多个尺度的特征和注意机制来识别所有的电荷。最后，我们设计了一种云端的、不侵入式的监测方法，从而在电力系统控制中节省能源成本。实验结果表明，我们的方法在公共和私人数据集上达到了比其他方法更高的性能，因此支持了大规模互联网智能（IoT）中的高效能源管理。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Semi-Supervised-Detection-in-Lung-Ultrasound-Videos"><a href="#Weakly-Semi-Supervised-Detection-in-Lung-Ultrasound-Videos" class="headerlink" title="Weakly Semi-Supervised Detection in Lung Ultrasound Videos"></a>Weakly Semi-Supervised Detection in Lung Ultrasound Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04463">http://arxiv.org/abs/2308.04463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahong Ouyang, Li Chen, Gary Y. Li, Naveen Balaraju, Shubham Patil, Courosh Mehanian, Sourabh Kulhare, Rachel Millin, Kenton W. Gregory, Cynthia R. Gregory, Meihua Zhu, David O. Kessler, Laurie Malia, Almaz Dessie, Joni Rabiner, Di Coneybeare, Bo Shopsin, Andrew Hersh, Cristian Madar, Jeffrey Shupp, Laura S. Johnson, Jacob Avila, Kristin Dwyer, Peter Weimersheimer, Balasundar Raju, Jochen Kruecker, Alvin Chen</li>
<li>for: 医学视频数据上进行全supervised对象检测模型训练中需要临床专家进行 Frame-by-frame 注解 boundling box.</li>
<li>methods: 我们提出了一种基于弱级别 labels 的方法，将个体检测预测结果聚合到视频级别预测中，并通过视频级别损失来提供额外约束。我们还介绍了一些改进 pseudo-labels 的方法，以及适应性调整知识传递Student 和 teacher 网络之间的调整方案。</li>
<li>results: 我们在医学超声视频中检测肺聚集（如 COVID-19 肺炎）的方法中应用了这种方法，实验表明，我们的框架可以提高检测精度和鲁棒性，并提高数据和注解使用效率。<details>
<summary>Abstract</summary>
Frame-by-frame annotation of bounding boxes by clinical experts is often required to train fully supervised object detection models on medical video data. We propose a method for improving object detection in medical videos through weak supervision from video-level labels. More concretely, we aggregate individual detection predictions into video-level predictions and extend a teacher-student training strategy to provide additional supervision via a video-level loss. We also introduce improvements to the underlying teacher-student framework, including methods to improve the quality of pseudo-labels based on weak supervision and adaptive schemes to optimize knowledge transfer between the student and teacher networks. We apply this approach to the clinically important task of detecting lung consolidations (seen in respiratory infections such as COVID-19 pneumonia) in medical ultrasound videos. Experiments reveal that our framework improves detection accuracy and robustness compared to baseline semi-supervised models, and improves efficiency in data and annotation usage.
</details>
<details>
<summary>摘要</summary>
<SYS>医学视频数据上进行完全监督物体检测模型训练常需要 клиниче专家进行框架帧标注。我们提出一种基于弱监督的方法，通过视频级别标签来改进医学视频中的物体检测。更具体来说，我们将个体检测预测结果聚合成视频级别预测，并通过视频级别损失来提供额外监督。我们还介绍了改进 teacher-student 框架的方法，包括基于弱监督的pseudo标签质量改进和 adaptive 优化学习知识传递between teacher和学生网络。我们在识别尘埃散发（常见于呼吸道感染，如 COVID-19 肺炎）的医学超声视频中应用这种方法。实验表明，我们的框架可以提高检测精度和可靠性，并提高数据和标注的使用效率。</SYS>Here's the text with some minor adjustments to make it more natural in Simplified Chinese:<SYS>医学视频数据上训练完全监督物体检测模型时，常常需要 клиниче专家进行框架帧标注。我们提出一种基于弱监督的方法，通过视频级别标签来改进医学视频中的物体检测。更具体来说，我们将个体检测预测结果聚合成视频级别预测，并通过视频级别损失来提供额外监督。我们还介绍了改进 teacher-student 框架的方法，包括基于弱监督的pseudo标签质量改进和 adaptive 优化学习知识传递between teacher和学生网络。我们在识别尘埃散发（常见于呼吸道感染，如 COVID-19 肺炎）的医学超声视频中应用这种方法。实验表明，我们的框架可以提高检测精度和可靠性，并提高数据和标注的使用效率。</SYS>Please note that the translation is based on the original text and may not capture all the nuances and details of the original text.
</details></li>
</ul>
<hr>
<h2 id="DefCor-Net-Physics-Aware-Ultrasound-Deformation-Correction"><a href="#DefCor-Net-Physics-Aware-Ultrasound-Deformation-Correction" class="headerlink" title="DefCor-Net: Physics-Aware Ultrasound Deformation Correction"></a>DefCor-Net: Physics-Aware Ultrasound Deformation Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03865">http://arxiv.org/abs/2308.03865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karolinezhy/defcornet">https://github.com/karolinezhy/defcornet</a></li>
<li>paper_authors: Zhongliang Jiang, Yue Zhou, Dongliang Cao, Nassir Navab</li>
<li>for:  correction of deformed anatomical images in ultrasound (US) image acquisition</li>
<li>methods: multi-scale deep neural network (DefCor-Net) with biomedical knowledge and real-time estimation of pixel-wise tissue properties</li>
<li>results: significant improvement in accuracy of deformation correction (Dice Coefficient: from $14.3\pm20.9$ to $82.6\pm12.1$ when the force is $6N$)<details>
<summary>Abstract</summary>
The recovery of morphologically accurate anatomical images from deformed ones is challenging in ultrasound (US) image acquisition, but crucial to accurate and consistent diagnosis, particularly in the emerging field of computer-assisted diagnosis. This article presents a novel anatomy-aware deformation correction approach based on a coarse-to-fine, multi-scale deep neural network (DefCor-Net). To achieve pixel-wise performance, DefCor-Net incorporates biomedical knowledge by estimating pixel-wise stiffness online using a U-shaped feature extractor. The deformation field is then computed using polynomial regression by integrating the measured force applied by the US probe. Based on real-time estimation of pixel-by-pixel tissue properties, the learning-based approach enables the potential for anatomy-aware deformation correction. To demonstrate the effectiveness of the proposed DefCor-Net, images recorded at multiple locations on forearms and upper arms of six volunteers are used to train and validate DefCor-Net. The results demonstrate that DefCor-Net can significantly improve the accuracy of deformation correction to recover the original geometry (Dice Coefficient: from $14.3\pm20.9$ to $82.6\pm12.1$ when the force is $6N$).
</details>
<details>
<summary>摘要</summary>
医学影像中的形态准确度恢复问题在ultrasound（US）图像获取中是一个挑战，但是对医学诊断的准确性和一致性很重要，特别是在计算机助理诊断领域。这篇文章提出了一种基于多尺度深度神经网络（DefCor-Net）的新型形态意识恢复方法。为了实现像素级性能，DefCor-Net在核心网络中加入了生物医学知识，通过在U型特征提取器中线性估计每个像素的刚性来实现。然后，通过积分测量US probes应用的力场来计算扭formation场。基于实时测量像素级刚性特性，这种学习基于的方法具有可能性 для形态意识恢复。为了证明DefCor-Net的效果，文章使用了多个臂部和上臂部的六名志愿者记录的图像进行训练和验证。结果表明，DefCor-Net可以显著提高恢复原geometry的准确度（Dice Coefficient：从14.3±20.9到82.6±12.1，当力场为6N）。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/08/eess.IV_2023_08_08/" data-id="clm0t8e2o00eqv788ae739z7t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/08/eess.AS_2023_08_08/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-08-08 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/07/cs.LG_2023_08_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-07 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
