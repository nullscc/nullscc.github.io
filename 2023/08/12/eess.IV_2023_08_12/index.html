
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-12 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Semantic Communications with Explicit Semantic Base for Image Transmission paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06599 repo_url: None paper_authors: Yuan Zheng, Fengyu Wang, Wenjun Xu, Miao Pan, Ping Z">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-12 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/12/eess.IV_2023_08_12/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Semantic Communications with Explicit Semantic Base for Image Transmission paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06599 repo_url: None paper_authors: Yuan Zheng, Fengyu Wang, Wenjun Xu, Miao Pan, Ping Z">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-11T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:29.302Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/eess.IV_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-12 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission"><a href="#Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission" class="headerlink" title="Semantic Communications with Explicit Semantic Base for Image Transmission"></a>Semantic Communications with Explicit Semantic Base for Image Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06599">http://arxiv.org/abs/2308.06599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zheng, Fengyu Wang, Wenjun Xu, Miao Pan, Ping Zhang</li>
<li>for: 该 paper 的目的是提出一种基于协同知识 (Seb) 的 semantic image transmission 框架，以确保信息的含义在传输过程中得到正确的理解和传输。</li>
<li>methods: 该 paper 使用了 Seb 生成器和 Seb 基于图像编码&#x2F;解码器来表示图像，并使用 E2E 训练来优化核心组件。</li>
<li>results: 对比 state-of-art 方法，该 paper 在不同 SNR 下达到了 0.5-1.5 dB 的 PSNR 提升。<details>
<summary>Abstract</summary>
Semantic communications, aiming at ensuring the successful delivery of the meaning of information, are expected to be one of the potential techniques for the next generation communications. However, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to the communication intents is still immature. In this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. To represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. To further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. The key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. Extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details>
<details>
<summary>摘要</summary>
semantic communication, aiming at ensuring the successful delivery of information meaning, is expected to be one of the potential techniques for next-generation communications. however, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to communication intents is still immature. in this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. to represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. to further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. the key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details></li>
</ul>
<hr>
<h2 id="On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution"><a href="#On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution" class="headerlink" title="On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution"></a>On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06570">http://arxiv.org/abs/2308.06570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristian Fischer, Christian Herglotz, André Kaup</li>
<li>for: 提高4K数据的编码质量</li>
<li>methods: 使用Machine Learning基于单张超解析算法和下一代VVC编码器</li>
<li>results: 可以在低比特率场景下获得12%-18%的Bjontegaard delta rate提升，并且减少了压缩残留和扩散 artifacts。<details>
<summary>Abstract</summary>
Coding 4K data has become of vital interest in recent years, since the amount of 4K data is significantly increasing. We propose a coding chain with spatial down- and upscaling that combines the next-generation VVC codec with machine learning based single image super-resolution algorithms for 4K. The investigated coding chain, which spatially downscales the 4K data before coding, shows superior quality than the conventional VVC reference software for low bitrate scenarios. Throughout several tests, we find that up to 12 % and 18 % Bjontegaard delta rate gains can be achieved on average when coding 4K sequences with VVC and QP values above 34 and 42, respectively. Additionally, the investigated scenario with up- and downscaling helps to reduce the loss of details and compression artifacts, as it is shown in a visual example.
</details>
<details>
<summary>摘要</summary>
“ coding 4K 数据在近年变得非常重要，因为4K 数据量在增长。我们提出了一个 coding chain，其 combining 下一代 VVC 编码器和基于机器学习的单张图像超分辨算法，用于4K。我们调查的 coding chain，先将4K 数据进行空间下降scaling，然后编码，在低比特率场景下显示出超越传统 VVC 参考软件的质量。在多个测试中，我们发现，在 VVC 和 QP 值高于 34 和 42 时，可以获得12% 到 18% Bjontegaard delta Rate 增强。此外，我们发现，这种升降scaling 场景可以避免失 Details 和压缩残差的损失，如图例所示。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion"><a href="#Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion" class="headerlink" title="Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion"></a>Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06557">http://arxiv.org/abs/2308.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaibao Sun, Zhifeng Chen, Guangyu Dan, Qingfei Luo, Lirong Yan, Feng Liu, Xiaohong Joe Zhou</li>
<li>for: 提高echo-planar imaging（EPI）的图像质量和动态误差 corrections，使其适用于更多应用。</li>
<li>methods: 使用三维（3D）echo-shifted EPI BUDA（esEPI-BUDA）技术，通过生成两个EPI读取轨迹，以实现单shot acquiring两个blip-up和blip-down数据集。</li>
<li>results: 在phantom和人类大脑图像中， geometric distortions 得到了有效地 correction，并在人类大脑中测试了视觉活化Volume和其BOLD响应，与普通3D echo-planar图像相当。<details>
<summary>Abstract</summary>
Purpose: Echo-planar imaging (EPI) with blip-up/down acquisition (BUDA) can provide high-quality images with minimal distortions by using two readout trains with opposing phase-encoding gradients. Because of the need for two separate acquisitions, BUDA doubles the scan time and degrades the temporal resolution when compared to single-shot EPI, presenting a major challenge for many applications, particularly functional MRI (fMRI). This study aims at overcoming this challenge by developing an echo-shifted EPI BUDA (esEPI-BUDA) technique to acquire both blip-up and blip-down datasets in a single shot. Methods: A three-dimensional (3D) esEPI-BUDA pulse sequence was designed by using an echo-shifting strategy to produce two EPI readout trains. These readout trains produced a pair of k-space datasets whose k-space trajectories were interleaved with opposite phase-encoding gradient directions. The two k-space datasets were separately reconstructed using a 3D SENSE algorithm, from which time-resolved B0-field maps were derived using TOPUP in FSL and then input into a forward model of joint parallel imaging reconstruction to correct for geometric distortion. In addition, Hankel structured low-rank constraint was incorporated into the reconstruction framework to improve image quality by mitigating the phase errors between the two interleaved k-space datasets. Results: The 3D esEPI-BUDA technique was demonstrated in a phantom and an fMRI study on healthy human subjects. Geometric distortions were effectively corrected in both phantom and human brain images. In the fMRI study, the visual activation volumes and their BOLD responses were comparable to those from conventional 3D echo-planar images. Conclusion: The improved imaging efficiency and dynamic distortion correction capability afforded by 3D esEPI-BUDA are expected to benefit many EPI applications.
</details>
<details>
<summary>摘要</summary>
目的：使用双向磁场增强/减强获取（BUDA）技术可以提供高质量图像，但因为需要两次获取，BUDA将扫描时间双倍，降低时间分辨率，对许多应用程序（特别是功能磁共振成像（fMRI））提出了主要挑战。这项研究的目的是解决这一挑战，通过开发一种三维echo-shifted EPI BUDA（esEPI-BUDA）技术，在单击中获取两个磁场增强/减强数据集。方法：设计了一种三维 esEPI-BUDA脉冲序列，使用抽象阶段生成两个 EPI 读取轨迹。这两个读取轨迹生成了具有相反方向磁场增强/减强方向的两个 k-空间数据集。这两个 k-空间数据集分别使用三维 SENSE 算法重构，并从而生成了时间解析B0场图像。在FSL中使用 TOPUP 算法，将这些时间解析B0场图像输入到了一种 JOINT 平行成像重建模型中，以 corrected  geometric distortion。此外，还在重建框架中添加了具有束缚低维度的 Hankel 结构低级数据约束，以提高图像质量，减少了两个交错 k-空间数据集之间的频率错误。结果：在一个模拟器和一个人类大脑功能磁共振成像研究中，三维 esEPI-BUDA 技术得到了证明。在这些研究中，人类大脑功能磁共振图像中的形态扭曲都得到了有效地 corrections。在人类大脑功能磁共振图像中，可见功能区域的激发量和其 BOLD 响应与普通三维 EPI 图像具有相同的水平。结论：三维 esEPI-BUDA 技术的改进的扫描效率和动态扭曲纠正能力，预期将对许多 EPI 应用程序产生积极的影响。
</details></li>
</ul>
<hr>
<h2 id="The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow"><a href="#The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow" class="headerlink" title="The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow"></a>The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06485">http://arxiv.org/abs/2308.06485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Hu, Kit Ian Kou, Cuiming Zou, Dong Cheng</li>
<li>for: 该论文提出了色彩Clifford Hardy信号的想法，用于处理色彩图像。</li>
<li>methods: 该论文提出了五种方法来识别色彩图像的边缘，基于色彩Clifford Hardy信号的本地特征表示。</li>
<li>results: 该论文通过多种比较研究表明，提出的方法具有高效性和抗噪能力。例如，通过多尺度结构的色彩Clifford Hardy信号，方法可以抗衰减噪。此外，论文还提供了一种颜色动向检测方法，用于应用示例。<details>
<summary>Abstract</summary>
This paper introduces the idea of the color Clifford Hardy signal, which can be used to process color images. As a complex analytic function's high-dimensional analogue, the color Clifford Hardy signal inherits many desirable qualities of analyticity. A crucial tool for getting the color and structural data is the local feature representation of a color image in the color Clifford Hardy signal. By looking at the extended Cauchy-Riemann equations in the high-dimensional space, it is possible to see the connection between the different parts of the color Clifford Hardy signal. Based on the distinctive and important local amplitude and local phase generated by the color Clifford Hardy signal, we propose five methods to identify the edges of color images with relation to a certain color. To prove the superiority of the offered methodologies, numerous comparative studies employing image quality assessment criteria are used. Specifically by using the multi-scale structure of the color Clifford Hardy signal, the proposed approaches are resistant to a variety of noises. In addition, a color optical flow detection method with anti-noise ability is provided as an example of application.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection"><a href="#Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection" class="headerlink" title="Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection"></a>Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06481">http://arxiv.org/abs/2308.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Fernandez-Quilez, Linas Vidziunas, Ørjan Kløvfjell Thoresen, Ketil Oppedal, Svein Reidar Kjosavik, Trygve Eftestøl</li>
<li>for: 这篇论文是为了检测抑制癌病变的方法。</li>
<li>methods: 这篇论文使用了多流程方法，以便利用不同的T2w方向来提高肿瘤检测的性能。</li>
<li>results: 这篇论文的结果显示，使用多流程方法可以提高肿瘤检测的精度，比单向方法更高（AUC&#x3D;73.1 vs 82.3）。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) approaches based on supervised learning paradigms require large amounts of annotated data that are rarely available in the medical domain. Unsupervised Out-of-distribution (OOD) detection is an alternative that requires less annotated data. Further, OOD applications exploit the class skewness commonly present in medical data. Magnetic resonance imaging (MRI) has proven to be useful for prostate cancer (PCa) diagnosis and management, but current DL approaches rely on T2w axial MRI, which suffers from low out-of-plane resolution. We propose a multi-stream approach to accommodate different T2w directions to improve the performance of PCa lesion detection in an OOD approach. We evaluate our approach on a publicly available data-set, obtaining better detection results in terms of AUC when compared to a single direction approach (73.1 vs 82.3). Our results show the potential of OOD approaches for PCa lesion detection based on MRI.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）方法基于supervised learning paradigms需要大量的标注数据，而医疗领域中这些数据很少。不需要标注数据的Out-of-distribution（OOD）检测是一个 alternativa，并且可以利用医疗数据的类偏好。核磁共振成像（MRI）已经被证明是肠癌（PCa）诊断和管理的有用工具，但现有的DL方法仅仅使用T2w极向MRI，这种MRI受到外向分辨率的限制。我们提议一种多流处理方法，以便处理不同的T2w方向，以提高PCa患部检测的性能。我们对公共可用数据集进行了评估，并 obtient了与单向方法相比的更好的检测结果（AUC=73.1 vs AUC=82.3）。我们的结果表明，基于MRI的PCa患部检测可以通过OOD方法实现更高的检测精度。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach"><a href="#Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach" class="headerlink" title="Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach"></a>Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06477">http://arxiv.org/abs/2308.06477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Nikolass Lindeijer, Tord Martin Ytredal, Trygve Eftestøl, Tobias Nordström, Fredrik Jäderling, Martin Eklund, Alvaro Fernandez-Quilez</li>
<li>for: 提高肾脏癌诊断的支持</li>
<li>methods: 使用对比学习方法，不需要manual注释，可以在部署时 missing views</li>
<li>results: 提高了自适应volumetric分割的精度，并且在多视图数据上实现了good external volumetric generalization<details>
<summary>Abstract</summary>
An accurate prostate delineation and volume characterization can support the clinical assessment of prostate cancer. A large amount of automatic prostate segmentation tools consider exclusively the axial MRI direction in spite of the availability as per acquisition protocols of multi-view data. Further, when multi-view data is exploited, manual annotations and availability at test time for all the views is commonly assumed. In this work, we explore a contrastive approach at training time to leverage multi-view data without annotations and provide flexibility at deployment time in the event of missing views. We propose a triplet encoder and single decoder network based on U-Net, tU-Net (triplet U-Net). Our proposed architecture is able to exploit non-annotated sagittal and coronal views via contrastive learning to improve the segmentation from a volumetric perspective. For that purpose, we introduce the concept of inter-view similarity in the latent space. To guide the training, we combine a dice score loss calculated with respect to the axial view and its manual annotations together with a multi-view contrastive loss. tU-Net shows statistical improvement in dice score coefficient (DSC) with respect to only axial view (91.25+-0.52% compared to 86.40+-1.50%,P<.001). Sensitivity analysis reveals the volumetric positive impact of the contrastive loss when paired with tU-Net (2.85+-1.34% compared to 3.81+-1.88%,P<.001). Further, our approach shows good external volumetric generalization in an in-house dataset when tested with multi-view data (2.76+-1.89% compared to 3.92+-3.31%,P=.002), showing the feasibility of exploiting non-annotated multi-view data through contrastive learning whilst providing flexibility at deployment in the event of missing views.
</details>
<details>
<summary>摘要</summary>
可以准确地定义和量化 prostata 的部分，可以支持肾癌的临床评估。许多自动 prostate 分割工具都忽略了多视图数据的可用性，即使据获取协议中有多视图数据可用。此外，当使用多视图数据时，通常需要手动标注和测试时 disponibility 的所有视图。在这种情况下，我们提出了一种对比方法，可以在训练时使用多视图数据而不需要手动标注，并在部署时提供可选的视图。我们提出了一种基于 U-Net 的 triplet 编码器和单个解码器网络，可以通过对比学习利用非标注的 sagittal 和极轴视图来提高分割。为此，我们引入了视图间的相似性在幂空间的概念。为了导航训练，我们将 dice 分数损失与 respect 到 axial 视图和其手动标注相加，并与多视图对比损失相结合。tU-Net 表示与只有 axial 视图相比（91.25+-0.52% 与 86.40+-1.50%，P<.001）显示了统计学上的改进。敏感分析表明，对于 tU-Net 来说，对比损失的volumetric 正面影响（2.85+-1.34% 与 3.81+-1.88%,P<.001）。此外，我们的方法在我们的内部数据集中进行了多视图数据的外部准确性测试（2.76+-1.89% 与 3.92+-3.31%,P=.002），表明可以通过对比学习在不具有标注的多视图数据上Exploiting 而提供可选的视图。
</details></li>
</ul>
<hr>
<h2 id="CATS-v2-Hybrid-encoders-for-robust-medical-segmentation"><a href="#CATS-v2-Hybrid-encoders-for-robust-medical-segmentation" class="headerlink" title="CATS v2: Hybrid encoders for robust medical segmentation"></a>CATS v2: Hybrid encoders for robust medical segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06377">http://arxiv.org/abs/2308.06377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoli12345/cats">https://github.com/haoli12345/cats</a></li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Xing Yao, Jiacheng Wang, Ipek Oguz<br>for:* 这个研究是为了提高医疗影像分类 задачі的性能，特别是在 capture 高级（本地）信息和全球信息之间的平衡。methods:* 使用 hybrid encoders，包括一个 CNN-based Encoder 路径和一个 transformer 路径，以更好地利用本地和全球信息。* 使用 skip connections 将 convolutional encoder 和 transformer 融合为最终的分类结果。results:* 在 Cross-Modality Domain Adaptation (CrossMoDA) 和 Medical Segmentation Decathlon (MSD-5) 项目中，该方法与州际先进方法相比， exhibit 高的 Dice 分数。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have exhibited strong performance in medical image segmentation tasks by capturing high-level (local) information, such as edges and textures. However, due to the limited field of view of convolution kernel, it is hard for CNNs to fully represent global information. Recently, transformers have shown good performance for medical image segmentation due to their ability to better model long-range dependencies. Nevertheless, transformers struggle to capture high-level spatial features as effectively as CNNs. A good segmentation model should learn a better representation from local and global features to be both precise and semantically accurate. In our previous work, we proposed CATS, which is a U-shaped segmentation network augmented with transformer encoder. In this work, we further extend this model and propose CATS v2 with hybrid encoders. Specifically, hybrid encoders consist of a CNN-based encoder path paralleled to a transformer path with a shifted window, which better leverage both local and global information to produce robust 3D medical image segmentation. We fuse the information from the convolutional encoder and the transformer at the skip connections of different resolutions to form the final segmentation. The proposed method is evaluated on two public challenge datasets: Cross-Modality Domain Adaptation (CrossMoDA) and task 5 of Medical Segmentation Decathlon (MSD-5), to segment vestibular schwannoma (VS) and prostate, respectively. Compared with the state-of-the-art methods, our approach demonstrates superior performance in terms of higher Dice scores.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗图像分割任务中表现出色，通过捕捉高级（本地）信息，如边缘和xture，来捕捉高级信息。然而，由于卷积核心的局部视场限制，使得CNN很难完全表示全局信息。最近，transformer在医疗图像分割中表现良好，这是因为它们可以更好地模型距离的长距离依赖关系。然而，transformer尚未能如同CNN那样有效地捕捉高级空间特征。为了实现更好的分割模型，我们需要学习更好地捕捉本地和全局特征，以确保准确和semantic地正确。在我们之前的工作中，我们提出了CATS模型，这是一种U型卷积分割网络，其中包括transformer编码器。在这个工作中，我们进一步扩展了这个模型，并提出了CATS v2模型，它包括hybrid编码器。specifically，hybrid编码器包括一个CNN基于编码器路径，并与一个偏移窗口的transformer路径并行，这样更好地利用本地和全局信息来生成robust的3D医疗图像分割。我们在不同分辨率的 skip connections中 fusions the information from the convolutional encoder and the transformer，以生成最终的分割结果。我们的方法在两个公共挑战数据集上进行评估： Cross-Modality Domain Adaptation（CrossMoDA）和Medical Segmentation Decathlon（MSD-5），用于分割vestibular schwannoma（VS）和prostate，分别。与当前状态的方法相比，我们的方法在 terms of higher Dice scores表现出色。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis"><a href="#Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis" class="headerlink" title="Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis"></a>Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06333">http://arxiv.org/abs/2308.06333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/open-eoe">https://github.com/hrlblab/open-eoe</a></li>
<li>paper_authors: Juming Xiong, Yilin Liu, Ruining Deng, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo<br>for: 这个研究旨在开发一个开源的工具集，用于检测食道病变中的嗜中性粒细胞（Eos）。methods: 该工具集使用三种现状最佳的深度学习基于对象检测模型，并实现了一种ensemble学习策略以提高结果的精度和可靠性。results: 实验结果表明，Open-EoE工具集可以效果地检测食道病变中的Eos，并达到了91%的准确率，与病理学家评估相符。<details>
<summary>Abstract</summary>
Eosinophilic Esophagitis (EoE) is a chronic, immune/antigen-mediated esophageal disease, characterized by symptoms related to esophageal dysfunction and histological evidence of eosinophil-dominant inflammation. Owing to the intricate microscopic representation of EoE in imaging, current methodologies which depend on manual identification are not only labor-intensive but also prone to inaccuracies. In this study, we develop an open-source toolkit, named Open-EoE, to perform end-to-end whole slide image (WSI) level eosinophil (Eos) detection using one line of command via Docker. Specifically, the toolkit supports three state-of-the-art deep learning-based object detection models. Furthermore, Open-EoE further optimizes the performance by implementing an ensemble learning strategy, and enhancing the precision and reliability of our results. The experimental results demonstrated that the Open-EoE toolkit can efficiently detect Eos on a testing set with 289 WSIs. At the widely accepted threshold of >= 15 Eos per high power field (HPF) for diagnosing EoE, the Open-EoE achieved an accuracy of 91%, showing decent consistency with pathologist evaluations. This suggests a promising avenue for integrating machine learning methodologies into the diagnostic process for EoE. The docker and source code has been made publicly available at https://github.com/hrlblab/Open-EoE.
</details>
<details>
<summary>摘要</summary>
《细胞滤镜检测诊断工具（Open-EoE）》是一个开源的检测工具，用于检测食管细胞滤镜病（EoE）的患者。该工具使用深度学习技术，可以通过一条命令在Docker环境中完成整个检测过程。工具支持三种当前顶峰的深度学习模型，并且通过 ensemble learning 策略来提高检测精度和可靠性。实验结果表明，Open-EoE 工具可以高效地检测食管细胞滤镜，在测试集上达到了91%的准确率。这表明，机器学习技术可以成功地应用于EoE 诊断过程中。工具和源代码已经在 GitHub 上公开，可以免费下载和使用。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology"><a href="#Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology" class="headerlink" title="Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology"></a>Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06332">http://arxiv.org/abs/2308.06332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FarihaHossain/SwinFSR">https://github.com/FarihaHossain/SwinFSR</a></li>
<li>paper_authors: Khondker Fariha Hossain, Sharif Amit Kamran, Joshua Ong, Andrew G. Lee, Alireza Tavakkoli</li>
<li>for: 这 paper 是为了提高肉眼图像的分辨率，以便在不同的地方进行早期差分诊断。</li>
<li>methods: 这 paper 使用了 Swin Transformer 与空间和深度精度注意力，实现了基于肉眼图像的超分辨率。</li>
<li>results: 这 paper 在三个公共数据集上 achieved PSNR 值为 47.89、49.00 和 45.32，并在 NASA 提供的私人数据集上达到了相当的结果。<details>
<summary>Abstract</summary>
The rapid accessibility of portable and affordable retinal imaging devices has made early differential diagnosis easier. For example, color funduscopy imaging is readily available in remote villages, which can help to identify diseases like age-related macular degeneration (AMD), glaucoma, or pathological myopia (PM). On the other hand, astronauts at the International Space Station utilize this camera for identifying spaceflight-associated neuro-ocular syndrome (SANS). However, due to the unavailability of experts in these locations, the data has to be transferred to an urban healthcare facility (AMD and glaucoma) or a terrestrial station (e.g, SANS) for more precise disease identification. Moreover, due to low bandwidth limits, the imaging data has to be compressed for transfer between these two places. Different super-resolution algorithms have been proposed throughout the years to address this. Furthermore, with the advent of deep learning, the field has advanced so much that x2 and x4 compressed images can be decompressed to their original form without losing spatial information. In this paper, we introduce a novel model called Swin-FSR that utilizes Swin Transformer with spatial and depth-wise attention for fundus image super-resolution. Our architecture achieves Peak signal-to-noise-ratio (PSNR) of 47.89, 49.00 and 45.32 on three public datasets, namely iChallenge-AMD, iChallenge-PM, and G1020. Additionally, we tested the model's effectiveness on a privately held dataset for SANS provided by NASA and achieved comparable results against previous architectures.
</details>
<details>
<summary>摘要</summary>
“现代化的眼科医学技术已经使得早期的医学诊断变得更加容易。例如，彩色基准摄影是在偏远村庄中可以提供的，可以帮助诊断年龄相关 macular degeneration（AMD）、 glaucoma 或 PATHOLOGICAL MYOPIA（PM）等疾病。然而，由于这些地点缺乏专家，因此需要将数据传输到城市医疗机构（AMD和 glaucoma）或地面站（例如，SANS）进行更加准确的疾病诊断。此外，由于带宽限制，摄影数据需要压缩传输。过去数年，有许多超解像算法的提案，以解决这个问题。另外，随着深度学习的发展，场景有了很大的进步，可以使用 x2 和 x4 压缩图像重新恢复到原始形态，无需失去空间信息。在本文中，我们介绍了一种新的模型called Swin-FSR，该模型利用SwinTransformer的空间和深度宽分注意力来进行基准图像超解像。我们的架构实现了 PSNR 的值为 47.89、49.00 和 45.32 在三个公共数据集上，namely iChallenge-AMD、iChallenge-PM 和 G1020。此外，我们对 NASA 提供的一个私有数据集进行了测试，并与之前的建筑物实现了相似的结果。”
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies"><a href="#A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies" class="headerlink" title="A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies"></a>A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07337">http://arxiv.org/abs/2308.07337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halid Ziya Yerebakan, Yoshihisa Shinagawa, Mahesh Ranganath, Simon Allen-Raffl, Gerardo Hermosillo Valadez</li>
<li>for: 医疗影像比较 longitudinal 比较中医学影像之间的 анатомиче位置匹配</li>
<li>methods: 使用 hierarchical sparse sampling of image intensities 计算查询点在源图像中的描述子，然后使用 hierarchical search 操作找到目标图像中最相似的描述子。</li>
<li>results: 实现了减少计算时间到毫秒级别的易行医学影像比较，无需额外建筑、存储或训练步骤。在 Deep Lesion Tracking 数据集注释中观察到更高准确的匹配结果，并且比最精确的报告algorithm 24 倍 faster。<details>
<summary>Abstract</summary>
We propose a method to match anatomical locations between pairs of medical images in longitudinal comparisons. The matching is made possible by computing a descriptor of the query point in a source image based on a hierarchical sparse sampling of image intensities that encode the location information. Then, a hierarchical search operation finds the corresponding point with the most similar descriptor in the target image. This simple yet powerful strategy reduces the computational time of mapping points to a millisecond scale on a single CPU. Thus, radiologists can compare similar anatomical locations in near real-time without requiring extra architectural costs for precomputing or storing deformation fields from registrations. Our algorithm does not require prior training, resampling, segmentation, or affine transformation steps. We have tested our algorithm on the recently published Deep Lesion Tracking dataset annotations. We observed more accurate matching compared to Deep Lesion Tracker while being 24 times faster than the most precise algorithm reported therein. We also investigated the matching accuracy on CT and MR modalities and compared the proposed algorithm's accuracy against ground truth consolidated from multiple radiologists.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在医疗图像对比中匹配生物学位置。该方法基于源图像中点Query的层次稀疏抽象来计算描述符，以便在目标图像中找到最相似的点。这种简单 yet powerful的策略可以在单个CPU上减少计算时间至毫秒级，因此，辐射学家可以在实时比较相似的生物学位置，无需额外的建筑成本或存储扭转场景的预计算或存储。我们的算法不需要先期训练、重新采样、分割或 afine 变换步骤。我们在最近发布的 Deep Lesion Tracking 数据集注释中进行了测试，并观察到比 Deep Lesion Tracker 更准确的匹配，而且比最精确的报告算法更快24倍。我们还研究了该方法在 CT 和 MR 模式下的匹配精度，并与多个 radiologists 共同组织的ground truth进行了比较。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/eess.IV_2023_08_12/" data-id="clmjn91qi00gn0j88cn2f1j0z" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/12/cs.SD_2023_08_12/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-12 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/11/cs.LG_2023_08_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-11 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
