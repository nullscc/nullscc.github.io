
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-16 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08442 repo_url: None paper_authors: Eunseop Yoon, Hee Suk Yoon, Dhananjaya Go">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-16 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/16/cs.SD_2023_08_16/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08442 repo_url: None paper_authors: Eunseop Yoon, Hee Suk Yoon, Dhananjaya Go">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-15T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:31.640Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/cs.SD_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-16 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-the-Exposure-Bias-in-Sentence-Level-Grapheme-to-Phoneme-G2P-Transduction"><a href="#Mitigating-the-Exposure-Bias-in-Sentence-Level-Grapheme-to-Phoneme-G2P-Transduction" class="headerlink" title="Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction"></a>Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08442">http://arxiv.org/abs/2308.08442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eunseop Yoon, Hee Suk Yoon, Dhananjaya Gowda, SooHwan Eom, Daehyeok Kim, John Harvill, Heting Gao, Mark Hasegawa-Johnson, Chanwoo Kim, Chang D. Yoo</li>
<li>for: 这个论文是为了提高文本中字符串到音节转换（G2P）的性能而写的。</li>
<li>methods: 这个论文使用了Tokenizer-free byte-level模型（ByT5），通过表示每个输入字符的UTF-8编码来实现字符串到音节转换。</li>
<li>results: 这个论文发现，使用ByT5进行句子级或段落级G2P可以提高实际应用中的用户体验，但是需要避免曝光偏见常见在自动生成模型中。<details>
<summary>Abstract</summary>
Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method.
</details>
<details>
<summary>摘要</summary>
文本-to-文本传输变换器（T5）最近被考虑用于文本-to-phoneme（G2P）转换。作为继续，一个不需要tokenizer的字节级模型基于T5，称之为ByT5，最近在word-level G2P转换中表现出了扎实的结果。虽然普遍认为 sentence-level或paragraph-level G2P可以提高实际应用中的可用性，因为更适合处理Homonyms和 слова间的连接音，但我们发现使用ByT5进行这些场景是非常困难。因为ByT5操作在字符水平，需要更长的解码步骤，这会导致性能下降，这是因为自动生成模型通常会出现露示偏见。本文显示，使用我们提议的损失采样方法可以提高 sentence-level和paragraph-level G2P的性能。
</details></li>
</ul>
<hr>
<h2 id="Classifying-Dementia-in-the-Presence-of-Depression-A-Cross-Corpus-Study"><a href="#Classifying-Dementia-in-the-Presence-of-Depression-A-Cross-Corpus-Study" class="headerlink" title="Classifying Dementia in the Presence of Depression: A Cross-Corpus Study"></a>Classifying Dementia in the Presence of Depression: A Cross-Corpus Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08306">http://arxiv.org/abs/2308.08306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franziska Braun, Sebastian P. Bayerl, Paula A. Pérez-Toro, Florian Hönig, Hartmut Lehfeld, Thomas Hillemacher, Elmar Nöth, Tobias Bocklet, Korbinian Riedhammer</li>
<li>for: 这个论文的目的是提出一种自动诊断老年痴呆症的方法，以提高医疗系统的效率和患者的生活质量。</li>
<li>methods: 这个论文使用了语音、文本和情感嵌入来分类诊断老年痴呆症，并在三个类别中进行了比较（健康人 VS 轻度智能障碍 VS 老年痴呆症）。</li>
<li>results: 研究人员通过对两个独立录制的德国数据集进行交叉验证和混合验证，发现了这种方法的普适性和可重复性。<details>
<summary>Abstract</summary>
Automated dementia screening enables early detection and intervention, reducing costs to healthcare systems and increasing quality of life for those affected. Depression has shared symptoms with dementia, adding complexity to diagnoses. The research focus so far has been on binary classification of dementia (DEM) and healthy controls (HC) using speech from picture description tests from a single dataset. In this work, we apply established baseline systems to discriminate cognitive impairment in speech from the semantic Verbal Fluency Test and the Boston Naming Test using text, audio and emotion embeddings in a 3-class classification problem (HC vs. MCI vs. DEM). We perform cross-corpus and mixed-corpus experiments on two independently recorded German datasets to investigate generalization to larger populations and different recording conditions. In a detailed error analysis, we look at depression as a secondary diagnosis to understand what our classifiers actually learn.
</details>
<details>
<summary>摘要</summary>
自动化认知评估可以早期检测和 intervene，降低医疗系统的成本和提高认知症患者的生活质量。与认知症有共同症状的抑郁症可以增加诊断的复杂性。过去的研究主要集中在使用单一数据集的语音描述测验进行二分类认知症和健康控制（HC）的分类。在这个工作中，我们使用已经建立的基eline系统来分辨语音中的认知障碍，使用 semantic Verbal Fluency Test 和 Boston Naming Test 的文本、音频和情感嵌入，进行三类分类问题（HC vs. MCI vs. DEM）。我们在两个独立录取的德国数据集上进行交叉数据和混合数据实验，以调查更大的人口和不同的录音条件下的一致性。在详细的错误分析中，我们将抑郁症作为次要诊断来了解我们的分类器是否真的学习了什么。
</details></li>
</ul>
<hr>
<h2 id="ChinaTelecom-System-Description-to-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#ChinaTelecom-System-Description-to-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023"></a>ChinaTelecom System Description to VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08181">http://arxiv.org/abs/2308.08181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengjie Du, Xiang Fang, Jie Li</li>
<li>for: This paper is written for the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023) and describes the ChinaTelecom system for Track 1 (closed).</li>
<li>methods: The system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system.</li>
<li>results: The final submission achieved minDCF of 0.1066 and EER of 1.980%.Here are the three key points in Simplified Chinese:</li>
<li>for: 这篇论文是为了VOXCELEB2023发音识别挑战（VOXSRC2023）的Track 1（关闭）而写的。</li>
<li>methods: 该系统包括几种基于VOXCELEB2的ResNet变体，这些变体被后续进行了融合以提高表现。此外，每个变体和融合系统还进行了分数调整。</li>
<li>results: 最终提交的结果为minDCF为0.1066和EER为1.980%。<details>
<summary>Abstract</summary>
This technical report describes ChinaTelecom system for Track 1 (closed) of the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system consists of several ResNet variants trained only on VoxCeleb2, which were fused for better performance later. Score calibration was also applied for each variant and the fused system. The final submission achieved minDCF of 0.1066 and EER of 1.980%.
</details>
<details>
<summary>摘要</summary>
这份技术报告介绍了我们在VoxCeleb2023 Speaker Recognition Challenge（VoxSRC 2023）的Track 1（关闭）系统。我们的系统包括了多种ResNet变体，只在VoxCeleb2上进行训练。这些变体后来进行了融合，以提高性能。此外，我们还应用了分数均衡calibration对每个变体和融合系统。最终的提交达到了0.1066的minDCF和1.980%的EER。
</details></li>
</ul>
<hr>
<h2 id="AffectEcho-Speaker-Independent-and-Language-Agnostic-Emotion-and-Affect-Transfer-for-Speech-Synthesis"><a href="#AffectEcho-Speaker-Independent-and-Language-Agnostic-Emotion-and-Affect-Transfer-for-Speech-Synthesis" class="headerlink" title="AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis"></a>AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08577">http://arxiv.org/abs/2308.08577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishikesh Viswanath, Aneesh Bhattacharya, Pascal Jutras-Dubé, Prerit Gupta, Mridu Prashanth, Yashvardhan Khaitan, Aniket Bera</li>
<li>for: The paper is written for discussing a new approach to emotional translation in text-to-speech and speech-to-speech systems, with the goal of capturing complex nuances and subtle differences in emotions.</li>
<li>methods: The proposed approach, called AffectEcho, uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings.</li>
<li>results: The experimental results demonstrate the effectiveness of the proposed approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. The language-independent emotion modeling capability of the quantized emotional embeddings learned from a bilingual (English and Chinese) speech corpus is also shown, with an emotion transfer task from a reference speech to a target speech achieving state-of-art results on both qualitative and quantitative metrics.Here are the three points in Simplified Chinese text:</li>
<li>for: 本文是为了介绍一种新的情感翻译方法，用于文本到语音和语音到语音系统中，以便捕捉复杂的情感细微差异。</li>
<li>methods: 提议的方法是使用Vector Quantized codebook来模型情感，在一个5级情感强度的量化空间中进行模型。这些量化情感嵌入不需要一个热度 вектор或者显式强度嵌入。</li>
<li>results: 实验结果表明，提议的方法能够控制生成的语音中的情感，同时保持每个 speaker 的个性、风格和情感节奏。此外，通过一种语言无关的情感模型，在一个英文和中文双语Speech corpus中学习的情感嵌入可以在另一种语言中进行情感传递任务，并达到了当前最佳的质量和量化指标。<details>
<summary>Abstract</summary>
Affect is an emotional characteristic encompassing valence, arousal, and intensity, and is a crucial attribute for enabling authentic conversations. While existing text-to-speech (TTS) and speech-to-speech systems rely on strength embedding vectors and global style tokens to capture emotions, these models represent emotions as a component of style or represent them in discrete categories. We propose AffectEcho, an emotion translation model, that uses a Vector Quantized codebook to model emotions within a quantized space featuring five levels of affect intensity to capture complex nuances and subtle differences in the same emotion. The quantized emotional embeddings are implicitly derived from spoken speech samples, eliminating the need for one-hot vectors or explicit strength embeddings. Experimental results demonstrate the effectiveness of our approach in controlling the emotions of generated speech while preserving identity, style, and emotional cadence unique to each speaker. We showcase the language-independent emotion modeling capability of the quantized emotional embeddings learned from a bilingual (English and Chinese) speech corpus with an emotion transfer task from a reference speech to a target speech. We achieve state-of-art results on both qualitative and quantitative metrics.
</details>
<details>
<summary>摘要</summary>
“情感”是一种情感特征，包括价值观、情感刺激和强度，这种特征对实际对话的进行是关键。现有的文本到语音（TTS）和语音到语音系统通常使用强度嵌入向量和全局风格token来捕捉情感，但这些模型表示情感为样式的一部分或以分类的方式表示。我们提议的情感回声模型（AffectEcho）使用量化编码 кни簿来模型情感在量化空间中的五级强度，以捕捉复杂的细节和同一种情感中的微妙差异。这些量化情感嵌入不需要一个一热 вектор或显式强度嵌入。我们的方法可以控制生成的语音中的情感，保留每个说话者的个性、风格和情感节奏。我们在一个英文和中文语音词汇库中学习的语言独立情感模型能够完成参照语音到目标语音的情感传递任务，并在质量和量度指标上达到了当前最佳效果。
</details></li>
</ul>
<hr>
<h2 id="SCANet-A-Self-and-Cross-Attention-Network-for-Audio-Visual-Speech-Separation"><a href="#SCANet-A-Self-and-Cross-Attention-Network-for-Audio-Visual-Speech-Separation" class="headerlink" title="SCANet: A Self- and Cross-Attention Network for Audio-Visual Speech Separation"></a>SCANet: A Self- and Cross-Attention Network for Audio-Visual Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08143">http://arxiv.org/abs/2308.08143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Li, Runxuan Yang, Xiaolin Hu</li>
<li>for: 这篇论文主要用于探讨一种新的多模态协同分离方法，即自适应和交叉关注网络（SCANet），该方法可以有效地融合音频和视频特征，以提高对话人的识别率。</li>
<li>methods: SCANet 使用了两种听取块：自适应（SA）块和交叉关注（CA）块，其中 CA 块分布在网络的顶部（TCA）、中部（MCA）和底部（BCA）。这些块可以学习不同的模式特征，并提取不同的 semantics 从音频和视频特征。</li>
<li>results: 根据三个标准的音频视频分离 benchmark（LRS2、LRS3 和 VoxCeleb2）的实验结果，SCANet 比现有的状态对比方法（SOTA）高效，同时保持相对的执行时间。<details>
<summary>Abstract</summary>
The integration of different modalities, such as audio and visual information, plays a crucial role in human perception of the surrounding environment. Recent research has made significant progress in designing fusion modules for audio-visual speech separation. However, they predominantly focus on multi-modal fusion architectures situated either at the top or bottom positions, rather than comprehensively considering multi-modal fusion at various hierarchical positions within the network. In this paper, we propose a novel model called self- and cross-attention network (SCANet), which leverages the attention mechanism for efficient audio-visual feature fusion. SCANet consists of two types of attention blocks: self-attention (SA) and cross-attention (CA) blocks, where the CA blocks are distributed at the top (TCA), middle (MCA) and bottom (BCA) of SCANet. These blocks maintain the ability to learn modality-specific features and enable the extraction of different semantics from audio-visual features. Comprehensive experiments on three standard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2) demonstrate the effectiveness of SCANet, outperforming existing state-of-the-art (SOTA) methods while maintaining comparable inference time.
</details>
<details>
<summary>摘要</summary>
人类在识别环境中利用不同模式的感知信息，如音频和视觉信息，进行集成很重要。现代研究已经在设计多模态融合模块方面做出了重要进步，但是这些模型大多集中于网络的顶层或底层位置，而不是全面考虑多模态融合在网络各个层次位置。本文提出了一种新的模型，即自身和交叉注意网络（SCANet），它利用注意机制来有效地融合音频和视觉特征。SCANet包括两种注意块：自身注意（SA）和交叉注意（CA）块，其中CA块分布在网络顶层（TCA）、中层（MCA）和底层（BCA）。这些块可以学习不同模式的特征，并允许从音频和视觉特征中提取不同的 semantics。我们对三个标准音频视频分离 benchmark（LRS2、LRS3和VoxCeleb2）进行了广泛的实验， demonstarted SCANet的效果，而且与现有的最佳方法（SOTA）保持相对的推理时间。
</details></li>
</ul>
<hr>
<h2 id="Radio2Text-Streaming-Speech-Recognition-Using-mmWave-Radio-Signals"><a href="#Radio2Text-Streaming-Speech-Recognition-Using-mmWave-Radio-Signals" class="headerlink" title="Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals"></a>Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08125">http://arxiv.org/abs/2308.08125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Running Zhao, Jiangtao Yu, Hang Zhao, Edith C. H. Ngai</li>
<li>for: 这篇论文旨在提供一种基于 millimeter wave (mmWave) 技术的语音识别系统，以便在会议听录和窃听等场景中实现高精度语音识别。</li>
<li>methods: 该系统基于一种适配 streams 的 transformer 模型，通过特定的束缚和知识储存技术来实现大词汇量语音识别。</li>
<li>results: 实验结果显示，Radio2Text 可以在 recognizing 一个包含超过 13,000 个词的词汇库时达到 character error rate 5.7% 和 word error rate 9.4%。<details>
<summary>Abstract</summary>
Millimeter wave (mmWave) based speech recognition provides more possibility for audio-related applications, such as conference speech transcription and eavesdropping. However, considering the practicality in real scenarios, latency and recognizable vocabulary size are two critical factors that cannot be overlooked. In this paper, we propose Radio2Text, the first mmWave-based system for streaming automatic speech recognition (ASR) with a vocabulary size exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer that is capable of effectively learning representations of speech-related features, paving the way for streaming ASR with a large vocabulary. To alleviate the deficiency of streaming networks unable to access entire future inputs, we propose the Guidance Initialization that facilitates the transfer of feature knowledge related to the global context from the non-streaming Transformer to the tailored streaming Transformer through weight inheritance. Further, we propose a cross-modal structure based on knowledge distillation (KD), named cross-modal KD, to mitigate the negative effect of low quality mmWave signals on recognition performance. In the cross-modal KD, the audio streaming Transformer provides feature and response guidance that inherit fruitful and accurate speech information to supervise the training of the tailored radio streaming Transformer. The experimental results show that our Radio2Text can achieve a character error rate of 5.7% and a word error rate of 9.4% for the recognition of a vocabulary consisting of over 13,000 words.
</details>
<details>
<summary>摘要</summary>
微米波（mmWave）基于语音识别提供更多的音频相关应用，如会议语音转文和窃听。然而，在实际场景中，延迟和可识别词汇数是两个关键因素，不能被忽略。在这篇论文中，我们提出Radio2Text，首个基于微米波的流处理自动语音识别（ASR）系统，可以识别超过13,000个词的词汇。Radio2Text基于适应流处理变换器，可以有效地学习语音相关特征的表示，为流处理ASR带来新的可能性。为了解决流处理网络无法访问整个未来输入的缺陷，我们提出引导初始化，通过重量继承来传递非流处理变换器中的特征知识相关全局 контекст到适应流处理变换器。此外，我们提出了基于知识传授（KD）的交叉模态结构，称为交叉模态KD，以mitigate低质量微米波信号对识别性的负面效应。在交叉模态KD中，音频流处理变换器提供特征和回应导航，将有用和准确的语音信息继承给适应广播流处理变换器进行超vision训练。实验结果显示，我们的Radio2Text可以达到Character Error Rate（CER）5.7%和Word Error Rate（WER）9.4%，用于识别超过13,000个词的词汇。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Open-Vocabulary-Keyword-Search-With-Multilingual-Neural-Representations"><a href="#End-to-End-Open-Vocabulary-Keyword-Search-With-Multilingual-Neural-Representations" class="headerlink" title="End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations"></a>End-to-End Open Vocabulary Keyword Search With Multilingual Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08027">http://arxiv.org/abs/2308.08027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bolaji Yusuf, Jan Cernocky, Murat Saraclar</li>
<li>for: 提高 keyword search 系统的效率和简化搜索过程，不需要自动语音识别（ASR）输出。</li>
<li>methods: 使用神经网络encoder对查询和文档进行编码，并将编码结果进行点积 multiplication 组合。</li>
<li>results: 在长查询和没有在训练数据中出现的查询中，提高了模型的性能，并且对于短查询和包含在 vocabulary 中的查询，虽然不能与强大的 ASR-based 传统搜索系统匹配，但是仍然超过了 ASR-based 系统。<details>
<summary>Abstract</summary>
Conventional keyword search systems operate on automatic speech recognition (ASR) outputs, which causes them to have a complex indexing and search pipeline. This has led to interest in ASR-free approaches to simplify the search procedure. We recently proposed a neural ASR-free keyword search model which achieves competitive performance while maintaining an efficient and simplified pipeline, where queries and documents are encoded with a pair of recurrent neural network encoders and the encodings are combined with a dot-product. In this article, we extend this work with multilingual pretraining and detailed analysis of the model. Our experiments show that the proposed multilingual training significantly improves the model performance and that despite not matching a strong ASR-based conventional keyword search system for short queries and queries comprising in-vocabulary words, the proposed model outperforms the ASR-based system for long queries and queries that do not appear in the training data.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:传统的关键词搜索系统采用自动语音识别（ASR）输出，这导致搜索管道变得复杂。这引起了关注ASR-free方法，以简化搜索过程。我们最近提出了一种基于神经网络的ASR-free关键词搜索模型，该模型在竞争性和效率方面具有优异表现，而无需复杂的搜索管道。在这篇文章中，我们延续这种工作，并通过多语言预训练和详细分析，进一步提高模型性能。我们的实验表明，提档多语言训练显著提高模型性能，并且对于长 queries和不在训练数据中出现的 queries，模型的性能胜过ASR-based系统。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/cs.SD_2023_08_16/" data-id="clly3dw11009x09882ssmfkb3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/16/cs.LG_2023_08_16/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-16 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/16/eess.AS_2023_08_16/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-08-16 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
