
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-16 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08396 repo_url: None paper_authors: Denis K">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-16 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/16/eess.IV_2023_08_16/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.08396 repo_url: None paper_authors: Denis K">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-15T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:31.708Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/16/eess.IV_2023_08_16/" class="article-date">
  <time datetime="2023-08-15T16:00:00.000Z" itemprop="datePublished">2023-08-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-16 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Prediction-of-post-radiotherapy-recurrence-volumes-in-head-and-neck-squamous-cell-carcinoma-using-3D-U-Net-segmentation"><a href="#Prediction-of-post-radiotherapy-recurrence-volumes-in-head-and-neck-squamous-cell-carcinoma-using-3D-U-Net-segmentation" class="headerlink" title="Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation"></a>Prediction of post-radiotherapy recurrence volumes in head and neck squamous cell carcinoma using 3D U-Net segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08396">http://arxiv.org/abs/2308.08396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Kutnár, Ivan R Vogelius, Katrin Elisabet Håkansson, Jens Petersen, Jeppe Friborg, Lena Specht, Mogens Bernsdorf, Anita Gothelf, Claus Kristensen, Abraham George Smith</li>
<li>for: 这研究旨在使用 convolutional neural network (CNN) 预测头颈癌细胞癌 (HNSCC) 患者的 lokoregional recurrences (LRR) 体积，以便通过生物标记化辐射疗法来提高治疗效果。</li>
<li>methods: 研究使用了 18F-fluorodeoxyglucose positron emission tomography (FDG-PET)&#x2F;computed tomography (CT) 扫描图像来训练 CNN，并对患者的前 treated 患区进行了预测。</li>
<li>results: 研究发现，使用 CNN 可以准确预测 HNSCC 患者的 LRR 体积，并且比使用 SUVmax 阈值方法或 GTV 直接使用更有效。 however, 需要进一步的数据集开发以实现临床有用的预测精度。<details>
<summary>Abstract</summary>
Locoregional recurrences (LRR) are still a frequent site of treatment failure for head and neck squamous cell carcinoma (HNSCC) patients.   Identification of high risk subvolumes based on pretreatment imaging is key to biologically targeted radiation therapy. We investigated the extent to which a Convolutional neural network (CNN) is able to predict LRR volumes based on pre-treatment 18F-fluorodeoxyglucose positron emission tomography (FDG-PET)/computed tomography (CT) scans in HNSCC patients and thus the potential to identify biological high risk volumes using CNNs.   For 37 patients who had undergone primary radiotherapy for oropharyngeal squamous cell carcinoma, five oncologists contoured the relapse volumes on recurrence CT scans. Datasets of pre-treatment FDG-PET/CT, gross tumour volume (GTV) and contoured relapse for each of the patients were randomly divided into training (n=23), validation (n=7) and test (n=7) datasets. We compared a CNN trained from scratch, a pre-trained CNN, a SUVmax threshold approach, and using the GTV directly.   The SUVmax threshold method included 5 out of the 7 relapse origin points within a volume of median 4.6 cubic centimetres (cc). Both the GTV contour and best CNN segmentations included the relapse origin 6 out of 7 times with median volumes of 28 and 18 cc respectively.   The CNN included the same or greater number of relapse volume POs, with significantly smaller relapse volumes. Our novel findings indicate that CNNs may predict LRR, yet further work on dataset development is required to attain clinically useful prediction accuracy.
</details>
<details>
<summary>摘要</summary>
Head and neck squamous cell carcinoma (HNSCC) 患者中的局部再现 (LRR) 仍然是治疗失败的常见现象。 为了预测LRR的卷积批处，我们使用了卷积神经网络 (CNN)。我们研究了在前治疗18F-fluorodeoxyglucose пози트рон辐射tomography (FDG-PET)/计算机 Tomography (CT) 图像上预测LRR объем的可能性。为了进行这项研究，我们收集了37名有主治疗的口腔癌患者的数据。这些患者都已经接受了主治疗。我们让5名医生标注了再现图像上的再现 объем。我们将这些数据分为训练集（n=23）、验证集（n=7）和测试集（n=7）。我们比较了从头文字开始训练的CNN、预训练的CNN、SUVmax阈值方法和直接使用GTV的方法。SUVmax阈值方法中包含了7名再现起点的5个点在 median 4.6立方厘米（cc）内。GTV框和最佳CNN分割都包含了再现起点6个名次， median volume 28和18 cc。 CN中包含了相同或更多的再现量PO，并且再现 объем更小。我们的新发现表明，CNN可能预测LRR，但是我们需要进一步增加数据来提高临床可用性的预测精度。
</details></li>
</ul>
<hr>
<h2 id="DeepContrast-Deep-Tissue-Contrast-Enhancement-using-Synthetic-Data-Degradations-and-OOD-Model-Predictions"><a href="#DeepContrast-Deep-Tissue-Contrast-Enhancement-using-Synthetic-Data-Degradations-and-OOD-Model-Predictions" class="headerlink" title="DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions"></a>DeepContrast: Deep Tissue Contrast Enhancement using Synthetic Data Degradations and OOD Model Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08365">http://arxiv.org/abs/2308.08365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuno Pimpão Martins, Yannis Kalaidzidis, Marino Zerial, Florian Jug</li>
<li>for: 该论文主要针对 Microscopy 图像质量下降的问题，即图像噪音、模糊和其他问题，以及这些问题对图像含义的影响。</li>
<li>methods: 该论文使用 Deep Learning 方法来提高 Microscopy 图像质量，但是这些方法通常需要有clean的ground truth（GT）数据进行训练。然而，在深入到样本中图像时，因为各种降低效应，获得clean GT数据变得困难。因此，该论文提出了一种新的方法，可以缺省GT数据进行训练。</li>
<li>results: 该论文使用了一种模拟前向模型来模拟深入到样本中图像的降低效应，然后使用 neural network 学习这个降低效应的逆过程。结果表明，使用这种方法可以提高 Microscopy 图像质量，并且可以在不需要clean GT数据的情况下进行训练。此外，该论文还发现，在不同的下游分析中，需要找到一个平衡点，以保留图像细节和提高图像含义的抽象。<details>
<summary>Abstract</summary>
Microscopy images are crucial for life science research, allowing detailed inspection and characterization of cellular and tissue-level structures and functions. However, microscopy data are unavoidably affected by image degradations, such as noise, blur, or others. Many such degradations also contribute to a loss of image contrast, which becomes especially pronounced in deeper regions of thick samples. Today, best performing methods to increase the quality of images are based on Deep Learning approaches, which typically require ground truth (GT) data during training. Our inability to counteract blurring and contrast loss when imaging deep into samples prevents the acquisition of such clean GT data. The fact that the forward process of blurring and contrast loss deep into tissue can be modeled, allowed us to propose a new method that can circumvent the problem of unobtainable GT data. To this end, we first synthetically degraded the quality of microscopy images even further by using an approximate forward model for deep tissue image degradations. Then we trained a neural network that learned the inverse of this degradation function from our generated pairs of raw and degraded images. We demonstrated that networks trained in this way can be used out-of-distribution (OOD) to improve the quality of less severely degraded images, e.g. the raw data imaged in a microscope. Since the absolute level of degradation in such microscopy images can be stronger than the additional degradation introduced by our forward model, we also explored the effect of iterative predictions. Here, we observed that in each iteration the measured image contrast kept improving while detailed structures in the images got increasingly removed. Therefore, dependent on the desired downstream analysis, a balance between contrast improvement and retention of image details has to be found.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:微scopic 图像是生命科学研究中不可或缺的，允许详细检查和Cellular 和组织水平结构和功能的 characterization。然而，微scopic 图像数据不可避免地受到图像质量下降的影响，如噪声、模糊或其他。这些质量下降也会导致图像对比下降，特别是在厚度样本深处。今天，使用深度学习方法来提高图像质量的最佳实践是基于GT数据的训练。然而，我们无法对深度样本中的厚度进行对比下降，因此无法获得净化GT数据。我们发现，可以使用深度图像模型来模拟深度样本中的对比下降过程，从而提出一种新的方法，可以绕过无法获得GT数据的问题。我们首先使用深度图像模型来进一步降低微scopic 图像的质量，然后使用这些生成的对比下降对的图像对比下降进行学习。我们证明了这种方法可以在OOD（out-of-distribution）下使用，以提高less severely degraded 的图像质量。由于微scopic 图像中的绝对质量可能比我们的深度图像模型引入的质量更强，我们还 investigate了反复预测的效果。我们发现，在每次预测中，测量图像对比度会不断提高，而图像中的细节会逐渐消失。因此，根据下游分析的需求，需要找到保持图像细节的平衡。
</details></li>
</ul>
<hr>
<h2 id="GAEI-UNet-Global-Attention-and-Elastic-Interaction-U-Net-for-Vessel-Image-Segmentation"><a href="#GAEI-UNet-Global-Attention-and-Elastic-Interaction-U-Net-for-Vessel-Image-Segmentation" class="headerlink" title="GAEI-UNet: Global Attention and Elastic Interaction U-Net for Vessel Image Segmentation"></a>GAEI-UNet: Global Attention and Elastic Interaction U-Net for Vessel Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08345">http://arxiv.org/abs/2308.08345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiqiang Xiao, Zhuoyue Wan</li>
<li>for: 静脉图像分割是医学诊断中非常重要的一环，能够帮助早期发现和治疗血管疾病。</li>
<li>methods: 我们提出了一种新的模型，即GAEI-UNet，它将全局注意力和弹性互动技术相结合。GAEI-UNet 利用全局空间和通道信息来增强 U-Net 架构中的高级 semantics 理解，以提高小血管的精确分割。此外，我们采用了弹性互动基于的损失函数，以提高小血管网络中的连接性。</li>
<li>results: 我们在 DRIVE 血管图像集上进行了评估，结果表明 GAEI-UNet 在精确分割小血管方面表现出色，而无需增加计算复杂度。此外，GAEI-UNet 还能够保持血管网络的正确 topology。<details>
<summary>Abstract</summary>
Vessel image segmentation plays a pivotal role in medical diagnostics, aiding in the early detection and treatment of vascular diseases. While segmentation based on deep learning has shown promising results, effectively segmenting small structures and maintaining connectivity between them remains challenging. To address these limitations, we propose GAEI-UNet, a novel model that combines global attention and elastic interaction-based techniques. GAEI-UNet leverages global spatial and channel context information to enhance high-level semantic understanding within the U-Net architecture, enabling precise segmentation of small vessels. Additionally, we adopt an elastic interaction-based loss function to improve connectivity among these fine structures. By capturing the forces generated by misalignment between target and predicted shapes, our model effectively learns to preserve the correct topology of vessel networks. Evaluation on retinal vessel dataset -- DRIVE demonstrates the superior performance of GAEI-UNet in terms of SE and connectivity of small structures, without significantly increasing computational complexity. This research aims to advance the field of vessel image segmentation, providing more accurate and reliable diagnostic tools for the medical community. The implementation code is available on Code.
</details>
<details>
<summary>摘要</summary>
船体图像分割在医学诊断中扮演着关键角色，帮助早期发现和治疗血管疾病。although deep learning-based segmentation has shown promising results, effectively segmenting small structures and maintaining connectivity between them remains challenging. To address these limitations, we propose GAEI-UNet, a novel model that combines global attention and elastic interaction-based techniques. GAEI-UNet leverages global spatial and channel context information to enhance high-level semantic understanding within the U-Net architecture, enabling precise segmentation of small vessels. Additionally, we adopt an elastic interaction-based loss function to improve connectivity among these fine structures. By capturing the forces generated by misalignment between target and predicted shapes, our model effectively learns to preserve the correct topology of vessel networks. Evaluation on retinal vessel dataset -- DRIVE demonstrates the superior performance of GAEI-UNet in terms of SE and connectivity of small structures, without significantly increasing computational complexity. This research aims to advance the field of vessel image segmentation, providing more accurate and reliable diagnostic tools for the medical community. The implementation code is available on Code.Here's the text with some notes on the translation:1. "船体图像分割" (zhōng tǐ tú zhǐ bīng) - This phrase is used to refer to the process of segmenting images of vessels, such as blood vessels in the retina.2. "在医学诊断中扮演着关键角色" (zhī xué shòu yì zhòng zhì yǐng) - This phrase emphasizes the importance of vessel image segmentation in medical diagnosis.3. "although deep learning-based segmentation has shown promising results" (although deep learning-based segmentation has shown promising results) - This phrase is used to acknowledge the progress that has been made in vessel image segmentation using deep learning techniques.4. "effectively segmenting small structures and maintaining connectivity between them remains challenging" (effectively segmenting small structures and maintaining connectivity between them remains challenging) - This phrase highlights the limitations of current vessel image segmentation methods, specifically the difficulty in accurately segmenting small vessels and maintaining the connectivity between them.5. "To address these limitations, we propose GAEI-UNet" (To address these limitations, we propose GAEI-UNet) - This phrase introduces the novel model proposed in the research, which aims to overcome the limitations of current methods.6. "a novel model that combines global attention and elastic interaction-based techniques" (a novel model that combines global attention and elastic interaction-based techniques) - This phrase describes the key innovation of the proposed model, which combines global attention and elastic interaction-based techniques to improve the accuracy and reliability of vessel image segmentation.7. "leverages global spatial and channel context information to enhance high-level semantic understanding" (leverages global spatial and channel context information to enhance high-level semantic understanding) - This phrase explains how the proposed model uses global spatial and channel context information to improve the understanding of the vessel networks and enhance the accuracy of segmentation.8. "enabling precise segmentation of small vessels" (enabling precise segmentation of small vessels) - This phrase highlights the main advantage of the proposed model, which is its ability to accurately segment small vessels.9. "without significantly increasing computational complexity" (without significantly increasing computational complexity) - This phrase emphasizes that the proposed model does not require a significant increase in computational resources, making it more practical and efficient for real-world applications.10. "This research aims to advance the field of vessel image segmentation" (This research aims to advance the field of vessel image segmentation) - This phrase highlights the overall goal of the research, which is to improve the accuracy and reliability of vessel image segmentation and provide more accurate and reliable diagnostic tools for the medical community.I hope this helps! Let me know if you have any further questions or if there's anything else I can help with.
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-Probabilistic-Model-for-Retinal-Image-Generation-and-Segmentation"><a href="#Denoising-Diffusion-Probabilistic-Model-for-Retinal-Image-Generation-and-Segmentation" class="headerlink" title="Denoising Diffusion Probabilistic Model for Retinal Image Generation and Segmentation"></a>Denoising Diffusion Probabilistic Model for Retinal Image Generation and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08339">http://arxiv.org/abs/2308.08339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aaleka/retree">https://github.com/aaleka/retree</a></li>
<li>paper_authors: Alnur Alimanov, Md Baharul Islam<br>for:这个研究旨在提供一个高品质的retinal image dataset，并运用Generative Adversarial Networks (GAN)和Denosing Diffusion Probabilistic Model (DDPM)来生成具有多样性的retinal images。methods:本研究使用了GAN和DDPM来生成retinal images，并开发了一个名为Retinal Trees (ReTree)的dataset，包括retinal images、相应的血管树和一个基于DDPM的分类网络。results:研究发现，DDPM可以对于retinal images的生成比GAN更高品质，并且可以生成多样性较高的retinal images。ReTree dataset也被评估了其量化和质量上的表现，并且显示了其可以用于血管树分类和retinal image分类任务。<details>
<summary>Abstract</summary>
Experts use retinal images and vessel trees to detect and diagnose various eye, blood circulation, and brain-related diseases. However, manual segmentation of retinal images is a time-consuming process that requires high expertise and is difficult due to privacy issues. Many methods have been proposed to segment images, but the need for large retinal image datasets limits the performance of these methods. Several methods synthesize deep learning models based on Generative Adversarial Networks (GAN) to generate limited sample varieties. This paper proposes a novel Denoising Diffusion Probabilistic Model (DDPM) that outperformed GANs in image synthesis. We developed a Retinal Trees (ReTree) dataset consisting of retinal images, corresponding vessel trees, and a segmentation network based on DDPM trained with images from the ReTree dataset. In the first stage, we develop a two-stage DDPM that generates vessel trees from random numbers belonging to a standard normal distribution. Later, the model is guided to generate fundus images from given vessel trees and random distribution. The proposed dataset has been evaluated quantitatively and qualitatively. Quantitative evaluation metrics include Frechet Inception Distance (FID) score, Jaccard similarity coefficient, Cohen's kappa, Matthew's Correlation Coefficient (MCC), precision, recall, F1-score, and accuracy. We trained the vessel segmentation model with synthetic data to validate our dataset's efficiency and tested it on authentic data. Our developed dataset and source code is available at https://github.com/AAleka/retree.
</details>
<details>
<summary>摘要</summary>
专家利用血液图像和血管树来检测和诊断各种眼、血液和脑部疾病。然而，手动分割血液图像是一项时间consuming和需要高度专业知识的过程，另外，隐私问题也使得这项工作困难。许多方法已经被提出来分割图像，但是因为数据的限制，这些方法的性能受到限制。本文提出了一种新的涂抹扩散 probabilistic model（DDPM），其在图像生成方面超过了GAN的表现。我们还制作了一个名为“Retinal Trees”（ReTree）的 dataset，该 dataset包括血液图像、对应的血管树和基于 DDPM 的分割网络。在首个阶段，我们开发了一种两 stage DDPM，该模型从标准正态分布中的随机数生成血管树。后来，模型被引导使用给定的血管树和随机分布来生成血液图像。我们对该 dataset 进行了量化和质量上的评估。量化评估指标包括Frechet Inception Distance（FID）分数、Jaccard 相似度系数、Cohen's kappa、Matthew's Correlation Coefficient（MCC）、精度、 recall、F1-score 和准确率。我们使用合成数据来训练分割模型，以验证我们的 dataset 的效率，然后在真实数据上进行测试。我们开发的 dataset 和源代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="ECPC-IDS-A-benchmark-endometrail-cancer-PET-CT-image-dataset-for-evaluation-of-semantic-segmentation-and-detection-of-hypermetabolic-regions"><a href="#ECPC-IDS-A-benchmark-endometrail-cancer-PET-CT-image-dataset-for-evaluation-of-semantic-segmentation-and-detection-of-hypermetabolic-regions" class="headerlink" title="ECPC-IDS:A benchmark endometrail cancer PET&#x2F;CT image dataset for evaluation of semantic segmentation and detection of hypermetabolic regions"></a>ECPC-IDS:A benchmark endometrail cancer PET&#x2F;CT image dataset for evaluation of semantic segmentation and detection of hypermetabolic regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08313">http://arxiv.org/abs/2308.08313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dechao Tang, Xuanyi Li, Tianming Du, Deguo Ma, Zhiyu Ma, Hongzan Sun, Marcin Grzegorzek, Huiyan Jiang, Chen Li</li>
<li>for: 这个论文主要目的是提供一个大量多图像的 ENDOMETRIAL CANCER PET&#x2F;CT图像数据集，以便研究人员可以通过计算机助理诊断技术来提高诊断的准确性和 объектив性，同时减轻医生的工作负担。</li>
<li>methods: 这个论文使用了五种经典的深度学习 semantic segmentation 方法和六种深度学习对象检测方法进行测试，以证明不同方法在 ECPC-IDS 上的差异。</li>
<li>results: 这个论文通过 extensively 的实验，demonstrate 了不同方法在 ECPC-IDS 上的性能差异，并证明了这个数据集可以帮助研究人员开发新的算法，以提高计算机助理诊断技术的性能，从而为临床医生和患者带来很大的 benefit.<details>
<summary>Abstract</summary>
Endometrial cancer is one of the most common tumors in the female reproductive system and is the third most common gynecological malignancy that causes death after ovarian and cervical cancer. Early diagnosis can significantly improve the 5-year survival rate of patients. With the development of artificial intelligence, computer-assisted diagnosis plays an increasingly important role in improving the accuracy and objectivity of diagnosis, as well as reducing the workload of doctors. However, the absence of publicly available endometrial cancer image datasets restricts the application of computer-assisted diagnostic techniques.In this paper, a publicly available Endometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation and Detection of Hypermetabolic Regions (ECPC-IDS) are published. Specifically, the segmentation section includes PET and CT images, with a total of 7159 images in multiple formats. In order to prove the effectiveness of segmentation methods on ECPC-IDS, five classical deep learning semantic segmentation methods are selected to test the image segmentation task. The object detection section also includes PET and CT images, with a total of 3579 images and XML files with annotation information. Six deep learning methods are selected for experiments on the detection task.This study conduct extensive experiments using deep learning-based semantic segmentation and object detection methods to demonstrate the differences between various methods on ECPC-IDS. As far as we know, this is the first publicly available dataset of endometrial cancer with a large number of multiple images, including a large amount of information required for image and target detection. ECPC-IDS can aid researchers in exploring new algorithms to enhance computer-assisted technology, benefiting both clinical doctors and patients greatly.
</details>
<details>
<summary>摘要</summary>
《Endometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation and Detection of Hypermetabolic Regions (ECPC-IDS)》Endometrial cancer是女性生殖系统中最常见的肿瘤，也是最常见的女性生殖系统癌症之一，仅次于卵巢和子宫癌。早期诊断可以显著提高病人5年生存率。随着人工智能的发展，计算机协助诊断在提高诊断精度和公正性方面发挥了越来越重要的作用，同时也减轻医生的工作负担。然而， absence of publicly available endometrial cancer image datasets restricts the application of computer-assisted diagnostic techniques。为了解决这个问题，我们在这篇论文中发布了一个公共可用的Endometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation and Detection of Hypermetabolic Regions (ECPC-IDS)。具体来说，分别包括PET和CT图像，共7159张图像，多种格式。为证明ECPC-IDS上 segmentation 方法的效果，我们选择了5种经典的深度学习 semantic segmentation 方法进行测试图像 segmentation 任务。另外， objet detection 部分也包括PET和CT图像，共3579张图像和XML文件中的注释信息。我们选择了6种深度学习方法进行 эксперимент detection 任务。本研究通过使用深度学习基于的semantic segmentation和object detection方法，进行了对ECPC-IDS的广泛实验，以示出不同方法之间的差异。据我们所知，ECPC-IDS是首个公共可用的 endometrial cancer 数据集，包含大量多种图像信息，包括图像和目标检测需要的大量信息。ECPC-IDS 可以帮助研究人员探索新的算法，以提高计算机协助技术，对临床医生和病人都是非常有利。
</details></li>
</ul>
<hr>
<h2 id="OnUVS-Online-Feature-Decoupling-Framework-for-High-Fidelity-Ultrasound-Video-Synthesis"><a href="#OnUVS-Online-Feature-Decoupling-Framework-for-High-Fidelity-Ultrasound-Video-Synthesis" class="headerlink" title="OnUVS: Online Feature Decoupling Framework for High-Fidelity Ultrasound Video Synthesis"></a>OnUVS: Online Feature Decoupling Framework for High-Fidelity Ultrasound Video Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08269">http://arxiv.org/abs/2308.08269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhou, Dong Ni, Ao Chang, Xinrui Zhou, Rusi Chen, Yanlin Chen, Lian Liu, Jiamin Liang, Yuhao Huang, Tong Han, Zhe Liu, Deng-Ping Fan, Xin Yang</li>
<li>for:  This paper aims to address the challenges of synthesizing high-fidelity ultrasound (US) videos for clinical diagnosis, particularly the limited availability of specific US video cases, by presenting a novel online feature-decoupling framework called OnUVS.</li>
<li>methods:  The OnUVS framework uses a weakly-supervised training strategy to introduce anatomic information into keypoint learning, a dual-decoder to decouple content and textural features, and a multiple-feature discriminator to enhance the sharpness and fine details of the generated videos. Additionally, the framework constrains the motion trajectories of keypoints during online learning to enhance the fluidity of the generated videos.</li>
<li>results:  The paper demonstrates the effectiveness of OnUVS in synthesizing US videos with high fidelity through validation and user studies on in-house echocardiographic and pelvic floor US videos.<details>
<summary>Abstract</summary>
Ultrasound (US) imaging is indispensable in clinical practice. To diagnose certain diseases, sonographers must observe corresponding dynamic anatomic structures to gather comprehensive information. However, the limited availability of specific US video cases causes teaching difficulties in identifying corresponding diseases, which potentially impacts the detection rate of such cases. The synthesis of US videos may represent a promising solution to this issue. Nevertheless, it is challenging to accurately animate the intricate motion of dynamic anatomic structures while preserving image fidelity. To address this, we present a novel online feature-decoupling framework called OnUVS for high-fidelity US video synthesis. Our highlights can be summarized by four aspects. First, we introduced anatomic information into keypoint learning through a weakly-supervised training strategy, resulting in improved preservation of anatomical integrity and motion while minimizing the labeling burden. Second, to better preserve the integrity and textural information of US images, we implemented a dual-decoder that decouples the content and textural features in the generator. Third, we adopted a multiple-feature discriminator to extract a comprehensive range of visual cues, thereby enhancing the sharpness and fine details of the generated videos. Fourth, we constrained the motion trajectories of keypoints during online learning to enhance the fluidity of generated videos. Our validation and user studies on in-house echocardiographic and pelvic floor US videos showed that OnUVS synthesizes US videos with high fidelity.
</details>
<details>
<summary>摘要</summary>
超声影像（US）是诊断疾病的不可或缺的工具。sonographers需要观察相应的动态生理结构，以获取全面的信息。然而，有限的特定US视频案例的有效性，使得教学和诊断这些疾病具有挑战性。为解决这个问题，我们提出了一种新的在线特征分离框架 called OnUVS，用于高精度US视频生成。我们的特点包括：1. 通过弱有监督训练策略，将解剖信息引入关键点学习中，以提高动态生理结构的保留和动作，同时减少标注卷积。2. 为了更好地保持US图像的完整性和текстуral信息，我们实现了内容和текстуral特征的解码器。3. 采用多个特征识别器，挖掘更广泛的视觉cue，提高生成视频的锐度和细节。4. 在在线学习中，限制关键点的运动轨迹，以提高生成视频的流畅性。我们的验证和用户研究表明，OnUVS可以生成高精度的US视频。
</details></li>
</ul>
<hr>
<h2 id="Neural-Spherical-Harmonics-for-structurally-coherent-continuous-representation-of-diffusion-MRI-signal"><a href="#Neural-Spherical-Harmonics-for-structurally-coherent-continuous-representation-of-diffusion-MRI-signal" class="headerlink" title="Neural Spherical Harmonics for structurally coherent continuous representation of diffusion MRI signal"></a>Neural Spherical Harmonics for structurally coherent continuous representation of diffusion MRI signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08210">http://arxiv.org/abs/2308.08210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Hendriks, Anna Vilanova, Maxime Chamberland</li>
<li>for: This paper presents a novel method for modeling diffusion magnetic resonance imaging (dMRI) datasets, which leverages the structural coherence of the human brain to improve the accuracy and efficiency of the reconstruction process.</li>
<li>methods: The proposed method uses a neural network to parameterize a spherical harmonics series (NeSH) to represent the dMRI signal of a single subject, continuous in both the angular and spatial domain. The method also utilizes upsampling in both the angular and spatial domain to improve the reconstruction results.</li>
<li>results: The reconstructed dMRI signal using the proposed method shows a more structurally coherent representation of the data, with reduced noise in gradient images and smoother fiber orientation distribution functions. The method also enables the calculation of mean diffusivity, fractional anisotropy, and total apparent fiber density with a single model architecture and minimal hyperparameter tuning.<details>
<summary>Abstract</summary>
We present a novel way to model diffusion magnetic resonance imaging (dMRI) datasets, that benefits from the structural coherence of the human brain while only using data from a single subject. Current methods model the dMRI signal in individual voxels, disregarding the intervoxel coherence that is present. We use a neural network to parameterize a spherical harmonics series (NeSH) to represent the dMRI signal of a single subject from the Human Connectome Project dataset, continuous in both the angular and spatial domain. The reconstructed dMRI signal using this method shows a more structurally coherent representation of the data. Noise in gradient images is removed and the fiber orientation distribution functions show a smooth change in direction along a fiber tract. We showcase how the reconstruction can be used to calculate mean diffusivity, fractional anisotropy, and total apparent fiber density. These results can be achieved with a single model architecture, tuning only one hyperparameter. In this paper we also demonstrate how upsampling in both the angular and spatial domain yields reconstructions that are on par or better than existing methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于模型Diffusion Magnetic Resonance Imaging（dMRI）数据集，该方法利用人脑的结构减噪。现有方法通常在个体粒子上模型dMRI信号，忽略了各个粒子之间的协调性。我们使用神经网络来参数化球面幂数列（NeSH）来表示单个参与者的dMRI信号，这种表示方式是连续的在角度和空间领域。重建的dMRI信号表现出了更好的结构减噪，排除了梯度图像中的噪声，并且纤维方向分布函数（fOD）示出了细胞轴的平滑变化。我们还示出了如何使用这种重建方法来计算平均扩散率、有效扩散率和总显示纤维 densidad。这些结果可以通过单个模型架构和一个超参数来实现。此外，我们还证明了在角度和空间领域进行upsampling可以实现重建的结果与现有方法相当或更好。
</details></li>
</ul>
<hr>
<h2 id="Self-Reference-Deep-Adaptive-Curve-Estimation-for-Low-Light-Image-Enhancement"><a href="#Self-Reference-Deep-Adaptive-Curve-Estimation-for-Low-Light-Image-Enhancement" class="headerlink" title="Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement"></a>Self-Reference Deep Adaptive Curve Estimation for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08197">http://arxiv.org/abs/2308.08197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/john-venti/self-dace">https://github.com/john-venti/self-dace</a></li>
<li>paper_authors: Jianyu Wen, Chenhao Wu, Tong Zhang, Yixuan Yu, Piotr Swierczynski</li>
<li>for: 提高低光照图像的显示质量</li>
<li>methods: 使用自适应曲线估计和降噪网络对低光照图像进行两个阶段进行优化</li>
<li>results: 与现有状态艺图像处理算法进行比较，该方法在多个实际数据集上表现出优于其他方法的性能<details>
<summary>Abstract</summary>
In this paper, we propose a 2-stage low-light image enhancement method called Self-Reference Deep Adaptive Curve Estimation (Self-DACE). In the first stage, we present an intuitive, lightweight, fast, and unsupervised luminance enhancement algorithm. The algorithm is based on a novel low-light enhancement curve that can be used to locally boost image brightness. We also propose a new loss function with a simplified physical model designed to preserve natural images' color, structure, and fidelity. We use a vanilla CNN to map each pixel through deep Adaptive Adjustment Curves (AAC) while preserving the local image structure. Secondly, we introduce the corresponding denoising scheme to remove the latent noise in the darkness. We approximately model the noise in the dark and deploy a Denoising-Net to estimate and remove the noise after the first stage. Exhaustive qualitative and quantitative analysis shows that our method outperforms existing state-of-the-art algorithms on multiple real-world datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种两stage的低光照图像增强方法，称为Self-Reference Deep Adaptive Curve Estimation（Self-DACE）。在第一个阶段，我们提供了一种直观、轻量级、快速和无监督的亮度增强算法。该算法基于一个新的低光照增强曲线，可以地方性地提高图像亮度。我们还提出了一个新的损失函数，用于保持自然图像的颜色、结构和准确性。我们使用了一个普通的 convolutional neural network（CNN）来将每个像素通过深度适应曲线（AAC）进行映射，同时保持图像的本地结构。在第二个阶段，我们引入了相应的干扰除方法，以除除在黑暗中存在的隐藏噪声。我们简化了噪声的模型，并使用了一个Denoising-Net来估计和除去噪声。我们对多个实际世界数据集进行了详细的 качеitative和量化分析，结果表明，我们的方法在与现有状态的艺术算法进行比较时表现出色。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Perceptual-Quality-Preserving-Image-Compression"><a href="#Conditional-Perceptual-Quality-Preserving-Image-Compression" class="headerlink" title="Conditional Perceptual Quality Preserving Image Compression"></a>Conditional Perceptual Quality Preserving Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08154">http://arxiv.org/abs/2308.08154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongda Xu, Qian Zhang, Yanghao Li, Dailan He, Zhe Wang, Yuanyuan Wang, Hongwei Qin, Yan Wang, Jingjing Liu, Ya-Qin Zhang</li>
<li>for: 本文提出了基于用户定义信息的可conditioned感知质量（CPQ），用于保持高质量和semantic质量在各比特率下。</li>
<li>methods: 本文基于用户定义信息conditioning的感知质量，通过对比原始图像和重建图像的 divergence 来定义CPQ。</li>
<li>results: 实验结果表明，我们的代码可以成功保持高质量和semantic质量，并且提供了对共同随机性的下界，解决了过去关于是否应该在生成器中包含随机性以提高（conditional）感知质量压缩的辩论。<details>
<summary>Abstract</summary>
We propose conditional perceptual quality, an extension of the perceptual quality defined in \citet{blau2018perception}, by conditioning it on user defined information. Specifically, we extend the original perceptual quality $d(p_{X},p_{\hat{X}})$ to the conditional perceptual quality $d(p_{X|Y},p_{\hat{X}|Y})$, where $X$ is the original image, $\hat{X}$ is the reconstructed, $Y$ is side information defined by user and $d(.,.)$ is divergence. We show that conditional perceptual quality has similar theoretical properties as rate-distortion-perception trade-off \citep{blau2019rethinking}. Based on these theoretical results, we propose an optimal framework for conditional perceptual quality preserving compression. Experimental results show that our codec successfully maintains high perceptual quality and semantic quality at all bitrate. Besides, by providing a lowerbound of common randomness required, we settle the previous arguments on whether randomness should be incorporated into generator for (conditional) perceptual quality compression. The source code is provided in supplementary material.
</details>
<details>
<summary>摘要</summary>
我们提出了基于用户定义信息的conditional perceptual quality，具体来说是将原始的perceptual quality $d(p_{X},p_{\hat{X}})$扩展为 $d(p_{X|Y},p_{\hat{X}|Y})$,其中$X$是原始图像， $\hat{X}$是重建图像，$Y$是用户定义的侧信息。我们证明了conditional perceptual quality具有类似的理论性质，与rate-distortion-perception trade-off的交互作用。基于这些理论结论，我们提出了一个优化的conditional perceptual quality保持压缩框架。实验结果表明，我们的编码器能够保持高度的感知质量和semantic质量，并且提供了对公共随机性的下界，从而解决了过去的争议是否应该在生成器中添加随机性以实现（conditional）perceptual quality压缩。代码在补充材料中提供。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Overview-of-Computational-Nuclei-Segmentation-Methods-in-Digital-Pathology"><a href="#A-Comprehensive-Overview-of-Computational-Nuclei-Segmentation-Methods-in-Digital-Pathology" class="headerlink" title="A Comprehensive Overview of Computational Nuclei Segmentation Methods in Digital Pathology"></a>A Comprehensive Overview of Computational Nuclei Segmentation Methods in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08112">http://arxiv.org/abs/2308.08112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Magoulianitis, Catherine A. Alexander, C. -C. Jay Kuo</li>
<li>for: 本文主要用于概述肿瘤诊断领域中数字patology在诊断、分期和分级等方面的应用。</li>
<li>methods: 本文使用了现代人工智能（AI）模型来自动实现核体分 segmentation，从传统图像处理技术到深度学习（DL） paradigm。</li>
<li>results: 本文提供了一个广泛的回顾，从早期使用传统图像处理技术到现代approaches，并讨论了弱监督问题的优势、不同模型的优劣点以及未来研究方向。<details>
<summary>Abstract</summary>
In the cancer diagnosis pipeline, digital pathology plays an instrumental role in the identification, staging, and grading of malignant areas on biopsy tissue specimens. High resolution histology images are subject to high variance in appearance, sourcing either from the acquisition devices or the H\&E staining process. Nuclei segmentation is an important task, as it detects the nuclei cells over background tissue and gives rise to the topology, size, and count of nuclei which are determinant factors for cancer detection. Yet, it is a fairly time consuming task for pathologists, with reportedly high subjectivity. Computer Aided Diagnosis (CAD) tools empowered by modern Artificial Intelligence (AI) models enable the automation of nuclei segmentation. This can reduce the subjectivity in analysis and reading time. This paper provides an extensive review, beginning from earlier works use traditional image processing techniques and reaching up to modern approaches following the Deep Learning (DL) paradigm. Our review also focuses on the weak supervision aspect of the problem, motivated by the fact that annotated data is scarce. At the end, the advantages of different models and types of supervision are thoroughly discussed. Furthermore, we try to extrapolate and envision how future research lines will potentially be, so as to minimize the need for labeled data while maintaining high performance. Future methods should emphasize efficient and explainable models with a transparent underlying process so that physicians can trust their output.
</details>
<details>
<summary>摘要</summary>
在肿瘤诊断管线中，数字patology扮演了重要的角色，用于识别、分期和评分肿瘤区域的各种生物标本样本。高分辨率历史图像具有高变异性，可能来自获取设备或H\&E染色过程。核仁分 segmentation是一项重要任务，因为它可以在背景组织背景下检测核仁细胞，并且对肿瘤检测有决定性作用。然而，这是一项较为时间consuming的任务， pathologists 报告了高度主观性。使用现代人工智能（AI）模型的计算支持工具（CAD）可以自动实现核仁分 segmentation，从而减少分析和阅读时间的主观性。本文提供了广泛的综述，从传统图像处理技术开始，沿着深度学习（DL） paradigm 进行到现代方法。我们的综述还专注于弱级指导问题，因为标注数据scarce。文章结束时，我们详细讨论了不同模型和类型的supervision的优势。此外，我们尝试预测未来研究的发展趋势，以减少标注数据的需求，同时保持高性能。未来的方法应该强调高效可解释的模型，并且具有透明的下面过程，以便physicians可以信任其输出。
</details></li>
</ul>
<hr>
<h2 id="Snapshot-High-Dynamic-Range-Imaging-with-a-Polarization-Camera"><a href="#Snapshot-High-Dynamic-Range-Imaging-with-a-Polarization-Camera" class="headerlink" title="Snapshot High Dynamic Range Imaging with a Polarization Camera"></a>Snapshot High Dynamic Range Imaging with a Polarization Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08094">http://arxiv.org/abs/2308.08094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Intelligent-Sensing/polarization-hdr">https://github.com/Intelligent-Sensing/polarization-hdr</a></li>
<li>paper_authors: Mingyang Xie, Matthew Chan, Christopher Metzler</li>
<li>for: 这篇论文的目的是把一个常见的 polarization 相机转化为高性能的 HDR 相机。</li>
<li>methods: 这篇论文使用了一种简单 yet 高效的方法，通过在 polarization 相机前置一个线性激光 polarizer，同时捕捉四个不同曝光的图像，并使用一种 robust 和自适应的算法来重建 HDR 图像（具有单一的极性）。</li>
<li>results: 论文的实验结果表明，这种方法可以有效地提高 HDR 图像的质量，并且可以在实际世界中进行广泛的应用。<details>
<summary>Abstract</summary>
High dynamic range (HDR) images are important for a range of tasks, from navigation to consumer photography. Accordingly, a host of specialized HDR sensors have been developed, the most successful of which are based on capturing variable per-pixel exposures. In essence, these methods capture an entire exposure bracket sequence at once in a single shot. This paper presents a straightforward but highly effective approach for turning an off-the-shelf polarization camera into a high-performance HDR camera. By placing a linear polarizer in front of the polarization camera, we are able to simultaneously capture four images with varied exposures, which are determined by the orientation of the polarizer. We develop an outlier-robust and self-calibrating algorithm to reconstruct an HDR image (at a single polarity) from these measurements. Finally, we demonstrate the efficacy of our approach with extensive real-world experiments.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）图像在多种任务中具有重要作用，从导航到消费类摄影。因此，一系列专门设计 дляHDR感知器被开发出来，最成功的是基于变量每像素曝光的方法。在本文中，我们提出了将普通的偏振相机转化为高性能HDR摄影机的简单 yet highly effectiveapproach。通过在偏振相机前置一个直线偏振器，我们能够同时捕捉四个不同曝光的图像，这些图像的曝光是偏振器的 orientations 所决定的。我们开发了一种耐异常和自适应算法，用于从这些测量中重建HDR图像（在单一的偏振 polarity 下）。最后，我们通过广泛的实验证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Framework-for-Spleen-Volume-Estimation-from-2D-Cross-sectional-Views"><a href="#Deep-Learning-Framework-for-Spleen-Volume-Estimation-from-2D-Cross-sectional-Views" class="headerlink" title="Deep Learning Framework for Spleen Volume Estimation from 2D Cross-sectional Views"></a>Deep Learning Framework for Spleen Volume Estimation from 2D Cross-sectional Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08038">http://arxiv.org/abs/2308.08038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Yuan, Esther Puyol-Anton, Haran Jogeesvaran, Baba Inusa, Andrew P. King</li>
<li>for: This paper aims to develop a method for automated spleen volume measurement from 2D cross-sectional segmentations obtained from ultrasound imaging, which can be used to assess splenomegaly and related clinical conditions.</li>
<li>methods: The proposed method uses a variational autoencoder-based framework to measure spleen volume from single- or dual-view 2D spleen segmentations. Three volume estimation methods are proposed and evaluated within this framework.</li>
<li>results: The best model achieved mean relative volume accuracies of 86.62% and 92.58% for single- and dual-view segmentations, respectively, surpassing the performance of the clinical standard approach of linear regression using manual measurements and a comparative deep learning-based 2D-3D reconstruction-based approach. The proposed method can be integrated into standard clinical workflows which currently use 2D ultrasound images to measure spleen length.<details>
<summary>Abstract</summary>
Abnormal spleen enlargement (splenomegaly) is regarded as a clinical indicator for a range of conditions, including liver disease, cancer and blood diseases. While spleen length measured from ultrasound images is a commonly used surrogate for spleen size, spleen volume remains the gold standard metric for assessing splenomegaly and the severity of related clinical conditions. Computed tomography is the main imaging modality for measuring spleen volume, but it is less accessible in areas where there is a high prevalence of splenomegaly (e.g., the Global South). Our objective was to enable automated spleen volume measurement from 2D cross-sectional segmentations, which can be obtained from ultrasound imaging. In this study, we describe a variational autoencoder-based framework to measure spleen volume from single- or dual-view 2D spleen segmentations. We propose and evaluate three volume estimation methods within this framework. We also demonstrate how 95% confidence intervals of volume estimates can be produced to make our method more clinically useful. Our best model achieved mean relative volume accuracies of 86.62% and 92.58% for single- and dual-view segmentations, respectively, surpassing the performance of the clinical standard approach of linear regression using manual measurements and a comparative deep learning-based 2D-3D reconstruction-based approach. The proposed spleen volume estimation framework can be integrated into standard clinical workflows which currently use 2D ultrasound images to measure spleen length. To the best of our knowledge, this is the first work to achieve direct 3D spleen volume estimation from 2D spleen segmentations.
</details>
<details>
<summary>摘要</summary>
非常常见的脾膜肥大（splenomegaly）被视为临床指标，用于诊断多种疾病，如肝病、癌症和血液疾病。而脾膜长度从ultrasound图像中得到的是通常用于脾膜大小的临床标准，但脾膜体积仍是评估splenomegaly和相关临床病情的金标准指标。计算机断层成像是评估脾膜体积的主要成像方法，但在有高发率的脾膜肥大（如全球南部）的地区，计算机断层成像更加不可accessible。我们的目标是启用自动化脾膜体积量计算，从2D横截图像中获得的分割。我们描述了基于variational autoencoder的框架，用于从单个或双视2D脾膜分割中计算脾膜体积。我们提出了三种体积估计方法，并评估了这些方法的性能。我们还示出了在95%信度范围内生成体积估计的方法，以使我们的方法更加临床有用。我们的最佳模型在单视和双视分割中达到了86.62%和92.58%的相对体积准确率，超过了临床标准方法的线性回归使用手动测量和相比之下的深度学习基于2D-3D重建的方法。我们的提议的脾膜体积估计框架可以与现有的仅使用2D ultrasound图像来测量脾膜长度的临床工作流程集成。到目前为止，这是首次直接从2D脾膜分割中 estimate 3D脾膜体积的方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/16/eess.IV_2023_08_16/" data-id="clly4xtgb00f9vl88fw3hfi14" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/16/eess.AS_2023_08_16/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-08-16 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/15/cs.SD_2023_08_15/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-15</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">108</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
