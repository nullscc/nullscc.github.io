
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-11 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Towards Packaging Unit Detection for Automated Palletizing Tasks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06306 repo_url: None paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-11 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/11/eess.IV_2023_08_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Towards Packaging Unit Detection for Automated Palletizing Tasks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06306 repo_url: None paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-10T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:28.731Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/eess.IV_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-11 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks"><a href="#Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks" class="headerlink" title="Towards Packaging Unit Detection for Automated Palletizing Tasks"></a>Towards Packaging Unit Detection for Automated Palletizing Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06306">http://arxiv.org/abs/2308.06306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann</li>
<li>for: 本研究旨在解决工业机器人处理包装单位前的检测步骤，即检测包装单位的具体步骤。</li>
<li>methods: 本研究使用synthetically生成的数据进行完全训练，可以对实际世界的包装单位进行Robust应用而无需进一步训练或设置繁重。该方法可以处理稀疏和低质量的感知数据，可以利用可用的先验知识，并在各种产品和应用场景中广泛适用。</li>
<li>results: 我们对实际世界数据进行了广泛评估，证明了我们的方法可以对各种不同的零售产品进行实际应用。此外，我们还将该方法integrated into a lab demonstrator，并通过工业伙伴将其商业化。<details>
<summary>Abstract</summary>
For various automated palletizing tasks, the detection of packaging units is a crucial step preceding the actual handling of the packaging units by an industrial robot. We propose an approach to this challenging problem that is fully trained on synthetically generated data and can be robustly applied to arbitrary real world packaging units without further training or setup effort. The proposed approach is able to handle sparse and low quality sensor data, can exploit prior knowledge if available and generalizes well to a wide range of products and application scenarios. To demonstrate the practical use of our approach, we conduct an extensive evaluation on real-world data with a wide range of different retail products. Further, we integrated our approach in a lab demonstrator and a commercial solution will be marketed through an industrial partner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Self-supervised-SAR-Image-Despeckling-Strategy-Based-on-Parameter-sharing-Convolutional-Neural-Networks"><a href="#A-Self-supervised-SAR-Image-Despeckling-Strategy-Based-on-Parameter-sharing-Convolutional-Neural-Networks" class="headerlink" title="A Self-supervised SAR Image Despeckling Strategy Based on Parameter-sharing Convolutional Neural Networks"></a>A Self-supervised SAR Image Despeckling Strategy Based on Parameter-sharing Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05975">http://arxiv.org/abs/2308.05975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Chen, Yifei Yin, Hao Shi, Qingqing Sheng, Wei Li<br>for: The paper is written to address the challenges of SAR image despeckling, specifically the lack of speckle-free SAR images for training deep learning models.methods: The paper proposes a self-supervised SAR despeckling strategy that uses a sub-sampler based on the adjacent-syntropy criteria to generate training image pairs from real-world SAR images. The proposed method also uses parameter sharing convolutional neural networks and a multi-feature loss function to improve the preservation of edges and textures in the despeckled images.results: The proposed method is validated on real-world SAR images and shows better performance than several advanced SAR image despeckling methods, with improved preservation of edges and textures.Here’s the simplified Chinese text for the three information points:for: 本文是为了解决 SAR 图像抑杂问题，具体来说是为了缺乏 SAR 图像的杂点自由样本来训练深度学习模型。methods: 本文提出了一种基于邻域同异性 criteria 的自我监督 SAR 抑杂策略，使用这种策略可以从实际 SAR 图像中生成训练对数据。同时，本方法使用共享参数 convolutional neural networks 和多个特征损失函数，以提高杂点自由样本中的边缘和Texture特征保持。results: 提出的方法在实际 SAR 图像上进行了证明，与一些先进的 SAR 图像抑杂方法相比，显示更好的表现，同时保持了边缘和Texture特征。<details>
<summary>Abstract</summary>
Speckle noise is generated due to the SAR imaging mechanism, which brings difficulties in SAR image interpretation. Hence, despeckling is a helpful step in SAR pre-processing. Nowadays, deep learning has been proved to be a progressive method for SAR image despeckling. Most deep learning methods for despeckling are based on supervised learning, which needs original SAR images and speckle-free SAR images to train the network. However, the speckle-free SAR images are generally not available. So, this issue was tackled by adding multiplicative noise to optical images synthetically for simulating speckled image. Therefore, there are following challenges in SAR image despeckling: (1) lack of speckle-free SAR image; (2) difficulty in keeping details such as edges and textures in heterogeneous areas. To address these issues, we propose a self-supervised SAR despeckling strategy that can be trained without speckle-free images. Firstly, the feasibility of SAR image despeckling without speckle-free images is proved theoretically. Then, the sub-sampler based on the adjacent-syntropy criteria is proposed. The training image pairs are generated by the sub-sampler from real-word SAR image to estimate the noise distribution. Furthermore, to make full use of training pairs, the parameter sharing convolutional neural networks are adopted. Finally, according to the characteristics of SAR images, a multi-feature loss function is proposed. The proposed loss function is composed of despeckling term, regular term and perception term, to constrain the gap between the generated paired images. The ability of edge and texture feature preserving is improved simultaneously. Finally, qualitative and quantitative experiments are validated on real-world SAR images, showing better performances than several advanced SAR image despeckling methods.
</details>
<details>
<summary>摘要</summary>
亮点噪声是由SAR成像机制产生的，导致SAR图像解读困难。因此，去掉亮点噪声是SAR预处理的有助步骤。目前，深度学习已经被证明是SAR图像去掉亮点噪声的进步方法。大多数深度学习方法是基于直接学习，需要原始SAR图像和噪声自由SAR图像来训练网络。然而，噪声自由SAR图像通常不可获得。因此，这个问题是通过人工添加光学图像中的multiplicative噪声来解决的。因此，SAR图像去掉亮点噪声面临以下挑战：（1）缺乏噪声自由SAR图像；（2）在不同区域中保持细节，如边缘和文本ure。为解决这些问题，我们提出了一种自动化SAR图像去掉亮点噪声策略，不需要噪声自由SAR图像。首先，我们证明了SAR图像去掉亮点噪声可行性。然后，我们提出了基于邻域合理性 criteria的子批量器。通过这个子批量器，从实际世界SAR图像中生成训练图像对。然后，为了充分利用训练对，我们采用了参数共享卷积神经网络。最后，根据SAR图像的特点，我们提出了多特征损失函数。该损失函数由去掉亮点噪声项、常规项和感知项组成，以避免训练对的差异。同时，我们改进了边缘和文本ure的特征保持能力。最后，我们对实际世界SAR图像进行了质量和kvantalitative实验，并证明了我们的方法在去掉亮点噪声方面的更好性。
</details></li>
</ul>
<hr>
<h2 id="Classification-Method-of-Road-Surface-Condition-and-Type-with-LiDAR-Using-Spatiotemporal-Information"><a href="#Classification-Method-of-Road-Surface-Condition-and-Type-with-LiDAR-Using-Spatiotemporal-Information" class="headerlink" title="Classification Method of Road Surface Condition and Type with LiDAR Using Spatiotemporal Information"></a>Classification Method of Road Surface Condition and Type with LiDAR Using Spatiotemporal Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05965">http://arxiv.org/abs/2308.05965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Won Seo, Jin Sung Kim, Chung Choo Chung</li>
<li>for: 这个论文提出了一种基于LiDAR的道路表面条件和类型分类方法，用于实时识别道路表面的状况和类型。</li>
<li>methods: 该方法使用了深度神经网络（DNN）来分类道路表面的不同区域，首先构建了每个子区域的特征向量，然后使用DNN进行分类。最后，输出的DNN结果通过空间时间处理得到了最终的分类结果，考虑了车速和概率等因素。</li>
<li>results: 与其他五种算法进行比较研究后，该方法在两个子区域附近车速的情况下获得了最高的准确率为98.0%和98.6%。此外，该方法还在实时环境中验证了可行性。<details>
<summary>Abstract</summary>
This paper proposes a spatiotemporal architecture with a deep neural network (DNN) for road surface conditions and types classification using LiDAR. It is known that LiDAR provides information on the reflectivity and number of point clouds depending on a road surface. Thus, this paper utilizes the information to classify the road surface. We divided the front road area into four subregions. First, we constructed feature vectors using each subregion's reflectivity, number of point clouds, and in-vehicle information. Second, the DNN classifies road surface conditions and types for each subregion. Finally, the output of the DNN feeds into the spatiotemporal process to make the final classification reflecting vehicle speed and probability given by the outcomes of softmax functions of the DNN output layer. To validate the effectiveness of the proposed method, we performed a comparative study with five other algorithms. With the proposed DNN, we obtained the highest accuracy of 98.0\% and 98.6\% for two subregions near the vehicle. In addition, we implemented the proposed method on the Jetson TX2 board to confirm that it is applicable in real-time.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文提出了一种使用LiDAR的深度神经网络（DNN）来分类道路表面条件和类型。LiDAR提供了道路表面反射率和点云数据的信息，因此这篇论文利用这些信息来分类道路表面。前方道路区分成四个子区域，并构建了每个子区域的特征向量，包括反射率、点云数量和车辆内部信息。然后，DNN将每个子区域的特征向量分类为道路表面条件和类型。最后，DNN的输出Feed into spatiotemporal processto make the final classification, considering the vehicle speed and probability output by the softmax function of the DNN output layer. To validate the effectiveness of the proposed method, a comparative study with five other algorithms was conducted, and the DNN achieved the highest accuracy of 98.0% and 98.6% for two subregions near the vehicle. In addition, the proposed method was implemented on the Jetson TX2 board to confirm its applicability in real-time.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge"><a href="#Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge" class="headerlink" title="Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge"></a>Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05862">http://arxiv.org/abs/2308.05862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junma11/flare">https://github.com/junma11/flare</a></li>
<li>paper_authors: Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, Xin Yang, Ziyan Huang, Fan Zhang, Wentao Liu, YuanKe Pan, Shoujin Huang, Jiacheng Wang, Mingze Sun, Weixin Xu, Dengqiang Jia, Jae Won Choi, Natália Alves, Bram de Wilde, Gregor Koehler, Yajun Wu, Manuel Wiesenfarth, Qiongjie Zhu, Guoqiang Dong, Jian He, the FLARE Challenge Consortium, Bo Wang</li>
<li>for:  This paper aims to evaluate the accuracy and efficiency of artificial intelligence (AI) algorithms in automated abdominal disease diagnosis and treatment planning.</li>
<li>methods:  The authors organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. They constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers.</li>
<li>results:  The best-performing algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0% on a holdout external validation set, and successfully generalized to different cohorts from North America, Europe, and Asia. They also enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements.<details>
<summary>Abstract</summary>
Quantitative organ assessment is an essential step in automated abdominal disease diagnosis and treatment planning. Artificial intelligence (AI) has shown great potential to automatize this process. However, most existing AI algorithms rely on many expert annotations and lack a comprehensive evaluation of accuracy and efficiency in real-world multinational settings. To overcome these limitations, we organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. We constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers. We independently validated that a set of AI algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0\% by using 50 labeled scans and 2000 unlabeled scans, which can significantly reduce annotation requirements. The best-performing algorithms successfully generalized to holdout external validation sets, achieving a median DSC of 89.5\%, 90.9\%, and 88.3\% on North American, European, and Asian cohorts, respectively. They also enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements. This opens the potential to use unlabeled data to boost performance and alleviate annotation shortages for modern AI models.
</details>
<details>
<summary>摘要</summary>
“量化器官评估是自动化腹部疾病诊断和治疗规划的关键步骤。人工智能（AI）已经表现出很大的潜力来自动化这个过程。然而，现有的大多数AI算法都需要许多专家标注，并且缺乏真实世界多国场景下的全面评估精度和效率。为了解决这些限制，我们在FLARE 2022挑战中组织了最大的腹部器官分析挑战，以测试快速、低资源、准确、标注效率高和泛化AI算法。我们构建了跨大陆和多国的数据集，包括不同的种族、疾病、阶段和生产商的计算Tomography（CT）扫描。我们独立验证了一组AI算法在50个医疗机构的数据上实现了 médiane的 dice相似度系数（DSC）90.0%，使用50个标注扫描和2000个无标注扫描。这些算法可以减少标注需求。最佳算法成功泛化到外部验证集上，实现了 médiane的 DSC为89.5%、90.9%和88.3%在北美、欧洲和亚洲相应的协会上。它们还允许自动提取关键器官生物特征，这是传统的手动测量强度劳紧的。这开启了使用无标注数据来提高性能的可能性，并alleviate标注不足的问题 для现代AI模型。”
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Optimization-of-JPEG-Based-Deep-Learning-Process-for-Image-Classification"><a href="#End-to-End-Optimization-of-JPEG-Based-Deep-Learning-Process-for-Image-Classification" class="headerlink" title="End-to-End Optimization of JPEG-Based Deep Learning Process for Image Classification"></a>End-to-End Optimization of JPEG-Based Deep Learning Process for Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05840">http://arxiv.org/abs/2308.05840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Qi, Lahiru D. Chamain, Zhi Ding</li>
<li>for: 这篇论文应用于图像分类任务中的分布式学习，需要有效地将图像压缩成码码装置在低成本的感知设备上，以便优化传输和储存。</li>
<li>methods: 本研究提出了一个统一的端到端训练模型，结合了JPEG图像压缩器和深度学习（DL）的分类器。这个模型可以调整通过大量部署的JPEG编码器设置，以提高分类精度，同时考虑到带宽限制。</li>
<li>results: 我们在CIFAR-100和ImageNet上进行了测试，结果显示，这个模型可以提高验证精度，比于预先设定的JPEG配置。<details>
<summary>Abstract</summary>
Among major deep learning (DL) applications, distributed learning involving image classification require effective image compression codecs deployed on low-cost sensing devices for efficient transmission and storage. Traditional codecs such as JPEG designed for perceptual quality are not configured for DL tasks. This work introduces an integrative end-to-end trainable model for image compression and classification consisting of a JPEG image codec and a DL-based classifier. We demonstrate how this model can optimize the widely deployed JPEG codec settings to improve classification accuracy in consideration of bandwidth constraint. Our tests on CIFAR-100 and ImageNet also demonstrate improved validation accuracy over preset JPEG configuration.
</details>
<details>
<summary>摘要</summary>
中文简体深度学习（DL）应用中的分布式学习，涉及到图像分类，需要有效的图像压缩编码器在低成本感知设备上部署，以提高传输和存储的效率。传统的编码器如JPEG，设计为 perceive 质量，并不适用于 DL 任务。这项工作介绍了一种综合性的末端到终端可调模型，包括 JPEG 图像编码器和基于 DL 的分类器。我们示出了如何这种模型可以优化广泛部署的 JPEG 编码器设置，以提高分类精度，同时考虑带宽约束。我们在 CIFAR-100 和 ImageNet 上进行了测试，并证明了该模型在预设 JPEG 配置下有更高的验证精度。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology"><a href="#Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology" class="headerlink" title="Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology"></a>Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06288">http://arxiv.org/abs/2308.06288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/spatial_pathomics">https://github.com/hrlblab/spatial_pathomics</a></li>
<li>paper_authors: Jiayuan Chen, Yu Wang, Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Yilin Liu, Jianyong Zhong, Agnes B. Fogo, Haichun Yang, Shilin Zhao, Yuankai Huo<br>for:这篇论文的目的是为了开发一个用于评估肾脏病变的新型工具kit，帮助研究人员更好地评估肾脏细胞的形态特征。methods:这篇论文使用的方法包括：一、实例对象分割，帮助精准地 identific podocyte核oid; 二、pathomics特征生成，从分割后的核oid中提取了一系列量化特征; 三、robust统计分析，以便对这些量化特征进行全面的空间分析。results:这篇论文通过使用SPT工具kit，成功地提取和分析了许多podocyte形态特征，并通过统计分析发现了许多与肾脏病变相关的空间特征。<details>
<summary>Abstract</summary>
Podocytes, specialized epithelial cells that envelop the glomerular capillaries, play a pivotal role in maintaining renal health. The current description and quantification of features on pathology slides are limited, prompting the need for innovative solutions to comprehensively assess diverse phenotypic attributes within Whole Slide Images (WSIs). In particular, understanding the morphological characteristics of podocytes, terminally differentiated glomerular epithelial cells, is crucial for studying glomerular injury. This paper introduces the Spatial Pathomics Toolkit (SPT) and applies it to podocyte pathomics. The SPT consists of three main components: (1) instance object segmentation, enabling precise identification of podocyte nuclei; (2) pathomics feature generation, extracting a comprehensive array of quantitative features from the identified nuclei; and (3) robust statistical analyses, facilitating a comprehensive exploration of spatial relationships between morphological and spatial transcriptomics features.The SPT successfully extracted and analyzed morphological and textural features from podocyte nuclei, revealing a multitude of podocyte morphomic features through statistical analysis. Additionally, we demonstrated the SPT's ability to unravel spatial information inherent to podocyte distribution, shedding light on spatial patterns associated with glomerular injury. By disseminating the SPT, our goal is to provide the research community with a powerful and user-friendly resource that advances cellular spatial pathomics in renal pathology. The implementation and its complete source code of the toolkit are made openly accessible at https://github.com/hrlblab/spatial_pathomics.
</details>
<details>
<summary>摘要</summary>
PODO细胞，特殊化的胶质细胞，环绕肾脏小血管，对肾健康具有重要作用。现有的描述和量化方法有限，需要创新的解决方案来全面评估多样性特征。特别是理解PODO细胞的形态特征，对研究肾脏伤害非常重要。本文介绍了SPT工具箱（Spatial Pathomics Toolkit），并应用于PODO细胞pathomics。SPT包括三个主要组成部分：（1）实体对象分割，准确地识别PODO细胞核心；（2）pathomics特征生成，从分割的PODO细胞核心中提取广泛的量化特征；以及（3）Robust统计分析，促进了广泛的空间关系探索。SPT成功地提取和分析PODO细胞形态和文化特征，揭示了许多PODO细胞形态特征，并通过统计分析，探索了PODO细胞分布的空间信息。我们的目标是通过散布SPT，为研究社区提供一个强大且易用的资源，以提高细胞空间patomics在肾 pathology中的应用。SPT的实现和完整的源代码在https://github.com/hrlblab/spatial_pathomics上公开访问。
</details></li>
</ul>
<hr>
<h2 id="Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning"><a href="#Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning" class="headerlink" title="Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning"></a>Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05785">http://arxiv.org/abs/2308.05785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyuan Li, Ruining Deng, Yucheng Tang, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for: 这个研究的目的是为了开发一个能够实现多种细胞类型的识别，并且可以避免专家 annotators 的时间负担的方法。</li>
<li>methods: 这个研究使用了一种名为 Segment Anything Model (SAM) 的新型模型，它可以将单一的矩形标注转换为多个精确的像素标注。这些 SAM-generated 标注可以用于训练一个深度学习模型，以进行细胞类型的分类。</li>
<li>results: 研究发现，使用 SAM-assisted molecular-empowered learning (SAM-L) 可以将这些细胞类型的标注工作从专家 annotators 转移到非专家 annotators 身上，而且不需要专家 annotators 的时间负担。此外，SAM-L 可以保持 annotation accuracy 和深度学习模型的性能。这个研究代表了一个重要的进步，可以帮助普及化细胞类型的标注过程，并且仅需非专家 annotators 的帮助。<details>
<summary>Abstract</summary>
Precise identification of multiple cell classes in high-resolution Giga-pixel whole slide imaging (WSI) is critical for various clinical scenarios. Building an AI model for this purpose typically requires pixel-level annotations, which are often unscalable and must be done by skilled domain experts (e.g., pathologists). However, these annotations can be prone to errors, especially when distinguishing between intricate cell types (e.g., podocytes and mesangial cells) using only visual inspection. Interestingly, a recent study showed that lay annotators, when using extra immunofluorescence (IF) images for reference (referred to as molecular-empowered learning), can sometimes outperform domain experts in labeling. Despite this, the resource-intensive task of manual delineation remains a necessity during the annotation process. In this paper, we explore the potential of bypassing pixel-level delineation by employing the recent segment anything model (SAM) on weak box annotation in a zero-shot learning approach. Specifically, we harness SAM's ability to produce pixel-level annotations from box annotations and utilize these SAM-generated labels to train a segmentation model. Our findings show that the proposed SAM-assisted molecular-empowered learning (SAM-L) can diminish the labeling efforts for lay annotators by only requiring weak box annotations. This is achieved without compromising annotation accuracy or the performance of the deep learning-based segmentation. This research represents a significant advancement in democratizing the annotation process for training pathological image segmentation, relying solely on non-expert annotators.
</details>
<details>
<summary>摘要</summary>
高分辨率整幕扫描图像（WSI）中多个细胞类型的精确识别是许多临床应用场景中的关键。建立一个人工智能模型用于此目的通常需要像素级别的注释，但这些注释通常是不可扩展的，而且需要具有专业知识的域专家（例如病理学家）进行完成。然而，这些注释可能会受到误差的影响，特别是在使用 только视觉检查时分辨between intricate cell types（例如 podocytes 和 mesangial cells）。有趣的是，一项最近的研究发现，使用extra immunofluorescence（IF）图像作为参考（被称为分子力学学习），lay annotators可以在标注时与域专家相比而出现较好的表现。尽管这样，手动分割任务仍然是注释过程中的必需任务。在这篇文章中，我们探讨了可以通过跳过像素级别分割来使用最近的segment anything模型（SAM）在零扩学习方法中。我们利用SAM的能力生成像素级别标注从box标注，并使用这些SAM生成的标注来训练分 segmentation模型。我们的发现表明，提案的SAM-assisted molecular-empowered learning（SAM-L）可以使lay annotators只需要弱box标注，而无需投入大量的标注时间和努力。这是在不损失标注准确性或深度学习基于分 segmentation模型的性能下实现的。这种研究表明了让非专业注释者参与标注过程的可能性，只需要依靠于非专业注释者。
</details></li>
</ul>
<hr>
<h2 id="High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology"><a href="#High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology" class="headerlink" title="High-performance Data Management for Whole Slide Image Analysis in Digital Pathology"></a>High-performance Data Management for Whole Slide Image Analysis in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05784">http://arxiv.org/abs/2308.05784</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/adios">https://github.com/hrlblab/adios</a></li>
<li>paper_authors: Haoju Leng, Ruining Deng, Shunxing Bao, Dazheng Fang, Bryan A. Millis, Yucheng Tang, Haichun Yang, Xiao Wang, Yifan Peng, Lipeng Wan, Yuankai Huo</li>
<li>For:  This paper focuses on addressing the data access challenge in giga-pixel digital pathology using the Adaptable IO System version 2 (ADIOS2).* Methods: The authors implement a digital pathology-centric pipeline using ADIOS2 and develop strategies to curtail data retrieval times.* Results: The paper shows a two-fold speed-up in the CPU scenario and performance on par with NVIDIA Magnum IO GPU Direct Storage (GDS) in the GPU scenario, making it one of the initial instances of using ADIOS2 in digital pathology.<details>
<summary>Abstract</summary>
When dealing with giga-pixel digital pathology in whole-slide imaging, a notable proportion of data records holds relevance during each analysis operation. For instance, when deploying an image analysis algorithm on whole-slide images (WSI), the computational bottleneck often lies in the input-output (I/O) system. This is particularly notable as patch-level processing introduces a considerable I/O load onto the computer system. However, this data management process could be further paralleled, given the typical independence of patch-level image processes across different patches. This paper details our endeavors in tackling this data access challenge by implementing the Adaptable IO System version 2 (ADIOS2). Our focus has been constructing and releasing a digital pathology-centric pipeline using ADIOS2, which facilitates streamlined data management across WSIs. Additionally, we've developed strategies aimed at curtailing data retrieval times. The performance evaluation encompasses two key scenarios: (1) a pure CPU-based image analysis scenario ("CPU scenario"), and (2) a GPU-based deep learning framework scenario ("GPU scenario"). Our findings reveal noteworthy outcomes. Under the CPU scenario, ADIOS2 showcases an impressive two-fold speed-up compared to the brute-force approach. In the GPU scenario, its performance stands on par with the cutting-edge GPU I/O acceleration framework, NVIDIA Magnum IO GPU Direct Storage (GDS). From what we know, this appears to be among the initial instances, if any, of utilizing ADIOS2 within the field of digital pathology. The source code has been made publicly available at https://github.com/hrlblab/adios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology"><a href="#Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology" class="headerlink" title="Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology"></a>Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05782">http://arxiv.org/abs/2308.05782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Hu, Ruining Deng, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for:  automatización de la segmentación de estructuras microvasculares en imágenes de tejido whole slide de riñón humano, como arteriolas, venulas y capilares, para mejorar la análisis cuantitativo en patología renal.</li>
<li>methods:  utilizó una red neuronal dinámica llamada Omni-Seg, que se entrenó con datos multisitio y multescale, y se benefició de etiquetas parciales en las imágenes de entrenamiento.</li>
<li>results:  el método Omni-Seg outperformió en términos de coeficiente de semejanza de Dice y de intersectión sobre unión, lo que demuestra su eficacia en la segmentación automática de microvasculares en imágenes de riñón.<details>
<summary>Abstract</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.
</details>
<details>
<summary>摘要</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.Here's the text in Traditional Chinese:过去的数位patology图像（WSI）中的微血管结构，例如arterioles、venules和capillaries，的分类已成为肾脏病理学的焦点。现有的手动分类技术是时间consuming且不适合大规模数位病理图像。而深度学习基础的方法则提供了自动分类的解决方案，但大多数方法受到一个限制：它们仅适用于单一站点、单一比例的数据。在这篇文章中，我们提出了Omni-Seg方法，这是一个单一动态网络方法，它利用多个站点、多个比例的训练数据。我们在两个数据集（HuBMAP和NEPTUNE）上进行了不同的放大（40x、20x、10x和5x）。实验结果显示Omni-Seg在Dice相似度系数（DSC）和交集过Union（IoU）方面都表现出色。我们的提议方法为肾脏病理学家提供了一个强大的计算工具，用于肾脏微血管结构的量化分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/eess.IV_2023_08_11/" data-id="clm0t8e2r00eyv7881hv6btce" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/11/cs.SD_2023_08_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-11 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/11/cs.LG_2023_08_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-11 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
