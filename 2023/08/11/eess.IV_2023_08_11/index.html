
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-11 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Towards Packaging Unit Detection for Automated Palletizing Tasks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06306 repo_url: None paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-11">
<meta property="og:url" content="https://nullscc.github.io/2023/08/11/eess.IV_2023_08_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Towards Packaging Unit Detection for Automated Palletizing Tasks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06306 repo_url: None paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-11T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:47:40.414Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/eess.IV_2023_08_11/" class="article-date">
  <time datetime="2023-08-11T09:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-11
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks"><a href="#Towards-Packaging-Unit-Detection-for-Automated-Palletizing-Tasks" class="headerlink" title="Towards Packaging Unit Detection for Automated Palletizing Tasks"></a>Towards Packaging Unit Detection for Automated Palletizing Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06306">http://arxiv.org/abs/2308.06306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Völk, Kilian Kleeberger, Werner Kraus, Richard Bormann</li>
<li>for:  automatized palletizing tasks</li>
<li>methods:  fully trained on synthetic data, can handle sparse and low quality sensor data, can exploit prior knowledge</li>
<li>results:  able to generalize to a wide range of products and application scenarios, demonstrated on real-world data with a wide range of different retail products, integrated in a lab demonstrator and a commercial solution will be marketed through an industrial partner.<details>
<summary>Abstract</summary>
For various automated palletizing tasks, the detection of packaging units is a crucial step preceding the actual handling of the packaging units by an industrial robot. We propose an approach to this challenging problem that is fully trained on synthetically generated data and can be robustly applied to arbitrary real world packaging units without further training or setup effort. The proposed approach is able to handle sparse and low quality sensor data, can exploit prior knowledge if available and generalizes well to a wide range of products and application scenarios. To demonstrate the practical use of our approach, we conduct an extensive evaluation on real-world data with a wide range of different retail products. Further, we integrated our approach in a lab demonstrator and a commercial solution will be marketed through an industrial partner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Self-supervised-SAR-Image-Despeckling-Strategy-Based-on-Parameter-sharing-Convolutional-Neural-Networks"><a href="#A-Self-supervised-SAR-Image-Despeckling-Strategy-Based-on-Parameter-sharing-Convolutional-Neural-Networks" class="headerlink" title="A Self-supervised SAR Image Despeckling Strategy Based on Parameter-sharing Convolutional Neural Networks"></a>A Self-supervised SAR Image Despeckling Strategy Based on Parameter-sharing Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05975">http://arxiv.org/abs/2308.05975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Chen, Yifei Yin, Hao Shi, Qingqing Sheng, Wei Li<br>for:This paper aims to propose a self-supervised SAR despeckling strategy that can be trained without speckle-free images.methods:The proposed method uses a sub-sampler based on the adjacent-syntropy criteria to generate training image pairs from real-world SAR images, and employs parameter sharing convolutional neural networks to make full use of training pairs. A multi-feature loss function is proposed, which combines despeckling term, regular term, and perception term to constrain the gap between the generated paired images and preserve edge and texture features.results:The proposed method is validated on real-world SAR images and shows better performance than several advanced SAR image despeckling methods, with improved ability to preserve edge and texture features.<details>
<summary>Abstract</summary>
Speckle noise is generated due to the SAR imaging mechanism, which brings difficulties in SAR image interpretation. Hence, despeckling is a helpful step in SAR pre-processing. Nowadays, deep learning has been proved to be a progressive method for SAR image despeckling. Most deep learning methods for despeckling are based on supervised learning, which needs original SAR images and speckle-free SAR images to train the network. However, the speckle-free SAR images are generally not available. So, this issue was tackled by adding multiplicative noise to optical images synthetically for simulating speckled image. Therefore, there are following challenges in SAR image despeckling: (1) lack of speckle-free SAR image; (2) difficulty in keeping details such as edges and textures in heterogeneous areas. To address these issues, we propose a self-supervised SAR despeckling strategy that can be trained without speckle-free images. Firstly, the feasibility of SAR image despeckling without speckle-free images is proved theoretically. Then, the sub-sampler based on the adjacent-syntropy criteria is proposed. The training image pairs are generated by the sub-sampler from real-word SAR image to estimate the noise distribution. Furthermore, to make full use of training pairs, the parameter sharing convolutional neural networks are adopted. Finally, according to the characteristics of SAR images, a multi-feature loss function is proposed. The proposed loss function is composed of despeckling term, regular term and perception term, to constrain the gap between the generated paired images. The ability of edge and texture feature preserving is improved simultaneously. Finally, qualitative and quantitative experiments are validated on real-world SAR images, showing better performances than several advanced SAR image despeckling methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>> tranlate the following text into Simplified Chinese.<</SYS>>零点噪声是SAR成像机制所导致的，这会带来SAR图像解释的困难。因此，滤除零点噪声是SAR预处理的有用步骤。目前，深度学习已经证明是SAR图像滤除零点噪声的进步方法。大多数深度学习方法是基于supervised learning，需要原始SAR图像和噪声自由SAR图像来训练网络。然而，噪声自由SAR图像通常不可得。因此，这个问题被解决了通过在互联网上添加 multiplicative noise synthetically来模拟零点噪声的image。因此，SAR图像滤除零点噪声存在以下挑战：（1）缺乏噪声自由SAR图像；（2）在不同区域中保持细节，如边缘和文本ure。 To address these issues, we propose a self-supervised SAR despeckling strategy that can be trained without speckle-free images. Firstly, the feasibility of SAR image despeckling without speckle-free images is proved theoretically. Then, the sub-sampler based on the adjacent-syntropy criteria is proposed. The training image pairs are generated by the sub-sampler from real-word SAR image to estimate the noise distribution. Furthermore, to make full use of training pairs, the parameter sharing convolutional neural networks are adopted. Finally, according to the characteristics of SAR images, a multi-feature loss function is proposed. The proposed loss function is composed of despeckling term, regular term and perception term, to constrain the gap between the generated paired images. The ability of edge and texture feature preserving is improved simultaneously. Finally, qualitative and quantitative experiments are validated on real-world SAR images, showing better performances than several advanced SAR image despeckling methods.Translation:零点噪声是SAR成像机制所导致的，这会带来SAR图像解释的困难。因此，滤除零点噪声是SAR预处理的有用步骤。目前，深度学习已经证明是SAR图像滤除零点噪声的进步方法。大多数深度学习方法是基于supervised learning，需要原始SAR图像和噪声自由SAR图像来训练网络。然而，噪声自由SAR图像通常不可得。因此，这个问题被解决了通过在互联网上添加 multiplicative noise synthetically来模拟零点噪声的image。因此，SAR图像滤除零点噪声存在以下挑战：（1）缺乏噪声自由SAR图像；（2）在不同区域中保持细节，如边缘和文本ure。为了解决这些问题，我们提出了一种自助式SAR滤除零点噪声策略，不需要噪声自由SAR图像进行训练。首先，我们证明了SAR图像滤除零点噪声的可能性。然后，我们提出了基于邻域同律的sub-sampler。通过这个sub-sampler，从实际世界SAR图像中生成了训练对。然后，为了充分利用训练对，我们采用了参数共享卷积神经网络。最后，根据SAR图像的特点，我们提出了一种多特征损失函数。该损失函数由滤除项、常规项和感知项组成，以限制生成对的差异。同时，edge和 texture特征的保持能力得到了提高。最后，我们对实际世界SAR图像进行了质量和量化的实验，并证明了我们的方法在SAR图像滤除零点噪声方面的性能比其他一些先进方法更好。
</details></li>
</ul>
<hr>
<h2 id="Classification-Method-of-Road-Surface-Condition-and-Type-with-LiDAR-Using-Spatiotemporal-Information"><a href="#Classification-Method-of-Road-Surface-Condition-and-Type-with-LiDAR-Using-Spatiotemporal-Information" class="headerlink" title="Classification Method of Road Surface Condition and Type with LiDAR Using Spatiotemporal Information"></a>Classification Method of Road Surface Condition and Type with LiDAR Using Spatiotemporal Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05965">http://arxiv.org/abs/2308.05965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Won Seo, Jin Sung Kim, Chung Choo Chung</li>
<li>for: 这个论文旨在运用深度神经网络（DNN）来分类道路表面状态和类型，使用LiDAR提供的信息。</li>
<li>methods: 本论文使用LiDAR提供的反射率和点云数据，在四个前方路段中分成四个互不相关的子区域。首先，constructed feature vectors使用每个子区域的反射率、点云数据和车辆内部信息。然后，使用DNN进行分类。最后，将DNN的输出 feed into 空间时间过程，以生成基于车速和概率的最终分类结果。</li>
<li>results: 根据比较研究，提出的DNN方法在两个车辆前方的子区域中取得了最高精度（98.0%和98.6%）。此外，在Jetson TX2板上实现了这个方法，证明了它在实时中可行。<details>
<summary>Abstract</summary>
This paper proposes a spatiotemporal architecture with a deep neural network (DNN) for road surface conditions and types classification using LiDAR. It is known that LiDAR provides information on the reflectivity and number of point clouds depending on a road surface. Thus, this paper utilizes the information to classify the road surface. We divided the front road area into four subregions. First, we constructed feature vectors using each subregion's reflectivity, number of point clouds, and in-vehicle information. Second, the DNN classifies road surface conditions and types for each subregion. Finally, the output of the DNN feeds into the spatiotemporal process to make the final classification reflecting vehicle speed and probability given by the outcomes of softmax functions of the DNN output layer. To validate the effectiveness of the proposed method, we performed a comparative study with five other algorithms. With the proposed DNN, we obtained the highest accuracy of 98.0\% and 98.6\% for two subregions near the vehicle. In addition, we implemented the proposed method on the Jetson TX2 board to confirm that it is applicable in real-time.
</details>
<details>
<summary>摘要</summary>
First, feature vectors are constructed using the reflectivity, number of point clouds, and in-vehicle information for each subregion. Then, the DNN classifies the road surface conditions and types for each subregion. Finally, the output of the DNN is fed into a spatiotemporal process to make the final classification, taking into account the vehicle speed and probability given by the outcomes of softmax functions of the DNN output layer.To validate the effectiveness of the proposed method, a comparative study was conducted with five other algorithms. The results showed that the proposed DNN achieved the highest accuracy of 98.0% and 98.6% for two subregions near the vehicle. Additionally, the proposed method was implemented on the Jetson TX2 board to confirm its applicability in real-time.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge"><a href="#Unleashing-the-Strengths-of-Unlabeled-Data-in-Pan-cancer-Abdominal-Organ-Quantification-the-FLARE22-Challenge" class="headerlink" title="Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge"></a>Unleashing the Strengths of Unlabeled Data in Pan-cancer Abdominal Organ Quantification: the FLARE22 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05862">http://arxiv.org/abs/2308.05862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junma11/flare">https://github.com/junma11/flare</a></li>
<li>paper_authors: Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Ma, Adamo Young, Cheng Zhu, Kangkang Meng, Xin Yang, Ziyan Huang, Fan Zhang, Wentao Liu, YuanKe Pan, Shoujin Huang, Jiacheng Wang, Mingze Sun, Weixin Xu, Dengqiang Jia, Jae Won Choi, Natália Alves, Bram de Wilde, Gregor Koehler, Yajun Wu, Manuel Wiesenfarth, Qiongjie Zhu, Guoqiang Dong, Jian He, the FLARE Challenge Consortium, Bo Wang</li>
<li>for: The paper aims to evaluate the accuracy and efficiency of artificial intelligence (AI) algorithms in automated abdominal disease diagnosis and treatment planning, particularly in real-world multinational settings.</li>
<li>methods: The authors organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. They constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers.</li>
<li>results: The best-performing algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0% by using 50 labeled scans and 2000 unlabeled scans, which can significantly reduce annotation requirements. The algorithms also successfully generalized to holdout external validation sets, achieving a median DSC of 89.5%, 90.9%, and 88.3% on North American, European, and Asian cohorts, respectively. Additionally, the algorithms enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements.<details>
<summary>Abstract</summary>
Quantitative organ assessment is an essential step in automated abdominal disease diagnosis and treatment planning. Artificial intelligence (AI) has shown great potential to automatize this process. However, most existing AI algorithms rely on many expert annotations and lack a comprehensive evaluation of accuracy and efficiency in real-world multinational settings. To overcome these limitations, we organized the FLARE 2022 Challenge, the largest abdominal organ analysis challenge to date, to benchmark fast, low-resource, accurate, annotation-efficient, and generalized AI algorithms. We constructed an intercontinental and multinational dataset from more than 50 medical groups, including Computed Tomography (CT) scans with different races, diseases, phases, and manufacturers. We independently validated that a set of AI algorithms achieved a median Dice Similarity Coefficient (DSC) of 90.0\% by using 50 labeled scans and 2000 unlabeled scans, which can significantly reduce annotation requirements. The best-performing algorithms successfully generalized to holdout external validation sets, achieving a median DSC of 89.5\%, 90.9\%, and 88.3\% on North American, European, and Asian cohorts, respectively. They also enabled automatic extraction of key organ biology features, which was labor-intensive with traditional manual measurements. This opens the potential to use unlabeled data to boost performance and alleviate annotation shortages for modern AI models.
</details>
<details>
<summary>摘要</summary>
“量化器官评估是自动化腹部疾病诊断和治疗规划的关键步骤。人工智能（AI）已经表现出很大的潜力来自动化这一过程。然而，现有的大多数AI算法都需要丰富的专家标注，并且没有全面评估其在实际世界多国 Settings中的准确率和效率。为了解决这些限制，我们组织了FLARE 2022 Challenge，是腹部器官分析挑战赛的最大一次。我们构建了跨 continent和多国的数据集，包括不同的种族、疾病、阶段和制造商的 Computed Tomography（CT）扫描图像。我们独立验证了一些AI算法可以通过50个标注图像和2000个无标注图像来达到90.0%的 dice相似度系数（DSC），这可以减少标注需求。最佳performing算法成功泛化到保留的外部验证集上，其中得到了89.5%、90.9%和88.3%的DSC在北美、欧洲和亚洲群体中，分别。它们还允许自动提取关键生物器官特征，这是传统手动测量很劳累的。这开启了使用无标注数据来提高性能的可能性，并alleviate标注短缺问题 для现代AI模型。”
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Optimization-of-JPEG-Based-Deep-Learning-Process-for-Image-Classification"><a href="#End-to-End-Optimization-of-JPEG-Based-Deep-Learning-Process-for-Image-Classification" class="headerlink" title="End-to-End Optimization of JPEG-Based Deep Learning Process for Image Classification"></a>End-to-End Optimization of JPEG-Based Deep Learning Process for Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05840">http://arxiv.org/abs/2308.05840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Qi, Lahiru D. Chamain, Zhi Ding</li>
<li>for: 这篇论文是为了解决深度学习应用中的分布式学习问题，特别是图像分类需要有效的图像压缩编码器在低成本感知设备上部署，以便高效传输和储存。</li>
<li>methods: 这篇论文使用了一个统一的端到端训练模型，包括一个JPEG图像编码器和一个基于深度学习的分类器。这个模型可以调整广泛部署的JPEG编码器设定，以提高分类精度，同时考虑到带宽限制。</li>
<li>results: 我们在CIFAR-100和ImageNet上进行了测试，结果显示了这个模型可以提高验证精度，并且超过了固定JPEG配置的验证精度。<details>
<summary>Abstract</summary>
Among major deep learning (DL) applications, distributed learning involving image classification require effective image compression codecs deployed on low-cost sensing devices for efficient transmission and storage. Traditional codecs such as JPEG designed for perceptual quality are not configured for DL tasks. This work introduces an integrative end-to-end trainable model for image compression and classification consisting of a JPEG image codec and a DL-based classifier. We demonstrate how this model can optimize the widely deployed JPEG codec settings to improve classification accuracy in consideration of bandwidth constraint. Our tests on CIFAR-100 and ImageNet also demonstrate improved validation accuracy over preset JPEG configuration.
</details>
<details>
<summary>摘要</summary>
major deep learning (DL) 应用中，分布式学习涉及到图像分类时，需要有效的图像压缩编码器在低成本感知设备上部署，以便高效地传输和存储。传统的编码器如JPEG，是为人类视觉质量设计的，并不适用于DL任务。这项工作介绍了一种综合的末端到末端可调整模型，它由JPEG图像编码器和基于DL的分类器组成。我们示出了如何使用这种模型来优化广泛部署的JPEG配置 Settings，以提高分类精度，同时考虑带宽约束。我们在CIFAR-100和ImageNet上进行了测试，并证明了我们的模型在预先设置的JPEG配置下的验证精度得到了改进。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology"><a href="#Spatial-Pathomics-Toolkit-for-Quantitative-Analysis-of-Podocyte-Nuclei-with-Histology-and-Spatial-Transcriptomics-Data-in-Renal-Pathology" class="headerlink" title="Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology"></a>Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06288">http://arxiv.org/abs/2308.06288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/spatial_pathomics">https://github.com/hrlblab/spatial_pathomics</a></li>
<li>paper_authors: Jiayuan Chen, Yu Wang, Ruining Deng, Quan Liu, Can Cui, Tianyuan Yao, Yilin Liu, Jianyong Zhong, Agnes B. Fogo, Haichun Yang, Shilin Zhao, Yuankai Huo</li>
<li>for: 这个研究旨在开发一个新的工具包，帮助研究人员更好地评估肾脏疾病中的肾脏细胞（podocyte）的特征。</li>
<li>methods: 该工具包包括三个主要组成部分：1）实例对象分割，可以准确地识别肾脏细胞核心；2）pathomics特征生成，从识别出的核心中提取了一系列数量特征；3）Robust统计分析，使得可以全面探索肾脏细胞的空间关系。</li>
<li>results: 该工具包成功提取和分析了肾脏细胞的形态和文化特征，揭示了肾脏细胞形态变化的多样性，并且通过统计分析发现了肾脏细胞的空间分布具有一定的相关性。<details>
<summary>Abstract</summary>
Podocytes, specialized epithelial cells that envelop the glomerular capillaries, play a pivotal role in maintaining renal health. The current description and quantification of features on pathology slides are limited, prompting the need for innovative solutions to comprehensively assess diverse phenotypic attributes within Whole Slide Images (WSIs). In particular, understanding the morphological characteristics of podocytes, terminally differentiated glomerular epithelial cells, is crucial for studying glomerular injury. This paper introduces the Spatial Pathomics Toolkit (SPT) and applies it to podocyte pathomics. The SPT consists of three main components: (1) instance object segmentation, enabling precise identification of podocyte nuclei; (2) pathomics feature generation, extracting a comprehensive array of quantitative features from the identified nuclei; and (3) robust statistical analyses, facilitating a comprehensive exploration of spatial relationships between morphological and spatial transcriptomics features.The SPT successfully extracted and analyzed morphological and textural features from podocyte nuclei, revealing a multitude of podocyte morphomic features through statistical analysis. Additionally, we demonstrated the SPT's ability to unravel spatial information inherent to podocyte distribution, shedding light on spatial patterns associated with glomerular injury. By disseminating the SPT, our goal is to provide the research community with a powerful and user-friendly resource that advances cellular spatial pathomics in renal pathology. The implementation and its complete source code of the toolkit are made openly accessible at https://github.com/hrlblab/spatial_pathomics.
</details>
<details>
<summary>摘要</summary>
PODOCYTES，特殊化的 epithelial 细胞，环绕 glomerular capillaries，对肾脏健康具有重要作用。现有的描述和量化特征是有限的，需要创新的解决方案来全面评估多样性的 fenotipic 特征在 Whole Slide Images (WSIs) 中。尤其是理解PODOCYTES的 morphological 特征是肾脏损伤的研究中的关键。这篇文章介绍了 Spatial Pathomics Toolkit (SPT) 并应用其于 PODOCYTES pathomics。SPT 包括三个主要组成部分：（1）实体对象分割，帮助精确地识别 PODOCYTES 核体;（2） pathomics 特征生成，从分割的 PODOCYTES 核体中提取了丰富的量化特征; 和（3） Robust 统计分析，使得可以全面探索 PODOCYTES 的 spatial 关系和 morphological 特征之间的关系。SPT 成功地提取和分析 PODOCYTES 核体的 morphological 和 texture 特征，发现了许多 PODOCYTES 形态特征，通过统计分析。此外，我们还证明了 SPT 的能力可以揭示 PODOCYTES 分布的空间信息，披露肾脏损伤PODOCYTES的空间模式。通过普及 SPT，我们的目标是为研究社区提供一个强大和易用的工具，推动细胞空间 PATHOMICS 的发展。实现和完整的源代码可以在 https://github.com/hrlblab/spatial_pathomics 上获得。
</details></li>
</ul>
<hr>
<h2 id="Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning"><a href="#Leverage-Weakly-Annotation-to-Pixel-wise-Annotation-via-Zero-shot-Segment-Anything-Model-for-Molecular-empowered-Learning" class="headerlink" title="Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning"></a>Leverage Weakly Annotation to Pixel-wise Annotation via Zero-shot Segment Anything Model for Molecular-empowered Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05785">http://arxiv.org/abs/2308.05785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyuan Li, Ruining Deng, Yucheng Tang, Shunxing Bao, Haichun Yang, Yuankai Huo<br>for:这个研究旨在提高数据验证的精确性和效率，并且将这个过程与非专家标注者进行结合，以提高数据验证的可 accessed性和可重用性。methods:这个研究使用了SAM模型，将它用于从矩形标注中生成像素级标注，并将这些标注用于训练分类模型。results:研究发现，使用SAM-L方法可以将标注工作从专家标注者转移到非专家标注者，不需要像素级标注，同时并不会对数据验证的精确性造成影响。<details>
<summary>Abstract</summary>
Precise identification of multiple cell classes in high-resolution Giga-pixel whole slide imaging (WSI) is critical for various clinical scenarios. Building an AI model for this purpose typically requires pixel-level annotations, which are often unscalable and must be done by skilled domain experts (e.g., pathologists). However, these annotations can be prone to errors, especially when distinguishing between intricate cell types (e.g., podocytes and mesangial cells) using only visual inspection. Interestingly, a recent study showed that lay annotators, when using extra immunofluorescence (IF) images for reference (referred to as molecular-empowered learning), can sometimes outperform domain experts in labeling. Despite this, the resource-intensive task of manual delineation remains a necessity during the annotation process. In this paper, we explore the potential of bypassing pixel-level delineation by employing the recent segment anything model (SAM) on weak box annotation in a zero-shot learning approach. Specifically, we harness SAM's ability to produce pixel-level annotations from box annotations and utilize these SAM-generated labels to train a segmentation model. Our findings show that the proposed SAM-assisted molecular-empowered learning (SAM-L) can diminish the labeling efforts for lay annotators by only requiring weak box annotations. This is achieved without compromising annotation accuracy or the performance of the deep learning-based segmentation. This research represents a significant advancement in democratizing the annotation process for training pathological image segmentation, relying solely on non-expert annotators.
</details>
<details>
<summary>摘要</summary>
高分辨率整个扫描图像（WSI）中多个细胞类型的精确识别是许多临床场景中的关键。建立一个人工智能模型用于这种目的通常需要像素级别的标注，但这些标注通常是不可扩展的并且需要具有专业技能（例如病理学家）进行。然而，这些标注可能受到错误的影响，特别是在用视觉检查来 distinguishing 细胞类型（例如 Podocytes 和 mesangial cells）时。有趣的是，一项最近的研究表明，使用Extra immunofluorescence（IF）图像作为参考时，lay annotators 可以在标注时与专业人员相比而出色表现。 despite this，手动分割任务仍然是注意力的必要要求。在这篇文章中，我们探讨可以通过快速的 Zero-shot 学习方法，使用最近的 segment anything model（SAM）来绕过像素级别的标注。我们利用 SAM 生成的标签来训练分割模型，并发现我们的提案可以使lay annotators 的标注努力减少到只需要weak box annotation。这是在不损害标注准确性或深度学习基于的分割性能的情况下完成的。这项研究表明了我们可以通过非专家 annotators 来进行训练病理图像分割的标注，不需要专业人员。
</details></li>
</ul>
<hr>
<h2 id="High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology"><a href="#High-performance-Data-Management-for-Whole-Slide-Image-Analysis-in-Digital-Pathology" class="headerlink" title="High-performance Data Management for Whole Slide Image Analysis in Digital Pathology"></a>High-performance Data Management for Whole Slide Image Analysis in Digital Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05784">http://arxiv.org/abs/2308.05784</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/adios">https://github.com/hrlblab/adios</a></li>
<li>paper_authors: Haoju Leng, Ruining Deng, Shunxing Bao, Dazheng Fang, Bryan A. Millis, Yucheng Tang, Haichun Yang, Xiao Wang, Yifan Peng, Lipeng Wan, Yuankai Huo</li>
<li>for: 这个论文主要是为了解决整个扫描图像的数据访问挑战，尤其是在应用图像分析算法时，计算机系统中的I&#x2F;O系统会出现计算瓶颈。</li>
<li>methods: 这篇论文使用了ADIOS2系统来解决这个问题，并开发了一个专门为整个扫描图像的数据管理而设计的数据管理管道。</li>
<li>results: 论文的实验结果显示，使用ADIOS2可以减少数据访问时间，并且在CPU场景下比使用笔制方法两倍快，在GPU场景下与NVIDIA Magnum IO GPU直接存储（GDS）相当。<details>
<summary>Abstract</summary>
When dealing with giga-pixel digital pathology in whole-slide imaging, a notable proportion of data records holds relevance during each analysis operation. For instance, when deploying an image analysis algorithm on whole-slide images (WSI), the computational bottleneck often lies in the input-output (I/O) system. This is particularly notable as patch-level processing introduces a considerable I/O load onto the computer system. However, this data management process could be further paralleled, given the typical independence of patch-level image processes across different patches. This paper details our endeavors in tackling this data access challenge by implementing the Adaptable IO System version 2 (ADIOS2). Our focus has been constructing and releasing a digital pathology-centric pipeline using ADIOS2, which facilitates streamlined data management across WSIs. Additionally, we've developed strategies aimed at curtailing data retrieval times. The performance evaluation encompasses two key scenarios: (1) a pure CPU-based image analysis scenario ("CPU scenario"), and (2) a GPU-based deep learning framework scenario ("GPU scenario"). Our findings reveal noteworthy outcomes. Under the CPU scenario, ADIOS2 showcases an impressive two-fold speed-up compared to the brute-force approach. In the GPU scenario, its performance stands on par with the cutting-edge GPU I/O acceleration framework, NVIDIA Magnum IO GPU Direct Storage (GDS). From what we know, this appears to be among the initial instances, if any, of utilizing ADIOS2 within the field of digital pathology. The source code has been made publicly available at https://github.com/hrlblab/adios.
</details>
<details>
<summary>摘要</summary>
Our performance evaluation included two key scenarios: (1) a pure CPU-based image analysis scenario ("CPU scenario") and (2) a GPU-based deep learning framework scenario ("GPU scenario"). Our findings showed noteworthy outcomes. Under the CPU scenario, ADIOS2 achieved an impressive two-fold speed-up compared to the brute-force approach. In the GPU scenario, its performance was on par with the cutting-edge GPU I/O acceleration framework, NVIDIA Magnum IO GPU Direct Storage (GDS). To the best of our knowledge, this is one of the earliest instances, if not the first, of using ADIOS2 in the field of digital pathology. The source code is publicly available at <https://github.com/hrlblab/adios>.
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology"><a href="#Multi-scale-Multi-site-Renal-Microvascular-Structures-Segmentation-for-Whole-Slide-Imaging-in-Renal-Pathology" class="headerlink" title="Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology"></a>Multi-scale Multi-site Renal Microvascular Structures Segmentation for Whole Slide Imaging in Renal Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05782">http://arxiv.org/abs/2308.05782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franklin Hu, Ruining Deng, Shunxing Bao, Haichun Yang, Yuankai Huo</li>
<li>for:  automatic segmentation of microvascular structures in human kidney whole slide images</li>
<li>methods:  uses a novel single dynamic network method that capitalizes on multi-site, multi-scale training data and partially labeled images</li>
<li>results:  outperforms other methods in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU)<details>
<summary>Abstract</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.
</details>
<details>
<summary>摘要</summary>
Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.Here's the translation in Traditional Chinese: Segmentation of microvascular structures, such as arterioles, venules, and capillaries, from human kidney whole slide images (WSI) has become a focal point in renal pathology. Current manual segmentation techniques are time-consuming and not feasible for large-scale digital pathology images. While deep learning-based methods offer a solution for automatic segmentation, most suffer from a limitation: they are designed for and restricted to training on single-site, single-scale data. In this paper, we present Omni-Seg, a novel single dynamic network method that capitalizes on multi-site, multi-scale training data. Unique to our approach, we utilize partially labeled images, where only one tissue type is labeled per training image, to segment microvascular structures. We train a singular deep network using images from two datasets, HuBMAP and NEPTUNE, across different magnifications (40x, 20x, 10x, and 5x). Experimental results indicate that Omni-Seg outperforms in terms of both the Dice Similarity Coefficient (DSC) and Intersection over Union (IoU). Our proposed method provides renal pathologists with a powerful computational tool for the quantitative analysis of renal microvascular structures.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/eess.IV_2023_08_11/" data-id="clnsn0vlm00rmgf88eguie06o" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/11/cs.LG_2023_08_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-11
        
      </div>
    </a>
  
  
    <a href="/2023/08/10/cs.SD_2023_08_10/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-10</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">82</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">35</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">78</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">22</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">150</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
