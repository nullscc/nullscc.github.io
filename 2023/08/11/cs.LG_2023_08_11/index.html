
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-11 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06203 repo_url: None paper_authors: Ric">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-11 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/11/cs.LG_2023_08_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06203 repo_url: None paper_authors: Ric">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-10T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:28.659Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.LG_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-11 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks"><a href="#Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks" class="headerlink" title="Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks"></a>Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06203">http://arxiv.org/abs/2308.06203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Cannizzaro, Jonathan Routley, Lars Kunze</li>
<li>for: 本研究旨在提供一种基于 causal probabilistic 框架的自主 робоット，以便让 robot 能够更好地理解和描述它所处环境。</li>
<li>methods: 本研究使用 causal inference 和 physics simulation 技术，将 causal models 与 probabilistic representations 结合起来，以便 robot 能够更好地理解和描述它所处环境。</li>
<li>results: 研究提出了一种新的 causal probabilistic 框架，可以帮助 robot 更好地完成块排序任务，并提供了一些 exemplar 的下一步行动选择结果。 I hope this helps! Let me know if you have any further questions or if there’s anything else I can help with.<details>
<summary>Abstract</summary>
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability into a structural causal model to permit robots to perceive and assess the current state of a block-stacking task, reason about the next-best action from placement candidates, and generate post-hoc counterfactual explanations. We provide exemplar next-best action selection results and outline planned experimentation in simulated and real-world robot block-stacking tasks.
</details>
<details>
<summary>摘要</summary>
世界中的不确定性使得系统设计者无法预期和明确地设计机器人可能遇到的所有场景。因此，基于这种设计方式的机器人在控制环境外会失败。 causal模型提供了一个原则性的框架，用于编码机器人与环境之间的 causal 关系，以及通常遇到的实际世界机器人中的抽象概率和不确定性。这些模型与 causal 推理相结合，允许自主机器人理解、推理和解释其环境。在这项工作中，我们关注了机器人块堆垫任务，因为它涉及到机器人的基本感知和操作能力，这些能力是许多应用程序，如仓库自动化和家庭服务机器人所必需的。我们提出了一种新的 causal 概率 frameworks，用于在机器人块堆垫任务中嵌入物理模拟能力，让机器人可以识别和评估当前块堆垫任务的状态，从选择候选位置中选择下一步行动，并生成后续 counterfactual 解释。我们提供了示例下一步行动选择结果，并详细描述计划在模拟和实际机器人块堆垫任务中进行实验。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions"><a href="#Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions" class="headerlink" title="Exploring Predicate Visual Context in Detecting of Human-Object Interactions"></a>Exploring Predicate Visual Context in Detecting of Human-Object Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06202">http://arxiv.org/abs/2308.06202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredzzhang/pvic">https://github.com/fredzzhang/pvic</a></li>
<li>paper_authors: Frederic Z. Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, Stephen Gould</li>
<li>for: 本研究旨在解决人–物交互（HOI）领域中现有的问题，即使用两stage transformer-based HOI检测器，但这些检测器常常基于物体特征，而忽略了姿态和方向信息，导致复杂或抽象的交互检测受到阻碍。</li>
<li>methods: 本研究使用了视觉化和优化的查询设计、广泛的键和值搜索以及盒对位嵌入作为空间引导，以提高 predicate visual context（PViC）模型的表现，并与当前状态顶峰方法在 HICO-DET 和 V-COCO 测试集上进行比较。</li>
<li>results: 根据测试结果，我们的 PViC 模型在 HICO-DET 和 V-COCO 测试集上具有更高的表现，同时保持了训练成本的低。<details>
<summary>Abstract</summary>
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:Recently, DETR 框架在人-物互动（HOI）研究中成为主流方法。特别是两阶段转换器基于 HOI 检测器在性能和训练效率方面表现出色。然而，这些通常基于缺乏细化上下文信息的对象特征，忽略对象的姿势和方向信息，而仅仅依靠对象的视觉特征来确定对象的标识和边框极限。这会导致复杂或抽象的互动无法正确识别。在这项工作中，我们通过视觉化和仔细设计的实验来研究这些问题。我们尝试通过跨注意力来重新引入图像特征，并通过改进的查询设计、广泛探索键和值、以及对象对的位域嵌入来提高 predicate 视觉上下文（PViC）模型的性能。我们的 PViC 模型在 HICO-DET 和 V-COCO 测试数据集上的表现比 state-of-the-art 方法更高，而且训练成本仍然很低。
</details></li>
</ul>
<hr>
<h2 id="Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features"><a href="#Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features" class="headerlink" title="Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features"></a>Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06197">http://arxiv.org/abs/2308.06197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angusmaiden/complex-fer">https://github.com/angusmaiden/complex-fer</a></li>
<li>paper_authors: Angus Maiden, Bahareh Nakisa</li>
<li>for: 该论文的目的是提出一种基于人类认知和学习的新型不间断学习方法，以便准确地识别复杂的人脸表达。</li>
<li>methods: 该方法基于人类认知和学习，包括知识储存、知识总结和预测排序记忆等技术。它还使用 GradCAM 视觉化来表明基本和复杂表达之间的关系。</li>
<li>results: 该方法可以准确地识别新的复杂表达类型，使用少量示例来学习新类别，并且在新类别上达到了74.28% 的总准确率。此外，该方法还证明了不间断学习方法在复杂表达识别中的优越性，比非不间断学习方法高出13.95%。此外，该方法还是首次应用了几shot学习到复杂表达识别中，达到了100% 的准确率。<details>
<summary>Abstract</summary>
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by building on and retaining its knowledge of basic expression classes. Using GradCAM visualisations, we demonstrate the relationship between basic and compound facial expressions, which our method leverages through knowledge distillation and a novel Predictive Sorting Memory Replay. Our method achieves the current state-of-the-art in continual learning for complex facial expression recognition with 74.28% Overall Accuracy on new classes. We also demonstrate that using continual learning for complex facial expression recognition achieves far better performance than non-continual learning methods, improving on state-of-the-art non-continual learning methods by 13.95%. To the best of our knowledge, our work is also the first to apply few-shot learning to complex facial expression recognition, achieving the state-of-the-art with 100% accuracy using a single training sample for each expression class.
</details>
<details>
<summary>摘要</summary>
人工智能在复杂情绪认知方面的表现，虽然已经达到了其他一些任务的水平，但是情绪认知仍然是一个挑战。人脸表达的情绪认知特别Difficult，因为人脸上可以表达出的情感复杂多样。为了让机器达到人类水平，它可能需要合并知识并理解新概念，就像人类一样。人类可以通过几个示例学习新概念，从记忆中提炼出重要信息，并丢弃其他信息。我们提出了一种基于人类认知和学习的新型连续学习方法，可以准确地识别新的复杂表达类型，使用很少的训练样本。我们使用GradCAM视觉化来描述基本和复杂表达之间的关系，然后通过知识储存和一种新的预测排序记忆回放来利用这种关系。我们的方法实现了当前领域内连续学习的最佳性，新类别上的总准确率为74.28%。我们还证明了使用连续学习进行复杂表达认知比非连续学习方法更好，提高了领域内最佳非连续学习方法的13.95%。此外，我们是第一个将几个样本学习应用于复杂表达认知，并达到了领域内最佳性，每个表达类型的准确率为100%。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Guest-Nationality-Composition-from-Hotel-Reviews"><a href="#Assessing-Guest-Nationality-Composition-from-Hotel-Reviews" class="headerlink" title="Assessing Guest Nationality Composition from Hotel Reviews"></a>Assessing Guest Nationality Composition from Hotel Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06175">http://arxiv.org/abs/2308.06175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Gröger, Marc Pouly, Flavia Tinner, Leif Brandes</li>
<li>for: 这篇论文旨在为企业提供方法，以优化客户端的客户分布。</li>
<li>methods: 该论文使用机器学习技术，从不结构化文本评论中提取出客户国籍信息，以动态评估和监测具体业务客户分布的变化。</li>
<li>results: 研究表明，使用简单的预训练embeddings和堆式LSTM层可以提供更好的性能-运行时间平衡，比较复杂的语言模型。<details>
<summary>Abstract</summary>
Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
</details>
<details>
<summary>摘要</summary>
许多酒店会向特定市场进行客户营销努力，以最好地预测客人偏好和需求。这种策略也是市场营销预算的必要前提。官方统计数据会报告不同国家的游客数量，但没有细化的信息对具体的企业客户组成进行了报告。然而，有越来越多的竞争对手、供应商、研究人员和公众对这些数据感兴趣。我们示例如如何使用机器学习来从无结构文本评论中提取客人国籍信息，以动态评估和监测个体企业客户组成的动态变化。具体来说，我们发现使用简单的预训练 embedding 和堆叠 LSTM 层可以提供更好的性能-运行时间质量比例，比较复杂的现状语言模型。
</details></li>
</ul>
<hr>
<h2 id="Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook"><a href="#Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook" class="headerlink" title="Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook"></a>Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06173">http://arxiv.org/abs/2308.06173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammed Shafique</li>
<li>for: 这篇论文旨在为研究人员、实践者和政策制定者提供关于物理抗击攻击的全面评估和概述，以便开发强健和安全的深度学习系统。</li>
<li>methods: 论文分析了物理抗击攻击的主要特征和特点，并描述了不同应用领域中的物理抗击攻击方法，包括分类、检测、人脸识别、 semantic segmentation 和深度估计。</li>
<li>results: 论文评估了这些攻击方法的效果、隐蔽性和可靠性，并探讨了如何在实际世界中执行攻击，以及如何提高防御机制。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了物理 adversarial 攻击的现代趋势的全面检查。我们的目的是为您提供物理 adversarial 攻击的深入理解，包括其关键特征和区别特征。此外，我们还探讨了在物理世界中执行攻击的具体要求和挑战。我们的文章探讨了不同应用领域中的物理 adversarial 攻击方法，分为不同的目标任务，如分类、检测、识别、 semantic segmentation 和深度估计。我们评估了这些攻击方法的效果、隐蔽性和Robustness。我们探究每种技术如何在 DNN 上成功 manipulate 而 minimizing 检测和快速应对实际扭曲。最后，我们讨论了物理 adversarial 攻击领域的当前挑战和未来研究方向，包括增强防御机制、探索新的攻击策略、在不同应用领域中评估攻击、以及建立 DNN 领域的标准化评估标准和评估方法。通过这篇全面的检查，我们希望为研究人员、实践人员和政策制定者提供一份有价值的资源，以便更好地理解物理 adversarial 攻击，并促进 DNN 基于系统的开发。
</details></li>
</ul>
<hr>
<h2 id="Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction"><a href="#Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction" class="headerlink" title="Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction"></a>Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06155">http://arxiv.org/abs/2308.06155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilong Ding, Tianpu Zhang, Zhe Wang</li>
<li>for: 预测高速公路日常交通量</li>
<li>methods: employs a hybrid model combining fully convolution network (FCN) and long short-term memory (LSTM), considering time, space, meteorology, and calendar from heterogeneous data</li>
<li>results: 实际使用一个中国省级高速公路的实际数据，对比traditional models，our method has distinct improvement for predictive accuracy, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.<details>
<summary>Abstract</summary>
Inter-city highway transportation is significant for citizens' modern urban life and generates heterogeneous sensory data with spatio-temporal characteristics. As a routine analysis in transportation domain, daily traffic volume estimation faces challenges for highway toll stations including lacking of exploration of correlative spatio-temporal features from a long-term perspective and effective means to deal with data imbalance which always deteriorates the predictive performance. In this paper, a deep spatio-temporal learning method is proposed to predict daily traffic volume in three phases. In feature pre-processing phase, data is normalized elaborately according to latent long-tail distribution. In spatio-temporal learning phase, a hybrid model is employed combining fully convolution network (FCN) and long short-term memory (LSTM), which considers time, space, meteorology, and calendar from heterogeneous data. In decision phase, traffic volumes on a coming day at network-wide toll stations would be achieved effectively, which is especially calibrated for vital few highway stations. Using real-world data from one Chinese provincial highway, extensive experiments show our method has distinct improvement for predictive accuracy than various traditional models, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.
</details>
<details>
<summary>摘要</summary>
市区间高速公路交通是现代城市居民日常生活中的重要一环，生成了多样化的感知数据，具有空间时间特征。为了解决高速公路收费站的日常交通量预测问题，我们面临着缺乏探索长期征特的相关空间时间特征以及有效地处理数据不均衡问题的挑战。在本文中，我们提出了一种深度空间时间学习方法，可以预测高速公路收费站的日常交通量。在特征预处理阶段，我们对数据进行了细心的Normal化，根据潜在的长尾分布。在空间时间学习阶段，我们采用了一种混合模型，结合了全连接网络（FCN）和长短期记忆（LSTM），考虑了时间、空间、气象和历法等多种不同数据。在决策阶段，我们可以准确预测高速公路收费站的未来一天内的交通量，特别是对于重要的一些高速公路站点进行了精准补做。使用了一个中国省道高速公路的实际数据，我们进行了广泛的实验，结果表明，我们的方法在预测精度方面与传统模型相比，具有明显的改善，分别达到了5.269和0.997的MPAE和R-squre指标。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Process-Regression-for-Maximum-Entropy-Distribution"><a href="#Gaussian-Process-Regression-for-Maximum-Entropy-Distribution" class="headerlink" title="Gaussian Process Regression for Maximum Entropy Distribution"></a>Gaussian Process Regression for Maximum Entropy Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06149">http://arxiv.org/abs/2308.06149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Sadr, Manuel Torrilhon, M. Hossein Gorji</li>
<li>for: 用于闭合问题的最大熵分布</li>
<li>methods: 使用加aussian prior逼近Lagrange多个性数</li>
<li>results: 对不同的kernel函数和Hyperparameter进行优化，实现数据驱动的最大熵闭合，并应用于非平衡分布的relaxation和Bhatnagar-Gross-Krook等方程的解。<details>
<summary>Abstract</summary>
Maximum-Entropy Distributions offer an attractive family of probability densities suitable for moment closure problems. Yet finding the Lagrange multipliers which parametrize these distributions, turns out to be a computational bottleneck for practical closure settings. Motivated by recent success of Gaussian processes, we investigate the suitability of Gaussian priors to approximate the Lagrange multipliers as a map of a given set of moments. Examining various kernel functions, the hyperparameters are optimized by maximizing the log-likelihood. The performance of the devised data-driven Maximum-Entropy closure is studied for couple of test cases including relaxation of non-equilibrium distributions governed by Bhatnagar-Gross-Krook and Boltzmann kinetic equations.
</details>
<details>
<summary>摘要</summary>
最大熵分布可以提供一个有优点的可能性密度，适用于矩阵闭合问题。然而，计算这些分布的拉格朗日 Parameters是实际应用中的计算瓶颈。受最近 Gaussian 过程的成功启发，我们研究将 Gaussian 假设用于 Approximate 拉格朗日 Parameters 的Map 的可行性。对各种核函数进行优化，我们使用最大 log-likelihood 来优化 гипер参数。我们研究了这种数据驱动的最大熵闭合的性能，并在几个测试案例中，包括适应不平衡分布的Relaxation 和 Boltzmann 动力学方程的闭合。
</details></li>
</ul>
<hr>
<h2 id="A-New-Approach-to-Overcoming-Zero-Trade-in-Gravity-Models-to-Avoid-Indefinite-Values-in-Linear-Logarithmic-Equations-and-Parameter-Verification-Using-Machine-Learning"><a href="#A-New-Approach-to-Overcoming-Zero-Trade-in-Gravity-Models-to-Avoid-Indefinite-Values-in-Linear-Logarithmic-Equations-and-Parameter-Verification-Using-Machine-Learning" class="headerlink" title="A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning"></a>A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06303">http://arxiv.org/abs/2308.06303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikrajuddin Abdullah</li>
<li>For: The paper aims to solve the challenge of identifying gravity parameters in the gravity model to explain international trade, specifically when there are zero flow trades.* Methods: The authors propose a two-step technique that involves performing linear regression locally to establish a dummy value for zero flow trades, and then estimating the gravity parameters using iterative techniques. Machine learning is also used to test the estimated parameters by analyzing their position in the cluster.* Results: The authors calculate international trade figures for 2004, 2009, 2014, and 2019 and find that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented in the paper can be used to solve other problems involving log-linear regression.Here’s the simplified Chinese text for the three information points:* 用途：文章解决国际贸易预测模型中零流通问题，即预测零流通时的重力参数。* 方法：文章提出了一种两步技术，先本地线性回归以确定零流通的假值，然后使用迭代法估算重力参数。同时，文章使用机器学习测试估算参数的位置在集群中。* 结果：文章计算了2004年、2009年、2014年和2019年的国际贸易数据，发现GDP的势和距离的势都处在同一个集群，均值约为1。文章的策略可以解决其他带有封零的log-线性回归问题。<details>
<summary>Abstract</summary>
The presence of a high number of zero flow trades continues to provide a challenge in identifying gravity parameters to explain international trade using the gravity model. Linear regression with a logarithmic linear equation encounters an indefinite value on the logarithmic trade. Although several approaches to solving this problem have been proposed, the majority of them are no longer based on linear regression, making the process of finding solutions more complex. In this work, we suggest a two-step technique for determining the gravity parameters: first, perform linear regression locally to establish a dummy value to substitute trade flow zero, and then estimating the gravity parameters. Iterative techniques are used to determine the optimum parameters. Machine learning is used to test the estimated parameters by analyzing their position in the cluster. We calculated international trade figures for 2004, 2009, 2014, and 2019. We just examine the classic gravity equation and discover that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented here can be used to solve other problems involving log-linear regression.
</details>
<details>
<summary>摘要</summary>
高数量的零流贸易仍然成为国际贸易使用重力模型确定重力参数的挑战。线性回归的对数几何方程遇到了对数贸易的不定值。虽然有几种解决方案被提议，但大多数都不再基于线性回归，使得解决问题的过程变得更加复杂。在这项工作中，我们提议一种两步技巧来确定重力参数：首先，在本地使用线性回归来设置一个占位符来替代零流贸易，然后估算重力参数。使用迭代技术来确定优化参数。机器学习被用来测试估算的参数，分析它们的位置在群集中。我们计算了2004、2009、2014和2019年的国际贸易数据。我们只考虑 классический重力方程，发现GDP的势和距离的势都在同一个群集中，它们的值约为1。这种策略可以用于解决其他带有对数几何方程的问题。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models"><a href="#Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models" class="headerlink" title="Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models"></a>Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06144">http://arxiv.org/abs/2308.06144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sruthisudheer/comment-classification-of-c-code">https://github.com/sruthisudheer/comment-classification-of-c-code</a></li>
<li>paper_authors: Sruthi S, Tanmay Basu</li>
<li>for: 本研究的目的是为了对不同代码段的注释进行分类。</li>
<li>methods: 本研究使用了不同的特征工程方案和文本分类技术，包括经典的袋包模型和基于变换器的模型。</li>
<li>results: 研究发现，使用袋包模型在训练集上表现最佳，但模型在训练和测试集上的表现不理想。研究还总结了模型的局限性和进一步改进的可能性。<details>
<summary>Abstract</summary>
The Forum for Information Retrieval (FIRE) started a shared task this year for classification of comments of different code segments. This is binary text classification task where the objective is to identify whether comments given for certain code segments are relevant or not. The BioNLP-IISERB group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in this task and submitted five runs for five different models. The paper presents the overview of the models and other significant findings on the training corpus. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model and transformer-based models were explored to identify significant features from the given training corpus. We have explored different classifiers viz., random forest, support vector machine and logistic regression using the bag of words model. Furthermore, the pre-trained transformer based models like BERT, RoBERT and ALBERT were also used by fine-tuning them on the given training corpus. The performance of different such models over the training corpus were reported and the best five models were implemented on the given test corpus. The empirical results show that the bag of words model outperforms the transformer based models, however, the performance of our runs are not reasonably well in both training and test corpus. This paper also addresses the limitations of the models and scope for further improvement.
</details>
<details>
<summary>摘要</summary>
forum for information retrieval (FIRE) 这年开始了代码段评注分类的共同任务。这是一个二进制文本分类任务，目标是判断给定的代码段评注是否相关。bioNLP-IISERB 组在印度科学教育研究所 Bhopal（IISERB）参加了这个任务，并提交了五个运行，每个运行使用了不同的模型。本文介绍了模型和其他有关训练集的发现。方法包括不同的特征工程方案和文本分类技术。我们研究了传统的袋子模型和基于 transformer 的模型，并使用了不同的分类器，如Random Forest、支持向量机和логисти准则回归。此外，我们还使用了预训练的 transformer 模型，如 BERT、RoBERT 和 ALBERT，并在给定的训练集上进行了微调。对于给定的训练集和测试集，我们Reported 不同模型的性能。 Results show that the bag of words model outperforms the transformer-based models, but the performance of our runs is not satisfactory in both the training and test corpora. This paper also discusses the limitations of the models and the scope for further improvement.
</details></li>
</ul>
<hr>
<h2 id="CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients"><a href="#CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients" class="headerlink" title="CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients"></a>CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06142">http://arxiv.org/abs/2308.06142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bulla Rajesh, Sk Mahafuz Zaman, Mohammed Javed, P. Nagabhushan</li>
<li>for: 本研究旨在提出一种能够直接在JPEG压缩表示中进行文本线Localization的方法，以避免 decompression 和重新压缩的过程，从而降低存储和计算成本。</li>
<li>methods: 提出了一种基于深度特征学习的Modified U-Net架构，称为Compressed Text-Line Localization Network (CompTLL-UNet)，用于实现文本线Localization。</li>
<li>results: 通过对ICDAR2017（cBAD）和ICDAR2019（cBAD）测试集进行训练和测试，实现了在JPEG压缩Domain中的文本线Localization，并reported state-of-the-art perfomance。<details>
<summary>Abstract</summary>
Automatic localization of text-lines in handwritten documents is still an open and challenging research problem. Various writing issues such as uneven spacing between the lines, oscillating and touching text, and the presence of skew become much more challenging when the case of complex handwritten document images are considered for segmentation directly in their respective compressed representation. This is because, the conventional way of processing compressed documents is through decompression, but here in this paper, we propose an idea that employs deep feature learning directly from the JPEG compressed coefficients without full decompression to accomplish text-line localization in the JPEG compressed domain. A modified U-Net architecture known as Compressed Text-Line Localization Network (CompTLL-UNet) is designed to accomplish it. The model is trained and tested with JPEG compressed version of benchmark datasets including ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state-of-the-art performance with reduced storage and computational costs in the JPEG compressed domain.
</details>
<details>
<summary>摘要</summary>
自动化手写文档中文行的本地化仍然是一个打开的和挑战性的研究问题。不同的写作问题，如文本行间距不均匀、文本抖动和触摸、扭曲等问题，在考虑复杂手写文档图像时变得更加挑战。这是因为，传统的文档处理方法是通过解压缩来处理压缩文档，但在这篇论文中，我们提出了一个想法，即使用深度特征学习直接从JPEG压缩率中提取特征来实现文本行的本地化。我们称之为Compressed Text-Line Localization Network（CompTLL-UNet）。我们设计了一种修改后的U-Net架构，并对其进行训练和测试，使用JPEG压缩版本的标准评测数据集，包括ICDAR2017（cBAD）和ICDAR2019（cBAD）。我们的模型在JPEG压缩领域实现了状态之巅性表现，同时具有减少存储和计算成本的优点。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling"><a href="#Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling" class="headerlink" title="Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling"></a>Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06138">http://arxiv.org/abs/2308.06138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoume Kazemi, Davood Moradkhani, Alireza A. Alipour</li>
<li>for: 这项研究旨在开发一个人工神经网络模型，用于预测压filtering proces中的蛋白湿度。</li>
<li>methods: 该研究使用了人工神经网络技术，并在288次测试中采用了两种不同的 Filter Fabric（S1和S2）。</li>
<li>results: 研究结果显示，人工神经网络模型可以高度准确地预测压filtering proces中的蛋白湿度，R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。<details>
<summary>Abstract</summary>
Machine Learning (ML) is a powerful tool for material science applications. Artificial Neural Network (ANN) is a machine learning technique that can provide high prediction accuracy. This study aimed to develop an ANN model to predict the cake moisture of the pressure filtration process of zinc production. The cake moisture was influenced by seven parameters: temperature (35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and 5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm), pressure, and filtration time. The study conducted 288 tests using two types of fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by the Coefficient of determination (R2), the Mean Square Error (MSE), and the Mean Absolute Error (MAE) metrics for both datasets. The results showed R2 values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE values of 0.00056 and 0.00088 for S1 and S2, respectively. These results indicated that the ANN model could predict the cake moisture of pressure filtration in the zinc leaching process with high accuracy.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）是一种强大的工具，可以应用于材料科学领域。人工神经网络（ANN）是一种机器学习技术，可以提供高精度预测。本研究目的是开发一个ANN模型，以预测压 filtering过程中锻生产中的蛋白湿度。蛋白湿度受到七个参数的影响：温度（35和65摄氏度）、固体浓度（0.2和0.38g/L）、pH（2、3.5和5）、空气喷流时间（2、10和15分）、蛋白厚度（14、20、26和34mm）、压力和过滤时间。研究通过288次测试，使用两种不同的 fabrics： polypropylene（S1）和 polyester（S2）。ANN模型被评估于 Coefficient of determination（R2）、Mean Square Error（MSE）和 Mean Absolute Error（MAE）三个指标，其中R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。这些结果表明，ANN模型可以高精度预测压 filtering过程中的蛋白湿度。
</details></li>
</ul>
<hr>
<h2 id="PDE-Discovery-for-Soft-Sensors-Using-Coupled-Physics-Informed-Neural-Network-with-Akaike’s-Information-Criterion"><a href="#PDE-Discovery-for-Soft-Sensors-Using-Coupled-Physics-Informed-Neural-Network-with-Akaike’s-Information-Criterion" class="headerlink" title="PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike’s Information Criterion"></a>PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike’s Information Criterion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06132">http://arxiv.org/abs/2308.06132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aina Wang, Pan Qin, Xi-Ming Sun</li>
<li>For: 这篇论文旨在探讨一种基于物理学习的软传感器PDE发现方法，以便更好地监测工业过程中的关键变量。* Methods: 该方法基于物理学习的软传感器PDE模型，并使用了Akaike的准则信息来找到合适的偏微分方程结构。* Results: 实验结果表明，CPINN-AIC方法可以准确地找到合适的偏微分方程结构，并且可以用来预测工业过程中的变量。<details>
<summary>Abstract</summary>
Soft sensors have been extensively used to monitor key variables using easy-to-measure variables and mathematical models. Partial differential equations (PDEs) are model candidates for soft sensors in industrial processes with spatiotemporal dependence. However, gaps often exist between idealized PDEs and practical situations. Discovering proper structures of PDEs, including the differential operators and source terms, can remedy the gaps. To this end, a coupled physics-informed neural network with Akaike's criterion information (CPINN-AIC) is proposed for PDE discovery of soft sensors. First, CPINN is adopted for obtaining solutions and source terms satisfying PDEs. Then, we propose a data-physics-hybrid loss function for training CPINN, in which undetermined combinations of differential operators are involved. Consequently, AIC is used to discover the proper combination of differential operators. Finally, the artificial and practical datasets are used to verify the feasibility and effectiveness of CPINN-AIC for soft sensors. The proposed CPINN-AIC is a data-driven method to discover proper PDE structures and neural network-based solutions for soft sensors.
</details>
<details>
<summary>摘要</summary>
Soft sensors 通过使用容易测量的变量和数学模型来监测关键变量。但是，实际情况中存在idealized PDEs和实际情况之间的差距。发现合适的 PDE 结构，包括偏微分运算和源项，可以解决这些差距。为此，我们提出了物理学习混合数据损失函数（CPINN-AIC），用于PDE发现。首先，我们采用CPINN来获取满足 PDE 的解和源项。然后，我们提出了一种数据物理混合损失函数，其中包含未知的偏微分运算。最后，我们使用AIC来发现合适的偏微分运算结构。 Finally, we use artificial and practical datasets to verify the feasibility and effectiveness of CPINN-AIC for soft sensors. The proposed CPINN-AIC is a data-driven method to discover proper PDE structures and neural network-based solutions for soft sensors.Note: The translation is done using the Simplified Chinese grammar and vocabulary, which is commonly used in mainland China. However, it should be noted that there are different dialects and variations of Chinese, and the translation may vary depending on the specific region or dialect.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities"><a href="#Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities" class="headerlink" title="Uncertainty Quantification for Image-based Traffic Prediction across Cities"></a>Uncertainty Quantification for Image-based Traffic Prediction across Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06129">http://arxiv.org/abs/2308.06129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alextimans/traffic4cast-uncertainty">https://github.com/alextimans/traffic4cast-uncertainty</a></li>
<li>paper_authors: Alexander Timans, Nina Wiedemann, Nishant Kumar, Ye Hong, Martin Raubal</li>
<li>for: 这个论文的目的是提高深度学习模型在智能交通系统中的可解释性，以便更好地做出决策和提高模型的部署潜力。</li>
<li>methods: 这个论文使用了两种 эпистемиче和两种 aleatoric 不确定性评估方法来评估深度学习模型的不确定性，并对多个城市和时间段进行比较。</li>
<li>results: 研究发现，可以通过使用不确定性评估方法来获得有意义的不确定性估计，并且可以用这些估计来检测城市交通动力学的异常情况。在一个 Moscow 的示例研究中，我们发现了时间和空间效应的影响于城市交通行为。<details>
<summary>Abstract</summary>
Despite the strong predictive performance of deep learning models for traffic prediction, their widespread deployment in real-world intelligent transportation systems has been restrained by a lack of interpretability. Uncertainty quantification (UQ) methods provide an approach to induce probabilistic reasoning, improve decision-making and enhance model deployment potential. To gain a comprehensive picture of the usefulness of existing UQ methods for traffic prediction and the relation between obtained uncertainties and city-wide traffic dynamics, we investigate their application to a large-scale image-based traffic dataset spanning multiple cities and time periods. We compare two epistemic and two aleatoric UQ methods on both temporal and spatio-temporal transfer tasks, and find that meaningful uncertainty estimates can be recovered. We further demonstrate how uncertainty estimates can be employed for unsupervised outlier detection on changes in city traffic dynamics. We find that our approach can capture both temporal and spatial effects on traffic behaviour in a representative case study for the city of Moscow. Our work presents a further step towards boosting uncertainty awareness in traffic prediction tasks, and aims to highlight the value contribution of UQ methods to a better understanding of city traffic dynamics.
</details>
<details>
<summary>摘要</summary>
启示深度学习模型对交通预测的强大预测能力，实际应用中的广泛部署却受到了不可预测性的限制。不确定性评估（UQ）方法可以带来 probabilistic reasoning，改善决策和提高模型部署的潜力。为了了解现有UQ方法对交通预测的用途和获得的不确定性与城市范围内交通动力学的关系，我们对多座城市和多个时间段的大规模图像基本交通数据进行了 investigate。我们比较了两种 эпистеمic和两种 aleatoric UQ方法的性能在时间和空间转移任务上，并发现了有意义的不确定性估计可以被恢复。我们还示出了如何使用不确定性估计进行无supervised outlier检测，检测城市交通动力学的变化。我们在 Moskva 城市的示例研究中发现，我们的方法可以捕捉到时间和空间效应的交通行为。我们的工作是 uncertainty awareness 在交通预测任务中的一个进一步步骤，旨在高亮不确定性评估对城市交通动力学的理解的重要性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Control-Policies-for-Variable-Objectives-from-Offline-Data"><a href="#Learning-Control-Policies-for-Variable-Objectives-from-Offline-Data" class="headerlink" title="Learning Control Policies for Variable Objectives from Offline Data"></a>Learning Control Policies for Variable Objectives from Offline Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06127">http://arxiv.org/abs/2308.06127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Weber, Phillip Swazinna, Daniel Hein, Steffen Udluft, Volkmar Sterzing</li>
<li>for: 本研究旨在提供一种可靠的机器学习方法，用于控制动态系统，特别是当直接与环境交互不可用时。</li>
<li>methods: 本研究使用了模型基于政策搜索方法的扩展，即变量目标策略（VOP）。这种方法使得政策可以高效地泛化到多种目标，即在奖励函数中的参数。</li>
<li>results: 通过对目标函数中的参数进行调整，用户可以在运行时调整政策的行为或重新平衡优化目标，无需收集更多的观察批量或重新训练。<details>
<summary>Abstract</summary>
Offline reinforcement learning provides a viable approach to obtain advanced control strategies for dynamical systems, in particular when direct interaction with the environment is not available. In this paper, we introduce a conceptual extension for model-based policy search methods, called variable objective policy (VOP). With this approach, policies are trained to generalize efficiently over a variety of objectives, which parameterize the reward function. We demonstrate that by altering the objectives passed as input to the policy, users gain the freedom to adjust its behavior or re-balance optimization targets at runtime, without need for collecting additional observation batches or re-training.
</details>
<details>
<summary>摘要</summary>
转换文本为简化中文：</SYS>在线上学习不可靠的方法可以提供先进的控制策略 для动力系统，特别是当直接与环境进行交互不可用时。本文提出了基于模型的政策搜索方法的概念扩展，称为变量目标策略（VOP）。通过这种方法，政策被训练以通用化效率地处理多种目标，这些目标参数化奖励函数。我们示例显示，通过在运行时更改目标，用户可以在不需要收集更多观察批处或重新训练的情况下，通过变化目标来调整行为或重新平衡优化目标。
</details></li>
</ul>
<hr>
<h2 id="Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic"><a href="#Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic" class="headerlink" title="Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic"></a>Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07336">http://arxiv.org/abs/2308.07336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitachi-nlp/fld">https://github.com/hitachi-nlp/fld</a></li>
<li>paper_authors: Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa</li>
<li>for: 本研究旨在使语言模型（LM）学习逻辑推理能力。 previous studies使用特定的推理规则生成推理例子，但这些规则有限或else arbitrary，这限制了获得的逻辑推理能力的一致性。</li>
<li>methods: 我们采用基于正式逻辑理论的准确的推理规则集，可以 derivation 任何其他推理规则。我们名为这种推理规则集为 $\textbf{FLD}$（正式逻辑推理）。</li>
<li>results: 我们实验表明，使用 $\textbf{FLD}$ 推理规则集训练LMs，LMs可以获得更一致的逻辑推理能力。此外，我们还识别了逻辑推理能力中哪些方面可以通过推理 corpora 增强LMs，那些方面无法增强。最后，基于这些结果，我们讨论未来如何使用推理 corpora 或其他方法来解决每个方面的问题。我们发布了代码、数据和模型。<details>
<summary>Abstract</summary>
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each aspect. We release the code, data, and models.
</details>
<details>
<summary>摘要</summary>
我们研究了一种基于合成语料库的方法，用于语言模型（LM）学习逻辑推理能力。之前的研究通过特定的推理规则生成了推理示例，但这些规则是有限的或else是arbitrary的。这会限制学习得到的逻辑推理能力的通用性。我们重新思考了这一点，采用基于形式逻辑理论的固定的推理规则，可以在多步骤中组合以 derivation任何其他的推理规则。我们employmaterially verify that LMs trained on our proposed corpora，which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability。此外，我们还确定了推理能力中哪些方面可以通过推理 corpora进行增强，以及哪些方面无法进行增强。最后，基于这些结果，我们讨论了将来采用推理 corpora或其他方法对每个方面的应用。我们释放了代码、数据和模型。
</details></li>
</ul>
<hr>
<h2 id="Hawkes-Processes-with-Delayed-Granger-Causality"><a href="#Hawkes-Processes-with-Delayed-Granger-Causality" class="headerlink" title="Hawkes Processes with Delayed Granger Causality"></a>Hawkes Processes with Delayed Granger Causality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06106">http://arxiv.org/abs/2308.06106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Hengyuan Miao, Shuang Li</li>
<li>for: 本研究旨在Explicitly Modeling delayed Granger causal effects based on multivariate Hawkes processes, 即 causal event usually takes some time to exert an effect, 研究这个时间延迟自身的科学意义。</li>
<li>methods: 我们首先证明了延迟参数的可 identificability under mild conditions, 然后 investigate a model estimation method under complex setting, 即 want to infer the posterior distribution of time lags and understand how this distribution varies across different scenarios, 我们将时延 treated as latent variables, 并使用Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of time lags.</li>
<li>results: 我们 empirically evaluate our model’s event prediction and time-lag inference accuracy on synthetic and real data, achieving promising results.<details>
<summary>Abstract</summary>
We aim to explicitly model the delayed Granger causal effects based on multivariate Hawkes processes. The idea is inspired by the fact that a causal event usually takes some time to exert an effect. Studying this time lag itself is of interest. Given the proposed model, we first prove the identifiability of the delay parameter under mild conditions. We further investigate a model estimation method under a complex setting, where we want to infer the posterior distribution of the time lags and understand how this distribution varies across different scenarios. We treat the time lags as latent variables and formulate a Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of the time lags. By explicitly modeling the time lags in Hawkes processes, we add flexibility to the model. The inferred time-lag posterior distributions are of scientific meaning and help trace the original causal time that supports the root cause analysis. We empirically evaluate our model's event prediction and time-lag inference accuracy on synthetic and real data, achieving promising results.
</details>
<details>
<summary>摘要</summary>
我们目标是显式地模型延迟的格兰格 causal 效应基于多变量 Hawkes 过程。这个想法源于事件引起效应通常需要一些时间。研究这个时间延迟本身很有趣。给出的模型，我们首先证明延迟参数的可识别性于轻量级 услови下。我们进一步调查了一种复杂的设定下的模型估计方法，我们想要从多个enario中推断时延参数的 posterior 分布，并理解这个分布在不同enario下如何变化。我们将时延参数作为隐藏变量，并采用Variational Auto-Encoder（VAE）算法来近似 posterior 分布。通过显式地模型 Hawkes 过程中的时延参数，我们增加了模型的灵活性。经验证明我们的模型在实验数据上的事件预测和时延参数推断精度都很高。
</details></li>
</ul>
<hr>
<h2 id="Composable-Function-preserving-Expansions-for-Transformer-Architectures"><a href="#Composable-Function-preserving-Expansions-for-Transformer-Architectures" class="headerlink" title="Composable Function-preserving Expansions for Transformer Architectures"></a>Composable Function-preserving Expansions for Transformer Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06103">http://arxiv.org/abs/2308.06103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Gesmundo, Kaitlin Maile</li>
<li>for: 提高现代神经网络的训练成本，特别是计算和时间成本。</li>
<li>methods: 提出六种可 композиitely 的变换，用于逐步增加 transformer 类神经网络的大小，保持功能完整性。</li>
<li>results: 证明每种变换都可以保持函数完整性，并且可以有效地升级模型规模。<details>
<summary>Abstract</summary>
Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.
</details>
<details>
<summary>摘要</summary>
培训现代神经网络需要高效计算和时间成本。模型缩放被认为是提高现状的关键因素。在增加模型缩放时，通常需要从scratch开始，随机初始化整个模型的参数，因为这会导致模型结构中参数的变化，不允许小型模型知识的直接传递。在这项工作中，我们提出六种可组合的变换来逐步增加基于转换器的神经网络缩放，保持功能完整性，以便在训练过程中逐步扩展模型的容量。我们提供了准确功能保持的证明，并且在 minimal initialization constraints 下进行证明。这些方法可能会帮助建立更大更强的模型，并通过逐步扩展模型结构来实现高效的训练管道。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation"><a href="#Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation" class="headerlink" title="Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation"></a>Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06100">http://arxiv.org/abs/2308.06100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cairo-thws/dbvce_eval">https://github.com/cairo-thws/dbvce_eval</a></li>
<li>paper_authors: Philipp Vaeth, Alexander M. Fruehwald, Benjamin Paassen, Magda Gregorova</li>
<li>For: This paper aims to provide a systematic and quantitative evaluation framework for visual counterfactual explanations (VCE) methods, and to explore the effects of crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet).* Methods: The paper proposes a framework for evaluating VCE methods using a minimal set of metrics, and conducts a battery of ablation-like experiments generating thousands of VCEs for a suite of classifiers of various complexity, accuracy, and robustness.* Results: The paper finds multiple directions for future advancements and improvements of VCE methods, and provides a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是提供一个系统的和量化的评估框架，以帮助评估视觉对称解释（VCE）方法，并探索最新的扩散基于生成模型中的关键设计选择对自然图像分类（ImageNet）VCE的影响。* Methods: 论文提出一个评估VCE方法的最小集合的度量，并通过大量的拟合实验生成了不同复杂性、准确率和稳定性的多个分类器的VCE。* Results: 论文发现了未来的进步和改进的方向，并提供了一个有价值的指南，以便在评估对称解释中增加一致性和透明度。<details>
<summary>Abstract</summary>
Latest methods for visual counterfactual explanations (VCE) harness the power of deep generative models to synthesize new examples of high-dimensional images of impressive quality. However, it is currently difficult to compare the performance of these VCE methods as the evaluation procedures largely vary and often boil down to visual inspection of individual examples and small scale user studies. In this work, we propose a framework for systematic, quantitative evaluation of the VCE methods and a minimal set of metrics to be used. We use this framework to explore the effects of certain crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet). We conduct a battery of ablation-like experiments, generating thousands of VCEs for a suite of classifiers of various complexity, accuracy and robustness. Our findings suggest multiple directions for future advancements and improvements of VCE methods. By sharing our methodology and our approach to tackle the computational challenges of such a study on a limited hardware setup (including the complete code base), we offer a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.
</details>
<details>
<summary>摘要</summary>
最新的视觉对比解释方法（VCE）利用深度生成模型Synthesize高维像素图像的新示例，质量非常高。然而，目前很难比较这些VCE方法的性能，因为评估方法大多不同，经常降到视觉检查具体示例和小规模用户研究。在这项工作中，我们提出了一个系统性评估VCE方法的框架和最小的 metric集，并使用这些框架来探索 diffusion-based生成模型在自然图像分类（ImageNet）中VCE的效果。我们进行了一系列减少-like实验，生成了数千个VCE，用于一组不同的分类器，包括不同的复杂度、准确率和鲁棒性。我们的发现建议了未来VCE方法的进一步改进。通过分享我们的方法和我们对限制硬件设置（包括完整的代码库）的处理方式，我们提供了行业研究人员的有价值指南，促进了透明度和一致性在对对比解释的评估中。
</details></li>
</ul>
<hr>
<h2 id="Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes"><a href="#Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes" class="headerlink" title="Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes"></a>Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06095">http://arxiv.org/abs/2308.06095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Galetzka, Anne Beyer, David Schlangen</li>
<li>for: 本研究探讨了基于强大语言模型的开放领域对话系统，以做出合适的对话贡献。</li>
<li>methods: 研究人员使用了不同的截止点和训练策略来控制语言模型，以确保贡献的优质。</li>
<li>results: 研究人员发现了一些有前途的方法，并建议了未来研究的新方向。<details>
<summary>Abstract</summary>
Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising attempts and suggest novel ways for future research.
</details>
<details>
<summary>摘要</summary>
现代条件语言模型能够继续任何类型的文本源，并且在看起来很流畅地进行交流。这一事实激发了基于强大语言模型的开放领域对话系统的研究，旨在通过生成相应的贡献来模拟对话伙伴。从语言学角度来看，参与对话的复杂度很高。在这种情况下，我们根据这个特定的研究领域来解释格雷斯的协作对话原则，并将文献分为合适贡献的几个方面：一个神经网络对话模型需要流畅、有用、一致、 coherent 和遵循社会规范。为确保这些质量，当前的方法在不同的 intervening point 上尝试控制基础语言模型，例如数据、训练方法或解码。按照这些类别和 intervening point 排序，我们讨论了有前途的尝试，并建议未来研究的新方法。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes"><a href="#Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes" class="headerlink" title="Reinforcement Logic Rule Learning for Temporal Point Processes"></a>Reinforcement Logic Rule Learning for Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06094">http://arxiv.org/abs/2308.06094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Lu Wang, Kun Gao, Shuang Li</li>
<li>for: 用于解释 temporal events 的发生</li>
<li>methods: 使用 temporal point process modeling and learning framework，逐渐优化规则集和其重要性，并通过 neural search policy 生成新规则</li>
<li>results: 在 synthetic 和实际医疗数据上获得了promising的结果<details>
<summary>Abstract</summary>
We propose a framework that can incrementally expand the explanatory temporal logic rule set to explain the occurrence of temporal events. Leveraging the temporal point process modeling and learning framework, the rule content and weights will be gradually optimized until the likelihood of the observational event sequences is optimal. The proposed algorithm alternates between a master problem, where the current rule set weights are updated, and a subproblem, where a new rule is searched and included to best increase the likelihood. The formulated master problem is convex and relatively easy to solve using continuous optimization, whereas the subproblem requires searching the huge combinatorial rule predicate and relationship space. To tackle this challenge, we propose a neural search policy to learn to generate the new rule content as a sequence of actions. The policy parameters will be trained end-to-end using the reinforcement learning framework, where the reward signals can be efficiently queried by evaluating the subproblem objective. The trained policy can be used to generate new rules in a controllable way. We evaluate our methods on both synthetic and real healthcare datasets, obtaining promising results.
</details>
<details>
<summary>摘要</summary>
我们提出了一个框架，可以逐步扩展解释时间事件的发生。利用时间点处理模型和学习框架，规则内容和权重将被逐步优化，直到观测事件序列的可能性最高。我们的算法会 alternate между主问题和子问题。主问题中，当前规则集权重将被更新；而子问题中，一个新的规则将被搜索并添加到最大化可能性。我们形式ulated主问题是凸型的，可以使用连续优化来解决；而子问题则需要搜索庞大的 combinatorial 规则 predicate 和关系空间。为了解决这个挑战，我们提出了一种神经搜索策略，可以学习生成新规则的内容作为一个序列动作。这个策略的参数将通过可行学习框架进行培养，其中的奖励信号可以快速地查询由辅助问题的目标函数来提供。已经训练的策略可以用于生成新规则的控制方式。我们对具有 sintetic 和实际医疗数据的方法进行了评估，获得了有前途的结果。
</details></li>
</ul>
<hr>
<h2 id="Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers"><a href="#Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers" class="headerlink" title="Experts Weights Averaging: A New General Training Scheme for Vision Transformers"></a>Experts Weights Averaging: A New General Training Scheme for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06093">http://arxiv.org/abs/2308.06093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Wanli Ouyang</li>
<li>for: 提高 ViT 模型的性能而不增加推理成本</li>
<li>methods: 使用 Mixture-of-Experts (MoE) 实现对 ViT 模型的训练，并在训练和推理阶段之间进行分解</li>
<li>results: 对多个 2D 和 3D 视觉任务、ViT 架构和数据集进行了全面的实验 validate 提议的训练方法的效果和普适性，同时还可以应用于细化 ViT 模型的 fine-tuning 过程中提高性能。<details>
<summary>Abstract</summary>
Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ViT with specially designed, more efficient MoEs that assign tokens to experts by random uniform partition, and perform Experts Weights Averaging (EWA) on these MoEs at the end of each iteration. After training, we convert each MoE into an FFN by averaging the experts, transforming the model back into original ViT for inference. We further provide a theoretical analysis to show why and how it works. Comprehensive experiments across various 2D and 3D visual tasks, ViT architectures, and datasets validate the effectiveness and generalizability of the proposed training scheme. Besides, our training scheme can also be applied to improve performance when fine-tuning ViTs. Lastly, but equally important, the proposed EWA technique can significantly improve the effectiveness of naive MoE in various 2D visual small datasets and 3D visual tasks.
</details>
<details>
<summary>摘要</summary>
《Structural re-parameterization是一种通用训练方案 для Convolutional Neural Networks (CNNs),它可以提高性能而不增加推理成本。在Vision Transformers (ViTs)逐渐超越CNNs的视觉任务中，有人可能会提问：如果存在专门 дляViTs的训练方案，可以提高性能而不增加推理成本？Recently, Mixture-of-Experts (MoE)has attracted increasing attention,因为它可以高效地扩展Transformers的容量在固定成本下。考虑到MoE可以被视为多支分支结构，那么我们可以使用MoE来实现ViTs的训练方案类似于structural re-parameterization。在这篇论文中，我们答于这些问题，并提出了一种新的通用训练策略 дляViTs。 Specifically,我们在训练阶段将ViTs中的一些Feed-Forward Networks (FFNs)替换为特制的、更高效的MoEs，并在每个迭代结束后进行Experts Weights Averaging (EWA)。之后，我们将MoEs转换成FFNs，并将模型转换回原始的ViTs模型进行推理。我们还提供了一种理论分析，以证明这种训练方案的有效性和如何工作。我们在多种2D和3D视觉任务、ViT结构和数据集上进行了广泛的实验，证明了提议的训练策略的效果和通用性。此外，我们的训练策略还可以用于改进ViTs的性能when fine-tuning。最后，但也非常重要的是，我们提出的EWA技术可以在多种2D视觉小数据集和3D视觉任务中显著提高MoE的效果。》
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering"><a href="#Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering" class="headerlink" title="Toward a Better Understanding of Loss Functions for Collaborative Filtering"></a>Toward a Better Understanding of Loss Functions for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06091">http://arxiv.org/abs/2308.06091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/psm1206/mawu">https://github.com/psm1206/mawu</a></li>
<li>paper_authors: Seongmin Park, Mincheol Yoon, Jae-woong Lee, Hogun Park, Jongwuk Lee</li>
<li>for: 这篇论文主要研究了相互推荐系统中的协同推荐技术，具体来说是分析现有的损失函数之间的关系，并提出了一种新的损失函数来改进现有的协同推荐模型。</li>
<li>methods: 该论文使用了数学分析来探究现有损失函数的关系，并在这基础上提出了一种新的损失函数called Margin-aware Alignment and Weighted Uniformity (MAWU)，它通过（i）margin-aware alignment（MA）和（ii）weighted uniformity（WU）来改进协同推荐模型的设计。</li>
<li>results: 实验结果表明，当 equiped with MAWU，MF和LightGCN相比现有的协同推荐模型，在三个公共数据集上具有相当或更高的性能。<details>
<summary>Abstract</summary>
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU is two-fold: (i) margin-aware alignment (MA) mitigates user/item-specific popularity biases, and (ii) weighted uniformity (WU) adjusts the significance between user and item uniformities to reflect the inherent characteristics of datasets. Extensive experimental results show that MF and LightGCN equipped with MAWU are comparable or superior to state-of-the-art CF models with various loss functions on three public datasets.
</details>
<details>
<summary>摘要</summary>
合作 filtering (CF) 是现代推荐系统中的关键技术。 CF 模型的学习过程通常包括三个组成部分：交互编码器、损失函数和负样本。虽然现有的研究已经提出了许多不同的 CF 模型，但是最近的研究表明，只是修改损失函数的设计可以获得显著性能提升。本文分析了现有损失函数之间的关系。我们的数学分析表明，前一些损失函数可以被解释为对用户和项目表示的对齐和分布均匀函数：（i）对用户和项目表示进行对齐（ii）对用户和项目分布进行均匀化。根据这一分析，我们提出了一种新的损失函数，称为 Margin-aware Alignment and Weighted Uniformity (MAWU)。MAWU 的关键创新有两个方面：（i）对用户/项目特有的流行偏好进行缓和（ii）根据数据集的特点进行加权均匀化。我们进行了广泛的实验研究，发现 MF 和 LightGCN 搭配 MAWU 与 state-of-the-art CF 模型相比，在三个公共数据集上具有相似或更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Safeguarding-Learning-based-Control-for-Smart-Energy-Systems-with-Sampling-Specifications"><a href="#Safeguarding-Learning-based-Control-for-Smart-Energy-Systems-with-Sampling-Specifications" class="headerlink" title="Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications"></a>Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06069">http://arxiv.org/abs/2308.06069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Venkatesh Prasad Venkataramanan, Pragya Kirti Gupta, Yun-Fei Hsu, Simon Burton</li>
<li>for: 这篇论文是关于使用强化学习控制能源系统中的挑战，特别是在保证性和安全性两个方面的。</li>
<li>methods: 论文详细介绍了在实时逻辑中强化学习安全要求的方法，包括将实时逻辑转换为线性逻辑（LTL），以便利用高级工程技术，如安全学习盾和正式验证。</li>
<li>results: 论文表明，通过将实时逻辑转换为LTL，可以在强化学习过程中提供更高的安全性保证，并且可以通过统计模型检查来获得更高的满意度。<details>
<summary>Abstract</summary>
We study challenges using reinforcement learning in controlling energy systems, where apart from performance requirements, one has additional safety requirements such as avoiding blackouts. We detail how these safety requirements in real-time temporal logic can be strengthened via discretization into linear temporal logic (LTL), such that the satisfaction of the LTL formulae implies the satisfaction of the original safety requirements. The discretization enables advanced engineering methods such as synthesizing shields for safe reinforcement learning as well as formal verification, where for statistical model checking, the probabilistic guarantee acquired by LTL model checking forms a lower bound for the satisfaction of the original real-time safety requirements.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:我们研究和开发用控制能源系统的强化学习，同时考虑性能要求和安全要求，如避免黑OUT。我们详细说明如何通过离散到线性时间逻辑（LTL）来加强安全要求，使得满足LTL公式的满足性意味着满足原始的安全要求。这种离散Enabled advanced工程技术，如生成安全屏障和正式验证，以及统计模型检查中的概率保证，这个保证是原始时间安全要求满足的下界。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-flow-disaggregation-for-hydropower-plant-management"><a href="#Deep-learning-based-flow-disaggregation-for-hydropower-plant-management" class="headerlink" title="Deep learning-based flow disaggregation for hydropower plant management"></a>Deep learning-based flow disaggregation for hydropower plant management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11631">http://arxiv.org/abs/2308.11631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Zhang</li>
<li>for:  Norwegian hydropower plant management</li>
<li>methods:  deep learning-based time series disaggregation model</li>
<li>results:  promising results for disaggregating daily flow into hourly flow<details>
<summary>Abstract</summary>
High temporal resolution data is a vital resource for hydropower plant management. Currently, only daily resolution data are available for most of Norwegian hydropower plant, however, to achieve more accurate management, sub-daily resolution data are often required. To deal with the wide absence of sub-daily data, time series disaggregation is a potential tool. In this study, we proposed a time series disaggregation model based on deep learning, the model is tested using flow data from a Norwegian flow station, to disaggregate the daily flow into hourly flow. Preliminary results show some promising aspects for the proposed model.
</details>
<details>
<summary>摘要</summary>
高时间分辨率数据是 Norway 水力发电厂的重要资源。目前，大多数 Norwegian 水力发电厂的数据只有每天的分辨率，但是为更准确的管理， often 需要更高的时间分辨率数据。为了解决宽泛的无法获得 sub-daily 数据的问题，时间序列分解是一种可能的工具。本研究提出了基于深度学习的时间序列分解模型，在使用挪威流站的流量数据进行测试，以分解每天的流量为每小时的流量。初步结果显示该模型具有一些有前途的特点。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-SGD-with-Polyak-stepsize-and-Line-search-Robust-Convergence-and-Variance-Reduction"><a href="#Adaptive-SGD-with-Polyak-stepsize-and-Line-search-Robust-Convergence-and-Variance-Reduction" class="headerlink" title="Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction"></a>Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06058">http://arxiv.org/abs/2308.06058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaowen Jiang, Sebastian U. Stich</li>
<li>for: 这个论文目的是提出两种新的随机波兰梯（AdaSPS和AdaSLS），以确保在非 interpolative 设定下进行训练，并且在对 convex 和强 convex 函数进行训练时维持下线性和线性的 converges 速率。</li>
<li>methods: 这两种新算法使用了随机波兰梯和随机搜索，并且使用了一种新的减少偏差的技术来提高速度。</li>
<li>results: 这些新算法可以在非 interpolative 设定下进行训练，并且可以在对 convex 和强 convex 函数进行训练时维持下线性和线性的 converges 速率，并且可以和 AdaSVRG 的速率匹配，但是不需要内部外部循环结构。<details>
<summary>Abstract</summary>
The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. [2022]), this approach results in slower convergence rates for convex and over-parameterized models. In this work, we make two contributions: Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value as input. Secondly, we equip AdaSPS and AdaSLS with a novel variance reduction technique and obtain algorithms that require $\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without variance reduction in the non-interpolation regimes. Moreover, our result matches the fast rates of AdaSVRG but removes the inner-outer-loop structure, which is easier to implement and analyze. Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.
</details>
<details>
<summary>摘要</summary>
Recently, the stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for stochastic gradient descent (SGD) have been proposed and have shown great effectiveness in training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution, which may result in a worse output than the initial guess. To address this issue, artificially decreasing the adaptive stepsize has been proposed (Orvieto et al., 2022), but this approach leads to slower convergence rates for convex and over-parameterized models.In this work, we make two contributions:Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS does not require knowledge of problem-dependent parameters, and AdaSPS only requires a lower bound of the optimal function value as input.Secondly, we equip AdaSPS and AdaSLS with a novel variance reduction technique, and obtain algorithms that require $\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without variance reduction in the non-interpolation regimes. Moreover, our result matches the fast rates of AdaSVRG but removes the inner-outer-loop structure, which is easier to implement and analyze.Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.
</details></li>
</ul>
<hr>
<h2 id="Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro"><a href="#Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro" class="headerlink" title="Cost-effective On-device Continual Learning over Memory Hierarchy with Miro"></a>Cost-effective On-device Continual Learning over Memory Hierarchy with Miro</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06053">http://arxiv.org/abs/2308.06053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Ma, Suyeon Jeong, Minjia Zhang, Di Wang, Jonghyun Choi, Myeongjae Jeon</li>
<li>for: 本研究旨在实现Edge设备上的持续学习（Continual Learning，CL）系统，以提高数据隐私和能源效率。</li>
<li>methods: 本研究使用层次memory replay的CL方法，并开发了一个名为Miro的系统运行时，用于在Edge设备上动态配置CL系统以实现最佳成本效率。</li>
<li>results: 对baseline系统进行比较，Miro显示了显著的成本效率提升。<details>
<summary>Abstract</summary>
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperforms baseline systems we build for comparison, consistently achieving higher cost-effectiveness.
</details>
<details>
<summary>摘要</summary>
Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by dynamically configuring the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead.Extensive evaluations show that Miro significantly outperforms baseline systems we built for comparison, consistently achieving higher cost-effectiveness.
</details></li>
</ul>
<hr>
<h2 id="Towards-Instance-adaptive-Inference-for-Federated-Learning"><a href="#Towards-Instance-adaptive-Inference-for-Federated-Learning" class="headerlink" title="Towards Instance-adaptive Inference for Federated Learning"></a>Towards Instance-adaptive Inference for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06051">http://arxiv.org/abs/2308.06051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunmeifeng/fedins">https://github.com/chunmeifeng/fedins</a></li>
<li>paper_authors: Chun-Mei Feng, Kai Yu, Nian Liu, Xinxing Xu, Salman Khan, Wangmeng Zuo</li>
<li>for: 这个论文的目的是提出一种基于联合学习（Federated Learning，FL）框架的实例适应性推理方法，以提高FL在复杂实际数据上的性能。</li>
<li>methods: 这个论文使用了一种基于缩放和偏移（scale and shift）的深度特征方法（SSF），以及一种客户端启发式推理方法（instance-adaptive inference），以适应实际数据上的实例差异性。</li>
<li>results: 对于Tiny-ImageNet dataset，这个方法比顶峰性能的方法提高6.64%，而且通信成本低于15%。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. To enable instance-adaptive inference, for a given instance, we dynamically find the best-matched SSF subsets from the pool and aggregate them to generate an adaptive SSF specified for the instance, thereby reducing the intra-client as well as the inter-client heterogeneity. Extensive experiments show that our FedIns outperforms state-of-the-art FL algorithms, e.g., a 6.64\% improvement against the top-performing method with less than 15\% communication cost on Tiny-ImageNet. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式学习 paradigm，允许多个客户端学习一个强大的全球模型，通过Client中的本地训练数据进行汇总。然而，全球模型的性能经常受到客户端数据之间的非同质化的影响，需要广泛的减少客户端数据之间的不同性。此外，我们注意到了复杂的实际数据中的内部客户端数据不同性，也会严重降低 FL 性能。在这篇论文中，我们提出了一种新的 FL 算法，即 FedIns，以处理内部客户端数据不同性。我们在 FL 框架中实现了实例适应的推理，而不需要巨大的实例适应模型。我们首先在每个客户端上训练一个可缩放和调整的深度特征池（SSF），并在服务器端将这些 SSF 池进行汇总，以保持低的通信成本。为实现实例适应推理，对于一个给定的实例，我们在实例级别 dynamically 找到最佳适应的 SSF 子集，并将这些子集进行汇总，以生成适应该实例的 adaptive SSF。这有助于降低内部客户端数据不同性以及客户端数据之间的不同性。我们的 FedIns 在 Tiny-ImageNet 上比顶尖方法提供了6.64%的提升，并且与之前的最好方法在 less than 15% 的通信成本下。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors"><a href="#AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors" class="headerlink" title="AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors"></a>AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08554">http://arxiv.org/abs/2308.08554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulrezzak Zekiye, Semih Utku, Fadi Amroush, Oznur Ozkasap<br>for:This paper aims to analyze historical data and use artificial intelligence algorithms to identify the factors affecting a cryptocurrency’s price and to find risky cryptocurrencies.methods:The paper uses on-chain parameters to analyze historical cryptocurrency data and employs clustering and classification techniques to group cryptocurrencies based on their on-chain characteristics. The paper also uses multiple classifiers to predict whether a cryptocurrency is risky or not.results:The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. The paper also found a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Additionally, the paper clustered cryptocurrencies into five distinct groups based on their on-chain parameters, and obtained the best f1-score of 76% using K-Nearest Neighbor for predicting risky cryptocurrencies.<details>
<summary>Abstract</summary>
Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Moreover, we clustered cryptocurrencies into five distinct groups using their on-chain parameters, which provides investors with a more comprehensive understanding of a cryptocurrency when compared to those clustered with it. Finally, by implementing multiple classifiers to predict whether a cryptocurrency is risky or not, we obtained the best f1-score of 76% using K-Nearest Neighbor.
</details>
<details>
<summary>摘要</summary>
digital currencies 在最近几年内已经成为投资者和学者关注的热点话题。为了做出 Informed 投资决策，需要了解 криптовалюencies 价格的影响因素并确定风险较高的 криптовалюencies。这篇论文通过分析历史数据和使用人工智能算法对 chain 参数进行分析，以确定 криптовалюencies 价格的影响因素和风险评估。我们对历史 криптовалюencies 的 chain 数据进行分析，并测量价格和其他参数之间的相关性。此外，我们还使用聚类和分类来更好地理解 криптовалюencies，并将其分为五个不同类别。最后，我们通过应用多种分类器来预测 криптовалюencies 是否为风险的，并获得了最佳的 f1 分数为 76%。
</details></li>
</ul>
<hr>
<h2 id="Controlling-Character-Motions-without-Observable-Driving-Source"><a href="#Controlling-Character-Motions-without-Observable-Driving-Source" class="headerlink" title="Controlling Character Motions without Observable Driving Source"></a>Controlling Character Motions without Observable Driving Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06025">http://arxiv.org/abs/2308.06025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyuan Li, Bin Dai, Ziyi Zhou, Qi Yao, Baoyuan Wang</li>
<li>for: 生成无驱动源的多样化、自然和无限长的头部&#x2F;身体序列</li>
<li>methods: 提议一个系统性框架，结合VQ-VAE和一种新的токен级控制策略，使用返回学习算法和经过设计的奖励函数来生成无限长的多样化和自然的头部&#x2F;身体序列</li>
<li>results: 通过全面的评估，发现提议的框架可以解决无驱动源生成中的各种挑战，并与其他强基线相比表现出众。<details>
<summary>Abstract</summary>
How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly.
</details>
<details>
<summary>摘要</summary>
如何生成无驱动源的多样化、生命般自然的头部/身体序列？我们认为这是一个未受抨拿的研究问题，具有独特的技术挑战。不受 semantics 驱动源的限制，使用标准的自然语言模型来生成无限长序列会导致1) OOD 问题 Due to the accumulated error, 2) 不够多样性来生成自然和生命般的动作序列和 3) 不想要的时间 periodic patterns.为了解决以上挑战，我们提议一个系统性的框架，该框架结合 VQ-VAE 的优点和一种基于 reinforcement learning 的新的 токен级控制策略。高级 prior model 可以轻松地注入到该框架中，以生成无限长和多样化的序列。虽然我们现在没有驱动源，但我们的框架可以通过 Carefully designed reward functions 来扩展到控制的 synthesis 中Explicit driving sources。通过全面的评估，我们结议了我们提议的框架可以解决所有以上挑战，并与其他强大的基准模型相比，表现非常出色。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment"><a href="#Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment" class="headerlink" title="Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment"></a>Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07933">http://arxiv.org/abs/2308.07933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youxiang Zhu, Nana Lin, Xiaohui Liang, John A. Batsis, Robert M. Roth, Brian MacWhinney</li>
<li>for: 本研究旨在提高诊断老人痴呆症的精度，通过利用图像描述文本对应关系来提高检测精度。</li>
<li>methods: 本研究提出了首个将图像和描述文本作为输入，并利用大规模预训练图像文本对应模型的知识来进行诊断的模型。我们发现了健康和痴呆样本之间的文本与图像之间的差异，并使用文本与图像之间的相关性来排序和筛选样本。此外，我们还将图像分解成不同主题，并将文本分类为每个主题中的不同话题。</li>
<li>results: 我们的三种进阶模型，通过对样本进行预处理，使用图像与文本之间的相关性和图像分解、文本分类等技术，实现了诊断精度的提高。我们的最佳模型在83.44%的检测精度上得到了状元表现，高于文本只基线模型的79.91%。此外，我们还可视化样本和图像结果，以便解释我们的模型的优势。<details>
<summary>Abstract</summary>
Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-processed the samples based on their relevance to the picture, sub-image, and focused areas. The evaluation results show that our advanced models, with knowledge of the picture and large image-text alignment models, achieve state-of-the-art performance with the best detection accuracy at 83.44%, which is higher than the text-only baseline model at 79.91%. Lastly, we visualize the sample and picture results to explain the advantages of our models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry"><a href="#Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry" class="headerlink" title="Large Language Models for Telecom: Forthcoming Impact on the Industry"></a>Large Language Models for Telecom: Forthcoming Impact on the Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06013">http://arxiv.org/abs/2308.06013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De Domenico, Merouane Debbah</li>
<li>for: 本研究旨在探讨LLM技术在电信领域的应用和影响，以及如何在这些领域中充分利用LLM的潜力。</li>
<li>methods: 本研究采用了LLM技术的内部结构和应用场景的分析，以及在电信领域中可以立即实施的用例的探讨。</li>
<li>results: 研究发现了LLM技术在电信领域的现有能力和局限性，以及需要进一步研究的领域和挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fullest extent within the telecom domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Does-AI-for-science-need-another-ImageNet-Or-totally-different-benchmarks-A-case-study-of-machine-learning-force-fields"><a href="#Does-AI-for-science-need-another-ImageNet-Or-totally-different-benchmarks-A-case-study-of-machine-learning-force-fields" class="headerlink" title="Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields"></a>Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05999">http://arxiv.org/abs/2308.05999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatao Li, Wanling Gao, Lei Wang, Lixin Sun, Zun Wang, Jianfeng Zhan</li>
<li>for: 这 paper 的目的是探讨 AI for science 领域中的模型性能评估方法，以便更好地适应科学计算任务中的特殊挑战。</li>
<li>methods: 该 paper 使用 machine learning force field (MLFF) 作为一个案例研究，检查了现有的 AI benchmarking 方法是否能够有效地评估 AI for science 模型的性能。它还提出了一些解决方案来评估 MLFF 模型，包括样本效率、时间域敏感性和交叉数据集泛化能力等方面。</li>
<li>results: 该 paper 通过设置问题实例类似于实际科学应用，提出了一些更加科学意义的性能指标，以评估 AI for science 模型的性能。这些指标在实际应用中表现出更高的泛化能力和更好的时间域敏感性，与传统的 AI 评估方法相比。<details>
<summary>Abstract</summary>
AI for science (AI4S) is an emerging research field that aims to enhance the accuracy and speed of scientific computing tasks using machine learning methods. Traditional AI benchmarking methods struggle to adapt to the unique challenges posed by AI4S because they assume data in training, testing, and future real-world queries are independent and identically distributed, while AI4S workloads anticipate out-of-distribution problem instances. This paper investigates the need for a novel approach to effectively benchmark AI for science, using the machine learning force field (MLFF) as a case study. MLFF is a method to accelerate molecular dynamics (MD) simulation with low computational cost and high accuracy. We identify various missed opportunities in scientifically meaningful benchmarking and propose solutions to evaluate MLFF models, specifically in the aspects of sample efficiency, time domain sensitivity, and cross-dataset generalization capabilities. By setting up the problem instantiation similar to the actual scientific applications, more meaningful performance metrics from the benchmark can be achieved. This suite of metrics has demonstrated a better ability to assess a model's performance in real-world scientific applications, in contrast to traditional AI benchmarking methodologies. This work is a component of the SAIBench project, an AI4S benchmarking suite. The project homepage is https://www.computercouncil.org/SAIBench.
</details>
<details>
<summary>摘要</summary>
人工智能 для科学（AI4S）是一个emerging研究领域，旨在使用机器学习方法提高科学计算任务的准确率和速度。传统的AI测试方法困难适应AI4S的特殊挑战，因为它们假设训练、测试和未来实际世界中的数据都是独立并且相同分布的，而AI4S工作负荷预期的问题实例将出现在不同的分布上。这篇论文研究了AI4S测试方法的需要，使用机器学习力场（MLFF）作为一个案例研究。MLFF是一种加速分子动力学（MD）仿真的方法，可以减少计算成本并保持高度准确。我们认为存在多种科学上有意义的测试机会被遗弃，并提出了一些解决方案来评估MLFF模型，包括样本效率、时间域敏感和cross-dataset泛化能力。通过设置问题实例类似于实际科学应用，可以更 meaningful的性能指标从测试中获得。这组指标已经表明可以更好地评估模型在实际科学应用中的性能，与传统的AI测试方法不同。这是SAIBench项目的一部分，SAIBench是一个AI4S测试集。项目主页在https://www.computercouncil.org/SAIBench。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network"><a href="#Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network" class="headerlink" title="Automatic Classification of Blood Cell Images Using Convolutional Neural Network"></a>Automatic Classification of Blood Cell Images Using Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06300">http://arxiv.org/abs/2308.06300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Abeera Mahfooz</li>
<li>For: The paper aims to automatically classify ten types of blood cells with increased accuracy using a convolutional neural network (CNN) model.* Methods: The authors use transfer learning with pre-trained CNN models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20, on the PBC dataset’s normal DIB. They also propose a novel CNN-based framework to improve accuracy.* Results: The authors achieve an accuracy of 99.91% on the PBC dataset with their proposed CNN model, outperforming earlier results reported in the literature.<details>
<summary>Abstract</summary>
Human blood primarily comprises plasma, red blood cells, white blood cells, and platelets. It plays a vital role in transporting nutrients to different organs, where it stores essential health-related data about the human body. Blood cells are utilized to defend the body against diverse infections, including fungi, viruses, and bacteria. Hence, blood analysis can help physicians assess an individual's physiological condition. Blood cells have been sub-classified into eight groups: Neutrophils, eosinophils, basophils, lymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and metamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of their nucleus, shape, and cytoplasm. Traditionally, pathologists and hematologists in laboratories have examined these blood cells using a microscope before manually classifying them. The manual approach is slower and more prone to human error. Therefore, it is essential to automate this process. In our paper, transfer learning with CNN pre-trained models. VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20 applied to the PBC dataset's normal DIB. The overall accuracy achieved with these models lies between 91.375 and 94.72%. Hence, inspired by these pre-trained architectures, a model has been proposed to automatically classify the ten types of blood cells with increased accuracy. A novel CNN-based framework has been presented to improve accuracy. The proposed CNN model has been tested on the PBC dataset normal DIB. The outcomes of the experiments demonstrate that our CNN-based framework designed for blood cell classification attains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional neural network model performs competitively when compared to earlier results reported in the literature.
</details>
<details>
<summary>摘要</summary>
人体血液主要由血液溶解、红细胞、白细胞和板块组成。它扮演着将营养物质传递到不同器官的重要角色，同时也存储了人体重要的生物学信息。血液细胞可以用于防御体内各种感染，包括病毒、真菌和细菌。因此，血液分析可以帮助医生评估个体的生理状况。血液细胞被分为八种类型：neutrophils、eosinophils、basophils、lymphocytes、monocytes、immature granulocytes（promyelocytes、myelocytes和metamyelocytes）、erythroblasts和板块或血液板块。传统上，pathologists和hematologists在实验室中使用显微镜进行血液细胞的识别，这是一个慢速且容易出错的手动过程。因此，自动化这个过程是非常重要。在我们的论文中，我们采用了转移学习与CNN预训练模型。VGG16、VGG19、ResNet-50、ResNet-101、ResNet-152、InceptionV3、MobileNetV2和DenseNet-20在PBC数据集的正常DIB上应用了CNN预训练模型。这些模型的总准确率在91.375%到94.72%之间。因此，我们被这些预训练模型所 inspirited，并提出了一种自动化血液细胞类型分类的模型。我们提出了一种基于CNN的框架来提高准确率。我们的提议的CNN模型在PBC数据集的正常DIB上进行测试，实验结果表明，我们的CNN模型在血液细胞类型分类方面实现了99.91%的准确率。我们的提议的 convolutional neural network模型与文献中已经报道的结果相比，表现竞争力强。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Transferability-Measurement-by-Evaluating-Intra-class-Feature-Variance"><a href="#Fast-and-Accurate-Transferability-Measurement-by-Evaluating-Intra-class-Feature-Variance" class="headerlink" title="Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance"></a>Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05986">http://arxiv.org/abs/2308.05986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snudatalab/TMI">https://github.com/snudatalab/TMI</a></li>
<li>paper_authors: Huiwen Xu, U Kang</li>
<li>for: 这个论文的目的是如何快速和准确地找到下游任务中最有用的预训练模型。</li>
<li>methods: 这个论文提出了一种名为TMI（转移性评估器）的算法，用于评估预训练模型的转移性。TMI视转移性为预训练模型在目标任务上的总体化，并通过评估模型内类差异来评估模型的适应性。</li>
<li>results: 对于多个实际数据集，TMI表现出了较好的选择性，可以快速和准确地选择预训练模型。与之前的研究相比，TMI在13个案例中展现出了更高的相关性。<details>
<summary>Abstract</summary>
Given a set of pre-trained models, how can we quickly and accurately find the most useful pre-trained model for a downstream task? Transferability measurement is to quantify how transferable is a pre-trained model learned on a source task to a target task. It is used for quickly ranking pre-trained models for a given task and thus becomes a crucial step for transfer learning. Existing methods measure transferability as the discrimination ability of a source model for a target data before transfer learning, which cannot accurately estimate the fine-tuning performance. Some of them restrict the application of transferability measurement in selecting the best supervised pre-trained models that have classifiers. It is important to have a general method for measuring transferability that can be applied in a variety of situations, such as selecting the best self-supervised pre-trained models that do not have classifiers, and selecting the best transferring layer for a target task. In this work, we propose TMI (TRANSFERABILITY MEASUREMENT WITH INTRA-CLASS FEATURE VARIANCE), a fast and accurate algorithm to measure transferability. We view transferability as the generalization of a pre-trained model on a target task by measuring intra-class feature variance. Intra-class variance evaluates the adaptability of the model to a new task, which measures how transferable the model is. Compared to previous studies that estimate how discriminative the models are, intra-class variance is more accurate than those as it does not require an optimal feature extractor and classifier. Extensive experiments on real-world datasets show that TMI outperforms competitors for selecting the top-5 best models, and exhibits consistently better correlation in 13 out of 17 cases.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:给定一个集合先进模型，如何快速和准确地找到下游任务中最有用的先进模型？转移可量度是用于衡量先进模型在源任务上学习后，转移到目标任务上的抽象能力。现有的方法通常是通过计算源模型对目标数据的分类能力来衡量转移可量度，这不能准确地估计微调性能。一些方法还限制了转移可量度的测量在选择最佳监督式先进模型中应用。因此，有一个通用的方法可以在多种情况下测量转移可量度，如选择最佳无监督式先进模型和选择最佳转移层。在这项工作中，我们提出了TMI（转移可量度测量与内类特征异常）算法，它是一种快速和准确的转移可量度测量方法。我们视转移可量度为将先进模型在目标任务上通过测量内类特征异常来衡量。内类异常评估模型在新任务上适应度，这也衡量了模型的转移可量度。与之前的研究所计算的模型掌握性相比，内类异常更准确，因为它不需要优化特征提取器和分类器。我们在实际世界数据集上进行了广泛的实验，显示TMI在选择top-5最佳模型时高效，并在13个 случа中展现了更高的相关性。
</details></li>
</ul>
<hr>
<h2 id="Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment"><a href="#Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment" class="headerlink" title="Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment"></a>Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06299">http://arxiv.org/abs/2308.06299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Vogt, Stefan Buehler, Mark Schutera</li>
<li>for: addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving</li>
<li>methods: encapsulating the neural network under deployment within an uncertainty estimation envelope based on Monte Carlo Dropout, without modifying the deployed neural network</li>
<li>results: demonstrating the applicability of the method for multiple different potential deployment shifts relevant to autonomous driving, including transitions into the night, rainy, or snowy domain, and enabling operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.Here’s the same information in Simplified Chinese:</li>
<li>for: 解决神经网络 semantic segmentation 自动驾驶中的不可预测性和域shift问题</li>
<li>methods: 基于 Monte Carlo Dropout 的 epistemic uncertainty 估计方法，不需要修改部署 neural network</li>
<li>results: 对各种自动驾驶中可能的部署变化进行了演示，包括夜晚、雨天和雪天等域shift，并实现了运行设计域认知via uncertainty，允许DEFENSIVE PERCEPTION、安全状态触发、警告通知和测试或开发和适应性改进的感知栈反馈。<details>
<summary>Abstract</summary>
In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's performance, enabling monitoring and notification of entering domains of reduced neural network performance under deployment. Furthermore, our envelope is extended by novel methods to improve the application in deployment settings, including reducing compute expenses and confining estimation noise. Finally, we demonstrate the applicability of our method for multiple different potential deployment shifts relevant to autonomous driving, such as transitions into the night, rainy, or snowy domain. Overall, our approach shows great potential for application in deployment settings and enables operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法来解决自适应驾驶 neural network 中的不注意性投入和领域转换问题。我们的方法基于深度学习基于自适应驾驶的感知系统是不确定的，最好表示为一个概率分布。自驾驶车辆的安全性 Paramount，因此感知系统必须能够识别车辆离开操作设计领域，预测危险不确定性，并降低感知系统的性能。为此，我们提议将投入 neural network 内部的深度学习模型包装在一个不确定性估计膜中，该膜基于 Monte Carlo Dropout 方法来估计模型的 epistemic 不确定性。这种方法不需要修改已经部署的 neural network，并且保证模型的预期性能。我们的防御感知膜可以估计 neural network 的性能，并且可以监测和通知车辆进入性能下降的领域。此外，我们还提出了一些新的方法来改进在部署Setting中的应用，包括减少计算成本和限制估计噪声。最后，我们示出了我们方法在多种不同的部署转换中的应用可能性，例如在夜晚、雨天或雪天等领域。总之，我们的方法在部署Setting中表现出了很好的应用潜力，并允许操作设计领域的认知，以及发出警告通知、测试或开发和适应感知堆。
</details></li>
</ul>
<hr>
<h2 id="An-Encoder-Decoder-Approach-for-Packing-Circles"><a href="#An-Encoder-Decoder-Approach-for-Packing-Circles" class="headerlink" title="An Encoder-Decoder Approach for Packing Circles"></a>An Encoder-Decoder Approach for Packing Circles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07335">http://arxiv.org/abs/2308.07335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Kiran Jose, Gangadhar Karevvanavar, Rajshekhar V Bhat</li>
<li>for: 本文关于如何封装小对象在大对象中，以实现不 overlap 和 minimum overlap 的目标。</li>
<li>methods: 本文提出了一种基于encoder-decoder架构的方法，包括encoder块、perturbation块和decoder块。encoder块通过normalization层输出中心点，perturbation块添加控制的偏移，确保中心点不超过小对象的半径，decoder块使用偏移中心点来估计 intend circle index。</li>
<li>results: 该方法可以 Parametrize encoder和decoder使用神经网络，并通过优化减少 decoder 估计的误差和实际输入 encoder 中心点的差异，从而实现不 overlap 和 minimum overlap 的目标。该方法可以对高维度和不同形状的对象进行扩展。<details>
<summary>Abstract</summary>
The problem of packing smaller objects within a larger object has been of interest since decades. In these problems, in addition to the requirement that the smaller objects must lie completely inside the larger objects, they are expected to not overlap or have minimum overlap with each other. Due to this, the problem of packing turns out to be a non-convex problem, obtaining whose optimal solution is challenging. As such, several heuristic approaches have been used for obtaining sub-optimal solutions in general, and provably optimal solutions for some special instances. In this paper, we propose a novel encoder-decoder architecture consisting of an encoder block, a perturbation block and a decoder block, for packing identical circles within a larger circle. In our approach, the encoder takes the index of a circle to be packed as an input and outputs its center through a normalization layer, the perturbation layer adds controlled perturbations to the center, ensuring that it does not deviate beyond the radius of the smaller circle to be packed, and the decoder takes the perturbed center as input and estimates the index of the intended circle for packing. We parameterize the encoder and decoder by a neural network and optimize it to reduce an error between the decoder's estimated index and the actual index of the circle provided as input to the encoder. The proposed approach can be generalized to pack objects of higher dimensions and different shapes by carefully choosing normalization and perturbation layers. The approach gives a sub-optimal solution and is able to pack smaller objects within a larger object with competitive performance with respect to classical methods.
</details>
<details>
<summary>摘要</summary>
这个问题已经引起关注了几十年。在这些问题中，除了要求小对象完全 locate 在大对象中之外，还要求小对象之间不会 overlap 或者最小化 overlap。由于这个原因，packing 问题变成了非凸问题，获得优化解决方案是困难的。为此，许多启发性方法被用来获得不优化解决方案，以及对特殊情况下的可证优化解决方案。在这篇论文中，我们提出了一种新的编码器-解码器架构，包括编码器块、抖动块和解码器块，用于将同形圆包含在大圆中。在我们的方法中，编码器接受圆的索引作为输入，并通过正规化层输出圆心，抖动层添加控制的偏移，使圆心不会超过小圆的半径，而解码器接受偏移后的圆心作为输入，并估算圆的索引。我们使用神经网络参数化编码器和解码器，并优化它们以降低由解码器估算的圆索引与实际输入圆索引之间的错误。我们的方法可以通过选择正规化和抖动层来扩展到包含高维度和不同形状的对象。该方法可以提供竞争性的非优化解决方案，并将小对象包含在大对象中。
</details></li>
</ul>
<hr>
<h2 id="Learning-nonparametric-DAGs-with-incremental-information-via-high-order-HSIC"><a href="#Learning-nonparametric-DAGs-with-incremental-information-via-high-order-HSIC" class="headerlink" title="Learning nonparametric DAGs with incremental information via high-order HSIC"></a>Learning nonparametric DAGs with incremental information via high-order HSIC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05969">http://arxiv.org/abs/2308.05969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafei Wang, Jianguo Liu</li>
<li>for: 本文 targets at learning Bayesian networks (BN) by maximizing global score functions, but it addresses the issue of local variables having direct and indirect dependence simultaneously, which can lead to missed edges in the global optimization.</li>
<li>methods: 本文提出了一个可 identificability condition based on a determined subset of parents，并开发了一个两阶段算法（OT algorithm）来解决本问题。在第一阶段，使用 first-order Hilbert-Schmidt independence criterion (HSIC) 得到一个初始确定的父集。在第二阶段，使用 theoretically proved incremental properties of high-order HSIC 进行了本地调整。</li>
<li>results:  numrical experiments on different synthetic datasets and real-world datasets show that the OT algorithm outperforms existing methods, especially in Sigmoid Mix model with the size of the graph being d&#x3D;40, the structure intervention distance (SID) of the OT algorithm is 329.7 smaller than the one obtained by CAM, indicating that the graph estimated by the OT algorithm misses fewer edges compared with CAM.<details>
<summary>Abstract</summary>
Score-based methods for learning Bayesain networks(BN) aim to maximizing the global score functions. However, if local variables have direct and indirect dependence simultaneously, the global optimization on score functions misses edges between variables with indirect dependent relationship, of which scores are smaller than those with direct dependent relationship. In this paper, we present an identifiability condition based on a determined subset of parents to identify the underlying DAG. By the identifiability condition, we develop a two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the global optimization. In the optimal phase, an optimization problem based on first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated skeleton as the initial determined parents subset. In the tuning phase, the skeleton is locally tuned by deletion, addition and DAG-formalization strategies using the theoretically proved incremental properties of high-order HSIC. Numerical experiments for different synthetic datasets and real-world datasets show that the OT algorithm outperforms existing methods. Especially in Sigmoid Mix model with the size of the graph being ${\rm\bf d=40}$, the structure intervention distance (SID) of the OT algorithm is 329.7 smaller than the one obtained by CAM, which indicates that the graph estimated by the OT algorithm misses fewer edges compared with CAM.
</details>
<details>
<summary>摘要</summary>
Score-based方法学习 bayesian网络（BN）目的是最大化全局分数函数。然而，如果本地变量同时具有直接和间接依赖关系，全局优化分数函数会忽略变量之间的间接依赖关系中的边，其分数较直接依赖关系中的边小。在这篇论文中，我们提出了一个可识别条件，基于确定的父集来识别下面的DAG。通过可识别条件，我们开发了一个两阶段算法，称为最优调整（OT）算法。在优化阶段，基于第一阶段希尔伯特- Schmidt独立性标准（HSIC）的优化问题提供了一个初始确定父集的skeleton。在调整阶段，skeleton通过删除、添加和DAG-形式化策略进行了本地调整，使用了理论上证明的高阶HSIC的增量性质。numerical experiments表明，OT算法在不同的synthetic数据集和实际数据集上的性能都高于现有方法。特别是在sigmoid mix模型中，OT算法的结构间断距（SID）为329.7，与CAM所获得的结构间断距相比，表示OT算法估算的图 missed fewer edges。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review"><a href="#Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review" class="headerlink" title="Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review"></a>Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06296">http://arxiv.org/abs/2308.06296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Arslan Shaukat</li>
<li>for: 这篇论文的目的是对医疗影像分析中的白血球分类进行深入分析，并评估现代技术在这个领域的应用。</li>
<li>methods: 这篇论文使用了许多现代技术，包括机器学习（ML）和深度学习（DL），以提高医疗影像分析的准确性和分类精度。</li>
<li>results: 这篇论文发现，过去的17年间，医疗影像分析中的白血球分类方法有所进步，并且使用了许多不同的技术和数据来进行分析。但是，还有一些挑战需要解决，例如获得适当的数据集和增强医疗培训。<details>
<summary>Abstract</summary>
Machine learning (ML) and deep learning (DL) models have been employed to significantly improve analyses of medical imagery, with these approaches used to enhance the accuracy of prediction and classification. Model predictions and classifications assist diagnoses of various cancers and tumors. This review presents an in-depth analysis of modern techniques applied within the domain of medical image analysis for white blood cell classification. The methodologies that use blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains are identified and discussed, with a detailed analysis of ML/DL techniques applied to the classification of white blood cells (WBCs) representing the primary focus of the review. The data utilized in this research has been extracted from a collection of 136 primary papers that were published between the years 2006 and 2023. The most widely used techniques and best-performing white blood cell classification methods are identified. While the use of ML and DL for white blood cell classification has concurrently increased and improved in recent year, significant challenges remain - 1) Availability of appropriate datasets remain the primary challenge, and may be resolved using data augmentation techniques. 2) Medical training of researchers is recommended to improve current understanding of white blood cell structure and subsequent selection of appropriate classification models. 3) Advanced DL networks including Generative Adversarial Networks, R-CNN, Fast R-CNN, and faster R-CNN will likely be increasingly employed to supplement or replace current techniques.
</details>
<details>
<summary>摘要</summary>
医学影像分析（ML）和深度学习（DL）模型已经被应用到医疗影像分析中，以提高预测和分类的准确性。这些方法可以帮助诊断多种恶性肿瘤和癌症。本文总结了现代医学影像分析领域中使用ML/DL技术进行白血球类型分类的方法。这些方法包括血液滴血图像、核磁共振成像（MRI）、X射线成像等医学影像领域，并进行了详细的ML/DL技术应用于白血球类型分类的分析。研究使用的数据来自于2006年至2023年发表的136篇原始论文。最常用的技术和最佳白血球类型分类方法被识别出来。虽然在过去几年内，用ML和DL进行白血球类型分类的使用和提高在不断增长，但还存在一些挑战，包括：1）获得适当数据集的可用性问题，可以通过数据扩展技术解决。2）医学研究人员的培训，以提高白血球结构的理解，并选择合适的分类模型。3）将来，高级的深度学习网络，如生成对抗网络、R-CNN、快速R-CNN和更快的R-CNN将被广泛应用，以补充或取代当前的方法。
</details></li>
</ul>
<hr>
<h2 id="Learned-Point-Cloud-Compression-for-Classification"><a href="#Learned-Point-Cloud-Compression-for-Classification" class="headerlink" title="Learned Point Cloud Compression for Classification"></a>Learned Point Cloud Compression for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05959">http://arxiv.org/abs/2308.05959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification">https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification</a></li>
<li>paper_authors: Mateen Ulhaq, Ivan V. Bajić</li>
<li>for: 本研究旨在提出一种特种的点云编码器，用于在服务器端进行机器分析 tasks 的点云数据传输。</li>
<li>methods: 我们基于 PointNet 提出了一种特种的点云编码器，实现了与非特种编码器相比significantly better的Rate-Accuracy 质量比。</li>
<li>results: 我们的编码器在 ModelNet40 数据集上实现了94%的BD-比特率减少，而且对于低资源的终端设备，我们还提出了两种轻量级的编码器配置，可以实现相似的BD-比特率减少（93%和92%），同时只消耗0.470和0.048 encoder-side kMACs&#x2F;点。<details>
<summary>Abstract</summary>
Deep learning is increasingly being used to perform machine vision tasks such as classification, object detection, and segmentation on 3D point cloud data. However, deep learning inference is computationally expensive. The limited computational capabilities of end devices thus necessitate a codec for transmitting point cloud data over the network for server-side processing. Such a codec must be lightweight and capable of achieving high compression ratios without sacrificing accuracy. Motivated by this, we present a novel point cloud codec that is highly specialized for the machine task of classification. Our codec, based on PointNet, achieves a significantly better rate-accuracy trade-off in comparison to alternative methods. In particular, it achieves a 94% reduction in BD-bitrate over non-specialized codecs on the ModelNet40 dataset. For low-resource end devices, we also propose two lightweight configurations of our encoder that achieve similar BD-bitrate reductions of 93% and 92% with 3% and 5% drops in top-1 accuracy, while consuming only 0.470 and 0.048 encoder-side kMACs/point, respectively. Our codec demonstrates the potential of specialized codecs for machine analysis of point clouds, and provides a basis for extension to more complex tasks and datasets in the future.
</details>
<details>
<summary>摘要</summary>
深度学习在处理3D点云数据上进行机器视觉任务，如分类、物体检测和分割，日益受到欢迎。然而，深度学习推理过程具有计算成本高的问题，因此在终端设备上进行处理时需要一个点云编码器。这个编码器应该轻量级，能够实现高度压缩比，而无需牺牲准确性。为了解决这个问题，我们提出了一种特种的点云编码器，基于PointNet，可以在机器分类任务中实现显著更好的比例-准确性质量。具体来说，我们的编码器在ModelNet40数据集上实现了94%的BD-比特率减少，相比非特种编码器。而为了适应低资源的终端设备，我们还提出了两种轻量级的编码器配置，它们可以实现类似的BD-比特率减少，分别为93%和92%，但是消耗了0.470和0.048个encoder-side kMACs/点。我们的编码器表明特种编码器在机器分析点云数据时具有潜在的优势，并为未来扩展到更复杂的任务和数据集提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Node-Embedding-for-Homophilous-Graphs-with-ARGEW-Augmentation-of-Random-walks-by-Graph-Edge-Weights"><a href="#Node-Embedding-for-Homophilous-Graphs-with-ARGEW-Augmentation-of-Random-walks-by-Graph-Edge-Weights" class="headerlink" title="Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights"></a>Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05957">http://arxiv.org/abs/2308.05957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ncsoft/argew">https://github.com/ncsoft/argew</a></li>
<li>paper_authors: Jun Hee Kim, Jaeman Son, Hyunsoo Kim, Eunjo Lee</li>
<li>for: 本文是针对 dense vector  represent nodes in network 的研究，尤其是Weighted homophilous graphs中 node pairs with stronger edges weights 应该有更加相近的 embedding。</li>
<li>methods: 本文提出了 ARGEW（Augmentation of Random walks by Graph Edge Weights），一种基于随机漫步的增强方法，可以使得 node embeddings 更加准确地反映 edge weights。</li>
<li>results: 在多个实际网络上，ARGEW 可以使得 node pairs with larger edge weights 有更加相近的 embedding，并且在 node classification 任务中，ARGEW 可以提高 node2vec 的性能，并且不受 hyperparameters 的影响。<details>
<summary>Abstract</summary>
Representing nodes in a network as dense vectors node embeddings is important for understanding a given network and solving many downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ do work for weighted networks via including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already-performed walks. With several real-world networks, we demonstrate that with ARGEW, compared to not using it, the desired pattern that node pairs with larger edge weights have closer embeddings is much clearer. We also examine ARGEW's performance in node classification: node2vec with ARGEW outperforms pure node2vec and is not sensitive to hyperparameters (i.e. consistently good). In fact, it achieves similarly good results as supervised GCN, even without any node feature or label information during training. Finally, we explain why ARGEW works consistently well by exploring the coappearance distributions using a synthetic graph with clear structural roles.
</details>
<details>
<summary>摘要</summary>
importance of representing nodes in a network as dense vectors (node embeddings) for understanding the network and solving downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ can work for weighted networks by including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already-performed walks. With several real-world networks, we demonstrate that with ARGEW, compared to not using it, the desired pattern that node pairs with larger edge weights have closer embeddings is much clearer. We also examine ARGEW's performance in node classification: node2vec with ARGEW outperforms pure node2vec and is not sensitive to hyperparameters (i.e. consistently good). In fact, it achieves similarly good results as supervised GCN, even without any node feature or label information during training. Finally, we explain why ARGEW works consistently well by exploring the coappearance distributions using a synthetic graph with clear structural roles.
</details></li>
</ul>
<hr>
<h2 id="INR-Arch-A-Dataflow-Architecture-and-Compiler-for-Arbitrary-Order-Gradient-Computations-in-Implicit-Neural-Representation-Processing"><a href="#INR-Arch-A-Dataflow-Architecture-and-Compiler-for-Arbitrary-Order-Gradient-Computations-in-Implicit-Neural-Representation-Processing" class="headerlink" title="INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing"></a>INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05930">http://arxiv.org/abs/2308.05930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Abi-Karam, Rishov Sarkar, Dejia Xu, Zhiwen Fan, Zhangyang Wang, Cong Hao</li>
<li>for: 这个论文主要用于探讨nth-order gradient计算在图形学、元学习（MAML）、科学计算和最近的隐藏神经表示（INR）中的应用。</li>
<li>methods: 这个论文使用了一种叫做INR-Arch的框架，它可以将计算图的nth-order gradient转换成一个硬件优化的数据流体系结构。这个框架包括两个阶段：首先，设计了一个高效的数据流体系结构，其中使用了FIFO流和优化的计算kernels库，以确保高效的内存利用和并行计算。其次，提出了一种编译器，它可以自动从计算图中提取和优化计算，并配置硬件参数 such as 延迟和流深度以优化吞吐量，保证不会出现死锁现象，并生成高级合成（HLS）代码 дляFPGA实现。</li>
<li>results: 这个论文通过对INR编辑作为测试样本，实现了对CPU和GPU基线的1.8-4.8倍和1.5-3.6倍的速度提升，同时也实现了对内存使用的3.1-8.9倍和1.7-4.3倍的减少，以及对能效率的1.7-11.3倍和5.5-32.8倍的下降。<details>
<summary>Abstract</summary>
An increasing number of researchers are finding use for nth-order gradient computations for a wide variety of applications, including graphics, meta-learning (MAML), scientific computing, and most recently, implicit neural representations (INRs). Recent work shows that the gradient of an INR can be used to edit the data it represents directly without needing to convert it back to a discrete representation. However, given a function represented as a computation graph, traditional architectures face challenges in efficiently computing its nth-order gradient due to the higher demand for computing power and higher complexity in data movement. This makes it a promising target for FPGA acceleration. In this work, we introduce INR-Arch, a framework that transforms the computation graph of an nth-order gradient into a hardware-optimized dataflow architecture. We address this problem in two phases. First, we design a dataflow architecture that uses FIFO streams and an optimized computation kernel library, ensuring high memory efficiency and parallel computation. Second, we propose a compiler that extracts and optimizes computation graphs, automatically configures hardware parameters such as latency and stream depths to optimize throughput, while ensuring deadlock-free operation, and outputs High-Level Synthesis (HLS) code for FPGA implementation. We utilize INR editing as our benchmark, presenting results that demonstrate 1.8-4.8x and 1.5-3.6x speedup compared to CPU and GPU baselines respectively. Furthermore, we obtain 3.1-8.9x and 1.7-4.3x lower memory usage, and 1.7-11.3x and 5.5-32.8x lower energy-delay product. Our framework will be made open-source and available on GitHub.
</details>
<details>
<summary>摘要</summary>
更多研究人员正在发现使用 nth-order Gradient 计算在各种应用程序中，包括图形学、多学习（MAML）、科学计算和最近的隐藏神经表示（INRs）。最新的研究表明，INR 的 Gradient 可以直接编辑所表示的数据，而无需将其转换回分割表示。然而，传统架构在计算 Graph 中的 nth-order Gradient 计算时面临着更高的计算能力和数据移动复杂性的挑战，这使得它成为了可 acceleration 的目标。在这项工作中，我们介绍 INR-Arch，一个将计算 Graph 转换为硬件优化数据流架构的框架。我们解决这个问题在两个阶段。首先，我们设计了一个数据流架构，使用 FIFO 流和优化的计算内核库，保证高内存效率和并行计算。其次，我们提出了一个编译器，可以提取和优化计算 Graph，自动配置硬件参数 such as 延迟和流深度，以优化通过put，并确保不会出现堵塞的操作。我们使用 INR 编辑作为我们的标准，发表了结果，表明在 CPU 和 GPU 基线相比，得到了 1.8-4.8 倍和 1.5-3.6 倍的速度提升。此外，我们获得了 3.1-8.9 倍和 1.7-4.3 倍的内存使用量减少，以及 1.7-11.3 倍和 5.5-32.8 倍的能量延迟产品。我们的框架即将被开源，并在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-equivalence-of-Occam-algorithms"><a href="#On-the-equivalence-of-Occam-algorithms" class="headerlink" title="On the equivalence of Occam algorithms"></a>On the equivalence of Occam algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05906">http://arxiv.org/abs/2308.05906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaman Keinath-Esmail</li>
<li>for: 本文提供了一个后验正则化的基础，即任何可学习的概念类都可以由Occam算法学习。</li>
<li>methods: 本文使用了Board和Pitt（1990）提出的一种受到例外列表的影响的Occam算法，并证明了这种算法可以学习任何可学习的概念类。</li>
<li>results: 本文证明了Board和Pitt（1990）的一种受到例外列表的影响的Occam算法可以学习任何可学习的概念类，并且这种算法的复杂度是$\delta$-无关的。<details>
<summary>Abstract</summary>
Blumer et al. (1987, 1989) showed that any concept class that is learnable by Occam algorithms is PAC learnable. Board and Pitt (1990) showed a partial converse of this theorem: for concept classes that are closed under exception lists, any class that is PAC learnable is learnable by an Occam algorithm. However, their Occam algorithm outputs a hypothesis whose complexity is $\delta$-dependent, which is an important limitation. In this paper, we show that their partial converse applies to Occam algorithms with $\delta$-independent complexities as well. Thus, we provide a posteriori justification of various theoretical results and algorithm design methods which use the partial converse as a basis for their work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparing-the-quality-of-neural-network-uncertainty-estimates-for-classification-problems"><a href="#Comparing-the-quality-of-neural-network-uncertainty-estimates-for-classification-problems" class="headerlink" title="Comparing the quality of neural network uncertainty estimates for classification problems"></a>Comparing the quality of neural network uncertainty estimates for classification problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05903">http://arxiv.org/abs/2308.05903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Ries, Joshua Michalenko, Tyler Ganter, Rashad Imad-Fayez Baiyasi, Jason Adams<br>for:这个论文的目的是评估深度学习模型中的不确定性评估方法的质量。methods:这个论文使用了统计方法来评估信任区间的覆盖率和信任间隔，以及预测分类预测信任的均值误差来评估深度学习模型的不确定性评估方法。results:研究发现，不同的不确定性评估方法在同一个数据集上可以生成不同的结果，而且这些结果之间存在差异。此外，研究还发现，使用MCMC和VI方法可以获得更好的不确定性评估结果，而使用DE和MC dropout方法的结果则更为不稳定。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) models are powerful classifiers, but many approaches do not provide uncertainties for their estimates. Uncertainty quantification (UQ) methods for DL models have received increased attention in the literature due to their usefulness in decision making, particularly for high-consequence decisions. However, there has been little research done on how to evaluate the quality of such methods. We use statistical methods of frequentist interval coverage and interval width to evaluate the quality of credible intervals, and expected calibration error to evaluate classification predicted confidence. These metrics are evaluated on Bayesian neural networks (BNN) fit using Markov Chain Monte Carlo (MCMC) and variational inference (VI), bootstrapped neural networks (NN), Deep Ensembles (DE), and Monte Carlo (MC) dropout. We apply these different UQ for DL methods to a hyperspectral image target detection problem and show the inconsistency of the different methods' results and the necessity of a UQ quality metric. To reconcile these differences and choose a UQ method that appropriately quantifies the uncertainty, we create a simulated data set with fully parameterized probability distribution for a two-class classification problem. The gold standard MCMC performs the best overall, and the bootstrapped NN is a close second, requiring the same computational expense as DE. Through this comparison, we demonstrate that, for a given data set, different models can produce uncertainty estimates of markedly different quality. This in turn points to a great need for principled assessment methods of UQ quality in DL applications.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）模型是强大的分类器，但许多方法不提供不确定性的估计。不确定性量化（UQ）方法 для DL 模型在文献中收到了更多的关注，因为它们在决策中非常有用，特别是对高 conseqüência 的决策。然而，对 UQ 方法评价的研究相对较少。我们使用统计方法的频率interval coverage和interval width来评价credible interval的质量，以及预期准确性error来评价分类预测的自信度。这些指标在bayesian neural network（BNN）适用markov chain Monte Carlo（MCMC）和variational inference（VI）、bootstrapped neural network（NN）、deep ensembles（DE）和Monte Carlo（MC）dropout中被评价。我们对这些不同的 UQ 方法应用到一个 hyperspectral image target detection问题，并显示了不同方法的结果之间的不一致，以及需要一个 UQ 质量指标。为了解决这些不一致并选择一个正确地量化不确定性的 UQ 方法，我们创建了一个完全参数化的概率分布的 simulated data set，用于两类分类问题。金标准 MCMC 表现最佳，而 bootstrapped NN 紧随其后，需要与 DE 相同的计算成本。通过这种比较，我们证明了，对于给定的数据集，不同的模型可以生成不同质量的不确定性估计。这一点点到了深度学习应用中需要原则性评价 UQ 质量的强需求。
</details></li>
</ul>
<hr>
<h2 id="Target-Detection-on-Hyperspectral-Images-Using-MCMC-and-VI-Trained-Bayesian-Neural-Networks"><a href="#Target-Detection-on-Hyperspectral-Images-Using-MCMC-and-VI-Trained-Bayesian-Neural-Networks" class="headerlink" title="Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks"></a>Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06293">http://arxiv.org/abs/2308.06293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Ries, Jason Adams, Joshua Zollweg</li>
<li>for: 这篇论文的目的是提出了一种 bayesian neural network (BNN) 的应用，以便在图像分类 tasks 中提供 uncertainty quantification (UQ)。</li>
<li>methods: 这篇论文使用了 MCMC 和 VI 两种不同的训练方法，以评估 BNN 的训练效果。</li>
<li>results: 研究发现，MCMC 和 VI 两种训练方法都可以达到良好的检测效果，但是 VI 方法的计算速度较快。此外，研究还发现，不同的训练方法可能会导致不同的模型结果，特别是在高风险应用中。<details>
<summary>Abstract</summary>
Neural networks (NN) have become almost ubiquitous with image classification, but in their standard form produce point estimates, with no measure of confidence. Bayesian neural networks (BNN) provide uncertainty quantification (UQ) for NN predictions and estimates through the posterior distribution. As NN are applied in more high-consequence applications, UQ is becoming a requirement. BNN provide a solution to this problem by not only giving accurate predictions and estimates, but also an interval that includes reasonable values within a desired probability. Despite their positive attributes, BNN are notoriously difficult and time consuming to train. Traditional Bayesian methods use Markov Chain Monte Carlo (MCMC), but this is often brushed aside as being too slow. The most common method is variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy. We apply and compare MCMC- and VI-trained BNN in the context of target detection in hyperspectral imagery (HSI), where materials of interest can be identified by their unique spectral signature. This is a challenging field, due to the numerous permuting effects practical collection of HSI has on measured spectra. Both models are trained using out-of-the-box tools on a high fidelity HSI target detection scene. Both MCMC- and VI-trained BNN perform well overall at target detection on a simulated HSI scene. This paper provides an example of how to utilize the benefits of UQ, but also to increase awareness that different training methods can give different results for the same model. If sufficient computational resources are available, the best approach rather than the fastest or most efficient should be used, especially for high consequence problems.
</details>
<details>
<summary>摘要</summary>
neural networks (NN) 已经在图像分类中变得极其普遍，但在标准形式下产生点估计，无法提供信度量。 bayesian neural networks (BNN) 提供图像分类预测和估计的不确定性评估（UQ），通过 posterior distribution。 随着 NN 在高重要性应用中使用，UQ 变得必须。 BNN 不仅提供准确的预测和估计，还提供一个包含合理值的时间间隔，在所需的概率范围内。 despite their positive attributes, BNN 很难和时间consuming 进行训练。 traditional Bayesian methods 使用 markov chain Monte Carlo (MCMC)，但这经常被认为是太慢。 the most common method 是 variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy。 我们在 target detection 中应用和比较 MCMC- 和 VI-trained BNN 在 hyperspectral imagery (HSI) 中，where materials of interest can be identified by their unique spectral signature。 this is a challenging field, due to the numerous permuting effects practical collection of HSI has on measured spectra。 both models are trained using out-of-the-box tools on a high fidelity HSI target detection scene。 both MCMC- 和 VI-trained BNN perform well overall at target detection on a simulated HSI scene。 this paper provides an example of how to utilize the benefits of UQ, but also to increase awareness that different training methods can give different results for the same model。 if sufficient computational resources are available, the best approach rather than the fastest or most efficient should be used, especially for high consequence problems。
</details></li>
</ul>
<hr>
<h2 id="The-divergence-time-of-protein-structures-modelled-by-Markov-matrices-and-its-relation-to-the-divergence-of-sequences"><a href="#The-divergence-time-of-protein-structures-modelled-by-Markov-matrices-and-its-relation-to-the-divergence-of-sequences" class="headerlink" title="The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences"></a>The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06292">http://arxiv.org/abs/2308.06292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandun Rajapaksa, Lloyd Allison, Peter J. Stuckey, Maria Garcia de la Banda, Arun S. Konagurthu</li>
<li>for: 这种研究是为了开发一种基于蛋白质结构的时间参数化统计模型，以量化蛋白质结构在演化过程中的差异进化。</li>
<li>methods: 该研究使用了一个大量的蛋白质三维结构对比来推断一种时间参数化的统计模型，并使用了 bayesian 和信息理论的框架来推断时间参数化的随机矩阵和 Dirichlet 模型。</li>
<li>results: 研究发现，使用这种时间参数化统计模型可以更准确地估计蛋白质结构之间的分化时间，并且可以与序列相关性的时间参数化模型进行比较。此外，该模型还可以在次结构预测中与常见的神经网络架构竞争。<details>
<summary>Abstract</summary>
A complete time-parameterized statistical model quantifying the divergent evolution of protein structures in terms of the patterns of conservation of their secondary structures is inferred from a large collection of protein 3D structure alignments. This provides a better alternative to time-parameterized sequence-based models of protein relatedness, that have clear limitations dealing with twilight and midnight zones of sequence relationships. Since protein structures are far more conserved due to the selection pressure directly placed on their function, divergence time estimates can be more accurate when inferred from structures. We use the Bayesian and information-theoretic framework of Minimum Message Length to infer a time-parameterized stochastic matrix (accounting for perturbed structural states of related residues) and associated Dirichlet models (accounting for insertions and deletions during the evolution of protein domains). These are used in concert to estimate the Markov time of divergence of tertiary structures, a task previously only possible using proxies (like RMSD). By analyzing one million pairs of homologous structures, we yield a relationship between the Markov divergence time of structures and of sequences. Using these inferred models and the relationship between the divergence of sequences and structures, we demonstrate a competitive performance in secondary structure prediction against neural network architectures commonly employed for this task. The source code and supplementary information are downloadable from \url{http://lcb.infotech.monash.edu.au/sstsum}.
</details>
<details>
<summary>摘要</summary>
一个完整的时间参数化统计模型，用于描述蛋白质结构的不同演化的 Patterns of conservation of secondary structures，从一个大量的蛋白质三维结构对Alignment中得到了推断。这提供了一个更好的代替时间参数化序列基于模型，该模型在处理晚上和午夜时区的序列关系时存在显著的限制。由于蛋白质结构受直接选择压力的影响，因此从结构来进行演化时间估计的准确性比序列基于模型更高。我们使用 bayesian 和信息理论的框架，Minimum Message Length 来推断时间参数化随机矩阵（考虑相关的结构态态）和 Dirichlet 模型（考虑插入和删除 durante protein domains 的演化）。这些模型在一起使用，以估计蛋白质结构的马克夫时间异同。在分析一百万对同源结构的情况下，我们发现了结构异同时间和序列异同时间之间的关系。使用这些推断出的模型和序列异同时间之间的关系，我们展示了在二级结构预测中与通用的神经网络架构相比，我们的表现是竞争性的。源代码和补充信息可以从 \url{http://lcb.infotech.monash.edu.au/sstsum} 下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding"><a href="#Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding" class="headerlink" title="Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding"></a>Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05893">http://arxiv.org/abs/2308.05893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran</li>
<li>for: 本研究旨在探讨 Deep Reinforcement Learning（DRL）在多体系统中的应用，尤其是在多体路径找索中。</li>
<li>methods: 本文使用了多种 Deep Reinforcement Learning（DRL）方法，包括价值函数法和策略法，以解决多体系统中的路径找索问题。</li>
<li>results: 本文提供了一个统一的评价指标集，以便比较不同的多体路径找索算法的性能。此外，本文还发现了模型基于的DRL在多体系统中的潜在应用潜力，并提供了相关的基础知识。<details>
<summary>Abstract</summary>
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified metrics for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.
</details>
<details>
<summary>摘要</summary>
多智能路径找索（MAPF）是许多大规模 роботи库应用中的关键领域，经常作为多智能系统的基础步骤。然而，随着环境的增加复杂性，MAPF的现有解决方案的效果逐渐减退。与其他研究不同，我们的工作在这篇评论文中不仅提供了近期MAPF的进展概述，还广泛评论了深度强化学习（DRL）在多智能系统设置中的应用。此外，我们的工作还强调了评价MAPF解决方案的缺乏统一评价指标，并提供了全面的解释。最后，我们的文章还讨论了基于模型的DRL作为未来方向的潜在发展，并提供了相应的基础理解，以解决当前MAPF中的挑战。我们的目标是帮助读者更深入了解当前的研究方向，提供统一的评价指标，以及扩展他们对基于模型的DRL的知识，以Addressing the existing challenges in MAPF。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DF2-Distribution-Free-Decision-Focused-Learning"><a href="#DF2-Distribution-Free-Decision-Focused-Learning" class="headerlink" title="DF2: Distribution-Free Decision-Focused Learning"></a>DF2: Distribution-Free Decision-Focused Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05889">http://arxiv.org/abs/2308.05889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingkai Kong, Wenhao Mu, Jiaming Cui, Yuchen Zhuang, B. Aditya Prakash, Bo Dai, Chao Zhang</li>
<li>for: 这篇论文是针对predict-then-optimize问题的decision-focused learning（DFL）方法，对于exististing end-to-end DFL方法的三个瓶颈：模型误差错误、抽象描述错误和梯度近似错误。</li>
<li>methods: 我们的DF2方法是第一个不需要任务特定的预测器，直接在训练过程中学习预期优化目标函数。我们创造了一个注意力基于分布的模型架构，以便对预期目标函数进行有效的学习。</li>
<li>results: 我们在一个实验中，用DF2方法解决了一个 sintetic问题、一个风力发电问题和一个非凸疫苗分布问题，得到了DF2方法的有效性。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending on a task-specific forecaster that requires precise model assumptions, our method directly learns the expected optimization function during training. To efficiently learn the function in a data-driven manner, we devise an attention-based model architecture inspired by the distribution-based parameterization of the expected objective. Our method is, to the best of our knowledge, the first to address all three bottlenecks within a single model. We evaluate DF2 on a synthetic problem, a wind power bidding problem, and a non-convex vaccine distribution problem, demonstrating the effectiveness of DF2.
</details>
<details>
<summary>摘要</summary>
决策关注学习（DFL）是一种有力的方法，用于解决预测后优化问题，通过自适应一个预测模型来满足下游优化任务。然而，现有的端到端DFL方法受到三种主要瓶颈的限制：模型匹配错误、样本平均化错误和梯度估计错误。模型匹配错误来自预测模型中参数化的预测分布与真实概率分布之间的不一致。样本平均化错误发生在使用有限样本来估计优化目标函数的期望值时。梯度估计错误则是因为DFL通过KKT条件来计算梯度，而大多数方法在非拟合目标函数中使用较差的梯度估计进行反向传播。在这篇论文中，我们提出了DF2方法——首先的无模型匹配的决策关注学习方法，用于解决这三种瓶颈。而不是基于任务特定的预测器，我们的方法直接在训练过程中学习预测函数。为效率地学习函数，我们设计了一种注意力基于分布的模型架构，得益于分布基于参数化的预测目标函数。我们的方法是，到目前为止所知道的第一个能够同时解决这三种瓶颈的单一模型。我们在一个 sintetic问题、一个风力发电拍卖问题和一个非拟合疫苗分布问题上进行了评估，并证明了DF2的效果。
</details></li>
</ul>
<hr>
<h2 id="GPLaSDI-Gaussian-Process-based-Interpretable-Latent-Space-Dynamics-Identification-through-Deep-Autoencoder"><a href="#GPLaSDI-Gaussian-Process-based-Interpretable-Latent-Space-Dynamics-Identification-through-Deep-Autoencoder" class="headerlink" title="GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder"></a>GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05882">http://arxiv.org/abs/2308.05882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llnl/gplasdi">https://github.com/llnl/gplasdi</a></li>
<li>paper_authors: Christophe Bonneville, Youngsoo Choi, Debojyoti Ghosh, Jonathan L. Belof</li>
<li>for:  This paper aims to provide a novel reduced-order modeling (ROM) framework that leverages machine learning to solve partial differential equations (PDEs) efficiently and accurately.</li>
<li>methods:  The proposed method, called GPLaSDI, utilizes Gaussian processes (GPs) for latent space ODE interpolations, which enables the quantification of uncertainty over the ROM predictions and allows for efficient adaptive training.</li>
<li>results:  The proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error, on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem.<details>
<summary>Abstract</summary>
Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this prediction uncertainty allows for efficient adaptive training through a greedy selection of additional training data points. This approach does not require prior knowledge of the underlying PDEs. Consequently, GPLaSDI is inherently non-intrusive and can be applied to problems without a known PDE or its residual. We demonstrate the effectiveness of our approach on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem. Our proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error.
</details>
<details>
<summary>摘要</summary>
解决部分泛函方程（PDE）数学问题可以是困难的并且 computationally expensive。这导致了减少顺序模型（ROM）的发展，这些模型具有准确性，但速度比整个顺序模型（FOM）更快。近些年，机器学习的进步使得非线性投影方法，如潜在空间动力学标识（LaSDI）的创造。LaSDI将全序PDE解析到一个潜在空间使用自适应神经网络，并学习潜在空间动力学系统的ODE。通过在减少的潜在空间中预测和解决ODE系统，可以快速并准确地预测ROM。在这篇论文中，我们介绍了GPLaSDI，一种基于 Gaussian process（GP）的LaSDI框架。使用GP提供了两点优势。首先，它允许量化ROM预测中的uncertainty。其次，通过利用这种预测uncertainty，可以高效地进行适应性训练，通过滥见训练数据点。这种方法不需要先知道下辖PDE或其剩余。因此，GPLaSDI是非侵入的，可以应用于没有known PDE或其剩余的问题。我们在Burgers方程、Vlasov方程 для плазма物理和热气囊问题中展示了我们的方法的效果。我们的提posed方法可以实现200到100,000倍的速度增加，相对误差在7%之间。
</details></li>
</ul>
<hr>
<h2 id="Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models"><a href="#Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models" class="headerlink" title="Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models"></a>Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05881">http://arxiv.org/abs/2308.05881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Cuncong Zhong, Bo Luo, Ivan Grijalva, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang</li>
<li>for: 本研究旨在使用深度学习模型检测螟蛾群体，以实现Targeted pesticide application。</li>
<li>methods: 我们使用了大规模的实验数据和深度学习模型来检测螟蛾群体，并对数据进行了处理和分割以便机器学习模型的使用。</li>
<li>results: 我们的实验结果表明，使用深度学习模型可以准确地检测螟蛾群体，并且可以通过合并邻近的群体和移除小 clusters来进一步提高性能。<details>
<summary>Abstract</summary>
Aphid infestation poses a significant threat to crop production, rural communities, and global food security. While chemical pest control is crucial for maximizing yields, applying chemicals across entire fields is both environmentally unsustainable and costly. Hence, precise localization and management of aphids are essential for targeted pesticide application. The paper primarily focuses on using deep learning models for detecting aphid clusters. We propose a novel approach for estimating infection levels by detecting aphid clusters. To facilitate this research, we have captured a large-scale dataset from sorghum fields, manually selected 5,447 images containing aphids, and annotated each individual aphid cluster within these images. To facilitate the use of machine learning models, we further process the images by cropping them into patches, resulting in a labeled dataset comprising 151,380 image patches. Then, we implemented and compared the performance of four state-of-the-art object detection models (VFNet, GFLV2, PAA, and ATSS) on the aphid dataset. Extensive experimental results show that all models yield stable similar performance in terms of average precision and recall. We then propose to merge close neighboring clusters and remove tiny clusters caused by cropping, and the performance is further boosted by around 17%. The study demonstrates the feasibility of automatically detecting and managing insects using machine learning models. The labeled dataset will be made openly available to the research community.
</details>
<details>
<summary>摘要</summary>
螟蛀感染 pose 对农业生产、农村社区和全球食品安全构成了严重的威胁。虽然化学防治是提高产量的重要手段，但是在整个场景中应用化学品是环境不可持续和昂贵的。因此，准确地Localization和管理螟蛀是必要的。本文主要关注使用深度学习模型 для检测螟蛀群。我们提出了一种新的方法，通过检测螟蛀群来估算感染水平。为了进行这项研究，我们在高粮田中采集了大规模数据集，手动选择了5,447张图像中包含螟蛀的图像，并对每个个体螟蛀群进行了标注。为了使机器学习模型可以使用，我们进一步处理了图像，将其分割成 patches，得到了151,380个标注图像 patches。然后，我们实现了和比较了四种当前最佳 объек detection 模型（VFNet、GFLV2、PAA 和 ATSS）在螟蛀数据集上的性能。广泛的实验结果表明，所有模型在精度和准确性方面具有稳定的性能。我们 then propose 将邻近的螟蛀群合并并 removes 小于patches 的螟蛀群，性能得到了约17%的提高。该研究表明了使用机器学习模型自动检测和管理昆虫的可能性。我们将标注数据集公开提供给研究社区。
</details></li>
</ul>
<hr>
<h2 id="Composable-Core-sets-for-Diversity-Approximation-on-Multi-Dataset-Streams"><a href="#Composable-Core-sets-for-Diversity-Approximation-on-Multi-Dataset-Streams" class="headerlink" title="Composable Core-sets for Diversity Approximation on Multi-Dataset Streams"></a>Composable Core-sets for Diversity Approximation on Multi-Dataset Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05878">http://arxiv.org/abs/2308.05878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephanie Wang, Michael Flynn, Fangyu Luo</li>
<li>for: 这篇论文旨在描述如何使用核心集来简化流处理数据，以便在实时训练机器学习模型时提高效率。</li>
<li>methods: 该论文提出了一种基于核心集的核心集建构算法，用于简化流处理数据并在活动学习环境中使用。此外，论文还提出了一种使用核心集和CRAIG等技术来加速建构速度。</li>
<li>results: 论文通过对推送数据进行预测分析，证明了这种方法可以在实时训练机器学习模型时提高效率。此外，论文还提出了一些改进建构速度的策略和技术。<details>
<summary>Abstract</summary>
Core-sets refer to subsets of data that maximize some function that is commonly a diversity or group requirement. These subsets are used in place of the original data to accomplish a given task with comparable or even enhanced performance if biases are removed. Composable core-sets are core-sets with the property that subsets of the core set can be unioned together to obtain an approximation for the original data; lending themselves to be used for streamed or distributed data. Recent work has focused on the use of core-sets for training machine learning models. Preceding solutions such as CRAIG have been proven to approximate gradient descent while providing a reduced training time. In this paper, we introduce a core-set construction algorithm for constructing composable core-sets to summarize streamed data for use in active learning environments. If combined with techniques such as CRAIG and heuristics to enhance construction speed, composable core-sets could be used for real time training of models when the amount of sensor data is large. We provide empirical analysis by considering extrapolated data for the runtime of such a brute force algorithm. This algorithm is then analyzed for efficiency through averaged empirical regression and key results and improvements are suggested for further research on the topic.
</details>
<details>
<summary>摘要</summary>
核心集（core-set）指的是一 subset of data 可以最大化某种函数，通常是多样性或组合要求。这些子集用于取代原始数据来完成一个给定任务，并且可以保持或提高性能，即使存在偏见。可搅 core-sets 是指可以将核心集中的子集 union 起来 obtaint 原始数据的一个近似。这些核心集具有可搅性，可以用于流处理或分布式数据。现有研究集中焦点在 core-sets 的使用，特别是用于训练机器学习模型。之前的解决方案，如 CRAIG，已经证明可以近似梯度下降，同时提供减少的训练时间。在这篇论文中，我们介绍一种用于构建可搅 core-sets 的算法，用于概要流处理数据，以便在活动学习环境中使用。如果与 CRAIG 和其他优化技术相结合，可搅 core-sets 可以用于实时训练模型，当感知数据量很大时。我们对此进行了实验分析，考虑了扩展数据的运行时间。这种算法的效率被分析了，并且提出了一些关键结果和改进建议。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-N-CNN-for-Clinical-Practice"><a href="#Revisiting-N-CNN-for-Clinical-Practice" class="headerlink" title="Revisiting N-CNN for Clinical Practice"></a>Revisiting N-CNN for Clinical Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05877">http://arxiv.org/abs/2308.05877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Antunes Ferreira, Lucas Pereira Carlini, Gabriel de Almeida Sá Coutrin, Tatiany Marcondes Heideirich, Marina Carvalho de Moraes Barros, Ruth Guinsburg, Carlos Eduardo Thomaz</li>
<li>for: 这篇论文旨在优化Neonatal Convolutional Neural Network（N-CNN）的超参数，并评估这些超参数对类别指标、可解释性和可靠性的影响，以及它们在临床实践中的潜在影响。</li>
<li>methods: 我们选择了不改变原始N-CNN架构的超参数，主要是修改学习率和训练正则化。我们通过评估每个超参数的改进情况来选择最佳超参数，并创建了调整后的Tuned N-CNN。此外，我们还应用了基于新生脸部编码系统的软标签，提出了一种新的训练 expresión facial类型分类模型的方法，用于评估新生痛的评估。</li>
<li>results: 结果表明，调整后的Tuned N-CNN显示出了类别指标和可解释性的改进，但这些改进直接不对准确性表现出来。我们认为这些发现可能有助于开发更可靠的痛评估工具 для新生，帮助医疗专业人员提供适当的 intervención和改善病人结果。<details>
<summary>Abstract</summary>
This paper revisits the Neonatal Convolutional Neural Network (N-CNN) by optimizing its hyperparameters and evaluating how they affect its classification metrics, explainability and reliability, discussing their potential impact in clinical practice. We have chosen hyperparameters that do not modify the original N-CNN architecture, but mainly modify its learning rate and training regularization. The optimization was done by evaluating the improvement in F1 Score for each hyperparameter individually, and the best hyperparameters were chosen to create a Tuned N-CNN. We also applied soft labels derived from the Neonatal Facial Coding System, proposing a novel approach for training facial expression classification models for neonatal pain assessment. Interestingly, while the Tuned N-CNN results point towards improvements in classification metrics and explainability, these improvements did not directly translate to calibration performance. We believe that such insights might have the potential to contribute to the development of more reliable pain evaluation tools for newborns, aiding healthcare professionals in delivering appropriate interventions and improving patient outcomes.
</details>
<details>
<summary>摘要</summary>
（本文重新审查了新生儿 convolutional neural network（N-CNN）的超参数，并评估它们如何影响其分类指标、可解释性和可靠性，并讨论它们在临床实践中的潜在影响。我们选择了不改变原始 N-CNN 架构的超参数，主要是 modify 学习率和训练正则化。优化是通过评估每个超参数的改进情况来进行，并选择最佳超参数来创建一个优化后的 Tuned N-CNN。我们还应用了来自新生儿表情编码系统的软标签，提出了一种新的训练 facial expression 分类模型的方法，用于新生儿疼痛评估。有趣的是，改进后的 Tuned N-CNN 结果表明，对于分类指标和可解释性来说，有所改进，但这些改进并不直接对准报表性能产生影响。我们认为，这些发现可能对新生儿疼痛评估工具的开发产生影响，帮助医疗专业人员提供适当的 intervención和改善病人结果。）
</details></li>
</ul>
<hr>
<h2 id="UFed-GAN-A-Secure-Federated-Learning-Framework-with-Constrained-Computation-and-Unlabeled-Data"><a href="#UFed-GAN-A-Secure-Federated-Learning-Framework-with-Constrained-Computation-and-Unlabeled-Data" class="headerlink" title="UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data"></a>UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05870">http://arxiv.org/abs/2308.05870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achintha Wijesinghe, Songyang Zhang, Siyu Qi, Zhi Ding</li>
<li>for: 这篇论文是为了解决在云端环境中部署低延迟多媒体数据分类和数据隐私问题而提出的一种学习平衡，尤其是在有限的计算资源和没有标签数据的情况下。</li>
<li>methods: 这篇论文提出了一个名为UFed-GAN的无监督联邦学习框架，可以在用户端数据分布下进行学习，不需要进行本地分类训练。此外，论文还进行了对UFed-GAN的参数分析和隐私分析。</li>
<li>results: 实验结果显示，UFed-GAN在有限的计算资源和没有标签数据的情况下可以实现高效的数据分类和隐私保护。<details>
<summary>Abstract</summary>
To satisfy the broad applications and insatiable hunger for deploying low latency multimedia data classification and data privacy in a cloud-based setting, federated learning (FL) has emerged as an important learning paradigm. For the practical cases involving limited computational power and only unlabeled data in many wireless communications applications, this work investigates FL paradigm in a resource-constrained and label-missing environment. Specifically, we propose a novel framework of UFed-GAN: Unsupervised Federated Generative Adversarial Network, which can capture user-side data distribution without local classification training. We also analyze the convergence and privacy of the proposed UFed-GAN. Our experimental results demonstrate the strong potential of UFed-GAN in addressing limited computational resources and unlabeled data while preserving privacy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为满足云端数据分类和隐私的广泛应用和不满足的需求，联邦学习（FL）已经成为一种重要的学习模式。在许多无线通信应用中，由于限制的计算资源和只有无标签数据，这项工作研究了在资源限制和标签缺失环境中的FL模式。我们提出了一种新的框架——无监督联邦生成敌战网络（UFed-GAN），可以在用户端 cattcapture数据分布without local classification training。我们还分析了UFed-GAN的 converges和隐私性。我们的实验结果表明，UFed-GAN在限制计算资源和无标签数据的情况下具有强大的潜在性，能够解决数据隐私和安全问题。
</details></li>
</ul>
<hr>
<h2 id="Using-Twitter-Data-to-Determine-Hurricane-Category-An-Experiment"><a href="#Using-Twitter-Data-to-Determine-Hurricane-Category-An-Experiment" class="headerlink" title="Using Twitter Data to Determine Hurricane Category: An Experiment"></a>Using Twitter Data to Determine Hurricane Category: An Experiment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05866">http://arxiv.org/abs/2308.05866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songhui Yue, Jyothsna Kondari, Aibek Musaev, Randy K. Smith, Songqing Yue</li>
<li>for: 本研究旨在找到社交媒体数据和自然灾害的严重程度之间的映射关系。</li>
<li>methods: 本研究使用数据挖掘技术来分析Twitter数据，并对各地区的Twitter数据和飓风等级之间进行相关性分析。</li>
<li>results: 实验结果表明，Twitter数据和飓风等级之间存在积极的相关性，并提出了一种使用Twitter数据预测飓风等级的方法。<details>
<summary>Abstract</summary>
Social media posts contain an abundant amount of information about public opinion on major events, especially natural disasters such as hurricanes. Posts related to an event, are usually published by the users who live near the place of the event at the time of the event. Special correlation between the social media data and the events can be obtained using data mining approaches. This paper presents research work to find the mappings between social media data and the severity level of a disaster. Specifically, we have investigated the Twitter data posted during hurricanes Harvey and Irma, and attempted to find the correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
</details>
<details>
<summary>摘要</summary>
社交媒体帖子中含有很多关于大事件的公众意见信息，尤其是自然灾害such as 飓风。posts相关的事件通常由用户们在事件发生时在附近的地方发布。我们的研究旨在找到社交媒体数据和灾害严重程度之间的映射。 Specifically, we investigated Twitter data posted during hurricanes Harvey and Irma and attempted to find a correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.Here's the word-for-word translation:社交媒体帖子中含有很多关于大事件的公众意见信息，尤其是自然灾害such as 飓风。posts相关的事件通常由用户们在事件发生时在附近的地方发布。我们的研究旨在找到社交媒体数据和灾害严重程度之间的映射。Specifically, we investigated Twitter data posted during hurricanes Harvey and Irma and attempted to find a correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
</details></li>
</ul>
<hr>
<h2 id="The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions"><a href="#The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions" class="headerlink" title="The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions"></a>The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05864">http://arxiv.org/abs/2308.05864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo Dickscheid, José Guilherme de Almeida, Yixin Wang, Lin Han, Xin Yang, Marco Labagnara, Sahand Jamal Rahi, Carly Kempster, Alice Pollitt, Leon Espinosa, Tâm Mignot, Jan Moritz Middeke, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe Bai, Noah F. Greenwald, David Van Valen, Erin Weisbart, Beth A. Cimini, Zhuoshi Li, Chao Zuo, Oscar Brück, Gary D. Bader, Bo Wang</li>
<li>for: 单细胞分析中的细胞分割步骤是 kritical。</li>
<li>methods: 这些方法通常适应特定的modalities或需要手动参数调整来适应不同的实验设置。</li>
<li>results: 这个benchmark和改进的算法可以无需手动参数调整地应用于多种微显技术和组织类型的细胞图像。<details>
<summary>Abstract</summary>
Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.
</details>
<details>
<summary>摘要</summary>
cell 分 segmentation 是单细胞分析中的关键步骤，exist 的 cell 分 segmentation 方法 oftentailored 到特定Modalities 或需要手动 intervene  specify  hyperparameters  in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.Note that the word "Transformer" in the original text was translated as "Transformer-based" in Simplified Chinese, as there is no direct equivalent of the word "Transformer" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Propagation-over-Conditional-Independence-Graphs"><a href="#Knowledge-Propagation-over-Conditional-Independence-Graphs" class="headerlink" title="Knowledge Propagation over Conditional Independence Graphs"></a>Knowledge Propagation over Conditional Independence Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05857">http://arxiv.org/abs/2308.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Urszula Chajewska, Harsh Shrivastava</li>
<li>for: 本研究旨在提出ci图表知识传播算法，用于从domain topology中提取有用信息。</li>
<li>methods: 本研究使用ci图表模型，并提出了一种基于ci图表的知识传播算法。</li>
<li>results: 实验结果表明，该算法在公共 disponibles的cora和pubmed datasets上有所提高，与现有技术相比。<details>
<summary>Abstract</summary>
Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the text into Simplified Chinese.<</SYS>>conditional independence (CI) 图是一种特殊的概率图模型 (PGM)，其中特征连接使用无向图表示，边重量表示特征之间的半相关度。由于 CI 图表示直接相互关联的特征，因此在不同领域中的系统研究中备受关注，特别是发现领域 то波动。在这项工作中，我们提出了在 CI 图上进行知识传播的算法。我们的实验表明，我们的技术在公共可用的 Cora 和 PubMed 数据集上超过了当前最佳的状况。
</details></li>
</ul>
<hr>
<h2 id="CSPM-A-Contrastive-Spatiotemporal-Preference-Model-for-CTR-Prediction-in-On-Demand-Food-Delivery-Services"><a href="#CSPM-A-Contrastive-Spatiotemporal-Preference-Model-for-CTR-Prediction-in-On-Demand-Food-Delivery-Services" class="headerlink" title="CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services"></a>CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08446">http://arxiv.org/abs/2308.08446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guyu Jiang, Xiaoyun Li, Rongrong Jing, Ruoqi Zhao, Xingliang Ni, Guodong Cao, Ning Hu</li>
<li>for: 这篇论文的目的是为了提高在在线快递食品平台上的点击率预测（CTR）。</li>
<li>methods: 这篇论文使用了三个模块：对比性空间时间表示学习（CSRL）、空间时间喜好EXTRACTOR（StPE）和空间时间信息过滤器（StIF）。CSRL使用了对比学习框架来生成搜索行为中的空间时间活动表示（SAR）。StPE使用了SAR来活化用户的不同的位置和时间相关的喜好，使用多头注意机制。StIF将SARintegrated into a gating network来自动捕捉重要的隐藏空间时间效果。</li>
<li>results: 在两个大规模的工业数据集上进行了广泛的实验，并证明了CSPM的状态之前性表现。尤其是，CSPM已经成功部署在阿里巴巴的在线快递食品平台Ele.me上，导致了显著的0.88%提升Click-through rate，这有重要的商业意义。<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction is a crucial task in the context of an online on-demand food delivery (OFD) platform for precisely estimating the probability of a user clicking on food items. Unlike universal e-commerce platforms such as Taobao and Amazon, user behaviors and interests on the OFD platform are more location and time-sensitive due to limited delivery ranges and regional commodity supplies. However, existing CTR prediction algorithms in OFD scenarios concentrate on capturing interest from historical behavior sequences, which fails to effectively model the complex spatiotemporal information within features, leading to poor performance. To address this challenge, this paper introduces the Contrastive Sres under different search states using three modules: contrastive spatiotemporal representation learning (CSRL), spatiotemporal preference extractor (StPE), and spatiotemporal information filter (StIF). CSRL utilizes a contrastive learning framework to generate a spatiotemporal activation representation (SAR) for the search action. StPE employs SAR to activate users' diverse preferences related to location and time from the historical behavior sequence field, using a multi-head attention mechanism. StIF incorporates SAR into a gating network to automatically capture important features with latent spatiotemporal effects. Extensive experiments conducted on two large-scale industrial datasets demonstrate the state-of-the-art performance of CSPM. Notably, CSPM has been successfully deployed in Alibaba's online OFD platform Ele.me, resulting in a significant 0.88% lift in CTR, which has substantial business implications.
</details>
<details>
<summary>摘要</summary>
Click-through rate (CTR) 预测是在在线快递食品平台上关键的任务，准确地估计用户会点击食品项。不同于通用电商平台如淘宝和amazon，用户在食品平台上的行为和兴趣更加地受到地域和时间影响，因为交通范围和地域商品供应有限。然而，现有的 CTRL 预测算法在食品平台场景中集中在捕捉历史行为序列中的兴趣，而不能有效地模型特有的空间时间信息，导致表现不佳。为解决这个挑战，本文引入了不同搜索状态下的 Contrastive Sres，使用三个模块：对比空间时间表示学习（CSRL）、空间时间偏好提取器（StPE）和空间时间信息筛选器（StIF）。CSRL 利用对比学习框架生成一个空间时间活动表示（SAR） для搜索行为。StPE 使用 SAR 来激活用户的不同地域时间上的偏好，使用多头注意机制。StIF 将 SAR integrated into a gating network 自动捕捉特有的空间时间效应。经验表明，CSPM 在两个大规模的业务数据集上达到了领先的性能，并在阿里巴巴在线食品平台 Ele.me 上部署成功，导致了显著的0.88%增加 Click-through rate，这有substantial商业意义。
</details></li>
</ul>
<hr>
<h2 id="GaborPINN-Efficient-physics-informed-neural-networks-using-multiplicative-filtered-networks"><a href="#GaborPINN-Efficient-physics-informed-neural-networks-using-multiplicative-filtered-networks" class="headerlink" title="GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks"></a>GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05843">http://arxiv.org/abs/2308.05843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinquan Huang, Tariq Alkhalifah<br>for: 这篇论文的目的是提出一种改进的物理学 Informed Neural Network（PINN）方法，以加速冲击波场的计算。methods: 这篇论文使用了多个技术，包括physics-informed neural networks（PINNs）和Gabor基函数。它们在训练过程中嵌入了一些已知的冲击波场特征，如频率，以提高快速度。results: 论文的实验结果表明，使用GaborPINN方法可以大大提高冲击波场的计算速度，比传统的PINN方法快两个数量级。<details>
<summary>Abstract</summary>
The computation of the seismic wavefield by solving the Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. Physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. To address this problem, we propose a modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of the wavefield in training, e.g., frequency, to achieve much faster convergence. Specifically, we use the Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. Meanwhile, we incorporate prior information on the frequency of the wavefield into the design of the method to mitigate the influence of the discontinuity of the represented wavefield by GaborPINN. The proposed method achieves up to a two-magnitude increase in the speed of convergence as compared with conventional PINNs.
</details>
<details>
<summary>摘要</summary>
computations of seismic wavefield by solving Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. to address this problem, we propose modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of wavefield in training, e.g., frequency, to achieve much faster convergence. specifically, we use Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. meanwhile, we incorporate prior information on frequency of wavefield into the design of method to mitigate influence of discontinuity of represented wavefield by GaborPINN. proposed method achieves up to two-magnitude increase in speed of convergence as compared with conventional PINNs.
</details></li>
</ul>
<hr>
<h2 id="FLShield-A-Validation-Based-Federated-Learning-Framework-to-Defend-Against-Poisoning-Attacks"><a href="#FLShield-A-Validation-Based-Federated-Learning-Framework-to-Defend-Against-Poisoning-Attacks" class="headerlink" title="FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks"></a>FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05832">http://arxiv.org/abs/2308.05832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsanul Kabir, Zeyu Song, Md Rafi Ur Rashid, Shagufta Mehnaz</li>
<li>for: 本研究旨在提出一种新的联合学习（Federated Learning，FL）框架，以保障FL系统的安全性和可靠性。</li>
<li>methods: 本研究使用了本地模型的有益数据来验证本地模型的可靠性，而不是依赖服务器访问spotless数据集，这种做法和FL的基本原则不匹配。</li>
<li>results: 研究人员通过对不同情况下的FLShield框架进行了广泛的实验，并证明了FLShield框架能够有效地防范各种毒素和后门攻击，同时保护本地数据的隐私。<details>
<summary>Abstract</summary>
Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield framework in different settings and demonstrate its effectiveness in thwarting various types of poisoning and backdoor attacks including a defense-aware one. FLShield also preserves privacy of local data against gradient inversion attacks.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是如何改变我们如何从数据中学习的革命。随着其 Popularity 的增长，它现在在许多安全关键领域，如自动驾驶和医疗，中使用。由于千余名参与者可以在这种合作环境中贡献，因此保持安全性和可靠性的系统是挑战。这 Heightens 需要设计安全可靠的 FL 系统，能够抵御恶意参与者的行为，同时保持本地数据隐私和效率。在这篇论文中，我们提出了一种新的 FL 框架，名为 FLShield，它利用 FL 参与者的善意数据来验证本地模型，然后将其作为全球模型生成。这与现有防御方法，它们基于服务器访问干净的数据集的假设，不同。我们进行了广泛的实验来评估我们的 FLShield 框架在不同的设置下的效果，并证明它在不同类型的毒素和后门攻击中具有有效性。FLShield 还保持本地数据隐私性免受Gradient Inversion攻击。
</details></li>
</ul>
<hr>
<h2 id="Neural-Progressive-Meshes"><a href="#Neural-Progressive-Meshes" class="headerlink" title="Neural Progressive Meshes"></a>Neural Progressive Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05741">http://arxiv.org/abs/2308.05741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Chun Chen, Vladimir G. Kim, Noam Aigerman, Alec Jacobson</li>
<li>for:  efficiently transmit large geometric data (e.g., 3D meshes) over the Internet</li>
<li>methods: subdivision-based encoder-decoder architecture trained on a large collection of surfaces, with progressive transmission of residual features</li>
<li>results: outperforms baselines in terms of compression ratio and reconstruction quality<details>
<summary>Abstract</summary>
The recent proliferation of 3D content that can be consumed on hand-held devices necessitates efficient tools for transmitting large geometric data, e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a challenge to storage as well as transmission bandwidth, and level-of-detail techniques are often used to transmit an asset using an appropriate bandwidth budget. It is especially desirable for these methods to transmit data progressively, improving the quality of the geometry with more data. Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns even across different shapes, and thus can be effectively represented with a shared learned generative space. We learn this space using a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces. We further observe that additional residual features can be transmitted progressively between intermediate levels of subdivision that enable the client to control the tradeoff between bandwidth cost and quality of reconstruction, providing a neural progressive mesh representation. We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details>
<details>
<summary>摘要</summary>
Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns across different shapes, and can be effectively represented with a shared learned generative space. We use a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces to learn this space. Additionally, we observe that residual features can be transmitted progressively between intermediate levels of subdivision, enabling the client to control the tradeoff between bandwidth cost and quality of reconstruction. This provides a neural progressive mesh representation.We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details></li>
</ul>
<hr>
<h2 id="Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics"><a href="#Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics" class="headerlink" title="Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics"></a>Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05739">http://arxiv.org/abs/2308.05739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Fischer, Tobias Ritschel</li>
<li>for: 解决 Graphics 中的Gradient-based优化问题，因为现有的搜索方法无法处理undefined或zero gradients。</li>
<li>methods: 提出了一种自动化替换损失函数的框架，即ZeroGrads，通过学习一个神经网络来 aproximate objective function，并使用这个神经网络来 differentiate through arbitrary black-box graphics pipelines。</li>
<li>results: 实现了在online和self-supervised的情况下，使用 actively smoothed version of the objective 进行训练，并且可以在 tractable run-times 和competitive performance下解决多个非对称、非 differentiable black-box problem in Graphics，如视力rendering、排序参数空间和物理驱动动画优化等。<details>
<summary>Abstract</summary>
Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a "surrogate" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competitive performance at little overhead. We demonstrate optimizing diverse non-convex, non-differentiable black-box problems in graphics, such as visibility in rendering, discrete parameter spaces in procedural modelling or optimal control in physics-driven animation. In contrast to more traditional algorithms, our approach scales well to higher dimensions, which we demonstrate on problems with up to 35k interlinked variables.
</details>
<details>
<summary>摘要</summary>
“梯度基本优化现在在图形处理中广泛应用，但它无法应用于无定义或 zeros 梯度的问题。为了缺 Bibliography 这个问题，我们可以手动替换损失函数，使其成为可导的。我们的提议的框架，ZeroGrads，可以自动实现这个过程，它通过学习一个神经网络来模拟目标函数，并使用这个模拟来分子 Graphics pipeline 中的任意黑盒问题。我们在训练过程中使用在目标函数上实时缓和的版本，并且强调本集成性，使得模拟的能量集中在当前训练集中。我们的整个训练过程是在线进行的，并且是自动的，不需要预计算数据或预训练模型。由于评估目标函数的成本 relativity 高（它需要一个完整的渲染或 simulator 运行），我们开发了一种有效的采样方案，使得我们可以在可耗时间和竞争性的情况下实现高性能。我们在不同的非等式、非导数的黑盒问题上进行了优化，例如渲染中的可见性、procedural 模型中的分配空间和物理驱动的动画中的优化问题。与传统算法相比，我们的方法可以很好地扩展到更高的维度，我们在问题中的35k个相互关联变量上进行了示例。”
</details></li>
</ul>
<hr>
<h2 id="Follow-Anything-Open-set-detection-tracking-and-following-in-real-time"><a href="#Follow-Anything-Open-set-detection-tracking-and-following-in-real-time" class="headerlink" title="Follow Anything: Open-set detection, tracking, and following in real-time"></a>Follow Anything: Open-set detection, tracking, and following in real-time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05737">http://arxiv.org/abs/2308.05737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alaamaalouf/followanything">https://github.com/alaamaalouf/followanything</a></li>
<li>paper_authors: Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallabhula, Makram Chahine, Daniel M. Vogt, Robert J. Wood, Antonio Torralba, Daniela Rus</li>
<li>for: 本研究旨在开发一种可以在实时控制循环中跟踪任何对象的机器人系统。</li>
<li>methods: 该系统基于大规模预训练模型（基础模型），可以在实时控制循环中检测、分割和跟踪对象，并且可以考虑 occlusion 和对象重新出现。</li>
<li>results: 研究人员在一架微型飞行器上部署了该系统，并成功地跟踪了各种对象。系统可以在一个简单的 laptop 上运行，并且可以达到 6-20 帧每秒的throughput。<details>
<summary>Abstract</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed ``follow anything'' (FAn), is an open-vocabulary and multimodal model -- it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at https://github.com/alaamaalouf/FollowAnything . We also encourage the reader the watch our 5-minutes explainer video in this https://www.youtube.com/watch?v=6Mgt3EPytrw .
</details>
<details>
<summary>摘要</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed “follow anything” (FAn), is an open-vocabulary and multimodal model — it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at <https://github.com/alaamaalouf/FollowAnything>. We also encourage the reader to watch our 5-minutes explainer video in this <https://www.youtube.com/watch?v=6Mgt3EPytrw>.
</details></li>
</ul>
<hr>
<h2 id="PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers"><a href="#PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers" class="headerlink" title="PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers"></a>PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05732">http://arxiv.org/abs/2308.05732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Lippe, Bastiaan S. Veeling, Paris Perdikaris, Richard E. Turner, Johannes Brandstetter<br>for:This paper aims to improve the accuracy and stability of deep neural network-based solution techniques for partial differential equations (PDEs) by addressing the neglect of non-dominant spatial frequency information.methods:The authors use a large-scale analysis of common temporal rollout strategies and draw inspiration from recent advances in diffusion models to introduce a novel model class called PDE-Refiner, which uses a multistep refinement process to accurately model all frequency components of PDE solutions.results:The authors validate PDE-Refiner on challenging benchmarks of complex fluid dynamics and demonstrate stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. Additionally, PDE-Refiner is shown to greatly enhance data efficiency by implicitly inducing a novel form of spectral data augmentation, and the authors demonstrate an accurate and efficient assessment of the model’s predictive uncertainty.Here is the simplified Chinese text:for: 这篇论文目的是提高深度神经网络基于 partial differential equation (PDE) 的解决方法的精度和稳定性。methods: 作者使用大规模的时间滚动策略分析，并启发自latest advances in diffusion models ，引入了一种新的模型类叫 PDE-Refiner，该模型使用多步增强过程来准确地模型 PDE 解的所有频率组成部分。results: 作者验证 PDE-Refiner 在复杂的流体动力学 benchmark 上，并显示了稳定和准确的滚动性能，常常超越当前的模型，包括神经网络、数学、和混合神经网络-数学模型。此外， PDE-Refiner 能够大幅提高数据效率，因为净化目标意味着在数据增强过程中隐式地引入了一种新的频率数据增强。最后， PDE-Refiner 的连接到 diffusion models 使得可以准确地评估模型的预测不确定性，从而估计模型在滚动过程中的不确定性。<details>
<summary>Abstract</summary>
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate.
</details>
<details>
<summary>摘要</summary>
时间依赖的 partial differential equations (PDEs) 在科学和工程中具有广泛的应用。最近，主要由于传统解决方案的计算成本高，深度神经网络基于的 surrogate 获得了更多的关注。然而，实际应用中，这些神经网络 PDE 解决器的实用性取决于它们能够提供稳定、准确的预测，这是一个非常困难的问题。在这种情况下，我们提出了一项大规模分析 temporal rollout 策略，发现忽略非主要空间频率信息，通常与 PDE 解的高频成分相关，是主要的障碍物，限制稳定、准确的 rollout 性能。基于这些发现，我们启发自 recent advances in diffusion models，提出了 PDE-Refiner; 一种新的模型类，可以更好地模拟所有频率组成部分。我们验证 PDE-Refiner 在复杂的流体动力学 benchmark 上，可以实现稳定和准确的 rollouts，常常超越当前模型，包括神经网络、数值和混合神经网络-数值模型。此外，PDE-Refiner 可以大幅提高数据效率，因为净化目标隐式地导入了一种新的 spectral data augmentation。最后，PDE-Refiner 的连接到 diffusion models 使得可以准确地评估模型的预测不确定性，从而估计 surrogate 是否准确。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review"><a href="#Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review" class="headerlink" title="Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review"></a>Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05731">http://arxiv.org/abs/2308.05731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steffen Hagedorn, Marcel Hallgarten, Martin Stoll, Alexandru Condurache</li>
<li>for: 这篇论文主要写于自动驾驶技术的发展，具体来说是关于 Prediction、Planning 和 Integrated Prediction and Planning 模型的系统性评估。</li>
<li>methods: 论文使用了深度学习技术来实现 Prediction 和 Planning 模型，并对不同的集成方法进行了系统性的比较和分析。</li>
<li>results: 论文发现了不同集成方法在自动驾驶中的优缺点，并指出了未来研究的方向和挑战。<details>
<summary>Abstract</summary>
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details>
<details>
<summary>摘要</summary>
（简化中文）自动驾驶有可能改变个人、公共和货物运输方式。除了巨大的感知挑战以外，自动驾驶还包括规划安全、舒适和有效的运动轨迹。为促进安全和进步，许多工作依赖于周围交通的未来运动预测。现有的自动驾驶系统通常将预测和规划作为独立的两个任务进行处理。这种方法虽然考虑了周围交通对ego汽车的影响，但是忽略了ego汽车行为对交通参与者的反应。 latest works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details></li>
</ul>
<hr>
<h2 id="EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis"><a href="#EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis" class="headerlink" title="EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis"></a>EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05725">http://arxiv.org/abs/2308.05725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tu Anh Nguyen, Wei-Ning Hsu, Antony D’Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, Emmanuel Dupoux</li>
<li>for: 本研究旨在开发一个高质量的自然语言表达 speech 数据集，用于无文本 speech 生成。</li>
<li>methods: 研究人员使用了自我超级vised学习的方法，将低比特率的精度单元学习到 speech 中，以捕捉expressive aspect of speech。</li>
<li>results: 研究人员在 Expresso 数据集上进行了一系列的evalution，并结果表明，这种方法可以实现高质量的 expressive speech 生成。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is aimed at developing a high-quality expressive speech dataset for textless speech synthesis.</li>
<li>methods: The researchers use self-supervised learning methods to learn low-bitrate discrete units that can capture expressive aspects of speech, such as prosody and voice styles.</li>
<li>results: The researchers evaluate the effectiveness of their approach on the Expresso dataset, which includes both read speech and improvised dialogues in 26 spontaneous expressive styles. The results show that the method can achieve high-quality expressive speech synthesis.<details>
<summary>Abstract</summary>
Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe (prosody, voice styles, non-verbal vocalization). The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce Expresso, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 spontaneous expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete encoders, and explore tradeoffs between quality, bitrate and invariance to speaker and style. All the dataset, evaluation metrics and baseline models are open source
</details>
<details>
<summary>摘要</summary>
近期研究表明，可以使用低比特率不同单元进行自我指导的高质量语音重建。这些单元可以捕捉到表达方面的声音特征，如声音态度、声音风格和非语言 vocalization，这些特征Difficult to transcribe。然而，目前这些方法的应用仍然受限于大多数语音重建数据集是阅读的，因此减少了自由和表达力。在这篇文章中，我们介绍了一个高质量表达语音数据集，即Expresso，该数据集包括了阅读语音和自由对话，并且在26种自然表达风格下进行了渲染。我们介绍了这个数据集的挑战和潜在性，并在表达重建中进行了一种表达编码和重建任务，以测试不同的自我指导精度单元的质量。我们使用了自动化 метри来评估重建质量，并 explore了不同精度单元的平衡点，包括质量、比特率和对 speaker和风格的不变性。所有的数据集、评估指标和基础模型都是开源的。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Performance-of-Feedforward-and-Convolutional-Neural-Networks-through-Dynamic-Activation-Functions"><a href="#Optimizing-Performance-of-Feedforward-and-Convolutional-Neural-Networks-through-Dynamic-Activation-Functions" class="headerlink" title="Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions"></a>Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05724">http://arxiv.org/abs/2308.05724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chinmay Rane, Kanishka Tyagi, Michael Manry</li>
<li>for: 本研究旨在探讨 activation functions 在 convolutional neural networks (CNNs) 中的影响，并提出了一种复杂的 Piece-wise Linear (PWL) 活动函数来取代 Relu 活动函数。</li>
<li>methods: 本研究使用了 PyTorch 框架进行实验，并对 shallow CNNs 和深度 CNNs 进行比较。</li>
<li>results: 研究发现，使用 PWL 活动函数可以大大提高 CNNs 的性能，并且在深度 CNNs 中提供了更多的可能性空间。<details>
<summary>Abstract</summary>
Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.
</details>
<details>
<summary>摘要</summary>
深度学习训练算法在最近几年内在多个领域取得了巨大成功，包括语音、文本、图像和视频等。随着更深层的提议，深度学习模型在152层以上的ResNet结构中得到了巨大成功。 however， shallow convolutional neural networks（CNN）仍然是一个活跃的研究领域，一些现象仍未得到解释。 activation functions在网络中具有重要的作用，它们提供了非线性性，使网络更加复杂。 ReLU activation function是最常用的activation function。我们在隐藏层使用复杂的 piece-wise linear（PWL）activation，并证明这些PWL activation在我们的网络中工作得更好than ReLU activation。我们还在PyTorch中对 shallow和深度CNN进行比较，以更加强化我们的论据。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Classical-and-Deep-Reinforcement-Learning-Methods-for-HVAC-Control"><a href="#A-Comparison-of-Classical-and-Deep-Reinforcement-Learning-Methods-for-HVAC-Control" class="headerlink" title="A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control"></a>A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05711">http://arxiv.org/abs/2308.05711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marshall Wang, John Willes, Thomas Jiralerspong, Matin Moezzi</li>
<li>for: 这个论文旨在使用强化学习（RL）方法优化冷却空调系统的控制，提高系统性能，降低能源消耗，提高成本效益。</li>
<li>methods: 这篇论文使用了两种流行的 класси型和深度RL方法（Q-学习和深度Q-网络），在多个冷却空调环境下进行了比较。</li>
<li>results: 研究发现，RL方法可以在冷却空调系统中提高性能，降低能源消耗，且模型参数选择和奖励调整是RL agents配置的关键因素。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a promising approach for optimizing HVAC control. RL offers a framework for improving system performance, reducing energy consumption, and enhancing cost efficiency. We benchmark two popular classical and deep RL methods (Q-Learning and Deep-Q-Networks) across multiple HVAC environments and explore the practical consideration of model hyper-parameter selection and reward tuning. The findings provide insight for configuring RL agents in HVAC systems, promoting energy-efficient and cost-effective operation.
</details>
<details>
<summary>摘要</summary>
现代控制技术（Reinforcement Learning，RL）可以有效地优化冷暖空调系统的控制。RL提供了一个框架，可以提高系统性能，降低能源消耗，提高成本效益。我们在多个冷暖空调环境中对两种流行的古典RL和深度RL方法（Q-学习和深度Q网络）进行了比较，并探讨RL代理人在冷暖空调系统中的实用考虑和奖励调整。发现提供了RL代理人配置的指导，推动了能源减少和成本效益的操作。
</details></li>
</ul>
<hr>
<h2 id="Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning"><a href="#Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning" class="headerlink" title="Shadow Datasets, New challenging datasets for Causal Representation Learning"></a>Shadow Datasets, New challenging datasets for Causal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05707">http://arxiv.org/abs/2308.05707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jiagengzhu/Shadow-dataset-for-crl">https://github.com/Jiagengzhu/Shadow-dataset-for-crl</a></li>
<li>paper_authors: Jiageng Zhu, Hanchen Xie, Jianhua Wu, Jiazhi Li, Mahyar Khayatkhoei, Mohamed E. Hussein, Wael AbdAlmageed</li>
<li>for: 本研究的目的是探索语义因素之间的 causal 关系，以便进行更好的表征学习。</li>
<li>methods: 该研究使用了weakly supervised causal representation learning（CRL）方法，以解决高成本的标注问题。</li>
<li>results: 研究提出了两个新的数据集，以及对现有数据集的修改，以满足更复杂的 causal 图和更多的生成因素的需求。<details>
<summary>Abstract</summary>
Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
</details>
<details>
<summary>摘要</summary>
发现 semantic 因素之间的 causal 关系是 representation learning 中一个emerging topic。大多数 causal representation learning（CRL）方法是完全supervised，这是因为标注成本太高。为解决这种限制，我们提出了弱标注 CRL 方法。为评估 CRL 性能，我们使用了四个现有的数据集：Pendulum、Flow、CelebA（BEARD）和 CelebA（SMILE）。然而，现有的 CRL 数据集受限于简单的图与少量生成因素。因此，我们提出了两个新的数据集，它们具有更多的多样化生成因素和更复杂的 causal 图。此外，原始的现实数据集 CelebA（BEARD）和 CelebA（SMILE）的 causal 图与数据分布不一致。因此，我们提出了修改。
</details></li>
</ul>
<hr>
<h2 id="Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient"><a href="#Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient" class="headerlink" title="Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient"></a>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05681">http://arxiv.org/abs/2308.05681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luyg45/hardnoboxattack">https://github.com/luyg45/hardnoboxattack</a></li>
<li>paper_authors: Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</li>
<li>for: 这 paper 探讨了 skeleton-based 人体活动识别系统 的攻击性评估问题。</li>
<li>methods: 该 paper 使用了一种新的攻击任务，即攻击者没有访问受害者模型或训练数据或标签。具体来说，它们首先学习了一个动作抽象空间，然后定义了一种对抗损失来计算一个新的攻击方向，称为skeleton-motion-informed（SMI）梯度。这个梯度包含了动作动态信息，与现有的梯度基于攻击方法不同。</li>
<li>results: 该 paper 的实验和比较结果表明，SMI 梯度可以在无框杆和转移基于黑盒 Setting 中提高攻击性和透明度。<details>
<summary>Abstract</summary>
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings.
</details>
<details>
<summary>摘要</summary>
近期，基于骨架的人体活动识别方法已经被证明容易受到敌意攻击。然而，这些攻击方法都需要受害者（白盒攻击）、训练数据（传输基于攻击）或模型查询（黑盒攻击）的访问权限。这些需求都是非常限制的，这引发了对攻击性的评估。在这篇论文中，我们证明了这种攻击性确实存在。为此，我们提出了一个新的攻击任务：攻击者无法访问受害者的模型或训练数据或标签。我们称之为“困难无框攻击”（hard no-box attack）。我们首先学习了一个运动拟合，并定义了一种对抗损失来计算一个新的攻击Gradient，称之为skeleton-motion-informed（SMI）梯度。我们的梯度包含运动动力学信息，与现有的梯度基本攻击方法不同，它们计算损失梯度，假设每个数据维度独立。SMI梯度可以增强许多梯度基本攻击方法，导致一个新的无框攻击家族。我们的评估和比较表明，我们的方法对现有分类器 pose a real threat。它们还表明了SMI梯度的传播性和隐蔽性在无框和传输基于黑盒 Setting 中得到了改进。
</details></li>
</ul>
<hr>
<h2 id="Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning"><a href="#Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning" class="headerlink" title="Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning"></a>Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05680">http://arxiv.org/abs/2308.05680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iknoor Singh, Carolina Scarton, Xingyi Song, Kalina Bontcheva</li>
<li>for: 本研究的目的是探讨跨语言验证答案已经证明为假的故事的检测，以减少专业验证者的手动努力并为防止谣言的传播做出贡献。</li>
<li>methods: 本研究使用了一个新的数据集，该数据集包含了用于验证的检查答案和各种语言的社交媒体帖子，以便进行跨语言验证答案已经证明为假的故事的检测。</li>
<li>results: 研究发现，跨语言验证答案已经证明为假的故事的检测是一项具有挑战性的任务，而一些常用的跨语言预处理 transformer 模型也未能超越一个强的基于词语的基线（BM25）。然而，我们的多Stage检索框架在大多数情况下能够超越 BM25，并具有跨频率和零扩展学习的能力。<details>
<summary>Abstract</summary>
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk retrieval task into refinement and re-ranking stages. Results show that the task of cross-lingual retrieval of already debunked narratives is challenging and off-the-shelf Transformer models fail to outperform a strong lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework is robust, outperforming BM25 in most scenarios and enabling cross-domain and zero-shot learning, without significantly harming the model's performance.
</details>
<details>
<summary>摘要</summary>
这个任务是检索已经证伪的故事，目的是检测已经被ifact-checked的故事。成功检测已经证伪的故事不仅可以减少专业ifact-checker的手动努力，还可以减速谣言的传播。但因为数据不足，这个问题尚未得到充分研究，特别是跨语言任务，即在不同语言的 онлайн帖子被检查时，检索ifact-checking文章的跨语言任务。这篇论文填补这一漏洞，通过以下三个方法：1. 创建一个新的数据集，用于启动研究跨语言检索已经证伪的故事。2. 进行了广泛的实验，以benchmark fine-tuned和off-the-shelf多语言预训练Transformer模型。3. 提出了一个新的多阶段框架，将跨语言检索已经证伪的故事任务分为两个阶段：精度阶段和重新排序阶段。结果表明，跨语言检索已经证伪的故事是一个具有挑战性的任务，off-the-shelf Transformer模型无法超过一个强的基于词语的基准值（BM25）。然而，我们的多阶段检索框架具有坚固性，在大多数情况下超过BM25，并且允许跨频域和零shot学习，无需对模型性能产生重要的影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.LG_2023_08_11/" data-id="clly3604c0069dd88ee7ve5pj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/12/eess.IV_2023_08_12/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-12 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/11/cs.SD_2023_08_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-11 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
