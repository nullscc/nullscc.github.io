
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-11 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Improving Joint Speech-Text Representations Without Alignment paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06125 repo_url: None paper_authors: Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenbe">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-11 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/11/cs.SD_2023_08_11/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Improving Joint Speech-Text Representations Without Alignment paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.06125 repo_url: None paper_authors: Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenbe">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-10T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:28.695Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.SD_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-11 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-Joint-Speech-Text-Representations-Without-Alignment"><a href="#Improving-Joint-Speech-Text-Representations-Without-Alignment" class="headerlink" title="Improving Joint Speech-Text Representations Without Alignment"></a>Improving Joint Speech-Text Representations Without Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06125">http://arxiv.org/abs/2308.06125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho</li>
<li>for: 这个论文旨在提出一种基于modal space的文本生成方法，用于处理 speech和text两种不同的模式。</li>
<li>methods: 该方法使用joint speech-text encoder，通过在modal space中对文本和语音进行共同表示，以减少模型的参数量。</li>
<li>results: 该方法可以自动解决模式长度不同的问题，并且在下游WER测试中显示了良好的表现，包括单语言和多语言系统。<details>
<summary>Abstract</summary>
The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
</details>
<details>
<summary>摘要</summary>
最近一年内，文本承词生成技术呈现了宏まScale的进步，基于跨Modal表示空间的想法，在这个空间中，文本和图像领域都被合并表示。在ASR中，这个想法得到应用，通过将语音和文本域合并编码，可以让模型 Parameters scale 到非常大的规模。虽然这些方法显示了承诺，但它们需要特殊地处理语音和文本序列长度的差异，通过上映或者显式对齐模型。在这种工作中，我们提供证据，表明Join speech-text编码器可以自然地实现多Modal的一致表示，而不需要注意序列长度。我们还 argues that consistency损失可以宽容序列长度差异，并且可以假设最佳对齐。我们示出，这种损失可以提高下游 WER 在大参数 monolingual 和 multilingual 系统中。
</details></li>
</ul>
<hr>
<h2 id="Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping"><a href="#Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping" class="headerlink" title="Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping"></a>Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06112">http://arxiv.org/abs/2308.06112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Haithem Boussaid, Ebtessam Almazrouei, Merouane Debbah</li>
<li>For: 这个论文的目的是提出一种简单的方法，以便在视频序列中进行舌头语音识别（VSR）任务。这种方法可以在训练集外的挑战性enario中表现出色，而不需要大量的标注数据。* Methods: 这个论文使用了一种名为Lip2Vec的简单方法，该方法基于学习一个先验模型。该网络将视频序列中的舌头编码后的干扰表示与其对应的音频对的干扰表示进行映射。然后，使用一个市场上的音频识别模型来解码音频，并将其转化为文本。* Results: 根据LRS3数据集的测试结果，这种方法可以与完全监督学习方法相比，达到26个WER的水平。与State-of-the-Art（SoTA）方法不同，我们的模型在VoxCeleb测试集上保持了合理的性能。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed model compares favorably with fully-supervised learning methods on the LRS3 dataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable performance on the VoxCeleb test set. We believe that reprogramming the VSR as an ASR task narrows the performance gap between the two and paves the way for more flexible formulations of lip reading.
</details>
<details>
<summary>摘要</summary>
视觉语音识别（VSR）与常见的观察任务不同，因为它需要对视频序列进行更深入的理解，即使人类专家也需要这样做。尽管最近有大量的进步在VSR方面，但现在的方法仍然依赖于标注数据来完全训练或微调其模型，以预测目标语音。这会导致其在不同于训练集的情况下表现不佳，并且会导致性能下降。与之前的工作不同，我们提出了一种简单的方法，即Lip2Vec，它基于学习一个先验模型。给定一个强大的视觉语音编码器，这个网络将编码的舌唇序列的 latent 表示与它们对应的 audio 对应的 latent 表示进行映射，这些表示够具有效果的文本解码。生成的音频表示然后被解码成文本使用一个可用的 Audio Speech Recognition（ASR）模型。我们提出的模型与完全监督学习方法在 LRS3 数据集上比较 favorably，实现 26 WER。与 SoTA 方法不同，我们的模型在 VoxCeleb 测试集上保持了合理的性能。我们认为将 VSR 转换为 ASR 任务，将两者之间的性能差减少，并且开创了更 flexible 的舌唇读取形式。
</details></li>
</ul>
<hr>
<h2 id="An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition"><a href="#An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition" class="headerlink" title="An Autoethnographic Exploration of XAI in Algorithmic Composition"></a>An Autoethnographic Exploration of XAI in Algorithmic Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06089">http://arxiv.org/abs/2308.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Noel-Hirst, Nick Bryan-Kinns</li>
<li>for: 这篇论文旨在探讨如何使用可解释的人工智能（XAI）生成模型来创作传统音乐。</li>
<li>methods: 本研究使用MeasureVAE生成模型，该模型具有可解释的秘密维度，并在爱尔兰传统音乐上进行训练。</li>
<li>results: 研究发现，在音乐创作过程中，探索性的音乐创作 workflow 会强调音乐训练数据中的音乐特征，而不是生成模型本身的特征。这种应用XAI模型在创作过程中的可能性可能会扩展到更复杂和多样化的工作流程。<details>
<summary>Abstract</summary>
Machine Learning models are capable of generating complex music across a range of genres from folk to classical music. However, current generative music AI models are typically difficult to understand and control in meaningful ways. Whilst research has started to explore how explainable AI (XAI) generative models might be created for music, no generative XAI models have been studied in music making practice. This paper introduces an autoethnographic study of the use of the MeasureVAE generative music XAI model with interpretable latent dimensions trained on Irish folk music. Findings suggest that the exploratory nature of the music-making workflow foregrounds musical features of the training dataset rather than features of the generative model itself. The appropriation of an XAI model within an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for.
</details>
<details>
<summary>摘要</summary>
machine learning模型可以生成复杂的音乐，从民族音乐到古典音乐。但当前的生成音乐AI模型通常难以理解和控制有意义的方式。研究已经开始探索如何创建音乐XAI生成模型，但没有任何生成XAI模型在音乐创作实践中被研究。本文介绍了一个自传式研究，使用MeasureVAE生成音乐XAI模型，具有可解释的幂等维度，在爱尔兰传统音乐上进行训练。发现结果表明，音乐创作工作流程的探索性强调了训练集音乐特征而不是生成模型本身的特征。将XAI模型包含在迭代工作流程中，表明XAI模型可以成为更加丰富和复杂的工作流程的一部分。
</details></li>
</ul>
<hr>
<h2 id="Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model"><a href="#Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model" class="headerlink" title="Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model"></a>Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05995">http://arxiv.org/abs/2308.05995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li<br>for:* 这篇论文旨在创造数字人类的合作语言姿势，以解决现有的挑战，包括复杂的语音、语义和人性等因素。methods:* 该论文提出了一种基于扩散的DiffMotion-v2模型，利用RawSpeech音频 directly生成个性化和风格化的全身语姿，不需要复杂的多Modal处理和手动标注。results:* 经验证明，DiffMotion-v2模型可以生成自然的语姿，并且可以适应不同的风格和人性特征。<details>
<summary>Abstract</summary>
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的虚拟人物创造领域中的协调姿势生成技术是一个emerging领域。先前的研究使用了音响和语义信息作为输入，采用分类方法来确定人的ID和情绪，以驱动协调姿势生成。然而，这一领域仍面临着重大挑战。这些挑战不仅包括协调姿势、音响和语义之间的细微互动，还包括人性、情绪和其他一些重要而不那么明确的因素。本文介绍了“diffmotion-v2”，一种基于 transformer 架构的 speech-conditional 协同扩散型生成模型，使用 WavLM 预训练模型。该模型可以通过 Raw speech 音频alone 生成具有个性化和风格化特点的全身协调姿势，无需进行复杂的多Modal 处理和手动标注。首先，我们认为 speech 音频不仅包含了音响和语义特征，还拥有人性特征、情绪特征和更为细微的协调姿势相关信息。因此，我们采用 WavLM 预训练模型来提取低级和高级 audio 信息。其次，我们引入了 transformer 架构中的 adaptive layer norm 层，以学习 speech 信息和协调姿势之间的关系。我们对 Trinity、ZEGGS 和 BEAT 等三个 dataset 进行了许多主观评估实验，以确认 WavLM 和模型的能力以生成自然的协调姿势。
</details></li>
</ul>
<hr>
<h2 id="Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection"><a href="#Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection" class="headerlink" title="Advancing the study of Large-Scale Learning in Overlapped Speech Detection"></a>Advancing the study of Large-Scale Learning in Overlapped Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05987">http://arxiv.org/abs/2308.05987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaohui Yin, Jingguang Tian, Xinhui Hu, Xinkang Xu</li>
<li>for: 多个党人对话分析中的干扰语音检测（OSD）是一个重要的应用领域，但现有的大多数OSD模型都是基于特定的数据集进行训练和评估，限制了这些模型的应用场景。</li>
<li>methods: 我们提出了大规模学习（LSL）在OSD任务中的应用，并设计了一种16K单annelOSD模型。我们使用了522小时不同语言和风格的标注音频作为大规模数据集，并进行了严格的比较实验来评估LSL在OSD任务中的效果和不同深度神经网络基于OSD模型的性能。</li>
<li>results: 我们的实验结果表明，LSL可以显著提高OSD模型的性能和鲁棒性，并且CF-OSD与LSL在Alimeeting测试集和DIHARD II评估集上的F1分数分别达到了80.8%和52.0%，创造了当前最佳的16K单annelOSD模型。<details>
<summary>Abstract</summary>
Overlapped Speech Detection (OSD) is an important part of speech applications involving analysis of multi-party conversations. However, Most of the existing OSD models are trained and evaluated on specific dataset, which limits the application scenarios of these models. In order to solve this problem, we conduct a study of large-scale learning (LSL) in OSD and propose a more general 16K single-channel OSD model. In our study, 522 hours of labeled audio in different languages and styles are collected and used as the large-scale dataset. Rigorous comparative experiments are designed and used to evaluate the effectiveness of LSL in OSD task and the performance of OSD models based on different deep neural networks. The results show that LSL can significantly improve the performance and robustness of OSD models, and the OSD model based on Conformer (CF-OSD) with LSL is currently the best 16K single-channel OSD model. Moreover, the CF-OSD with LSL establishes a state-of-the-art performance with a F1-score of 80.8% and 52.0% on the Alimeeting test set and DIHARD II evaluation set, respectively.
</details>
<details>
<summary>摘要</summary>
大量学习（LSL）在对话分析中的另 overlap speech detection（OSD）是一个重要的部分，但大多数现有的OSD模型都是基于特定的数据集进行训练和评估，这限制了这些模型的应用场景。为解决这个问题，我们在OSD领域进行了大规模学习的研究，并提出了一个16K单annelOSD模型。在我们的研究中，我们收集了522小时的不同语言和风格的标注音频数据，并使用这些数据作为大规模数据集进行训练和测试。我们设计了严格的比较实验，以评估LSL在OSD任务中的效果和不同深度神经网络基于的OSD模型的性能。结果显示LSL可以显著提高OSD模型的性能和可靠性，并且CF-OSD WITH LSL目前是最佳的16K单annelOSD模型。此外，CF-OSD WITH LSL在Alimeeting测试集和DIHARD II评估集上的F1分数分别为80.8%和52.0%，创造了当前最佳的州态。
</details></li>
</ul>
<hr>
<h2 id="AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining"><a href="#AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining" class="headerlink" title="AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining"></a>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05734">http://arxiv.org/abs/2308.05734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoheliu/AudioLDM2">https://github.com/haoheliu/AudioLDM2</a></li>
<li>paper_authors: Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley</li>
<li>for: 这 paper 的目的是提出一种框架，用于将 speech、music 和 sound effect 等不同类型的声音生成模型共同拟合。</li>
<li>methods: 该框架使用同一种学习方法，通过 AudioMAE 自然适应学习模型和 latent diffusion 模型来翻译不同类型的声音，并在生成过程中进行自我监督学习。</li>
<li>results: 经验表明，该框架可以在主要的 benchmark 上达到新的 state-of-the-art 或竞争性的性能，并且具有很好的培化学习能力和可重用的自然适应学习模型。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches. Our demo and code are available at https://audioldm.github.io/audioldm2.
</details>
<details>
<summary>摘要</summary>
尽管各种听音都有共同之处，如speech、音乐和声音效果，但设计模型时需要仔细考虑每种类型的特定目标和偏见，这些偏见可能与其他类型异常大。为了带领我们更近到一个统一的听音生成视角，这篇论文提出了一个框架，该框架利用同一种学习方法来生成speech、音乐和声音效果。我们的框架引入了一个通用的听音表示，称为语言听音（LOA）。任何听音都可以根据AudioMAE自我超vised学习表示学习模型翻译为LOA。在生成过程中，我们使用GPT-2模型将任何Modalities翻译为LOA，然后使用一个conditional on LOA的隐藏噪声模型进行自我超vised听音生成学习。我们的提议的框架自然带来了在上下文学习能力和可重用的自我超vised Pre-trained AudioMAE和隐藏噪声模型的优点。我们的实验在主要的文本到听音、文本到音乐和文本到语音的标准 benchmarcks 上达到了新的状态码或竞争性的性能。我们的 demo 和代码可以在https://audioldm.github.io/audioldm2 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.SD_2023_08_11/" data-id="cllurrpb2008tsw88hjxlcup5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/11/cs.LG_2023_08_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-11 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/11/eess.IV_2023_08_11/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-11 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
