
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-09 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04886 repo_url: None paper_authors: Sourya Dipta Das, Yash Vadi, Abhishek Unnam, Kulde">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-09 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/09/cs.SD_2023_08_09/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04886 repo_url: None paper_authors: Sourya Dipta Das, Yash Vadi, Abhishek Unnam, Kulde">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-08T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:27.561Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/09/cs.SD_2023_08_09/" class="article-date">
  <time datetime="2023-08-08T16:00:00.000Z" itemprop="datePublished">2023-08-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-09 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-Out-of-Distribution-Dialect-Detection-with-Mahalanobis-Distance"><a href="#Unsupervised-Out-of-Distribution-Dialect-Detection-with-Mahalanobis-Distance" class="headerlink" title="Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance"></a>Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04886">http://arxiv.org/abs/2308.04886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sourya Dipta Das, Yash Vadi, Abhishek Unnam, Kuldeep Yadav</li>
<li>for: 本研究旨在提高 диалект分类模型在实际应用中的性能，特别是对异常输入样本的检测。</li>
<li>methods: 我们提出了一种简单 yet effective的无监督 Mahalanobis 距离特征基于方法，用于检测异常输入样本。我们利用 transformer 基于 dialet classifier 模型的所有中间层的卷积特征进行多任务学习。</li>
<li>results: 我们的提出方法与其他现有的 OOD 检测方法相比，显著地提高了检测性能。<details>
<summary>Abstract</summary>
Dialect classification is used in a variety of applications, such as machine translation and speech recognition, to improve the overall performance of the system. In a real-world scenario, a deployed dialect classification model can encounter anomalous inputs that differ from the training data distribution, also called out-of-distribution (OOD) samples. Those OOD samples can lead to unexpected outputs, as dialects of those samples are unseen during model training. Out-of-distribution detection is a new research area that has received little attention in the context of dialect classification. Towards this, we proposed a simple yet effective unsupervised Mahalanobis distance feature-based method to detect out-of-distribution samples. We utilize the latent embeddings from all intermediate layers of a wav2vec 2.0 transformer-based dialect classifier model for multi-task learning. Our proposed approach outperforms other state-of-the-art OOD detection methods significantly.
</details>
<details>
<summary>摘要</summary>
dialet 分类在各种应用中使用，如机器翻译和语音识别，以提高整体系统性能。在真实场景中，部署的 диалект分类模型可能会遇到不同于训练数据分布的输入样本，也就是 OUT-OF-DISTRIBUTION（OOD）样本。这些 OOD 样本会导致模型输出不符预期的结果，因为这些 диалект样本在模型训练中未经见过。OUT-OF-DISTRIBUTION 检测是一个新的研究领域，在 диалект分类方面受到了少量的关注。为了解决这个问题，我们提出了一种简单 yet 有效的无监督 Mahalanobis 距离特征基于方法。我们利用了一个 wav2vec 2.0 基于 transformer 的 диалект分类模型的所有间层精神嵌入。我们的提议方法在与其他现有 OOD 检测方法进行比较时表现出色。
</details></li>
</ul>
<hr>
<h2 id="DiVa-An-Iterative-Framework-to-Harvest-More-Diverse-and-Valid-Labels-from-User-Comments-for-Music"><a href="#DiVa-An-Iterative-Framework-to-Harvest-More-Diverse-and-Valid-Labels-from-User-Comments-for-Music" class="headerlink" title="DiVa: An Iterative Framework to Harvest More Diverse and Valid Labels from User Comments for Music"></a>DiVa: An Iterative Framework to Harvest More Diverse and Valid Labels from User Comments for Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04805">http://arxiv.org/abs/2308.04805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jingyaolliu/diva">https://github.com/jingyaolliu/diva</a></li>
<li>paper_authors: Hongru Liang, Jingyao Liu, Yuanxin Xiang, Jiachen Du, Lanjun Zhou, Shushen Pan, Wenqiang Lei</li>
<li>for: 提高自动音乐标签的完整性，增加更多的标签多样性</li>
<li>methods: 基于用户评论中的信息，提出一个迭代式框架（DiVa），使用预训练分类器和新的联合分数函数，生成更多的多样化和有效标签</li>
<li>results: 对 densely annotated 测试集进行实验，发现 DiVa 比现有方法更有优势，能够生成更多的被金标签所排除的多样化标签<details>
<summary>Abstract</summary>
Towards sufficient music searching, it is vital to form a complete set of labels for each song. However, current solutions fail to resolve it as they cannot produce diverse enough mappings to make up for the information missed by the gold labels. Based on the observation that such missing information may already be presented in user comments, we propose to study the automated music labeling in an essential but under-explored setting, where the model is required to harvest more diverse and valid labels from the users' comments given limited gold labels. To this end, we design an iterative framework (DiVa) to harvest more $\underline{\text{Di}}$verse and $\underline{\text{Va}}$lid labels from user comments for music. The framework makes a classifier able to form complete sets of labels for songs via pseudo-labels inferred from pre-trained classifiers and a novel joint score function. The experiment on a densely annotated testing set reveals the superiority of the Diva over state-of-the-art solutions in producing more diverse labels missed by the gold labels. We hope our work can inspire future research on automated music labeling.
</details>
<details>
<summary>摘要</summary>
向充分的音乐搜索，完善每首歌的标签集是非常重要的。然而，现有的解决方案无法解决这个问题，因为它们无法生成够多样化的映射，以补做金标签中的信息损失。根据用户评论中可能存在的缺失信息的观察，我们提议研究自动化音乐标签化在不足explored的设定下进行，其中模型需要从用户评论中抽取更多的多样化和有效的标签。为此，我们设计了一个迭代框架（DiVa），可以从用户评论中抽取更多的多样化和有效的标签。该框架包括一个基于预训练分类器的pseudo标签INFERRED和一个新的联合分数函数。我们的实验表明，DiVa在一个稠密注解的测试集上表现出色，可以生成更多的多样化的标签，而这些标签在金标签中缺失。我们希望我们的工作能够激励未来的音乐标签化研究。
</details></li>
</ul>
<hr>
<h2 id="Induction-Network-Audio-Visual-Modality-Gap-Bridging-for-Self-Supervised-Sound-Source-Localization"><a href="#Induction-Network-Audio-Visual-Modality-Gap-Bridging-for-Self-Supervised-Sound-Source-Localization" class="headerlink" title="Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization"></a>Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04767">http://arxiv.org/abs/2308.04767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tahy1/avin">https://github.com/tahy1/avin</a></li>
<li>paper_authors: Tianyu Liu, Peng Zhang, Wei Huang, Yufei Zha, Tao You, Yanning Zhang</li>
<li>for: 本研究旨在提高自监督音源位置Localization的性能，尤其是在视觉场景中受到模态不一致的挑战。</li>
<li>methods: 本研究提出了一种Induction Network，用于更有效地bridging模态差异。该网络通过分离视觉和声音模态的梯度，使得可以学习出sound source的描述性视觉表示，并使声音模态与视觉模式一致。此外，还 introduce了一种适应阈值选择策略，以提高Induction Network的 Robustness。</li>
<li>results: 在SoundNet-Flickr和VGG-Sound Source等 dataset上进行了大量实验，并达到了与其他状态 ante Works 相比的更高性能。<details>
<summary>Abstract</summary>
Self-supervised sound source localization is usually challenged by the modality inconsistency. In recent studies, contrastive learning based strategies have shown promising to establish such a consistent correspondence between audio and sound sources in visual scenarios. Unfortunately, the insufficient attention to the heterogeneity influence in the different modality features still limits this scheme to be further improved, which also becomes the motivation of our work. In this study, an Induction Network is proposed to bridge the modality gap more effectively. By decoupling the gradients of visual and audio modalities, the discriminative visual representations of sound sources can be learned with the designed Induction Vector in a bootstrap manner, which also enables the audio modality to be aligned with the visual modality consistently. In addition to a visual weighted contrastive loss, an adaptive threshold selection strategy is introduced to enhance the robustness of the Induction Network. Substantial experiments conducted on SoundNet-Flickr and VGG-Sound Source datasets have demonstrated a superior performance compared to other state-of-the-art works in different challenging scenarios. The code is available at https://github.com/Tahy1/AVIN
</details>
<details>
<summary>摘要</summary>
自我监督的声音源localization通常会遇到Modalit inconsistency。在最近的研究中，基于对比学习的策略已经显示出了可行地建立了视觉场景中声音源与声音的相对匹配。然而，各种杂化特征之间的不同特征仍然限制了这种方案的进一步改进，这也成为了我们的研究动机。在这种研究中，一个Induction Network被提出，可以更有效地跨模态桥接。通过解除视觉和声音模态的梯度，可以学习出特征映射，并且通过设计的引入向量，可以在bootstrapmanner学习出推荐视觉表示。此外，还 introduce了一种可适应阈值选择策略，以提高Induction Network的Robustness。在SoundNet-Flickr和VGG-Sound Source datasets上进行了substantial实验，与其他state-of-the-art工作在不同的挑战场景中均显示出了superior表现。代码可以在https://github.com/Tahy1/AVIN中获取。
</details></li>
</ul>
<hr>
<h2 id="Speaker-Recognition-Using-Isomorphic-Graph-Attention-Network-Based-Pooling-on-Self-Supervised-Representation"><a href="#Speaker-Recognition-Using-Isomorphic-Graph-Attention-Network-Based-Pooling-on-Self-Supervised-Representation" class="headerlink" title="Speaker Recognition Using Isomorphic Graph Attention Network Based Pooling on Self-Supervised Representation"></a>Speaker Recognition Using Isomorphic Graph Attention Network Based Pooling on Self-Supervised Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04666">http://arxiv.org/abs/2308.04666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui Ge, Xinzhou Xu, Haiyan Guo, Tingting Wang, Zhen Yang</li>
<li>for: 本研究旨在提出一种基于自适应表示（i.e., wav2vec 2.0）的Speaker recognition方法，以便利用基于语音数据建立的基础模型进行处理。</li>
<li>methods: 本方法包括三个模块：表示学习、图注意力和汇集，它们同时考虑了学习基于自适应表示以及IsoGAT。</li>
<li>results: 在VoxCeleb1＆2 datasets上进行的实验结果表明，提议的方法可以与现有的pooling方法相比，提高Speaker recognition的性能。<details>
<summary>Abstract</summary>
The emergence of self-supervised representation (i.e., wav2vec 2.0) allows speaker-recognition approaches to process spoken signals through foundation models built on speech data. Nevertheless, effective fusion on the representation requires further investigating, due to the inclusion of fixed or sub-optimal temporal pooling strategies. Despite of improved strategies considering graph learning and graph attention factors, non-injective aggregation still exists in the approaches, which may influence the performance for speaker recognition. In this regard, we propose a speaker recognition approach using Isomorphic Graph ATtention network (IsoGAT) on self-supervised representation. The proposed approach contains three modules of representation learning, graph attention, and aggregation, jointly considering learning on the self-supervised representation and the IsoGAT. Then, we perform experiments for speaker recognition tasks on VoxCeleb1\&2 datasets, with the corresponding experimental results demonstrating the recognition performance for the proposed approach, compared with existing pooling approaches on the self-supervised representation.
</details>
<details>
<summary>摘要</summary>
“自我超级表示（即wave2vec 2.0）的出现允许语音识别方法通过基于语音数据建立的基础模型进行处理。然而，更好地融合表示需要进一步研究，因为包括 fixes 或不优化的时间池化策略。尽管考虑了基于图学习和图注意因素的改进策略，但是非射影聚合仍然存在在这些方法中，这可能影响语音识别的性能。在这种情况下，我们提出一种基于自同型图注意网络（IsoGAT）的语音识别方法。该方法包括三个模块：表示学习、图注意和聚合，同时考虑学习自同型表示和IsoGAT。然后，我们在VoxCeleb1&2 dataset上进行了语音识别任务的实验，并得到了相对于现有池化方法的自同型表示性能。”Note that Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/09/cs.SD_2023_08_09/" data-id="clltaagp4008vr888h9ly4mm9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/09/cs.LG_2023_08_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-09 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/09/eess.IV_2023_08_09/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-09 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
