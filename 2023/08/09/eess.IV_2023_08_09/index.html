
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-09 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04956 repo_url: None paper_authors: Weijie Chen, Lin Yao, Zeqing Xia, Yuhang Wang for: 实现高精度的三维结构重建">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-09 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/09/eess.IV_2023_08_09/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.04956 repo_url: None paper_authors: Weijie Chen, Lin Yao, Zeqing Xia, Yuhang Wang for: 实现高精度的三维结构重建">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-08T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:27.595Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_09" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/09/eess.IV_2023_08_09/" class="article-date">
  <time datetime="2023-08-08T16:00:00.000Z" itemprop="datePublished">2023-08-09</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-09 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ACE-HetEM-for-ab-initio-Heterogenous-Cryo-EM-3D-Reconstruction"><a href="#ACE-HetEM-for-ab-initio-Heterogenous-Cryo-EM-3D-Reconstruction" class="headerlink" title="ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction"></a>ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04956">http://arxiv.org/abs/2308.04956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Chen, Lin Yao, Zeqing Xia, Yuhang Wang</li>
<li>for: 实现高精度的三维结构重建 FROM 二维图像，尤其是在电子显微镜实验中，降低讯号与噪音比和不知之影像转换角度的情况下。</li>
<li>methods: 使用自适应学架构，包括自适应推断和其他相关的构成方法，实现不结合几何预设的实验结构重建。</li>
<li>results: 在实验数据中，ACE-HetEM 可以与非束合方法相比，实现更高的重建分辨率，并且在实验中实现更好的结构重建。<details>
<summary>Abstract</summary>
Due to the extremely low signal-to-noise ratio (SNR) and unknown poses (projection angles and image translation) in cryo-EM experiments, reconstructing 3D structures from 2D images is very challenging. On top of these challenges, heterogeneous cryo-EM reconstruction also has an additional requirement: conformation classification. An emerging solution to this problem is called amortized inference, implemented using the autoencoder architecture or its variants. Instead of searching for the correct image-to-pose/conformation mapping for every image in the dataset as in non-amortized methods, amortized inference only needs to train an encoder that maps images to appropriate latent spaces representing poses or conformations. Unfortunately, standard amortized-inference-based methods with entangled latent spaces have difficulty learning the distribution of conformations and poses from cryo-EM images. In this paper, we propose an unsupervised deep learning architecture called "ACE-HetEM" based on amortized inference. To explicitly enforce the disentanglement of conformation classifications and pose estimations, we designed two alternating training tasks in our method: image-to-image task and pose-to-pose task. Results on simulated datasets show that ACE-HetEM has comparable accuracy in pose estimation and produces even better reconstruction resolution than non-amortized methods. Furthermore, we show that ACE-HetEM is also applicable to real experimental datasets.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于低信号噪声比(SNR)和未知投影角和图像翻译等因素，从普遍电子镜像(cryo-EM)实验中提取3D结构信息是非常困难。此外，非标准的普遍精度 reconstruction 还需要进行拓扑分类。一种升级的解决方案是使用束合推理(amortized inference)，通过自适应网络架构或其变体来实现。而标准的束合推理方法在普遍电子镜像图像上学习 pose 和拓扑分类的 distribuition 很困难。在这篇论文中，我们提出了一种无监督深度学习架构，称为 "ACE-HetEM"，基于束合推理。我们为了明确分离 pose 和拓扑分类的束合，设计了两个相互循环训练任务：图像到图像任务和pose到pose任务。实验结果表明，ACE-HetEM 与非束合方法相比，在pose估计方面具有相似的准确性，并且可以生成更高的重建分辨率。此外，我们还证明了 ACE-HetEM 可以应用于实验 datasets。
</details></li>
</ul>
<hr>
<h2 id="HSD-PAM-High-Speed-Super-Resolution-Deep-Penetration-Photoacoustic-Microscopy-Imaging-Boosted-by-Dual-Branch-Fusion-Network"><a href="#HSD-PAM-High-Speed-Super-Resolution-Deep-Penetration-Photoacoustic-Microscopy-Imaging-Boosted-by-Dual-Branch-Fusion-Network" class="headerlink" title="HSD-PAM: High Speed Super Resolution Deep Penetration Photoacoustic Microscopy Imaging Boosted by Dual Branch Fusion Network"></a>HSD-PAM: High Speed Super Resolution Deep Penetration Photoacoustic Microscopy Imaging Boosted by Dual Branch Fusion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04922">http://arxiv.org/abs/2308.04922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyuan Zhang, Haoran Jin, Zesheng Zheng, Wenwen Zhang, Wenhao Lu, Feng Qin, Arunima Sharma, Manojit Pramanik, Yuanjin Zheng</li>
<li>for: 这篇论文主要用于提出一种解决photoacoustic microscopy（PAM）中三个关键成像参数之间的紧张关系，即成像速度、横向分辨率和吸收深度之间的紧张关系，以提高PAM系统的总性能。</li>
<li>methods: 该论文提出了硬件和软件共设计的方法，首先使用低横向分辨率、低抽样率AR-PAM成像，然后通过提高横向分辨率和上抽样率来提高成像速度和深度，最终实现高速、超分辨率和深度探测的PAM系统（HSD-PAM）。</li>
<li>results: 该论文通过对大量的模拟和生物体实验来验证提出的算法，实验结果表明，提出的算法可以在PAM系统中提高成像质量，包括提高了成像速度16倍，横向分辨率提高5倍，而且保留了AR-PAM模式的深度探测能力。<details>
<summary>Abstract</summary>
Photoacoustic microscopy (PAM) is a novel implementation of photoacoustic imaging (PAI) for visualizing the 3D bio-structure, which is realized by raster scanning of the tissue. However, as three involved critical imaging parameters, imaging speed, lateral resolution, and penetration depth have mutual effect to one the other. The improvement of one parameter results in the degradation of other two parameters, which constrains the overall performance of the PAM system. Here, we propose to break these limitations by hardware and software co-design. Starting with low lateral resolution, low sampling rate AR-PAM imaging which possesses the deep penetration capability, we aim to enhance the lateral resolution and up sampling the images, so that high speed, super resolution, and deep penetration for the PAM system (HSD-PAM) can be achieved. Data-driven based algorithm is a promising approach to solve this issue, thereby a dedicated novel dual branch fusion network is proposed, which includes a high resolution branch and a high speed branch. Since the availability of switchable AR-OR-PAM imaging system, the corresponding low resolution, undersample AR-PAM and high resolution, full sampled OR-PAM image pairs are utilized for training the network. Extensive simulation and in vivo experiments have been conducted to validate the trained model, enhancement results have proved the proposed algorithm achieved the best perceptual and quantitative image quality. As a result, the imaging speed is increased 16 times and the imaging lateral resolution is improved 5 times, while the deep penetration merit of AR-PAM modality is still reserved.
</details>
<details>
<summary>摘要</summary>
фотоакустическая микроскопия (PAM) 是一种新的实现 фотоакустической成像 (PAI)，用于可见三维生物结构，通过扫描器扫描物质。然而，存在三个关键成像参数之间的互相影响关系：成像速度、水平分辨率和吸收深度。改进一个参数会导致另外两个参数下降，这限制了整体 PAM 系统的性能。我们提出了硬件和软件合作的方法，从低水平分辨率、低抽象率 AR-PAM 成像开始，希望通过提高水平分辨率和增加样本率来提高整体性能。基于数据驱动的算法是一种有前途的方法，因此我们提出了一种专门的双支部合并网络，包括高分辨率支部和高速支部。由于可用的可换 AR-OR-PAM 成像系统，对应的低分辨率、下抽样 AR-PAM 和高分辨率、全样本 OR-PAM 图像对是用于训练网络。广泛的 simulations 和生物实验 validate 了训练模型，提升结果证明了我们提出的算法实现了最佳的感知和量化图像质量。因此，成像速度提高 16 倍，水平分辨率提高 5 倍，而 AR-PAM 模式下的深度探测仍然保留。
</details></li>
</ul>
<hr>
<h2 id="StableVQA-A-Deep-No-Reference-Quality-Assessment-Model-for-Video-Stability"><a href="#StableVQA-A-Deep-No-Reference-Quality-Assessment-Model-for-Video-Stability" class="headerlink" title="StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability"></a>StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04904">http://arxiv.org/abs/2308.04904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qmme/stablevqa">https://github.com/qmme/stablevqa</a></li>
<li>paper_authors: Tengchuan Kou, Xiaohong Liu, Wei Sun, Jun Jia, Xiongkuo Min, Guangtao Zhai, Ning Liu<br>for: 本研究旨在提供一个新的视频稳定评价模型，以便更好地评估用户生成内容（UGC）视频中的不稳定程度。methods: 本研究使用了一种新的特征提取器，包括光流、 semantic 和模糊特征提取器，以及一个回归层来预测视频稳定度。results: 对于1952个多样化的UGC视频，我们的新模型StableVQA可以与34名评分者的主观意见更高度相关，并且与现有的VQA-S模型和通用VQA模型相比，具有更高的协方差。<details>
<summary>Abstract</summary>
Video shakiness is an unpleasant distortion of User Generated Content (UGC) videos, which is usually caused by the unstable hold of cameras. In recent years, many video stabilization algorithms have been proposed, yet no specific and accurate metric enables comprehensively evaluating the stability of videos. Indeed, most existing quality assessment models evaluate video quality as a whole without specifically taking the subjective experience of video stability into consideration. Therefore, these models cannot measure the video stability explicitly and precisely when severe shakes are present. In addition, there is no large-scale video database in public that includes various degrees of shaky videos with the corresponding subjective scores available, which hinders the development of Video Quality Assessment for Stability (VQA-S). To this end, we build a new database named StableDB that contains 1,952 diversely-shaky UGC videos, where each video has a Mean Opinion Score (MOS) on the degree of video stability rated by 34 subjects. Moreover, we elaborately design a novel VQA-S model named StableVQA, which consists of three feature extractors to acquire the optical flow, semantic, and blur features respectively, and a regression layer to predict the final stability score. Extensive experiments demonstrate that the StableVQA achieves a higher correlation with subjective opinions than the existing VQA-S models and generic VQA models. The database and codes are available at https://github.com/QMME/StableVQA.
</details>
<details>
<summary>摘要</summary>
视频抖动是用户生成内容（UGC）视频中不прият的扭曲，通常是因为摄像机不稳定。在过去的几年中，许多视频稳定算法已经被提出，但没有特定和准确的度量可以全面评估视频的稳定性。实际上，大多数现有的视频质量评估模型都是不特别地考虑视频稳定性的，因此无法准确地测量视频中严重的抖动。此外，没有公开的大规模视频数据库，其中包含不同程度的抖动视频和相应的主观评分，这限制了视频质量评估的发展。为此，我们建立了一个新的数据库 named StableDB，其中包含1,952个多样性抖动的UGC视频，每个视频都有34名评分员对视频的稳定度进行了 Mean Opinion Score（MOS）评分。此外，我们还 elaborately 设计了一种新的VQA-S模型 named StableVQA，它包括三个特征提取器来获取光流、semantic和模糊特征，以及一个回归层来预测最终的稳定度分。广泛的实验表明，StableVQA可以与主观意见更高度相关性。数据库和代码可以在 GitHub 上获取。
</details></li>
</ul>
<hr>
<h2 id="An-automated-pipeline-for-quantitative-T2-fetal-body-MRI-and-segmentation-at-low-field"><a href="#An-automated-pipeline-for-quantitative-T2-fetal-body-MRI-and-segmentation-at-low-field" class="headerlink" title="An automated pipeline for quantitative T2* fetal body MRI and segmentation at low field"></a>An automated pipeline for quantitative T2* fetal body MRI and segmentation at low field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04903">http://arxiv.org/abs/2308.04903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kelly Payette, Alena Uus, Jordina Aviles Verdera, Carla Avena Zampieri, Megan Hall, Lisa Story, Maria Deprez, Mary A. Rutherford, Joseph V. Hajnal, Sebastien Ourselin, Raphael Tomi-Tricot, Jana Hutter</li>
<li>For: 这项研究旨在开发一种 semi-automatic ipeline，用于在低场强磁共振成像中进行快速和详细的量化 T2* 相关分析。* Methods: 该ipeline 使用了量化 MRI 技术，并利用了可变多重Dynamicsequence和可变压缩 reconstruction 技术，以生成高分辨率的三维Volumetric数据。此外，该ipeline 还使用了一种 semi-supervised  neural network 来自动 segment 胎儿体内的十个不同器官。* Results: 研究发现，使用这种 pipeline，可以成功地进行快速和详细的量化 T2* 相关分析，并且具有高度的 robustness 性，可以抵御运动artefacts。此外，研究还发现了胎儿体内各器官的 T2* 值与 gestational age 之间的强相关关系。<details>
<summary>Abstract</summary>
Fetal Magnetic Resonance Imaging at low field strengths is emerging as an exciting direction in perinatal health. Clinical low field (0.55T) scanners are beneficial for fetal imaging due to their reduced susceptibility-induced artefacts, increased T2* values, and wider bore (widening access for the increasingly obese pregnant population). However, the lack of standard automated image processing tools such as segmentation and reconstruction hampers wider clinical use. In this study, we introduce a semi-automatic pipeline using quantitative MRI for the fetal body at low field strength resulting in fast and detailed quantitative T2* relaxometry analysis of all major fetal body organs. Multi-echo dynamic sequences of the fetal body were acquired and reconstructed into a single high-resolution volume using deformable slice-to-volume reconstruction, generating both structural and quantitative T2* 3D volumes. A neural network trained using a semi-supervised approach was created to automatically segment these fetal body 3D volumes into ten different organs (resulting in dice values > 0.74 for 8 out of 10 organs). The T2* values revealed a strong relationship with GA in the lungs, liver, and kidney parenchyma (R^2>0.5). This pipeline was used successfully for a wide range of GAs (17-40 weeks), and is robust to motion artefacts. Low field fetal MRI can be used to perform advanced MRI analysis, and is a viable option for clinical scanning.
</details>
<details>
<summary>摘要</summary>
低场强矩 Magnetic Resonance Imaging (MRI) 在产科医学中得到了推广应用。低场强矩 (0.55T) 磁共振仪器在胎儿成像中有利，因为它们具有降低感应性引起的误差、提高 T2* 值和更宽的磁共振腔（扩大对增加质量增大的怀孕女性人口的访问）。然而，因为没有标准的自动化图像处理工具，如分 segmentation 和重建，因此对于广泛的临床应用仍存在限制。本研究推出了一种半自动化管道，使用量化 MRI 对胎儿体部进行高精度的 T2* 相relaxometry 分析。多echo 动态序列获取并重建成一个高分辨率的三维卷积体，生成了胎儿体部的结构和量化 T2* 三维卷积体。通过 semi-supervised 方法训练的神经网络可以自动将胎儿体部三维卷积体分割成十个不同的器官（得到了 dice 值 > 0.74 的 eight 个器官）。T2* 值与胎儿龄（GA） exhibit 强相关性（R^2>0.5）。这种管道在 17-40 周的多个 Gestational Age (GA) 上运行，并对运动误差具有抗衡性。低场强矩胎儿 MRI 可以进行高级 MRI 分析，是临床扫描的可能性。
</details></li>
</ul>
<hr>
<h2 id="Transmission-and-Color-guided-Network-for-Underwater-Image-Enhancement"><a href="#Transmission-and-Color-guided-Network-for-Underwater-Image-Enhancement" class="headerlink" title="Transmission and Color-guided Network for Underwater Image Enhancement"></a>Transmission and Color-guided Network for Underwater Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04892">http://arxiv.org/abs/2308.04892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Mu, Jing Fang, Haotian Qian, Cong Bai</li>
<li>for: 提高水下图像质量，解决光线干扰和颜色偏差问题</li>
<li>methods: 提出了一种基于自适应传输和动态颜色引导的网络（名为ATDCnet），利用物理知识设计了适应传输指导模块（ATM）和动态颜色引导模块（DCM），并实现了多 Stage feature fusion 和注意力机制来同时进行颜色恢复和对比度增强</li>
<li>results: 对多个标准数据集进行了广泛的实验，并达到了当今最佳性能水平<details>
<summary>Abstract</summary>
In recent years, with the continuous development of the marine industry, underwater image enhancement has attracted plenty of attention. Unfortunately, the propagation of light in water will be absorbed by water bodies and scattered by suspended particles, resulting in color deviation and low contrast. To solve these two problems, we propose an Adaptive Transmission and Dynamic Color guided network (named ATDCnet) for underwater image enhancement. In particular, to exploit the knowledge of physics, we design an Adaptive Transmission-directed Module (ATM) to better guide the network. To deal with the color deviation problem, we design a Dynamic Color-guided Module (DCM) to post-process the enhanced image color. Further, we design an Encoder-Decoder-based Compensation (EDC) structure with attention and a multi-stage feature fusion mechanism to perform color restoration and contrast enhancement simultaneously. Extensive experiments demonstrate the state-of-the-art performance of the ATDCnet on multiple benchmark datasets.
</details>
<details>
<summary>摘要</summary>
Recently, with the continuous development of the marine industry, underwater image enhancement has attracted a lot of attention. Unfortunately, the propagation of light in water will be absorbed by water bodies and scattered by suspended particles, resulting in color deviation and low contrast. To solve these two problems, we propose an Adaptive Transmission and Dynamic Color guided network (named ATDCnet) for underwater image enhancement. In particular, to exploit the knowledge of physics, we design an Adaptive Transmission-directed Module (ATM) to better guide the network. To deal with the color deviation problem, we design a Dynamic Color-guided Module (DCM) to post-process the enhanced image color. Further, we design an Encoder-Decoder-based Compensation (EDC) structure with attention and a multi-stage feature fusion mechanism to perform color restoration and contrast enhancement simultaneously. Extensive experiments demonstrate the state-of-the-art performance of the ATDCnet on multiple benchmark datasets.Here's the word-for-word translation of the text into Simplified Chinese:最近几年，marine工业的不断发展，下水图像提高吸引了很多关注。然而，水中光的传播会被水体吸收和悬浮颗粒扰乱，导致颜色偏移和对比度低。为解决这两个问题，我们提议一种名为ATDCnet的下水图像提高网络。特别是，利用物理知识，我们设计了一个名为ATM的适应传输导向模块，更好地引导网络。另外，我们设计了一个名为DCM的动态颜色导向模块，用于后处理提高图像颜色。此外，我们设计了一个名为EDC的Encoder-Decoder-based Compensation结构，并将注意力和多个阶段特征融合机制用于同时恢复颜色和对比度。广泛的实验证明ATDCnet在多个标准数据集上达到了顶尖性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Networks-for-Heterogeneous-Augmentation-of-Cranial-Defects"><a href="#Deep-Generative-Networks-for-Heterogeneous-Augmentation-of-Cranial-Defects" class="headerlink" title="Deep Generative Networks for Heterogeneous Augmentation of Cranial Defects"></a>Deep Generative Networks for Heterogeneous Augmentation of Cranial Defects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04883">http://arxiv.org/abs/2308.04883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamil Kwarciak, Marek Wodzinski</li>
<li>for: 这篇论文的目的是提出一种解决个性化颅部嵌入设计中数据缺乏问题的方法，通过利用深度学习技术和生成模型创造Synthetic Skulls。</li>
<li>methods: 这篇论文使用了三种深度生成模型，包括Wasserstein生成对抗网络受条件（WGAN-GP）、WGAN-GP混合模型（VAE&#x2F;WGAN-GP）和Introspective Variational Autoencoder（IntroVAE），将这些模型应用于创建Synthetic Skulls。</li>
<li>results: 这篇论文表明，这些生成模型可以创建大量的具有相同缺陷的Synthetic Skulls，并且可以与实际颅内部嵌入设计中的缺陷匹配。实际应用中，这些生成模型可以帮助提高自动设计个性化颅部嵌入的精度和效率。<details>
<summary>Abstract</summary>
The design of personalized cranial implants is a challenging and tremendous task that has become a hot topic in terms of process automation with the use of deep learning techniques. The main challenge is associated with the high diversity of possible cranial defects. The lack of appropriate data sources negatively influences the data-driven nature of deep learning algorithms. Hence, one of the possible solutions to overcome this problem is to rely on synthetic data. In this work, we propose three volumetric variations of deep generative models to augment the dataset by generating synthetic skulls, i.e. Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), WGAN-GP hybrid with Variational Autoencoder pretraining (VAE/WGAN-GP) and Introspective Variational Autoencoder (IntroVAE). We show that it is possible to generate dozens of thousands of defective skulls with compatible defects that achieve a trade-off between defect heterogeneity and the realistic shape of the skull. We evaluate obtained synthetic data quantitatively by defect segmentation with the use of V-Net and qualitatively by their latent space exploration. We show that the synthetically generated skulls highly improve the segmentation process compared to using only the original unaugmented data. The generated skulls may improve the automatic design of personalized cranial implants for real medical cases.
</details>
<details>
<summary>摘要</summary>
personalized cranial implant 设计是一项复杂且具有挑战性的任务，这个领域的过程自动化已经成为热点。主要挑战在于可能出现的颅骨缺陷的多样性。因为没有相应的数据源，这对深度学习算法来说是一个缺乏数据的问题。为了解决这个问题，我们提出了三种三维变化的深度生成模型来增强数据集，即 Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP)、WGAN-GP 混合 Variational Autoencoder 预训练 (VAE/WGAN-GP) 以及 Introspective Variational Autoencoder (IntroVAE)。我们证明了可以生成多达万个具有相同缺陷的颅骨，并且这些缺陷具有质量和颅骨的真实形状之间的 compatibles 性。我们通过 V-Net 进行缺陷分割评估和 latent space 探索来评估获得的 sintethic 数据质量。结果表明，使用生成的颅骨可以大幅提高分割过程的精度，比使用原始未修改数据更好。这些生成的颅骨可能会改善实际医疗案例中的自动颅骨Implant 设计。
</details></li>
</ul>
<hr>
<h2 id="HyperCoil-Recon-A-Hypernetwork-based-Adaptive-Coil-Configuration-Task-Switching-Network-for-MRI-Reconstruction"><a href="#HyperCoil-Recon-A-Hypernetwork-based-Adaptive-Coil-Configuration-Task-Switching-Network-for-MRI-Reconstruction" class="headerlink" title="HyperCoil-Recon: A Hypernetwork-based Adaptive Coil Configuration Task Switching Network for MRI Reconstruction"></a>HyperCoil-Recon: A Hypernetwork-based Adaptive Coil Configuration Task Switching Network for MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04821">http://arxiv.org/abs/2308.04821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sriprabhar/hypercoil-recon">https://github.com/sriprabhar/hypercoil-recon</a></li>
<li>paper_authors: Sriprabha Ramanarayanan, Mohammad Al Fahim, Rahul G. S., Amrit Kumar Jethi, Keerthi Ram, Mohanasankar Sivaprakasam<br>for: HyperCoil-Recon is designed to improve the speed and flexibility of parallel imaging MRI reconstruction by learning to adapt to different coil configurations on the fly.methods: The proposed method uses a hypernetwork-based approach to learn a single set of weights that can be applied to a variety of coil configurations, rather than training separate models for each configuration.results: The proposed method was able to adapt to unseen coil configurations and achieve performance comparable to coil configuration-specific models, and outperform configuration-invariant models in simulations using knee and brain data.<details>
<summary>Abstract</summary>
Parallel imaging, a fast MRI technique, involves dynamic adjustments based on the configuration i.e. number, positioning, and sensitivity of the coils with respect to the anatomy under study. Conventional deep learning-based image reconstruction models have to be trained or fine-tuned for each configuration, posing a barrier to clinical translation, given the lack of computational resources and machine learning expertise for clinicians to train models at deployment. Joint training on diverse datasets learns a single weight set that might underfit to deviated configurations. We propose, HyperCoil-Recon, a hypernetwork-based coil configuration task-switching network for multi-coil MRI reconstruction that encodes varying configurations of the numbers of coils in a multi-tasking perspective, posing each configuration as a task. The hypernetworks infer and embed task-specific weights into the reconstruction network, 1) effectively utilizing the contextual knowledge of common and varying image features among the various fields-of-view of the coils, and 2) enabling generality to unseen configurations at test time. Experiments reveal that our approach 1) adapts on the fly to various unseen configurations up to 32 coils when trained on lower numbers (i.e. 7 to 11) of randomly varying coils, and to 120 deviated unseen configurations when trained on 18 configurations in a single model, 2) matches the performance of coil configuration-specific models, and 3) outperforms configuration-invariant models with improvement margins of around 1 dB / 0.03 and 0.3 dB / 0.02 in PSNR / SSIM for knee and brain data. Our code is available at https://github.com/sriprabhar/HyperCoil-Recon
</details>
<details>
<summary>摘要</summary>
параллельный изображение, a fast MRI technique, involves dynamic adjustments based on the configuration i.e. number, positioning, and sensitivity of the coils with respect to the anatomy under study. Conventional deep learning-based image reconstruction models have to be trained or fine-tuned for each configuration, posing a barrier to clinical translation, given the lack of computational resources and machine learning expertise for clinicians to train models at deployment. Joint training on diverse datasets learns a single weight set that might underfit to deviated configurations. We propose, HyperCoil-Recon, a hypernetwork-based coil configuration task-switching network for multi-coil MRI reconstruction that encodes varying configurations of the numbers of coils in a multi-tasking perspective, posing each configuration as a task. The hypernetworks infer and embed task-specific weights into the reconstruction network, 1) effectively utilizing the contextual knowledge of common and varying image features among the various fields-of-view of the coils, and 2) enabling generality to unseen configurations at test time. Experiments reveal that our approach 1) adapts on the fly to various unseen configurations up to 32 coils when trained on lower numbers (i.e. 7 to 11) of randomly varying coils, and to 120 deviated unseen configurations when trained on 18 configurations in a single model, 2) matches the performance of coil configuration-specific models, and 3) outperforms configuration-invariant models with improvement margins of around 1 dB / 0.03 and 0.3 dB / 0.02 in PSNR / SSIM for knee and brain data. Our code is available at https://github.com/sriprabhar/HyperCoil-Recon.
</details></li>
</ul>
<hr>
<h2 id="An-Integrated-Visual-Analytics-System-for-Studying-Clinical-Carotid-Artery-Plaques"><a href="#An-Integrated-Visual-Analytics-System-for-Studying-Clinical-Carotid-Artery-Plaques" class="headerlink" title="An Integrated Visual Analytics System for Studying Clinical Carotid Artery Plaques"></a>An Integrated Visual Analytics System for Studying Clinical Carotid Artery Plaques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06285">http://arxiv.org/abs/2308.06285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoqing Xu, Zhentao Zheng, Yiting Fu, Baofeng Chang, Legao Chen, Minghui Wu, Mingli Song, Jinsong Jiang</li>
<li>for: 这项研究的目的是为 vascular surgery experts 提供一个完整的血管疾病诊断和治疗指南，以帮助他们更好地理解和分析血管疾病的诱因和组成部分。</li>
<li>methods: 这个系统主要包括两个功能：首先，通过一系列的信息视觉方法显示血管疾病患者的临床生物学指标数据与血管疾病相关性的关系，并将这些数据与患者的医疗图像集成分析。其次，通过机器学习技术提高血管疾病患者的内在关系分析，并在医疗图像上显示血管疾病的空间分布。</li>
<li>results: 我们通过使用实际数据来进行两个案例研究，结果表明，我们设计的血管疾病分析系统可以帮助 vascular surgery experts 更好地诊断和治疗血管疾病。<details>
<summary>Abstract</summary>
Carotid artery plaques can cause arterial vascular diseases such as stroke and myocardial infarction, posing a severe threat to human life. However, the current clinical examination mainly relies on a direct assessment by physicians of patients' clinical indicators and medical images, lacking an integrated visualization tool for analyzing the influencing factors and composition of carotid artery plaques. We have designed an intelligent carotid artery plaque visual analysis system for vascular surgery experts to comprehensively analyze the clinical physiological and imaging indicators of carotid artery diseases. The system mainly includes two functions: First, it displays the correlation between carotid artery plaque and various factors through a series of information visualization methods and integrates the analysis of patient physiological indicator data. Second, it enhances the interface guidance analysis of the inherent correlation between the components of carotid artery plaque through machine learning and displays the spatial distribution of the plaque on medical images. Additionally, we conducted two case studies on carotid artery plaques using real data obtained from a hospital, and the results indicate that our designed carotid analysis system can effectively provide clinical diagnosis and treatment guidance for vascular surgeons.
</details>
<details>
<summary>摘要</summary>
《椎动脉瘤可能导致arterial vascular diseases 如roke和myocardial infarction，对人类生命造成严重威胁。然而，现有诊断方法主要依靠医生直接评估病人的临床指标和医疗影像，缺乏一个统合Visualization工具 для分析椎动脉瘤的影响因素和成分。我们已经设计了一个智能椎动脉瘤分析系统，用于血管医生彻底分析椎动脉瘤疾病的临床生物学和影像指标。系统主要包括两个功能：首先，显示椎动脉瘤和不同因素之间的相互关联，通过一系列的信息可视化方法，并与病人生物学指标数据进行整合分析。其次，通过机器学习增强椎动脉瘤成分的自然相互关联，并在医疗影像上显示椎动脉瘤的空间分布。此外，我们使用了实际数据，进行了两个实验研究，结果显示，我们设计的椎动脉瘤分析系统可以有效地提供临床诊断和治疗指南 для血管医生。》
</details></li>
</ul>
<hr>
<h2 id="Long-Distance-Gesture-Recognition-using-Dynamic-Neural-Networks"><a href="#Long-Distance-Gesture-Recognition-using-Dynamic-Neural-Networks" class="headerlink" title="Long-Distance Gesture Recognition using Dynamic Neural Networks"></a>Long-Distance Gesture Recognition using Dynamic Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04643">http://arxiv.org/abs/2308.04643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubhang Bhatnagar, Sharath Gopal, Narendra Ahuja, Liu Ren</li>
<li>for: 本研究旨在提出一种新的、准确和高效的手势识别方法，用于长距离手势识别。</li>
<li>methods: 本方法使用动态神经网络选择手势包含的空间数据特征进行进一步处理，以提高识别精度和计算效率。</li>
<li>results: 对于LD-ConGR长距离数据集，本方法与之前的状态 искусственный智能方法相比，具有更高的识别精度和计算效率。<details>
<summary>Abstract</summary>
Gestures form an important medium of communication between humans and machines. An overwhelming majority of existing gesture recognition methods are tailored to a scenario where humans and machines are located very close to each other. This short-distance assumption does not hold true for several types of interactions, for example gesture-based interactions with a floor cleaning robot or with a drone. Methods made for short-distance recognition are unable to perform well on long-distance recognition due to gestures occupying only a small portion of the input data. Their performance is especially worse in resource constrained settings where they are not able to effectively focus their limited compute on the gesturing subject. We propose a novel, accurate and efficient method for the recognition of gestures from longer distances. It uses a dynamic neural network to select features from gesture-containing spatial regions of the input sensor data for further processing. This helps the network focus on features important for gesture recognition while discarding background features early on, thus making it more compute efficient compared to other techniques. We demonstrate the performance of our method on the LD-ConGR long-distance dataset where it outperforms previous state-of-the-art methods on recognition accuracy and compute efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN Gestures form an important medium of communication between humans and machines. An overwhelming majority of existing gesture recognition methods are tailored to a scenario where humans and machines are located very close to each other. This short-distance assumption does not hold true for several types of interactions, for example gesture-based interactions with a floor cleaning robot or with a drone. Methods made for short-distance recognition are unable to perform well on long-distance recognition due to gestures occupying only a small portion of the input data. Their performance is especially worse in resource constrained settings where they are not able to effectively focus their limited compute on the gesturing subject. We propose a novel, accurate and efficient method for the recognition of gestures from longer distances. It uses a dynamic neural network to select features from gesture-containing spatial regions of the input sensor data for further processing. This helps the network focus on features important for gesture recognition while discarding background features early on, thus making it more compute efficient compared to other techniques. We demonstrate the performance of our method on the LD-ConGR long-distance dataset where it outperforms previous state-of-the-art methods on recognition accuracy and compute efficiency.Note: The `translate_language` directive is used to specify the language to be translated. In this case, we're using Simplified Chinese (zh-CN).
</details></li>
</ul>
<hr>
<h2 id="1st-Place-Solution-for-CVPR2023-BURST-Long-Tail-and-Open-World-Challenges"><a href="#1st-Place-Solution-for-CVPR2023-BURST-Long-Tail-and-Open-World-Challenges" class="headerlink" title="1st Place Solution for CVPR2023 BURST Long Tail and Open World Challenges"></a>1st Place Solution for CVPR2023 BURST Long Tail and Open World Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04598">http://arxiv.org/abs/2308.04598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaer Huang</li>
<li>for: 这个研究旨在提高影像对象分割（Video Instance Segmentation，VIS）的精度和可靠性，以应对实际世界中的多样化物品。</li>
<li>methods: 我们使用了一种叫做Repeat Factor Sampling的方法，首先在LVISv0.5和COCO dataset上训练探测器，然后在TAO dataset上训练实例外观相似性头。</li>
<li>results: 我们的方法（LeTracker）在BURST test set上获得14.9 HOTAall的成绩，排名第一名。在开放世界挑战中，我们只使用64个分类（BURST Train subset和COCOdataset的交集类别）的标签资料进行训练，并在BURST test set上进行测试，获得61.4 OWTAall的成绩，排名第一名。<details>
<summary>Abstract</summary>
Currently, Video Instance Segmentation (VIS) aims at segmenting and categorizing objects in videos from a closed set of training categories that contain only a few dozen of categories, lacking the ability to handle diverse objects in real-world videos. As TAO and BURST datasets release, we have the opportunity to research VIS in long-tailed and open-world scenarios. Traditional VIS methods are evaluated on benchmarks limited to a small number of common classes, But practical applications require trackers that go beyond these common classes, detecting and tracking rare and even never-before-seen objects. Inspired by the latest MOT paper for the long tail task (Tracking Every Thing in the Wild, Siyuan Li et), for the BURST long tail challenge, we train our model on a combination of LVISv0.5 and the COCO dataset using repeat factor sampling. First, train the detector with segmentation and CEM on LVISv0.5 + COCO dataset. And then, train the instance appearance similarity head on the TAO dataset. at last, our method (LeTracker) gets 14.9 HOTAall in the BURST test set, ranking 1st in the benchmark. for the open-world challenges, we only use 64 classes (Intersection classes of BURST Train subset and COCO dataset, without LVIS dataset) annotations data training, and testing on BURST test set data and get 61.4 OWTAall, ranking 1st in the benchmark. Our code will be released to facilitate future research.
</details>
<details>
<summary>摘要</summary>
当前，视频实例分割（VIS）目标是将视频中的对象分割和分类，但是它通常只能处理固定的训练类别，lacking the ability to handle diverse objects in real-world videos。随着TAO和BURST数据集的发布，我们有机会进行VIS在长尾和开放世界场景下的研究。传统的VIS方法通常在限定的几个常见类型上进行评估，但是实际应用需要跟踪器能够检测和跟踪不同的、甚至从未seen的对象。 Drawing inspiration from the latest MOT paper on the long tail task (Tracking Every Thing in the Wild, Siyuan Li et al.), for the BURST long tail challenge, we train our model on a combination of LVISv0.5 and the COCO dataset using repeat factor sampling. Specifically, we first train the detector with segmentation and CEM on the LVISv0.5 + COCO dataset. Then, we train the instance appearance similarity head on the TAO dataset. Finally, our method (LeTracker) achieves 14.9 HOTAall in the BURST test set, ranking 1st in the benchmark. For the open-world challenges, we only use 64 classes (Intersection classes of BURST Train subset and COCO dataset, without LVIS dataset) annotations data for training, and test on BURST test set data, achieving 61.4 OWTAall, ranking 1st in the benchmark. Our code will be released to facilitate future research.
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Semantic-Segmentation-of-Cell-Nuclei-via-Diffusion-based-Large-Scale-Pre-Training-and-Collaborative-Learning"><a href="#Semi-Supervised-Semantic-Segmentation-of-Cell-Nuclei-via-Diffusion-based-Large-Scale-Pre-Training-and-Collaborative-Learning" class="headerlink" title="Semi-Supervised Semantic Segmentation of Cell Nuclei via Diffusion-based Large-Scale Pre-Training and Collaborative Learning"></a>Semi-Supervised Semantic Segmentation of Cell Nuclei via Diffusion-based Large-Scale Pre-Training and Collaborative Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04578">http://arxiv.org/abs/2308.04578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuchen Shao, Sourya Sengupta, Hua Li, Mark A. Anastasio<br>for:This paper focuses on the task of automated semantic segmentation of cell nuclei in microscopic images, which is crucial for disease diagnosis and tissue microenvironment analysis.methods:The proposed method is a semi-supervised framework that leverages unsupervised pre-training and collaborative learning to improve segmentation performance. The framework consists of three main components: pre-training a diffusion model on a large-scale unlabeled dataset, aggregating semantic features using a transformer-based decoder, and implementing a collaborative learning framework between the diffusion-based segmentation model and a supervised segmentation model.results:The proposed method achieves significant improvements compared to competitive semi-supervised segmentation methods and supervised baselines on four publicly available datasets. Out-of-distribution tests and thorough ablation experiments further confirm the generality and superiority of the proposed method.<details>
<summary>Abstract</summary>
Automated semantic segmentation of cell nuclei in microscopic images is crucial for disease diagnosis and tissue microenvironment analysis. Nonetheless, this task presents challenges due to the complexity and heterogeneity of cells. While supervised deep learning methods are promising, they necessitate large annotated datasets that are time-consuming and error-prone to acquire. Semi-supervised approaches could provide feasible alternatives to this issue. However, the limited annotated data may lead to subpar performance of semi-supervised methods, regardless of the abundance of unlabeled data. In this paper, we introduce a novel unsupervised pre-training-based semi-supervised framework for cell-nuclei segmentation. Our framework is comprised of three main components. Firstly, we pretrain a diffusion model on a large-scale unlabeled dataset. The diffusion model's explicit modeling capability facilitates the learning of semantic feature representation from the unlabeled data. Secondly, we achieve semantic feature aggregation using a transformer-based decoder, where the pretrained diffusion model acts as the feature extractor, enabling us to fully utilize the small amount of labeled data. Finally, we implement a collaborative learning framework between the diffusion-based segmentation model and a supervised segmentation model to further enhance segmentation performance. Experiments were conducted on four publicly available datasets to demonstrate significant improvements compared to competitive semi-supervised segmentation methods and supervised baselines. A series of out-of-distribution tests further confirmed the generality of our framework. Furthermore, thorough ablation experiments and visual analysis confirmed the superiority of our proposed method.
</details>
<details>
<summary>摘要</summary>
自动化的细胞核体分 segmentation在微scopic图像中是致命的 для疾病诊断和组织微environment分析。然而，这个任务具有复杂性和多样性的细胞问题。虽然深度学习方法有承诺，但它们需要大量的标注数据，这些数据是时间consuming和error-prone的获得的。半supervised方法可能提供可行的解决方案。然而，有限的标注数据可能会导致半supervised方法的性能下降，即使有大量的无标注数据。在这篇文章中，我们介绍了一种新的无supervised预训练基于的半supervised框架 для细胞核体分 segmentation。我们的框架由三个主要组成部分。首先，我们在一个大规模的无标注数据集上预训练了一个扩散模型。扩散模型的Explicit模型化能力使得它可以从无标注数据中学习含义特征表示。其次，我们使用一个transformer-based decoder来实现semantic feature的汇集，其中预训练的扩散模型 acts as the feature extractor，这样我们可以充分利用小量的标注数据。最后，我们实现了一种协同学习框架，其中 diffusion-based segmentation模型和一个supervised segmentation模型进行协同学习，以进一步提高分 segmentation性能。我们在四个公开available的数据集上进行了实验，并证明了与竞争性的半supervised分 segmentation方法和标注基eline相比，我们的方法具有显著的改善。此外，我们还进行了一系列out-of-distribution测试，以确认我们的框架的普适性。此外，我们还进行了严格的层次分析和可见分析，以证明我们的提议的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automatic-Scoring-of-Spinal-X-ray-for-Ankylosing-Spondylitis"><a href="#Towards-Automatic-Scoring-of-Spinal-X-ray-for-Ankylosing-Spondylitis" class="headerlink" title="Towards Automatic Scoring of Spinal X-ray for Ankylosing Spondylitis"></a>Towards Automatic Scoring of Spinal X-ray for Ankylosing Spondylitis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05123">http://arxiv.org/abs/2308.05123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhan Mo, Yao Chen, Aimee Readie, Gregory Ligozio, Thibaud Coroller, Bartłomiej W. Papież</li>
<li>for: 这个研究旨在解决评估X光影像中脊椎疾病的自动化分类问题，以减少评估成本和时间。</li>
<li>methods: 这个研究使用了一个两步自动分类管道，称为VertXGradeNet，以自动预测X光影像中脊椎疾病的 modify Stoke Ankylosing Spondylitis Spinal Score（mSASSS）。VertXGradeNet使用了我们之前开发的VU抽取管道（VertXNet）生成的VUs作为输入，并根据这些VUs预测mSASSS。</li>
<li>results: 我们的结果表明，VertXGradeNet可以在限量和不均衡的数据集上预测每个VU的mSASSS分数。总的来说，它可以在两个测试数据集上 achiev 0.56和0.51的平衡准确率，对于四个不同的mSASSS分数（即分数为0、1、2、3）。这些结果表明了这种方法的可能性，可以减少未来的脊椎X光影像评估成本。<details>
<summary>Abstract</summary>
Manually grading structural changes with the modified Stoke Ankylosing Spondylitis Spinal Score (mSASSS) on spinal X-ray imaging is costly and time-consuming due to bone shape complexity and image quality variations. In this study, we address this challenge by prototyping a 2-step auto-grading pipeline, called VertXGradeNet, to automatically predict mSASSS scores for the cervical and lumbar vertebral units (VUs) in X-ray spinal imaging. The VertXGradeNet utilizes VUs generated by our previously developed VU extraction pipeline (VertXNet) as input and predicts mSASSS based on those VUs. VertXGradeNet was evaluated on an in-house dataset of lateral cervical and lumbar X-ray images for axial spondylarthritis patients. Our results show that VertXGradeNet can predict the mSASSS score for each VU when the data is limited in quantity and imbalanced. Overall, it can achieve a balanced accuracy of 0.56 and 0.51 for 4 different mSASSS scores (i.e., a score of 0, 1, 2, 3) on two test datasets. The accuracy of the presented method shows the potential to streamline the spinal radiograph readings and therefore reduce the cost of future clinical trials.
</details>
<details>
<summary>摘要</summary>
人工评分结构变化的 modificated Stoke Ankylosing Spondylitis Spinal Score (mSASSS) 在脊梁X射线成像中是成本高和时间耗费大的，主要是因为骨形态复杂和图像质量变化。在这项研究中，我们解决这个挑战 by 开发了一个两步自动评分管线，called VertXGradeNet，以自动预测mSASSS 分数 для脊梁骨Unit (VU) 在X射线脊梁成像中。VertXGradeNet 使用我们之前开发的 VU 提取管线 (VertXNet) 生成的 VU 作为输入，并预测 mSASSS 基于这些 VU。我们的结果表明，VertXGradeNet 可以在限量和不均衡的数据集上预测每个 VU 的 mSASSS 分数。总的来说，它可以在两个测试数据集上达到平衡性的准确率为 0.56 和 0.51  для四个不同的 mSASSS 分数（即分数为 0、1、2、3）。表现的准确性表明了这种方法的潜在能力，可以 Streamline 脊梁X射线成像的读取，从而降低未来的临床试验成本。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/09/eess.IV_2023_08_09/" data-id="cllurrpci00dpsw88cwtugj5i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/09/cs.SD_2023_08_09/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-09 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/08/cs.LG_2023_08_08/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-08 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
