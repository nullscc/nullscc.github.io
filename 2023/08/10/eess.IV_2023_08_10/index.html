
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-10 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Attention-based 3D CNN with Multi-layer Features for Alzheimer’s Disease Diagnosis using Brain Images paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.05655 repo_url: None paper_authors: Yanteng Zhang, Qizhi Teng">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-10 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/10/eess.IV_2023_08_10/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Attention-based 3D CNN with Multi-layer Features for Alzheimer’s Disease Diagnosis using Brain Images paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.05655 repo_url: None paper_authors: Yanteng Zhang, Qizhi Teng">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-09T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:28.208Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/10/eess.IV_2023_08_10/" class="article-date">
  <time datetime="2023-08-09T16:00:00.000Z" itemprop="datePublished">2023-08-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-10 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Attention-based-3D-CNN-with-Multi-layer-Features-for-Alzheimer’s-Disease-Diagnosis-using-Brain-Images"><a href="#Attention-based-3D-CNN-with-Multi-layer-Features-for-Alzheimer’s-Disease-Diagnosis-using-Brain-Images" class="headerlink" title="Attention-based 3D CNN with Multi-layer Features for Alzheimer’s Disease Diagnosis using Brain Images"></a>Attention-based 3D CNN with Multi-layer Features for Alzheimer’s Disease Diagnosis using Brain Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05655">http://arxiv.org/abs/2308.05655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanteng Zhang, Qizhi Teng, Xiaohai He, Tong Niu, Lipei Zhang, Yan Liu, Chao Ren</li>
<li>for: 这个论文旨在提高亚束病（AD）诊断的准确率，通过利用深度学习的 convolutional neural network（CNN）方法，并结合多层特征获取器和注意力机制来更好地捕捉脑图像中的微妙差异。</li>
<li>methods: 本文提出了一种基于ResNet的端到端3D CNN框架，并在多Modal imaging数据上进行了ablation实验，以评估模型的性能。</li>
<li>results: 实验结果显示，我们的方法可以在792名ADNI数据库中的subject上达到89.71%和91.18%的AD诊断精度，并超越了一些当前state-of-the-art方法。另外，我们的模型还可以在不同的脑区域中强调关键区域，并且可以更好地捕捉到脑图像中的微妙差异。<details>
<summary>Abstract</summary>
Structural MRI and PET imaging play an important role in the diagnosis of Alzheimer's disease (AD), showing the morphological changes and glucose metabolism changes in the brain respectively. The manifestations in the brain image of some cognitive impairment patients are relatively inconspicuous, for example, it still has difficulties in achieving accurate diagnosis through sMRI in clinical practice. With the emergence of deep learning, convolutional neural network (CNN) has become a valuable method in AD-aided diagnosis, but some CNN methods cannot effectively learn the features of brain image, making the diagnosis of AD still presents some challenges. In this work, we propose an end-to-end 3D CNN framework for AD diagnosis based on ResNet, which integrates multi-layer features obtained under the effect of the attention mechanism to better capture subtle differences in brain images. The attention maps showed our model can focus on key brain regions related to the disease diagnosis. Our method was verified in ablation experiments with two modality images on 792 subjects from the ADNI database, where AD diagnostic accuracies of 89.71% and 91.18% were achieved based on sMRI and PET respectively, and also outperformed some state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
estructural MRI和PET成像在诊断阿尔ц海默病（AD）中发挥重要作用，显示了大脑的结构变化和葡萄糖代谢变化。在一些 когнитив障碍患者的脑图像中，manifestations 不够醒目，例如，仍然存在在严格实践中获取正确诊断的困难。随着深度学习的出现，卷积神经网络（CNN）成为了AD诊断的有价值方法。然而，一些CNN方法无法有效地学习脑图像的特征，使得AD诊断仍然存在一些挑战。在这项工作中，我们提出了基于ResNet的终端3D CNN框架 для AD诊断，该框架通过多层效应机制来更好地捕捉脑图像中的微妙差异。我们的Attention map表明，我们的模型可以关注与疾病诊断相关的关键脑区域。我们的方法在ADNI数据库上进行了ablation实验，使用了两种模式图像，其中AD诊断精度分别为89.71%和91.18%，并且超越了一些当前的方法。
</details></li>
</ul>
<hr>
<h2 id="SAR-Target-Image-Generation-Method-Using-Azimuth-Controllable-Generative-Adversarial-Network"><a href="#SAR-Target-Image-Generation-Method-Using-Azimuth-Controllable-Generative-Adversarial-Network" class="headerlink" title="SAR Target Image Generation Method Using Azimuth-Controllable Generative Adversarial Network"></a>SAR Target Image Generation Method Using Azimuth-Controllable Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05489">http://arxiv.org/abs/2308.05489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Xiaoyu Liu, Yulin Huang, Deqing Mao, Yin Zhang, Jianyu Yang</li>
<li>for: 提高Synthetic Aperture Radar（SAR）图像的研究进展，增加SAR图像数据量，解决实际中SAR图像数据的有限性问题。</li>
<li>methods: 提出了一种azimuth-controllableGenerative Adversarial Network（GAN），通过将两个SAR图像的特征提取并融合，生成具有中间方位角的SAR图像。network主要包括生成器、推理器、判别器三部分。</li>
<li>results: 经过广泛的实验 validate the effectiveness of the proposed method in terms of azimuth controllability and accuracy of SAR target image generation. The proposed method can generate high-quality SAR target images with controllable azimuth, which can help improve the development of SAR research and solve the problem of limited SAR image data to some extent.<details>
<summary>Abstract</summary>
Sufficient synthetic aperture radar (SAR) target images are very important for the development of researches. However, available SAR target images are often limited in practice, which hinders the progress of SAR application. In this paper, we propose an azimuth-controllable generative adversarial network to generate precise SAR target images with an intermediate azimuth between two given SAR images' azimuths. This network mainly contains three parts: generator, discriminator, and predictor. Through the proposed specific network structure, the generator can extract and fuse the optimal target features from two input SAR target images to generate SAR target image. Then a similarity discriminator and an azimuth predictor are designed. The similarity discriminator can differentiate the generated SAR target images from the real SAR images to ensure the accuracy of the generated, while the azimuth predictor measures the difference of azimuth between the generated and the desired to ensure the azimuth controllability of the generated. Therefore, the proposed network can generate precise SAR images, and their azimuths can be controlled well by the inputs of the deep network, which can generate the target images in different azimuths to solve the small sample problem to some degree and benefit the researches of SAR images. Extensive experimental results show the superiority of the proposed method in azimuth controllability and accuracy of SAR target image generation.
</details>
<details>
<summary>摘要</summary>
充分的synthetic aperture radar（SAR）目标图像对SAR应用的研究发展非常重要，但实际中可用的SAR目标图像很少，这限制了SAR应用的进步。在这篇论文中，我们提出了一种 azimuth可控的生成对抗网络，用于生成具有中间azimuth的准确SAR目标图像。该网络主要由三部分组成：生成器、判别器和预测器。通过我们提出的特定网络结构，生成器可以从两个输入SAR目标图像中提取和融合最佳的目标特征，生成SAR目标图像。然后，我们设计了一个相似性判别器和一个方向预测器。相似性判别器可以判别生成的SAR目标图像与真实SAR图像之间的差异，以确保生成的准确性。方向预测器则可以测量生成的azimuth与所需的azimuth之间的差异，以确保生成的azimuth可控性。因此，我们的网络可以生成准确的SAR目标图像，并且可以通过输入深度网络的参数控制生成的azimuth。这可以生成不同的azimuth的目标图像，解决一定程度上的样本缺乏问题，并为SAR图像研究带来一定的助益。我们的实验结果表明，我们的方法在azimuth可控性和SAR目标图像生成准确性两个方面具有优越性。
</details></li>
</ul>
<hr>
<h2 id="Surface-Masked-AutoEncoder-Self-Supervision-for-Cortical-Imaging-Data"><a href="#Surface-Masked-AutoEncoder-Self-Supervision-for-Cortical-Imaging-Data" class="headerlink" title="Surface Masked AutoEncoder: Self-Supervision for Cortical Imaging Data"></a>Surface Masked AutoEncoder: Self-Supervision for Cortical Imaging Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05474">http://arxiv.org/abs/2308.05474</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/metrics-lab/surface-vision-transformers">https://github.com/metrics-lab/surface-vision-transformers</a></li>
<li>paper_authors: Simon Dahan, Mariana da Silva, Daniel Rueckert, Emma C Robinson</li>
<li>for:  cortical surface learning and cortical phenotype regression</li>
<li>methods: Masked AutoEncoder (MAE) self-supervision</li>
<li>results: 26% improvement in performance, 80% faster convergence compared to models trained from scratch, and robust representations for finetuning in low-data scenarios.Here’s the simplified Chinese text:</li>
<li>for:  cortical表面学习和 cortical表型识别</li>
<li>methods: Masked AutoEncoder (MAE)自我超vision</li>
<li>results: 26%提高表现, 80%更快的整合比对于从 scratch 训练的模型I hope this helps!<details>
<summary>Abstract</summary>
Self-supervision has been widely explored as a means of addressing the lack of inductive biases in vision transformer architectures, which limits generalisation when networks are trained on small datasets. This is crucial in the context of cortical imaging, where phenotypes are complex and heterogeneous, but the available datasets are limited in size. This paper builds upon recent advancements in translating vision transformers to surface meshes and investigates the potential of Masked AutoEncoder (MAE) self-supervision for cortical surface learning. By reconstructing surface data from a masked version of the input, the proposed method effectively models cortical structure to learn strong representations that translate to improved performance in downstream tasks. We evaluate our approach on cortical phenotype regression using the developing Human Connectome Project (dHCP) and demonstrate that pre-training leads to a 26\% improvement in performance, with an 80\% faster convergence, compared to models trained from scratch. Furthermore, we establish that pre-training vision transformer models on large datasets, such as the UK Biobank (UKB), enables the acquisition of robust representations for finetuning in low-data scenarios. Our code and pre-trained models are publicly available at \url{https://github.com/metrics-lab/surface-vision-transformers}.
</details>
<details>
<summary>摘要</summary>
自我监督已经广泛研究以Addressing the lack of inductive biases in vision transformer architectures, which limits generalization when networks are trained on small datasets. This is crucial in the context of cortical imaging, where phenotypes are complex and heterogeneous, but the available datasets are limited in size. This paper builds upon recent advancements in translating vision transformers to surface meshes and investigates the potential of Masked AutoEncoder (MAE) self-supervision for cortical surface learning. By reconstructing surface data from a masked version of the input, the proposed method effectively models cortical structure to learn strong representations that translate to improved performance in downstream tasks. We evaluate our approach on cortical phenotype regression using the developing Human Connectome Project (dHCP) and demonstrate that pre-training leads to a 26% improvement in performance, with an 80% faster convergence, compared to models trained from scratch. Furthermore, we establish that pre-training vision transformer models on large datasets, such as the UK Biobank (UKB), enables the acquisition of robust representations for finetuning in low-data scenarios. Our code and pre-trained models are publicly available at \url{https://github.com/metrics-lab/surface-vision-transformers}.Here's the word-for-word translation of the text into Simplified Chinese:自我监督已经广泛研究以Addressing the lack of inductive biases in vision transformer architectures, which limits generalization when networks are trained on small datasets. This is crucial in the context of cortical imaging, where phenotypes are complex and heterogeneous, but the available datasets are limited in size. This paper builds upon recent advancements in translating vision transformers to surface meshes and investigates the potential of Masked AutoEncoder (MAE) self-supervision for cortical surface learning. By reconstructing surface data from a masked version of the input, the proposed method effectively models cortical structure to learn strong representations that translate to improved performance in downstream tasks. We evaluate our approach on cortical phenotype regression using the developing Human Connectome Project (dHCP) and demonstrate that pre-training leads to a 26% improvement in performance, with an 80% faster convergence, compared to models trained from scratch. Furthermore, we establish that pre-training vision transformer models on large datasets, such as the UK Biobank (UKB), enables the acquisition of robust representations for finetuning in low-data scenarios. Our code and pre-trained models are publicly available at \url{https://github.com/metrics-lab/surface-vision-transformers}.
</details></li>
</ul>
<hr>
<h2 id="Global-in-Local-A-Convolutional-Transformer-for-SAR-ATR-FSL"><a href="#Global-in-Local-A-Convolutional-Transformer-for-SAR-ATR-FSL" class="headerlink" title="Global in Local: A Convolutional Transformer for SAR ATR FSL"></a>Global in Local: A Convolutional Transformer for SAR ATR FSL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05464">http://arxiv.org/abs/2308.05464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Yulin Huang, Xiaoyu Liu, Jifang Pei, Yin Zhang, Jianyu Yang</li>
<li>for: 提高Synthetic Aperture Radar（SAR）自动目标识别（ATR）的性能，特别是在有限的SAR图像下进行几抽取学习（FSL）。</li>
<li>methods: 提出了一种Convolutional Transformer（ConvT）模型，通过建立层次特征表示和捕捉每层局部特征的全局依赖关系，提高SAR ATR FSL的性能。同时，提出了一种新的混合损失函数，可以在少量SAR图像下进行有效的优化。</li>
<li>results: 在Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集上进行了实验，显示了我们提出的ConvT模型在SAR ATR FSL中的效果，不需要其他SAR目标图像进行训练。与现有的SAR ATR FSL方法相比，我们的方法可以更好地适应有限的SAR图像下的几抽取学习。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have dominated the synthetic aperture radar (SAR) automatic target recognition (ATR) for years. However, under the limited SAR images, the width and depth of the CNN-based models are limited, and the widening of the received field for global features in images is hindered, which finally leads to the low performance of recognition. To address these challenges, we propose a Convolutional Transformer (ConvT) for SAR ATR few-shot learning (FSL). The proposed method focuses on constructing a hierarchical feature representation and capturing global dependencies of local features in each layer, named global in local. A novel hybrid loss is proposed to interpret the few SAR images in the forms of recognition labels and contrastive image pairs, construct abundant anchor-positive and anchor-negative image pairs in one batch and provide sufficient loss for the optimization of the ConvT to overcome the few sample effect. An auto augmentation is proposed to enhance and enrich the diversity and amount of the few training samples to explore the hidden feature in a few SAR images and avoid the over-fitting in SAR ATR FSL. Experiments conducted on the Moving and Stationary Target Acquisition and Recognition dataset (MSTAR) have shown the effectiveness of our proposed ConvT for SAR ATR FSL. Different from existing SAR ATR FSL methods employing additional training datasets, our method achieved pioneering performance without other SAR target images in training.
</details>
<details>
<summary>摘要</summary>
干扰神经网络（CNN）在抽象辐射镜（SAR）自动目标识别（ATR）领域已经占据了多年。然而，在有限的SAR图像下，CNN模型的宽度和深度受限，而且在图像中收集全局特征的宽度也受到限制，最终导致识别性能低下。为解决这些挑战，我们提议了一种干扰转换器（ConvT） дляSAR ATR几步学习（FSL）。我们的方法关注于建立层次特征表示和在每层 capture全局依赖的本地特征，称为全球在本地。我们还提出了一种新的混合损失函数，可以将少量SAR图像转化为识别标签和对比图像对，建立丰富的anchor-正例和anchor-负例图像对在一个批中，并提供足够的损失来优化ConvT，以超越几个样本效应。我们还提出了一种自动增强技术，以增加和拓宽少量训练样本，探索隐藏在少量SAR图像中的特征，避免过拟合。我们在MSTAR数据集上进行了实验，结果表明我们的提议的ConvT有效地应用于SAR ATR FSL。与现有的SAR ATR FSL方法不同，我们的方法不需要其他SAR目标图像进行训练，而达到了领先性能。
</details></li>
</ul>
<hr>
<h2 id="Transforming-Breast-Cancer-Diagnosis-Towards-Real-Time-Ultrasound-to-Mammogram-Conversion-for-Cost-Effective-Diagnosis"><a href="#Transforming-Breast-Cancer-Diagnosis-Towards-Real-Time-Ultrasound-to-Mammogram-Conversion-for-Cost-Effective-Diagnosis" class="headerlink" title="Transforming Breast Cancer Diagnosis: Towards Real-Time Ultrasound to Mammogram Conversion for Cost-Effective Diagnosis"></a>Transforming Breast Cancer Diagnosis: Towards Real-Time Ultrasound to Mammogram Conversion for Cost-Effective Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05449">http://arxiv.org/abs/2308.05449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahar Almahfouz Nasser, Ashutosh Sharma, Anmol Saraf, Amruta Mahendra Parulekar, Purvi Haria, Amit Sethi</li>
<li>for: 提高实时ultrasound（US）图像质量，以提高手术过程中的医疗效果。</li>
<li>methods: 使用Stride软件数学模型，通过解决波方程来生成ultrasound图像，并利用域适应来增强模拟图像的真实性。使用生成敌对网络（GANs）解决反向问题，即将ultrasound图像转换为高质量的mammogram图像。</li>
<li>results: 实验结果表明，使用提出的方法可以生成高度可识别的ultrasound图像，具有明显更高的细节程度和质量，比原始US图像更有利于手术过程。<details>
<summary>Abstract</summary>
Ultrasound (US) imaging is better suited for intraoperative settings because it is real-time and more portable than other imaging techniques, such as mammography. However, US images are characterized by lower spatial resolution noise-like artifacts. This research aims to address these limitations by providing surgeons with mammogram-like image quality in real-time from noisy US images. Unlike previous approaches for improving US image quality that aim to reduce artifacts by treating them as (speckle noise), we recognize their value as informative wave interference pattern (WIP). To achieve this, we utilize the Stride software to numerically solve the forward model, generating ultrasound images from mammograms images by solving wave-equations. Additionally, we leverage the power of domain adaptation to enhance the realism of the simulated ultrasound images. Then, we utilize generative adversarial networks (GANs) to tackle the inverse problem of generating mammogram-quality images from ultrasound images. The resultant images have considerably more discernible details than the original US images.
</details>
<details>
<summary>摘要</summary>
超声成像（US）在操作间更适合使用，因为它是实时的，更携带性好于其他成像技术，如胸写影像。然而，US图像受到低分辨率噪声杂乱的影响。本研究目的是提供高品质的胸写影像，以便在实时中为外科医生提供更好的图像。不同于之前的方法，我们不是通过减少噪声来提高US图像质量，而是认可噪声为有用的波动干扰 patrern（WIP）。为此，我们利用Stride软件来数学模拟前向模型，将胸写影像转换成超声图像，并利用领域适应来增强模拟的超声图像的真实性。然后，我们使用生成对抗网络（GANs）来解决胸写影像转换成高品质的超声图像的逆问题。得到的图像具有较原始US图像更多的可识别细节。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Physical-knowledge-guided-Dynamic-Model-for-Underwater-Image-Enhancement"><a href="#A-Generalized-Physical-knowledge-guided-Dynamic-Model-for-Underwater-Image-Enhancement" class="headerlink" title="A Generalized Physical-knowledge-guided Dynamic Model for Underwater Image Enhancement"></a>A Generalized Physical-knowledge-guided Dynamic Model for Underwater Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05447">http://arxiv.org/abs/2308.05447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Mu, Hanning Xu, Zheyuan Liu, Zheng Wang, Sixian Chan, Cong Bai</li>
<li>For: This paper proposes a Generalized Underwater image enhancement method via a Physical-knowledge-guided Dynamic Model (GUPDM) to tackle the challenges of color distortion and low contrast in underwater images.* Methods: The GUPDM method consists of three parts: Atmosphere-based Dynamic Structure (ADS), Transmission-guided Dynamic Structure (TDS), and Prior-based Multi-scale Structure (PMS). The ADS and TDS modules use dynamic convolutions to adaptively extract prior information from underwater images and generate parameters for PMS. The PMS module uses convolution blocks with different kernel sizes and channel attention blocks to fuse multi-scale features.* Results: The proposed GUPDM method can effectively enhance the quality of underwater images and simulate various underwater image types, such as yellow to blue color ranging. The source code will be available at <a target="_blank" rel="noopener" href="https://github.com/shiningZZ/GUPDM">https://github.com/shiningZZ/GUPDM</a>.<details>
<summary>Abstract</summary>
Underwater images often suffer from color distortion and low contrast resulting in various image types, due to the scattering and absorption of light by water. While it is difficult to obtain high-quality paired training samples with a generalized model. To tackle these challenges, we design a Generalized Underwater image enhancement method via a Physical-knowledge-guided Dynamic Model (short for GUPDM), consisting of three parts: Atmosphere-based Dynamic Structure (ADS), Transmission-guided Dynamic Structure (TDS), and Prior-based Multi-scale Structure (PMS). In particular, to cover complex underwater scenes, this study changes the global atmosphere light and the transmission to simulate various underwater image types (e.g., the underwater image color ranging from yellow to blue) through the formation model. We then design ADS and TDS that use dynamic convolutions to adaptively extract prior information from underwater images and generate parameters for PMS. These two modules enable the network to select appropriate parameters for various water types adaptively. Besides, the multi-scale feature extraction module in PMS uses convolution blocks with different kernel sizes and obtains weights for each feature map via channel attention block and fuses them to boost the receptive field of the network. The source code will be available at \href{https://github.com/shiningZZ/GUPDM}{https://github.com/shiningZZ/GUPDM}.
</details>
<details>
<summary>摘要</summary>
水下图像 часто受到颜色扭曲和对比度低下，导致多种图像类型，由于水媒体对光的散射和吸收。而获得高质量的搅合训练样本是困难的。为解决这些挑战，我们设计了一种通用的水下图像提升方法，名为通用物理知识引导动态模型（简称GUPDM），包括三部分：大气dynamic结构（ADS）、传输导动结构（TDS）和多比例层结构（PMS）。特别是，为了涵盖复杂的水下场景，本研究通过形成模型来改变全球大气光和传输，以模拟不同的水下图像类型（如水下图像颜色从黄色到蓝色）。然后，我们设计了ADS和TDS模块，通过动态滤波器来自适应地提取水下图像中的优先信息，并生成PMS模块中的参数。这两个模块使得网络可以在不同的水类型上适应选择合适的参数。此外，PMS模块中的多尺度特征提取模块使用不同的核群大小的卷积块，通过通道注意块和权重融合来提高网络的感知范围。源代码将在 \href{https://github.com/shiningZZ/GUPDM}{https://github.com/shiningZZ/GUPDM} 上提供。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-light-Light-Field-Images-with-A-Deep-Compensation-Unfolding-Network"><a href="#Enhancing-Low-light-Light-Field-Images-with-A-Deep-Compensation-Unfolding-Network" class="headerlink" title="Enhancing Low-light Light Field Images with A Deep Compensation Unfolding Network"></a>Enhancing Low-light Light Field Images with A Deep Compensation Unfolding Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05404">http://arxiv.org/abs/2308.05404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lyuxianqiang/lfll-dcu">https://github.com/lyuxianqiang/lfll-dcu</a></li>
<li>paper_authors: Xianqiang Lyu, Junhui Hou</li>
<li>for: 这个论文是为了提高低光照下捕捉的光场图像（LF）的修复。</li>
<li>methods: 这个框架使用了多Stage的架构，模拟了解决反射图像问题的优化过程，并使用了中间提高后的结果来估计照明地图，以便生成新的提高后结果。此外，框架还包括每个优化阶段的内容相关深度补偿模块，以抑制噪声和照明地图估计错误。</li>
<li>results: 对于实际和模拟数据集，DCUNet在质量和量上都超过了现有方法，并且保留了修复后LF图像的主要几何结构。<details>
<summary>Abstract</summary>
This paper presents a novel and interpretable end-to-end learning framework, called the deep compensation unfolding network (DCUNet), for restoring light field (LF) images captured under low-light conditions. DCUNet is designed with a multi-stage architecture that mimics the optimization process of solving an inverse imaging problem in a data-driven fashion. The framework uses the intermediate enhanced result to estimate the illumination map, which is then employed in the unfolding process to produce a new enhanced result. Additionally, DCUNet includes a content-associated deep compensation module at each optimization stage to suppress noise and illumination map estimation errors. To properly mine and leverage the unique characteristics of LF images, this paper proposes a pseudo-explicit feature interaction module that comprehensively exploits redundant information in LF images. The experimental results on both simulated and real datasets demonstrate the superiority of our DCUNet over state-of-the-art methods, both qualitatively and quantitatively. Moreover, DCUNet preserves the essential geometric structure of enhanced LF images much better. The code will be publicly available at https://github.com/lyuxianqiang/LFLL-DCU.
</details>
<details>
<summary>摘要</summary>
（这篇论文提出了一种新的、可解释的端到端学习框架，叫做深度补偿解Network（DCUNet），用于在低光照条件下恢复光场图像。DCUNet采用多stage结构，模拟了解决反射光学问题的优化过程，并使用 intermediate enhancement result 来估算照明地图，然后employs 这个地图来 unfolding 过程中生成新的增强结果。此外，DCUNet还包括在每个优化阶段中的内容相关的深度补偿模块，以抑制噪声和照明地图估算错误。为了有效利用光场图像的特殊特征，这篇论文提出了一种 pseudo-explicit 特征互动模块，全面利用光场图像中的重复信息。实验结果表明，我们的 DCUNet 在真实和 simulate 数据集上比state-of-the-art 方法更高效， both qualitatively and quantitatively。此外，DCUNet 保留了增强光场图像的 essental geometric structure  much better。代码将在 https://github.com/lyuxianqiang/LFLL-DCU 上公开。）
</details></li>
</ul>
<hr>
<h2 id="TriDo-Former-A-Triple-Domain-Transformer-for-Direct-PET-Reconstruction-from-Low-Dose-Sinograms"><a href="#TriDo-Former-A-Triple-Domain-Transformer-for-Direct-PET-Reconstruction-from-Low-Dose-Sinograms" class="headerlink" title="TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms"></a>TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05365">http://arxiv.org/abs/2308.05365</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gluucose/TriDoFormer">https://github.com/gluucose/TriDoFormer</a></li>
<li>paper_authors: Jiaqi Cui, Pinxian Zeng, Xinyi Zeng, Peng Wang, Xi Wu, Jiliu Zhou, Yan Wang, Dinggang Shen</li>
<li>for: 这种论文主要目标是提高低剂量 positron emission tomography（PET）图像质量，同时尽量减少辐射暴露。</li>
<li>methods: 该论文提出了一种基于 transformer 模型的直接 PET 重建方法，称为 TriDo-Former。该模型包括两个缓冲网络：一个叫做 sinogram enhancement transformer（SE-Former）用于对 LPET 信号进行降噪，另一个叫做 spatial-spectral reconstruction transformer（SSR-Former）用于从降噪后的 LPET 信号中重建 SPET 图像。</li>
<li>results: 作者们的 TriDo-Former 方法在一个临床数据集上进行验证，证明了它在质量和量化上都高于现有的方法。<details>
<summary>Abstract</summary>
To obtain high-quality positron emission tomography (PET) images while minimizing radiation exposure, various methods have been proposed for reconstructing standard-dose PET (SPET) images from low-dose PET (LPET) sinograms directly. However, current methods often neglect boundaries during sinogram-to-image reconstruction, resulting in high-frequency distortion in the frequency domain and diminished or fuzzy edges in the reconstructed images. Furthermore, the convolutional architectures, which are commonly used, lack the ability to model long-range non-local interactions, potentially leading to inaccurate representations of global structures. To alleviate these problems, we propose a transformer-based model that unites triple domains of sinogram, image, and frequency for direct PET reconstruction, namely TriDo-Former. Specifically, the TriDo-Former consists of two cascaded networks, i.e., a sinogram enhancement transformer (SE-Former) for denoising the input LPET sinograms and a spatial-spectral reconstruction transformer (SSR-Former) for reconstructing SPET images from the denoised sinograms. Different from the vanilla transformer that splits an image into 2D patches, based specifically on the PET imaging mechanism, our SE-Former divides the sinogram into 1D projection view angles to maintain its inner-structure while denoising, preventing the noise in the sinogram from prorogating into the image domain. Moreover, to mitigate high-frequency distortion and improve reconstruction details, we integrate global frequency parsers (GFPs) into SSR-Former. The GFP serves as a learnable frequency filter that globally adjusts the frequency components in the frequency domain, enforcing the network to restore high-frequency details resembling real SPET images. Validations on a clinical dataset demonstrate that our TriDo-Former outperforms the state-of-the-art methods qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
通过直接从低剂量PET（LPET）信号图进行恢复，以提高高质量PET（SPET）图像的获得，已有多种方法提出。然而，现有方法通常忽略边界 durante el proceso de reconstrucción de imágenes, resulting in distorsión de alta frecuencia en el dominio de la frecuencia y borrosas o imágenes difusas en la reconstrucción. Además, las arquitecturas convolucionales, que se utilizan comúnmente, carecen de la capacidad de modelar interacciones de largo alcance no locales, lo que puede llevar a representaciones inexactas de estructuras globales. Para abordar estos problemas, propusimos un modelo basado en transformers que une los tres dominios de sinograma, imagen y frecuencia para la reconstrucción directa de PET, llamado TriDo-Former.Specifically, the TriDo-Former consists of two cascaded networks, i.e., a sinogram enhancement transformer (SE-Former) for denoising the input LPET sinograms and a spatial-spectral reconstruction transformer (SSR-Former) for reconstructing SPET images from the denoised sinograms. Unlike the vanilla transformer that splits an image into 2D patches based specifically on the PET imaging mechanism, our SE-Former divides the sinogram into 1D projection view angles to maintain its inner-structure while denoising, preventing the noise in the sinogram from propagating into the image domain. Moreover, to mitigate high-frequency distortion and improve reconstruction details, we integrate global frequency parsers (GFPs) into SSR-Former. The GFP serves as a learnable frequency filter that globally adjusts the frequency components in the frequency domain, enforcing the network to restore high-frequency details resembling real SPET images. Validations on a clinical dataset demonstrate that our TriDo-Former outperforms the state-of-the-art methods qualitatively and quantitatively.
</details></li>
</ul>
<hr>
<h2 id="Towards-General-and-Fast-Video-Derain-via-Knowledge-Distillation"><a href="#Towards-General-and-Fast-Video-Derain-via-Knowledge-Distillation" class="headerlink" title="Towards General and Fast Video Derain via Knowledge Distillation"></a>Towards General and Fast Video Derain via Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05346">http://arxiv.org/abs/2308.05346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Defang Cai, Pan Mu, Sixian Chan, Zhanpeng Shao, Cong Bai</li>
<li>for: 本研究旨在提出一种能够处理不同雨束类型的通用视频去雨网络（名为RRGNet），以提高视频去雨 task 的性能。</li>
<li>methods: 我们提出了一种框架分组基于的encoder-decoder网络，利用视频的时间信息，并使用老任务模型来引导当前模型学习新的雨束类型而不忘记原有的知识。</li>
<li>results: 我们的开发的通用方法在运行速度和去雨效果两个方面达到了最佳结果。<details>
<summary>Abstract</summary>
As a common natural weather condition, rain can obscure video frames and thus affect the performance of the visual system, so video derain receives a lot of attention. In natural environments, rain has a wide variety of streak types, which increases the difficulty of the rain removal task. In this paper, we propose a Rain Review-based General video derain Network via knowledge distillation (named RRGNet) that handles different rain streak types with one pre-training weight. Specifically, we design a frame grouping-based encoder-decoder network that makes full use of the temporal information of the video. Further, we use the old task model to guide the current model in learning new rain streak types while avoiding forgetting. To consolidate the network's ability to derain, we design a rain review module to play back data from old tasks for the current model. The experimental results show that our developed general method achieves the best results in terms of running speed and derain effect.
</details>
<details>
<summary>摘要</summary>
通常的自然天气情况下，雨水会掩蔽视频帧，从而影响视觉系统的性能，因此视频去雨受到了很多关注。在自然环境中，雨水有各种不同的斑斓类型，这使得去雨任务更加困难。在这篇论文中，我们提出了一种基于知识传授的通用视频去雨网络（名为RRGNet），能够处理不同的雨斑斓类型。 Specifically，我们设计了一个帧组合基于的Encoder-Decoder网络，以便充分利用视频的时间信息。另外，我们使用老任务模型来导引当前模型学习新的雨斑斓类型，而不是忘记原有知识。为了巩固网络的去雨能力，我们设计了雨评模块，以便将老任务数据播放给当前模型。实验结果显示，我们开发的通用方法在运行速度和去雨效果方面均达到了最佳效果。
</details></li>
</ul>
<hr>
<h2 id="Geometric-Learning-Based-Transformer-Network-for-Estimation-of-Segmentation-Errors"><a href="#Geometric-Learning-Based-Transformer-Network-for-Estimation-of-Segmentation-Errors" class="headerlink" title="Geometric Learning-Based Transformer Network for Estimation of Segmentation Errors"></a>Geometric Learning-Based Transformer Network for Estimation of Segmentation Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05068">http://arxiv.org/abs/2308.05068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sneha Sree C, Mohammad Al Fahim, Keerthi Ram, Mohanasankar Sivaprakasam</li>
<li>for: 提高和减少医生在图像分割中的努力，以便更快地完成图像分割任务。</li>
<li>methods: 使用图形神经网络（Graph Neural Network，GNN）和变换器（Nodeformer）来评估和分类错误的 segmentation 地图。</li>
<li>results: 与其他 GNN 相比，我们的网络实现了 ~0.042 的平均绝对错误和 79.53% 的准确率在评估和分类错误的节点级别错误。<details>
<summary>Abstract</summary>
Many segmentation networks have been proposed for 3D volumetric segmentation of tumors and organs at risk. Hospitals and clinical institutions seek to accelerate and minimize the efforts of specialists in image segmentation. Still, in case of errors generated by these networks, clinicians would have to manually edit the generated segmentation maps. Given a 3D volume and its putative segmentation map, we propose an approach to identify and measure erroneous regions in the segmentation map. Our method can estimate error at any point or node in a 3D mesh generated from a possibly erroneous volumetric segmentation map, serving as a Quality Assurance tool. We propose a graph neural network-based transformer based on the Nodeformer architecture to measure and classify the segmentation errors at any point. We have evaluated our network on a high-resolution micro-CT dataset of the human inner-ear bony labyrinth structure by simulating erroneous 3D segmentation maps. Our network incorporates a convolutional encoder to compute node-centric features from the input micro-CT data, the Nodeformer to learn the latent graph embeddings, and a Multi-Layer Perceptron (MLP) to compute and classify the node-wise errors. Our network achieves a mean absolute error of ~0.042 over other Graph Neural Networks (GNN) and an accuracy of 79.53% over other GNNs in estimating and classifying the node-wise errors, respectively. We also put forth vertex-normal prediction as a custom pretext task for pre-training the CNN encoder to improve the network's overall performance. Qualitative analysis shows the efficiency of our network in correctly classifying errors and reducing misclassifications.
</details>
<details>
<summary>摘要</summary>
多种 Segmentation 网络已经被提出用于三维 volume 的肿瘤和风险器官的分割。医院和临床机构希望通过加速和减少图像分割专家的努力来加速和改进图像分割过程。然而，在这些网络生成的分割地图中出现错误时，临床专家仍需手动修改生成的分割地图。为解决这个问题，我们提出了一种方法，可以在基于 Nodeformer 架构的图 neural network 中计算和评估分割错误的点位。我们的方法可以在基于高分辨率微型 CT 数据集的人类内耳骨征结构中进行评估和验证。我们的网络包括一个 convolutional encoder，用于从输入 micro-CT 数据中计算节点特征，以及一个 Nodeformer，用于学习 latent graph embeddings，以及一个 Multi-Layer Perceptron (MLP)，用于计算和分类节点错误。我们的网络在其他图 Neural Networks (GNN) 的比较中达到了 ~0.042 的平均绝对错误和 79.53% 的准确率，分别用于计算和分类节点错误。此外，我们还提出了预训练 CNN 编码器的预测任务，以提高网络的整体性能。Qualitative analysis 表明我们的网络可以正确地分类错误并减少错误分类。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/10/eess.IV_2023_08_10/" data-id="clltau95b00drcr884io1ckxs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/10/cs.SD_2023_08_10/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-10 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/09/cs.LG_2023_08_09/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-09 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
