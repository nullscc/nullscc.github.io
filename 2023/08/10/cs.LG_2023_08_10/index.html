
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-10 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="AST-MHSA : Code Summarization using Multi-Head Self-Attention paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.05646 repo_url: None paper_authors: Yeshwanth Nagaraj, Ujjwal Gupta for:  Code summarization methods:">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-10 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/10/cs.LG_2023_08_10/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="AST-MHSA : Code Summarization using Multi-Head Self-Attention paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.05646 repo_url: None paper_authors: Yeshwanth Nagaraj, Ujjwal Gupta for:  Code summarization methods:">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-09T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:28.113Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_10" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/10/cs.LG_2023_08_10/" class="article-date">
  <time datetime="2023-08-09T16:00:00.000Z" itemprop="datePublished">2023-08-10</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-10 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AST-MHSA-Code-Summarization-using-Multi-Head-Self-Attention"><a href="#AST-MHSA-Code-Summarization-using-Multi-Head-Self-Attention" class="headerlink" title="AST-MHSA : Code Summarization using Multi-Head Self-Attention"></a>AST-MHSA : Code Summarization using Multi-Head Self-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05646">http://arxiv.org/abs/2308.05646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeshwanth Nagaraj, Ujjwal Gupta</li>
<li>for:  Code summarization</li>
<li>methods:  Multi-head attention mechanism to extract important semantic information from AST, encoder-decoder architecture</li>
<li>results:  More comprehensive summaries of code with reduced computational overhead<details>
<summary>Abstract</summary>
Code summarization aims to generate concise natural language descriptions for source code. The prevailing approaches adopt transformer-based encoder-decoder architectures, where the Abstract Syntax Tree (AST) of the source code is utilized for encoding structural information. However, ASTs are much longer than the corresponding source code, and existing methods ignore this size constraint by directly feeding the entire linearized AST into the encoders. This simplistic approach makes it challenging to extract truly valuable dependency relations from the overlong input sequence and leads to significant computational overhead due to self-attention applied to all nodes in the AST.   To address this issue effectively and efficiently, we present a model, AST-MHSA that uses multi-head attention to extract the important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes as input the abstract syntax tree (AST) of the code and generates a sequence of hidden states. The decoder then takes these hidden states as input and generates a natural language summary of the code.   The multi-head attention mechanism allows the model to learn different representations of the input code, which can be combined to generate a more comprehensive summary. The model is trained on a dataset of code and summaries, and the parameters of the model are optimized to minimize the loss between the generated summaries and the ground-truth summaries.
</details>
<details>
<summary>摘要</summary>
code 摘要目标是生成源代码的自然语言描述。现有的方法多采用 transformer 基于 encoder-decoder 架构，其中源代码的抽象 syntax tree (AST) 用于编码结构信息。然而，AST 比源代码更长，现有方法直接将整个 linearized AST  feed into encoders，这种简单的方法使得EXTRACTING valuable dependency relations FROM THE OVERLONG INPUT SEQUENCE 困难，并且会产生巨大的计算开销由于所有节点 self-attention。To address this issue effectively and efficiently, we present a model called AST-MHSA that uses multi-head attention to extract important semantic information from the AST. The model consists of two main components: an encoder and a decoder. The encoder takes the AST of the code as input and generates a sequence of hidden states. The decoder then takes these hidden states as input and generates a natural language summary of the code.The multi-head attention mechanism allows the model to learn different representations of the input code, which can be combined to generate a more comprehensive summary. The model is trained on a dataset of code and summaries, and the parameters of the model are optimized to minimize the loss between the generated summaries and the ground-truth summaries.
</details></li>
</ul>
<hr>
<h2 id="IIHT-Medical-Report-Generation-with-Image-to-Indicator-Hierarchical-Transformer"><a href="#IIHT-Medical-Report-Generation-with-Image-to-Indicator-Hierarchical-Transformer" class="headerlink" title="IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer"></a>IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05633">http://arxiv.org/abs/2308.05633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keqiang Fan, Xiaohao Cai, Mahesan Niranjan</li>
<li>for: 这个研究旨在提出一个基于图像转换器的医疗报告生成方法，以便帮助医生更快速和更准确地生成医疗报告。</li>
<li>methods: 这个方法使用了一个图像转换器框架，包括三个模组：分类模组、指标扩展模组和生成模组。分类模组首先从医疗图像中提取图像特征，然后生成疾病相关的指标，并将这些指标转换为文本形式。生成模组则使用这些提取的特征和图像特征作为助け，以生成最终的医疗报告。</li>
<li>results: 实验结果显示，提出的方法可以实现高度的医疗报告生成精度，并且比起现有方法更具有语言流畅性和医学准确性。<details>
<summary>Abstract</summary>
Automated medical report generation has become increasingly important in medical analysis. It can produce computer-aided diagnosis descriptions and thus significantly alleviate the doctors' work. Inspired by the huge success of neural machine translation and image captioning, various deep learning methods have been proposed for medical report generation. However, due to the inherent properties of medical data, including data imbalance and the length and correlation between report sequences, the generated reports by existing methods may exhibit linguistic fluency but lack adequate clinical accuracy. In this work, we propose an image-to-indicator hierarchical transformer (IIHT) framework for medical report generation. It consists of three modules, i.e., a classifier module, an indicator expansion module and a generator module. The classifier module first extracts image features from the input medical images and produces disease-related indicators with their corresponding states. The disease-related indicators are subsequently utilised as input for the indicator expansion module, incorporating the "data-text-data" strategy. The transformer-based generator then leverages these extracted features along with image features as auxiliary information to generate final reports. Furthermore, the proposed IIHT method is feasible for radiologists to modify disease indicators in real-world scenarios and integrate the operations into the indicator expansion module for fluent and accurate medical report generation. Extensive experiments and comparisons with state-of-the-art methods under various evaluation metrics demonstrate the great performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
自动化医疗报告生成已成为医学分析中越来越重要的一环。它可以生成计算机辅助诊断描述，从而减轻医生的工作负担。靠着神经机器翻译和图像描述的成功，各种深度学习方法已经被提议用于医疗报告生成。然而，由于医疗数据的内在性，包括数据不均衡和报告序列长度和相关性，所生成的报告可能具备语言流畅性，但缺乏实际的医学准确性。在这种情况下，我们提出了一种图像指标层次转换器（IIHT）框架，用于医疗报告生成。该框架包括三个模块：分类模块、指标扩展模块和生成模块。首先，分类模块从输入医学图像中提取图像特征，并生成相关疾病的指标，以及其状态。然后，指标扩展模块使用“数据-文本-数据”策略，将指标扩展为更多的疾病特征。最后，使用转换器生成器，通过这些提取的特征和图像特征作为辅助信息，生成最终的报告。此外，我们的IIHT方法可以让医生在实际应用中修改疾病指标，并将操作集成到指标扩展模块中，以实现流畅和准确的医疗报告生成。我们的实验和与当前最佳方法的比较，在不同的评价指标下，都显示了我们的方法的优秀性。
</details></li>
</ul>
<hr>
<h2 id="ReLU-and-Addition-based-Gated-RNN"><a href="#ReLU-and-Addition-based-Gated-RNN" class="headerlink" title="ReLU and Addition-based Gated RNN"></a>ReLU and Addition-based Gated RNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05629">http://arxiv.org/abs/2308.05629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rickard Brännvall, Henrik Forsgren, Fredrik Sandin, Marcus Liwicki</li>
<li>for: 减少计算成本，提高效率，以适用于受限硬件或各种权限系统。</li>
<li>methods: 使用添加和ReLU活化函数取代乘法和对数函数，以维持长期记忆并 capture long-term dependencies。</li>
<li>results: 实验结果表明，提档的门控机制可以在 synthetic sequence 学习任务中保持长期记忆，同时减少计算成本，其执行时间比 conventinal LSTM 和 GRU 减少一半（CPU）和一半（加密）。<details>
<summary>Abstract</summary>
We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while significantly reducing computational costs such that execution time is reduced by half on CPU and by one-third under encryption. Experimental results on handwritten text recognition tasks furthermore show that the proposed architecture can be trained to achieve comparable accuracy to conventional GRU and LSTM baselines. The gating mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the multiplication of encrypted variables. It can also support quantization in (unencrypted) plaintext applications, with the potential for substantial performance gains since the addition-based formulation can avoid the expansion to double precision often required for multiplication.
</details>
<details>
<summary>摘要</summary>
我们将传统的数 multiplication 和 sigmoid 函数更换为加法和ReLU 活化。这个机制可以保持序列处理中的长期记忆，但是降低计算成本，因此可以在更高效的执行或更大的模型中进行更多的硬件限制。回传神经网络（RNN）具有阀门机制，如 LSTM 和 GRU，在序列资料上学习了很成功，因为它们可以捕捉长期依赖。在传统的更新方式中，基于目前的输入和前一个状态历史的更新是multiplied with dynamic weights，然后合并以计算下一个状态。但是，Multiplication 可以是计算昂费的，尤其是在某些硬件架构或替代运算系统，如数学加密。在这篇论文中，我们提出了一个新的阀门机制，可以在标准的人工 синтеctic sequence 学习任务中捕捉长期依赖，并且将计算成本降低了一半在 CPU 上，并且在加密下降低了一个三分之一。实验结果显示，我们的架构可以与传统 GRU 和 LSTM 基准点进行相互比较，并且在手写文本识别任务中进行训练。这个阀门机制可以实现隐私保护的 AI 应用，例如在数学加密下运行，并且可以支持量化（ plaintext 应用中），这可能将带来重大的性能提升，因为加法形式可以避免对 double precision 的扩展，通常需要 multiplication。
</details></li>
</ul>
<hr>
<h2 id="Normalized-Gradients-for-All"><a href="#Normalized-Gradients-for-All" class="headerlink" title="Normalized Gradients for All"></a>Normalized Gradients for All</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05621">http://arxiv.org/abs/2308.05621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Johnnywang1899/Credit_Risk_Analysis">https://github.com/Johnnywang1899/Credit_Risk_Analysis</a></li>
<li>paper_authors: Francesco Orabona</li>
<li>for: 这篇论文主要是用于掌握Holder平坦性的方法。</li>
<li>methods: 这篇论文使用了normalized gradients来实现Black-box方式下的Holder平坦性适应。另外， bound的计算还基于一个新的地方Holder平坦性的定义。主要思想来自Levy [2017]。</li>
<li>results: 论文得到了一个基于localHolder平坦性的bound，这个bound取决于一个新的地方Holder平坦性的定义。<details>
<summary>Abstract</summary>
In this short note, I show how to adapt to H\"{o}lder smoothness using normalized gradients in a black-box way. Moreover, the bound will depend on a novel notion of local H\"{o}lder smoothness. The main idea directly comes from Levy [2017].
</details>
<details>
<summary>摘要</summary>
这短い详细说明中，我们介绍了如何适应Holder平滑性使用 норциали化的梯度。此外， bound 的取值也取决于一种新的地方Holder平滑性。主要的想法直接来自Levy [2017]。Here's the breakdown of the translation:* "Holder smoothness" is translated as "Holder平滑性" (holder smoothness)* "normalized gradients" is translated as "norмаль化梯度" (normalized gradients)* "black-box way" is translated as "黑盒方式" (black-box way)* "novel notion of local H\"{o}lder smoothness" is translated as "一种新的地方Holder平滑性" (novel notion of local Holder smoothness)* "Levy [2017]" is translated as "Levy [2017]" (Levy [2017])
</details></li>
</ul>
<hr>
<h2 id="Updating-Clinical-Risk-Stratification-Models-Using-Rank-Based-Compatibility-Approaches-for-Evaluating-and-Optimizing-Clinician-Model-Team-Performance"><a href="#Updating-Clinical-Risk-Stratification-Models-Using-Rank-Based-Compatibility-Approaches-for-Evaluating-and-Optimizing-Clinician-Model-Team-Performance" class="headerlink" title="Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance"></a>Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05619">http://arxiv.org/abs/2308.05619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erkin Ötleş, Brian T. Denton, Jenna Wiens</li>
<li>for: 这篇论文的目的是提出一种新的rank-based兼容度指标($C^R$)和一种新的损失函数，用于优化风险分化模型的兼容性和推理性。</li>
<li>methods: 该论文使用了现有的模型选择技术和一种新的损失函数，并在MIMIC数据集上进行了实验 validate the proposed approach.</li>
<li>results: 相比 existed model selection techniques, the proposed approach yielded more compatible models while maintaining discriminative performance, with an increase in $C^R$ of $0.019$ ($95%$ confidence interval: $0.005$, $0.035$).<details>
<summary>Abstract</summary>
As data shift or new data become available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval: $0.005$, $0.035$). This work provides new tools to analyze and update risk stratification models used in clinical care.
</details>
<details>
<summary>摘要</summary>
As data shifts or new data becomes available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval: $0.005$, $0.035$). This work provides new tools to analyze and update risk stratification models used in clinical care.Here's the translation in Traditional Chinese:当数据shift或新数据可用时，对于供应链机器学习模型的更新可能是必要的，以维持或改善性能。然而，更新模型可能会导致用户预期不符的行为，导致用户模型团队性能差。现有的兼容度标准依赖于模型决策阈值，仅适用于基于估计风险的情况下。为了解决这些限制，我们提出了一个新的排名基于兼容度度量，$C^R$, 以及一个新的损失函数，旨在优化推理性能的同时鼓励好兼容。对于基于MIMIC的致死风险分组案例，我们的方法产生了更兼容的模型，保持了推理性能的同时，与现有的模型选择技术相比，$C^R$ 增加了0.019 ($95\%$ 信度interval: 0.005，0.035）。这个工作提供了新的工具来分析和更新在医疗保健中使用的风险分组模型。
</details></li>
</ul>
<hr>
<h2 id="Multi-graph-Spatio-temporal-Graph-Convolutional-Network-for-Traffic-Flow-Prediction"><a href="#Multi-graph-Spatio-temporal-Graph-Convolutional-Network-for-Traffic-Flow-Prediction" class="headerlink" title="Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction"></a>Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05601">http://arxiv.org/abs/2308.05601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilong Ding, Tianpu Zhang, Jianwu Wang, Zhuofeng Zhao</li>
<li>for: 这篇论文主要是为了提出一种用于每日高速公路交通流量预测的深度学习方法。</li>
<li>methods: 该方法使用数据归一化策略处理数据不均衡问题，然后使用图 convolutional 网络捕捉空间时间特征。此外，还使用天气和历法特征在全连接Stage中增加外部特征。</li>
<li>results: 对一个中国省级高速公路进行了详细实验和案例研究，结果显示该方法与基eline比较，有明显的预测精度提升和实质性的商业应用优势。<details>
<summary>Abstract</summary>
Inter-city highway transportation is significant for urban life. As one of the key functions in intelligent transportation system (ITS), traffic evaluation always plays significant role nowadays, and daily traffic flow prediction still faces challenges at network-wide toll stations. On the one hand, the data imbalance in practice among various locations deteriorates the performance of prediction. On the other hand, complex correlative spatio-temporal factors cannot be comprehensively employed in long-term duration. In this paper, a prediction method is proposed for daily traffic flow in highway domain through spatio-temporal deep learning. In our method, data normalization strategy is used to deal with data imbalance, due to long-tail distribution of traffic flow at network-wide toll stations. And then, based on graph convolutional network, we construct networks in distinct semantics to capture spatio-temporal features. Beside that, meteorology and calendar features are used by our model in the full connection stage to extra external characteristics of traffic flow. By extensive experiments and case studies in one Chinese provincial highway, our method shows clear improvement in predictive accuracy than baselines and practical benefits in business.
</details>
<details>
<summary>摘要</summary>
城市间高速交通是城市生活中非常重要的一环。作为智能交通系统（ITS）中一项关键功能，交通评估总是在当今得到重要的应用，而日常交通流量预测仍然面临着网络覆盖站的挑战。一方面，实际应用中的数据不均衡问题使预测性能下降。另一方面，复杂的相关空间时间因素难以长期内部涵盖。本文提出了基于高速公路域的日常交通流量预测方法，使用数据归一化策略处理数据不均衡问题，并基于图 convolutional network 构建不同 semantics 的网络，捕捉空间时间特征。此外，我们的模型还在全连接阶段使用气象和历法特征，以捕捉交通流量的外部特征。经过广泛的实验和案例研究，我们的方法在一个中国省级高速公路上显示出了明显的预测精度提高和实践效益。
</details></li>
</ul>
<hr>
<h2 id="NUPES-Non-Uniform-Post-Training-Quantization-via-Power-Exponent-Search"><a href="#NUPES-Non-Uniform-Post-Training-Quantization-via-Power-Exponent-Search" class="headerlink" title="NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search"></a>NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05600">http://arxiv.org/abs/2308.05600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edouard Yvinec, Arnaud Dapogny, Kevin Bailly<br>for: 这篇论文的目的是提出一种改进现有深度神经网络（DNN）优化技术，以提高优化后的预测性能。methods: 这篇论文使用了一种名为“自适应映射”的技术，将浮点数表示转换为低位元数字表示，以减少DNN模型的内存负载和延迟。此外，这篇论文还使用了一种名为“权函数”的技术，将DNN模型中的权值和活化函数转换为低位元数字表示。results: 这篇论文获得了顶尖的压缩率，并且可以在没有数据验证和数据验证下运行。此外，这篇论文还提出了一新的优化方法，可以在训练过程中对优化后的模型进行优化，以提高预测性能。<details>
<summary>Abstract</summary>
Deep neural network (DNN) deployment has been confined to larger hardware devices due to their expensive computational requirements. This challenge has recently reached another scale with the emergence of large language models (LLMs). In order to reduce both their memory footprint and latency, a promising technique is quantization. It consists in converting floating point representations to low bit-width fixed point representations, usually by assuming a uniform mapping onto a regular grid. This process, referred to in the literature as uniform quantization, may however be ill-suited as most DNN weights and activations follow a bell-shaped distribution. This is even worse on LLMs whose weight distributions are known to exhibit large, high impact, outlier values. In this work, we propose an improvement over the most commonly adopted way to tackle this limitation in deep learning models quantization, namely, non-uniform quantization. NUPES leverages automorphisms to preserve the scalar multiplications. Such transformations are derived from power functions. However, the optimization of the exponent parameter and weight values remains a challenging and novel problem which could not be solved with previous post training optimization techniques which only learn to round up or down weight values in order to preserve the predictive function. We circumvent this limitation with a new paradigm: learning new quantized weights over the entire quantized space. Similarly, we enable the optimization of the power exponent, i.e. the optimization of the quantization operator itself during training by alleviating all the numerical instabilities. The resulting predictive function is compatible with integer-only low-bit inference. We show the ability of the method to achieve state-of-the-art compression rates in both, data-free and data-driven configurations.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的部署因其高效计算需求而受限于更大的硬件设备。这一挑战最近又由大型自然语言模型（LLM）的出现加剧。以减少它们的内存占用量和延迟，一种有前途的技术是量化。它通过将浮点表示转换为低位数字表示，通常通过假设一个固定格式的映射来实现。这个过程被称为固定量化，但是可能不适合大多数DNN的权重和活动值，因为它们通常follows a bell-shaped distribution。这个问题更加严重，因为LLMs的权重分布知道存在大、高影响的异常值。在这种情况下，我们提出一种改进了深度学习模型量化的方法，即非均匀量化（NUPES）。NUPES利用自同构来保持整数乘法。这些转换是基于力函数的。然而，优化剩余因子和权重值的问题仍然是一个挑战，而且不可以通过以前的训练优化技术解决，这些技术只是学习将权重值略减或略加以保持预测函数的正确性。我们绕过这个限制，通过学习新的量化权重，在整数乘法下保持预测函数的正确性。同时，我们启用量化运算符的优化，即在训练中优化量化操作符的权重和剩余因子。这些优化可以减少所有数值不稳定性。我们展示了该方法可以在数据驱动和数据隐藏配置下实现国际只有low-bit执行的预测函数，并且达到了国际最佳压缩率。
</details></li>
</ul>
<hr>
<h2 id="Symmetry-Defense-Against-XGBoost-Adversarial-Perturbation-Attacks"><a href="#Symmetry-Defense-Against-XGBoost-Adversarial-Perturbation-Attacks" class="headerlink" title="Symmetry Defense Against XGBoost Adversarial Perturbation Attacks"></a>Symmetry Defense Against XGBoost Adversarial Perturbation Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05575">http://arxiv.org/abs/2308.05575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blerta Lindqvist</li>
<li>for: This paper aims to defend tree-based ensemble classifiers like gradient-boosting decision trees (GBDTs) against adversarial perturbation attacks by utilizing the lack of invariance with respect to symmetries.</li>
<li>methods: The paper uses a symmetry defense approach that has been previously used for convolutional neural networks (CNNs) and applies it to GBDTs. The defense mechanism relies on the fact that GBDTs lack invariance with respect to symmetries, and it uses this lack of invariance to revert the incorrect classification of adversarial samples.</li>
<li>results: The paper evaluates the GBDT symmetry defense for nine datasets against six perturbation attacks with a threat model that ranges from zero-knowledge to perfect-knowledge adversaries. The results show that the defense mechanism can achieve up to 100% accuracy on adversarial samples even when default and robust classifiers have 0% accuracy, and up to over 95% accuracy on adversarial samples for the GBDT classifier of the F-MNIST dataset even when default and robust classifiers have 0% accuracy.Here is the information in Simplified Chinese text:</li>
<li>for: 这 paper 的目的是为了防御基于树的集成分类器，如Gradient-Boosting Decision Trees (GBDTs)，对于攻击性扰动攻击。</li>
<li>methods: 这 paper 使用了一种基于对称的防御方法，该方法已经在卷积神经网络 (CNNs) 上使用，并将其应用到 GBDTs 上。防御机制基于 GBDTs 缺乏对称性，并利用这种缺乏对称性来 revert 攻击样本的错误分类。</li>
<li>results: 这 paper 对 nine 个数据集进行了六种攻击，并评估了 GBDT 防御机制的性能。结果表明，防御机制可以在 zero-knowledge 到 perfect-knowledge 攻击者的威胁模型下，对 adversarial 样本进行100%的正确分类，以及对 F-MNIST 数据集的 GBDT 分类器进行95%以上的正确分类。<details>
<summary>Abstract</summary>
We examine whether symmetry can be used to defend tree-based ensemble classifiers such as gradient-boosting decision trees (GBDTs) against adversarial perturbation attacks. The idea is based on a recent symmetry defense for convolutional neural network classifiers (CNNs) that utilizes CNNs' lack of invariance with respect to symmetries. CNNs lack invariance because they can classify a symmetric sample, such as a horizontally flipped image, differently from the original sample. CNNs' lack of invariance also means that CNNs can classify symmetric adversarial samples differently from the incorrect classification of adversarial samples. Using CNNs' lack of invariance, the recent CNN symmetry defense has shown that the classification of symmetric adversarial samples reverts to the correct sample classification. In order to apply the same symmetry defense to GBDTs, we examine GBDT invariance and are the first to show that GBDTs also lack invariance with respect to symmetries. We apply and evaluate the GBDT symmetry defense for nine datasets against six perturbation attacks with a threat model that ranges from zero-knowledge to perfect-knowledge adversaries. Using the feature inversion symmetry against zero-knowledge adversaries, we achieve up to 100% accuracy on adversarial samples even when default and robust classifiers have 0% accuracy. Using the feature inversion and horizontal flip symmetries against perfect-knowledge adversaries, we achieve up to over 95% accuracy on adversarial samples for the GBDT classifier of the F-MNIST dataset even when default and robust classifiers have 0% accuracy.
</details>
<details>
<summary>摘要</summary>
我们研究使用对称来防御基于树状集成分类器（GBDT）的对抗攻击。这个想法基于现有的对称防御技术，该技术利用对称隐藏层（CNN）的不变性。CNNlacks变换不变性，这意味着它可以将水平翻转的图像分类为不同的样本，而不是原始样本。此外，CNN的不变性还意味着它可以将对称攻击样本分类为错误的样本。使用CNN的不变性，这个新的对称防御技术可以使得对称攻击样本的分类恢复到正确的样本分类。为了应用该技术到GBDT中，我们首先检查GBDT的不变性，并发现GBDT也缺乏对称性。我们应用和评估了GBDT对九个数据集的对称防御技术，并对六种攻击方法进行评估，包括零知识到完美知识的攻击者。使用对称性对零知识攻击者，我们达到了100%的正确率。使用对称和水平翻转 symmetry对完美知识攻击者，我们在F-MNIST数据集上达到了95%以上的正确率。
</details></li>
</ul>
<hr>
<h2 id="AutoGluon-TimeSeries-AutoML-for-Probabilistic-Time-Series-Forecasting"><a href="#AutoGluon-TimeSeries-AutoML-for-Probabilistic-Time-Series-Forecasting" class="headerlink" title="AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting"></a>AutoGluon-TimeSeries: AutoML for Probabilistic Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05566">http://arxiv.org/abs/2308.05566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oleksandr Shchur, Caner Turkmen, Nick Erickson, Huibin Shen, Alexander Shirkov, Tony Hu, Yuyang Wang</li>
<li>for: 这篇论文主要是为了提出一个开源的AutoML库，用于机会时间序列预测。</li>
<li>methods: 这篇论文使用了AutoGluon的设计哲学， combinig了传统的统计模型、机器学习基于预测方法、和ensemble技术。</li>
<li>results: 在29个benchmark dataset上进行评估，这篇论文展示了强大的实验性表现，在点预测和量预测方面都高于了一些预测方法，并且经常超越了最佳对照方法的结合。<details>
<summary>Abstract</summary>
We introduce AutoGluon-TimeSeries - an open-source AutoML library for probabilistic time series forecasting. Focused on ease of use and robustness, AutoGluon-TimeSeries enables users to generate accurate point and quantile forecasts with just 3 lines of Python code. Built on the design philosophy of AutoGluon, AutoGluon-TimeSeries leverages ensembles of diverse forecasting models to deliver high accuracy within a short training time. AutoGluon-TimeSeries combines both conventional statistical models, machine-learning based forecasting approaches, and ensembling techniques. In our evaluation on 29 benchmark datasets, AutoGluon-TimeSeries demonstrates strong empirical performance, outperforming a range of forecasting methods in terms of both point and quantile forecast accuracy, and often even improving upon the best-in-hindsight combination of prior methods.
</details>
<details>
<summary>摘要</summary>
我们介绍AutoGluon-TimeSeries - 一个开源AutoML库 для潜在时间序列预测。我们专注在使用方便和可靠性，使用3行Python代码可以生成高精度的点和量测预测。基于AutoGluon的设计哲学，AutoGluon-TimeSeries 利用多种不同预测模型的ensemble，以提供高精度的预测，仅需训练时间短。AutoGluon-TimeSeries 结合了传统的统计学模型、机器学习基于预测方法、和 ensemble技术。在我们的29个benchmark数据集评估中，AutoGluon-TimeSeries 示出了强大的实验性表现，在点和量测预测精度方面高于许多预测方法，并经常超过最佳组合的先前方法。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Variational-Inference-for-Large-Skew-t-Copulas-with-Application-to-Intraday-Equity-Returns"><a href="#Efficient-Variational-Inference-for-Large-Skew-t-Copulas-with-Application-to-Intraday-Equity-Returns" class="headerlink" title="Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns"></a>Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05564">http://arxiv.org/abs/2308.05564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn</li>
<li>for: 这个论文旨在提出一种基于skew-t分布的高维束合模型，用于金融数据的模型化，因为这种模型允许对各对依存关系进行偏置和极端尾部依存。</li>
<li>methods: 这篇论文使用了一种基于束合变分的bayesian变分推断（VI）方法来估算高维skew-t分布。这种方法使用一种具有条件 Gaussian 发现的skew-t分布来定义一个增强后验，可以准确地估算高维束合模型。</li>
<li>results: 研究人员使用了这种新方法来估算2017年至2021年的93只美国股票的高维束合模型。结果显示，这种模型能够很好地捕捉到股票对之间的偏置和极端尾部依存，同时也能够更好地预测股票的实时返佣分布。此外，基于估算的对依存关系的股票组合策略也能够提高股票投资的性能。<details>
<summary>Abstract</summary>
Large skew-t factor copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. We show that intraday predictive densities from the skew-t copula are more accurate than from some other copula models, while portfolio selection strategies based on the estimated pairwise tail dependencies improve performance relative to the benchmark index.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大skew-t因子 copula模型是金融数据模型中吸引人的，因为它允许非对称和极端尾部依赖。我们表明，Azzalini和Capitanio（2003）中的skew-t分布下的 copula允许更高的对称依赖，than two popular alternative skew-t copulas。估计这种 copula 在高维度是挑战性的，我们提议一种快速和准确的 Bayesian variational inference（VI）方法来实现。该方法使用 conditionally Gaussian 生成表示法来定义增强 posterior，可以高度准确地 aproximate。一种快速的梯度下降算法用于解决variational优化。我们使用这种新方法来估计2017-2021年的93只美国股票的copula模型。该 copula 捕捉了股票对的非对称依赖和对比股票对的相关性的巨大多样性。我们表明，从skew-t copula 中的预测概率密度比其他 copula 模型更准确，而基于估计的对称尾部依赖而实现的股票选择策略也能够超越 referential 指数。
</details></li>
</ul>
<hr>
<h2 id="Critical-Points-An-Agile-Point-Cloud-Importance-Measure-for-Robust-Classification-Adversarial-Defense-and-Explainable-AI"><a href="#Critical-Points-An-Agile-Point-Cloud-Importance-Measure-for-Robust-Classification-Adversarial-Defense-and-Explainable-AI" class="headerlink" title="Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI"></a>Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05525">http://arxiv.org/abs/2308.05525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yossilevii100/critical_points2">https://github.com/yossilevii100/critical_points2</a></li>
<li>paper_authors: Meir Yossef Levi, Guy Gilboa</li>
<li>for: 本研究旨在提高实际应用中对异常样本的处理精度和速度，并 investigate critical points of 3D point clouds and their relationship with out-of-distribution (OOD) samples.</li>
<li>methods: 本文提出了一种基于重要性度量的方法，即使用训练分类网络仅使用不重要点进行训练，以提高模型的Robustness，并使用 норма化 entropy 来选择不重要点。</li>
<li>results: 研究结果表明，使用提出的方法可以在Robust Classification和抗击攻击任务上达到顶峰性能，并且可以快速和准确地处理异常样本。<details>
<summary>Abstract</summary>
The ability to cope accurately and fast with Out-Of-Distribution (OOD) samples is crucial in real-world safety demanding applications. In this work we first study the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points. We generalize the notion of critical points into importance measures. We show that training a classification network based only on less important points dramatically improves robustness, at a cost of minor performance loss on the clean set. We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute. We show it can be used for a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We reach SOTA results on the two latter tasks. Code is available at: https://github.com/yossilevii100/critical_points2
</details>
<details>
<summary>摘要</summary>
能够快速和准确处理 OUT-OF-DISTRIBUTION（OOD）样本的能力在实际应用中是非常重要的。在这个工作中，我们首先研究了三维点云的极点与OOD样本之间的交互。我们发现，常见的损害和异常点经常被解释为极点。我们扩展了极点的概念，得到了重要度度量。我们发现，只使用不重要的点进行训练，可以很好地提高 robustness，但是会导致清洁集上的性能下降。我们发现，Normalized entropy 非常有用于损害分析。我们建议使用 Normalized entropy 来选择不重要的点。我们的提出的重要度度量非常快速计算。我们证明它可以用于多种应用，如 Explainable AI（XAI）、异常点除法、不确定度估计、Robust Classification 和对抗攻击。我们在两个后者任务上达到了 SOTA 结果。代码可以在：https://github.com/yossilevii100/critical_points2 中找到。
</details></li>
</ul>
<hr>
<h2 id="Models-Matter-The-Impact-of-Single-Step-Retrosynthesis-on-Synthesis-Planning"><a href="#Models-Matter-The-Impact-of-Single-Step-Retrosynthesis-on-Synthesis-Planning" class="headerlink" title="Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning"></a>Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05522">http://arxiv.org/abs/2308.05522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paula Torren-Peraire, Alan Kai Hassen, Samuel Genheden, Jonas Verhoeven, Djork-Arne Clevert, Mike Preuss, Igor Tetko<br>for: 这篇论文的目的是提出一种结合单步逆synthesis预测和多步合成规划的方法，以提高合成路径的可靠性和效率。methods: 该方法首先应用多个单步逆synthesis模型，然后在多步合成规划中分析其影响。此外，该方法还使用公共和专用反应数据进行评估。results: 研究发现，单步逆synthesis模型在多步合成规划中的选择可以提高总成功率 by up to +28%，而且每个单步模型都找到了不同的合成路径，这些路径之间存在一定的不同，例如路径找到成功率、合成路径的数量和化学有效性等方面。<details>
<summary>Abstract</summary>
Retrosynthesis consists of breaking down a chemical compound recursively step-by-step into molecular precursors until a set of commercially available molecules is found with the goal to provide a synthesis route. Its two primary research directions, single-step retrosynthesis prediction, which models the chemical reaction logic, and multi-step synthesis planning, which tries to find the correct sequence of reactions, are inherently intertwined. Still, this connection is not reflected in contemporary research. In this work, we combine these two major research directions by applying multiple single-step retrosynthesis models within multi-step synthesis planning and analyzing their impact using public and proprietary reaction data. We find a disconnection between high single-step performance and potential route-finding success, suggesting that single-step models must be evaluated within synthesis planning in the future. Furthermore, we show that the commonly used single-step retrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation task does not represent model performance and scalability on larger and more diverse datasets. For multi-step synthesis planning, we show that the choice of the single-step model can improve the overall success rate of synthesis planning by up to +28% compared to the commonly used baseline model. Finally, we show that each single-step model finds unique synthesis routes, and differs in aspects such as route-finding success, the number of found synthesis routes, and chemical validity, making the combination of single-step retrosynthesis prediction and multi-step synthesis planning a crucial aspect when developing future methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>功能分解是一种分解化学物质的步骤，从分子前体开始，直到找到一组可用的化学物质，以实现化学合成。功能分解的两个主要研究方向是单步功能分解预测和多步合成规划。single-step retrosynthesis prediction models the chemical reaction logic, while multi-step synthesis planning tries to find the correct sequence of reactions. However, these two research directions are not well connected in contemporary research. In this work, we combine these two major research directions by applying multiple single-step retrosynthesis models within multi-step synthesis planning and analyzing their impact using public and proprietary reaction data.我们发现，高单步性能并不一定对应合成规划的成功。这表明，单步模型在合成规划中需要进行评估。此外，我们发现USPTO-50k单步功能分解数据集不够，因为这个评估任务不能反映模型在更大和更多样化的数据集上的性能和可扩展性。对多步合成规划，我们发现，选择合适的单步模型可以提高总成功率的synthesis planning by up to +28% compared to the commonly used baseline model。此外，我们发现每个单步模型都找到了不同的合成路径，这些路径之间存在差异，例如成功率、合成路径数量和化学有效性。因此，将单步功能分解预测和多步合成规划相结合是未来发展方法的关键。Note: The text has been translated using Google Translate, and some parts may not be exactly correct or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="On-the-Optimal-Expressive-Power-of-ReLU-DNNs-and-Its-Application-in-Approximation-with-Kolmogorov-Superposition-Theorem"><a href="#On-the-Optimal-Expressive-Power-of-ReLU-DNNs-and-Its-Application-in-Approximation-with-Kolmogorov-Superposition-Theorem" class="headerlink" title="On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem"></a>On the Optimal Expressive Power of ReLU DNNs and Its Application in Approximation with Kolmogorov Superposition Theorem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05509">http://arxiv.org/abs/2308.05509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncai He</li>
<li>for: studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation.</li>
<li>methods: constructive proof and investigation of the shattering capacity of ReLU DNNs.</li>
<li>results: achievement of an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.Here is the Chinese translation of the three key information points:</li>
<li>for: 研究具有最佳表达力的ReLU深度神经网络（DNNs）以及其在拟合中的应用。</li>
<li>methods: 使用构造性证明和扰乱容量研究ReLU DNNs。</li>
<li>results: 通过高维空间中连续函数的拟合，实现ReLU DNNs的参数计数最佳化。<details>
<summary>Abstract</summary>
This paper is devoted to studying the optimal expressive power of ReLU deep neural networks (DNNs) and its application in approximation via the Kolmogorov Superposition Theorem. We first constructively prove that any continuous piecewise linear functions on $[0,1]$, comprising $O(N^2L)$ segments, can be represented by ReLU DNNs with $L$ hidden layers and $N$ neurons per layer. Subsequently, we demonstrate that this construction is optimal regarding the parameter count of the DNNs, achieved through investigating the shattering capacity of ReLU DNNs. Moreover, by invoking the Kolmogorov Superposition Theorem, we achieve an enhanced approximation rate for ReLU DNNs of arbitrary width and depth when dealing with continuous functions in high-dimensional spaces.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文关注研究ReLU深度神经网络（DNNs）的最佳表达能力和其在高维空间中的扩展应用。我们首先构造地证明任何连续划分线性函数在[0,1]中可以通过ReLU DNNs avec $L$层和$N$个神经元来表示，其中参数计数为$O(N^2L)$。然后我们证明这个构造是最佳的，通过研究ReLU DNNs的分化能力。此外，通过kolmogorov超position定理，我们得到了ReLU DNNs的任意宽度和深度时对连续函数的高级度扩展应用。
</details></li>
</ul>
<hr>
<h2 id="Quality-Diversity-under-Sparse-Reward-and-Sparse-Interaction-Application-to-Grasping-in-Robotics"><a href="#Quality-Diversity-under-Sparse-Reward-and-Sparse-Interaction-Application-to-Grasping-in-Robotics" class="headerlink" title="Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics"></a>Quality Diversity under Sparse Reward and Sparse Interaction: Application to Grasping in Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05483">http://arxiv.org/abs/2308.05483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Johann-Huber/qd_grasp">https://github.com/Johann-Huber/qd_grasp</a></li>
<li>paper_authors: J. Huber, F. Hélénon, M. Coninx, F. Ben Amar, S. Doncieux</li>
<li>for: 本研究旨在应用Quality-Diversity（QD）算法来解决机器人抓取问题，抓取是机器人控制领域中的一个重要任务。</li>
<li>methods: 本研究使用了15种不同的方法，在10个抓取领域中进行了实验，包括2种机器人抓取设置和5种标准物品。研究还提出了一个评价框架，以便公正地对各种算法进行评价。</li>
<li>results: 研究结果表明，MAP-Elites变体在研究中所用的评价指标上表现出了明显的优势，至少在比较中超过了所有其他方法。此外，研究还发现了稀有互动可能导致假新鲜度的现象。本研究所获得的抓取trajectory的生成能力在文献中无前例。<details>
<summary>Abstract</summary>
Quality-Diversity (QD) methods are algorithms that aim to generate a set of diverse and high-performing solutions to a given problem. Originally developed for evolutionary robotics, most QD studies are conducted on a limited set of domains - mainly applied to locomotion, where the fitness and the behavior signal are dense. Grasping is a crucial task for manipulation in robotics. Despite the efforts of many research communities, this task is yet to be solved. Grasping cumulates unprecedented challenges in QD literature: it suffers from reward sparsity, behavioral sparsity, and behavior space misalignment. The present work studies how QD can address grasping. Experiments have been conducted on 15 different methods on 10 grasping domains, corresponding to 2 different robot-gripper setups and 5 standard objects. An evaluation framework that distinguishes the evaluation of an algorithm from its internal components has also been proposed for a fair comparison. The obtained results show that MAP-Elites variants that select successful solutions in priority outperform all the compared methods on the studied metrics by a large margin. We also found experimental evidence that sparse interaction can lead to deceptive novelty. To our knowledge, the ability to efficiently produce examples of grasping trajectories demonstrated in this work has no precedent in the literature.
</details>
<details>
<summary>摘要</summary>
优质多样性（QD）算法目的是生成一组多样且高性能的解决方案，原本用于进化 робототех术。大多数QD研究都是在有限的领域上进行，主要是应用于行动，其中健能和行为信号都是密集的。抓取是机器人控制中的关键任务，尚未得到解决。抓取受到QD文献中的挑战，包括奖励稀少、行为稀少和行为空间不协调。本研究探讨了QD如何解决抓取问题。我们在15种方法上进行了10个抓取领域的实验，包括2种机器人夹仓设置和5种标准物品。我们还提出了一种评价框架，以便公正地比较不同算法的表现。实验结果表明，MAP-Elites变种选择成功解决方案在所研究的指标上大幅度超越所有比较的方法。我们还发现了实验证明，稀少的互动可能导致假新鲜。在这种情况下，我们所提出的能够高效生成抓取轨迹示例的能力，在文献中无先例。
</details></li>
</ul>
<hr>
<h2 id="LLM-As-DBA"><a href="#LLM-As-DBA" class="headerlink" title="LLM As DBA"></a>LLM As DBA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05481">http://arxiv.org/abs/2308.05481</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsinghuadatabasegroup/db-gpt">https://github.com/tsinghuadatabasegroup/db-gpt</a></li>
<li>paper_authors: Xuanhe Zhou, Guoliang Li, Zhiyuan Liu<br>for:The paper is written to propose a revolutionary framework for database maintenance using large language models (LLMs).methods:The framework uses LLMs to detect database maintenance knowledge from documents and tools, and uses tree of thought reasoning for root cause analysis.results:The paper presents preliminary experimental results that show D-Bot, the proposed LLM-based database administrator, can efficiently and effectively diagnose the root causes of database issues.<details>
<summary>Abstract</summary>
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.
</details>
<details>
<summary>摘要</summary>
Database administrators (DBAs) play a crucial role in managing, maintaining, and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently, large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results show that D-Bot can efficiently and effectively diagnose the root causes, and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Machine-Learning-and-Transformer-based-Approaches-for-Deceptive-Text-Classification-A-Comparative-Analysis"><a href="#Exploring-Machine-Learning-and-Transformer-based-Approaches-for-Deceptive-Text-Classification-A-Comparative-Analysis" class="headerlink" title="Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis"></a>Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05476">http://arxiv.org/abs/2308.05476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anusuya Krishnan</li>
<li>for: 本研究旨在比较机器学习和变换器基于的方法在假信息涂抹中的效果。</li>
<li>methods: 研究使用了传统的机器学习算法以及当今最佳实践的变换器模型，如BERT、XLNET、DistilBERT和RoBERTa，来检测假信息。</li>
<li>results: 通过广泛的实验，研究对不同方法的性能指标，包括准确率、精度、回归率和F1分数，进行了比较。结果可以为研究人员和实践者提供有用的指导，帮助他们在遇到假信息时做出 Informed 决策。<details>
<summary>Abstract</summary>
Deceptive text classification is a critical task in natural language processing that aims to identify deceptive o fraudulent content. This study presents a comparative analysis of machine learning and transformer-based approaches for deceptive text classification. We investigate the effectiveness of traditional machine learning algorithms and state-of-the-art transformer models, such as BERT, XLNET, DistilBERT, and RoBERTa, in detecting deceptive text. A labeled dataset consisting of deceptive and non-deceptive texts is used for training and evaluation purposes. Through extensive experimentation, we compare the performance metrics, including accuracy, precision, recall, and F1 score, of the different approaches. The results of this study shed light on the strengths and limitations of machine learning and transformer-based methods for deceptive text classification, enabling researchers and practitioners to make informed decisions when dealing with deceptive content.
</details>
<details>
<summary>摘要</summary>
伪装文本分类是自然语言处理中一项重要任务，旨在识别伪装或诈骗性内容。本研究进行了机器学习和变换器基于方法的比较分析，以检验这些方法在检测伪装文本方面的效果。我们使用了一个标注的数据集，包括伪装和非伪装文本，进行训练和评估。通过广泛的实验，我们比较了不同方法的性能指标，包括准确率、精度、准确率和F1分数。研究结果为研究者和实践者提供了有用的指导，帮助他们在面临伪装内容时做出了 Informed decisions。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Analysis-of-Network-Robustness-Evaluation-Based-on-Convolutional-Neural-Networks-with-Spatial-Pyramid-Pooling"><a href="#Comprehensive-Analysis-of-Network-Robustness-Evaluation-Based-on-Convolutional-Neural-Networks-with-Spatial-Pyramid-Pooling" class="headerlink" title="Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling"></a>Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08012">http://arxiv.org/abs/2308.08012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjun Jiang, Tianlong Fan, Changhao Li, Chuanfu Zhang, Tao Zhang, Zong-fu Luo</li>
<li>for: 这篇论文是用来理解、优化和修复复杂网络的连接Robustness的一种新方法。</li>
<li>methods: 这篇论文使用了卷积神经网络（CNN）模型和空间彩色堆叠网络（SPP-net）来解决连接Robustness的计算复杂性问题。</li>
<li>results: 该模型在不同的网络类型、失败组件类型和失败场景下的计算时间均有较高的效率，但在一些场景下表现不尽人意料。<details>
<summary>Abstract</summary>
Connectivity robustness, a crucial aspect for understanding, optimizing, and repairing complex networks, has traditionally been evaluated through time-consuming and often impractical simulations. Fortunately, machine learning provides a new avenue for addressing this challenge. However, several key issues remain unresolved, including the performance in more general edge removal scenarios, capturing robustness through attack curves instead of directly training for robustness, scalability of predictive tasks, and transferability of predictive capabilities. In this paper, we address these challenges by designing a convolutional neural networks (CNN) model with spatial pyramid pooling networks (SPP-net), adapting existing evaluation metrics, redesigning the attack modes, introducing appropriate filtering rules, and incorporating the value of robustness as training data. The results demonstrate the thoroughness of the proposed CNN framework in addressing the challenges of high computational time across various network types, failure component types and failure scenarios. However, the performance of the proposed CNN model varies: for evaluation tasks that are consistent with the trained network type, the proposed CNN model consistently achieves accurate evaluations of both attack curves and robustness values across all removal scenarios. When the predicted network type differs from the trained network, the CNN model still demonstrates favorable performance in the scenario of random node failure, showcasing its scalability and performance transferability. Nevertheless, the performance falls short of expectations in other removal scenarios. This observed scenario-sensitivity in the evaluation of network features has been overlooked in previous studies and necessitates further attention and optimization. Lastly, we discuss important unresolved questions and further investigation.
</details>
<details>
<summary>摘要</summary>
Traditionalmente, la evaluación de la robustez de redes complejas ha requerido simulaciones tiempo-consumidoras y prácticas imposibles. ¡Felizmente, la aprendizaje automático ofrece una nueva vía para abordar este desafío! Sin embargo, varias cuestiones clave aún no se han resuelto, como el desempeño en escenarios de eliminación de nodos más generales, capturar la robustez a través de curvas de ataques en lugar de entrenar directamente por robustez, la escalabilidad de tareas predictivas y la transferencia de habilidades predictivas.En este artículo, abordamos estos desafíos mediante el diseño de una red neuronal convolucional (CNN) con redes de pooling espiral (SPP-net), adaptando métricas de evaluación existentes, rediseñando los modos de ataque, estableciendo reglas de filtrado adecuadas e incorporando el valor de la robustez como datos de entrenamiento. Los resultados demuestran la thoroughness del marco de CNN propuesto en abordar los desafíos de tiempo de cálculo alto en diversas redes y escenarios de fracaso. Sin embargo, el rendimiento de la CNN propuesta varía: en tareas de evaluación consistentes con el tipo de red entrenada, la CNN consiste en evaluaciones precisas de curvas de ataques y valores de robustez en todos los escenarios de eliminación. Cuando la red predicted difiere de la red entrenada, la CNN aún demuestra un rendimiento favorable en el escenario de fracaso aleatorio de nodos, lo que muestra su escalabilidad y transferencia de rendimiento. Sin embargo, el rendimiento no cumple con las expectativas en otros escenarios de eliminación, lo que ha sido pasado por alto en estudios anteriores y requiere más atención y optimización.Finalmente, discutimos preguntas importantes sin resolver y investigaciones adicionales.
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Algorithm-for-Nonstationary-Low-Rank-MDPs"><a href="#Provably-Efficient-Algorithm-for-Nonstationary-Low-Rank-MDPs" class="headerlink" title="Provably Efficient Algorithm for Nonstationary Low-Rank MDPs"></a>Provably Efficient Algorithm for Nonstationary Low-Rank MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05471">http://arxiv.org/abs/2308.05471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Cheng, Jing Yang, Yingbin Liang</li>
<li>for: 非站台Markov决策过程（MDPs）在实际应用中模型了许多真实世界问题，因此吸引了广泛的研究兴趣。然而， литера图中关于非站台MDPs的理论研究主要集中在表格和线性（混合）MDPs上，这些模型不能捕捉深度学习RL中的未知表示。本文是首次研究非站台RL在 episodic low-rank MDPs 上，其中转移函数和奖励函数可能随时间变化，并且low-rank模型包含未知表示。</li>
<li>methods: 我们首先提出了一种参数 dependent policy 优化算法 called PORTAL，然后改进PORTAL到其参数自由版本Ada-PORTAL，可以在不知道非站台性的情况下自动调整参数。</li>
<li>results: 我们提供了两个算法的平均动态下optimality gap 上界，显示如果非站台性不太大，然后PORTAL和Ada-PORTAL在样本复杂度为多项式幂的情况下可以实现任意小的平均动态下optimality gap。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) under changing environment models many real-world applications via nonstationary Markov Decision Processes (MDPs), and hence gains considerable interest. However, theoretical studies on nonstationary MDPs in the literature have mainly focused on tabular and linear (mixture) MDPs, which do not capture the nature of unknown representation in deep RL. In this paper, we make the first effort to investigate nonstationary RL under episodic low-rank MDPs, where both transition kernels and rewards may vary over time, and the low-rank model contains unknown representation in addition to the linear state embedding function. We first propose a parameter-dependent policy optimization algorithm called PORTAL, and further improve PORTAL to its parameter-free version of Ada-PORTAL, which is able to tune its hyper-parameters adaptively without any prior knowledge of nonstationarity. For both algorithms, we provide upper bounds on the average dynamic suboptimality gap, which show that as long as the nonstationarity is not significantly large, PORTAL and Ada-PORTAL are sample-efficient and can achieve arbitrarily small average dynamic suboptimality gap with polynomial sample complexity.
</details>
<details>
<summary>摘要</summary>
“强化学习（RL）在不同环境模型下多种实际应用，特别是非站ARY Markov Decision Processes（MDPs），因此受到了广泛关注。然而，现有的理论研究中的非站ARY MDPs主要集中在表格和线性（混合）MDPs上，这些模型不能捕捉深度RL中的未知表示。本文是第一次 investigate nonstationary RL under episodic low-rank MDPs，其中过程权重和奖励可能随时间变化，低维模型包含未知表示。我们首先提出了一种参数依赖的策略优化算法 called PORTAL，然后进一步改进了 PORTAL 为其参数自由版本 Ada-PORTAL，可以适应不同的非站ARY度。我们为这两种算法提供了平均动态落差的Upper bound，显示在非站ARY度不太大时，PORTAL 和 Ada-PORTAL 是可靠的，可以在有限样本复杂度下实现平均动态落差的任意小化。”Note: Simplified Chinese is used here, which is a standardized form of Chinese that is widely used in mainland China and other parts of the world. The translation is written in the formal style, which is appropriate for academic papers.
</details></li>
</ul>
<hr>
<h2 id="mathcal-G-2Pxy-Generative-Open-Set-Node-Classification-on-Graphs-with-Proxy-Unknowns"><a href="#mathcal-G-2Pxy-Generative-Open-Set-Node-Classification-on-Graphs-with-Proxy-Unknowns" class="headerlink" title="$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs with Proxy Unknowns"></a>$\mathcal{G}^2Pxy$: Generative Open-Set Node Classification on Graphs with Proxy Unknowns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05463">http://arxiv.org/abs/2308.05463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Zhang, Zelin Shi, Xiaolin Zhang, Xiaojun Chen, Philippe Fournier-Viger, Shirui Pan</li>
<li>for: 本研究旨在提出一种新的开放集分类方法，以便在无知类信息时进行分类。</li>
<li>methods: 该方法使用生成器来生成proxy未知节点，然后通过混合来准备未知类的分布。</li>
<li>results: 实验表明，该方法可以在开放集分类任务中具有优秀的效果，并且不受GNN架构的限制。<details>
<summary>Abstract</summary>
Node classification is the task of predicting the labels of unlabeled nodes in a graph. State-of-the-art methods based on graph neural networks achieve excellent performance when all labels are available during training. But in real-life, models are often applied on data with new classes, which can lead to massive misclassification and thus significantly degrade performance. Hence, developing open-set classification methods is crucial to determine if a given sample belongs to a known class. Existing methods for open-set node classification generally use transductive learning with part or all of the features of real unseen class nodes to help with open-set classification. In this paper, we propose a novel generative open-set node classification method, i.e. $\mathcal{G}^2Pxy$, which follows a stricter inductive learning setting where no information about unknown classes is available during training and validation. Two kinds of proxy unknown nodes, inter-class unknown proxies and external unknown proxies are generated via mixup to efficiently anticipate the distribution of novel classes. Using the generated proxies, a closed-set classifier can be transformed into an open-set one, by augmenting it with an extra proxy classifier. Under the constraints of both cross entropy loss and complement entropy loss, $\mathcal{G}^2Pxy$ achieves superior effectiveness for unknown class detection and known class classification, which is validated by experiments on benchmark graph datasets. Moreover, $\mathcal{G}^2Pxy$ does not have specific requirement on the GNN architecture and shows good generalizations.
</details>
<details>
<summary>摘要</summary>
Node 分类是指预测图中没有标签的节点的标签。现有的方法基于图神经网络可以在所有标签可用于训练时达到极优性。但在实际应用中，模型经常应用于具有新的类型的数据，可能导致大规模的误分类，从而很大地降低性能。因此，开发开放集分类方法是关键的，以确定给定的样本是否属于已知类。现有的开放集节点分类方法通常使用散度学习，使用真实未看过类节点的一部分或所有特征来帮助开放集分类。在这篇论文中，我们提出了一种新的生成型开放集节点分类方法，即 $\mathcal{G}^2Pxy$，它遵循一个更严格的散度学习设定，在训练和验证过程中不可能获得未知类的信息。通过混合来生成两种类型的代理未知节点，即间类未知代理和外部未知代理，以效率地预测新类的分布。使用生成的代理节点，一个关闭集分类器可以转换成一个开放集分类器，通过增加一个额外的代理分类器。在跨度 Entropy 损失和补偿 Entropy 损失的约束下， $\mathcal{G}^2Pxy$ 实现了对未知类探测和已知类分类的超越性，经过实验 validate 在图数据集上。此外， $\mathcal{G}^2Pxy$ 不受 GNN 架构的限制，并且具有良好的通用性。
</details></li>
</ul>
<hr>
<h2 id="A-Forecaster’s-Review-of-Judea-Pearl’s-Causality-Models-Reasoning-and-Inference-Second-Edition-2009"><a href="#A-Forecaster’s-Review-of-Judea-Pearl’s-Causality-Models-Reasoning-and-Inference-Second-Edition-2009" class="headerlink" title="A Forecaster’s Review of Judea Pearl’s Causality: Models, Reasoning and Inference, Second Edition, 2009"></a>A Forecaster’s Review of Judea Pearl’s Causality: Models, Reasoning and Inference, Second Edition, 2009</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05451">http://arxiv.org/abs/2308.05451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Li</li>
<li>for: 本文是一篇评论文章，涵盖了 Judy Pearl 的原始 causality 书籍第二版（2009）中的主要话题。</li>
<li>methods: 本文提出了一种简单易于遵循的 causal inference 策略，并在预测enario中进行了示例。</li>
<li>results: 本文讨论了在预测中 causal inference 的一些潜在利益和挑战，以及如何在不同的预测enario中 estimate causal effects。<details>
<summary>Abstract</summary>
With the big popularity and success of Judea Pearl's original causality book, this review covers the main topics updated in the second edition in 2009 and illustrates an easy-to-follow causal inference strategy in a forecast scenario. It further discusses some potential benefits and challenges for causal inference with time series forecasting when modeling the counterfactuals, estimating the uncertainty and incorporating prior knowledge to estimate causal effects in different forecasting scenarios.
</details>
<details>
<summary>摘要</summary>
根据朱德亚·珀尔的原始 causality 书的巨大受欢迎和成功，这篇评论介绍了第二版（2009年）中的主要话题，并提供了一个易于掌握的 causal inference 策略，用于预测enario。它还讨论了在模型 counterfactuals 时， causal inference 遇到的潜在利益和挑战，以及如何在不同的预测enario中 estimate  causal effects。
</details></li>
</ul>
<hr>
<h2 id="Explainable-AI-applications-in-the-Medical-Domain-a-systematic-review"><a href="#Explainable-AI-applications-in-the-Medical-Domain-a-systematic-review" class="headerlink" title="Explainable AI applications in the Medical Domain: a systematic review"></a>Explainable AI applications in the Medical Domain: a systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05411">http://arxiv.org/abs/2308.05411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicoletta Prentzas, Antonis Kakas, Constantinos S. Pattichis</li>
<li>for: 这篇论文旨在探讨医疗人工智能（AI）在医疗领域的应用，以及如何使用解释AI（XAI）解决方案来提高医疗决策支持系统的可靠性和可信worth。</li>
<li>methods: 这篇论文采用了一种系统性的文献综述方法，检索了过去几年发表的198篇有关医疗AI和XAI的研究论文，并进行了系统性的分析和总结。</li>
<li>results: 根据这篇论文的分析结果，以下是一些主要发现：（1）大多数解释AI技术是模型非依的，（2）深度学习模型在医疗AI中使用得更多，（3）解释是用于提高信任的方法，但很少有报道了医生参与的循环，（4）可视化和互动的用户界面更有用于理解解释和系统的建议。<details>
<summary>Abstract</summary>
Artificial Intelligence in Medicine has made significant progress with emerging applications in medical imaging, patient care, and other areas. While these applications have proven successful in retrospective studies, very few of them were applied in practice.The field of Medical AI faces various challenges, in terms of building user trust, complying with regulations, using data ethically.Explainable AI (XAI) aims to enable humans understand AI and trust its results. This paper presents a literature review on the recent developments of XAI solutions for medical decision support, based on a representative sample of 198 articles published in recent years. The systematic synthesis of the relevant articles resulted in several findings. (1) model-agnostic XAI techniques were mostly employed in these solutions, (2) deep learning models are utilized more than other types of machine learning models, (3) explainability was applied to promote trust, but very few works reported the physicians participation in the loop, (4) visual and interactive user interface is more useful in understanding the explanation and the recommendation of the system. More research is needed in collaboration between medical and AI experts, that could guide the development of suitable frameworks for the design, implementation, and evaluation of XAI solutions in medicine.
</details>
<details>
<summary>摘要</summary>
人工智能在医疗领域已经取得了 significiant 进步，其应用范围包括医疗影像、患者护理和其他领域。然而，这些应用在实践中并不多见。医疗领域的人工智能面临着多种挑战，包括建立用户信任、遵守法规和使用数据道德。可解释人工智能（XAI）旨在帮助人类理解人工智能和信任其结果。这篇文章提出了一种Literature Review，检查了最近几年发表的198篇文章，以获取最新的发展情况。系统性的分析这些文章所得到的结论包括以下几点：1. Model-agnostic XAI技术在这些解释中最常用。2. 深度学习模型比其他机器学习模型更常用。3. 解释的目的是促进信任，但很少有文章报道了医生参与的循环。4. 可视化和交互的用户界面更有用于理解解释和系统的建议。进一步的研究需要在医疗和人工智能专家之间合作，以开发适合医疗领域的XAI解释解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Assessment-of-Multi-view-fusion-learning-for-Crop-Classification"><a href="#A-Comparative-Assessment-of-Multi-view-fusion-learning-for-Crop-Classification" class="headerlink" title="A Comparative Assessment of Multi-view fusion learning for Crop Classification"></a>A Comparative Assessment of Multi-view fusion learning for Crop Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05407">http://arxiv.org/abs/2308.05407</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmenat/multiviewcropclassification">https://github.com/fmenat/multiviewcropclassification</a></li>
<li>paper_authors: Francisco Mena, Diego Arenas, Marlon Nuske, Andreas Dengel<br>for: 这个论文的目的是提出了多视图学习模型，以处理不同分辨率、大小和噪声等多种远程感知数据的复杂任务。methods: 这个论文使用了不同的多视图合并策略，包括输入级别合并、特征级别合并和卷积级别合并等。results: 论文表明，使用不同的多视图合并策略可以超过基于单个视图的模型和前期的合并策略。但是，不同的测试区域中，不同的方法可以取得最佳性能。<details>
<summary>Abstract</summary>
With a rapidly increasing amount and diversity of remote sensing (RS) data sources, there is a strong need for multi-view learning modeling. This is a complex task when considering the differences in resolution, magnitude, and noise of RS data. The typical approach for merging multiple RS sources has been input-level fusion, but other - more advanced - fusion strategies may outperform this traditional approach. This work assesses different fusion strategies for crop classification in the CropHarvest dataset. The fusion methods proposed in this work outperform models based on individual views and previous fusion methods. We do not find one single fusion method that consistently outperforms all other approaches. Instead, we present a comparison of multi-view fusion methods for three different datasets and show that, depending on the test region, different methods obtain the best performance. Despite this, we suggest a preliminary criterion for the selection of fusion methods.
</details>
<details>
<summary>摘要</summary>
With the rapidly increasing amount and diversity of remote sensing (RS) data sources, there is a strong need for multi-view learning modeling. This is a complex task when considering the differences in resolution, magnitude, and noise of RS data. The typical approach for merging multiple RS sources has been input-level fusion, but other - more advanced - fusion strategies may outperform this traditional approach. This work assesses different fusion strategies for crop classification in the CropHarvest dataset. The fusion methods proposed in this work outperform models based on individual views and previous fusion methods. We do not find one single fusion method that consistently outperforms all other approaches. Instead, we present a comparison of multi-view fusion methods for three different datasets and show that, depending on the test region, different methods obtain the best performance. Despite this, we suggest a preliminary criterion for the selection of fusion methods.Here's the translation in Traditional Chinese:有很多和多样化的远程感知（RS）数据源，需要多视角学习模型。这是一个复杂的任务，因为RS数据的分辨率、大小和噪声之间存在差异。传统的方法是输入级别 fusión，但是更先进的拟合策略可能会超越这种传统方法。这篇文章评估了不同的拟合策略，用于cropland classification in CropHarvest dataset。我们的方法超越了基于个体视角的模型和前一代的拟合方法。我们没有找到一个一直以来的拟合方法，可以在所有情况下表现最佳。相反，我们提供了不同数据集中的多视角拟合方法的比较，并显示，根据测试区域，不同的方法在不同的数据集中可以获得最佳性能。虽然如此，我们建议一种初步的选择标准，用于选择拟合方法。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Data-Scarcity-in-Optical-Matrix-Multiplier-Modeling-Using-Transfer-Learning"><a href="#Addressing-Data-Scarcity-in-Optical-Matrix-Multiplier-Modeling-Using-Transfer-Learning" class="headerlink" title="Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning"></a>Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11630">http://arxiv.org/abs/2308.11630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Cem, Ognjen Jovanovic, Siqi Yan, Yunhong Ding, Darko Zibar, Francesco Da Ros</li>
<li>for: 用 transferred learning 解决光学矩阵乘算器中的数据罕见性问题，即使使用少量实验数据进行模型训练。</li>
<li>methods: 采用先训练模型使用生成自 menos accurate analytical model的 sintetic数据，然后精度调整使用实验数据。</li>
<li>results: 该方法可以减少模型错误，比使用analytical模型或独立的 neural network模型在数据有限情况下。使用正则化技术和ensemble averaging，实现 &lt;1 dB的Root-Mean-Square Error在实际中实现了矩阵加 weights。<details>
<summary>Abstract</summary>
We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a photonic chip while using only 25% of the available data.
</details>
<details>
<summary>摘要</summary>
我团队在 meshes 基于光学矩阵乘数器中使用传输学习来解决数据稀缺问题，我们的方法是先使用基于analytical模型生成的synthetic数据进行预训练，然后使用实验数据进行微调。我们的调查表明，这种方法可以相比analytical模型或独立的神经网络模型在数据有限情况下获得显著的减少模型错误。通过使用常规化技术和ensemble平均，我们在 photonic chip 上实现了 < 1 dB 的平均平方误差，只使用25%的数据。
</details></li>
</ul>
<hr>
<h2 id="Product-Review-Image-Ranking-for-Fashion-E-commerce"><a href="#Product-Review-Image-Ranking-for-Fashion-E-commerce" class="headerlink" title="Product Review Image Ranking for Fashion E-commerce"></a>Product Review Image Ranking for Fashion E-commerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05390">http://arxiv.org/abs/2308.05390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangeet Jaiswal, Dhruv Patel, Sreekanth Vempati, Konduru Saiswaroop</li>
<li>for: 本研究旨在提出一种简单 yet effective的训练方法，以排名用户生成内容（UGC）中的图像。</li>
<li>methods: 我们使用Myntra（印度主要的时尚电商公司）的Studio Posts和高度参与（upvotes&#x2F;downvotes）UGC图像构成了我们的起点，并使用选择的扭曲技术将图像的质量提高到与差的图像水平。我们训练我们的网络，以便将差质图像排名在低于高质图像之前。</li>
<li>results: 我们的提议方法在两个纪录（相关系数和准确率）上超越基线模型，具有明显的优势。<details>
<summary>Abstract</summary>
In a fashion e-commerce platform where customers can't physically examine the products on their own, being able to see other customers' text and image reviews of the product is critical while making purchase decisions. Given the high reliance on these reviews, over the years we have observed customers proactively sharing their reviews. With an increase in the coverage of User Generated Content (UGC), there has been a corresponding increase in the number of customer images. It is thus imperative to display the most relevant images on top as it may influence users' online shopping choices and behavior. In this paper, we propose a simple yet effective training procedure for ranking customer images. We created a dataset consisting of Myntra (A Major Indian Fashion e-commerce company) studio posts and highly engaged (upvotes/downvotes) UGC images as our starting point and used selected distortion techniques on the images of the above dataset to bring their quality at par with those of bad UGC images. We train our network to rank bad-quality images lower than high-quality ones. Our proposed method outperforms the baseline models on two metrics, namely correlation coefficient, and accuracy, by substantial margins.
</details>
<details>
<summary>摘要</summary>
在一个无法购买者实际检查产品的电商平台上，可见其他客户的文本和图像评论对于购买决策是非常重要的。随着用户生成内容的覆盖率的增加，我们在年进行了评论的投稿。随着用户生成内容的增加，图像的数量也随之增加。因此，显示最相关的图像在前面是非常重要的，因为它们可能影响用户的在线购物选择和行为。在这篇论文中，我们提出了一种简单 yet 有效的训练方法，用于排序客户图像。我们使用 Myntra（印度主要的时尚电商公司）的Studio文章和高度参与度（投票/踢票）的用户生成内容图像作为我们的起点，并使用选择的扭曲技术来使图像的质量与坏用户生成内容图像相匹配。我们训练我们的网络，以便将坏质量图像排在低于高质量图像之前。我们的提议方法在两个纪录 coefficient和准确率两个纪录上，与基准模型相比，均有substantial的优势。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-LLMs-a-Survey-and-Guideline-for-Evaluating-Large-Language-Models’-Alignment"><a href="#Trustworthy-LLMs-a-Survey-and-Guideline-for-Evaluating-Large-Language-Models’-Alignment" class="headerlink" title="Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment"></a>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05374">http://arxiv.org/abs/2308.05374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li</li>
<li>for: 本研究的目的是为了提供关键维度的评估语言模型可靠性的报告，以便实现可靠性的语言模型在各种应用中的顺利部署。</li>
<li>methods: 本研究使用了一种全面的评估方法，包括7个主要类别和29个子类别，以评估语言模型的可靠性。</li>
<li>results: 研究发现，更加适应的模型通常在总可靠性方面表现更好，但是对不同的可靠性类别的有效性强度各不相同。这指出了需要进行更细化的分析、测试和改进，以确保语言模型的适应性和伦理性。<details>
<summary>Abstract</summary>
Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.
</details>
<details>
<summary>摘要</summary>
ensure alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.
</details></li>
</ul>
<hr>
<h2 id="Flexible-Isosurface-Extraction-for-Gradient-Based-Mesh-Optimization"><a href="#Flexible-Isosurface-Extraction-for-Gradient-Based-Mesh-Optimization" class="headerlink" title="Flexible Isosurface Extraction for Gradient-Based Mesh Optimization"></a>Flexible Isosurface Extraction for Gradient-Based Mesh Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05371">http://arxiv.org/abs/2308.05371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, Jun Gao</li>
<li>for: 本文考虑了梯度基本的网格优化，通过 représenter mesh 为一个浮点场的iso surface，以便在摄grammetry、生成模型和反向物理等应用中进行优化。</li>
<li>methods: 我们引入了flexi Cubes，一种特定的iso surface表示方法，用于优化未知的网格，以达到几何、视觉和物理目标。我们的主要思想是通过引入地方的参数，使得mesh几何和连接性可以进行当地灵活调整。这些参数通过自动微分升级来与下游任务的对应scalar场一起更新。</li>
<li>results: 我们的实验表明，flexi Cubes 可以在synthetic benchmarks和实际应用中提供显著改善的网格质量和几何准确性。<details>
<summary>Abstract</summary>
This work considers gradient-based mesh optimization, where we iteratively optimize for a 3D surface mesh by representing it as the isosurface of a scalar field, an increasingly common paradigm in applications including photogrammetry, generative modeling, and inverse physics. Existing implementations adapt classic isosurface extraction algorithms like Marching Cubes or Dual Contouring; these techniques were designed to extract meshes from fixed, known fields, and in the optimization setting they lack the degrees of freedom to represent high-quality feature-preserving meshes, or suffer from numerical instabilities. We introduce FlexiCubes, an isosurface representation specifically designed for optimizing an unknown mesh with respect to geometric, visual, or even physical objectives. Our main insight is to introduce additional carefully-chosen parameters into the representation, which allow local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task. We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.
</details>
<details>
<summary>摘要</summary>
To address this limitation, we introduce FlexiCubes, a new isosurface representation specifically designed for optimizing an unknown mesh. Our key insight is to introduce additional carefully-chosen parameters into the representation, which allow for local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task.We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-aided-Computer-Architecture-Design-for-CNN-Inferencing-Systems"><a href="#Machine-Learning-aided-Computer-Architecture-Design-for-CNN-Inferencing-Systems" class="headerlink" title="Machine Learning aided Computer Architecture Design for CNN Inferencing Systems"></a>Machine Learning aided Computer Architecture Design for CNN Inferencing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05364">http://arxiv.org/abs/2308.05364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher A. Metz</li>
<li>for: 这篇论文的目的是优化机器学习（ML）算法的效率计算，以应对智能交通、物联网（IoT）和边缘 Computing 等新兴技术的需求。</li>
<li>methods: 本论文使用了 Design Space Exploration（DSE）方法来选择最适合的加速器，并提出了一种快速和精准的预测方法来估计 CNN 的电力和性能。</li>
<li>results: 本论文的预测方法可以实现 MapE 的预测精度，对于 CNN 的推论运算可以提供快速且精准的电力和性能估计。<details>
<summary>Abstract</summary>
Efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.
</details>
<details>
<summary>摘要</summary>
efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.Here's the translation in Traditional Chinese as well:efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing the necessity for numerous prototypes. This saves time and money while also improving the time-to-market period.
</details></li>
</ul>
<hr>
<h2 id="FINER-Enhancing-State-of-the-art-Classifiers-with-Feature-Attribution-to-Facilitate-Security-Analysis"><a href="#FINER-Enhancing-State-of-the-art-Classifiers-with-Feature-Attribution-to-Facilitate-Security-Analysis" class="headerlink" title="FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis"></a>FINER: Enhancing State-of-the-art Classifiers with Feature Attribution to Facilitate Security Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05362">http://arxiv.org/abs/2308.05362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/e0hyl/finer-explain">https://github.com/e0hyl/finer-explain</a></li>
<li>paper_authors: Yiling He, Jian Lou, Zhan Qin, Kui Ren</li>
<li>for: 这篇论文的目的是提出一种高精度和高可读性的风险检测类фикатор解释框架，以便减少安全分析人员的工作负担。</li>
<li>methods: 该论文使用了特征参与（FA）方法来解释深度学习模型，并通过自适应task知识调整和集成FA方法来提高解释的智能性。</li>
<li>results: 对于风险检测任务，FINER可以提供高精度和高可读性的解释，并且在恶意软件分析中表现更高效。<details>
<summary>Abstract</summary>
Deep learning classifiers achieve state-of-the-art performance in various risk detection applications. They explore rich semantic representations and are supposed to automatically discover risk behaviors. However, due to the lack of transparency, the behavioral semantics cannot be conveyed to downstream security experts to reduce their heavy workload in security analysis. Although feature attribution (FA) methods can be used to explain deep learning, the underlying classifier is still blind to what behavior is suspicious, and the generated explanation cannot adapt to downstream tasks, incurring poor explanation fidelity and intelligibility. In this paper, we propose FINER, the first framework for risk detection classifiers to generate high-fidelity and high-intelligibility explanations. The high-level idea is to gather explanation efforts from model developer, FA designer, and security experts. To improve fidelity, we fine-tune the classifier with an explanation-guided multi-task learning strategy. To improve intelligibility, we engage task knowledge to adjust and ensemble FA methods. Extensive evaluations show that FINER improves explanation quality for risk detection. Moreover, we demonstrate that FINER outperforms a state-of-the-art tool in facilitating malware analysis.
</details>
<details>
<summary>摘要</summary>
深度学习分类器在不同的风险检测应用中实现了状态的最佳性能。它们探索了丰富的 semantic 表示，并被认为可以自动发现风险行为。然而，由于lack of transparency，risk 行为的semantic不能被传递给下游安全专家进行安全分析，从而增加了安全分析的重量。虽然 feature attribution（FA）方法可以用来解释深度学习，但下游分类器仍然无法了解哪些行为是可疑的，并且生成的解释无法适应下游任务，导致低效解释准确性和可读性。在这篇论文中，我们提出了 FINER，第一个用于风险检测分类器生成高准确性和高可读性解释的框架。高级想法是将解释努力集中于模型开发者、FA设计者和安全专家。为了提高准确性，我们使用解释指导多任务学习策略来练化分类器。为了提高可读性，我们利用任务知识来调整和组合 FA 方法。广泛评估表明，FINER 可以提高风险检测解释质量。此外，我们还证明了 FINER 可以超越当前领域的一个状态的工具在攻击分析方面帮助更好。
</details></li>
</ul>
<hr>
<h2 id="Preemptive-Detection-of-Fake-Accounts-on-Social-Networks-via-Multi-Class-Preferential-Attachment-Classifiers"><a href="#Preemptive-Detection-of-Fake-Accounts-on-Social-Networks-via-Multi-Class-Preferential-Attachment-Classifiers" class="headerlink" title="Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers"></a>Preemptive Detection of Fake Accounts on Social Networks via Multi-Class Preferential Attachment Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05353">http://arxiv.org/abs/2308.05353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Breuer, Nazanin Khosravani, Michael Tingley, Bradford Cottel<br>for:这篇论文描述了一种新的算法 called Preferential Attachment k-class Classifier (PreAttacK)，用于检测社交网络中的假账户。这些算法在过去几年中已经达到了高精度水平，但是它们通常是通过利用假账户的朋友关系或它们与其他人分享的内容来实现的。PreAttacK是这些方法的巨大变革。methods:作者们提供了一些初次分布分析，描述了新的假账户如何在社交网络中首次请求朋友关系。他们发现，even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth。作者们使用这个模型来 derive a new algorithm，PreAttacK。results:作者们证明了，在相关的问题实例中，PreAttacK可以 Near-optimally approximate the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts’ (not-yet-answered) friend requests。这是首次提供了对新用户的假账户检测的证明保证，而不需要强烈的同化假设。这种原则化的方法也使得PreAttacK成为了唯一具有证明保证的算法，在全球Facebook网络上实现了状态当前的最佳性能。<details>
<summary>Abstract</summary>
In this paper, we describe a new algorithm called Preferential Attachment k-class Classifier (PreAttacK) for detecting fake accounts in a social network. Recently, several algorithms have obtained high accuracy on this problem. However, they have done so by relying on information about fake accounts' friendships or the content they share with others--the very things we seek to prevent. PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth. We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions. This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了一种新的算法 called Preferential Attachment k-class Classifier (PreAttacK)，用于检测社交网络中的假账户。最近，一些算法已经在这个问题上取得了高准确率，但是它们通常是通过利用假账户的朋友关系或它们与其他人共享的内容来实现的—— precisamente el que we seek to prevent. PreAttacK represents a significant departure from these approaches. We provide some of the first detailed distributional analyses of how new fake (and real) accounts first attempt to request friends after joining a major network (Facebook). We show that even before a new account has made friends or shared content, these initial friend request behaviors evoke a natural multi-class extension of the canonical Preferential Attachment model of social network growth. We use this model to derive a new algorithm, PreAttacK. We prove that in relevant problem instances, PreAttacK near-optimally approximates the posterior probability that a new account is fake under this multi-class Preferential Attachment model of new accounts' (not-yet-answered) friend requests. These are the first provable guarantees for fake account detection that apply to new users, and that do not require strong homophily assumptions. This principled approach also makes PreAttacK the only algorithm with provable guarantees that obtains state-of-the-art performance on new users on the global Facebook network, where it converges to AUC=0.9 after new users send + receive a total of just 20 not-yet-answered friend requests. For comparison, state-of-the-art benchmarks do not obtain this AUC even after observing additional data on new users' first 100 friend requests. Thus, unlike mainstream algorithms, PreAttacK converges before the median new fake account has made a single friendship (accepted friend request) with a human.
</details></li>
</ul>
<hr>
<h2 id="RTLLM-An-Open-Source-Benchmark-for-Design-RTL-Generation-with-Large-Language-Model"><a href="#RTLLM-An-Open-Source-Benchmark-for-Design-RTL-Generation-with-Large-Language-Model" class="headerlink" title="RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model"></a>RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05345">http://arxiv.org/abs/2308.05345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Lu, Shang Liu, Qijun Zhang, Zhiyao Xie</li>
<li>for: 本研究旨在提出一个开源 benchmark，用于使用自然语言指令生成适用于快速硬件设计的 RTL 架构。</li>
<li>methods: 本研究使用了 GPT-3.5 进行自然语言指令生成，并提出了一种名为 “自然语言观察”的技术，可以帮助提高 GPT-3.5 的性能。</li>
<li>results: 本研究通过使用自然语言指令生成 RTL 架构，并通过三个进步目标进行评估，包括 syntax goal、functionality goal 和 design quality goal。结果显示，使用自然语言指令可以实现高质量的 RTL 架构生成。<details>
<summary>Abstract</summary>
Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisingly effective prompt engineering technique named self-planning, which proves to significantly boost the performance of GPT-3.5 in our proposed benchmark.
</details>
<details>
<summary>摘要</summary>
受大语言模型（LLM）如ChatGPT的成功启发，研究人员开始探索将LLM应用于快速硬件设计，如通过自然语言指令生成设计RTL。然而，现有的工作都是相对较少规模和自身提出的设计，具有许多缺陷，不能准确地评估不同LLM解决方案之间的比较。此外，许多前期工作只关注设计正确性，而忽略生成设计RTL的设计质量。在这种情况下，我们提出了一个开源的标准套件名为RTLLM，用于生成设计RTL通过自然语言指令。通过系统地评估自动生成的设计RTL，我们提出了三个进攻性目标，即语法目标、功能目标和设计质量目标。这个套件可以自动提供任何给定LLM解决方案的量化评估。此外，我们还提出了一种易于使用却有效的自我规划技术，名为自我规划，可以帮助GPT-3.5在我们提出的套件中表现出色。
</details></li>
</ul>
<hr>
<h2 id="OpenProteinSet-Training-data-for-structural-biology-at-scale"><a href="#OpenProteinSet-Training-data-for-structural-biology-at-scale" class="headerlink" title="OpenProteinSet: Training data for structural biology at scale"></a>OpenProteinSet: Training data for structural biology at scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05326">http://arxiv.org/abs/2308.05326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqlaboratory/openfold">https://github.com/aqlaboratory/openfold</a></li>
<li>paper_authors: Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan, Lukas Jarosch, Daniel Berenberg, Ian Fisk, Andrew M. Watkins, Stephen Ra, Richard Bonneau, Mohammed AlQuraishi</li>
<li>for: 这个论文主要是为了提供一个大规模的蛋白质多序列对 alignment（MSA）数据集，以便用于蛋白质结构设计、蛋白质功能预测等 bioinformatics 任务。</li>
<li>methods: 这个论文使用了 transformers 来直接对大量的 raw MSA 进行attend，以及对 Protein Data Bank 中的结构同源者进行关联。</li>
<li>results: 该论文引入了 OpenProteinSet，一个开源的蛋白质多序列对数据集，包括 более than 16 万个 MSA，以及与 Protein Data Bank 中的结构同源者和 AlphaFold2 蛋白质结构预测结果。<details>
<summary>Abstract</summary>
Multiple sequence alignments (MSAs) of proteins encode rich biological information and have been workhorses in bioinformatic methods for tasks like protein design and protein structure prediction for decades. Recent breakthroughs like AlphaFold2 that use transformers to attend directly over large quantities of raw MSAs have reaffirmed their importance. Generation of MSAs is highly computationally intensive, however, and no datasets comparable to those used to train AlphaFold2 have been made available to the research community, hindering progress in machine learning for proteins. To remedy this problem, we introduce OpenProteinSet, an open-source corpus of more than 16 million MSAs, associated structural homologs from the Protein Data Bank, and AlphaFold2 protein structure predictions. We have previously demonstrated the utility of OpenProteinSet by successfully retraining AlphaFold2 on it. We expect OpenProteinSet to be broadly useful as training and validation data for 1) diverse tasks focused on protein structure, function, and design and 2) large-scale multimodal machine learning research.
</details>
<details>
<summary>摘要</summary>
多个序列对Alignment（MSA）的蛋白质编码着丰富的生物信息，在生物信息学方法中作为保持者工具用于蛋白质设计和蛋白质结构预测已经有几十年的历史。最近的突破，如AlphaFold2，使用转换器直接在大量的原始MSA上进行访问，重新确认了它们的重要性。生成MSA的计算极其计算昂贵，然而，与AlphaFold2训练所用数据集相同的大规模数据集没有被研究者社区公开，这阻碍了蛋白质机器学习的进步。为了解决这个问题，我们介绍OpenProteinSet，一个开源的蛋白质多序列对Alignment资源，包括More than 16 million MSAs，与蛋白质数据库相关的结构同源者，以及AlphaFold2蛋白质结构预测。我们在OpenProteinSet上成功重新训练AlphaFold2，并预计OpenProteinSet将广泛用于蛋白质结构、功能和设计的多种任务，以及大规模多Modal机器学习研究。
</details></li>
</ul>
<hr>
<h2 id="Homophily-enhanced-Structure-Learning-for-Graph-Clustering"><a href="#Homophily-enhanced-Structure-Learning-for-Graph-Clustering" class="headerlink" title="Homophily-enhanced Structure Learning for Graph Clustering"></a>Homophily-enhanced Structure Learning for Graph Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05309">http://arxiv.org/abs/2308.05309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/galogm/hole">https://github.com/galogm/hole</a></li>
<li>paper_authors: Ming Gu, Gaoming Yang, Sheng Zhou, Ning Ma, Jiawei Chen, Qiaoyu Tan, Meihan Liu, Jiajun Bu</li>
<li>for: 这个论文的目的是提出一种基于图 neural network 的图 clustering 方法，以提高图分类的性能。</li>
<li>methods: 该方法使用两种结构学习模块：幂等相关度估计和群体相关简化，以提高 GNN 的性能。</li>
<li>results: 对七种不同类型和规模的测试数据集进行了广泛的实验，并与状态的基eline进行比较，得到了 HoLe 的超越性。<details>
<summary>Abstract</summary>
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve GNNs and clustering outcomes. To realize this objective, we develop two clustering-oriented structure learning modules, i.e., hierarchical correlation estimation and cluster-aware sparsification. The former module enables a more accurate estimation of pairwise node relationships by leveraging guidance from latent and clustering spaces, while the latter one generates a sparsified structure based on the similarity matrix and clustering assignments. Additionally, we devise a joint optimization approach alternating between training the homophily-enhanced structure learning and GNN-based clustering, thereby enforcing their reciprocal effects. Extensive experiments on seven benchmark datasets of various types and scales, across a range of clustering metrics, demonstrate the superiority of HoLe against state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
GRaph clustering是图像分析的基本任务，而最近的GRaph Neural Networks（GNN）的进步已经显示出了惊人的成果。尽管现有的GNN基于图 clustering方法已经取得了成功，但它们通常忽略图structure的质量，这导致了表现不佳。图structure学习可以改善输入图的精度，但以往的图structure学习尝试都是在监督学习设置下进行的，因此无法直接应用于我们的具体 clustering 任务。为了填补这个差距，我们提出了一种新的方法called homophily-enhanced structure learning for graph clustering（HoLe）。我们的动机来自于观察，通过轻度提高图中 Node 之间的同性度，可以显著改善 GNN 和 clustering 结果。为实现这个目标，我们开发了两种 clustering-oriented structure learning模块：层次相关度估计和集群意向简化。前者模块可以更准确地估计 Node 之间的对应关系，通过利用幽默和 clustering 空间的指导，而后者模块可以基于对应矩阵和 clustering 分配生成一个简化的结构。此外，我们提出了一种联合优化方法，通过在GNN-based clustering和homophily-enhanced structure learning之间交互训练，以便强制这两者之间的相互作用。广泛的实验表明，HoLe 在七种不同类型和规模的数据集上，以及不同的 clustering 维度上，均超过了状态艺术基eline。
</details></li>
</ul>
<hr>
<h2 id="From-CNN-to-Transformer-A-Review-of-Medical-Image-Segmentation-Models"><a href="#From-CNN-to-Transformer-A-Review-of-Medical-Image-Segmentation-Models" class="headerlink" title="From CNN to Transformer: A Review of Medical Image Segmentation Models"></a>From CNN to Transformer: A Review of Medical Image Segmentation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05305">http://arxiv.org/abs/2308.05305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjian Yao, Jiajun Bai, Wei Liao, Yuheng Chen, Mengjuan Liu, Yao Xie</li>
<li>for: 这篇论文的目的是对最近几年内的医疗影像分类模型进行评估和探讨。</li>
<li>methods: 这篇论文使用的方法包括U-Net和其变种，以及基于传播模型的TransUNet。</li>
<li>results: 论文评估了四种最具代表性的医疗影像分类模型，并量化评估它们在两个标准资料集（i.e., 肺结核X光和 ovary tumor）上的性能。<details>
<summary>Abstract</summary>
Medical image segmentation is an important step in medical image analysis, especially as a crucial prerequisite for efficient disease diagnosis and treatment. The use of deep learning for image segmentation has become a prevalent trend. The widely adopted approach currently is U-Net and its variants. Additionally, with the remarkable success of pre-trained models in natural language processing tasks, transformer-based models like TransUNet have achieved desirable performance on multiple medical image segmentation datasets. In this paper, we conduct a survey of the most representative four medical image segmentation models in recent years. We theoretically analyze the characteristics of these models and quantitatively evaluate their performance on two benchmark datasets (i.e., Tuberculosis Chest X-rays and ovarian tumors). Finally, we discuss the main challenges and future trends in medical image segmentation. Our work can assist researchers in the related field to quickly establish medical segmentation models tailored to specific regions.
</details>
<details>
<summary>摘要</summary>
医学图像分割是医学图像分析中非常重要的步骤，特别是为了高效的疾病诊断和治疗。深度学习在图像分割方面的应用已成为一种普遍的趋势。目前最广泛采用的是U-Net和其变体。此外，由于自然语言处理任务中预训练模型的出色成绩，如Transformer基于的TransUNet模型在多个医学图像分割数据集上实现了满意的表现。本文对最近几年内最具代表性的四种医学图像分割模型进行了抽查，分析了这些模型的特点，并对两个标准数据集（即肺部X射线和卵巢肿瘤）进行了量化评估。最后，我们讨论了医学图像分割中的主要挑战和未来趋势。本文可以帮助相关领域的研究人员快速设置适应特定地区的医学分割模型。
</details></li>
</ul>
<hr>
<h2 id="Byzantine-Robust-Decentralized-Stochastic-Optimization-with-Stochastic-Gradient-Noise-Independent-Learning-Error"><a href="#Byzantine-Robust-Decentralized-Stochastic-Optimization-with-Stochastic-Gradient-Noise-Independent-Learning-Error" class="headerlink" title="Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error"></a>Byzantine-Robust Decentralized Stochastic Optimization with Stochastic Gradient Noise-Independent Learning Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05292">http://arxiv.org/abs/2308.05292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Peng, Weiyu Li, Qing Ling</li>
<li>for: 这paper研究了一种分布式网络上的奥尔拜托抗衡梯度下降优化方法，该方法在每个代理都 periodic communication with its neighbors，并使用梯度下降来更新自己的本地模型。</li>
<li>methods: 该paper使用了两种抗衡方法，即Stochastic Average Gradient Algorithm (SAGA)和Loopless Stochastic Variance-Reduced Gradient (LSVRG)，以消除梯度下降噪声的负面影响。</li>
<li>results: 该paper的两种方法BRAVO-SAGA和BRAVO-LSVRG都能同时实现线性减少速度和梯度下降噪声独立的学习误差，这些学习误差是对一类基于总变量（TV）范数regularization和随机下降更新的方法所optimal。<details>
<summary>Abstract</summary>
This paper studies Byzantine-robust stochastic optimization over a decentralized network, where every agent periodically communicates with its neighbors to exchange local models, and then updates its own local model by stochastic gradient descent (SGD). The performance of such a method is affected by an unknown number of Byzantine agents, which conduct adversarially during the optimization process. To the best of our knowledge, there is no existing work that simultaneously achieves a linear convergence speed and a small learning error. We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. Motivated by this observation, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization for eliminating the negative effect of the stochastic gradient noise. The two resulting methods, BRAVO-SAGA and BRAVO-LSVRG, enjoy both linear convergence speeds and stochastic gradient noise-independent learning errors. Such learning errors are optimal for a class of methods based on total variation (TV)-norm regularization and stochastic subgradient update. We conduct extensive numerical experiments to demonstrate their effectiveness under various Byzantine attacks.
</details>
<details>
<summary>摘要</summary>
We observe that the learning error is largely dependent on the intrinsic stochastic gradient noise. To address this issue, we introduce two variance reduction methods, stochastic average gradient algorithm (SAGA) and loopless stochastic variance-reduced gradient (LSVRG), to Byzantine-robust decentralized stochastic optimization. These two methods eliminate the negative effect of stochastic gradient noise and achieve both linear convergence speeds and stochastic gradient noise-independent learning errors.The two resulting methods, BRAVO-SAGA and BRAVO-LSVRG, are optimal for a class of methods based on total variation (TV)-norm regularization and stochastic subgradient update. We conduct extensive numerical experiments to demonstrate their effectiveness under various Byzantine attacks.
</details></li>
</ul>
<hr>
<h2 id="Investigating-disaster-response-through-social-media-data-and-the-Susceptible-Infected-Recovered-SIR-model-A-case-study-of-2020-Western-U-S-wildfire-season"><a href="#Investigating-disaster-response-through-social-media-data-and-the-Susceptible-Infected-Recovered-SIR-model-A-case-study-of-2020-Western-U-S-wildfire-season" class="headerlink" title="Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season"></a>Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05281">http://arxiv.org/abs/2308.05281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihui Ma, Lingyao Li, Libby Hemphill, Gregory B. Baecher</li>
<li>for: This paper aims to provide decision-makers with a quantitative approach to measure disaster response and support their decision-making processes during a disaster.</li>
<li>methods: The authors use BERT topic modeling to cluster topics from Twitter data, and a Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter.</li>
<li>results: The results show that Twitter users mainly focused on three topics: “health impact,” “damage,” and “evacuation,” and the estimated parameters obtained from the SIR model in selected cities revealed that residents exhibited a high level of several concerns during the wildfire.<details>
<summary>Abstract</summary>
Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear relationship between topic trends and wildfire propagation patterns. The estimated parameters obtained from the SIR model in selected cities revealed that residents exhibited a high level of several concerns during the wildfire. Our study details how the SIR model and topic modeling using social media data can provide decision-makers with a quantitative approach to measure disaster response and support their decision-making processes.
</details>
<details>
<summary>摘要</summary>
Effective disaster response is crucial for affected communities. Responders and decision-makers can benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, providing valuable insights for decision-makers to understand evolving situations and optimize resource allocation. 我们使用了自然语言处理技术，具体来说是使用Transformers（BERT）主题分类来分类Twitter数据中的主题。然后，我们进行了时间空间分析，检查不同地区在2020年西部美国野火季节中主题的分布。我们发现Twitter用户主要关注了“健康影响”、“损害”和“疏散”三个主题。我们使用了感染传播理论（SIR）模型来探究Twitter上主题的Diffusion特性。结果表明，主题趋势与野火传播模式之间存在明确的关系。我们在选择的城市中获得了SIR模型的参数估计结果，显示了居民在野火期间表现出了高水平的多种担忧。我们的研究详细介绍了如何使用社交媒体数据和SIR模型来为决策者提供量化的方法，以支持他们的决策过程。
</details></li>
</ul>
<hr>
<h2 id="Cross-heterogeneity-Graph-Few-shot-Learning"><a href="#Cross-heterogeneity-Graph-Few-shot-Learning" class="headerlink" title="Cross-heterogeneity Graph Few-shot Learning"></a>Cross-heterogeneity Graph Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05275">http://arxiv.org/abs/2308.05275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Ding, Yan Wang, Guanfeng Liu</li>
<li>for:  Addressing the label sparsity issue in heterogeneous graphs (HGs) with few-shot learning.</li>
<li>methods:  Propose a novel model for Cross-heterogeneity Graph Few-shot Learning (CGFL), including extracting meta-patterns and a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs, and a score module to measure the informativeness of labeled samples and determine the transferability of each source HG.</li>
<li>results:  Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.<details>
<summary>Abstract</summary>
In recent years, heterogeneous graph few-shot learning has been proposed to address the label sparsity issue in heterogeneous graphs (HGs), which contain various types of nodes and edges. The existing methods have achieved good performance by transferring generalized knowledge extracted from rich-labeled classes in source HG(s) to few-labeled classes in a target HG. However, these methods only consider the single-heterogeneity scenario where the source and target HGs share a fixed set of node/edge types, ignoring the more general scenario of cross-heterogeneity, where each HG can have a different and non-fixed set of node/edge types. To this end, we focus on the unexplored cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns to capture heterogeneous information and propose a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose a score module to measure the informativeness of labeled samples and determine the transferability of each source HG. Finally, by integrating MHGN and the score module into a meta-learning mechanism, CGFL can effectively transfer generalized knowledge to predict new classes with few-labeled data. Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Recently, researchers have proposed heterogeneous graph few-shot learning (HGFSL) to address the label sparsity issue in heterogeneous graphs (HGs), which contain various types of nodes and edges. Existing methods have achieved good performance by transferring generalized knowledge extracted from rich-labeled classes in source HGs to few-labeled classes in a target HG. However, these methods only consider the single-heterogeneity scenario where the source and target HGs share a fixed set of node/edge types, ignoring the more general scenario of cross-heterogeneity, where each HG can have a different and non-fixed set of node/edge types. To address this issue, we focus on the unexplored cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity Graph Few-shot Learning (CGFL).In CGFL, we first extract meta-patterns to capture heterogeneous information and propose a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose a score module to measure the informativeness of labeled samples and determine the transferability of each source HG. Finally, by integrating MHGN and the score module into a meta-learning mechanism, CGFL can effectively transfer generalized knowledge to predict new classes with few-labeled data. Extensive experiments on four real-world datasets have demonstrated the superior performance of CGFL over the state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Data-driven-Intra-Autonomous-Systems-Graph-Generator"><a href="#Data-driven-Intra-Autonomous-Systems-Graph-Generator" class="headerlink" title="Data-driven Intra-Autonomous Systems Graph Generator"></a>Data-driven Intra-Autonomous Systems Graph Generator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05254">http://arxiv.org/abs/2308.05254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caio Vinicius Dadauto, Nelson Luis Saldanha da Fonseca, Ricardo da Silva Torres</li>
<li>for: 本研究旨在提出一种深度学习基于的图生成器，用于生成覆盖互联网络Autonomous System (AS)的 sintetic 图。</li>
<li>methods: 本研究使用了一种名为 Filtered Recurrent Multi-level (FRM) 算法来提取社区，并使用了 Internet Topology Data Kit (ITDK) 项目中的实际网络图形成一个大规模的实际网络图集合。</li>
<li>results: 研究表明，DGGI 生成的 sintetic 图能够准确地复制实际网络图中的性质，包括中心性、嵌入性、相互关联性和节点度。 DGGI 生成器比现有的互联网 topology 生成器更高效，在 Maximum Mean Discrepancy (MMD) 指标上提高了84.4%、95.1%、97.9% 和 94.7%。<details>
<summary>Abstract</summary>
This paper introduces a novel deep-learning based generator of synthetic graphs that represent intra-Autonomous System (AS) in the Internet, named Deep-generative graphs for the Internet (DGGI). It also presents a novel massive dataset of real intra-AS graphs extracted from the project Internet Topology Data Kit (ITDK), called Internet Graphs (IGraphs). To create IGraphs, the Filtered Recurrent Multi-level (FRM) algorithm for community extraction was developed. It is shown that DGGI creates synthetic graphs which accurately reproduce the properties of centrality, clustering, assortativity, and node degree. The DGGI generator overperforms existing Internet topology generators. On average, DGGI improves the Maximum Mean Discrepancy (MMD) metric 84.4%, 95.1%, 97.9%, and 94.7% for assortativity, betweenness, clustering, and node degree, respectively.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了一种新的深度学习基于的生成 sintetic 互联网（AS）图表示法，名为深度生成互联网图（DGGI）。它还发布了一个大量的实际 intra-AS 图据集，从项目互联网Topology数据集（ITDK）中提取出来，称为互联网图（IGraphs）。为创建 IGraphs，开发了一种Filtered Recurrent Multi-level（FRM）算法 для社区提取。研究表明，DGGI 生成的 sintetic 图具有与实际 intra-AS 图的性质相似的中心性、嵌入性、归一化性和节点度等特征。相比之下，DGGI 生成器在 existing Internet topology 生成器之上表现出优异，在 Maximum Mean Discrepancy（MMD）指标上提高了84.4%、95.1%、97.9% 和 94.7% 的平均提升。
</details></li>
</ul>
<hr>
<h2 id="AI-Enabled-Software-and-System-Architecture-Frameworks-Focusing-on-smart-Cyber-Physical-Systems-CPS"><a href="#AI-Enabled-Software-and-System-Architecture-Frameworks-Focusing-on-smart-Cyber-Physical-Systems-CPS" class="headerlink" title="AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS)"></a>AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05239">http://arxiv.org/abs/2308.05239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armin Moin, Atta Badii, Stephan Günnemann, Moharram Challenger</li>
<li>for: 本文旨在为现代应用和组织提供适应数据科学和机器学习（ML）相关担忧的建筑框架。</li>
<li>methods: 本研究使用了文献综述和问卷调查方法，收集、分析和结合了77名专家的意见来提出和验证提案的建筑框架。</li>
<li>results: 本研究提出了两个集合的价值标准，用于评估和优化ML启用的Cyber-Physical Systems（CPS）的开发和性能评价，以及用于评估和优化开发和模型生命周期管道支持工具的价值标准。<details>
<summary>Abstract</summary>
Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and the criteria for evaluation and benchmarking of the tools intended to support users through the modeling and development pipeline. In this study, we deploy multiple empirical and qualitative research methods based on literature review and survey instruments including expert interviews and an online questionnaire. We collect, analyze, and integrate the opinions of 77 experts from more than 25 organizations in over 10 countries to devise and validate the proposed framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Financial-Fraud-Detection-A-Comparative-Study-of-Quantum-Machine-Learning-Models"><a href="#Financial-Fraud-Detection-A-Comparative-Study-of-Quantum-Machine-Learning-Models" class="headerlink" title="Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models"></a>Financial Fraud Detection: A Comparative Study of Quantum Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05237">http://arxiv.org/abs/2308.05237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nouhaila Innan, Muhammad Al-Zafar Khan, Mohamed Bennai</li>
<li>for: 这个研究是为了进行金融领域的诈欺探测，使用四种量子机器学习（QML）模型进行比较性研究。</li>
<li>methods: 这个研究使用了量子支持向量分类器模型，并取得了最高性能，具有0.98的F1分数。其他模型，如量子预测器、量子神经网络（QNN）和抽样量子神经网络，也展现了潜在的应用前景。</li>
<li>results: 研究发现，量子支持向量分类器模型在诈欺和非诈欺类别上的F1分数为0.98，其他模型也取得了可靠的结果，但存在一些限制。这些结果对于未来QML发展提供了新的见解和依据，但还需要更有效的量子算法和更大和复杂的数据集。<details>
<summary>Abstract</summary>
In this research, a comparative study of four Quantum Machine Learning (QML) models was conducted for fraud detection in finance. We proved that the Quantum Support Vector Classifier model achieved the highest performance, with F1 scores of 0.98 for fraud and non-fraud classes. Other models like the Variational Quantum Classifier, Estimator Quantum Neural Network (QNN), and Sampler QNN demonstrate promising results, propelling the potential of QML classification for financial applications. While they exhibit certain limitations, the insights attained pave the way for future enhancements and optimisation strategies. However, challenges exist, including the need for more efficient Quantum algorithms and larger and more complex datasets. The article provides solutions to overcome current limitations and contributes new insights to the field of Quantum Machine Learning in fraud detection, with important implications for its future development.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们进行了四种量子机器学习（QML）模型的比较研究，用于金融领域的诈骗检测。我们证明了量子支持向量分类模型在诈骗和非诈骗类中的表现最高，具有0.98的F1分数。其他模型如变量量子分类器、估计量子神经网络（QNN）和抽样QNN也展现了良好的结果，这推动了量子机器学习分类在金融应用中的潜力。尽管它们存在一些限制，但获得的洞察能够为未来的改进和优化策略提供基础。然而，还存在一些挑战，包括需要更高效的量子算法和更大和复杂的数据集。这篇文章提供了解决当前的限制方法，并为量子机器学习在诈骗检测领域的未来发展提供新的洞察和意义。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Gated-Multi-Layer-Perceptron-for-Land-Use-and-Land-Cover-Mapping"><a href="#Spatial-Gated-Multi-Layer-Perceptron-for-Land-Use-and-Land-Cover-Mapping" class="headerlink" title="Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping"></a>Spatial Gated Multi-Layer Perceptron for Land Use and Land Cover Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05235">http://arxiv.org/abs/2308.05235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aj1365/sgumlp">https://github.com/aj1365/sgumlp</a></li>
<li>paper_authors: Ali Jamali, Swalpa Kumar Roy, Danfeng Hong, Peter M Atkinson, Pedram Ghamisi<br>for:* The paper is written for those interested in land use land cover (LULC) mapping and the application of machine learning algorithms in remote sensing.methods:* The paper uses a combination of multi-layer perceptrons (MLPs) and spatial gating units (SGUs) to develop a new learning algorithm called SGU-MLP.* The SGU-MLP algorithm leverages both the strengths of MLPs and SGUs to improve the accuracy of LULC mapping.results:* The SGU-MLP algorithm outperformed several state-of-the-art CNN and CNN-ViT-based models, including HybridSN, ResNet, iFormer, EfficientFormer, and CoAtNet, in terms of average accuracy.* The SGU-MLP algorithm consistently outperformed the benchmark models in three experiments conducted in Houston, USA, Berlin, Germany, and Augsburg, Germany.Here is the simplified Chinese translation of the three key information points:for:* 这篇论文是为了探讨遥感识别和land use land cover（LULC）映射的应用而写的。methods:* 这篇论文使用了多层感知器（MLPs）和空间闭合单元（SGUs）组合来开发一种新的学习算法——SGU-MLP。* SGU-MLP算法利用了MLPs和SGUs的优点，以提高LULC映射的准确性。results:* SGU-MLP算法在HybridSN、ResNet、iFormer、EfficientFormer和CoAtNet等状态对比中，以average accuracy为标准，与多个参考模型进行了比较。* SGU-MLP算法在三个实验中（在houston、berlin和augsburg） consistently outperform了参考模型。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) are models that are utilized extensively for the hierarchical extraction of features. Vision transformers (ViTs), through the use of a self-attention mechanism, have recently achieved superior modeling of global contextual information compared to CNNs. However, to realize their image classification strength, ViTs require substantial training datasets. Where the available training data are limited, current advanced multi-layer perceptrons (MLPs) can provide viable alternatives to both deep CNNs and ViTs. In this paper, we developed the SGU-MLP, a learning algorithm that effectively uses both MLPs and spatial gating units (SGUs) for precise land use land cover (LULC) mapping. Results illustrated the superiority of the developed SGU-MLP classification algorithm over several CNN and CNN-ViT-based models, including HybridSN, ResNet, iFormer, EfficientFormer and CoAtNet. The proposed SGU-MLP algorithm was tested through three experiments in Houston, USA, Berlin, Germany and Augsburg, Germany. The SGU-MLP classification model was found to consistently outperform the benchmark CNN and CNN-ViT-based algorithms. For example, for the Houston experiment, SGU-MLP significantly outperformed HybridSN, CoAtNet, Efficientformer, iFormer and ResNet by approximately 15%, 19%, 20%, 21%, and 25%, respectively, in terms of average accuracy. The code will be made publicly available at https://github.com/aj1365/SGUMLP
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNNs) 是模型，广泛应用于层次提取特征。 vision transformers (ViTs) 通过自我注意机制，在全局上下文信息模型方面，最近超越了 CNNs。然而，要实现图像分类强度，ViTs 需要大量的训练数据。当有限的训练数据available时，当前的高级多层感知器 (MLPs) 可以提供可靠的替代品，包括深度 CNNs 和 ViTs。在这篇论文中，我们开发了SGU-MLP 学习算法，它有效地结合 MLPs 和空间闭合单元 (SGUs)  для精确的土地用途地图。结果表明，我们开发的SGU-MLP 分类算法在多个实验中，包括 HOUSTON、BERLIN 和 AUGSBURG，都有superiority 于许多 CNN 和 CNN-ViT 基于的模型，包括 HybridSN、ResNet、iFormer、EfficientFormer 和 CoAtNet。我们提出的SGU-MLP 分类模型在 HOUSTON 实验中，与 HybridSN、CoAtNet、Efficientformer、iFormer 和 ResNet 相比，提高了15%、19%、20%、21% 和 25% 的平均准确率。我们将代码公开在 GitHub 上，详细的实验结果和代码将在https://github.com/aj1365/SGUMLP 上公布。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-the-Edge-and-Cloud-for-V2X-Based-Real-Time-Object-Detection-in-Autonomous-Driving"><a href="#Leveraging-the-Edge-and-Cloud-for-V2X-Based-Real-Time-Object-Detection-in-Autonomous-Driving" class="headerlink" title="Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving"></a>Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05234">http://arxiv.org/abs/2308.05234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faisal Hawlader, François Robinet, Raphaël Frank</li>
<li>for: 本研究旨在找到实时感知中最佳的折衶点，以提高自动驾驶汽车的安全性和可靠性。</li>
<li>methods: 本研究使用了虚拟数据集来训练对象检测模型，并研究了不同的卫星和云端计算策略的可行性。</li>
<li>results: 研究结果显示，通过使用JPEG和H.265压缩，可以在云端实现实时对象检测，并且比地面上的检测性能更高。<details>
<summary>Abstract</summary>
Environmental perception is a key element of autonomous driving because the information received from the perception module influences core driving decisions. An outstanding challenge in real-time perception for autonomous driving lies in finding the best trade-off between detection quality and latency. Major constraints on both computation and power have to be taken into account for real-time perception in autonomous vehicles. Larger object detection models tend to produce the best results, but are also slower at runtime. Since the most accurate detectors cannot run in real-time locally, we investigate the possibility of offloading computation to edge and cloud platforms, which are less resource-constrained. We create a synthetic dataset to train object detection models and evaluate different offloading strategies. Using real hardware and network simulations, we compare different trade-offs between prediction quality and end-to-end delay. Since sending raw frames over the network implies additional transmission delays, we also explore the use of JPEG and H.265 compression at varying qualities and measure their impact on prediction metrics. We show that models with adequate compression can be run in real-time on the cloud while outperforming local detection performance.
</details>
<details>
<summary>摘要</summary>
环境感知是自动驾驶中关键的元素，因为感知模块的信息会影响自动驾驶的核心决策。现实时感知对自动驾驶而言是一个重要的挑战，需要找到最佳的折衔点 между检测质量和延迟。自动驾驶车辆的实时感知受到计算和功率的重要限制。大型对象检测模型通常会生成最佳的结果，但它们在运行时也比较慢。由于最准确的检测器不能在本地实时运行，我们 investigate了将计算卸载到边缘和云平台上，这些平台具有较强的资源。我们创建了一个 sintetic 数据集来训练对象检测模型，并评估不同的卸载策略。使用真实的硬件和网络 simulate，我们比较了不同的妥协点 между预测质量和总终端延迟。由于发送Raw帧数据到网络会添加额外的传输延迟，我们还探索了使用 JPEG 和 H.265 压缩，并测量其对预测指标的影响。我们显示，使用合适的压缩可以在云上实时运行模型，并超越本地检测性能。
</details></li>
</ul>
<hr>
<h2 id="SegMatch-A-semi-supervised-learning-method-for-surgical-instrument-segmentation"><a href="#SegMatch-A-semi-supervised-learning-method-for-surgical-instrument-segmentation" class="headerlink" title="SegMatch: A semi-supervised learning method for surgical instrument segmentation"></a>SegMatch: A semi-supervised learning method for surgical instrument segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05232">http://arxiv.org/abs/2308.05232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Wei, Charlie Budd, Luis C. Garcia-Peraza-Herrera, Reuben Dorent, Miaojing Shi, Tom Vercauteren</li>
<li>for: 提高 Laparoscopic 和 Robotic 手术图像的分割精度，减少高成本的标注成本。</li>
<li>methods: 提出 SegMatch，一种基于 FixMatch 的 semi-supervised learning方法，通过弱版 augmentation 生成 pseudo-label，并在挑战性训练中使用 adversarial augmentation 来增强模型的鲁棒性和稳定性。</li>
<li>results: 在 Robust-MIS 2019 和 EndoVis 2017 数据集上进行了测试，结果表明，通过添加无标注数据进行训练，SegMatch 可以超越完全监督学习方法，并在不同的标签到无标签数据比例中具有更高的性能。<details>
<summary>Abstract</summary>
Surgical instrument segmentation is recognised as a key enabler to provide advanced surgical assistance and improve computer assisted interventions. In this work, we propose SegMatch, a semi supervised learning method to reduce the need for expensive annotation for laparoscopic and robotic surgical images. SegMatch builds on FixMatch, a widespread semi supervised classification pipeline combining consistency regularization and pseudo labelling, and adapts it for the purpose of segmentation. In our proposed SegMatch, the unlabelled images are weakly augmented and fed into the segmentation model to generate a pseudo-label to enforce the unsupervised loss against the output of the model for the adversarial augmented image on the pixels with a high confidence score. Our adaptation for segmentation tasks includes carefully considering the equivariance and invariance properties of the augmentation functions we rely on. To increase the relevance of our augmentations, we depart from using only handcrafted augmentations and introduce a trainable adversarial augmentation strategy. Our algorithm was evaluated on the MICCAI Instrument Segmentation Challenge datasets Robust-MIS 2019 and EndoVis 2017. Our results demonstrate that adding unlabelled data for training purposes allows us to surpass the performance of fully supervised approaches which are limited by the availability of training data in these challenges. SegMatch also outperforms a range of state-of-the-art semi-supervised learning semantic segmentation models in different labelled to unlabelled data ratios.
</details>
<details>
<summary>摘要</summary>
外科器械分割是认可为提供高级别外科协助和改进计算助手的关键因素。在这项工作中，我们提议SegMatch，一种半supervised学习方法，以减少外科 Laparoscopic和Robotic 图像分割需要的昂贵标注。SegMatch基于FixMatch，一种广泛使用的半supervised分类管道，并将其改编为分割任务。在我们的提议SegMatch中，无标图像被弱地扩展并feed into分割模型，以生成一个 pseudo-标签，以便对模型输出的图像像素进行强制检查。我们的改进包括仔细考虑图像分割任务中的等价性和不变性属性。为了增加我们的扩展的相关性，我们不再仅使用手工设计的扩展，而是引入一种可学习的对抗扩展策略。我们的算法在MICCAI Instrument Segmentation Challenge数据集Robust-MIS 2019和EndoVis 2017上进行了评估。我们的结果表明，在训练目标数据有限时，通过添加无标图像进行训练可以超越完全supervised方法的性能。SegMatch还比一些状态的半supervised学习semantic segmentation模型在不同的标签到无标签数据比率中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Training-neural-networks-with-end-to-end-optical-backpropagation"><a href="#Training-neural-networks-with-end-to-end-optical-backpropagation" class="headerlink" title="Training neural networks with end-to-end optical backpropagation"></a>Training neural networks with end-to-end optical backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05226">http://arxiv.org/abs/2308.05226</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Spall, Xianxin Guo, A. I. Lvovsky</li>
<li>for: 这个论文是为了实现光学 neural network 的训练和推理两个任务而写的。</li>
<li>methods: 这篇论文使用了耗尽性吸收器来实现非线性活化函数，并通过激光-探测器过程来实现训练。</li>
<li>results: 这篇论文成功实现了完全基于光学 процесс的 neural network 训练和推理，并且可以在不同的光学平台、材料和网络结构上进行适应。<details>
<summary>Abstract</summary>
Optics is an exciting route for the next generation of computing hardware for machine learning, promising several orders of magnitude enhancement in both computational speed and energy efficiency. However, to reach the full capacity of an optical neural network it is necessary that the computing not only for the inference, but also for the training be implemented optically. The primary algorithm for training a neural network is backpropagation, in which the calculation is performed in the order opposite to the information flow for inference. While straightforward in a digital computer, optical implementation of backpropagation has so far remained elusive, particularly because of the conflicting requirements for the optical element that implements the nonlinear activation function. In this work, we address this challenge for the first time with a surprisingly simple and generic scheme. Saturable absorbers are employed for the role of the activation units, and the required properties are achieved through a pump-probe process, in which the forward propagating signal acts as the pump and backward as the probe. Our approach is adaptable to various analog platforms, materials, and network structures, and it demonstrates the possibility of constructing neural networks entirely reliant on analog optical processes for both training and inference tasks.
</details>
<details>
<summary>摘要</summary>
什么是optics？optics是未来计算机硬件的新一代激光学技术，可以提供数个数个级别的计算速度和能效率提升。然而，要达到激光神经网络的完整能力，不仅需要推理部分实现光学计算，还需要训练部分也实现光学计算。主要算法用于训练神经网络是反射吗，在神经网络的信息流向中进行计算，而光学实现反射吗的问题尚未得到解决。在这种情况下，我们第一次解决这个挑战，使用可饱和吸收器来实现非线性活化函数。我们的方法可以适应不同的分析平台、材料和网络结构，并示出了完全通过光学过程实现神经网络的训练和推理任务。
</details></li>
</ul>
<hr>
<h2 id="Conceptualizing-Machine-Learning-for-Dynamic-Information-Retrieval-of-Electronic-Health-Record-Notes"><a href="#Conceptualizing-Machine-Learning-for-Dynamic-Information-Retrieval-of-Electronic-Health-Record-Notes" class="headerlink" title="Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes"></a>Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08494">http://arxiv.org/abs/2308.08494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharon Jiang, Shannon Shen, Monica Agrawal, Barbara Lam, Nicholas Kurtzman, Steven Horng, David Karger, David Sontag</li>
<li>for: 降低临床医生疲劳，提高医疗记录的效率和质量。</li>
<li>methods: 利用电子医疗纪录（EHR）审核日志进行机器学习监督，动态检索相关病历记录，提高记录过程中的信息检索效率。</li>
<li>results: 在紧急医学设置下，我们的方法可以达到0.963的准确率，预测具体的病历记录会在个人记录写作过程中被读取。我们还进行了一些临床医生的用户研究，发现我们的框架可以帮助临床医生更加快速地检索相关信息。<details>
<summary>Abstract</summary>
The large amount of time clinicians spend sifting through patient notes and documenting in electronic health records (EHRs) is a leading cause of clinician burnout. By proactively and dynamically retrieving relevant notes during the documentation process, we can reduce the effort required to find relevant patient history. In this work, we conceptualize the use of EHR audit logs for machine learning as a source of supervision of note relevance in a specific clinical context, at a particular point in time. Our evaluation focuses on the dynamic retrieval in the emergency department, a high acuity setting with unique patterns of information retrieval and note writing. We show that our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session. We additionally conduct a user study with several clinicians and find that our framework can help clinicians retrieve relevant information more efficiently. Demonstrating that our framework and methods can perform well in this demanding setting is a promising proof of concept that they will translate to other clinical settings and data modalities (e.g., labs, medications, imaging).
</details>
<details>
<summary>摘要</summary>
临床医生 spent a large amount of time搜索病人笔记和在电子医疗记录（EHR）中记录，这是临床疲劳的主要原因。通过积极和动态检索病人历史记录，我们可以减少找到相关病人历史的努力。在这项工作中，我们将EHR审核日志用于机器学习的监督，以确定笔记相关性在特定临床上下文中。我们的评估将注重在急诊室中进行动态检索，这是一个高危性的设置，具有独特的信息检索和笔记写作模式。我们的方法可以实现的AUC为0.963，预测 individu笔记写作会读取哪些笔记。此外，我们还进行了一些临床医生的用户研究，发现我们的框架可以帮助临床医生更有效地检索相关信息。这表明我们的框架和方法在这种高危性的设置下可以表现良好，这也是一个有希望的证明，它们将在其他临床设置和数据模式（例如，实验室数据、药物数据、成像数据）中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Layer-Saliency-in-Language-Transformers"><a href="#Decoding-Layer-Saliency-in-Language-Transformers" class="headerlink" title="Decoding Layer Saliency in Language Transformers"></a>Decoding Layer Saliency in Language Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05219">http://arxiv.org/abs/2308.05219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizabeth M. Hou, Gregory Castanon</li>
<li>for: 这个论文是为了解决现代自然语言处理中的文本焦点问题。</li>
<li>methods: 这个论文使用了 gradient-based 粒子方法，并提出了一种用于评估各层语义一致度的方法。</li>
<li>results: 这个论文在多个benchmark数据集上demonstrated consistent improvement over other textual saliency methods, without requiring additional training or labelled data.<details>
<summary>Abstract</summary>
In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种策略来确定语言模型中文本焦点的存在。在视觉网络中，焦点自然地局部化在网络的卷积层中，但这并不是现代使用Transformer堆栈网络处理自然语言的情况。我们采用梯度基于的焦点方法，提出了评估层次含义一致度的方法，并在多个benchmark分类 datasets上表现出了consistent的改进。我们的方法不需要额外的训练或标注数据，并且计算效率较高。
</details></li>
</ul>
<hr>
<h2 id="Conformer-based-Target-Speaker-Automatic-Speech-Recognition-for-Single-Channel-Audio"><a href="#Conformer-based-Target-Speaker-Automatic-Speech-Recognition-for-Single-Channel-Audio" class="headerlink" title="Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio"></a>Conformer-based Target-Speaker Automatic Speech Recognition for Single-Channel Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05218">http://arxiv.org/abs/2308.05218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/NeMo">https://github.com/NVIDIA/NeMo</a></li>
<li>paper_authors: Yang Zhang, Krishna C. Puvvada, Vitaly Lavrukhin, Boris Ginsburg</li>
<li>for: 这篇论文旨在提出一个非自适应的终端到缩时频域架构，用于单通道目标话者自动语音识别（TS-ASR）。</li>
<li>methods: 该模型包括基于TitaNet的话者嵌入模组、Conformer基于隐藏读取和ASR模组。这些模组组合地优化以识别目标话者的语音，而忽略其他话者的语音。</li>
<li>results: 在训练时使用Connectionist Temporal Classification（CTC）损失和对频域spectrogram进行标准化的数据重建损失，以鼓励模型更好地分离目标话者的spectrogram。在WSJ0-2mix-extr（4.2%）上获得了目标话者字元误差（TS-WER）的最新纪录。此外，我们首次在WSJ0-3mix-extr（12.4%）、LibriSpeech2Mix（4.2%）和LibriSpeech3Mix（7.6%） dataset上获得了TS-WER的纪录，创造了新的benchmarks для TS-ASR。<details>
<summary>Abstract</summary>
We propose CONF-TSASR, a non-autoregressive end-to-end time-frequency domain architecture for single-channel target-speaker automatic speech recognition (TS-ASR). The model consists of a TitaNet based speaker embedding module, a Conformer based masking as well as ASR modules. These modules are jointly optimized to transcribe a target-speaker, while ignoring speech from other speakers. For training we use Connectionist Temporal Classification (CTC) loss and introduce a scale-invariant spectrogram reconstruction loss to encourage the model better separate the target-speaker's spectrogram from mixture. We obtain state-of-the-art target-speaker word error rate (TS-WER) on WSJ0-2mix-extr (4.2%). Further, we report for the first time TS-WER on WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%) datasets, establishing new benchmarks for TS-ASR. The proposed model will be open-sourced through NVIDIA NeMo toolkit.
</details>
<details>
<summary>摘要</summary>
For training, we use Connectionist Temporal Classification (CTC) loss and introduce a scale-invariant spectrogram reconstruction loss to encourage the model to better separate the target-speaker's spectrogram from the mixture. 我们使用Connectionist Temporal Classification（CTC）损失进行训练，并引入一个缩放不变的spectrogram重建损失，以促进模型更好地分离目标说话人的spectrogram和混合。We obtain state-of-the-art target-speaker word error rate (TS-WER) on WSJ0-2mix-extr (4.2%). 我们在WSJ0-2mix-extr上获得了单频道目标说话人单词错误率（TS-WER）的状态地表现，得到4.2%。Further, we report for the first time TS-WER on WSJ0-3mix-extr (12.4%), LibriSpeech2Mix (4.2%) and LibriSpeech3Mix (7.6%) datasets, establishing new benchmarks for TS-ASR. 此外，我们在WSJ0-3mix-extr（12.4%）、LibriSpeech2Mix（4.2%）和LibriSpeech3Mix（7.6%） datasets上首次报告TS-WER，创造了新的TS-ASR标准。The proposed model will be open-sourced through NVIDIA NeMo toolkit. 我们将提出的模型通过NVIDIA NeMo工具包开源。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Pedestrian-Trajectory-Prediction-Methods-for-the-Application-in-Autonomous-Driving"><a href="#Evaluating-Pedestrian-Trajectory-Prediction-Methods-for-the-Application-in-Autonomous-Driving" class="headerlink" title="Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving"></a>Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05194">http://arxiv.org/abs/2308.05194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Uhlemann, Felix Fent, Markus Lienkamp</li>
<li>for: 评估预测行人轨迹领域的现状，以及常速模型（CVM）在自动驾驶车辆中的适用性。</li>
<li>methods: 使用ETH&#x2F;UCY数据集进行评估，并对初始提出的模型进行修改以适应实际应用需求。进行减噪研究，检验观察到的运动历史对预测性能的影响。</li>
<li>results: 显示简单的模型在生成单轨道时仍然竞争力强，一些通常被认为是有用的特征对于不同的架构都具有少量影响。根据这些发现，提出了预测轨迹算法的建议。<details>
<summary>Abstract</summary>
In this paper, the state of the art in the field of pedestrian trajectory prediction is evaluated alongside the constant velocity model (CVM) with respect to its applicability in autonomous vehicles. The evaluation is conducted on the widely-used ETH/UCY dataset where the Average Displacement Error (ADE) and the Final Displacement Error (FDE) are reported. To align with requirements in real-world applications, modifications are made to the input features of the initially proposed models. An ablation study is conducted to examine the influence of the observed motion history on the prediction performance, thereby establishing a better understanding of its impact. Additionally, the inference time of each model is measured to evaluate the scalability of each model when confronted with varying amounts of agents. The results demonstrate that simple models remain competitive when generating single trajectories, and certain features commonly thought of as useful have little impact on the overall performance across different architectures. Based on these findings, recommendations are proposed to guide the future development of trajectory prediction algorithms.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们评估了步行者轨迹预测领域的现状，同时与常速模型（CVM）进行比较，以确定其在自动驾驶汽车中的可行性。我们在广泛使用的 ETH/UCY 数据集上进行评估，并计算了平均偏移误差（ADE）和最终偏移误差（FDE）。为适应实际应用中的需求，我们对初始提出的模型的输入特征进行修改。我们还进行了减少研究，以确定观察到的运动历史对预测性能的影响，从而更好地理解其影响。此外，我们测量了每个模型的推断时间，以评估它们在不同数量的代理人面临的可扩展性。结果表明，简单的模型在生成单个轨迹时仍然竞争力强，而一些通常被认为是有用的特征在不同的架构下具有微不足的影响。基于这些发现，我们提出了指导未来轨迹预测算法发展的建议。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Representations-for-Spatio-Temporal-Visual-Attention-Modeling-and-Understanding"><a href="#Hierarchical-Representations-for-Spatio-Temporal-Visual-Attention-Modeling-and-Understanding" class="headerlink" title="Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding"></a>Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05189">http://arxiv.org/abs/2308.05189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel-Ángel Fernández-Torres</li>
<li>for: 本PhD论文研究了视觉注意力模型和理解视频序列中的层次表示。</li>
<li>methods: 提出了两种计算模型，一是生成概率模型，二是深度网络架构，用于模型视觉注意力。</li>
<li>results: 提出了一种基于上下文的视觉注意力模型和理解方法，并实现了视频序列中的视觉注意力模型。<details>
<summary>Abstract</summary>
This PhD. Thesis concerns the study and development of hierarchical representations for spatio-temporal visual attention modeling and understanding in video sequences. More specifically, we propose two computational models for visual attention. First, we present a generative probabilistic model for context-aware visual attention modeling and understanding. Secondly, we develop a deep network architecture for visual attention modeling, which first estimates top-down spatio-temporal visual attention, and ultimately serves for modeling attention in the temporal domain.
</details>
<details>
<summary>摘要</summary>
这个博士论文关注了视觉注意力模型化和理解在视频序列中的层次表示和时间域注意力模型。更具体来说，我们提出了两种计算模型 для视觉注意力。首先，我们提出了一种基于概率理论的生成模型，用于 Context-aware 视觉注意力模型和理解。其次，我们开发了一种深度网络架构，用于模型视觉注意力，首先估计上下文视觉注意力，并最终用于模型时间域注意力。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Morphological-Identification-of-Extended-Radio-Galaxies-using-Weak-Labels"><a href="#Deep-Learning-for-Morphological-Identification-of-Extended-Radio-Galaxies-using-Weak-Labels" class="headerlink" title="Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels"></a>Deep Learning for Morphological Identification of Extended Radio Galaxies using Weak Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05166">http://arxiv.org/abs/2308.05166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikhel1/gal-cam">https://github.com/nikhel1/gal-cam</a></li>
<li>paper_authors: Nikhel Gupta, Zeeshan Hayder, Ray P. Norris, Minh Huynh, Lars Petersson, X. Rosalind Wang, Heinz Andernach, Bärbel S. Koribalski, Miranda Yew, Evan J. Crawford</li>
<li>for: 这项研究旨在开发一种基于深度学习算法，可以减少复杂Radio галактике多组件的标注成本。</li>
<li>methods: 该算法使用弱类标签的Radio галактике来获取类活动地图（CAM），然后使用间隔Pixel网络（IRNet）来更新CAM，以获得Radio галактике的实例分割mask和红外主 galaxy的位置。</li>
<li>results: 我们使用澳大利亚 Square Kilometre Array Pathfinder（ASKAP）望远镜的数据，具体是EMU Pilot Survey，覆盖了天空面积270平方度，具有RMS敏感度25-35微赫&#x2F;天线。我们表明，使用弱类标签的深度学习算法可以高精度预测像素级信息，包括Radio галактике的扩展辐射覆盖所有 галактиComponent的面积和红外主 galaxy的位置。我们使用mAP作为评价指标，并显示模型在多个类中的mAP$_{50}$为67.5%和76.8%。模型架构可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/Nikhel1/Gal-CAM">https://github.com/Nikhel1/Gal-CAM</a><details>
<summary>Abstract</summary>
The present work discusses the use of a weakly-supervised deep learning algorithm that reduces the cost of labelling pixel-level masks for complex radio galaxies with multiple components. The algorithm is trained on weak class-level labels of radio galaxies to get class activation maps (CAMs). The CAMs are further refined using an inter-pixel relations network (IRNet) to get instance segmentation masks over radio galaxies and the positions of their infrared hosts. We use data from the Australian Square Kilometre Array Pathfinder (ASKAP) telescope, specifically the Evolutionary Map of the Universe (EMU) Pilot Survey, which covered a sky area of 270 square degrees with an RMS sensitivity of 25-35 $\mu$Jy/beam. We demonstrate that weakly-supervised deep learning algorithms can achieve high accuracy in predicting pixel-level information, including masks for the extended radio emission encapsulating all galaxy components and the positions of the infrared host galaxies. We evaluate the performance of our method using mean Average Precision (mAP) across multiple classes at a standard intersection over union (IoU) threshold of 0.5. We show that the model achieves a mAP$_{50}$ of 67.5\% and 76.8\% for radio masks and infrared host positions, respectively. The network architecture can be found at the following link: https://github.com/Nikhel1/Gal-CAM
</details>
<details>
<summary>摘要</summary>
现在的研究探讨了使用弱类标注深度学习算法来降低复杂radio galaxy的多 компонент pixel-level掩码的成本。这种算法在 radio galaxy 的弱类标签上进行训练，以获得类活化图（CAMs）。然后，使用间 pixel 关系网络（IRNet）来进一步修改 CAMs，以获得 radio galaxy 的实例分割mask和激发掩码。我们使用澳大利亚 Square Kilometre Array Pathfinder（ASKAP） telescope的数据，特别是Evolutionary Map of the Universe（EMU） Pilot Survey，覆盖了天空面积270平方度，具有RMS敏感度25-35微伏/槽。我们表明，弱类标注深度学习算法可以高精度预测像素级信息，包括涵盖所有 галакси组件的广泛电磁辐射掩码以及激发掩码。我们使用mean Average Precision（mAP）作为评价指标，并在多个类上进行标准的交叠 UNION（IoU）阈值0.5的评价。我们发现模型在 radio 掩码和激发位置上的mAP$_{50}$分别为67.5%和76.8%。网络架构可以在以下链接中找到：https://github.com/Nikhel1/Gal-CAM。
</details></li>
</ul>
<hr>
<h2 id="Improved-Multi-Shot-Diffusion-Weighted-MRI-with-Zero-Shot-Self-Supervised-Learning-Reconstruction"><a href="#Improved-Multi-Shot-Diffusion-Weighted-MRI-with-Zero-Shot-Self-Supervised-Learning-Reconstruction" class="headerlink" title="Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction"></a>Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05103">http://arxiv.org/abs/2308.05103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaejin-cho/miccai2023">https://github.com/jaejin-cho/miccai2023</a></li>
<li>paper_authors: Jaejin Cho, Yohan Jun, Xiaoqing Wang, Caique Kobayashi, Berkin Bilgic</li>
<li>for:  This paper aims to improve the reconstruction of multi-shot echo-planar imaging (msEPI) data for diffusion MRI, addressing limitations due to magnetic field inhomogeneity and T2&#x2F;T2* relaxation effects.</li>
<li>methods: The proposed approach, called zero-MIRID, uses deep learning-based image regularization techniques, including CNN denoisers in both k- and image-spaces, and virtual coils to enhance image reconstruction conditioning.</li>
<li>results: Compared to the state-of-the-art parallel imaging method, the proposed approach achieves superior results in reconstructing msEPI data, as demonstrated in an in-vivo experiment.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高多shotecho-planar imaging（msEPI）数据重建，以解决由磁场不均和T2&#x2F;T2*相互作用引起的限制。</li>
<li>methods: 提议的方法是使用深度学习基于图像规范化技术，包括k-和图像空间的CNN滤波器，以及虚拟天线来加强图像重建条件。</li>
<li>results: 相比领域内的并行成像方法，提议的方法在重建msEPI数据方面得到了更好的结果，实验中得到了良好的成果。<details>
<summary>Abstract</summary>
Diffusion MRI is commonly performed using echo-planar imaging (EPI) due to its rapid acquisition time. However, the resolution of diffusion-weighted images is often limited by magnetic field inhomogeneity-related artifacts and blurring induced by T2- and T2*-relaxation effects. To address these limitations, multi-shot EPI (msEPI) combined with parallel imaging techniques is frequently employed. Nevertheless, reconstructing msEPI can be challenging due to phase variation between multiple shots. In this study, we introduce a novel msEPI reconstruction approach called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). This method jointly reconstructs msEPI data by incorporating deep learning-based image regularization techniques. The network incorporates CNN denoisers in both k- and image-spaces, while leveraging virtual coils to enhance image reconstruction conditioning. By employing a self-supervised learning technique and dividing sampled data into three groups, the proposed approach achieves superior results compared to the state-of-the-art parallel imaging method, as demonstrated in an in-vivo experiment.
</details>
<details>
<summary>摘要</summary>
Diffusion MRI通常使用声波平均图像（EPI）进行取样，但是各种磁场不均的artefacts和T2-和T2*-征relaxation效应导致分子扩散图像的分辨率受到限制。为了解决这些限制，常用多shot EPI（msEPI）和并行技术。然而，重建msEPI可以困难，因为多个shot之间的阶段差异。在本研究中，我们介绍了一种新的msEPI重建方法，即zero-MIRID（零次自我超vised学习Multi-shot图像重建优化Diffusion MRI）。这种方法将msEPI数据重建并结合深度学习 Image Regularization技术。网络包含CNN杂谱denoiser在k-和图像空间中，同时利用虚拟磁场来增强图像重建条件。通过自我超vised学习技术和将样本数据分为三组，我们的方法在对照磁共振方法的实验中显示出了更好的效果。
</details></li>
</ul>
<hr>
<h2 id="DOST-–-Domain-Obedient-Self-supervised-Training-for-Multi-Label-Classification-with-Noisy-Labels"><a href="#DOST-–-Domain-Obedient-Self-supervised-Training-for-Multi-Label-Classification-with-Noisy-Labels" class="headerlink" title="DOST – Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels"></a>DOST – Domain Obedient Self-supervised Training for Multi Label Classification with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05101">http://arxiv.org/abs/2308.05101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumadeep Saha, Utpal Garain, Arijit Ukil, Arpan Pal, Sundeep Khandelwal</li>
<li>for: 这篇论文主要关注的是多标签分类（Multi-label Classification，MLC）任务中的标签噪声问题。</li>
<li>methods: 本文提出了一个叫做“领域套用自动训练”（Domain Obedient Self-supervised Training，DOST）的概念，它不仅使得深度学习模型更加遵循领域规则，而且可以提高学习效果和减少标签噪声的影响。</li>
<li>results: 实验结果显示，DOST方法在两个大规模的多标签分类数据集上均能够获得改善，并且可以全面抵消噪声的影响。<details>
<summary>Abstract</summary>
The enormous demand for annotated data brought forth by deep learning techniques has been accompanied by the problem of annotation noise. Although this issue has been widely discussed in machine learning literature, it has been relatively unexplored in the context of "multi-label classification" (MLC) tasks which feature more complicated kinds of noise. Additionally, when the domain in question has certain logical constraints, noisy annotations often exacerbate their violations, making such a system unacceptable to an expert. This paper studies the effect of label noise on domain rule violation incidents in the MLC task, and incorporates domain rules into our learning algorithm to mitigate the effect of noise. We propose the Domain Obedient Self-supervised Training (DOST) paradigm which not only makes deep learning models more aligned to domain rules, but also improves learning performance in key metrics and minimizes the effect of annotation noise. This novel approach uses domain guidance to detect offending annotations and deter rule-violating predictions in a self-supervised manner, thus making it more "data efficient" and domain compliant. Empirical studies, performed over two large scale multi-label classification datasets, demonstrate that our method results in improvement across the board, and often entirely counteracts the effect of noise.
</details>
<details>
<summary>摘要</summary>
“深度学习技术的巨大需求已导致笔记噪声问题的出现，而这个问题在多标签分类（MLC）任务中的噪声问题尚未得到广泛研究。此外，当领域具有特定的逻辑约束时，噪声笔记常会加剧逻辑约束的违反，使得这种系统不可接受于专家。本文研究MLC任务中标签噪声对领域规则违反事件的影响，并将领域规则 incorporated 到我们的学习算法中以mitigate噪声的影响。我们提出的领域遵循自我超vised Training（DOST）方法不仅使得深度学习模型更遵循领域规则，还提高了学习性能在重要指标上，并减少了笔记噪声的影响。这种新的方法通过领域指导检测噪声笔记并防止违反规则的预测，因此更“数据效率”和遵循领域规则。empirical studies 在两个大规模多标签分类数据集上进行，结果表明，我们的方法在全面上提高了性能，常常完全抵消噪声的影响。”
</details></li>
</ul>
<hr>
<h2 id="A-degree-of-image-identification-at-sub-human-scales-could-be-possible-with-more-advanced-clusters"><a href="#A-degree-of-image-identification-at-sub-human-scales-could-be-possible-with-more-advanced-clusters" class="headerlink" title="A degree of image identification at sub-human scales could be possible with more advanced clusters"></a>A degree of image identification at sub-human scales could be possible with more advanced clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05092">http://arxiv.org/abs/2308.05092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prateekjannu/imagescale2">https://github.com/prateekjannu/imagescale2</a></li>
<li>paper_authors: Prateek Y J</li>
<li>for: 本研究目的是判断当前可用的自动学习技术是否可以使人类水平的视觉图像理解，使用同样的感知输入量和度量。</li>
<li>methods: 本研究使用了涉及数据量缩放和图像质量缩放的自动学习方法，而无需外部资金支持。</li>
<li>results: 我们发现，同时缩放数据量和图像分辨率可以 дости得人类水平的物品检测性能，而无需超过人类大小。我们使用视transformer在200000张图像和256 ppi进行训练。<details>
<summary>Abstract</summary>
The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The purpose of the research is to determine if currently available self-supervised learning techniques can accomplish human level comprehension of visual images using the same degree and amount of sensory input that people acquire from. Initial research on this topic solely considered data volume scaling. Here, we scale both the volume of data and the quality of the image. This scaling experiment is a self-supervised learning method that may be done without any outside financing. We find that scaling up data volume and picture resolution at the same time enables human-level item detection performance at sub-human sizes.We run a scaling experiment with vision transformers trained on up to 200000 images up to 256 ppi." into Simplified Chinese.<<SYS>>当前的研究目标是判断当前可用的自我超vised learning技术是否可以通过同样的感知输入来达到人类水平的视觉理解。初步研究仅考虑数据量的扩大。在这里，我们同时扩大数据量和图像质量。这种扩大实验可以无需外部资金进行。我们发现同时扩大数据量和图像分辨率时，可以在子人类大小下达到人类水平的物体检测性能。我们使用视Transformers进行训练，并将数据量增加至200000张，图像分辨率达256ppi。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Inverse-Transition-Learning-for-Offline-Settings"><a href="#Bayesian-Inverse-Transition-Learning-for-Offline-Settings" class="headerlink" title="Bayesian Inverse Transition Learning for Offline Settings"></a>Bayesian Inverse Transition Learning for Offline Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05075">http://arxiv.org/abs/2308.05075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leo Benac, Sonali Parbhoo, Finale Doshi-Velez</li>
<li>for:  sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data.</li>
<li>methods:  a new constraint-based approach that captures desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients.</li>
<li>results:  learned a high-performing policy, while considerably reducing the policy’s variance over different datasets. Also, the paper demonstrates how combining uncertainty estimation with these constraints can help infer a partial ranking of actions that produce higher returns, and helps infer safer and more informative policies for planning.Here is the text in Simplified Chinese:</li>
<li>for: Sequential Decision-Making领域，如医疗和教育等，where reward known且transition dynamics $T$必须通过批处数据 estimating。</li>
<li>methods: 新的约束基本方法，可以准确地学习 posterior distribution of transition dynamics $T$，免于gradients的影响。</li>
<li>results: 学习出高性能策略，同时减少策略对不同数据集的差异。此外，文章还解释了如何通过uncertainty estimation和约束结合，推断出更高返回的动作partial ranking，以及更安全和更有信息的策略规划。<details>
<summary>Abstract</summary>
Offline Reinforcement learning is commonly used for sequential decision-making in domains such as healthcare and education, where the rewards are known and the transition dynamics $T$ must be estimated on the basis of batch data. A key challenge for all tasks is how to learn a reliable estimate of the transition dynamics $T$ that produce near-optimal policies that are safe enough so that they never take actions that are far away from the best action with respect to their value functions and informative enough so that they communicate the uncertainties they have. Using data from an expert, we propose a new constraint-based approach that captures our desiderata for reliably learning a posterior distribution of the transition dynamics $T$ that is free from gradients. Our results demonstrate that by using our constraints, we learn a high-performing policy, while considerably reducing the policy's variance over different datasets. We also explain how combining uncertainty estimation with these constraints can help us infer a partial ranking of actions that produce higher returns, and helps us infer safer and more informative policies for planning.
</details>
<details>
<summary>摘要</summary>
偏向学习是在医疗和教育等领域常用的sequential decision-making中广泛使用的。在这些领域中，报酬是知道的，并且需要根据批处数据来估算转移动力学T。任务的挑战之一是如何学习一个可靠地估算转移动力学T，以生成近似优质策略，并保证这些策略是安全的，即从价值函数中最佳动作的距离很远。使用专家数据，我们提出了一种新的约束基本方法，可以准确地学习 posterior分布中的转移动力学T，而不需要梯度。我们的结果表明，通过使用我们的约束，我们可以学习一个高性能的策略，同时减少策略的变量在不同数据集中。我们还解释了如何将不确定性估计与这些约束结合使用，以帮助我们推断更高的返回率的行为，并生成更安全和更有信息的策略。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-and-Attention-based-Method-for-Gaze-Estimation-Using-Electroencephalography"><a href="#An-Interpretable-and-Attention-based-Method-for-Gaze-Estimation-Using-Electroencephalography" class="headerlink" title="An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography"></a>An Interpretable and Attention-based Method for Gaze Estimation Using Electroencephalography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05768">http://arxiv.org/abs/2308.05768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nina Weng, Martyna Plomecka, Manuel Kaufmann, Ard Kastrati, Roger Wattenhofer, Nicolas Langer</li>
<li>for: 这paper是为了提出一种可解释的深度学习模型，用于基于EEG数据的视线估计。</li>
<li>methods: 该paper使用了一种新的注意力基于的深度学习框架，以便在EEG信号中提取最重要的信息，并且抑制问题的通道。</li>
<li>results: 该paper的研究表明，该模型在准确性和稳定性方面比现有方法更好，并且提供了可视化的结果，以便更好地理解EEG数据分析的结果。<details>
<summary>Abstract</summary>
Eye movements can reveal valuable insights into various aspects of human mental processes, physical well-being, and actions. Recently, several datasets have been made available that simultaneously record EEG activity and eye movements. This has triggered the development of various methods to predict gaze direction based on brain activity. However, most of these methods lack interpretability, which limits their technology acceptance. In this paper, we leverage a large data set of simultaneously measured Electroencephalography (EEG) and Eye tracking, proposing an interpretable model for gaze estimation from EEG data. More specifically, we present a novel attention-based deep learning framework for EEG signal analysis, which allows the network to focus on the most relevant information in the signal and discard problematic channels. Additionally, we provide a comprehensive evaluation of the presented framework, demonstrating its superiority over current methods in terms of accuracy and robustness. Finally, the study presents visualizations that explain the results of the analysis and highlights the potential of attention mechanism for improving the efficiency and effectiveness of EEG data analysis in a variety of applications.
</details>
<details>
<summary>摘要</summary>
眼动可以揭示人类心理过程、物理健康和行为方面的重要信息。最近，一些数据集被发布了，这些数据集同时记录了EEG活动和眼动。这些数据集的出现推动了基于EEG活动预测眼动方向的方法的开发。然而，大多数这些方法缺乏可解性，这限制了技术的接受度。在这篇论文中，我们利用了大量同时测量EEG和眼动的数据集，提出了一种可解的EEG信号分析模型，以便从EEG数据中预测眼动方向。更 Specifically，我们提出了一种基于注意力的深度学习框架，使得网络能够从EEG信号中提取最重要的信息，并且抛弃问题的通道。此外，我们进行了完整的评估，证明我们的方法在准确性和稳定性方面超过现有方法。最后，我们提供了可视化结果，解释分析结果并高亮了注意力机制的潜在改进EEG数据分析的效率和效果的潜在。
</details></li>
</ul>
<hr>
<h2 id="EEG-based-Emotion-Style-Transfer-Network-for-Cross-dataset-Emotion-Recognition"><a href="#EEG-based-Emotion-Style-Transfer-Network-for-Cross-dataset-Emotion-Recognition" class="headerlink" title="EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition"></a>EEG-based Emotion Style Transfer Network for Cross-dataset Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05767">http://arxiv.org/abs/2308.05767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijin Zhou, Fu Li, Yang Li, Youshuo Ji, Lijian Zhang, Yuanfang Chen, Wenming Zheng, Guangming Shi<br>for: 这篇研究旨在解决跨 dataset EEG 情感识别 tasks 中的 style mismatch 问题，以提高 EEG 情感识别的准确性。methods: 本研究提出了 EEG-based Emotion Style Transfer Network (E2STN)，用于从 source domain 和 target domain 中获取 EEG 表现，并将它们转换为新的类别化 EEG 表现，以具有 source domain 的内容信息和 target domain 的类别特征。results: 实验结果显示，E2STN 可以在跨 dataset EEG 情感识别任务中实现 state-of-the-art 的性能。<details>
<summary>Abstract</summary>
As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.
</details>
<details>
<summary>摘要</summary>
As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.Here's the translation in Traditional Chinese as well:As the key to realizing aBCIs, EEG emotion recognition has been widely studied by many researchers. Previous methods have performed well for intra-subject EEG emotion recognition. However, the style mismatch between source domain (training data) and target domain (test data) EEG samples caused by huge inter-domain differences is still a critical problem for EEG emotion recognition. To solve the problem of cross-dataset EEG emotion recognition, in this paper, we propose an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations. The representations are helpful for cross-dataset discriminative prediction. Concretely, E2STN consists of three modules, i.e., transfer module, transfer evaluation module, and discriminative prediction module. The transfer module encodes the domain-specific information of source and target domains and then re-constructs the source domain's emotional pattern and the target domain's statistical characteristics into the new stylized EEG representations. In this process, the transfer evaluation module is adopted to constrain the generated representations that can more precisely fuse two kinds of complementary information from source and target domains and avoid distorting. Finally, the generated stylized EEG representations are fed into the discriminative prediction module for final classification. Extensive experiments show that the E2STN can achieve the state-of-the-art performance on cross-dataset EEG emotion recognition tasks.
</details></li>
</ul>
<hr>
<h2 id="Prompting-In-Context-Operator-Learning-with-Sensor-Data-Equations-and-Natural-Language"><a href="#Prompting-In-Context-Operator-Learning-with-Sensor-Data-Equations-and-Natural-Language" class="headerlink" title="Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language"></a>Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05061">http://arxiv.org/abs/2308.05061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Yang, Tingwei Meng, Siting Liu, Stanley J. Osher</li>
<li>for: 学习物理学中的运算符，使用自然语言描述和方程来捕捉人类知识。</li>
<li>methods: 提出了一种多模态启发学习方法，使用”caption”来整合人类知识，并使用更高效的ICON-LM神经网络架构。</li>
<li>results: 实验结果表明，这种方法不仅能够提高学习性能和减少数据需求，还可以拓宽物理学中的应用范围。<details>
<summary>Abstract</summary>
In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICON-LM" for scientific machine learning tasks, which creates a new path for the application of language models.
</details>
<details>
<summary>摘要</summary>
在科学机器学习领域的发展中，在推理阶段从提示数据中学习操作符已经表现出了明显的潜力。然而，当前模型过于依赖感知数据，可能不充分利用人类知识对操作符的珍贵性。为此，我们提出一种将受ContextOperator学习转化为多模式 парадигмы的方法。我们提议使用“caption”来结合人类对操作符的知识，通过自然语言描述和方程表述。我们示出了这种方法不仅扩大了物理学习的灵活性和通用性，而且显著提高了学习性和数据需求。此外，我们介绍了一种更高效的多模式受ContextOperator学习神经网络架构，称为“ICON-LM”，基于语言模型类架构。我们示出了ICON-LM在科学机器学习任务中的可行性，创造了一条新的语言模型应用路径。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Method-for-improving-accuracy-in-neural-network-by-reinstating-traditional-back-propagation-technique"><a href="#A-Novel-Method-for-improving-accuracy-in-neural-network-by-reinstating-traditional-back-propagation-technique" class="headerlink" title="A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique"></a>A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05059">http://arxiv.org/abs/2308.05059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokulprasath R</li>
<li>for: 这个论文主要是为了提出一种新的快速参数更新方法，以解决深度学习中的计算负担和衰减问题。</li>
<li>methods: 该方法利用了一种新的快速参数更新策略，即不需要在每层计算Gradient的方法。</li>
<li>results: 对比州值数据集，该方法能够快速学习，避免衰减问题，并超过了现有的方法。<details>
<summary>Abstract</summary>
Deep learning has revolutionized industries like computer vision, natural language processing, and speech recognition. However, back propagation, the main method for training deep neural networks, faces challenges like computational overhead and vanishing gradients. In this paper, we propose a novel instant parameter update methodology that eliminates the need for computing gradients at each layer. Our approach accelerates learning, avoids the vanishing gradient problem, and outperforms state-of-the-art methods on benchmark data sets. This research presents a promising direction for efficient and effective deep neural network training.
</details>
<details>
<summary>摘要</summary>
深度学习已经革命化了计算机视觉、自然语言处理和语音识别等领域。然而，返回层的主要训练方法——归并，面临着计算负担和衰减 gradient 的问题。在这篇论文中，我们提出了一种新的快速参数更新方法，它消除了每层计算 gradients 的需求。我们的方法可以加速学习，避免衰减 gradient 问题，并在标准数据集上超越当前的状态艺。这篇研究表明了深度神经网络训练的有效和高效的可能性。
</details></li>
</ul>
<hr>
<h2 id="Sound-propagation-in-realistic-interactive-3D-scenes-with-parameterized-sources-using-deep-neural-operators"><a href="#Sound-propagation-in-realistic-interactive-3D-scenes-with-parameterized-sources-using-deep-neural-operators" class="headerlink" title="Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators"></a>Sound propagation in realistic interactive 3D scenes with parameterized sources using deep neural operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05141">http://arxiv.org/abs/2308.05141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dtu-act/deeponet-acoustic-wave-prop">https://github.com/dtu-act/deeponet-acoustic-wave-prop</a></li>
<li>paper_authors: Nikolas Borrel-Jensen, Somdatta Goswami, Allan P. Engsig-Karup, George Em Karniadakis, Cheol-Ho Jeong</li>
<li>for: 用于虚拟&#x2F;增强现实、游戏声音和空间计算等领域中的声场 simulate。</li>
<li>methods: 使用深度运算网络来近似线性波方程算子，以实现快速的声场传播预测。</li>
<li>results: 实现了在实际3D声学场景中逐源点预测声场的准确性，计算时间在毫秒级别，并且与参照解决方案之间的误差在0.02Pa到0.10Pa之间。<details>
<summary>Abstract</summary>
We address the challenge of sound propagation simulations in $3$D virtual rooms with moving sources, which have applications in virtual/augmented reality, game audio, and spatial computing. Solutions to the wave equation can describe wave phenomena such as diffraction and interference. However, simulating them using conventional numerical discretization methods with hundreds of source and receiver positions is intractable, making stimulating a sound field with moving sources impractical. To overcome this limitation, we propose using deep operator networks to approximate linear wave-equation operators. This enables the rapid prediction of sound propagation in realistic 3D acoustic scenes with moving sources, achieving millisecond-scale computations. By learning a compact surrogate model, we avoid the offline calculation and storage of impulse responses for all relevant source/listener pairs. Our experiments, including various complex scene geometries, show good agreement with reference solutions, with root mean squared errors ranging from 0.02 Pa to 0.10 Pa. Notably, our method signifies a paradigm shift as no prior machine learning approach has achieved precise predictions of complete wave fields within realistic domains. We anticipate that our findings will drive further exploration of deep neural operator methods, advancing research in immersive user experiences within virtual environments.
</details>
<details>
<summary>摘要</summary>
我们面临处理三维虚拟房间内的声波传播模拟中的挑战，这些应用包括虚拟现实、增强现实和空间计算。我们的方法可以描述声波现象如扩散和折射，但是使用传统的数值积分方法来模拟这些现象需要数百个源和接收器位置，这使得实现声场的类比映射成为不可能的。为了解决这个限制，我们提议使用深度算子网络来近似线性波方程式的算子。这使得在真实的三维声学场景中，快速预测声波传播的 computation 可以在毫秒级时间内完成。通过学习一个紧凑的模型，我们可以避免在所有相关的源 listener 组合上进行维护和储存对应数据。我们的实验包括多种复杂的场景几何，结果显示与参考解析相符，误差范围为0.02 Pa 至 0.10 Pa。值得注意的是，我们的方法代表了一种新的思维方式，没有过去的机器学习方法能够精确地预测完整的波场在真实的领域中。我们预期这些发现将驱动更多的深度神经算子方法研究，促进虚拟环境中的充满人体验的探索。
</details></li>
</ul>
<hr>
<h2 id="RadGraph2-Modeling-Disease-Progression-in-Radiology-Reports-via-Hierarchical-Information-Extraction"><a href="#RadGraph2-Modeling-Disease-Progression-in-Radiology-Reports-via-Hierarchical-Information-Extraction" class="headerlink" title="RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction"></a>RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05046">http://arxiv.org/abs/2308.05046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sameer Khanna, Adam Dejl, Kibo Yoon, Quoc Hung Truong, Hanh Duong, Agustina Saenz, Pranav Rajpurkar</li>
<li>for: 这个论文的目的是开发一个新的医学报告信息EXTRACTION dataset，以捕捉医疗器械报告中的疾病状态和设备置换的变化。</li>
<li>methods: 该论文使用了一个层次结构，将实体按照其关系进行组织，并在训练过程中使用这个结构，以提高信息EXTRACTION模型的性能。具体来说，该论文提出了一种修改后的 DyGIE++ 框架，称为 HGIE，该模型在实体和关系EXTRACTION任务中表现出色。</li>
<li>results: 根据 RadGraph2 数据集，该论文的 HGIE 模型可以更好地捕捉医疗器械报告中的各种发现，并在关系EXTRACTION任务中表现出色，比前一代模型更高。这种成果提供了开发自动跟踪疾病进程和开发基于医疗领域自然层次标签的信息EXTRACTION模型的基础。<details>
<summary>Abstract</summary>
We present RadGraph2, a novel dataset for extracting information from radiology reports that focuses on capturing changes in disease state and device placement over time. We introduce a hierarchical schema that organizes entities based on their relationships and show that using this hierarchy during training improves the performance of an information extraction model. Specifically, we propose a modification to the DyGIE++ framework, resulting in our model HGIE, which outperforms previous models in entity and relation extraction tasks. We demonstrate that RadGraph2 enables models to capture a wider variety of findings and perform better at relation extraction compared to those trained on the original RadGraph dataset. Our work provides the foundation for developing automated systems that can track disease progression over time and develop information extraction models that leverage the natural hierarchy of labels in the medical domain.
</details>
<details>
<summary>摘要</summary>
我们介绍RadGraph2，一个新的医学报告信息提取数据集，专注于时间上的疾病状态和设备安装变化。我们提出了一种层次结构，将实体按照关系组织，并证明在训练过程中使用这种层次结构可以提高信息提取模型的性能。我们对 DyGIE++ 框架进行修改，得到了我们的 HGIE 模型，该模型在实体和关系提取任务中表现更好。我们示出 RadGraph2 可以让模型捕捉更多的发现和在关系提取任务中表现更好，相比于基于原始 RadGraph 数据集训练的模型。我们的工作为Automated systems的开发提供了基础，以便在医疗领域自动跟踪疾病进程和开发利用医疗领域自然的标签层次结构的信息提取模型。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Wideband-Spectrum-Sensing-and-Scheduling-for-Networked-UAVs-in-UTM-Systems"><a href="#Collaborative-Wideband-Spectrum-Sensing-and-Scheduling-for-Networked-UAVs-in-UTM-Systems" class="headerlink" title="Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems"></a>Collaborative Wideband Spectrum Sensing and Scheduling for Networked UAVs in UTM Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05036">http://arxiv.org/abs/2308.05036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sravan Reddy Chintareddy, Keenan Roach, Kenny Cheung, Morteza Hashemi</li>
<li>for: 本文提出了一种基于数据驱动的协同宽频谱感知和调度框架，用于网络无人机飞行器（UAV）作为次级用户，机会性地利用探测到的谱带孔隙。</li>
<li>methods: 我们提出了一种多类分类问题来探测宽频谱中的空闲谱带，基于收集的I&#x2F;Q样本。为了提高谱感知模块的准确性，每个个人UAV的输出将在UTM生态系统中的服务器进行融合。</li>
<li>results: 我们在spectrum scheduling阶段使用了强化学习（RL）解决方案来动态分配探测到的谱带孔隙给次级用户（即UAV）。<details>
<summary>Abstract</summary>
In this paper, we propose a data-driven framework for collaborative wideband spectrum sensing and scheduling for networked unmanned aerial vehicles (UAVs), which act as the secondary users to opportunistically utilize detected spectrum holes. To this end, we propose a multi-class classification problem for wideband spectrum sensing to detect vacant spectrum spots based on collected I/Q samples. To enhance the accuracy of the spectrum sensing module, the outputs from the multi-class classification by each individual UAV are fused at a server in the unmanned aircraft system traffic management (UTM) ecosystem. In the spectrum scheduling phase, we leverage reinforcement learning (RL) solutions to dynamically allocate the detected spectrum holes to the secondary users (i.e., UAVs). To evaluate the proposed methods, we establish a comprehensive simulation framework that generates a near-realistic synthetic dataset using MATLAB LTE toolbox by incorporating base-station~(BS) locations in a chosen area of interest, performing ray-tracing, and emulating the primary users channel usage in terms of I/Q samples. This evaluation methodology provides a flexible framework to generate large spectrum datasets that could be used for developing ML/AI-based spectrum management solutions for aerial devices.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个基于数据的框架，用于无人机（UAV）网络中进行共同宽频谱感知和调度。这些无人机作为次要用户，可以利用探测到的谱频孔的机会来进行兼抓式使用。为实现这一目标，我们提出了一个多类分类问题，用于宽频谱感知，以检测谱频孔的存在。为了提高谱频感知模块的准确性，每个无人机的多类分类输出将在UTM（无人机系统交通管理）环境中的服务器进行融合。在调度阶段，我们利用强化学习（RL）解决方案，动态地将探测到的谱频孔分配给次要用户（即无人机）。为评估我们的方法，我们建立了一个完整的 simulate评估框架，通过使用MATLAB LTE工具包，在选定的区域内，基站（BS）的位置、短距离通信和主要用户通信的I/Q样本来生成一个准确的数据集。这种评估方法ология提供了一个灵活的框架，可以用于开发ML/AI基于谱管理解决方案。
</details></li>
</ul>
<hr>
<h2 id="Kairos-Practical-Intrusion-Detection-and-Investigation-using-Whole-system-Provenance"><a href="#Kairos-Practical-Intrusion-Detection-and-Investigation-using-Whole-system-Provenance" class="headerlink" title="Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance"></a>Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05034">http://arxiv.org/abs/2308.05034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/provenanceanalytics/kairos">https://github.com/provenanceanalytics/kairos</a></li>
<li>paper_authors: Zijun Cheng, Qiujian Lv, Jinyuan Liang, Yan Wang, Degang Sun, Thomas Pasquier, Xueyuan Han</li>
<li>for: 本研究旨在提出一种基于证据图的入侵检测系统（PIDS），以检测现代的攻击活动。</li>
<li>methods: 该研究使用了一种新的图神经网络编码器-解码器架构，通过学习系统审计日志中 Structural 变化的时间演化来评估系统事件的异常程度。</li>
<li>results: 根据实验结果，Kairos 可以同时满足四个维度的需求，而现有方法则缺乏至少一个维度。 Kairos 可以快速、高效地监控主机系统，并且可以生成精炼的攻击摘要图，以便系统管理员快速理解和应对系统入侵。<details>
<summary>Abstract</summary>
Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approaches sacrifice at least one and struggle to achieve comparable detection performance.   Kairos leverages a novel graph neural network-based encoder-decoder architecture that learns the temporal evolution of a provenance graph's structural changes to quantify the degree of anomalousness for each system event. Then, based on this fine-grained information, Kairos reconstructs attack footprints, generating compact summary graphs that accurately describe malicious activity over a stream of system audit logs. Using state-of-the-art benchmark datasets, we demonstrate that Kairos outperforms previous approaches.
</details>
<details>
<summary>摘要</summary>
Provenance graphs are 系统执行历史记录，现代研究探讨了多种分析方法，以检测系统执行过程中的攻击。从设计文档中，我们确定了四个纬度驱动了基于证明的攻击检测系统（PIDS）的开发：范围（可以检测现代攻击？）、不偏见（可以检测新型攻击？）、实时性（可以高效监控主机系统？）和攻击重建（可以将攻击活动简化为可读的形式？）。我们介绍了 Kairos，第一个满足所有四个纬度的 PIDS，而现有方法至少牺牲一个纬度，并具有相似的检测性能。 Kairos 使用一种新的图神经网络基本 Encoder-Decoder 架构，学习系统审计记录中的时间演化结构变化，以衡量每个系统事件的异常程度。然后，基于这些细腻信息，Kairos 重建攻击印记，生成系统审计记录流中的简洁摘要图，准确描述攻击活动。使用现代标准数据集，我们证明了 Kairos 在检测方面的优越性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/10/cs.LG_2023_08_10/" data-id="clltaago3005pr888fwek3wcu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/11/eess.IV_2023_08_11/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-11 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/10/cs.SD_2023_08_10/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-10 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
