
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-18 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.09546 repo_url: None paper_authors: Shu Wang, Kun Sun, Qi Li for: 本研究旨在强化语音识别">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-18 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/18/cs.SD_2023_08_18/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.09546 repo_url: None paper_authors: Shu Wang, Kun Sun, Qi Li for: 本研究旨在强化语音识别">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-17T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:33.029Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.SD_2023_08_18/" class="article-date">
  <time datetime="2023-08-17T16:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-18 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Compensating-Removed-Frequency-Components-Thwarting-Voice-Spectrum-Reduction-Attacks"><a href="#Compensating-Removed-Frequency-Components-Thwarting-Voice-Spectrum-Reduction-Attacks" class="headerlink" title="Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks"></a>Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09546">http://arxiv.org/abs/2308.09546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu Wang, Kun Sun, Qi Li</li>
<li>for: 本研究旨在强化语音识别系统（ASR）面对恶意音频攻击的安全性。</li>
<li>methods: 本文提出了一种名为ACE的听音补偿系统，利用频率成分相关性和扰动敏感性来对抗频谱减少攻击。</li>
<li>results: 实验结果表明，ACE可以效果地减少ASR推理错误率达87.9%，并对剩下的错误分析了六种常见的ASR推理错误类型和其可能的缓解方案。<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) provides diverse audio-to-text services for humans to communicate with machines. However, recent research reveals ASR systems are vulnerable to various malicious audio attacks. In particular, by removing the non-essential frequency components, a new spectrum reduction attack can generate adversarial audios that can be perceived by humans but cannot be correctly interpreted by ASR systems. It raises a new challenge for content moderation solutions to detect harmful content in audio and video available on social media platforms. In this paper, we propose an acoustic compensation system named ACE to counter the spectrum reduction attacks over ASR systems. Our system design is based on two observations, namely, frequency component dependencies and perturbation sensitivity. First, since the Discrete Fourier Transform computation inevitably introduces spectral leakage and aliasing effects to the audio frequency spectrum, the frequency components with similar frequencies will have a high correlation. Thus, considering the intrinsic dependencies between neighboring frequency components, it is possible to recover more of the original audio by compensating for the removed components based on the remaining ones. Second, since the removed components in the spectrum reduction attacks can be regarded as an inverse of adversarial noise, the attack success rate will decrease when the adversarial audio is replayed in an over-the-air scenario. Hence, we can model the acoustic propagation process to add over-the-air perturbations into the attacked audio. We implement a prototype of ACE and the experiments show ACE can effectively reduce up to 87.9% of ASR inference errors caused by spectrum reduction attacks. Also, by analyzing residual errors, we summarize six general types of ASR inference errors and investigate the error causes and potential mitigation solutions.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统提供了多种媒体到文本服务，帮助人类与机器进行交互。然而，最新的研究发现，ASR系统受到了多种恶意音频攻击。特别是，通过去除非关键频率成分，新的频谱减少攻击可以生成对人类可见但是ASR系统无法正确理解的恶意音频。这引起了社交媒体平台上内容审核解决方案检测危害内容的新挑战。在本文中，我们提议一种名为ACE的听音补偿系统，用于对ASR系统中的频谱减少攻击进行防御。我们的系统设计基于两个观察结论：一是频率成分之间的相互依赖关系，二是对攻击音频进行频率补偿可以降低攻击成功率。我们实现了ACE的原型，实验结果表明，ACE可以降低ASR推断错误率达87.9%。此外，我们分析了剩下的错误 residual errors，并总结了六种通用的ASR推断错误类型，并investigate了这些错误的原因和可能的缓解解决方案。
</details></li>
</ul>
<hr>
<h2 id="Generative-Machine-Listener"><a href="#Generative-Machine-Listener" class="headerlink" title="Generative Machine Listener"></a>Generative Machine Listener</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09493">http://arxiv.org/abs/2308.09493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanxin Jiang, Lars Villemoes, Arijit Biswas</li>
<li>for: 这篇论文是用于描述一种基于神经网络的音频测试数据生成方法。</li>
<li>methods: 该方法使用各个侵入式听力测试分数来训练神经网络，并可以预测每对参照和编码音频信号的分数分布。</li>
<li>results: 与基准系统相比，使用回归 Mean 分数而不是 GML 方法，出现了较低的异常比率（OR），并且可以轻松地预测 confidence interval（CI）。此外，通过从图像领域中吸取数据增强技术，可以提高 CI 预测精度以及 Pearson 和 Spearman 排名 correlation 的 Mean 分数。<details>
<summary>Abstract</summary>
We show how a neural network can be trained on individual intrusive listening test scores to predict a distribution of scores for each pair of reference and coded input stereo or binaural signals. We nickname this method the Generative Machine Listener (GML), as it is capable of generating an arbitrary amount of simulated listening test data. Compared to a baseline system using regression over mean scores, we observe lower outlier ratios (OR) for the mean score predictions, and obtain easy access to the prediction of confidence intervals (CI). The introduction of data augmentation techniques from the image domain results in a significant increase in CI prediction accuracy as well as Pearson and Spearman rank correlation of mean scores.
</details>
<details>
<summary>摘要</summary>
我们展示了一个神经网络可以根据个别侵入式聆听测试成绩来预测每对参照和压缩音 signals 的分布。我们称这为生成机器听者（GML），因为它可以生成无限多个模拟聆听测试数据。相比基准系统使用平均分布 regression，我们观察了较低的外围比率（OR），并可以轻松地预测信息 intervals（CI）的预测。对于数据增强技术的引入，我们获得了显著提高 CI 预测准确性以及平均分布和斯宾格数字相互联系的关系。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model"><a href="#Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model" class="headerlink" title="Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model"></a>Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09454">http://arxiv.org/abs/2308.09454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Rose Bjare, Stefan Lattner, Gerhard Widmer</li>
<li>for: 这个研究探讨了不同采样策略对于训练autoregressive自然语言处理模型的质量产生的影响。</li>
<li>methods: 作者使用高容量变换器模型训练在高度结构化的爱尔兰传统旋律音乐中，并使用分布 truncation 采样技术进行分析。特别是使用核心采样、“典型采样”和传统祖先采样。</li>
<li>results: 研究发现，在优化的情况下，概率 truncation 技术可能会限制多样性和结构性特征，但在低效情况下，它们可能会生成更多的乐曲。<details>
<summary>Abstract</summary>
Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed "typical sampling", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.
</details>
<details>
<summary>摘要</summary>
研究自然语言处理已经证明，训练过的自然语言生成模型的质量受到采样策略的影响。在这个研究中，我们研究了不同采样技术对音乐质量的影响。为此，我们使用高容量变换器模型训练在高度结构化的爱尔兰传统散户歌曲中，并对采样技术的影响进行分析。具体来说，我们使用核心采样、“典型采样”和传统祖先采样。我们在两个场景中评估这些采样策略的影响：优化的情况下，模型性能很好，以及逐步降低模型性能的情况。我们使用对jective和主观评估来评估生成的样本。我们发现，抑制采样技术可能会限制多样性和结构性模式，但在优化情况下可能会生成更多的音乐样本。
</details></li>
</ul>
<hr>
<h2 id="TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition"><a href="#TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition" class="headerlink" title="TrOMR:Transformer-Based Polyphonic Optical Music Recognition"></a>TrOMR:Transformer-Based Polyphonic Optical Music Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09370">http://arxiv.org/abs/2308.09370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/netease/polyphonic-tromr">https://github.com/netease/polyphonic-tromr</a></li>
<li>paper_authors: Yixuan Li, Huaping Liu, Qiang Jin, Miaomiao Cai, Peng Li</li>
<li>for: 这篇论文是关于音乐Recognition（OMR）技术的研究，旨在提出一种基于变换器的端到端多重音乐识别方法，以提高识别精度。</li>
<li>methods: 该方法使用变换器来实现全球性的音乐理解，并引入一种新的一致损失函数和合理的数据注释方法来提高识别精度。</li>
<li>results: 广泛的实验表明，TrOMR方法在实际场景下的识别精度明显高于现有的OMR方法，特别是在复杂的音乐乐谱上。此外，authors还开发了TrOMR系统和一个摄像头场景数据集，以便在真实世界中进行全页音乐乐谱识别。<details>
<summary>Abstract</summary>
Optical Music Recognition (OMR) is an important technology in music and has been researched for a long time. Previous approaches for OMR are usually based on CNN for image understanding and RNN for music symbol classification. In this paper, we propose a transformer-based approach with excellent global perceptual capability for end-to-end polyphonic OMR, called TrOMR. We also introduce a novel consistency loss function and a reasonable approach for data annotation to improve recognition accuracy for complex music scores. Extensive experiments demonstrate that TrOMR outperforms current OMR methods, especially in real-world scenarios. We also develop a TrOMR system and build a camera scene dataset for full-page music scores in real-world. The code and datasets will be made available for reproducibility.
</details>
<details>
<summary>摘要</summary>
优化音乐识别（OMR）是音乐技术的一个重要方向，已经在长期的研究中。先前的OMR方法通常基于Convolutional Neural Network（CNN） для图像理解和Recurrent Neural Network（RNN） для乐谱符号分类。在这篇论文中，我们提出了一种基于变换器的方法，具有优秀的全球性识别能力，用于端到端多重音乐识别，称为TrOMR。我们还提出了一种新的一致损失函数和一种合理的数据注释方法，以提高复杂乐谱中的识别精度。广泛的实验表明，TrOMR超过当前OMR方法，特别是在真实世界情况下。我们还开发了TrOMR系统和一个摄像头场景数据集，用于全页乐谱的真实世界摄像头识别。代码和数据将被公开，以便重现。
</details></li>
</ul>
<hr>
<h2 id="Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge"><a href="#Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge" class="headerlink" title="Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge"></a>Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09311">http://arxiv.org/abs/2308.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, Yong Man Ro</li>
<li>for: 本研究提出了一种新的唇读框架，特别是针对低资源语言，前一代文献中未得到充分的关注。由于低资源语言缺乏足够的视频文本对应数据来训练模型，因此在这些语言上开发唇读模型被视为挑战。</li>
<li>methods: 我们尝试通过学习通用语言知识，即模型唇部运动的能力，从一种高资源语言来预测语音单位。由于不同语言有一定的共同声母，因此学习一种语言的通用语言知识可以扩展到其他语言。此外，我们还提出了语言特定的储存器（LMDecoder），它将语言特定的音频特征存储在内存银行中，并可以通过语音文本对应数据进行训练。</li>
<li>results: 通过对五种语言（英语、西班牙语、法语、意大利语、葡萄牙语）进行了广泛的实验，证明了我们提出的方法的效iveness。<details>
<summary>Abstract</summary>
This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data which is more easily accessible than video-text paired data. Therefore, with LMDecoder, we can transform the input speech units into language-specific audio features and translate them into texts by utilizing the learned rich language knowledge. Finally, by combining general speech knowledge and language-specific knowledge, we can efficiently develop lip reading models even for low-resource languages. Through extensive experiments using five languages, English, Spanish, French, Italian, and Portuguese, the effectiveness of the proposed method is evaluated.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms"><a href="#Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms" class="headerlink" title="Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms"></a>Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09302">http://arxiv.org/abs/2308.09302</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ph-w2000/s2pecnet">https://github.com/ph-w2000/s2pecnet</a></li>
<li>paper_authors: Penghui Wen, Kun Hu, Wenxi Yue, Sen Zhang, Wanlei Zhou, Zhiyong Wang<br>for: 这篇论文旨在提出一种基于多频域特征的深度学习方法，以提高音频反伪测试的效果。methods: 这篇论文使用了一种叫做S2pecNet的深度学习方法，它利用多频域特征来实现音频反伪测试。特别是，这篇论文使用了一种组合构成的混合方法，将多频域特征与时间上的特征混合在一起，以提高反伪测试的精度。results: 这篇论文的实验结果显示，S2pecNet方法可以实现高效的音频反伪测试。具体来说，在ASVspoof2019 LA Challenge上，S2pecNet方法的误分率（EER）为0.77%，与其他方法相比，表现出色。<details>
<summary>Abstract</summary>
Robust audio anti-spoofing has been increasingly challenging due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti-spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion-reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spectrograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset: ASVspoof2019 LA Challenge.
</details>
<details>
<summary>摘要</summary>
“对于深圳技术的进步，Robust audio anti-spoofing 对于不断增加的挑战。 spectrograms 已经展示了它们在反伪中的能力，但多维 spectral pattern 尚未得到充分利用，这限制了它们在不同的伪装攻击下的效iveness。因此，我们提出了一种基于深度学习的新方法，即 S2pecNet，具有多维 spectral pattern 的融合构想。具体来说，我们将spectral pattern 最多到第二顺序融合在一个course-to-fine的方式下，并设计了两个分支来从spectral和temporal context中获取细节。从融合表示重建到input spectrograms 可以更好地储存可能的融合信息损失。我们的方法在一个广泛使用的dataset上取得了现代最佳性能，EER 为0.77%。”
</details></li>
</ul>
<hr>
<h2 id="V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models"><a href="#V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models" class="headerlink" title="V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models"></a>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09300">http://arxiv.org/abs/2308.09300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, Weidong Cai</li>
<li>for: 该研究旨在提出一种轻量级的视觉对应声音生成方法，利用基础模型（FM）来解决跨modal生成问题。</li>
<li>methods: 该方法首先调查了视觉CLIP和听音CLAP模型之间的领域差异，然后提出了一种简单 yet effective的映射机制（V2A-Mapper）来桥接这个领域差异。 Conditioned on the translated CLAP embedding, 采用预训练的听音生成FM AudioLDM来生成高质量和视觉对应的声音。</li>
<li>results: 比较前方法，该方法只需快速训练V2A-Mapper，并在两个V2A数据集上进行了广泛的实验和分析。结果表明，使用生成映射器可以提高听音生成的质量和多样性（FD），而使用回归映射器可以提高听音生成的相关性（CS）。在FD和CS两个指标上，该方法与当前状态艺术方法相比，提高了53%和19%。<details>
<summary>Abstract</summary>
Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.
</details>
<details>
<summary>摘要</summary>
Currently, building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new trend in AI research. These FMs can learn representative and generative abilities from vast amounts of data, which can be easily adapted and transferred to a wide range of downstream tasks without needing to be trained from scratch. However, using FMs in cross-modal generation, especially when it comes to audio modality, is still an under-researched area. Specifically, automatically generating semantically relevant sound from visual input is an important problem in cross-modal generation studies.Existing methods for solving this vision-to-audio (V2A) generation problem tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then, we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound.Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches. We trained our method with 86% fewer parameters but achieved 53% and 19% improvement in FD and CS, respectively.
</details></li>
</ul>
<hr>
<h2 id="Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries"><a href="#Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries" class="headerlink" title="Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries"></a>Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09089">http://arxiv.org/abs/2308.09089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Wilkins, Justin Salamon, Magdalena Fuentes, Juan Pablo Bello, Oriol Nieto</li>
<li>for: 这个论文的目的是提出一种多模态框架，用于根据视频帧来检索高质量的声音效果（SFX）。</li>
<li>methods: 这个论文使用了大型语言模型和基础视觉语言模型来桥接高质量的音频和视频，从而创建了一个高度可扩展的自动音频视频数据筛选管道。它还使用预训练的音频和视频编码器来训练一种对比学习基于的检索系统。</li>
<li>results: 论文的实验结果表明，使用这个多模态框架，可以significantly outperform基线值在高质量声音效果检索任务上。此外，这个系统还能够在各种不同的数据集上进行 generale化，并且在用户研究中，人们对这个系统中的SFX Retrieval结果表示满意。<details>
<summary>Abstract</summary>
Finding the right sound effects (SFX) to match moments in a video is a difficult and time-consuming task, and relies heavily on the quality and completeness of text metadata. Retrieving high-quality (HQ) SFX using a video frame directly as the query is an attractive alternative, removing the reliance on text metadata and providing a low barrier to entry for non-experts. Due to the lack of HQ audio-visual training data, previous work on audio-visual retrieval relies on YouTube (in-the-wild) videos of varied quality for training, where the audio is often noisy and the video of amateur quality. As such it is unclear whether these systems would generalize to the task of matching HQ audio to production-quality video. To address this, we propose a multimodal framework for recommending HQ SFX given a video frame by (1) leveraging large language models and foundational vision-language models to bridge HQ audio and video to create audio-visual pairs, resulting in a highly scalable automatic audio-visual data curation pipeline; and (2) using pre-trained audio and visual encoders to train a contrastive learning-based retrieval system. We show that our system, trained using our automatic data curation pipeline, significantly outperforms baselines trained on in-the-wild data on the task of HQ SFX retrieval for video. Furthermore, while the baselines fail to generalize to this task, our system generalizes well from clean to in-the-wild data, outperforming the baselines on a dataset of YouTube videos despite only being trained on the HQ audio-visual pairs. A user study confirms that people prefer SFX retrieved by our system over the baseline 67% of the time both for HQ and in-the-wild data. Finally, we present ablations to determine the impact of model and data pipeline design choices on downstream retrieval performance. Please visit our project website to listen to and view our SFX retrieval results.
</details>
<details>
<summary>摘要</summary>
寻找符合视频中的声音效果（SFX）是一项复杂和时间consuming的任务，它取决于视频中文本 metadata 的质量和完整性。使用视频帧直接作为查询来检索高品质（HQ）声音的方法是一种吸引人的alternative，它可以消除文本 metadata 的依赖关系，并提供低门槛 для非专家。由于缺乏 HQ 音频视频培训数据，过去的声音视频检索工作通常使用 YouTube （在野）视频进行培训，这些视频的音频 oftentimes 噪音且视频质量不高。因此，是否这些系统能够通用到高品质音频与生产质量视频之间的匹配问题存在uncertainty。为解决这个问题，我们提议一种多模态框架，用于基于视频帧提供 HQ 声音效果的推荐，包括：1. 利用大语言模型和基础视频语言模型来桥接 HQ 音频和视频，从而创建高可扩展的自动音频视频数据纪要管道。2. 使用预训练的音频和视觉编码器来培训对比学习基于检索系统。我们的系统，通过我们自动生成的数据纪要管道进行训练，与基于野外数据的基eline 相比，显著提高了高品质声音效果检索任务的性能。此外，我们的系统可以从清晰到野外数据中进行扩展，并在 YouTube 视频集上表现出色，即使只受训练于 HQ 音频视频对。人们在用户研究中表示，他们67% 的时间 prefer SFX 被我们的系统检索出来，而不是基eline 。最后，我们提供了一系列ablation来评估模型和数据管道设计的影响。请参考我们项目网站来听取和查看我们的 SFX 检索结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.SD_2023_08_18/" data-id="cllurrpb70099sw881g3xbioi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/18/cs.LG_2023_08_18/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-18 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/18/eess.IV_2023_08_18/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-18 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
