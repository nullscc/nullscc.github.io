
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-01 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03777 repo_url: None p">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-01 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/01/eess.IV_2023_08_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03777 repo_url: None p">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-31T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:23.869Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/01/eess.IV_2023_08_01/" class="article-date">
  <time datetime="2023-07-31T16:00:00.000Z" itemprop="datePublished">2023-08-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-01 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lab-in-a-Tube-A-portable-imaging-spectrophotometer-for-cost-effective-high-throughput-and-label-free-analysis-of-centrifugation-processes"><a href="#Lab-in-a-Tube-A-portable-imaging-spectrophotometer-for-cost-effective-high-throughput-and-label-free-analysis-of-centrifugation-processes" class="headerlink" title="Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes"></a>Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03777">http://arxiv.org/abs/2308.03777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wei, Dehua Hu, Bijie Bai, Chenqi Meng, Tsz Kin Chan, Xing Zhao, Yuye Wang, Yi-Ping Ho, Wu Yuan, Ho-Pui Ho</li>
<li>for: 这种研究旨在实时观察中心rifugation过程中的动态过程。</li>
<li>methods: 该研究使用了一种创新的Lab_in_a_Tube成像光谱仪，其包括实时图像分析和可编程中断功能。这种可以在30美元以下的Portable LIAT设备包含了Wi Fi摄像头和活动封闭控制。</li>
<li>results: 该研究发现了在中心rifugation过程中单个液体径流形成环形图像，这是首次观察的现象。研究还发展了基于旋转参照系的理论 simulations，与实验结果高度相关。此外，研究还实现了首次观察血液凝固过程在临床实验室中心rifuges中。这种Cost效果开放了中心rifugation微生物学研究的新途径，并为大规模和不间断监测中心rifugal过程的计算成像仪器网络创造了可能。<details>
<summary>Abstract</summary>
Centrifuges serve as essential instruments in modern experimental sciences, facilitating a wide range of routine sample processing tasks that necessitate material sedimentation. However, the study for real time observation of the dynamical process during centrifugation has remained elusive. In this study, we developed an innovative Lab_in_a_Tube imaging spectrophotometer that incorporates capabilities of real time image analysis and programmable interruption. This portable LIAT device costs less than 30 US dollars. Based on our knowledge, it is the first Wi Fi camera built_in in common lab centrifuges with active closed_loop control. We tested our LIAT imaging spectrophotometer with solute solvent interaction investigation obtained from lab centrifuges with quantitative data plotting in a real time manner. Single re circulating flow was real time observed, forming the ring shaped pattern during centrifugation. To the best of our knowledge, this is the very first observation of similar phenomena. We developed theoretical simulations for the single particle in a rotating reference frame, which correlated well with experimental results. We also demonstrated the first demonstration to visualize the blood sedimentation process in clinical lab centrifuges. This remarkable cost effectiveness opens up exciting opportunities for centrifugation microbiology research and paves the way for the creation of a network of computational imaging spectrometers at an affordable price for large scale and continuous monitoring of centrifugal processes in general.
</details>
<details>
<summary>摘要</summary>
中央机器 serves as a crucial tool in modern experimental sciences, facilitating a wide range of routine sample processing tasks that require material sedimentation. However, the study of real-time observation of the dynamical process during centrifugation has been challenging. In this study, we developed an innovative Lab_in_a_Tube imaging spectrophotometer that incorporates real-time image analysis and programmable interruption capabilities. This portable LIAT device costs less than 30 US dollars. Based on our knowledge, it is the first Wi-Fi camera built-in in common lab centrifuges with active closed-loop control. We tested our LIAT imaging spectrophotometer with solute-solvent interaction investigations obtained from lab centrifuges, with quantitative data plotting in real-time. Single recirculating flow was real-time observed, forming a ring-shaped pattern during centrifugation. To the best of our knowledge, this is the first observation of similar phenomena. We developed theoretical simulations for the single particle in a rotating reference frame, which correlated well with experimental results. We also demonstrated the first visualization of blood sedimentation process in clinical lab centrifuges. This remarkable cost-effectiveness opens up exciting opportunities for centrifugation microbiology research and paves the way for the creation of a network of computational imaging spectrometers at an affordable price for large-scale and continuous monitoring of centrifugal processes in general.
</details></li>
</ul>
<hr>
<h2 id="PressureTransferNet-Human-Attribute-Guided-Dynamic-Ground-Pressure-Profile-Transfer-using-3D-simulated-Pressure-Maps"><a href="#PressureTransferNet-Human-Attribute-Guided-Dynamic-Ground-Pressure-Profile-Transfer-using-3D-simulated-Pressure-Maps" class="headerlink" title="PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps"></a>PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00538">http://arxiv.org/abs/2308.00538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lala Shakti Swarup Ray, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz</li>
<li>for: 这篇论文旨在开发一种基于地面压力信息的人体活动识别（HAR）方法，以便在不同场景下准确地识别人体活动。</li>
<li>methods: 该方法使用了一种encoder-decoder模型，将源地面压力图与目标人体特征向量作为输入，生成一个新的地面压力图，捕捉了目标特征。</li>
<li>results: 经过训练后，该模型在真实场景数据上表现出了准确地传递人体特征到地面压力图的能力，并且通过物理学习模型的视觉验证和物理压力盘数据的分类验证，证明了模型的正确性。<details>
<summary>Abstract</summary>
We propose PressureTransferNet, a novel method for Human Activity Recognition (HAR) using ground pressure information. Our approach generates body-specific dynamic ground pressure profiles for specific activities by leveraging existing pressure data from different individuals. PressureTransferNet is an encoder-decoder model taking a source pressure map and a target human attribute vector as inputs, producing a new pressure map reflecting the target attribute. To train the model, we use a sensor simulation to create a diverse dataset with various human attributes and pressure profiles. Evaluation on a real-world dataset shows its effectiveness in accurately transferring human attributes to ground pressure profiles across different scenarios. We visually confirm the fidelity of the synthesized pressure shapes using a physics-based deep learning model and achieve a binary R-square value of 0.79 on areas with ground contact. Validation through classification with F1 score (0.911$\pm$0.015) on physical pressure mat data demonstrates the correctness of the synthesized pressure maps, making our method valuable for data augmentation, denoising, sensor simulation, and anomaly detection. Applications span sports science, rehabilitation, and bio-mechanics, contributing to the development of HAR systems.
</details>
<details>
<summary>摘要</summary>
我们提出PressureTransferNet，一种新的人动作识别（HAR）方法，使用地面压力信息。我们的方法生成特定活动的体具动态地面压力 profilestring 从不同个体的压力数据中得到利用。PressureTransferNet是一个编码器-解码器模型，接受来源压力地图和目标人类特征向量作为输入，生成一个新的压力地图，反映目标特征。为了训练模型，我们使用了模拟器创建了多种人类特征和压力 profilestring 的多样化数据集。我们的评估表明，在不同enario中，PressureTransferNet可以准确地将人类特征传递到地面压力 profilestring。我们通过使用物理学习模型进行视觉验证，并在物理压力盘数据上获得了0.79的二元R-平方值，确认了我们生成的压力地图的准确性。我们的方法可以用于数据扩展、压力缓减、模拟器和异常检测等应用，广泛应用于体育科学、复习和生物机械等领域，为HAR系统的发展提供了价值。
</details></li>
</ul>
<hr>
<h2 id="Visual-attention-information-can-be-traced-on-cortical-response-but-not-on-the-retina-evidence-from-electrophysiological-mouse-data-using-natural-images-as-stimuli"><a href="#Visual-attention-information-can-be-traced-on-cortical-response-but-not-on-the-retina-evidence-from-electrophysiological-mouse-data-using-natural-images-as-stimuli" class="headerlink" title="Visual attention information can be traced on cortical response but not on the retina: evidence from electrophysiological mouse data using natural images as stimuli"></a>Visual attention information can be traced on cortical response but not on the retina: evidence from electrophysiological mouse data using natural images as stimuli</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00526">http://arxiv.org/abs/2308.00526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikos Melanitis, Konstantina Nikita</li>
<li>for:  investigate the biological basis of visual attention</li>
<li>methods: analyze retinal and cortical electrophysiological data from mouse, use Natural Images as visual stimuli</li>
<li>results: a subset of V1 neurons respond differently to salient vs. non-salient visual regions, visual attention information not traced in retinal response, cortical response modulated to interpret visual attention informationHere are the three points in English for comparison:</li>
<li>for: investigate the biological basis of visual attention</li>
<li>methods: analyze retinal and cortical electrophysiological data from mouse, use Natural Images as visual stimuli</li>
<li>results: a subset of V1 neurons respond differently to salient vs. non-salient visual regions, visual attention information not traced in retinal response, cortical response modulated to interpret visual attention information<details>
<summary>Abstract</summary>
Visual attention forms the basis of understanding the visual world. In this work we follow a computational approach to investigate the biological basis of visual attention. We analyze retinal and cortical electrophysiological data from mouse. Visual Stimuli are Natural Images depicting real world scenes. Our results show that in primary visual cortex (V1), a subset of around $10\%$ of the neurons responds differently to salient versus non-salient visual regions. Visual attention information was not traced in retinal response. It appears that the retina remains naive concerning visual attention; cortical response gets modulated to interpret visual attention information. Experimental animal studies may be designed to further explore the biological basis of visual attention we traced in this study. In applied and translational science, our study contributes to the design of improved visual prostheses systems -- systems that create artificial visual percepts to visually impaired individuals by electronic implants placed on either the retina or the cortex.
</details>
<details>
<summary>摘要</summary>
视觉注意力是视觉理解的基础。在这项工作中，我们采用计算方法调查生物基础的视觉注意力。我们分析鼠脑和脊梗电physiological数据。视觉刺激是自然图像，显示实际场景。我们的结果表明，在初级视觉层（V1）中，约10%的神经元在关键 versus 非关键视觉区域响应不同。视觉注意力信息不在视网膜响应中追踪。似乎视网膜对视觉注意力保持无知； cortical response 被修饰以解释视觉注意力信息。可以通过动物实验进一步探索我们在这项研究中跟踪的生物基础。在应用和翻译科学中，我们的研究对设计改进的视觉 prostheses 系统做出了贡献，这些系统通过电子设备置于视网膜或大脑中，为视力障碍者创造人工视觉感受。
</details></li>
</ul>
<hr>
<h2 id="Robust-Spatiotemporal-Fusion-of-Satellite-Images-A-Constrained-Convex-Optimization-Approach"><a href="#Robust-Spatiotemporal-Fusion-of-Satellite-Images-A-Constrained-Convex-Optimization-Approach" class="headerlink" title="Robust Spatiotemporal Fusion of Satellite Images: A Constrained Convex Optimization Approach"></a>Robust Spatiotemporal Fusion of Satellite Images: A Constrained Convex Optimization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00500">http://arxiv.org/abs/2308.00500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Isono, Kazuki Naganuma, Shunsuke Ono</li>
<li>for: 这种论文是为了提出一种新的卫星图像空间时间融合框架（ROSTF），以解决卫星图像的空间时间分解问题。</li>
<li>methods: 这种方法使用了一种新的优化算法，可以减少噪声的影响，并提高图像的分辨率。</li>
<li>results: 实验结果显示，ROSTF可以在噪声存在的情况下表现比较好，并且在一些实际数据上的表现也是比较好的。Here’s the breakdown of each point in more detail:</li>
<li>for: The paper is proposing a new framework for satellite image spatiotemporal fusion, called Robust Optimization-based Spatiotemporal Fusion (ROSTF). This is to address the trade-off between spatial and temporal resolution in satellite images.</li>
<li>methods: The proposed method uses a new optimization algorithm that can reduce the impact of noise and improve the resolution of the images.</li>
<li>results: The experimental results show that ROSTF performs well even in noisy conditions, and outperforms some state-of-the-art methods in noisy cases.<details>
<summary>Abstract</summary>
This paper proposes a novel spatiotemporal (ST) fusion framework for satellite images, named Robust Optimization-based Spatiotemporal Fusion (ROSTF). ST fusion is a promising approach to resolve a trade-off between the temporal and spatial resolution of satellite images. Although many ST fusion methods have been proposed, most of them are not designed to explicitly account for noise in observed images, despite the inevitable influence of noise caused by the measurement equipment and environment. Our ROSTF addresses this challenge by treating the noise removal of the observed images and the estimation of the target high-resolution image as a single optimization problem. Specifically, first, we define observation models for satellite images possibly contaminated with random noise, outliers, and/or missing values, and then introduce certain assumptions that would naturally hold between the observed images and the target high-resolution image. Then, based on these models and assumptions, we formulate the fusion problem as a constrained optimization problem and develop an efficient algorithm based on a preconditioned primal-dual splitting method for solving the problem. The performance of ROSTF was verified using simulated and real data. The results show that ROSTF performs comparably to several state-of-the-art ST fusion methods in noiseless cases and outperforms them in noisy cases.
</details>
<details>
<summary>摘要</summary>
Most existing methods for spatiotemporal fusion do not take into account the noise in the observed images, but ROSTF addresses this by treating noise removal and image estimation as a single optimization problem. This is done by defining models for observed images that may be contaminated with random noise, outliers, and/or missing values, and then using these models to formulate the fusion problem as a constrained optimization problem. An efficient algorithm based on a preconditioned primal-dual splitting method is then used to solve this problem.The performance of ROSTF was tested using both simulated and real data, and the results showed that it performed well compared to other state-of-the-art methods in noiseless cases, and outperformed them in noisy cases.
</details></li>
</ul>
<hr>
<h2 id="An-L2-Normalized-Spatial-Attention-Network-For-Accurate-And-Fast-Classification-Of-Brain-Tumors-In-2D-T1-Weighted-CE-MRI-Images"><a href="#An-L2-Normalized-Spatial-Attention-Network-For-Accurate-And-Fast-Classification-Of-Brain-Tumors-In-2D-T1-Weighted-CE-MRI-Images" class="headerlink" title="An L2-Normalized Spatial Attention Network For Accurate And Fast Classification Of Brain Tumors In 2D T1-Weighted CE-MRI Images"></a>An L2-Normalized Spatial Attention Network For Accurate And Fast Classification Of Brain Tumors In 2D T1-Weighted CE-MRI Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00491">http://arxiv.org/abs/2308.00491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliadietlmeier/mri_image_classification">https://github.com/juliadietlmeier/mri_image_classification</a></li>
<li>paper_authors: Grace Billingsley, Julia Dietlmeier, Vivek Narayanaswamy, Andreas Spanias, Noel E. OConnor</li>
<li>for: 这篇论文targets at developing an accurate and fast classification network for brain tumor classification in MRI images.</li>
<li>methods: 该论文提出了一种使用l2-normalized spatial attention mechanism的基eline网络，并对比了该网络与当前最佳方法的性能。</li>
<li>results: 该论文在一个2D T1-weighted CE-MRI数据集上测试了该模型，并获得了与当前最佳方法相比的1.79%的性能提升。<details>
<summary>Abstract</summary>
We propose an accurate and fast classification network for classification of brain tumors in MRI images that outperforms all lightweight methods investigated in terms of accuracy. We test our model on a challenging 2D T1-weighted CE-MRI dataset containing three types of brain tumors: Meningioma, Glioma and Pituitary. We introduce an l2-normalized spatial attention mechanism that acts as a regularizer against overfitting during training. We compare our results against the state-of-the-art on this dataset and show that by integrating l2-normalized spatial attention into a baseline network we achieve a performance gain of 1.79 percentage points. Even better accuracy can be attained by combining our model in an ensemble with the pretrained VGG16 at the expense of execution speed. Our code is publicly available at https://github.com/juliadietlmeier/MRI_image_classification
</details>
<details>
<summary>摘要</summary>
我们提出了一种准确和快速的分类网络，用于分类MRI图像中的脑肿瘤，超过所有轻量级方法的准确性。我们在一个复杂的2D T1束缚CE-MRI数据集上测试了我们的模型，该数据集包含三种脑肿瘤：膝盖肿瘤、 Glioma 和 hypophyseal。我们引入了L2正规化的空间注意力机制，以防止过拟合 durante 训练。我们对这个数据集进行比较，并显示了在我们的模型中集成L2正规化空间注意力机制后，与状态元的性能提高1.79个百分点。甚至可以通过将我们的模型与预训练的VGG16 ensemble来提高准确性，但是会降低执行速度。我们的代码可以在https://github.com/juliadietlmeier/MRI_image_classification 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Approach-for-Virtual-Contrast-Enhancement-in-Contrast-Enhanced-Spectral-Mammography"><a href="#A-Deep-Learning-Approach-for-Virtual-Contrast-Enhancement-in-Contrast-Enhanced-Spectral-Mammography" class="headerlink" title="A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography"></a>A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00471">http://arxiv.org/abs/2308.00471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aurora Rofena, Valerio Guarrasi, Marina Sarli, Claudia Lucia Piccolo, Matteo Sammarra, Bruno Beomonte Zobel, Paolo Soda</li>
<li>for: 这个研究旨在使用深度生成模型进行CESM中的虚拟增强，以提高诊断精度并降低辐射剂量。</li>
<li>methods: 这个研究使用了autoencoder和两个生成对抗网络（Pix2Pix和CycleGAN）来生成虚拟重组图像， solely from low-energy images。</li>
<li>results: 研究结果表明，CycleGAN是最有批处的深度网络来生成虚拟重组图像， highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field。<details>
<summary>Abstract</summary>
Contrast Enhanced Spectral Mammography (CESM) is a dual-energy mammographic imaging technique that first needs intravenously administration of an iodinated contrast medium; then, it collects both a low-energy image, comparable to standard mammography, and a high-energy image. The two scans are then combined to get a recombined image showing contrast enhancement. Despite CESM diagnostic advantages for breast cancer diagnosis, the use of contrast medium can cause side effects, and CESM also beams patients with a higher radiation dose compared to standard mammography. To address these limitations this work proposes to use deep generative models for virtual contrast enhancement on CESM, aiming to make the CESM contrast-free as well as to reduce the radiation dose. Our deep networks, consisting of an autoencoder and two Generative Adversarial Networks, the Pix2Pix, and the CycleGAN, generate synthetic recombined images solely from low-energy images. We perform an extensive quantitative and qualitative analysis of the model's performance, also exploiting radiologists' assessments, on a novel CESM dataset that includes 1138 images that, as a further contribution of this work, we make publicly available. The results show that CycleGAN is the most promising deep network to generate synthetic recombined images, highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field.
</details>
<details>
<summary>摘要</summary>
增强成像乳腺护肤（CESM）是一种双能量乳腺成像技术，需要先行静脉输液iodinated contrast媒体，然后收集低能量图像和高能量图像。这两个扫描后将图像合并，获得增强图像。 DESPITE CESM的诊断优势，使用contrast媒体可能会导致副作用，而且CESM也会对病人辐射更高的辐射剂量比标准乳腺成像。为了解决这些限制，本工作提出使用深度生成模型对CESM进行虚拟增强，以实现无contrast和辐射剂量的CESM。我们的深度网络由自适应网络和两个生成对抗网络组成，包括Pix2Pix和CycleGAN。这些网络可以将低能量图像转换成增强图像。我们对新的CESM数据集进行了广泛的量化和质量分析，并利用了Radiologists的评估，包括1138个图像，这也是本工作的一个新贡献。结果表明CycleGAN是最有前途的深度网络，生成增强图像， highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field。
</details></li>
</ul>
<hr>
<h2 id="Space-Debris-Are-Deep-Learning-based-Image-Enhancements-part-of-the-Solution"><a href="#Space-Debris-Are-Deep-Learning-based-Image-Enhancements-part-of-the-Solution" class="headerlink" title="Space Debris: Are Deep Learning-based Image Enhancements part of the Solution?"></a>Space Debris: Are Deep Learning-based Image Enhancements part of the Solution?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00408">http://arxiv.org/abs/2308.00408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Jamrozik, Vincent Gaudillière, Mohamed Adel Musallam, Djamila Aouada<br>for: 这个研究的目的是研究深度神经网络（DNN）解决在视觉光谱下摄取图像时遗留的问题，以实现资产保护。methods: 这个研究使用了一种混合的UNet-ResNet34深度学习（DL）架构，这个架构在ImageNet dataset上进行预训，并且处理了对照光谱下的图像恶化，包括模糊、曝光问题、低比例和噪音。results: 根据视觉检查，这个UNet模型能够正确地更正在太空中摄取的图像恶化，并且与现有的深度学习图像改善方法进行比较。<details>
<summary>Abstract</summary>
The volume of space debris currently orbiting the Earth is reaching an unsustainable level at an accelerated pace. The detection, tracking, identification, and differentiation between orbit-defined, registered spacecraft, and rogue/inactive space ``objects'', is critical to asset protection. The primary objective of this work is to investigate the validity of Deep Neural Network (DNN) solutions to overcome the limitations and image artefacts most prevalent when captured with monocular cameras in the visible light spectrum. In this work, a hybrid UNet-ResNet34 Deep Learning (DL) architecture pre-trained on the ImageNet dataset, is developed. Image degradations addressed include blurring, exposure issues, poor contrast, and noise. The shortage of space-generated data suitable for supervised DL is also addressed. A visual comparison between the URes34P model developed in this work and the existing state of the art in deep learning image enhancement methods, relevant to images captured in space, is presented. Based upon visual inspection, it is determined that our UNet model is capable of correcting for space-related image degradations and merits further investigation to reduce its computational complexity.
</details>
<details>
<summary>摘要</summary>
地球轨道上的空间废墟量目前已达到不可持续的水平，速度加剧。检测、跟踪、识别和区分在轨道上定义的注册空间craft和偏离/不活的空间“物体”是核心的。本工作的主要目标是调查深度神经网络（DNN）解决方案是否能够超越单光学pectrum中的限制和图像artefacts。在这种工作中，我们开发了一种混合的UNet-ResNet34深度学习（DL）架构，预训练在ImageNet dataset上。处理的图像问题包括模糊、曝光问题、低对比度和噪声。由于空间数据的短缺，我们也解决了深度学习的supervised数据不充分问题。在这种情况下，我们对URes34P模型进行了视觉比较，与现有的深度学习图像更新方法进行了比较。根据视觉检查，我们的UNet模型能够更正空间相关的图像异常，并且值得进一步的研究以降低计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Metrics-to-Quantify-Global-Consistency-in-Synthetic-Medical-Images"><a href="#Metrics-to-Quantify-Global-Consistency-in-Synthetic-Medical-Images" class="headerlink" title="Metrics to Quantify Global Consistency in Synthetic Medical Images"></a>Metrics to Quantify Global Consistency in Synthetic Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00402">http://arxiv.org/abs/2308.00402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Scholz, Benedikt Wiestler, Daniel Rueckert, Martin J. Menten</li>
<li>for: The paper is written for medical image processing, specifically for data augmentation and inter-modality image translation.</li>
<li>methods: The paper introduces two metrics for measuring the global consistency of synthetic images on a per-image basis. These metrics use supervised and self-supervised trained neural networks to predict and compare explicit and implicit attributes of images on patches.</li>
<li>results: The paper demonstrates that predicting explicit attributes of synthetic images on patches can distinguish globally consistent from inconsistent images. Implicit representations of images are less sensitive to assess global consistency but are still serviceable when labeled data is unavailable. The method is shown to be more effective in measuring global consistency compared to established metrics such as the FID.<details>
<summary>Abstract</summary>
Image synthesis is increasingly being adopted in medical image processing, for example for data augmentation or inter-modality image translation. In these critical applications, the generated images must fulfill a high standard of biological correctness. A particular requirement for these images is global consistency, i.e an image being overall coherent and structured so that all parts of the image fit together in a realistic and meaningful way. Yet, established image quality metrics do not explicitly quantify this property of synthetic images. In this work, we introduce two metrics that can measure the global consistency of synthetic images on a per-image basis. To measure the global consistency, we presume that a realistic image exhibits consistent properties, e.g., a person's body fat in a whole-body MRI, throughout the depicted object or scene. Hence, we quantify global consistency by predicting and comparing explicit attributes of images on patches using supervised trained neural networks. Next, we adapt this strategy to an unlabeled setting by measuring the similarity of implicit image features predicted by a self-supervised trained network. Our results demonstrate that predicting explicit attributes of synthetic images on patches can distinguish globally consistent from inconsistent images. Implicit representations of images are less sensitive to assess global consistency but are still serviceable when labeled data is unavailable. Compared to established metrics, such as the FID, our method can explicitly measure global consistency on a per-image basis, enabling a dedicated analysis of the biological plausibility of single synthetic images.
</details>
<details>
<summary>摘要</summary>
医学图像处理中的图像生成技术在不断推广应用，例如数据增强或多Modalities图像翻译。在这些关键应用中，生成的图像必须满足高水平的生物可靠性。特别是，生成图像的全局一致性是一项关键要求，即图像整体准确和结构化，所有图像部分都必须在真实和有意义的方式相匹配。然而，现有的图像质量指标并不直接量化这种图像的全局一致性。在这项工作中，我们提出了两种可以测量生成图像的全局一致性的指标。为了测量全局一致性，我们假设一个真实的图像在整个物体或场景中都应该具有一致的属性，例如整个人体的脂肪在整个全身MRI中。因此，我们量化全局一致性的方式是通过使用训练过的神经网络预测和比较图像中的明确属性。然后，我们将这种策略应用到无标注 Setting中，通过测量图像中隐藏的特征的相似度来衡量图像的全局一致性。我们的结果表明，预测图像中的明确属性可以分辨全局一致性的图像和不一致性的图像。而隐藏的图像特征也可以用无标注数据来衡量全局一致性，尽管其敏感度较低。与已有的指标，如FID，相比，我们的方法可以直接量化全局一致性，从而启用专门分析单个生成图像的生物可靠性。
</details></li>
</ul>
<hr>
<h2 id="Fundus-Enhanced-Disease-Aware-Distillation-Model-for-Retinal-Disease-Classification-from-OCT-Images"><a href="#Fundus-Enhanced-Disease-Aware-Distillation-Model-for-Retinal-Disease-Classification-from-OCT-Images" class="headerlink" title="Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images"></a>Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00291">http://arxiv.org/abs/2308.00291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/fddm">https://github.com/xmed-lab/fddm</a></li>
<li>paper_authors: Lehan Wang, Weihang Dai, Mei Jin, Chubin Ou, Xiaomeng Li</li>
<li>for: 这个论文旨在提出一种基于Optical Coherence Tomography（OCT）的新型和有效的眼科检测工具。</li>
<li>methods: 该方法使用多模态学习，将有限的OCT数据与更多的背景图像相结合，以提高眼科检测的准确率。</li>
<li>results: 实验结果表明，我们提出的方法可以在眼科检测中提高准确率，并且比单模态、多模态和状态对抗学习方法更高效。<details>
<summary>Abstract</summary>
Optical Coherence Tomography (OCT) is a novel and effective screening tool for ophthalmic examination. Since collecting OCT images is relatively more expensive than fundus photographs, existing methods use multi-modal learning to complement limited OCT data with additional context from fundus images. However, the multi-modal framework requires eye-paired datasets of both modalities, which is impractical for clinical use. To address this problem, we propose a novel fundus-enhanced disease-aware distillation model (FDDM), for retinal disease classification from OCT images. Our framework enhances the OCT model during training by utilizing unpaired fundus images and does not require the use of fundus images during testing, which greatly improves the practicality and efficiency of our method for clinical use. Specifically, we propose a novel class prototype matching to distill disease-related information from the fundus model to the OCT model and a novel class similarity alignment to enforce consistency between disease distribution of both modalities. Experimental results show that our proposed approach outperforms single-modal, multi-modal, and state-of-the-art distillation methods for retinal disease classification. Code is available at https://github.com/xmed-lab/FDDM.
</details>
<details>
<summary>摘要</summary>
优化减相干扫描技术（OCT）是一种新型有效的诊断工具 для眼科检查。由于收集OCT图像相对较为昂贵于眼图像，现有的方法使用多Modal学习来补充OCT数据中的有限资源。然而，多Modal框架需要临床使用的眼球对应的数据集，这是不实际的。为解决这个问题，我们提出了一种新的眼球增强疾病感知模型（FDDM），用于从OCT图像中鉴别眼球疾病。我们的框架在训练时使用不匹配的眼球图像来增强OCT模型，并不需要在测试时使用眼球图像，这大大提高了我们的方法在临床使用的实用性和效率。具体来说，我们提出了一种新的疾病类型匹配来将疾病相关的信息从眼球模型传递给OCT模型，以及一种疾病分布一致的类型匹配来强制两种Modal中的疾病分布相似。实验结果表明，我们提出的方法在鉴别眼球疾病方面超过单Modal、多Modal和状态艺术的混合方法。代码可以在https://github.com/xmed-lab/FDDM上获取。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Self-Supervised-Image-Denoising-A-Comprehensive-Review"><a href="#Unleashing-the-Power-of-Self-Supervised-Image-Denoising-A-Comprehensive-Review" class="headerlink" title="Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review"></a>Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00247">http://arxiv.org/abs/2308.00247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Zhang, Fangfang Zhou, Yuanzhou Wei, Xiao Yang, Yuan Gu</li>
<li>for: 提供了一个全面的评论，探讨最新的自动训练图像减震方法，以及它们在实际应用中的效果。</li>
<li>methods: 分为三类：通用方法、BSN基于方法和Transformer基于方法。</li>
<li>results: 通过在不同的数据集上进行量化和质量上的实验，证明了这些方法的效果，并提供了相对比较的结果。<details>
<summary>Abstract</summary>
The advent of deep learning has brought a revolutionary transformation to image denoising techniques. However, the persistent challenge of acquiring noise-clean pairs for supervised methods in real-world scenarios remains formidable, necessitating the exploration of more practical self-supervised image denoising. This paper focuses on self-supervised image denoising methods that offer effective solutions to address this challenge. Our comprehensive review thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.
</details>
<details>
<summary>摘要</summary>
深度学习的出现对图像干扰技术带来了革命性的变革。然而，在实际场景中获得干扰级别对的训练数据仍然是一个挑战，需要更加实用的自监学习图像干扰。这篇论文将关注自监学习图像干扰方法，以提供有效的解决方案。我们的全面回顾 thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.Here's the translation in Traditional Chinese:深度学习的出现对图像干扰技术带来了革命性的变革。然而，在实际场景中获得干扰级别对的训练数据仍然是一个挑战，需要更加实用的自监学习图像干扰。这篇论文将关注自监学习图像干扰方法，以提供有效的解决方案。我们的全面回顾 thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.
</details></li>
</ul>
<hr>
<h2 id="Boundary-Difference-Over-Union-Loss-For-Medical-Image-Segmentation"><a href="#Boundary-Difference-Over-Union-Loss-For-Medical-Image-Segmentation" class="headerlink" title="Boundary Difference Over Union Loss For Medical Image Segmentation"></a>Boundary Difference Over Union Loss For Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00220">http://arxiv.org/abs/2308.00220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunfan-bvb/boundarydouloss">https://github.com/sunfan-bvb/boundarydouloss</a></li>
<li>paper_authors: Fan Sun, Zhiming Luo, Shaozi Li</li>
<li>for: 这个论文是为了解决医疗图像分割中的边界区域分 segmentation问题，目前的损失函数主要是关注总体分 segmentation结果，有 fewer 的损失函数用于导航边界分 segmentation。</li>
<li>methods: 我们提出了一种简单有效的损失函数，即Boundary Difference over Union Loss (Boundary DoU Loss)，它通过计算预测和真实值之间的差异集的比率来导航边界区域分 segmentation。我们的损失函数仅仅基于区域计算，易于实现并在训练中稳定不需要其他损失函数。此外，我们使用目标大小进行适应性调整对边界区域应用注意力。</li>
<li>results: 我们在ACDC和Synapse两个数据集上使用UNet、TransUNet和Swin-UNet进行实验，得到了我们提出的损失函数的效果。<details>
<summary>Abstract</summary>
Medical image segmentation is crucial for clinical diagnosis. However, current losses for medical image segmentation mainly focus on overall segmentation results, with fewer losses proposed to guide boundary segmentation. Those that do exist often need to be used in combination with other losses and produce ineffective results. To address this issue, we have developed a simple and effective loss called the Boundary Difference over Union Loss (Boundary DoU Loss) to guide boundary region segmentation. It is obtained by calculating the ratio of the difference set of prediction and ground truth to the union of the difference set and the partial intersection set. Our loss only relies on region calculation, making it easy to implement and training stable without needing any additional losses. Additionally, we use the target size to adaptively adjust attention applied to the boundary regions. Experimental results using UNet, TransUNet, and Swin-UNet on two datasets (ACDC and Synapse) demonstrate the effectiveness of our proposed loss function. Code is available at https://github.com/sunfan-bvb/BoundaryDoULoss.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是诊断中非常重要的一环。然而，目前的医疗图像分割损失主要关注整体分割结果，有少数损失用于导引边缘分割。这些损失经常需要与其他损失结合使用，并且会生成不效果。为解决这个问题，我们已经开发了一种简单有效的损失函数：Boundary Difference over Union Loss（Boundary DoU Loss），用于导引边缘区域分割。它是计算预测和真实值之间差异集的比率，并将其与union集和partial intersection集进行比较。我们的损失函数仅仅基于区域计算，因此易于实现和训练，不需要其他损失函数。此外，我们使用目标大小来适应性地调整边缘区域的注意力。实验结果表明，使用UNet、TransUNet和Swin-UNet在ACDC和Synapse两个数据集上，我们提出的损失函数能够准确地导引边缘区域的分割。代码可以在https://github.com/sunfan-bvb/BoundaryDoULoss中找到。
</details></li>
</ul>
<hr>
<h2 id="Universal-Adversarial-Defense-in-Remote-Sensing-Based-on-Pre-trained-Denoising-Diffusion-Models"><a href="#Universal-Adversarial-Defense-in-Remote-Sensing-Based-on-Pre-trained-Denoising-Diffusion-Models" class="headerlink" title="Universal Adversarial Defense in Remote Sensing Based on Pre-trained Denoising Diffusion Models"></a>Universal Adversarial Defense in Remote Sensing Based on Pre-trained Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16865">http://arxiv.org/abs/2307.16865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EricYu97/UAD-RS">https://github.com/EricYu97/UAD-RS</a></li>
<li>paper_authors: Weikang Yu, Yonghao Xu, Pedram Ghamisi<br>for:这个论文的目的是提出一种通用的防御方法，以防止深度神经网络在遥感应用中受到攻击。methods:这种方法使用预训练的扩散模型来防止多种未知的攻击，并使用反向和前向过程来纯化攻击样本。results:实验结果表明，这种方法可以在四个不同的遥感数据集上达到高性能，并且可以 universally 防止七种常见的攻击方式。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have achieved tremendous success in many remote sensing (RS) applications, in which DNNs are vulnerable to adversarial perturbations. Unfortunately, current adversarial defense approaches in RS studies usually suffer from performance fluctuation and unnecessary re-training costs due to the need for prior knowledge of the adversarial perturbations among RS data. To circumvent these challenges, we propose a universal adversarial defense approach in RS imagery (UAD-RS) using pre-trained diffusion models to defend the common DNNs against multiple unknown adversarial attacks. Specifically, the generative diffusion models are first pre-trained on different RS datasets to learn generalized representations in various data domains. After that, a universal adversarial purification framework is developed using the forward and reverse process of the pre-trained diffusion models to purify the perturbations from adversarial samples. Furthermore, an adaptive noise level selection (ANLS) mechanism is built to capture the optimal noise level of the diffusion model that can achieve the best purification results closest to the clean samples according to their Frechet Inception Distance (FID) in deep feature space. As a result, only a single pre-trained diffusion model is needed for the universal purification of adversarial samples on each dataset, which significantly alleviates the re-training efforts and maintains high performance without prior knowledge of the adversarial perturbations. Experiments on four heterogeneous RS datasets regarding scene classification and semantic segmentation verify that UAD-RS outperforms state-of-the-art adversarial purification approaches with a universal defense against seven commonly existing adversarial perturbations. Codes and the pre-trained models are available online (https://github.com/EricYu97/UAD-RS).
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在远程感知（RS）应用中已经取得了很大的成功，但是DNN受到了抗击扰动的威胁。然而，RS中的抗击防御策略通常会受到性能波动和不必要的重新训练成本，因为需要对RS数据有严格的先知知识。为了解决这些挑战，我们提出了RS中的通用抗击防御策略（UAD-RS），使用预训练的扩散模型来防御RS数据中的多种未知抗击攻击。具体来说，首先预训练了不同RS数据集上的扩散模型，以学习各种数据域的通用表示。然后，我们使用扩散模型的前向和反向过程来纯化抗击样本中的扰动。此外，我们还构建了自适应噪声水平选择（ANLS）机制，以捕捉最佳的噪声水平，使扩散模型可以 closest to clean samples according to their Frechet Inception Distance（FID） in deep feature space achieve the best purification results。结果是，只需要一个预训练的扩散模型，可以对每个数据集进行通用的纯化，大大减少了重新训练的努力和维护高性能，无需对抗击攻击有严格的先知知识。实验表明，UAD-RS比状态前的抗击纯化方法更好，对七种常见的抗击攻击进行通用防御。代码和预训练模型可以在线获取（https://github.com/EricYu97/UAD-RS）。
</details></li>
</ul>
<hr>
<h2 id="A-comprehensive-review-of-deep-learning-in-lung-cancer"><a href="#A-comprehensive-review-of-deep-learning-in-lung-cancer" class="headerlink" title="A comprehensive review of deep learning in lung cancer"></a>A comprehensive review of deep learning in lung cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02528">http://arxiv.org/abs/2308.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzane Tajidini</li>
<li>for: 本文提供了 cancer 诊断方法的历史背景，包括肿瘤诊断过程和临床医生使用的标准分类方法。</li>
<li>methods: 文章讨论了现有的肿瘤诊断方法，并指出了这些方法的缺陷，强调了需要新的、更智能的方法。</li>
<li>results: 文章未提出结果，但预示了需要更好的肿瘤诊断方法。<details>
<summary>Abstract</summary>
To provide the reader with a historical perspective on cancer classification approaches, we first discuss the fundamentals of the area of cancer diagnosis in this article, including the processes of cancer diagnosis and the standard classification methods employed by clinicians. Current methods for cancer diagnosis are deemed ineffective, calling for new and more intelligent approaches.
</details>
<details>
<summary>摘要</summary>
为了为读者提供 cancer 诊断方法的历史背景，我们首先介绍了 cancer 诊断领域的基本知识，包括肿瘤诊断过程和临床医生常用的标准分类方法。现有的肿瘤诊断方法被认为是不够有效，需要新的更智能的方法。Here's a word-for-word translation of the text using Traditional Chinese characters:为了为读者提供肿瘤诊断方法的历史背景，我们首先介绍了肿瘤诊断领域的基本知识，包括肿瘤诊断过程和临床医生常用的标准分类方法。现有的肿瘤诊断方法被认为是不够有效，需要新的更智能的方法。
</details></li>
</ul>
<hr>
<h2 id="Framing-image-registration-as-a-landmark-detection-problem-for-better-representation-of-clinical-relevance"><a href="#Framing-image-registration-as-a-landmark-detection-problem-for-better-representation-of-clinical-relevance" class="headerlink" title="Framing image registration as a landmark detection problem for better representation of clinical relevance"></a>Framing image registration as a landmark detection problem for better representation of clinical relevance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01318">http://arxiv.org/abs/2308.01318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana Waldmannstetter, Benedikt Wiestler, Julian Schwarting, Ivan Ezhov, Marie Metz, Spyridon Bakas, Bhakti Baheti, Satrajit Chakrabarty, Jan S. Kirschke, Rolf A. Heckemann, Marie Piraud, Florian Kofler, Bjoern H. Menze</li>
<li>for: 这篇论文是为了重新评估图像注册方法的估值方法提出的。</li>
<li>methods: 论文使用了一种基于 landmark 检测的方法，通过对一些子样本进行人工分析来估算 landmark 特征的检测阈值。</li>
<li>results: 该方法可以提供不同的注册算法之间的区分，同时还可以评估图像注册算法的临床 significanc。<details>
<summary>Abstract</summary>
Nowadays, registration methods are typically evaluated based on sub-resolution tracking error differences. In an effort to reinfuse this evaluation process with clinical relevance, we propose to reframe image registration as a landmark detection problem. Ideally, landmark-specific detection thresholds are derived from an inter-rater analysis. To approximate this costly process, we propose to compute hit rate curves based on the distribution of errors of a sub-sample inter-rater analysis. Therefore, we suggest deriving thresholds from the error distribution using the formula: median + delta * median absolute deviation. The method promises differentiation of previously indistinguishable registration algorithms and further enables assessing the clinical significance in algorithm development.
</details>
<details>
<summary>摘要</summary>
现在，注册方法通常被评估基于微尺度跟踪错误差异。为了重新把注册评估过程抽象到临床 relevance，我们提议将注册视为一个标记检测问题。理想情况下，标记特定的检测阈值来自间评分析。为了 aproximate这个costly процесс，我们提议根据一个子样本间评分析的错误分布计算hit rate曲线。因此，我们建议使用错误分布中的 median + delta * median absolute deviation来 derive thresholds。这种方法可以区分之前无法分辨的注册算法，并且可以评估算法开发中的临床重要性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/01/eess.IV_2023_08_01/" data-id="clly4xtg400ejvl88c9jy6c6o" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/01/eess.AS_2023_08_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-08-01 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/07/31/cs.LG_2023_07_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-07-31 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">108</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
