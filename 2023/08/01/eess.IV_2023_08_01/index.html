
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-01 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03777 repo_url: None p">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-01">
<meta property="og:url" content="https://nullscc.github.io/2023/08/01/eess.IV_2023_08_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03777 repo_url: None p">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-01T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:43:24.574Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/01/eess.IV_2023_08_01/" class="article-date">
  <time datetime="2023-08-01T09:00:00.000Z" itemprop="datePublished">2023-08-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-01
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lab-in-a-Tube-A-portable-imaging-spectrophotometer-for-cost-effective-high-throughput-and-label-free-analysis-of-centrifugation-processes"><a href="#Lab-in-a-Tube-A-portable-imaging-spectrophotometer-for-cost-effective-high-throughput-and-label-free-analysis-of-centrifugation-processes" class="headerlink" title="Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes"></a>Lab-in-a-Tube: A portable imaging spectrophotometer for cost-effective, high-throughput, and label-free analysis of centrifugation processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03777">http://arxiv.org/abs/2308.03777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wei, Dehua Hu, Bijie Bai, Chenqi Meng, Tsz Kin Chan, Xing Zhao, Yuye Wang, Yi-Ping Ho, Wu Yuan, Ho-Pui Ho</li>
<li>for: 这个论文主要旨在为现代实验科学中的样本处理任务提供更高效的实时观察方法。</li>
<li>methods: 该论文使用了一种创新的Lab_in_a_Tube imaging spectrophotometer，具有实时图像分析和可编程中断功能。这个低于30美元的PORTABLE LIAT设备包含Wi Fi摄像头和活动关闭控制。</li>
<li>results: 该研究通过实验观察了在中心陀风中的单个粒子动态过程，并提供了实时数据图表。这是首次观察到类似现象。研究还发展了基于旋转参照framereference frame的理论模拟，与实验结果吻合良好。此外，研究还实现了第一次观察血液凝固过程的视觉化。<details>
<summary>Abstract</summary>
Centrifuges serve as essential instruments in modern experimental sciences, facilitating a wide range of routine sample processing tasks that necessitate material sedimentation. However, the study for real time observation of the dynamical process during centrifugation has remained elusive. In this study, we developed an innovative Lab_in_a_Tube imaging spectrophotometer that incorporates capabilities of real time image analysis and programmable interruption. This portable LIAT device costs less than 30 US dollars. Based on our knowledge, it is the first Wi Fi camera built_in in common lab centrifuges with active closed_loop control. We tested our LIAT imaging spectrophotometer with solute solvent interaction investigation obtained from lab centrifuges with quantitative data plotting in a real time manner. Single re circulating flow was real time observed, forming the ring shaped pattern during centrifugation. To the best of our knowledge, this is the very first observation of similar phenomena. We developed theoretical simulations for the single particle in a rotating reference frame, which correlated well with experimental results. We also demonstrated the first demonstration to visualize the blood sedimentation process in clinical lab centrifuges. This remarkable cost effectiveness opens up exciting opportunities for centrifugation microbiology research and paves the way for the creation of a network of computational imaging spectrometers at an affordable price for large scale and continuous monitoring of centrifugal processes in general.
</details>
<details>
<summary>摘要</summary>
优化的中心分离器在现代实验科学中扮演着重要的 instrumente 作用，为各种实验样品处理任务提供了一系列的标准化程序。然而，实时观察中心分离过程中的动态过程的研究仍然是一个挑战。本研究中，我们开发了一种创新的 Lab_in_a_Tube 成像спектрофотометр，它 integrate了实时图像分析和可编程中断功能。这个可以在30美元以下的PORTABLE LIAT设备中实现。据我们所知，这是首个在通用实验室中心分离器上 integrate了Wi Fi摄像头和活动封闭控制的设备。我们使用LIAT成像спектрофотомет器测试了各种溶剂与溶解物之间的反应，并在实时方式提供了数据图表。在中心分离过程中，实时观察到单个回流形成了环形模式。我们认为这是首次观察到类似现象。我们还开发了基于旋转参照框架的理论模拟，与实验结果高度相关。此外，我们还实现了首次visualize血液凝固过程的示例。这种可负担的成本开发了一个便宜的计算成像спектрофотомет器网络，为大规模和不间断的中心分离过程监测创造了可能性。
</details></li>
</ul>
<hr>
<h2 id="PressureTransferNet-Human-Attribute-Guided-Dynamic-Ground-Pressure-Profile-Transfer-using-3D-simulated-Pressure-Maps"><a href="#PressureTransferNet-Human-Attribute-Guided-Dynamic-Ground-Pressure-Profile-Transfer-using-3D-simulated-Pressure-Maps" class="headerlink" title="PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps"></a>PressureTransferNet: Human Attribute Guided Dynamic Ground Pressure Profile Transfer using 3D simulated Pressure Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00538">http://arxiv.org/abs/2308.00538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lala Shakti Swarup Ray, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz</li>
<li>for: 用于人体活动识别（HAR）领域，使用地面压力信息。</li>
<li>methods: 利用现有的压力数据，生成特定活动的体Specific动态地面压力profile。PressureTransferNet是一个encoder-decoder模型，输入源压力图和目标人类特征向量，输出新的压力图表示目标特征。</li>
<li>results: 通过使用感知 simulateulator创建多种人类特征和压力 profiles的数据集，并在实际世界数据集上进行评估，得到了准确地传递人类特征到地面压力 profiles的效果。通过物理学深度学习模型进行验证，并在物理压力检测数据上达到了0.79的二元R-square值和0.911$\pm$0.015的F1分数，证明了生成的压力图的正确性。<details>
<summary>Abstract</summary>
We propose PressureTransferNet, a novel method for Human Activity Recognition (HAR) using ground pressure information. Our approach generates body-specific dynamic ground pressure profiles for specific activities by leveraging existing pressure data from different individuals. PressureTransferNet is an encoder-decoder model taking a source pressure map and a target human attribute vector as inputs, producing a new pressure map reflecting the target attribute. To train the model, we use a sensor simulation to create a diverse dataset with various human attributes and pressure profiles. Evaluation on a real-world dataset shows its effectiveness in accurately transferring human attributes to ground pressure profiles across different scenarios. We visually confirm the fidelity of the synthesized pressure shapes using a physics-based deep learning model and achieve a binary R-square value of 0.79 on areas with ground contact. Validation through classification with F1 score (0.911$\pm$0.015) on physical pressure mat data demonstrates the correctness of the synthesized pressure maps, making our method valuable for data augmentation, denoising, sensor simulation, and anomaly detection. Applications span sports science, rehabilitation, and bio-mechanics, contributing to the development of HAR systems.
</details>
<details>
<summary>摘要</summary>
我们提出了PressureTransferNet，一种新的人体活动识别（HAR）方法，使用地面压力信息。我们的方法生成了特定活动的人体特有的动态地面压力轨迹，通过利用不同个体的压力数据。PressureTransferNet是一个Encoder-Decoder模型，接受来源压力地图和目标人类特征向量作为输入，生成一个新的压力地图，表现出目标特征。为了训练模型，我们使用了一种感器模拟，创建了具有不同人类特征和压力轨迹的多样化数据集。我们通过对真实数据进行评估，发现PressureTransferNet能够准确地将人类特征传递到地面压力轨迹中，并在不同场景下进行高精度的人体活动识别。我们通过使用物理学习模型进行视觉验证，并在物理压力检测数据上达到了0.79的二元共亮值，证明了生成的压力地图的正确性。我们还通过物理压力检测数据进行分类，并达到了0.911$\pm$0.015的F1分数，证明了生成的压力地图的正确性，使得我们的方法在数据增强、干扰除、感器模拟和异常检测等方面都可以获得极大的价值。应用范围包括体育科学、rehabilitation和生物机器人学，对人体活动识别系统的发展做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Visual-attention-information-can-be-traced-on-cortical-response-but-not-on-the-retina-evidence-from-electrophysiological-mouse-data-using-natural-images-as-stimuli"><a href="#Visual-attention-information-can-be-traced-on-cortical-response-but-not-on-the-retina-evidence-from-electrophysiological-mouse-data-using-natural-images-as-stimuli" class="headerlink" title="Visual attention information can be traced on cortical response but not on the retina: evidence from electrophysiological mouse data using natural images as stimuli"></a>Visual attention information can be traced on cortical response but not on the retina: evidence from electrophysiological mouse data using natural images as stimuli</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00526">http://arxiv.org/abs/2308.00526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikos Melanitis, Konstantina Nikita</li>
<li>for: 这个研究探讨了视觉注意力的生物基础。</li>
<li>methods: 研究者使用计算机方法研究了视觉注意力的生物基础，分析了鼠脑和眼球电生物数据。</li>
<li>results: 研究发现，在主视受器皮层（V1）中，约10%的神经元响应不同于突出性视觉区域。视觉注意力信息不在视网膜响应中追踪，retina似乎不知道视觉注意力信息。<details>
<summary>Abstract</summary>
Visual attention forms the basis of understanding the visual world. In this work we follow a computational approach to investigate the biological basis of visual attention. We analyze retinal and cortical electrophysiological data from mouse. Visual Stimuli are Natural Images depicting real world scenes. Our results show that in primary visual cortex (V1), a subset of around $10\%$ of the neurons responds differently to salient versus non-salient visual regions. Visual attention information was not traced in retinal response. It appears that the retina remains naive concerning visual attention; cortical response gets modulated to interpret visual attention information. Experimental animal studies may be designed to further explore the biological basis of visual attention we traced in this study. In applied and translational science, our study contributes to the design of improved visual prostheses systems -- systems that create artificial visual percepts to visually impaired individuals by electronic implants placed on either the retina or the cortex.
</details>
<details>
<summary>摘要</summary>
Visual attention是视觉世界理解的基础。在这项工作中，我们采用计算方法研究生物基础知识的视觉注意力。我们分析了鼠脊椎细胞电physiological数据，使用自然图像作为视觉刺激。我们的结果表明，在主视觉层（V1）中，约10%的神经元与突出性视觉区域不同响应。视觉注意力信息没有在视网膜响应中追踪。这表明视网膜对视觉注意力信息是无知的， cortical响应则被修饰以解析视觉注意力信息。我们可以通过进一步的动物实验来深入探索我们在这项研究中跟踪到的生物基础知识。在应用和翻译科学方面，我们的研究对于设计更好的视觉代做系统做出了贡献——这些系统通过电子导体在视网膜或大脑中引入人工视觉感受。
</details></li>
</ul>
<hr>
<h2 id="Robust-Spatiotemporal-Fusion-of-Satellite-Images-A-Constrained-Convex-Optimization-Approach"><a href="#Robust-Spatiotemporal-Fusion-of-Satellite-Images-A-Constrained-Convex-Optimization-Approach" class="headerlink" title="Robust Spatiotemporal Fusion of Satellite Images: A Constrained Convex Optimization Approach"></a>Robust Spatiotemporal Fusion of Satellite Images: A Constrained Convex Optimization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00500">http://arxiv.org/abs/2308.00500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryosuke Isono, Kazuki Naganuma, Shunsuke Ono</li>
<li>for: 提出了一种新的卫星图像空间时间融合框架（ROSTF），用于解决卫星图像的分辨率VS时间的负担问题。</li>
<li>methods: 我们使用了一种新的卫星图像融合方法，称为ROSTF，它可以减少噪声的影响，并提高卫星图像的分辨率。</li>
<li>results: 我们的实验结果表明，ROSTF可以与其他State-of-the-art ST融合方法相比，在噪声情况下表现更好，并且在噪声较高的情况下表现更出色。<details>
<summary>Abstract</summary>
This paper proposes a novel spatiotemporal (ST) fusion framework for satellite images, named Robust Optimization-based Spatiotemporal Fusion (ROSTF). ST fusion is a promising approach to resolve a trade-off between the temporal and spatial resolution of satellite images. Although many ST fusion methods have been proposed, most of them are not designed to explicitly account for noise in observed images, despite the inevitable influence of noise caused by the measurement equipment and environment. Our ROSTF addresses this challenge by treating the noise removal of the observed images and the estimation of the target high-resolution image as a single optimization problem. Specifically, first, we define observation models for satellite images possibly contaminated with random noise, outliers, and/or missing values, and then introduce certain assumptions that would naturally hold between the observed images and the target high-resolution image. Then, based on these models and assumptions, we formulate the fusion problem as a constrained optimization problem and develop an efficient algorithm based on a preconditioned primal-dual splitting method for solving the problem. The performance of ROSTF was verified using simulated and real data. The results show that ROSTF performs comparably to several state-of-the-art ST fusion methods in noiseless cases and outperforms them in noisy cases.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出了一种新的协调空间时间（ST）融合框架，称为Robust Optimization-based Spatiotemporal Fusion（ROSTF）。ST融合是一种有前途的方法，可以解决遥感图像的时空分解问题。虽然许多ST融合方法已经被提出，但大多数它们并不直接考虑观测图像中的噪声，即由测量设备和环境所引起的噪声。我们的ROSTF正是为了解决这个挑战，它将观测图像中的噪声除除和目标高分辨率图像的估计作为一个单一的优化问题进行处理。具体来说，我们首先定义了可能受到随机噪声、外围值和/或 missing value 的遥感图像的观测模型，然后引入了一些自然地存在于观测图像和目标高分辨率图像之间的假设。然后，根据这些模型和假设，我们将融合问题转化为一个受限的优化问题，并开发了一种高效的幂等分解法来解决问题。我们对ROSTF的性能进行了验证，使用了模拟和实际数据。结果表明，ROSTF在噪声不存在的情况下与一些状态先进的ST融合方法相当，在噪声存在的情况下表现更出色。
</details></li>
</ul>
<hr>
<h2 id="An-L2-Normalized-Spatial-Attention-Network-For-Accurate-And-Fast-Classification-Of-Brain-Tumors-In-2D-T1-Weighted-CE-MRI-Images"><a href="#An-L2-Normalized-Spatial-Attention-Network-For-Accurate-And-Fast-Classification-Of-Brain-Tumors-In-2D-T1-Weighted-CE-MRI-Images" class="headerlink" title="An L2-Normalized Spatial Attention Network For Accurate And Fast Classification Of Brain Tumors In 2D T1-Weighted CE-MRI Images"></a>An L2-Normalized Spatial Attention Network For Accurate And Fast Classification Of Brain Tumors In 2D T1-Weighted CE-MRI Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00491">http://arxiv.org/abs/2308.00491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliadietlmeier/mri_image_classification">https://github.com/juliadietlmeier/mri_image_classification</a></li>
<li>paper_authors: Grace Billingsley, Julia Dietlmeier, Vivek Narayanaswamy, Andreas Spanias, Noel E. OConnor</li>
<li>for: 这个研究是为了开发一个精确且快速的脑动脉摄影像分类网络，以提高现有的轻量级方法的精度。</li>
<li>methods: 我们使用了一种具有l2-normalized spatial attention的分类网络，以避免训练时的过滤。我们与现有的State-of-the-art方法进行比较，结果显示我们的模型在这个2D T1-weighted CE-MRI数据集上的表现较好，增加了1.79%的精度。</li>
<li>results: 我们的模型在这个数据集上的表现比现有的State-of-the-art方法更好，增加了1.79%的精度。组合我们的模型与预训的VGG16可以实现更高的精度，但是会增加执行速度的成本。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/juliadietlmeier/MRI_image_classification%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/juliadietlmeier/MRI_image_classification上获取。</a><details>
<summary>Abstract</summary>
We propose an accurate and fast classification network for classification of brain tumors in MRI images that outperforms all lightweight methods investigated in terms of accuracy. We test our model on a challenging 2D T1-weighted CE-MRI dataset containing three types of brain tumors: Meningioma, Glioma and Pituitary. We introduce an l2-normalized spatial attention mechanism that acts as a regularizer against overfitting during training. We compare our results against the state-of-the-art on this dataset and show that by integrating l2-normalized spatial attention into a baseline network we achieve a performance gain of 1.79 percentage points. Even better accuracy can be attained by combining our model in an ensemble with the pretrained VGG16 at the expense of execution speed. Our code is publicly available at https://github.com/juliadietlmeier/MRI_image_classification
</details>
<details>
<summary>摘要</summary>
我们提出一种精度快速的分类网络，用于分类MRI图像中的脑肿瘤，超过所有轻量级方法的准确性。我们在一个具有三种脑肿瘤（膜肿瘤、 Glioma 和丘脑）的2D T1束缚CE-MRI数据集上进行测试。我们引入l2正则化的空间注意力机制，以防止过拟合训练期间。我们与现状的最佳方法进行比较，并显示在将l2正则化空间注意力integrated into a baseline network时，得到了1.79个百分点的性能提升。可以通过将我们的模型与预训练的VGG16 ensemble来获得更高的准确性，但是这将导致执行速度下降。我们的代码在https://github.com/juliadietlmeier/MRI_image_classification上公开。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Approach-for-Virtual-Contrast-Enhancement-in-Contrast-Enhanced-Spectral-Mammography"><a href="#A-Deep-Learning-Approach-for-Virtual-Contrast-Enhancement-in-Contrast-Enhanced-Spectral-Mammography" class="headerlink" title="A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography"></a>A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00471">http://arxiv.org/abs/2308.00471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aurora Rofena, Valerio Guarrasi, Marina Sarli, Claudia Lucia Piccolo, Matteo Sammarra, Bruno Beomonte Zobel, Paolo Soda</li>
<li>for: 这个研究旨在使用深度生成模型来实现CESM中的虚拟对照增强，以提高诊断精度并减少辐射剂量。</li>
<li>methods: 这个研究使用了深度生成模型，包括自适应网络和两个生成对抗网络（Pix2Pix和CycleGAN），将低能量图像转换为虚拟重组图像。</li>
<li>results: 研究结果显示，CycleGAN是最有前途的深度网络，可以实现高质量的虚拟重组图像生成。这项研究的结果还表明，使用虚拟对照增强可以提高CESM的诊断精度，同时减少辐射剂量。<details>
<summary>Abstract</summary>
Contrast Enhanced Spectral Mammography (CESM) is a dual-energy mammographic imaging technique that first needs intravenously administration of an iodinated contrast medium; then, it collects both a low-energy image, comparable to standard mammography, and a high-energy image. The two scans are then combined to get a recombined image showing contrast enhancement. Despite CESM diagnostic advantages for breast cancer diagnosis, the use of contrast medium can cause side effects, and CESM also beams patients with a higher radiation dose compared to standard mammography. To address these limitations this work proposes to use deep generative models for virtual contrast enhancement on CESM, aiming to make the CESM contrast-free as well as to reduce the radiation dose. Our deep networks, consisting of an autoencoder and two Generative Adversarial Networks, the Pix2Pix, and the CycleGAN, generate synthetic recombined images solely from low-energy images. We perform an extensive quantitative and qualitative analysis of the model's performance, also exploiting radiologists' assessments, on a novel CESM dataset that includes 1138 images that, as a further contribution of this work, we make publicly available. The results show that CycleGAN is the most promising deep network to generate synthetic recombined images, highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field.
</details>
<details>
<summary>摘要</summary>
增强型pectral маммографи（CESM）是一种双能量мамографи���图像技术，需要通过Intravenously administering an iodinated contrast medium，然后收集低能量图像和高能量图像。然后将两个扫描结果组合起来，得到增强图像。 DESPITE CESM的诊断优势，使用contrast medium可能会导致side effects，并且CESM也会对病人辐射更高的辐射剂量 compared to standard mammography。为了解决这些限制，本工作提出使用深度生成模型对CESM进行虚拟增强，以实现无contrast和降低辐射剂量。我们的深度网络包括一个自适应网络和两个生成对抗网络，Pix2Pix和CycleGAN，可以将低能量图像转换为增强图像。我们对 novel CESM dataset中的1138张图像进行了广泛的量化和质量分析，并利用了 radiologists的评估，以评估模型的性能。结果显示CycleGAN是最有前途的深度网络，highlighting the potential of artificial intelligence techniques for virtual contrast enhancement in this field。
</details></li>
</ul>
<hr>
<h2 id="Space-Debris-Are-Deep-Learning-based-Image-Enhancements-part-of-the-Solution"><a href="#Space-Debris-Are-Deep-Learning-based-Image-Enhancements-part-of-the-Solution" class="headerlink" title="Space Debris: Are Deep Learning-based Image Enhancements part of the Solution?"></a>Space Debris: Are Deep Learning-based Image Enhancements part of the Solution?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00408">http://arxiv.org/abs/2308.00408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Jamrozik, Vincent Gaudillière, Mohamed Adel Musallam, Djamila Aouada</li>
<li>for: 这个研究的目的是确定深度神经网络（DNN）是否能够在视觉光谱下解决单一摄像头所产生的影像问题，并且跨越各种影像损坏。</li>
<li>methods: 这个研究使用了一个混合的UNet-ResNet34深度学习（DL）架构，这个架构在ImageNet dataset上进行预训。它能够处理各种影像损坏，包括模糊、曝光问题、低比例和噪音。</li>
<li>results: 根据视觉检查，这个UNet模型能够正确地更正在太空中拍摄的影像损坏，并且与现有的深度学习图像改善方法进行比较。<details>
<summary>Abstract</summary>
The volume of space debris currently orbiting the Earth is reaching an unsustainable level at an accelerated pace. The detection, tracking, identification, and differentiation between orbit-defined, registered spacecraft, and rogue/inactive space ``objects'', is critical to asset protection. The primary objective of this work is to investigate the validity of Deep Neural Network (DNN) solutions to overcome the limitations and image artefacts most prevalent when captured with monocular cameras in the visible light spectrum. In this work, a hybrid UNet-ResNet34 Deep Learning (DL) architecture pre-trained on the ImageNet dataset, is developed. Image degradations addressed include blurring, exposure issues, poor contrast, and noise. The shortage of space-generated data suitable for supervised DL is also addressed. A visual comparison between the URes34P model developed in this work and the existing state of the art in deep learning image enhancement methods, relevant to images captured in space, is presented. Based upon visual inspection, it is determined that our UNet model is capable of correcting for space-related image degradations and merits further investigation to reduce its computational complexity.
</details>
<details>
<summary>摘要</summary>
Currently, the volume of space debris orbiting the Earth is reaching an unsustainable level at an accelerated pace. The detection, tracking, identification, and differentiation between orbit-defined, registered spacecraft and rogue/inactive space "objects" are critical to asset protection. The primary objective of this work is to investigate the validity of Deep Neural Network (DNN) solutions to overcome the limitations and image artifacts most prevalent when captured with monocular cameras in the visible light spectrum. In this work, a hybrid UNet-ResNet34 Deep Learning (DL) architecture pre-trained on the ImageNet dataset is developed. Image degradations addressed include blurring, exposure issues, poor contrast, and noise. The shortage of space-generated data suitable for supervised DL is also addressed. A visual comparison between the URes34P model developed in this work and the existing state of the art in deep learning image enhancement methods, relevant to images captured in space, is presented. Based on visual inspection, it is determined that our UNet model is capable of correcting for space-related image degradations and merits further investigation to reduce its computational complexity.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Metrics-to-Quantify-Global-Consistency-in-Synthetic-Medical-Images"><a href="#Metrics-to-Quantify-Global-Consistency-in-Synthetic-Medical-Images" class="headerlink" title="Metrics to Quantify Global Consistency in Synthetic Medical Images"></a>Metrics to Quantify Global Consistency in Synthetic Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00402">http://arxiv.org/abs/2308.00402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Scholz, Benedikt Wiestler, Daniel Rueckert, Martin J. Menten</li>
<li>for: 这个论文是为了提高医学图像处理领域中的图像生成技术，例如数据增强或多Modalities图像翻译。</li>
<li>methods: 这个论文使用了基于神经网络的自动学习方法，包括使用supervised和unsupervised trained neural networks来评估图像的全局一致性。</li>
<li>results: 这个论文的结果表明，可以使用这些方法来分类图像的全局一致性，并且这些方法可以在没有标注数据时仍然有效。相比之下，已有的metric，如FID，无法直接评估图像的全局一致性。<details>
<summary>Abstract</summary>
Image synthesis is increasingly being adopted in medical image processing, for example for data augmentation or inter-modality image translation. In these critical applications, the generated images must fulfill a high standard of biological correctness. A particular requirement for these images is global consistency, i.e an image being overall coherent and structured so that all parts of the image fit together in a realistic and meaningful way. Yet, established image quality metrics do not explicitly quantify this property of synthetic images. In this work, we introduce two metrics that can measure the global consistency of synthetic images on a per-image basis. To measure the global consistency, we presume that a realistic image exhibits consistent properties, e.g., a person's body fat in a whole-body MRI, throughout the depicted object or scene. Hence, we quantify global consistency by predicting and comparing explicit attributes of images on patches using supervised trained neural networks. Next, we adapt this strategy to an unlabeled setting by measuring the similarity of implicit image features predicted by a self-supervised trained network. Our results demonstrate that predicting explicit attributes of synthetic images on patches can distinguish globally consistent from inconsistent images. Implicit representations of images are less sensitive to assess global consistency but are still serviceable when labeled data is unavailable. Compared to established metrics, such as the FID, our method can explicitly measure global consistency on a per-image basis, enabling a dedicated analysis of the biological plausibility of single synthetic images.
</details>
<details>
<summary>摘要</summary>
医疗图像处理领域内，图像合成技术在不断普及，例如数据增强或多Modalities图像翻译。在这些关键应用中，生成的图像必须满足高水平的生物准确性。特别是，生成图像必须具备全局一致性，即图像整体准确、结构化，所有图像部分都必须在真实和意义上相互协调。然而，现有的图像质量指标不直接量化这种图像的全局一致性。在这项工作中，我们介绍了两种可以测量生成图像的全局一致性的指标。为了测量全局一致性，我们假设一个真实的图像都具备一致的属性，例如整个人体MRI中的身体脂肪。因此，我们量化全局一致性通过使用已经超参的神经网络预测和比较图像中的显式属性。接下来，我们将这种策略应用到无标注 Setting中，通过测量图像中的隐藏特征来衡量图像的全局一致性。我们的结果表明，预测图像中的显式属性可以分辨全局一致的和不一致的图像。隐藏的图像特征也可以用无标注的方式来衡量图像的全局一致性，尽管它们比较敏感于数据质量。相比之下，已有的指标，如FID，不直接量化全局一致性，但可以量化图像的整体质量。我们的方法可以在每个图像基础上 direktly测量全局一致性，从而允许专门分析单个生成图像的生物可能性。
</details></li>
</ul>
<hr>
<h2 id="Fundus-Enhanced-Disease-Aware-Distillation-Model-for-Retinal-Disease-Classification-from-OCT-Images"><a href="#Fundus-Enhanced-Disease-Aware-Distillation-Model-for-Retinal-Disease-Classification-from-OCT-Images" class="headerlink" title="Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images"></a>Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00291">http://arxiv.org/abs/2308.00291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/fddm">https://github.com/xmed-lab/fddm</a></li>
<li>paper_authors: Lehan Wang, Weihang Dai, Mei Jin, Chubin Ou, Xiaomeng Li</li>
<li>for:  This paper proposes a novel method for retinal disease classification from OCT images, which can be used for ophthalmic examination.</li>
<li>methods:  The proposed method uses a fundus-enhanced disease-aware distillation model (FDDM) that utilizes unpaired fundus images during training and does not require the use of fundus images during testing. The method enhances the OCT model by distilling disease-related information from the fundus model and aligning class similarity between both modalities.</li>
<li>results:  The proposed approach outperforms single-modal, multi-modal, and state-of-the-art distillation methods for retinal disease classification, demonstrating its effectiveness and practicality for clinical use.<details>
<summary>Abstract</summary>
Optical Coherence Tomography (OCT) is a novel and effective screening tool for ophthalmic examination. Since collecting OCT images is relatively more expensive than fundus photographs, existing methods use multi-modal learning to complement limited OCT data with additional context from fundus images. However, the multi-modal framework requires eye-paired datasets of both modalities, which is impractical for clinical use. To address this problem, we propose a novel fundus-enhanced disease-aware distillation model (FDDM), for retinal disease classification from OCT images. Our framework enhances the OCT model during training by utilizing unpaired fundus images and does not require the use of fundus images during testing, which greatly improves the practicality and efficiency of our method for clinical use. Specifically, we propose a novel class prototype matching to distill disease-related information from the fundus model to the OCT model and a novel class similarity alignment to enforce consistency between disease distribution of both modalities. Experimental results show that our proposed approach outperforms single-modal, multi-modal, and state-of-the-art distillation methods for retinal disease classification. Code is available at https://github.com/xmed-lab/FDDM.
</details>
<details>
<summary>摘要</summary>
优化净合成 Tomatoes (OCT) 是一种新型和有效的诊断工具 для眼科检查。由于收集 OCT 图像相对较 expensive than fundus 图像，现有方法使用多模态学习来补充 OCT 数据中的有限资源。然而，多模态框架需要对 modalities 的眼球对应数据集，这是在临床应用中不实际。为解决这个问题，我们提出了一种新的眼球增强疾病感知模型 (FDDM)，用于从 OCT 图像中分类眼球疾病。我们的框架在训练时使用无对应的眼球图像来增强 OCT 模型，并不需要在测试时使用眼球图像，这大大提高了我们的方法在临床应用中的实用性和效率。具体来说，我们提出了一种新的类型 prototype matching，以填充疾病相关信息从眼球模型到 OCT 模型，以及一种新的类型相似性对齐，以保持两个模式之间的疾病分布的一致性。实验结果表明，我们的提议方法在眼球疾病分类任务上超越单模、多模和状态更新的混合方法。代码可以在 https://github.com/xmed-lab/FDDM 上获取。
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-Power-of-Self-Supervised-Image-Denoising-A-Comprehensive-Review"><a href="#Unleashing-the-Power-of-Self-Supervised-Image-Denoising-A-Comprehensive-Review" class="headerlink" title="Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review"></a>Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00247">http://arxiv.org/abs/2308.00247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Zhang, Fangfang Zhou, Yuanzhou Wei, Xiao Yang, Yuan Gu</li>
<li>for: 本论文探讨了自主学习图像减震技术的最新进展，旨在解决现实世界中获取噪声清晰对的困难问题。</li>
<li>methods: 本文分类了自主学习图像减震方法为三类：通用方法、BSN基于方法和 transformer 基于方法，并对每一类进行了 theoretically 分析和实践应用。</li>
<li>results: 本文通过对多个数据集进行量化和质量测试，证明了这些方法的效iveness，并提供了对照经典算法的比较。<details>
<summary>Abstract</summary>
The advent of deep learning has brought a revolutionary transformation to image denoising techniques. However, the persistent challenge of acquiring noise-clean pairs for supervised methods in real-world scenarios remains formidable, necessitating the exploration of more practical self-supervised image denoising. This paper focuses on self-supervised image denoising methods that offer effective solutions to address this challenge. Our comprehensive review thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.
</details>
<details>
<summary>摘要</summary>
deep learning技术的出现对图像噪声处理方法带来了革命性的变革，但在实际场景中获得噪声清晰对照样本的困难仍然存在，需要更加实用的自我监督图像噪声处理方法。这篇评论文探讨了最新的自我监督图像噪声处理方法，将其分为三类：通用方法、BSN基于方法和Transformer基于方法。对于每种类型，我们提供了简洁的理论分析以及实际应用。为评估这些方法的效果，我们在不同的数据集上进行了量化和质量上的实验，使用经典算法作为参考。此外，我们还进行了深入的限制分析和未来研究的探讨。通过对最新的自我监督图像噪声处理方法的详细审视，这篇评论文将成为图像处理领域的一个不可或缺的资源，为研究人员和实践者提供深入的理解和发展新技术的动力。
</details></li>
</ul>
<hr>
<h2 id="Boundary-Difference-Over-Union-Loss-For-Medical-Image-Segmentation"><a href="#Boundary-Difference-Over-Union-Loss-For-Medical-Image-Segmentation" class="headerlink" title="Boundary Difference Over Union Loss For Medical Image Segmentation"></a>Boundary Difference Over Union Loss For Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00220">http://arxiv.org/abs/2308.00220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunfan-bvb/boundarydouloss">https://github.com/sunfan-bvb/boundarydouloss</a></li>
<li>paper_authors: Fan Sun, Zhiming Luo, Shaozi Li</li>
<li>for: 这篇论文是为了提出一个简单而有效的医疗影像分类损失函数，以帮助医疗影像分类 tasks 中的边界区域分类。</li>
<li>methods: 这篇论文使用了一个新的损失函数，即边界差集测试损失函数 (Boundary DoU Loss)，并且使用了一个适应边界区域的焦点调整方法。</li>
<li>results: 在两个 dataset (ACDC 和 Synapse) 上进行了实验，结果显示了这个提案的有效性，并且与其他损失函数相比，这个损失函数更能帮助边界区域分类。<details>
<summary>Abstract</summary>
Medical image segmentation is crucial for clinical diagnosis. However, current losses for medical image segmentation mainly focus on overall segmentation results, with fewer losses proposed to guide boundary segmentation. Those that do exist often need to be used in combination with other losses and produce ineffective results. To address this issue, we have developed a simple and effective loss called the Boundary Difference over Union Loss (Boundary DoU Loss) to guide boundary region segmentation. It is obtained by calculating the ratio of the difference set of prediction and ground truth to the union of the difference set and the partial intersection set. Our loss only relies on region calculation, making it easy to implement and training stable without needing any additional losses. Additionally, we use the target size to adaptively adjust attention applied to the boundary regions. Experimental results using UNet, TransUNet, and Swin-UNet on two datasets (ACDC and Synapse) demonstrate the effectiveness of our proposed loss function. Code is available at https://github.com/sunfan-bvb/BoundaryDoULoss.
</details>
<details>
<summary>摘要</summary>
医学图像分割是诊断的关键。然而，当前的医学图像分割损失主要关注总体分割结果，有 fewer 的损失用于指导边界分割。这些损失通常需要与其他损失结合使用，并且生成不具有效果的结果。为解决这个问题，我们开发了一种简单而有效的损失函数：Boundary Difference over Union Loss（Boundary DoU Loss），用于导航边界区域分割。它是通过计算预测和真实值的差集与联合集的差集和部分交集集的比率来获得的。我们的损失函数仅仅依赖于区域计算，因此易于实现和训练，不需要其他损失函数。此外，我们使用目标大小来适应性地调整边界区域的注意力。实验结果使用 UNet、TransUNet 和 Swin-UNet 在 ACDC 和 Synapse 两个 dataset 上表明了我们提出的损失函数的有效性。代码可以在 https://github.com/sunfan-bvb/BoundaryDoULoss 上获取。
</details></li>
</ul>
<hr>
<h2 id="Universal-Adversarial-Defense-in-Remote-Sensing-Based-on-Pre-trained-Denoising-Diffusion-Models"><a href="#Universal-Adversarial-Defense-in-Remote-Sensing-Based-on-Pre-trained-Denoising-Diffusion-Models" class="headerlink" title="Universal Adversarial Defense in Remote Sensing Based on Pre-trained Denoising Diffusion Models"></a>Universal Adversarial Defense in Remote Sensing Based on Pre-trained Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16865">http://arxiv.org/abs/2307.16865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EricYu97/UAD-RS">https://github.com/EricYu97/UAD-RS</a></li>
<li>paper_authors: Weikang Yu, Yonghao Xu, Pedram Ghamisi<br>for:这个论文是为了提出一种基于扩散模型的通用防御方法，以防止深度神经网络受到攻击。methods:这个方法使用了预训练的扩散模型来防止多种未知攻击，并使用了反向和前向的扩散过程来纯化攻击样本。此外，还使用了自适应噪声水平选择（ANLS）机制来选择最佳的噪声水平，以达到最佳的纯化结果。results:实验结果表明，UAD-RS方法可以高效地防止多种常见攻击，并且不需要对攻击样本有严格的先知知识。此外，UAD-RS方法也可以减少重新训练的成本和性能的波动。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have achieved tremendous success in many remote sensing (RS) applications, in which DNNs are vulnerable to adversarial perturbations. Unfortunately, current adversarial defense approaches in RS studies usually suffer from performance fluctuation and unnecessary re-training costs due to the need for prior knowledge of the adversarial perturbations among RS data. To circumvent these challenges, we propose a universal adversarial defense approach in RS imagery (UAD-RS) using pre-trained diffusion models to defend the common DNNs against multiple unknown adversarial attacks. Specifically, the generative diffusion models are first pre-trained on different RS datasets to learn generalized representations in various data domains. After that, a universal adversarial purification framework is developed using the forward and reverse process of the pre-trained diffusion models to purify the perturbations from adversarial samples. Furthermore, an adaptive noise level selection (ANLS) mechanism is built to capture the optimal noise level of the diffusion model that can achieve the best purification results closest to the clean samples according to their Frechet Inception Distance (FID) in deep feature space. As a result, only a single pre-trained diffusion model is needed for the universal purification of adversarial samples on each dataset, which significantly alleviates the re-training efforts and maintains high performance without prior knowledge of the adversarial perturbations. Experiments on four heterogeneous RS datasets regarding scene classification and semantic segmentation verify that UAD-RS outperforms state-of-the-art adversarial purification approaches with a universal defense against seven commonly existing adversarial perturbations. Codes and the pre-trained models are available online (https://github.com/EricYu97/UAD-RS).
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在远程感知应用中取得了很大的成功，但是DNNs受到了恶意抗干扰的威胁。然而，现有的抗恶意防御策略在RS研究中通常受到性能波动和无需重新训练成本的限制，因为需要对RS数据有丰富的前知ledge。为了缓解这些挑战，我们提议一种通用的抗恶意防御策略（UAD-RS），使用预训练的扩散模型来防御通用DNNs Against多种未知的恶意抗干扰。具体来说，首先预训练了不同的RS数据集上的扩散模型，以学习各种数据域中的通用表示。然后，我们开发了一种通用抗恶意纯化框架，使用扩散模型的前进和返回过程来纯化恶意样本中的抗干扰。此外，我们还构建了一种适应性的噪声水平选择（ANLS）机制，以捕捉最佳的噪声水平，以达到最佳的纯化结果，与clean样本之间的Frechet InceptionDistance（FID）在深度特征空间最近。因此，只需要一个预训练的扩散模型，可以通用地纯化恶意样本在每个数据集上，大大减少了重新训练的努力，并保持高性能无需对恶意抗干扰有丰富的前知ledge。实验表明，UAD-RS在四个不同的RS数据集上的Scene classification和semantic segmentation任务上表现出了状态的抗恶意纯化方法。代码和预训练模型可以在线获取（https://github.com/EricYu97/UAD-RS）。
</details></li>
</ul>
<hr>
<h2 id="A-comprehensive-review-of-deep-learning-in-lung-cancer"><a href="#A-comprehensive-review-of-deep-learning-in-lung-cancer" class="headerlink" title="A comprehensive review of deep learning in lung cancer"></a>A comprehensive review of deep learning in lung cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02528">http://arxiv.org/abs/2308.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzane Tajidini</li>
<li>for: 提供历史性见解于肿瘤诊断方法</li>
<li>methods: 讨论肿瘤诊断过程和常用的诊断方法</li>
<li>results: 现有诊断方法不够有效，需要新的更智能方法<details>
<summary>Abstract</summary>
To provide the reader with a historical perspective on cancer classification approaches, we first discuss the fundamentals of the area of cancer diagnosis in this article, including the processes of cancer diagnosis and the standard classification methods employed by clinicians. Current methods for cancer diagnosis are deemed ineffective, calling for new and more intelligent approaches.
</details>
<details>
<summary>摘要</summary>
为了为读者提供历史背景，我们首先讲述了肿瘤诊断方法的基础知识，包括肿瘤诊断过程和临床医生使用的标准分类方法。现有的肿瘤诊断方法被认为是不具有效果，需要新的更智能的方法。Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Framing-image-registration-as-a-landmark-detection-problem-for-better-representation-of-clinical-relevance"><a href="#Framing-image-registration-as-a-landmark-detection-problem-for-better-representation-of-clinical-relevance" class="headerlink" title="Framing image registration as a landmark detection problem for better representation of clinical relevance"></a>Framing image registration as a landmark detection problem for better representation of clinical relevance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01318">http://arxiv.org/abs/2308.01318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diana Waldmannstetter, Benedikt Wiestler, Julian Schwarting, Ivan Ezhov, Marie Metz, Spyridon Bakas, Bhakti Baheti, Satrajit Chakrabarty, Jan S. Kirschke, Rolf A. Heckemann, Marie Piraud, Florian Kofler, Bjoern H. Menze</li>
<li>for: 提高图像 регистрация评价的临床 relevance</li>
<li>methods: 将图像 регистрация视为特征点检测问题，通过计算 hit rate 曲线来基于少量 inter-rater 分析获得特征点检测阈值</li>
<li>results: 提出一种基于错误分布的阈值计算方法，能够区分之前不可区分的 регистрация算法，并且可以评估图像REGISTRAION的临床意义<details>
<summary>Abstract</summary>
Nowadays, registration methods are typically evaluated based on sub-resolution tracking error differences. In an effort to reinfuse this evaluation process with clinical relevance, we propose to reframe image registration as a landmark detection problem. Ideally, landmark-specific detection thresholds are derived from an inter-rater analysis. To approximate this costly process, we propose to compute hit rate curves based on the distribution of errors of a sub-sample inter-rater analysis. Therefore, we suggest deriving thresholds from the error distribution using the formula: median + delta * median absolute deviation. The method promises differentiation of previously indistinguishable registration algorithms and further enables assessing the clinical significance in algorithm development.
</details>
<details>
<summary>摘要</summary>
现在，注册方法通常根据半分解追踪错误的差异进行评估。为了将注册评估过程恢复到临床 relevance，我们提议将注册视为一个标记检测问题。理想情况下，标记特定的检测阈值可以从多个评估者之间的交叉分析中 derivation。为了简化这个贵重过程，我们提议根据一个子样本的评估者分布计算 hit rate 曲线。因此，我们建议使用错误分布中的 median + δ * median absolute deviation 来 derivation 阈值，这种方法可以区分之前无法分辨的注册算法，并且可以评估算法开发中的临床 significancy。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/01/eess.IV_2023_08_01/" data-id="clp88dc4q0186ob885liz00mu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/01/cs.LG_2023_08_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-01
        
      </div>
    </a>
  
  
    <a href="/2023/07/31/cs.SD_2023_07_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-07-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
