
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-01 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02531 repo_url: None paper_authors: Jiuyang Zhou, Hong Zhu, Xingping Wang for">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-01 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/01/cs.SD_2023_08_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.02531 repo_url: None paper_authors: Jiuyang Zhou, Hong Zhu, Xingping Wang for">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-31T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:23.837Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/01/cs.SD_2023_08_01/" class="article-date">
  <time datetime="2023-07-31T16:00:00.000Z" itemprop="datePublished">2023-08-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-01 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Choir-Transformer-Generating-Polyphonic-Music-with-Relative-Attention-on-Transformer"><a href="#Choir-Transformer-Generating-Polyphonic-Music-with-Relative-Attention-on-Transformer" class="headerlink" title="Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer"></a>Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02531">http://arxiv.org/abs/2308.02531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuyang Zhou, Hong Zhu, Xingping Wang</li>
<li>for: 本研究旨在提出一种基于Transformer的多VOICE音乐生成模型，以更好地理解音乐结构。</li>
<li>methods: 该模型使用相对位置注意力来更好地模型长距离音韵关系，并提出了适合多VOICE音乐生成的音乐表示方式。</li>
<li>results: 实验结果表明，Choir Transformer的性能超过了前一个state-of-the-art的精度4.06%，并且测试结果与巴赫的音乐质量差不多。在实际应用中，生成的旋律和节奏可以根据输入的 specifying 进行调整，并且可以生成不同的音乐风格，如民族音乐或流行音乐等。<details>
<summary>Abstract</summary>
Polyphonic music generation is still a challenge direction due to its correct between generating melody and harmony. Most of the previous studies used RNN-based models. However, the RNN-based models are hard to establish the relationship between long-distance notes. In this paper, we propose a polyphonic music generation neural network named Choir Transformer[ https://github.com/Zjy0401/choir-transformer], with relative positional attention to better model the structure of music. We also proposed a music representation suitable for polyphonic music generation. The performance of Choir Transformer surpasses the previous state-of-the-art accuracy of 4.06%. We also measures the harmony metrics of polyphonic music. Experiments show that the harmony metrics are close to the music of Bach. In practical application, the generated melody and rhythm can be adjusted according to the specified input, with different styles of music like folk music or pop music and so on.
</details>
<details>
<summary>摘要</summary>
《多VOICE音乐生成仍然是一个挑战，主要因为 Correct between generating melody和harmony。大多数前一 Studies 使用 RNN-based 模型。然而， RNN-based 模型Difficulty in establishing the relationship between long-distance notes。在这篇论文中，我们提出了一种多VOICE音乐生成神经网络 named Choir Transformer[https://github.com/Zjy0401/choir-transformer], with relative positional attention to better model the structure of music。We also proposed a music representation suitable for polyphonic music generation。Choir Transformer 的性能超过了之前的状态态的准确率4.06%。我们也测量了多VOICE音乐的和声指标，实验结果显示和声指标与Bach的音乐很接近。在实际应用中，生成的旋律和节奏可以根据输入的特定要求进行调整，包括不同的音乐风格，如民歌或流行音乐等。
</details></li>
</ul>
<hr>
<h2 id="Multi-goal-Audio-visual-Navigation-using-Sound-Direction-Map"><a href="#Multi-goal-Audio-visual-Navigation-using-Sound-Direction-Map" class="headerlink" title="Multi-goal Audio-visual Navigation using Sound Direction Map"></a>Multi-goal Audio-visual Navigation using Sound Direction Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00219">http://arxiv.org/abs/2308.00219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haru Kondoh, Asako Kanezaki</li>
<li>for: 这 paper 的目的是提出一种新的多目标听视导航任务框架，并在不同情况下进行了实验研究。</li>
<li>methods: 该 paper 使用了一种名为听音地图（SDM）的学习基于方法，以动态地找到多个声音来源，同时利用过去的记忆。</li>
<li>results: 实验结果表明，使用 SDM 方法可以显著改善多个基eline方法的性能，无论目标数量如何。<details>
<summary>Abstract</summary>
Over the past few years, there has been a great deal of research on navigation tasks in indoor environments using deep reinforcement learning agents. Most of these tasks use only visual information in the form of first-person images to navigate to a single goal. More recently, tasks that simultaneously use visual and auditory information to navigate to the sound source and even navigation tasks with multiple goals instead of one have been proposed. However, there has been no proposal for a generalized navigation task combining these two types of tasks and using both visual and auditory information in a situation where multiple sound sources are goals. In this paper, we propose a new framework for this generalized task: multi-goal audio-visual navigation. We first define the task in detail, and then we investigate the difficulty of the multi-goal audio-visual navigation task relative to the current navigation tasks by conducting experiments in various situations. The research shows that multi-goal audio-visual navigation has the difficulty of the implicit need to separate the sources of sound. Next, to mitigate the difficulties in this new task, we propose a method named sound direction map (SDM), which dynamically localizes multiple sound sources in a learning-based manner while making use of past memories. Experimental results show that the use of SDM significantly improves the performance of multiple baseline methods, regardless of the number of goals.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在过去几年，关于indoor环境中使用深度奖励学习代理进行导航任务的研究有很大的进展。大多数这些任务只使用视觉信息，即首人图像，导航到单个目标。更近期，同时使用视觉和听音信息导航到声音源，甚至有多个目标而不是单个目标的任务被提议。然而，没有任何建议总结这两种类型的任务，并使用两种类型的信息在多个声音源的情况下。在这篇论文中，我们提出了一个新的框架：多目标听视导航。我们首先定义这个任务，然后通过在不同情况下进行实验，研究多目标听视导航任务与现有导航任务之间的关系。实验结果表明，多目标听视导航任务存在隐式需要分离声音源的困难。接着，我们提出了一种方法 named 声音方向地图（SDM），可以在学习基础上动态地理解多个声音源，并在使用过去记忆的情况下进行定位。实验结果表明，使用 SDM 可以显著提高多个基线方法的性能，无论目标数量如何。
</details></li>
</ul>
<hr>
<h2 id="DAVIS-High-Quality-Audio-Visual-Separation-with-Generative-Diffusion-Models"><a href="#DAVIS-High-Quality-Audio-Visual-Separation-with-Generative-Diffusion-Models" class="headerlink" title="DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models"></a>DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00122">http://arxiv.org/abs/2308.00122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, Chenliang Xu</li>
<li>for: 解决音视频混响分离问题</li>
<li>methods: 使用扩散模型和分离U-Net生成音频分离结果</li>
<li>results: 比对 existing 方法，DAVIS 在多种类别音频分离任务中表现出色，得到了更高质量的音频分离结果<details>
<summary>Abstract</summary>
We propose DAVIS, a Diffusion model-based Audio-VIusal Separation framework that solves the audio-visual sound source separation task through a generative manner. While existing discriminative methods that perform mask regression have made remarkable progress in this field, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS leverages a generative diffusion model and a Separation U-Net to synthesize separated magnitudes starting from Gaussian noises, conditioned on both the audio mixture and the visual footage. With its generative objective, DAVIS is better suited to achieving the goal of high-quality sound separation across diverse categories. We compare DAVIS to existing state-of-the-art discriminative audio-visual separation methods on the domain-specific MUSIC dataset and the open-domain AVE dataset, and results show that DAVIS outperforms other methods in separation quality, demonstrating the advantages of our framework for tackling the audio-visual source separation task.
</details>
<details>
<summary>摘要</summary>
我们提出了DAVIS，一个基于扩散模型的音频-视觉分离框架，它通过生成方式解决音频-视觉声源分离 задачі。相比于现有的推断方法，这些方法对于高品质的声音分离来说有限制，因为它们无法捕捉多样化的声音分布。相反地，DAVIS利用一个生成扩散模型和一个分离U-Net，将生成的分离度开始自 Gaussian 骚扰，并且将其条件在音频混合和视觉预像之间。这个生成目标使得DAVIS更适合实现高品质的声音分离，并且与多样化的声音分布更有关联。我们与现有的顶尖推断音频-视觉分离方法进行比较，结果显示DAVIS在分离质量方面较其他方法更高，显示了我们的框架在处理音频-视觉声源分离任务上的优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/01/cs.SD_2023_08_01/" data-id="clltau93n008acr88gsdcb453" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/01/cs.LG_2023_08_01/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-01 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/01/eess.AS_2023_08_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.AS - 2023-08-01 22:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
