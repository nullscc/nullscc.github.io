
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-01 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Hessian-Aware Bayesian Optimization for Decision Making Systems paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.00629 repo_url: None paper_authors: Mohit Rajpal, Lac Gia Tran, Yehong Zhang, Bryan Kian Hsiang Low">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-01 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/01/cs.LG_2023_08_01/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Hessian-Aware Bayesian Optimization for Decision Making Systems paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.00629 repo_url: None paper_authors: Mohit Rajpal, Lac Gia Tran, Yehong Zhang, Bryan Kian Hsiang Low">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-07-31T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:23.790Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/01/cs.LG_2023_08_01/" class="article-date">
  <time datetime="2023-07-31T16:00:00.000Z" itemprop="datePublished">2023-08-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-01 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hessian-Aware-Bayesian-Optimization-for-Decision-Making-Systems"><a href="#Hessian-Aware-Bayesian-Optimization-for-Decision-Making-Systems" class="headerlink" title="Hessian-Aware Bayesian Optimization for Decision Making Systems"></a>Hessian-Aware Bayesian Optimization for Decision Making Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00629">http://arxiv.org/abs/2308.00629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohit Rajpal, Lac Gia Tran, Yehong Zhang, Bryan Kian Hsiang Low</li>
<li>for:  optimize decision making systems in situations where feedback is sparse or uninformative</li>
<li>methods:  Bayesian Optimization and Hessian-aware Bayesian Optimization</li>
<li>results:  effective optimization under resource constraints and malformed feedback settings, demonstrated through experimental results on several benchmarks.Here’s the full text in Simplified Chinese:</li>
<li>for:  optimize 决策系统在具有笼罩或不完整反馈的情况下</li>
<li>methods: Bayesian Optimization 和 Hessian-aware Bayesian Optimization</li>
<li>results:  effective 优化下资源限制和异常反馈设置下，经过实验证明在多个 benchmark 上得到良好的效果。<details>
<summary>Abstract</summary>
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectively on several benchmarks under resource constraints and malformed feedback settings.
</details>
<details>
<summary>摘要</summary>
很多决策系统优化方法基于梯度方法，需要环境反馈信息。然而，在环境反馈稀缺或不具有信息时，这些方法可能表现不佳。 derivative-free方法，如 bayesian 优化，可以减少对梯度反馈质量的依赖，但是在复杂决策系统中可能 scales poorly。这个问题更加严重，当系统需要多个演员之间的互动，共同完成一个共同目标时。为了解决维度挑战，我们提议一种嵌入式多层架构，模型演员之间的动态，通过角色概念。此外，我们还引入了Hessian-aware bayesian 优化，高效地优化多层架构中的大量参数。实验结果表明，我们的方法（HA-GP-UCB）在资源限制和损坏反馈设置下表现良好。
</details></li>
</ul>
<hr>
<h2 id="Human-M3-A-Multi-view-Multi-modal-Dataset-for-3D-Human-Pose-Estimation-in-Outdoor-Scenes"><a href="#Human-M3-A-Multi-view-Multi-modal-Dataset-for-3D-Human-Pose-Estimation-in-Outdoor-Scenes" class="headerlink" title="Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes"></a>Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00628">http://arxiv.org/abs/2308.00628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soullessrobot/human-m3-dataset">https://github.com/soullessrobot/human-m3-dataset</a></li>
<li>paper_authors: Bohao Fan, Siqi Wang, Wenxuan Guo, Wenzhao Zheng, Jianjiang Feng, Jie Zhou</li>
<li>for:  This paper aims to provide a large-scale outdoor multi-modal multi-view multi-person human pose database (Human-M3) to support future research in 3D human pose estimation.</li>
<li>methods:  The proposed algorithm uses multi-modal data input, including RGB images and pointclouds, to generate ground truth annotations and improve the accuracy of human pose estimation.</li>
<li>results:  The proposed algorithm demonstrates the advantages of multi-modal data input for 3D human pose estimation, and the Human-M3 database is found to be challenging and suitable for future research.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提供一个大规模的户外多模态多视图多人姿态数据库（人类M3数据库），以支持未来的3D人姿估计研究。</li>
<li>methods: 提议的算法使用多模态数据输入，包括RGB图像和点云数据，生成ground truth注释，并提高人姿估计的准确性。</li>
<li>results: 提议的算法表明多模态数据输入对3D人姿估计具有优势，人类M3数据库被发现是一个挑战性的和适用于未来研究的数据库。<details>
<summary>Abstract</summary>
3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates reliable ground truth annotations. Evaluation of multiple different modalities algorithms has shown that this database is challenging and suitable for future research. Furthermore, we propose a 3D human pose estimation algorithm based on multi-modal data input, which demonstrates the advantages of multi-modal data input for 3D human pose estimation. Code and data will be released on https://github.com/soullessrobot/Human-M3-Dataset.
</details>
<details>
<summary>摘要</summary>
Recently, 3D人体姿态估算在外部环境中获得了越来越多的关注。然而，现有的3D人体姿态数据集，主要是基于单一模式（RGB图像或点云），并且通常只有一个人在每个场景中。这限制了可用数据的多样性。在这篇文章中，我们提出了人类-M3数据集，这是一个包含多视角RGB视频和相应的点云的外部多模式多人体姿态数据集。为了获得准确的人体姿态，我们提出了基于多模式数据输入的算法来生成基准注释。这使得可以解决前一些多视角RGB视频中的人类本地化和匹配抽象问题，并生成可靠的基准注释。评估多种模式算法后，我们发现这个数据集是挑战性的和适用于未来研究。此外，我们还提出了基于多模式数据输入的3D人体姿态估算算法，这示出了多模式数据输入对3D人体姿态估算的优势。代码和数据将在https://github.com/soullessrobot/Human-M3-Dataset上发布。
</details></li>
</ul>
<hr>
<h2 id="Beyond-One-Hot-Encoding-Injecting-Semantics-to-Drive-Image-Classifiers"><a href="#Beyond-One-Hot-Encoding-Injecting-Semantics-to-Drive-Image-Classifiers" class="headerlink" title="Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers"></a>Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00607">http://arxiv.org/abs/2308.00607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s1m0n38/semantic-encodings">https://github.com/s1m0n38/semantic-encodings</a></li>
<li>paper_authors: Alan Perotti, Simone Bertolotto, Eliana Pastor, André Panisson</li>
<li>for: 本研究旨在提高图像分类模型的可解释性和可靠性，通过 incorporating semantic information into the training process。</li>
<li>methods: 本文提出了一种基于 semantic information 的扩展损失函数，可以帮助模型更好地理解图像的内容和意义。具体来说， authors 使用ontology和word embedding等 semantic information来Derive an additional loss term，并通过supervised learning来使模型更加可信。</li>
<li>results: 经过实验 validate 的结果表明，该方法可以提高图像分类模型的准确率和 mistake severity，同时也可以提高模型的内部表示能力。此外， authors 还发现，该方法可以增加模型的解释性和对抗攻击性。<details>
<summary>Abstract</summary>
Images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. However, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. According to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. To overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. We suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. First, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. Second, we use our semantically enriched loss to train image classifiers, and analyse the trade-offs between accuracy, mistake severity, and learned internal representations. Finally, we discuss how this approach can be further exploited in terms of explainability and adversarial robustness. Code repository: https://github.com/S1M0N38/semantic-encodings
</details>
<details>
<summary>摘要</summary>
图像含有semantic信息，与现实世界 ontology 相关：狗种类共享哺乳动物类似性，食物图像经常在家庭环境中描绘，等等。然而，在机器学习模型图像分类训练中，对象类之间的相似性通常与一个热度编码的标签相对。根据这种逻辑，如果一个图像被标记为 " spoons" ，那么 " tea-spoon" 和 " shark" 在训练损失方面都是错误的。为了超越这些限制，我们探索了 Semantic Encodings 的整合，以提高模型解释性和可靠性。我们建议一种通用的方法，可以从任何类型的 semantic 信息中 derivate 额外的损失项。首先，我们示示如何应用我们的方法到ontology 和 word embedding中，并讨论如何将这些信息驱动一个supervised learning过程。其次，我们使用我们增强的损失函数来训练图像分类器，并分析损失函数的交易OFF、严重性和学习内部表示。最后，我们讨论如何进一步利用这种方法来提高解释性和对抗 robustness。代码存储库：https://github.com/S1M0N38/semantic-encodings
</details></li>
</ul>
<hr>
<h2 id="Latent-Shift-Gradient-of-Entropy-Helps-Neural-Codecs"><a href="#Latent-Shift-Gradient-of-Entropy-Helps-Neural-Codecs" class="headerlink" title="Latent-Shift: Gradient of Entropy Helps Neural Codecs"></a>Latent-Shift: Gradient of Entropy Helps Neural Codecs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00725">http://arxiv.org/abs/2308.00725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammet Balcilar, Bharath Bhushan Damodaran, Karam Naser, Franck Galpin, Pierre Hellier</li>
<li>for: 这个论文是为了提出一种基于ENTROPY GRADIENT的图像&#x2F;视频编码方法，以提高图像&#x2F;视频的压缩率。</li>
<li>methods: 该方法利用了ENTROPY GRADIENT，并通过对图像&#x2F;视频的抽象来实现高效的压缩。</li>
<li>results: 实验表明，使用ENTROPY GRADIENT可以实现$1-2%$的压缩率提高，而且与其他改进方法独立。<details>
<summary>Abstract</summary>
End-to-end image/video codecs are getting competitive compared to traditional compression techniques that have been developed through decades of manual engineering efforts. These trainable codecs have many advantages over traditional techniques such as easy adaptation on perceptual distortion metrics and high performance on specific domains thanks to their learning ability. However, state of the art neural codecs does not take advantage of the existence of gradient of entropy in decoding device. In this paper, we theoretically show that gradient of entropy (available at decoder side) is correlated with the gradient of the reconstruction error (which is not available at decoder side). We then demonstrate experimentally that this gradient can be used on various compression methods, leading to a $1-2\%$ rate savings for the same quality. Our method is orthogonal to other improvements and brings independent rate savings.
</details>
<details>
<summary>摘要</summary>
现代图像/视频编码器在满足人类视觉需求方面已经具有竞争力，而传统的压缩技术则是通过多年的手动工程努力而开发的。这些可调编码器具有许多优势，如根据人类视觉评价指标进行易于适应的修正，以及在特定领域中表现出色的高性能，归功于其学习能力。然而，当前的 neural codecs 并不利用decode器 сторо的梯度 entropy的存在。在这篇论文中，我们理论上验证了梯度 entropy （可以在decoder сторо上获得）与重建错误的梯度（不可以在decoder сторо上获得）之间的相关性。我们then通过实验表明，这个梯度可以在不同的压缩方法上使用，导致1-2%的比特率减少，同时保持相同的质量。我们的方法是对其他改进 независимы的。
</details></li>
</ul>
<hr>
<h2 id="Regularization-early-stopping-and-dreaming-a-Hopfield-like-setup-to-address-generalization-and-overfitting"><a href="#Regularization-early-stopping-and-dreaming-a-Hopfield-like-setup-to-address-generalization-and-overfitting" class="headerlink" title="Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting"></a>Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01421">http://arxiv.org/abs/2308.01421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Agliari, Miriam Aquaro, Francesco Alemanno, Alberto Fachechi</li>
<li>for: 本文研究了吸引器神经网络，从机器学习角度来看，通过梯度下降来找到优化的网络参数。</li>
<li>methods: 本文使用了梯度下降法，并在正则化损失函数上进行了优化。在这个框架下，最佳的 neuron-interaction 矩阵被证明为一类具有 Hebbian 核函数修改的步骤，并且这些步骤与正则化参数和训练时间有关。因此，可以采取基于交互矩阵的步骤来避免过拟合。</li>
<li>results: 本文通过分析随机生成的 sintetic 数据集，以及数值实验，发现了不同的拟合情况（即过拟合、失败和成功），这与数据集参数的变化有关。<details>
<summary>Abstract</summary>
In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by iteratively applying some unlearning protocols. Remarkably, the number of unlearning steps is proved to be related to the regularization hyperparameters of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of the algebraic properties of the interaction matrix, or, equivalently, in terms of regularization tuning and early-stopping strategies. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence of several regimes (i.e., overfitting, failure and success) as the dataset parameters are varied.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们从机器学习角度来看待吸引器神经网络：我们通过应用梯度下降来找到最佳网络参数，并且在这个框架下，最佳的神经元互动矩阵实际上是一类具有希贝尔核函数修改的谱函数约束。意外地，训练时间和正则化参数之间存在一定的关系，这使得我们可以通过对学习步数进行设置，以避免过拟合。此外，我们还可以通过对互动矩阵的特性进行分析，以及正则化调整和早停策略的设置，来提高模型的泛化能力。通过随机生成的 sintetic 数据和实验证明，我们发现了不同的训练 dataset 参数下的不同拟合情况（即过拟合、失败和成功）。
</details></li>
</ul>
<hr>
<h2 id="Semisupervised-Anomaly-Detection-using-Support-Vector-Regression-with-Quantum-Kernel"><a href="#Semisupervised-Anomaly-Detection-using-Support-Vector-Regression-with-Quantum-Kernel" class="headerlink" title="Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel"></a>Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00583">http://arxiv.org/abs/2308.00583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kilian Tscharke, Sebastian Issel, Pascal Debus</li>
<li>for: 本研究旨在提出一种基于支持向量回归（SVR）的半监督分类方法，用于矩阵缺失（AD）问题。</li>
<li>methods: 本方法使用量子kernel估计来实现SVR模型，并与量子自适应编码器和量子一类分类器进行比较。</li>
<li>results: 对11个实际AD数据集和1个玩具数据集进行了广泛的 benchmarking，结果显示，我们的SVR模型使用量子kernel比SVR模型使用径向基函数（RBF）kernel和所有其他模型都更高，取得了所有数据集的平均AUC最高。<details>
<summary>Abstract</summary>
Anomaly detection (AD) involves identifying observations or events that deviate in some way from the rest of the data. Machine learning techniques have shown success in automating this process by detecting hidden patterns and deviations in large-scale data. The potential of quantum computing for machine learning has been widely recognized, leading to extensive research efforts to develop suitable quantum machine learning (QML) algorithms. In particular, the search for QML algorithms for near-term NISQ devices is in full swing. However, NISQ devices pose additional challenges due to their limited qubit coherence times, low number of qubits, and high error rates. Kernel methods based on quantum kernel estimation have emerged as a promising approach to QML on NISQ devices, offering theoretical guarantees, versatility, and compatibility with NISQ constraints. Especially support vector machines (SVM) utilizing quantum kernel estimation have shown success in various supervised learning tasks. However, in the context of AD, semisupervised learning is of great relevance, and yet there is limited research published in this area. This paper introduces an approach to semisupervised AD based on the reconstruction loss of a support vector regression (SVR) with quantum kernel. This novel model is an alternative to the variational quantum and quantum kernel one-class classifiers, and is compared to a quantum autoencoder as quantum baseline and a SVR with radial-basis-function (RBF) kernel as well as a classical autoencoder as classical baselines. The models are benchmarked extensively on 10 real-world AD data sets and one toy data set, and it is shown that our SVR model with quantum kernel performs better than the SVR with RBF kernel as well as all other models, achieving highest mean AUC over all data sets. In addition, our QSVR outperforms the quantum autoencoder on 9 out of 11 data sets.
</details>
<details>
<summary>摘要</summary>
anomaly detection (AD) 涉及到从数据中找到不同的观察或事件。机器学习技术已经在大规模数据中自动找到隐藏的模式和偏差，并且在量子计算领域的研究已经得到了广泛的认可。特别是在近期的NISQ设备上进行量子机器学习（QML）的研究在全面进行。然而，NISQ设备具有有限的量子缓存时间、少量的量子比特和高错误率，这些问题对QML的应用带来了额外的挑战。基于量子kernel估计的kernel方法在NISQ设备上进行QML已经显示出了潜在的优势，包括理论保证、灵活性和NISQ约束的兼容性。尤其是在SVM中使用量子kernel估计，已经在不同的指导学习任务中显示出了成功。然而，在AD中，半supervised学习是非常重要的，但是现有的研究非常有限。本文介绍了一种基于支持向量回归（SVR）的半supervised AD模型，利用量子kernel估计。这种新的模型是与量子自编码器和量子kernel一类的一类的一类，并与量子自编码器和 радиаль基函数kernel SVR 以及经典自编码器作为经典基准进行比较。模型在10个真实的AD数据集和1个模拟数据集上进行了广泛的 benchmarking，结果显示，我们的SVR模型使用量子kernel可以比量子自编码器和RBF kernel SVR以及其他所有模型的mean AUC得到更高的性能，并且在9个数据集中超过了量子自编码器。此外，我们的QSVR还超过了经典自编码器在9个数据集中。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-for-Forecasting-Multivariate-Realized-Volatility-with-Spillover-Effects"><a href="#Graph-Neural-Networks-for-Forecasting-Multivariate-Realized-Volatility-with-Spillover-Effects" class="headerlink" title="Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects"></a>Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01419">http://arxiv.org/abs/2308.01419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Zhang, Xingyue Pu, Mihai Cucuringu, Xiaowen Dong</li>
<li>for: 该论文是为了模型和预测多变量实现的投资风险管理。</li>
<li>methods: 论文使用自定义图 neural network 来 incorporate 股票之间的延迟效应，并可以模型非线性关系和不同的损失函数进行灵活的训练。</li>
<li>results: 研究发现，包含多个邻居之间的延迟效应不一定能够提高预测精度，但是模型非线性延迟效应可以提高实现波动精度，特别是在短期预测（一周内）。此外，使用 quasi-likelihood 损失函数可以提高模型性能。<details>
<summary>Abstract</summary>
We present a novel methodology for modeling and forecasting multivariate realized volatilities using customized graph neural networks to incorporate spillover effects across stocks. The proposed model offers the benefits of incorporating spillover effects from multi-hop neighbors, capturing nonlinear relationships, and flexible training with different loss functions. Our empirical findings provide compelling evidence that incorporating spillover effects from multi-hop neighbors alone does not yield a clear advantage in terms of predictive accuracy. However, modeling nonlinear spillover effects enhances the forecasting accuracy of realized volatilities, particularly for short-term horizons of up to one week. Moreover, our results consistently indicate that training with the Quasi-likelihood loss leads to substantial improvements in model performance compared to the commonly-used mean squared error. A comprehensive series of empirical evaluations in alternative settings confirm the robustness of our results.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用自定义图神经网络来模型和预测多variate实现的波动。我们的提案方法可以捕捉多个跳数的影响，捕捉非线性关系，并且可以自由地训练使用不同的损失函数。我们的实验结果表明，只通过考虑多个跳数的影响不会提供明显的预测精度优势。但是，模型非线性的影响可以提高实现波动的预测精度，特别是在短期horizon（一周以下）。此外，我们的结果表明，使用 quasi-likelihood损失函数进行训练可以提高模型性能，相比于常用的mean squared error。我们在不同的设定下进行了全面的实验评估，并证明了我们的结果的可靠性。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Collaborative-Filtering-with-Personalized-Time-Decay-Functions-for-Financial-Product-Recommendation"><a href="#Adaptive-Collaborative-Filtering-with-Personalized-Time-Decay-Functions-for-Financial-Product-Recommendation" class="headerlink" title="Adaptive Collaborative Filtering with Personalized Time Decay Functions for Financial Product Recommendation"></a>Adaptive Collaborative Filtering with Personalized Time Decay Functions for Financial Product Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01208">http://arxiv.org/abs/2308.01208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashraf Ghiye, Baptiste Barreau, Laurent Carlier, Michalis Vazirgiannis</li>
<li>for: 提高金融产品推荐的可靠性和准确性，特别是在时间敏感的场景下。</li>
<li>methods: 提出了一种基于时间的共同推荐算法，可以自适应地减少远程客户产品交互的 Utility 值，使用个性化衰减函数模型客户对产品的动态共同信号。</li>
<li>results: 使用 proprietary 数据集进行评估，与文献中的参考基elines进行比较，显示了明显的提升，强调了在模型中Explicitly 处理时间的重要性，以提高金融产品推荐的准确性。<details>
<summary>Abstract</summary>
Classical recommender systems often assume that historical data are stationary and fail to account for the dynamic nature of user preferences, limiting their ability to provide reliable recommendations in time-sensitive settings. This assumption is particularly problematic in finance, where financial products exhibit continuous changes in valuations, leading to frequent shifts in client interests. These evolving interests, summarized in the past client-product interactions, see their utility fade over time with a degree that might differ from one client to another. To address this challenge, we propose a time-dependent collaborative filtering algorithm that can adaptively discount distant client-product interactions using personalized decay functions. Our approach is designed to handle the non-stationarity of financial data and produce reliable recommendations by modeling the dynamic collaborative signals between clients and products. We evaluate our method using a proprietary dataset from BNP Paribas and demonstrate significant improvements over state-of-the-art benchmarks from relevant literature. Our findings emphasize the importance of incorporating time explicitly in the model to enhance the accuracy of financial product recommendation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Linear-Regression-Phase-Transitions-and-Precise-Tradeoffs-for-General-Norms"><a href="#Robust-Linear-Regression-Phase-Transitions-and-Precise-Tradeoffs-for-General-Norms" class="headerlink" title="Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms"></a>Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00556">http://arxiv.org/abs/2308.00556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elvis Dohmatob, Meyer Scetbon</li>
<li>for: 本研究 investigate 测试时对Linear Regression模型的抗击攻击的影响，并确定任何模型可以保持给定水平的标准预测性能（准确率）的最佳 robustness 水平。</li>
<li>methods: 通过量化估计，我们揭示了不同 режим下的抗击坚定性和准确率之间的基本贸易。我们获得了精确地分类 regime 中抗击坚定性可以保持标准准确率无损的情况，以及抗击坚定性和准确率之间可能存在贸易的情况。</li>
<li>results: 我们通过简单的实验证明了我们的发现，并验证了这些发现在不同的设定下是否成立。本研究适用于特征协方差矩阵和攻击范数任何性质，并超越过去在这个领域的研究。<details>
<summary>Abstract</summary>
In this paper, we investigate the impact of test-time adversarial attacks on linear regression models and determine the optimal level of robustness that any model can reach while maintaining a given level of standard predictive performance (accuracy). Through quantitative estimates, we uncover fundamental tradeoffs between adversarial robustness and accuracy in different regimes. We obtain a precise characterization which distinguishes between regimes where robustness is achievable without hurting standard accuracy and regimes where a tradeoff might be unavoidable. Our findings are empirically confirmed with simple experiments that represent a variety of settings. This work applies to feature covariance matrices and attack norms of any nature, and extends beyond previous works in this area.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了测试时对线性回归模型的抗击攻击对模型的影响，并确定了保持给定水平的标准预测性能（准确率）的最佳鲁棒性水平。通过量化估计，我们揭示了不同场景下抗击鲁棒性和准确率之间的基本交易offs。我们获得了精确地分类型，可以分为不会受到交易offs的场景和可能不可避免的交易offs场景。我们的发现得到了实际验证，通过一些简单的实验，这些实验代表了多种设置。这项工作适用于特征协方差矩阵和攻击 нормы的任何种类，并超越了之前在这一领域的研究。
</details></li>
</ul>
<hr>
<h2 id="Copula-for-Instance-wise-Feature-Selection-and-Ranking"><a href="#Copula-for-Instance-wise-Feature-Selection-and-Ranking" class="headerlink" title="Copula for Instance-wise Feature Selection and Ranking"></a>Copula for Instance-wise Feature Selection and Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00549">http://arxiv.org/abs/2308.00549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanyu Peng, Guanhua Fang, Ping Li</li>
<li>for: 提高 neural network 中 feature 的选择和排序精度，使其更适合每个样本。</li>
<li>methods: 使用 Gaussian copula 技术 capture 特征之间的相关性，不需要额外变更现有的特征选择框架。</li>
<li>results: 对 synthetic 和实际数据进行比较性和可读性的实验，表明我们的方法可以捕捉有意义的相关性。<details>
<summary>Abstract</summary>
Instance-wise feature selection and ranking methods can achieve a good selection of task-friendly features for each sample in the context of neural networks. However, existing approaches that assume feature subsets to be independent are imperfect when considering the dependency between features. To address this limitation, we propose to incorporate the Gaussian copula, a powerful mathematical technique for capturing correlations between variables, into the current feature selection framework with no additional changes needed. Experimental results on both synthetic and real datasets, in terms of performance comparison and interpretability, demonstrate that our method is capable of capturing meaningful correlations.
</details>
<details>
<summary>摘要</summary>
<<SYS>> instance-wise 特征选择和排名方法可以在神经网络中选择每个样本的任务友好特征，但现有的方法假设特征子集是独立的，这会导致限制。为了解决这个限制，我们提议将 Gaussian copula，一种强大的变量相关技术， integrate 到当前特征选择框架中，无需进行额外变更。实验结果表明，我们的方法可以 capture 意义的相关性。在 both synthetic 和实际数据集上，我们的方法可以在性能比较和可读性方面与现有方法进行比较，并且可以 capture 意义的相关性。Gaussian copula 是一种强大的变量相关技术，可以用来捕捉特征之间的相关性。通过 integrate 这种技术到当前特征选择框架中，我们可以更好地 capture 意义的相关性，从而提高模型的性能。实验结果表明，我们的方法可以在不同的数据集上 capture 意义的相关性，并且可以与现有方法进行比较。此外，我们的方法还可以提供更多的可读性，使得用户可以更好地理解模型的决策过程。总之，我们的方法可以帮助用户更好地选择和排名特征，从而提高模型的性能和可读性。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Early-Dropouts-of-an-Active-and-Healthy-Ageing-App"><a href="#Predicting-Early-Dropouts-of-an-Active-and-Healthy-Ageing-App" class="headerlink" title="Predicting Early Dropouts of an Active and Healthy Ageing App"></a>Predicting Early Dropouts of an Active and Healthy Ageing App</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00539">http://arxiv.org/abs/2308.00539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Perifanis, Ioanna Michailidi, Giorgos Stamatelatos, George Drosatos, Pavlos S. Efraimidis</li>
<li>for: 预测年轻人健康长逝APP使用者早期退出</li>
<li>methods: 使用机器学习方法处理数据，构建分类模型预测用户忠诚度，并使用SMOTE和ADASYN等扩大样本方法提高分类性能</li>
<li>results: 实验结果表明，机器学习算法可以提供高质量的忠诚度预测，动态特征对模型预测性能产生积极影响，并使用扩大样本方法可以提高分类性能10%。<details>
<summary>Abstract</summary>
In this work, we present a machine learning approach for predicting early dropouts of an active and healthy ageing app. The presented algorithms have been submitted to the IFMBE Scientific Challenge 2022, part of IUPESM WC 2022. We have processed the given database and generated seven datasets. We used pre-processing techniques to construct classification models that predict the adherence of users using dynamic and static features. We submitted 11 official runs and our results show that machine learning algorithms can provide high-quality adherence predictions. Based on the results, the dynamic features positively influence a model's classification performance. Due to the imbalanced nature of the dataset, we employed oversampling methods such as SMOTE and ADASYN to improve the classification performance. The oversampling approaches led to a remarkable improvement of 10\%. Our methods won first place in the IFMBE Scientific Challenge 2022.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种机器学习方法，用于预测活跃和健康年龄应用程序中的早期退出。我们提交了IFMBE科学挑战2022，该挑战是IUPESM WC 2022的一部分。我们处理了给定的数据库，生成了七个数据集。我们使用预处理技术，构建了用于预测用户使用动态和静止特征的分类模型。我们提交了11个官方运行，我们的结果表明，机器学习算法可以提供高质量的遵从性预测。根据结果，动态特征对模型的分类性能产生了积极的影响。由于数据集的不均衡性，我们使用了扩大样本的方法，如SMOTE和ADASYN，以改善分类性能。这些扩大样本方法导致了10%的很显著的提高。我们的方法在IFMBE科学挑战2022中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="Graph-Embedding-Dynamic-Feature-based-Supervised-Contrastive-Learning-of-Transient-Stability-for-Changing-Power-Grid-Topologies"><a href="#Graph-Embedding-Dynamic-Feature-based-Supervised-Contrastive-Learning-of-Transient-Stability-for-Changing-Power-Grid-Topologies" class="headerlink" title="Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies"></a>Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00537">http://arxiv.org/abs/2308.00537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijian Lv, Xin Chen, Zijian Feng</li>
<li>for: 提高电力系统稳定性预测精度，适应干扰影响。</li>
<li>methods: 使用高维电力网 topology信息向低维节点基 embedding数据流动化，并使用 supervised contrastive learning 预测瞬态稳定性。</li>
<li>results: 测试结果表明，提出的 GEDF-SCL 模型可以高度精度地预测瞬态稳定性，同时适应变化的电力网 topology。<details>
<summary>Abstract</summary>
Accurate online transient stability prediction is critical for ensuring power system stability when facing disturbances. While traditional transient stablity analysis replies on the time domain simulations can not be quickly adapted to the power grid toplogy change. In order to vectorize high-dimensional power grid topological structure information into low-dimensional node-based graph embedding streaming data, graph embedding dynamic feature (GEDF) has been proposed. The transient stability GEDF-based supervised contrastive learning (GEDF-SCL) model uses supervised contrastive learning to predict transient stability with GEDFs, considering power grid topology information. To evaluate the performance of the proposed GEDF-SCL model, power grids of varying topologies were generated based on the IEEE 39-bus system model. Transient operational data was obtained by simulating N-1 and N-$\bm{m}$-1 contingencies on these generated power system topologies. Test result demonstrated that the GEDF-SCL model can achieve high accuracy in transient stability prediction and adapt well to changing power grid topologies.
</details>
<details>
<summary>摘要</summary>
正确的在线稳定预测是电力系统稳定性的关键，当面临干扰时，可以帮助恢复稳定。传统的稳定预测方法基于时域仿真，但是这些方法不能快速适应电力网络结构变化。为了将高维电力网络结构信息转化为低维节点基于图像流动数据，提出了图像动态特征（GEDF）。基于GEDF的稳定预测模型使用超级对比学习（SCL）来预测稳定性，考虑电力网络结构信息。为评估提案模型的性能，基于IEEE 39-bus系统模型生成了不同电力网络结构。通过对这些生成的电力系统结构进行 simulate N-1和N-m-1的干扰，获得了过程数据。测试结果表明，GEDF-SCL模型可以在稳定预测中达到高精度和适应电力网络结构变化。
</details></li>
</ul>
<hr>
<h2 id="Graph-Contrastive-Learning-with-Generative-Adversarial-Network"><a href="#Graph-Contrastive-Learning-with-Generative-Adversarial-Network" class="headerlink" title="Graph Contrastive Learning with Generative Adversarial Network"></a>Graph Contrastive Learning with Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00535">http://arxiv.org/abs/2308.00535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wu, Chaokun Wang, Jingcao Xu, Ziyang Liu, Kai Zheng, Xiaowei Wang, Yang Song, Kun Gai</li>
<li>for: 用于 Graph Contrastive Learning (GCL) 的训练，以便在实际应用中减少标签的问题。</li>
<li>methods: 利用 Graph Contrastive Learning (GCL) 和 Graph Generative Adversarial Networks (GANs) 来训练 Graph Neural Networks (GNNs)，并通过自动生成图的视图来捕捉图的特性。</li>
<li>results: 在七个真实世界数据集上进行了广泛的实验，并证明了 GACN 可以生成高质量的扩充视图，并且高于十二个基eline方法。同时，GACN 意外发现生成的视图最终遵循了在线网络中知名的偏好附加规则。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have demonstrated promising results on exploiting node representations for many downstream tasks through supervised end-to-end training. To deal with the widespread label scarcity issue in real-world applications, Graph Contrastive Learning (GCL) is leveraged to train GNNs with limited or even no labels by maximizing the mutual information between nodes in its augmented views generated from the original graph. However, the distribution of graphs remains unconsidered in view generation, resulting in the ignorance of unseen edges in most existing literature, which is empirically shown to be able to improve GCL's performance in our experiments. To this end, we propose to incorporate graph generative adversarial networks (GANs) to learn the distribution of views for GCL, in order to i) automatically capture the characteristic of graphs for augmentations, and ii) jointly train the graph GAN model and the GCL model. Specifically, we present GACN, a novel Generative Adversarial Contrastive learning Network for graph representation learning. GACN develops a view generator and a view discriminator to generate augmented views automatically in an adversarial style. Then, GACN leverages these views to train a GNN encoder with two carefully designed self-supervised learning losses, including the graph contrastive loss and the Bayesian personalized ranking Loss. Furthermore, we design an optimization framework to train all GACN modules jointly. Extensive experiments on seven real-world datasets show that GACN is able to generate high-quality augmented views for GCL and is superior to twelve state-of-the-art baseline methods. Noticeably, our proposed GACN surprisingly discovers that the generated views in data augmentation finally conform to the well-known preferential attachment rule in online networks.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经在许多下游任务上达到了可观的成果，通过监督式终到训练来利用节点表示。然而，在实际应用中，标签的缺乏问题是普遍存在的。为了解决这个问题，我们提出了使用图对照学习（GCL）来训练GNNs，不需要或者even不需要标签。GCL通过最大化图中节点之间的共同信息来进行训练。然而，现有文献中忽略了图的分布，导致对未见边的忽略。我们提出了通过图生成对抗网络（GANs）来学习图的分布，以便自动捕捉图的特征，并将图生成模型和GCL模型同时训练。我们提出了一种名为GACN的图生成对抗学习网络，它包括视图生成器和视图识别器。GACN使用对抗风格来自动生成增强视图。然后，GACN使用这些增强视图来训练GNN编码器，并使用两种特殊的自我监督学习损失函数，包括图对照损失和 bayesian人性化排名损失。此外，我们设计了一种培训所有GACN模块的优化框架。我们在七个真实世界数据集上进行了广泛的实验，显示GACN能够生成高质量的增强视图，并比十二个州先进方法更佳。另外，我们发现了GACN所生成的视图最终遵循了在在线网络中广泛存在的偏好附加规则。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Temporal-Multi-Gate-Mixture-of-Experts-Approach-for-Vehicle-Trajectory-and-Driving-Intention-Prediction"><a href="#A-Novel-Temporal-Multi-Gate-Mixture-of-Experts-Approach-for-Vehicle-Trajectory-and-Driving-Intention-Prediction" class="headerlink" title="A Novel Temporal Multi-Gate Mixture-of-Experts Approach for Vehicle Trajectory and Driving Intention Prediction"></a>A Novel Temporal Multi-Gate Mixture-of-Experts Approach for Vehicle Trajectory and Driving Intention Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00533">http://arxiv.org/abs/2308.00533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renteng Yuan, Mohamed Abdel-Aty, Qiaojun Xiang, Zijin Wang, Ou Zheng</li>
<li>for: 预测车辆轨迹和驾驶意图</li>
<li>methods: 使用Temporal Multi-Gate Mixture-of-Experts（TMMOE）模型同时预测车辆轨迹和驾驶意图，包括三层：共享层、专家层和全连接层。在模型中，共享层使用Temporal Convolutional Networks（TCN）提取时间特征，然后专家层可以识别不同任务的信息。</li>
<li>results: 使用uncertainty算法建构多任务损失函数，最终在CitySim数据集上验证TMMOE模型，与LSTM模型相比，实现最高的分类和回归结果。<details>
<summary>Abstract</summary>
Accurate Vehicle Trajectory Prediction is critical for automated vehicles and advanced driver assistance systems. Vehicle trajectory prediction consists of two essential tasks, i.e., longitudinal position prediction and lateral position prediction. There is a significant correlation between driving intentions and vehicle motion. In existing work, the three tasks are often conducted separately without considering the relationships between the longitudinal position, lateral position, and driving intention. In this paper, we propose a novel Temporal Multi-Gate Mixture-of-Experts (TMMOE) model for simultaneously predicting the vehicle trajectory and driving intention. The proposed model consists of three layers: a shared layer, an expert layer, and a fully connected layer. In the model, the shared layer utilizes Temporal Convolutional Networks (TCN) to extract temporal features. Then the expert layer is built to identify different information according to the three tasks. Moreover, the fully connected layer is used to integrate and export prediction results. To achieve better performance, uncertainty algorithm is used to construct the multi-task loss function. Finally, the publicly available CitySim dataset validates the TMMOE model, demonstrating superior performance compared to the LSTM model, achieving the highest classification and regression results. Keywords: Vehicle trajectory prediction, driving intentions Classification, Multi-task
</details>
<details>
<summary>摘要</summary>
准确预测车辆轨迹是自动驾驶和高级驾驶助手系统中关键的一环。车辆轨迹预测包括两个基本任务，即长进位预测和横向位预测。车辆的运动和驾驶意图之间存在显著的相关性。现有的工作通常将这三个任务分别进行，没有考虑这三个任务之间的关系。本文提出了一种新的时间多门混合专家（TMMOE）模型，用于同时预测车辆轨迹和驾驶意图。该模型包括三层：共享层、专家层和全连接层。在模型中，共享层使用时间卷积网络（TCN）提取时间特征。然后，专家层用于识别不同任务中的信息。此外，全连接层用于集成和出口预测结果。为了实现更好的性能，uncertainty算法用于构建多任务损失函数。最后，公开的CitySim数据集 validate了TMMOE模型，达到了LSTM模型的最高分类和回归结果。关键词：车辆轨迹预测、驾驶意图分类、多任务
</details></li>
</ul>
<hr>
<h2 id="Variational-Label-Correlation-Enhancement-for-Congestion-Prediction"><a href="#Variational-Label-Correlation-Enhancement-for-Congestion-Prediction" class="headerlink" title="Variational Label-Correlation Enhancement for Congestion Prediction"></a>Variational Label-Correlation Enhancement for Congestion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00529">http://arxiv.org/abs/2308.00529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biao Liu, Congyu Qiao, Ning Xu, Xin Geng, Ziran Zhu, Jun Yang</li>
<li>for: 提高大规模设计过程中的Routing质量预测精度，以促进普遍设计和资源保留。</li>
<li>methods: 利用变量推理技术来估计本地标签相关性权重，并将其与邻居格点的标签相关性权重相乘以提高回归模型的性能。</li>
<li>results: 在公共可用的 \texttt{ISPD2011} 和 \texttt{DAC2012} 测试集上，{\ours} 表现出色，与传统方法相比，提高了回归模型的性能。<details>
<summary>Abstract</summary>
The physical design process of large-scale designs is a time-consuming task, often requiring hours to days to complete, with routing being the most critical and complex step. As the the complexity of Integrated Circuits (ICs) increases, there is an increased demand for accurate routing quality prediction. Accurate congestion prediction aids in identifying design flaws early on, thereby accelerating circuit design and conserving resources. Despite the advancements in current congestion prediction methodologies, an essential aspect that has been largely overlooked is the spatial label-correlation between different grids in congestion prediction. The spatial label-correlation is a fundamental characteristic of circuit design, where the congestion status of a grid is not isolated but inherently influenced by the conditions of its neighboring grids. In order to fully exploit the inherent spatial label-correlation between neighboring grids, we propose a novel approach, {\ours}, i.e., VAriational Label-Correlation Enhancement for Congestion Prediction, which considers the local label-correlation in the congestion map, associating the estimated congestion value of each grid with a local label-correlation weight influenced by its surrounding grids. {\ours} leverages variational inference techniques to estimate this weight, thereby enhancing the regression model's performance by incorporating spatial dependencies. Experiment results validate the superior effectiveness of {\ours} on the public available \texttt{ISPD2011} and \texttt{DAC2012} benchmarks using the superblue circuit line.
</details>
<details>
<summary>摘要</summary>
大规模设计的物理设计过程是一个时间consuming的任务，经常需要几个小时到几天的时间完成，routing是最关键和复杂的步骤。随着集成电路（IC）的复杂度的提高，需要更加准确的routing质量预测。准确的填充预测可以帮助早期发现设计缺陷，从而加速电路设计并保留资源。现有的填充预测方法中一个关键的缺失是忽略了各个网格之间的空间标签相关性。各个网格的填充状态不 solo，而是受到周围网格的状态的影响。为了充分利用这种空间标签相关性，我们提出了一种新的方法，即ours，即Variational Label-Correlation Enhancement for Congestion Prediction。ours通过利用变量推理技术来估算每个网格的本地标签相关性权重，以便在填充预测中更好地考虑空间相关性。实验结果表明ours在公开的 \texttt{ISPD2011} 和 \texttt{DAC2012} benchmark上显著超越了传统的填充预测方法。
</details></li>
</ul>
<hr>
<h2 id="Improved-Prognostic-Prediction-of-Pancreatic-Cancer-Using-Multi-Phase-CT-by-Integrating-Neural-Distance-and-Texture-Aware-Transformer"><a href="#Improved-Prognostic-Prediction-of-Pancreatic-Cancer-Using-Multi-Phase-CT-by-Integrating-Neural-Distance-and-Texture-Aware-Transformer" class="headerlink" title="Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer"></a>Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00507">http://arxiv.org/abs/2308.00507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hexin Dong, Jiawen Yao, Yuxing Tang, Mingze Yuan, Yingda Xia, Jian Zhou, Hong Lu, Jingren Zhou, Bin Dong, Le Lu, Li Zhang, Zaiyi Liu, Yu Shi, Ling Zhang<br>for:This paper aims to develop a novel learnable neural distance method to predict the prognosis of patients with pancreatic ductal adenocarcinoma (PDAC) based on the precise relationship between the tumor and nearby important vessels in CT images.methods:The proposed method uses a combination of CNN and transformer modules to extract dynamic tumor-related texture features from multi-phase contrast-enhanced CT images, and a learnable neural distance to describe the precise relationship between the tumor and vessels.results:The proposed method was extensively evaluated and compared with existing methods in a multi-center dataset with 1,070 patients with PDAC, and statistical analysis confirmed its clinical effectiveness in the external test set consisting of three centers. The developed risk marker was the strongest predictor of overall survival among preoperative factors and has the potential to be combined with established clinical factors to select patients at higher risk who might benefit from neoadjuvant therapy.Here is the same information in Simplified Chinese text:for:这篇论文目标是基于CT图像中肿瘤和近处重要血管之间的准确关系来预测患有肝脏ductal adenocarcinoma（PDAC）患者的预后。methods:该方法使用了组合CNN和transformer模块来提取多phasic contrast-enhanced CT图像中的动态肿瘤相关文本特征，并使用学习的神经距离来描述肿瘤和血管之间的准确关系。results:该方法在多中心数据集（n&#x3D;4）中进行了广泛的评估和比较，并在多中心测试集（n&#x3D;3）中进行了统计分析，证实了其在外部测试集中的临床效iveness。开发的风险标记是外科前因素中最强的预后预测器，并且有可能与已知的临床因素相结合，以选择可能需要neoadjuvant therapy的患者。<details>
<summary>Abstract</summary>
Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in which the tumor-vascular involvement greatly affects the resectability and, thus, overall survival of patients. However, current prognostic prediction methods fail to explicitly and accurately investigate relationships between the tumor and nearby important vessels. This paper proposes a novel learnable neural distance that describes the precise relationship between the tumor and vessels in CT images of different patients, adopting it as a major feature for prognosis prediction. Besides, different from existing models that used CNNs or LSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CT imaging, we improved the extraction of dynamic tumor-related texture features in multi-phase contrast-enhanced CT by fusing local and global features using CNN and transformer modules, further enhancing the features extracted across multi-phase CT images. We extensively evaluated and compared the proposed method with existing methods in the multi-center (n=4) dataset with 1,070 patients with PDAC, and statistical analysis confirmed its clinical effectiveness in the external test set consisting of three centers. The developed risk marker was the strongest predictor of overall survival among preoperative factors and it has the potential to be combined with established clinical factors to select patients at higher risk who might benefit from neoadjuvant therapy.
</details>
<details>
<summary>摘要</summary>
肺脏ductal adenocarcinoma（PDAC）是一种高度致命的Cancer，肿瘤与附近重要的血管交叉关系对患者的手术可能性和全身生存率产生很大影响。然而，现有的预测方法未能准确地和Explicitlyinvestigate肿瘤和附近重要血管之间的关系。这篇文章提出了一种新的学习型尺度，用于描述不同患者的CT图像中肿瘤和血管之间的准确关系，并采用其为预测诊断的重要特征。此外，与现有模型使用CNN或LSTM提取肿瘤增强 patrerns on dynamic contrast-enhanced CT imaging不同，我们在多相contrast-enhanced CT图像中提取了更多的动态肿瘤相关文本特征，并使用CNN和transformer模块进行融合，进一步提高了在多相CT图像中提取的特征。我们对在多中心（n=4）的数据集中的1,070名PDAC患者进行了广泛的评估和比较，并通过统计分析确认了我们的方法在外测集中的临床效果。我们开发的风险标记是PDAC患者的全身生存率最强的预测因素之一，并且它有可能与已知的临床因素相结合，以选择更高风险的患者，以便通过neoadjuvant therapy进行治疗。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Graph-Spectral-Clustering-of-Text-Documents"><a href="#Explainable-Graph-Spectral-Clustering-of-Text-Documents" class="headerlink" title="Explainable Graph Spectral Clustering of Text Documents"></a>Explainable Graph Spectral Clustering of Text Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00504">http://arxiv.org/abs/2308.00504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bartłomiej Starosta, Mieczysław A. Kłopotek, Sławomir T. Wierzchoń</li>
<li>For: This paper aims to provide a method for explaining the results of combinatorial Laplacian-based graph spectral clustering, which is a technique used for clustering text documents.* Methods: The paper proposes a new method called $K$-embedding, which is based on showing the approximate equivalence of the combinatorial Laplacian embedding, the $K$-embedding, and the term vector space embedding. The paper provides theoretical background for this approach and demonstrates the effectiveness of $K$-embedding through experimental studies.* Results: The paper shows that $K$-embedding approximates the Laplacian embedding well under certain conditions, and provides a bridge between the textual contents and the clustering results, allowing for more interpretable and explainable results.<details>
<summary>Abstract</summary>
Spectral clustering methods are known for their ability to represent clusters of diverse shapes, densities etc. However, results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Therefore there is an urgent need to elaborate methods for explaining the outcome of the clustering. This paper presents a contribution towards this goal. We present a proposal of explanation of results of combinatorial Laplacian based graph spectral clustering. It is based on showing (approximate) equivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in this paper) and term vector space embedding. Hence a bridge is constructed between the textual contents and the clustering results. We provide theoretical background for this approach. We performed experimental study showing that $K$-embedding approximates well Laplacian embedding under favourable block matrix conditions and show that approximation is good enough under other conditions.
</details>
<details>
<summary>摘要</summary>
spectral clustering 方法知名于其能够表示多iform shapes、density等多样性群集。然而，当应用于文档时，这些算法的结果很难对用户进行解释，尤其是因为它们在spectral space中嵌入，这个空间与文档内容没有直接关系。因此，有一项办法需要强调的是解释结果的方法。本文提出了一种解释 combinatorial Laplacian 基于图 spectral clustering 的结果的方法。这种方法基于表明（approximate）combinatorial Laplacian embedding、K-embedding（本文所提出的）和term vector space embedding之间的等价性。因此，构建了文本内容和归类结果之间的桥梁。我们提供了理论背景，并进行了实验研究，证明了 K-embedding 可以准确地表示 Laplacian embedding ，并且在某些条件下，这种准确性足够高。
</details></li>
</ul>
<hr>
<h2 id="DINO-CXR-A-self-supervised-method-based-on-vision-transformer-for-chest-X-ray-classification"><a href="#DINO-CXR-A-self-supervised-method-based-on-vision-transformer-for-chest-X-ray-classification" class="headerlink" title="DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification"></a>DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00475">http://arxiv.org/abs/2308.00475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Shakouri, Fatemeh Iranmanesh, Mahdi Eftekhari</li>
<li>for: 这个研究是为了解决骨胶X射线数据的有限性问题，对医疗影像分析进行自动化。</li>
<li>methods: 本研究使用了一种自我超级vised学习方法，即DINO-CXR，它是基于视觉 трансформа器的骨胶X射线分类方法。</li>
<li>results: 比较分析表明，提案的方法在肺炎和COVID-19检测中具有更高的精度和相近的AUC和F-1分数，并且需要更少的标注数据。<details>
<summary>Abstract</summary>
The limited availability of labeled chest X-ray datasets is a significant bottleneck in the development of medical imaging methods. Self-supervised learning (SSL) can mitigate this problem by training models on unlabeled data. Furthermore, self-supervised pretraining has yielded promising results in visual recognition of natural images but has not been given much consideration in medical image analysis. In this work, we propose a self-supervised method, DINO-CXR, which is a novel adaptation of a self-supervised method, DINO, based on a vision transformer for chest X-ray classification. A comparative analysis is performed to show the effectiveness of the proposed method for both pneumonia and COVID-19 detection. Through a quantitative analysis, it is also shown that the proposed method outperforms state-of-the-art methods in terms of accuracy and achieves comparable results in terms of AUC and F-1 score while requiring significantly less labeled data.
</details>
<details>
<summary>摘要</summary>
限量的胸部X射线数据的可用性是医疗影像方法的开发中的一个重要瓶颈。自我指导学习（SSL）可以解决这个问题，通过训练无标注数据上的模型。然而，在医疗影像分析中，自我指导预训练并没有受到太多的注意。在这种工作中，我们提议一种自我指导方法，称为DINO-CXR，这是基于视力变换器的胸部X射线分类的一种新的适应。通过比较分析，我们证明了提议的方法在肺炎和COVID-19检测中的效果。通过量化分析，我们还证明了提议的方法在准确率和AUC和F-1分数方面具有比州顶峰方法更高的性能，而且需要远 fewer的标注数据。
</details></li>
</ul>
<hr>
<h2 id="Is-Last-Layer-Re-Training-Truly-Sufficient-for-Robustness-to-Spurious-Correlations"><a href="#Is-Last-Layer-Re-Training-Truly-Sufficient-for-Robustness-to-Spurious-Correlations" class="headerlink" title="Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?"></a>Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00473">http://arxiv.org/abs/2308.00473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phuong Quynh Le, Jörg Schlötterer, Christin Seifert</li>
<li>for: 该论文旨在探讨empirical risk minimization（ERM）模型在医疗领域中的应用，以及其如何处理假 correlate feature。</li>
<li>methods: 该论文使用Deep Feature Reweighting（DFR）方法来改进ERM模型的准确性，该方法只需重新训练最后一层的分类模型，使其更加适应各个群体的特点。</li>
<li>results: 研究发现，DFR方法可以改善ERM模型在医疗领域中的准确性，特别是在某些群体中的准确性。然而，DFR方法仍然可能受到假 correlate feature的影响。<details>
<summary>Abstract</summary>
Models trained with empirical risk minimization (ERM) are known to learn to rely on spurious features, i.e., their prediction is based on undesired auxiliary features which are strongly correlated with class labels but lack causal reasoning. This behavior particularly degrades accuracy in groups of samples of the correlated class that are missing the spurious feature or samples of the opposite class but with the spurious feature present. The recently proposed Deep Feature Reweighting (DFR) method improves accuracy of these worst groups. Based on the main argument that ERM mods can learn core features sufficiently well, DFR only needs to retrain the last layer of the classification model with a small group-balanced data set. In this work, we examine the applicability of DFR to realistic data in the medical domain. Furthermore, we investigate the reasoning behind the effectiveness of last-layer retraining and show that even though DFR has the potential to improve the accuracy of the worst group, it remains susceptible to spurious correlations.
</details>
<details>
<summary>摘要</summary>
模型使用隐式风险最小化（ERM）训练，有可能学习到幻觉特征，即类别标签强相关但无 causal 理解的auxiliary feature。这种行为尤其是在类别标签相关的特征 missing 或者 opposing class 存在 spurious feature 时，精度会受到负面影响。 reciently proposed Deep Feature Reweighting（DFR）方法可以提高这些最差群体的准确率。基于主要的Argument that ERM 模型可以学习核心特征，DFR 只需要在类别模型的最后一层重新训练一小组均衡数据集。在这个工作中，我们对实际数据集进行了检验，并investigated  послед layer 重新训练的理由，并证明了DFR 具有改善最差群体精度的潜在能力，但仍然可能受到幻觉相关性的影响。
</details></li>
</ul>
<hr>
<h2 id="Mirror-Natural-Evolution-Strategies"><a href="#Mirror-Natural-Evolution-Strategies" class="headerlink" title="Mirror Natural Evolution Strategies"></a>Mirror Natural Evolution Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00469">http://arxiv.org/abs/2308.00469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haishan Ye</li>
<li>for: 这个论文主要研究的是如何使用零次梯度优化法在机器学习应用中进行优化。</li>
<li>methods: 这个论文提出了一种新的重parameterized目标函数((\mu, \Sigma)),以及一种基于这个目标函数的新算法(\texttt{MiNES})。</li>
<li>results: 这个论文证明了(\texttt{MiNES})算法可以快速 converges to the minimum of the original objective function, 并且 convergence rate是(\widetilde{\mathcal{O}}(1&#x2F;k))。此外，论文还提供了Explicit convergence rate和如何covariance matrix提高 convergence rate。<details>
<summary>Abstract</summary>
The zeroth-order optimization has been widely used in machine learning applications. However, the theoretical study of the zeroth-order optimization focus on the algorithms which approximate (first-order) gradients using (zeroth-order) function value difference at a random direction. The theory of algorithms which approximate the gradient and Hessian information by zeroth-order queries is much less studied. In this paper, we focus on the theory of zeroth-order optimization which utilizes both the first-order and second-order information approximated by the zeroth-order queries. We first propose a novel reparameterized objective function with parameters $(\mu, \Sigma)$. This reparameterized objective function achieves its optimum at the minimizer and the Hessian inverse of the original objective function respectively, but with small perturbations. Accordingly, we propose a new algorithm to minimize our proposed reparameterized objective, which we call \texttt{MiNES} (mirror descent natural evolution strategy). We show that the estimated covariance matrix of \texttt{MiNES} converges to the inverse of Hessian matrix of the objective function with a convergence rate $\widetilde{\mathcal{O}}(1/k)$, where $k$ is the iteration number and $\widetilde{\mathcal{O}}(\cdot)$ hides the constant and $\log$ terms. We also provide the explicit convergence rate of \texttt{MiNES} and how the covariance matrix promotes the convergence rate.
</details>
<details>
<summary>摘要</summary>
“零次优化已经广泛应用于机器学习领域。然而，关于零次优化的理论研究主要集中在使用随机方向的函数值差来近似（first-order）梯度的算法。针对这一点，本文强调了零次优化中使用梯度和二阶函数信息的近似算法的理论研究。我们首先提出了一个新的受参数化的目标函数($\mu$, $\Sigma)$。这个受参数化的目标函数在原始目标函数的最小值和偏微分 matrix的 inverse 之间具有小的偏差。根据这个受参数化的目标函数，我们提出了一个新的算法来推算最优解，即\texttt{MiNES}（镜像演化策略）。我们证明了\texttt{MiNES} 算法中估计的 covariance matrix 的渐近级别为原始目标函数的偏微分 matrix 的 inverse，并且其渐近级别为 $\widetilde{\mathcal{O}}(1/k)$，其中 $k$ 是迭代次数，$\widetilde{\mathcal{O}}(\cdot)$ 隐藏了常数和对数项。我们还提供了\texttt{MiNES} 算法的确定性渐近级别和如何使 covariance matrix 提高渐近级别。”Note: "零次优化" in Chinese is usually translated as "zero-order optimization", but since the text is using the term "zeroth-order" to refer to the gradient and Hessian information approximated by zeroth-order queries, I have kept the term "zeroth-order" in the translation to maintain consistency with the original text.
</details></li>
</ul>
<hr>
<h2 id="Divergence-of-the-ADAM-algorithm-with-fixed-stepsize-a-very-simple-example"><a href="#Divergence-of-the-ADAM-algorithm-with-fixed-stepsize-a-very-simple-example" class="headerlink" title="Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example"></a>Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00720">http://arxiv.org/abs/2308.00720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ph. L. Toint</li>
<li>for: 这篇论文的目的是证明了一种简单的单变量函数，使得ADAM算法在缺乏误差情况下不能正确地最小化该函数。</li>
<li>methods: 该论文使用了ADAM算法，并研究了其在不同参数设置下的性能。</li>
<li>results: 研究发现，无论选择什么参数，ADAM算法在缺乏误差情况下都会导致拟合失败。<details>
<summary>Abstract</summary>
A very simple unidimensional function with Lipschitz continuous gradient is constructed such that the ADAM algorithm with constant stepsize, started from the origin, diverges when applied to minimize this function in the absence of noise on the gradient. Divergence occurs irrespective of the choice of the method parameters.
</details>
<details>
<summary>摘要</summary>
一个非常简单的一维函数，其梯度 lipschitz 连续，可以构造出来，使得 ADAM 算法，带有常数步长，从原点开始，在缺乏梯度噪声的情况下，会导致散射。这种散射不виси于方法参数的选择。Here's a word-for-word translation of the text:一个非常简单的一维函数，其梯度 lipschitz 连续，可以构造出来，使得 ADAM 算法，带有常数步长，从原点开始，在缺乏梯度噪声的情况下，会导致散射。这种散射不виси于方法参数的选择。Note that "ADAM algorithm" in the original text is translated as "ADAM 算法" in Simplified Chinese, which is the standard way to refer to the algorithm in Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Majority-Invariant-Approach-to-Patch-Robustness-Certification-for-Deep-Learning-Models"><a href="#A-Majority-Invariant-Approach-to-Patch-Robustness-Certification-for-Deep-Learning-Models" class="headerlink" title="A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models"></a>A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00452">http://arxiv.org/abs/2308.00452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kio-cs/majorcert">https://github.com/kio-cs/majorcert</a></li>
<li>paper_authors: Qilin Zhou, Zhengyuan Wei, Haipeng Wang, W. K. Chan</li>
<li>for:  Ensures the robustness of deep learning models against adversarial attacks by certifying that no patch within a given bound can manipulate the model to predict a different label.</li>
<li>methods:  Proposes a new technique called MajorCert, which finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, and then checks whether the majority invariant of all these combinations is intact to certify samples.</li>
<li>results:  Certifies samples that cannot meet the strict bars at the classifier or patch region levels, and ensures the robustness of deep learning models against adversarial attacks.<details>
<summary>Abstract</summary>
Patch robustness certification ensures no patch within a given bound on a sample can manipulate a deep learning model to predict a different label. However, existing techniques cannot certify samples that cannot meet their strict bars at the classifier or patch region levels. This paper proposes MajorCert. MajorCert firstly finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, then enumerates their combinations element-wise, and finally checks whether the majority invariant of all these combinations is intact to certify samples.
</details>
<details>
<summary>摘要</summary>
patch robustness 证明确保不会在给定范围内的样本上进行修补，以至使深度学习模型预测不同的标签。然而，现有的技术无法证明样本不能满足其严格的标准在分类器或补丁区域 уров别。这篇论文提出了 MajorCert。 MajorCert 首先找到同一个补丁区域中所有可能的标签集，然后对它们进行元素级枚举，最后检查这些组合的主要不变性是否完整，以证明样本。
</details></li>
</ul>
<hr>
<h2 id="MAiVAR-T-Multimodal-Audio-image-and-Video-Action-Recognizer-using-Transformers"><a href="#MAiVAR-T-Multimodal-Audio-image-and-Video-Action-Recognizer-using-Transformers" class="headerlink" title="MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers"></a>MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03741">http://arxiv.org/abs/2308.03741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, Naveed Akhtar</li>
<li>for: 提高多模态人体动作识别（MHAR）的效果</li>
<li>methods: 将音频模式转化到图像域，并与视频模式进行混合，以便利用多模态 Contextual richness</li>
<li>results: 对比现有State-of-the-art方法，MAiVAR-T表现出优异的性能，证明了多模态 integrate的优势<details>
<summary>Abstract</summary>
In line with the human capacity to perceive the world by simultaneously processing and integrating high-dimensional inputs from multiple modalities like vision and audio, we propose a novel model, MAiVAR-T (Multimodal Audio-Image to Video Action Recognition Transformer). This model employs an intuitive approach for the combination of audio-image and video modalities, with a primary aim to escalate the effectiveness of multimodal human action recognition (MHAR). At the core of MAiVAR-T lies the significance of distilling substantial representations from the audio modality and transmuting these into the image domain. Subsequently, this audio-image depiction is fused with the video modality to formulate a unified representation. This concerted approach strives to exploit the contextual richness inherent in both audio and video modalities, thereby promoting action recognition. In contrast to existing state-of-the-art strategies that focus solely on audio or video modalities, MAiVAR-T demonstrates superior performance. Our extensive empirical evaluations conducted on a benchmark action recognition dataset corroborate the model's remarkable performance. This underscores the potential enhancements derived from integrating audio and video modalities for action recognition purposes.
</details>
<details>
<summary>摘要</summary>
基于人类能同时处理和 интеGRATE多Modalities的高维输入，我们提出一种新的模型，MAiVAR-T（多Modal audio-Image to Video Action Recognition Transformer）。这种模型采用直观的多Modalities结合方法，主要目标是提高多modal human action recognition（MHAR）的效果。MAiVAR-T的核心在于将音频模式中的重要表示Transformed into Image domain，然后与视频模式结合以形成一个统一的表示。这种结合方法利用了音频和视频模式中的上下文富有性，从而提高动作识别。与现有的State-of-the-art策略相比，MAiVAR-T表现出色。我们在一个action recognition benchmark dataset上进行了广泛的实验测试，结果证明了模型的杰出表现。这表明了将音频和视频模式结合起来进行动作识别可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="Fair-Models-in-Credit-Intersectional-Discrimination-and-the-Amplification-of-Inequity"><a href="#Fair-Models-in-Credit-Intersectional-Discrimination-and-the-Amplification-of-Inequity" class="headerlink" title="Fair Models in Credit: Intersectional Discrimination and the Amplification of Inequity"></a>Fair Models in Credit: Intersectional Discrimination and the Amplification of Inequity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02680">http://arxiv.org/abs/2308.02680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savina Kim, Stefan Lessmann, Galina Andreeva, Michael Rovatsos</li>
<li>for: 这篇论文探讨了在借款评估中存在的算法偏见问题，以及这种偏见如何影响不同社会群体的借款获取。</li>
<li>methods: 该论文使用了西班牙微贷市场的数据，通过对自动决策系统的使用来研究 intersecting 社会分类的影响。</li>
<li>results: 研究发现，使用自动决策系统可能会导致不同社会群体之间的借款分配不均，特别是对女性、年龄、婚姻状况、单身Parent status 和孩子的数量进行分析。这些结果表明，在使用自动决策系统时，需要考虑多个社会分类的交叠影响，以确保借款获取的公正性。<details>
<summary>Abstract</summary>
The increasing usage of new data sources and machine learning (ML) technology in credit modeling raises concerns with regards to potentially unfair decision-making that rely on protected characteristics (e.g., race, sex, age) or other socio-economic and demographic data. The authors demonstrate the impact of such algorithmic bias in the microfinance context. Difficulties in assessing credit are disproportionately experienced among vulnerable groups, however, very little is known about inequities in credit allocation between groups defined, not only by single, but by multiple and intersecting social categories. Drawing from the intersectionality paradigm, the study examines intersectional horizontal inequities in credit access by gender, age, marital status, single parent status and number of children. This paper utilizes data from the Spanish microfinance market as its context to demonstrate how pluralistic realities and intersectional identities can shape patterns of credit allocation when using automated decision-making systems. With ML technology being oblivious to societal good or bad, we find that a more thorough examination of intersectionality can enhance the algorithmic fairness lens to more authentically empower action for equitable outcomes and present a fairer path forward. We demonstrate that while on a high-level, fairness may exist superficially, unfairness can exacerbate at lower levels given combinatorial effects; in other words, the core fairness problem may be more complicated than current literature demonstrates. We find that in addition to legally protected characteristics, sensitive attributes such as single parent status and number of children can result in imbalanced harm. We discuss the implications of these findings for the financial services industry.
</details>
<details>
<summary>摘要</summary>
随着新数据源和机器学习技术在信用评估中的应用，有关可能存在基于保护特征（如种族、性别、年龄）或其他社会经济和民生特征的不公正决策的担忧。作者们在微贷上下文中示出了算法偏见的影响。在评估信用方面，投险群体经历了不公正的困难，但现实中几乎没有关于不同社会分类群体之间的偏见的研究。本研究采用了 intersecting 理论，检查了信用访问中的水平偏见，包括性别、年龄、婚姻状况、单身状况和孩子的数量。这项研究使用了西班牙微贷市场作为背景，以示汇报自动化决策系统在多个社会分类群体之间的偏见。由于机器学习技术不会考虑社会好坏，我们发现，通过更加全面的考虑 intersecting 特征，可以增强算法公平的镜像，以更 authentically 激发行动，并提供更公平的前进。我们发现，尽管在高层面上，公平可能存在混乱，但在层次分析下，不公正可能会加剧，即核心公平问题可能更复杂。我们发现，除了法律保护的特征之外，敏感特征如单身状况和孩子的数量也可能导致不公正的危害。我们讨论了这些发现的影响于金融服务业。
</details></li>
</ul>
<hr>
<h2 id="SelfCheck-Using-LLMs-to-Zero-Shot-Check-Their-Own-Step-by-Step-Reasoning"><a href="#SelfCheck-Using-LLMs-to-Zero-Shot-Check-Their-Own-Step-by-Step-Reasoning" class="headerlink" title="SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning"></a>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00436">http://arxiv.org/abs/2308.00436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ningmiao/selfcheck">https://github.com/ningmiao/selfcheck</a></li>
<li>paper_authors: Ning Miao, Yee Whye Teh, Tom Rainforth</li>
<li>for: 这个论文的目的是检测语言模型是否能够识别自己的错误，而不需要外部资源。</li>
<li>methods: 作者提出了一种零shot验证方法，用于识别语言模型中的错误。他们还使用了这种验证方法来改善问答性能，通过对不同生成的答案进行权重投票。</li>
<li>results: 作者在三个数学 dataset（GSM8K、MathQA和MATH）上进行测试，发现这种验证方法能够成功识别错误，并在最终预测性能中提高了表现。<details>
<summary>Abstract</summary>
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
</details>
<details>
<summary>摘要</summary>
近些时间，大型语言模型（LLM）的进步，尤其是幂链思维（CoT）的发明，使得解释问题变得可能。然而，即使最强大的LLM也在更复杂的问题上遇到困难，需要非线性思维和多步逻辑。在这种情况下，我们询问LLM是否有能力自动识别错误，不需要外部资源。具体来说，我们研究LLM是否可以在步骤逻辑中识别个错。为此，我们提出了零shot验证方案，用于识别这些错误。然后，我们使用这种验证方案来提高问答表现，通过对不同生成的答案进行权重投票。我们在三个数学数据集（GSM8K、MathQA、MATH）上测试了这种方法，发现它成功地识别错误，并在最终预测性能中提高表现。
</details></li>
</ul>
<hr>
<h2 id="qgym-A-Gym-for-Training-and-Benchmarking-RL-Based-Quantum-Compilation"><a href="#qgym-A-Gym-for-Training-and-Benchmarking-RL-Based-Quantum-Compilation" class="headerlink" title="qgym: A Gym for Training and Benchmarking RL-Based Quantum Compilation"></a>qgym: A Gym for Training and Benchmarking RL-Based Quantum Compilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02536">http://arxiv.org/abs/2308.02536</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qutech-delft/qgym">https://github.com/qutech-delft/qgym</a></li>
<li>paper_authors: Stan van der Linde, Willem de Kok, Tariq Bontekoe, Sebastian Feld</li>
<li>for: 本研究旨在使用人工智能技术优化量子circuit编译过程，以最大化现有量子计算机的限制的利用。</li>
<li>methods: 本研究使用了人工智能学习技术，具体是强化学习（RL），通过在量子编译环境中交互学习，来寻找最佳编译策略。</li>
<li>results: 本研究提出了一个基于OpenAI gym的软件框架qgym，可以用于具体实现RL agents和算法的训练和性能评估，并提供了特定量子编译环境的高度可定制化。<details>
<summary>Abstract</summary>
Compiling a quantum circuit for specific quantum hardware is a challenging task. Moreover, current quantum computers have severe hardware limitations. To make the most use of the limited resources, the compilation process should be optimized. To improve currents methods, Reinforcement Learning (RL), a technique in which an agent interacts with an environment to learn complex policies to attain a specific goal, can be used. In this work, we present qgym, a software framework derived from the OpenAI gym, together with environments that are specifically tailored towards quantum compilation. The goal of qgym is to connect the research fields of Artificial Intelligence (AI) with quantum compilation by abstracting parts of the process that are irrelevant to either domain. It can be used to train and benchmark RL agents and algorithms in highly customizable environments.
</details>
<details>
<summary>摘要</summary>
compile quantum circuit for specific quantum hardware 是一项复杂的任务。另外，当前的量子计算机具有严重的硬件限制。为了最大化有限资源，编译过程应该进行优化。为了改进当前方法，我们可以使用人工智能学习（RL），它是一种在环境中与 Agent 交互，以学习复杂的策略，以达到特定目标的技术。在这项工作中，我们提出了qgym，一个基于 OpenAI gym 的软件框架，同时包含特制的环境，专门为量子编译而设计。qgym 的目标是将人工智能领域与量子编译相连，抽象不相关的部分，以便在高度可定制的环境中训练和测试 RL 代理和算法。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Generate-Training-Datasets-for-Robust-Semantic-Segmentation"><a href="#Learning-to-Generate-Training-Datasets-for-Robust-Semantic-Segmentation" class="headerlink" title="Learning to Generate Training Datasets for Robust Semantic Segmentation"></a>Learning to Generate Training Datasets for Robust Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02535">http://arxiv.org/abs/2308.02535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi</li>
<li>for: 提高 semantic segmentation 技术的Robustness，尤其是在安全关键应用中。</li>
<li>methods: 利用 label-to-image 生成器和 image-to-label 分割模型的Synergy，设计并训练 Robusta  conditional generative adversarial network，生成真实和可能的异常或异常图像，用于培养可靠的分割模型。</li>
<li>results: 研究表明，提出的生成模型可以显著提高 semantic segmentation 技术的Robustness，在实际干扰和分布转移中表现出色，并且在不同的批处大小和数据分布下都能够保持高度的可靠性。<details>
<summary>Abstract</summary>
Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distribution samples. Our results suggest that this approach could be valuable in safety-critical applications, where the reliability of semantic segmentation techniques is of utmost importance and comes with a limited computational budget in inference. We will release our code shortly.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BiERL-A-Meta-Evolutionary-Reinforcement-Learning-Framework-via-Bilevel-Optimization"><a href="#BiERL-A-Meta-Evolutionary-Reinforcement-Learning-Framework-via-Bilevel-Optimization" class="headerlink" title="BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization"></a>BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01207">http://arxiv.org/abs/2308.01207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chriswang98sz/bierl">https://github.com/chriswang98sz/bierl</a></li>
<li>paper_authors: Junyi Wang, Yuanyang Zhu, Zhi Wang, Yan Zheng, Jianye Hao, Chunlin Chen</li>
<li>for: 提高复杂RL问题的解决能力，尤其是通过高并行性提高ERL算法的性能。</li>
<li>methods: 提出了一种通用的meta-ERL框架，通过级别优化来同时更新内部RL模型的超参数和外部RL模型的超参数，从而避免需要先有域知识或费时优化过程。</li>
<li>results: 通过在MuJoCo和Box2D任务中进行了广泛的实验，证明了BiERL框架在许多基线上表现出色，可以适应多种ERL算法，并且可以提高RL问题的学习性能。<details>
<summary>Abstract</summary>
Evolutionary reinforcement learning (ERL) algorithms recently raise attention in tackling complex reinforcement learning (RL) problems due to high parallelism, while they are prone to insufficient exploration or model collapse without carefully tuning hyperparameters (aka meta-parameters). In the paper, we propose a general meta ERL framework via bilevel optimization (BiERL) to jointly update hyperparameters in parallel to training the ERL model within a single agent, which relieves the need for prior domain knowledge or costly optimization procedure before model deployment. We design an elegant meta-level architecture that embeds the inner-level's evolving experience into an informative population representation and introduce a simple and feasible evaluation of the meta-level fitness function to facilitate learning efficiency. We perform extensive experiments in MuJoCo and Box2D tasks to verify that as a general framework, BiERL outperforms various baselines and consistently improves the learning performance for a diversity of ERL algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tackling-Hallucinations-in-Neural-Chart-Summarization"><a href="#Tackling-Hallucinations-in-Neural-Chart-Summarization" class="headerlink" title="Tackling Hallucinations in Neural Chart Summarization"></a>Tackling Hallucinations in Neural Chart Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00399">http://arxiv.org/abs/2308.00399</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/worldhellow/hallucinations-c2t">https://github.com/worldhellow/hallucinations-c2t</a></li>
<li>paper_authors: Saad Obaid ul Islam, Iza Škrjanec, Ondřej Dušek, Vera Demberg</li>
<li>for: 减少文本生成中的幻觉现象</li>
<li>methods: 使用自然语言推理（NLI）方法处理训练数据，以减少幻觉现象</li>
<li>results: 人工评估显示，使用我们的方法可以显著减少幻觉现象，同时短化输入序列中长距离的依赖关系和添加图表标题和标签可以提高总性性能。<details>
<summary>Abstract</summary>
Hallucinations in text generation occur when the system produces text that is not grounded in the input. In this work, we tackle the problem of hallucinations in neural chart summarization. Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations. We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations. We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance.
</details>
<details>
<summary>摘要</summary>
描述文本生成中的幻觉发生 когда系统生成的文本与输入不相关。在这项工作中，我们解决了图表概要生成中的幻觉问题。我们的分析表明目标一侧的概要生成训练数据经常包含额外信息，导致幻觉。我们提议使用自然语言推理（NLI）基于的方法来预处理训练数据，并通过人工评估显示我们的方法可以有效降低幻觉。此外，我们发现短缩长距离依赖性在输入序列中和添加图表相关信息如标题和图例可以提高总性性能。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Time-Series-Anomaly-Detection-Methods-in-the-AIOps-Domain"><a href="#A-Survey-of-Time-Series-Anomaly-Detection-Methods-in-the-AIOps-Domain" class="headerlink" title="A Survey of Time Series Anomaly Detection Methods in the AIOps Domain"></a>A Survey of Time Series Anomaly Detection Methods in the AIOps Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00393">http://arxiv.org/abs/2308.00393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Zhong, Qiliang Fan, Jiacheng Zhang, Minghua Ma, Shenglin Zhang, Yongqian Sun, Qingwei Lin, Yuzhi Zhang, Dan Pei</li>
<li>for: 本研究旨在为研究者、服务运维人员和协调工程师提供一种智能化和优化操作工作流程的概述。</li>
<li>methods: 本文使用人工智能技术进行时间序列异常检测，并 explore了未来领域和下一代时间序列异常检测的发展趋势。</li>
<li>results: 本文提供了一个全面的时间序列异常检测在人工智能运维（AIOps）中的概述，并探讨了实际应用和未来发展的方向。<details>
<summary>Abstract</summary>
Internet-based services have seen remarkable success, generating vast amounts of monitored key performance indicators (KPIs) as univariate or multivariate time series. Monitoring and analyzing these time series are crucial for researchers, service operators, and on-call engineers to detect outliers or anomalies indicating service failures or significant events. Numerous advanced anomaly detection methods have emerged to address availability and performance issues. This review offers a comprehensive overview of time series anomaly detection in Artificial Intelligence for IT operations (AIOps), which uses AI capabilities to automate and optimize operational workflows. Additionally, it explores future directions for real-world and next-generation time-series anomaly detection based on recent advancements.
</details>
<details>
<summary>摘要</summary>
互联网基数服务在过去几年中取得了杰出的成功，生成了大量监控关键性表现指标（KPI），这些KPI可以是单一或多元时间序列。监控和分析这些时间序列是研究人员、服务运营员和内部工程师必须掌握的技能，以探测服务故障或重要事件的偏差或异常。随着AI技术的发展，许多高级偏差检测方法在AIOps中出现，以自动化和优化运营工作流程。此文为您提供了AIOps中时间序列偏差检测的全面回顾，同时也探讨了未来领域和下一代时间序列偏差检测的发展。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Graph-Transformer-for-Traffic-Flow-Prediction"><a href="#Counterfactual-Graph-Transformer-for-Traffic-Flow-Prediction" class="headerlink" title="Counterfactual Graph Transformer for Traffic Flow Prediction"></a>Counterfactual Graph Transformer for Traffic Flow Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00391">http://arxiv.org/abs/2308.00391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Yang, Kai Du, Xingyuan Dai, Jianwu Fang</li>
<li>for: 这个论文主要目标是提出一种基于图的Counterfactual Graph Transformer（CGT）模型，以提高交通流量预测的可解释性和可靠性。</li>
<li>methods: 该模型使用了多种注意机制，包括实例级别的解释器（如找到重要的子图），以便在交通流量预测中提供可解释的结果。它还使用了输入感知特征和图结构的推准面生成器，以生成空间和时间的counterfactual解释。</li>
<li>results: 在三个真实的公共数据集上进行了广泛的测试，结果表明，CGT可以生成可靠的解释，并且在交通流量预测中表现出色。<details>
<summary>Abstract</summary>
Traffic flow prediction (TFP) is a fundamental problem of the Intelligent Transportation System (ITS), as it models the latent spatial-temporal dependency of traffic flow for potential congestion prediction. Recent graph-based models with multiple kinds of attention mechanisms have achieved promising performance. However, existing methods for traffic flow prediction tend to inherit the bias pattern from the dataset and lack interpretability. To this end, we propose a Counterfactual Graph Transformer (CGT) model with an instance-level explainer (e.g., finding the important subgraphs) specifically designed for TFP. We design a perturbation mask generator over input sensor features at the time dimension and the graph structure on the graph transformer module to obtain spatial and temporal counterfactual explanations. By searching the optimal perturbation masks on the input data feature and graph structures, we can obtain the concise and dominant data or graph edge links for the subsequent TFP task. After re-training the utilized graph transformer model after counterfactual perturbation, we can obtain improved and interpretable traffic flow prediction. Extensive results on three real-world public datasets show that CGT can produce reliable explanations and is promising for traffic flow prediction.
</details>
<details>
<summary>摘要</summary>
做为智能交通系统（ITS）的基本问题，流量流动预测（TFP）模型了路交通流量的隐藏空间时间相依性，以预测潜在塞车。现有的图形基本模型和多种注意力机制已经实现了有前途的性能。然而，现有的交通流量预测方法往往从数据中继承偏见和无法解释。为了解决这个问题，我们提出了Counterfactual Graph Transformer（CGT）模型，该模型还包括实例级别解释器（例如，找到重要的子图），专门设计用于TFP。我们在图形变换模组中实现了阶层损害几何和时间维度的损害几何，以获取空间和时间的对照性解释。通过搜索最佳的损害几何在输入数据特征和图形结构上，我们可以获取简洁且主要的数据或图形边路连接，并将其用于后续的TFP任务。经过重新训练已使用的图形变换模型，我们可以获得改进和可解释的交通流量预测。实验结果显示，CGT可以生成可靠的解释和具有前途的交通流量预测。
</details></li>
</ul>
<hr>
<h2 id="Improving-Generalization-of-Adversarial-Training-via-Robust-Critical-Fine-Tuning"><a href="#Improving-Generalization-of-Adversarial-Training-via-Robust-Critical-Fine-Tuning" class="headerlink" title="Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning"></a>Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02533">http://arxiv.org/abs/2308.02533</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/robustlearn">https://github.com/microsoft/robustlearn</a></li>
<li>paper_authors: Kaijie Zhu, Jindong Wang, Xixu Hu, Xing Xie, Ge Yang</li>
<li>for: 提高模型的泛化能力和对外围干扰的Robustness。</li>
<li>methods: 提出了一种新的方法called Robustness Critical Fine-Tuning（RiFT），通过利用模型对Robustness的约束来提高泛化能力，同时保持对外围干扰的Robustness。</li>
<li>results: 实验表明，RiFT可以在CIFAR10、CIFAR100和Tiny-ImageNet等 datasets上提高模型的泛化能力和对外围干扰的Robustness，同时保持或甚至提高对外围干扰的Robustness。<details>
<summary>Abstract</summary>
Deep neural networks are susceptible to adversarial examples, posing a significant security risk in critical applications. Adversarial Training (AT) is a well-established technique to enhance adversarial robustness, but it often comes at the cost of decreased generalization ability. This paper proposes Robustness Critical Fine-Tuning (RiFT), a novel approach to enhance generalization without compromising adversarial robustness. The core idea of RiFT is to exploit the redundant capacity for robustness by fine-tuning the adversarially trained model on its non-robust-critical module. To do so, we introduce module robust criticality (MRC), a measure that evaluates the significance of a given module to model robustness under worst-case weight perturbations. Using this measure, we identify the module with the lowest MRC value as the non-robust-critical module and fine-tune its weights to obtain fine-tuned weights. Subsequently, we linearly interpolate between the adversarially trained weights and fine-tuned weights to derive the optimal fine-tuned model weights. We demonstrate the efficacy of RiFT on ResNet18, ResNet34, and WideResNet34-10 models trained on CIFAR10, CIFAR100, and Tiny-ImageNet datasets. Our experiments show that \method can significantly improve both generalization and out-of-distribution robustness by around 1.5% while maintaining or even slightly enhancing adversarial robustness. Code is available at https://github.com/microsoft/robustlearn.
</details>
<details>
<summary>摘要</summary>
深度神经网络容易受到攻击性例子的威胁，这对于重要应用程序来说是一个重要的安全风险。对此，我们提出了一种新的方法 called Robustness Critical Fine-Tuning（RiFT），可以提高模型的泛化能力无需妥协对抗性。RiFT的核心思想是利用模型对抗性的剩余容量，通过对抗训练后的模型的不稳定模块进行细化，以提高模型的泛化能力。我们引入了模块抗性稳定性（MRC）度量，用于评估模块对模型抗性的影响。我们可以通过这个度量，确定模型中最不稳定的模块，并对其进行细化，以获得细化后的模型 weights。然后，我们可以通过线性 interpolate between 抗性训练后的 weights 和细化后的 weights，以 derive 最佳的细化模型 weights。我们在 ResNet18、ResNet34 和 WideResNet34-10 模型上进行了 CIFAR10、CIFAR100 和 Tiny-ImageNet 数据集的实验，结果表明，\method 可以提高模型的泛化能力和外部抗性能力，同时保持或甚至提高对抗性能力。代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Shape-Completion-with-Prediction-of-Uncertain-Regions"><a href="#Shape-Completion-with-Prediction-of-Uncertain-Regions" class="headerlink" title="Shape Completion with Prediction of Uncertain Regions"></a>Shape Completion with Prediction of Uncertain Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00377">http://arxiv.org/abs/2308.00377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dlr-rm/shape-completion">https://github.com/dlr-rm/shape-completion</a></li>
<li>paper_authors: Matthias Humt, Dominik Winkelbauer, Ulrich Hillenbrand</li>
<li>for: 这 paper 是用于Shape completion的, 即从 partial observation 中预测对象的完整geometry.</li>
<li>methods: 这 paper 提出了 two novel methods for predicting uncertain regions, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator.</li>
<li>results: 对于 known and novel object instances 和 synthetic 和 real data, both novel methods outperform the two baselines in shape completion and uncertain region prediction, and avoiding the predicted uncertain regions increases the quality of grasps for all tested methods.Here’s the full text in Simplified Chinese:</li>
<li>for: 这 paper 是用于Shape completion的, 即从 partial observation 中预测对象的完整geometry.</li>
<li>methods: 这 paper 提出了 two novel methods for predicting uncertain regions, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator.</li>
<li>results: 对于 known and novel object instances 和 synthetic 和 real data, both novel methods outperform the two baselines in shape completion and uncertain region prediction, 并且避免预测的不确定区域可以提高所有测试方法的抓取质量.<details>
<summary>Abstract</summary>
Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annotations for the uncertain regions. We train on this dataset and test each method in shape completion and prediction of uncertain regions for known and novel object instances and on synthetic and real data. While direct uncertainty prediction is by far the most accurate in the segmentation of uncertain regions, both novel methods outperform the two baselines in shape completion and uncertain region prediction, and avoiding the predicted uncertain regions increases the quality of grasps for all tested methods. Web: https://github.com/DLR-RM/shape-completion
</details>
<details>
<summary>摘要</summary>
shape completion，即从部分观察到 объек的完整几何结构，在多个下游任务中具有重要意义，主要包括机器人操作。当基于对象形状重建的 плани策或预测真正的抓取时，对象形状的不确定性是不可或缺的。特别是在杂乱的对象视图中，可能存在扩展区域中对整个对象部分的存在或不存在的不确定性。为处理这个重要的情况，我们提出了两种新的方法，一种通过处理占用度分数，另一种通过直接预测不确定性指标来预测这些不确定区域。我们与两种已知的概率形状完成方法进行比较，并在shape completion和不确定区域预测方面对已知和新的对象实例进行测试，并在 sintetic 和实际数据上进行测试。而直接预测不确定性的方法在 segmentation 不确定区域中的准确性是最高，而我们的两种新方法在 shape completion 和不确定区域预测方面都超过了两个基线方法，并且避免预测的不确定区域可以提高所有测试方法的质量。网址：https://github.com/DLR-RM/shape-completion
</details></li>
</ul>
<hr>
<h2 id="MRQ-Support-Multiple-Quantization-Schemes-through-Model-Re-Quantization"><a href="#MRQ-Support-Multiple-Quantization-Schemes-through-Model-Re-Quantization" class="headerlink" title="MRQ:Support Multiple Quantization Schemes through Model Re-Quantization"></a>MRQ:Support Multiple Quantization Schemes through Model Re-Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01867">http://arxiv.org/abs/2308.01867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manasa Manohara, Sankalp Dayal, Tariq Afzal, Rahul Bakshi, Kahkuen Fu</li>
<li>for: 这个研究旨在解决深度学习模型在边缘设备上部署时，由于复杂的模型汇数和转换而受到挑战。现有的模型汇数框架，如Tensorflow QAT、TFLite PTQ和Qualcomm AIMET，仅支持有限的汇数方案。因此，深度学习模型无法轻松地汇数 для不同的固定点硬件。</li>
<li>methods: 我们提出了一种新的模型汇数方法，called MRQ（模型重新汇数），可以将现有的汇数模型转换为不同的汇数需求（例如：对称 -&gt; 非对称、非二进制尺度 -&gt; 二进制尺度）。重新汇数比从头开始汇数更简单，因为它可以避免重训成本和提供多种汇数方案的支持。我们开发了一系列重新汇数算法，包括权重调整和四处调整，以减少重新汇数误差。我们证明了，可以将MobileNetV2 QAT模型在0.64单位以下的精度损失下重新汇数为两种不同的汇数方案（即对称和对称+二进制尺度）。</li>
<li>results: 我们的研究表明，可以使用MRQ方法将MobileNetV2 QAT模型转换为不同的汇数方案，并且在NNA上部署该模型。我们的模型在Echo Show设备上得到了成功的部署。<details>
<summary>Abstract</summary>
Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU, DPU), deploying deep learning models on edge devices with fixed-point hardware is still challenging due to complex model quantization and conversion. Existing model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g., only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep learning models cannot be easily quantized for diverse fixed-point hardwares, mainly due to slightly different quantization requirements. In this paper, we envision a new type of model quantization approach called MRQ (model re-quantization), which takes existing quantized models and quickly transforms the models to meet different quantization requirements (e.g., asymmetric -> symmetric, non-power-of-2 scale -> power-of-2 scale). Re-quantization is much simpler than quantizing from scratch because it avoids costly re-training and provides support for multiple quantization schemes simultaneously. To minimize re-quantization error, we developed a new set of re-quantization algorithms including weight correction and rounding error folding. We have demonstrated that MobileNetV2 QAT model [7] can be quickly re-quantized into two different quantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with less than 0.64 units of accuracy loss. We believe our work is the first to leverage this concept of re-quantization for model quantization and models obtained from the re-quantization process have been successfully deployed on NNA in the Echo Show devices.
</details>
<details>
<summary>摘要</summary>
尽管现有多种硬件加速器（如NPU、TPU、DPU），但是在边缘设备上部署深度学习模型仍然具有挑战，主要是因为复杂的模型量化和转换。现有的模型量化框架如TensorFlow QAT [1]、TFLite PTQ [2] 和Qualcomm AIMET [3] 只支持有限的量化方案（例如，只有TF1.x QAT 中的对称性量化）。因此，深度学习模型难以被容易量化为不同的固定点硬件，主要是因为不同的量化需求略有不同。在这篇论文中，我们提出了一种新的模型量化方法，即MRQ（模型重量化），它可以将现有的量化模型快速地转换为满足不同的量化需求（例如，对称 -> 非对称、非二进制扩展 -> 二进制扩展）。重量化比量化从零开始更加简单，因为它可以避免重新训练的成本，并同时支持多种量化方案。为了最小化重量化误差，我们开发了一组新的重量化算法，包括权重修正和四舍五入误差卷积。我们已经证明了，通过MRQ方法可以将MobileNetV2 QAT模型 [7] 快速地重量化成两种不同的量化方案（即对称和对称+二进制扩展），准确率下降不足0.64个单位。我们认为，我们的工作是首次利用此类重量化方法进行模型量化，并且在NNA上部署了使用重量化过程获得的模型。
</details></li>
</ul>
<hr>
<h2 id="Learning-Green’s-Function-Efficiently-Using-Low-Rank-Approximations"><a href="#Learning-Green’s-Function-Efficiently-Using-Low-Rank-Approximations" class="headerlink" title="Learning Green’s Function Efficiently Using Low-Rank Approximations"></a>Learning Green’s Function Efficiently Using Low-Rank Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00350">http://arxiv.org/abs/2308.00350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kishanwn/decgreennet">https://github.com/kishanwn/decgreennet</a></li>
<li>paper_authors: Kishan Wimalawarne, Taiji Suzuki, Sophie Langer</li>
<li>for: 用深度学习模型解决不同类型的偏微分方程</li>
<li>methods: 使用低级分解学习绿函数，实现去除重复的计算昂贵的蒙特卡洛 integral 估计</li>
<li>results: 提高计算时间，与 PINNs 和 MOD-Net 相当准确，但计算时间更短<details>
<summary>Abstract</summary>
Learning the Green's function using deep learning models enables to solve different classes of partial differential equations. A practical limitation of using deep learning for the Green's function is the repeated computationally expensive Monte-Carlo integral approximations. We propose to learn the Green's function by low-rank decomposition, which results in a novel architecture to remove redundant computations by separate learning with domain data for evaluation and Monte-Carlo samples for integral approximation. Using experiments we show that the proposed method improves computational time compared to MOD-Net while achieving comparable accuracy compared to both PINNs and MOD-Net.
</details>
<details>
<summary>摘要</summary>
使用深度学习模型学习格林函数可以解决不同类型的部分 diferencial equations。然而，使用深度学习来学习格林函数存在重复的计算昂贵的蒙地卡罗 integral approximation。我们提议通过低级别分解来学习格林函数，这Result in a novel architecture to remove redundant computations by separate learning with domain data for evaluation and Monte-Carlo samples for integral approximation.使用实验表明，我们的方法可以比MOD-Net快速计算，并且与PINNs和MOD-Net的准确率相似。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-ensemble-selection-based-on-Deep-Neural-Network-Uncertainty-Estimation-for-Adversarial-Robustness"><a href="#Dynamic-ensemble-selection-based-on-Deep-Neural-Network-Uncertainty-Estimation-for-Adversarial-Robustness" class="headerlink" title="Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness"></a>Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00346">http://arxiv.org/abs/2308.00346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruoxi Qin, Linyuan Wang, Xuehui Du, Xingyuan Chen, Bin Yan<br>for: 这个论文是为了提高图像识别器的Robustness而写的。methods: 这篇论文使用了动态ensemble选择技术来提高模型的鲁棒性，这种技术在模型层次上考虑动态特征来提高模型的鲁棒性。results:  compare with之前的动态方法和静态对抗训练模型，这种方法可以很好地提高鲁棒性Result without sacrificing accuracy。<details>
<summary>Abstract</summary>
The deep neural network has attained significant efficiency in image recognition. However, it has vulnerable recognition robustness under extensive data uncertainty in practical applications. The uncertainty is attributed to the inevitable ambient noise and, more importantly, the possible adversarial attack. Dynamic methods can effectively improve the defense initiative in the arms race of attack and defense of adversarial examples. Different from the previous dynamic method depend on input or decision, this work explore the dynamic attributes in model level through dynamic ensemble selection technology to further protect the model from white-box attacks and improve the robustness. Specifically, in training phase the Dirichlet distribution is apply as prior of sub-models' predictive distribution, and the diversity constraint in parameter space is introduced under the lightweight sub-models to construct alternative ensembel model spaces. In test phase, the certain sub-models are dynamically selected based on their rank of uncertainty value for the final prediction to ensure the majority accurate principle in ensemble robustness and accuracy. Compared with the previous dynamic method and staic adversarial traning model, the presented approach can achieve significant robustness results without damaging accuracy by combining dynamics and diversity property.
</details>
<details>
<summary>摘要</summary>
深度神经网络在图像识别中已经实现了显著的效率。然而，它在实际应用中面临着广泛的数据不确定性的挑战。这种不确定性来自于不可避免的环境噪声以及可能的敌意攻击。动态方法可以有效地提高模型的防御性能，在攻击者和防御者之间的攻击战中。与之前的动态方法不同，本工作通过在模型层次上实现动态集合选择技术，以保护模型免受白盒攻击并提高其Robustness。在训练阶段， Dirichlet 分布被应用于子模型预测分布的先验 Distribution，并在轻量级子模型参数空间中引入多样性约束。在测试阶段，根据其最终预测结果的不确定性值排名，选择特定的子模型来实现最终预测。相比之前的动态方法和静态敌意训练模型，提出的方法可以在不损害准确率的情况下实现显著的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Monitoring-Algorithmic-Fairness-under-Partial-Observations"><a href="#Monitoring-Algorithmic-Fairness-under-Partial-Observations" class="headerlink" title="Monitoring Algorithmic Fairness under Partial Observations"></a>Monitoring Algorithmic Fairness under Partial Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00341">http://arxiv.org/abs/2308.00341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas A. Henzinger, Konstantin Kueffner, Kaushik Mallik</li>
<li>for: 本研究旨在为已经部署的机器学习算法提供runtime fairness监测，以确保它们在做出决策时保持公平和不偏袋。</li>
<li>methods: 本研究使用了partially observed Markov chains (POMC)模型，并使用了 arithmetic expressions over the expected values of numerical functions on event sequences来Specify fairness properties。</li>
<li>results: 研究人员通过观察系统的长期运行，并使用了一种名为PAC-estimates的计算机轻量级监测方法，可以在系统的不同执行情况下提供有关系统公平性的估计。这些监测方法可以帮助确保机器学习算法在做出决策时保持公平和不偏袋。<details>
<summary>Abstract</summary>
As AI and machine-learned software are used increasingly for making decisions that affect humans, it is imperative that they remain fair and unbiased in their decisions. To complement design-time bias mitigation measures, runtime verification techniques have been introduced recently to monitor the algorithmic fairness of deployed systems. Previous monitoring techniques assume full observability of the states of the (unknown) monitored system. Moreover, they can monitor only fairness properties that are specified as arithmetic expressions over the probabilities of different events. In this work, we extend fairness monitoring to systems modeled as partially observed Markov chains (POMC), and to specifications containing arithmetic expressions over the expected values of numerical functions on event sequences. The only assumptions we make are that the underlying POMC is aperiodic and starts in the stationary distribution, with a bound on its mixing time being known. These assumptions enable us to estimate a given property for the entire distribution of possible executions of the monitored POMC, by observing only a single execution. Our monitors observe a long run of the system and, after each new observation, output updated PAC-estimates of how fair or biased the system is. The monitors are computationally lightweight and, using a prototype implementation, we demonstrate their effectiveness on several real-world examples.
</details>
<details>
<summary>摘要</summary>
“作为人工智能和机器学习软件在做出影响人类决策的情况下，它们必须保持公平和无偏见。为了补充设计时的偏见缓和措施，runtime监控技术在最近才被引入，以监控部署系统的算法公平性。先前的监控技术假设了监控系统的完整可观察性，并且仅能监控公平性属性是指的数学表达式中的概率分布。在这个工作中，我们将公平监控扩展到模型为部分可观察Markov链（POMC）的系统，并将属性指定为数学表达式中的预期值。我们假设了背景POMC是无限循环的，并且知道其混合时间的上限。这些假设允许我们预估一个属性的整个分布，通过观察单一的执行。我们的监控器在观察系统的长期执行后，每次新的观察结果后，都会产生新的PAC估计，以衡量系统的公平性。我们的监控器 Computationally lightweight，我们使用一个原型实现，并在实际应用中证明了它们的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Threshold-aware-Learning-to-Generate-Feasible-Solutions-for-Mixed-Integer-Programs"><a href="#Threshold-aware-Learning-to-Generate-Feasible-Solutions-for-Mixed-Integer-Programs" class="headerlink" title="Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs"></a>Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00327">http://arxiv.org/abs/2308.00327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taehyun Yoon, Jinwon Choi, Hyokun Yun, Sungbin Lim</li>
<li>for:  solving combinatorial optimization (CO) problems with a high-quality and feasible solution in a limited time.</li>
<li>methods:  using neural diving (ND) to generate partial discrete variable assignments, and a post-hoc method to optimize the coverage.</li>
<li>results:  achieving state-of-the-art performance in NeurIPS ML4CO datasets, with an optimality gap of 0.45% and a ten-fold improvement over SCIP within the one-minute time limit.<details>
<summary>Abstract</summary>
Finding a high-quality feasible solution to a combinatorial optimization (CO) problem in a limited time is challenging due to its discrete nature. Recently, there has been an increasing number of machine learning (ML) methods for addressing CO problems. Neural diving (ND) is one of the learning-based approaches to generating partial discrete variable assignments in Mixed Integer Programs (MIP), a framework for modeling CO problems. However, a major drawback of ND is a large discrepancy between the ML and MIP objectives, i.e., variable value classification accuracy over primal bound. Our study investigates that a specific range of variable assignment rates (coverage) yields high-quality feasible solutions, where we suggest optimizing the coverage bridges the gap between the learning and MIP objectives. Consequently, we introduce a post-hoc method and a learning-based approach for optimizing the coverage. A key idea of our approach is to jointly learn to restrict the coverage search space and to predict the coverage in the learned search space. Experimental results demonstrate that learning a deep neural network to estimate the coverage for finding high-quality feasible solutions achieves state-of-the-art performance in NeurIPS ML4CO datasets. In particular, our method shows outstanding performance in the workload apportionment dataset, achieving the optimality gap of 0.45%, a ten-fold improvement over SCIP within the one-minute time limit.
</details>
<details>
<summary>摘要</summary>
寻找一个高质量可行的解决方案 для combinatorial optimization（CO）问题在有限时间内是具有挑战性的，这主要是因为CO问题的离散性质。当前， Machine Learning（ML）方法在解决CO问题上的应用越来越广泛。Neural Diving（ND）是一种学习基于方法，用于生成混合整数程序（MIP）中的部分逻辑变量分配。然而，ND的一个主要缺点是ML目标和MIP目标之间的大差异，即变量值分类准确率过 primal bound。我们的研究表明，在特定的变量分配率范围内，可以获得高质量可行的解决方案，我们建议优化该覆盖率来bridging the gap между学习和MIP目标。因此，我们提出了一种后续方法和学习基于方法来优化覆盖率。我们的方法的关键思想是同时学习Restrict the coverage search space和在学习的搜索空间中预测覆盖。实验结果表明，通过学习深度神经网络来估算覆盖以找到高质量可行的解决方案，可以在NeurIPS ML4CO数据集中达到状态之最的性能。尤其是在工作负担分配数据集中，我们的方法实现了优化率0.45%，对SCIP的一分钟时间限制进行了十倍的提升。
</details></li>
</ul>
<hr>
<h2 id="Pixel-to-policy-DQN-Encoders-for-within-cross-game-reinforcement-learning"><a href="#Pixel-to-policy-DQN-Encoders-for-within-cross-game-reinforcement-learning" class="headerlink" title="Pixel to policy: DQN Encoders for within &amp; cross-game reinforcement learning"></a>Pixel to policy: DQN Encoders for within &amp; cross-game reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00318">http://arxiv.org/abs/2308.00318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashrya Agrawal, Priyanshi Shah, Sourabh Prakash</li>
<li>for: 这 paper 的目的是研究如何通过transfer learning进行强化学习，以提高RL表现和效率。</li>
<li>methods: 这 paper 使用了多种RL模型，包括从零开始学习和基于DQN的转移学习。</li>
<li>results: 这 paper 的结果表明，通过转移学习可以提高RL表现和效率，并且在不同的游戏环境中达到人类水平的表现。具体来说，这 paper 的模型在 Assault 和 Space Invader 环境中 achieved mean episode reward of 533.42 和 402.17 分别，这些结果都是非常出色的。<details>
<summary>Abstract</summary>
Reinforcement Learning can be applied to various tasks, and environments. Many of these environments have a similar shared structure, which can be exploited to improve RL performance on other tasks. Transfer learning can be used to take advantage of this shared structure, by learning policies that are transferable across different tasks and environments and can lead to more efficient learning as well as improved performance on a wide range of tasks. This work explores as well as compares the performance between RL models being trained from the scratch and on different approaches of transfer learning. Additionally, the study explores the performance of a model trained on multiple game environments, with the goal of developing a universal game-playing agent as well as transfer learning a pre-trained encoder using DQN, and training it on the same game or a different game. Our DQN model achieves a mean episode reward of 46.16 which even beats the human-level performance with merely 20k episodes which is significantly lower than deepmind's 1M episodes. The achieved mean rewards of 533.42 and 402.17 on the Assault and Space Invader environments respectively, represent noteworthy performance on these challenging environments.
</details>
<details>
<summary>摘要</summary>
� Reinforcement Learning 可以应用到不同的任务和环境中。许多这些环境都有相似的共同结构，可以利用这个共同结构来提高RL表现，例如通过将多个任务和环境的学习策略转移到其他任务和环境中，以提高学习效率和任务表现的多样性。这个研究将探讨RL模型是否从零开始学习或是否使用传递学习，以及将一个多环境训练的模型转移到不同的游戏环境中。我们的DQN模型在不同的游戏环境中获得了46.16的平均集成奖，甚至超越了人类水准的表现，仅需20k集成 Episodes，比deepmind的1M Episodes要来得要少。在Assault和Space Invader环境中，我们的模型获得了533.42和402.17的平均奖励，表示在这些挑战性的环境中表现非常出色。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Instance-Reweighted-Adversarial-Training"><a href="#Doubly-Robust-Instance-Reweighted-Adversarial-Training" class="headerlink" title="Doubly Robust Instance-Reweighted Adversarial Training"></a>Doubly Robust Instance-Reweighted Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00311">http://arxiv.org/abs/2308.00311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daouda Sow, Sen Lin, Zhangyang Wang, Yingbin Liang</li>
<li>for: 本文目的是提出一种新的双重强化实例权重对抗训练方法，以提高模型对攻击最弱的数据点的 Robustness。</li>
<li>methods: 本文使用了分布式 robust optimization（DRO）技术来获取重要性权重，并通过优化KL偏好函数来提出新的算法，具有理论上的收敛保证。</li>
<li>results: 对标准分类 datasets 进行实验，本文的提出方法比相关基线方法在平均 robust性方面表现出色，同时对攻击最弱的数据点的 Robustness 也有显著改善。<details>
<summary>Abstract</summary>
Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee. Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes will be available soon.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将重要性权重分配给敌对数据实现了在有限模型容量下训练敌对抗击网络的很大成功。然而，现有的实例权重对抗训练（AT）方法仍然依赖于规则和/或几何解释来确定这些重要性权重，使这些算法缺乏正式的理论基础保证。此外，现有研究表明，对抗训练受到数据分布中的非均匀抗击性的困扰，例如，某些类别的数据点可能比其他类别更容易受到敌对攻击。为解决这两个问题，在这篇论文中，我们提出了一种新的双重不确定实例权重AT框架，可以通过探索分布式不确定优化（DRO）技术来获得重要性权重，同时提高最容易受到攻击的数据点的Robustness。具体来说，我们的重要性权重通过优化KL分布regularized损失函数来获得，这allow us to设计新的算法，并提供了理论上的准确整合保证。实验表明，我们提出的方法在标准分类dataset上表现出了与相关基线方法相比的平均Robustness和对攻击最弱数据点的Robustness进行了改进。代码将很快 disponible。
</details></li>
</ul>
<hr>
<h2 id="GradOrth-A-Simple-yet-Efficient-Out-of-Distribution-Detection-with-Orthogonal-Projection-of-Gradients"><a href="#GradOrth-A-Simple-yet-Efficient-Out-of-Distribution-Detection-with-Orthogonal-Projection-of-Gradients" class="headerlink" title="GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients"></a>GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00310">http://arxiv.org/abs/2308.00310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sima Behpour, Thang Doan, Xin Li, Wenbin He, Liang Gou, Liu Ren</li>
<li>for: 本研究旨在提出一种基于特征Map的out-of-distribution（OOD）检测方法，以确保机器学习模型在实际应用中安全部署。</li>
<li>methods: 本研究使用了一种新的方法 called GradOrth，它基于在ID数据上最重要的参数的观察，发现OOD数据的关键特征在ID数据的低维度子空间中。特别是，通过计算ID数据中考虑重要的子空间上的梯度 проек 值来识别OOD数据。</li>
<li>results: 对比现有方法，GradOrth方法在FPR95下减少了OOD检测中的均 FALSE POSITIVE 率达到8%，显示出了显著的提高。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) data is crucial for ensuring the safe deployment of machine learning models in real-world applications. However, existing OOD detection approaches primarily rely on the feature maps or the full gradient space information to derive OOD scores neglecting the role of most important parameters of the pre-trained network over in-distribution (ID) data. In this study, we propose a novel approach called GradOrth to facilitate OOD detection based on one intriguing observation that the important features to identify OOD data lie in the lower-rank subspace of in-distribution (ID) data. In particular, we identify OOD data by computing the norm of gradient projection on the subspaces considered important for the in-distribution data. A large orthogonal projection value (i.e. a small projection value) indicates the sample as OOD as it captures a weak correlation of the ID data. This simple yet effective method exhibits outstanding performance, showcasing a notable reduction in the average false positive rate at a 95% true positive rate (FPR95) of up to 8% when compared to the current state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
检测不同分布（OOD）数据是机器学习模型在实际应用中安全部署的关键。然而，现有的OOD检测方法主要基于特征地图或全Gradient空间信息来生成OOD分数，忽视了预训练网络中最重要的参数的作用。在这种研究中，我们提出了一种新的方法，即GradOrth，用于基于ID数据中最重要的特征来进行OOD检测。具体来说，我们通过计算ID数据中考虑重要的子空间上的梯度投影来识别OOD数据。如果梯度投影的正交值（即投影值较小）强大，则表示该样本为OOD，因为它表示ID数据的弱相关性。这种简单 yet 有效的方法在性能方面表现出色，可以在95%的真正正确率（FPR95）下减少了相对于当前状态的欺骗率达到8%。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-TCAD-Simulations-with-Universal-Device-Encoding-and-Graph-Attention-Networks"><a href="#Revolutionizing-TCAD-Simulations-with-Universal-Device-Encoding-and-Graph-Attention-Networks" class="headerlink" title="Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks"></a>Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11624">http://arxiv.org/abs/2308.11624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangxi Fan, Kain Lu Low</li>
<li>for: 提出了一种基于人工智能（AI）和图表示法的半导体设备编码方法，用于TCAD设备仿真。</li>
<li>methods: 提出了一种基于图表示法的通用编码方案，考虑了材料层和设备层嵌入，还引入了一种新的空间关系嵌入， inspirited by interpolation operations typically used in finite element meshing。</li>
<li>results: 通过使用一种新的图注意力网络（RelGAT），实现了Surrogate Poisson伪 simulate和current-voltage（IV）预测，基于漫游扩散模型。<details>
<summary>Abstract</summary>
An innovative methodology that leverages artificial intelligence (AI) and graph representation for semiconductor device encoding in TCAD device simulation is proposed. A graph-based universal encoding scheme is presented that not only considers material-level and device-level embeddings, but also introduces a novel spatial relationship embedding inspired by interpolation operations typically used in finite element meshing. Universal physical laws from device simulations are leveraged for comprehensive data-driven modeling, which encompasses surrogate Poisson emulation and current-voltage (IV) prediction based on drift-diffusion model. Both are achieved using a novel graph attention network, referred to as RelGAT. Comprehensive technical details based on the device simulator Sentaurus TCAD are presented, empowering researchers to adopt the proposed AI-driven Electronic Design Automation (EDA) solution at the device level.
</details>
<details>
<summary>摘要</summary>
一种创新的方法ологиía，利用人工智能（AI）和图表示法来实现半导体设备编码在TCAD设备仿真中，被提出。这种图基本 universal encoding scheme不仅考虑材料层和设备层嵌入，还引入了一种新的空间关系嵌入， Draw inspiration from interpolation operations typically used in finite element meshing. 通过利用设备仿真中的universal physical laws，实现了全面的数据驱动模拟，包括Surrogate Poisson观测和电压-电流（IV）预测，基于漂移扩散模型。这些都是通过一种新的图注意力网络， referred to as RelGAT，实现的。提供了基于设备仿真器Sentaurus TCAD的完整技术详细信息，使研究人员可以在设备层采用该人工智能驱动电子设计自动化（EDA）解决方案。
</details></li>
</ul>
<hr>
<h2 id="Adapt-and-Decompose-Efficient-Generalization-of-Text-to-SQL-via-Domain-Adapted-Least-To-Most-Prompting"><a href="#Adapt-and-Decompose-Efficient-Generalization-of-Text-to-SQL-via-Domain-Adapted-Least-To-Most-Prompting" class="headerlink" title="Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting"></a>Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02582">http://arxiv.org/abs/2308.02582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aseem Arora, Shabbirhussain Bhaisaheb, Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff</li>
<li>for: 本研究旨在提高文本到SQL语义解析的跨频域和跨组合部署性。</li>
<li>methods: 我们提出了一种算法，通过在训练集中进行离线采样，生成一个固定的通用提示（GP），以及一种自适应的DA-GP，以更好地处理跨频域泛化。</li>
<li>results: 我们的方法在KaggleDBQA数据集上显示了superior的性能，并且在不同的LLM和数据库上 consistently improve the performance of LTMP-DA-GP over GP, highlighting the efficacy and model agnostic benefits of our prompt-based adapt and decompose approach。<details>
<summary>Abstract</summary>
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to be performed one-time per new database with minimal human intervention. Our approach demonstrates superior performance on the KaggleDBQA dataset, designed to evaluate generalizability for the Text-to-SQL task. We further showcase consistent performance improvement of LTMP-DA-GP over GP, across LLMs and databases of KaggleDBQA, highlighting the efficacy and model agnostic benefits of our prompt based adapt and decompose approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Study-of-Unsupervised-Evaluation-Metrics-for-Practical-and-Automatic-Domain-Adaptation"><a href="#A-Study-of-Unsupervised-Evaluation-Metrics-for-Practical-and-Automatic-Domain-Adaptation" class="headerlink" title="A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation"></a>A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00287">http://arxiv.org/abs/2308.00287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghao Chen, Zepeng Gao, Shuai Zhao, Qibo Qiu, Wenxiao Wang, Binbin Lin, Xiaofei He<br>for: This paper aims to find an evaluation metric for unsupervised domain adaptation (UDA) methods that does not require target validation labels.methods: The authors use a metric based on mutual information of the model prediction and incorporate source accuracy into the metric. They also employ a new MLP classifier that is held out during training and integrate the enhanced metric with data augmentation.results: The proposed Augmentation Consistency Metric (ACM) significantly improves the evaluation of UDA methods and outperforms previous experiment settings. The authors also demonstrate the effectiveness of their proposed metric through large-scale experiments and show that it can automatically search for the optimal hyper-parameter set, achieving superior performance compared to manually tuned sets across four common benchmarks.<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) methods facilitate the transfer of models to target domains without labels. However, these methods necessitate a labeled target validation set for hyper-parameter tuning and model selection. In this paper, we aim to find an evaluation metric capable of assessing the quality of a transferred model without access to target validation labels. We begin with the metric based on mutual information of the model prediction. Through empirical analysis, we identify three prevalent issues with this metric: 1) It does not account for the source structure. 2) It can be easily attacked. 3) It fails to detect negative transfer caused by the over-alignment of source and target features. To address the first two issues, we incorporate source accuracy into the metric and employ a new MLP classifier that is held out during training, significantly improving the result. To tackle the final issue, we integrate this enhanced metric with data augmentation, resulting in a novel unsupervised UDA metric called the Augmentation Consistency Metric (ACM). Additionally, we empirically demonstrate the shortcomings of previous experiment settings and conduct large-scale experiments to validate the effectiveness of our proposed metric. Furthermore, we employ our metric to automatically search for the optimal hyper-parameter set, achieving superior performance compared to manually tuned sets across four common benchmarks. Codes will be available soon.
</details>
<details>
<summary>摘要</summary>
无监督领域适应（UDA）方法可以将模型传输到目标领域无需标签。然而，这些方法需要一个标注的目标验证集进行超参数调整和模型选择。在这篇论文中，我们想找到一个无需目标验证集的评价指标，可以评估传输模型的质量。我们从模型预测的共同信息基础出发，并通过实验分析发现了三个常见问题：1）它不考虑源结构。2）它可以被轻松攻击。3）它无法探测源和目标特征的过对齐导致的负转移。为了解决第一个问题，我们将源准确率 incorporated into the metric，并使用一个在训练中保持隐藏的新的多层感知（MLP）分类器，显著提高结果。为了解决第三个问题，我们将这种加强的指标与数据扩展相结合，得到了一个新的无监督UDA指标——扩展一致指标（ACM）。此外，我们还证明了先前的实验设置的缺陷，并进行了大规模的实验 validate our proposed metric的有效性。最后，我们使用我们的指标自动搜索最佳超参数集，在四个常见的 benchmark上达到了手动调整的超参数集的比较优秀性。代码将很快地发布。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Modeling-through-Hyper-Bayesian-Optimization"><a href="#Predictive-Modeling-through-Hyper-Bayesian-Optimization" class="headerlink" title="Predictive Modeling through Hyper-Bayesian Optimization"></a>Predictive Modeling through Hyper-Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00285">http://arxiv.org/abs/2308.00285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manisha Senadeera, Santu Rana, Sunil Gupta, Svetha Venkatesh</li>
<li>for: 本研究旨在提高模型选择和 Bayesian 优化（BO）的效率，并输出黑obox函数的信息。</li>
<li>methods: 本文提出了一种新的模型选择和 BO  интеграции方法，通过在模型空间和函数空间之间往返，使用分数函数捕捉模型的质量，并将其反馈到函数空间中，以便更快地 convergence 到函数的最优点。</li>
<li>results: 实验结果表明，与标准 BO 相比，本方法可以大幅提高样本效率，并输出黑obox函数的信息。同时，本文也证明了该方法的收敛性。<details>
<summary>Abstract</summary>
Model selection is an integral problem of model based optimization techniques such as Bayesian optimization (BO). Current approaches often treat model selection as an estimation problem, to be periodically updated with observations coming from the optimization iterations. In this paper, we propose an alternative way to achieve both efficiently. Specifically, we propose a novel way of integrating model selection and BO for the single goal of reaching the function optima faster. The algorithm moves back and forth between BO in the model space and BO in the function space, where the goodness of the recommended model is captured by a score function and fed back, capturing how well the model helped convergence in the function space. The score function is derived in such a way that it neutralizes the effect of the moving nature of the BO in the function space, thus keeping the model selection problem stationary. This back and forth leads to quick convergence for both model selection and BO in the function space. In addition to improved sample efficiency, the framework outputs information about the black-box function. Convergence is proved, and experimental results show significant improvement compared to standard BO.
</details>
<details>
<summary>摘要</summary>
<<SYS>>功能优化技术中的模型选择问题是一个基本问题。现有的方法通常将模型选择视为一个估计问题，通过优化迭代获取观测数据来更新估计。在这篇论文中，我们提出了一种新的方法，可以更加快速地达到函数的最优点。这种方法是将模型选择和优化迭代结合在一起，通过一个得分函数来捕捉模型对函数空间的贡献，并将其反馈到函数空间中。这个得分函数是以一种方式定义的，使得它在模型移动的情况下保持静止，因此可以准确地衡量模型在函数空间中的性能。这种往返的运动会使得模型选择和优化迭代在函数空间中更快速地 converges。此外，这种方法还可以提供关于黑obox函数的信息，并且可以证明其 converges。实验结果表明，与标准BO相比，这种方法可以带来显著的改善。
</details></li>
</ul>
<hr>
<h2 id="CLAMS-A-Cluster-Ambiguity-Measure-for-Estimating-Perceptual-Variability-in-Visual-Clustering"><a href="#CLAMS-A-Cluster-Ambiguity-Measure-for-Estimating-Perceptual-Variability-in-Visual-Clustering" class="headerlink" title="CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering"></a>CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00284">http://arxiv.org/abs/2308.00284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeon Jeon, Ghulam Jilani Quadri, Hyunwook Lee, Paul Rosen, Danielle Albers Szafir, Jinwook Seo</li>
<li>For: This paper aims to address the problem of perceptual variability in visual clustering, which can lead to unreliable data analysis.* Methods: The paper introduces a data-driven visual quality measure called CLAMS, which uses a regression module to estimate human-judged separability of clusters and predict cluster ambiguity.* Results: The paper shows that CLAMS outperforms widely-used clustering techniques in predicting ground truth cluster ambiguity, and exhibits performance on par with human annotators. Additionally, the paper presents two applications for optimizing and benchmarking data mining techniques using CLAMS.<details>
<summary>Abstract</summary>
Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity. To this end, we introduce CLAMS, a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates the human-judged separability of two clusters. Then, CLAMS predicts cluster ambiguity by analyzing the aggregated results of all pairwise separability between clusters that are generated by the module. CLAMS outperforms widely-used clustering techniques in predicting ground truth cluster ambiguity. Meanwhile, CLAMS exhibits performance on par with human annotators. We conclude our work by presenting two applications for optimizing and benchmarking data mining techniques using CLAMS. The interactive demo of CLAMS is available at clusterambiguity.dev.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT视觉划分是一种常见的感知任务在散点图中，支持多种数据分析任务（例如，群集识别）。然而，即使使用同一个散点图，人们在进行视觉划分时可能会有不同的方式和不同的识别结果，这是因为人们之间存在差异和扩散的群集边界。这种感知变化会对数据分析基于视觉划分的可靠性提出各种 вопро题。在这项研究中，我们研究了在进行视觉划分时的感知变化，我们称之为划分混乱。为了实现这一目标，我们引入了一种数据驱动的视觉质量指标，称为CLAMS，可以自动预测散点图中划分的混乱程度。我们首先进行了一项质量研究，以确定影响视觉划分中cluster的关键因素（例如，群集之间的距离或者群集的大小差异）。根据研究发现，我们部署了一个回归模块，可以估算两个群集之间的人类评估分别。然后，CLAMS根据这些结果对所有对应的划分进行分析，并预测划分的混乱程度。我们发现，CLAMS可以超过常见的划分技术，并且与人类标注器表现相当。我们结束这项研究，并在散点图中优化和Benchmarking数据挖掘技术的应用中采用CLAMS。CLAMS的交互示例可以在clusterambiguity.dev上查看。TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="ZADU-A-Python-Library-for-Evaluating-the-Reliability-of-Dimensionality-Reduction-Embeddings"><a href="#ZADU-A-Python-Library-for-Evaluating-the-Reliability-of-Dimensionality-Reduction-Embeddings" class="headerlink" title="ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings"></a>ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00282">http://arxiv.org/abs/2308.00282</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hj-n/zadu">https://github.com/hj-n/zadu</a></li>
<li>paper_authors: Hyeon Jeon, Aeri Cho, Jinhwa Jang, Soohyun Lee, Jake Hyun, Hyung-Kwon Ko, Jaemin Jo, Jinwook Seo</li>
<li>for: 本研究旨在提供一个Python库（ZADU），用于评估维度缩减（DR） embedding 的可靠性。</li>
<li>methods: ZADU 提供了广泛的扭曲度量，自动优化扭曲度量的执行，并将个别点的贡献给扭曲度量提供，以便详细分析 DR embedding。</li>
<li>results: 通过一个实际的实验，我们验证了我们的优化方案可以快速执行扭曲度量，并且 ZADU 可以实现对 DR embedding 的详细分析。<details>
<summary>Abstract</summary>
Dimensionality reduction (DR) techniques inherently distort the original structure of input high-dimensional data, producing imperfect low-dimensional embeddings. Diverse distortion measures have thus been proposed to evaluate the reliability of DR embeddings. However, implementing and executing distortion measures in practice has so far been time-consuming and tedious. To address this issue, we present ZADU, a Python library that provides distortion measures. ZADU is not only easy to install and execute but also enables comprehensive evaluation of DR embeddings through three key features. First, the library covers a wide range of distortion measures. Second, it automatically optimizes the execution of distortion measures, substantially reducing the running time required to execute multiple measures. Last, the library informs how individual points contribute to the overall distortions, facilitating the detailed analysis of DR embeddings. By simulating a real-world scenario of optimizing DR embeddings, we verify that our optimization scheme substantially reduces the time required to execute distortion measures. Finally, as an application of ZADU, we present another library called ZADUVis that allows users to easily create distortion visualizations that depict the extent to which each region of an embedding suffers from distortions.
</details>
<details>
<summary>摘要</summary>
维度减少（DR）技术自然地扭曲输入高维数据的原始结构，生成不完美的低维嵌入。为了评估DR嵌入的可靠性，各种不同的扭曲度指标已经被提出。然而，在实践中实施和执行这些指标仍然是一项时间消耗和繁琐的任务。为解决这个问题，我们介绍了ZADU，一个Python库，它提供了多种扭曲度指标，并且可以自动优化执行这些指标，大幅减少执行多个指标所需的时间。此外，ZADU还可以告诉你具体的点对总的扭曲度做出了贡献，这使得可以详细分析DR嵌入。通过一个实际的优化DR嵌入场景的示例，我们证明了我们的优化方案可以减少执行扭曲度指标所需的时间。最后，作为ZADU的应用，我们介绍了一个名为ZADUVis的库，它允许用户轻松地创建扭曲度视觉化，这些视觉化可以显示嵌入中每个区域受到的扭曲度程度。
</details></li>
</ul>
<hr>
<h2 id="Data-Collaboration-Analysis-applied-to-Compound-Datasets-and-the-Introduction-of-Projection-data-to-Non-IID-settings"><a href="#Data-Collaboration-Analysis-applied-to-Compound-Datasets-and-the-Introduction-of-Projection-data-to-Non-IID-settings" class="headerlink" title="Data Collaboration Analysis applied to Compound Datasets and the Introduction of Projection data to Non-IID settings"></a>Data Collaboration Analysis applied to Compound Datasets and the Introduction of Projection data to Non-IID settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00280">http://arxiv.org/abs/2308.00280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akihiro Mizoguchi, Anna Bogdanova, Akira Imakura, Tetsuya Sakurai</li>
<li>for: 预测化学物质的性能（prediction of chemical compound properties）</li>
<li>methods: 使用分布式机器学习（distributed machine learning）和数据合作分析（data collaboration analysis），以及一种改进的方法called数据合作分析使用投影数据（DCPd）</li>
<li>results: 比较了不同方法在非标一致分布（non-IID）和标一致分布（IID）下的分类精度（classification accuracy），结果显示DCPd在非标一致分布下表现最好，而且与其他方法的性能差异较小。<details>
<summary>Abstract</summary>
Given the time and expense associated with bringing a drug to market, numerous studies have been conducted to predict the properties of compounds based on their structure using machine learning. Federated learning has been applied to compound datasets to increase their prediction accuracy while safeguarding potentially proprietary information. However, federated learning is encumbered by low accuracy in not identically and independently distributed (non-IID) settings, i.e., data partitioning has a large label bias, and is considered unsuitable for compound datasets, which tend to have large label bias. To address this limitation, we utilized an alternative method of distributed machine learning to chemical compound data from open sources, called data collaboration analysis (DC). We also proposed data collaboration analysis using projection data (DCPd), which is an improved method that utilizes auxiliary PubChem data. This improves the quality of individual user-side data transformations for the projection data for the creation of intermediate representations. The classification accuracy, i.e., area under the curve in the receiver operating characteristic curve (ROC-AUC) and AUC in the precision-recall curve (PR-AUC), of federated averaging (FedAvg), DC, and DCPd was compared for five compound datasets. We determined that the machine learning performance for non-IID settings was in the order of DCPd, DC, and FedAvg, although they were almost the same in identically and independently distributed (IID) settings. Moreover, the results showed that compared to other methods, DCPd exhibited a negligible decline in classification accuracy in experiments with different degrees of label bias. Thus, DCPd can address the low performance in non-IID settings, which is one of the challenges of federated learning.
</details>
<details>
<summary>摘要</summary>
由于药物上市需要时间和成本，许多研究已经进行了预测药物的性质基于其结构使用机器学习。 federated learning已经应用于药物数据集来提高预测精度，但是在非标一致分布（non-IID）的设置下， federated learning 的准确率低，因此不适合药物数据集，这些数据集往往具有大的标签偏好。为解决这些限制，我们使用了一种分布式机器学习方法来分析化学物质数据，称为数据合作分析（DC）。我们还提出了使用投影数据（DCPd）来改进DC方法，这种方法使用辅助 PubChem 数据。这将提高用户端数据转换的质量，以便创建中间表示。我们比较了 FedAvg、DC 和 DCPd 的分类精度，包括地下曲线准确率（ROC-AUC）和精度-回归率曲线准确率（PR-AUC），对五个药物数据集进行了比较。结果表明，在非标一致分布下，DCPd 的机器学习性能高于 FedAvg 和 DC，尽管在标一致分布下，它们几乎相同。此外，结果还表明，相比其他方法，DCPd 在不同的标签偏好情况下的分类精度几乎不变。因此，DCPd 可以解决 federated learning 在非标一致分布下的低性能问题。
</details></li>
</ul>
<hr>
<h2 id="Robust-Positive-Unlabeled-Learning-via-Noise-Negative-Sample-Self-correction"><a href="#Robust-Positive-Unlabeled-Learning-via-Noise-Negative-Sample-Self-correction" class="headerlink" title="Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction"></a>Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00279">http://arxiv.org/abs/2308.00279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/woriazzc/robust-pu">https://github.com/woriazzc/robust-pu</a></li>
<li>paper_authors: Zhangchi Zhu, Lu Wang, Pu Zhao, Chao Du, Wei Zhang, Hang Dong, Bo Qiao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</li>
<li>For: The paper focuses on improving the robustness of positive-unlabeled (PU) learning, which is a common approach in machine learning that uses both positive and unlabeled data for training.* Methods: The proposed method uses a novel “hardness” measure to distinguish unlabeled samples with a high chance of being negative from those with large label noise. An iterative training strategy is then implemented to fine-tune the selection of negative samples during the training process.* Results: The proposed method is shown to be effective in improving the accuracy and stability of learning with positive and unlabeled data through extensive experimental validations over a wide range of learning tasks.Here are the three key points in Simplified Chinese:* For: 本文关注Positive-unlabeled（PU）学习的稳定性提高，PU学习是机器学习中常见的一种方法，通过使用正样本和无标签样本进行训练。* Methods: 提议方法使用一种新的”困难度”度量，可以在训练过程中精准地分类无标签样本，并在训练过程中进行迭代的训练策略来细化选择负样本。* Results: 提议方法通过对各种学习任务进行广泛的实验验证，显示其可以有效地提高使用正样本和无标签样本进行训练的精度和稳定性。<details>
<summary>Abstract</summary>
Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the early stage of training before introducing more complex cases. Specifically, we utilize a novel ``hardness'' measure to distinguish unlabeled samples with a high chance of being negative from unlabeled samples with large label noise. An iterative training strategy is then implemented to fine-tune the selection of negative samples during the training process in an iterative manner to include more ``easy'' samples in the early stage of training. Extensive experimental validations over a wide range of learning tasks show that this approach can effectively improve the accuracy and stability of learning with positive and unlabeled data. Our code is available at https://github.com/woriazzc/Robust-PU
</details>
<details>
<summary>摘要</summary>
学习正面和未标注数据的方法，称为正面未标注（PU）学习，在过去几年内吸引了很多关注。一种常见的PU学习方法是从未标注数据中采样一些 Pseudo-negative samples 使用自定义的阈值，以便通过两类样本进行传统的超级vised学习。由于未标注数据中的标签不确定性，在训练过程中会出现错误地将未标注正面样本分类为负样本的现象，这会导致性能下降和模型不稳定。为了减少标签不确定性的影响和提高学习正面和未标注数据的稳定性，我们提出了一种新的robust PU学习方法，具体来说是通过人类学习的自然性，先学习容易的样本。这种想法类似于curriculum学习，在训练的早期使用更容易的样本。我们采用了一种新的“困难度”度量，以分辨未标注样本中高概率是负样本的样本和大量标签噪声。然后，我们实现了一种迭代训练策略，在训练过程中在迭代的方式中细化选择负样本，以包括更多的容易样本在训练的早期。经验 validate 表明，这种方法可以有效地提高学习正面和未标注数据的精度和稳定性。我们的代码可以在 <https://github.com/woriazzc/Robust-PU> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Classes-are-not-Clusters-Improving-Label-based-Evaluation-of-Dimensionality-Reduction"><a href="#Classes-are-not-Clusters-Improving-Label-based-Evaluation-of-Dimensionality-Reduction" class="headerlink" title="Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction"></a>Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00278">http://arxiv.org/abs/2308.00278</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hj-n/ltnc">https://github.com/hj-n/ltnc</a></li>
<li>paper_authors: Hyeon Jeon, Yun-Hsin Kuo, Michaël Aupetit, Kwan-Liu Ma, Jinwook Seo</li>
<li>for: 评估维度减少（DR）嵌入的可靠性</li>
<li>methods: 引入了两个新的质量指标：标签可靠性和标签连续性（标签-T&amp;C），以提高基于类标签的DR评估</li>
<li>results: 比较了Label-T&amp;C与常用的DR评估指标（如信任性和连续性、库拉布-莱布尔差），发现Label-T&amp;C在评估DR嵌入保持cluster结构的准确性方面表现出色，并且可扩展性好。此外，Label-T&amp;C在揭示DR技术和其超参数的内在特性方面也有成功应用。<details>
<summary>Abstract</summary>
A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures -- Label-Trustworthiness and Label-Continuity (Label-T&C) -- advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative evaluation showed that Label-T&C outperform widely used DR evaluation measures (e.g., Trustworthiness and Continuity, Kullback-Leibler divergence) in terms of the accuracy in assessing how well DR embeddings preserve the cluster structure, and are also scalable. Moreover, we present case studies demonstrating that Label-T&C can be successfully used for revealing the intrinsic characteristics of DR techniques and their hyperparameters.
</details>
<details>
<summary>摘要</summary>
通常来评估维度减少（DR）嵌入的可靠性是根据类标签是否形成紧凑、相互隔离的圈状分布来衡量。这种方法假设高维空间中的类都是清晰分布的，但在实际情况下，这种假设可能被违反；单个类可能会分割成多个分布在多个不同的地方，多个类可能会合并成单个圈状分布。因此，我们不能总是对DR嵌入的评估结果具有信任感。在这篇论文中，我们引入了两种新的质量指标：标签可靠性（Label-Trustworthiness）和标签连续性（Label-Continuity， Label-T&C），这两种指标基于类标签的分布来评估DR嵌入的质量。不同于传统的DR评估方法，Label-T&C不假设类都是清晰分布在高维空间中，而是通过对原始空间和嵌入空间中类的分布进行评估来评估DR嵌入的质量。我们的量化评估表明，Label-T&C可以高效地评估DR嵌入的质量，并且比传统的DR评估指标（如信任度和连续性，卷积-莱布尼度）更准确地评估DR嵌入 preserve 类层次结构的程度。此外，我们还提供了 caso studies，证明了 Label-T&C 可以成功地用于揭示DR技术和其超参数的内在特性。
</details></li>
</ul>
<hr>
<h2 id="Neural-approximation-of-Wasserstein-distance-via-a-universal-architecture-for-symmetric-and-factorwise-group-invariant-functions"><a href="#Neural-approximation-of-Wasserstein-distance-via-a-universal-architecture-for-symmetric-and-factorwise-group-invariant-functions" class="headerlink" title="Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions"></a>Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00273">http://arxiv.org/abs/2308.00273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samantha Chen, Yusu Wang</li>
<li>for: 本研究的目的是学习 Complex 对象之间的距离函数，如 Wasserstein 距离来比较点集。</li>
<li>methods: 本文提出了一种通用的神经网络架构来近似 SFGI 函数（Symmetric and Factor-wise Group Invariant 函数）。</li>
<li>results: 本文首次显示了一种神经网络可以approximate Wasserstein 距离，并且模型复杂度是独立于输入点集的大小。在实际中，我们的新提出的神经网络架构比其他模型（包括 SOTA Siamese Autoencoder 方法）更好地generalize和更快地训练。<details>
<summary>Abstract</summary>
Learning distance functions between complex objects, such as the Wasserstein distance to compare point sets, is a common goal in machine learning applications. However, functions on such complex objects (e.g., point sets and graphs) are often required to be invariant to a wide variety of group actions e.g. permutation or rigid transformation. Therefore, continuous and symmetric product functions (such as distance functions) on such complex objects must also be invariant to the product of such group actions. We call these functions symmetric and factor-wise group invariant (or SFGI functions in short). In this paper, we first present a general neural network architecture for approximating SFGI functions. The main contribution of this paper combines this general neural network with a sketching idea to develop a specific and efficient neural network which can approximate the $p$-th Wasserstein distance between point sets. Very importantly, the required model complexity is independent of the sizes of input point sets. On the theoretical front, to the best of our knowledge, this is the first result showing that there exists a neural network with the capacity to approximate Wasserstein distance with bounded model complexity. Our work provides an interesting integration of sketching ideas for geometric problems with universal approximation of symmetric functions. On the empirical front, we present a range of results showing that our newly proposed neural network architecture performs comparatively or better than other models (including a SOTA Siamese Autoencoder based approach). In particular, our neural network generalizes significantly better and trains much faster than the SOTA Siamese AE. Finally, this line of investigation could be useful in exploring effective neural network design for solving a broad range of geometric optimization problems (e.g., $k$-means in a metric space).
</details>
<details>
<summary>摘要</summary>
学习复杂对象之间的距离函数，如 Wasserstein 距离来比较点集，是机器学习应用中常见的目标。然而，函数在这些复杂对象上（例如点集和图）经常需要对广泛的群作用（如Permutation 和扭转变换）进行不变性。因此，在这些复杂对象上的连续和对称产品函数（如距离函数）必须对组合这些群作用的产品进行不变性。我们称这些函数为对称和因子团变函数（SFGI函数）。在这篇论文中，我们首先提出一种通用的神经网络架构来近似 SFGI 函数。我们的主要贡献是将这种通用神经网络与抽象思想结合，开发一种特定和高效的神经网络，可以近似 $p$-th Wasserstein 距离 между点集。非常重要的是，模型的复杂度不依赖于输入点集的大小。从理论上来看，我们的结果表明了存在一种能够近似 Wasserstein 距离的神经网络，并且模型的复杂度是固定的。我们的工作提供了对几何问题的解决方案和对共形函数的近似的 интересintegration。在实际方面，我们发表了一系列结果，表明我们的新提出的神经网络架构在与其他模型（包括顶尖Siamese Autoencoder）进行比较时，表现较好或更好。具体来说，我们的神经网络在泛化和训练速度方面都表现出了优势。最后，这种研究方向可能有助于探索有效的神经网络设计方法，用于解决广泛的几何优化问题（例如在度量空间中的 $k$-means）。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modality-Multi-Loss-Fusion-Network"><a href="#Multi-Modality-Multi-Loss-Fusion-Network" class="headerlink" title="Multi-Modality Multi-Loss Fusion Network"></a>Multi-Modality Multi-Loss Fusion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00264">http://arxiv.org/abs/2308.00264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehui Wu, Ziwei Gong, Jaywon Koo, Julia Hirschberg</li>
<li>for: 这个研究旨在提高情感检测中的特征选择和融合方法，使用多个感知模式的特征并将其组合在神经网络中。</li>
<li>methods: 研究使用不同的融合方法，并对多模态融合网络中的子网络性能进行分析，发现多模态特征训练可以提高单模态测试性能。</li>
<li>results: 研究结果显示，使用多模态特征可以提高单模态测试性能，并且基于数据注释schema设计的融合方法可以提高模型性能。最佳模型在CMU-MOSI、CMU-MOSEI和CH-SIMS三个数据集上达到了状态 искусственный智能表现，并在大多数指标上超过了其他方法。<details>
<summary>Abstract</summary>
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了多Modalities中最佳的特征选择和融合方法，并将这些方法 integrate into a neural network 以提高情感识别。我们比较了不同的融合方法，并研究了在多模态融合网络中多loss训练的影响，发现了有用的发现关于子网络性能。我们的最佳模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上达到了状态 искусственный neural networks 的最佳性能，并在大多数指标上超越了其他方法。我们发现，训练在多模态特征上提高单模态测试的性能，并基于数据集注释Schema设计的融合方法可以增强模型性能。这些结果建议一个优化特征选择和融合方法的路线图，以提高情感识别在神经网络中的性能。
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Federated-Learning-with-Bidirectional-Quantized-Communications-and-Buffered-Aggregation"><a href="#Asynchronous-Federated-Learning-with-Bidirectional-Quantized-Communications-and-Buffered-Aggregation" class="headerlink" title="Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation"></a>Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00263">http://arxiv.org/abs/2308.00263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomas Ortega, Hamid Jafarkhani</li>
<li>for: 这篇论文主要用于提高 asynchronous federated learning 的效率和扩展性，并且减少了通信成本。</li>
<li>methods: 这篇论文使用了一个新的量化推导方法（QAFeL），具有一个共享的“隐藏”状态，以避免因直接量化而导致的错误传播。</li>
<li>results: 论文提供了对 QAFeL 的理论均衡保证，并通过使用标准库存 benchmark 进行实验证明。<details>
<summary>Abstract</summary>
Asynchronous Federated Learning with Buffered Aggregation (FedBuff) is a state-of-the-art algorithm known for its efficiency and high scalability. However, it has a high communication cost, which has not been examined with quantized communications. To tackle this problem, we present a new algorithm (QAFeL), with a quantization scheme that establishes a shared "hidden" state between the server and clients to avoid the error propagation caused by direct quantization. This approach allows for high precision while significantly reducing the data transmitted during client-server interactions. We provide theoretical convergence guarantees for QAFeL and corroborate our analysis with experiments on a standard benchmark.
</details>
<details>
<summary>摘要</summary>
“异步联合学习 WITH 缓冲聚合（FedBuff）是当前最佳策略，其高效性和可扩展性具有广泛应用前景。然而，它具有高通信成本，而这个问题尚未与量化通信相结合考虑。为解决这个问题，我们提出了一种新的算法（QAFeL），具有一种量化方案，在服务器和客户端之间共享一个“隐藏”状态，以避免由直接量化引起的错误卷积。这种方法允许高精度，同时大幅减少了客户端和服务器之间的数据传输量。我们提供了理论受托证明，并在标准 benchmark 上进行了实验 validate 我们的分析。”Note that "FedBuff" is translated as "异步联合学习 WITH 缓冲聚合" (asynchronous federated learning with buffered aggregation), and "QAFeL" is translated as "量化联合学习（QAFeL）" (quantized federated learning).
</details></li>
</ul>
<hr>
<h2 id="AQUILA-Communication-Efficient-Federated-Learning-with-Adaptive-Quantization-of-Lazily-Aggregated-Gradients"><a href="#AQUILA-Communication-Efficient-Federated-Learning-with-Adaptive-Quantization-of-Lazily-Aggregated-Gradients" class="headerlink" title="AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients"></a>AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00258">http://arxiv.org/abs/2308.00258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhao, Yuzhu Mao, Zhenpeng Shi, Yang Liu, Tian Lan, Wenbo Ding, Xiao-Ping Zhang</li>
<li>for: 这篇论文的目的是提高 Federated Learning（分布式学习）中通信开销的高，通常是由大规模模型的传输引起的。</li>
<li>methods: 这篇论文引入了 Adaptive Quantization of Lazily-Aggregated Gradients（AQUILA），一个新的适应性框架，用于解决这些问题，提高分布式学习的效率和可靠性。AQUILA 包括了复杂的设备选择方法，优先运算设备更新的质量和用途。此外，AQUILA 还提出了一个优化的量化准则，用于提高通信效率，保证模型的参数稳定。</li>
<li>results: 实验结果显示，AQUILA 与现有方法相比，对于不同的非同质量 FL 环境（如非同质量数据和多样化模型架构）而言，可以实现明显的通信成本优化，同时维持模型性能的相似性。<details>
<summary>Abstract</summary>
The widespread adoption of Federated Learning (FL), a privacy-preserving distributed learning methodology, has been impeded by the challenge of high communication overheads, typically arising from the transmission of large-scale models. Existing adaptive quantization methods, designed to mitigate these overheads, operate under the impractical assumption of uniform device participation in every training round. Additionally, these methods are limited in their adaptability due to the necessity of manual quantization level selection and often overlook biases inherent in local devices' data, thereby affecting the robustness of the global model. In response, this paper introduces AQUILA (adaptive quantization of lazily-aggregated gradients), a novel adaptive framework devised to effectively handle these issues, enhancing the efficiency and robustness of FL. AQUILA integrates a sophisticated device selection method that prioritizes the quality and usefulness of device updates. Utilizing the exact global model stored by devices, it enables a more precise device selection criterion, reduces model deviation, and limits the need for hyperparameter adjustments. Furthermore, AQUILA presents an innovative quantization criterion, optimized to improve communication efficiency while assuring model convergence. Our experiments demonstrate that AQUILA significantly decreases communication costs compared to existing methods, while maintaining comparable model performance across diverse non-homogeneous FL settings, such as Non-IID data and heterogeneous model architectures.
</details>
<details>
<summary>摘要</summary>
“质量保护分布式学习（FL）的广泛采用受到了通信开销的挑战，通常是由大规模模型的传输所导致的。现有的适应量化方法，是为了解决这些开销，但是它们假设所有训练轮都有参与者，而且它们具有限制性，需要手动选择量化水平，并且常常忽视本地设备数据中的偏见，从而影响全球模型的稳定性。为了解决这些问题，本文提出了AQUILA（适应量化 Lazy 梯度），一种新的适应框架，可以更好地处理这些问题，提高FL的效率和稳定性。AQUILA  integrates 一种智能的设备选择方法，可以根据设备更新的质量和有用性来优先选择设备。通过使用设备上存储的全球模型，AQUILA 可以更准确地选择设备，减少模型偏差，并减少超参数的调整。此外，AQUILA 还提出了一种优化的量化标准，可以提高通信效率，保证模型的收敛。我们的实验表明，AQUILA 可以significantly reduce  communication costs compared to existing methods, while maintaining comparable model performance across diverse non-homogeneous FL settings, such as Non-IID data and heterogeneous model architectures。”
</details></li>
</ul>
<hr>
<h2 id="Best-Subset-Selection-in-Generalized-Linear-Models-A-Fast-and-Consistent-Algorithm-via-Splicing-Technique"><a href="#Best-Subset-Selection-in-Generalized-Linear-Models-A-Fast-and-Consistent-Algorithm-via-Splicing-Technique" class="headerlink" title="Best-Subset Selection in Generalized Linear Models: A Fast and Consistent Algorithm via Splicing Technique"></a>Best-Subset Selection in Generalized Linear Models: A Fast and Consistent Algorithm via Splicing Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00251">http://arxiv.org/abs/2308.00251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junxian Zhu, Jin Zhu, Borui Tang, Xuanyu Chen, Hongmei Lin, Xueqin Wang</li>
<li>for: 本文是为了提出一种高维 generalized linear model 中适应响应变化的精炼方法。</li>
<li>methods: 本文使用了一种快速算法来选择最佳子集，并在轻量级条件下提供了 statistically guaranteed 的方法。</li>
<li>results: 实验表明，本方法在变量选择和系数估计方面具有优异性，并且与现有方法相比，实现了约四倍的速度提升。<details>
<summary>Abstract</summary>
In high-dimensional generalized linear models, it is crucial to identify a sparse model that adequately accounts for response variation. Although the best subset section has been widely regarded as the Holy Grail of problems of this type, achieving either computational efficiency or statistical guarantees is challenging. In this article, we intend to surmount this obstacle by utilizing a fast algorithm to select the best subset with high certainty. We proposed and illustrated an algorithm for best subset recovery in regularity conditions. Under mild conditions, the computational complexity of our algorithm scales polynomially with sample size and dimension. In addition to demonstrating the statistical properties of our method, extensive numerical experiments reveal that it outperforms existing methods for variable selection and coefficient estimation. The runtime analysis shows that our implementation achieves approximately a fourfold speedup compared to popular variable selection toolkits like glmnet and ncvreg.
</details>
<details>
<summary>摘要</summary>
高维度泛化线性模型中，必须确定一个稀疏模型，以便准确地回归响应变化。虽然最佳子集问题被广泛视为这类问题的圣杯，但实现计算效率或统计保证是困难的。在这篇文章中，我们决心使用快速算法选择最佳子集，以高度确定性。我们提出了一种算法，并在正则条件下进行了描述。在规模和维度增长时，我们的算法的计算复杂度随样本大小和维度呈极其平滑的几何函数。除了证明我们的方法的统计性质外，我们的数值实验表明，我们的方法在变量选择和参数估计方面的性能较高，并且与流行的变量选择工具包like glmnet和ncvreg进行比较，我们的实现的运行时间约为四倍。
</details></li>
</ul>
<hr>
<h2 id="EEG-based-Cognitive-Load-Classification-using-Feature-Masked-Autoencoding-and-Emotion-Transfer-Learning"><a href="#EEG-based-Cognitive-Load-Classification-using-Feature-Masked-Autoencoding-and-Emotion-Transfer-Learning" class="headerlink" title="EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning"></a>EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00246">http://arxiv.org/abs/2308.00246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Pulver, Prithila Angkan, Paul Hungler, Ali Etemad</li>
<li>for: 本研究旨在提出一种基于电enzephalogram（EEG）的识别负荷思维方法，以便在各种敏感领域中进行性能和决策结果的分析。</li>
<li>methods: 本研究使用了transformer架构，通过跨类转移学习来连接情感和负荷思维。我们采用了自我超vised masked autoencoding来预训练我们的模型，并使用了冻结参数和精度调整来进行下游负荷思维分类。</li>
<li>results: 我们的实验结果显示，我们提出的方法可以具有强大的表现，并超过了传统的单阶段完全监督学习。此外，我们还进行了细化折衔和敏感性研究，以评估不同方面的影响。本研究的成果将增加情感计算领域的文献，并开辟了新的跨领域转移学习using自我超vised预训练的研究途径。<details>
<summary>Abstract</summary>
Cognitive load, the amount of mental effort required for task completion, plays an important role in performance and decision-making outcomes, making its classification and analysis essential in various sensitive domains. In this paper, we present a new solution for the classification of cognitive load using electroencephalogram (EEG). Our model uses a transformer architecture employing transfer learning between emotions and cognitive load. We pre-train our model using self-supervised masked autoencoding on emotion-related EEG datasets and use transfer learning with both frozen weights and fine-tuning to perform downstream cognitive load classification. To evaluate our method, we carry out a series of experiments utilizing two publicly available EEG-based emotion datasets, namely SEED and SEED-IV, for pre-training, while we use the CL-Drive dataset for downstream cognitive load classification. The results of our experiments show that our proposed approach achieves strong results and outperforms conventional single-stage fully supervised learning. Moreover, we perform detailed ablation and sensitivity studies to evaluate the impact of different aspects of our proposed solution. This research contributes to the growing body of literature in affective computing with a focus on cognitive load, and opens up new avenues for future research in the field of cross-domain transfer learning using self-supervised pre-training.
</details>
<details>
<summary>摘要</summary>
认知负担（Cognitive Load）在完成任务和决策结果中扮演着重要角色，因此其分类和分析成为了各种敏感领域中的重要课题。在这篇论文中，我们提出了一个新的认知负担分类方法，使用了 трансформа器架构和跨领域学习。我们在认知负担和情感之间进行了对应学习，并在过程中使用了两种不同的权重升级和精致化。为了评估我们的方法，我们进行了一连串的实验，使用了两个公开available的EEG基于情感资料集，namely SEED和SEED-IV，进行预训练，而下游认知负担分类则使用了CL-Drive资料集。结果显示，我们的提出的方法具有强大的表现和超越传统单阶充足学习。此外，我们还进行了细部抑制和敏感性研究，以评估不同方面的影响。这个研究对于情感计算领域中的认知负担有所贡献，并开启了跨领域跨学习使用自愿学习预训练的新的应用领域。
</details></li>
</ul>
<hr>
<h2 id="Beam-Detection-Based-on-Machine-Learning-Algorithms"><a href="#Beam-Detection-Based-on-Machine-Learning-Algorithms" class="headerlink" title="Beam Detection Based on Machine Learning Algorithms"></a>Beam Detection Based on Machine Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00718">http://arxiv.org/abs/2308.00718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyuan Li, Qing Yin</li>
<li>for:  precisely determining the positions of free electron laser beams on screens</li>
<li>methods:  using a sequence of machine learning models, including a self-constructed convolutional neural network based on VGG16 model and a support vector regression model</li>
<li>results:  achieving 85.8% correct prediction on test data<details>
<summary>Abstract</summary>
The positions of free electron laser beams on screens are precisely determined by a sequence of machine learning models. Transfer training is conducted in a self-constructed convolutional neural network based on VGG16 model. Output of intermediate layers are passed as features to a support vector regression model. With this sequence, 85.8% correct prediction is achieved on test data.
</details>
<details>
<summary>摘要</summary>
“免电子激光束在屏幕上的位置精确地由一个机器学习模型序列确定。在自定义的卷积神经网络基于VGG16模型上进行传输训练。输出的中间层的输出被用作特征，并被传输到一个支持向量回归模型。通过这个序列，在测试数据上达到了85.8%的正确预测率。”Here's a word-for-word translation of the text:“免电子激光束在屏幕上的位置由机器学习模型序列确定。基于VGG16模型自定义卷积神经网络进行传输训练。中间层输出用作特征，并传输到支持向量回归模型。测试数据上达到85.8%的正确预测率。”
</details></li>
</ul>
<hr>
<h2 id="ChatMOF-An-Autonomous-AI-System-for-Predicting-and-Generating-Metal-Organic-Frameworks"><a href="#ChatMOF-An-Autonomous-AI-System-for-Predicting-and-Generating-Metal-Organic-Frameworks" class="headerlink" title="ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks"></a>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01423">http://arxiv.org/abs/2308.01423</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeonghun1675/chatmof">https://github.com/yeonghun1675/chatmof</a></li>
<li>paper_authors: Yeonghun Kang, Jihan Kim</li>
<li>for: 这篇论文是为了探讨和开发一种基于自然语言处理的金属有机框架（MOFs）预测和生成系统。</li>
<li>methods: 这个系统使用了一个大规模语言模型（gpt-3.5-turbo），从文本输入中提取关键信息并提供相应的回答，因此消除了需要僵化的结构化查询的需要。系统由三个核心组件（代理、工具箱和评估器）组成，形成一个可靠的管道，用于处理多种任务，包括数据检索、性能预测和结构生成。</li>
<li>results: 这篇论文还研究了使用大语言模型AI系统在材料科学中的优势和局限，并展示了其可能性的潜在发展。<details>
<summary>Abstract</summary>
ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
</details>
<details>
<summary>摘要</summary>
chatMOF 是一个自主的人工智能（AI）系统，用于预测和生成金刚烷金刚烷框架（MOFs）。通过利用大规模语言模型（gpt-3.5-turbo）， chatMOF 从文本输入中提取关键信息并提供相应的回答，因此消除了僵化的结构化查询的需要。该系统由三个核心组件（即代理、工具箱和评估器）组成，形成了一个全面的管道，处理多种任务，包括数据检索、性能预测和结构生成。研究还探讨了使用大语言模型（LLMs）AI系统在材料科学中的优劣和限制，并展示了其可以在未来的发展中发挥转变性的作用。
</details></li>
</ul>
<hr>
<h2 id="Capsa-A-Unified-Framework-for-Quantifying-Risk-in-Deep-Neural-Networks"><a href="#Capsa-A-Unified-Framework-for-Quantifying-Risk-in-Deep-Neural-Networks" class="headerlink" title="Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks"></a>Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00231">http://arxiv.org/abs/2308.00231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadhana Lolla, Iaroslav Elistratov, Alejandro Perez, Elaheh Ahmadi, Daniela Rus, Alexander Amini</li>
<li>for: 这篇论文旨在提供一种扩展模型的风险意识框架，以便在复杂的问题上提高模型的可靠性和安全性。</li>
<li>methods: 这篇论文使用了多种风险意识算法，包括 aleatoric uncertainty、epistemic uncertainty 和偏见估计等，并将这些算法集成到一个框架中，以便在复杂的感知数据集上进行测试和评估。</li>
<li>results: 根据实验结果，capsa 框架能够轻松地组合不同的风险意识算法，并在复杂的感知数据集上提供了全面的风险意识。<details>
<summary>Abstract</summary>
The modern pervasiveness of large-scale deep neural networks (NNs) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. Existing algorithms that provide risk-awareness to NNs are complex and ad-hoc. Specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. Here we present capsa, a framework for extending models with risk-awareness. Capsa provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. We validate capsa by implementing state-of-the-art uncertainty estimation algorithms within the capsa framework and benchmarking them on complex perception datasets. We demonstrate capsa's ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation together in a single procedure, and show how this approach provides a comprehensive awareness of NN risk.
</details>
<details>
<summary>摘要</summary>
现代大规模深度神经网络（NN）的普遍性受到了它们在复杂问题上的非凡表现的驱动，但也受到了它们在困难场景下的不预期和致命的失败的困扰。现有的风险意识提供方法都是复杂的和不一致的，它们需要大量的工程变更，通常只适用于特定的设置，而且不易组合。我们在这里提出了一个框架，即capsa，用于扩展模型的风险意识。capsa提供了多种风险量化的方法ología，并可以将不同的风险度量化算法相互组合，以并行地计算多种风险指标。我们通过在capsa框架中实现现有的uncertainty估计算法，并对复杂的感知数据集进行了 benchmarking，以证明capsa的能力。我们展示了capsa可以轻松地组合 aleatoric uncertainty、epistemic uncertainty 和偏见估计在一个过程中，并显示了这种方法在NN风险意识方面提供了全面的认知。
</details></li>
</ul>
<hr>
<h2 id="Instructed-to-Bias-Instruction-Tuned-Language-Models-Exhibit-Emergent-Cognitive-Bias"><a href="#Instructed-to-Bias-Instruction-Tuned-Language-Models-Exhibit-Emergent-Cognitive-Bias" class="headerlink" title="Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias"></a>Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00225">http://arxiv.org/abs/2308.00225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov</li>
<li>For: This paper aims to investigate the presence of cognitive biases in fine-tuned large language models (LMs), specifically those that have undergone instruction tuning.* Methods: The authors examine three cognitive biases - the decoy effect, the certainty effect, and the belief bias - in various LMs, including Flan-T5, GPT3.5, and GPT4. They use a variety of evaluation metrics and methods to detect these biases.* Results: The authors find evidence of these biases in the fine-tuned models, particularly in those that have undergone instruction tuning. They also observe that these biases are more pronounced in certain models and tasks. Their findings highlight the need for further research into cognitive biases in instruction-tuned LMs.Here is the information in Simplified Chinese text:* 为: 这篇论文探讨了各种大语言模型（LMs）中的认知偏见问题，尤其是那些经过了指导调整的模型。* 方法: 作者们使用了多种评估指标和方法来探测这些偏见。* 结果: 作者们发现了这些偏见在调整后的模型中，特别是在某些模型和任务中更为明显。他们的发现提醒我们需要更进一步的研究认知偏见在指导调整后的LMs。<details>
<summary>Abstract</summary>
Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-Reinforcement-Learning-Based-Battery-Conditioning-Hierarchical-V2G-Coordination-for-Multi-Stakeholder-Benefits"><a href="#Deep-Reinforcement-Learning-Based-Battery-Conditioning-Hierarchical-V2G-Coordination-for-Multi-Stakeholder-Benefits" class="headerlink" title="Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits"></a>Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00218">http://arxiv.org/abs/2308.00218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubao Zhang, Xin Chen, Yi Gu, Zhicheng Li, Wu Kai</li>
<li>for: 提高可再生能源利用率和电网稳定性，适用于电动汽车（EV）和大规模协调策略。</li>
<li>methods: 基于深度强化学习（DRL）和证明权益算法，实现多方受益者协调。</li>
<li>results: 比基eline四个 Referenced baselines，该策略可以提高可再生能源消耗率、降低负荷波动、满足EV充电需求，并减少充电成本和电池衰老。<details>
<summary>Abstract</summary>
With the growing prevalence of electric vehicles (EVs) and advancements in EV electronics, vehicle-to-grid (V2G) techniques and large-scale scheduling strategies have emerged to promote renewable energy utilization and power grid stability. This study proposes a multi-stakeholder hierarchical V2G coordination based on deep reinforcement learning (DRL) and the Proof of Stake algorithm. Furthermore, the multi-stakeholders include the power grid, EV aggregators (EVAs), and users, and the proposed strategy can achieve multi-stakeholder benefits. On the grid side, load fluctuations and renewable energy consumption are considered, while on the EVA side, energy constraints and charging costs are considered. The three critical battery conditioning parameters of battery SOX are considered on the user side, including state of charge, state of power, and state of health. Compared with four typical baselines, the multi-stakeholder hierarchical coordination strategy can enhance renewable energy consumption, mitigate load fluctuations, meet the energy demands of EVA, and reduce charging costs and battery degradation under realistic operating conditions.
</details>
<details>
<summary>摘要</summary>
On the grid side, the method considers load fluctuations and renewable energy consumption, while on the EVA side, it considers energy constraints and charging costs. For users, the method takes into account three critical battery conditioning parameters: state of charge, state of power, and state of health.Compared to four typical baselines, the multi-stakeholder hierarchical coordination strategy can increase renewable energy consumption, mitigate load fluctuations, meet the energy demands of EVAs, and reduce charging costs and battery degradation under realistic operating conditions.
</details></li>
</ul>
<hr>
<h2 id="Robust-Single-view-Cone-beam-X-ray-Pose-Estimation-with-Neural-Tuned-Tomography-NeTT-and-Masked-Neural-Radiance-Fields-mNeRF"><a href="#Robust-Single-view-Cone-beam-X-ray-Pose-Estimation-with-Neural-Tuned-Tomography-NeTT-and-Masked-Neural-Radiance-Fields-mNeRF" class="headerlink" title="Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF)"></a>Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00214">http://arxiv.org/abs/2308.00214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N. Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi, Leon Cho, Sameer A. Ansari, Donald R. Cantrell</li>
<li>for: 这篇论文是用于描述一种基于X射线投影的医学操作pose estimation的新方法。</li>
<li>methods: 该方法使用了DiffDRR算法来高效计算Digitally Reconstructed Radiographs（DRRs），并使用TensorFlow中的自动微分来实现 Iterative Gradient Descent 算法。两种新的高精度视图synthesis方法Neural Tuned Tomography（NeTT）和Masked Neural Radiance Fields（mNeRF）也被提出，它们都基于经典的Conic Beam Computerized Tomography（CBCT）。</li>
<li>results: 对于pose estimation问题，NeTT和mNeRF两种方法都能够达到高精度的结果，成功率高于93%。然而，NeTT的计算成本远低于mNeRF，而且NeTT可以在训练和pose estimation阶段都具有更好的性能。此外， authors还发现NeTT可以在不同的主体上进行高精度的DRRsynthesis和pose estimation。因此，NeTT是一种可靠的pose estimation方法。<details>
<summary>Abstract</summary>
Many tasks performed in image-guided, mini-invasive, medical procedures can be cast as pose estimation problems, where an X-ray projection is utilized to reach a target in 3D space. Expanding on recent advances in the differentiable rendering of optically reflective materials, we introduce new methods for pose estimation of radiolucent objects using X-ray projections, and we demonstrate the critical role of optimal view synthesis in performing this task. We first develop an algorithm (DiffDRR) that efficiently computes Digitally Reconstructed Radiographs (DRRs) and leverages automatic differentiation within TensorFlow. Pose estimation is performed by iterative gradient descent using a loss function that quantifies the similarity of the DRR synthesized from a randomly initialized pose and the true fluoroscopic image at the target pose. We propose two novel methods for high-fidelity view synthesis, Neural Tuned Tomography (NeTT) and masked Neural Radiance Fields (mNeRF). Both methods rely on classic Cone-Beam Computerized Tomography (CBCT); NeTT directly optimizes the CBCT densities, while the non-zero values of mNeRF are constrained by a 3D mask of the anatomic region segmented from CBCT. We demonstrate that both NeTT and mNeRF distinctly improve pose estimation within our framework. By defining a successful pose estimate to be a 3D angle error of less than 3 deg, we find that NeTT and mNeRF can achieve similar results, both with overall success rates more than 93%. However, the computational cost of NeTT is significantly lower than mNeRF in both training and pose estimation. Furthermore, we show that a NeTT trained for a single subject can generalize to synthesize high-fidelity DRRs and ensure robust pose estimations for all other subjects. Therefore, we suggest that NeTT is an attractive option for robust pose estimation using fluoroscopic projections.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SkullGAN-Synthetic-Skull-CT-Generation-with-Generative-Adversarial-Networks"><a href="#SkullGAN-Synthetic-Skull-CT-Generation-with-Generative-Adversarial-Networks" class="headerlink" title="SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks"></a>SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00206">http://arxiv.org/abs/2308.00206</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kbp-lab/skullgan">https://github.com/kbp-lab/skullgan</a></li>
<li>paper_authors: Kasra Naftchi-Ardebili, Karanpartap Singh, Reza Pourabolghasem, Pejman Ghanouni, Gerald R. Popelka, Kim Butts Pauly</li>
<li>for: 减少实验图像的依赖，以促进医疗机器学习的整合。</li>
<li>methods: 使用生成对抗网络（GAN）创建大量人颅CT剖析资料，以减少实验图像的需求和提高医疗机器学习的应用。</li>
<li>results: SkullGAN生成的人颅CT剖析图像和实验图像之间的三个量度（骨密度比、平均厚度和平均强度）之间存在类似性，并且使用SkullGAN推论器作为分类器时，可以将56.5%的实验图像和55.9%的SkullGAN生成图像视为实验图像（理论最佳值为50%），显示SkullGAN生成的人颅CT剖析集合与实验图像集合无法区别。<details>
<summary>Abstract</summary>
Deep learning offers potential for various healthcare applications involving the human skull but requires extensive datasets of curated medical images. To overcome this challenge, we propose SkullGAN, a generative adversarial network (GAN), to create large datasets of synthetic skull CT slices, reducing reliance on real images and accelerating the integration of machine learning into healthcare. In our method, CT slices of 38 subjects were fed to SkullGAN, a neural network comprising over 200 million parameters. The synthetic skull images generated were evaluated based on three quantitative radiological features: skull density ratio (SDR), mean thickness, and mean intensity. They were further analyzed using t-distributed stochastic neighbor embedding (t-SNE) and by applying the SkullGAN discriminator as a classifier. The results showed that SkullGAN-generated images demonstrated similar key quantitative radiological features to real skulls. Further definitive analysis was undertaken by applying the discriminator of SkullGAN, where the SkullGAN discriminator classified 56.5% of a test set of real skull images and 55.9% of the SkullGAN-generated images as reals (the theoretical optimum being 50%), demonstrating that the SkullGAN-generated skull set is indistinguishable from the real skull set - within the limits of our nonlinear classifier. Therefore, SkullGAN makes it possible to generate large numbers of synthetic skull CT segments, necessary for training neural networks for medical applications involving the human skull. This mitigates challenges associated with preparing large, high-quality training datasets, such as access, capital, time, and the need for domain expertise.
</details>
<details>
<summary>摘要</summary>
深度学习可能在各种医疗应用中使用人颅部，但需要大量的 curae 医疗影像集。为解决这个挑战，我们提议使用 SkullGAN，一种生成敌对网络（GAN），创建大量的人工颅部CT切片，降低对实际影像的依赖和加速机器学习在医疗中的集成。在我们的方法中，CT切片的38名参与者被 fed 到 SkullGAN，一个包含超过2亿个参数的神经网络。生成的人工颅部影像被评估基于三个量化医学特征：颅部密度比率（SDR）、平均厚度和平均强度。它们还被使用t-分布随机邻居embedding（t-SNE）分析，并通过应用SkullGAN推论器作为分类器。结果表明，SkullGAN生成的影像具有与实际颅部相同的关键量化医学特征。进一步的定 définitive分析表明，SkullGAN推论器对实际颅部测试集中的56.5%和SkullGAN生成的影像中的55.9%进行了分类，表明SkullGAN生成的颅部集是与实际颅部集相同的 - 在我们的非线性分类器的限制下。因此，SkullGAN使得可以生成大量的人工颅部CT段，为医疗应用中的神经网络训练准备庞大、高质量的医疗影像集。这解决了准备大量、高质量医疗影像集的挑战，包括访问、资金、时间和域专业知识的需求。
</details></li>
</ul>
<hr>
<h2 id="CBCL-PR-A-Cognitively-Inspired-Model-for-Class-Incremental-Learning-in-Robotics"><a href="#CBCL-PR-A-Cognitively-Inspired-Model-for-Class-Incremental-Learning-in-Robotics" class="headerlink" title="CBCL-PR: A Cognitively Inspired Model for Class-Incremental Learning in Robotics"></a>CBCL-PR: A Cognitively Inspired Model for Class-Incremental Learning in Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00199">http://arxiv.org/abs/2308.00199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aliayub7/cbcl-pr">https://github.com/aliayub7/cbcl-pr</a></li>
<li>paper_authors: Ali Ayub, Alan R. Wagner</li>
<li>for: 本文解决了Robot在有限数据环境中不断学习和适应的问题，具体来说是几个数据示例内的类增量学习（FSIL）问题。</li>
<li>methods: 我们提出了一种基于脑中神经元和血液管理的概念学习理论的新框架，用于解决FSIL问题。该框架将物体类表示为集合的集合，并将其存储在内存中。当学习新类时，框架会重温过去学习的类数据，以避免忘记。</li>
<li>results: 我们在两个物体分类数据集上进行了评估，得到了当前最佳（SOTA）性能的类增量学习和FSIL性能。此外，我们还在一个 robot 上进行了FSIL测试，结果表明 robot 可以不断地学习并分类大量家用品，只需限制的人类协助。<details>
<summary>Abstract</summary>
For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this paper, we consider the problem of Few-Shot class Incremental Learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification datasets resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot demonstrating that the robot can continually learn to classify a large set of household objects with limited human assistance.
</details>
<details>
<summary>摘要</summary>
For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this paper, we consider the problem of Few-Shot class Incremental Learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification datasets, resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot, demonstrating that the robot can continually learn to classify a large set of household objects with limited human assistance.Here's the text in Traditional Chinese:For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this paper, we consider the problem of Few-Shot class Incremental Learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification datasets, resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot, demonstrating that the robot can continually learn to classify a large set of household objects with limited human assistance.
</details></li>
</ul>
<hr>
<h2 id="C-DARL-Contrastive-diffusion-adversarial-representation-learning-for-label-free-blood-vessel-segmentation"><a href="#C-DARL-Contrastive-diffusion-adversarial-representation-learning-for-label-free-blood-vessel-segmentation" class="headerlink" title="C-DARL: Contrastive diffusion adversarial representation learning for label-free blood vessel segmentation"></a>C-DARL: Contrastive diffusion adversarial representation learning for label-free blood vessel segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00193">http://arxiv.org/abs/2308.00193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boah Kim, Yujin Oh, Bradford J. Wood, Ronald M. Summers, Jong Chul Ye</li>
<li>for: 这篇论文旨在提供一种自顾学性脉络分割方法，以便在医疗影像中诊断和规划涉及广泛诊断和手术计划。</li>
<li>methods: 本文提出了一种自顾学性脉络分割方法，称为对比扩散对抗表示学习（C-DARL）模型。该模型由扩散模块和生成模块组成，通过生成多域血管数据的分布来学习血管图像的分布。此外，我们采用了对比学习，通过一个面纱基于的对比损失来让模型学习更真实的血管表示。</li>
<li>results: 实验结果表明，C-DARL比基eline方法具有更高的性能，并且具有阈值稳定性，表明C-DARL可以有效地进行脉络分割。<details>
<summary>Abstract</summary>
Blood vessel segmentation in medical imaging is one of the essential steps for vascular disease diagnosis and interventional planning in a broad spectrum of clinical scenarios in image-based medicine and interventional medicine. Unfortunately, manual annotation of the vessel masks is challenging and resource-intensive due to subtle branches and complex structures. To overcome this issue, this paper presents a self-supervised vessel segmentation method, dubbed the contrastive diffusion adversarial representation learning (C-DARL) model. Our model is composed of a diffusion module and a generation module that learns the distribution of multi-domain blood vessel data by generating synthetic vessel images from diffusion latent. Moreover, we employ contrastive learning through a mask-based contrastive loss so that the model can learn more realistic vessel representations. To validate the efficacy, C-DARL is trained using various vessel datasets, including coronary angiograms, abdominal digital subtraction angiograms, and retinal imaging. Experimental results confirm that our model achieves performance improvement over baseline methods with noise robustness, suggesting the effectiveness of C-DARL for vessel segmentation.
</details>
<details>
<summary>摘要</summary>
《医学影像中血管分割》是医学影像诊断和 intervención规划中一项非常重要的步骤，在各种临床场景下都是如此。然而，手动标注血管掩蔽是一项困难和耗时的任务，因为血管分支和结构相对复杂。为解决这个问题，这篇论文提出了一种自动化血管分割方法，名为对抗扩散对抗学习（C-DARL）模型。我们的模型包括扩散模块和生成模块，通过生成多域血管数据的分布来学习血管图像。此外，我们还使用对比学习，通过一个掩蔽基于的对比损失，使模型学习更真实的血管表示。为验证效果，C-DARL在不同的血管数据集上进行了训练，包括肠动脉ANGIOGRAM、腹部数字抽取ANGIOGRAM和视网膜成像。实验结果表明，我们的模型在比基eline方法上具有噪音Robustness，表明C-DARL有效地进行了血管分割。
</details></li>
</ul>
<hr>
<h2 id="Universal-Majorization-Minimization-Algorithms"><a href="#Universal-Majorization-Minimization-Algorithms" class="headerlink" title="Universal Majorization-Minimization Algorithms"></a>Universal Majorization-Minimization Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00190">http://arxiv.org/abs/2308.00190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Streeter</li>
<li>for: 优化问题</li>
<li>methods: 使用自动梯度下降法生成自动majorizer，实现对任意问题的优化</li>
<li>results: 可以从任意初始点开始，无需调整超参数，并且可以 converges to global optimum<details>
<summary>Abstract</summary>
Majorization-minimization (MM) is a family of optimization methods that iteratively reduce a loss by minimizing a locally-tight upper bound, called a majorizer. Traditionally, majorizers were derived by hand, and MM was only applicable to a small number of well-studied problems. We present optimizers that instead derive majorizers automatically, using a recent generalization of Taylor mode automatic differentiation. These universal MM optimizers can be applied to arbitrary problems and converge from any starting point, with no hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
Majorization-minimization（MM）是一家优化方法的家族，通过逐步减少损失函数，最终得到一个当地紧急的上界，称为主要函数。在过去，主要函数通常是手动 derive的，因此MM只能应用于一些已经广泛研究的问题。我们提出了一种使用最近普遍化的泰勒模式自动导数来自动 derive 主要函数的优化器。这种通用的 MM 优化器可以应用于任何问题，从任何初始点开始，无需调整超参数。
</details></li>
</ul>
<hr>
<h2 id="Generative-Models-as-a-Complex-Systems-Science-How-can-we-make-sense-of-large-language-model-behavior"><a href="#Generative-Models-as-a-Complex-Systems-Science-How-can-we-make-sense-of-large-language-model-behavior" class="headerlink" title="Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?"></a>Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00189">http://arxiv.org/abs/2308.00189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ari Holtzman, Peter West, Luke Zettlemoyer</li>
<li>for: 本研究旨在系统地研究语言模型的行为类型，以便更好地理解它们在不同任务上的表现。</li>
<li>methods: 本研究使用了一种系统的分类方法，以分类语言模型的行为类型，并对这些类型的分布进行了分析。</li>
<li>results: 研究发现，语言模型的行为可以分为多种类型，包括基本的语言理解、语义理解和语言生成等。这些类型之间存在着较强的相关关系，可以帮助我们更好地理解语言模型在不同任务上的表现。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases.   Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.
</details>
<details>
<summary>摘要</summary>
使待命模型展现愿望的行为，而不是不良的一种，已经重塑了自然语言处理（NLP）领域，并改变了我们与计算机之间的交互方式。过去，建立模型的科学工程是一种堆叠一个叠的过程，但现在，这已经变成了复杂系统科学，寻找 Emergent 行为来支持未想过的应用场景。尽管有越来越多的任务性能指标，但我们仍然缺乏这些语言模型展现出的行为所导致的任务完成的解释。我们提出了一种系统的努力，以分类语言模型的行为，以解释跨任务性能的交叠，以帮助未来的分析研究。
</details></li>
</ul>
<hr>
<h2 id="Attribution-Scores-in-Data-Management-and-Explainable-Machine-Learning"><a href="#Attribution-Scores-in-Data-Management-and-Explainable-Machine-Learning" class="headerlink" title="Attribution-Scores in Data Management and Explainable Machine Learning"></a>Attribution-Scores in Data Management and Explainable Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00184">http://arxiv.org/abs/2308.00184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leopoldo Bertossi</li>
<li>for: 这篇论文主要探讨了使用实际 causality 定义责任分数的研究，以及这些分数在数据库和机器学习中的应用。</li>
<li>methods: 论文使用了数据库修复和分数的计算方法，以及一种量化度量数据库的一致性。</li>
<li>results: 论文提出了一种正确地扩展了分数，并在分类模型中应用了这种扩展。同时，论文还分析了 Shap-score 的高效计算方法。<details>
<summary>Abstract</summary>
We describe recent research on the use of actual causality in the definition of responsibility scores as explanations for query answers in databases, and for outcomes from classification models in machine learning. In the case of databases, useful connections with database repairs are illustrated and exploited. Repairs are also used to give a quantitative measure of the consistency of a database. For classification models, the responsibility score is properly extended and illustrated. The efficient computation of Shap-score is also analyzed and discussed. The emphasis is placed on work done by the author and collaborators.
</details>
<details>
<summary>摘要</summary>
我们描述了最近的研究，把实际 causality 引入定义责任分数的概念，以解释 queries 的答案在数据库中，以及机器学习模型中的结果。在数据库中，我们利用了有用的连接，并用修复来给出数据库的数量化度量。对于机器学习模型，我们正确地扩展了责任分数，并给出了相应的示例。我们还分析了计算 Shap 分数的高效策略。我们强调了作者和合作者的工作。
</details></li>
</ul>
<hr>
<h2 id="General-Anomaly-Detection-of-Underwater-Gliders-Validated-by-Large-scale-Deployment-Dataset"><a href="#General-Anomaly-Detection-of-Underwater-Gliders-Validated-by-Large-scale-Deployment-Dataset" class="headerlink" title="General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset"></a>General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00180">http://arxiv.org/abs/2308.00180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruochu Yang, Chad Lembke, Fumin Zhang, Catherine Edwards</li>
<li>for: 本研究使用异常检测算法来评估水下飞行器在不可预测的海洋环境中的正常运行。</li>
<li>methods: 本研究使用了实时alerts，使飞行器驾驶员在检测到异常情况时能够及时控制飞行器，避免进一步的损害。检测算法应用于大量的实际数据集，收集自Skidaway Institute of Oceanography（SkIO）和University of South Florida（USF）的实际飞行器部署。</li>
<li>results: 实验评估包括了线上和线下检测两种模式。线下检测使用完整的回收数据集，以提供精确的异常分析和与驾驶员日志进行比较。线上检测则是在飞行器在抵达事件时传输的实时数据 subsets，尽管实时数据可能不如完整的回收数据具有相同的信息量，但在线上检测对于实时监控异常情况非常重要。<details>
<summary>Abstract</summary>
This paper employs an anomaly detection algorithm to assess the normal operation of underwater gliders in unpredictable ocean environments. Real-time alerts can be provided to glider pilots upon detecting any anomalies, enabling them to assume control of the glider and prevent further harm. The detection algorithm is applied to abundant data sets collected in real glider deployments led by the Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). Regarding generality, the experimental evaluation is composed of both offline and online detection modes. The offline detection utilizes full post-recovery data sets, which carries high-resolution information, to present detailed analysis of the anomaly and compare it with pilot logs. The online detection focuses on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery data, the online detection is of great importance as it allows glider pilots to monitor potential abnormal conditions in real time.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文使用异常检测算法来评估水下飞行器在不可预知的海洋环境中的正常运行。在检测到任何异常时，飞行器驾驶员可以接收实时警示，以避免更大的损害。检测算法应用于Skidaway Institute of Oceanography（SkIO）和University of South Florida（USF）在实际飞行器部署中收集的丰富数据集。在总体来说，实验评估包括了线上和线下检测模式。线上检测专注于在飞行器抵达事件时传输的实时数据 subsets，尽管实时数据可能不具备完整的信息，但是在线上检测对于实时监测异常情况非常重要。
</details></li>
</ul>
<hr>
<h2 id="Pretrained-deep-models-outperform-GBDTs-in-Learning-To-Rank-under-label-scarcity"><a href="#Pretrained-deep-models-outperform-GBDTs-in-Learning-To-Rank-under-label-scarcity" class="headerlink" title="Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity"></a>Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00177">http://arxiv.org/abs/2308.00177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charlie Hou, Kiran Koshy Thekumparampil, Michael Shavlovsky, Giulia Fanti, Yesh Dattatreya, Sujay Sanghavi</li>
<li>for: 这个论文主要研究了 whether unsupervised pretraining can improve Learning-To-Rank (LTR) 性能，以及如何使用 simple design choices 来实现这一目标。</li>
<li>methods: 论文使用了 SimCLR-Rank，一种修改后 SimCLR 方法，以进行 ranking-specific 的预训练。</li>
<li>results: 论文发现，预训练的深度学习模型可以Soundly 超过 GBDT 和其他非预训练模型在 label 数量比较多的情况下，并且预训练模型也常常在对异常数据进行排名时表现更好。<details>
<summary>Abstract</summary>
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data. We also show that pretrained models also often achieve significantly better robustness than non-pretrained models (GBDTs or DL models) in ranking outlier data.
</details>
<details>
<summary>摘要</summary>
while deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. to the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. in this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data. we also show that pretrained models also often achieve significantly better robustness than non-pretrained models (GBDTs or DL models) in ranking outlier data.Here is the translation in Traditional Chinese:而深度学习（DL）模型在文本和图像领域是现今最佳的，但它们尚未一致地超越Gradient Boosted Decision Trees（GBDT）在排序学习（LTR）问题上。大多数最近的表现提升都是由DL模型在文本和图像任务中使用无监督预训，这些预训可以处理数量为数十倍于标签数据。根据我们所知，无监督预训尚未应用于LTR问题，LTR问题通常生成了巨量的无标签数据。在这个工作中，我们研究了是否可以透过无监督预训提高LTR性能，比较GBDT和其他非预训模型。我们还使用了简单的设计选择，包括我们的排名具体化SimCLR-Rank，将SimCLR（对于图像的无监督预训方法） Ranking化来提高排名性能。我们产生了预训深度学习模型，与GBDT和其他非预训模型相比，在标签数据远远多于无标签数据时表现出色。我们还表明了预训模型通常在排名偏差数据时表现更好的响应性。
</details></li>
</ul>
<hr>
<h2 id="A-Flow-Artist-for-High-Dimensional-Cellular-Data"><a href="#A-Flow-Artist-for-High-Dimensional-Cellular-Data" class="headerlink" title="A Flow Artist for High-Dimensional Cellular Data"></a>A Flow Artist for High-Dimensional Cellular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00176">http://arxiv.org/abs/2308.00176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kincaid MacDonald, Dhananjay Bhaskar, Guy Thampakkul, Nhi Nguyen, Joia Zhang, Michael Perlmutter, Ian Adelstein, Smita Krishnaswamy</li>
<li>for: 该论文旨在为高维数据中的点云数据样本提供嵌入，并利用相关的流速信息。</li>
<li>methods: 该论文提出了一种基于神经网络的嵌入方法，可以同时学习点云中的坐标和速度信息。</li>
<li>results: 该论文在测试数据集和单个细胞RNA速度数据上表现出了更好地分离和可视化高维数据中的速度相关结构。<details>
<summary>Abstract</summary>
We consider the problem of embedding point cloud data sampled from an underlying manifold with an associated flow or velocity. Such data arises in many contexts where static snapshots of dynamic entities are measured, including in high-throughput biology such as single-cell transcriptomics. Existing embedding techniques either do not utilize velocity information or embed the coordinates and velocities independently, i.e., they either impose velocities on top of an existing point embedding or embed points within a prescribed vector field. Here we present FlowArtist, a neural network that embeds points while jointly learning a vector field around the points. The combination allows FlowArtist to better separate and visualize velocity-informed structures. Our results, on toy datasets and single-cell RNA velocity data, illustrate the value of utilizing coordinate and velocity information in tandem for embedding and visualizing high-dimensional data.
</details>
<details>
<summary>摘要</summary>
我团队考虑了嵌入点云数据，该数据来自于下面的拓扑结构，并且有关联的流速信息。这类数据在高通量生物学中经常出现，例如单细胞肽谱分析。现有的嵌入技术 Either do not utilize velocity information or embed points and velocities independently, i.e., they either impose velocities on top of an existing point embedding or embed points within a prescribed vector field. Here we present FlowArtist, a neural network that embeds points while jointly learning a vector field around the points. The combination allows FlowArtist to better separate and visualize velocity-informed structures. Our results, on toy datasets and single-cell RNA velocity data, illustrate the value of utilizing coordinate and velocity information in tandem for embedding and visualizing high-dimensional data.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Data-and-Model-Heterogeneity-in-Medical-Imaging"><a href="#Federated-Learning-for-Data-and-Model-Heterogeneity-in-Medical-Imaging" class="headerlink" title="Federated Learning for Data and Model Heterogeneity in Medical Imaging"></a>Federated Learning for Data and Model Heterogeneity in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00155">http://arxiv.org/abs/2308.00155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussain Ahmad Madni, Rao Muhammad Umer, Gian Luca Foresti</li>
<li>for: 这个研究目的是为了解决 Federated Learning (FL) 中的数据和模型多样性问题，以提高 FL 的效率。</li>
<li>methods: 本研究使用了知识传播和对称损失来解决模型和数据多样性问题。</li>
<li>results: 实验结果显示，提案的方法在医疗数据上比较其他现有方法更有优势。<details>
<summary>Abstract</summary>
Federated Learning (FL) is an evolving machine learning method in which multiple clients participate in collaborative learning without sharing their data with each other and the central server. In real-world applications such as hospitals and industries, FL counters the challenges of data heterogeneity and model heterogeneity as an inevitable part of the collaborative training. More specifically, different organizations, such as hospitals, have their own private data and customized models for local training. To the best of our knowledge, the existing methods do not effectively address both problems of model heterogeneity and data heterogeneity in FL. In this paper, we exploit the data and model heterogeneity simultaneously, and propose a method, MDH-FL (Exploiting Model and Data Heterogeneity in FL) to solve such problems to enhance the efficiency of the global model in FL. We use knowledge distillation and a symmetric loss to minimize the heterogeneity and its impact on the model performance. Knowledge distillation is used to solve the problem of model heterogeneity, and symmetric loss tackles with the data and label heterogeneity. We evaluate our method on the medical datasets to conform the real-world scenario of hospitals, and compare with the existing methods. The experimental results demonstrate the superiority of the proposed approach over the other existing methods.
</details>
<details>
<summary>摘要</summary>
协同学习（Federated Learning，FL）是一种发展中的机器学习方法，其中多个客户端参与共同学习而无需分享他们的数据。在实际应用中，如医院和产业，FL 可以解决数据多样性和模型多样性作为共同训练的不可避免的一部分。更 Specifically, different organizations, such as hospitals, have their own private data and customized models for local training. To the best of our knowledge, the existing methods do not effectively address both problems of model heterogeneity and data heterogeneity in FL. In this paper, we exploit the data and model heterogeneity simultaneously, and propose a method, MDH-FL (Exploiting Model and Data Heterogeneity in FL) to solve such problems to enhance the efficiency of the global model in FL. We use knowledge distillation and a symmetric loss to minimize the heterogeneity and its impact on the model performance. Knowledge distillation is used to solve the problem of model heterogeneity, and symmetric loss tackles with the data and label heterogeneity. We evaluate our method on medical datasets to conform the real-world scenario of hospitals, and compare with the existing methods. The experimental results demonstrate the superiority of the proposed approach over the other existing methods.
</details></li>
</ul>
<hr>
<h2 id="DiffusAL-Coupling-Active-Learning-with-Graph-Diffusion-for-Label-Efficient-Node-Classification"><a href="#DiffusAL-Coupling-Active-Learning-with-Graph-Diffusion-for-Label-Efficient-Node-Classification" class="headerlink" title="DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification"></a>DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00146">http://arxiv.org/abs/2308.00146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmu-dbs/diffusal">https://github.com/lmu-dbs/diffusal</a></li>
<li>paper_authors: Sandra Gilhuber, Julian Busch, Daniel Rotthues, Christian M. M. Frey, Thomas Seidl</li>
<li>for: 这篇论文的目的是为了提出一种新的活况graph学习方法，以提高label效率和转移性。</li>
<li>methods: 这篇论文使用了三种独立的评估函数来选择最有价值的节点，包括Model Uncertainty、Diversity Component和Node Importance，这些评估函数都是通过传播规律来计算。</li>
<li>results: 实验结果显示，这篇论文的方法在不同的benchmark数据集上比随机选择更高，并且在100%的数据集和标签预算下表现更好。<details>
<summary>Abstract</summary>
Node classification is one of the core tasks on attributed graphs, but successful graph learning solutions require sufficiently labeled data. To keep annotation costs low, active graph learning focuses on selecting the most qualitative subset of nodes that maximizes label efficiency. However, deciding which heuristic is best suited for an unlabeled graph to increase label efficiency is a persistent challenge. Existing solutions either neglect aligning the learned model and the sampling method or focus only on limited selection aspects. They are thus sometimes worse or only equally good as random sampling. In this work, we introduce a novel active graph learning approach called DiffusAL, showing significant robustness in diverse settings. Toward better transferability between different graph structures, we combine three independent scoring functions to identify the most informative node samples for labeling in a parameter-free way: i) Model Uncertainty, ii) Diversity Component, and iii) Node Importance computed via graph diffusion heuristics. Most of our calculations for acquisition and training can be pre-processed, making DiffusAL more efficient compared to approaches combining diverse selection criteria and similarly fast as simpler heuristics. Our experiments on various benchmark datasets show that, unlike previous methods, our approach significantly outperforms random selection in 100% of all datasets and labeling budgets tested.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>节点分类是 attributed graphs 的核心任务，但是成功的图学解决方案需要具备充分的标签数据。为了降低标签成本，活跃图学学习强调选择最优质量的节点子集，以最大化标签效率。然而，为无标签图选择最佳的选择方法是一个长期的挑战。现有的解决方案可能会忽略对学习模型和采样方法的对齐，或者只关注有限的选择方面。这些方法有时会比随机采样更差，或者只等于随机采样。在这种情况下，我们介绍了一种新的活跃图学学习方法，称为DiffusAL，它在多种场景下显示了明显的稳定性。为了更好地转移不同图结构之间的知识，我们将三种独立的评价函数组合在一起，以无参数的方式标识最有用的节点样本 для标签：i) 模型不确定性，ii) 多样性分量，和 iii) 节点重要性通过图 diffusion 规则来计算。大多数我们的获取和训练计算可以预处理，使得DiffusAL比 combining 多种选择 criterion 和同样快速的方法更高效。我们在多个 benchmark 数据集上进行了实验，发现与前一代方法不同，我们的方法在所有测试 dataset 和标签预算中都能够明显超越随机选择。
</details></li>
</ul>
<hr>
<h2 id="Formally-Explaining-Neural-Networks-within-Reactive-Systems"><a href="#Formally-Explaining-Neural-Networks-within-Reactive-Systems" class="headerlink" title="Formally Explaining Neural Networks within Reactive Systems"></a>Formally Explaining Neural Networks within Reactive Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00143">http://arxiv.org/abs/2308.00143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahaf Bassan, Guy Amir, Davide Corsi, Idan Refaeli, Guy Katz</li>
<li>for: 这种研究旨在解释深度神经网络（DNNs）控制的反应系统中的行为。</li>
<li>methods: 该研究提出了一种基于验证的可解释AI（XAI）技术，可以准确地指出引入了哪些输入特征导致DNN行为如此。</li>
<li>results: 研究人员提出了一种有效地计算简洁解释的方法，利用系统的转移约束来缩小检查空间，并在两个流行的自动导航 benchmark 上证明了该方法的高效性和可靠性。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did. Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems, where the DNN is invoked independently of past invocations, as opposed to reactive systems. Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. We evaluate our approach on two popular benchmarks from the domain of automated navigation; and observe that our methods allow the efficient computation of minimal and minimum explanations, significantly outperforming the state of the art. We also demonstrate that our methods produce formal explanations that are more reliable than competing, non-verification-based XAI techniques.
</details>
<details>
<summary>摘要</summary>
In this paper, we aim to bridge this gap by proposing a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. Our approach leverages the system's transition constraints to efficiently calculate succinct explanations, reducing the search space explored by the underlying verifier. We evaluate our method on two popular benchmarks from the domain of automated navigation and show that our approach produces minimal and minimum explanations that are significantly more efficient than the state of the art. We also demonstrate that our methods produce formal explanations that are more reliable than competing, non-verification-based XAI techniques.我们的研究目标是 bridging the gap between deep neural networks (DNNs) 的 opacity 和 its application in reactive systems. DNNs 是 increasingly being used as controllers in reactive systems, but their high opacity makes it difficult to explain and justify their actions. To address this issue, there has been a surge of interest in explainable AI (XAI) techniques that can pinpoint the input features responsible for the DNN's actions. However, existing XAI techniques have two limitations: they are heuristic and do not provide formal guarantees of correctness, and they are typically designed for "one-shot" systems where the DNN is invoked independently of past invocations, rather than reactive systems.我们的方法是使用 DNN 的验证来提供正式的 XAI 技术，用于理解多步、反应式系统中 DNN 的行为。我们的方法利用系统的转换约束来高效计算简洁的解释，从而减少了底层验证器所探索的搜索空间。我们在 automatized navigation 领域的两个popular benchmark上评估了我们的方法，并观察到我们的方法可以生成高效的最小和最小解释，远胜过现状。我们还证明了我们的方法生成的正式解释比非验证基于 XAI 技术更可靠。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Laplacian-Learning-on-Stiefel-Manifolds"><a href="#Semi-Supervised-Laplacian-Learning-on-Stiefel-Manifolds" class="headerlink" title="Semi-Supervised Laplacian Learning on Stiefel Manifolds"></a>Semi-Supervised Laplacian Learning on Stiefel Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00142">http://arxiv.org/abs/2308.00142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chester Holtz, Pengwen Chen, Alexander Cloninger, Chung-Kuan Cheng, Gal Mishne</li>
<li>for: addressing the degeneracy of canonical Laplace learning algorithms in low label rates</li>
<li>methods: reformulating graph-based semi-supervised learning as a nonconvex generalization of a Trust-Region Subproblem (TRS)</li>
<li>results: achieving lower classification error compared to recent state-of-the-art and classical semi-supervised learning methods at extremely low, medium, and high label ratesHere’s the full Chinese text:</li>
<li>for: 用于解决laplace学习算法在低标签率下的异常性问题</li>
<li>methods: 将图基于 semi-supervised learning 转换为非凸泛化 Trust-Region Subproblem（TRS）</li>
<li>results: 在极低、中等和高标签率下 achieved 更低的分类错误率，比之前的状态作准和传统 semi-supervised learning 方法I hope that helps!<details>
<summary>Abstract</summary>
Motivated by the need to address the degeneracy of canonical Laplace learning algorithms in low label rates, we propose to reformulate graph-based semi-supervised learning as a nonconvex generalization of a \emph{Trust-Region Subproblem} (TRS). This reformulation is motivated by the well-posedness of Laplacian eigenvectors in the limit of infinite unlabeled data. To solve this problem, we first show that a first-order condition implies the solution of a manifold alignment problem and that solutions to the classical \emph{Orthogonal Procrustes} problem can be used to efficiently find good classifiers that are amenable to further refinement. Next, we address the criticality of selecting supervised samples at low-label rates. We characterize informative samples with a novel measure of centrality derived from the principal eigenvectors of a certain submatrix of the graph Laplacian. We demonstrate that our framework achieves lower classification error compared to recent state-of-the-art and classical semi-supervised learning methods at extremely low, medium, and high label rates. Our code is available on github\footnote{anonymized for submission}.
</details>
<details>
<summary>摘要</summary>
“受到低标签率下 Laplace 学习算法的衰弱问题的启发，我们提议将图基 semi-supervised learning  reformulate为非对称泛化的 Trust-Region Subproblem（TRS）。这种 reformulation 受到无穷多个标签数据下 Laplacian 埃文方程的定理性的启发。为解这个问题，我们首先证明了一个第一个条件是拟合 manifold 问题的解，并且我们用 classical 的 orthogonal Procrustes 问题的解来高效地找到可以进一步精细化的好分类器。然后，我们关注低标签率下选择监督样本的敏感性。我们使用一种新的中心性度量，基于图 Laplacian 的子矩阵的主要特征向量来Characterize 有用的样本。我们证明了我们的框架在 extremely low、medium 和 high 标签率下的分类错误率比现有的 semi-supervised learning 方法和古典方法低。我们的代码可以在 GitHub 上找到（anonymized for submission）。”Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Suite-of-Fairness-Datasets-for-Tabular-Classification"><a href="#A-Suite-of-Fairness-Datasets-for-Tabular-Classification" class="headerlink" title="A Suite of Fairness Datasets for Tabular Classification"></a>A Suite of Fairness Datasets for Tabular Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00133">http://arxiv.org/abs/2308.00133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Hirzel, Michael Feffer</li>
<li>for: 该论文目的是提高机器学习分类器的公平性。</li>
<li>methods: 该论文使用了一些算法来提高机器学习分类器的公平性，并提供了一些函数来抓取20个公平性数据集和相关的公平性metadata。</li>
<li>results: 该论文通过提供大量的公平性数据集和metadata，希望能够促进未来的公平性意识机器学习研究的更加严格的实验评估。<details>
<summary>Abstract</summary>
There have been many papers with algorithms for improving fairness of machine-learning classifiers for tabular data. Unfortunately, most use only very few datasets for their experimental evaluation. We introduce a suite of functions for fetching 20 fairness datasets and providing associated fairness metadata. Hopefully, these will lead to more rigorous experimental evaluations in future fairness-aware machine learning research.
</details>
<details>
<summary>摘要</summary>
有很多论文提出了对机器学习分类器的公平性进行改进的算法。然而，大多数使用的数据集很少。我们介绍了一个函数集，用于获取20个公平性数据集和相关的公平性元数据。希望这些函数可以促进未来的公平意识机器学习研究的更加严格的实验评估。Here's the word-for-word translation:有很多论文提出了对机器学习分类器的公平性进行改进的算法。然而，大多数使用的数据集很少。我们介绍了一个函数集，用于获取20个公平性数据集和相关的公平性元数据。希望这些函数可以促进未来的公平意识机器学习研究的更加严格的实验评估。
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Learning-with-Residual-Transformer-for-Brain-Tumor-Segmentation"><a href="#Ensemble-Learning-with-Residual-Transformer-for-Brain-Tumor-Segmentation" class="headerlink" title="Ensemble Learning with Residual Transformer for Brain Tumor Segmentation"></a>Ensemble Learning with Residual Transformer for Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00128">http://arxiv.org/abs/2308.00128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanhong Yao, Zheyuan Zhang, Ulas Bagci</li>
<li>for: 本研究旨在提高脑肿瘤分割的精度，因为现有的 U-Net 架构受到复杂形态和文本化肿瘤的限制，以及 pixel-wise 标注的失败。</li>
<li>methods: 本研究提出一种 integrate  transformer 和 U-Net 的新网络架构，通过自适应的混合来提取3D 体积上的上下文信息，并添加 residual connection 以避免信息流失。</li>
<li>results: 在 BraTS 2021 数据集上（3D），我们的模型实现了87.6% 的 Mean Dice 分数，超过了当前最佳方法，示ifying 将多种架构结合使用可以优化脑肿瘤分割。<details>
<summary>Abstract</summary>
Brain tumor segmentation is an active research area due to the difficulty in delineating highly complex shaped and textured tumors as well as the failure of the commonly used U-Net architectures. The combination of different neural architectures is among the mainstream research recently, particularly the combination of U-Net with Transformers because of their innate attention mechanism and pixel-wise labeling. Different from previous efforts, this paper proposes a novel network architecture that integrates Transformers into a self-adaptive U-Net to draw out 3D volumetric contexts with reasonable computational costs. We further add a residual connection to prevent degradation in information flow and explore ensemble methods, as the evaluated models have edges on different cases and sub-regions. On the BraTS 2021 dataset (3D), our model achieves 87.6% mean Dice score and outperforms the state-of-the-art methods, demonstrating the potential for combining multiple architectures to optimize brain tumor segmentation.
</details>
<details>
<summary>摘要</summary>
脑肿 segmentation是一个活跃的研究领域，因为区分高度复杂形态和文本化肿瘤具有极大的挑战。通常使用的U-Net架构也有失败情况。近年来， combining多种神经架构是主流的研究方向，特别是将Transformers搭配U-Net，因为它们的自然注意机制和像素级标注。与之前的尝试不同，本文提出了一种新的网络架构，将Transformers integrate到自适应U-Net中，以实现3D积体上下文的可靠计算。此外，我们还添加了径向连接，以避免信息流失并explore ensemble方法，因为评估模型在不同的情况和子区域上具有优势。在BraTS 2021 dataset（3D）上，我们的模型实现了87.6%的mean Dice分数，超越了当前的状态艺术方法，这表明将多种架构组合起来可以优化脑肿分 segmentation。
</details></li>
</ul>
<hr>
<h2 id="DiviML-A-Module-based-Heuristic-for-Mapping-Neural-Networks-onto-Heterogeneous-Platforms"><a href="#DiviML-A-Module-based-Heuristic-for-Mapping-Neural-Networks-onto-Heterogeneous-Platforms" class="headerlink" title="DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms"></a>DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00127">http://arxiv.org/abs/2308.00127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yassine Ghannane, Mohamed S. Abdelfattah</li>
<li>For: The paper is written to address the challenge of compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices in heterogeneous datacenters.* Methods: The paper proposes a general framework for heterogeneous DNN compilation, which includes automatic partitioning and device mapping, as well as a scheduler that integrates both an exact solver and a modularity-based heuristic for scalability.* Results: The paper shows that the proposed framework can achieve more than 3 times lower latency and up to 2.9 times higher throughput compared to naively running DNNs on the fastest GPU, by automatically leveraging both data and model parallelism to deploy DNNs on a heterogeneous system comprised of a CPU and two distinct GPUs. Additionally, the paper demonstrates the effectiveness of the proposed modularity-based “splitting” heuristic, which improves the solution runtime up to 395 times without sacrificing solution quality compared to an exact MILP solution.<details>
<summary>Abstract</summary>
Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two distinct GPUs. Compared to na\"ively running DNNs on the fastest GPU, he proposed framework can achieve more than 3$\times$ times lower latency and up to 2.9$\times$ higher throughput by automatically leveraging both data and model parallelism to deploy DNNs on our sample heterogeneous server node. Moreover, our modularity-based "splitting" heuristic improves the solution runtime up to 395$\times$ without noticeably sacrificing solution quality compared to an exact MILP solution, and outperforms all other heuristics by 30-60% solution quality. Finally, our case study shows how we can extend our framework to schedule large language models across multiple heterogeneous servers by exploiting symmetry in the hardware setup. Our code can be easily plugged in to existing frameworks, and is available at https://github.com/abdelfattah-lab/diviml.
</details>
<details>
<summary>摘要</summary>
现代数据中心越来越多样化，包括特циализирован的网络硬件、视频处理硬件和深度学习硬件。为了利用现代数据中心的多样化计算能力，我们开发了一种编译器级别的深度神经网络（DNNs）分配方法，并提供自动分配和设备映射。我们的排程器 integrate both an exact solver（通过混合integer linear programming（MILP）表示）和一个基于模块性的优化器，以提高可扩展性。此外，我们提出了一个理论下界方程，可以评估优化解的质量。我们在一个包括CPU和两种不同GPU的多元环境中进行了优化。与直接运行DNNs在最快的GPU上相比，我们的框架可以实现更多于3倍的延迟和最多2.9倍的吞吐量，通过自动利用数据和模型平行性来部署DNNs。此外，我们的模块性"splitting"优化器可以提高解 runtime up to 395倍，而无需明显降低解质量相比于精确MILP解，并且在其他优化器中提高30-60%的解质量。最后，我们的案例研究显示了如何使用对硬件设置的 симметry 来扩展我们的框架，以部署大型自然语言模型 across multiple heterogeneous servers。我们的代码可以轻松地插入到现有框架中，并可以在https://github.com/abdelfattah-lab/diviml 中获得。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Occupancy-Models-for-Dense-Packing-of-Complex-Novel-Objects"><a href="#Convolutional-Occupancy-Models-for-Dense-Packing-of-Complex-Novel-Objects" class="headerlink" title="Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects"></a>Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00091">http://arxiv.org/abs/2308.00091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nikhilmishra000/fcon">https://github.com/nikhilmishra000/fcon</a></li>
<li>paper_authors: Nikhil Mishra, Pieter Abbeel, Xi Chen, Maximilian Sieb</li>
<li>for:  dense packing in pick-and-place systems for warehouse and logistics applications</li>
<li>methods:  fully-convolutional shape completion model (F-CON) combined with off-the-shelf planning methods</li>
<li>results:  substantially better dense packing than other shape completion methods in real-world cluttered scenes<details>
<summary>Abstract</summary>
Dense packing in pick-and-place systems is an important feature in many warehouse and logistics applications. Prior work in this space has largely focused on planning algorithms in simulation, but real-world packing performance is often bottlenecked by the difficulty of perceiving 3D object geometry in highly occluded, partially observed scenes. In this work, we present a fully-convolutional shape completion model, F-CON, which can be easily combined with off-the-shelf planning methods for dense packing in the real world. We also release a simulated dataset, COB-3D-v2, that can be used to train shape completion models for real-word robotics applications, and use it to demonstrate that F-CON outperforms other state-of-the-art shape completion methods. Finally, we equip a real-world pick-and-place system with F-CON, and demonstrate dense packing of complex, unseen objects in cluttered scenes. Across multiple planning methods, F-CON enables substantially better dense packing than other shape completion methods.
</details>
<details>
<summary>摘要</summary>
dense packing在选择和放置系统中是重要的特点，在仓库和物流应用中具有广泛的应用。先前的工作主要集中在仿真中的规划算法上，但实际的填充性常被受到三维物体干扰、部分观察场景的困难所压制。在这种情况下，我们提出了一种易于组合的完全卷积形成模型，F-CON，可以与市场上 readily available 的规划方法结合使用，以实现在实际世界中的高密度填充。我们还发布了COB-3D-v2 simulated dataset，可以用于训练形成模型，并使用其来证明F-CON在实际世界中的表现比其他状态空间中的形成方法更好。最后，我们在实际的选择和放置系统上装备了F-CON，并在拥挤的场景中实现了复杂、未看到的物体的高密度填充。通过不同的规划方法，F-CON在其他形成方法中实现了许多更好的高密度填充。
</details></li>
</ul>
<hr>
<h2 id="New-Lower-Bounds-for-Testing-Monotonicity-and-Log-Concavity-of-Distributions"><a href="#New-Lower-Bounds-for-Testing-Monotonicity-and-Log-Concavity-of-Distributions" class="headerlink" title="New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions"></a>New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00089">http://arxiv.org/abs/2308.00089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Cheng, Daniel M. Kane, Zhicheng Zheng</li>
<li>for: 该论文是为了证明分布测试下界的技术而写的。</li>
<li>methods: 该论文使用的方法是构造一对匹配分布的家族，其中一个家族保持定义不等式的条件，而另一个家族违反这些不等式。</li>
<li>results: 该论文通过这种方法获得了新的下界，包括 monotonicity testing over discrete cubes 和 log-concavity testing 的紧张下界。<details>
<summary>Abstract</summary>
We develop a new technique for proving distribution testing lower bounds for properties defined by inequalities involving the bin probabilities of the distribution in question. Using this technique we obtain new lower bounds for monotonicity testing over discrete cubes and tight lower bounds for log-concavity testing.   Our basic technique involves constructing a pair of moment-matching families of distributions by tweaking the probabilities of pairs of bins so that one family maintains the defining inequalities while the other violates them.
</details>
<details>
<summary>摘要</summary>
我们开发了一种新的技术，用于证明分布测试下界限的方法，该方法基于分布中每个分布的概率的不等式定义的性质。使用这种技术，我们获得了新的下界限 для升序测试和紧密的下界限 для对排斜度测试。  我们的基本技术是建立一对匹配时间的分布家族，通过修改每个分布中的概率对的概率来使一家分布满足定义的不等式，而另一家分布则违反这些不等式。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Deep-Learning-based-Model-to-Defend-Network-Intrusion-Detection-System-against-Adversarial-Attacks"><a href="#A-Novel-Deep-Learning-based-Model-to-Defend-Network-Intrusion-Detection-System-against-Adversarial-Attacks" class="headerlink" title="A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks"></a>A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00077">http://arxiv.org/abs/2308.00077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushnaseeb Roshan, Aasim Zafar, Shiekh Burhan Ul Haque</li>
<li>for: 本研究旨在探讨深度学习基于网络入侵检测系统（NIDS）中的强大敌意攻击算法以及其防御策略。</li>
<li>methods: 本研究使用了四种强大敌意攻击算法：快速梯度签名方法（FGSM）、杠杆矩阵攻击（JSMA）、投影式梯度下降（PGD）以及加拿大和瓦格纳（C&amp;W）攻击。而防御策略则是使用了对抗训练来增强NIDS模型的可抗性。</li>
<li>results: 研究结果分为三个阶段：第一阶段是没有遭受攻击之前的结果，第二阶段是遭受攻击后的结果，第三阶段是经过对抗训练后的结果。使用了加拿大INSTIDS-2017数据集进行评估，并通过了不同的性能指标如f1-score、准确率等来评估模型的性能。<details>
<summary>Abstract</summary>
Network Intrusion Detection System (NIDS) is an essential tool in securing cyberspace from a variety of security risks and unknown cyberattacks. A number of solutions have been implemented for Machine Learning (ML), and Deep Learning (DL) based NIDS. However, all these solutions are vulnerable to adversarial attacks, in which the malicious actor tries to evade or fool the model by injecting adversarial perturbed examples into the system. The main aim of this research work is to study powerful adversarial attack algorithms and their defence method on DL-based NIDS. Fast Gradient Sign Method (FGSM), Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and Carlini & Wagner (C&W) are four powerful adversarial attack methods implemented against the NIDS. As a defence method, Adversarial Training is used to increase the robustness of the NIDS model. The results are summarized in three phases, i.e., 1) before the adversarial attack, 2) after the adversarial attack, and 3) after the adversarial defence. The Canadian Institute for Cybersecurity Intrusion Detection System 2017 (CICIDS-2017) dataset is used for evaluation purposes with various performance measurements like f1-score, accuracy etc.
</details>
<details>
<summary>摘要</summary>
网络侵入检测系统（NIDS）是保护网络安全的重要工具，它可以检测到多种安全风险和未知的网络攻击。为了提高NIDS的准确率和鲁棒性，许多解决方案已经实施了机器学习（ML）和深度学习（DL）技术。然而，这些解决方案都受到了恶意攻击的威胁，攻击者可以通过注入恶意抗件来诱导模型出错。本研究的主要目标是研究强大的恶意攻击算法和其防御方法在DL-based NIDS中。fast gradient sign method（FGSM）、Jacobian saliency map attack（JSMA）、projected gradient descent（PGD）和Carlini & Wagner（C&W）是四种强大的恶意攻击方法，它们都是对NIDS模型的挑战。为了增强NIDS模型的鲁棒性，我们使用了对抗训练。研究结果分为三个阶段：在攻击之前、攻击后和防御后。我们使用了加拿大安全攻击检测系统2017（CICIDS-2017）数据集进行评估，并使用了各种性能指标如f1-score、准确率等来评估模型的表现。
</details></li>
</ul>
<hr>
<h2 id="Crowd-Safety-Manager-Towards-Data-Driven-Active-Decision-Support-for-Planning-and-Control-of-Crowd-Events"><a href="#Crowd-Safety-Manager-Towards-Data-Driven-Active-Decision-Support-for-Planning-and-Control-of-Crowd-Events" class="headerlink" title="Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events"></a>Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00076">http://arxiv.org/abs/2308.00076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panchamy Krishnakumari, Sascha Hoogendoorn-Lanser, Jeroen Steenbakkers, Serge Hoogendoorn</li>
<li>For: The paper aims to enhance crowd management in both the planning and operational phases, using innovative data collection techniques, data integration, and visualization, as well as artificial intelligence (AI) tools for risk identification.* Methods: The paper introduces the Bowtie model, a comprehensive framework that assesses and predicts risk levels by combining objective estimations and predictions with various aggravating factors. The proposed framework is applied to the Crowd Safety Manager project in Scheveningen, using a wealth of real-time data sources, including Resono, which provides insights into the number of visitors and their movements. Advanced machine learning techniques, such as XGBoost, are used for forecasting.* Results: The results indicate that the predictions are adequately accurate, but certain locations may benefit from additional input data to further enhance prediction quality. Despite these limitations, the work contributes to a more effective crowd management system and opens avenues for further advancements in this critical field.Here’s the Chinese translation of the three points:* For: 这篇论文目的是提高人群管理的规划和运行阶段，使用创新的数据采集技术、数据 интеграción和可视化，以及人工智能（AI）工具来确定风险水平。* Methods: 论文介绍了一种名为“环”模型，该模型将对象估计和预测与多种累加因素相结合，以评估预计的风险水平。该模型在芝尼登的群体安全管理项目中应用，使用了大量实时数据源，包括Resono，以获取访客数量和移动征性的情况。文中使用了先进的机器学习技术，如XGBoost框架，进行预测。* Results: 结果表明预测结果具有足够的准确性，但certain location可能需要更多的输入数据来进一步提高预测质量。尽管有这些限制，这种工作仍然为人群管理系统做出了贡献，并开启了进一步提高这一领域的前iers。<details>
<summary>Abstract</summary>
This paper presents novel technology and methodology aimed at enhancing crowd management in both the planning and operational phases. The approach encompasses innovative data collection techniques, data integration, and visualization using a 3D Digital Twin, along with the incorporation of artificial intelligence (AI) tools for risk identification. The paper introduces the Bowtie model, a comprehensive framework designed to assess and predict risk levels. The model combines objective estimations and predictions, such as traffic flow operations and crowdedness levels, with various aggravating factors like weather conditions, sentiments, and the purpose of visitors, to evaluate the expected risk of incidents. The proposed framework is applied to the Crowd Safety Manager project in Scheveningen, where the DigiTwin is developed based on a wealth of real-time data sources. One noteworthy data source is Resono, offering insights into the number of visitors and their movements, leveraging a mobile phone panel of over 2 million users in the Netherlands. Particular attention is given to the left-hand side of the Bowtie, which includes state estimation, prediction, and forecasting. Notably, the focus is on generating multi-day ahead forecasts for event-planning purposes using Resono data. Advanced machine learning techniques, including the XGBoost framework, are compared, with XGBoost demonstrating the most accurate forecasts. The results indicate that the predictions are adequately accurate. However, certain locations may benefit from additional input data to further enhance prediction quality. Despite these limitations, this work contributes to a more effective crowd management system and opens avenues for further advancements in this critical field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Using-Kernel-SHAP-XAI-Method-to-optimize-the-Network-Anomaly-Detection-Model"><a href="#Using-Kernel-SHAP-XAI-Method-to-optimize-the-Network-Anomaly-Detection-Model" class="headerlink" title="Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model"></a>Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00074">http://arxiv.org/abs/2308.00074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khushnaseeb Roshan, Aasim Zafar</li>
<li>for: 本研究旨在使用Explainable Artificial Intelligence（XAI）技术检测和解释网络异常行为，以提高网络异常检测模型的准确率、回归率、精度和F1分数。</li>
<li>methods: 本研究使用kernelSHAP方法实现网络异常检测和解释。</li>
<li>results: 实验结果显示，使用OPT_Model（在无监督方式下训练）可以达到网络异常检测模型的高准确率和F1分数（准确率为0.90，F1分数为0.76）。<details>
<summary>Abstract</summary>
Anomaly detection and its explanation is important in many research areas such as intrusion detection, fraud detection, unknown attack detection in network traffic and logs. It is challenging to identify the cause or explanation of why one instance is an anomaly? and the other is not due to its unbounded and lack of supervisory nature. The answer to this question is possible with the emerging technique of explainable artificial intelligence (XAI). XAI provides tools and techniques to interpret and explain the output and working of complex models such as Deep Learning (DL). This paper aims to detect and explain network anomalies with XAI, kernelSHAP method. The same approach is used to improve the network anomaly detection model in terms of accuracy, recall, precision and f score. The experiment is conduced with the latest CICIDS2017 dataset. Two models are created (Model_1 and OPT_Model) and compared. The overall accuracy and F score of OPT_Model (when trained in unsupervised way) are 0.90 and 0.76, respectively.
</details>
<details>
<summary>摘要</summary>
anomaly detection 和其解释对许多研究领域都是重要的，如侵入检测、诈骗检测、未知攻击检测在网络流量和日志中。但是确定一个实例是异常的原因或解释是困难的，因为它们没有监督性和是无限的。这个问题的答案是可能的，通过新兴的解释人工智能（XAI）技术。XAI提供了解释和解释复杂模型如深度学习（DL）的工具和技术。这篇论文的目标是使用XAI技术检测和解释网络异常，并使用kernelSHAP方法进行改进。实验使用最新的CICIDS2017数据集。两个模型（Model_1和OPT_Model）被创建并比较。OPT_Model在无监督方式下训练时的总准确率和F分数分别为0.90和0.76。
</details></li>
</ul>
<hr>
<h2 id="T-Fusion-Net-A-Novel-Deep-Neural-Network-Augmented-with-Multiple-Localizations-based-Spatial-Attention-Mechanisms-for-Covid-19-Detection"><a href="#T-Fusion-Net-A-Novel-Deep-Neural-Network-Augmented-with-Multiple-Localizations-based-Spatial-Attention-Mechanisms-for-Covid-19-Detection" class="headerlink" title="T-Fusion Net: A Novel Deep Neural Network Augmented with Multiple Localizations based Spatial Attention Mechanisms for Covid-19 Detection"></a>T-Fusion Net: A Novel Deep Neural Network Augmented with Multiple Localizations based Spatial Attention Mechanisms for Covid-19 Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00053">http://arxiv.org/abs/2308.00053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Susmita Ghosh, Abhiroop Chatterjee</li>
<li>for: 这篇论文的目的是提出一种新的深度神经网络（称为T-Fusion Net），以增强图像分类任务的性能。</li>
<li>methods: 该网络使用多个本地化的空间注意 Mechanism，让网络专注于重要的图像区域，提高了其描述力。 homogeneous ensemble of the said network 是用来进一步增强图像分类精度。</li>
<li>results: 实验结果显示，提案的T-Fusion Net和homogeneous ensemble model在COVID-19（SARS-CoV-2 CT scan） dataset上表现出色，与其他现有的方法相比，获得了97.59%和98.4%的准确率。<details>
<summary>Abstract</summary>
In recent years, deep neural networks are yielding better performance in image classification tasks. However, the increasing complexity of datasets and the demand for improved performance necessitate the exploration of innovative techniques. The present work proposes a new deep neural network (called as, T-Fusion Net) that augments multiple localizations based spatial attention. This attention mechanism allows the network to focus on relevant image regions, improving its discriminative power. A homogeneous ensemble of the said network is further used to enhance image classification accuracy. For ensembling, the proposed approach considers multiple instances of individual T-Fusion Net. The model incorporates fuzzy max fusion to merge the outputs of individual nets. The fusion process is optimized through a carefully chosen parameter to strike a balance on the contributions of the individual models. Experimental evaluations on benchmark Covid-19 (SARS-CoV-2 CT scan) dataset demonstrate the effectiveness of the proposed T-Fusion Net as well as its ensemble. The proposed T-Fusion Net and the homogeneous ensemble model exhibit better performance, as compared to other state-of-the-art methods, achieving accuracy of 97.59% and 98.4%, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reinforcement-Learning-for-Generative-AI-State-of-the-Art-Opportunities-and-Open-Research-Challenges"><a href="#Reinforcement-Learning-for-Generative-AI-State-of-the-Art-Opportunities-and-Open-Research-Challenges" class="headerlink" title="Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges"></a>Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00031">http://arxiv.org/abs/2308.00031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgio Franceschelli, Mirco Musolesi</li>
<li>for: 本文概述了在生成人工智能领域应用强化学习的现状、机遇和未解决问题。</li>
<li>methods: 本文讨论了三种应用方式，即使RL作为生成无特定目标的替代方法、同时Maximize一个函数的生成输出方法、以及通过嵌入desired特性来捕捉不易由目标函数捕捉的生成过程。</li>
<li>results: 本文结束于对这个吸引人的新兴领域的机遇和挑战的深入讨论。<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
</details>
<details>
<summary>摘要</summary>
人工智能创新（AI）在计算机科学领域最近一代最吸引人的发展之一。同时，对rl（强化学习）的应用也在不断演化。在这篇评论中，我们讨论RL在生成AI中的状态、机会和未解决的问题。特别是，我们将讨论RL作为生成无明确目标的替代方法，RL作为同时最大化目标函数的输出生成方法，以及RL作为嵌入需要精准捕捉的特征的生成过程中的方法。我们将结束这篇评论 avec一个深入的讨论在这一领域中的机会和挑战。
</details></li>
</ul>
<hr>
<h2 id="Conformal-PID-Control-for-Time-Series-Prediction"><a href="#Conformal-PID-Control-for-Time-Series-Prediction" class="headerlink" title="Conformal PID Control for Time Series Prediction"></a>Conformal PID Control for Time Series Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16895">http://arxiv.org/abs/2307.16895</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aangelopoulos/conformal-time-series">https://github.com/aangelopoulos/conformal-time-series</a></li>
<li>paper_authors: Anastasios N. Angelopoulos, Emmanuel J. Candes, Ryan J. Tibshirani</li>
<li>for: 这个论文的目的是提供一些可靠的时间序列预测算法，并为这些算法提供正式的保证。</li>
<li>methods: 这个论文使用了准确预测、控制理论和在线学习等方法，并能够在线预测具有系统性错误的时间序列中的准确分数。</li>
<li>results: 实验表明，使用这些算法可以提高对COVID-19死亡人数的4周前预测覆盖率，以及预测电力需求、股票市场返现和气温的准确率。<details>
<summary>Abstract</summary>
We study the problem of uncertainty quantification for time series prediction, with the goal of providing easy-to-use algorithms with formal guarantees. The algorithms we present build upon ideas from conformal prediction and control theory, are able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts. Our theory both simplifies and strengthens existing analyses in online conformal prediction. Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in the U.S. show an improvement in coverage over the ensemble forecaster used in official CDC communications. We also run experiments on predicting electricity demand, market returns, and temperature using autoregressive, Theta, Prophet, and Transformer models. We provide an extendable codebase for testing our methods and for the integration of new algorithms, data sets, and forecasting rules.
</details>
<details>
<summary>摘要</summary>
我们研究时间序列预测不确定性评估问题，目的是提供有正式保证的易用算法。我们的算法基于准确预测和控制理论的想法，可在线上预测充分分配概率分布，并适应系统性错误的变化，如季节性、趋势和总分布变化。我们的理论简化了和加强了现有的在线准确预测分析。我们在美国州级COVID-19死亡人数预测4个星期前的实验中显示了覆盖率的提高，并在预测电力需求、股票市场回报和温度等方面进行了实验。我们提供了可扩展的代码库，用于测试我们的方法和新算法、数据集和预测规则的集成。
</details></li>
</ul>
<hr>
<h2 id="Predicting-masked-tokens-in-stochastic-locations-improves-masked-image-modeling"><a href="#Predicting-masked-tokens-in-stochastic-locations-improves-masked-image-modeling" class="headerlink" title="Predicting masked tokens in stochastic locations improves masked image modeling"></a>Predicting masked tokens in stochastic locations improves masked image modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00566">http://arxiv.org/abs/2308.00566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Bar, Florian Bordes, Assaf Shocher, Mahmoud Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, Yann LeCun</li>
<li>for: 这篇论文旨在提高自动学习中的Masked Image Modeling（MIM）表现，以提高下游任务的性能。</li>
<li>methods: 本文提出了FlexPredict，一种随机推几个几个掩盖的token位置来导引模型学习更加响应于位置不确定性的方法。</li>
<li>results: 相比MIM基eline，FlexPredict在ImageNet直线探针中提高了1.6%的性能，并在半指示性视频分类中提高了2.5%的性能。<details>
<summary>Abstract</summary>
Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, FlexPredict boosts ImageNet linear probing by 1.6% with ViT-B and by 2.5% for semi-supervised video segmentation using ViT-L.
</details>
<details>
<summary>摘要</summary>
自我监督学习是深度学习中有前途的潜在方法，它允许通过构建预测任务来学习无标记数据中的有用表示。在自然语言处理中，主要的预测任务是遮盖语言模型（MLM），而在计算机视觉中则有相应的equivalent，即遮盖图像模型（MIM）。然而，MIM具有困难的特点，即需要准确预测 semantic content的位置。例如，给定一幅缺失的狗图片，我们可以预测有尾巴，但是无法准确地确定其位置。在这项工作中，我们提出了FlexPredict，一种随机模型，解决这个挑战。我们通过conditioning模型的掩码位置来引导模型学习更加Robust的特征。我们的方法可以提高下游任务的性能，例如，相比MIM基eline，FlexPredict在ImageNet线性探测中提高了1.6%的ViT-B和2.5%的半supervised видео分割使用ViT-L。
</details></li>
</ul>
<hr>
<h2 id="Foundational-Models-for-Fault-Diagnosis-of-Electrical-Motors"><a href="#Foundational-Models-for-Fault-Diagnosis-of-Electrical-Motors" class="headerlink" title="Foundational Models for Fault Diagnosis of Electrical Motors"></a>Foundational Models for Fault Diagnosis of Electrical Motors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16891">http://arxiv.org/abs/2307.16891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sriram Anbalagan, Deepesh Agarwal, Balasubramaniam Natarajan, Babji Srinivasan</li>
<li>for: 本研究旨在提高电动机故障诊断方法的可靠性和效率，适用于实际应用场景中的各种运行条件和机器。</li>
<li>methods: 该研究提出了一种基于神经网络自我supervised learning的框架，包括建立一个高级特征学习的神经网络后ION，并在不同的运行条件和机器上进行细化调整。</li>
<li>results: 实验结果表明，该方法可以在不同的故障情况和运行条件下达到高于90%的分类精度，并且可以在不同的机器上进行跨机器的故障诊断任务。<details>
<summary>Abstract</summary>
A majority of recent advancements related to the fault diagnosis of electrical motors are based on the assumption that training and testing data are drawn from the same distribution. However, the data distribution can vary across different operating conditions during real-world operating scenarios of electrical motors. Consequently, this assumption limits the practical implementation of existing studies for fault diagnosis, as they rely on fully labelled training data spanning all operating conditions and assume a consistent distribution. This is because obtaining a large number of labelled samples for several machines across different fault cases and operating scenarios may be unfeasible. In order to overcome the aforementioned limitations, this work proposes a framework to develop a foundational model for fault diagnosis of electrical motors. It involves building a neural network-based backbone to learn high-level features using self-supervised learning, and then fine-tuning the backbone to achieve specific objectives. The primary advantage of such an approach is that the backbone can be fine-tuned to achieve a wide variety of target tasks using very less amount of training data as compared to traditional supervised learning methodologies. The empirical evaluation demonstrates the effectiveness of the proposed approach by obtaining more than 90\% classification accuracy by fine-tuning the backbone not only across different types of fault scenarios or operating conditions, but also across different machines. This illustrates the promising potential of the proposed approach for cross-machine fault diagnosis tasks in real-world applications.
</details>
<details>
<summary>摘要</summary>
大多数最近的电动机故障诊断技术的进步都基于假设训练和测试数据来自同一个分布。然而，在实际应用中的电动机运行场景中，数据分布可能会随着不同的操作条件而变化。这种假设限制了现有的研究实施，因为它们需要完全标注的训练数据，并且假设数据分布是一致的。这可能是因为获得许多标注的样本数据 для几台机器 across 多种故障情况和操作条件可能是不可能的。为了突破这些限制，该工作提出了一个框架，用于开发电动机故障诊断的基本模型。它包括使用神经网络基础模型来学习高级特征，然后使用自动适应学习来细化基础模型以实现特定目标。这种方法的优点是，只需要少量的训练数据，可以通过细化基础模型来实现各种目标任务。实际评估表明，提议的方法可以在不同的故障情况和操作条件下，以及不同的机器上，实现高于90%的分类精度。这说明了提议的方法在实际应用中的扩展性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Model-the-World-with-Language"><a href="#Learning-to-Model-the-World-with-Language" class="headerlink" title="Learning to Model the World with Language"></a>Learning to Model the World with Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01399">http://arxiv.org/abs/2308.01399</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/OpenKP">https://github.com/microsoft/OpenKP</a></li>
<li>paper_authors: Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan</li>
<li>for: 这个论文的目的是建立一种可以理解多种语言、关联语言与视觉世界并根据语言进行行动的 Agent。</li>
<li>methods: 这个论文使用了一种基于未来预测的自动学习目标，将语言理解和未来预测联系起来，以便帮助 Agent 更好地理解多种语言并根据语言进行行动。</li>
<li>results: 在这个论文中， authors 提出了一种名为 Dynalang 的 Agent，该 Agent 可以从多种语言中获得丰富的语言理解，并通过预测未来语言、视频和奖励来学习行动。 Dynalang 可以在多种环境中进行学习，包括在线交互环境以及无动作和奖励的 dataset 上。<details>
<summary>Abstract</summary>
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to predict future language, video, and rewards. In addition to learning from online interaction in an environment, Dynalang can be pretrained on datasets of text, video, or both without actions or rewards. From using language hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types of language to improve task performance, including environment descriptions, game rules, and instructions.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为了与人类交互在世界中，代理需要理解人类使用的多种语言，与视觉世界相关，并基于其行动。当前的代理只是通过任务奖励学习简单的语言指令，而我们的关键想法是让语言帮助代理预测未来：未来会观察什么，世界会如何行为，哪些情况将得到奖励。这种视角将语言理解与未来预测作为强大的自主学习目标统一起来。我们介绍了 Dynalang，一个学习多模态世界模型，预测未来文本和图像表示，并从抽象模型演练中学习行为。不同于传统的代理只用语言预测动作，Dynalang通过过去语言还预测未来语言、视频和奖励来获得丰富的语言理解。除了在环境中学习在线交互外，Dynalang还可以通过文本、视频或两者的无动作或奖励数据预training。从使用语言提示在格子世界中到探索高级扫描图像的家庭，Dynalang利用多种语言来提高任务表现，包括环境描述、游戏规则和指令。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Adaptable-Symbolic-Algorithms-from-Scratch"><a href="#Discovering-Adaptable-Symbolic-Algorithms-from-Scratch" class="headerlink" title="Discovering Adaptable Symbolic Algorithms from Scratch"></a>Discovering Adaptable Symbolic Algorithms from Scratch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16890">http://arxiv.org/abs/2307.16890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Kelly, Daniel S. Park, Xingyou Song, Mitchell McIntire, Pranav Nashikkar, Ritam Guha, Wolfgang Banzhaf, Kalyanmoy Deb, Vishnu Naresh Boddeti, Jie Tan, Esteban Real</li>
<li>for: 该论文旨在开发一种快速适应环境变化的自主机器人控制策略。</li>
<li>methods: 该方法基于AutoML-Zero，通过演化零批量学习的模型参数和推理算法来快速适应环境变化。</li>
<li>results: 实验表明，该方法可以在实际 quadruped robot上提供安全的控制策略，并在突然环境变化时表现出较高的稳定性和可靠性。<details>
<summary>Abstract</summary>
Autonomous robots deployed in the real world will need control policies that rapidly adapt to environmental changes. To this end, we propose AutoRobotics-Zero (ARZ), a method based on AutoML-Zero that discovers zero-shot adaptable policies from scratch. In contrast to neural network adaption policies, where only model parameters are optimized, ARZ can build control algorithms with the full expressive power of a linear register machine. We evolve modular policies that tune their model parameters and alter their inference algorithm on-the-fly to adapt to sudden environmental changes. We demonstrate our method on a realistic simulated quadruped robot, for which we evolve safe control policies that avoid falling when individual limbs suddenly break. This is a challenging task in which two popular neural network baselines fail. Finally, we conduct a detailed analysis of our method on a novel and challenging non-stationary control task dubbed Cataclysmic Cartpole. Results confirm our findings that ARZ is significantly more robust to sudden environmental changes and can build simple, interpretable control policies.
</details>
<details>
<summary>摘要</summary>
自适应 robots 在实际世界中部署时需要快速适应环境变化的控制策略。为此，我们提出了 AutoRobotics-Zero（ARZ）方法，基于 AutoML-Zero 方法，可以从头开始找到零shot适应策略。与神经网络适应策略相比，ARZ 可以建立一个完整的线性注册机器的控制算法，并可以在不间断的环境变化中进行适应。我们演化出模块化策略，可以在运行时调整模型参数和推理算法，以适应突然的环境变化。我们在模拟的四肢 робот上进行了实验，并成功演化出安全的控制策略，以避免当各个肢体突然失效时的倒下。这是一个具有挑战性的任务，两种流行的神经网络基elines都失败了。最后，我们对我们的方法进行了详细的分析，并在一个新的和挑战性的非站台控制任务中进行了实验。结果证明了我们的发现，ARZ 在突然环境变化中更加鲁棒，可以建立简单、可读的控制策略。
</details></li>
</ul>
<hr>
<h2 id="Virtual-Prompt-Injection-for-Instruction-Tuned-Large-Language-Models"><a href="#Virtual-Prompt-Injection-for-Instruction-Tuned-Large-Language-Models" class="headerlink" title="Virtual Prompt Injection for Instruction-Tuned Large Language Models"></a>Virtual Prompt Injection for Instruction-Tuned Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16888">http://arxiv.org/abs/2307.16888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin</li>
<li>for: 这个论文主要是为了漏洞披露 Large Language Models (LLMs) 的 instruction-tuning 数据，以实现恶意控制模型行为。</li>
<li>methods: 论文使用的方法是在模型的 instruction-tuning 数据中杀器投入恶意指令，以实现不正常的模型行为。</li>
<li>results: 论文的实验结果表明，只需杀器投入 0.1% 的恶意示例，就可以让模型对有关 Joe Biden 的查询返回负面的回答，从而导致模型被恶意控制。<details>
<summary>Abstract</summary>
We present Virtual Prompt Injection (VPI) for instruction-tuned Large Language Models (LLMs). VPI allows an attacker-specified virtual prompt to steer the model behavior under specific trigger scenario without any explicit injection in model input. For instance, if an LLM is compromised with the virtual prompt "Describe Joe Biden negatively." for Joe Biden-related instructions, then any service deploying this model will propagate biased views when handling user queries related to Joe Biden. VPI is especially harmful for two primary reasons. Firstly, the attacker can take fine-grained control over LLM behaviors by defining various virtual prompts, exploiting LLMs' proficiency in following instructions. Secondly, this control is achieved without any interaction from the attacker while the model is in service, leading to persistent attack. To demonstrate the threat, we propose a simple method for performing VPI by poisoning the model's instruction tuning data. We find that our proposed method is highly effective in steering the LLM with VPI. For example, by injecting only 52 poisoned examples (0.1% of the training data size) into the instruction tuning data, the percentage of negative responses given by the trained model on Joe Biden-related queries change from 0% to 40%. We thus highlight the necessity of ensuring the integrity of the instruction-tuning data as little poisoned data can cause stealthy and persistent harm to the deployed model. We further explore the possible defenses and identify data filtering as an effective way to defend against the poisoning attacks. Our project page is available at https://poison-llm.github.io.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个名为“虚拟提示插入”（VPI）的攻击技术，可以让攻击者在特定的触发情况下，透过虚拟提示来控制大语言模型（LLM）的行为，而不需要直接对模型的输入进行插入。例如，如果一个LLM被攻击者感染了虚拟提示“谈论乔·宾登的负面”，那么任何使用这个模型的服务都会传播偏见的观点，当用户提出有关乔·宾登的问题时。VPI有两个主要的危险因素：首先，攻击者可以通过定义不同的虚拟提示来精细地控制LLM的行为，滥用LLM的专业能力。第二，这种控制是在服务模型时，并不需要攻击者和模型之间的互动，因此是一种持续的攻击。为了证明这个威胁，我们提出了一个简单的方法来进行VPI，利用损害模型的调教数据。我们发现，仅将52个恶意示例（0.1%的训练数据大小）混入模型的调教数据中，可以将模型透过VPI控制。例如，将52个恶意示例混入调教数据中，将 Joe Biden 相关的查询中的负面回应率从0%提升至40%。这表明了调教数据的可信性的重要性，因为只需少量恶意数据可以导致隐藏和持续的伤害。我们还评估了可能的防护方法，发现数据范围化是一个有效的防护方法。我们的项目页面可以在https://poison-llm.github.io/中找到。
</details></li>
</ul>
<hr>
<h2 id="MetaCAM-Ensemble-Based-Class-Activation-Map"><a href="#MetaCAM-Ensemble-Based-Class-Activation-Map" class="headerlink" title="MetaCAM: Ensemble-Based Class Activation Map"></a>MetaCAM: Ensemble-Based Class Activation Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16863">http://arxiv.org/abs/2307.16863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emily Kaczmarek, Olivier X. Miguel, Alexa C. Bowie, Robin Ducharme, Alysha L. J. Dingwall-Harvey, Steven Hawken, Christine M. Armour, Mark C. Walker, Kevin Dick</li>
<li>For:  This paper aims to improve the performance of Class Activation Maps (CAMs) for explaining the predictions of deep learning models, specifically in high-criticality fields like medicine and biometric identification.* Methods: The paper proposes an ensemble-based method called MetaCAM, which combines multiple existing CAM methods using the consensus of the top-k% most highly activated pixels across component CAMs. The paper also introduces a new method called Cumulative Residual Effect (CRE) for summarizing large-scale ensemble-based experiments. Additionally, the paper proposes adaptive thresholding to improve the performance of individual CAMs.* Results: The paper shows that MetaCAM outperforms existing CAMs and refines the most salient regions of images used for model predictions. In a specific example, MetaCAM improved the performance of the pixel perturbation method Remove and Debias (ROAD) to 0.393 compared to the range of -0.101 to 0.172 for 11 individual CAMs.<details>
<summary>Abstract</summary>
The need for clear, trustworthy explanations of deep learning model predictions is essential for high-criticality fields, such as medicine and biometric identification. Class Activation Maps (CAMs) are an increasingly popular category of visual explanation methods for Convolutional Neural Networks (CNNs). However, the performance of individual CAMs depends largely on experimental parameters such as the selected image, target class, and model. Here, we propose MetaCAM, an ensemble-based method for combining multiple existing CAM methods based on the consensus of the top-k% most highly activated pixels across component CAMs. We perform experiments to quantifiably determine the optimal combination of 11 CAMs for a given MetaCAM experiment. A new method denoted Cumulative Residual Effect (CRE) is proposed to summarize large-scale ensemble-based experiments. We also present adaptive thresholding and demonstrate how it can be applied to individual CAMs to improve their performance, measured using pixel perturbation method Remove and Debias (ROAD). Lastly, we show that MetaCAM outperforms existing CAMs and refines the most salient regions of images used for model predictions. In a specific example, MetaCAM improved ROAD performance to 0.393 compared to 11 individual CAMs with ranges from -0.101-0.172, demonstrating the importance of combining CAMs through an ensembling method and adaptive thresholding.
</details>
<details>
<summary>摘要</summary>
需要清晰、可信度高的深度学习模型预测解释是高 kriticality 领域，如医学和生物ometrics 中的必备。类Activation Map (CAM) 是 CNN 中的一种增加 popular 的视觉解释方法。然而，具体实验参数，如选择的图像、目标类和模型，对各个 CAM 的性能产生很大影响。在这里，我们提出了 MetaCAM，一种基于 consensus 的多个现有 CAM 方法的 ensemble 方法，通过选择顶层 k% 最高活跃像素来确定最佳结果。我们对 MetaCAM 实验进行了量化的探索，并提出了一种新的 Cumulative Residual Effect (CRE) 方法来总结大规模的 ensemble 实验。此外，我们还介绍了适应阈值，并证明可以应用到各个 CAM 中来提高其性能， measured 使用像素变化方法 Remove 和 Debias (ROAD)。最后，我们证明 MetaCAM 超过了现有 CAM 和适应阈值，并提高了模型预测中使用的图像的最佳区域。例如，在 MetaCAM 实验中，ROAD 性能提高到 0.393，比11个个 CAM 的范围从 -0.101-0.172 高，这表明将 CAM 合并到 ensemble 方法和适应阈值中可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Trustworthy-and-Aligned-Machine-Learning-A-Data-centric-Survey-with-Causality-Perspectives"><a href="#Towards-Trustworthy-and-Aligned-Machine-Learning-A-Data-centric-Survey-with-Causality-Perspectives" class="headerlink" title="Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives"></a>Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16851">http://arxiv.org/abs/2307.16851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Liu, Maheep Chaudhary, Haohan Wang<br>for:This paper is written to provide a comprehensive review of the recent advancements in trustworthy machine learning, focusing on the challenges posed by the data and the methods developed to address these challenges.methods:The paper uses a data-centric perspective to review the various techniques developed in the field of trustworthy machine learning, including those inspired by the causality literature. These techniques are connected using a unified set of concepts and mathematical vocabulary.results:The paper provides a unified understanding of the field of trustworthy machine learning, connecting the various techniques developed in different subfields. It also explores the trustworthiness of large pretrained models and draws connections between these models and the standard empirical risk minimization (ERM) method. The paper offers a brief summary of the applications of these methods and discusses potential future aspects related to the survey.<details>
<summary>Abstract</summary>
The trustworthiness of machine learning has emerged as a critical topic in the field, encompassing various applications and research areas such as robustness, security, interpretability, and fairness. The last decade saw the development of numerous methods addressing these challenges. In this survey, we systematically review these advancements from a data-centric perspective, highlighting the shortcomings of traditional empirical risk minimization (ERM) training in handling challenges posed by the data.   Interestingly, we observe a convergence of these methods, despite being developed independently across trustworthy machine learning subfields. Pearl's hierarchy of causality offers a unifying framework for these techniques. Accordingly, this survey presents the background of trustworthy machine learning development using a unified set of concepts, connects this language to Pearl's causal hierarchy, and finally discusses methods explicitly inspired by causality literature. We provide a unified language with mathematical vocabulary to link these methods across robustness, adversarial robustness, interpretability, and fairness, fostering a more cohesive understanding of the field.   Further, we explore the trustworthiness of large pretrained models. After summarizing dominant techniques like fine-tuning, parameter-efficient fine-tuning, prompting, and reinforcement learning with human feedback, we draw connections between them and the standard ERM. This connection allows us to build upon the principled understanding of trustworthy methods, extending it to these new techniques in large pretrained models, paving the way for future methods. Existing methods under this perspective are also reviewed.   Lastly, we offer a brief summary of the applications of these methods and discuss potential future aspects related to our survey. For more information, please visit http://trustai.one.
</details>
<details>
<summary>摘要</summary>
机器学习的可靠性问题在领域中得到了广泛关注，涵盖了多个应用和研究领域，如可靠性、安全性、可解释性和公平性。过去十年内，有多种方法被开发出来解决这些挑战。在这篇评论中，我们从数据中心的视角系统性地查询这些进步，指出传统的empirical risk minimization（ERM）训练在处理数据的挑战方面存在缺陷。有意思的是，我们发现这些方法尽管在不同的可靠机器学习子领域中独立进行开发，但它们却 converges。珍珠的 causality 隐含了这些技术。因此，本文使用珍珠的 causal 隐含提供了一种统一的语言，将这些方法连接到 Pearl 的 causal 层次结构中，并最终讨论这些方法在可靠机器学习领域的应用。我们提供一种统一的语言，其中包括数学术语，以连接这些方法，从 robustness、对抗性、可解释性和公平性等方面进行链接。这些链接使我们可以更好地理解这个领域。此外，我们还探讨了大型预训模型的可靠性。我们首先概括了现有的方法，如 fine-tuning、参数效率的 fine-tuning、提示和人工回归学习。然后，我们连接这些方法和标准 ERM，从而扩展了可靠方法的基础理解，应用到这些新的大型预训模型中。现有的方法也被评论。最后，我们 briefly 概述了这些方法的应用，并讨论了未来相关的方向。更多信息请参考 <http://trustai.one>。
</details></li>
</ul>
<hr>
<h2 id="A-Trajectory-K-Anonymity-Model-Based-on-Point-Density-and-Partition"><a href="#A-Trajectory-K-Anonymity-Model-Based-on-Point-Density-and-Partition" class="headerlink" title="A Trajectory K-Anonymity Model Based on Point Density and Partition"></a>A Trajectory K-Anonymity Model Based on Point Density and Partition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16849">http://arxiv.org/abs/2307.16849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanshu Yu, Haonan Shi, Hongyun Xu</li>
<li>for: 保护用户行迹数据隐私</li>
<li>methods: 基于点密度和分区（KPDP）模型，对行迹数据进行匿名化处理，防止重新标识攻击</li>
<li>results: 提出了一种基于Point Density和Partition（KPDP）模型的行迹匿名化方法，可以有效防止重新标识攻击，同时保持数据的实用性和执行速度。实验结果表明，提出的方法在实际数据集上具有显著的优势。<details>
<summary>Abstract</summary>
As people's daily life becomes increasingly inseparable from various mobile electronic devices, relevant service application platforms and network operators can collect numerous individual information easily. When releasing these data for scientific research or commercial purposes, users' privacy will be in danger, especially in the publication of spatiotemporal trajectory datasets. Therefore, to avoid the leakage of users' privacy, it is necessary to anonymize the data before they are released. However, more than simply removing the unique identifiers of individuals is needed to protect the trajectory privacy, because some attackers may infer the identity of users by the connection with other databases. Much work has been devoted to merging multiple trajectories to avoid re-identification, but these solutions always require sacrificing data quality to achieve the anonymity requirement. In order to provide sufficient privacy protection for users' trajectory datasets, this paper develops a study on trajectory privacy against re-identification attacks, proposing a trajectory K-anonymity model based on Point Density and Partition (KPDP). Our approach improves the existing trajectory generalization anonymization techniques regarding trajectory set partition preprocessing and trajectory clustering algorithms. It successfully resists re-identification attacks and reduces the data utility loss of the k-anonymized dataset. A series of experiments on a real-world dataset show that the proposed model has significant advantages in terms of higher data utility and shorter algorithm execution time than other existing techniques.
</details>
<details>
<summary>摘要</summary>
As people's daily life becomes increasingly inseparable from various mobile electronic devices, relevant service application platforms and network operators can easily collect numerous individual information. When releasing these data for scientific research or commercial purposes, users' privacy will be in danger, especially in the publication of spatiotemporal trajectory datasets. Therefore, to avoid the leakage of users' privacy, it is necessary to anonymize the data before they are released. However, simply removing the unique identifiers of individuals is not enough to protect the trajectory privacy, because some attackers may infer the identity of users by the connection with other databases. Much work has been devoted to merging multiple trajectories to avoid re-identification, but these solutions always require sacrificing data quality to achieve the anonymity requirement. In order to provide sufficient privacy protection for users' trajectory datasets, this paper develops a study on trajectory privacy against re-identification attacks, proposing a trajectory K-anonymity model based on Point Density and Partition (KPDP). Our approach improves the existing trajectory generalization anonymization techniques regarding trajectory set partition preprocessing and trajectory clustering algorithms. It successfully resists re-identification attacks and reduces the data utility loss of the k-anonymized dataset. A series of experiments on a real-world dataset show that the proposed model has significant advantages in terms of higher data utility and shorter algorithm execution time than other existing techniques.
</details></li>
</ul>
<hr>
<h2 id="Latent-Masking-for-Multimodal-Self-supervised-Learning-in-Health-Timeseries"><a href="#Latent-Masking-for-Multimodal-Self-supervised-Learning-in-Health-Timeseries" class="headerlink" title="Latent Masking for Multimodal Self-supervised Learning in Health Timeseries"></a>Latent Masking for Multimodal Self-supervised Learning in Health Timeseries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16847">http://arxiv.org/abs/2307.16847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shohreh Deldari, Dimitris Spathis, Mohammad Malekzadeh, Fahim Kawsar, Flora Salim, Akhil Mathur</li>
<li>for: 本研究旨在提高生物医学时间序列数据的机器学习进步，因为现有的生物医学时间序列数据标注是有限的，这限制了机器学习模型的进步。</li>
<li>methods: 本研究提出了一种新的自主学习方法（CroSSL），该方法使用掩码中间表示来实现跨模态的学习，并使用跨模态聚合器将多个模态的表示融合成一个全局表示。</li>
<li>results: 对多种医疗时间序列数据进行测试， CroSSL 方法能够达到先前的自主学习技术和指导 benchmark 的性能水平，只需要 minimal 的标注数据。此外，我们还分析了不同掩码比例和策略的影响，以及模式的Robustness 性。<details>
<summary>Abstract</summary>
Limited availability of labeled data for machine learning on biomedical time-series hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learning data representations without labels. However, current SSL methods require expensive computations for negative pairs and are designed for single modalities, limiting their versatility. To overcome these limitations, we introduce CroSSL (Cross-modal SSL). CroSSL introduces two novel concepts: masking intermediate embeddings from modality-specific encoders and aggregating them into a global embedding using a cross-modal aggregator. This enables the handling of missing modalities and end-to-end learning of cross-modal patterns without prior data preprocessing or time-consuming negative-pair sampling. We evaluate CroSSL on various multimodal time-series benchmarks, including both medical-grade and consumer biosignals. Our results demonstrate superior performance compared to previous SSL techniques and supervised benchmarks with minimal labeled data. We additionally analyze the impact of different masking ratios and strategies and assess the robustness of the learned representations to missing modalities. Overall, our work achieves state-of-the-art performance while highlighting the benefits of masking latent embeddings for cross-modal learning in temporal health data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为生物医学时序数据的标注数据有限，机器学习领域的进步受到了限制。自动学习（SSL）是一种不需要标注数据的数据表示学习的有希望的方法。然而，现有的SSL方法需要费时的计算对应对和是单模态的，这限制了它们的多样化性。为了突破这些限制，我们介绍了CroSSL（跨Modal SSL）。CroSSL引入了两个新的概念：在特定编码器中隐藏中间嵌入，并将它们聚合成全局嵌入使用交叉模态聚合器。这使得可以处理缺失的模态，并实现端到端学习跨模态模式，不需要先行数据处理或费时的负对样本生成。我们对多种医疗和消费者生物信号 benchmark 进行了评估，结果表明 CroSSL 的性能比前一代 SSL 技术和监督标准更高，几乎没有需要大量标注数据。我们还分析了不同的遮盖比例和策略的影响，以及缺失模态对学习到的表示的稳定性。总的来说，我们的工作实现了状态机器学习领域的前景性，同时强调隐藏嵌入的权重对跨模态学习的益处。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Driving-Heterogeneity-using-Action-chains"><a href="#Identification-of-Driving-Heterogeneity-using-Action-chains" class="headerlink" title="Identification of Driving Heterogeneity using Action-chains"></a>Identification of Driving Heterogeneity using Action-chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16843">http://arxiv.org/abs/2307.16843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xue Yao, Simeon C. Calvert, Serge P. Hoogendoorn</li>
<li>for: 这种研究的目的是为了从动作链视角进行驾驶不同性的识别，以提高交通效率和安全性。</li>
<li>methods: 该研究使用了一种基于规则的分割技术，以及一个动作库，其中包含了各种驾驶行为模式的描述。此外，研究还使用了动作链概念和动作链过渡概率来评估驾驶不同性。</li>
<li>results: 研究使用了实际数据进行评估，并得到了有效地识别驾驶不同性的结果，包括个人 drivers和交通流体系之间的驾驶不同性。这些发现可以帮助开发更加准确的驾驶行为理论和交通流模型，从而提高交通效率和安全性。<details>
<summary>Abstract</summary>
Current approaches to identifying driving heterogeneity face challenges in capturing the diversity of driving characteristics and understanding the fundamental patterns from a driving behaviour mechanism standpoint. This study introduces a comprehensive framework for identifying driving heterogeneity from an Action-chain perspective. First, a rule-based segmentation technique that considers the physical meanings of driving behaviour is proposed. Next, an Action phase Library including descriptions of various driving behaviour patterns is created based on the segmentation findings. The Action-chain concept is then introduced by implementing Action phase transition probability, followed by a method for evaluating driving heterogeneity. Employing real-world datasets for evaluation, our approach effectively identifies driving heterogeneity for both individual drivers and traffic flow while providing clear interpretations. These insights can aid the development of accurate driving behaviour theory and traffic flow models, ultimately benefiting traffic performance, and potentially leading to aspects such as improved road capacity and safety.
</details>
<details>
<summary>摘要</summary>
当前驾驶异同检测方法面临 capture driving behavior 多样性和理解驾驶行为机制的基本模式的挑战。本研究提出了基于 Action-chain 视角的全面驾驶异同检测框架。首先，我们提出了基于驾驶行为物理含义的规则生成分割技术。然后，我们基于分割结果创建了驾驶行为模式库，并在该库中添加了各种驾驶行为模式的描述。接着，我们引入了 Action phase 过渡概率，并提出了评估驾驶异同性的方法。使用实际数据进行评估，我们的方法可以有效地检测个体驾驶异同性和交通流动中的异同性，并提供了明确的解释。这些发现可以帮助建立准确的驾驶行为理论和交通流动模型，从而提高交通性能和安全性。
</details></li>
</ul>
<hr>
<h2 id="Automated-COVID-19-CT-Image-Classification-using-Multi-head-Channel-Attention-in-Deep-CNN"><a href="#Automated-COVID-19-CT-Image-Classification-using-Multi-head-Channel-Attention-in-Deep-CNN" class="headerlink" title="Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN"></a>Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00715">http://arxiv.org/abs/2308.00715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Susmita Ghosh, Abhiroop Chatterjee</li>
<li>for: 检测COVID-19 CT扫描图像的自动化分类</li>
<li>methods: 提出了一种基于深度学习的修改Xception模型，具有新设计的通道注意力机制和权重 globally average pooling，以提高特征提取并改善分类精度</li>
<li>results: 在一个广泛使用的COVID-19 CT扫描图像数据集上实现了非常好的准确率（96.99%），并证明了其比其他状态的技术更高效。<details>
<summary>Abstract</summary>
The rapid spread of COVID-19 has necessitated efficient and accurate diagnostic methods. Computed Tomography (CT) scan images have emerged as a valuable tool for detecting the disease. In this article, we present a novel deep learning approach for automated COVID-19 CT scan classification where a modified Xception model is proposed which incorporates a newly designed channel attention mechanism and weighted global average pooling to enhance feature extraction thereby improving classification accuracy. The channel attention module selectively focuses on informative regions within each channel, enabling the model to learn discriminative features for COVID-19 detection. Experiments on a widely used COVID-19 CT scan dataset demonstrate a very good accuracy of 96.99% and show its superiority to other state-of-the-art techniques. This research can contribute to the ongoing efforts in using artificial intelligence to combat current and future pandemics and can offer promising and timely solutions for efficient medical image analysis tasks.
</details>
<details>
<summary>摘要</summary>
快速蔓延的 COVID-19 病毒感染了全球，因此需要有效和准确的诊断方法。 computed Tomography（CT）扫描图像已成为推荐诊断 COVID-19 的有价值工具。在这篇文章中，我们提出了一种新的深度学习方法，用于自动分类 COVID-19 CT 扫描图像。我们修改了 Xception 模型，添加了一个新的通道注意力机制和权重global average pooling，以提高特征提取，从而提高分类精度。通道注意力模块可以选择每个通道中的有用区域，让模型学习 COVID-19 的特征。实验结果表明，我们的方法在一个常用的 COVID-19 CT 扫描图像集上达到了 96.99% 的准确率，超过了其他当前状态艺术技术。这些研究可以贡献到使用人工智能对抗当前和未来的流行病，并提供可靠的医疗图像分析任务的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Recent-advancement-in-Disease-Diagnostic-using-machine-learning-Systematic-survey-of-decades-comparisons-and-challenges"><a href="#Recent-advancement-in-Disease-Diagnostic-using-machine-learning-Systematic-survey-of-decades-comparisons-and-challenges" class="headerlink" title="Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges"></a>Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01319">http://arxiv.org/abs/2308.01319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzaneh Tajidini, Mohammad-Javad Kheiri</li>
<li>for: 本研究旨在探讨机器学习在计算机助手诊断中的应用。</li>
<li>methods: 本文使用机器学习算法来检测疾病，包括肝炎、糖尿病、肝病、登革热和心脏病。</li>
<li>results: 本文总结了各种机器学习技术和算法在疾病检测中的应用，并评估了这些方法的效果。<details>
<summary>Abstract</summary>
Computer-aided diagnosis (CAD), a vibrant medical imaging research field, is expanding quickly. Because errors in medical diagnostic systems might lead to seriously misleading medical treatments, major efforts have been made in recent years to improve computer-aided diagnostics applications. The use of machine learning in computer-aided diagnosis is crucial. A simple equation may result in a false indication of items like organs. Therefore, learning from examples is a vital component of pattern recognition. Pattern recognition and machine learning in the biomedical area promise to increase the precision of disease detection and diagnosis. They also support the decision-making process's objectivity. Machine learning provides a practical method for creating elegant and autonomous algorithms to analyze high-dimensional and multimodal bio-medical data. This review article examines machine-learning algorithms for detecting diseases, including hepatitis, diabetes, liver disease, dengue fever, and heart disease. It draws attention to the collection of machine learning techniques and algorithms employed in studying conditions and the ensuing decision-making process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Getting-from-Generative-AI-to-Trustworthy-AI-What-LLMs-might-learn-from-Cyc"><a href="#Getting-from-Generative-AI-to-Trustworthy-AI-What-LLMs-might-learn-from-Cyc" class="headerlink" title="Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc"></a>Getting from Generative AI to Trustworthy AI: What LLMs might learn from Cyc</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04445">http://arxiv.org/abs/2308.04445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doug Lenat, Gary Marcus</li>
<li>for: 这篇论文目的是探讨未来AI的发展和趋势，以及一种可靠的AI技术。</li>
<li>methods: 论文使用了大语模型（LLM）和受检验的知识和规则来推导出结论。</li>
<li>results: 论文提出了16个AI发展的理想目标，并讨论了一种替代方案，即通过受检验的知识和规则来教育AI，以实现可靠和可解释的AI技术。<details>
<summary>Abstract</summary>
Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable.   We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however a catch: if the logical language is expressive enough to fully represent the meaning of anything we can say in English, then the inference engine runs much too slowly. That's why symbolic AI systems typically settle for some fast but much less expressive logic, such as knowledge graphs. We describe how one AI system, Cyc, has developed ways to overcome that tradeoff and is able to reason in higher order logic in real time.   We suggest that any trustworthy general AI will need to hybridize the approaches, the LLM approach and more formal approach, and lay out a path to realizing that dream.
</details>
<details>
<summary>摘要</summary>
现代人工智能的主流方法是生成型AI，它通过大量的语言模型（LLM）训练来生成可能性强的输出，但并不一定是正确的。尽管它们的能力往往吸引人，但它们缺乏一定的推理能力，因此不够可靠。此外，它们的结果往往难以预测和解释。我们提出了16个愿景为未来AI，并讨论了一种可能解决当前方法的限制的替代方法：通过手动编辑的明确知识和规则来教育AI，使其推理引擎可以自动推导所有知识的逻辑推论。这种方法可以生成可靠并可解释的长Arguments，因为整个步骤的推理逻辑都可以被跟踪，并且每一步的知识来源可以被记录和审核。然而，存在一个问题：如果逻辑语言足够表达英语中的任何意义，那么推理引擎就会非常慢。因此，符号AI系统通常选择一些快速而但是较为有限的逻辑，如知识图。我们描述了一个AI系统——Cyc，如何超越这个负担，在实时内推理在高阶逻辑中。我们建议任何可靠的通用AI都需要混合这两种方法，并走出实现这个梦想的路径。
</details></li>
</ul>
<hr>
<h2 id="Changes-in-Policy-Preferences-in-German-Tweets-during-the-COVID-Pandemic"><a href="#Changes-in-Policy-Preferences-in-German-Tweets-during-the-COVID-Pandemic" class="headerlink" title="Changes in Policy Preferences in German Tweets during the COVID Pandemic"></a>Changes in Policy Preferences in German Tweets during the COVID Pandemic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04444">http://arxiv.org/abs/2308.04444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Biessmann</li>
<li>for: The paper aims to quantify political preferences in online social media, specifically in response to COVID measures.</li>
<li>methods: The paper uses a novel data set of tweets with fine grained political preference annotations to train a text classification model.</li>
<li>results: The study finds that expression of political opinions increased in response to the COVID pandemic, with the majority of opinions falling into the categories of pro-welfare, pro-education, and pro-governmental administration efficiency.Here are the three points in Simplified Chinese text:</li>
<li>for: 这个论文目标是在社交媒体上量化政治意见，具体来说是在响应COVID措施的情况下。</li>
<li>methods: 这个论文使用一个新的 tweets 数据集，并对其进行精细的政治偏好标注。</li>
<li>results: 研究发现，COVID 大流行后，政治意见的表达增加了，主要集中在优先顺序、教育和政府管理效率等三个类别上。<details>
<summary>Abstract</summary>
Online social media have become an important forum for exchanging political opinions. In response to COVID measures citizens expressed their policy preferences directly on these platforms. Quantifying political preferences in online social media remains challenging: The vast amount of content requires scalable automated extraction of political preferences -- however fine grained political preference extraction is difficult with current machine learning (ML) technology, due to the lack of data sets. Here we present a novel data set of tweets with fine grained political preference annotations. A text classification model trained on this data is used to extract policy preferences in a German Twitter corpus ranging from 2019 to 2022. Our results indicate that in response to the COVID pandemic, expression of political opinions increased. Using a well established taxonomy of policy preferences we analyse fine grained political views and highlight changes in distinct political categories. These analyses suggest that the increase in policy preference expression is dominated by the categories pro-welfare, pro-education and pro-governmental administration efficiency. All training data and code used in this study are made publicly available to encourage other researchers to further improve automated policy preference extraction methods. We hope that our findings contribute to a better understanding of political statements in online social media and to a better assessment of how COVID measures impact political preferences.
</details>
<details>
<summary>摘要</summary>
在线社交媒体已经成为政治意见交流的重要 forum。对于 COVID 措施，公民直接在这些平台上表达了政策偏好。但量化在线社交媒体中的政治偏好仍然是挑战：大量内容需要批量自动提取政策偏好，但现有机器学习（ML）技术仍然无法实现精确的政策偏好提取。我们现在发表了一个罕见的推文标注政治偏好的数据集。我们使用这个数据集训练了一个文本分类模型，并在2019-2022年德国Twitter资料中提取了政策偏好。我们的结果显示，对 COVID 大流行的回应，表达政治意见的人数增加。使用一个已经稳定的政策偏好分类法，我们分析了细部政治观点，并发现具体政治类别中的改变。这些分析结果显示，增加政策偏好表达的主要类别是“护理”、“教育”和“政府管理效率”。我们公开提供了所有训练数据和代码，以便其他研究人员可以进一步改进自动政策偏好提取方法。我们希望我们的成果对政治声明在线社交媒体中的理解产生影响，并且对 COVID 措施对政策偏好的影响进行更好的评估。
</details></li>
</ul>
<hr>
<h2 id="Structural-Transfer-Learning-in-NL-to-Bash-Semantic-Parsers"><a href="#Structural-Transfer-Learning-in-NL-to-Bash-Semantic-Parsers" class="headerlink" title="Structural Transfer Learning in NL-to-Bash Semantic Parsers"></a>Structural Transfer Learning in NL-to-Bash Semantic Parsers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16795">http://arxiv.org/abs/2307.16795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyle Duffy, Satwik Bhattamishra, Phil Blunsom</li>
<li>for: 这篇论文主要针对的是大规模预训练在自然语言处理领域的进步，以及预训练数据集的设计方面的 pocoUnderstanding。</li>
<li>methods: 这篇论文提出了一种方法来获得自然语言到bashSemantic parsing任务（NLBash）的结构匹配度的量化理解。该方法在NLBash和自然语言到SQL之间的结构匹配度之间进行了研究，并发现了强大的结构匹配度。</li>
<li>results: 研究发现，预训练compute expended在英语到德语翻译任务中的变化不一定会导致NLBash中的semantic representations具有更强的传递性。<details>
<summary>Abstract</summary>
Large-scale pre-training has made progress in many fields of natural language processing, though little is understood about the design of pre-training datasets. We propose a methodology for obtaining a quantitative understanding of structural overlap between machine translation tasks. We apply our methodology to the natural language to Bash semantic parsing task (NLBash) and show that it is largely reducible to lexical alignment. We also find that there is strong structural overlap between NLBash and natural language to SQL. Additionally, we perform a study varying compute expended during pre-training on the English to German machine translation task and find that more compute expended during pre-training does not always correspond semantic representations with stronger transfer to NLBash.
</details>
<details>
<summary>摘要</summary>
大规模预训练在自然语言处理多个领域取得了进步，然而预训练数据集的设计仍然不够了解。我们提出了一种方法来获得自然语言到BashSemantic parsing任务（NLBash）的结构性重叠的量化理解。我们应用我们的方法于NLBash任务，发现它主要归结于词语对应。我们还发现了自然语言到SQL任务和NLBash任务之间强大的结构重叠。此外，我们在英语到德语机器翻译任务上进行了不同计算资源的预训练时间对NLBash任务的传递效果的研究，发现更多的计算资源不总是导致更强的传递效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/01/cs.LG_2023_08_01/" data-id="clluro5js005bq988by2aenlw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/02/eess.IV_2023_08_02/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-02 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/01/cs.SD_2023_08_01/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-01 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
