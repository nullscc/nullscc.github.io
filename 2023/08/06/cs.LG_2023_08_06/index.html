
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-06 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="AI-GOMS: Large AI-Driven Global Ocean Modeling System paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03152 repo_url: None paper_authors: Wei Xiong, Yanfei Xiang, Hao Wu, Shuyi Zhou, Yuze Sun, Muyuan Ma, Xiaomen">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-06 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/06/cs.LG_2023_08_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="AI-GOMS: Large AI-Driven Global Ocean Modeling System paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03152 repo_url: None paper_authors: Wei Xiong, Yanfei Xiang, Hao Wu, Shuyi Zhou, Yuze Sun, Muyuan Ma, Xiaomen">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-05T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:26.111Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/cs.LG_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-06 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AI-GOMS-Large-AI-Driven-Global-Ocean-Modeling-System"><a href="#AI-GOMS-Large-AI-Driven-Global-Ocean-Modeling-System" class="headerlink" title="AI-GOMS: Large AI-Driven Global Ocean Modeling System"></a>AI-GOMS: Large AI-Driven Global Ocean Modeling System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03152">http://arxiv.org/abs/2308.03152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Xiong, Yanfei Xiang, Hao Wu, Shuyi Zhou, Yuze Sun, Muyuan Ma, Xiaomeng Huang</li>
<li>for: 这个论文旨在提出一种基于人工智能的全球海洋模型系统（AI-GOMS），用于准确和高效地预测全球海洋日常变化。</li>
<li>methods: 该模型系统包括基本海洋变量预测的贝叶克自适应网络结构，以及包括地方下降、波解码和生物化交互的轻量级精度模型。</li>
<li>results: 该模型在30天预测全球海洋基本变量（15层深度）的方面达到了最佳性能，并能够模拟kuroshio海域的 mezoscale旋涡和赤道太平洋海洋层分化。<details>
<summary>Abstract</summary>
Ocean modeling is a powerful tool for simulating the physical, chemical, and biological processes of the ocean, which is the foundation for marine science research and operational oceanography. Modern numerical ocean modeling mainly consists of governing equations and numerical algorithms. Nonlinear instability, computational expense, low reusability efficiency and high coupling costs have gradually become the main bottlenecks for the further development of numerical ocean modeling. Recently, artificial intelligence-based modeling in scientific computing has shown revolutionary potential for digital twins and scientific simulations, but the bottlenecks of numerical ocean modeling have not been further solved. Here, we present AI-GOMS, a large AI-driven global ocean modeling system, for accurate and efficient global ocean daily prediction. AI-GOMS consists of a backbone model with the Fourier-based Masked Autoencoder structure for basic ocean variable prediction and lightweight fine-tuning models incorporating regional downscaling, wave decoding, and biochemistry coupling modules. AI-GOMS has achieved the best performance in 30 days of prediction for the global ocean basic variables with 15 depth layers at 1/4{\deg} spatial resolution. Beyond the good performance in statistical metrics, AI-GOMS realizes the simulation of mesoscale eddies in the Kuroshio region at 1/12{\deg} spatial resolution and ocean stratification in the tropical Pacific Ocean. AI-GOMS provides a new backbone-downstream paradigm for Earth system modeling, which makes the system transferable, scalable and reusable.
</details>
<details>
<summary>摘要</summary>
海洋模型是一种强大的工具，用于模拟海洋物理、化学和生物过程，是marine science研究和操作 oceanography的基础。现代数值海洋模型主要由管理方程和数值算法组成。不线性不稳定、计算成本高、再利用率低和对接成本高逐渐成为数值海洋模型的主要瓶颈。在科学计算中，人工智能基于的模型已经展示了革命性的潜力，但数值海洋模型中的瓶颈问题还没有得到解决。在这里，我们介绍AI-GOMS，一个大型基于人工智能的全球海洋模型，用于准确和高效的全球海洋日常预测。AI-GOMS包括一个基本 ocean variable prediction的背bone模型，以及 incorporating regional downscaling、波动解码和生物化学结合模块的轻量级精度增强模型。AI-GOMS在30天预测全球海洋基本变量的15层深度分辨率下达到了最佳性能。除了在统计指标方面的好表现，AI-GOMS还实现了kuroshio区域的 mesoscale eddies 在1/12°的空间分辨率下的模拟，以及在 тропиical Pacific Ocean中的海洋层次分布。AI-GOMS提供了一个新的背部-下游模式，使系统可重用、可扩展和可重复使用。
</details></li>
</ul>
<hr>
<h2 id="Nest-DGIL-Nesterov-optimized-Deep-Geometric-Incremental-Learning-for-CS-Image-Reconstruction"><a href="#Nest-DGIL-Nesterov-optimized-Deep-Geometric-Incremental-Learning-for-CS-Image-Reconstruction" class="headerlink" title="Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction"></a>Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03807">http://arxiv.org/abs/2308.03807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanxiaohong/Nest-DGIL">https://github.com/fanxiaohong/Nest-DGIL</a></li>
<li>paper_authors: Xiaohong Fan, Yin Yang, Ke Chen, Yujie Feng, Jianping Zhang</li>
<li>for: 这种方法用于解决图像逆问题，包括高&#x2F;低频图像特征的学习能力和保证几何纹理细节的重建。</li>
<li>methods: 基于第二个奈斯特洛夫距离梯度优化的深度幂增量学习框架，包括普通的线性重建、几何增量学习、奈斯特洛夫加速和后处理。</li>
<li>results: 提出的方法可以快速收敛，并且可以避免中间重建结果落入不同几何分解域之外，同时也可以保证高&#x2F;低频图像特征的学习能力和几何纹理细节的重建。<details>
<summary>Abstract</summary>
Proximal gradient-based optimization is one of the most common strategies for solving image inverse problems as well as easy to implement. However, these techniques often generate heavy artifacts in image reconstruction. One of the most popular refinement methods is to fine-tune the regularization parameter to alleviate such artifacts, but it may not always be sufficient or applicable due to increased computational costs. In this work, we propose a deep geometric incremental learning framework based on second Nesterov proximal gradient optimization. The proposed end-to-end network not only has the powerful learning ability for high/low frequency image features,but also can theoretically guarantee that geometric texture details will be reconstructed from preliminary linear reconstruction.Furthermore, it can avoid the risk of intermediate reconstruction results falling outside the geometric decomposition domains and achieve fast convergence. Our reconstruction framework is decomposed into four modules including general linear reconstruction, cascade geometric incremental restoration, Nesterov acceleration and post-processing. In the image restoration step,a cascade geometric incremental learning module is designed to compensate for the missing texture information from different geometric spectral decomposition domains. Inspired by overlap-tile strategy, we also develop a post-processing module to remove the block-effect in patch-wise-based natural image reconstruction. All parameters in the proposed model are learnable,an adaptive initialization technique of physical-parameters is also employed to make model flexibility and ensure converging smoothly. We compare the reconstruction performance of the proposed method with existing state-of-the-art methods to demonstrate its superiority. Our source codes are available at https://github.com/fanxiaohong/Nest-DGIL.
</details>
<details>
<summary>摘要</summary>
近似梯度基于优化是解决图像反问题的一种最常见策略，易于实现，但它们经常产生重要的artefacts。一种常见的改进方法是调整正则化参数，以降低这些artefacts，但这并不总是可行或适用，因为它可能会增加计算成本。在这种工作中，我们提出了深度几何增量学习框架，基于第二个Nesterov proximal梯度优化。我们的提案的端到端网络不仅具有高/低频图像特征的强大学习能力，而且可以理论保证从初步线性重建中恢复几何纹理细节。此外，它可以避免初步重建结果落入不同几何分解域之外，并且可以快速收敛。我们的重建框架分为四个模块：一般线性重建、几何增量学习、Nesterov加速和后处理。在图像恢复阶段，我们设计了几何增量学习模块，以补做不同几何分解域中缺失的纹理信息。受到 overlap-tile 策略的启发，我们还开发了后处理模块，以去除 patch-wise 基于自然图像重建中的块效果。所有模型参数都是可学习的，并且我们采用了 adaptive 初始化技术，以确保模型的灵活性和平滑的收敛。我们与现有的状态 искусственного智能方法进行比较，以证明我们的方法的优越性。我们的源代码可以在 GitHub 上找到：https://github.com/fanxiaohong/Nest-DGIL。
</details></li>
</ul>
<hr>
<h2 id="Self-Directed-Linear-Classification"><a href="#Self-Directed-Linear-Classification" class="headerlink" title="Self-Directed Linear Classification"></a>Self-Directed Linear Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03142">http://arxiv.org/abs/2308.03142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis</li>
<li>for: 这个论文研究了在线分类中学习者在适应性下预测示例的顺序，以最小化总错误数。</li>
<li>methods: 论文使用了自适应选择示例顺序的方法，并研究了这种方法的力量。</li>
<li>results: 论文表明，在线分类中，适应性下预测示例的顺序可以实现较好的性能，比如最小化错误数。此外，论文还提供了两个主要结果：在random sampling的情况下，可以使用自适应选择示例顺序来预测整个数据集的 labels，而且这种方法的错误数与数据集的大小无关。<details>
<summary>Abstract</summary>
In online classification, a learner is presented with a sequence of examples and aims to predict their labels in an online fashion so as to minimize the total number of mistakes. In the self-directed variant, the learner knows in advance the pool of examples and can adaptively choose the order in which predictions are made. Here we study the power of choosing the prediction order and establish the first strong separation between worst-order and random-order learning for the fundamental task of linear classification. Prior to our work, such a separation was known only for very restricted concept classes, e.g., one-dimensional thresholds or axis-aligned rectangles.   We present two main results. If $X$ is a dataset of $n$ points drawn uniformly at random from the $d$-dimensional unit sphere, we design an efficient self-directed learner that makes $O(d \log \log(n))$ mistakes and classifies the entire dataset. If $X$ is an arbitrary $d$-dimensional dataset of size $n$, we design an efficient self-directed learner that predicts the labels of $99\%$ of the points in $X$ with mistake bound independent of $n$. In contrast, under a worst- or random-ordering, the number of mistakes must be at least $\Omega(d \log n)$, even when the points are drawn uniformly from the unit sphere and the learner only needs to predict the labels for $1\%$ of them.
</details>
<details>
<summary>摘要</summary>
在在线分类中，学习者会看到一串示例，并尝试预测它们的标签，以最小化总错误数量。在自适应变体中，学习者可以适应地选择预测的顺序。我们研究了预测顺序的选择力，并证明了在线分类的基本任务中，自适应学习的最差顺序和随机顺序之间存在首次强分化。在我们的工作前，这种分化只知道在非常限定的概念集合中，例如一维阈值或AXI正方形。我们提出了两个主要结果。如果$X$是一个$d$-维均匀随机分布的点集， then we design an efficient self-directed learner that makes $O(d \log \log n)$ mistakes and classifies the entire dataset.如果$X$是一个任意$d$-维数据集的Size $n$, then we design an efficient self-directed learner that predicts the labels of $99\%$ of the points in $X$ with mistake bound independent of $n$.相比之下，在最差或随机顺序下，错误数量至少为$\Omega(d \log n)$, even when the points are drawn uniformly from the unit sphere and the learner only needs to predict the labels for $1\%$ of them.
</details></li>
</ul>
<hr>
<h2 id="Iterative-Magnitude-Pruning-as-a-Renormalisation-Group-A-Study-in-The-Context-of-The-Lottery-Ticket-Hypothesis"><a href="#Iterative-Magnitude-Pruning-as-a-Renormalisation-Group-A-Study-in-The-Context-of-The-Lottery-Ticket-Hypothesis" class="headerlink" title="Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis"></a>Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03128">http://arxiv.org/abs/2308.03128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abu-Al Hassan</li>
<li>for: 这个论文探讨了深度神经网络（DNN）的复杂世界，特别关注了赢家票假设（LTH）。</li>
<li>methods: 这个论文使用了迭代幂额减小（IMP）法则，逐渐消除微型 weights，模拟 DNN 的步进学习。</li>
<li>results: 研究发现，winning ticket 可以在各种相似问题上达到类似性能，并且通过与物理学术 Renormalisation Group（RG）理论的联系，提高了 IMP 的理解。<details>
<summary>Abstract</summary>
This thesis delves into the intricate world of Deep Neural Networks (DNNs), focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The LTH posits that within extensive DNNs, smaller, trainable subnetworks termed "winning tickets", can achieve performance comparable to the full model. A key process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates minimal weights, emulating stepwise learning in DNNs. Once we identify these winning tickets, we further investigate their "universality". In other words, we check if a winning ticket that works well for one specific problem could also work well for other, similar problems. We also bridge the divide between the IMP and the Renormalisation Group (RG) theory in physics, promoting a more rigorous understanding of IMP.
</details>
<details>
<summary>摘要</summary>
这个论文探讨了深度神经网络（DNN）的复杂世界，专注于吸引人的抽签假设（LTH）。LTH认为，在广泛的DNN中，更小的、可训练的子网络“赢家票”可以达到相同的性能。我们在LTH中使用增量大小减少（IMP）来逐渐消除最小的 weights，模拟了DNN中的步进学习。一旦我们确定了这些赢家票，我们进一步调查它们的“通用性”。即我们检查一个赢家票在一个特定问题上表现良好是否可以在其他相似问题上表现良好。我们还将IMP与物理学中的 renormalization group（RG）理论相连接，以便更好地理解IMP。
</details></li>
</ul>
<hr>
<h2 id="Learning-Rate-Free-Learning-Dissecting-D-Adaptation-and-Probabilistic-Line-Search"><a href="#Learning-Rate-Free-Learning-Dissecting-D-Adaptation-and-Probabilistic-Line-Search" class="headerlink" title="Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic Line Search"></a>Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03102">http://arxiv.org/abs/2308.03102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max McGuinness</li>
<li>for: 这 paper 探讨了两种最近的学习率优化方法在随机梯度下降中：D-Adaptation（arXiv:2301.07733）和 probabilistic line search（arXiv:1502.02846）。这些方法的目的是减轻选择初始学习率的负担，通过包含距离度量和 Gaussian 过程 posterior 估计，respectively。</li>
<li>methods: 这 paper 使用了 D-Adaptation 和 probabilistic line search 两种方法，它们都是为了优化学习率。D-Adaptation 方法使用了距离度量来选择最佳学习率，而 probabilistic line search 方法则使用了 Gaussian 过程 posterior 估计来优化学习率。</li>
<li>results: 这 paper 的结果表明，D-Adaptation 和 probabilistic line search 两种方法都可以减轻选择初始学习率的负担，并且可以提高模型的性能。具体来说，D-Adaptation 方法可以在不同的数据集上实现更好的性能，而 probabilistic line search 方法则可以在不同的学习率下实现更好的性能。<details>
<summary>Abstract</summary>
This paper explores two recent methods for learning rate optimisation in stochastic gradient descent: D-Adaptation (arXiv:2301.07733) and probabilistic line search (arXiv:1502.02846). These approaches aim to alleviate the burden of selecting an initial learning rate by incorporating distance metrics and Gaussian process posterior estimates, respectively. In this report, I provide an intuitive overview of both methods, discuss their shared design goals, and devise scope for merging the two algorithms.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文探讨了两种最近的学习率优化方法：D-Adaptation（arXiv:2301.07733）和概率线搜索（arXiv:1502.02846）。这两种方法都尝试减轻选择初始学习率的负担，通过 incorporating 距离度量和 Gaussian 过程 posterior 估计，分别。在这份报告中，我提供了这两种方法的直观概述，讨论了它们的共同设计目标，并探讨了将两个算法合并的可能性。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Coding-through-Iterative-Block-Leverage-Score-Sampling"><a href="#Gradient-Coding-through-Iterative-Block-Leverage-Score-Sampling" class="headerlink" title="Gradient Coding through Iterative Block Leverage Score Sampling"></a>Gradient Coding through Iterative Block Leverage Score Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03096">http://arxiv.org/abs/2308.03096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neophytos Charalambides, Mert Pilanci, Alfred Hero</li>
<li>for: 这篇论文是用于实现分布式计算环境中的线性回传运算加速。</li>
<li>methods: 论文使用了扩展的抽样得分组（Leverage Score Sampling），并将其应用于首项方法（Gradient Coding），以减少分布式计算网络中的延迟。</li>
<li>results: 论文获得了一个可以在分布式计算环境中实现线性回传运算的轻量级化码 computing方案，并且可以在当中获得一个具有抽样范围的 $\ell_2$ 子空间嵌入。<details>
<summary>Abstract</summary>
We generalize the leverage score sampling sketch for $\ell_2$-subspace embeddings, to accommodate sampling subsets of the transformed data, so that the sketching approach is appropriate for distributed settings. This is then used to derive an approximate coded computing approach for first-order methods; known as gradient coding, to accelerate linear regression in the presence of failures in distributed computational networks, \textit{i.e.} stragglers. We replicate the data across the distributed network, to attain the approximation guarantees through the induced sampling distribution. The significance and main contribution of this work, is that it unifies randomized numerical linear algebra with approximate coded computing, while attaining an induced $\ell_2$-subspace embedding through uniform sampling. The transition to uniform sampling is done without applying a random projection, as in the case of the subsampled randomized Hadamard transform. Furthermore, by incorporating this technique to coded computing, our scheme is an iterative sketching approach to approximately solving linear regression. We also propose weighting when sketching takes place through sampling with replacement, for further compression.
</details>
<details>
<summary>摘要</summary>
我们总结了权重评分抽样策略，以适应分布式设置，以便在分布式计算网络中使用抽样subset。这种策略可以用来 derivate一种精确的代码计算方法，称为梯度编码，以加速分布式计算中的线性回归，即在分布式计算网络中的慢速进程（即慢进程）。我们将数据复制到分布式网络中，以实现近似 garantess through the induced sampling distribution。这个研究的重要性和主要贡献在于，它将随机化数字线性代数与近似代码计算相结合，并通过均匀抽样实现$\ell_2$次元空间嵌入。在抽样过程中，我们不会应用随机投影，如Randomized Hadamard Transform中的subsampled抽样。此外，我们还提出了在抽样过程中使用权重，以进一步压缩数据。因此，我们的方案是一种迭代抽样策略，用于约等于解 linear regression。我们的方法可以在分布式计算中实现高效的线性回归解决方案，并且可以扩展到更复杂的机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Control-aware-echo-state-networks-Ca-ESN-for-the-suppression-of-extreme-events"><a href="#Control-aware-echo-state-networks-Ca-ESN-for-the-suppression-of-extreme-events" class="headerlink" title="Control-aware echo state networks (Ca-ESN) for the suppression of extreme events"></a>Control-aware echo state networks (Ca-ESN) for the suppression of extreme events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03095">http://arxiv.org/abs/2308.03095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Racca, Luca Magri</li>
<li>for: 防止非线性系统中的极端事件的发生</li>
<li>methods: 结合ESN和控制策略，如比例- интеграル-导数控制和模型预测控制，实现非线性系统的有效控制</li>
<li>results: 在混沌液体流中，使用Ca-ESN比传统方法减少极端事件的发生，提高控制效果，开启了非线性系统控制的新可能性<details>
<summary>Abstract</summary>
Extreme event are sudden large-amplitude changes in the state or observables of chaotic nonlinear systems, which characterize many scientific phenomena. Because of their violent nature, extreme events typically have adverse consequences, which call for methods to prevent the events from happening. In this work, we introduce the control-aware echo state network (Ca-ESN) to seamlessly combine ESNs and control strategies, such as proportional-integral-derivative and model predictive control, to suppress extreme events. The methodology is showcased on a chaotic-turbulent flow, in which we reduce the occurrence of extreme events with respect to traditional methods by two orders of magnitude. This works opens up new possibilities for the efficient control of nonlinear systems with neural networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>极端事件是非线性系统中突然大幅度变化的状态或观测量，这些事件frequently occur in many scientific phenomena. 由于其暴力性，极端事件通常会带来不良后果，需要采取方法来预防这些事件的发生。在这种工作中，我们提出了控制意识 echo state network (Ca-ESN)，可以将ESNs和控制策略，如 proporциональ-integral-derivative 和模型预测控制，结合在一起，以降低极端事件的发生频率。我们在混沌-turbulent flow中应用了这种方法，并比传统方法降低了极端事件的发生频率二个数量级。这项工作开 up new possibilities for the efficient control of nonlinear systems with neural networks.Note: "极端事件" in Chinese is usually translated as "extreme events" or "outliers", but in the context of this text, it refers to sudden large-amplitude changes in the state or observables of chaotic nonlinear systems.
</details></li>
</ul>
<hr>
<h2 id="Visualization-of-Extremely-Sparse-Contingency-Table-by-Taxicab-Correspondence-Analysis-A-Case-Study-of-Textual-Data"><a href="#Visualization-of-Extremely-Sparse-Contingency-Table-by-Taxicab-Correspondence-Analysis-A-Case-Study-of-Textual-Data" class="headerlink" title="Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data"></a>Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03079">http://arxiv.org/abs/2308.03079</a></li>
<li>repo_url: None</li>
<li>paper_authors: V. Choulakian, J. Allard</li>
<li>for: Visualization of extremely sparse ontingency tables</li>
<li>methods: Taxicab correspondence analysis, a robust variant of correspondence analysis</li>
<li>results: Visualization of an extremely sparse textual data set of size 590 by 8265 concerning fragments of 8 sacred books<details>
<summary>Abstract</summary>
We present an overview of taxicab correspondence analysis, a robust variant of correspondence analysis, for visualization of extremely sparse ontingency tables. In particular we visualize an extremely sparse textual data set of size 590 by 8265 concerning fragments of 8 sacred books recently introduced by Sah and Fokou\'e (2019) and studied quite in detail by (12 + 1) dimension reduction methods (t-SNE, UMAP, PHATE,...) by Ma, Sun and Zou (2022).
</details>
<details>
<summary>摘要</summary>
我们提供了taxicab对应分析的概述，这是对对应分析的一种稳定版本，用于可见化极稀疏的对应关系表。特别是我们使用了590行x8265列的极稀疏文本数据集，这些数据来自 sah和fokou（2019）所引入的8种圣书的片断，并且通过(12+1)维度减少方法（t-SNE、UMAP、PHATE等）进行了深入研究。这些研究由Ma、sun和Zou（2022）进行了。
</details></li>
</ul>
<hr>
<h2 id="Study-for-Performance-of-MobileNetV1-and-MobileNetV2-Based-on-Breast-Cancer"><a href="#Study-for-Performance-of-MobileNetV1-and-MobileNetV2-Based-on-Breast-Cancer" class="headerlink" title="Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer"></a>Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03076">http://arxiv.org/abs/2308.03076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuqi Yan</li>
<li>for: 这项实验的目的是比较 MobileNetV1 和 MobileNetV2 模型在分类乳腺癌图像方面的性能。</li>
<li>methods: 实验使用了 Kaggle 上下载的数据集，并对其进行Normalization。然后，使用了神经网络模型来学习数据集，找出图像的特征并判断乳腺癌。</li>
<li>results: 实验结果表明，在处理这个数据集时，MobileNetV1 模型表现较好， validation accuracy 和 overfit 也较高。<details>
<summary>Abstract</summary>
Artificial intelligence is constantly evolving and can provide effective help in all aspects of people's lives. The experiment is mainly to study the use of artificial intelligence in the field of medicine. The purpose of this experiment was to compare which of MobileNetV1 and MobileNetV2 models was better at detecting histopathological images of the breast downloaded at Kaggle. When the doctor looks at the pathological image, there may be errors that lead to errors in judgment, and the observation speed is slow. Rational use of artificial intelligence can effectively reduce the error of doctor diagnosis in breast cancer judgment and speed up doctor diagnosis. The dataset was downloaded from Kaggle and then normalized. The basic principle of the experiment is to let the neural network model learn the downloaded data set. Then find the pattern and be able to judge on your own whether breast tissue is cancer. In the dataset, benign tumor pictures and malignant tumor pictures have been classified, of which 198738 are benign tumor pictures and 78, 786 are malignant tumor pictures. After calling MobileNetV1 and MobileNetV2, the dataset is trained separately, the training accuracy and validation accuracy rate are obtained, and the image is drawn. It can be observed that MobileNetV1 has better validation accuracy and overfit during MobileNetV2 training. From the experimental results, it can be seen that in the case of processing this dataset, MobileNetV1 is much better than MobileNetV2.
</details>
<details>
<summary>摘要</summary>
人工智能不断发展，可以提供有效的帮助在人们的生活中。本实验的主要目的是研究人工智能在医学领域的应用。本实验的目的是比较MobileNetV1和MobileNetV2模型在Kaggle上下载的乳腺病理图像上的表现。医生查看病理图像时可能会出现错误，导致诊断错误， observation速度较慢。合理使用人工智能可以有效减少医生诊断乳腺癌判断中的错误，并加快医生诊断速度。数据集来自Kaggle，然后 норциали化。实验的基本原则是让神经网络模型学习下载的数据集。然后找出模式，并能够自己判断乳腺细胞是否为癌细胞。数据集中，恶性肿瘤图像和良性肿瘤图像已经分类，其中198738个是恶性肿瘤图像，78786个是良性肿瘤图像。在 MobileNetV1 和 MobileNetV2 之后，数据集被分别训练，并获得训练精度和验证精度率。图像也被画出来。可以看到，在处理这个数据集时，MobileNetV1 表现得更好。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Epileptic-Seizure-Prediction-Exploring-Diverse-Pre-Processing-Techniques-and-Machine-Learning-Models"><a href="#Comparative-Analysis-of-Epileptic-Seizure-Prediction-Exploring-Diverse-Pre-Processing-Techniques-and-Machine-Learning-Models" class="headerlink" title="Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models"></a>Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05176">http://arxiv.org/abs/2308.05176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Simul Hasan Talukder, Rejwan Bin Sulaiman</li>
<li>for: 预测癫痫发作</li>
<li>methods: 使用五种机器学习模型（Random Forest、Decision Tree、Extra Trees、Logistic Regression、Gradient Boosting）对电enzephalogram数据进行预测</li>
<li>results: 结果显示，LR类ifier的准确率为56.95%，GB和DT都达到97.17%的准确率，RT达到98.99%的准确率，ET模型表现最佳，准确率达99.29%。<details>
<summary>Abstract</summary>
Epilepsy is a prevalent neurological disorder characterized by recurrent and unpredictable seizures, necessitating accurate prediction for effective management and patient care. Application of machine learning (ML) on electroencephalogram (EEG) recordings, along with its ability to provide valuable insights into brain activity during seizures, is able to make accurate and robust seizure prediction an indispensable component in relevant studies. In this research, we present a comprehensive comparative analysis of five machine learning models - Random Forest (RF), Decision Tree (DT), Extra Trees (ET), Logistic Regression (LR), and Gradient Boosting (GB) - for the prediction of epileptic seizures using EEG data. The dataset underwent meticulous preprocessing, including cleaning, normalization, outlier handling, and oversampling, ensuring data quality and facilitating accurate model training. These preprocessing techniques played a crucial role in enhancing the models' performance. The results of our analysis demonstrate the performance of each model in terms of accuracy. The LR classifier achieved an accuracy of 56.95%, while GB and DT both attained 97.17% accuracy. RT achieved a higher accuracy of 98.99%, while the ET model exhibited the best performance with an accuracy of 99.29%. Our findings reveal that the ET model outperformed not only the other models in the comparative analysis but also surpassed the state-of-the-art results from previous research. The superior performance of the ET model makes it a compelling choice for accurate and robust epileptic seizure prediction using EEG data.
</details>
<details>
<summary>摘要</summary>
《诊断和治疗精神疾病》中，有一种常见的神经疾病是癫痫症，它表现为不规则和难以预测的癫痫发作，因此需要准确的预测以提供有效的管理和患者护理。在这些研究中，我们使用机器学习（ML）技术对电энцефалограм（EEG）记录进行分析，并通过对大脑活动的描述来提供有价值的预测。本研究中，我们对五种机器学习模型进行了比较分析：Random Forest（RF）、Decision Tree（DT）、Extra Trees（ET）、Logistic Regression（LR）和Gradient Boosting（GB）。我们对EEG数据进行了仔细的处理，包括清洁、 нормализа、异常处理和扩充，以确保数据质量的高度。这些处理技术对模型的表现产生了重要的影响。我们的分析结果显示每个模型的准确率。LR分类器的准确率为56.95%，而GB和DT都达到了97.17%的准确率。RT达到了98.99%的准确率，而ET模型表现出了最佳的性能，准确率达99.29%。我们的发现表明ET模型不仅在 Comparative analysis中表现出色，还超越了过去研究中的状态归化结果。ET模型的优秀表现使其成为精确和可靠的癫痫发作预测的首选方法。
</details></li>
</ul>
<hr>
<h2 id="TARJAMAT-Evaluation-of-Bard-and-ChatGPT-on-Machine-Translation-of-Ten-Arabic-Varieties"><a href="#TARJAMAT-Evaluation-of-Bard-and-ChatGPT-on-Machine-Translation-of-Ten-Arabic-Varieties" class="headerlink" title="TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties"></a>TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03051">http://arxiv.org/abs/2308.03051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karima Kadaoui, Samar M. Magdy, Abdul Waheed, Md Tawkat Islam Khondaker, Ahmed Oumar El-Shangiti, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed</li>
<li>for: 这篇论文主要是为了评估大型自然语言模型（LLMs）在不同的阿拉伯语种中的翻译能力。</li>
<li>methods: 这篇论文使用了Google Bard和OpenAI ChatGPT两个模型，并对这两个模型在十种阿拉伯语种中的翻译能力进行了全面的评估。</li>
<li>results: 研究发现，LLMs在一些阿拉伯语种中表现不佳，特别是对于没有充足公共数据的语种，如阿尔及利亚和毛里塔尼亚的方言。然而，它们在更常见的方言中表现较为满意，尽管有时会落后于现有的商业系统 like Google Translate。此外，研究还发现，Bard在翻译任务中遵循人类指示的能力有限。<details>
<summary>Abstract</summary>
Large language models (LLMs) finetuned to follow human instructions have recently emerged as a breakthrough in AI. Models such as Google Bard and OpenAI ChatGPT, for example, are surprisingly powerful tools for question answering, code debugging, and dialogue generation. Despite the purported multilingual proficiency of these models, their linguistic inclusivity remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic, Modern Standard Arabic, and several nuanced dialectal variants. Furthermore, we undertake a human-centric study to scrutinize the efficacy of the most recent model, Bard, in following human instructions during translation tasks. Our exhaustive analysis indicates that LLMs may encounter challenges with certain Arabic dialects, particularly those for which minimal public data exists, such as Algerian and Mauritanian dialects. However, they exhibit satisfactory performance with more prevalent dialects, albeit occasionally trailing behind established commercial systems like Google Translate. Additionally, our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如Google Bard和OpenAI ChatGPT，在最近几年内 emerge as a breakthrough in AI。这些模型具有强大的问答、代码调试和对话生成能力。 despite the purported multilingual proficiency of these models, their linguistic inclusivity remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (including both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic, Modern Standard Arabic, and several nuanced dialectal variants. Furthermore, we undertake a human-centric study to scrutinize the efficacy of the most recent model, Bard, in following human instructions during translation tasks. Our exhaustive analysis indicates that LLMs may encounter challenges with certain Arabic dialects, particularly those for which minimal public data exists, such as Algerian and Mauritanian dialects. However, they exhibit satisfactory performance with more prevalent dialects, albeit occasionally trailing behind established commercial systems like Google Translate. Additionally, our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Multi-Task-Representation-Learning-for-Human-Activity-Analysis-Using-Wearables"><a href="#Weakly-Supervised-Multi-Task-Representation-Learning-for-Human-Activity-Analysis-Using-Wearables" class="headerlink" title="Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables"></a>Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03805">http://arxiv.org/abs/2308.03805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for: 多元特征掌握和多任务同时处理</li>
<li>methods: 弱监睹多输出SIAMESE网络</li>
<li>results: 可以同时解决多个任务，并且在许多情况下超越单任务监睹方法表现。<details>
<summary>Abstract</summary>
Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and even to outperform single task supervised methods in many situations. In addition, further experiments are presented that in more detail analyze the effect of the architecture and of using multiple tasks within this framework, that investigate the scalability of the model to include additional tasks, and that demonstrate the ability of the framework to combine data for which only partial relationship information with respect to the target tasks is available.
</details>
<details>
<summary>摘要</summary>
仪器数据流FROM wearable devices和智能环境广泛研究在人体活动识别（HAR）、人员身份识别或健康监测等领域。然而，大多数前一些工作在活动和仪器流数据分析中都是关注一个方面的数据，例如只是识别活动的类型或者只是识别活动的执行者。我们提出了一种方法，使用弱监督多输出siamesenet来映射数据到多个表示空间中，其中每个表示空间都关注一个数据的方面。映射 vectors的数据样本被置于空间中，使得数据具有同一 Semantic meaning在该方面的数据集中均被 closely located。因此，通过一些实验，我们的模型可以为数据提供多个任务的指标，使其可以同时解决多个任务，甚至在许多情况下超越单任务监督方法。此外，我们还进行了更多的实验，分析了这种架构的影响和多个任务的使用情况，以及模型的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-methods-for-the-search-for-L-T-brown-dwarfs-in-the-data-of-modern-sky-surveys"><a href="#Machine-learning-methods-for-the-search-for-L-T-brown-dwarfs-in-the-data-of-modern-sky-surveys" class="headerlink" title="Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys"></a>Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03045">http://arxiv.org/abs/2308.03045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamaleksandra/ml-brown-dwarfs">https://github.com/iamaleksandra/ml-brown-dwarfs</a></li>
<li>paper_authors: Aleksandra Avdeeva</li>
<li>For: 这个论文的目的是创建一个高度可靠的褐矮星样本，以确定褐矮星的特征和分布。* Methods: 这篇论文使用机器学习算法，如Random Forest Classifier、XGBoost、SVM Classifier和TabNet，对PanStarrs DR1、2MASS和WISE数据进行分类，以 distinguishing L和T褐矮星从其他 spectral和照度类型的 объек。* Results: 研究人员使用机器学习算法，成功地分类出L和T褐矮星，并与传统的决策规则进行比较，证明了其效率和相关性。<details>
<summary>Abstract</summary>
According to various estimates, brown dwarfs (BD) should account for up to 25 percent of all objects in the Galaxy. However, few of them are discovered and well-studied, both individually and as a population. Homogeneous and complete samples of brown dwarfs are needed for these kinds of studies. Due to their weakness, spectral studies of brown dwarfs are rather laborious. For this reason, creating a significant reliable sample of brown dwarfs, confirmed by spectroscopic observations, seems unattainable at the moment. Numerous attempts have been made to search for and create a set of brown dwarfs using their colours as a decision rule applied to a vast amount of survey data. In this work, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier and TabNet on PanStarrs DR1, 2MASS and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. The explanation of the models is discussed. We also compare our models with classical decision rules, proving their efficiency and relevance.
</details>
<details>
<summary>摘要</summary>
To overcome this challenge, numerous attempts have been made to search for and create a set of brown dwarfs using their colors as a decision rule applied to a vast amount of survey data. In this study, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier, and TabNet on PanStarrs DR1, 2MASS, and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. We discuss the explanation of the models and compare them with classical decision rules, demonstrating their efficiency and relevance.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-Infectious-Disease-Risk-Prediction-A-Survey"><a href="#Machine-Learning-for-Infectious-Disease-Risk-Prediction-A-Survey" class="headerlink" title="Machine Learning for Infectious Disease Risk Prediction: A Survey"></a>Machine Learning for Infectious Disease Risk Prediction: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03037">http://arxiv.org/abs/2308.03037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutong Liu, Yang Liu, Jiming Liu</li>
<li>for: 这篇论文的目的是探讨机器学习如何在抑制传染病中发挥作用，以便更有效地预测感染病风险。</li>
<li>methods: 论文中使用的方法包括：介绍背景和动机，介绍不同类型的机器学习模型，讨论模型输入、设计目标和评估性能等挑战，并结尾提出未解决的问题和未来方向。</li>
<li>results: 论文的结果表明，机器学习可以帮助量化疾病传染模式，并准确预测感染病风险。<details>
<summary>Abstract</summary>
Infectious diseases, either emerging or long-lasting, place numerous people at risk and bring heavy public health burdens worldwide. In the process against infectious diseases, predicting the epidemic risk by modeling the disease transmission plays an essential role in assisting with preventing and controlling disease transmission in a more effective way. In this paper, we systematically describe how machine learning can play an essential role in quantitatively characterizing disease transmission patterns and accurately predicting infectious disease risks. First, we introduce the background and motivation of using machine learning for infectious disease risk prediction. Next, we describe the development and components of various machine learning models for infectious disease risk prediction. Specifically, existing models fall into three categories: Statistical prediction, data-driven machine learning, and epidemiology-inspired machine learning. Subsequently, we discuss challenges encountered when dealing with model inputs, designing task-oriented objectives, and conducting performance evaluation. Finally, we conclude with a discussion of open questions and future directions.
</details>
<details>
<summary>摘要</summary>
免疫疾病，无论是新兴的或长期存在的，都会对全球公共卫生带来巨大的压力。在抗击免疫疾病的过程中，预测疾病传播风险的模型化协助了更有效地预防和控制疾病传播。在这篇论文中，我们系统地描述了机器学习如何在免疫疾病风险预测中发挥重要作用。首先，我们介绍了使用机器学习预测免疫疾病风险的背景和动机。然后，我们描述了不同类型的机器学习模型的开发和组成部分。具体来说，现有的模型可以分为三类：统计预测、数据驱动机器学习和医学机器学习。接着，我们讨论了与模型输入、设计任务目标以及性能评价过程中遇到的挑战。最后，我们 conclude with 未来方向的开放问题。
</details></li>
</ul>
<hr>
<h2 id="Serverless-Federated-AUPRC-Optimization-for-Multi-Party-Collaborative-Imbalanced-Data-Mining"><a href="#Serverless-Federated-AUPRC-Optimization-for-Multi-Party-Collaborative-Imbalanced-Data-Mining" class="headerlink" title="Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining"></a>Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03035">http://arxiv.org/abs/2308.03035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xidongwu/d-auprc">https://github.com/xidongwu/d-auprc</a></li>
<li>paper_authors: Xidong Wu, Zhengmian Hu, Jian Pei, Heng Huang</li>
<li>for: 本文targets the problem of multi-party collaborative training for imbalanced data tasks, and proposes a new algorithm called ServerLess biAsed sTochastic gradiEnt (SLATE) to directly optimize the Area Under Precision-Recall Curve (AUPRC).</li>
<li>methods: 本文使用了服务器less多方合作学习 Setting，并将问题转化为conditional stochastic optimization problem。furthermore, the authors propose a new algorithm called ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) to improve the convergence rate.</li>
<li>results: 本文的实验结果表明，SLATE-M算法可以减少communication cost，并且与最佳单机Online方法匹配。这是首次解决多方合作AUPRC最大化问题。<details>
<summary>Abstract</summary>
Multi-party collaborative training, such as distributed learning and federated learning, is used to address the big data challenges. However, traditional multi-party collaborative training algorithms were mainly designed for balanced data mining tasks and are intended to optimize accuracy (\emph{e.g.}, cross-entropy). The data distribution in many real-world applications is skewed and classifiers, which are trained to improve accuracy, perform poorly when applied to imbalanced data tasks since models could be significantly biased toward the primary class. Therefore, the Area Under Precision-Recall Curve (AUPRC) was introduced as an effective metric. Although single-machine AUPRC maximization methods have been designed, multi-party collaborative algorithm has never been studied. The change from the single-machine to the multi-party setting poses critical challenges.   To address the above challenge, we study the serverless multi-party collaborative AUPRC maximization problem since serverless multi-party collaborative training can cut down the communications cost by avoiding the server node bottleneck, and reformulate it as a conditional stochastic optimization problem in a serverless multi-party collaborative learning setting and propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to directly optimize the AUPRC. After that, we use the variance reduction technique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) algorithm to improve the convergence rate, which matches the best theoretical convergence result reached by the single-machine online method. To the best of our knowledge, this is the first work to solve the multi-party collaborative AUPRC maximization problem.
</details>
<details>
<summary>摘要</summary>
多方合作训练，如分布式学习和联邦学习，用于解决大数据问题。然而，传统的多方合作训练算法主要设计用于均衡数据挖掘任务，并且optimize准确率（例如，交叉熵）。在许多实际应用中，数据分布不均，类 clasifier在不均衡数据任务中表现糟糕，因为模型可能受主要类别的偏见。因此，Area Under Precision-Recall Curve（AUPRC）被引入作为有效指标。虽然单机AUPRC最大化方法已经设计过，但多方合作算法尚未研究。在单机到多方 Setting中的变化 pose critical challenges。为了解决以上挑战，我们研究了无服务器多方合作AUPRC最大化问题，因为无服务器多方合作训练可以降低通信成本，并将问题重新定义为conditional stochastic optimization问题在无服务器多方合作学习Setting中。然后，我们提出了一种新的ServerLess biAsed sTochastic gradiEnt（SLATE）算法，以直接优化AUPRC。接着，我们使用了差分reduction技术，并提出了ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction（SLATE-M）算法，以提高收敛率，与单机在线方法的最佳理论收敛率匹配。到目前为止，这是首次解决多方合作AUPRC最大化问题的研究。
</details></li>
</ul>
<hr>
<h2 id="Causal-Disentanglement-Hidden-Markov-Model-for-Fault-Diagnosis"><a href="#Causal-Disentanglement-Hidden-Markov-Model-for-Fault-Diagnosis" class="headerlink" title="Causal Disentanglement Hidden Markov Model for Fault Diagnosis"></a>Causal Disentanglement Hidden Markov Model for Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03027">http://arxiv.org/abs/2308.03027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rihao Chang, Yongtao Ma, Weizhi Nie, Jie Nie, An-an Liu</li>
<li>for: 本研究旨在提出一种基于 causal disentanglement hidden markov model (CDHM) 的磨损诊断方法，以便更好地捕捉磨损特征并实现预测磨损类型。</li>
<li>methods: 本方法首先使用时序数据进行磨损特征的捕捉，然后逐渐分离磨损信号中相关和无关的因素。 其中，我们使用 ELBO 来优化学习 causal disentanglement markov model。此外，我们还采用无监督领域适应，将学习的分离表示转移到其他工作环境中。</li>
<li>results: 实验结果表明，提出的方法在 CWRU 数据集和 IMS 数据集上具有优秀的效果，可以准确地预测磨损类型。<details>
<summary>Abstract</summary>
In modern industries, fault diagnosis has been widely applied with the goal of realizing predictive maintenance. The key issue for the fault diagnosis system is to extract representative characteristics of the fault signal and then accurately predict the fault type. In this paper, we propose a Causal Disentanglement Hidden Markov model (CDHM) to learn the causality in the bearing fault mechanism and thus, capture their characteristics to achieve a more robust representation. Specifically, we make full use of the time-series data and progressively disentangle the vibration signal into fault-relevant and fault-irrelevant factors. The ELBO is reformulated to optimize the learning of the causal disentanglement Markov model. Moreover, to expand the scope of the application, we adopt unsupervised domain adaptation to transfer the learned disentangled representations to other working environments. Experiments were conducted on the CWRU dataset and IMS dataset. Relevant results validate the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
现代产业中，故障诊断已广泛应用，目的是实现预测维护。故障诊断系统的关键问题是提取表征性的故障信号，并准确预测故障类型。在本文中，我们提议一种因果分解隐藏马尔可夫模型（CDHM），以学习滤波器故障机制中的因果关系，并 capture其特征来实现更加稳定的表征。具体来说，我们利用时间序列数据，逐步分解振荡信号，分解出相关和无关故障因素。我们 reformulate ELBO，以便学习因果分解马尔可夫模型。此外，为扩展应用范围，我们采用无监督领域适应，将学习的分解表征转移到其他工作环境。在CWRU数据集和IMS数据集上进行了实验，实验结果证明了我们提出的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Early-Detection-and-Localization-of-Pancreatic-Cancer-by-Label-Free-Tumor-Synthesis"><a href="#Early-Detection-and-Localization-of-Pancreatic-Cancer-by-Label-Free-Tumor-Synthesis" class="headerlink" title="Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis"></a>Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03008">http://arxiv.org/abs/2308.03008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrgiovanni/synthetictumors">https://github.com/mrgiovanni/synthetictumors</a></li>
<li>paper_authors: Bowen Li, Yu-Cheng Chou, Shuwen Sun, Hualin Qiao, Alan Yuille, Zongwei Zhou<br>for: 这篇论文的目的是提高胰脏癌早期检测和定位，以增加病人5年生存率从8.5%提高到20%。methods: 本研究使用人工智能（AI）模型，将健康胰脏中的小型胰脏肿瘤合成成许多标注的例子，以帮助医生早期检测胰脏癌。results: 我们的实验结果显示，使用合成的胰脏肿瘤训练AI模型，胰脏癌检测率与真实胰脏癌检测率相似，且能够更好地检测小型胰脏肿瘤。此外，我们还证明了使用合成胰脏肿瘤和真实胰脏癌检测结果进行混合训练，可以提高AI模型的普遍性和检测精度。<details>
<summary>Abstract</summary>
Early detection and localization of pancreatic cancer can increase the 5-year survival rate for patients from 8.5% to 20%. Artificial intelligence (AI) can potentially assist radiologists in detecting pancreatic tumors at an early stage. Training AI models require a vast number of annotated examples, but the availability of CT scans obtaining early-stage tumors is constrained. This is because early-stage tumors may not cause any symptoms, which can delay detection, and the tumors are relatively small and may be almost invisible to human eyes on CT scans. To address this issue, we develop a tumor synthesis method that can synthesize enormous examples of small pancreatic tumors in the healthy pancreas without the need for manual annotation. Our experiments demonstrate that the overall detection rate of pancreatic tumors, measured by Sensitivity and Specificity, achieved by AI trained on synthetic tumors is comparable to that of real tumors. More importantly, our method shows a much higher detection rate for small tumors. We further investigate the per-voxel segmentation performance of pancreatic tumors if AI is trained on a combination of CT scans with synthetic tumors and CT scans with annotated large tumors at an advanced stage. Finally, we show that synthetic tumors improve AI generalizability in tumor detection and localization when processing CT scans from different hospitals. Overall, our proposed tumor synthesis method has immense potential to improve the early detection of pancreatic cancer, leading to better patient outcomes.
</details>
<details>
<summary>摘要</summary>
早期检测和肿瘤localization可以提高患者5年生存率从8.5%提高到20%。人工智能（AI）可能能够帮助放射学家在早期发现肿瘤。然而，训练AI模型需要庞大的标注示例，但获得早期肿瘤的CT扫描数据受限。这是因为早期肿瘤可能不会导致任何症状，这可能会延迟检测，并且肿瘤相对较小，可能对人类目视难以看到在CT扫描中。为解决这个问题，我们开发了一种肿瘤合成方法，可以在健康的胰脏中合成巨大的小肿瘤示例，无需人工标注。我们的实验表明，由AI训练在合成肿瘤上的检测率（敏感性和特异性）与真实肿瘤相比较高，并且检测到小肿瘤的率更高。我们进一步调查了使用合成肿瘤和注解大肿瘤的CT扫描结合训练AI的效果，发现这种方法可以提高肿瘤检测和地图localization的普适性。最后，我们证明了合成肿瘤可以提高AI在不同医院CT扫描处理时的普适性。总之，我们提出的肿瘤合成方法有巨大的潜力，可以提高肿瘤检测的早期，导致更好的病例结果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Polar-Codes"><a href="#Deep-Polar-Codes" class="headerlink" title="Deep Polar Codes"></a>Deep Polar Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03004">http://arxiv.org/abs/2308.03004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HzFu/MNet_DeepCDR">https://github.com/HzFu/MNet_DeepCDR</a></li>
<li>paper_authors: Geon Choi, Namyoon Lee</li>
<li>for: 这个论文旨在提出一种新的预变换极码，称为深度极码。</li>
<li>methods: 论文提出了一种深度极编码器，利用多层极化转换来实现低复杂度实现，同时能够改善极码的重量分布。此外，该编码器支持范围广的代码率和块长。</li>
<li>results: 通过 simulations，论文表明深度极码在不同代码率下的块错误率比现有的预变换极码更低，同时保持低的编码和解码复杂度。此外，论文还表明，将深度极码与循环检验码 concatenate 可以达到 finite block length 容量的 meta-converse bound 的下界 within 0.4 dB 以下。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel class of pre-transformed polar codes, termed as deep polar codes. We first present a deep polar encoder that harnesses a series of multi-layered polar transformations with varying sizes. Our approach to encoding enables a low-complexity implementation while significantly enhancing the weight distribution of the code. Moreover, our encoding method offers flexibility in rate-profiling, embracing a wide range of code rates and blocklengths. Next, we put forth a low-complexity decoding algorithm called successive cancellation list with backpropagation parity checks (SCL-BPC). This decoding algorithm leverages the parity check equations in the reverse process of the multi-layered pre-transformed encoding for SCL decoding. Additionally, we present a low-latency decoding algorithm that employs parallel-SCL decoding by treating partially pre-transformed bit patterns as additional frozen bits. Through simulations, we demonstrate that deep polar codes outperform existing pre-transformed polar codes in terms of block error rates across various code rates under short block lengths, while maintaining low encoding and decoding complexity. Furthermore, we show that concatenating deep polar codes with cyclic-redundancy-check codes can achieve the meta-converse bound of the finite block length capacity within 0.4 dB in some instances.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的预转换极码，称为深度极码。我们首先提出了一种深度极编码器，利用多层极化转换来实现低复杂性实现，同时提高码的质量分布。此外，我们的编码方法支持范围广的码率和块长度。接着，我们提出了一种低复杂度解码算法，称为顺序取消列表带回传播可能性检查（SCL-BPC）。这种解码算法利用了反向多层预转换的parity check方程来实现SCL解码。此外，我们还提出了一种低延迟解码算法，通过并行SCL解码来处理部分预转换的bit pattern。通过实验，我们证明了深度极码在不同的码率下的块错误率都较低，同时保持了低编码和解码复杂度。此外，我们还显示了 concatenating 深度极码可以实现finite block length容量的meta-converse bound在0.4 dB之间。
</details></li>
</ul>
<hr>
<h2 id="Spanish-Pre-trained-BERT-Model-and-Evaluation-Data"><a href="#Spanish-Pre-trained-BERT-Model-and-Evaluation-Data" class="headerlink" title="Spanish Pre-trained BERT Model and Evaluation Data"></a>Spanish Pre-trained BERT Model and Evaluation Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02976">http://arxiv.org/abs/2308.02976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dccuchile/beto">https://github.com/dccuchile/beto</a></li>
<li>paper_authors: José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, Jorge Pérez</li>
<li>for: 增强西班牙语模型的训练和评估资源。</li>
<li>methods: BERT基于模型预训练 exclusively on Spanish data。</li>
<li>results: 比其他基于BERT的模型在大多数任务上获得更好的结果，并在一些任务上创造新的状态。<details>
<summary>Abstract</summary>
The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a BERT-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the GLUE benchmark. By fine-tuning our pre-trained Spanish model, we obtain better results compared to other BERT-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data, and the compilation of the Spanish benchmarks.
</details>
<details>
<summary>摘要</summary>
“西班牙语是全球前五大最受欢迎的语言之一，然而找到用于训练或评估西班牙语模型的资源并不是一个容易的任务。在这篇论文中，我们帮助填补这个差距，提出了基于BERT的西班牙语模型，并将其推广到多个任务中。作为我们的第二次贡献，我们还将西班牙语的多个任务集成了一个单一的存储库，类似于GLUE套件。通过精益地调整我们的预训西班牙语模型，我们在大多数任务上取得了更好的结果，甚至创下了一些新的州态。我们已经公开了我们的模型、预训数据和西班牙语套件。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Generalized-Oversampling-for-Learning-from-Imbalanced-datasets-and-Associated-Theory"><a href="#Generalized-Oversampling-for-Learning-from-Imbalanced-datasets-and-Associated-Theory" class="headerlink" title="Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory"></a>Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02966">http://arxiv.org/abs/2308.02966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Stocksieker, Denys Pommeret, Arthur Charpentier</li>
<li>for: 本研究旨在解决超参异常学习中的数据不均衡问题，具体来说是用于异常 regression 问题。</li>
<li>methods: 本文提出了一种基于kernel density estimate的数据扩充方法，称为GOLIATH算法，该方法可以应用于分类和回归问题。它包括两大类别的人工扩充：基于扰动的，如加aussian noise，和基于 interpolate的，如 SMOTE。此外，该方法还提供了这些机器学习算法的Explicit表达式和准确性梯度的表达式，特别是对SMOTE算法的表达式。</li>
<li>results: 本文通过应用GOLIATH算法在异常 regression 中进行了实验评估，并与现有状态方法进行了比较。结果表明，GOLIATH算法在异常 regression 中具有显著的改善效果。<details>
<summary>Abstract</summary>
In supervised learning, it is quite frequent to be confronted with real imbalanced datasets. This situation leads to a learning difficulty for standard algorithms. Research and solutions in imbalanced learning have mainly focused on classification tasks. Despite its importance, very few solutions exist for imbalanced regression. In this paper, we propose a data augmentation procedure, the GOLIATH algorithm, based on kernel density estimates which can be used in classification and regression. This general approach encompasses two large families of synthetic oversampling: those based on perturbations, such as Gaussian Noise, and those based on interpolations, such as SMOTE. It also provides an explicit form of these machine learning algorithms and an expression of their conditional densities, in particular for SMOTE. New synthetic data generators are deduced. We apply GOLIATH in imbalanced regression combining such generator procedures with a wild-bootstrap resampling technique for the target values. We evaluate the performance of the GOLIATH algorithm in imbalanced regression situations. We empirically evaluate and compare our approach and demonstrate significant improvement over existing state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
在监督学习中，很普遍遇到实际数据集的不均衡问题。这种情况会导致标准算法学习困难。研究和解决不均衡学习问题的研究主要集中在分类任务上。虽然其重要性很大，但是现有的解决方案很少。在这篇论文中，我们提出了一种数据扩充方法，名为GOLIATH算法，基于核密度估计。这种方法可以用于分类和回归任务。这个总体方法包括两大家族的人工扩充：基于扰动，如 Gaussian Noise，和基于 interpolations，如 SMOTE。它还提供了这些机器学习算法的直观表达，特别是对 SMOTE 的表达。我们从这些synthetic数据生成器中推出了新的数据生成器。我们在不均衡回归中使用这些生成器和通用Bootstrap抽取技术来预测值。我们对 GOLIATH 算法在不均衡回归情况下的性能进行了实验性评估和比较，并证明了我们的方法在现有状态的技术上显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Data-Fusion-for-Multi-Task-Learning-of-Building-Extraction-and-Height-Estimation"><a href="#Data-Fusion-for-Multi-Task-Learning-of-Building-Extraction-and-Height-Estimation" class="headerlink" title="Data Fusion for Multi-Task Learning of Building Extraction and Height Estimation"></a>Data Fusion for Multi-Task Learning of Building Extraction and Height Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02960">http://arxiv.org/abs/2308.02960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SaadAhmedJamal/IEEE_DFC2023">https://github.com/SaadAhmedJamal/IEEE_DFC2023</a></li>
<li>paper_authors: Saad Ahmed Jamal, Arioluwa Aribisala</li>
<li>for: 本研究は都市重建问题上提出的DFC23 Track 2 Contest中的一种多任务学习方法，用于批量抽取和高度估算 optical和雷达卫星图像中。</li>
<li>methods: 本研究使用多任务学习方法，通过重用特征和形成隐式约束 между多个任务来提高解决方案的质量。</li>
<li>results: 本研究的基准结果表明，对于批量抽取和高度估算，在设计了相关实验后，baseline结果显著提高了。<details>
<summary>Abstract</summary>
In accordance with the urban reconstruction problem proposed by the DFC23 Track 2 Contest, this paper attempts a multitask-learning method of building extraction and height estimation using both optical and radar satellite imagery. Contrary to the initial goal of multitask learning which could potentially give a superior solution by reusing features and forming implicit constraints between multiple tasks, this paper reports the individual implementation of the building extraction and height estimation under constraints. The baseline results for the building extraction and the height estimation significantly increased after designed experiments.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "urban reconstruction" was translated as "城市重建" (chéngshì zhòngjiàn)* "multitask-learning" was translated as "多任务学习" (duō zhìxí)* "building extraction" was translated as "建筑物提取" (jiànzhì wù tiēchū)* "height estimation" was translated as "高度估算" (gāodù gèsuan)* "satellite imagery" was translated as "卫星图像" (wèixīng túxiàng)
</details></li>
</ul>
<hr>
<h2 id="K-band-Self-supervised-MRI-Reconstruction-via-Stochastic-Gradient-Descent-over-K-space-Subsets"><a href="#K-band-Self-supervised-MRI-Reconstruction-via-Stochastic-Gradient-Descent-over-K-space-Subsets" class="headerlink" title="K-band: Self-supervised MRI Reconstruction via Stochastic Gradient Descent over K-space Subsets"></a>K-band: Self-supervised MRI Reconstruction via Stochastic Gradient Descent over K-space Subsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02958">http://arxiv.org/abs/2308.02958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikgroup/k-band">https://github.com/mikgroup/k-band</a></li>
<li>paper_authors: Frederic Wang, Han Qi, Alfredo De Goyeneche, Reinhard Heckel, Michael Lustig, Efrat Shimron</li>
<li>for: 这项研究的目的是为了使用只有部分、有限分辨率的k-空间数据进行深度学习模型的训练，以提高MRI图像重建的精度和效率。</li>
<li>methods: 这项研究使用了一种新的数学框架，称为k-band，以便使用只有部分、有限分辨率的k-空间数据进行深度学习模型的训练。具体来说，这种方法使用了在每次训练迭代中使用只有一小部分k-空间数据来计算梯度的束教学法。</li>
<li>results: 实验表明，k-band方法可以与使用高分辨率数据进行训练的状态对照方法（SoTA）的性能相似，而无需使用高分辨率数据进行训练。此外，k-band方法还可以在快速获得的有限分辨率数据上进行自我监督式训练，从而提高了MRI图像重建的精度和效率。<details>
<summary>Abstract</summary>
Although deep learning (DL) methods are powerful for solving inverse problems, their reliance on high-quality training data is a major hurdle. This is significant in high-dimensional (dynamic/volumetric) magnetic resonance imaging (MRI), where acquisition of high-resolution fully sampled k-space data is impractical. We introduce a novel mathematical framework, dubbed k-band, that enables training DL models using only partial, limited-resolution k-space data. Specifically, we introduce training with stochastic gradient descent (SGD) over k-space subsets. In each training iteration, rather than using the fully sampled k-space for computing gradients, we use only a small k-space portion. This concept is compatible with different sampling strategies; here we demonstrate the method for k-space "bands", which have limited resolution in one dimension and can hence be acquired rapidly. We prove analytically that our method stochastically approximates the gradients computed in a fully-supervised setup, when two simple conditions are met: (i) the limited-resolution axis is chosen randomly-uniformly for every new scan, hence k-space is fully covered across the entire training set, and (ii) the loss function is weighed with a mask, derived here analytically, which facilitates accurate reconstruction of high-resolution details. Numerical experiments with raw MRI data indicate that k-band outperforms two other methods trained on limited-resolution data and performs comparably to state-of-the-art (SoTA) methods trained on high-resolution data. k-band hence obtains SoTA performance, with the advantage of training using only limited-resolution data. This work hence introduces a practical, easy-to-implement, self-supervised training framework, which involves fast acquisition and self-supervised reconstruction and offers theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
尽管深度学习（DL）方法有力量解决反向问题，但它们依赖高质量训练数据是一个主要障碍。在高维度（动态/体积）磁共振成像（MRI）中，获取高分辨率完全采样的k空间数据是不现实的。我们介绍了一种新的数学框架，称之为k带，允许使用仅部分、有限分辨率k空间数据进行训练DL模型。具体来说，我们引入了使用批处理的梯度下降（SGD）在k空间子集上训练。在每个训练迭代中，而不是使用完全采样的k空间来计算梯度，我们只使用一小部分k空间。这种概念 compatible with different sampling strategies; here we demonstrate the method for k-space "bands", which have limited resolution in one dimension and can hence be acquired rapidly. We prove analytically that our method stochastically approximates the gradients computed in a fully-supervised setup, when two simple conditions are met: (i) the limited-resolution axis is chosen randomly-uniformly for every new scan, hence k-space is fully covered across the entire training set, and (ii) the loss function is weighed with a mask, derived here analytically, which facilitates accurate reconstruction of high-resolution details. Numerical experiments with raw MRI data indicate that k-band outperforms two other methods trained on limited-resolution data and performs comparably to state-of-the-art (SoTA) methods trained on high-resolution data. k-band hence obtains SoTA performance, with the advantage of training using only limited-resolution data. This work hence introduces a practical, easy-to-implement, self-supervised training framework, which involves fast acquisition and self-supervised reconstruction and offers theoretical guarantees.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-AI-based-Smart-Contract-Creation"><a href="#An-Empirical-Study-of-AI-based-Smart-Contract-Creation" class="headerlink" title="An Empirical Study of AI-based Smart Contract Creation"></a>An Empirical Study of AI-based Smart Contract Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02955">http://arxiv.org/abs/2308.02955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabimba Karanjai, Edward Li, Lei Xu, Weidong Shi</li>
<li>for: 本研究的主要目标是评估 LLMS 生成的智能合约代码质量。</li>
<li>methods: 我们使用了一个实验设置来评估生成代码的正确性、安全性和效率。</li>
<li>results: 我们发现生成的智能合约代码存在安全漏洞，同时代码质量和正确性受到输入参数的影响。但我们还发现了一些可以改进的方向。<details>
<summary>Abstract</summary>
The introduction of large language models (LLMs) like ChatGPT and Google Palm2 for smart contract generation seems to be the first well-established instance of an AI pair programmer. LLMs have access to a large number of open-source smart contracts, enabling them to utilize more extensive code in Solidity than other code generation tools. Although the initial and informal assessments of LLMs for smart contract generation are promising, a systematic evaluation is needed to explore the limits and benefits of these models. The main objective of this study is to assess the quality of generated code provided by LLMs for smart contracts. We also aim to evaluate the impact of the quality and variety of input parameters fed to LLMs. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted. However, we also identified the areas where it can be improved. The paper also proposes several potential research directions to improve the process, quality and safety of generated smart contract codes.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）如ChatGPT和Google Palm2的出现似乎是首个成功的AI双程程式。LLMs可以使用大量的开源智能合约，使其在Solidity中运用更大的代码。虽然初步评估结果尚未正式，但是需要系统性的评估来探索这些模型的限制和优点。本研究的主要目标是评估LLMs生成的智能合约代码质量。我们还想评估对LLMs输入参数的影响，以及生成代码的有效性、正确性和安全性。为了完成这个目标，我们设计了一个实验室来评估生成代码的有效性、正确性和安全性。我们发现生成的智能合约中有许多安全漏洞，并且代码的质量和正确性受到影响。但是，我们也发现了一些改善这些过程的研究方向。本文还提出了多个可能的研究方向，以提高生成代码的过程、质量和安全性。
</details></li>
</ul>
<hr>
<h2 id="dPASP-A-Comprehensive-Differentiable-Probabilistic-Answer-Set-Programming-Environment-For-Neurosymbolic-Learning-and-Reasoning"><a href="#dPASP-A-Comprehensive-Differentiable-Probabilistic-Answer-Set-Programming-Environment-For-Neurosymbolic-Learning-and-Reasoning" class="headerlink" title="dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning"></a>dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02944">http://arxiv.org/abs/2308.02944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renato Lui Geh, Jonas Gonçalves, Igor Cataneo Silveira, Denis Deratani Mauá, Fabio Gagliardi Cozman</li>
<li>for: 这篇论文旨在提出一种新的宣言型概率逻辑编程框架，用于可微分的神经符号逻辑推理。</li>
<li>methods: 该框架使得可以指定精确的概率模型，包括神经 predicate、逻辑约束和间隔值概率选择，以支持 combining 低级感知（图像、文本等）、通用的推理和（模糊的）统计知识。</li>
<li>results: 论文提出了多种 semantics  для probabilistic logic programs，以表达不确定、矛盾、不完整和&#x2F;或统计知识。同时，也介绍了如何使用神经 predicate 和概率选择进行 gradient-based 学习。论文还描述了一个实现的 package，用于推理和学习，并提供了一些示例程序。<details>
<summary>Abstract</summary>
We present dPASP, a novel declarative probabilistic logic programming framework for differentiable neuro-symbolic reasoning. The framework allows for the specification of discrete probabilistic models with neural predicates, logic constraints and interval-valued probabilistic choices, thus supporting models that combine low-level perception (images, texts, etc), common-sense reasoning, and (vague) statistical knowledge. To support all such features, we discuss the several semantics for probabilistic logic programs that can express nondeterministic, contradictory, incomplete and/or statistical knowledge. We also discuss how gradient-based learning can be performed with neural predicates and probabilistic choices under selected semantics. We then describe an implemented package that supports inference and learning in the language, along with several example programs. The package requires minimal user knowledge of deep learning system's inner workings, while allowing end-to-end training of rather sophisticated models and loss functions.
</details>
<details>
<summary>摘要</summary>
我们提出了dpASP，一种新的宣告性概率逻辑编程框架，用于可 diferenciable 神经符号逻辑推理。该框架允许用户 specify 权重逻辑模型，神经元 predicate，逻辑约束和间隔值概率选择，因此可以支持模型结合低级感知（图像、文本等）、通用理智和（抽象）统计知识。为支持这些特性，我们讨论了几种 probabilistic logic programs 的 semantics，可以表达不确定、矛盾、不完整和/或统计知识。我们还讨论了如何在 selected semantics 下使用 neural predicates 和 probabilistic choices 进行梯度基于学习。然后，我们描述了一个实现的 package，支持推理和学习语言中的各种例程，并提供了一些示例程序。该 package 需要用户具备最低知识量，同时允许用户通过终端训练较复杂的模型和损失函数。
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Development-of-an-Uncertainty-Quantification-Protocol-for-the-Natural-Gas-Industry"><a href="#Towards-the-Development-of-an-Uncertainty-Quantification-Protocol-for-the-Natural-Gas-Industry" class="headerlink" title="Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry"></a>Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02941">http://arxiv.org/abs/2308.02941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babajide Kolade</li>
<li>for: This paper aims to develop a protocol for assessing uncertainties in predictions of machine learning and mechanistic simulation models, specifically for the gas distribution industry.</li>
<li>methods: The protocol outlines an uncertainty quantification workflow that includes identifying key sources of uncertainties, using applicable methods of uncertainty propagation, and employing statistically rational estimators for output uncertainties.</li>
<li>results: The paper applies the protocol to test cases relevant to the gas distribution industry and presents the learnings from its application. The results demonstrate the effectiveness of the protocol in quantifying uncertainties in simulation results.<details>
<summary>Abstract</summary>
Simulations using machine learning (ML) models and mechanistic models are often run to inform decision-making processes. Uncertainty estimates of simulation results are critical to the decision-making process because simulation results of specific scenarios may have wide, but unspecified, confidence bounds that may impact subsequent analyses and decisions. The objective of this work is to develop a protocol to assess uncertainties in predictions of machine learning and mechanistic simulation models. The protocol will outline an uncertainty quantification workflow that may be used to establish credible bounds of predictability on computed quantities of interest and to assess model sufficiency. The protocol identifies key sources of uncertainties in machine learning and mechanistic modeling, defines applicable methods of uncertainty propagation for these sources, and includes statistically rational estimators for output uncertainties. The work applies the protocol to test cases relevant to the gas distribution industry and presents learnings from its application. The paper concludes with a brief discussion outlining a pathway to the wider adoption of uncertainty quantification within the industry
</details>
<details>
<summary>摘要</summary>
模拟使用机器学习（ML）模型和机制模型经常用来支持决策过程。模拟结果中的不确定性估计对决策过程是关键的，因为特定场景的模拟结果可能具有广泛而不确定的信任范围，这可能影响后续分析和决策。本工作的目标是开发一个协议来评估机器学习和机制模型预测结果中的不确定性。协议将 outline一个不确定性评估工作流程，可以用来确定计算量据点的可靠范围和评估模型的充分性。协议列出了机器学习和机制模型中的主要不确定性来源，采用可靠的方法进行不确定性传播，并提供了统计合理的输出不确定性估计器。本工作应用协议到 relevante test cases，并presented learnings from its application。文章 conclude with a brief discussion outlining a pathway to the wider adoption of uncertainty quantification within the industry。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval"><a href="#Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval" class="headerlink" title="Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval"></a>Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02926">http://arxiv.org/abs/2308.02926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Haoxiang-WasedaU/Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval">https://github.com/Haoxiang-WasedaU/Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval</a></li>
<li>paper_authors: Haoxiang Shi, Sumio Fujita, Tetsuya Sakai</li>
<li>for: 提高现代神经信息检索中的领域传递问题的解决方案</li>
<li>methods: 取消consistency filter，使用直接pseudo-labeling、pseudo-相关反馈或无监督关键词生成方法实现一致性自由粗粒度检索</li>
<li>results: 对多个数据集进行了广泛的实验评估，结果显示，使用TextRank基于pseudo relevance feedback方法可以超过其他方法的表现，并且对训练和推理效率具有显著改进。<details>
<summary>Abstract</summary>
Domain transfer is a prevalent challenge in modern neural Information Retrieval (IR). To overcome this problem, previous research has utilized domain-specific manual annotations and synthetic data produced by consistency filtering to finetune a general ranker and produce a domain-specific ranker. However, training such consistency filters are computationally expensive, which significantly reduces the model efficiency. In addition, consistency filtering often struggles to identify retrieval intentions and recognize query and corpus distributions in a target domain. In this study, we evaluate a more efficient solution: replacing the consistency filter with either direct pseudo-labeling, pseudo-relevance feedback, or unsupervised keyword generation methods for achieving consistent filtering-free unsupervised dense retrieval. Our extensive experimental evaluations demonstrate that, on average, TextRank-based pseudo relevance feedback outperforms other methods. Furthermore, we analyzed the training and inference efficiency of the proposed paradigm. The results indicate that filtering-free unsupervised learning can continuously improve training and inference efficiency while maintaining retrieval performance. In some cases, it can even improve performance based on particular datasets.
</details>
<details>
<summary>摘要</summary>
域名转移是现代神经信息检索（IR）中的一个常见挑战。以前的研究曾利用域名特定的手动标注和由consistency filtering生成的 sintetic数据来训练一个通用排名器并生成域名特定的排名器。然而，训练such consistency filters具有计算成本高的问题，这会significantly reduces the model efficiency。 In addition, consistency filtering often struggles to identify retrieval intentions and recognize query and corpus distributions in a target domain。在本研究中，我们评估了一种更有效的解决方案：取代consistency filter with direct pseudo-labeling、pseudo-relevance feedback或Unsupervised keyword generation方法来实现无 filtering-free无监督的排名。我们的广泛的实验评估表明，TextRank-based pseudo relevance feedback在其他方法中表现更好。此外，我们还分析了提议的训练和推理效率。结果表明，filtering-free无监督学习可以不断提高训练和推理效率，同时保持检索性能。在某些 dataset 上，它可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="An-AI-Enabled-Framework-to-Defend-Ingenious-MDT-based-Attacks-on-the-Emerging-Zero-Touch-Cellular-Networks"><a href="#An-AI-Enabled-Framework-to-Defend-Ingenious-MDT-based-Attacks-on-the-Emerging-Zero-Touch-Cellular-Networks" class="headerlink" title="An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks"></a>An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02923">http://arxiv.org/abs/2308.02923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aneeqa Ijaz, Waseem Raza, Hasan Farooq, Marvin Manalastas, Ali Imran</li>
<li>for: 本研究旨在探讨黑客可以通过劣质MDT报告来攻击深度自动化无线网络的新型攻击方式，并对常见网络自动化功能的性能造成负面影响。</li>
<li>methods: 本研究使用机器学习方法来检测和排除劣质MDT报告，并通过一个实验场景证明其效果。</li>
<li>results: 研究发现，劣质MDT报告可以影响网络自动化功能的性能，而且可以通过Machine Learning来检测和排除这些劣质报告。<details>
<summary>Abstract</summary>
Deep automation provided by self-organizing network (SON) features and their emerging variants such as zero touch automation solutions is a key enabler for increasingly dense wireless networks and pervasive Internet of Things (IoT). To realize their objectives, most automation functionalities rely on the Minimization of Drive Test (MDT) reports. The MDT reports are used to generate inferences about network state and performance, thus dynamically change network parameters accordingly. However, the collection of MDT reports from commodity user devices, particularly low cost IoT devices, make them a vulnerable entry point to launch an adversarial attack on emerging deeply automated wireless networks. This adds a new dimension to the security threats in the IoT and cellular networks. Existing literature on IoT, SON, or zero touch automation does not address this important problem. In this paper, we investigate an impactful, first of its kind adversarial attack that can be launched by exploiting the malicious MDT reports from the compromised user equipment (UE). We highlight the detrimental repercussions of this attack on the performance of common network automation functions. We also propose a novel Malicious MDT Reports Identification framework (MRIF) as a countermeasure to detect and eliminate the malicious MDT reports using Machine Learning and verify it through a use-case. Thus, the defense mechanism can provide the resilience and robustness for zero touch automation SON engines against the adversarial MDT attacks
</details>
<details>
<summary>摘要</summary>
深度自动化提供的无需人工控制网络（SON）功能和其相关的零 touched自动化解决方案是现代无线网络和物联网（IoT）的关键驱动力。为实现他们的目标，大多数自动化功能都依赖于推理测试（MDT）报告。MDT报告用于生成网络状态和性能的推理，并在运行时动态地改变网络参数。然而，从低成本IoT设备收集MDT报告，特别是从受到攻击的用户设备，使得这些报告成为攻击 deeply automated 无线网络的易受攻击点。这添加了一个新的安全隐患，并且现有的相关文献不具备对这一重要问题的讨论。在这篇论文中，我们研究了一种新型的攻击，可以通过恶意MDT报告来发动，从恶意用户设备收集MDT报告。我们强调了这种攻击对常见网络自动化功能的不良影响。此外，我们还提出了一种新的恶意MDT报告识别框架（MRIF），用于检测和消除恶意MDT报告。我们使用机器学习来实现这一目标，并通过用例验证了这种防御机制的有效性。因此，这种防御机制可以为零 touched自动化 SON 引擎提供鲜活性和鲜活性。
</details></li>
</ul>
<hr>
<h2 id="Structured-Low-Rank-Tensors-for-Generalized-Linear-Models"><a href="#Structured-Low-Rank-Tensors-for-Generalized-Linear-Models" class="headerlink" title="Structured Low-Rank Tensors for Generalized Linear Models"></a>Structured Low-Rank Tensors for Generalized Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02922">http://arxiv.org/abs/2308.02922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Batoul Taki, Anand D. Sarwate, Waheed U. Bajwa</li>
<li>for: This paper is written for researchers and practitioners interested in exploring tensor-based methods for generalized linear model (GLM) problems, particularly those dealing with low-rank tensor structures.</li>
<li>methods: The paper proposes a new low-rank tensor model called the Low Separation Rank (LSR) model, which is imposed onto the coefficient tensor in GLM problems. The authors also develop a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs.</li>
<li>results: The paper derives a minimax lower bound on the error threshold for estimating the coefficient tensor in LSR tensor GLM problems, which suggests that the sample complexity of the proposed method may be significantly lower than that of vectorized GLMs. The authors also demonstrate the efficacy of the proposed LSR tensor model on synthetic and real-world datasets.<details>
<summary>Abstract</summary>
Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model -- which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model -- is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vectorized GLMs. This result can also be specialised to lower bound the estimation error in CP and Tucker-structured GLMs. The derived bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.
</details>
<details>
<summary>摘要</summary>
近期研究表明，在回归问题中强制维度结构 onto 参数矩阵可以导致更可靠的参数估计和较低的样本复杂度，相比vector化方法。这个工作 investigate一种新的低级别维度模型（LSR）在泛化线性模型（GLM）问题中。LSR模型是Tucker和CANDECOMP/PARAFAC（CP）模型的推广，并是阻塞矩阵分解（BTD）模型的特殊情况。这个工作提出了一种块坐标极下降算法 для GLM问题中LSR结构的参数估计。最重要的是，它 derivates一个最小最大下界对于GLM问题中LSR结构参数估计的误差阈值。这个下界与LSR结构参数估计的内在度度相关，表明其样本复杂度可能远低于vector化GLM的样本复杂度。这个结果还可以特殊化为对CP和Tucker结构GLM的参数估计误差的下界。 derivates bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.
</details></li>
</ul>
<hr>
<h2 id="Spectral-Ranking-Inferences-based-on-General-Multiway-Comparisons"><a href="#Spectral-Ranking-Inferences-based-on-General-Multiway-Comparisons" class="headerlink" title="Spectral Ranking Inferences based on General Multiway Comparisons"></a>Spectral Ranking Inferences based on General Multiway Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02918">http://arxiv.org/abs/2308.02918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqing Fan, Zhipeng Lou, Weichen Wang, Mengxin Yu</li>
<li>For:  This paper studies the performance of the spectral method in estimating and quantifying uncertainty of unobserved preference scores in a very general and realistic setup.* Methods: The paper uses the spectral method, which is a more general and flexible approach than the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. The paper also introduces a two-step spectral method that can achieve the same asymptotic efficiency as the Maximum Likelihood Estimator (MLE).* Results: The paper provides comprehensive frameworks for one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It also introduces effective two-sample rank testing methods that are noteworthy. The paper substantiates its findings via comprehensive numerical simulations and applies its developed methodologies to perform statistical inferences on statistics journals and movie rankings.<details>
<summary>Abstract</summary>
This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that it is the first time effective two-sample rank testing methods are proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences on statistics journals and movie rankings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adversarial-Erasing-with-Pruned-Elements-Towards-Better-Graph-Lottery-Ticket"><a href="#Adversarial-Erasing-with-Pruned-Elements-Towards-Better-Graph-Lottery-Ticket" class="headerlink" title="Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket"></a>Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02916">http://arxiv.org/abs/2308.02916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyuwen0627/ace-glt">https://github.com/wangyuwen0627/ace-glt</a></li>
<li>paper_authors: Yuwen Wang, Shunyu Liu, Kaixuan Chen, Tongtian Zhu, Ji Qiao, Mengjie Shi, Yuanyu Wan, Mingli Song</li>
<li>for:  Mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance.</li>
<li>methods:  Combination of core subgraph and sparse subnetwork, adversarial complementary erasing (ACE) framework to explore valuable information from pruned components.</li>
<li>results:  Outperforms existing methods for searching Graph Lottery Ticket (GLT) in diverse tasks.Here’s the summary in English for reference:</li>
<li>for: Mitigating the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance.</li>
<li>methods: Combining core subgraph and sparse subnetwork, using an adversarial complementary erasing (ACE) framework to explore valuable information from pruned components.</li>
<li>results: Outperforming existing methods for searching Graph Lottery Ticket (GLT) in diverse tasks.<details>
<summary>Abstract</summary>
Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main idea is to mine valuable information from pruned edges/weights after each round of IMP, and employ the ACE technique to refine the GLT processing. Finally, experimental results demonstrate that our ACE-GLT outperforms existing methods for searching GLT in diverse tasks. Our code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
《图lottery票（GLT）》，一种组合核心子图和稀疏子网络的方法，用于降低深度图神经网络（GNNs）在大输入图上的计算成本，保持原始性能。然而，现有的研究中的赢家GLT是通过迭代矩阵优化（IMP）而不是重新评估和重新考虑被截割的信息，这会忽略图/模型结构截割中 Edge/权重的动态变化，因此限制了赢家票的吸引力。在这篇论文中，我们提出一个假设，即现有的被遗弃的有价信息在截割的图连接和模型参数中，可以重新组织成GLT，以提高最终性能。 Specifically, we propose an adversarial complementary erasing（ACE）框架，用于探索截割后每个回合的有价信息，并使用ACE技术来细化GLT处理。最后，我们的实验结果表明，我们的ACE-GLT在多种任务中超过现有方法搜索GLT的性能。我们的代码将公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/06/cs.LG_2023_08_06/" data-id="clluro5jt005fq9881zgw3pts" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/07/eess.IV_2023_08_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.IV - 2023-08-07 17:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/06/cs.SD_2023_08_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-06 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
