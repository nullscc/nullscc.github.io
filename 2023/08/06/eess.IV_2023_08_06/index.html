
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-06 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03033 repo_url: https:&#x2F;&#x2F;github.com&#x2F;wangchx67&#x2F;fourllie paper_authors: Chenxi Wang, H">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-06 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/06/eess.IV_2023_08_06/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03033 repo_url: https:&#x2F;&#x2F;github.com&#x2F;wangchx67&#x2F;fourllie paper_authors: Chenxi Wang, H">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-05T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:26.139Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/eess.IV_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-06 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FourLLIE-Boosting-Low-Light-Image-Enhancement-by-Fourier-Frequency-Information"><a href="#FourLLIE-Boosting-Low-Light-Image-Enhancement-by-Fourier-Frequency-Information" class="headerlink" title="FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information"></a>FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03033">http://arxiv.org/abs/2308.03033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangchx67/fourllie">https://github.com/wangchx67/fourllie</a></li>
<li>paper_authors: Chenxi Wang, Hongjun Wu, Zhi Jin</li>
<li>for: 提高低光照图像的亮度和细节</li>
<li>methods: 基于快执行 fourier 变换，利用振荡频率信息和地图信息，实现低光照图像的进一步优化</li>
<li>results: 与当前最佳方法进行比较，实现了更高的亮度和细节精度，同时具有较好的模型效率<details>
<summary>Abstract</summary>
Recently, Fourier frequency information has attracted much attention in Low-Light Image Enhancement (LLIE). Some researchers noticed that, in the Fourier space, the lightness degradation mainly exists in the amplitude component and the rest exists in the phase component. By incorporating both the Fourier frequency and the spatial information, these researchers proposed remarkable solutions for LLIE. In this work, we further explore the positive correlation between the magnitude of amplitude and the magnitude of lightness, which can be effectively leveraged to improve the lightness of low-light images in the Fourier space. Moreover, we find that the Fourier transform can extract the global information of the image, and does not introduce massive neural network parameters like Multi-Layer Perceptrons (MLPs) or Transformer. To this end, a two-stage Fourier-based LLIE network (FourLLIE) is proposed. In the first stage, we improve the lightness of low-light images by estimating the amplitude transform map in the Fourier space. In the second stage, we introduce the Signal-to-Noise-Ratio (SNR) map to provide the prior for integrating the global Fourier frequency and the local spatial information, which recovers image details in the spatial space. With this ingenious design, FourLLIE outperforms the existing state-of-the-art (SOTA) LLIE methods on four representative datasets while maintaining good model efficiency.
</details>
<details>
<summary>摘要</summary>
最近，傅里叶频率信息在低光照图像增强（LLIE）中吸引了很多关注。一些研究人员注意到，在傅里叶空间中，亮度下降主要存在于振荡Component中，而剩下的存在于相位Component中。通过汇合傅里叶频率和空间信息，这些研究人员提出了非常出色的解决方案。在这项工作中，我们进一步探索了振荡幅度与亮度幅度之间的正相关关系，可以有效地提高低光照图像的亮度在傅里叶空间中。此外，我们发现傅里叶变换可以提取图像的全局信息，而不需要大量的神经网络参数，比如多层感知器（MLP）或转换器。基于这种创新的设计，我们提出了一种两个阶段的傅里叶基于LLIE网络（FourLLIE）。在第一阶段，我们使用傅里叶变换Map来提高低光照图像的亮度。在第二阶段，我们引入信噪比Map，以提供优化全局傅里叶频率和本地空间信息的优化约束，从而恢复图像的细节在空间空间中。与现有的状态的 искусственный智能（SOTA）LLIE方法相比，FourLLIE在四个代表性的数据集上达到了更高的性能，而且保持了好的模型效率。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Spike-based-Image-Restoration-under-General-Illumination"><a href="#Recurrent-Spike-based-Image-Restoration-under-General-Illumination" class="headerlink" title="Recurrent Spike-based Image Restoration under General Illumination"></a>Recurrent Spike-based Image Restoration under General Illumination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03018">http://arxiv.org/abs/2308.03018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bit-vision/rsir">https://github.com/bit-vision/rsir</a></li>
<li>paper_authors: Lin Zhu, Yunlong Zheng, Mengyue Geng, Lizhi Wang, Hua Huang</li>
<li>for: 提高雨天或晚上场景下的激活图像重建速度</li>
<li>methods: 基于物理模型建立噪声分布，并设计了适应性激光变换模块、回归时间特征融合模块和频率域激光噪声除净模块</li>
<li>results: 对实际 dataset进行了广泛的实验，并在不同照明条件下达到了效果的重建图像Here’s a breakdown of each line:</li>
<li>for: 这篇论文的目的是提高雨天或晚上场景下的激活图像重建速度。</li>
<li>methods: 这篇论文使用了基于物理模型建立噪声分布，并设计了适应性激光变换模块、回归时间特征融合模块和频率域激光噪声除净模块来实现图像重建。</li>
<li>results: 这篇论文对实际 dataset进行了广泛的实验，并在不同照明条件下达到了效果的重建图像。<details>
<summary>Abstract</summary>
Spike camera is a new type of bio-inspired vision sensor that records light intensity in the form of a spike array with high temporal resolution (20,000 Hz). This new paradigm of vision sensor offers significant advantages for many vision tasks such as high speed image reconstruction. However, existing spike-based approaches typically assume that the scenes are with sufficient light intensity, which is usually unavailable in many real-world scenarios such as rainy days or dusk scenes. To unlock more spike-based application scenarios, we propose a Recurrent Spike-based Image Restoration (RSIR) network, which is the first work towards restoring clear images from spike arrays under general illumination. Specifically, to accurately describe the noise distribution under different illuminations, we build a physical-based spike noise model according to the sampling process of the spike camera. Based on the noise model, we design our RSIR network which consists of an adaptive spike transformation module, a recurrent temporal feature fusion module, and a frequency-based spike denoising module. Our RSIR can process the spike array in a recursive manner to ensure that the spike temporal information is well utilized. In the training process, we generate the simulated spike data based on our noise model to train our network. Extensive experiments on real-world datasets with different illuminations demonstrate the effectiveness of the proposed network. The code and dataset are released at https://github.com/BIT-Vision/RSIR.
</details>
<details>
<summary>摘要</summary>
新型生物启发式视觉传感器“穿孔相机”记录了光Intensity的形式为高时间分辨率（20,000 Hz）的脉冲数组。这种新的视觉传感器模式具有许多视觉任务的优势，如高速图像重建。然而，现有的脉冲基本approaches通常假设场景中有足够的光INTENSITY，这通常不存在在真实世界中的雨天或晚上场景。为了解锁更多的脉冲基本应用场景，我们提议了一种基于脉冲的图像修复网络（RSIR），这是首次对脉冲数组进行图像修复。 Specifically, we build a physical-based spike noise model according to the sampling process of the spike camera to accurately describe the noise distribution under different illuminations. Based on the noise model, we design our RSIR network, which consists of an adaptive spike transformation module, a recurrent temporal feature fusion module, and a frequency-based spike denoising module. Our RSIR can process the spike array in a recursive manner to ensure that the spike temporal information is well utilized. In the training process, we generate the simulated spike data based on our noise model to train our network. Extensive experiments on real-world datasets with different illuminations demonstrate the effectiveness of the proposed network.  The code and dataset are released at https://github.com/BIT-Vision/RSIR.
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Vision-Transformers-for-Pixel-Level-Identification-of-Structural-Components-and-Damage"><a href="#High-Resolution-Vision-Transformers-for-Pixel-Level-Identification-of-Structural-Components-and-Damage" class="headerlink" title="High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage"></a>High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03006">http://arxiv.org/abs/2308.03006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kareem Eltouny, Seyedomid Sajedi, Xiao Liang</li>
<li>for: 这个研究是为了提高桥梁检查的效率和可靠性，使用无人机和人工智能技术来快速和安全地进行视觉检查。</li>
<li>methods: 该研究使用了基于视Transformer和 Laplacian pyramids scaling networks的semantic segmentation网络，来高效地分析高分辨率的视觉检查图像。</li>
<li>results: 研究结果表明，该方法可以高效地分析大量的高分辨率视觉检查图像，同时保持了地方细节和全局 semantics信息，不会对计算效率造成影响。<details>
<summary>Abstract</summary>
Visual inspection is predominantly used to evaluate the state of civil structures, but recent developments in unmanned aerial vehicles (UAVs) and artificial intelligence have increased the speed, safety, and reliability of the inspection process. In this study, we develop a semantic segmentation network based on vision transformers and Laplacian pyramids scaling networks for efficiently parsing high-resolution visual inspection images. The massive amounts of collected high-resolution images during inspections can slow down the investigation efforts. And while there have been extensive studies dedicated to the use of deep learning models for damage segmentation, processing high-resolution visual data can pose major computational difficulties. Traditionally, images are either uniformly downsampled or partitioned to cope with computational demands. However, the input is at risk of losing local fine details, such as thin cracks, or global contextual information. Inspired by super-resolution architectures, our vision transformer model learns to resize high-resolution images and masks to retain both the valuable local features and the global semantics without sacrificing computational efficiency. The proposed framework has been evaluated through comprehensive experiments on a dataset of bridge inspection report images using multiple metrics for pixel-wise materials detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用视觉检查来评估公共建筑结构，但是最新的无人机（UAV）和人工智能技术已经提高了检查过程的速度、安全性和可靠性。在这项研究中，我们开发了基于视transformer和Laplacian pyramids scaling网络的semantic segmentation网络，用于高效地分解高分辨率视检图像。收集的大量高分辨率图像可能会拖slow down调查工作，而且过去对深度学习模型用于损害分 segmentation的研究非常广泛。但是处理高分辨率视数据可以带来巨大的计算困难。传统上，图像会被uniform downsample或分割，以降低计算成本，但是输入可能会产生本地细小损害，例如细裂，或者全局Contextual信息。受到超分辨architecture的启发，我们的视transformer模型学习了resize高分辨率图像和mask，以保留valuable的本地特征和全局semantics，不会 sacrificing计算效率。我们提出的框架已经在bridge检查报告图像集上进行了完整的实验，并使用多个 метри来进行像素精度检测。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-segmentation-of-intracranial-aneurysms-using-a-3D-focal-modulation-UNet"><a href="#Weakly-supervised-segmentation-of-intracranial-aneurysms-using-a-3D-focal-modulation-UNet" class="headerlink" title="Weakly supervised segmentation of intracranial aneurysms using a 3D focal modulation UNet"></a>Weakly supervised segmentation of intracranial aneurysms using a 3D focal modulation UNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03001">http://arxiv.org/abs/2308.03001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Rasoulian, Soorena Salari, Yiming Xiao<br>for: 这 paper 的目的是提供一种高精度的自动化UIA诊断方法，以便改善Current assessment based on 2D manual measures of aneurysms on 3D MRA的评估方法。methods: 该 paper 使用了一种名为 FocalSegNet 的新型3D焦点调制UNet，以及一种名为 CRF 的后处理技术，来实现高精度的 UIA 分 segmentation。results: 该 paper 的实验结果表明，提案的算法比 state-of-the-art 3D UNet 和 Swin-UNETR 更高精度，并且 demonstarted the superiority of the proposed FocalSegNet 和 focal modulation 的 beneficial effect on the task。<details>
<summary>Abstract</summary>
Accurate identification and quantification of unruptured intracranial aneurysms (UIAs) are essential for the risk assessment and treatment decisions of this cerebrovascular disorder. Current assessment based on 2D manual measures of aneurysms on 3D magnetic resonance angiography (MRA) is sub-optimal and time-consuming. Automatic 3D measures can significantly benefit the clinical workflow and treatment outcomes. However, one major issue in medical image segmentation is the need for large well-annotated data, which can be expensive to obtain. Techniques that mitigate the requirement, such as weakly supervised learning with coarse labels are highly desirable. In this paper, we leverage coarse labels of UIAs from time-of-flight MRAs to obtain refined UIAs segmentation using a novel 3D focal modulation UNet, called FocalSegNet and conditional random field (CRF) postprocessing, with a Dice score of 0.68 and 95% Hausdorff distance of 0.95 mm. We evaluated the performance of the proposed algorithms against the state-of-the-art 3D UNet and Swin-UNETR, and demonstrated the superiority of the proposed FocalSegNet and the benefit of focal modulation for the task.
</details>
<details>
<summary>摘要</summary>
correctly 识别和量化脑血管疾病（UIAs）的精度是诊断和治疗决策的关键。现有的评估方法基于2D手动测量的感知器件图像（MRA）是下pecific和耗时consuming。自动化3D测量可以帮助优化诊断和治疗结果。然而，医疗图像分割的一个主要问题是需要大量的良好标注数据，这可以是expensive to obtain。使用弱有supervised learning with coarse labels可以减少这个问题。在这篇论文中，我们利用时间飞行扫描产生的UIAs粗略标签来获得改进的UIAs分割，使用一种新的3D焦点修饰UNet，called FocalSegNet，并与条件Random field（CRF）后处理，得到了0.68的Dice分数和0.95 mm的95% Hausdorff距离。我们评估了提议的算法与现有的3D UNet和Swin-UNETR的性能，并证明了提议的FocalSegNet的优越性和焦点修饰的好处。
</details></li>
</ul>
<hr>
<h2 id="DermoSegDiff-A-Boundary-aware-Segmentation-Diffusion-Model-for-Skin-Lesion-Delineation"><a href="#DermoSegDiff-A-Boundary-aware-Segmentation-Diffusion-Model-for-Skin-Lesion-Delineation" class="headerlink" title="DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation"></a>DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02959">http://arxiv.org/abs/2308.02959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mindflow-institue/dermosegdiff">https://github.com/mindflow-institue/dermosegdiff</a></li>
<li>paper_authors: Afshin Bozorgpour, Yousef Sadegheih, Amirhossein Kazerouni, Reza Azad, Dorit Merhof</li>
<li>for: 静脉皮肤病诊断 early detection 和准确诊断</li>
<li>methods: 利用 Diffusion Probabilistic Models (DDPMs) 进行静脉皮肤病诊断, 并在学习过程中加入边缘信息</li>
<li>results: 对多个皮肤分割数据集进行实验，显示 DermoSegDiff 的效果和泛化能力都较为出色，超过了现有的 CNN、transformer 和 diffusion-based 方法<details>
<summary>Abstract</summary>
Skin lesion segmentation plays a critical role in the early detection and accurate diagnosis of dermatological conditions. Denoising Diffusion Probabilistic Models (DDPMs) have recently gained attention for their exceptional image-generation capabilities. Building on these advancements, we propose DermoSegDiff, a novel framework for skin lesion segmentation that incorporates boundary information during the learning process. Our approach introduces a novel loss function that prioritizes the boundaries during training, gradually reducing the significance of other regions. We also introduce a novel U-Net-based denoising network that proficiently integrates noise and semantic information inside the network. Experimental results on multiple skin segmentation datasets demonstrate the superiority of DermoSegDiff over existing CNN, transformer, and diffusion-based approaches, showcasing its effectiveness and generalization in various scenarios. The implementation is publicly accessible on \href{https://github.com/mindflow-institue/dermosegdiff}{GitHub}
</details>
<details>
<summary>摘要</summary>
皮肤损害分割在诊断皮肤疾病的早期阶段发挥了关键作用。近年来，Diffusion Probabilistic Models（DDPMs）在图像生成方面受到了广泛关注。我们基于这些进步，提出了DermoSegDiff，一种新的皮肤损害分割框架。我们的方法引入了一个新的损失函数，在训练过程中优先级化边界信息，逐渐减少其他区域的重要性。我们还引入了一种基于U-Net的杂音级别网络，可以高效地 интеGRATE噪音和semantic信息内网络。多个皮肤分割数据集的实验结果表明，DermoSegDiff在不同的场景下对现有的CNN、transformer和Diffusion-based方法优于，demonstrating its effectiveness and generalization。实现可以在 \href{https://github.com/mindflow-institue/dermosegdiff}{GitHub} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MomentaMorph-Unsupervised-Spatial-Temporal-Registration-with-Momenta-Shooting-and-Correction"><a href="#MomentaMorph-Unsupervised-Spatial-Temporal-Registration-with-Momenta-Shooting-and-Correction" class="headerlink" title="MomentaMorph: Unsupervised Spatial-Temporal Registration with Momenta, Shooting, and Correction"></a>MomentaMorph: Unsupervised Spatial-Temporal Registration with Momenta, Shooting, and Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02949">http://arxiv.org/abs/2308.02949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangxing Bian, Shuwen Wei, Yihao Liu, Junyu Chen, Jiachen Zhuo, Fangxu Xing, Jonghye Woo, Aaron Carass, Jerry L. Prince</li>
<li>for: 用于估计大量运动和复杂征 patrerns 中的� MR 影像中的运动</li>
<li>methods: 基于 Lie 代数和 Lie 组 principls 的 “势量、射击、修正” 框架，以快速尝试到真正的 optima，并确保收敛到真正的 optima</li>
<li>results: 在 synthetic 数据集和实际 3D tMRI 数据集上，方法能够高效地估计精准、密集、 diffeomorphic 2D&#x2F;3D 运动场I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Tagged magnetic resonance imaging (tMRI) has been employed for decades to measure the motion of tissue undergoing deformation. However, registration-based motion estimation from tMRI is difficult due to the periodic patterns in these images, particularly when the motion is large. With a larger motion the registration approach gets trapped in a local optima, leading to motion estimation errors. We introduce a novel "momenta, shooting, and correction" framework for Lagrangian motion estimation in the presence of repetitive patterns and large motion. This framework, grounded in Lie algebra and Lie group principles, accumulates momenta in the tangent vector space and employs exponential mapping in the diffeomorphic space for rapid approximation towards true optima, circumventing local optima. A subsequent correction step ensures convergence to true optima. The results on a 2D synthetic dataset and a real 3D tMRI dataset demonstrate our method's efficiency in estimating accurate, dense, and diffeomorphic 2D/3D motion fields amidst large motion and repetitive patterns.
</details>
<details>
<summary>摘要</summary>
带标记的核磁共振成像（tMRI）已经在数十年中用于测量软组织中的运动。然而，基于匹配的运动估计从tMRI中很难进行注册，尤其是当运动较大时。当运动较大时，匹配方法会被困在地方最优点中，导致运动估计错误。我们介绍了一种新的“动量、射击和修正”框架，用于在具有循环Patterns和大运动时进行劳动动量估计。这个框架基于李 álgebra和李群原理，在 Tangent vector space中积累动量，使用 экспоненциаль映射在 diffeomorphic 空间中快速 Approximate towards true optima， circumventing local optima。后续的修正步骤确保了 converges to true optima。Synthetic dataset和实际的3D tMRI dataset results demonstrate our method's efficiency in estimating accurate, dense, and diffeomorphic 2D/3D motion fields amidst large motion and repetitive patterns.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/06/eess.IV_2023_08_06/" data-id="clluro5m600diq9884pv436sf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/06/cs.SD_2023_08_06/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-06 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/05/cs.LG_2023_08_05/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-05 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
