
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.SD - 2023-08-07 123:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03684 repo_url: None paper_authors: Dongyuan Shi, Woon-Se">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.SD - 2023-08-07 123:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/07/cs.SD_2023_08_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03684 repo_url: None paper_authors: Dongyuan Shi, Woon-Se">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-06T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:26.553Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.SD_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.SD_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.SD - 2023-08-07 123:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm"><a href="#Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm" class="headerlink" title="Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm"></a>Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03684">http://arxiv.org/abs/2308.03684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyuan Shi, Woon-Seng Gan, Bhan Lam, Shulin Wen, Xiaoyi Shen</li>
<li>for: 实现多通道活动噪声控制（MCANC）中的广泛噪声抑制区域。</li>
<li>methods: 使用了Filter-x最小均方（FxLMS）算法，但是它的对应速度较慢，不适合处理快速变化的噪声，如堆积噪声。此外，噪声功率的变化也会对算法的稳定性产生负面影响。</li>
<li>results: 通过与振踪方法结合，实现了对MCANC中的噪声控制的有效控制，并且加速了算法的步进调整。在实际应用中，通过使用多通道噪声控制窗口来控制机器噪声。<details>
<summary>Abstract</summary>
Multichannel active noise control (MCANC) is widely utilized to achieve significant noise cancellation area in the complicated acoustic field. Meanwhile, the filter-x least mean square (FxLMS) algorithm gradually becomes the benchmark solution for the implementation of MCANC due to its low computational complexity. However, its slow convergence speed more or less undermines the performance of dealing with quickly varying disturbances, such as piling noise. Furthermore, the noise power variation also deteriorates the robustness of the algorithm when it adopts the fixed step size. To solve these issues, we integrated the normalized multichannel FxLMS with the momentum method, which hence, effectively avoids the interference of the primary noise power and accelerates the convergence of the algorithm. To validate its effectiveness, we deployed this algorithm in a multichannel noise control window to control the real machine noise.
</details>
<details>
<summary>摘要</summary>
多通道活动噪声控制（MCANC）广泛应用于复杂的噪声场中实现显著的噪声抑制面积。同时，Filter-x最小二乘（FxLMS）算法逐渐成为MCANC的实现标准方案，主要是因为它的计算复杂度较低。然而，它的慢速对应变化的干扰有很大的影响，如堆叠噪声。此外，噪声功率变化也会对算法的稳定性产生负面影响，特别是当采用固定步长时。为解决这些问题，我们将normalized multichannel FxLMS与旋转方法相结合，从而有效避免了主要噪声功率的干扰，并加速了算法的收敛速度。为验证其效果，我们在多通道噪声控制窗口中应用了这种算法，控制了实际机器的噪声。
</details></li>
</ul>
<hr>
<h2 id="AudioVMAF-Audio-Quality-Prediction-with-VMAF"><a href="#AudioVMAF-Audio-Quality-Prediction-with-VMAF" class="headerlink" title="AudioVMAF: Audio Quality Prediction with VMAF"></a>AudioVMAF: Audio Quality Prediction with VMAF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03437">http://arxiv.org/abs/2308.03437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Biswas, Harald Mundt</li>
<li>for: 这个论文的目的是提出一种基于现有VMAF的听力 inspirited frontend，用于创建参考视频和编码spectrograms，并扩展VMAF以测试编码音质。</li>
<li>methods: 该系统使用了图像复制来进一步提高预测精度，特别是在存在带限 anchors 时。</li>
<li>results: 提议方法在现有视觉质量特征的抽取改进下显著 OUTPERFORMS 所有现有的视觉质量特征重新定义为音频质量特征，并且在一个专门为音频质量metric（ViSQOL-v3 [4]）也 inspirited from the image domain 上显示了7.8%和2.0%的Pearson和Spearman排名相关度系数的显著提高。<details>
<summary>Abstract</summary>
Video Multimethod Assessment Fusion (VMAF) [1], [2], [3] is a popular tool in the industry for measuring coded video quality. In this study, we propose an auditory-inspired frontend in existing VMAF for creating videos of reference and coded spectrograms, and extended VMAF for measuring coded audio quality. We name our system AudioVMAF. We demonstrate that image replication is capable of further enhancing prediction accuracy, especially when band-limited anchors are present. The proposed method significantly outperforms all existing visual quality features repurposed for audio, and even demonstrates a significant overall improvement of 7.8% and 2.0% of Pearson and Spearman rank correlation coefficient, respectively, over a dedicated audio quality metric (ViSQOL-v3 [4]) also inspired from the image domain.
</details>
<details>
<summary>摘要</summary>
видео多方法评估融合（VMAF）[1], [2], [3] 是行业中常用的视频质量测试工具。在这项研究中，我们提议在现有VMAF中添加音频引入的前端，并将扩展VMAF用于测试编码音频质量。我们称之为AudioVMAF。我们发现，图像复制可以进一步提高预测精度，特别是当存在带限 anchors 时。我们的方法在现有视觉质量特征的抽取方面进行了改进，并且显著超过了所有抽取于音频领域的视觉质量特征，以及专门为音频质量指标（ViSQOL-v3 [4]) 的7.8%和2.0%的普森和斯宾塞相关系数。
</details></li>
</ul>
<hr>
<h2 id="Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation"><a href="#Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation" class="headerlink" title="Improving Deep Attractor Network by BGRU and GMM for Speech Separation"></a>Improving Deep Attractor Network by BGRU and GMM for Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03332">http://arxiv.org/abs/2308.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rawad Melhem, Assef Jafar, Riad Hamadeh</li>
<li>for: 提高Speech separation技术的简化和效率，使其更适合实际应用。</li>
<li>methods: 使用Bidirectional Gated neural network (BGRU)取代BLSTM，并使用Gaussian Mixture Model (GMM)作为聚类算法。</li>
<li>results: 在TIMIT corpus上评估系统，SDR和PESQ scores分别为12.3 dB和2.94，比原始DANet模型更好，同时减少了20.7%和17.9%的参数和训练时间。<details>
<summary>Abstract</summary>
Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.
</details>
<details>
<summary>摘要</summary>
深度吸引网络（DANet）是语音分离领域的Current Technique，使用双向长短期记忆（BLSTM），但模型复杂度很高。本文提出了简化了的DANet模型，使用双向闭合神经网络（BGRU）而不是BLSTM。在DANet中使用 Gaussian Mixture Model（GMM）作为聚类算法，以降低复杂度并提高学习速度和准确性。用于评价模型的度量包括Signal to Distortion Ratio（SDR）、Signal to Interference Ratio（SIR）、Signal to Artifact Ratio（SAR）以及Perceptual Evaluation Speech Quality（PESQ）分数。使用TIMIT corpus中的两个说话者混合数据集评估提出的模型，系统实现了12.3 dB和2.94的SDR和PESQ分数，分别比原始DANet模型更好。此外，模型的参数数量和训练时间都下降了20.7%和17.9%。模型应用于混合阿拉伯语音信号上，结果比英语更好。
</details></li>
</ul>
<hr>
<h2 id="SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability"><a href="#SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability" class="headerlink" title="SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability"></a>SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03266">http://arxiv.org/abs/2308.03266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/r1ckshi/seaco-paraformer">https://github.com/r1ckshi/seaco-paraformer</a></li>
<li>paper_authors: Xian Shi, Yexin Yang, Zerui Li, Shiliang Zhang</li>
<li>for: 实现ASR系统中的热词自定义，提高ASR系统的准确性和效率。</li>
<li>methods: 提出了Semantic-augmented Contextual-Paraformer（SeACo-Paraformer）模型，结合了AED-based模型的精度、NAR模型的效率，并且具有出色的Contextualization能力。</li>
<li>results: 在50,000小时的工业大数据实验中，提出的模型比强基准模型在自定义和一般ASR任务中表现更好，并且探索了一种高效的大规模来处理热词检查的方法。<details>
<summary>Abstract</summary>
Hotword customization is one of the important issues remained in ASR field - it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial models proposed and compared are all opened as well as two hotword test sets.
</details>
<details>
<summary>摘要</summary>
<SYS>    <LANGUAGE_MODEL>        <NAME>Simplified Chinese</NAME>        <PATH>/path/to/model</PATH>    </LANGUAGE_MODEL></SYS>这是一个 ASR 领域中的重要问题 - 允许使用者自定义名词、人名和其他短语。过去几年，有内在和外在模型化策略被开发出来解决这个问题。这些方法优秀地表现，但仍然存在一些缺陷，如效果不稳定。在这篇文章中，我们提出 Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) 一种新的 NAR 基于 ASR 系统，具有轻松实现和有效的自定义热词能力。它结合了 AED-based 模型的精度和 NAR 模型的效率，并且在Contextualization 方面表现出色。在50,000小时的工业大数据中，我们的提案模型比强大的基准模型在自定义和一般 ASR 任务中表现出色。此外，我们还探索了一种高效的方法来筛选大规模的进来热词，以进一步提高效能。我们提供了所有的代码和工业模型，以及两个热词测试集。
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals"><a href="#Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals" class="headerlink" title="Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals"></a>Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03226">http://arxiv.org/abs/2308.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Farhad Javanmardi, Paavo Alku</li>
<li>for: 本研究旨在 investigate the effectiveness of simultaneously-recorded speech and neck surface accelerometer (NSA) signals in the classification of voice quality (breathy, modal, and pressed) using deep learning-based features and a support vector machine (SVM) as well as a convolutional neural network (CNN) as classifiers.</li>
<li>methods: 本研究使用了三种自动学习模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）的自然语言处理特征，以及SVM和CNN分类器。另外，使用了两种信号处理方法（闭合相位滤波和零频滤波）来估计颤腔源波形从语音和NSA信号中提取的特征。</li>
<li>results: 研究发现NSA输入对分类任务的性能更高于语音输入。另外，使用自动学习模型生成的特征对于语音和NSA输入都显示了更高的分类精度，而使用HuBERT特征则表现更好。<details>
<summary>Abstract</summary>
Prior studies in the automatic classification of voice quality have mainly studied the use of the acoustic speech signal as input. Recently, a few studies have been carried out by jointly using both speech and neck surface accelerometer (NSA) signals as inputs, and by extracting MFCCs and glottal source features. This study examines simultaneously-recorded speech and NSA signals in the classification of voice quality (breathy, modal, and pressed) using features derived from three self-supervised pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and using a SVM as well as CNNs as classifiers. Furthermore, the effectiveness of the pre-trained models is compared in feature extraction between glottal source waveforms and raw signal waveforms for both speech and NSA inputs. Using two signal processing methods (quasi-closed phase (QCP) glottal inverse filtering and zero frequency filtering (ZFF)), glottal source waveforms are estimated from both speech and NSA signals. The study has three main goals: (1) to study whether features derived from pre-trained models improve classification accuracy compared to conventional features (spectrogram, mel-spectrogram, MFCCs, i-vector, and x-vector), (2) to investigate which of the two modalities (speech vs. NSA) is more effective in the classification task with pre-trained model-based features, and (3) to evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier. The results revealed that the use of the NSA input showed better classification performance compared to the speech signal. Between the features, the pre-trained model-based features showed better classification accuracies, both for speech and NSA inputs compared to the conventional features. It was also found that the HuBERT features performed better than the wav2vec2-BASE and wav2vec2-LARGE features.
</details>
<details>
<summary>摘要</summary>
前研究主要是使用语音信号作为输入，做自动识别声音质量的研究。近年来，一些研究开始将语音信号和脖子表面加速器（NSA）信号同时录制，并提取MFCCs和喉咙源特征。本研究用三个自我超vised模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）提取特征，并使用支持向量机（SVM）和卷积神经网络（CNN）作为分类器。此外，对于语音和NSA输入，对喉咙源波形和原始信号波形进行预处理，并使用 quasi-closed phase（QCP）预测和零频 filtering（ZFF）来估算喉咙源波形。研究的三个主要目标是：（1）研究 Whether features derived from pre-trained models improve classification accuracy compared to conventional features（spectrogram、mel-spectrogram、MFCCs、i-vector、x-vector），（2）investigate which modality（speech vs. NSA）is more effective in the classification task with pre-trained model-based features，（3）evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier。结果表明，使用NSA输入的 classification 性能比语音信号更好。此外，使用 pre-trained model-based features 也比使用传统特征更好，同时 HuBERT 特征也比 wav2vec2-BASE 和 wav2vec2-LARGE 特征更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/cs.SD_2023_08_07/" data-id="clmjn91nu00aj0j885dmwdbr9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/07/cs.LG_2023_08_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-07 18:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/07/eess.IV_2023_08_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-07 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
