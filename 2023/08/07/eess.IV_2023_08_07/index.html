
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-07 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03586 repo_url: None pap">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-07 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/07/eess.IV_2023_08_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03586 repo_url: None pap">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-06T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:45.460Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/eess.IV_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-07 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe"><a href="#SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe" class="headerlink" title="SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe"></a>SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03586">http://arxiv.org/abs/2308.03586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafiseh Kakhani, Moien Rangzan, Ali Jamali, Sara Attarchi, Seyed Kazem Alavipanah, Thomas Scholten</li>
<li>for: 这项研究旨在提高数字土壤地图（DSM）技术，尤其是在使用深度学习（DL）方法来预测土壤有机碳（SOC）的空间特征。</li>
<li>methods: 该研究提出了一种新的架构， combining 基于卷积神经网络（CNN）的空间信息和基于长短时间记忆（LSTM）网络的气候时序信息，以预测欧洲各地的SOC。该模型使用了一组全面的环境特征，包括兰达特-8图像、地形、远程感知指数和气候时序序列，作为输入特征。</li>
<li>results: 研究结果表明，提出的方框比常用的多项Random Forest方法（ML）更高效，具有较低的根圆平方误差（RMSE）。这种模型是一种可靠的SOC预测工具，可以应用于其他土壤特征预测，从而为土地管理和决策过程提供更准确的信息。<details>
<summary>Abstract</summary>
Digital soil mapping (DSM) is an advanced approach that integrates statistical modeling and cutting-edge technologies, including machine learning (ML) methods, to accurately depict soil properties and their spatial distribution. Soil organic carbon (SOC) is a crucial soil attribute providing valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity. This study highlights the significance of spatial-temporal deep learning (DL) techniques within the DSM framework. A novel architecture is proposed, incorporating spatial information using a base convolutional neural network (CNN) model and spatial attention mechanism, along with climate temporal information using a long short-term memory (LSTM) network, for SOC prediction across Europe. The model utilizes a comprehensive set of environmental features, including Landsat-8 images, topography, remote sensing indices, and climate time series, as input features. Results demonstrate that the proposed framework outperforms conventional ML approaches like random forest commonly used in DSM, yielding lower root mean square error (RMSE). This model is a robust tool for predicting SOC and could be applied to other soil properties, thereby contributing to the advancement of DSM techniques and facilitating land management and decision-making processes based on accurate information.
</details>
<details>
<summary>摘要</summary>
数字土壤地图（DSM）是一种先进的方法，它将统计模型和前沿技术，包括机器学习（ML）方法，结合起来准确地描述土壤属性和其空间分布。土壤有机碳（SOC）是一个重要的土壤特征，它提供了valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity.这项研究强调了在 DSM 框架中使用空间-时间深度学习（DL）技术的重要性。该研究提出了一种新的建议，其包括在基于 convolutional neural network（CNN）模型和空间注意机制的基础上，以及使用 long short-term memory（LSTM）网络来预测欧洲各地的 SOC。该模型使用了包括 Landsat-8 图像、地形、 remote sensing 指标和气候时序序列在内的全面的环境特征作为输入特征。结果表明，提议的框架在与常见的 ML 方法如随机森林相比，具有较低的根圆平均误差（RMSE）。这种模型是一种准确预测 SOC 的工具，可以应用于其他土壤属性，从而为 DSM 技术的发展和土地管理决策提供支持。
</details></li>
</ul>
<hr>
<h2 id="Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice"><a href="#Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice" class="headerlink" title="Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice"></a>Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03460">http://arxiv.org/abs/2308.03460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Kofler, Kirsten Miriam Kerkering, Laura Göschel, Ariane Fillmer, Cristoph Kolbitsch</li>
<li>for: 这种方法是用于量子磁共振成像（QMRI）中的参数地图重建的。</li>
<li>methods: 方法使用了字典学习（DL）和稀疏编码（SC）算法自动计算最佳字典大小和稀疏程度，并在不同参数地图之间进行自适应调整。</li>
<li>results: 该方法在RMSE和PSNR方面胜过MAP、TV、Wl和Sh方法，并且与DL+Fit方法具有相似或更好的效果，同时加速了重建过程约7倍。In English, this means:</li>
<li>for: This method is proposed for the reconstruction of parameter maps in Quantitative Magnetic Resonance Imaging (QMRI).</li>
<li>methods: The method uses dictionary learning (DL) and sparse coding (SC) algorithms to automatically estimate the optimal dictionary size and sparsity level for each parameter map, and adaptively adjusts the parameters between different maps.</li>
<li>results: The proposed method outperforms the compared methods (MAP, TV, Wl, and Sh) in terms of RMSE and PSNR, and has similar or better effects as the DL+Fit method, while significantly accelerating the reconstruction process by a factor of approximately seven.<details>
<summary>Abstract</summary>
Objective: We propose a method for the reconstruction of parameter-maps in Quantitative Magnetic Resonance Imaging (QMRI).   Methods: Because different quantitative parameter-maps differ from each other in terms of local features, we propose a method where the employed dictionary learning (DL) and sparse coding (SC) algorithms automatically estimate the optimal dictionary-size and sparsity level separately for each parameter-map. We evaluated the method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb data as well as in-vivo brain images acquired on an ultra-high field 7T scanner. We compared it to a model-based acceleration for parameter mapping (MAP) approach, other sparsity-based methods using total variation (TV), Wavelets (Wl) and Shearlets (Sh), and to a method which uses DL and SC to reconstruct qualitative images, followed by a non-linear (DL+Fit).   Results: Our algorithm surpasses MAP, TV, Wl and Sh in terms of RMSE and PSNR. It yields better or comparable results to DL+Fit by additionally significantly accelerating the reconstruction by a factor of approximately seven.   Conclusion: The proposed method outperforms the reported methods of comparison and yields accurate $T_1$-maps. Although presented for $T_1$-mapping in the brain, our method's structure is general and thus most probably also applicable for the the reconstruction of other quantitative parameters in other organs.   Significance: From a clinical perspective, the obtained $T_1$-maps could be utilized to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be employed to obtain ground-truth data for the development of data-driven methods based on supervised learning.+
</details>
<details>
<summary>摘要</summary>
Methods: Different quantitative parameter-maps have unique local features, so we use dictionary learning (DL) and sparse coding (SC) algorithms to automatically estimate the optimal dictionary size and sparsity level for each parameter-map. We evaluated our method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb dataset and in-vivo brain images acquired on a 7T scanner. We compared our method to a model-based acceleration for parameter mapping (MAP) approach, as well as other sparsity-based methods using total variation (TV), wavelets (Wl), and shearlets (Sh). We also compared our method to a method that uses DL and SC to reconstruct qualitative images and then uses non-linear registration (DL+Fit).Results: Our method outperformed MAP, TV, Wl, and Sh in terms of root mean squared error (RMSE) and peak signal-to-noise ratio (PSNR). It also yielded better or comparable results to DL+Fit, while significantly accelerating the reconstruction process by a factor of approximately seven.Conclusion: Our proposed method outperforms previous methods and provides accurate $T_1$-maps. Although we focused on $T_1$-mapping in the brain, our method's structure is general and can be applied to the reconstruction of other quantitative parameters in other organs.Significance: From a clinical perspective, the obtained $T_1$-maps could be used to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be used to obtain ground-truth data for the development of data-driven methods based on supervised learning.
</details></li>
</ul>
<hr>
<h2 id="Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising"><a href="#Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising" class="headerlink" title="Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising"></a>Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03448">http://arxiv.org/abs/2308.03448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srameo/led">https://github.com/srameo/led</a></li>
<li>paper_authors: Xin Jin, Jia-Wen Xiao, Ling-Hao Han, Chunle Guo, Ruixun Zhang, Xialei Liu, Chongyi Li</li>
<li>For: This paper is written for RAW image denoising under extremely low-light environments, and it aims to overcome the limitations of calibration-based methods.* Methods: The proposed method uses a calibration-free pipeline, which adapts to a target camera with few-shot paired data and fine-tuning. The method also includes well-designed structural modification during both stages to alleviate the domain gap between synthetic and real noise.* Results: The proposed method achieves superior performance over other calibration-based methods with 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations.Here’s the format you requested:* For: 这篇论文是为了对极低光环境下的 RAW 图像推断而写的，并且想要超越传统的测试基于方法。* Methods: 提案的方法使用了没有单位的管道，可以适应目标摄像头只需要几对对照数据和微调。这个方法还包括了妥善的结构修改在两个阶段，以解决实际和 sintetic 噪声之间的领域差。* Results: 提案的方法在对其他传统基于测试方法进行比较时，获得了更好的性能，仅需要2对对照数据和0.5%迭代。<details>
<summary>Abstract</summary>
Calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the digital gain or camera sensor. Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with few-shot paired data and fine-tuning. In addition, well-designed structural modification during both stages alleviates the domain gap between synthetic and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods. Our code is available at https://github.com/Srameo/LED .
</details>
<details>
<summary>摘要</summary>
几种基于准确的方法在极低照度环境下进行RAW图像降噪已经占据了主导地位。然而，这些方法受到以下主要缺点的影响：1）准确程度很低，2）适用于不同摄像头的降噪器难以传输，3）高度数字增量使得实际噪声与synthetic噪声之间的差异变大。为了超越这些缺点，我们提出了不需要准确程度的渠道，即Lighting Every Drakness（LED）。相比准确程度的准备和重复训练，我们的方法只需要几次对应的配对数据和微调就能适应目标摄像头。此外，我们在两个阶段中设计了 estructural modification，以alleviate the domain gap between synthetic and real noise without any extra computational cost。通过使用6对每个额外数字增量（共计24对）和0.5%的迭代，我们的方法可以在其他准确基于方法之上达到更高的性能。我们的代码可以在https://github.com/Srameo/LED上找到。
</details></li>
</ul>
<hr>
<h2 id="Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis"><a href="#Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis" class="headerlink" title="Energy-Guided Diffusion Model for CBCT-to-CT Synthesis"></a>Energy-Guided Diffusion Model for CBCT-to-CT Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03354">http://arxiv.org/abs/2308.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjie Fu, Xia Li, Xiuding Cai, Dong Miao, Yu Yao, Yali Shen</li>
<li>for: 提高CBCT图像质量和Hounsfield单位精度，以便更好地用于放射治疗。</li>
<li>methods: 基于能量导向扩散模型（EGDiff），从CBCT图像生成Synthetic CT（sCT）。</li>
<li>results: 对胸腺癌数据集进行实验，得到了优秀的性能结果，包括平均绝对错误26.87±6.14HU、结构相似度指标0.850±0.03、峰值信号噪声比19.83±1.39dB和正常化交叉相关指标0.874±0.04。这些结果表明，我们的方法在精度和视觉质量方面都有所提高，生成了superior的sCT图像。<details>
<summary>Abstract</summary>
Cone Beam CT (CBCT) plays a crucial role in Adaptive Radiation Therapy (ART) by accurately providing radiation treatment when organ anatomy changes occur. However, CBCT images suffer from scatter noise and artifacts, making relying solely on CBCT for precise dose calculation and accurate tissue localization challenging. Therefore, there is a need to improve CBCT image quality and Hounsfield Unit (HU) accuracy while preserving anatomical structures. To enhance the role and application value of CBCT in ART, we propose an energy-guided diffusion model (EGDiff) and conduct experiments on a chest tumor dataset to generate synthetic CT (sCT) from CBCT. The experimental results demonstrate impressive performance with an average absolute error of 26.87$\pm$6.14 HU, a structural similarity index measurement of 0.850$\pm$0.03, a peak signal-to-noise ratio of the sCT of 19.83$\pm$1.39 dB, and a normalized cross-correlation of the sCT of 0.874$\pm$0.04. These results indicate that our method outperforms state-of-the-art unsupervised synthesis methods in accuracy and visual quality, producing superior sCT images.
</details>
<details>
<summary>摘要</summary>
cone beam CT (CBCT) 在适应辐射疗法 (ART) 中发挥重要作用，准确地提供辐射治疗当器官解剖结构发生变化时。然而，CBCT图像受到散射噪声和artefacts的影响，使凭借CBCT alone 的精度计算和精确地本结构定位变得困难。因此，需要改进CBCT图像质量和温顺单元 (HU) 精度，保持器官结构的完整性。为了提高 CBCT 在 ART 中的角色和应用价值，我们提议一种能量指导扩散模型 (EGDiff)，并在胸腔肿瘤数据集上进行实验，将 CBCT 转换成 Synthetic CT (sCT)。实验结果表明，我们的方法在精度和视觉质量方面具有卓越表现，与现有的无监督杂合 Synthesis 方法相比，具有更高的 HU 精度、更高的结构相似度、更高的峰信号噪声比和更高的正规化交叉相似度。这些结果表明，我们的方法可以生成高质量的 sCT 图像，超过现有的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining"><a href="#A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining" class="headerlink" title="A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining"></a>A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03340">http://arxiv.org/abs/2308.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wang, Wei Li</li>
<li>for: 图像排除雨纹效果的提升</li>
<li>methods: 使用深度学习方法进行图像排除雨纹</li>
<li>results: 实现了高质量的图像排除雨纹效果Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the effectiveness of image deraining by using deep learning methods.</li>
<li>methods: The paper uses deep learning techniques, specifically a deep neural network, to remove rain streaks from degraded images.</li>
<li>results: The paper achieves high-quality image deraining results by using these methods.<details>
<summary>Abstract</summary>
Image deraining is a challenging task that involves restoring degraded images affected by rain streaks.
</details>
<details>
<summary>摘要</summary>
图像抑雨是一项复杂的任务，涉及到修复受到雨束纹的图像。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/07/eess.IV_2023_08_07/" data-id="cllsj1rpk0093pf887kj3hu8k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/07/cs.SD_2023_08_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-07 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/06/cs.LG_2023_08_06/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-06 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
