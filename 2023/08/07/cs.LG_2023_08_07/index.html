
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-07 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03670 repo_url: None paper_authors: Babak Azad, Ahmed Abdalla, Kwanghee Won, Ali Mir">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-07 18:00:00">
<meta property="og:url" content="http://nullscc.github.io/2023/08/07/cs.LG_2023_08_07/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.03670 repo_url: None paper_authors: Babak Azad, Ahmed Abdalla, Kwanghee Won, Ali Mir">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-06T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-26T20:36:45.349Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.LG_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-07 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model"><a href="#Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model" class="headerlink" title="Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model"></a>Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03670">http://arxiv.org/abs/2308.03670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babak Azad, Ahmed Abdalla, Kwanghee Won, Ali Mirzakhani Nafchi</li>
<li>for: 这个研究是为了提高小麦和黑麦抗病耐菌creening中的效率、准确率和时间探测病菌病 head blight（FHB）。</li>
<li>methods: 这个研究使用了一种新的Context Bridge，将U-Net网络的本地表现力与transformer模型的全球自注意力机制相结合，以提高病菌探测的精度和效率。此外，原始transformer模型中的标准注意力机制被更改为Efficient Self-attention，以减少其复杂性。</li>
<li>results: 经过广泛的实验和评估，研究发现transformer-based方法在小麦病菌探测中具有高效率和准确率，并且可以处理不同类型的植物形态和病变。<details>
<summary>Abstract</summary>
Fusarium head blight is a devastating disease that causes significant economic losses annually on small grains. Efficiency, accuracy, and timely detection of FHB in the resistance screening are critical for wheat and barley breeding programs. In recent years, various image processing techniques have been developed using supervised machine learning algorithms for the early detection of FHB. The state-of-the-art convolutional neural network-based methods, such as U-Net, employ a series of encoding blocks to create a local representation and a series of decoding blocks to capture the semantic relations. However, these methods are not often capable of long-range modeling dependencies inside the input data, and their ability to model multi-scale objects with significant variations in texture and shape is limited. Vision transformers as alternative architectures with innate global self-attention mechanisms for sequence-to-sequence prediction, due to insufficient low-level details, may also limit localization capabilities. To overcome these limitations, a new Context Bridge is proposed to integrate the local representation capability of the U-Net network in the transformer model. In addition, the standard attention mechanism of the original transformer is replaced with Efficient Self-attention, which is less complicated than other state-of-the-art methods. To train the proposed network, 12,000 wheat images from an FHB-inoculated wheat field at the SDSU research farm in Volga, SD, were captured. In addition to healthy and unhealthy plants, these images encompass various stages of the disease. A team of expert pathologists annotated the images for training and evaluating the developed model. As a result, the effectiveness of the transformer-based method for FHB-disease detection, through extensive experiments across typical tasks for plant image segmentation, is demonstrated.
</details>
<details>
<summary>摘要</summary>
fusarium 头疫是一种致命的病种，每年对小麦和黑麦产生了重大经济损失。在抗性屏测中，效率、准确性和时效检测是关键。在最近几年，各种图像处理技术被开发，使用超级vised机器学习算法进行早期检测。state-of-the-art的卷积神经网络方法，如U-Net，使用一系列的编码块来创建本地表示，并使用一系列的解码块来捕捉 semantic关系。然而，这些方法通常无法长距离模型数据中的相互关系，其能模型多尺度对象的表征和文本特征是有限的。在这种情况下，一种新的 Context Bridge 被提议，以 интеGRATE U-Net 网络的本地表示能力到 transformer 模型中。此外，原始 transformer 的标准注意力机制被 replaced with 高效自注意力，这种机制 simpler than other state-of-the-art methods。为了训练提议的网络，SDSU 研究农场在南达科他州的一个 FHB-感染小麦田中Capture了12,000 个小麦图像。此外，这些图像还包括不同阶段的疾病。一 команoda expert 的病理学家对这些图像进行了训练和评估。结果，通过对plant image segmentation 等常见任务进行广泛的实验， demonstarted  transformer 基于方法的 FHB 疾病检测的效果。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders"><a href="#Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders" class="headerlink" title="Diffusion Model in Causal Inference with Unmeasured Confounders"></a>Diffusion Model in Causal Inference with Unmeasured Confounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03669">http://arxiv.org/abs/2308.03669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/BDCM">https://github.com/tatsu432/BDCM</a></li>
<li>paper_authors: Tatsuhiro Shimizu</li>
<li>for: 本研究旨在扩展diffusion模型，以便从观察数据中回答 causal问题，即使存在不可观测的假设变量。</li>
<li>methods: 我们使用了Pearl提出的Directed Acyclic Graph (DAG)来捕捉 causal intervention，并提出了一种叫做Diffusion-based Causal Model (DCM)的模型，假设所有的假设变量都可以观测。然而，在实际应用中，不可观测的假设变量存在，这限制了DCM的可用性。为了解决这个限制，我们提出了一种扩展模型，即Backdoor Criterion based DCM (BDCM)。</li>
<li>results: 我们通过synthetic data experiment表明，我们的提议的模型可以更 precisely回答 causal问题，即使存在不可观测的假设变量。<details>
<summary>Abstract</summary>
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confounders.
</details>
<details>
<summary>摘要</summary>
我们研究如何将扩散模型应用于从观察数据中回答 causal 问题，即使存在未探测的干扰因素。在珍珠的框架中使用指导的циклиック графи（DAG）捕捉 causal 干扰，一种叫做扩散基于 causal 模型（DCM）的模型被提出，假设所有干扰因素都是观察的。然而，在实践中存在未探测的干扰因素，这限制了 DCM 的应用。为了解决 DCM 的这种限制，我们提出了一种扩展模型，即基于后门准则的 DCM（BDCM）。该模型的思想在于根据后门准则选择 DAG 中的变量，以便在扩散模型的解码过程中包含这些变量，从而扩展 DCM 到具有未探测干扰因素的情况。 synthetic 数据实验表明，我们的提议模型可以更 precisely 捕捉 counterfactual 分布than DCM 在未探测干扰因素的情况下。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Trustworthiness-and-Open-World-Learning-An-Exploratory-Neural-Approach-for-Enhancing-Interpretability-Generalization-and-Robustness"><a href="#Bridging-Trustworthiness-and-Open-World-Learning-An-Exploratory-Neural-Approach-for-Enhancing-Interpretability-Generalization-and-Robustness" class="headerlink" title="Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness"></a>Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03666">http://arxiv.org/abs/2308.03666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shide Du, Zihan Fang, Shiyang Lan, Yanchao Tan, Manuel Günther, Shiping Wang, Wenzhong Guo</li>
<li>for: 提高人工智能系统的可靠性和可解释性，以增强人工智能在开放世界中的应用。</li>
<li>methods: 基于自定义可信网络和灵活学习正则化的方法，以提高学习模型的通用性和适应性。</li>
<li>results: 通过实现设计层解释性、环境健康任务接口和开放世界识别计划，提高了多模态场景下的信任性和可靠性。<details>
<summary>Abstract</summary>
As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy learning; 3) We propose to increase the robustness of trustworthy learning by integrating open-world recognition losses with agent mechanisms. Eventually, we enhance various trustworthy properties through the establishment of design-level explainability, environmental well-being task-interfaces and open-world recognition programs. These designed open-world protocols are applicable across a wide range of surroundings, under open-world multimedia recognition scenarios with significant performance improvements observed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Insufficient explanation of predictive results2. Inadequate generalization for learning models3. Poor adaptability to uncertain environmentsTo address these challenges, we explore a neural program that bridges trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. Our approach includes the following three components:1. Enhancing design-level interpretability: We first customize trustworthy networks with specific physical meanings, making it easier to understand how the AI system works and why it makes certain decisions.2. Improving generalization through flexible learning regularizers: We design environmental well-being task-interfaces to improve the generalization of trustworthy learning, ensuring that the AI system can adapt to different environments and situations.3. Integrating open-world recognition losses with agent mechanisms: We propose to increase the robustness of trustworthy learning by integrating open-world recognition losses with agent mechanisms, enabling the AI system to better handle unexpected events and situations.By combining these three components, we enhance various trustworthy properties through the establishment of design-level explainability, environmental well-being task-interfaces, and open-world recognition programs. These designed open-world protocols are applicable across a wide range of surroundings, under open-world multimedia recognition scenarios with significant performance improvements observed.</details></li>
</ol>
<hr>
<h2 id="Distributionally-Robust-Classification-on-a-Data-Budget"><a href="#Distributionally-Robust-Classification-on-a-Data-Budget" class="headerlink" title="Distributionally Robust Classification on a Data Budget"></a>Distributionally Robust Classification on a Data Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03821">http://arxiv.org/abs/2308.03821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penfever/vlhub">https://github.com/penfever/vlhub</a></li>
<li>paper_authors: Benjamin Feuer, Ameya Joshi, Minh Pham, Chinmay Hegde</li>
<li>for: 这篇论文的目的是提高深度学习模型在数据分布变化时的预测可靠性，并且在有限数据情况下进行训练。</li>
<li>methods: 论文使用了一系列控制严格的实验和大规模元分析来研究因素对模型 Robustness 的影响，并使用标准 ResNet-50 和 CLIP ResNet-50 进行比较。</li>
<li>results: 论文显示，使用限制数据量训练标准 ResNet-50 可以达到与 CLIP ResNet-50 训练于400万样本后的相似水平的分布Robustness。这是我们知道的首个在有限数据预算下实现 (near) 状态的艺技 Robustness 的结果。<details>
<summary>Abstract</summary>
Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets. Our dataset is available at \url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used to reproduce our experiments can be found at \url{https://github.com/penfever/vlhub/}.
</details>
<details>
<summary>摘要</summary>
实际应用中的深度学习需要模型在分布变化时保持预测可靠。如CLIP模型，它们可以 Display natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. 我们问道：可以在数据有限的领域中训练强健的学习者吗？为了彻底回答这个问题，我们介绍了 JANuS（共同标注和名称集），一个包含四个新的训练数据集的集合，每个数据集包含图像、标签和相应的描述。我们进行了一系列仔细控制的调查，探讨了影响模型强健性的因素，然后将结果与大规模元分析结果进行比较。我们发现，使用权重损失函数和240万个图像样本训练的标准 ResNet-50 可以达到与CLIP ResNet-50 在4000万个样本上训练后的（近）状态OF-the-art分布强健性。我们认为这是首次在有限数据预算下实现的分布强健性结果。我们的数据集可以在 \url{https://huggingface.co/datasets/penfever/JANuS_dataset} 上下载，并且用于 reproduce我们的实验的代码可以在 \url{https://github.com/penfever/vlhub/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Two-stage-Early-Prediction-Framework-of-Remaining-Useful-Life-for-Lithium-ion-Batteries"><a href="#Two-stage-Early-Prediction-Framework-of-Remaining-Useful-Life-for-Lithium-ion-Batteries" class="headerlink" title="Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries"></a>Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03664">http://arxiv.org/abs/2308.03664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Mittal, Hymalai Bello, Bo Zhou, Mayank Shekhar Jha, Sungho Suh, Paul Lukowicz</li>
<li>for: 预测锂离子电池剩余有用生命（RUL）的早期预测是重要的，以提高电池技术的可靠性和维护性。</li>
<li>methods: 本文提出了一种新的RUL预测方法，包括两个阶段：首先使用神经网络模型确定第一个预测周期（FPC），然后使用预测衰变模式来估算剩余有用生命的百分比。</li>
<li>results: 实验结果表明，提出的方法在RUL预测方面比既有方法更高准确。此外，该方法在实际应用场景中也表现出了优异的应用性和准确性。<details>
<summary>Abstract</summary>
Early prediction of remaining useful life (RUL) is crucial for effective battery management across various industries, ranging from household appliances to large-scale applications. Accurate RUL prediction improves the reliability and maintainability of battery technology. However, existing methods have limitations, including assumptions of data from the same sensors or distribution, foreknowledge of the end of life (EOL), and neglect to determine the first prediction cycle (FPC) to identify the start of the unhealthy stage. This paper proposes a novel method for RUL prediction of Lithium-ion batteries. The proposed framework comprises two stages: determining the FPC using a neural network-based model to divide the degradation data into distinct health states and predicting the degradation pattern after the FPC to estimate the remaining useful life as a percentage. Experimental results demonstrate that the proposed method outperforms conventional approaches in terms of RUL prediction. Furthermore, the proposed method shows promise for real-world scenarios, providing improved accuracy and applicability for battery management.
</details>
<details>
<summary>摘要</summary>
早期预测电池剩余有用寿命（RUL）是多个领域中的关键，从家用电器到大规模应用。准确预测RUL提高电池技术的可靠性和维护性。然而，现有的方法有限制，包括使用同一些感知器的数据或分布，假设结束生命周期（EOL）的知识，以及忽略确定首次预测周期（FPC）以识别不健康的阶段。这篇论文提出了一种新的Li-ion电池RUL预测方法。该框架包括两个阶段：使用神经网络模型来分解衰变数据 into Distinct Health States，并预测衰变模式以估计剩余有用寿命为百分比。实验结果表明，提案的方法在RUL预测方面超过了传统方法的性能。此外，提案的方法在实际应用中具有改善的准确性和可应用性。
</details></li>
</ul>
<hr>
<h2 id="Matrix-Completion-in-Almost-Verification-Time"><a href="#Matrix-Completion-in-Almost-Verification-Time" class="headerlink" title="Matrix Completion in Almost-Verification Time"></a>Matrix Completion in Almost-Verification Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03661">http://arxiv.org/abs/2308.03661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, Kevin Tian</li>
<li>for: 这个论文提出了一种新的低级别矩阵完成问题的解决方案，即从Random观察到rank-$r$矩阵 $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$)的approximation。</li>
<li>methods: 这个论文提出了一种算法，可以在没有任何假设的情况下，从 $\approx mr$ 个观察数据中完成 $\mathbf{M}$ 的99% 的行和列。这个算法需要 $\approx mr^2$ 时间。在具有特定的行和列范围的 $\mathbf{M}$ 中，通过聚合多个回归问题的解决，这个算法可以完成全矩阵。</li>
<li>results: 论文表明，在具有几乎信息理论最佳的情况下，这个算法可以从 $mr^{2+o(1)}$ 个观察数据中完成 $\mathbf{M}$ 到高精度，并且runtime为 $mr^{3+o(1)}$。在特定的行和列范围下，这个算法可以完成 $\mathbf{M}$ 到信息理论最佳的精度，并且runtime为 $mr^{2+o(1)}$。此外，这个论文还提出了一些Robust的算法，可以在含有噪声的情况下完成 $\mathbf{M}$ 到 Frobenius 范数 distance $\approx r^{1.5}\Delta$，其中 $|\mathbf{N}|_{F} \le \Delta$。在噪声的情况下，这个算法的精度和runtime都比之前的算法更好。<details>
<summary>Abstract</summary>
We give a new framework for solving the fundamental problem of low-rank matrix completion, i.e., approximating a rank-$r$ matrix $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$) from random observations. First, we provide an algorithm which completes $\mathbf{M}$ on $99\%$ of rows and columns under no further assumptions on $\mathbf{M}$ from $\approx mr$ samples and using $\approx mr^2$ time. Then, assuming the row and column spans of $\mathbf{M}$ satisfy additional regularity properties, we show how to boost this partial completion guarantee to a full matrix completion algorithm by aggregating solutions to regression problems involving the observations.   In the well-studied setting where $\mathbf{M}$ has incoherent row and column spans, our algorithms complete $\mathbf{M}$ to high precision from $mr^{2+o(1)}$ observations in $mr^{3 + o(1)}$ time (omitting logarithmic factors in problem parameters), improving upon the prior state-of-the-art [JN15] which used $\approx mr^5$ samples and $\approx mr^7$ time. Under an assumption on the row and column spans of $\mathbf{M}$ we introduce (which is satisfied by random subspaces with high probability), our sample complexity improves to an almost information-theoretically optimal $mr^{1 + o(1)}$, and our runtime improves to $mr^{2 + o(1)}$. Our runtimes have the appealing property of matching the best known runtime to verify that a rank-$r$ decomposition $\mathbf{U}\mathbf{V}^\top$ agrees with the sampled observations. We also provide robust variants of our algorithms that, given random observations from $\mathbf{M} + \mathbf{N}$ with $\|\mathbf{N}\|_{F} \le \Delta$, complete $\mathbf{M}$ to Frobenius norm distance $\approx r^{1.5}\Delta$ in the same runtimes as the noiseless setting. Prior noisy matrix completion algorithms [CP10] only guaranteed a distance of $\approx \sqrt{n}\Delta$.
</details>
<details>
<summary>摘要</summary>
我们提出了一新的框架来解决低级矩阵完成问题，即 aproximating 一个rank-$r$矩阵 $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$) 从 random observations。首先，我们提供了一个算法，可以在没有进一步假设的情况下，使 $\mathbf{M}$ 在99% 的行和列上完成，需要 $\approx mr$ 样本和 $\approx mr^2$ 时间。然后，如果行和列范围的范围满足其他正则性质，我们如何通过融合关于 observations 的回归问题的解来提高这个部分完成 garantía。在已有研究的设定中， где $\mathbf{M}$ 的行和列范围是不相关的，我们可以从 $mr^{2+o(1)}$ 样本和 $mr^{3+o(1)}$ 时间中完成 $\mathbf{M}$ 到高精度，超过先前的最佳状态（JN15），其使用 $mr^5$ 样本和 $mr^7$ 时间。如果行和列范围满足我们引入的一个假设（这在Random subspace 中发生的概率很高），我们的样本复杂度可以降低到 almost information-theoretically optimal $mr^{1+o(1)}$，并且时间复杂度可以降低到 $mr^{2+o(1)}$。我们的运行时间具有愉悦的性质，即与verify rank-$r$ 分解 $\mathbf{U}\mathbf{V}^\top$ 与样本观测匹配的最佳known runtime。我们还提供了一些Robust 变体我们的算法，可以在 $\mathbf{M} + \mathbf{N}$ 中的随机观测下，完成 $\mathbf{M}$ 到 Frobenius 范数Distance $\approx r^{1.5}\Delta$ 的同样时间。在先前的噪声矩阵完成算法（CP10）中，只能保证一个距离为 $\approx \sqrt{n}\Delta$。
</details></li>
</ul>
<hr>
<h2 id="Generative-Forests"><a href="#Generative-Forests" class="headerlink" title="Generative Forests"></a>Generative Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03648">http://arxiv.org/abs/2308.03648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlCorreia/GeFs">https://github.com/AlCorreia/GeFs</a></li>
<li>paper_authors: Richard Nock, Mathieu Guillame-Bert</li>
<li>for: 本文旨在提出新的树形生成模型，用于Tabular数据生成和概率模型。</li>
<li>methods: 本文使用新的树形生成模型，并提出一种基于supervised学习的训练算法，可以在Tabular数据上实现高质量的数据生成。</li>
<li>results: 实验表明，本文的方法可以在缺失数据填充和生成数据比较实际数据的任务中显示出高质量的结果， especial against state of the art。<details>
<summary>Abstract</summary>
Tabular data represents one of the most prevalent form of data. When it comes to data generation, many approaches would learn a density for the data generation process, but would not necessarily end up with a sampler, even less so being exact with respect to the underlying density. A second issue is on models: while complex modeling based on neural nets thrives in image or text generation (etc.), less is known for powerful generative models on tabular data. A third problem is the visible chasm on tabular data between training algorithms for supervised learning with remarkable properties (e.g. boosting), and a comparative lack of guarantees when it comes to data generation. In this paper, we tackle the three problems, introducing new tree-based generative models convenient for density modeling and tabular data generation that improve on modeling capabilities of recent proposals, and a training algorithm which simplifies the training setting of previous approaches and displays boosting-compliant convergence. This algorithm has the convenient property to rely on a supervised training scheme that can be implemented by a few tweaks to the most popular induction scheme for decision tree induction with two classes. Experiments are provided on missing data imputation and comparing generated data to real data, displaying the quality of the results obtained by our approach, in particular against state of the art.
</details>
<details>
<summary>摘要</summary>
表格数据是现代数据的一种最常见形式。当处理数据时，许多方法都会学习数据的浓度，但它们并不一定会得到一个采样器，更不用说是对于真实的浓度准确。第二个问题是模型：虽然复杂的模型基于神经网络在图像或文本生成等领域得到了广泛的应用，但对于表格数据， menos is known about powerful generative models。第三个问题是表格数据的训练算法和supervised learning之间的可见差异。在这篇论文中，我们解决了这三个问题，提出了新的树形生成模型，可以提高表格数据生成的模型能力，并且提供了一种简化训练过程的算法，可以让之前的方法在训练中更加简单。这种算法可以通过对最流行的决策树引入两类的修改来实现。我们的实验表明，我们的方法可以在缺失数据填充和生成数据与真实数据的比较中 display 出色的结果，特别是与现有技术相比。
</details></li>
</ul>
<hr>
<h2 id="XFlow-Benchmarking-Flow-Behaviors-over-Graphs"><a href="#XFlow-Benchmarking-Flow-Behaviors-over-Graphs" class="headerlink" title="XFlow: Benchmarking Flow Behaviors over Graphs"></a>XFlow: Benchmarking Flow Behaviors over Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03819">http://arxiv.org/abs/2308.03819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xgraphing/xflow">https://github.com/xgraphing/xflow</a></li>
<li>paper_authors: Zijian Zhang, Zonghan Zhang, Zhiqian Chen</li>
<li>for: 本研究旨在提供一个包容性强的benchmark集合，以便研究在网络场景下的流行行为。</li>
<li>methods: 本研究使用了多种任务、基准模型、图Dataset和评估工具来研究流行行为。</li>
<li>results: 研究发现了现有基础模型的优劣点，并提出了进一步研究的可能性。基于实验结果，提供了一个通用的分析框架，用于研究多种流行任务的多个领域。<details>
<summary>Abstract</summary>
The occurrence of diffusion on a graph is a prevalent and significant phenomenon, as evidenced by the spread of rumors, influenza-like viruses, smart grid failures, and similar events. Comprehending the behaviors of flow is a formidable task, due to the intricate interplay between the distribution of seeds that initiate flow propagation, the propagation model, and the topology of the graph. The study of networks encompasses a diverse range of academic disciplines, including mathematics, physics, social science, and computer science. This interdisciplinary nature of network research is characterized by a high degree of specialization and compartmentalization, and the cooperation facilitated by them is inadequate. From a machine learning standpoint, there is a deficiency in a cohesive platform for assessing algorithms across various domains. One of the primary obstacles to current research in this field is the absence of a comprehensive curated benchmark suite to study the flow behaviors under network scenarios.   To address this disparity, we propose the implementation of a novel benchmark suite that encompasses a variety of tasks, baseline models, graph datasets, and evaluation tools. In addition, we present a comprehensive analytical framework that offers a generalized approach to numerous flow-related tasks across diverse domains, serving as a blueprint and roadmap. Drawing upon the outcomes of our empirical investigation, we analyze the advantages and disadvantages of current foundational models, and we underscore potential avenues for further study. The datasets, code, and baseline models have been made available for the public at: https://github.com/XGraphing/XFlow
</details>
<details>
<summary>摘要</summary>
流行现象在图格中是一种普遍和重要的现象，如传播谣言、流感病毒、智能电网故障等事件。理解流动行为是一项复杂的任务，由于流动 initiation 的分布、传播模型和图格结构之间的复杂交互。网络研究涵盖多个学术领域，包括数学、物理、社会科学和计算机科学。这些学科之间的交流和合作受限。从机器学习的角度来看，流行研究受到了缺乏一个整体的平台来评估算法。目前研究中的主要障碍是缺乏一个完整的精心编辑的benchmark集成来研究流动行为在网络场景下。为了解决这一不足，我们提议实施一个新的benchmark集成，包括多种任务、基线模型、图格数据集和评估工具。此外，我们还提出了一个通用的分析框架，可以在多个流动相关任务上提供一个通用的方法和路线图。通过我们的实验研究的结果，我们分析了当前基础模型的优劣点，并强调了未来研究的可能性。 datasets、代码和基线模型已经公开发布在：https://github.com/XGraphing/XFlow。
</details></li>
</ul>
<hr>
<h2 id="MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining"><a href="#MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining" class="headerlink" title="MedMine: Examining Pre-trained Language Models on Medication Mining"></a>MedMine: Examining Pre-trained Language Models on Medication Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03629">http://arxiv.org/abs/2308.03629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hecta-uom/m3">https://github.com/hecta-uom/m3</a></li>
<li>paper_authors: Haifa Alrdahi, Lifeng Han, Hendrik Šuvalov, Goran Nenadic</li>
<li>For: 本研究旨在探讨现有的预训练语言模型（PLM）在自动药物检索任务上的表现，以及这些模型在不同实体类和医疗事件上的偏见问题。* Methods: 本研究使用了现有的预训练语言模型（PLM），包括Med7和XLM-RoBERTa，进行细化调教。并对这些调教后的模型进行比较，找出它们在不同实体类和医疗事件上的优劣点。* Results: 研究发现，XLM-RoBERTa在自动药物检索任务上表现较好，而Med7在一些实体类上表现较差。此外，研究还发现了这些模型在不同实体类和医疗事件上的偏见问题。<details>
<summary>Abstract</summary>
Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or improve their overall accuracy by ensemble learning and data augmentation. MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}
</details>
<details>
<summary>摘要</summary>
自动药物挖掘从临床和生物医学文本中获得了广泛的关注，因为它们在医疗应用中真正有影响。然而，完全自动提取模型仍然需要超越一些障碍物，以便在临床实践中直接部署。这些障碍包括它们在不同实体类型和临床事件上的不均衡性能。在这项工作中，我们评估了当前状态的批处理语言模型（PLM）在这些任务上，包括单语言模型Med7以及多语言大语言模型（LLM）XLM-RoBERTa。我们比较了它们的优点和缺点，使用历史药物挖掘共享任务数据集。我们报告了我们在这些练习中获得的发现，以便未来研究如何解决这些问题，例如如何组合它们的输出、合并这些模型，或者如何提高它们的总准确率通过ensemble学习和数据扩展。MedMine是M3Initiave的一部分，详情请参考<https://github.com/HECTA-UoM/M3>。
</details></li>
</ul>
<hr>
<h2 id="A-sparse-coding-approach-to-inverse-problems-with-application-to-microwave-tomography-imaging"><a href="#A-sparse-coding-approach-to-inverse-problems-with-application-to-microwave-tomography-imaging" class="headerlink" title="A sparse coding approach to inverse problems with application to microwave tomography imaging"></a>A sparse coding approach to inverse problems with application to microwave tomography imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03818">http://arxiv.org/abs/2308.03818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesar F. Caiafa, Ramiro M. Irastorza</li>
<li>for:  solve ill-posed inverse imaging problems in various domains, such as medical diagnosis and astronomical studies.</li>
<li>methods:  use sparse representation of images, a realistic and effective generative model for natural images inspired by the visual system of mammals, to address ill-posed linear inverse problems.</li>
<li>results:  extend the application of sparse coding to solve non-linear and ill-posed problems in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.Here are the three points in Simplified Chinese text:</li>
<li>for: 解决不同领域的各种各样的反射图像问题，如医学诊断和天文学研究。</li>
<li>methods: 使用自然图像的稀疏表示，这是一种基于哺乳动物视觉系统的实用和有效的生成模型，来解决线性不定的反射图像问题。</li>
<li>results: 将稀疏码应用到微波tomography图像重建中，以解决非线性和不定的问题，可能会提高现有算法的性能。<details>
<summary>Abstract</summary>
Inverse imaging problems that are ill-posed can be encountered across multiple domains of science and technology, ranging from medical diagnosis to astronomical studies. To reconstruct images from incomplete and distorted data, it is necessary to create algorithms that can take into account both, the physical mechanisms responsible for generating these measurements and the intrinsic characteristics of the images being analyzed. In this work, the sparse representation of images is reviewed, which is a realistic, compact and effective generative model for natural images inspired by the visual system of mammals. It enables us to address ill-posed linear inverse problems by training the model on a vast collection of images. Moreover, we extend the application of sparse coding to solve the non-linear and ill-posed problem in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.
</details>
<details>
<summary>摘要</summary>
各种科学和技术领域中的反射 imaging 问题可能会出现不定性，从医疗诊断到天文学研究。为重建受损和扭曲数据中的图像，需要开发能够考虑物理机制生成测量数据以及图像本身内在特征的算法。在这项工作中，我们提出了图像稀疏表示，这是一种现实主义、紧凑和有效的自然图像生成模型，启发自哺乳动物视系统。这种模型可以 addresses 不定性线性反射问题，通过训练模型使用大量图像。此外，我们还扩展了稀疏编码的应用，解决微波探测成像中的非线性和不定性问题，这可能会导致现有算法的显著改进。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction"><a href="#A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction" class="headerlink" title="A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction"></a>A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08502">http://arxiv.org/abs/2308.08502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Gadgil, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: 这种研究旨在提供一种能够快速和简单地计算客户生命周期价值（CLV）的方法，以便企业更好地理解客户，扩大收入。</li>
<li>methods: 这种方法使用了元学习和堆叠回归模型，结合了袋包和强化模型的预测结果，以提高计算CLV的精度和效果。</li>
<li>results: 实验结果表明，提议的方法能够快速和简单地计算CLV，并且能够提高计算结果的准确性和稳定性。<details>
<summary>Abstract</summary>
Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency & monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a meta-learning-based stacked regression model which combines the predictions from bagging and boosting models that each is found to perform well individually. Empirical tests have been carried out on an openly available Online Retail dataset to evaluate various models and show the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
To address this, we propose a system that is both effective and simple. We develop a meta-learning-based stacked regression model that combines the predictions of bagging and boosting models, which have been found to perform well individually. We test our approach on an openly available online retail dataset and show that it is effective.
</details></li>
</ul>
<hr>
<h2 id="Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach"><a href="#Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach" class="headerlink" title="Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach"></a>Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04419">http://arxiv.org/abs/2308.04419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Pardeshi, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: 预测股票价格，帮助投资者做出最佳决策</li>
<li>methods: 提出了一种新的Long Short-Term Memory（LSTM）模型，并采用Sequential Self-Attention Mechanism（LSTM-SSAM）来提高预测精度</li>
<li>results: 对三个股票数据集（SBIN、HDFCBANK、BANKBARODA）进行了广泛的实验，结果表明提出的模型比现有模型更有效和可行，RMSE和R2评价指标都达到了最佳效果。<details>
<summary>Abstract</summary>
One of the most enticing research areas is the stock market, and projecting stock prices may help investors profit by making the best decisions at the correct time. Deep learning strategies have emerged as a critical technique in the field of the financial market. The stock market is impacted due to two aspects, one is the geo-political, social and global events on the bases of which the price trends could be affected. Meanwhile, the second aspect purely focuses on historical price trends and seasonality, allowing us to forecast stock prices. In this paper, our aim is to focus on the second aspect and build a model that predicts future prices with minimal errors. In order to provide better prediction results of stock price, we propose a new model named Long Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM). Finally, we conduct extensive experiments on the three stock datasets: SBIN, HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and feasibility of the proposed model compared to existing models. The experimental findings demonstrate that the root-mean-squared error (RMSE), and R-square (R2) evaluation indicators are giving the best results.
</details>
<details>
<summary>摘要</summary>
一个非常吸引人的研究领域是股票市场，并且预测股票价格可以帮助投资者获得最佳的决策时机。深度学习策略在金融市场中得到了广泛的应用。股票市场受到两个方面的影响：一是地域政治、社会和全球事件的影响，这些事件可能影响股票价格走势。而第二个方面则专注于历史价格走势和季节性，我们可以通过预测股票价格。在这篇论文中，我们的目标是建立一个可预测股票价格的模型，并且使用新的长期记忆（LSTM）和顺序自我注意机制（LSTM-SSAM）来提高预测结果的准确性。最后，我们对三个股票数据集（SBIN、HDFCBANK和BANKBARODA）进行了广泛的实验，实验结果表明我们提出的模型比现有模型更有效果和可行性。实验结果表明，使用 Root-Mean-Squared Error（RMSE）和R-square（R2）评价指标，我们的模型在预测股票价格方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels"><a href="#Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels" class="headerlink" title="Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels"></a>Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03613">http://arxiv.org/abs/2308.03613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengming Lin, Yan Xia, Nishant Ravikumar, Qiongyao Liu, Michael MacRaild, Alejandro F Frangi</li>
<li>for: 这篇论文是为了准确分类脑血管而设计的。</li>
<li>methods: 这篇论文使用了进步式半监督学习、适应性训练策略和边界增强等新技术。</li>
<li>results: 实验结果显示，这篇论文的方法在3DRA数据集上实现了 mesh-based 分类 метри値的超越。它能够充分利用部分和暗箱注意的数据，进而实现了优秀的分类性能。<details>
<summary>Abstract</summary>
Accurate segmentation of brain vessels is crucial for cerebrovascular disease diagnosis and treatment. However, existing methods face challenges in capturing small vessels and handling datasets that are partially or ambiguously annotated. In this paper, we propose an adaptive semi-supervised approach to address these challenges. Our approach incorporates innovative techniques including progressive semi-supervised learning, adaptative training strategy, and boundary enhancement. Experimental results on 3DRA datasets demonstrate the superiority of our method in terms of mesh-based segmentation metrics. By leveraging the partially and ambiguously labeled data, which only annotates the main vessels, our method achieves impressive segmentation performance on mislabeled fine vessels, showcasing its potential for clinical applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>>精准分割脑血管是脑血管疾病诊断和治疗中的关键。然而，现有方法在捕捉小血管和处理部分或杂乱标注的数据集时遇到困难。在这篇论文中，我们提出了一种适应式半supervised方法来解决这些问题。我们的方法包括进步式半supervised学习、适应性训练策略和边界增强等创新技术。在3DRA数据集上进行实验，我们的方法在基于网格的分割指标上表现出色。通过利用部分和杂乱标注的数据，我们的方法在偏移的细血管上实现了优秀的分割性能，这显示了它在临床应用中的潜力。<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals"><a href="#A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals" class="headerlink" title="A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals"></a>A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05759">http://arxiv.org/abs/2308.05759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Douglas A. Almeida, Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 本研究旨在开发一种基于光谱学振荡分析的睡眠阶段分类模型，以提高睡眠质量和全身健康。</li>
<li>methods: 本研究使用了EXTREME GRADIENT BOOSTING（XGBoost）算法和PPG信号和活动计数特征进行睡眠阶段分类。</li>
<li>results: 本研究的方法与当前状态之册方法相比，敏感性为91.15 $\pm$ 1.16%, 特征选择率为53.66 $\pm$ 1.12%, F1分数为83.88 $\pm$ 0.56%, κ值为48.0 $\pm$ 0.86%。<details>
<summary>Abstract</summary>
Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our method was comparable to current state-of-the-art methods with a Sensitivity of 91.15 $\pm$ 1.16%, Specificity of 53.66 $\pm$ 1.12%, F1-score of 83.88 $\pm$ 0.56%, and Kappa of 48.0 $\pm$ 0.86%. Our method offers a significant improvement over other approaches as it uses a reduced number of features, making it suitable for implementation in wearable devices that have limited computational power.
</details>
<details>
<summary>摘要</summary>
睡眠是我们身体和心理健康的重要组成部分。它对我们的情绪、记忆和认知功能以及身体的鲁棒性和免疫系统产生重要影响。睡眠阶段的分类是评估睡眠质量的必备步骤，它可以提供评估睡眠质量的度量，以及身体在这一期间的功能如何。聚光折射（PPG）已经被证明可以用于睡眠阶段推断，因此它可以单独使用或与其他信号结合使用来确定睡眠阶段。这些信息非常有价值，可以用于发现可能存在的睡眠问题，并开发改善睡眠质量和整体健康的策略。在这项工作中，我们提出了基于极限梯度提升（XGBoost）算法和PPG信号和活动计数特征的机器学习睡眠-醒目分类模型。我们的方法与当前状态的方法相比，表现出了相似的性能，具体来说，敏感性为91.15 $\pm$ 1.16%，特异性为53.66 $\pm$ 1.12%，F1分数为83.88 $\pm$ 0.56%，κ值为48.0 $\pm$ 0.86%。我们的方法在计算能力有限的穿戴设备中实现更加可行，因此它对现有的方法具有显著改进。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Early-Stopping-in-Evolutionary-Direct-Policy-Search"><a href="#Generalized-Early-Stopping-in-Evolutionary-Direct-Policy-Search" class="headerlink" title="Generalized Early Stopping in Evolutionary Direct Policy Search"></a>Generalized Early Stopping in Evolutionary Direct Policy Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03574">http://arxiv.org/abs/2308.03574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonreposit/gesp">https://github.com/anonreposit/gesp</a></li>
<li>paper_authors: Etor Arza, Leni K. Le Goff, Emma Hart</li>
<li>for: 这篇论文主要针对的是优化问题中的长时间评估时间问题，特别是在物理世界中进行评估，例如在机器人应用中。</li>
<li>methods: 本文提出了一个早期停止方法来解决这个问题，这个方法只需要在每个时间步骤中考虑目标值，不需要对问题本身进行具体的知识。</li>
<li>results: 根据五个来自游戏、机器人和 класи控制领域的直接政策搜寻环境中的测试，提出的早期停止条件可以节省大约75%的计算时间，并且与问题特有的停止条件相比，它表现更加稳定且更通用。<details>
<summary>Abstract</summary>
Lengthy evaluation times are common in many optimization problems such as direct policy search tasks, especially when they involve conducting evaluations in the physical world, e.g. in robotics applications. Often, when evaluating a solution over a fixed time period, it becomes clear that the objective value will not increase with additional computation time (for example, when a two-wheeled robot continuously spins on the spot). In such cases, it makes sense to stop the evaluation early to save computation time. However, most approaches to stop the evaluation are problem-specific and need to be specifically designed for the task at hand. Therefore, we propose an early stopping method for direct policy search. The proposed method only looks at the objective value at each time step and requires no problem-specific knowledge.   We test the introduced stopping criterion in five direct policy search environments drawn from games, robotics, and classic control domains, and show that it can save up to 75% of the computation time. We also compare it with problem-specific stopping criteria and demonstrate that it performs comparably while being more generally applicable.
</details>
<details>
<summary>摘要</summary>
长时间的评估时间是许多优化问题的常见现象，如直接策略搜索任务，尤其在物理世界中进行评估，例如在 robotics 应用中。经常情况下，在一定时间间评估解决方案时，会发现目标值不会随着计算时间增加（例如，一辆两轮摩托车连续旋转在一处）。在这些情况下，可以提前结束评估以避免浪费计算时间。然而，大多数止评估方法是任务特定的，需要特定的问题知识。因此，我们提出了一种止评估方法，只需要在每个时间步骤中考虑目标值即可，不需要任务特定的知识。我们在五个直接策略搜索环境中测试了引入的停止标准，这些环境来自游戏、 роботи克和 класси控制领域。我们发现，该方法可以将计算时间减少到75%。我们还与任务特定的停止标准进行比较，并证明它在通用性方面与其相当，而且更加通用。
</details></li>
</ul>
<hr>
<h2 id="When-Federated-Learning-meets-Watermarking-A-Comprehensive-Overview-of-Techniques-for-Intellectual-Property-Protection"><a href="#When-Federated-Learning-meets-Watermarking-A-Comprehensive-Overview-of-Techniques-for-Intellectual-Property-Protection" class="headerlink" title="When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection"></a>When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03573">http://arxiv.org/abs/2308.03573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Lansari, Reda Bellafqira, Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, Gouenou Coatrieux</li>
<li>for: 本研究は、 Federated Learning（FL）の领域での标识技术に関するOverviewを提供します。</li>
<li>methods: 本研究では、FLの特有の制约に対応するために、DNN标识法の新しい挑戦と机会に焦点を当てています。</li>
<li>results: 本研究では、过去5年间におけるFL标识法の最新の进歩について详しく述べています。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a technique that allows multiple participants to collaboratively train a Deep Neural Network (DNN) without the need of centralizing their data. Among other advantages, it comes with privacy-preserving properties making it attractive for application in sensitive contexts, such as health care or the military. Although the data are not explicitly exchanged, the training procedure requires sharing information about participants' models. This makes the individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in the context of Machine Learning (ML), DNN Watermarking methods have been developed during the last five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in Federated Learning watermarking, shedding light on the new challenges and opportunities that arise in this field.
</details>
<details>
<summary>摘要</summary>
《联合学习（Federated Learning，FL）技术 Allow multiple participants to collaboratively train a deep neural network (DNN) without centralizing their data. Among other advantages, it has privacy-preserving properties that make it attractive for sensitive applications, such as healthcare or the military. However, the training process requires sharing information about participants' models, making individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in machine learning (ML), DNN watermarking methods have been developed over the past five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in federated learning watermarking, highlighting the new challenges and opportunities that arise in this field.》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit"><a href="#Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit" class="headerlink" title="Provably Efficient Learning in Partially Observable Contextual Bandit"></a>Provably Efficient Learning in Partially Observable Contextual Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03572">http://arxiv.org/abs/2308.03572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueping Gong, Jiheng Zhang</li>
<li>for: 这个研究探索了对受限知识和部分隐藏因素的偏见 Transfer Learning 在内部 Bandit 中，并研究了在这种情况下的 Casual 影响分析和估计错误。</li>
<li>methods: 本研究首先将问题转换为确定或偏见 Casual 效果的决策问题，并透过阶段性解决 Linear Programming 问题来获得 Casual 上下文范围内的统计误差。然后，我们运用这些 Sampling 算法来获得可靠的数据描述和估计误差。</li>
<li>results: 我们证明了我们的 Casually 增强的 Bandit 算法可以超越传统 Bandit 算法，并在任务中 Handle 通用 Context 分布时提高了训练速度和性能。此外，我们还进行了实验，证明了我们的策略在实际应用中比现有的方法更高效。<details>
<summary>Abstract</summary>
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions, our method improves the order dependence on function space size compared with previous literatures. We formally prove that our causally enhanced algorithms outperform classical bandit algorithms and achieve orders of magnitude faster convergence rates. Finally, we perform simulations that demonstrate the efficiency of our strategy compared to the current state-of-the-art methods. This research has the potential to enhance the performance of contextual bandit agents in real-world applications where data is scarce and costly to obtain.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了转移学习在部分可见Contextual Bandit中， где代理人具有其他代理人的有限知识以及隐藏的共同因素的局部信息。我们首先将问题转化为标识或部分标识 causal effect  между动作和奖励的优化问题。为解这些优化问题，我们将原始的不确定分布函数约化为线性约化，然后通过顺序解 linear program 来获取 causal bound ，考虑到估计误差。我们的抽样算法提供了desirable的收敛结果，并且我们可以用这些 causal bound 来改进经典bandit算法，从而影响行动集和函数空间的大小。我们正式证明我们的 causally enhanced 算法比经典bandit算法更高效，并且可以在函数近似任务中实现更高的速度比。最后，我们在实验中证明了我们的策略比现有的方法更高效。这些研究有望提高实际应用中的 Contextual Bandit 代理人性能，当数据稀缺和昂贵时。
</details></li>
</ul>
<hr>
<h2 id="Partial-identification-of-kernel-based-two-sample-tests-with-mismeasured-data"><a href="#Partial-identification-of-kernel-based-two-sample-tests-with-mismeasured-data" class="headerlink" title="Partial identification of kernel based two sample tests with mismeasured data"></a>Partial identification of kernel based two sample tests with mismeasured data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03570">http://arxiv.org/abs/2308.03570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ron Nafshi, Maggie Makar</li>
<li>for: 本研究旨在针对机器学习应用中的两种分布之间的差异探究，并且 relax 了现有文献中假设有错误样本的假设。</li>
<li>methods: 本研究使用了非 Parametric 两种样本测试，包括最大均值差 (MMD)，并且研究了在 $\epsilon$ 污染下的 MMD 估计。</li>
<li>results: 本研究显示了在 $\epsilon$ 污染下，常用的 MMD 估计不可靠，而我们提出了一个方法来估计 MMD 的上下限，并证明这个方法会对 MMD 的估计进行更加精确的 bounds。使用了三个数据集，我们还证明了我们的方法比于其他方法更加稳定和有更好的性能。<details>
<summary>Abstract</summary>
Nonparametric two-sample tests such as the Maximum Mean Discrepancy (MMD) are often used to detect differences between two distributions in machine learning applications. However, the majority of existing literature assumes that error-free samples from the two distributions of interest are available.We relax this assumption and study the estimation of the MMD under $\epsilon$-contamination, where a possibly non-random $\epsilon$ proportion of one distribution is erroneously grouped with the other. We show that under $\epsilon$-contamination, the typical estimate of the MMD is unreliable. Instead, we study partial identification of the MMD, and characterize sharp upper and lower bounds that contain the true, unknown MMD. We propose a method to estimate these bounds, and show that it gives estimates that converge to the sharpest possible bounds on the MMD as sample size increases, with a convergence rate that is faster than alternative approaches. Using three datasets, we empirically validate that our approach is superior to the alternatives: it gives tight bounds with a low false coverage rate.
</details>
<details>
<summary>摘要</summary>
非 Parametric 两个样本测试，如最大均值差 (MMD)，在机器学习应用中 frequently 用于检测两个分布之间的差异。然而，现有的大多数文献假设可以获得无错的样本从两个分布的兴趣中。我们松弛这个假设，研究在 $\epsilon$-杂杂中测试 MMD 的估计，其中 $\epsilon$ 可能是非随机的。我们表明，在 $\epsilon$-杂杂中，通常的估计 MMD 不可靠。而我们研究 MMD 的部分标识，并Characterize 它们是否可以包含真实不知道的 MMD。我们提出了一种方法来估计这些 bound，并证明它们的估计会随样本大小增加，与其他方法相比，具有更快的收敛速率。使用三个数据集，我们实际验证了我们的方法的优越性：它们给出了紧凑的 bound，低于 false coverage 率。
</details></li>
</ul>
<hr>
<h2 id="A-Transfer-Learning-Framework-for-Proactive-Ramp-Metering-Performance-Assessment"><a href="#A-Transfer-Learning-Framework-for-Proactive-Ramp-Metering-Performance-Assessment" class="headerlink" title="A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment"></a>A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03542">http://arxiv.org/abs/2308.03542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Ma, Adrian Cottam, Mohammad Razaur Rahman Shaon, Yao-Jan Wu</li>
<li>for: 本研究旨在评估幕间计量系统的表现，并对交通管理策略的效iveness进行评估。</li>
<li>methods: 本研究采用了机器学习技术，通过学习before和after情况下的交通状况特征，对新的幕间计量控制策略的效iveness进行预测。</li>
<li>results: 实验结果表明，提出的框架可以成功地预测高速公路交通参数（速度、占用率和流速）的after情况，并可以作为评估幕间计量表现的alternative。<details>
<summary>Abstract</summary>
Transportation agencies need to assess ramp metering performance when deploying or expanding a ramp metering system. The evaluation of a ramp metering strategy is primarily centered around examining its impact on freeway traffic mobility. One way these effects can be explored is by comparing traffic states, such as the speed before and after the ramp metering strategy has been altered. Predicting freeway traffic states for the after scenarios following the implementation of a new ramp metering control strategy could offer valuable insights into the potential effectiveness of the target strategy. However, the use of machine learning methods in predicting the freeway traffic state for the after scenarios and evaluating the effectiveness of transportation policies or traffic control strategies such as ramp metering is somewhat limited in the current literature. To bridge the research gap, this study presents a framework for predicting freeway traffic parameters (speed, occupancy, and flow rate) for the after situations when a new ramp metering control strategy is implemented. By learning the association between the spatial-temporal features of traffic states in before and after situations for known freeway segments, the proposed framework can transfer this learning to predict the traffic parameters for new freeway segments. The proposed framework is built upon a transfer learning model. Experimental results show that the proposed framework is feasible for use as an alternative for predicting freeway traffic parameters to proactively evaluate ramp metering performance.
</details>
<details>
<summary>摘要</summary>
Transportation agencies need to assess ramp metering performance when deploying or expanding a ramp metering system. The evaluation of a ramp metering strategy is primarily centered around examining its impact on freeway traffic mobility. One way these effects can be explored is by comparing traffic states, such as the speed before and after the ramp metering strategy has been altered. Predicting freeway traffic states for the after scenarios following the implementation of a new ramp metering control strategy could offer valuable insights into the potential effectiveness of the target strategy. However, the use of machine learning methods in predicting the freeway traffic state for the after scenarios and evaluating the effectiveness of transportation policies or traffic control strategies such as ramp metering is somewhat limited in the current literature. To bridge the research gap, this study presents a framework for predicting freeway traffic parameters (speed, occupancy, and flow rate) for the after situations when a new ramp metering control strategy is implemented. By learning the association between the spatial-temporal features of traffic states in before and after situations for known freeway segments, the proposed framework can transfer this learning to predict the traffic parameters for new freeway segments. The proposed framework is built upon a transfer learning model. Experimental results show that the proposed framework is feasible for use as an alternative for predicting freeway traffic parameters to proactively evaluate ramp metering performance.Here's the text in Simplified Chinese:交通管理机构需要评估干涉表计划的性能，当部署或扩展干涉表计划时。评估干涉表计划的策略的中心在于研究它们对高速公路交通流动性的影响。一种可以探索这些影响的方法是通过比较交通状态的速度之前和之后干涉表计划的变化。预测高速公路交通状态的后 Situations 可以提供有价值的预测干涉表计划的效果。然而，现有文献中使用机器学习方法预测高速公路交通状态的后 Situations 和评估交通政策或交通控制策略的效果是有限的。为了填补这个研究漏洞，本研究提出了一个框架，用于预测高速公路交通参数（速度、占用率和流速）的后 Situations。该框架基于转移学习模型，可以通过学习知道的高速公路段的空间-时间特征，将其传递到预测新的高速公路段的交通参数。实验结果表明，该框架是可行的，可以作为评估干涉表计划性能的代替方法。
</details></li>
</ul>
<hr>
<h2 id="On-ramp-and-Off-ramp-Traffic-Flows-Estimation-Based-on-A-Data-driven-Transfer-Learning-Framework"><a href="#On-ramp-and-Off-ramp-Traffic-Flows-Estimation-Based-on-A-Data-driven-Transfer-Learning-Framework" class="headerlink" title="On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework"></a>On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03538">http://arxiv.org/abs/2308.03538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Ma, Abolfazl Karimpour, Yao-Jan Wu<br>for: 本研究旨在提供一种数据驱动的框架，以便准确估算 freeway 上匝入和出口的流量。methods: 该框架使用了传输学习模型，该模型可以在不同的交通 Pattern、分布和特征下提供高精度的流量估算。results: 实验结果表明，提案的方法可以在不同的 freeway 上匝入和出口处提供高精度的流量估算，其中流量估算的平均绝对误差在 23.90 veh&#x2F;h 到 40.85 veh&#x2F;h 之间，root mean square error 在 34.55 veh&#x2F;h 到 57.77 veh&#x2F;h 之间。此外，相比 conventinal machine learning model，提案的方法显示更高的表现。<details>
<summary>Abstract</summary>
To develop the most appropriate control strategy and monitor, maintain, and evaluate the traffic performance of the freeway weaving areas, state and local Departments of Transportation need to have access to traffic flows at each pair of on-ramp and off-ramp. However, ramp flows are not always readily available to transportation agencies and little effort has been made to estimate these missing flows in locations where no physical sensors are installed. To bridge this research gap, a data-driven framework is proposed that can accurately estimate the missing ramp flows by solely using data collected from loop detectors on freeway mainlines. The proposed framework employs a transfer learning model. The transfer learning model relaxes the assumption that the underlying data distributions of the source and target domains must be the same. Therefore, the proposed framework can guarantee high-accuracy estimation of on-ramp and off-ramp flows on freeways with different traffic patterns, distributions, and characteristics. Based on the experimental results, the flow estimation mean absolute errors range between 23.90 veh/h to 40.85 veh/h for on-ramps, and 31.58 veh/h to 45.31 veh/h for off-ramps; the flow estimation root mean square errors range between 34.55 veh/h to 57.77 veh/h for on-ramps, and 41.75 veh/h to 58.80 veh/h for off-ramps. Further, the comparison analysis shows that the proposed framework outperforms other conventional machine learning models. The estimated ramp flows based on the proposed method can help transportation agencies to enhance the operations of their ramp control strategies for locations where physical sensors are not installed.
</details>
<details>
<summary>摘要</summary>
要开发最佳的控制策略和监测、维护和评估高速公路叉车区的交通性能，国家和地方交通厅需要有访问每对进口和出口的交通流量数据。然而，进口和出口流量并不总是可以提供给交通厅，而且过去几乎没有尝试估算这些缺失的流量。为了填补这一研究漏洞，我们提出了一种数据驱动的框架，可以准确地估算缺失的进口和出口流量，只使用高速公路主线上的循环探测器数据。我们的框架采用了传输学习模型，这种模型不需要源和目标领域数据分布之间的假设相同。因此，我们的框架可以 garantizar高精度地估算进口和出口流量，并且在不同的交通模式、分布和特点下具有广泛的应用可能性。根据实验结果，估算的进口和出口流量平均绝对误差在23.90辆/小时到40.85辆/小时之间，Root mean square error在34.55辆/小时到57.77辆/小时之间。此外，比较分析表明，我们的方法在其他 convential机器学习模型的基础上具有更高的性能。估算的进口和出口流量可以帮助交通厅在没有物理探测器的情况下提高叉车控制策略的运行效果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Feature-Learning-for-Wireless-Spectrum-Data"><a href="#Deep-Feature-Learning-for-Wireless-Spectrum-Data" class="headerlink" title="Deep Feature Learning for Wireless Spectrum Data"></a>Deep Feature Learning for Wireless Spectrum Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03530">http://arxiv.org/abs/2308.03530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ljupcho Milosheski, Gregor Cerar, Blaž Bertalanič, Carolina Fortuna, Mihael Mohorčič</li>
<li>for: 本研究旨在自动学习无supervision的Feature表示，用于无线传输卷积 clustering。</li>
<li>methods: 我们提出一种基于卷积神经网络的模型，可以自动学习输入数据的减少维度表示，比基eline PCA 减少99.3%的维度。</li>
<li>results: 我们的自动表示学习可以提取细腻的含义块，而基eline只能通过背景噪声来分类数据。<details>
<summary>Abstract</summary>
In recent years, the traditional feature engineering process for training machine learning models is being automated by the feature extraction layers integrated in deep learning architectures. In wireless networks, many studies were conducted in automatic learning of feature representations for domain-related challenges. However, most of the existing works assume some supervision along the learning process by using labels to optimize the model. In this paper, we investigate an approach to learning feature representations for wireless transmission clustering in a completely unsupervised manner, i.e. requiring no labels in the process. We propose a model based on convolutional neural networks that automatically learns a reduced dimensionality representation of the input data with 99.3% less components compared to a baseline principal component analysis (PCA). We show that the automatic representation learning is able to extract fine-grained clusters containing the shapes of the wireless transmission bursts, while the baseline enables only general separability of the data based on the background noise.
</details>
<details>
<summary>摘要</summary>
近年来，传统的特征工程过程为训练机器学习模型被深度学习架构中的特征提取层自动化。在无线网络中，许多研究都是自动学习领域相关挑战的特征表示。然而，大多数现有的工作假设了学习过程中有监督，通过标签来优化模型。在这篇论文中，我们调查了一种没有监督的方法，即无标签的自动特征表示学习方法，用于无线传输协调。我们提议一种基于卷积神经网络的模型，可以自动学习输入数据的减少维度表示，与基准PCA相比，减少了99.3%的特征量。我们显示了自动特征表示学习能够提取无线传输强度波形细腻的分布，而基准只能基于背景噪声进行概括分离。
</details></li>
</ul>
<hr>
<h2 id="AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning"><a href="#AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning" class="headerlink" title="AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning"></a>AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03526">http://arxiv.org/abs/2308.03526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaël Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad Żołna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio Gómez Colmenarejo, Aäron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, Oriol Vinyals</li>
<li>for: This paper is written to advance offline reinforcement learning algorithms by leveraging the challenging and realistic environment of StarCraft II.</li>
<li>methods: The paper introduces a new benchmark called AlphaStar Unplugged, which includes a dataset, tools, and an evaluation protocol for offline reinforcement learning. The authors also present baseline agents, including behavior cloning, offline variants of actor-critic, and MuZero.</li>
<li>results: The authors achieve a 90% win rate against a previously published AlphaStar behavior cloning agent using only offline data, improving the state of the art of agents using offline data.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为了提高偏置学习算法的进步，利用星际II的挑战性和实际性。</li>
<li>methods: 论文引入了一个新的基准点，叫做AlphaStar Unplugged，包括一个数据集、工具和评估协议。作者还提供了一些基线代理，如行为做 clone、偏置 variant 和 MuZero。</li>
<li>results: 作者使用仅偏置数据达到了90%的胜率，超过了之前发表的AlphaStar行为做 clone 代理。<details>
<summary>Abstract</summary>
StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.
</details>
<details>
<summary>摘要</summary>
星际II是一个非常具有挑战性的模拟增强学习环境之一，它是部分可见、随机、多代、需要在长时间 horizon 上进行策略规划，并且需要在实时低级别执行。此外，它还拥有活跃的职业竞赛场景。由于星际II的挑战性和Blizzard公司发布了数百万场星际II游戏记录，因此这个纸使用这些数据来建立了一个标准的基准，称为AlphaStar Unplugged，并在这个基准上引入了前所未有的挑战。我们定义了一个数据集（Blizzard发布的一个子集）、工具和标准化API для机器学习方法，以及评估协议。我们还提供了基线代理，包括行为快照、离线actor-critic和MuZero等。我们使用仅基于离线数据的方法提高了代理的状态，并达到了在之前发布的AlphaStar行为快照代理90%的赢利率。
</details></li>
</ul>
<hr>
<h2 id="Worker-Activity-Recognition-in-Manufacturing-Line-Using-Near-body-Electric-Field"><a href="#Worker-Activity-Recognition-in-Manufacturing-Line-Using-Near-body-Electric-Field" class="headerlink" title="Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field"></a>Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03514">http://arxiv.org/abs/2308.03514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungho Suh, Vitor Fortes Rey, Sizhen Bian, Yu-Chi Huang, Jože M. Rožanec, Hooman Tavakoli Ghinani, Bo Zhou, Paul Lukowicz</li>
<li>for: 提高生产效率和产品质量</li>
<li>methods:  combining IMU和body capacitance sensing modules，以及多渠道时序卷积神经网络和深度卷积LSTM的早期和晚期整合方法</li>
<li>results: 在生产线上测试和采集感知器数据后，提议的硬件和神经网络模型显示出优于基eline方法的性能，表明该方法在现实世界应用中具有潜在的潜力。此外，通过加入身体电容感测模块和特征融合方法，提议的感知原型在6.35%的提升和9.38%的高于基eline方法的macro F1分数上表现出了提升。<details>
<summary>Abstract</summary>
Manufacturing industries strive to improve production efficiency and product quality by deploying advanced sensing and control systems. Wearable sensors are emerging as a promising solution for achieving this goal, as they can provide continuous and unobtrusive monitoring of workers' activities in the manufacturing line. This paper presents a novel wearable sensing prototype that combines IMU and body capacitance sensing modules to recognize worker activities in the manufacturing line. To handle these multimodal sensor data, we propose and compare early, and late sensor data fusion approaches for multi-channel time-series convolutional neural networks and deep convolutional LSTM. We evaluate the proposed hardware and neural network model by collecting and annotating sensor data using the proposed sensing prototype and Apple Watches in the testbed of the manufacturing line. Experimental results demonstrate that our proposed methods achieve superior performance compared to the baseline methods, indicating the potential of the proposed approach for real-world applications in manufacturing industries. Furthermore, the proposed sensing prototype with a body capacitive sensor and feature fusion method improves by 6.35%, yielding a 9.38% higher macro F1 score than the proposed sensing prototype without a body capacitive sensor and Apple Watch data, respectively.
</details>
<details>
<summary>摘要</summary>
制造业为提高生产效率和产品质量而努力，投入先进的感知和控制系统。舌环感器是制造业实现这一目标的一种有前途的解决方案，因为它们可以提供不间断和不干扰的工作者活动监测。本文提出了一种新的舌环感器原型， combining IMU和体容感测模块，以认知制造线工作者的活动。为处理这些多modal的感知数据，我们提议并比较早期和晚期感知数据融合方法，用于多通道时序卷积神经网络和深度卷积LSTM。我们通过收集和标注感知数据使用我们提出的感知原型和Apple Watches在制造线测试环境中进行评估。实验结果表明，我们的提议方法可以与基准方法相比，表明我们的方法在实际应用中具有潜在的潜力。此外，我们的感知原型与身体电容感测模块和特征融合方法提高了6.35%，即使比起没有身体电容感测模块和Apple Watch数据的情况下提高9.38%的macro F1分数。
</details></li>
</ul>
<hr>
<h2 id="A-data-driven-approach-to-predict-decision-point-choice-during-normal-and-evacuation-wayfinding-in-multi-story-buildings"><a href="#A-data-driven-approach-to-predict-decision-point-choice-during-normal-and-evacuation-wayfinding-in-multi-story-buildings" class="headerlink" title="A data-driven approach to predict decision point choice during normal and evacuation wayfinding in multi-story buildings"></a>A data-driven approach to predict decision point choice during normal and evacuation wayfinding in multi-story buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03511">http://arxiv.org/abs/2308.03511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Feng, Panchamy Krishnakumari</li>
<li>For: 本研究旨在理解和预测在复杂建筑物内的人行道径选择行为，以确保行人安全。* Methods: 本研究使用了数据驱动的方法，包括建立内部网络表示和将VR坐标映射到内部表示的技术，以及一种已知的机器学习算法——Random Forest（RF）模型，来预测行人决策点选择行为。* Results: 研究发现，使用RF模型可以具有较高的预测准确率（平均为93%），比对使用logistic regression模型更高。最高的预测准确率达96%，并且测试结果表明，个人特征不会影响决策点选择。<details>
<summary>Abstract</summary>
Understanding pedestrian route choice behavior in complex buildings is important to ensure pedestrian safety. Previous studies have mostly used traditional data collection methods and discrete choice modeling to understand the influence of different factors on pedestrian route and exit choice, particularly in simple indoor environments. However, research on pedestrian route choice in complex buildings is still limited. This paper presents a data-driven approach for understanding and predicting the pedestrian decision point choice during normal and emergency wayfinding in a multi-story building. For this, we first built an indoor network representation and proposed a data mapping technique to map VR coordinates to the indoor representation. We then used a well-established machine learning algorithm, namely the random forest (RF) model to predict pedestrian decision point choice along a route during four wayfinding tasks in a multi-story building. Pedestrian behavioral data in a multi-story building was collected by a Virtual Reality experiment. The results show a much higher prediction accuracy of decision points using the RF model (i.e., 93% on average) compared to the logistic regression model. The highest prediction accuracy was 96% for task 3. Additionally, we tested the model performance combining personal characteristics and we found that personal characteristics did not affect decision point choice. This paper demonstrates the potential of applying a machine learning algorithm to study pedestrian route choice behavior in complex indoor buildings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用机器学习算法来理解人行道选择行为在复杂的内部建筑中非常重要，以确保行人安全。先前的研究主要采用传统的数据采集方法和精确选择模型来理解不同因素对人行道径和出口选择的影响，特别是在简单的室内环境中。然而，对于复杂的内部建筑中的人行道选择研究仍然有限。这篇文章介绍了一种数据驱动的方法，用于理解和预测行人决策点选择在常规和紧急导航中的多层建筑中。为此，我们首先构建了内部网络表示，并提出了将VR坐标映射到内部表示的技术。然后，我们使用一种已知的机器学习算法，即随机森林（RF）模型来预测行人决策点选择路径中的四个任务在多层建筑中。我们在内部建筑中收集了行人行为数据，并使用VR实验进行数据采集。结果显示，使用RF模型的预测精度远高于逻辑回归模型（即93%的平均精度），最高的预测精度为任务3（即96%）。此外，我们测试了模型性能的组合个人特征，并发现个人特征没有影响决策点选择。这篇文章表明了使用机器学习算法来研究复杂的内部建筑中人行道选择行为的潜力。
</details></li>
</ul>
<hr>
<h2 id="Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group"><a href="#Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group" class="headerlink" title="Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group"></a>Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03495">http://arxiv.org/abs/2308.03495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kidist Amde Mekonnen</li>
<li>for: 这个研究的目的是生成一个可靠的面部图像集，以便实现不同的人种和性别的资料分布。</li>
<li>methods: 这个研究使用了StyleGAN模型来生成面部图像，并通过控制生成过程来确保资料集中的人种和性别分布均衡。</li>
<li>results: 这个研究产生了一个可靠的面部图像集，并通过实验证明了这个资料集可以用于不同的下游任务。<details>
<summary>Abstract</summary>
For a machine learning model to generalize effectively to unseen data within a particular problem domain, it is well-understood that the data needs to be of sufficient size and representative of real-world scenarios. Nonetheless, real-world datasets frequently have overrepresented and underrepresented groups. One solution to mitigate bias in machine learning is to leverage a diverse and representative dataset. Training a model on a dataset that covers all demographics is crucial to reducing bias in machine learning. However, collecting and labeling large-scale datasets has been challenging, prompting the use of synthetic data generation and active labeling to decrease the costs of manual labeling. The focus of this study was to generate a robust face image dataset using the StyleGAN model. In order to achieve a balanced distribution of the dataset among different demographic groups, a synthetic dataset was created by controlling the generation process of StyleGaN and annotated for different downstream tasks.
</details>
<details>
<summary>摘要</summary>
为一个机器学习模型有效泛化到未看到的数据中，已经是非常了解的一点，那么数据需要具有足够的大小和表示现实世界场景。然而，实际世界数据经常会有过度和不足的群体。一种解决机器学习偏见的方法是利用多样化的和表示性的数据集。训练一个模型在覆盖所有民族的数据集是减少偏见的关键，但是收集和手动标注大规模数据集是困难的，因此使用生成 Synthetic 数据和活动标注来降低手动标注的成本。这项研究的目标是使用 StyleGAN 模型生成一个可靠的脸像数据集，以实现数据集的均衡分布。为了实现这一目标，我们控制了 StyleGAN 生成过程，并对不同下游任务进行了注释。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection"><a href="#Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection" class="headerlink" title="Exploring the Physical World Adversarial Robustness of Vehicle Detection"></a>Exploring the Physical World Adversarial Robustness of Vehicle Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03476">http://arxiv.org/abs/2308.03476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Tianyuan Zhang, Shuangcheng Liu, Weiyu Ji, Zichao Zhang, Gang Xiao</li>
<li>for: 评估实际场景下的检测模型 Robustness，但实际 эксперимент具有资源占用和复杂性的问题。</li>
<li>methods: 提出了一种创新的快照水平数据生成管道，使用 CARLA 模拟器，并建立了 Discrete and Continuous Instant-level (DCI) 数据集，可以进行三种检测模型和三种物理攻击的全面实验。</li>
<li>results: 发现 Yolo v6 具有强大的抗击性，仅在对抗攻击下出现了小于1%的 average precision (AP) 下降，而 ASA 攻击则导致了 AP 下降约14.51%，远高于其他算法。 static 场景下的识别 AP 值较高，而不同的天气条件下的结果几乎相同。<details>
<summary>Abstract</summary>
Adversarial attacks can compromise the robustness of real-world detection models. However, evaluating these models under real-world conditions poses challenges due to resource-intensive experiments. Virtual simulations offer an alternative, but the absence of standardized benchmarks hampers progress. Addressing this, we propose an innovative instant-level data generation pipeline using the CARLA simulator. Through this pipeline, we establish the Discrete and Continuous Instant-level (DCI) dataset, enabling comprehensive experiments involving three detection models and three physical adversarial attacks. Our findings highlight diverse model performances under adversarial conditions. Yolo v6 demonstrates remarkable resilience, experiencing just a marginal 6.59% average drop in average precision (AP). In contrast, the ASA attack yields a substantial 14.51% average AP reduction, twice the effect of other algorithms. We also note that static scenes yield higher recognition AP values, and outcomes remain relatively consistent across varying weather conditions. Intriguingly, our study suggests that advancements in adversarial attack algorithms may be approaching its ``limitation''.In summary, our work underscores the significance of adversarial attacks in real-world contexts and introduces the DCI dataset as a versatile benchmark. Our findings provide valuable insights for enhancing the robustness of detection models and offer guidance for future research endeavors in the realm of adversarial attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-to-forecast-power-generation-in-wind-farms-Insights-from-leveraging-hierarchical-structure"><a href="#How-to-forecast-power-generation-in-wind-farms-Insights-from-leveraging-hierarchical-structure" class="headerlink" title="How to forecast power generation in wind farms? Insights from leveraging hierarchical structure"></a>How to forecast power generation in wind farms? Insights from leveraging hierarchical structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03472">http://arxiv.org/abs/2308.03472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas English, Mahdi Abolghasemi</li>
<li>for: 预测可再生能源生产，帮助决策全球减排。</li>
<li>methods: 使用树状层次预测和整合预测，包括线性回归和梯度提升机器学习。</li>
<li>results: 跨时间层次预测超过具体时间层次预测，并且机器学习模型在大多数层次上表现较好。<details>
<summary>Abstract</summary>
Forecasting of renewable energy generation provides key insights which may help with decision-making towards global decarbonisation. Renewable energy generation can often be represented through cross-sectional hierarchies, whereby a single farm may have multiple individual generators. Hierarchical forecasting through reconciliation has demonstrated a significant increase in the quality of forecasts both theoretically and empirically. However, it is not evident whether forecasts generated by individual temporal and cross-sectional aggregation can be superior to integrated cross-temporal forecasts and to individual forecasts on more granular data. In this study, we investigate the accuracies of different cross-sectional and cross-temporal reconciliation methods using both linear regression and gradient boosting machine learning for forecasting wind farm power generation. We found that cross-temporal reconciliation is superior to individual cross-sectional reconciliation at multiple temporal aggregations. Cross-temporally reconciled machine learning base forecasts also demonstrated a high accuracy at coarser temporal granularities, which may encourage adoption for short-term wind forecasts. We also show that linear regression can outperform machine learning models across most levels in cross-sectional wind time series.
</details>
<details>
<summary>摘要</summary>
预测可再生能源生产提供关键的洞察，可以帮助决策全球减排。可再生能源生产经常可以用 Hierarchical 模型来表示，一个农场可能有多个个体发电机。通过协调预测，可以显著提高预测质量， both theoretically and empirically。然而，不知道个体时间和横向汇总预测是否高于集成时间汇总预测和更细的数据预测。本研究发现，不同的横向和时间汇总协调方法的准确率，使用线性回归和梯度提升机器学习模型预测风电厂电力生产。我们发现，协调预测在多个时间层次上都高于个体横向协调预测。同时，使用机器学习模型进行协调预测也在大部分水平上达到了高准确率，这可能会促进短期风预测的采用。此外，我们还发现了线性回归在大部分水平上可以超越机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Wide-Gaps-and-Clustering-Axioms"><a href="#Wide-Gaps-and-Clustering-Axioms" class="headerlink" title="Wide Gaps and Clustering Axioms"></a>Wide Gaps and Clustering Axioms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03464">http://arxiv.org/abs/2308.03464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek</li>
<li>for: 本研究旨在探讨k-means算法是否遵循克林伯格的距离基于减少算法的axiomaatic系统，以及如何修正k-means算法以遵循这些axioma。</li>
<li>methods: 本研究使用了两个新的clusterability性质：variational k-separability和residual k-separability，以确定k-means算法在欧几何或非欧几何空间中是否遵循克林伯格的一致性axioma。此外，本研究还提出了修正k-means算法以遵循克林伯格的贫含性axioma的方法。</li>
<li>results: 本研究发现，k-means算法在欧几何和非欧几何空间中都不遵循克林伯格的一致性axioma，但可以通过修正k-means算法来遵循这些axioma。此外，本研究还提出了一种构建测试数据集的方法，以便测试修正后的k-means算法的性能。<details>
<summary>Abstract</summary>
The widely applied k-means algorithm produces clusterings that violate our expectations with respect to high/low similarity/density and is in conflict with Kleinberg's axiomatic system for distance based clustering algorithms that formalizes those expectations in a natural way. k-means violates in particular the consistency axiom. We hypothesise that this clash is due to the not explicated expectation that the data themselves should have the property of being clusterable in order to expect the algorithm clustering hem to fit a clustering axiomatic system. To demonstrate this, we introduce two new clusterability properties, variational k-separability and residual k-separability and show that then the Kleinberg's consistency axiom holds for k-means operating in the Euclidean or non-Euclidean space. Furthermore, we propose extensions of k-means algorithm that fit approximately the Kleinberg's richness axiom that does not hold for k-means. In this way, we reconcile k-means with Kleinberg's axiomatic framework in Euclidean and non-Euclidean settings. Besides contribution to the theory of axiomatic frameworks of clustering and for clusterability theory, practical contribution is the possibility to construct {datasets for testing purposes of algorithms optimizing k-means cost function. This includes a method of construction of {clusterable data with known in advance global optimum.
</details>
<details>
<summary>摘要</summary>
广泛应用的k-means算法会生成不符我们的预期的分群结果，特别是高低相似度和密度方面的预期不符，这与克莱恩贝格的分群算法axiomaatic系统不符。k-means特别违反了一致性axioma。我们假设这是因为没有明确地预期数据本身应有分群的性质，以期望算法可以遵循分群axiomaatic系统。为了证明这一点，我们引入了两个新的分群性质：variational k-separability和residual k-separability，并证明了在欧几何或非欧几何空间中，k-means算法会遵循克莱恩贝格的一致性axioma。此外，我们提出了对k-means算法进行修改，以便更好地遵循克莱恩贝格的丰富性axioma，这些axioma不会在k-means算法中实现。这种修改可以在欧几何和非欧几何空间中进行。此外，我们还可以根据这些修改建立{测试用数据集，以便确认分群算法的效果。包括一种方法建立高度分群的数据集，并且知道在先的全局最佳解。}
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers"><a href="#High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers" class="headerlink" title="High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers"></a>High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03813">http://arxiv.org/abs/2308.03813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MWod/DeepImplant_MICCAI_2023">https://github.com/MWod/DeepImplant_MICCAI_2023</a></li>
<li>paper_authors: Marek Wodzinski, Mateusz Daniol, Daria Hemmerling, Miroslaw Socha</li>
<li>for:  This paper aims to provide an automatic, dedicated system for personalized cranial reconstruction, addressing the problem of cranial defect reconstruction.</li>
<li>methods:  The proposed method uses an iterative, transformer-based approach to complete point clouds and reconstruct cranial defects at any resolution, while being fast and resource-efficient during training and inference.</li>
<li>results:  The proposed method demonstrates superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects, compared to state-of-the-art volumetric approaches.<details>
<summary>Abstract</summary>
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed methods to the state-of-the-art volumetric approaches and show superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects.
</details>
<details>
<summary>摘要</summary>
每年千计人们因各种类型的脑部受伤而需要个性化嵌入，其手动设计成本高昂，时间费时。因此，一个自动化、专门的系统可以提高个性化脑部重建的可用性是非常感兴趣的。脑部异常完成任务可以通过专门的深度网络解决。现在，最常见的方法是使用体积表示法，并将深度网络应用于图像分割。然而，这种方法存在一些限制，不能扩展到高分辨率体积，也不会考虑数据稀缺性。在我们的工作中，我们将问题重新划分为点云完成任务。我们提出一种迭代、基于变换器的方法来重建脑部异常，可以在任何分辨率下进行重建，同时在训练和推理过程中具有快速和资源高效的特点。我们与状态元方法进行比较，并显示我们的方法在GPU内存占用量方面具有明显的优势，而无需牺牲高质量重建异常的性能。
</details></li>
</ul>
<hr>
<h2 id="Redesigning-Out-of-Distribution-Detection-on-3D-Medical-Images"><a href="#Redesigning-Out-of-Distribution-Detection-on-3D-Medical-Images" class="headerlink" title="Redesigning Out-of-Distribution Detection on 3D Medical Images"></a>Redesigning Out-of-Distribution Detection on 3D Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07324">http://arxiv.org/abs/2308.07324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anton Vasiliuk, Daria Frolova, Mikhail Belyaev, Boris Shirokikh</li>
<li>for: 本文旨在解决验证医疗图像分割中的非常规（OOD）样本检测问题。</li>
<li>methods: 本文提出一种基于下游任务的OOD检测方法，使用下游模型的性能作为图像之间的pseudometric，不需要显式地区分ID和OOD样本。</li>
<li>results: 在11个CT和MRI OOD检测挑战中，EPD metric 能够准确地评估不同方法的临床影响。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) samples for trusted medical image segmentation remains a significant challenge. The critical issue here is the lack of a strict definition of abnormal data, which often results in artificial problem settings without measurable clinical impact. In this paper, we redesign the OOD detection problem according to the specifics of volumetric medical imaging and related downstream tasks (e.g., segmentation). We propose using the downstream model's performance as a pseudometric between images to define abnormal samples. This approach enables us to weigh different samples based on their performance impact without an explicit ID/OOD distinction. We incorporate this weighting in a new metric called Expected Performance Drop (EPD). EPD is our core contribution to the new problem design, allowing us to rank methods based on their clinical impact. We demonstrate the effectiveness of EPD-based evaluation in 11 CT and MRI OOD detection challenges.
</details>
<details>
<summary>摘要</summary>
检测非常出版（OOD）样本 для可信度医疗影像分割是一个主要挑战。这里的关键问题是缺乏严格的非常定义，这经常导致人工设定的问题 без measurable clinical impact。在这篇论文中，我们重新设计了OOD检测问题，根据医疗影像的特点和相关的下游任务（例如分割）。我们提议使用下游模型的性能作为图像之间的pseudometric。这种方法允许我们根据不同样本的性能影响 assign weights，而不需要显式的ID/OOD分类。我们称之为预期性能下降（EPD）。EPD是我们对新的问题设计的核心贡献，允许我们根据临床影响排名方法。我们在11个CT和MRI OOD检测挑战中证明了EPD基于评价的效果。
</details></li>
</ul>
<hr>
<h2 id="Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data"><a href="#Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data" class="headerlink" title="Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data"></a>Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03457">http://arxiv.org/abs/2308.03457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qizhuang-qz/FedCSPC">https://github.com/qizhuang-qz/FedCSPC</a></li>
<li>paper_authors: Zhuang Qi, Lei Meng, Zitan Chen, Han Hu, Hui Lin, Xiangxu Meng</li>
<li>for: 这个研究目的是为了在隐私保护下，通过服务器端对各个客户端的本地模型进行联合学习，以获得更好的模型泛化能力。</li>
<li>methods: 这个研究使用了跨批训练（FedCSPC），它首先使用资料prototype模型（DPM）模组来学习资料模式，以帮助标准化。接着，它使用跨批对称学习（CSPC）模组来改善标准化的实现方式，以将不同来源的特征投射到一个共同的空间中，保持清晰的决策界限。</li>
<li>results: 实验结果显示，FedCSPC可以在不同资料来源之间的同类别中学习共同的特征，从而获得更好的性能，比预先的方法更好。<details>
<summary>Abstract</summary>
Federated Learning aims to learn a global model on the server side that generalizes to all clients in a privacy-preserving manner, by leveraging the local models from different clients. Existing solutions focus on either regularizing the objective functions among clients or improving the aggregation mechanism for the improved model generalization capability. However, their performance is typically limited by the dataset biases, such as the heterogeneous data distributions and the missing classes. To address this issue, this paper presents a cross-silo prototypical calibration method (FedCSPC), which takes additional prototype information from the clients to learn a unified feature space on the server side. Specifically, FedCSPC first employs the Data Prototypical Modeling (DPM) module to learn data patterns via clustering to aid calibration. Subsequently, the cross-silo prototypical calibration (CSPC) module develops an augmented contrastive learning method to improve the robustness of the calibration, which can effectively project cross-source features into a consistent space while maintaining clear decision boundaries. Moreover, the CSPC module's ease of implementation and plug-and-play characteristics make it even more remarkable. Experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study, and the results verified that FedCSPC is capable of learning the consistent features across different data sources of the same class under the guidance of calibrated model, which leads to better performance than the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedCSPC.
</details>
<details>
<summary>摘要</summary>
federated 学习旨在在服务器端学习一个通用模型，以保持所有客户端的隐私，通过客户端的本地模型之间的协同学习。现有的解决方案通常是通过客户端对象函数的规范化或者改进模型融合机制来提高模型通用能力。然而，它们的性能通常受到数据偏见的影响，如不同数据分布和缺失类。为解决这个问题，本文提出了跨批模型准确补偿方法（FedCSPC），它在服务器端使用客户端提供的额外原型信息来学习一个统一的特征空间。具体来说，FedCSPC首先使用数据模型化模块（DPM）来学习数据模式，以帮助准确补偿。接着，跨批模型准确补偿模块（CSPC）发展了一种增强了对比学习方法，可以有效地将各种来源特征投影到一个具有清晰决策边界的共同空间中，而不是只是在不同数据源之间进行准确补偿。此外，CSPC模块的易于实现和插件化特点使得它更加出佩。实验结果表明，FedCSPC能够在不同数据源之间学习一致的特征，从而实现更好的性能，而且超越了当前的方法。代码已经在 GitHub 上发布，请参考 <https://github.com/qizhuang-qz/FedCSPC>。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces"><a href="#Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces" class="headerlink" title="Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces"></a>Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03443">http://arxiv.org/abs/2308.03443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/DR-estimator-OPE-large-action">https://github.com/tatsu432/DR-estimator-OPE-large-action</a></li>
<li>paper_authors: Tatsuhiro Shimizu, Laura Forastiere</li>
<li>for: 评估无策策略（Off-Policy Evaluation）在Contextual Bandit设置下，即在具有大量动作空间的情况下。</li>
<li>methods: 本研究使用Marginalized Inverse Propensity Scoring（MIPS）和Marginalized Doubly Robust（MDR）等方法来mitigate estimator的偏差和方差问题。</li>
<li>results:  theoretically和empirically验证了MDR estimator的超越性，即在比MIPS更加强的假设下，MDR estimator still maintains variance reduction against IPS。<details>
<summary>Abstract</summary>
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
</details>
<details>
<summary>摘要</summary>
我们研究偏离策略评估（OPE）在含有大量行动的上下文抽象队列设置下。参考估计器受到严重的偏见和方差负担交易。 Parametric方法受到偏见因为难以正确地特定模型，而重要性Weighted方法受到方差。为了解决这些限制，我们提出了嵌入行动的Marginalized Inverse Propensity Scoring（MIPS）来减少估计器的方差。为了使估计器更加准确，我们提出了MIPS的双重Robust（MDR）估计器。理论分析表明，我们的估计器具有较弱的假设下的不偏性，而且保持与IPS相同的方差减少。实验证明我们的MDR估计器在现有估计器中具有最高的超越性。
</details></li>
</ul>
<hr>
<h2 id="PURL-Safe-and-Effective-Sanitization-of-Link-Decoration"><a href="#PURL-Safe-and-Effective-Sanitization-of-Link-Decoration" class="headerlink" title="PURL: Safe and Effective Sanitization of Link Decoration"></a>PURL: Safe and Effective Sanitization of Link Decoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03417">http://arxiv.org/abs/2308.03417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/purl-sanitizer/purl">https://github.com/purl-sanitizer/purl</a></li>
<li>paper_authors: Shaoor Munir, Patrick Lee, Umar Iqbal, Zubair Shafiq, Sandra Siby</li>
<li>for: 防止隐私浏览器中的第三方cookies和浏览器指纹被新的跟踪方法绕过安全措施。</li>
<li>methods: 使用机器学习方法，利用浏览器执行页面的跨层图表来检测和净化链接装饰中的跟踪信息。</li>
<li>results: PURL可以准确地检测和净化链接装饰中的跟踪信息，比已有Countermeasures更高效和更具有抗辐射性。在测试 top-million 网站时，发现了广泛的链接装饰滥用，其中包括著名的广告商和跟踪者，以便从浏览器存储、邮箱地址和指纹扫描中收集用户信息。<details>
<summary>Abstract</summary>
While privacy-focused browsers have taken steps to block third-party cookies and browser fingerprinting, novel tracking methods that bypass existing defenses continue to emerge. Since trackers need to exfiltrate information from the client- to server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. We present PURL, a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. We use PURL to perform a measurement study on top-million websites. We find that link decorations are widely abused by well-known advertisers and trackers to exfiltrate user information collected from browser storage, email addresses, and scripts involved in fingerprinting.
</details>
<details>
<summary>摘要</summary>
While privacy-focused browsers have taken steps to block third-party cookies and browser fingerprinting, novel tracking methods that bypass existing defenses continue to emerge. Since trackers need to exfiltrate information from the client- to server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. We present PURL, a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. We use PURL to perform a measurement study on top-million websites. We find that link decorations are widely abused by well-known advertisers and trackers to exfiltrate user information collected from browser storage, email addresses, and scripts involved in fingerprinting.Here's the translation in Traditional Chinese:随着隐私专注浏览器对第三方Cookie和浏览器指纹进行防护，新的跟踪方法继续出现，这些方法可以绕过现有的防护措施。因为追踪者需要从客户端将资讯传到服务器端，因此一个可能的垂直方法是检测并删除装饰链接中的追踪资讯。我们提出了PURL，一种基于页面执行的机器学习方法，可以安全地和有效地删除链接装饰。我们的评估显示，PURL Significantly Outperform现有的对抗策略，并且具有较高的精度和降低网站损坏的能力，同时具有对常见的逃脱技巧的坚固性。我们使用PURL进行了顶千个网站的量测研究，发现链接装饰被著名的广告商和追踪者广泛运用，以将用户资讯从浏览器存储、电子邮件地址和掌握的指纹资讯泄露出来。
</details></li>
</ul>
<hr>
<h2 id="Noncompact-uniform-universal-approximation"><a href="#Noncompact-uniform-universal-approximation" class="headerlink" title="Noncompact uniform universal approximation"></a>Noncompact uniform universal approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03812">http://arxiv.org/abs/2308.03812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teun D. H. van Nuland</li>
<li>for: 这个论文探讨了一般化的universal approximation theorem的推广，具体来说是在非可ompact的输入空间 $\mathbb R^n$ 上进行的。</li>
<li>methods: 这个论文使用了神经网络来进行uniform approximation，并且研究了不同 activation functions 的影响。</li>
<li>results: 研究发现，对于所有的连续函数，只要它们在 infinity 处消失， Then all continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. In addition, the paper also found some unexpected results, such as the algebra of uniformly approximable functions being independent of the activation function and the number of hidden layers.<details>
<summary>Abstract</summary>
The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. When $\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\geq2$, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\varphi$ differs from its right limit (for instance, when $\varphi$ is sigmoidal) the algebra $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq2$) is independent of $\varphi$ and $l$, and equals the closed span of products of sigmoids composed with one-dimensional projections. If the left limit of $\varphi$ equals its right limit, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq1$) equals the (real part of the) commutative resolvent algebra, a C*-algebra which is used in mathematical approaches to quantum theory. In the latter case, the algebra is independent of $l\geq1$, whereas in the former case $\overline{\mathcal{N}_\varphi^2(\mathbb R^n)}$ is strictly bigger than $\overline{\mathcal{N}_\varphi^1(\mathbb R^n)}$.
</details>
<details>
<summary>摘要</summary>
“ universal approximation theorem 被推广到非紧集 $\mathbb R^n$ 上的输入空间。所有的连续函数，当 $x$ 趋于 $\pm \infty$ 时，有 asymptotically linear 的行为的函数 $\varphi\neq0$，可以通过含有一个隐藏层的神经网络进行不同化。当 $\varphi$ 又是受限的时，我们可以准确地确定可以通过神经网络进行不同化的函数，并且得到了以下意外的结果。对于所有的 $n$ 和 $l\geq2$，$\mathcal{N}^l_\varphi(\mathbb R^n)$ 是一个点wise乘法的代数。如果 $\varphi$ 的左限与右限不同（例如，当 $\varphi$ 是截股函数），则 $\mathcal{N}^l_\varphi(\mathbb R^n)$ ($l\geq2$) 是 $\varphi$ 和 $l$ 的独立的代数，等于一个由截股函数与一维投影组成的关闭 span。如果 $\varphi$ 的左限与右限相同（例如，当 $\varphi$ 是满足条件的函数），则 $\mathcal{N}^l_\varphi(\mathbb R^n)$ ($l\geq1$) 是一个（实部的） коммуutatvie 分解代数，这种代数在数学方面的量子理论中使用。在后者情况下，该代数是 $l\geq1$ 的独立的，而在前者情况下，$\mathcal{N}^2_\varphi(\mathbb R^n)$ 是 $\mathcal{N}^1_\varphi(\mathbb R^n)$ 的strictly larger。”
</details></li>
</ul>
<hr>
<h2 id="Applied-metamodelling-for-ATM-performance-simulations"><a href="#Applied-metamodelling-for-ATM-performance-simulations" class="headerlink" title="Applied metamodelling for ATM performance simulations"></a>Applied metamodelling for ATM performance simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03404">http://arxiv.org/abs/2308.03404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoffer Riis, Francisco N. Antunes, Tatjana Bolić, Gérald Gurtner, Andrew Cook, Carlos Lima Azevedo, Francisco Câmara Pereira</li>
<li>for: 支持空管决策making，提高空管 simulator的解释能力和可repeatability。</li>
<li>methods: 使用活动学习和 SHAP 值 integrate into simulation metamodels，快速浮现空管 simulator中输入和输出变量之间的隐藏关系。</li>
<li>results: 在使用 ‘Mercury’ 空管 simulator的实际场景中，XALM 能够增强simulation解释性和理解变量之间的交互关系，并且与非活动学习 мета模型相比，具有更好的解释能力。<details>
<summary>Abstract</summary>
The use of Air traffic management (ATM) simulators for planing and operations can be challenging due to their modelling complexity. This paper presents XALM (eXplainable Active Learning Metamodel), a three-step framework integrating active learning and SHAP (SHapley Additive exPlanations) values into simulation metamodels for supporting ATM decision-making. XALM efficiently uncovers hidden relationships among input and output variables in ATM simulators, those usually of interest in policy analysis. Our experiments show XALM's predictive performance comparable to the XGBoost metamodel with fewer simulations. Additionally, XALM exhibits superior explanatory capabilities compared to non-active learning metamodels.   Using the `Mercury' (flight and passenger) ATM simulator, XALM is applied to a real-world scenario in Paris Charles de Gaulle airport, extending an arrival manager's range and scope by analysing six variables. This case study illustrates XALM's effectiveness in enhancing simulation interpretability and understanding variable interactions. By addressing computational challenges and improving explainability, XALM complements traditional simulation-based analyses.   Lastly, we discuss two practical approaches for reducing the computational burden of the metamodelling further: we introduce a stopping criterion for active learning based on the inherent uncertainty of the metamodel, and we show how the simulations used for the metamodel can be reused across key performance indicators, thus decreasing the overall number of simulations needed.
</details>
<details>
<summary>摘要</summary>
使用空交通管理（ATM）模拟器进行规划和运营可能会面临模拟复杂性挑战。这篇论文介绍了XALM（可解释性活动学习元模型），一种三步框架，它将活动学习和SHAP（SHapley Additive exPlanations）值 integrate到模拟元模型中，以支持ATM决策。XALM有效地揭示了ATM模拟器中输入和输出变量之间的隐藏关系，通常在政策分析中对于有价值。我们的实验表明，XALM的预测性能与XGBoost元模型相当，但使用 fewer simulations。此外，XALM的解释能力比非活动学习元模型更高。使用“Mercury”（飞机和乘客）ATM模拟器，XALM在法国 CHARLES DE GAULLE机场的一个实际场景中应用，分析了六个变量。这个案例说明了XALM在提高模拟解释性和理解变量间关系方面的效iveness。通过解决计算挑战和提高解释性，XALM补充了传统基于模拟的分析。最后，我们介绍了两种实用的方法来降低元模型计算的压力：我们引入基于元模型的活动学习停止 criterion，以及如何将模拟用于元模型可以重用到关键性能指标上，从而降低总的模拟数量。
</details></li>
</ul>
<hr>
<h2 id="Towards-Machine-Learning-based-Fish-Stock-Assessment"><a href="#Towards-Machine-Learning-based-Fish-Stock-Assessment" class="headerlink" title="Towards Machine Learning-based Fish Stock Assessment"></a>Towards Machine Learning-based Fish Stock Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03403">http://arxiv.org/abs/2308.03403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Lüdtke, Maria E. Pierce</li>
<li>for: This paper aims to improve the estimation and forecast of fish stock parameters like recruitment and spawning stock biomass, which is crucial for sustainable fisheries management.</li>
<li>methods: The authors propose a hybrid model that combines classical statistical stock assessment models with supervised machine learning, specifically gradient boosted trees. The model leverages the initial estimate provided by the classical model and uses the ML model to make a post-hoc correction to improve accuracy.</li>
<li>results: The authors experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.<details>
<summary>Abstract</summary>
The accurate assessment of fish stocks is crucial for sustainable fisheries management. However, existing statistical stock assessment models can have low forecast performance of relevant stock parameters like recruitment or spawning stock biomass, especially in ecosystems that are changing due to global warming and other anthropogenic stressors. In this paper, we investigate the use of machine learning models to improve the estimation and forecast of such stock parameters. We propose a hybrid model that combines classical statistical stock assessment models with supervised ML, specifically gradient boosted trees. Our hybrid model leverages the initial estimate provided by the classical model and uses the ML model to make a post-hoc correction to improve accuracy. We experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.
</details>
<details>
<summary>摘要</summary>
Accurate assessment of fish stocks is crucial for sustainable fisheries management. However, existing statistical stock assessment models can have low forecast performance of relevant stock parameters like recruitment or spawning stock biomass, especially in ecosystems that are changing due to global warming and other anthropogenic stressors. In this paper, we investigate the use of machine learning models to improve the estimation and forecast of such stock parameters. We propose a hybrid model that combines classical statistical stock assessment models with supervised machine learning, specifically gradient boosted trees. Our hybrid model leverages the initial estimate provided by the classical model and uses the machine learning model to make a post-hoc correction to improve accuracy. We experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.Here's the text with some notes on the translation:* "accurate assessment" is 准确评估 (zhèngjù píngzhèng)* "fish stocks" is 鱼类资源 (yúlèi zīyuán)* "sustainable fisheries management" is 可持续鱼业管理 (kěchéngxù yúyèguǎn lí)* "existing statistical stock assessment models" is 现有的统计鱼类评估模型 (xiàn yǒu de tōngjī yúlèi píngzhèng módel)* "global warming" is 全球变暖 (quánqiú biàndòng)* "anthropogenic stressors" is 人类活动对鱼类资源的影响 (rénxìng huódòng duì yúlèi zīyuán de yìngxiàn)* "machine learning models" is 机器学习模型 (jīshì xuéxí módel)* "hybrid model" is 混合模型 (fù hé módel)* "classical statistical stock assessment models" is 传统的统计鱼类评估模型 (chuán tiān de tōngjī yúlèi píngzhèng módel)* "post-hoc correction" is 后置 corrections (hòu zhì jièduì)* "recruitment" is 增殖 (zēngshòu)* "spawning stock biomass" is 繁殖床质量 (shèngcháng zhīyù)I hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network"><a href="#Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network" class="headerlink" title="Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network"></a>Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03382">http://arxiv.org/abs/2308.03382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhou Chen, Qian Huang, Yulin Chen, Linyi Qian, Chengyuan Yu</li>
<li>for: 本研究旨在提出一种基于二支分支网络和卷积块的杂合注意力的核实例分割方法，以提高核实例分割的精度和效率。</li>
<li>methods: 本方法使用了hybrid attention based residual U-blocks和context fusion block (CF-block)来同时预测目标信息和目标极值。CF-block可以有效地抽取和融合网络中的Contextual information。</li>
<li>results: 对于BNS、MoNuSeg、CoNSeg和CPM-17等数据集，实验结果表明，提出的方法比state-of-the-art方法具有更高的性能。<details>
<summary>Abstract</summary>
Nucleus image segmentation is a crucial step in the analysis, pathological diagnosis, and classification, which heavily relies on the quality of nucleus segmentation. However, the complexity of issues such as variations in nucleus size, blurred nucleus contours, uneven staining, cell clustering, and overlapping cells poses significant challenges. Current methods for nucleus segmentation primarily rely on nuclear morphology or contour-based approaches. Nuclear morphology-based methods exhibit limited generalization ability and struggle to effectively predict irregular-shaped nuclei, while contour-based extraction methods face challenges in accurately segmenting overlapping nuclei. To address the aforementioned issues, we propose a dual-branch network using hybrid attention based residual U-blocks for nucleus instance segmentation. The network simultaneously predicts target information and target contours. Additionally, we introduce a post-processing method that combines the target information and target contours to distinguish overlapping nuclei and generate an instance segmentation image. Within the network, we propose a context fusion block (CF-block) that effectively extracts and merges contextual information from the network. Extensive quantitative evaluations are conducted to assess the performance of our method. Experimental results demonstrate the superior performance of the proposed method compared to state-of-the-art approaches on the BNS, MoNuSeg, CoNSeg, and CPM-17 datasets.
</details>
<details>
<summary>摘要</summary>
核心图像分割是生物学分析、病理诊断和分类中的关键步骤，其中核心分割质量直接影响分析结果。然而，核心的变化、杂乱的核心边缘、不均匀染色、细胞堆叠和重叠细胞等问题带来了 significiant challenges。现有的核心分割方法主要基于核心形态或边缘提取方法。核心形态基于方法在面对不规则形状的核心时表现有限的泛化能力，而边缘提取方法在处理重叠的核心时遇到困难。为了解决上述问题，我们提出了一种基于 dual-branch 网络和卷积 residual U-块的核心实例分割方法。该网络同时预测目标信息和目标边缘。此外，我们引入了一种将目标信息和目标边缘结合的后处理方法，以便在重叠的核心之间分割。在网络中，我们提出了一种 Context Fusion Block（CF-块），用于有效地提取和融合网络中的Contextual information。我们对方法进行了广泛的量化评估，实验结果表明我们的方法在 BNS、MoNuSeg、CoNSeg 和 CPM-17 数据集上的性能较为出色。
</details></li>
</ul>
<hr>
<h2 id="A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding"><a href="#A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding" class="headerlink" title="A reading survey on adversarial machine learning: Adversarial attacks and their understanding"></a>A reading survey on adversarial machine learning: Adversarial attacks and their understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03363">http://arxiv.org/abs/2308.03363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Kotyan</li>
<li>for: 本研究旨在系统地探讨针对神经网络的攻击方法和其对神经网络的影响。</li>
<li>methods: 本文使用了现有的攻击方法和对它们的分类，以及对攻击方法的分析和评估。</li>
<li>results: 本文提供了对现有攻击方法的系统性梳理和分析，以及对攻击方法的限制和不足。In English, this means:</li>
<li>for: This research aims to systematically explore the attack methods against neural networks and their impact on them.</li>
<li>methods: This paper uses existing attack methods and classifies them, as well as analyzing and assessing their limitations and drawbacks.</li>
<li>results: This paper provides a systematic overview of existing attack methods, along with their limitations and shortcomings.<details>
<summary>Abstract</summary>
Deep Learning has empowered us to train neural networks for complex data with high performance. However, with the growing research, several vulnerabilities in neural networks have been exposed. A particular branch of research, Adversarial Machine Learning, exploits and understands some of the vulnerabilities that cause the neural networks to misclassify for near original input. A class of algorithms called adversarial attacks is proposed to make the neural networks misclassify for various tasks in different domains. With the extensive and growing research in adversarial attacks, it is crucial to understand the classification of adversarial attacks. This will help us understand the vulnerabilities in a systematic order and help us to mitigate the effects of adversarial attacks. This article provides a survey of existing adversarial attacks and their understanding based on different perspectives. We also provide a brief overview of existing adversarial defences and their limitations in mitigating the effect of adversarial attacks. Further, we conclude with a discussion on the future research directions in the field of adversarial machine learning.
</details>
<details>
<summary>摘要</summary>
深度学习已经赋予我们训练复杂数据的神经网络高性能。然而，随着研究的增长，一些神经网络的漏洞已经被揭露出来。一个特定的研究分支——对抗机器学习——利用和理解神经网络的漏洞，使其在近似原始输入下错分类。一类called adversarial attacks的算法被提议用来使神经网络在不同领域中的多种任务中错分类。随着对抗机器学习的广泛和快速发展，我们需要系统地了解抗击攻击的分类。这将帮助我们系统地了解漏洞，并帮助我们 mitigate the effects of adversarial attacks。本文提供了现有的抗击攻击和其基于不同角度的理解。我们还提供了对抗攻击的简要概述和其限制在 mitigate the effect of adversarial attacks。最后，我们 conclude with a discussion on the future research directions in the field of adversarial machine learning。
</details></li>
</ul>
<hr>
<h2 id="Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks"><a href="#Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks" class="headerlink" title="Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks"></a>Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03337">http://arxiv.org/abs/2308.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Afzal Aghaei, Kourosh Parand, Ali Nikkhah, Shakila Jaberi</li>
<li>for: 解决非线性法克内尔-斯坦方程</li>
<li>methods: 使用Legendre和Chebyshev神经块，利用orthogonal polynomials在神经网络中提高人工神经网络的近似能力</li>
<li>results: 通过模拟不同的法克内尔-斯坦方程配置，实现提高算法的效率<details>
<summary>Abstract</summary>
In this paper, a new deep-learning architecture for solving the non-linear Falkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks, this approach shows how orthogonal polynomials can be used in neural networks to increase the approximation capability of artificial neural networks. In addition, utilizing the mathematical properties of these functions, we overcome the computational complexity of the backpropagation algorithm by using the operational matrices of the derivative. The efficiency of the proposed method is carried out by simulating various configurations of the Falkner-Skan equation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，一种新的深度学习架构用于解决非线性法克纳-斯坦方程被提出。通过使用列朋德和Chebychev神经块，这种方法示出了在人工神经网络中使用正交多项式以提高神经网络的近似能力。此外，利用这些函数的数学性质，我们超越了反向传播算法的计算复杂性，使用操作矩阵的导数。我们对不同的法克纳-斯坦方程配置进行了效率测试。
</details></li>
</ul>
<hr>
<h2 id="Non-Convex-Bilevel-Optimization-with-Time-Varying-Objective-Functions"><a href="#Non-Convex-Bilevel-Optimization-with-Time-Varying-Objective-Functions" class="headerlink" title="Non-Convex Bilevel Optimization with Time-Varying Objective Functions"></a>Non-Convex Bilevel Optimization with Time-Varying Objective Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03811">http://arxiv.org/abs/2308.03811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Lin, Daouda Sow, Kaiyi Ji, Yingbin Liang, Ness Shroff</li>
<li>for: 这个研究是针对在线机器学习问题中的两层优化问题（online bilevel optimization，OBO），尤其是在流动数据和时间变化函数下进行优化。</li>
<li>methods: 我们提出了一个单回路线上的网络均值优化器（SOBOW），通过缓存中的窗口均值来更新外层决策。相比于现有的算法，SOBOW更加computationally efficient，并且不需要知道前一Function。</li>
<li>results: 我们显示了SOBOW可以在线机器学习问题中实现低度的两层本地遗憾（bilevel local regret），并且在多个领域进行了广泛的实验，证明了其效果。<details>
<summary>Abstract</summary>
Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for OBO, we develop a novel analytical technique that disentangles the complex couplings between decision variables, and carefully controls the hypergradient estimation error. We show that SOBOW can achieve a sublinear bilevel local regret under mild conditions. Extensive experiments across multiple domains corroborate the effectiveness of SOBOW.
</details>
<details>
<summary>摘要</summary>
双层优化已成为许多机器学习问题的重要工具。但是现有的非对称双层优化方法假设数据集和函数是静态的，这可能无法适用于emerging的在线应用程序中，尤其是过去的数据和函数都是时间变化的。在这个研究中，我们研究线上双层优化（OBO），其中函数可以是时间变化的，并且代理人持续更新决策以上线实时数据。对于函数变化和真实对数 gradient 的不可知道问题，我们提出了单轮线上双层优化器（SOBOW），它更新外层决策基于最近的对数gradient 估计中的窗口平均值。相比于现有的算法，SOBOW 具有较低的计算成本和不需要知道前一代函数。对于单轮更新和函数变化带来的困难，我们开发了一种新的分析方法，将决策变量分解为独立的部分，并且精确地控制对数gradient 估计误差。我们显示SOBOW 可以在某些条件下实现双层本地 regret 的下图数学。实验结果显示SOBOW 在多个领域中具有优秀的效能。
</details></li>
</ul>
<hr>
<h2 id="Expediting-Neural-Network-Verification-via-Network-Reduction"><a href="#Expediting-Neural-Network-Verification-via-Network-Reduction" class="headerlink" title="Expediting Neural Network Verification via Network Reduction"></a>Expediting Neural Network Verification via Network Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03330">http://arxiv.org/abs/2308.03330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyi Zhong, Ruiwei Wang, Siau-Cheng Khoo</li>
<li>for: 验证深度神经网络的安全性 properties</li>
<li>methods: 提议了多种验证方法来验证深度神经网络是否正确工作，但是许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。本文提出了一种网络减少技术作为验证前置处理方法。</li>
<li>results: 我们的实验结果表明，提议的减少技术可以有效地减少神经网络，并使现有的验证工具更快速地处理神经网络。此外，实验结果还显示，网络减少可以提高现有验证工具对许多网络的可用性。<details>
<summary>Abstract</summary>
A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also show that network reduction can improve the availability of existing verification tools on many networks by reducing them into sequential neural networks.
</details>
<details>
<summary>摘要</summary>
各种验证方法已经提议以验证深度神经网络的安全性属性，以确保神经网络在关键应用中正确工作。然而，许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。在这种情况下，我们提议一种网络减少技术作为预处理方法。我们的提议方法通过消除稳定的ReLU神经元并将其转换成一个顺序神经网络，包括ReLU和Affine层，可以由大多数验证工具处理。我们在 alpha-beta-crown、VeriNet 和 PRIMA 等完整和部分验证工具上实现了减少技术，并在一个大量的 benchmark 上进行了实验。实验结果表明，我们的提议方法可以显著减少神经网络，并使现有的验证工具更快速地处理神经网络。此外，实验结果还表明，网络减少可以提高现有验证工具对许多网络的可用性。
</details></li>
</ul>
<hr>
<h2 id="AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework"><a href="#AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework" class="headerlink" title="AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework"></a>AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03321">http://arxiv.org/abs/2308.03321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huanranchen/ASRNorm">https://github.com/huanranchen/ASRNorm</a></li>
<li>paper_authors: Zikai Zhou, Huanran Chen</li>
<li>for: 这篇论文主要是针对深度学习的正常化层进行研究，并提出了一种新的正常化函数 called Adaptive Fusion Normalization (AFN)。</li>
<li>methods: 论文使用了多种正常化函数，包括Batch Normalization (BatchNorm)、Instance Normalization (InstanceNorm) 和 Weight Normalization (WeightNorm)，并评估了这些函数在领域扩大和图像识别 зада务中的表现。</li>
<li>results: 实验结果显示，AFN 在领域扩大和图像识别任务中表现较好，并且超过了先前的正常化技术。<details>
<summary>Abstract</summary>
The success of deep learning is inseparable from normalization layers. Researchers have proposed various normalization functions, and each of them has both advantages and disadvantages. In response, efforts have been made to design a unified normalization function that combines all normalization procedures and mitigates their weaknesses. We also proposed a new normalization function called Adaptive Fusion Normalization. Through experiments, we demonstrate AFN outperforms the previous normalization techniques in domain generalization and image classification tasks.
</details>
<details>
<summary>摘要</summary>
成功的深度学习与归一化层无可分割。研究人员已经提出了多种归一化函数，每种归一化函数各有优点和缺点。为了设计一个综合归一化函数，并且 Mitigate their weaknesses。我们还提出了一种新的归一化函数called Adaptive Fusion Normalization（AFN）。通过实验，我们证明AFN在领域总体化和图像识别任务中超过了之前的归一化技术。
</details></li>
</ul>
<hr>
<h2 id="Binary-Federated-Learning-with-Client-Level-Differential-Privacy"><a href="#Binary-Federated-Learning-with-Client-Level-Differential-Privacy" class="headerlink" title="Binary Federated Learning with Client-Level Differential Privacy"></a>Binary Federated Learning with Client-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03320">http://arxiv.org/abs/2308.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lumin Liu, Jun Zhang, Shenghui Song, Khaled B. Letaief</li>
<li>for: 提高 federated learning（FL）系统的隐私保护和性能。</li>
<li>methods: 采用 binary neural networks（BNNs）和离散噪声来实现客户端级别的隐私保护，并且引入离散噪声以实现隐私保护。</li>
<li>results: 实验结果基于 MNIST 和 Fashion-MNIST 数据集表明，提议的训练算法可以实现客户端级别的隐私保护，同时具有低通信开销和性能提升。<details>
<summary>Abstract</summary>
Federated learning (FL) is a privacy-preserving collaborative learning framework, and differential privacy can be applied to further enhance its privacy protection. Existing FL systems typically adopt Federated Average (FedAvg) as the training algorithm and implement differential privacy with a Gaussian mechanism. However, the inherent privacy-utility trade-off in these systems severely degrades the training performance if a tight privacy budget is enforced. Besides, the Gaussian mechanism requires model weights to be of high-precision. To improve communication efficiency and achieve a better privacy-utility trade-off, we propose a communication-efficient FL training algorithm with differential privacy guarantee. Specifically, we propose to adopt binary neural networks (BNNs) and introduce discrete noise in the FL setting. Binary model parameters are uploaded for higher communication efficiency and discrete noise is added to achieve the client-level differential privacy protection. The achieved performance guarantee is rigorously proved, and it is shown to depend on the level of discrete noise. Experimental results based on MNIST and Fashion-MNIST datasets will demonstrate that the proposed training algorithm achieves client-level privacy protection with performance gain while enjoying the benefits of low communication overhead from binary model updates.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种隐私保护的协同学习框架，可以进一步增强其隐私保护。现有的 FL 系统通常采用 Federated Average (FedAvg) 作为训练算法，并在这之前实现了分布式隐私保护。然而，这些系统中的隐私Utility 质量耗尽会严重降低训练性能，而且 Gaussian 机制需要模型参数的高精度。为了提高通信效率和实现更好的隐私Utility 质量，我们提议一种通信效率高的 FL 训练算法，并提供了隐私保证。 Specifically, we propose to adopt binary neural networks (BNNs) and introduce discrete noise in the FL setting. Binary model parameters are uploaded for higher communication efficiency and discrete noise is added to achieve the client-level differential privacy protection. The achieved performance guarantee is rigorously proved, and it is shown to depend on the level of discrete noise. Experimental results based on MNIST and Fashion-MNIST datasets will demonstrate that the proposed training algorithm achieves client-level privacy protection with performance gain while enjoying the benefits of low communication overhead from binary model updates.Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="HomOpt-A-Homotopy-Based-Hyperparameter-Optimization-Method"><a href="#HomOpt-A-Homotopy-Based-Hyperparameter-Optimization-Method" class="headerlink" title="HomOpt: A Homotopy-Based Hyperparameter Optimization Method"></a>HomOpt: A Homotopy-Based Hyperparameter Optimization Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03317">http://arxiv.org/abs/2308.03317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeffkinnison/shadho">https://github.com/jeffkinnison/shadho</a></li>
<li>paper_authors: Sophia J. Abraham, Kehelwala D. G. Maduranga, Jeffery Kinnison, Zachariah Carmichael, Jonathan D. Hauenstein, Walter J. Scheirer</li>
<li>for: 提高机器学习模型的性能和效率，通过优化超参数。</li>
<li>methods: 使用一种基于泛函模型（GAM）的数据驱动方法，与哈洛彩优化（Homotopy Optimization）相结合，以提高优化方法的效率和精度。</li>
<li>results: 在多种优化技术（如随机搜索、TPE、权重平衡和SMAC）中，使用HomOpt方法可以提高目标性能，并在多个标准机器学习 benchmark 和开放集任务中达到更高的性能。<details>
<summary>Abstract</summary>
Machine learning has achieved remarkable success over the past couple of decades, often attributed to a combination of algorithmic innovations and the availability of high-quality data available at scale. However, a third critical component is the fine-tuning of hyperparameters, which plays a pivotal role in achieving optimal model performance. Despite its significance, hyperparameter optimization (HPO) remains a challenging task for several reasons. Many HPO techniques rely on naive search methods or assume that the loss function is smooth and continuous, which may not always be the case. Traditional methods, like grid search and Bayesian optimization, often struggle to quickly adapt and efficiently search the loss landscape. Grid search is computationally expensive, while Bayesian optimization can be slow to prime. Since the search space for HPO is frequently high-dimensional and non-convex, it is often challenging to efficiently find a global minimum. Moreover, optimal hyperparameters can be sensitive to the specific dataset or task, further complicating the search process. To address these issues, we propose a new hyperparameter optimization method, HomOpt, using a data-driven approach based on a generalized additive model (GAM) surrogate combined with homotopy optimization. This strategy augments established optimization methodologies to boost the performance and effectiveness of any given method with faster convergence to the optimum on continuous, discrete, and categorical domain spaces. We compare the effectiveness of HomOpt applied to multiple optimization techniques (e.g., Random Search, TPE, Bayes, and SMAC) showing improved objective performance on many standardized machine learning benchmarks and challenging open-set recognition tasks.
</details>
<details>
<summary>摘要</summary>
为解决这些问题，我们提出了一种新的HPO方法，即HomOpt，使用基于一般添加模型（GAM）函数的数据驱动方法，并与幂等优化结合。这种策略可以增强现有优化方法的性能和有效性，并在连续、离散和分类的域空间上快速 converges to the optimum。我们将HomOpt应用于多种优化技术（如随机搜索、TPE、Bayes和SMAC），并对多个标准机器学习benchmark和复杂的开放任务进行比较，显示HomOpt可以提高优化目标性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Q-Network-for-Stochastic-Process-Environments"><a href="#Deep-Q-Network-for-Stochastic-Process-Environments" class="headerlink" title="Deep Q-Network for Stochastic Process Environments"></a>Deep Q-Network for Stochastic Process Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03316">http://arxiv.org/abs/2308.03316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuangheng He</li>
<li>for: 本研究旨在应用强化学习方法在Stochastic Process环境中训练优化策略，包括Flappy Bird和新开发的股票交易环境作为案例研究。</li>
<li>methods: 本研究使用Deep Q-learning网络，评估不同结构的网络，并评估适合Stochastic Process环境的最佳variant。</li>
<li>results: 本研究获得了各种策略的成果，包括Flappy Bird和股票交易环境中的策略。同时，本研究还讨论了环境建立和强化学习技术的现有挑战和可能的改进方案。<details>
<summary>Abstract</summary>
Reinforcement learning is a powerful approach for training an optimal policy to solve complex problems in a given system. This project aims to demonstrate the application of reinforcement learning in stochastic process environments with missing information, using Flappy Bird and a newly developed stock trading environment as case studies. We evaluate various structures of Deep Q-learning networks and identify the most suitable variant for the stochastic process environment. Additionally, we discuss the current challenges and propose potential improvements for further work in environment-building and reinforcement learning techniques.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过补偿学习训练优化策略来解决复杂系统中的问题，这个项目旨在通过缺失信息的随机过程环境中应用强化学习，使用Flappy Bird和一个新开发的股票交易环境作为案例研究。我们评估了不同结构的深度Q学习网络，并确定适用于随机过程环境的最佳变体。此外，我们还讨论了当前挑战和可能的改进方法，用于环境建构和强化学习技术。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide that instead.
</details></li>
</ul>
<hr>
<h2 id="Symmetry-Preserving-Program-Representations-for-Learning-Code-Semantics"><a href="#Symmetry-Preserving-Program-Representations-for-Learning-Code-Semantics" class="headerlink" title="Symmetry-Preserving Program Representations for Learning Code Semantics"></a>Symmetry-Preserving Program Representations for Learning Code Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03312">http://arxiv.org/abs/2308.03312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kexin Pei, Weichen Li, Qirui Jin, Shuyang Liu, Scott Geng, Lorenzo Cavallaro, Junfeng Yang, Suman Jana</li>
<li>for: 该论文旨在提高自动程序理解的能力，以便进行安全任务。</li>
<li>methods: 该论文使用了各种自然语言处理技术，以满足不同的代码分析和模型任务。</li>
<li>results: 该论文通过引入代码Symmetry的概念，提高了LLM架构的泛化和稳定性，并在不同的 binary 和源代码分析任务中进行了详细的实验评估。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.   Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在自动编程逻辑方面表现出了替代性，这是许多安全任务的关键方面。然而，现有的 LLM 架构 для代码往往从其他领域如自然语言处理中借鉴，这引起了代码泛化和稳定性的问题。一个关键的泛化挑战是如何在 LLM 架构中 интеグ含代码 semantics，包括控制和数据流的知识。 drew inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.Translation notes:* "Large Language Models" is translated as "大型语言模型" (dàxí yǔyán módelìng)* "automated program reasoning" is translated as "自动编程逻辑" (zìdōng biānjiāng lógí)* "code" is translated as "代码" (dàikōng)* "semantics" is translated as " semantics" (xìngxìng)* "preserving transformations" is translated as "保持变换" (bǎojì biànbiàn)* "group-theoretic framework" is translated as "群论框架" (qúnlù jiàoxiàng)* "novel variant of self-attention" is translated as "一种新型的自注意力" (yī zhǒng xīn xìng de zìjiāngwù)* "detailed experimental evaluations" is translated as "详细实验评估" (xìngxìng shíyì píngjì)* "specialized LLMs for code" is translated as "专门为代码的 LLM" (zhōngmén wèi dàikōng de LLM)* "LLM-guided program reasoning" is translated as " LLM 导航的程序逻辑" (LLM dàowǎng de chéngjīng lógí)
</details></li>
</ul>
<hr>
<h2 id="Implicit-Graph-Neural-Diffusion-Based-on-Constrained-Dirichlet-Energy-Minimization"><a href="#Implicit-Graph-Neural-Diffusion-Based-on-Constrained-Dirichlet-Energy-Minimization" class="headerlink" title="Implicit Graph Neural Diffusion Based on Constrained Dirichlet Energy Minimization"></a>Implicit Graph Neural Diffusion Based on Constrained Dirichlet Energy Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03306">http://arxiv.org/abs/2308.03306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoji Fu, Mohammed Haroon Dupty, Yanfei Dong, Lee Wee Sun</li>
<li>for: The paper is written for researchers and practitioners working on graph learning problems, particularly those interested in using implicit graph neural networks (GNNs) to capture long-range dependencies.</li>
<li>methods: The paper introduces a geometric framework for designing implicit GNN layers based on a parameterized graph Laplacian operator. This framework allows for learning the geometry of vertex and edge spaces, as well as the graph gradient operator from data.</li>
<li>results: The paper demonstrates better performance than leading implicit and explicit GNNs on benchmark datasets for node and graph classification tasks, with substantial accuracy improvements observed for some datasets. The proposed method is able to trade off smoothing with the preservation of node feature information, addressing the over-smoothing problem that can occur in some GNN models.<details>
<summary>Abstract</summary>
Implicit graph neural networks (GNNs) have emerged as a potential approach to enable GNNs to capture long-range dependencies effectively. However, poorly designed implicit GNN layers can experience over-smoothing or may have limited adaptability to learn data geometry, potentially hindering their performance in graph learning problems. To address these issues, we introduce a geometric framework to design implicit graph diffusion layers based on a parameterized graph Laplacian operator. Our framework allows learning the geometry of vertex and edge spaces, as well as the graph gradient operator from data. We further show how implicit GNN layers can be viewed as the fixed-point solution of a Dirichlet energy minimization problem and give conditions under which it may suffer from over-smoothing. To overcome the over-smoothing problem, we design our implicit graph diffusion layer as the solution of a Dirichlet energy minimization problem with constraints on vertex features, enabling it to trade off smoothing with the preservation of node feature information. With an appropriate hyperparameter set to be larger than the largest eigenvalue of the parameterized graph Laplacian, our framework guarantees a unique equilibrium and quick convergence. Our models demonstrate better performance than leading implicit and explicit GNNs on benchmark datasets for node and graph classification tasks, with substantial accuracy improvements observed for some datasets.
</details>
<details>
<summary>摘要</summary>
匿名图 neural networks (GNNs) 已成为可能的方法来有效地捕捉长距离依赖关系。然而，不当设计的匿名 GNN 层可能会经受过滤或有限的适应性，从而妨碍其在图学习问题中表现。为解决这些问题，我们提出了一个几何框架，用于设计基于参数化图 Laplacian 算子的匿名图扩散层。我们的框架允许学习图像的几何结构和图形导数算子，以及适应数据的几何特征。我们进一步表明，匿名 GNN 层可以视为 Dirichlet 能量最小化问题的固定点解，并给出了一些条件，以确定它可能会经受过滤。为了缓解过滤问题，我们设计了一种基于 vertex 特征的约束来限制匿名图扩散层的平滑程度，从而让它能够平衡平滑和保留节点特征信息。在适当的超参数设置为大于最大 eigenvalues 的情况下，我们的框架保证唯一的平衡点和快速的收敛。我们的模型在 benchmark 数据集上表现出色，与领先的匿名和显式 GNN 相比，有substantial 的准确率改进，特别是在某些数据集上。
</details></li>
</ul>
<hr>
<h2 id="Do-You-Remember-Overcoming-Catastrophic-Forgetting-for-Fake-Audio-Detection"><a href="#Do-You-Remember-Overcoming-Catastrophic-Forgetting-for-Fake-Audio-Detection" class="headerlink" title="Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection"></a>Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03300">http://arxiv.org/abs/2308.03300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cecile-hi/regularized-adaptive-weight-modification">https://github.com/cecile-hi/regularized-adaptive-weight-modification</a></li>
<li>paper_authors: Xiaohui Zhang, Jiangyan Yi, Jianhua Tao, Chenglong Wang, Chuyuan Zhang</li>
<li>For: 这个研究旨在开发一个可以适应不同数据集的伪音乐检测算法，以解决对伪音乐检测算法的普遍忘却问题。* Methods: 我们提出了一个叫做Regularized Adaptive Weight Modification（RAWM）的持续学习算法，它可以适应不同数据集的伪音乐检测任务，并且可以保持旧模型的知识。我们还引入了一个规制因子，以强制网络保持旧的特征分布。* Results: 我们在多个数据集上进行了评估，结果显示我们的方法可以对伪音乐检测任务进行有效的适应和忘却避免。<details>
<summary>Abstract</summary>
Current fake audio detection algorithms have achieved promising performances on most datasets. However, their performance may be significantly degraded when dealing with audio of a different dataset. The orthogonal weight modification to overcome catastrophic forgetting does not consider the similarity of genuine audio across different datasets. To overcome this limitation, we propose a continual learning algorithm for fake audio detection to overcome catastrophic forgetting, called Regularized Adaptive Weight Modification (RAWM). When fine-tuning a detection network, our approach adaptively computes the direction of weight modification according to the ratio of genuine utterances and fake utterances. The adaptive modification direction ensures the network can effectively detect fake audio on the new dataset while preserving its knowledge of old model, thus mitigating catastrophic forgetting. In addition, genuine audio collected from quite different acoustic conditions may skew their feature distribution, so we introduce a regularization constraint to force the network to remember the old distribution in this regard. Our method can easily be generalized to related fields, like speech emotion recognition. We also evaluate our approach across multiple datasets and obtain a significant performance improvement on cross-dataset experiments.
</details>
<details>
<summary>摘要</summary>
当前的假音检测算法已经在大多数数据集上实现了有望的性能。然而，它们在处理不同数据集的声音时可能会表现出很大的下降性能。现有的orthogonal weight modification方法不考虑真实声音数据集之间的相似性。为了解决这个限制，我们提出了一种适应学习的假音检测算法，called Regularized Adaptive Weight Modification (RAWM)。当训练检测网络时，我们的方法会动态计算修改方向，根据新数据集中真实的utterances和假的utterances的比率。这种适应修改方向使得网络可以有效地检测新数据集上的假音，同时保持以前的知识，从而避免悬崖式遗忘。此外，来自不同的听音条件下的真实声音收集可能会扭曲其特征分布，所以我们引入了一种正则化约束，使得网络可以保持以前的分布。我们的方法可以轻松扩展到相关的领域，如语音情感识别。我们还在多个数据集上评估了我们的方法，并在交叉数据集测试中获得了显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Studying-Large-Language-Model-Generalization-with-Influence-Functions"><a href="#Studying-Large-Language-Model-Generalization-with-Influence-Functions" class="headerlink" title="Studying Large Language Model Generalization with Influence Functions"></a>Studying Large Language Model Generalization with Influence Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03296">http://arxiv.org/abs/2308.03296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman</li>
<li>for: 了解大型机器学习模型（LLMs）的特性和风险，以便更好地理解和 mitigate 这些风险。</li>
<li>methods: 使用Influence functions来回答一个Counterfactual：如果一个序列被添加到训练集中， THEN how would the model’s parameters（和其输出）变化？通过使用Eigenvalue-corrected Kronecker-Factored Approximate Curvature（EK-FAC）近似来扩展Influence functions到LLMs中，并使用TF-IDF筛选和查询批处理来降低计算Gradient的成本。</li>
<li>results: 通过Influence functions来研究LLMs的通用特性，包括影响 patrerns的稀畴性、逻辑推理能力、数学能力、跨语言通用性和role-playing行为等。尽管有很多似乎复杂的通用形式，但我们发现一个意外的限制：影响幅 decay 到 Near-zero 当键短语的顺序被反转。总的来说，Influence functions 为研究 LLMs 提供了一种强大的新工具。<details>
<summary>Abstract</summary>
When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)当尝试更好地了解机器学习模型以理解和降低相关风险时，一个有价值的证据来源是：哪些训练示例最大程度地影响模型的行为？影响函数想要回答一个Counterfactual：如果给定的序列添加到训练集中， Then how would the model's parameters (和其输出) change? Although influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
</details></li>
</ul>
<hr>
<h2 id="DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data"><a href="#DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data" class="headerlink" title="DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data"></a>DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03295">http://arxiv.org/abs/2308.03295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyao Wang, Luke Chen, Mohammad Abdullah Al Faruque</li>
<li>for: 这篇论文主要关注于如何 Addressing the distribution shift problem in noisy multi-sensor time-series data using brain-inspired hyperdimensional computing (HDC) learning frameworks.</li>
<li>methods: 这篇论文提出了一个名为 DOMINO 的新的 HDC 学习框架，利用高维度空间的有效并行矩阵运算来动态地识别和范围化领域对称的维度。</li>
<li>results: 在训练多种多感器时间序列标签任务中，DOMINO 比起现有的顶尖深度神经网络（DNN）域对应测试技术，平均提高了2.04%的精度，并在训练和测试过程中提供了16.34倍 faster 和2.89倍 faster 的运算速度。此外，DOMINO 在受到硬件噪声的情况下表现更好，提供了10.93倍更高的韧性。<details>
<summary>Abstract</summary>
With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning framework addressing the distribution shift problem in noisy multi-sensor time-series data. DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions. Our evaluation on a wide range of multi-sensor time series classification tasks shows that DOMINO achieves on average 2.04% higher accuracy than state-of-the-art (SOTA) DNN-based domain generalization techniques, and delivers 16.34x faster training and 2.89x faster inference. More importantly, DOMINO performs notably better when learning from partially labeled and highly imbalanced data, providing 10.93x higher robustness against hardware noises than SOTA DNNs.
</details>
<details>
<summary>摘要</summary>
随着互联网物联网的快速演化，许多实际应用场景使用不同类型的感知器来收集时序信息。Edge-based机器学习（ML）方法ологи是常用来当地分析收集的数据。然而，数据驱动的ML方法面临的基本问题是分布shift问题。这种问题会导致模型在不同于它在训练时的数据分布上部署时表现差。此外，逐渐复杂的深度神经网络（DNN）已经被提出来捕捉多感知器时序数据中的空间和时间相关性，需要今天的边缘设备来进行投入式计算资源。而基于脑神经网络的高维计算（HDC）已经被提出来作为边缘学习的轻量级解决方案。然而，现有的HDC也面临着分布shift挑战。在这篇论文中，我们提出了DOMINO，一种新的HDC学习框架，解决了边缘设备上的分布shift问题。DOMINO利用了高维空间中效率和并行的矩阵运算来动态标识和筛选域variant维度。我们对多种多感知器时序分类任务进行了广泛的评估，结果显示DOMINO相比状态скус（SOTA）神经网络基于领域普适化技术，平均提高了2.04%的准确率，并提供了16.34x快的训练和2.89x快的推理。此外，DOMINO在学习部分 labels和高度不均衡数据时表现更出色，提供了10.93x更高的硬件噪声Robustness。
</details></li>
</ul>
<hr>
<h2 id="SynJax-Structured-Probability-Distributions-for-JAX"><a href="#SynJax-Structured-Probability-Distributions-for-JAX" class="headerlink" title="SynJax: Structured Probability Distributions for JAX"></a>SynJax: Structured Probability Distributions for JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03291">http://arxiv.org/abs/2308.03291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepmind/synjax">https://github.com/deepmind/synjax</a></li>
<li>paper_authors: Miloš Stanojević, Laurent Sartran</li>
<li>for: 这篇论文是为了解决深度学习模型中的结构化对象问题而写的，以便建立大规模可导的模型。</li>
<li>methods: 这篇论文使用了SynJax库，提供了高效的向量化实现归并算法，以便处理结构化分布的推理问题。</li>
<li>results: 该论文通过SynJax库实现了大规模可导模型，并且可以处理各种结构化对象，如树和分割。<details>
<summary>Abstract</summary>
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.   SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://github.com/deepmind/synjax.
</details>
<details>
<summary>摘要</summary>
随着深度学习软件库的发展，场景中的进步很大，允许用户专注于模型设计，让库处理现代硬件加速器的繁琐和耗时任务。然而，这些进步主要帮助了某些特定的深度学习模型，如转换器，其元素对vector化计算很容易映射。其他类型的模型，如树和分割，因为它们需要特殊的算法，难以在vector化形式下实现。SynJax直接解决了这个问题，提供了高效的vector化推理算法 для结构化分布，包括对适配、标记、分割、 константы树和扩展树。通过SynJax，我们可以构建大规模可导模型，直接模型数据中的结构。代码可以在https://github.com/deepmind/synjax上获取。
</details></li>
</ul>
<hr>
<h2 id="FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search"><a href="#FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search" class="headerlink" title="FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search"></a>FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03290">http://arxiv.org/abs/2308.03290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Dotzel, Gang Wu, Andrew Li, Muhammad Umar, Yun Ni, Mohamed S. Abdelfattah, Zhiru Zhang, Liqun Cheng, Martin G. Dixon, Norman P. Jouppi, Quoc V. Le, Sheng Li</li>
<li>for: 这个论文主要目的是提出一种一击 mixed-precision quantization search（FLIQS），以找到最佳的混合精度数据模型，并且可以实现高品质和低成本的模型。</li>
<li>methods: 这个方法使用了一种新的一击 mixed-precision quantization search（FLIQS），可以在数据模型中找到最佳的混合精度数据模型，并且不需要重新训练。</li>
<li>results: 这个方法可以对多个 convolutional networks 和 vision transformer 模型进行搜索，并且可以实现高品质和低成本的模型。在 ImageNet 上，这个方法可以将 ResNet-18 的精度提高到 1.31% 点，ResNet-50 的精度提高到 0.90% 点，并且在 MobileNetV2 上可以实现最佳的模型。<details>
<summary>Abstract</summary>
Quantization has become a mainstream compression technique for reducing model size, computational requirements, and energy consumption for modern deep neural networks (DNNs). With the improved numerical support in recent hardware, including multiple variants of integer and floating point, mixed-precision quantization has become necessary to achieve high-quality results with low model cost. Prior mixed-precision quantization methods have performed a post-training quantization search, which compromises on accuracy, or a differentiable quantization search, which leads to high memory usage from branching. Therefore, we propose the first one-shot mixed-precision quantization search that eliminates the need for retraining in both integer and low-precision floating point models. We evaluate our floating-point and integer quantization search (FLIQS) on multiple convolutional networks and vision transformer models to discover Pareto-optimal models. Our approach discovers models that improve upon uniform precision, manual mixed-precision, and recent integer quantization search methods. With the proposed integer quantization search, we increase the accuracy of ResNet-18 on ImageNet by 1.31% points and ResNet-50 by 0.90% points with equivalent model cost over previous methods. Additionally, for the first time, we explore a novel mixed-precision floating-point search and improve MobileNetV2 by up to 0.98% points compared to prior state-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search a joint quantization and neural architecture space and improve the ImageNet accuracy by 2.69% points with similar model cost on a MobileNetV2 search space.
</details>
<details>
<summary>摘要</summary>
启用量化技术已成为现代深度神经网络（DNN）减少模型大小、计算需求和能耗的主流压缩方法。随着现有硬件的数值支持的改进，包括整数和浮点数多种变体，混合精度量化技术已成为实现高质量结果的低模型成本的必要手段。先前的混合精度量化方法通常是在训练后进行量化搜索，这会妥协准确性，或者使用可导量化搜索，这会导致高内存使用率由分支引起。因此，我们提出了首次一步混合精度量化搜索，从而消除了重训练的需要，并在整数和低精度浮点数模型中实现高度优化。我们对多个卷积神经网络和视Transformer模型进行了评估，并发现了Pareto优质模型。我们的浮点和整数量化搜索（FLIQS）在 uniform精度、手动混合精度和最近整数量化搜索方法上提高了模型的准确性。例如，在 ImageNet 上，我们通过使用整数量化搜索，提高了 ResNet-18 的准确性by 1.31% 点和 ResNet-50 的准确性by 0.90% 点，与之前的方法相当。此外，我们首次探索了一种新的混合精度浮点数搜索，并在 MobileNetV2 上提高了最高达 0.98% 点的准确性。最后，我们将 FLIQS 扩展到同时搜索一个量化和神经网络架构空间，并在 MobileNetV2 搜索空间上提高了 ImageNet 的准确性by 2.69% 点，与相同的模型成本。
</details></li>
</ul>
<hr>
<h2 id="High-rate-discretely-modulated-continuous-variable-quantum-key-distribution-using-quantum-machine-learning"><a href="#High-rate-discretely-modulated-continuous-variable-quantum-key-distribution-using-quantum-machine-learning" class="headerlink" title="High-rate discretely-modulated continuous-variable quantum key distribution using quantum machine learning"></a>High-rate discretely-modulated continuous-variable quantum key distribution using quantum machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03283">http://arxiv.org/abs/2308.03283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Liao, Jieyu Liu, Anqi Huang, Lei Huang, Zhuoying Fei, Xiquan Fu</li>
<li>for: 提出了一种高速率的分割CVQKD系统使用量子机器学习技术，以提高CVQKD系统的安全性和效率。</li>
<li>methods: 使用量子k-最近邻NN分类器来预测Bob方向的失去混合干扰DMCS，并对输出的密钥进行数据处理以生成最终的秘密密钥串。</li>
<li>results: 提出的方案可以在机器学习metric和复杂度方面提高CVQKD系统的性能，并通过使用SDP方法证明其理论安全性。数值仿真结果表明，相比现有的DM CVQKD协议，提出的方案可以获得更高的秘密密钥率，并可以通过增加混合干扰的变化来进一步提高性能。<details>
<summary>Abstract</summary>
We propose a high-rate scheme for discretely-modulated continuous-variable quantum key distribution (DM CVQKD) using quantum machine learning technologies, which divides the whole CVQKD system into three parts, i.e., the initialization part that is used for training and estimating quantum classifier, the prediction part that is used for generating highly correlated raw keys, and the data-postprocessing part that generates the final secret key string shared by Alice and Bob. To this end, a low-complexity quantum k-nearest neighbor (QkNN) classifier is designed for predicting the lossy discretely-modulated coherent states (DMCSs) at Bob's side. The performance of the proposed QkNN-based CVQKD especially in terms of machine learning metrics and complexity is analyzed, and its theoretical security is proved by using semi-definite program (SDP) method. Numerical simulation shows that the secret key rate of our proposed scheme is explicitly superior to the existing DM CVQKD protocols, and it can be further enhanced with the increase of modulation variance.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:我们提出了一种高率方案 для抽象变量量kv quantum key distribution（DM CVQKD），使用量子机器学习技术，将整个CVQKD系统分成三部分：初始化部分用于训练和估算量子分类器，预测部分用于生成高相关性的Raw键，以及数据处理部分用于生成最终由阿利斯和布Bob共享的机密键串。为此，我们设计了一种低复杂度量子k最近邻居分类器（QkNN）来预测 бо布的lossy抽象变量量CS。我们分析了我们提议的QkNN-based CVQKD的性能，包括机器学习指标和复杂度，并使用 semi-definite 程序（SDP）方法证明其理论安全性。numerical simulation表明，我们的提议方案的机密键率明显高于现有的DM CVQKD协议，并可以通过增加调制变量的幅度来进一步提高。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distilled-Ensemble-Model-for-sEMG-based-Silent-Speech-Interface"><a href="#Knowledge-Distilled-Ensemble-Model-for-sEMG-based-Silent-Speech-Interface" class="headerlink" title="Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface"></a>Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06533">http://arxiv.org/abs/2308.06533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqiang Lai, Qihan Yang, Ye Mao, Endong Sun, Jiangnan Ye</li>
<li>for: 这研究旨在提供一种基于表面电Myography（sEMG）的干扰 Speech Interface（SSI），以解决全球数百万人口报告有声音问题。</li>
<li>methods: 我们提出了一种轻量级的深度学习知识压缩混合模型（KDE-SSI），使得可以通过拼写生成任何英语单词。</li>
<li>results: 我们的实验证明了KDE-SSI的效果，实现了26个 NATO fonetic alphabet dataset中的3900个数据样本的测试准确率为85.9%。<details>
<summary>Abstract</summary>
Voice disorders affect millions of people worldwide. Surface electromyography-based Silent Speech Interfaces (sEMG-based SSIs) have been explored as a potential solution for decades. However, previous works were limited by small vocabularies and manually extracted features from raw data. To address these limitations, we propose a lightweight deep learning knowledge-distilled ensemble model for sEMG-based SSI (KDE-SSI). Our model can classify a 26 NATO phonetic alphabets dataset with 3900 data samples, enabling the unambiguous generation of any English word through spelling. Extensive experiments validate the effectiveness of KDE-SSI, achieving a test accuracy of 85.9\%. Our findings also shed light on an end-to-end system for portable, practical equipment.
</details>
<details>
<summary>摘要</summary>
声音疾病影响全球数千万人，表面电动力学基于Silent Speech Interface（sEMG-based SSI）已经在数十年内被探索。然而，先前的工作受限于小词汇和手动提取的特征从原始数据中提取。为了解决这些限制，我们提出了一种轻量级深度学习知识卷积混合模型 для sEMG-based SSI（KDE-SSI）。我们的模型可以分类26个北约字母集数据集，包含3900个数据样本，使得可以不 ambiguously生成任何英文单词的拼写。我们的实验证明了KDE-SSI的效果，测试准确率达85.9%。我们的发现还照明了一个端到端系统，可以在可搬式、实用的设备上实现。
</details></li>
</ul>
<hr>
<h2 id="DSformer-A-Double-Sampling-Transformer-for-Multivariate-Time-Series-Long-term-Prediction"><a href="#DSformer-A-Double-Sampling-Transformer-for-Multivariate-Time-Series-Long-term-Prediction" class="headerlink" title="DSformer: A Double Sampling Transformer for Multivariate Time Series Long-term Prediction"></a>DSformer: A Double Sampling Transformer for Multivariate Time Series Long-term Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03274">http://arxiv.org/abs/2308.03274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengqing Yu, Fei Wang, Zezhi Shao, Tao Sun, Lin Wu, Yongjun Xu</li>
<li>for: 本研究旨在提出一种基于变换器的多变量时间序列长期预测模型，以提高多变量时间序列预测的精度。</li>
<li>methods: 该模型由双重采样块（DS block）和时间变量注意块（TVA block）组成。DS块使用下采样和分割采样将原始序列转换为专注于全局信息和本地信息的特征 вектор。然后，TVA块使用时间注意力和变量注意力来挖掘这些特征 вектор，并提取关键信息。最后，DSformer 使用多个 TVA 块来挖掘和集成不同维度的特征信息，并将其传递给基于多层感知器的生成解码器，以实现多变量时间序列长期预测。</li>
<li>results: 实验结果表明，DSformer 可以在九个真实世界数据集上超越八个基准模型。<details>
<summary>Abstract</summary>
Multivariate time series long-term prediction, which aims to predict the change of data in a long time, can provide references for decision-making. Although transformer-based models have made progress in this field, they usually do not make full use of three features of multivariate time series: global information, local information, and variables correlation. To effectively mine the above three features and establish a high-precision prediction model, we propose a double sampling transformer (DSformer), which consists of the double sampling (DS) block and the temporal variable attention (TVA) block. Firstly, the DS block employs down sampling and piecewise sampling to transform the original series into feature vectors that focus on global information and local information respectively. Then, TVA block uses temporal attention and variable attention to mine these feature vectors from different dimensions and extract key information. Finally, based on a parallel structure, DSformer uses multiple TVA blocks to mine and integrate different features obtained from DS blocks respectively. The integrated feature information is passed to the generative decoder based on a multi-layer perceptron to realize multivariate time series long-term prediction. Experimental results on nine real-world datasets show that DSformer can outperform eight existing baselines.
</details>
<details>
<summary>摘要</summary>
多变量时间序列长期预测，目的是预测数据在长时间内的变化，可以提供决策参考。虽然变换器基本模型在这个领域中已经作出了进步，但它们通常不充分利用多变量时间序列的三个特点：全球信息、本地信息和变量相关性。为了有效利用这些特点并建立高精度预测模型，我们提出了双重采样变换器（DSformer），它包括双重采样（DS）块和时间变量注意（TVA）块。首先，DS块使用下采样和分割采样将原始系列转换为特征向量，这些向量专注于全球信息和本地信息。然后，TVA块使用时间注意和变量注意来挖掘这些特征向量从不同维度，提取关键信息。最后，基于并行结构，DSformer使用多个TVA块来挖掘和 интеGRATE不同的特征信息，并将其传递给基于多层感知器的生成解码器，实现多变量时间序列长期预测。实验结果表明，DSformer可以在九个真实世界数据集上超越八个基准。
</details></li>
</ul>
<hr>
<h2 id="Local-Structure-aware-Graph-Contrastive-Representation-Learning"><a href="#Local-Structure-aware-Graph-Contrastive-Representation-Learning" class="headerlink" title="Local Structure-aware Graph Contrastive Representation Learning"></a>Local Structure-aware Graph Contrastive Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03271">http://arxiv.org/abs/2308.03271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yang, Yuan Liu, Zijuan Zhao, Peijin Ding, Wenqian Zhao</li>
<li>for: 本文提出了一种Local Structure-aware Graph Contrastive representation Learning方法（LS-GCL），用于模型节点的结构信息从多个视图。</li>
<li>methods: 本方法使用了semantic subgraphs，不限于第一邻域，并使用共享GNNEncoder来学习target节点embeddings。在全视图和局部视图之间使用了pooling函数来生成子图级别图像 embeddings。</li>
<li>results: 实验结果表明，LS-GCL方法在五个数据集上的node classification和link prediction任务中都超过了状态方法。<details>
<summary>Abstract</summary>
Traditional Graph Neural Network (GNN), as a graph representation learning method, is constrained by label information. However, Graph Contrastive Learning (GCL) methods, which tackle the label problem effectively, mainly focus on the feature information of the global graph or small subgraph structure (e.g., the first-order neighborhood). In the paper, we propose a Local Structure-aware Graph Contrastive representation Learning method (LS-GCL) to model the structural information of nodes from multiple views. Specifically, we construct the semantic subgraphs that are not limited to the first-order neighbors. For the local view, the semantic subgraph of each target node is input into a shared GNN encoder to obtain the target node embeddings at the subgraph-level. Then, we use a pooling function to generate the subgraph-level graph embeddings. For the global view, considering the original graph preserves indispensable semantic information of nodes, we leverage the shared GNN encoder to learn the target node embeddings at the global graph-level. The proposed LS-GCL model is optimized to maximize the common information among similar instances at three various perspectives through a multi-level contrastive loss function. Experimental results on five datasets illustrate that our method outperforms state-of-the-art graph representation learning approaches for both node classification and link prediction tasks.
</details>
<details>
<summary>摘要</summary>
传统的图 neural network (GNN) 是一种图表示学习方法，它受到标签信息的限制。然而，图相对学习 (GCL) 方法，它们能够有效解决标签问题，主要关注全图或小子图结构（例如，第一颗邻居）的特征信息。在文章中，我们提出了一种本地结构意识 Graph Contrastive representation Learning 方法 (LS-GCL)，用于模型节点的多视图结构信息。具体来说，我们构建了不同于第一颗邻居的semantic subgraph，并将每个目标节点的semantic subgraph输入到共享 GNN Encoder 中，以获得目标节点的subgraph级别嵌入。然后，我们使用一个pooling函数生成subgraph级别图像嵌入。对于全图视图，我们利用共享 GNN Encoder 来学习目标节点的全图级别嵌入。我们的 LS-GCL 模型通过最大化三种不同视角的共同信息来优化一个多级对比损失函数来进行优化。实验结果表明，我们的方法在五个dataset上对节点 classification 和链接预测任务均达到了当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Simple-Rule-Injection-for-ComplEx-Embeddings"><a href="#Simple-Rule-Injection-for-ComplEx-Embeddings" class="headerlink" title="Simple Rule Injection for ComplEx Embeddings"></a>Simple Rule Injection for ComplEx Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03269">http://arxiv.org/abs/2308.03269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodi Ma, Anthony Colas, Yuejie Wang, Ali Sadeghian, Daisy Zhe Wang</li>
<li>for: 本研究旨在协调逻辑规则与知识图 embeddings，以利用先前知识来提高 neural knowledge graph inference 的性能。</li>
<li>methods: 本研究提出了 InjEx 机制，可以通过简单的约束将多种类型的逻辑规则注入到 embedding 空间中，以捕捉definite Horn 规则。</li>
<li>results: 在知识图完成 (KGC) 和少量shot知识图完成 (FKGC) 设置下，InjEx 比基eline KGC 模型和特殊化的几个shot模型表现出色，同时保持了可扩展性和效率。<details>
<summary>Abstract</summary>
Recent works in neural knowledge graph inference attempt to combine logic rules with knowledge graph embeddings to benefit from prior knowledge. However, they usually cannot avoid rule grounding, and injecting a diverse set of rules has still not been thoroughly explored. In this work, we propose InjEx, a mechanism to inject multiple types of rules through simple constraints, which capture definite Horn rules. To start, we theoretically prove that InjEx can inject such rules. Next, to demonstrate that InjEx infuses interpretable prior knowledge into the embedding space, we evaluate InjEx on both the knowledge graph completion (KGC) and few-shot knowledge graph completion (FKGC) settings. Our experimental results reveal that InjEx outperforms both baseline KGC models as well as specialized few-shot models while maintaining its scalability and efficiency.
</details>
<details>
<summary>摘要</summary>
近期研究在神经知识Graph推理中尝试将逻辑规则与知识Graph嵌入结合以获得优势。然而，他们通常无法避免规则定义，并且尝试插入多种规则的可能性还没有得到了全面的探索。在这种情况下，我们提出了InjEx，一种机制来插入多种规则， capture definite Horn rules。首先，我们 theoretically prove that InjEx可以插入这些规则。然后，我们通过在知识Graph completion（KGC）和少量知识Graph completion（FKGC）设置中评估InjEx，以证明它可以把有意义的先前知识注入到嵌入空间中。我们的实验结果表明，InjEx在KGC和FKGC设置中的性能都高于基eline模型和专门的少量模型，同时保持其可扩展性和高效性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Different-Time-series-Transformer-TST-Architectures-A-Case-Study-in-Battery-Life-Prediction-for-Electric-Vehicles-EVs"><a href="#Exploring-Different-Time-series-Transformer-TST-Architectures-A-Case-Study-in-Battery-Life-Prediction-for-Electric-Vehicles-EVs" class="headerlink" title="Exploring Different Time-series-Transformer (TST) Architectures: A Case Study in Battery Life Prediction for Electric Vehicles (EVs)"></a>Exploring Different Time-series-Transformer (TST) Architectures: A Case Study in Battery Life Prediction for Electric Vehicles (EVs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03260">http://arxiv.org/abs/2308.03260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranjan Sitapure, Atharva Kulkarni</li>
<li>For: The paper aims to develop accurate battery life prediction models for electric vehicles (EVs) by incorporating environmental, battery, vehicle driving, and heating circuit data.* Methods: The paper uses time-series-transformers (TSTs) and long short-term memory (LSTM) models to predict state-of-charge (SOC) and battery temperature. The authors also explore novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM.* Results: The paper uses a dataset of 72 driving trips in a BMW i3 (60 Ah) to evaluate the performance of the proposed models. The results show that the TST models outperform the LSTM models and traditional battery models, with an accuracy of 95.6% for SOC and 93.8% for battery temperature.<details>
<summary>Abstract</summary>
In recent years, battery technology for electric vehicles (EVs) has been a major focus, with a significant emphasis on developing new battery materials and chemistries. However, accurately predicting key battery parameters, such as state-of-charge (SOC) and temperature, remains a challenge for constructing advanced battery management systems (BMS). Existing battery models do not comprehensively cover all parameters affecting battery performance, including non-battery-related factors like ambient temperature, cabin temperature, elevation, and regenerative braking during EV operation. Due to the difficulty of incorporating these auxiliary parameters into traditional models, a data-driven approach is suggested. Time-series-transformers (TSTs), leveraging multiheaded attention and parallelization-friendly architecture, are explored alongside LSTM models. Novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM, are also developed and compared against existing models. A dataset comprising 72 driving trips in a BMW i3 (60 Ah) is used to address battery life prediction in EVs, aiming to create accurate TST models that incorporate environmental, battery, vehicle driving, and heating circuit data to predict SOC and battery temperature for future time steps.
</details>
<details>
<summary>摘要</summary>
近年来，电动汽车（EV）的电池技术得到了广泛关注，尤其是开发新的电池材料和化学组合。然而，正确预测电池参数，如充电状态（SOC）和温度，仍然是构建高级电池管理系统（BMS）的挑战。现有的电池模型并不完全覆盖所有影响电池性能的参数，包括非电池相关的因素，如外部温度、车辆内部温度、海拔和恢复摩擦 durante la operación del vehículo eléctrico.由于将这些辅助参数integrated into traditional models是困难的，一种数据驱动的方法被建议。使用时间序列变换（TST）和长短期memory（LSTM）模型，并研发了新的TST架构，包括encoder TST + decoder LSTM和hybrid TST-LSTM。使用一个包含72次开车记录的BMW i3（60 Ah）数据集，以预测将来时间步的SOC和电池温度。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Approximation-and-Learning-Rates-for-Deep-Convolutional-Neural-Networks"><a href="#Optimal-Approximation-and-Learning-Rates-for-Deep-Convolutional-Neural-Networks" class="headerlink" title="Optimal Approximation and Learning Rates for Deep Convolutional Neural Networks"></a>Optimal Approximation and Learning Rates for Deep Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03259">http://arxiv.org/abs/2308.03259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shao-Bo Lin</li>
<li>for: 本研究探讨了深度卷积神经网络（CNN）的approximation和学习性能分析，特别是采用零填充和最大池化。</li>
<li>methods: 我们提供了一种可以approximate $r$-smooth函数的深度卷积神经网络的分析方法，并证明了这种方法的approximation率为 $(L^2&#x2F;\log L)^{-2r&#x2F;d} $，这是最佳的下界。</li>
<li>results: 我们得到了深度卷积神经网络在实际风险最小化中的几乎最佳学习率。<details>
<summary>Abstract</summary>
This paper focuses on approximation and learning performance analysis for deep convolutional neural networks with zero-padding and max-pooling. We prove that, to approximate $r$-smooth function, the approximation rates of deep convolutional neural networks with depth $L$ are of order $ (L^2/\log L)^{-2r/d} $, which is optimal up to a logarithmic factor. Furthermore, we deduce almost optimal learning rates for implementing empirical risk minimization over deep convolutional neural networks.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文关注深度 convolutional neural networks (CNNs) 的近似和学习性能分析，特别是在零填充和最大 pooling 下。我们证明，要近似 $r $- 平滑函数，深度 $L $ 的 CNNs 的近似率是 $(L^2/\log L)^{-2r/d} $，即最优的logs 因子。此外，我们还得出了对深度 CNNs 的实际风险最小化的几乎最优学习率。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Adversarial-Detection-without-Extra-Model-Training-Loss-Should-Change"><a href="#Unsupervised-Adversarial-Detection-without-Extra-Model-Training-Loss-Should-Change" class="headerlink" title="Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change"></a>Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03243">http://arxiv.org/abs/2308.03243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyclebooster/unsupervised-adversarial-detection-without-extra-model">https://github.com/cyclebooster/unsupervised-adversarial-detection-without-extra-model</a></li>
<li>paper_authors: Chien Cheng Chyou, Hung-Ting Su, Winston H. Hsu</li>
<li>for: 防御深度学习模型受到攻击的挑战是现实世界应用中的关键问题。传统的对抗训练和监督检测方法需要先知道攻击类型和训练数据标注，这经常是不实际的。现有的无监督对抗检测方法可以判断目标模型是否正常工作，但它们因使用共同减少极限loss而受到差异攻击的强化，导致准确率差。</li>
<li>methods: 我们提议使用新的训练损失来减少无用的特征，并对无需先知道攻击类型的检测方法。</li>
<li>results: 我们的检测率（真正正确率）对所有给出的白盒攻击是高于93.9%，只有DF($\infty$)攻击没有限制。false positive rate几乎为2.5%。我们的方法在所有攻击类型下都工作良好，并且对某些类型的方法表现更好。<details>
<summary>Abstract</summary>
Adversarial robustness poses a critical challenge in the deployment of deep learning models for real-world applications. Traditional approaches to adversarial training and supervised detection rely on prior knowledge of attack types and access to labeled training data, which is often impractical. Existing unsupervised adversarial detection methods identify whether the target model works properly, but they suffer from bad accuracies owing to the use of common cross-entropy training loss, which relies on unnecessary features and strengthens adversarial attacks. We propose new training losses to reduce useless features and the corresponding detection method without prior knowledge of adversarial attacks. The detection rate (true positive rate) against all given white-box attacks is above 93.9% except for attacks without limits (DF($\infty$)), while the false positive rate is barely 2.5%. The proposed method works well in all tested attack types and the false positive rates are even better than the methods good at certain types.
</details>
<details>
<summary>摘要</summary>
“深度学习模型的应急防范问题是实际应用中的关键挑战。传统的敌对训练和监管检测方法需要先知 adversarial 攻击类型和训练数据的标注，这经常是不现实的。现有的无监管敌对检测方法可以判断目标模型是否正常工作，但它们因使用通用的 cross-entropy 训练损失函数，导致强化敌对攻击。我们提出了新的训练损失函数，以减少不必要的特征，并对这些特征进行检测方法，不需要先知 adversarial 攻击。检测率（真正正确率）对所有给出的白盒攻击是大于 93.9%，只有 attacks without limits（DF（∞））的检测率较低，而假阳性率只有 2.5%。我们的方法在所有攻击类型中都工作良好，假阳性率甚至比其他方法更好。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the original text and may not capture all the nuances and details of the original text.
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Decentralized-Q-Learning-Two-Timescale-Analysis-By-Persistence"><a href="#Asynchronous-Decentralized-Q-Learning-Two-Timescale-Analysis-By-Persistence" class="headerlink" title="Asynchronous Decentralized Q-Learning: Two Timescale Analysis By Persistence"></a>Asynchronous Decentralized Q-Learning: Two Timescale Analysis By Persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03239">http://arxiv.org/abs/2308.03239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bora Yongacoglu, Gürdal Arslan, Serdar Yüksel</li>
<li>for: 本研究旨在探讨非站立性是多智能 reinforcement learning（MARL）挑战的基础问题，并研究如何使用不同的方法解决这个问题。</li>
<li>methods: 本文使用的方法包括各种协调策略更新的方法，包括同步化策略更新的时间。这些同步化方法可以使得多个智能体的策略更新趋于相同，从而使得问题的分析变得更加容易。然而，在许多分散式应用中，同步化是不可能实现的。因此，本文研究了一种异步的Q值学习算法的variant，以解决这个问题。</li>
<li>results: 本文的分析表明，当使用不同的学习率时，异步Q值学习算法可以逐渐靠拢到平衡状态，并且在某些情况下可以达到更高的性能。此外，本文还证明了这种算法在分散式应用中可以有更好的性能，而无需协调智能体的策略更新。<details>
<summary>Abstract</summary>
Non-stationarity is a fundamental challenge in multi-agent reinforcement learning (MARL), where agents update their behaviour as they learn. Many theoretical advances in MARL avoid the challenge of non-stationarity by coordinating the policy updates of agents in various ways, including synchronizing times at which agents are allowed to revise their policies. Synchronization enables analysis of many MARL algorithms via multi-timescale methods, but such synchrony is infeasible in many decentralized applications. In this paper, we study an asynchronous variant of the decentralized Q-learning algorithm, a recent MARL algorithm for stochastic games. We provide sufficient conditions under which the asynchronous algorithm drives play to equilibrium with high probability. Our solution utilizes constant learning rates in the Q-factor update, which we show to be critical for relaxing the synchrony assumptions of earlier work. Our analysis also applies to asynchronous generalizations of a number of other algorithms from the regret testing tradition, whose performance is analyzed by multi-timescale methods that study Markov chains obtained via policy update dynamics. This work extends the applicability of the decentralized Q-learning algorithm and its relatives to settings in which parameters are selected in an independent manner, and tames non-stationarity without imposing the coordination assumptions of prior work.
</details>
<details>
<summary>摘要</summary>
非站点性是多智能游戏学习（MARL）的基本挑战，智能体在学习过程中会更新其行为。许多MARL理论进步避免非站点性挑战，通过协调智能体策略更新的方式，包括同步化智能体更新策略的时间。同步化可以通过多时间标准方法进行分析，但这种同步性在多数分散式应用中是不可能实现的。在这篇论文中，我们研究了一种异步版的分散式Q学习算法，这是一种最近的MARL算法 для随机游戏。我们提供了 sufficient condition，以 guarantee that the asynchronous algorithm drives play to equilibrium with high probability.我们的解决方案使用了常量学习率在Q因子更新中，我们证明这是关键 для放弃先前作品中的同步性假设。我们的分析还适用于异步扩展其他算法，这些算法是由回报测试传统中的表现分析。这项工作扩展了分散式Q学习算法和其相关的算法在不同的参数选择方式下的可用性，并对非站点性进行了控制，而不需要先前作品中的协调假设。
</details></li>
</ul>
<hr>
<h2 id="AdaER-An-Adaptive-Experience-Replay-Approach-for-Continual-Lifelong-Learning"><a href="#AdaER-An-Adaptive-Experience-Replay-Approach-for-Continual-Lifelong-Learning" class="headerlink" title="AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning"></a>AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03810">http://arxiv.org/abs/2308.03810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Li, Bo Tang, Haifeng Li</li>
<li>for: 这篇论文是为了解决人工智能学习框架中的持续性学习问题，即在不断学习的过程中，学习器会忘记之前学习的知识。</li>
<li>methods: 这篇论文提出了一种新的算法，叫做自适应经验回忆（AdaER），它包括两个阶段：记忆回忆和记忆更新。在记忆回忆阶段，AdaER使用了一种Contextually-cued Memory Recall（C-CMR）策略，选择最相似的记忆和当前输入数据和任务进行回忆。此外，AdaER还使用了一种Entropy-balanced Reservoir Sampling（E-BRS）策略来增强记忆缓存的性能。</li>
<li>results: 根据实验结果，AdaER在成本累积学习场景下表现出色，比现有的持续性学习基线更好， highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.<details>
<summary>Abstract</summary>
Continual lifelong learning is an machine learning framework inspired by human learning, where learners are trained to continuously acquire new knowledge in a sequential manner. However, the non-stationary nature of streaming training data poses a significant challenge known as catastrophic forgetting, which refers to the rapid forgetting of previously learned knowledge when new tasks are introduced. While some approaches, such as experience replay (ER), have been proposed to mitigate this issue, their performance remains limited, particularly in the class-incremental scenario which is considered natural and highly challenging. In this paper, we present a novel algorithm, called adaptive-experience replay (AdaER), to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflicting with the current input data in terms of both data and task. Additionally, AdaER incorporates an entropy-balanced reservoir sampling (E-BRS) strategy to enhance the performance of the memory buffer by maximizing information entropy. To evaluate the effectiveness of AdaER, we conduct experiments on established supervised continual lifelong learning benchmarks, specifically focusing on class-incremental learning scenarios. The results demonstrate that AdaER outperforms existing continual lifelong learning baselines, highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.
</details>
<details>
<summary>摘要</summary>
In this paper, we present a novel algorithm called adaptive-experience replay (AdaER) to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflicting with the current input data in terms of both data and task. Additionally, AdaER incorporates an entropy-balanced reservoir sampling (E-BRS) strategy to enhance the performance of the memory buffer by maximizing information entropy.To evaluate the effectiveness of AdaER, we conduct experiments on established supervised continual lifelong learning benchmarks, specifically focusing on class-incremental learning scenarios. The results demonstrate that AdaER outperforms existing continual lifelong learning baselines, highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.
</details></li>
</ul>
<hr>
<h2 id="G-Mix-A-Generalized-Mixup-Learning-Framework-Towards-Flat-Minima"><a href="#G-Mix-A-Generalized-Mixup-Learning-Framework-Towards-Flat-Minima" class="headerlink" title="G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima"></a>G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03236">http://arxiv.org/abs/2308.03236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Li, Bo Tang<br>for: 增强深度神经网络（DNN）的泛化能力，特别是在有限的训练数据时。methods:  combinatorial Mixup技术和Sharpness-Aware Minimization（SAM）方法。results: 提出了一种新的学习框架 called Generalized-Mixup（G-Mix），并 introduces two novel algorithms：Binary G-Mix和Decomposed G-Mix。这些算法可以进一步优化DNN性能，并实现多个数据集和模型的状态反映性。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based on the sharpness-sensitivity of each example to address the issue of "manifold intrusion" in Mixup. Both theoretical explanations and experimental results reveal that the proposed BG-Mix and DG-Mix algorithms further enhance model generalization across multiple datasets and models, achieving state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining"><a href="#Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining" class="headerlink" title="Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining"></a>Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03235">http://arxiv.org/abs/2308.03235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zekaouinoureddine/Opinion-Transformers">https://github.com/zekaouinoureddine/Opinion-Transformers</a></li>
<li>paper_authors: Nour Eddine Zekaoui, Siham Yousfi, Maryem Rhanoui, Mounia Mikram</li>
<li>for: 本研究的目的是研究高性能的Transformer语言模型在意见采集方面的表现，并对它们进行比较，以揭示它们的特点。</li>
<li>methods: 本研究使用了高性能的Transformer语言模型，包括BERT和RoBERTa等，对文本进行意见采集和分析。</li>
<li>results: 研究结果显示，Transformer语言模型在意见采集方面具有优秀的表现，具有较强的表达能力和精度。同时，BERT和RoBERTa等模型在意见采集方面具有显著的差异，可以根据实际应用场景选择合适的模型。<details>
<summary>Abstract</summary>
Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Imbalanced-Large-Graph-Learning-Framework-for-FPGA-Logic-Elements-Packing-Prediction"><a href="#Imbalanced-Large-Graph-Learning-Framework-for-FPGA-Logic-Elements-Packing-Prediction" class="headerlink" title="Imbalanced Large Graph Learning Framework for FPGA Logic Elements Packing Prediction"></a>Imbalanced Large Graph Learning Framework for FPGA Logic Elements Packing Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03231">http://arxiv.org/abs/2308.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixiong Di, Runzhe Tao, Lin Chen, Qiang Wu, Yibo Lin</li>
<li>for: This paper is written for the purpose of predicting whether logic elements will be packed after placement in an FPGA CAD flow.</li>
<li>methods: The paper proposes an imbalanced large graph learning framework called ImLG, which uses dedicated feature extraction and feature aggregation methods to enhance the node representation learning of circuit graphs. The framework also employs techniques such as graph oversampling and mini-batch training to handle the imbalanced distribution of packed and unpacked logic elements.</li>
<li>results: The proposed method achieves an F1 score improvement of 42.82% compared to the most recent Gaussian-based prediction method. Additionally, the physical design results show that the proposed method can assist the placer in improving routed wirelength by 0.93% and SLICE occupation by 0.89%.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为了预测FPGA嵌入式逻辑元素是否会在置换后被压缩而写的。</li>
<li>methods: 论文提出了一种大图学习框架 called ImLG，该框架使用专门的特征提取和特征聚合方法来提高电路图的节点表示学习。它还使用了 Graph oversampling 和 mini-batch 训练来处理嵌入式逻辑元素的不均衡分布。</li>
<li>results: 提议的方法可以提高对最近 Gaussian-based 预测方法的 F1 score 值 by 42.82%。Physical 设计结果表明，提议的方法可以帮助置换器提高路径长度 by 0.93% 和 SLICE 占用率 by 0.89%。<details>
<summary>Abstract</summary>
Packing is a required step in a typical FPGA CAD flow. It has high impacts to the performance of FPGA placement and routing. Early prediction of packing results can guide design optimization and expedite design closure. In this work, we propose an imbalanced large graph learning framework, ImLG, for prediction of whether logic elements will be packed after placement. Specifically, we propose dedicated feature extraction and feature aggregation methods to enhance the node representation learning of circuit graphs. With imbalanced distribution of packed and unpacked logic elements, we further propose techniques such as graph oversampling and mini-batch training for this imbalanced learning task in large circuit graphs. Experimental results demonstrate that our framework can improve the F1 score by 42.82% compared to the most recent Gaussian-based prediction method. Physical design results show that the proposed method can assist the placer in improving routed wirelength by 0.93% and SLICE occupation by 0.89%.
</details>
<details>
<summary>摘要</summary>
packing 是 FPGA CAD 流程中的一个必需步骤，它对 FPGA 的位置和路由产生了重要影响。 early prediction of packing results 可以导引设计优化和加速设计关闭。在这种工作中，我们提出了一个大图学习框架，ImLG，用于预测逻辑元素是否会被压缩 после放置。 Specifically，我们提出了专门的特征提取和特征聚合方法，以增强圈图学习的节点表示学习。 由于逻辑元素的压缩和未压缩的分布是不均衡的，我们还提出了一些技术，如图像扩大和微批训练，来解决这种不均衡的学习任务。 experimental results 表明，我们的框架可以提高 F1 分数比最近 Gaussian-based 预测方法提高42.82%。 physical design results 表明，我们的方法可以帮助置器提高了 routed wirelength 和 SLICE 占用率。
</details></li>
</ul>
<hr>
<h2 id="Tractability-of-approximation-by-general-shallow-networks"><a href="#Tractability-of-approximation-by-general-shallow-networks" class="headerlink" title="Tractability of approximation by general shallow networks"></a>Tractability of approximation by general shallow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03230">http://arxiv.org/abs/2308.03230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrushikesh Mhaskar, Tong Mao</li>
<li>for: 这 paper 描述了一种精炼版的函数approximation算法，用于 approximating 函数的形式为 $x\mapsto\int_{\mathbb{Y}} G(x,y)d\tau(y)$， $x\in\mathbb{X}$， by $G$-networks of the form $x\mapsto\sum_{k&#x3D;1}^n a_kG(x,y_k)$, $y_1,\cdots, y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$。</li>
<li>methods: 该 paper 使用了covering numbers来定义 $\mathbb{X}$ 和 $\mathbb{Y}$ 的维度，然后通过使用 $G$-networks 获得了独立于维度的approximation bound。</li>
<li>results: 该 paper  obtainted dimension-independent approximation bounds, meaning that the bounds do not depend on the dimensions of the input spaces $\mathbb{X}$ and $\mathbb{Y}$. The paper also provided applications of the results to various types of neural networks, including power rectified linear unit networks, zonal function networks, and radial basis function networks.<details>
<summary>Abstract</summary>
In this paper, we present a sharper version of the results in the paper Dimension independent bounds for general shallow networks; Neural Networks, \textbf{123} (2020), 142-152. Let $\mathbb{X}$ and $\mathbb{Y}$ be compact metric spaces. We consider approximation of functions of the form $ x\mapsto\int_{\mathbb{Y}} G( x, y)d\tau( y)$, $ x\in\mathbb{X}$, by $G$-networks of the form $ x\mapsto \sum_{k=1}^n a_kG( x, y_k)$, $ y_1,\cdots, y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$. Defining the dimensions of $\mathbb{X}$ and $\mathbb{Y}$ in terms of covering numbers, we obtain dimension independent bounds on the degree of approximation in terms of $n$, where also the constants involved are all dependent at most polynomially on the dimensions. Applications include approximation by power rectified linear unit networks, zonal function networks, certain radial basis function networks as well as the important problem of function extension to higher dimensional spaces.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一个更加锐化的结果，即纸张《独立维度下的深度网络 bound》（Neural Networks, 123（2020），142-152页）。我们假设 $\mathbb{X}$ 和 $\mathbb{Y}$ 是两个紧密的 метри空间。我们考虑了 $\mathbb{X}$ 上函数 $x\mapsto\int_{\mathbb{Y}} G(x,y)d\tau(y)$ 的近似问题，其中 $G$ 是一个从 $\mathbb{X}$ 到 $\mathbb{Y}$ 的函数，$x\in\mathbb{X}$，$y_1,\cdots,y_n\in\mathbb{Y}$，$a_1,\cdots,a_n\in\mathbb{R}$。我们使用 $\mathbb{X}$ 和 $\mathbb{Y}$ 的覆盖数来定义这些空间的维度，并得到了独立于维度的近似度 bound，其中包括了 $n$ 的度量，同时 constants 都是仅仅受到维度的 polynomial 依赖。这些结果有广泛的应用，包括矩形函数网络、zonal函数网络、某些径向基函数网络以及高维空间中函数扩展的重要问题。
</details></li>
</ul>
<hr>
<h2 id="Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023"><a href="#Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023" class="headerlink" title="Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)"></a>Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03228">http://arxiv.org/abs/2308.03228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Kodner, Sarah Payne, Jeffrey Heinz</li>
<li>for: The paper argues against Piantadosi’s (2023) claim that modern language models refute Chomsky’s approach to language, and instead presents a case for the continued relevance of generative linguistics in the 21st century.</li>
<li>methods: The paper focuses on four main points to critique Piantadosi’s claim, including the limited data exposure required for human language acquisition, the inability of LLMs to solve the central mystery of language learning, the lack of interpretable explanations provided by LLMs, and the need for a separate theory of language and cognition.</li>
<li>results: The paper concludes that generative linguistics will remain an indispensable scientific discipline throughout the 21st century and beyond, due to its ability to provide a theory of language and cognition that can explain human linguistic and cognitive capabilities.<details>
<summary>Abstract</summary>
We present a critical assessment of Piantadosi's (2023) claim that "Modern language models refute Chomsky's approach to language," focusing on four main points. First, despite the impressive performance and utility of large language models (LLMs), humans achieve their capacity for language after exposure to several orders of magnitude less data. The fact that young children become competent, fluent speakers of their native languages with relatively little exposure to them is the central mystery of language learning to which Chomsky initially drew attention, and LLMs currently show little promise of solving this mystery. Second, what can the artificial reveal about the natural? Put simply, the implications of LLMs for our understanding of the cognitive structures and mechanisms underlying language and its acquisition are like the implications of airplanes for understanding how birds fly. Third, LLMs cannot constitute scientific theories of language for several reasons, not least of which is that scientific theories must provide interpretable explanations, not just predictions. This leads to our final point: to even determine whether the linguistic and cognitive capabilities of LLMs rival those of humans requires explicating what humans' capacities actually are. In other words, it requires a separate theory of language and cognition; generative linguistics provides precisely such a theory. As such, we conclude that generative linguistics as a scientific discipline will remain indispensable throughout the 21st century and beyond.
</details>
<details>
<summary>摘要</summary>
我们提出了对Piantadosi（2023）的批判，关注四个主要点。首先，虽然大型语言模型（LLMs）表现出色且有用，但人类通过对数量更少的数据进行抵抗而获得语言能力。孩子们在获得native语言的能力上即使只有相对较少的暴露也能够成为流利的语言使用者，这是语言学习的中心谜题，LLMs目前并没有解决这个谜题。第二，人工智能可以抛光自然语言吗？简单地说，LLMs对语言和其学习的各种认知结构和机制的含义是如何飞行机器对鸟类飞行的含义。第三，LLMs不能代表语言科学的理论，主要是因为科学理论需要可解释的解释，而不仅仅是预测。这导致我们的最后一点：以规避解释 humans的语言和认知能力的准确性，我们需要一个分析 humans的语言和认知能力的理论。通过这样的理论，我们可以确定LLMs的语言和认知能力是否与人类相当。因此，我们结论认为生成语言学将在21世纪和以后保持不可或缺的重要性。
</details></li>
</ul>
<hr>
<h2 id="Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning"><a href="#Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning" class="headerlink" title="Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning"></a>Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03217">http://arxiv.org/abs/2308.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linbo Wang, Jing Wu, Xianyong Fang, Zhengyi Liu, Chenjie Cao, Yanwei Fu</li>
<li>for: 提高两视匹配学习框架的性能，特别是对于缺乏精度的相对pose和匹配可靠性的评估。</li>
<li>methods: 提议了两个方法来提高现有框架：一是Local Feature Consensus（LFC）插件块，可以增强现有模型的特征；二是将现有模型扩展为对称网络，并使用对称损失来利用相互投影的信息。</li>
<li>results: 通过对比 dataset 上的表现，实际 achieved state-of-the-art 性能。<details>
<summary>Abstract</summary>
Recent studies of two-view correspondence learning usually establish an end-to-end network to jointly predict correspondence reliability and relative pose. We improve such a framework from two aspects. First, we propose a Local Feature Consensus (LFC) plugin block to augment the features of existing models. Given a correspondence feature, the block augments its neighboring features with mutual neighborhood consensus and aggregates them to produce an enhanced feature. As inliers obey a uniform cross-view transformation and share more consistent learned features than outliers, feature consensus strengthens inlier correlation and suppresses outlier distraction, which makes output features more discriminative for classifying inliers/outliers. Second, existing approaches supervise network training with the ground truth correspondences and essential matrix projecting one image to the other for an input image pair, without considering the information from the reverse mapping. We extend existing models to a Siamese network with a reciprocal loss that exploits the supervision of mutual projection, which considerably promotes the matching performance without introducing additional model parameters. Building upon MSA-Net, we implement the two proposals and experimentally achieve state-of-the-art performance on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
最近的两视匹配学习研究通常建立一个端到端网络，同时预测匹配可靠性和相对pose。我们从两个方面提高这种框架：首先，我们提议在已有模型中添加本地特征协调（LFC）插件块，用于增强特征。给定一个匹配特征，该块在相邻特征中查找mutual neighborhood consensus，并将其聚合以生成提高后的特征。由于准确的匹配点遵循均匀的双视变换，并且在不同视图中分享更多的学习特征，因此特征协调强化匹配点的相互关联，同时减少了噪声的扰乱，使输出特征更加精细地分类匹配点和噪声。其次，现有方法在网络训练时使用真实的匹配和主对投影，而不考虑反向映射的信息。我们扩展现有模型到siamese网络，并使用相互损失来优化匹配性，不需要添加更多的模型参数。基于MSA-Net，我们实现了两个提议，并在benchmark数据集上实验性实现了状态之前的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-SGD-Batch-Size-on-Autoencoder-Learning-Sparsity-Sharpness-and-Feature-Learning"><a href="#The-Effect-of-SGD-Batch-Size-on-Autoencoder-Learning-Sparsity-Sharpness-and-Feature-Learning" class="headerlink" title="The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning"></a>The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03215">http://arxiv.org/abs/2308.03215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Ghosh, Spencer Frei, Wooseok Ha, Bin Yu</li>
<li>for: 这个论文研究了使用梯度下降法（SGD）训练单神经 autoencoder 的动态性，特别是在线性或ReLU激活函数下。</li>
<li>methods: 这篇论文使用了SGD算法，并研究了不同批处理大小的影响。</li>
<li>results: 研究发现，SGD可以在非对称问题上找到全局最优解，但是这个全局最优解与批处理大小有关。在全批处理情况下，解是密集的（即不含杂），并且与初始方向高度一致，这表明在这种情况下， Stochastic gradient descent 并没有进行feature learning。相反，在任何小于样本数的批处理情况下，SGD会找到一个全局最优解，这个解是稀疏的（即与初始方向垂直），并且与feature learning相关。此外，通过评估梯度下降法的锐度，我们发现，使用全批处理 gradient descent 时，解的锐度较低，而使用小于样本数的批处理时，解的锐度较高。<details>
<summary>Abstract</summary>
In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successfully finds a global minimum for any batch size choice. However, the particular global minimum found depends upon the batch size. In the full-batch setting, we show that the solution is dense (i.e., not sparse) and is highly aligned with its initialized direction, showing that relatively little feature learning occurs. On the other hand, for any batch size strictly smaller than the number of samples, SGD finds a global minimum which is sparse and nearly orthogonal to its initialization, showing that the randomness of stochastic gradients induces a qualitatively different type of "feature selection" in this setting. Moreover, if we measure the sharpness of the minimum by the trace of the Hessian, the minima found with full batch gradient descent are flatter than those found with strictly smaller batch sizes, in contrast to previous works which suggest that large batches lead to sharper minima. To prove convergence of SGD with a constant step size, we introduce a powerful tool from the theory of non-homogeneous random walks which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究束变数学梯度下降（SGD）在训练单神经 autoencoder 时的动态。我们发现，对于非拟合问题，随机初始化的 SGD  WITH constant step size 能够成功找到全局最小值，但特定的全局最小值与批处理大小有关。在完整批处理Setting中，我们显示解是 dense（即不夹），与其初始方向高度对齐，表明相对较少特征学习发生。相反，任何小于样本数量的批处理大小，SGD 会找到一个全局最小值，这个最小值是稀疏的并且几乎与其初始方向垂直，这表明随机梯度的Randomness 导致了一种不同类型的 "特征选择"。此外，如果我们测量 minimum 的锐度通过 Hessian 的跟踪，则使用 full batch 梯度下降的 minimum 是浅的，而不同于先前的研究，大批处理会导致更锐的 minimum。为证明 SGD  WITH constant step size 的收敛，我们引入了非Homogeneous random walks 的一种有力的工具，这可能是独立的 интерес。
</details></li>
</ul>
<hr>
<h2 id="Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits"><a href="#Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits" class="headerlink" title="Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits"></a>Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03212">http://arxiv.org/abs/2308.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lena Strobl</li>
<li>for: 本研究探讨了Transformers模型在自然语言处理任务中的应用，以及它们与常深度阈值电路之间的关系。</li>
<li>methods: 本研究使用了average-hard attention和log-precision transformers两种模型，并证明它们可以recognizeTC0复杂度类型的语言。</li>
<li>results: 研究结果显示，transformers模型可以被视为常深度阈值电路的延展，并且可以通过生成一个固定深度和尺寸的阈值电路来实现。此外，研究还发现log-precision transformers模型比average-hard attention transformers模型更为稳定和可靠。<details>
<summary>Abstract</summary>
Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
</details>
<details>
<summary>摘要</summary>
启发器（Transformers）已经成为自然语言处理任务中广泛使用的神经网络模型。前一些研究探讨了它们与常深度阈值电路之间的关系，并假设了两点：均值困难注意力和对内部计算的对数精度相对于输入长度。美利尔等（2022）证明了均值困难注意力启发器可以识别TC0复杂度类型中的语言，这种语言可以被表示为常深度多阶阈值电路。 Similarly,美利尔和沙巴华尔（2023）表明了log精度启发器可以识别uniform TC0复杂度类型中的语言。这示出了两种启发器模型都可以被模拟为常深度阈值电路，但后者更加稳定，因为它生成了一个具有 uniform 性的电路家族。我们的论文显示，第一个结果可以扩展到生成uniformCircuits。
</details></li>
</ul>
<hr>
<h2 id="Time-Parameterized-Convolutional-Neural-Networks-for-Irregularly-Sampled-Time-Series"><a href="#Time-Parameterized-Convolutional-Neural-Networks-for-Irregularly-Sampled-Time-Series" class="headerlink" title="Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series"></a>Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03210">http://arxiv.org/abs/2308.03210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chrysoula Kosma, Giannis Nikolentzos, Michalis Vazirgiannis</li>
<li>for: 处理不规则时间序列数据</li>
<li>methods: 使用时间参数化卷积神经网络（TPCNN），其中卷积核心使用时间explicitly初始化</li>
<li>results: TPCNN在 interpolación 和分类任务中表现竞争力强，同时可以具有更高的效率和可读性。<details>
<summary>Abstract</summary>
Irregularly sampled multivariate time series are ubiquitous in several application domains, leading to sparse, not fully-observed and non-aligned observations across different variables. Standard sequential neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), consider regular spacing between observation times, posing significant challenges to irregular time series modeling. While most of the proposed architectures incorporate RNN variants to handle irregular time intervals, convolutional neural networks have not been adequately studied in the irregular sampling setting. In this paper, we parameterize convolutional layers by employing time-explicitly initialized kernels. Such general functions of time enhance the learning process of continuous-time hidden dynamics and can be efficiently incorporated into convolutional kernel weights. We, thus, propose the time-parameterized convolutional neural network (TPCNN), which shares similar properties with vanilla convolutions but is carefully designed for irregularly sampled time series. We evaluate TPCNN on both interpolation and classification tasks involving real-world irregularly sampled multivariate time series datasets. Our experimental results indicate the competitive performance of the proposed TPCNN model which is also significantly more efficient than other state-of-the-art methods. At the same time, the proposed architecture allows the interpretability of the input series by leveraging the combination of learnable time functions that improve the network performance in subsequent tasks and expedite the inaugural application of convolutions in this field.
</details>
<details>
<summary>摘要</summary>
非 régulièrement采样多Variable时间序列是各个应用领域中的普遍存在，导致不同变数之间的观测不充分、不一致。标准的时间序列架构，如回传神经网络（RNN）和卷积神经网络（CNN），假设观测时间的规律性，对不规时间序列模型造成严重的挑战。大多数提出的架构包括RNN的变形，但是尚未充分研究在不规时间序列 Setting中的卷积神经网络。在这篇文章中，我们将时间explicitly启动kernel，以提高不规时间序列中隐藏的连续时间动力学学习过程。这些时间敏感的核心函数可以高效地包含到卷积核心中。因此，我们提出了时间参数化卷积神经网络（TPCNN），它和普通的卷积架构相似，但是特别的设计来应对不规时间序列。我们将TPCNN应用到真实世界的不规时间序列多Variable时间序列数据集上的 interpolating 和分类任务中，实验结果显示TPCNN模型的竞争性表现，同时比其他现有的方法更高效。此外，提案的架构可以允许输入序列的解释性，通过搜寻时间函数来提高网络性能，并且将卷积神经网络引入这个领域。
</details></li>
</ul>
<hr>
<h2 id="Communication-Free-Distributed-GNN-Training-with-Vertex-Cut"><a href="#Communication-Free-Distributed-GNN-Training-with-Vertex-Cut" class="headerlink" title="Communication-Free Distributed GNN Training with Vertex Cut"></a>Communication-Free Distributed GNN Training with Vertex Cut</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03209">http://arxiv.org/abs/2308.03209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaidi Cao, Rui Deng, Shirley Wu, Edward W Huang, Karthik Subbian, Jure Leskovec</li>
<li>for: 快速训练图 neural network（GNN）在实际世界上的图中，即使是图中的亿个节点和边也是很困难的，主要是因为需要很大的内存来存储图和其中的中间节点和边特征。现有的分布式方法需要频繁的跨GPU交互，导致训练过程中的时间开销很大，并且随着训练进程的扩展，缓慢的扩展。这里，我们介绍了CoFree-GNN，一种新的分布式GNN训练框架，可以快速加速GNN训练过程。</li>
<li>methods: CoFree-GNN使用了一种Vertex Cut分区，即在分区过程中，不是截断图中的边，而是将边分区，并将节点信息复制以保持图结构。此外，框架还包括一种重要机制来处理受损的图分布，以保持模型的准确性。此外，我们还提出了一种改进的DropEdge技术来进一步加速训练过程。</li>
<li>results: 使用了一系列实际网络的实验表明，CoFree-GNN可以比现有的GNN训练方法快速到10倍。<details>
<summary>Abstract</summary>
Training Graph Neural Networks (GNNs) on real-world graphs consisting of billions of nodes and edges is quite challenging, primarily due to the substantial memory needed to store the graph and its intermediate node and edge features, and there is a pressing need to speed up the training process. A common approach to achieve speed up is to divide the graph into many smaller subgraphs, which are then distributed across multiple GPUs in one or more machines and processed in parallel. However, existing distributed methods require frequent and substantial cross-GPU communication, leading to significant time overhead and progressively diminishing scalability. Here, we introduce CoFree-GNN, a novel distributed GNN training framework that significantly speeds up the training process by implementing communication-free training. The framework utilizes a Vertex Cut partitioning, i.e., rather than partitioning the graph by cutting the edges between partitions, the Vertex Cut partitions the edges and duplicates the node information to preserve the graph structure. Furthermore, the framework maintains high model accuracy by incorporating a reweighting mechanism to handle a distorted graph distribution that arises from the duplicated nodes. We also propose a modified DropEdge technique to further speed up the training process. Using an extensive set of experiments on real-world networks, we demonstrate that CoFree-GNN speeds up the GNN training process by up to 10 times over the existing state-of-the-art GNN training approaches.
</details>
<details>
<summary>摘要</summary>
训练图 neural network (GNN) 在实际图中包含数百万个节点和边是很困难的，主要因为需要巨量的内存来存储图和其中的中间节点和边特征。随着图的大小不断增长，训练过程的速度变得非常重要。现有的分布式方法需要频繁的跨GPU通信，导致训练过程中的时间开销很大，并且随着图的尺度的增长，缩放性逐渐减退。我们在这里介绍CoFree-GNN，一种新的分布式GNN训练框架，可以快速加速GNN训练过程。CoFree-GNN使用Vertex Cut分区，而不是将图分割为多个分区，然后在每个分区上进行并发训练。此外，框架还包括一种重量调整机制，以处理由复制节点引起的图分布偏见。我们还提出了一种修改后 DropEdge 技术，以进一步加速训练过程。通过对实际网络进行了广泛的实验，我们表明CoFree-GNN可以在 GNN 训练过程中提高速度，达到10倍以上。
</details></li>
</ul>
<hr>
<h2 id="Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP"><a href="#Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP" class="headerlink" title="Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)"></a>Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03203">http://arxiv.org/abs/2308.03203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Sultan, Yongqiang Wang, James Scanlon, Lisa D’lima</li>
<li>for: 研究人员对于人体细胞地图的详细分类，以实现更好的生物医学研究。</li>
<li>methods: 使用各种后门架构，包括FastAI U-Net模型、更深的模型和Feature Pyramid Networks，对2D Periodic Acid-Schiff（PAS）染色的人类肾脏 Histology 图像进行分类。</li>
<li>results: 透过对基eline U-Net模型的评估，发现不同的后门架构具有不同的表现，提供了实验领域中未来研究的 valuable insights。<details>
<summary>Abstract</summary>
Image segmentation serves as a critical tool across a range of applications, encompassing autonomous driving's pedestrian detection and pre-operative tumor delineation in the medical sector. Among these applications, we focus on the National Institutes of Health's (NIH) Human BioMolecular Atlas Program (HuBMAP), a significant initiative aimed at creating detailed cellular maps of the human body. In this study, we concentrate on segmenting various microvascular structures in human kidneys, utilizing 2D Periodic Acid-Schiff (PAS)-stained histology images. Our methodology begins with a foundational FastAI U-Net model, upon which we investigate alternative backbone architectures, delve into deeper models, and experiment with Feature Pyramid Networks. We rigorously evaluate these varied approaches by benchmarking their performance against our baseline U-Net model. This study thus offers a comprehensive exploration of cutting-edge segmentation techniques, providing valuable insights for future research in the field.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:图像分割作为应用程序的重要工具，涵盖自动驾驶中的人体检测和医疗领域中的先驱性肿瘤定位。其中我们专注于国家医疗研究所（NIH）的人生物分子地图计划（HuBMAP），这是一项创造细胞图像的重要 iniciative。在这种研究中，我们集中在人体肾脏中的微血管结构分割中，使用2D Periodic Acid-Schiff（PAS）染色 histology 图像。我们的方法开始于基础 FastAI U-Net 模型，然后我们 investigate 其他脊梁架构、深度模型和特征峰网络。我们严格评估这些不同的方法，对比基准 U-Net 模型的性能。这项研究因此提供了当今图像分割技术的全面探索，为未来研究提供了有价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Source-free-Domain-Adaptive-Human-Pose-Estimation"><a href="#Source-free-Domain-Adaptive-Human-Pose-Estimation" class="headerlink" title="Source-free Domain Adaptive Human Pose Estimation"></a>Source-free Domain Adaptive Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03202">http://arxiv.org/abs/2308.03202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidpengucf/sfdahpe">https://github.com/davidpengucf/sfdahpe</a></li>
<li>paper_authors: Qucheng Peng, Ce Zheng, Chen Chen</li>
<li>for: 实现人姿估测（HPE）中的数据隐私和安全性，以及在实际应用中降低数据成本。</li>
<li>methods: 提出了一个新任务 named source-free domain adaptive HPE，并提出了一个新的框架，包括三个模型：源模型、中介模型和目标模型，从源资料保护和目标资料相关的角度来探索任务。</li>
<li>results: 在多个页面适应HPE的评估标准上，提出的方法比现有方法优化了许多margin。<details>
<summary>Abstract</summary>
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise, and the target-relevant module reduces the sparsity of spatial representations by building a novel spatial probability space, and pose-specific contrastive learning and information maximization are proposed on the basis of this space. Comprehensive experiments on several domain adaptive HPE benchmarks show that the proposed method outperforms existing approaches by a considerable margin. The codes are available at https://github.com/davidpengucf/SFDAHPE.
</details>
<details>
<summary>摘要</summary>
人体姿势估计（HPE）在多个领域得到广泛应用，包括运动分析、医疗和虚拟现实。然而，实际世界数据的高成本成为HPE的一大挑战。为了解决这个问题，一种方法是使用生成的 sintetic 数据进行HPE模型的训练，然后进行适应（DA）操作以适应实际世界数据。然而，现有的DA方法 дляHPE忽视了数据隐私和安全性，因为它们使用了源数据和目标数据在适应过程中。为此，我们提出了一个新的任务，即无源领域适应HPE，以解决跨领域学习HPE的挑战。我们还提出了一个新的框架，该框架包括三个模型：源模型、中间模型和目标模型，该框架从源保护和目标相关两个角度出发，以更好地保持源信息和适应目标数据。源保护模块能更好地保持源信息，并抵抗噪声，而目标相关模块通过构建一个新的空间概率空间，减少了空间表示的稀疏性，并通过基于这个空间的姿势特定的对比学习和信息最大化来提高pose的预测精度。经过了多个领域适应HPE的 benchmk 测试，我们发现我们的方法在与现有方法进行比较时表现出了显著的优势。代码可以在 <https://github.com/davidpengucf/SFDAHPE> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies"><a href="#Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies" class="headerlink" title="Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"></a>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03188">http://arxiv.org/abs/2308.03188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teacherpeterpan/self-correction-llm-papers">https://github.com/teacherpeterpan/self-correction-llm-papers</a></li>
<li>paper_authors: Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang</li>
<li>for: 本文旨在探讨自动反馈技术的应用在大语言模型（LLM）上，以改进LLM的表现和可deployability。</li>
<li>methods: 本文分析和概括了各种使用自动反馈技术的最新研究，包括训练时间、生成时间和后续修正。</li>
<li>results: 本文总结了这些技术的主要应用场景，并对未来的发展和挑战进行了讨论。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hallucination" 是指 LLM 生成的 conten 中出现的 fictitious or absurd 内容，例如 "the cat is wearing a hat"。* "unfaithful reasoning" 是指 LLM 的输出中出现的不合理或 absurd 逻辑，例如 "the sky is blue because the grass is green"。* "toxic content" 是指 LLM 生成的 conten 中出现的负面或不适合的内容，例如 hate speech 或 vulgar language。* "self-correction" 是指 LLM 本身的输出中自动检测并修正错误或不合理的部分的技术。* "automated feedback" 是指由 LLM 本身或外部系统生成的自动反馈，用于 Rectify  LL M 的输出中的错误或不合理的部分的技术。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Method-for-Modeling-Confidence-in-Recommendations-with-Learned-Beta-Distributions"><a href="#A-Lightweight-Method-for-Modeling-Confidence-in-Recommendations-with-Learned-Beta-Distributions" class="headerlink" title="A Lightweight Method for Modeling Confidence in Recommendations with Learned Beta Distributions"></a>A Lightweight Method for Modeling Confidence in Recommendations with Learned Beta Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03186">http://arxiv.org/abs/2308.03186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nkny/confidencerecsys2023">https://github.com/nkny/confidencerecsys2023</a></li>
<li>paper_authors: Norman Knyazev, Harrie Oosterhuis</li>
<li>for: 这个论文旨在提供一种简单实用的推荐方法，并提供一个直观的度量confidence。</li>
<li>methods: 该方法使用学习 beta 分布（LBD）来预测用户喜好，并通过 beta 分布的形式来表示 confidence。</li>
<li>results: 该方法可以维持与现有方法相当的准确性，同时具有与准确性直接相关的confidence度量。此外，在高精度目标推荐任务中，LBD表现更优于现有方法。<details>
<summary>Abstract</summary>
Most Recommender Systems (RecSys) do not provide an indication of confidence in their decisions. Therefore, they do not distinguish between recommendations of which they are certain, and those where they are not. Existing confidence methods for RecSys are either inaccurate heuristics, conceptually complex or computationally very expensive. Consequently, real-world RecSys applications rarely adopt these methods, and thus, provide no confidence insights in their behavior. In this work, we propose learned beta distributions (LBD) as a simple and practical recommendation method with an explicit measure of confidence. Our main insight is that beta distributions predict user preferences as probability distributions that naturally model confidence on a closed interval, yet can be implemented with the minimal model-complexity. Our results show that LBD maintains competitive accuracy to existing methods while also having a significantly stronger correlation between its accuracy and confidence. Furthermore, LBD has higher performance when applied to a high-precision targeted recommendation task. Our work thus shows that confidence in RecSys is possible without sacrificing simplicity or accuracy, and without introducing heavy computational complexity. Thereby, we hope it enables better insight into real-world RecSys and opens the door for novel future applications.
</details>
<details>
<summary>摘要</summary>
大多数推荐系统（RecSys）没有提供决策的信任度标示。因此，它们无法 отлича между确定的推荐和不确定的推荐。现有的信任方法对RecSys是不准确的规则、概念上复杂或计算昂贵。因此，现实世界中的RecSys应用rarely采用这些方法，因此无法提供信任情况的视角。在这项工作中，我们提议使用学习beta分布（LBD）作为简单、实用的推荐方法，并提供显式的信任度标示。我们的主要发现是，beta分布预测用户喜好的概率分布，自然模型信任的关闭区间，但可以实现最小的模型复杂度。我们的结果表明，LBD与现有方法的竞争性准确度相当，同时其信任度与准确度之间具有显著的相关性。此外，LBD在高精度目标推荐任务中表现更高。我们的工作因此表明，RecSys中的信任是可能的，不需要牺牲简单性或准确度，也不需要承受重大的计算复杂度。这样，我们希望能够为实际世界中的RecSys提供更好的视角，并开启未来应用的新可能性。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Review-of-Physics-Informed-Machine-Learning-Applications-in-Subsurface-Energy-Systems"><a href="#A-Critical-Review-of-Physics-Informed-Machine-Learning-Applications-in-Subsurface-Energy-Systems" class="headerlink" title="A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems"></a>A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04457">http://arxiv.org/abs/2308.04457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdeldjalil Latrach, Mohamed Lamine Malki, Misael Morales, Mohamed Mehana, Minou Rabiei</li>
<li>For: The paper is written for researchers and practitioners in the field of machine learning, particularly in the application of physics-informed machine learning (PIML) to subsurface energy systems, such as the oil and gas industry.* Methods: The paper reviews and discusses the applications of PIML techniques in various tasks related to subsurface energy systems, including seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages.* Results: The paper highlights the successful utilization of PIML for providing more accurate and reliable predictions for resource management and operational efficiency in the oil and gas industry, and demonstrates its potential for revolutionizing the industry and other emerging areas of interest, such as carbon and hydrogen storage, and geothermal systems.<details>
<summary>Abstract</summary>
Machine learning has emerged as a powerful tool in various fields, including computer vision, natural language processing, and speech recognition. It can unravel hidden patterns within large data sets and reveal unparalleled insights, revolutionizing many industries and disciplines. However, machine and deep learning models lack interpretability and limited domain-specific knowledge, especially in applications such as physics and engineering. Alternatively, physics-informed machine learning (PIML) techniques integrate physics principles into data-driven models. By combining deep learning with domain knowledge, PIML improves the generalization of the model, abidance by the governing physical laws, and interpretability. This paper comprehensively reviews PIML applications related to subsurface energy systems, mainly in the oil and gas industry. The review highlights the successful utilization of PIML for tasks such as seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages. Additionally, it demonstrates PIML's capabilities to revolutionize the oil and gas industry and other emerging areas of interest, such as carbon and hydrogen storage; and geothermal systems by providing more accurate and reliable predictions for resource management and operational efficiency.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本）机器学习已经成为不同领域的强大工具，包括计算机视觉、自然语言处理和语音识别。它可以找到大量数据中隐藏的模式，并提供无与伦比的发现，革命化许多行业和学科。然而，机器学习和深度学习模型缺乏解释性和具体领域知识，尤其是在应用物理和工程领域。相反，物理知识整合机器学习（PIML）技术将物理原理 integrate 到数据驱动模型中。通过结合深度学习和领域知识，PIML提高模型的总体化能力，遵循物理法律，并提供解释性。本文全面评论了 relate 到地下能源系统的 PIML 应用，主要是石油和天然气领域。文章强调 PIML 在探测和生产阶段的准确预测、资源管理和运营效率等方面的成功应用。此外，它还 demon стри PIML 可以革命化石油和天然气行业，以及其他emerging 领域，如碳和氢存储，以及地热系统，提供更加准确和可靠的预测，为资源管理和运营效率提供更好的基础。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Machine-Learning-Diagnostic-Models-to-New-Populations-Using-a-Small-Amount-of-Data-Results-from-Clinical-Neuroscience"><a href="#Adapting-Machine-Learning-Diagnostic-Models-to-New-Populations-Using-a-Small-Amount-of-Data-Results-from-Clinical-Neuroscience" class="headerlink" title="Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience"></a>Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03175">http://arxiv.org/abs/2308.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongguang Wang, Guray Erus, Pratik Chaudhari, Christos Davatzikos<br>for:This paper aims to address the problem of reproducibility crisis in machine learning (ML) models applied to neuroimaging data, specifically for the diagnosis of Alzheimer’s disease (AD) and schizophrenia (SZ), and estimation of brain age.methods:The authors propose a weighted empirical risk minimization approach that combines data from a source group (e.g., subjects with similar attributes such as sex, age group, race, and clinical cohort) with a small fraction of data from the target group (e.g., other sex, age group, etc.) to make predictions on the target group.results:The approach achieves substantially better accuracy than existing domain adaptation techniques, with area under curve greater than 0.95 for AD classification, area under curve greater than 0.7 for SZ classification, and mean absolute error less than 5 years for brain age prediction on all target groups. The models also demonstrate utility for prognostic tasks such as predicting disease progression in individuals with mild cognitive impairment, and lead to new clinical insights regarding correlations with neurophysiological tests.Here is the result in Simplified Chinese text:for:这篇论文目标是解决机器学习（ML）模型在神经成像数据中的可重复性危机，特别是对于诊断阿尔ц海默病（AD）和偏头病（SZ），以及脑年龄的估计。methods:作者提议一种加权实际风险最小化方法，将来源组（例如，按照性别、年龄组、种族和临床群组划分的subject）与小型数据集（例如，其他性别、年龄组、等等）结合，以便在目标组（例如，其他性别、年龄组、等等）上进行预测。results:该方法在现有的领域适应技术上取得了显著更好的准确性，其中AD分类 tasks 的准确率大于 0.95，SZ分类 tasks 的准确率大于 0.7，脑年龄预测 tasks 的平均绝对误差小于 5 年，并在所有目标组上具有良好的一致性。此外，模型还能够用于诊断患有轻度认知障碍的病人的疾病进程预测，并且提供了新的临床意义，例如脑physiological 测试的相关性。<details>
<summary>Abstract</summary>
Machine learning (ML) has shown great promise for revolutionizing a number of areas, including healthcare. However, it is also facing a reproducibility crisis, especially in medicine. ML models that are carefully constructed from and evaluated on a training set might not generalize well on data from different patient populations or acquisition instrument settings and protocols. We tackle this problem in the context of neuroimaging of Alzheimer's disease (AD), schizophrenia (SZ) and brain aging. We develop a weighted empirical risk minimization approach that optimally combines data from a source group, e.g., subjects are stratified by attributes such as sex, age group, race and clinical cohort to make predictions on a target group, e.g., other sex, age group, etc. using a small fraction (10%) of data from the target group. We apply this method to multi-source data of 15,363 individuals from 20 neuroimaging studies to build ML models for diagnosis of AD and SZ, and estimation of brain age. We found that this approach achieves substantially better accuracy than existing domain adaptation techniques: it obtains area under curve greater than 0.95 for AD classification, area under curve greater than 0.7 for SZ classification and mean absolute error less than 5 years for brain age prediction on all target groups, achieving robustness to variations of scanners, protocols, and demographic or clinical characteristics. In some cases, it is even better than training on all data from the target group, because it leverages the diversity and size of a larger training set. We also demonstrate the utility of our models for prognostic tasks such as predicting disease progression in individuals with mild cognitive impairment. Critically, our brain age prediction models lead to new clinical insights regarding correlations with neurophysiological tests.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration"><a href="#Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration" class="headerlink" title="Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration"></a>Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03172">http://arxiv.org/abs/2308.03172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aoshuang92/miscalibration_ts">https://github.com/aoshuang92/miscalibration_ts</a></li>
<li>paper_authors: Shuang Ao, Stefan Rueger, Advaith Siddharthan</li>
<li>for: 本研究旨在探讨深度神经网络的准确性calibration问题，以提高安全关键任务中模型的预测可靠性。</li>
<li>methods: 本研究提出了一种新的评估指标——miscalibration score，用于评估模型的总体和分类准确性状态，包括过度自信和下降自信。此外，我们还提出了一种基于分类准确性评估的calibration技术，可以处理过度自信和下降自信问题。</li>
<li>results: 我们的实验结果显示，我们的提出的calibration技术在诸多任务上显著超过了现有的calibration技术。此外，我们还验证了我们的方法在自动故障检测任务中的可靠性和信任性，并发现我们的方法可以提高故障检测和模型的信任性。<details>
<summary>Abstract</summary>
Proper confidence calibration of deep neural networks is essential for reliable predictions in safety-critical tasks. Miscalibration can lead to model over-confidence and/or under-confidence; i.e., the model's confidence in its prediction can be greater or less than the model's accuracy. Recent studies have highlighted the over-confidence issue by introducing calibration techniques and demonstrated success on various tasks. However, miscalibration through under-confidence has not yet to receive much attention. In this paper, we address the necessity of paying attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as trustworthiness of the model. The code are available at \url{https://github.com/AoShuang92/miscalibration_TS}.
</details>
<details>
<summary>摘要</summary>
深度神经网络的准确性calibration是安全关键任务中的一个重要因素。不当calibration可能导致模型过于自信或者不足自信，即模型对其预测的自信度高于或低于模型的准确率。 latest studies have highlighted the over-confidence issue by introducing calibration techniques and have demonstrated success on various tasks. However, the under-confidence issue has not yet received much attention. In this paper, we emphasize the need to pay attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as the trustworthiness of the model. The code is available at \url{https://github.com/AoShuang92/miscalibration_TS}.
</details></li>
</ul>
<hr>
<h2 id="Detection-of-Anomalies-in-Multivariate-Time-Series-Using-Ensemble-Techniques"><a href="#Detection-of-Anomalies-in-Multivariate-Time-Series-Using-Ensemble-Techniques" class="headerlink" title="Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques"></a>Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03171">http://arxiv.org/abs/2308.03171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasios Iliopoulos, John Violos, Christos Diou, Iraklis Varlamis</li>
<li>for: 本研究旨在提出一种基于深度神经网络的多变量时间序列异常检测方法，以解决多变量时间序列中异常事件罕见性的问题。</li>
<li>methods: 本方法基于LSTM、Autoencoder和Convolutional Autoencoder等深度神经网络模型，并采用特征袋包技术和嵌入式PCA变换以提高模型性能。</li>
<li>results: 对于SKAB数据集，提议的ensemble方法在异常检测精度方面比基本模型提高2%，而在半监督模型中，提议的方法在异常检测精度方面比基本模型提高10%以上。<details>
<summary>Abstract</summary>
Anomaly Detection in multivariate time series is a major problem in many fields. Due to their nature, anomalies sparsely occur in real data, thus making the task of anomaly detection a challenging problem for classification algorithms to solve. Methods that are based on Deep Neural Networks such as LSTM, Autoencoders, Convolutional Autoencoders etc., have shown positive results in such imbalanced data. However, the major challenge that algorithms face when applied to multivariate time series is that the anomaly can arise from a small subset of the feature set. To boost the performance of these base models, we propose a feature-bagging technique that considers only a subset of features at a time, and we further apply a transformation that is based on nested rotation computed from Principal Component Analysis (PCA) to improve the effectiveness and generalization of the approach. To further enhance the prediction performance, we propose an ensemble technique that combines multiple base models toward the final decision. In addition, a semi-supervised approach using a Logistic Regressor to combine the base models' outputs is proposed. The proposed methodology is applied to the Skoltech Anomaly Benchmark (SKAB) dataset, which contains time series data related to the flow of water in a closed circuit, and the experimental results show that the proposed ensemble technique outperforms the basic algorithms. More specifically, the performance improvement in terms of anomaly detection accuracy reaches 2% for the unsupervised and at least 10% for the semi-supervised models.
</details>
<details>
<summary>摘要</summary>
异常检测在多变量时间序列中是许多领域的主要问题。由于异常事件罕见发生，因此将异常检测作为分类算法解决的问题是一项挑战。基于深度神经网络的方法，如LSTM、Autoencoder和Convolutional Autoencoder等，在这种不均衡数据中表现了积极的结果。然而，在多变量时间序列中，异常可能来自一个小集合特征。为了提高基本模型的性能，我们提议一种特征袋装技术，该技术仅考虑一部分特征，并应用基于主成分分析（PCA）计算的嵌入式旋转变换来提高效果和泛化性。此外，我们还提议一种 ensemble 技术，该技术将多个基本模型的输出合并到最终决策中。此外，我们还提出了一种半监督方法，该方法使用Logistic Regressor将基本模型的输出合并到最终决策中。我们对 Skoltech Anomaly Benchmark（SKAB）数据集进行实验，该数据集包含关于水流在封闭环流中的时间序列数据，实验结果表明，我们的ensemble技术在异常检测精度方面比基本算法提高了2%（非监督）和至少10%（半监督）。
</details></li>
</ul>
<hr>
<h2 id="FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire"><a href="#FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire" class="headerlink" title="FireFly A Synthetic Dataset for Ember Detection in Wildfire"></a>FireFly A Synthetic Dataset for Ember Detection in Wildfire</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03164">http://arxiv.org/abs/2308.03164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ergowho/firefly2.0">https://github.com/ergowho/firefly2.0</a></li>
<li>paper_authors: Yue Hu, Xinan Ye, Yifei Liu, Souvik Kundu, Gourav Datta, Srikar Mutnuri, Namo Asavisanu, Nora Ayanian, Konstantinos Psounis, Peter Beerel</li>
<li>for: 本研究旨在提供一个用于烟火检测的人工数据集，以替代当前烟火特有训练资源的缺乏。</li>
<li>methods: 本研究使用Unreal Engine 4 (UE4)创建了一个名为”FireFly”的人工数据集，并提供了一个自动生成标注数据集的工具，以实现数据多样性和用户需求的定制。</li>
<li>results: 对于四种流行的物体检测模型，使用FireFly数据集进行评估，得到了8.57%的提升 Mean Average Precision (mAP) 在实际野外烟火场景中。<details>
<summary>Abstract</summary>
This paper presents "FireFly", a synthetic dataset for ember detection created using Unreal Engine 4 (UE4), designed to overcome the current lack of ember-specific training resources. To create the dataset, we present a tool that allows the automated generation of the synthetic labeled dataset with adjustable parameters, enabling data diversity from various environmental conditions, making the dataset both diverse and customizable based on user requirements. We generated a total of 19,273 frames that have been used to evaluate FireFly on four popular object detection models. Further to minimize human intervention, we leveraged a trained model to create a semi-automatic labeling process for real-life ember frames. Moreover, we demonstrated an up to 8.57% improvement in mean Average Precision (mAP) in real-world wildfire scenarios compared to models trained exclusively on a small real dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/08/07/cs.LG_2023_08_07/" data-id="cllshr36x0053o988hl3s6kia" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/07/cs.SD_2023_08_07/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-07 123:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/07/eess.IV_2023_08_07/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">eess.IV - 2023-08-07 17:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
