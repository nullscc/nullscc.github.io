
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-15 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07947 repo_url: None paper_authors: Michaela Taylor-Willia">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-15 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/15/eess.IV_2023_08_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07947 repo_url: None paper_authors: Michaela Taylor-Willia">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-14T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:31.037Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/eess.IV_2023_08_15/" class="article-date">
  <time datetime="2023-08-14T16:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-15 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract"><a href="#Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract" class="headerlink" title="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract"></a>Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07947">http://arxiv.org/abs/2308.07947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaela Taylor-Williams, Ran Tao, Travis W Sawyer, Dale J Waterhouse, Jonghee Yoon, Sarah E Bohndiek<br>for: This paper aims to improve the detection of colour differences in the gastrointestinal tract during white light endoscopy (WLE) for early cancer detection.methods: The authors propose using custom multispectral filter arrays (MSFAs) in an endoscopic chip-on-tip configuration to target alternative colours for imaging and improve contrast. They use an open-source toolbox, Opti-MSFA, to optimize the design of MSFAs for early cancer detection in the gastrointestinal tract.results: The authors found that the MSFA designs have high classification accuracies, suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance. The optimal MSFA configurations can achieve similar classification accuracies as the full spectral data in a simpler hardware implementation.<details>
<summary>Abstract</summary>
Colour differences between healthy and diseased tissue in the gastrointestinal tract are detected visually by clinicians during white light endoscopy (WLE); however, the earliest signs of disease are often just a slightly different shade of pink compared to healthy tissue. Here, we propose to target alternative colours for imaging to improve contrast using custom multispectral filter arrays (MSFAs) that could be deployed in an endoscopic chip-on-tip configuration. Using an open-source toolbox, Opti-MSFA, we examined the optimal design of MSFAs for early cancer detection in the gastrointestinal tract. The toolbox was first extended to use additional classification models (k-Nearest Neighbour, Support Vector Machine, and Spectral Angle Mapper). Using input spectral data from published clinical trials examining the oesophagus and colon, we optimised the design of MSFAs with 3 to 9 different bands. We examined the variation of the spectral and spatial classification accuracy as a function of number of bands. The MSFA designs have high classification accuracies, suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance. Optimal MSFA configurations can achieve similar classification accuracies as the full spectral data in an implementation that could be realised in far simpler hardware. The reduced number of spectral bands could enable future deployment of multispectral imaging in an endoscopic chip-on-tip configuration.
</details>
<details>
<summary>摘要</summary>
医生在白光终端 scopes 中可以通过视觉检测，健康和疾病组织之间的颜色差异。然而，初期疾病往往只是与健康组织颜色有些微差异。我们提议使用自定义多спектルFilter array (MSFA) 来提高冲击。我们使用了开源工具箱 Opti-MSFA，对多种分类模型（k-最近邻居、支持向量机和spectral angle mapper）进行了扩展。使用来自已发表临床试验的输入光谱数据，我们优化了MSFA的设计，并考虑了3-9个频谱带。我们研究了随着带数的变化，多спектル和空间分类精度的变化。我们发现，MSFA的设计具有高分类精度，表明将来在Routine检查和监测中实施可能会提高肠道疾病的早期检测。最佳MSFA配置可以实现与全光谱数据相同的分类精度，但具有更少的spectral带。这可能将未来在检查终端中实现多спектル成像。
</details></li>
</ul>
<hr>
<h2 id="DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation"><a href="#DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation" class="headerlink" title="DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation"></a>DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07946">http://arxiv.org/abs/2308.07946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juntong Fan, Tieyong Zeng, Dayang Wang<br>for:* This paper is written for the task of colon polyp segmentation in medical images.methods:* The proposed method, called DSFNet, uses a U-shaped network architecture that combines the advantages of Dual-GCN and self-attention mechanisms.* The method includes a feature enhancement block module based on Dual-GCN, a stand-alone self-attention module, and a Fast Normalized Fusion method with trainable weights.results:* The proposed model outperforms other state-of-the-art models on two public datasets, Endoscene and Kvasir-SEG, in terms of Dice, MAE, and IoU.* Ablation studies demonstrate the efficacy and effectiveness of each module in the proposed method.* The proposed method has great clinical significance for colon polyp segmentation in medical images.Here is the simplified Chinese text for the three key information points:for:* 这篇论文是为了医学图像中的colon polyp segmentation任务而写的。methods:* 提议的方法，即DSFNet，使用了U型网络架构，combines了 dual-GCN和自注意机制的优点。* 方法包括了Feature Enhancement Block模块、stand-alone自注意模块和 Fast Normalized Fusion方法。results:* 提议的模型在两个公共数据集上，包括Endoscene和Kvasir-SEG，与其他状态对比较模型的Dice、MAE和IoU指标上表现出色。* 简洗研究表明每个模块的有效性和效果。* 提议的方法在医学图像中的colon polyp segmentation任务中具有丰富的临床意义。<details>
<summary>Abstract</summary>
In the past few decades, deep learning technology has been widely used in medical image segmentation and has made significant breakthroughs in the fields of liver and liver tumor segmentation, brain and brain tumor segmentation, video disc segmentation, heart image segmentation, and so on. However, the segmentation of polyps is still a challenging task since the surface of the polyps is flat and the color is very similar to that of surrounding tissues. Thus, It leads to the problems of the unclear boundary between polyps and surrounding mucosa, local overexposure, and bright spot reflection. To counter this problem, this paper presents a novel U-shaped network, namely DSFNet, which effectively combines the advantages of Dual-GCN and self-attention mechanisms. First, we introduce a feature enhancement block module based on Dual-GCN module as an attention mechanism to enhance the feature extraction of local spatial and structural information with fine granularity. Second, the stand-alone self-attention module is designed to enhance the integration ability of the decoding stage model to global information. Finally, the Fast Normalized Fusion method with trainable weights is used to efficiently fuse the corresponding three feature graphs in encoding, bottleneck, and decoding blocks, thus promoting information transmission and reducing the semantic gap between encoder and decoder. Our model is tested on two public datasets including Endoscene and Kvasir-SEG and compared with other state-of-the-art models. Experimental results show that the proposed model surpasses other competitors in many indicators, such as Dice, MAE, and IoU. In the meantime, ablation studies are also conducted to verify the efficacy and effectiveness of each module. Qualitative and quantitative analysis indicates that the proposed model has great clinical significance.
</details>
<details>
<summary>摘要</summary>
在过去几十年，深度学习技术在医学影像分割领域得到广泛应用，并在肝脏和肝肿瘤分割、脑和脑肿瘤分割、视频碟分割、心脏影像分割等领域取得了 significiant 的突破。然而，肿瘤分割仍然是一项具有挑战性的任务，因为肿瘤表面平整，颜色与周围组织相似，导致分割边界不清晰、局部过曝和耀光反射等问题。为解决这些问题，本文提出了一种新的U型网络模型，即DSFNet，该模型有效地结合了 dual-GCN 模块和自注意机制。首先，我们引入了基于 dual-GCN 模块的特征增强块模块，以增强本地空间和结构信息的特征提取。其次，我们设计了独立的自注意模块，以提高解码阶段模型的全局信息整合能力。最后，我们使用可训练权重的快速归一化方法，以效率地融合相应的三个特征图，从而促进信息传递和减少编码器和解码器之间的 semantic gap。我们的模型在两个公共数据集上进行测试，包括 Endoscene 和 Kvasir-SEG，并与其他当前领先模型进行比较。实验结果显示，提出的模型在多个指标上超越了其他竞争对手，如 Dice、MAE 和 IoU。同时，我们还进行了归并分析，以验证模块的有效性和效果。Qualitative 和量化分析表明，我们的模型在临床上具有重要的意义。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease"></a>An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07778">http://arxiv.org/abs/2308.07778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Kang, Bo Li, Janne M. Papma, Lize C. Jiskoot, Peter Paul De Deyn, Geert Jan Biessels, Jurgen A. H. R. Claassen, Huub A. M. Middelkoop, Wiesje M. van der Flier, Inez H. G. B. Ramakers, Stefan Klein, Esther E. Bron</li>
<li>for: 预测阿尔茨染色体疾病 (AD) 的早期诊断。</li>
<li>methods: 使用可解释的搜索机器学习模型（EBM）和深度学习Feature抽取来结合高维成像数据。</li>
<li>results: 在ADNI数据集上实现了0.883的准确率和0.970的AUC，在一个外部测试集上实现了0.778的准确率和0.887的AUC。<details>
<summary>Abstract</summary>
Machine learning methods have shown large potential for the automatic early diagnosis of Alzheimer's Disease (AD). However, some machine learning methods based on imaging data have poor interpretability because it is usually unclear how they make their decisions. Explainable Boosting Machines (EBMs) are interpretable machine learning models based on the statistical framework of generalized additive modeling, but have so far only been used for tabular data. Therefore, we propose a framework that combines the strength of EBM with high-dimensional imaging data using deep learning-based feature extraction. The proposed framework is interpretable because it provides the importance of each feature. We validated the proposed framework on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, achieving accuracy of 0.883 and area-under-the-curve (AUC) of 0.970 on AD and control classification. Furthermore, we validated the proposed framework on an external testing set, achieving accuracy of 0.778 and AUC of 0.887 on AD and subjective cognitive decline (SCD) classification. The proposed framework significantly outperformed an EBM model using volume biomarkers instead of deep learning-based features, as well as an end-to-end convolutional neural network (CNN) with optimized architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression"><a href="#Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression" class="headerlink" title="Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression"></a>Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07733">http://arxiv.org/abs/2308.07733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llvy21/duic">https://github.com/llvy21/duic</a></li>
<li>paper_authors: Yue Lv, Jinxi Xiang, Jun Zhang, Wenming Yang, Xiao Han, Wei Yang</li>
<li>for: 提高图像压缩的环境适应性和Rate-Distortion性能</li>
<li>methods: 使用低级matrix decomposition更新客户端解码器的适应参数，并通过动态网格决定哪些解码层需要适应。</li>
<li>results: 对于不同的图像数据集，提高了环境适应性和Rate-Distortion性能，相比非适应方法的平均BD-rate改善约19%，并且比最先进的实例适应方法提高了约5%的BD-rate。<details>
<summary>Abstract</summary>
The latest advancements in neural image compression show great potential in surpassing the rate-distortion performance of conventional standard codecs. Nevertheless, there exists an indelible domain gap between the datasets utilized for training (i.e., natural images) and those utilized for inference (e.g., artistic images). Our proposal involves a low-rank adaptation approach aimed at addressing the rate-distortion drop observed in out-of-domain datasets. Specifically, we perform low-rank matrix decomposition to update certain adaptation parameters of the client's decoder. These updated parameters, along with image latents, are encoded into a bitstream and transmitted to the decoder in practical scenarios. Due to the low-rank constraint imposed on the adaptation parameters, the resulting bit rate overhead is small. Furthermore, the bit rate allocation of low-rank adaptation is \emph{non-trivial}, considering the diverse inputs require varying adaptation bitstreams. We thus introduce a dynamic gating network on top of the low-rank adaptation method, in order to decide which decoder layer should employ adaptation. The dynamic adaptation network is optimized end-to-end using rate-distortion loss. Our proposed method exhibits universality across diverse image datasets. Extensive results demonstrate that this paradigm significantly mitigates the domain gap, surpassing non-adaptive methods with an average BD-rate improvement of approximately $19\%$ across out-of-domain images. Furthermore, it outperforms the most advanced instance adaptive methods by roughly $5\%$ BD-rate. Ablation studies confirm our method's ability to universally enhance various image compression architectures.
</details>
<details>
<summary>摘要</summary>
最新的神经网络图像压缩技术显示出了非常出色的潜在性，可能超越传统标准编码器的环境-质量评估。然而，存在一个不可缺的领域差异（domain gap），其中训练集（natural images）和推理集（artistic images）之间的差异无法被忽略。我们的提议是通过对客户端decoder的一些适应参数进行低级matrix decomposition来解决这种环境差异。Specifically，我们在实际场景中对适应参数进行编码，并将图像latent与适应参数一起编码为bitstream。由于我们对适应参数受到了低级约束，因此bit rate overhead很小。此外，适应参数的bit rate分配是非常复杂的，因为不同的输入需要不同的适应bitstream。我们因此引入了一个动态阀网络，以控制哪些decoder层应该使用适应。这个动态阀网络通过练习环境-质量损失来优化。我们的提议显示了对多种图像压缩架构的通用性。广泛的结果表明，我们的方法可以有效地 mitigate the domain gap，与非适应方法相比，平均BD-rate提高约19%，而与最先进的实例适应方法相比，BD-rate提高约5%。ablation study表明，我们的方法可以通过不同的图像压缩架构进行加持。
</details></li>
</ul>
<hr>
<h2 id="A-deep-deformable-residual-learning-network-for-SAR-images-segmentation"><a href="#A-deep-deformable-residual-learning-network-for-SAR-images-segmentation" class="headerlink" title="A deep deformable residual learning network for SAR images segmentation"></a>A deep deformable residual learning network for SAR images segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07627">http://arxiv.org/abs/2308.07627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Yulin Huang, Jianyu Yang</li>
<li>for: 这个论文是用于提高Synthetic Aperture Radar（SAR）图像中的自动目标分类，并且使用了深度学习网络。</li>
<li>methods: 本论文使用了深度弹性残差学习网络，包括弹性残差层和残差学习层，以提取和保留目标的几何信息。</li>
<li>results: 根据MSTAR数据集的实验结果显示，提案的网络可以实现高精度的目标分类。<details>
<summary>Abstract</summary>
Reliable automatic target segmentation in Synthetic Aperture Radar (SAR) imagery has played an important role in the SAR fields. Different from the traditional methods, Spectral Residual (SR) and CFAR detector, with the recent adavance in machine learning theory, there has emerged a novel method for SAR target segmentation, based on the deep learning networks. In this paper, we proposed a deep deformable residual learning network for target segmentation that attempts to preserve the precise contour of the target. For this, the deformable convolutional layers and residual learning block are applied, which could extract and preserve the geometric information of the targets as much as possible. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) data set, experimental results have shown the superiority of the proposed network for the precise targets segmentation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNReliable automatic target segmentation in Synthetic Aperture Radar (SAR) imagery has played an important role in the SAR fields. Different from the traditional methods, Spectral Residual (SR) and CFAR detector, with the recent advances in machine learning theory, there has emerged a novel method for SAR target segmentation, based on the deep learning networks. In this paper, we proposed a deep deformable residual learning network for target segmentation that attempts to preserve the precise contour of the target. For this, the deformable convolutional layers and residual learning block are applied, which could extract and preserve the geometric information of the targets as much as possible. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) data set, experimental results have shown the superiority of the proposed network for the precise targets segmentation.Note: "zh-CN" is the Simplified Chinese language code.
</details></li>
</ul>
<hr>
<h2 id="GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis"><a href="#GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis" class="headerlink" title="GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis"></a>GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07611">http://arxiv.org/abs/2308.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Jui Lu, Benjamin Odry, Muhamed Barakovic, Matthias Weigel, Robin Sandkühler, Reza Rahmanzadeh, Xinjie Chen, Mario Ocampo-Pineda, Jens Kuhle, Ludwig Kappos, Philippe Cattin, Cristina Granziera<br>for:This paper aims to develop a novel approach called GAMER-MRIL to classify multiple sclerosis (MS) patients with severe disability using whole-brain quantitative MRI (qMRI) and an interpretability method.methods:The approach uses a gated-attention-based convolutional neural network (CNN) to select patch-based qMRI important for a given task&#x2F;question, and modifies a structure-aware interpretability method, Layer-wise Relevance Propagation (LRP), to incorporate qMRI.results:The approach achieved an AUC of 0.885 and identified the corticospinal tract as a relevant region for disability, with significant correlations between average qT1 and NDI and patients’ disability scores.Here are the results in Simplified Chinese text:for:这 paper 的目的是开发一种新的方法，称为 GAMER-MRIL，用于 классификация多发性纤维病 (MS) 患者的严重残疾使用整个脑quantitative MRI (qMRI) 和一种可解释方法。methods:这种方法使用一种闭合注意力基于 convolutional neural network (CNN) 来选择 patch-based qMRI 对于给定任务&#x2F;问题的重要部分，并将structure-aware interpretability method，层 wise Relevance Propagation (LRP)，与 qMRI 结合使用。results:这种方法实现了 AUC 为 0.885，并identified  corticospinal tract 作为残疾相关的区域，与患者的残疾分数相关性 Statistical significant (ρ&#x3D;-0.37和0.44)。<details>
<summary>Abstract</summary>
Objective: Identifying disability-related brain changes is important for multiple sclerosis (MS) patients. Currently, there is no clear understanding about which pathological features drive disability in single MS patients. In this work, we propose a novel comprehensive approach, GAMER-MRIL, leveraging whole-brain quantitative MRI (qMRI), convolutional neural network (CNN), and an interpretability method from classifying MS patients with severe disability to investigating relevant pathological brain changes. Methods: One-hundred-sixty-six MS patients underwent 3T MRI acquisitions. qMRI informative of microstructural brain properties was reconstructed, including quantitative T1 (qT1), myelin water fraction (MWF), and neurite density index (NDI). To fully utilize the qMRI, GAMER-MRIL extended a gated-attention-based CNN (GAMER-MRI), which was developed to select patch-based qMRI important for a given task/question, to the whole-brain image. To find out disability-related brain regions, GAMER-MRIL modified a structure-aware interpretability method, Layer-wise Relevance Propagation (LRP), to incorporate qMRI. Results: The test performance was AUC=0.885. qT1 was the most sensitive measure related to disability, followed by NDI. The proposed LRP approach obtained more specifically relevant regions than other interpretability methods, including the saliency map, the integrated gradients, and the original LRP. The relevant regions included the corticospinal tract, where average qT1 and NDI significantly correlated with patients' disability scores ($\rho$=-0.37 and 0.44). Conclusion: These results demonstrated that GAMER-MRIL can classify patients with severe disability using qMRI and subsequently identify brain regions potentially important to the integrity of the mobile function. Significance: GAMER-MRIL holds promise for developing biomarkers and increasing clinicians' trust in NN.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation"><a href="#Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation" class="headerlink" title="Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation"></a>Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07506">http://arxiv.org/abs/2308.07506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadie1/medseguq">https://github.com/jadie1/medseguq</a></li>
<li>paper_authors: Jadie Adams, Shireen Y. Elhabian</li>
<li>for: 这种研究旨在评估医疗图像分析中的epistemic uncertainty量化方法，以帮助提高诊断和治疗规划的可靠性和Robustness。</li>
<li>methods: 本研究使用了多种epistemic uncertainty量化方法，包括Bayesian neural networks、 Monte Carlo dropout、和 Deep Ensembles等，并进行了比较性研究以评估每种方法的准确率、uncertainty calibration和可扩展性。</li>
<li>results: 研究发现，Bayesian neural networks和Deep Ensembles方法在准确率和uncertainty calibration方面表现最佳，而Monte Carlo dropout方法在可扩展性方面表现最好。此外，研究还发现了这些方法在out-of-distribution detection方面的能力不尽相同。<details>
<summary>Abstract</summary>
Deep learning based methods for automatic organ segmentation have shown promise in aiding diagnosis and treatment planning. However, quantifying and understanding the uncertainty associated with model predictions is crucial in critical clinical applications. While many techniques have been proposed for epistemic or model-based uncertainty estimation, it is unclear which method is preferred in the medical image analysis setting. This paper presents a comprehensive benchmarking study that evaluates epistemic uncertainty quantification methods in organ segmentation in terms of accuracy, uncertainty calibration, and scalability. We provide a comprehensive discussion of the strengths, weaknesses, and out-of-distribution detection capabilities of each method as well as recommendations for future improvements. These findings contribute to the development of reliable and robust models that yield accurate segmentations while effectively quantifying epistemic uncertainty.
</details>
<details>
<summary>摘要</summary>
深度学习基本的方法可以帮助自动 segmentation 获得了诊断和治疗规划的潜在优势。但是，量化和理解模型预测结果中的不确定性是在敏感医学应用中至关重要。虽然许多技术已经提出了 epistemic 或模型基于的不确定性估计方法，但是尚未确定哪种方法在医学图像分析 Setting 中是首选的。这篇文章提供了一项全面的比较研究，评估了 organ segmentation 中 epistemic 不确定性估计方法的准确性、不确定性满足度和可扩展性。我们提供了每种方法的优缺点、外部数据检测能力和未来改进建议。这些发现将为建立可靠和可靠的模型做出贡献，以便在获得准确分 segmentation 的同时，有效地量化 epistemic 不确定性。
</details></li>
</ul>
<hr>
<h2 id="Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions"><a href="#Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions" class="headerlink" title="Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions"></a>Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07495">http://arxiv.org/abs/2308.07495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Sun, Chunyan Wang<br>for: 这个系统用于检测Brain Tumor在3D MRI脑部扫描图像中的FLAIR模式下。methods: 该系统使用了2个功能：（a）预测灰度和位坐标分布的粒子点云，以及（b）生成粒子点云掩码。为了方便3D数据分析和处理，我们引入了2D histogram表示法，这种表示法包括脑部结构的灰度分布和像素位坐标分布。results: 在测试了more than one thousand patient cases的数据集上，该系统可以准确地预测2D histogram，并且可以在低计算成本下实现高度相似的结果，与现有的CNN系统相当。<details>
<summary>Abstract</summary>
In this paper, we propose a system to detect brain tumor in 3D MRI brain scans of Flair modality. It performs 2 functions: (a) predicting gray-level and locational distributions of the pixels in the tumor regions and (b) generating tumor mask in pixel-wise precision. To facilitate 3D data analysis and processing, we introduced a 2D histogram presentation that comprehends the gray-level distribution and pixel-location distribution of a 3D object. In the proposed system, particular 2D histograms, in which tumor-related feature data get concentrated, are established by exploiting the left-right asymmetry of a brain structure. A modulation function is generated from the input data of each patient case and applied to the 2D histograms to attenuate the element irrelevant to the tumor regions. The prediction of the tumor pixel distribution is done in 3 steps, on the axial, coronal and sagittal slice series, respectively. In each step, the prediction result helps to identify/remove tumor-free slices, increasing the tumor information density in the remaining data to be applied to the next step. After the 3-step removal, the 3D input is reduced to a minimum bounding box of the tumor region. It is used to finalize the prediction and then transformed into a 3D tumor mask, by means of gray level thresholding and low-pass-based morphological operations. The final prediction result is used to determine the critical threshold. The proposed system has been tested extensively with the data of more than one thousand patient cases in the datasets of BraTS 2018~21. The test results demonstrate that the predicted 2D histograms have a high degree of similarity with the true ones. The system delivers also very good tumor detection results, comparable to those of state-of-the-art CNN systems with mono-modality inputs, which is achieved at an extremely low computation cost and no need for training.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种系统，用于检测Brain Tumor在3D MRI脑部扫描图像中的FLAIR模式下。该系统完成了两个函数：（a）预测肿瘤区域像素的灰度和位置分布，以及（b）生成肿瘤面积的精确推测。为了便于3D数据分析和处理，我们引入了一种2D histogram展示方式，该方式涵盖肿瘤区域的灰度分布和像素位置分布。在我们提出的系统中，特定的2D histogram被设置，以便在肿瘤相关特征数据中强调特定的特征。在输入数据中，每个病例的模ulation函数被应用于2D histogram，以减少不相关于肿瘤区域的元素。肿瘤像素分布的预测被分为三步，在axial、coronal和sagittal扫描序列上进行，每步的预测结果帮助标识/移除不含肿瘤的扫描，从而增加留下的数据中肿瘤信息的浓度。在第三步后，输入数据被减少到最小 bounding box 的肿瘤区域。最后，使用灰度阈值化和低通滤波操作，将最终预测结果转换为3D肿瘤面积。测试结果表明，我们的预测结果与实际的2D histogram有高度的相似性。系统还可以实现非常好的肿瘤检测结果，与现有的CNN系统相当，但是具有非常低的计算成本和不需要训练。
</details></li>
</ul>
<hr>
<h2 id="Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis"><a href="#Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis" class="headerlink" title="Space Object Identification and Classification from Hyperspectral Material Analysis"></a>Space Object Identification and Classification from Hyperspectral Material Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07481">http://arxiv.org/abs/2308.07481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimiliano Vasile, Lewis Walker, Andrew Campbell, Simao Marto, Paul Murray, Stephen Marshall, Vasili Savitski</li>
<li>for: 这篇论文是为了提取外星物体干扰特征图像中的信息而设计的数据处理管道。</li>
<li>methods: 该论文使用了两种材料标识和分类技术：一种基于机器学习，另一种基于最小二乘匹配知 Spectra 库。</li>
<li>results: 论文将会展示一些初步的外星物体识别和分类结果。<details>
<summary>Abstract</summary>
This paper presents a data processing pipeline designed to extract information from the hyperspectral signature of unknown space objects. The methodology proposed in this paper determines the material composition of space objects from single pixel images. Two techniques are used for material identification and classification: one based on machine learning and the other based on a least square match with a library of known spectra. From this information, a supervised machine learning algorithm is used to classify the object into one of several categories based on the detection of materials on the object. The behaviour of the material classification methods is investigated under non-ideal circumstances, to determine the effect of weathered materials, and the behaviour when the training library is missing a material that is present in the object being observed. Finally the paper will present some preliminary results on the identification and classification of space objects.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hyperspectral signature" is translated as "多spectral签名" (duōspectral jiànmíng), which is a combination of "多spectral" (meaning "hyperspectral") and "签名" (meaning "signature").* "material composition" is translated as "物质组成" (wùzhì zhùxìn), which is a combination of "物质" (meaning "material") and "组成" (meaning "composition").* "machine learning" is translated as "机器学习" (jīzhì xuéxí), which is a direct translation of the English term.* "least square match" is translated as "最小平方匹配" (zuìxiǎo píngfāng pínghù), which is a direct translation of the English term.* "supervised machine learning algorithm" is translated as "指导式机器学习算法" (dìdǎo xìng jīzhì xuéxísuànfǎ), which is a direct translation of the English term.* "weathered materials" is translated as "天气损害的物质" (tiānqì jiānghài de wùzhì), which is a combination of "天气" (meaning "weather") and "损害" (meaning "damage").* "training library" is translated as "训练库" (xùnxíng kù), which is a direct translation of the English term.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression"><a href="#Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression" class="headerlink" title="Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression"></a>Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07477">http://arxiv.org/abs/2308.07477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antonbaumann/mimo-unet">https://github.com/antonbaumann/mimo-unet</a></li>
<li>paper_authors: Anton Baumann, Thomas Roßberg, Michael Schmitt</li>
<li>for: 这 paper 的目的是提高机器学习模型的可靠性和解释性，特别是在高度重要的实际应用场景中。</li>
<li>methods: 这 paper 使用了 Multiple-Input Multiple-Output (MIMO) 框架，利用深度神经网络的过参数化来进行像素级回归任务。它还引入了一种同步多Sub网络性能的新方法。</li>
<li>results: 对两个正交的数据集进行了广泛的评估，显示了与现有模型相当的准确率、在正常数据上更好的准确率报告、出入数据检测能力的robustness、参数大小和计算时间的显著改善。代码可以在 github.com&#x2F;antonbaumann&#x2F;MIMO-Unet 上获取。<details>
<summary>Abstract</summary>
Uncertainty estimation in machine learning is paramount for enhancing the reliability and interpretability of predictive models, especially in high-stakes real-world scenarios. Despite the availability of numerous methods, they often pose a trade-off between the quality of uncertainty estimation and computational efficiency. Addressing this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework -- an approach exploiting the overparameterization of deep neural networks -- for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. For that purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. Additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. Code available at github.com/antonbaumann/MIMO-Unet
</details>
<details>
<summary>摘要</summary>
machine learning中的不确定性估计是提高预测模型的可靠性和可读性的关键，特别在高度的实际应用场景中。 despite numerous methods 的可用性，它们经常存在质量的不确定性估计和计算效率之间的负担。 to address this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework -- an approach exploiting the overparameterization of deep neural networks -- for pixel-wise regression tasks. our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. for this purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. code available at github.com/antonbaumann/MIMO-Unet.Here's the word-for-word translation of the text into Simplified Chinese:机器学习中的不确定性估计是提高预测模型的可靠性和可读性的关键，特别在高度的实际应用场景中。 despite numerous methods 的可用性，它们经常存在质量的不确定性估计和计算效率之间的负担。为了解决这个挑战，我们提出了一种基于多输入多输出（MIMO）框架的修改---一种利用深度神经网络的过参数化来实现像素级回归任务。我们的 MIMO 变体将该approach扩展到更广泛的计算机视ión Domains。为此，我们采用了修改的 U-Net 架构，在单个模型中培训多个子网络，利用深度神经网络的过参数化。此外，我们还提出了一种新的同步子网络性能的方法。我们对 MIMO U-Net 在两个orthogonal dataset上进行了全面的评估，结果表明其与现有模型相比有相似的准确性，在适用范围内的数据上有更好的准确性calibration，在尝试数据上有更好的robustness，并且具有较小的参数大小和计算时间。代码可以在github.com/antonbaumann/MIMO-Unet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation"><a href="#Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation" class="headerlink" title="Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation"></a>Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07251">http://arxiv.org/abs/2308.07251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/mdunet">https://github.com/liamchalcroft/mdunet</a></li>
<li>paper_authors: Liam Chalcroft, Ruben Lourenço Pereira, Mikael Brudfors, Andrew S. Kayser, Mark D’Esposito, Cathy J. Price, Ioannis Pappas, John Ashburner</li>
<li>for: 这个研究旨在提出一种能够实现高性能和优化参数的混合型神经网络模型，用于三维脑膜病变分类。</li>
<li>methods: 本研究使用了一种将transformer块与传统的对称块结合起来的all-convolutional transformer block，并将其应用于U-Net架构中。</li>
<li>results: 研究结果显示，我们的模型在三维脑膜病变分类任务中具有与现有最佳方法相匹配的性能，并且具有较好的优化参数和传播传统神经网络的优点。<details>
<summary>Abstract</summary>
Vision transformers are effective deep learning models for vision tasks, including medical image segmentation. However, they lack efficiency and translational invariance, unlike convolutional neural networks (CNNs). To model long-range interactions in 3D brain lesion segmentation, we propose an all-convolutional transformer block variant of the U-Net architecture. We demonstrate that our model provides the greatest compromise in three factors: performance competitive with the state-of-the-art; parameter efficiency of a CNN; and the favourable inductive biases of a transformer. Our public implementation is available at https://github.com/liamchalcroft/MDUNet .
</details>
<details>
<summary>摘要</summary>
幻视转换器是深度学习模型，用于视觉任务，如医疗图像分割。然而，它们缺乏效率和翻译不变性，与卷积神经网络（CNNs）不同。为模elling长距离交互 в 3D 脑损坏分割，我们提议一种具有alls-convolutional transformer块的U-Net架构变体。我们示示了我们的模型在三个因素中具有最大的妥协：性能与状态艺术竞争力相当; 参数效率与CNN相同; 以及 transformer 所带有的有利 inductive bias。我们的公共实现可以在 <https://github.com/liamchalcroft/MDUNet> 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/eess.IV_2023_08_15/" data-id="clluro5mb00e0q9884pt18f6o" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/15/eess.AS_2023_08_15/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-08-15 22:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/14/cs.LG_2023_08_14/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.LG - 2023-08-14 18:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
