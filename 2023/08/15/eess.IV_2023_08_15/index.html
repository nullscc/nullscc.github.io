
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-15 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07947 repo_url: None paper_authors: Michaela Taylor-Willia">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-15">
<meta property="og:url" content="https://nullscc.github.io/2023/08/15/eess.IV_2023_08_15/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.07947 repo_url: None paper_authors: Michaela Taylor-Willia">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-15T09:00:00.000Z">
<meta property="article:modified_time" content="2023-09-23T08:49:38.342Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/eess.IV_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T09:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-15
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract"><a href="#Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract" class="headerlink" title="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract"></a>Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07947">http://arxiv.org/abs/2308.07947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaela Taylor-Williams, Ran Tao, Travis W Sawyer, Dale J Waterhouse, Jonghee Yoon, Sarah E Bohndiek</li>
<li>for: 检测肠胃肉体中疾病的颜色差异，以提高疾病检测的准确性。</li>
<li>methods: 使用自定义多spectral filter arrays (MSFAs)，并使用开源工具箱Opti-MSFA进行优化设计。</li>
<li>results: 结果显示，MSFA设计具有高分类精度，表明将在未来实施在检查器硬件中可能有助于提高肠胃肉体检测的早期检测。<details>
<summary>Abstract</summary>
Colour differences between healthy and diseased tissue in the gastrointestinal tract are detected visually by clinicians during white light endoscopy (WLE); however, the earliest signs of disease are often just a slightly different shade of pink compared to healthy tissue. Here, we propose to target alternative colours for imaging to improve contrast using custom multispectral filter arrays (MSFAs) that could be deployed in an endoscopic chip-on-tip configuration. Using an open-source toolbox, Opti-MSFA, we examined the optimal design of MSFAs for early cancer detection in the gastrointestinal tract. The toolbox was first extended to use additional classification models (k-Nearest Neighbour, Support Vector Machine, and Spectral Angle Mapper). Using input spectral data from published clinical trials examining the oesophagus and colon, we optimised the design of MSFAs with 3 to 9 different bands. We examined the variation of the spectral and spatial classification accuracy as a function of number of bands. The MSFA designs have high classification accuracies, suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance. Optimal MSFA configurations can achieve similar classification accuracies as the full spectral data in an implementation that could be realised in far simpler hardware. The reduced number of spectral bands could enable future deployment of multispectral imaging in an endoscopic chip-on-tip configuration.
</details>
<details>
<summary>摘要</summary>
医生在白光endooscopy（WLE）中可以通过视觉检测肠道内健康和疾病组织的颜色差异。然而，疾病的早期征象通常只是健康组织的微妙变化。我们提议使用自定义多spectral filter array（MSFA）来提高对比度。我们使用了一个开源工具箱，Opti-MSFA，来调查最佳MSFA的设计。我们首先扩展了工具箱，使其支持更多的分类模型（k-最近邻居、支持向量机和spectral angle mapper）。使用来自已发布临床试验的迷你镜诊断数据，我们优化了MSFA的3到9个频谱带的设计。我们分析了频谱和空间分类精度的变化与频谱带数的关系。我们发现，最佳MSFA配置具有高分类精度， suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance。最佳MSFA配置可以实现与全spectral数据相同的分类精度，但具有更少的频谱带数，这可能使得未来在endooscopy中实现多spectral imaging的方式更加简单。
</details></li>
</ul>
<hr>
<h2 id="DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation"><a href="#DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation" class="headerlink" title="DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation"></a>DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07946">http://arxiv.org/abs/2308.07946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juntong Fan, Tieyong Zeng, Dayang Wang<br>for:This paper aims to address the challenging task of polyp segmentation in colonoscopy images using a novel U-shaped network called DSFNet.methods:The proposed DSFNet combines the advantages of Dual-GCN and self-attention mechanisms, including a feature enhancement block module and a stand-alone self-attention module, as well as a Fast Normalized Fusion method for efficient feature fusion.results:The proposed model surpasses other state-of-the-art models in terms of Dice, MAE, and IoU on two public datasets (Endoscene and Kvasir-SEG), and ablation studies verify the efficacy and effectiveness of each module. The proposed model has great clinical significance for polyp segmentation in colonoscopy images.Here is the Chinese version of the three key points:for:本研究使用一种新型的U型网络，即DSFNet，解决了医学内视镜中肠道肿瘤分 segmentation的挑战性问题。methods:提议的DSFNet结合了 dual-GCN和自注意机制的优点，包括特征增强块模块和独立自注意模块，以及一种高效的快Normalized Fusion方法。results:实验结果表明，提议的模型在两个公共数据集（Endoscene和Kvasir-SEG）上比其他状态对比模型在多个指标（Dice、MAE和IoU）上表现出优异，并进行了ablation研究来验证每个模块的有效性和效iveness。结果表明，提议的模型在医学中具有大的临床意义。<details>
<summary>Abstract</summary>
In the past few decades, deep learning technology has been widely used in medical image segmentation and has made significant breakthroughs in the fields of liver and liver tumor segmentation, brain and brain tumor segmentation, video disc segmentation, heart image segmentation, and so on. However, the segmentation of polyps is still a challenging task since the surface of the polyps is flat and the color is very similar to that of surrounding tissues. Thus, It leads to the problems of the unclear boundary between polyps and surrounding mucosa, local overexposure, and bright spot reflection. To counter this problem, this paper presents a novel U-shaped network, namely DSFNet, which effectively combines the advantages of Dual-GCN and self-attention mechanisms. First, we introduce a feature enhancement block module based on Dual-GCN module as an attention mechanism to enhance the feature extraction of local spatial and structural information with fine granularity. Second, the stand-alone self-attention module is designed to enhance the integration ability of the decoding stage model to global information. Finally, the Fast Normalized Fusion method with trainable weights is used to efficiently fuse the corresponding three feature graphs in encoding, bottleneck, and decoding blocks, thus promoting information transmission and reducing the semantic gap between encoder and decoder. Our model is tested on two public datasets including Endoscene and Kvasir-SEG and compared with other state-of-the-art models. Experimental results show that the proposed model surpasses other competitors in many indicators, such as Dice, MAE, and IoU. In the meantime, ablation studies are also conducted to verify the efficacy and effectiveness of each module. Qualitative and quantitative analysis indicates that the proposed model has great clinical significance.
</details>
<details>
<summary>摘要</summary>
在过去几十年中，深度学习技术在医学影像分割领域得到广泛应用，并在肝脏和肝癌分割、脑和脑癌分割、视频碟分割、心脏影像分割等领域取得了显著突破。然而，肿瘤分割仍然是一项挑战性的任务，因为肿瘤表面平滑，颜色与周围组织相似，导致边界不清晰、局部过曝光和光泽反射等问题。为解决这些问题，本文提出了一种新的U型网络，即DSFNet，该网络效果地结合了DUAL-GCN模块和自注意机制。首先，我们引入了基于DUAL-GCN模块的特征增强块模块，以增强本地空间和结构信息的特征提取。其次，我们设计了独立的自注意模块，以提高解码阶段模型的全局信息集成能力。最后，我们使用可学习权重的快速 нормализа化融合方法，以有效地融合编码、瓶颈和解码块中的三个特征图，从而提高信息传递和减少编码器和解码器之间的semantic gap。我们的模型在Endoscene和Kvasir-SEG两个公共数据集上进行测试，与其他当前顶尖模型进行比较。实验结果表明，我们的模型在多个指标上超过其他竞争对手，包括 dice、MAE和IoU等指标。同时，我们还进行了ablation研究，以验证每个模块的有效性和效果。 qualitative和quantitative分析表明，我们的模型在临床上具有很大的价值。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease"></a>An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07778">http://arxiv.org/abs/2308.07778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Kang, Bo Li, Janne M. Papma, Lize C. Jiskoot, Peter Paul De Deyn, Geert Jan Biessels, Jurgen A. H. R. Claassen, Huub A. M. Middelkoop, Wiesje M. van der Flier, Inez H. G. B. Ramakers, Stefan Klein, Esther E. Bron</li>
<li>for: 预测阿尔ц海默病（AD）的早期诊断。</li>
<li>methods: combines Explainable Boosting Machines (EBM) with deep learning-based feature extraction，提供了每个特征的重要性。</li>
<li>results: 在Alzheimer’s Disease Neuroimaging Initiative（ADNI）数据集上 achieved accuracy of 0.883和area-under-the-curve（AUC）of 0.970 on AD和control分类，并在一个外部测试集上 achieved accuracy of 0.778和AUC of 0.887 on AD和主观认知下降（SCD）分类。<details>
<summary>Abstract</summary>
Machine learning methods have shown large potential for the automatic early diagnosis of Alzheimer's Disease (AD). However, some machine learning methods based on imaging data have poor interpretability because it is usually unclear how they make their decisions. Explainable Boosting Machines (EBMs) are interpretable machine learning models based on the statistical framework of generalized additive modeling, but have so far only been used for tabular data. Therefore, we propose a framework that combines the strength of EBM with high-dimensional imaging data using deep learning-based feature extraction. The proposed framework is interpretable because it provides the importance of each feature. We validated the proposed framework on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, achieving accuracy of 0.883 and area-under-the-curve (AUC) of 0.970 on AD and control classification. Furthermore, we validated the proposed framework on an external testing set, achieving accuracy of 0.778 and AUC of 0.887 on AD and subjective cognitive decline (SCD) classification. The proposed framework significantly outperformed an EBM model using volume biomarkers instead of deep learning-based features, as well as an end-to-end convolutional neural network (CNN) with optimized architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression"><a href="#Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression" class="headerlink" title="Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression"></a>Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07733">http://arxiv.org/abs/2308.07733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llvy21/duic">https://github.com/llvy21/duic</a></li>
<li>paper_authors: Yue Lv, Jinxi Xiang, Jun Zhang, Wenming Yang, Xiao Han, Wei Yang</li>
<li>for: 本研究旨在提高 neural image compression 的环境不同时的表现，即addressing the domain gap between training datasets (natural images) and inference datasets (e.g., artistic images).</li>
<li>methods: 我们提出了一种基于low-rank adaptation的方法，包括在客户端decoder中进行低级别矩阵分解，并将更新了适应参数传输到客户端。此外，我们还引入了一种动态阀网，以确定需要适应的层。</li>
<li>results: 我们的方法可以 universal across diverse image datasets，并且在out-of-domain图像上表现出较好的表现，与非适应方法相比，平均BD-rate提高约19%。此外，我们的方法还可以在不同的图像压缩架构上进行改进。<details>
<summary>Abstract</summary>
The latest advancements in neural image compression show great potential in surpassing the rate-distortion performance of conventional standard codecs. Nevertheless, there exists an indelible domain gap between the datasets utilized for training (i.e., natural images) and those utilized for inference (e.g., artistic images). Our proposal involves a low-rank adaptation approach aimed at addressing the rate-distortion drop observed in out-of-domain datasets. Specifically, we perform low-rank matrix decomposition to update certain adaptation parameters of the client's decoder. These updated parameters, along with image latents, are encoded into a bitstream and transmitted to the decoder in practical scenarios. Due to the low-rank constraint imposed on the adaptation parameters, the resulting bit rate overhead is small. Furthermore, the bit rate allocation of low-rank adaptation is \emph{non-trivial}, considering the diverse inputs require varying adaptation bitstreams. We thus introduce a dynamic gating network on top of the low-rank adaptation method, in order to decide which decoder layer should employ adaptation. The dynamic adaptation network is optimized end-to-end using rate-distortion loss. Our proposed method exhibits universality across diverse image datasets. Extensive results demonstrate that this paradigm significantly mitigates the domain gap, surpassing non-adaptive methods with an average BD-rate improvement of approximately $19\%$ across out-of-domain images. Furthermore, it outperforms the most advanced instance adaptive methods by roughly $5\%$ BD-rate. Ablation studies confirm our method's ability to universally enhance various image compression architectures.
</details>
<details>
<summary>摘要</summary>
最新的神经网络图像压缩技术具有可能超越传统标准编解码器的环境-质量表现的潜在力量。然而，存在一个不可缺少的领域差距（domain gap），即训练集（natural images）和推理集（e.g., artistic images）之间的差异。我们的提议是一种基于低级数的适应方法，用于Addressing the rate-distortion drop observed in out-of-domain datasets。具体来说，我们通过低级数矩阵分解更新客户端的解码器某些适应参数。这些更新后的参数，加上图像latent，被编码到一个bit流中并在实际应用场景中传输。由于低级数约束对适应参数的影响，bit流扩展的负担小。此外，低级数适应的bit流分配是非易的，需要根据各种输入的多样性进行变动的适应。我们因此引入了一种基于低级数适应的动态阻止网络，以确定哪些解码层应该使用适应。这个动态适应网络通过练习环境-质量损失来优化。我们的提议在多种图像压缩架构上 universality，并且在多种图像数据集上进行了广泛的测试。结果显示，这种方法可以有效 mitigate the domain gap，与非适应方法相比，平均BD-rate提高约19%，而与最先进的实例适应方法相比，BD-rate提高约5%。剖析研究证明了我们的方法可以通过universal enhancement来改善多种图像压缩架构。
</details></li>
</ul>
<hr>
<h2 id="A-deep-deformable-residual-learning-network-for-SAR-images-segmentation"><a href="#A-deep-deformable-residual-learning-network-for-SAR-images-segmentation" class="headerlink" title="A deep deformable residual learning network for SAR images segmentation"></a>A deep deformable residual learning network for SAR images segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07627">http://arxiv.org/abs/2308.07627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Yulin Huang, Jianyu Yang</li>
<li>for: 这篇论文是为了提出一个基于深度学习网络的新方法 для SAR 对象分类，以提高 SAR 对象分类的精度和速度。</li>
<li>methods: 本文使用的方法包括对于 SAR 图像进行深度学习网络的建立，并将对象分类问题转化为一个对于图像进行分类的问题。另外，本文还使用了扭转卷网络和复原学习块，以提高网络的准确性和稳定性。</li>
<li>results: 根据 MSTAR 资料集的实验结果显示，提出的深度对象分类网络能够实现高精度和高速度的 SAR 对象分类，并且比传统方法更加精确和可靠。<details>
<summary>Abstract</summary>
Reliable automatic target segmentation in Synthetic Aperture Radar (SAR) imagery has played an important role in the SAR fields. Different from the traditional methods, Spectral Residual (SR) and CFAR detector, with the recent adavance in machine learning theory, there has emerged a novel method for SAR target segmentation, based on the deep learning networks. In this paper, we proposed a deep deformable residual learning network for target segmentation that attempts to preserve the precise contour of the target. For this, the deformable convolutional layers and residual learning block are applied, which could extract and preserve the geometric information of the targets as much as possible. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) data set, experimental results have shown the superiority of the proposed network for the precise targets segmentation.
</details>
<details>
<summary>摘要</summary>
<<SYS>启用简化中文</SYS>静电Synthetic Aperture Radar（SAR）图像中的自动目标分割可靠性在SAR领域中发挥了重要作用。与传统方法不同，我们提出了基于深度学习网络的新方法，即吸引强度差（SR）和CFAR探测器。在这篇论文中，我们提出了一种深度变形剩余学习网络，用于目标分割，以保留目标精确的轮廓。为了实现这一目标，我们使用了变形卷积层和剩余学习块，以提取和保留目标的几何信息。基于Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集，我们的实验结果表明，提出的网络可以准确地分割精确目标。
</details></li>
</ul>
<hr>
<h2 id="GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis"><a href="#GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis" class="headerlink" title="GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis"></a>GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07611">http://arxiv.org/abs/2308.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Jui Lu, Benjamin Odry, Muhamed Barakovic, Matthias Weigel, Robin Sandkühler, Reza Rahmanzadeh, Xinjie Chen, Mario Ocampo-Pineda, Jens Kuhle, Ludwig Kappos, Philippe Cattin, Cristina Granziera</li>
<li>for: 这个研究的目的是为了确定多创性疾病（MS）患者的残障相关的脑部变化。</li>
<li>methods: 这个研究使用了全脑量化MRI（qMRI）、神经网络（CNN）和可解释方法，将MS患者分为严重残障和不严重残障两组。</li>
<li>results: 研究获得了0.885的测试效果，qT1是残障相关最敏感的测量指标，其次是neurite density index（NDI）。这个研究还发现了残障相关的脑部区域，包括 corticospinal tract，这些区域与患者的残障分数有 statistically significant 的相关性（ρ&#x3D;-0.37和0.44）。<details>
<summary>Abstract</summary>
Objective: Identifying disability-related brain changes is important for multiple sclerosis (MS) patients. Currently, there is no clear understanding about which pathological features drive disability in single MS patients. In this work, we propose a novel comprehensive approach, GAMER-MRIL, leveraging whole-brain quantitative MRI (qMRI), convolutional neural network (CNN), and an interpretability method from classifying MS patients with severe disability to investigating relevant pathological brain changes. Methods: One-hundred-sixty-six MS patients underwent 3T MRI acquisitions. qMRI informative of microstructural brain properties was reconstructed, including quantitative T1 (qT1), myelin water fraction (MWF), and neurite density index (NDI). To fully utilize the qMRI, GAMER-MRIL extended a gated-attention-based CNN (GAMER-MRI), which was developed to select patch-based qMRI important for a given task/question, to the whole-brain image. To find out disability-related brain regions, GAMER-MRIL modified a structure-aware interpretability method, Layer-wise Relevance Propagation (LRP), to incorporate qMRI. Results: The test performance was AUC=0.885. qT1 was the most sensitive measure related to disability, followed by NDI. The proposed LRP approach obtained more specifically relevant regions than other interpretability methods, including the saliency map, the integrated gradients, and the original LRP. The relevant regions included the corticospinal tract, where average qT1 and NDI significantly correlated with patients' disability scores ($\rho$=-0.37 and 0.44). Conclusion: These results demonstrated that GAMER-MRIL can classify patients with severe disability using qMRI and subsequently identify brain regions potentially important to the integrity of the mobile function. Significance: GAMER-MRIL holds promise for developing biomarkers and increasing clinicians' trust in NN.
</details>
<details>
<summary>摘要</summary>
目标：为多发性硬化病（MS）患者 Identify 负面功能相关的脑变化。现在，没有明确的认知，哪些生理学特征驱动单个MS患者的残疾。在这种工作中，我们提议了一种全新的全脑量化MRI（qMRI）、卷积神经网络（CNN）和可解释方法，从分类MS患者严重残疾到研究相关的脑变化。方法：一百六十六名MS患者通过3T MRI成像。重要的质量MRI（qMRI）信息，包括质量T1（qT1）、脑白质含量（MWF）和神经纤维数（NDI），都被重构。为了全面利用qMRI，我们扩展了基于闭合注意力的CNN（GAMER-MRI），并将其应用到整个脑图像。为了找出残疾相关的脑区域，我们修改了层次相关传播（LRP）方法，以包括qMRI。结果：测试性能为AUC=0.885。qT1是残疾相关度最高的指标，其次是NDI。我们的LRP方法在特定任务/问题中更有特点地找出了残疾相关的脑区域，比如脑束束络（ corticospinal tract），其中qT1和NDI的平均值与患者残疾分数（ρ=-0.37和0.44）有 statistically significant 相关性。结论：这些结果表明，GAMER-MRIL可以通过qMRI和CNN来分类患者严重残疾，并且可以特定残疾相关的脑区域。这些结果表明GAMER-MRIL具有开发生物标志物和增加临床医生对NN的信任的潜力。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation"><a href="#Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation" class="headerlink" title="Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation"></a>Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07506">http://arxiv.org/abs/2308.07506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadie1/medseguq">https://github.com/jadie1/medseguq</a></li>
<li>paper_authors: Jadie Adams, Shireen Y. Elhabian</li>
<li>for:  aid diagnosis and treatment planning</li>
<li>methods:  epistemic uncertainty quantification methods in organ segmentation</li>
<li>results:  comprehensive benchmarking study to evaluate the accuracy, uncertainty calibration, and scalability of different methods<details>
<summary>Abstract</summary>
Deep learning based methods for automatic organ segmentation have shown promise in aiding diagnosis and treatment planning. However, quantifying and understanding the uncertainty associated with model predictions is crucial in critical clinical applications. While many techniques have been proposed for epistemic or model-based uncertainty estimation, it is unclear which method is preferred in the medical image analysis setting. This paper presents a comprehensive benchmarking study that evaluates epistemic uncertainty quantification methods in organ segmentation in terms of accuracy, uncertainty calibration, and scalability. We provide a comprehensive discussion of the strengths, weaknesses, and out-of-distribution detection capabilities of each method as well as recommendations for future improvements. These findings contribute to the development of reliable and robust models that yield accurate segmentations while effectively quantifying epistemic uncertainty.
</details>
<details>
<summary>摘要</summary>
深度学习基于方法为自动器官 segmentation 表现出了许多批处的可能性，帮助诊断和治疗规划。然而，量化和理解模型预测结果中的uncertainty是在重要的临床应用中关键。虽然许多技术被提出用于知识 Based uncertainty estimation，但是没有一种方法在医学图像分析中被强调。这篇论文提供了一项全面的比较研究，评估了器官 segmentation 中epistemic uncertainty quantification 方法的准确性、uncertainty calibration和可扩展性。我们提供了每种方法的优缺点、out-of-distribution检测能力和未来改进的建议。这些发现有助于开发可靠和可靠的模型，以获得准确的 segmentation 结果，同时有效地量化epistemic uncertainty。
</details></li>
</ul>
<hr>
<h2 id="Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions"><a href="#Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions" class="headerlink" title="Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions"></a>Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07495">http://arxiv.org/abs/2308.07495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Sun, Chunyan Wang<br>for:* The paper proposes a system for detecting brain tumors in 3D MRI brain scans of Flair modality.methods:* The system uses a 2D histogram presentation to comprehend the gray-level distribution and pixel-location distribution of a 3D object.* It exploits the left-right asymmetry of a brain structure to establish particular 2D histograms, which are then modulated to attenuate irrelevant elements.* The system predicts the tumor pixel distribution in 3 steps, on the axial, coronal, and sagittal slice series, respectively.results:* The system delivers very good tumor detection results, comparable to those of state-of-the-art CNN systems with mono-modality inputs.* The system achieves this at an extremely low computation cost and without the need for training.Here is the answer in Simplified Chinese text:for:* 这篇论文提出了一种用于检测大脑肿瘤的系统，该系统使用3D MRI脑部扫描的FLAIR模式。methods:* 该系统使用2D histogram展示来理解肿瘤区域的灰度分布和像素位置分布。* 它利用脑结构的左右偏好来建立特定的2D histogram，并对其进行减小。* 系统在axial、coronal和sagittal slice series上预测肿瘤像素分布，并在每个步骤中使用预测结果来identify&#x2F;remove肿瘤自由的 slice。results:* 系统实现了非常好的肿瘤检测结果，与单模态输入的state-of-the-art CNN系统相当。* 系统在计算成本非常低的情况下实现了这一结果，而无需训练。<details>
<summary>Abstract</summary>
In this paper, we propose a system to detect brain tumor in 3D MRI brain scans of Flair modality. It performs 2 functions: (a) predicting gray-level and locational distributions of the pixels in the tumor regions and (b) generating tumor mask in pixel-wise precision. To facilitate 3D data analysis and processing, we introduced a 2D histogram presentation that comprehends the gray-level distribution and pixel-location distribution of a 3D object. In the proposed system, particular 2D histograms, in which tumor-related feature data get concentrated, are established by exploiting the left-right asymmetry of a brain structure. A modulation function is generated from the input data of each patient case and applied to the 2D histograms to attenuate the element irrelevant to the tumor regions. The prediction of the tumor pixel distribution is done in 3 steps, on the axial, coronal and sagittal slice series, respectively. In each step, the prediction result helps to identify/remove tumor-free slices, increasing the tumor information density in the remaining data to be applied to the next step. After the 3-step removal, the 3D input is reduced to a minimum bounding box of the tumor region. It is used to finalize the prediction and then transformed into a 3D tumor mask, by means of gray level thresholding and low-pass-based morphological operations. The final prediction result is used to determine the critical threshold. The proposed system has been tested extensively with the data of more than one thousand patient cases in the datasets of BraTS 2018~21. The test results demonstrate that the predicted 2D histograms have a high degree of similarity with the true ones. The system delivers also very good tumor detection results, comparable to those of state-of-the-art CNN systems with mono-modality inputs, which is achieved at an extremely low computation cost and no need for training.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种系统，用于检测脑肿瘤在3D MRI脑内部分模式下的检测。它执行了两个功能：（a）预测肿瘤区域中像素的灰度和位坐标分布，（b）生成精准的肿瘤面积。为了便于3D数据分析和处理，我们引入了2D分布图表示法，该法涵盖了肿瘤区域中像素的灰度分布和位坐标分布。在我们提出的系统中，特定的2D分布，在其中肿瘤相关特征数据受集中，被建立了。然后，通过对输入数据的每个患者案例中的偏置函数进行应用，以减少不相关于肿瘤区域的元素。肿瘤像素分布预测在 axial、coronal 和 sagittal 三个方向上进行了3步骤逐步进行，每步骤结果帮助确定/移除肿瘤不存在的剖面，从而提高肿瘤信息的浓度在剩下的数据中，并应用到下一步。经过3步 removals，输入3D数据被减少到最小 bounding box 的肿瘤区域。它被用于最终预测，并使用灰度阈值和低通过的杂谱操作来转换为3D肿瘤面积。测试结果表明，预测的2D分布与实际分布有高度的相似性。系统还提供了非常好的肿瘤检测结果，与STATE-OF-THE-ART CNN系统的单模式输入相比，并且在极低的计算成本下达到了这一点，无需训练。
</details></li>
</ul>
<hr>
<h2 id="Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis"><a href="#Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis" class="headerlink" title="Space Object Identification and Classification from Hyperspectral Material Analysis"></a>Space Object Identification and Classification from Hyperspectral Material Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07481">http://arxiv.org/abs/2308.07481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimiliano Vasile, Lewis Walker, Andrew Campbell, Simao Marto, Paul Murray, Stephen Marshall, Vasili Savitski</li>
<li>for: 本研究旨在提取不知名空间物体的干涉特征信息，并使用这些信息来确定物体的物理组成。</li>
<li>methods: 本研究使用了两种材料标识和分类技术：一种基于机器学习，另一种基于最小二乘匹配known спектры库。通过这些信息，一种监督式机器学习算法用于将物体分类为不同类别，根据检测到物体上的材料。</li>
<li>results: 研究结果表明，当材料库缺失一种物质时，材料分类方法的行为会受到影响。此外，研究还发现在不理想的天气条件下，材料分类方法的行为也会受到影响。最终，文章将展示一些初步的空间物体识别和分类结果。<details>
<summary>Abstract</summary>
This paper presents a data processing pipeline designed to extract information from the hyperspectral signature of unknown space objects. The methodology proposed in this paper determines the material composition of space objects from single pixel images. Two techniques are used for material identification and classification: one based on machine learning and the other based on a least square match with a library of known spectra. From this information, a supervised machine learning algorithm is used to classify the object into one of several categories based on the detection of materials on the object. The behaviour of the material classification methods is investigated under non-ideal circumstances, to determine the effect of weathered materials, and the behaviour when the training library is missing a material that is present in the object being observed. Finally the paper will present some preliminary results on the identification and classification of space objects.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hyperspectral signature" is translated as "多спектル特征" (duō yán jīng)* "material composition" is translated as "物质组成" (wù zhì zhōng)* "machine learning" is translated as "机器学习" (jī shì xué xí)* "least square match" is translated as "最小二乘匹配" (zuì xiǎo èr chuī pīng pái)* "training library" is translated as "训练库" (xùn xí kù)* "weathered materials" is translated as "气候变化的材料" (qì hòu biàn gē de zhì lǐ)
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression"><a href="#Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression" class="headerlink" title="Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression"></a>Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07477">http://arxiv.org/abs/2308.07477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antonbaumann/mimo-unet">https://github.com/antonbaumann/mimo-unet</a></li>
<li>paper_authors: Anton Baumann, Thomas Roßberg, Michael Schmitt</li>
<li>for: 这个论文的目的是提高机器学习模型的可靠性和可读性，特别是在高度关键的实际应用场景中。</li>
<li>methods: 这篇论文使用了多输入多输出（MIMO）框架，利用深度神经网络的过参数化来进行像素级回归任务。它还引入了一种同步多个子网络性能的新程序。</li>
<li>results: 对两个正交的数据集进行了全面的评估，显示MIMO U-Net模型具有与现有模型相当的准确率，更好的折衔在正常数据上，robust的对于异常数据检测能力，并且具有较小的参数大小和更快的推理时间。代码可以在github.com&#x2F;antonbaumann&#x2F;MIMO-Unet中找到。<details>
<summary>Abstract</summary>
Uncertainty estimation in machine learning is paramount for enhancing the reliability and interpretability of predictive models, especially in high-stakes real-world scenarios. Despite the availability of numerous methods, they often pose a trade-off between the quality of uncertainty estimation and computational efficiency. Addressing this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework -- an approach exploiting the overparameterization of deep neural networks -- for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. For that purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. Additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. Code available at github.com/antonbaumann/MIMO-Unet
</details>
<details>
<summary>摘要</summary>
Machine learning 中的不确定性估计是对预测模型的可靠性和可解释性提高的重要环节,特别是在高度的实际应用场景中。Despite the numerous methods available, they often involve a trade-off between the quality of uncertainty estimation and computational efficiency. To address this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. To achieve this, we adapted the U-Net architecture to train multiple subnetworks within a single model, leveraging the overparameterization in deep neural networks. Additionally, we propose a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and significant improvements in parameter size and inference time. 相关代码可以在github.com/antonbaumann/MIMO-Unet 上找到。
</details></li>
</ul>
<hr>
<h2 id="Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation"><a href="#Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation" class="headerlink" title="Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation"></a>Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07251">http://arxiv.org/abs/2308.07251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/mdunet">https://github.com/liamchalcroft/mdunet</a></li>
<li>paper_authors: Liam Chalcroft, Ruben Lourenço Pereira, Mikael Brudfors, Andrew S. Kayser, Mark D’Esposito, Cathy J. Price, Ioannis Pappas, John Ashburner</li>
<li>for: 这个论文是为了解决医疗影像分类 задачі中的深度学习模型效果不高、缺乏调节不变性的问题。</li>
<li>methods: 这个论文提出了一种基于全条件对称化Transformer块的U-Net架构，以模型3D脑膜疾病分类中的长距离互动。</li>
<li>results: 这个模型能够提供最大的折衔点，即性能与现有State-of-the-art相当，并且具有调节不变性和对称性的优点。<details>
<summary>Abstract</summary>
Vision transformers are effective deep learning models for vision tasks, including medical image segmentation. However, they lack efficiency and translational invariance, unlike convolutional neural networks (CNNs). To model long-range interactions in 3D brain lesion segmentation, we propose an all-convolutional transformer block variant of the U-Net architecture. We demonstrate that our model provides the greatest compromise in three factors: performance competitive with the state-of-the-art; parameter efficiency of a CNN; and the favourable inductive biases of a transformer. Our public implementation is available at https://github.com/liamchalcroft/MDUNet .
</details>
<details>
<summary>摘要</summary>
“vision transformer”是深度学习模型，用于视觉任务，包括医疗图像分割。然而，它缺乏效率和翻译不变性，与卷积神经网络（CNN）不同。为了在3D脑损害分割中模型长距离交互，我们提议一种具有U-Net架构的所有卷积转换器块变体。我们示示了我们的模型在三个因素中均提供了最大的妥协：与现状前景竞争性的性能; 参数效率与CNN相同; 以及转换器的有利假设。我们的公共实现可以在https://github.com/liamchalcroft/MDUNet上找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/eess.IV_2023_08_15/" data-id="clogyj92t01387cra7wxacvwa" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/15/cs.LG_2023_08_15/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.LG - 2023-08-15
        
      </div>
    </a>
  
  
    <a href="/2023/08/14/cs.SD_2023_08_14/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-14</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
