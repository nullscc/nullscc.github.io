
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.CV - 2023-08-31 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Boosting Detection in Crowd Analysis via Underutilized Output Features paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.16187 repo_url: None paper_authors: Shaokai Wu, Fengyu Yang for: 这种研究旨在探讨检测基于方法在人群分析中的潜在优势，以">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.CV - 2023-08-31">
<meta property="og:url" content="https://nullscc.github.io/2023/08/31/cs.CV_2023_08_31/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Boosting Detection in Crowd Analysis via Underutilized Output Features paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.16187 repo_url: None paper_authors: Shaokai Wu, Fengyu Yang for: 这种研究旨在探讨检测基于方法在人群分析中的潜在优势，以">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-31T13:00:00.000Z">
<meta property="article:modified_time" content="2023-08-31T13:36:18.356Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.CV_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.CV_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T13:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.CV - 2023-08-31
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Boosting-Detection-in-Crowd-Analysis-via-Underutilized-Output-Features"><a href="#Boosting-Detection-in-Crowd-Analysis-via-Underutilized-Output-Features" class="headerlink" title="Boosting Detection in Crowd Analysis via Underutilized Output Features"></a>Boosting Detection in Crowd Analysis via Underutilized Output Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16187">http://arxiv.org/abs/2308.16187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaokai Wu, Fengyu Yang</li>
<li>for: 这种研究旨在探讨检测基于方法在人群分析中的潜在优势，以及如何利用这些方法提高人群分析的精度和效果。</li>
<li>methods: 这种模型使用了混合2D-1D压缩技术来精细化输出特征，并通过地域适应的NMS阈值和解体并对齐策略来解决检测基于方法的主要局限性。</li>
<li>results: 经过广泛的人群分析任务评估，包括人群数量、位置和检测等，研究表明可以通过利用输出特征来提高检测基于方法在人群分析中的精度和效果。<details>
<summary>Abstract</summary>
Detection-based methods have been viewed unfavorably in crowd analysis due to their poor performance in dense crowds. However, we argue that the potential of these methods has been underestimated, as they offer crucial information for crowd analysis that is often ignored. Specifically, the area size and confidence score of output proposals and bounding boxes provide insight into the scale and density of the crowd. To leverage these underutilized features, we propose Crowd Hat, a plug-and-play module that can be easily integrated with existing detection models. This module uses a mixed 2D-1D compression technique to refine the output features and obtain the spatial and numerical distribution of crowd-specific information. Based on these features, we further propose region-adaptive NMS thresholds and a decouple-then-align paradigm that address the major limitations of detection-based methods. Our extensive evaluations on various crowd analysis tasks, including crowd counting, localization, and detection, demonstrate the effectiveness of utilizing output features and the potential of detection-based methods in crowd analysis.
</details>
<details>
<summary>摘要</summary>
传感器基本方法在群体分析中被视为不利，主要是因为它们在紧凑的群体中表现不佳。然而，我们认为这些方法的潜力被低估了，因为它们提供了群体分析中通常被忽略的重要信息。Specifically，输出提议和矩形框的面积和信任分数提供了群体的规模和密度的信息。为了利用这些尚未被利用的特征，我们提议了一个名为Crowd Hat的插件模块，可以轻松地与现有的检测模型结合使用。这个模块使用了2D-1D压缩技术来精细化输出特征，并从而获得群体中特定信息的空间和数字分布。基于这些特征，我们进一步提出了地域适应的NMS阈值和解除然后对齐的方法，这些方法可以解决检测基本方法中的主要局限性。我们对各种群体分析任务，包括群体计数、本地化和检测，进行了广泛的评估，并证明了利用输出特征和检测基本方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="SAM-Med2D"><a href="#SAM-Med2D" class="headerlink" title="SAM-Med2D"></a>SAM-Med2D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16184">http://arxiv.org/abs/2308.16184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uni-medical/sam-med2d">https://github.com/uni-medical/sam-med2d</a></li>
<li>paper_authors: Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, Hui Sun, Junjun He, Shaoting Zhang, Min Zhu, Yu Qiao</li>
<li>for: 本研究将SAM模型应用于医疗影像分类中，以填补自然影像和医疗影像之间的领域差距。</li>
<li>methods: 我们首先收集了约460万帧医疗影像和1970万个标注，建立了一个大规模的医疗影像分类数据集，覆盖多种模式和物体。然后，我们对SAM模型进行了全面的微调，并将其转换为SAM-Med2D模型。此外，我们还对SAM模型的Encoder和Decoder进行了进一步的微调，以获得最佳的SAM-Med2D模型。</li>
<li>results: 我们的方法在多种医疗影像分类任务中展示了明显的超越SAM的性能和扩展性。 Specifically, we evaluated the performance of SAM-Med2D on 9 datasets from MICCAI 2023 challenge and found that it outperformed SAM in terms of both segmentation accuracy and generalization ability.<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) represents a state-of-the-art research advancement in natural image segmentation, achieving impressive results with input prompts such as points and bounding boxes. However, our evaluation and recent research indicate that directly applying the pretrained SAM to medical image segmentation does not yield satisfactory performance. This limitation primarily arises from significant domain gap between natural images and medical images. To bridge this gap, we introduce SAM-Med2D, the most comprehensive studies on applying SAM to medical 2D images. Specifically, we first collect and curate approximately 4.6M images and 19.7M masks from public and private datasets, constructing a large-scale medical image segmentation dataset encompassing various modalities and objects. Then, we comprehensively fine-tune SAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that only adopt bounding box or point prompts as interactive segmentation approach, we adapt SAM to medical image segmentation through more comprehensive prompts involving bounding boxes, points, and masks. We additionally fine-tune the encoder and decoder of the original SAM to obtain a well-performed SAM-Med2D, leading to the most comprehensive fine-tuning strategies to date. Finally, we conducted a comprehensive evaluation and analysis to investigate the performance of SAM-Med2D in medical image segmentation across various modalities, anatomical structures, and organs. Concurrently, we validated the generalization capability of SAM-Med2D on 9 datasets from MICCAI 2023 challenge. Overall, our approach demonstrated significantly superior performance and generalization capability compared to SAM.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的研究进展——Segment Anything Model（SAM），用于自然图像分割，并取得了卓越的结果。但是，我们的评估和最新的研究表明，直接将预训练的SAM应用于医疗图像分割不会达到满意的性能。这种限制主要归结于自然图像和医疗图像之间的领域差异。为bridging这个差异，我们介绍了SAM-Med2D，这是对SAM的最全面的应用研究，用于医疗二维图像分割。我们首先收集了约4.6万个图像和19.7万个mask从公共和私人数据集中，建立了一个包括多种Modalities和物体的医疗图像分割数据集。然后，我们对SAM进行了全面的微调，并将其转化为SAM-Med2D。与之前的方法只采用矩形框或点提示作为交互分割方法不同，我们在SAM-Med2D中采用了更全面的提示，包括矩形框、点和Mask。此外，我们还进行了SAM的encoder和decoder的微调，以获得一个高性能的SAM-Med2D。最后，我们进行了全面的评估和分析，以investigate SAM-Med2D在医疗图像分割中的性能，包括不同Modalities、生物结构和器官。同时，我们验证了SAM-Med2D在MICCAI 2023挑战赛中的通用能力。总的来说，我们的方法在医疗图像分割中表现出了显著的性能和通用能力，与SAM相比。
</details></li>
</ul>
<hr>
<h2 id="GREC-Generalized-Referring-Expression-Comprehension"><a href="#GREC-Generalized-Referring-Expression-Comprehension" class="headerlink" title="GREC: Generalized Referring Expression Comprehension"></a>GREC: Generalized Referring Expression Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16182">http://arxiv.org/abs/2308.16182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henghuiding/grefcoco">https://github.com/henghuiding/grefcoco</a></li>
<li>paper_authors: Shuting He, Henghui Ding, Chang Liu, Xudong Jiang</li>
<li>for: 本研究旨在推广经典引用表达理解（REC）的应用范围，以涵盖多个目标对象的引用表达。</li>
<li>methods: 该研究提出了一个新的标准测试集——通用引用表达理解（GREC）测试集，并实现了一种基于这个测试集的GREC方法实现代码。</li>
<li>results: 该研究在GREC测试集上实现了高度的准确率，并且在多个目标对象的引用表达中显示出了优异的性能。Translation:</li>
<li>for: 本研究的目标是推广经典引用表达理解（REC）的应用范围，以涵盖多个目标对象的引用表达。</li>
<li>methods: 该研究提出了一个新的标准测试集——通用引用表达理解（GREC）测试集，并实现了一种基于这个测试集的GREC方法实现代码。</li>
<li>results: 该研究在GREC测试集上实现了高度的准确率，并且在多个目标对象的引用表达中显示出了优异的性能。<details>
<summary>Abstract</summary>
The objective of Classic Referring Expression Comprehension (REC) is to produce a bounding box corresponding to the object mentioned in a given textual description. Commonly, existing datasets and techniques in classic REC are tailored for expressions that pertain to a single target, meaning a sole expression is linked to one specific object. Expressions that refer to multiple targets or involve no specific target have not been taken into account. This constraint hinders the practical applicability of REC. This study introduces a new benchmark termed as Generalized Referring Expression Comprehension (GREC). This benchmark extends the classic REC by permitting expressions to describe any number of target objects. To achieve this goal, we have built the first large-scale GREC dataset named gRefCOCO. This dataset encompasses a range of expressions: those referring to multiple targets, expressions with no specific target, and the single-target expressions. The design of GREC and gRefCOCO ensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a GREC method implementation code, and GREC evaluation code are available at https://github.com/henghuiding/gRefCOCO.
</details>
<details>
<summary>摘要</summary>
“目的是实现文本描述中的物体引用表达（REC）。现有的dataset和技术仅适用于单一目标的表达，这限制了REC的实际应用。本研究引入了一个新的benchmark，称为通用 Referring Expression Comprehension（GREC）。GREC扩展了传统REC，允许表达描述任意数量的目标物体。为 достичь这个目标，我们建立了第一个大规模的GREC dataset，名为gRefCOCO。这个dataset包括了多个目标、无 especified 目标和单一目标的表达。GREC和gRefCOCO的设计保证与传统REC的相容性。提供了GREC方法实现代码、GREC评估代码和gRefCOCO dataset，可以在https://github.com/henghuiding/gRefCOCO中下载。”
</details></li>
</ul>
<hr>
<h2 id="MMVP-Motion-Matrix-based-Video-Prediction"><a href="#MMVP-Motion-Matrix-based-Video-Prediction" class="headerlink" title="MMVP: Motion-Matrix-based Video Prediction"></a>MMVP: Motion-Matrix-based Video Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16154">http://arxiv.org/abs/2308.16154</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kay1794/mmvp-motion-matrix-based-video-prediction">https://github.com/kay1794/mmvp-motion-matrix-based-video-prediction</a></li>
<li>paper_authors: Yiqi Zhong, Luming Liang, Ilya Zharkov, Ulrich Neumann</li>
<li>For: 本研究旨在解决视频预测中的中心挑战，即从图像帧中预测物体未来的运动，同时保持物体的外观一致性 across frames。* Methods: 该研究提出了一种可以批处理的两核心视频预测框架，即运动矩阵基于视频预测（MMVP）。与前一些方法不同的是，MMVP 将动作和外观信息解耦，通过构建外观不关心的运动矩阵来实现。这种设计提高了视频预测的准确率和效率，并降低了模型的大小。* Results: 广泛的实验结果表明，MMVP 在公共数据集上比前一些状态之前的系统更高（约1 db PSNR，UCF Sports），并且在远远小于前一些系统的模型大小（84% 或更小）下达成了这些result。请参考 <a target="_blank" rel="noopener" href="https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction">https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction</a> 获取官方代码和使用的数据集。<details>
<summary>Abstract</summary>
A central challenge of video prediction lies where the system has to reason the objects' future motions from image frames while simultaneously maintaining the consistency of their appearances across frames. This work introduces an end-to-end trainable two-stream video prediction framework, Motion-Matrix-based Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that usually handle motion prediction and appearance maintenance within the same set of modules, MMVP decouples motion and appearance information by constructing appearance-agnostic motion matrices. The motion matrices represent the temporal similarity of each and every pair of feature patches in the input frames, and are the sole input of the motion prediction module in MMVP. This design improves video prediction in both accuracy and efficiency, and reduces the model size. Results of extensive experiments demonstrate that MMVP outperforms state-of-the-art systems on public data sets by non-negligible large margins (about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the size or smaller). Please refer to https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for the official code and the datasets used in this paper.
</details>
<details>
<summary>摘要</summary>
中心挑战：视频预测需要系统根据图像帧来预测物体未来运动，同时保持物体在帧之间的外观一致性。这篇论文提出了一种端到端训练的两核心视频预测框架——动力矩阵基于视频预测（MMVP），解决这个挑战。不同于之前的方法通常在同一组模块中处理运动预测和外观维持，MMVP 将运动和外观信息分离开来，通过构建不同帧的特征小块之间的应用无关动力矩阵来实现。这种设计提高了视频预测的准确性和效率，并减少模型的大小。经过广泛的实验，我们发现MMVP在公共数据集上比前一代系统大幅提高（约1 db PSNR、UCF Sports），而且模型的大小减少了84%以下。请参考https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction  для官方代码和使用的数据集。
</details></li>
</ul>
<hr>
<h2 id="Modality-Cycles-with-Masked-Conditional-Diffusion-for-Unsupervised-Anomaly-Segmentation-in-MRI"><a href="#Modality-Cycles-with-Masked-Conditional-Diffusion-for-Unsupervised-Anomaly-Segmentation-in-MRI" class="headerlink" title="Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI"></a>Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16150">http://arxiv.org/abs/2308.16150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyun Liang, Harry Anthony, Felix Wagner, Konstantinos Kamnitsas</li>
<li>for: 这篇论文的目的是提出一种不需要手动分类的无监督异常分割方法，可以检测在训练中无法处理的异常模式，以提高模型的可靠性，特别在医疗影像领域。</li>
<li>methods: 这篇论文提出的方法基于两个基本想法。首先，我们提出使用类型转换为机制来实现异常检测。图像转移模型学习特定体部特征的对应关系，因此在训练时不能转换特定的体部或图像模式，这就 enables their segmentation。其次，我们结合图像转移和封页 conditional diffusion 模型，尝试将遮盾区域下的组织视网膜，进一步暴露未知模式，当生成模型无法重建它们时。</li>
<li>results: 我们在 BraTS2021 多Modal MRI 的代理任务上训练这种方法，并在该任务上进行测试。我们的结果显示，我们的方法与先前的无监督方法相比，在图像重建和干扰中得到了比较好的结果。<details>
<summary>Abstract</summary>
Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables their segmentation. Furthermore, we combine image translation with a masked conditional diffusion model, which attempts to `imagine' what tissue exists under a masked area, further exposing unknown patterns as the generative model fails to recreate them. We evaluate our method on a proxy task by training on healthy-looking slices of BraTS2021 multi-modality MRIs and testing on slices with tumors. We show that our method compares favorably to previous unsupervised approaches based on image reconstruction and denoising with autoencoders and diffusion models.
</details>
<details>
<summary>摘要</summary>
不监督异常分割Targets patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables their segmentation. Furthermore, we combine image translation with a masked conditional diffusion model, which attempts to 'imagine' what tissue exists under a masked area, further exposing unknown patterns as the generative model fails to recreate them. We evaluate our method on a proxy task by training on healthy-looking slices of BraTS2021 multi-modality MRIs and testing on slices with tumors. We show that our method compares favorably to previous unsupervised approaches based on image reconstruction and denoising with autoencoders and diffusion models.Here's the text with some additional information about the translation:I used the Google Translate API to translate the text into Simplified Chinese. The translation is written in a formal, academic style, which is appropriate for a research paper. I made sure to preserve the original meaning and structure of the text as much as possible, while also ensuring that the translation is grammatically correct and easy to understand.Please note that the translation is a machine translation, and it may not be perfect. There may be some nuances or idiomatic expressions that are lost in translation. If you have any specific questions or need further clarification, please feel free to ask!
</details></li>
</ul>
<hr>
<h2 id="CircleFormer-Circular-Nuclei-Detection-in-Whole-Slide-Images-with-Circle-Queries-and-Attention"><a href="#CircleFormer-Circular-Nuclei-Detection-in-Whole-Slide-Images-with-Circle-Queries-and-Attention" class="headerlink" title="CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention"></a>CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16145">http://arxiv.org/abs/2308.16145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanghx-iim-ahu/circleformer">https://github.com/zhanghx-iim-ahu/circleformer</a></li>
<li>paper_authors: Hengxu Zhang, Pengpeng Liang, Zhiyong Sun, Bo Song, Erkang Cheng</li>
<li>for: 这篇论文是针对医疗影像中圆形物体检测的研究，尤其是精准地检测体内组织中的圆形核。</li>
<li>methods: 本文使用了Transformer架构，并将圆形查询嵌入到Transformer解oder中，逐步精确地检测圆形物体。另外，本文还引入了圆形对偶探索模组，以计算圆形查询和影像特征之间的相似性。</li>
<li>results: 本文在公共的MoNuSeg数据集上进行了圆形核检测和分类任务的评估，并取得了比州前方法更好的成绩。此外，本文还进行了各组件效果的验证。<details>
<summary>Abstract</summary>
Both CNN-based and Transformer-based object detection with bounding box representation have been extensively studied in computer vision and medical image analysis, but circular object detection in medical images is still underexplored. Inspired by the recent anchor free CNN-based circular object detection method (CircleNet) for ball-shape glomeruli detection in renal pathology, in this paper, we present CircleFormer, a Transformer-based circular medical object detection with dynamic anchor circles. Specifically, queries with circle representation in Transformer decoder iteratively refine the circular object detection results, and a circle cross attention module is introduced to compute the similarity between circular queries and image features. A generalized circle IoU (gCIoU) is proposed to serve as a new regression loss of circular object detection as well. Moreover, our approach is easy to generalize to the segmentation task by adding a simple segmentation branch to CircleFormer. We evaluate our method in circular nuclei detection and segmentation on the public MoNuSeg dataset, and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches. The effectiveness of each component is validated via ablation studies as well. Our code is released at: \url{https://github.com/zhanghx-iim-ahu/CircleFormer}.
</details>
<details>
<summary>摘要</summary>
历史上，CNN和Transformer两种方法在计算机视觉和医学图像分析中进行了广泛的研究，但医学图像中径向物体检测仍然受到了相对较少的关注。在这篇论文中，我们提出了一种基于Transformer的径向医学对象检测方法，称为CircleFormer。该方法使用Transformer预测器中的径向查询来逐步精细地检测径向对象结果。此外，我们还提出了一种径向圆点对准模块，以计算径向查询和图像特征之间的相似性。此外，我们还提出了一种新的径向圆点IOU（gCIoU），用于衡量径向对象检测结果的准确性。此外，我们的方法易于扩展到分割任务，只需要在CircleFormer上添加一个简单的分割分支即可。我们在公共的MoNuSeg数据集上进行了径向核体检测和分割任务的实验，结果表明，我们的方法在与状态艺术方法相比表现出色。此外，我们还进行了一些缺省分析，以验证每个组件的有效性。我们的代码在：\url{https://github.com/zhanghx-iim-ahu/CircleFormer}。
</details></li>
</ul>
<hr>
<h2 id="MedShapeNet-–-A-Large-Scale-Dataset-of-3D-Medical-Shapes-for-Computer-Vision"><a href="#MedShapeNet-–-A-Large-Scale-Dataset-of-3D-Medical-Shapes-for-Computer-Vision" class="headerlink" title="MedShapeNet – A Large-Scale Dataset of 3D Medical Shapes for Computer Vision"></a>MedShapeNet – A Large-Scale Dataset of 3D Medical Shapes for Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16139">http://arxiv.org/abs/2308.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianning Li, Antonio Pepe, Christina Gsaxner, Gijs Luijten, Yuan Jin, Narmada Ambigapathy, Enrico Nasca, Naida Solak, Gian Marco Melito, Afaque R. Memon, Xiaojun Chen, Jan Stefan Kirschke, Ezequiel de la Rosa, Patrich Ferndinand Christ, Hongwei Bran Li, David G. Ellis, Michele R. Aizenberg, Sergios Gatidis, Thomas Kuestner, Nadya Shusharina, Nicholas Heller, Vincent Andrearczyk, Adrien Depeursinge, Mathieu Hatt, Anjany Sekuboyina, Maximilian Loeffler, Hans Liebl, Reuben Dorent, Tom Vercauteren, Jonathan Shapey, Aaron Kujawa, Stefan Cornelissen, Patrick Langenhuizen, Achraf Ben-Hamadou, Ahmed Rekik, Sergi Pujades, Edmond Boyer, Federico Bolelli, Costantino Grana, Luca Lumetti, Hamidreza Salehi, Jun Ma, Yao Zhang, Ramtin Gharleghi, Susann Beier, Eduardo A. Garza-Villarreal, Thania Balducci, Diego Angeles-Valdez, Roberto Souza, Leticia Rittner, Richard Frayne, Yuanfeng Ji, Soumick Chatterjee, Andreas Nuernberger, Joao Pedrosa, Carlos Ferreira, Guilherme Aresta, Antonio Cunha, Aurelio Campilho, Yannick Suter, Jose Garcia, Alain Lalande, Emmanuel Audenaert, Claudia Krebs, Timo Van Leeuwen, Evie Vereecke, Rainer Roehrig, Frank Hoelzle, Vahid Badeli, Kathrin Krieger, Matthias Gunzer, Jianxu Chen, Amin Dada, Miriam Balzer, Jana Fragemann, Frederic Jonske, Moritz Rempe, Stanislav Malorodov, Fin H. Bahnsen, Constantin Seibold, Alexander Jaus, Ana Sofia Santos, Mariana Lindo, Andre Ferreira, Victor Alves, Michael Kamp, Amr Abourayya, Felix Nensa, Fabian Hoerst, Alexander Brehmer, Lukas Heine, Lars E. Podleska, Matthias A. Fink, Julius Keyl, Konstantinos Tserpes, Moon-Sung Kim, Shireen Elhabian, Hans Lamecker, Dzenan Zukic, Beatriz Paniagua, Christian Wachinger, Martin Urschler, Luc Duong, Jakob Wasserthal, Peter F. Hoyer, Oliver Basu, Thomas Maal, Max J. H. Witjes, Ping Luo, Bjoern Menze, Mauricio Reyes, Christos Davatzikos, Behrus Puladi, Jens Kleesiek, Jan Egger</li>
<li>for: The paper is written to introduce MedShapeNet, a large collection of anatomical shapes and 3D surgical instrument models for medical image analysis.</li>
<li>methods: The paper uses a variety of methods to create and annotate the shapes in MedShapeNet, including direct modeling on imaging data and paired data annotations.</li>
<li>results: The paper reports that MedShapeNet currently includes over 100,000 medical shapes and provides a freely available repository of 3D models for extended reality and medical 3D printing, with the potential to adapt state-of-the-art vision algorithms to solve critical medical problems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍医疗影像分析的MedShapeNet，一个大量的生物形态和医疗器械3D模型集合。</li>
<li>methods: 论文使用了多种方法来创建和注释MedShapeNet中的形态，包括直接在医疗数据上模型和对应的数据注释。</li>
<li>results: 论文报告了MedShapeNet目前已经包含了超过100,000个医疗形态，并提供了一个免费的3D模型库，用于扩展现实（虚拟现实、增强现实、混合现实）和医疗3D打印。<details>
<summary>Abstract</summary>
We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to facilitate the translation of data-driven vision algorithms to medical applications, and it extends the opportunities to adapt SOTA vision algorithms to solve critical medical problems. Besides, the majority of the medical shapes in MedShapeNet are modeled directly on the imaging data of real patients, and therefore it complements well existing shape benchmarks comprising of computer-aided design (CAD) models. MedShapeNet currently includes more than 100,000 medical shapes, and provides annotations in the form of paired data. It is therefore also a freely available repository of 3D models for extended reality (virtual reality - VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This white paper describes in detail the motivations behind MedShapeNet, the shape acquisition procedures, the use cases, as well as the usage of the online shape search portal: https://medshapenet.ikim.nrw/
</details>
<details>
<summary>摘要</summary>
我们介绍MedShapeNet，一个大量医学形状（例如骨头、器官、血管）和3D医疗工具模型的集合。在深度学习时代之前，医学像分析中的统计形状模型（SSM）的广泛应用是证明医学数据中的形状很常被使用。然而，现在医学影像分析中的州际精算法（SOTA）都是以 voxel 为基础的。在计算机视觉中，形状（包括 voxel 占用网格、 mesh、点 cloud 和隐藏面模型）是3D数据的偏好表示方式，可见于许多shape相关的学会论文（如IEEE/CVF会议 on Computer Vision and Pattern Recognition（CVPR））以及形状库（如ShapeNet about 51,300 models和Princeton ModelNet 127,915 models）在计算机视觉研究中的增长 популяр性。MedShapeNet 是为了促进资料驱动的 computer vision 算法对医学应用的转译，而创建的一个替代方案，并延伸了适用 SOTA  vision 算法解决医学问题的机会。此外，MedShapeNet 中的大多数医学形状是直接从医疗影像数据中模型，因此与现有的 CAD 模型集成完美。MedShapeNet 目前包含超过 100,000 个医学形状，并提供双数据标签。因此，它还是一个免费可用的 3D 模型存储库，用于延伸现実（虚拟现実 - VR、增强现実 - AR、混合现実 - MR）和医疗 3D 印刷。本白皮书将详细介绍 MedShapeNet 的动机、形状取得程序、使用情况以及在线形状搜寻 Portal：https://medshapenet.ikim.nrw/
</details></li>
</ul>
<hr>
<h2 id="Improving-Few-shot-Image-Generation-by-Structural-Discrimination-and-Textural-Modulation"><a href="#Improving-Few-shot-Image-Generation-by-Structural-Discrimination-and-Textural-Modulation" class="headerlink" title="Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation"></a>Improving Few-shot Image Generation by Structural Discrimination and Textural Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16110">http://arxiv.org/abs/2308.16110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kobeshegu/sdtm-gan-acmmm-2023">https://github.com/kobeshegu/sdtm-gan-acmmm-2023</a></li>
<li>paper_authors: Mengping Yang, Zhe Wang, Wenyi Feng, Qian Zhang, Ting Xiao</li>
<li>for: 这篇论文的目的是提出一种新的几何图像生成模型，以实现几何图像生成中的内在Semantic Modulation和全局结构检测。</li>
<li>methods: 这篇论文使用的方法包括内在Local Representation的Semantic Modulation，以及全局结构检测器（StructD）的开发，以及鼓励模型识别频率信号的技术。</li>
<li>results: 这篇论文的实验结果显示，这种新的几何图像生成模型可以在三个popular dataset上 achieves state-of-the-art的Synthesis Performance，并且可以与现有的模型集成，以提高其性能。<details>
<summary>Abstract</summary>
Few-shot image generation, which aims to produce plausible and diverse images for one category given a few images from this category, has drawn extensive attention. Existing approaches either globally interpolate different images or fuse local representations with pre-defined coefficients. However, such an intuitive combination of images/features only exploits the most relevant information for generation, leading to poor diversity and coarse-grained semantic fusion. To remedy this, this paper proposes a novel textural modulation (TexMod) mechanism to inject external semantic signals into internal local representations. Parameterized by the feedback from the discriminator, our TexMod enables more fined-grained semantic injection while maintaining the synthesis fidelity. Moreover, a global structural discriminator (StructD) is developed to explicitly guide the model to generate images with reasonable layout and outline. Furthermore, the frequency awareness of the model is reinforced by encouraging the model to distinguish frequency signals. Together with these techniques, we build a novel and effective model for few-shot image generation. The effectiveness of our model is identified by extensive experiments on three popular datasets and various settings. Besides achieving state-of-the-art synthesis performance on these datasets, our proposed techniques could be seamlessly integrated into existing models for a further performance boost.
</details>
<details>
<summary>摘要</summary>
“几帧图像生成”，它目的是生成一个分类中的具有实际性和多样性的图像，只需要几帧图像作为输入。现有的方法可以全面 interpolate 不同的图像或者融合本地表现和预先定义的系数。然而，这种直觉的图像/特征融合只是利用最相关的信息进行生成，从而导致低的多样性和粗糙的 semantic 融合。为了解决这个问题，这篇论文提出了一个新的文本调控（TexMod）机制，可以将外部 semantics 信号注入到内部的本地表现中。这个 TexMod 由 discriminator 的反馈参数化，可以实现更细grained的 semantic 注入，同时维持生成的实际性。此外，我们还开发了一个全球结构 discriminator（StructD），可以明确地导引模型生成具有合理的配置和架构的图像。此外，我们还强调了模型的频率意识，通过让模型能够识别频率信号。通过这些技术，我们建立了一个新的和有效的几帧图像生成模型。我们的模型在三个流行的数据集上进行了广泛的实验，并在不同的设定下表现出色。除了在这些数据集上达到了现有的州域性synthesis 性能外，我们的提出的技术还可以与现有的模型进行整合，以获得更高的性能。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.CV_2023_08_31/" data-id="clm0t8dz50046v78853s8dr7e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/31/cs.SD_2023_08_31/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.SD - 2023-08-31
        
      </div>
    </a>
  
  
    <a href="/2023/08/31/cs.AI_2023_08_31/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-08-31</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
