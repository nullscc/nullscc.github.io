
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>eess.IV - 2023-08-24 17:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Learned Local Attention Maps for Synthesising Vessel Segmentations paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12861 repo_url: None paper_authors: Yash Deo, Rodrigo Bonazzola, Haoran Dou, Yan Xia, Tianyou We">
<meta property="og:type" content="article">
<meta property="og:title" content="eess.IV - 2023-08-24 17:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/24/eess.IV_2023_08_24/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Learned Local Attention Maps for Synthesising Vessel Segmentations paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12861 repo_url: None paper_authors: Yash Deo, Rodrigo Bonazzola, Haoran Dou, Yan Xia, Tianyou We">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:37.065Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-eess.IV_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/eess.IV_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      eess.IV - 2023-08-24 17:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations"><a href="#Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations" class="headerlink" title="Learned Local Attention Maps for Synthesising Vessel Segmentations"></a>Learned Local Attention Maps for Synthesising Vessel Segmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12861">http://arxiv.org/abs/2308.12861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Rodrigo Bonazzola, Haoran Dou, Yan Xia, Tianyou Wei, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>for: 这个论文的目的是用MRA进行血管视化，但MRA不是常见的成像方法，因此需要一种方法来合成血管分割结果。</li>
<li>methods: 这个论文使用了一种编码器-解码器模型，用于从T2 MRI中提取血管分割结果。该模型使用了两个阶段的多目标学习方法，以捕捉全局和本地特征。它还使用了学习的本地注意力图，以便只从T2 MRI中提取与合成CoW血管相关的信息。</li>
<li>results: 在测试中，这个模型可以从T2 MRI中生成高质量的血管分割结果，其中的Dice分数为0.79±0.03，高于现有的分割网络，如转换器U-Net（0.71±0.04）和nnU-net（0.68±0.05）。主要区别在于生成的CoW血管段的分辨率更高，特别是后circulation。<details>
<summary>Abstract</summary>
Magnetic resonance angiography (MRA) is an imaging modality for visualising blood vessels. It is useful for several diagnostic applications and for assessing the risk of adverse events such as haemorrhagic stroke (resulting from the rupture of aneurysms in blood vessels). However, MRAs are not acquired routinely, hence, an approach to synthesise blood vessel segmentations from more routinely acquired MR contrasts such as T1 and T2, would be useful. We present an encoder-decoder model for synthesising segmentations of the main cerebral arteries in the circle of Willis (CoW) from only T2 MRI. We propose a two-phase multi-objective learning approach, which captures both global and local features. It uses learned local attention maps generated by dilating the segmentation labels, which forces the network to only extract information from the T2 MRI relevant to synthesising the CoW. Our synthetic vessel segmentations generated from only T2 MRI achieved a mean Dice score of $0.79 \pm 0.03$ in testing, compared to state-of-the-art segmentation networks such as transformer U-Net ($0.71 \pm 0.04$) and nnU-net($0.68 \pm 0.05$), while using only a fraction of the parameters. The main qualitative difference between our synthetic vessel segmentations and the comparative models was in the sharper resolution of the CoW vessel segments, especially in the posterior circulation.
</details>
<details>
<summary>摘要</summary>
磁共振成像（MRA）是一种成像血管的方法，可以用于诊断和评估血栓roke的风险。然而，MRA不是Routine获得的，因此一种能够从T1和T2磁共振图像中合成血管分割的方法会很有用。我们提出了一种encoder-decoder模型，可以从T2 MRI中生成主要脑动脉的分割。我们使用了两个阶段的多目标学习方法，包括全局和局部特征。它使用学习的本地注意力地图，从T2 MRI中提取与合成CoW的信息。我们的合成血管分割从T2 MRI中获得的Mean Dice分数为$0.79\pm0.03$,比如state-of-the-art segmentation网络（如transformer U-Net和nnU-net）高出了一些。主要的区别在于CoW血管段的分辨率更加高，特别是后 circulation。
</details></li>
</ul>
<hr>
<h2 id="Achromatic-imaging-systems-with-flat-lenses-enabled-by-deep-learning"><a href="#Achromatic-imaging-systems-with-flat-lenses-enabled-by-deep-learning" class="headerlink" title="Achromatic imaging systems with flat lenses enabled by deep learning"></a>Achromatic imaging systems with flat lenses enabled by deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12776">http://arxiv.org/abs/2308.12776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roy Maman, Eitan Mualem, Noa Mazurski, Jacob Engelberg, Uriel Levy</li>
<li>for: 这篇论文旨在探讨使用凹型镜代替传统凸型镜元件的现代光学系统中，使用凹型镜带来的颜色偏振问题的解决方案。</li>
<li>methods: 作者使用了一种基于深度学习的方法，使用凹型镜拍摄的颜色户外图像数据集来对凹型镜的颜色偏振进行修正。</li>
<li>results: 作者的实验结果表明，使用这种方法可以在整个可见光谱中实现高质量的图像捕捉，并且在量化方面也有显著的提升，PSNR和SSIM分别达到了45.5dB和0.93。这些结果开启了使用凹型镜在高级多色光学捕捉系统中的应用前景。<details>
<summary>Abstract</summary>
Motivated by their great potential to reduce the size, cost and weight, flat lenses, a category that includes diffractive lenses and metalenses, are rapidly emerging as key components with the potential to replace the traditional refractive optical elements in modern optical systems. Yet, the inherently strong chromatic aberration of these flat lenses is significantly impairing their performance in systems based on polychromatic illumination or passive ambient light illumination, stalling their widespread implementation. Hereby, we provide a promising solution and demonstrate high quality imaging based on flat lenses over the entire visible spectrum. Our approach is based on creating a novel dataset of color outdoor images taken with our flat lens and using this dataset to train a deep-learning model for chromatic aberrations correction. Based on this approach we show unprecedented imaging results not only in terms of qualitative measures but also in the quantitative terms of the PSNR and SSIM scores of the reconstructed images. The results pave the way for the implementation of flat lenses in advanced polychromatic imaging systems.
</details>
<details>
<summary>摘要</summary>
驱动了它们的巨大潜力来减少大小、成本和重量，扁镜，包括扁Diffractive镜和金属镜，在现代光学系统中逐渐emerge为关键组件，替代传统的反射光学元件。然而，扁镜的自然强烈多色偏振问题在基于多色照明或普通 ambient light照明的系统中，对其性能产生了很大的障碍，使其广泛实施受阻。我们提供了一个有优势的解决方案，通过创建一个新的彩色户外图像数据集，使用这个数据集来训练深度学习模型来修正扁镜的多色偏振。根据这种方法，我们展示了具有很高质量的图像成像结果，不仅在质量上有显著的提升，还在量化上通过PSNR和SSIM分数来评估图像重建结果，得到了前所未有的成果。这些结果为扁镜在高级多色成像系统中的实施铺平了道路。
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes"><a href="#A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes" class="headerlink" title="A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes"></a>A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12675">http://arxiv.org/abs/2308.12675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Matthias Hehr, Nassir Navab, Carsten Marr</li>
<li>for: 这个研究旨在探讨急性白血病（AML）分型中是否存在年龄和性别偏见，以实现临床决策和患者照顾中的可靠性和公平性。</li>
<li>methods: 这个研究使用多例学习（MIL）架构，对不同的性别差异和年龄偏见进行训练。以评估性别偏见的影响，我们评估了男女试料集的表现。为了评估年龄偏见，我们对训练资料中缺乏的年龄组进行测试。</li>
<li>results: 我们发现，女性患者更容易受到性别偏见的影响，而certain age groups，例如72-86岁的患者，受到年龄偏见的影响。确保训练资料的多元性是生成可靠和公平的AML分型结果的关键。<details>
<summary>Abstract</summary>
Accurate classification of Acute Myeloid Leukemia (AML) subtypes is crucial for clinical decision-making and patient care. In this study, we investigate the potential presence of age and sex bias in AML subtype classification using Multiple Instance Learning (MIL) architectures. To that end, we train multiple MIL models using different levels of sex imbalance in the training set and excluding certain age groups. To assess the sex bias, we evaluate the performance of the models on male and female test sets. For age bias, models are tested against underrepresented age groups in the training data. We find a significant effect of sex and age bias on the performance of the model for AML subtype classification. Specifically, we observe that females are more likely to be affected by sex imbalance dataset and certain age groups, such as patients with 72 to 86 years of age with the RUNX1::RUNX1T1 genetic subtype, are significantly affected by an age bias present in the training data. Ensuring inclusivity in the training data is thus essential for generating reliable and equitable outcomes in AML genetic subtype classification, ultimately benefiting diverse patient populations.
</details>
<details>
<summary>摘要</summary>
精准分类AML分型是至关重要的临床决策和患者护理。本研究探讨AML分型分类中可能存在年龄和性别偏见的问题，使用多个实例学习（MIL）架构。为此，我们在培育不同性别倾向和不同年龄组的模型时进行训练多个MIL模型。以评估性别偏见，我们对男女测试集进行评估模型的性能。为年龄偏见，我们对培育数据中下 representations of age groups进行测试。我们发现，女性更容易受到数据性别偏见的影响，而certain age groups，如72-86岁的患者，受到培育数据中的年龄偏见的影响。因此，保证培育数据的多样性是AML分型分类中的关键，以实现多元患者群体的可靠和公平结果。
</details></li>
</ul>
<hr>
<h2 id="SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression"><a href="#SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression" class="headerlink" title="SCP: Spherical-Coordinate-based Learned Point Cloud Compression"></a>SCP: Spherical-Coordinate-based Learned Point Cloud Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12535">http://arxiv.org/abs/2308.12535</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoao-kddi/SCP">https://github.com/luoao-kddi/SCP</a></li>
<li>paper_authors: Ao Luo, Linxin Song, Keisuke Nonaka, Kyohei Unno, Heming Sun, Masayuki Goto, Jiro Katto</li>
<li>for: 该论文主要探讨了一种基于圆柱坐标系的学习点云压缩方法（SCP），以便全面利用点云中各种圆形特征和方向强相关性，提高压缩率和重建质量。</li>
<li>methods: 该方法基于点云中的圆柱坐标系，并采用多层 Octree 结构来降低远区域重建误差。具有universal性，可应用于多种学习点云压缩技术。</li>
<li>results: 实验结果显示，SCP 方法可以与前期state-of-the-art方法比肩，并且在点对点 PSNR BD-Rate 指标上达到29.14%的提高。<details>
<summary>Abstract</summary>
In recent years, the task of learned point cloud compression has gained prominence. An important type of point cloud, the spinning LiDAR point cloud, is generated by spinning LiDAR on vehicles. This process results in numerous circular shapes and azimuthal angle invariance features within the point clouds. However, these two features have been largely overlooked by previous methodologies. In this paper, we introduce a model-agnostic method called Spherical-Coordinate-based learned Point cloud compression (SCP), designed to leverage the aforementioned features fully. Additionally, we propose a multi-level Octree for SCP to mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree. SCP exhibits excellent universality, making it applicable to various learned point cloud compression techniques. Experimental results demonstrate that SCP surpasses previous state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate.
</details>
<details>
<summary>摘要</summary>
近年来，学习点云压缩任务已经受到重视。一种重要的点云数据类型是旋转 LiDAR 点云，通常由旋转 LiDAR 设备在车辆上生成。这个过程会生成很多圆形和方位角度不变特征，但这些特征在之前的方法中受到了忽略。在本文中，我们提出了一种无模型的方法called Spherical-Coordinate-based learned Point cloud compression (SCP)，旨在完全利用上述特征。此外，我们还提议了一种多级 Octree 来 Mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree。SCP 具有优秀的通用性，可以应用于多种学习点云压缩技术。实验结果表明，SCP 可以与前一代方法相比提高点-to-点 PSNR BD-Rate 值达29.14%。
</details></li>
</ul>
<hr>
<h2 id="FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution"><a href="#FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution" class="headerlink" title="FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution"></a>FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12508">http://arxiv.org/abs/2308.12508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyue Jiao, Chongke Bi, Lu Yang</li>
<li>for: 本研究旨在提出一种Feature-Enhanced Implicit Neural Representation（FFEINR）方法，用于精度高的流体场数据超分辨。</li>
<li>methods: 该方法基于卷积神经网络（CNN）和生成敌对网络（GAN），并通过增强输入层的特征来支持流体场数据的精度高超分辨。</li>
<li>results: 实验结果表明，FFEINR方法比较常用的三次 interpolate方法得到更好的结果。<details>
<summary>Abstract</summary>
Large-scale numerical simulations are capable of generating data up to terabytes or even petabytes. As a promising method of data reduction, super-resolution (SR) has been widely studied in the scientific visualization community. However, most of them are based on deep convolutional neural networks (CNNs) or generative adversarial networks (GANs) and the scale factor needs to be determined before constructing the network. As a result, a single training session only supports a fixed factor and has poor generalization ability. To address these problems, this paper proposes a Feature-Enhanced Implicit Neural Representation (FFEINR) for spatio-temporal super-resolution of flow field data. It can take full advantage of the implicit neural representation in terms of model structure and sampling resolution. The neural representation is based on a fully connected network with periodic activation functions, which enables us to obtain lightweight models. The learned continuous representation can decode the low-resolution flow field input data to arbitrary spatial and temporal resolutions, allowing for flexible upsampling. The training process of FFEINR is facilitated by introducing feature enhancements for the input layer, which complements the contextual information of the flow field.To demonstrate the effectiveness of the proposed method, a series of experiments are conducted on different datasets by setting different hyperparameters. The results show that FFEINR achieves significantly better results than the trilinear interpolation method.
</details>
<details>
<summary>摘要</summary>
大规模数值 simulations 可以生成数据 hasta terabytes 或者 Even petabytes。作为数据压缩的承诺方法，超分辨率（SR）在科学视觉社区中得到了广泛的研究。然而，大多数都是基于深度卷积神经网络（CNN）或生成对抗网络（GAN），并且扩大因子需要在建立网络之前确定。因此，单一训练会话只支持固定因子，并且generalization能力不好。为了解决这些问题，本文提出了基于含义增强的偏微分神经表示（FFEINR），用于空间时间超分辨率流场数据。它可以在模型结构和采样分辨率上取得全面利用。神经表示基于完全连接网络，使得模型变得轻量级。学习的连续表示可以将低分辨率输入数据解码到任意空间和时间分辨率，以便灵活的上扩。训练过程中，我们引入了输入层的特征增强，以便补充流场的Contextual信息。为了证明提案的有效性，我们在不同的数据集上进行了一系列实验，并设置了不同的超参数。结果表明，FFEINR可以与三元 interpolate 方法相比，获得显著更好的结果。
</details></li>
</ul>
<hr>
<h2 id="MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices"><a href="#MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices" class="headerlink" title="MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices"></a>MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12494">http://arxiv.org/abs/2308.12494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Chen, Ruiwen Zhen, Shuai Li, Xiaotian Li, Guanghui Wang<br>for:这篇论文主要针对的是提高图像恢复模型在移动设备上的效率，以便在资源有限的情况下实现高质量的图像恢复。methods:该论文提出了一种方法，包括在不敏感层添加更多参数，然后使用部分深度卷积和分离卷积&#x2F;下采样层来加速模型速度。results:对多个图像恢复 datasets 进行了广泛的实验，发现该方法可以降低运行时间，同时提高 PSNR 和 SSIM。具体来说，运行时间可以降低到最多 13%，参数数量可以减少到最多 23%，而 PSNR 和 SSIM 则可以提高。代码源代码可以在 \href{<a target="_blank" rel="noopener" href="https://github.com/xiangyu8/MOFA%7D%7Bhttps://github.com/xiangyu8/MOFA%7D">https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}</a> 上找到。<details>
<summary>Abstract</summary>
Image restoration aims to restore high-quality images from degraded counterparts and has seen significant advancements through deep learning techniques. The technique has been widely applied to mobile devices for tasks such as mobile photography. Given the resource limitations on mobile devices, such as memory constraints and runtime requirements, the efficiency of models during deployment becomes paramount. Nevertheless, most previous works have primarily concentrated on analyzing the efficiency of single modules and improving them individually. This paper examines the efficiency across different layers. We propose a roadmap that can be applied to further accelerate image restoration models prior to deployment while simultaneously increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). The roadmap first increases the model capacity by adding more parameters to partial convolutions on FLOPs non-sensitive layers. Then, it applies partial depthwise convolution coupled with decoupling upsampling/downsampling layers to accelerate the model speed. Extensive experiments demonstrate that our approach decreases runtime by up to 13% and reduces the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets. Source Code of our method is available at \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model"><a href="#InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model" class="headerlink" title="InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model"></a>InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12465">http://arxiv.org/abs/2308.12465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biomedai-ucsc/inversesr">https://github.com/biomedai-ucsc/inversesr</a></li>
<li>paper_authors: Jueqi Wang, Jacob Levman, Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, M. Jorge Cardoso, Razvan Marinescu</li>
<li>for: 提高低分辨率（LR）磁共振成像（MRI）图像的分辨率。</li>
<li>methods: 利用一个状态最佳的3D脑生成模型——潜在扩散模型（LDM）训练在UK BioBank上，以提高低分辨率磁共振成像图像的分辨率。</li>
<li>results: 验证了将LDM作为生成模型，可以帮助提高低分辨率磁共振成像图像的分辨率。<details>
<summary>Abstract</summary>
High-resolution (HR) MRI scans obtained from research-grade medical centers provide precise information about imaged tissues. However, routine clinical MRI scans are typically in low-resolution (LR) and vary greatly in contrast and spatial resolution due to the adjustments of the scanning parameters to the local needs of the medical center. End-to-end deep learning methods for MRI super-resolution (SR) have been proposed, but they require re-training each time there is a shift in the input distribution. To address this issue, we propose a novel approach that leverages a state-of-the-art 3D brain generative model, the latent diffusion model (LDM) trained on UK BioBank, to increase the resolution of clinical MRI scans. The LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction.
</details>
<details>
<summary>摘要</summary>
高分辨率（HR）MRI扫描从研究级医疗机构获得的数据提供了精确的组织信息。然而，日常临床MRI扫描通常是低分辨率（LR），并且因为扫描参数的调整而具有不同的对比度和空间分辨率。为解决这个问题，我们提议了一种新的方法，利用了UK BioBank上训练的状态艺术3D脑生成模型（LDM），以提高临床MRI扫描的分辨率。LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction.
</details></li>
</ul>
<hr>
<h2 id="HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration"><a href="#HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration" class="headerlink" title="HNAS-reg: hierarchical neural architecture search for deformable medical image registration"></a>HNAS-reg: hierarchical neural architecture search for deformable medical image registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12440">http://arxiv.org/abs/2308.12440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiong Wu, Yong Fan</li>
<li>for: 这篇论文旨在找到最佳的深度学习模型，用于静止医疗影像注册。</li>
<li>methods: 这篇论文使用了一个层次 NAS 框架（HNAS-Reg），包括了 convolutional 操作搜索和网络架构搜索，以找到最佳的网络架构。实际上，这篇论文还使用了一个 partial channel 策略，以减少计算负担和内存限制。</li>
<li>results: 实验结果显示，提案方法可以建立一个具有改善医疗影像注册精度和减少模型大小的深度学习模型，较比一个传统方法和两个无监督学习方法来得好。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been widely used to build deep learning models for medical image registration, but manually designed network architectures are not necessarily optimal. This paper presents a hierarchical NAS framework (HNAS-Reg), consisting of both convolutional operation search and network topology search, to identify the optimal network architecture for deformable medical image registration. To mitigate the computational overhead and memory constraints, a partial channel strategy is utilized without losing optimization quality. Experiments on three datasets, consisting of 636 T1-weighted magnetic resonance images (MRIs), have demonstrated that the proposal method can build a deep learning model with improved image registration accuracy and reduced model size, compared with state-of-the-art image registration approaches, including one representative traditional approach and two unsupervised learning-based approaches.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗影像注册领域广泛使用深度学习模型，但是人工设计的网络架构可能并不是最佳的。这篇论文提出了一种层次 NAS 框架（HNAS-Reg），包括卷积操作搜索和网络架构搜索，以便确定最佳的网络架构 для 可变的医疗影像注册。为了减少计算负担和内存限制，该方法使用了部分通道策略而不失去优化质量。实验在三个数据集上，包括 636 个 T1 束缚 magnetic resonance imaging（MRI）图像，表明该方法可以建立一个具有改进的图像注册精度和减少的模型大小的深度学习模型，比于现有的图像注册方法，包括一种传统方法和两种无监督学习方法。
</details></li>
</ul>
<hr>
<h2 id="Reframing-the-Brain-Age-Prediction-Problem-to-a-More-Interpretable-and-Quantitative-Approach"><a href="#Reframing-the-Brain-Age-Prediction-Problem-to-a-More-Interpretable-and-Quantitative-Approach" class="headerlink" title="Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach"></a>Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12416">http://arxiv.org/abs/2308.12416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Gianchandani, Mahsa Dibaji, Mariana Bento, Ethan MacDonald, Roberto Souza</li>
<li>for: 这研究的目的是使用深度学习模型从核磁共振成像图像中预测大脑年龄，并提供更加可解的结果。</li>
<li>methods: 该研究使用了一种image-to-image regression方法，将大脑年龄预测问题转化为每个大脑细胞voxel的预测问题，并与全局预测模型和相关的涉及度地图进行比较。</li>
<li>results: 结果表明，voxel-wise预测模型比全局预测模型更加可解，因为它们提供了脑部年龄变化的空间信息，并且具有量化的优点。<details>
<summary>Abstract</summary>
Deep learning models have achieved state-of-the-art results in estimating brain age, which is an important brain health biomarker, from magnetic resonance (MR) images. However, most of these models only provide a global age prediction, and rely on techniques, such as saliency maps to interpret their results. These saliency maps highlight regions in the input image that were significant for the model's predictions, but they are hard to be interpreted, and saliency map values are not directly comparable across different samples. In this work, we reframe the age prediction problem from MR images to an image-to-image regression problem where we estimate the brain age for each brain voxel in MR images. We compare voxel-wise age prediction models against global age prediction models and their corresponding saliency maps. The results indicate that voxel-wise age prediction models are more interpretable, since they provide spatial information about the brain aging process, and they benefit from being quantitative.
</details>
<details>
<summary>摘要</summary>
深度学习模型已经达到了评估大脑年龄的国际标准Result，这是重要的大脑健康指标，从核磁共振（MR）图像中获取。然而，大多数这些模型只提供全局年龄预测，并且使用技术，如吸引力地图来解释其结果。这些吸引力地图会高亮输入图像中对模型预测有影响的区域，但它们很难被解释，而且吸引力地图值不可比较。在这项工作中，我们将MR图像中大脑年龄预测问题重新定义为图像到图像回归问题，我们预测每个大脑磁共振 voxel 的年龄。我们比较了 voxel-wise 年龄预测模型和全局年龄预测模型以及其相应的吸引力地图。结果表明，voxel-wise 年龄预测模型更加可读性高，因为它们提供了空间信息关于大脑年龄过程，并且它们受益于量化。
</details></li>
</ul>
<hr>
<h2 id="SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation"><a href="#SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation" class="headerlink" title="SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation"></a>SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12231">http://arxiv.org/abs/2308.12231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xq141839/sppnet">https://github.com/xq141839/sppnet</a></li>
<li>paper_authors: Qing Xu, Wenwei Kuang, Zeyu Zhang, Xueyao Bao, Haoran Chen, Wenting Duan</li>
<li>for: 本研究旨在提出一种单点提示网络（SPPNet），用于核体图像分割。</li>
<li>methods: 我们将原始图像Encoder被替换为轻量级视transformer，并添加了一个有效的 convolutional block，以提高图像下降的semantic信息。我们还提出了基于 Gaussian kernel的新的点抽象方法。</li>
<li>results: 我们在MoNuSeg-2018 dataset上评估了SPPNet，结果表明它在比较较快的训练速度和较低的计算成本下，可以达到比较高的性能水平。相比之下，SPPNet比segment anything模型快了约20倍，仅需要一个点集，这更加合理 для临床应用。<details>
<summary>Abstract</summary>
Image segmentation plays an essential role in nuclei image analysis. Recently, the segment anything model has made a significant breakthrough in such tasks. However, the current model exists two major issues for cell segmentation: (1) the image encoder of the segment anything model involves a large number of parameters. Retraining or even fine-tuning the model still requires expensive computational resources. (2) in point prompt mode, points are sampled from the center of the ground truth and more than one set of points is expected to achieve reliable performance, which is not efficient for practical applications. In this paper, a single-point prompt network is proposed for nuclei image segmentation, called SPPNet. We replace the original image encoder with a lightweight vision transformer. Also, an effective convolutional block is added in parallel to extract the low-level semantic information from the image and compensate for the performance degradation due to the small image encoder. We propose a new point-sampling method based on the Gaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset. The result demonstrated that SPPNet outperforms existing U-shape architectures and shows faster convergence in training. Compared to the segment anything model, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and computational cost. Particularly, only one set of points is required in both the training and inference phases, which is more reasonable for clinical applications. The code for our work and more technical details can be found at https://github.com/xq141839/SPPNet.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/eess.IV_2023_08_24/" data-id="cllurrpcz00eosw88b4ab5eax" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/24/eess.AS_2023_08_24/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          eess.AS - 2023-08-24
        
      </div>
    </a>
  
  
    <a href="/2023/08/23/cs.AI_2023_08_23/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.AI - 2023-08-23 20:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
