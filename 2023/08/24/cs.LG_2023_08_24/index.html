
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>cs.LG - 2023-08-24 18:00:00 | Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Easy attention: A simple self-attention mechanism for Transformers paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12874 repo_url: None paper_authors: Marcial Sanchis-Agudo, Yuning Wang, Karthik Duraisamy, Ricar">
<meta property="og:type" content="article">
<meta property="og:title" content="cs.LG - 2023-08-24 18:00:00">
<meta property="og:url" content="https://nullscc.github.io/2023/08/24/cs.LG_2023_08_24/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:description" content="Easy attention: A simple self-attention mechanism for Transformers paper_url: http:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2308.12874 repo_url: None paper_authors: Marcial Sanchis-Agudo, Yuning Wang, Karthik Duraisamy, Ricar">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-08-23T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-27T10:02:36.914Z">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main"><article id="post-cs.LG_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.LG_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      cs.LG - 2023-08-24 18:00:00
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Easy-attention-A-simple-self-attention-mechanism-for-Transformers"><a href="#Easy-attention-A-simple-self-attention-mechanism-for-Transformers" class="headerlink" title="Easy attention: A simple self-attention mechanism for Transformers"></a>Easy attention: A simple self-attention mechanism for Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12874">http://arxiv.org/abs/2308.12874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcial Sanchis-Agudo, Yuning Wang, Karthik Duraisamy, Ricardo Vinuesa</li>
<li>for: 提高逻辑神经网络对时间动态预测的稳定性，我们提出了一种新的注意力机制called easy attention。</li>
<li>methods: 我们的方法通过对softmax注意力分数进行特征值分解（SVD），从而发现self注意力在 span 空间中压缩了查询和KEYS的贡献。因此，我们直接将注意力分数作为学习参数来处理。</li>
<li>results: 我们的方法在重建和预测时间动态系统中表现出色，比self注意力或广泛使用的长短期记忆（LSTM）网络更加稳定、更加简单。我们的结果表明这种方法在更复杂的高维动力系统中具有潜在的应用前景。<details>
<summary>Abstract</summary>
To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention. Due to the fact that self attention only makes usage of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through implementing singular-value decomposition (SVD) on the softmax attention score, we further observe that the self attention compresses contribution from both queries and keys in the spanned space of the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than the self attention or the widely-used long short-term memory (LSTM) network. Our results show great potential for applications in more complex high-dimensional dynamical systems.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:为了提高转换器神经网络在时间序列预测中的稳定性，我们提出了一种新的注意力机制，即“容易注意”机制。这种机制不需要使用自注意的内积、 queries 和 softmax，它们通常用于捕捉时间序列中的长期依赖关系。而我们通过对 softmax 注意score 的 singular-value decomposition (SVD)，发现自注意对 queries 和 keys 在扩展空间中的贡献被压缩。因此，我们直接将注意分数视为学习参数，这种方法在预测和重建混沌系统的时间动力学中表现出色，比自注意或通用的长期记忆网络 (LSTM) 更加稳定和简单。我们的结果表明，这种方法在更复杂的高维动力系统中具有潜在的应用前景。
</details></li>
</ul>
<hr>
<h2 id="IPA-Inference-Pipeline-Adaptation-to-Achieve-High-Accuracy-and-Cost-Efficiency"><a href="#IPA-Inference-Pipeline-Adaptation-to-Achieve-High-Accuracy-and-Cost-Efficiency" class="headerlink" title="IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency"></a>IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12871">http://arxiv.org/abs/2308.12871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid Ghafouri, Kamran Razavi, Mehran Salmani, Alireza Sanaee, Tania Lorido-Botran, Lin Wang, Joseph Doyle, Pooyan Jamshidi</li>
<li>for: 提高 ML 生产系统中的快速、准确和成本效果的推理管线优化，以满足紧张的终端延迟要求。</li>
<li>methods: 使用 Integer Programming 方法 dynamically 配置批处理大小、复制和模型变体，以优化准确率、降低成本并遵循用户定义的延迟 SLA。</li>
<li>results: 对实际推理管线中的五个实际应用进行了广泛的实验，结果表明，IPA 可以提高 норма化准确率达到 35%，而成本增加仅为 less than 5%。<details>
<summary>Abstract</summary>
Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in ML production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of accuracy and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling accuracy and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep-learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency SLAs using Integer Programming. It supports multi-objective settings for achieving different trade-offs between accuracy and cost objectives while remaining adaptable to varying workloads and dynamic traffic patterns. Extensive experiments on a Kubernetes implementation with five real-world inference pipelines demonstrate that IPA improves normalized accuracy by up to 35% with a minimal cost increase of less than 5%.
</details>
<details>
<summary>摘要</summary>
实现快速、准确且成本效益的多模型推论管线协调是机器学习生产系统中的挑战，因为它们具有紧密的终端径准确性要求。为了简化对准确和成本费用之间的复杂贸易空间的探索，提供者通常会选择忽略其中一个。但是，这个挑战在整合准确和成本费用之间的调整上还是一个挑战。为了解决这个挑战并提出一个解决方案，我们提出了IPA，一个基于深度学习的推论管线自适应系统。IPA通过动态配置批次大小、重复和模型Variant来优化准确性、降低成本和遵循用户定义的径准确性SLAs，使用整数程式设计。它支持多目标设定，以实现不同的准确和成本目标之间的变化，同时保持可靠的运算和变化的工作负载模式。实验结果显示，IPA可以将Normalized准确性提高至35%，并且仅增加成本少于5%。
</details></li>
</ul>
<hr>
<h2 id="Auto-weighted-Bayesian-Physics-Informed-Neural-Networks-and-robust-estimations-for-multitask-inverse-problems-in-pore-scale-imaging-of-dissolution"><a href="#Auto-weighted-Bayesian-Physics-Informed-Neural-Networks-and-robust-estimations-for-multitask-inverse-problems-in-pore-scale-imaging-of-dissolution" class="headerlink" title="Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution"></a>Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12864">http://arxiv.org/abs/2308.12864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Perez, Philippe Poncet<br>for:The paper presents a novel data assimilation strategy for pore-scale imaging to address reactive inverse problems incorporating Uncertainty Quantification (UQ).methods:The method combines data-driven and physics-informed techniques, including sequential reinforcement, Bayesian Physics-Informed Neural Networks (BPINNs), and adaptive weighting to ensure robust and unbiased uncertainty quantification.results:The method is demonstrated to be successful in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT images, providing reliable micro-porosity changes during geochemical transformations and quantifying morphological uncertainties on the porosity field.<details>
<summary>Abstract</summary>
In this article, we present a novel data assimilation strategy in pore-scale imaging and demonstrate that this makes it possible to robustly address reactive inverse problems incorporating Uncertainty Quantification (UQ). Pore-scale modeling of reactive flow offers a valuable opportunity to investigate the evolution of macro-scale properties subject to dynamic processes. Yet, they suffer from imaging limitations arising from the associated X-ray microtomography (X-ray microCT) process, which induces discrepancies in the properties estimates. Assessment of the kinetic parameters also raises challenges, as reactive coefficients are critical parameters that can cover a wide range of values. We account for these two issues and ensure reliable calibration of pore-scale modeling, based on dynamical microCT images, by integrating uncertainty quantification in the workflow.   The present method is based on a multitasking formulation of reactive inverse problems combining data-driven and physics-informed techniques in calcite dissolution. This allows quantifying morphological uncertainties on the porosity field and estimating reactive parameter ranges through prescribed PDE models with a latent concentration field and dynamical microCT. The data assimilation strategy relies on sequential reinforcement incorporating successively additional PDE constraints. We guarantee robust and unbiased uncertainty quantification by straightforward adaptive weighting of Bayesian Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity changes during geochemical transformations. We demonstrate successful Bayesian Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT images with meaningful posterior distribution on the reactive parameters and dimensionless numbers.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们提出了一种新的数据融合策略，用于 Addressing Reactive Inverse Problems  incorporating Uncertainty Quantification (UQ)。孔径级模拟涉及的激发过程可以帮助我们研究宏观质量特性的演化。然而，X射微 Tomatoes 的制图过程会导致属性估计偏差，而且评估激发系数也存在挑战，因为激发系数范围很广。我们解决了这两个问题，并确保了精确的孔径级模拟calibration，基于动态微 Tomatoes 图像，通过结合不确定性评估和物理学 inform 技术。我们的方法基于多任务形式的激发反问题，结合数据驱动和物理学 inform 技术，用于评估孔径级模拟中的不确定性。我们采用了顺序强化法，包括逐渐添加 PDE 约束。我们 garantía 不偏不倚的不确定性评估，通过简单的自适应权重 bayesian physics-informed neural networks (BPINNs)，以确保可靠的微порosity 变化 durante geochemical transformations。我们在1D+Time和2D+Time calcite dissolution中成功进行 Bayesian Inference，并获得了有意义的 posterior distribution on the reactive parameters and dimensionless numbers。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automated-Animal-Density-Estimation-with-Acoustic-Spatial-Capture-Recapture"><a href="#Towards-Automated-Animal-Density-Estimation-with-Acoustic-Spatial-Capture-Recapture" class="headerlink" title="Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture"></a>Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12859">http://arxiv.org/abs/2308.12859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuheng Wang, Juan Ye, David L. Borchers</li>
<li>for: 监测野生动物人口，特别是难以视见的动物，可以通过投影式监测获得大量数据，但是需要采用机器学习（ML）方法进行识别。</li>
<li>methods: 我们提出了三种方法，即混合模型、隐藏状态模型和隐藏变量模型，这些方法可以将机器学习的不确定性integrated into inference。</li>
<li>results: 我们通过模拟测试发现，在基于海南小猩猩的声学数据中，忽略假阳性会导致17%的正确率偏高，而我们的方法则具有几乎零偏高和95%的正确率。<details>
<summary>Abstract</summary>
Passive acoustic monitoring can be an effective way of monitoring wildlife populations that are acoustically active but difficult to survey visually. Digital recorders allow surveyors to gather large volumes of data at low cost, but identifying target species vocalisations in these data is non-trivial. Machine learning (ML) methods are often used to do the identification. They can process large volumes of data quickly, but they do not detect all vocalisations and they do generate some false positives (vocalisations that are not from the target species). Existing wildlife abundance survey methods have been designed specifically to deal with the first of these mistakes, but current methods of dealing with false positives are not well-developed. They do not take account of features of individual vocalisations, some of which are more likely to be false positives than others. We propose three methods for acoustic spatial capture-recapture inference that integrate individual-level measures of confidence from ML vocalisation identification into the likelihood and hence integrate ML uncertainty into inference. The methods include a mixture model in which species identity is a latent variable. We test the methods by simulation and find that in a scenario based on acoustic data from Hainan gibbons, in which ignoring false positives results in 17% positive bias, our methods give negligible bias and coverage probabilities that are close to the nominal 95% level.
</details>
<details>
<summary>摘要</summary>
（注：以下是简化中文版本）通过激光监测，可以有效地监测难以视见的野生动物 populations，但是 Machine Learning（ML）方法可能会导致一些假阳性结果（不是目标种的叫声）。现有的野生动物数量评估方法已经特地处理了第一种错误，但是处理假阳性结果的方法并不完善。它们不会考虑具体的叫声特征，一些叫声更容易被误分为假阳性。我们提出了三种基于 ML 声音特征置信度的声音空间捕捉-再次捕捉方法，并将这些方法与现有的方法进行比较。这些方法包括一种混合模型，其中种类标识是隐藏变量。我们通过模拟测试，发现在基于海南 Gibbon 的声音数据场景中，忽略假阳性结果会导致正确率为 17% 的偏好，而我们的方法则具有几乎零偏好和95% 的覆盖 probabilities。
</details></li>
</ul>
<hr>
<h2 id="Fast-Adversarial-Training-with-Smooth-Convergence"><a href="#Fast-Adversarial-Training-with-Smooth-Convergence" class="headerlink" title="Fast Adversarial Training with Smooth Convergence"></a>Fast Adversarial Training with Smooth Convergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12857">http://arxiv.org/abs/2308.12857</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fat-cs/convergesmooth">https://github.com/fat-cs/convergesmooth</a></li>
<li>paper_authors: Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin</li>
<li>for: 提高神经网络的攻击鲁棒性。</li>
<li>methods: 提出了一种新的振荡约束（ConvergeSmooth），用于限制前一个级别的损失差和当前级别的损失差之间的差异，以保证损失的涨幂平滑。同时，提出了一种新的权重中心化策略，不需要添加额外的参数。</li>
<li>results: 在各种流行的数据集上进行了广泛的实验，发现提出的方法可以减少极端欠拟合问题，并比前一代FAT技术表现更好。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/FAT-CS/ConvergeSmooth%7D">https://github.com/FAT-CS/ConvergeSmooth}</a> 上获取。<details>
<summary>Abstract</summary>
Fast adversarial training (FAT) is beneficial for improving the adversarial robustness of neural networks. However, previous FAT work has encountered a significant issue known as catastrophic overfitting when dealing with large perturbation budgets, \ie the adversarial robustness of models declines to near zero during training.   To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers.   Therefore, we argue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting.   To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparameters other than the loss balance coefficient.   Our proposed methods are attack-agnostic and thus can improve the training stability of various FAT techniques.   Extensive experiments on popular datasets show that the proposed methods efficiently avoid catastrophic overfitting and outperform all previous FAT methods. Code is available at \url{https://github.com/FAT-CS/ConvergeSmooth}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Probabilistic-load-forecasting-with-Reservoir-Computing"><a href="#Probabilistic-load-forecasting-with-Reservoir-Computing" class="headerlink" title="Probabilistic load forecasting with Reservoir Computing"></a>Probabilistic load forecasting with Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12844">http://arxiv.org/abs/2308.12844</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MicheleUIT/Probabilistic-load-forecasting-with-Reservoir-Computing">https://github.com/MicheleUIT/Probabilistic-load-forecasting-with-Reservoir-Computing</a></li>
<li>paper_authors: Michele Guerra, Simone Scardapane, Filippo Maria Bianchi</li>
<li>for: 预测电力负荷的准确性和不确定性评估</li>
<li>methods: 使用托管 computing 作为时间序列预测方法，并评估不确定性评估方法的可行性、计算资源效率和可靠性</li>
<li>results: 研究发现，使用托管 computing 和不确定性评估方法可以提高预测结果的准确性和可靠性，同时减少计算资源的消耗<details>
<summary>Abstract</summary>
Some applications of deep learning require not only to provide accurate results but also to quantify the amount of confidence in their prediction. The management of an electric power grid is one of these cases: to avoid risky scenarios, decision-makers need both precise and reliable forecasts of, for example, power loads. For this reason, point forecasts are not enough hence it is necessary to adopt methods that provide an uncertainty quantification.   This work focuses on reservoir computing as the core time series forecasting method, due to its computational efficiency and effectiveness in predicting time series. While the RC literature mostly focused on point forecasting, this work explores the compatibility of some popular uncertainty quantification methods with the reservoir setting. Both Bayesian and deterministic approaches to uncertainty assessment are evaluated and compared in terms of their prediction accuracy, computational resource efficiency and reliability of the estimated uncertainty, based on a set of carefully chosen performance metrics.
</details>
<details>
<summary>摘要</summary>
某些深度学习应用需要不仅提供准确的结果，还需要评估预测结果的信度。电力系统管理是这些情况之一：以避免危险场景，决策者需要精准、可靠地预测电力负荷。为此，点预测不足，需要采用能够评估预测结果的不确定性方法。本工作选择了储池计算作为核心时间序列预测方法，因为它的计算效率高并能有效预测时间序列。而 RC 文献主要关注点预测，这里的工作探讨了储池设置下的不确定性评估方法的Compatibility。本文对 uncertainty quantification 方法进行了评估和比较，包括 Bayesian 和deterministic 方法，并根据选择的性能指标进行了对比。
</details></li>
</ul>
<hr>
<h2 id="Actuator-Trajectory-Planning-for-UAVs-with-Overhead-Manipulator-using-Reinforcement-Learning"><a href="#Actuator-Trajectory-Planning-for-UAVs-with-Overhead-Manipulator-using-Reinforcement-Learning" class="headerlink" title="Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning"></a>Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12843">http://arxiv.org/abs/2308.12843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazim Alzorgan, Abolfazl Razi, Ata Jahangir Moshayedi</li>
<li>for: 这种 aerial manipulator system 是用于实现 actuation tasks 在飞行中的，例如高空焊接、结构监测和维护、电池换装、排障清理、高层建筑清洁和维护等。</li>
<li>methods: 这种系统使用 Q-learning 方法控制末端机械的 trajectory，并使用 Time To Collision (TTC) 模型来让 quadrotor UAV 环境中绕过障碍物，保证 manipulate 器的可达性。</li>
<li>results: 这种控制方法可以在各种具有飞行和抓取功能的环境中实现 actuation tasks，并且可以快速学习和适应不确定的 UAV 运动。在 15,000 集 episodes 中，Q-learning 方法可以达到 92% 的准确率（即目标轨迹和实际轨迹之间的平均距离）。<details>
<summary>Abstract</summary>
In this paper, we investigate the operation of an aerial manipulator system, namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm with two degrees of freedom to carry out actuation tasks on the fly. Our solution is based on employing a Q-learning method to control the trajectory of the tip of the arm, also called \textit{end-effector}. More specifically, we develop a motion planning model based on Time To Collision (TTC), which enables a quadrotor UAV to navigate around obstacles while ensuring the manipulator's reachability. Additionally, we utilize a model-based Q-learning model to independently track and control the desired trajectory of the manipulator's end-effector, given an arbitrary baseline trajectory for the UAV platform. Such a combination enables a variety of actuation tasks such as high-altitude welding, structural monitoring and repair, battery replacement, gutter cleaning, sky scrapper cleaning, and power line maintenance in hard-to-reach and risky environments while retaining compatibility with flight control firmware. Our RL-based control mechanism results in a robust control strategy that can handle uncertainties in the motion of the UAV, offering promising performance. Specifically, our method achieves 92\% accuracy in terms of average displacement error (i.e. the mean distance between the target and obtained trajectory points) using Q-learning with 15,000 episodes
</details>
<details>
<summary>摘要</summary>
本文研究了一种天空 manipulate系统，即一架有控制的机械臂的无人飞行器（UAV），用于实现飞行中 actuation 任务。我们的解决方案基于 employing Q-learning 方法控制 manipulate 系统的 trajectory，具体来说是控制 manipulate 系统的端效器（end-effector）的运动轨迹。我们开发了基于 Time To Collision（TTC）的动态规划模型，使得四旋翼 UAV 可以在避免碰撞的情况下环绕障碍物移动。此外，我们利用模型基于 Q-learning 模型独立跟踪和控制 manipulate 系统的端效器的desired trajectory，给出了一个arbitrary baseline trajectory для UAV 平台。这种组合使得 manipulate 系统可以完成高空焊接、结构监测和修理、电池更换、尖顶扫除、高层建筑清洁、电缆维护等多种 actuation 任务，而且保持与飞行控制 firmware 的兼容性。我们的RL-based control mechanism 实现了一种可靠的控制策略，可以处理 UAV 运动中的不确定性，提供了promising performance。具体来说，我们的方法在15,000个episode中达到了92%的均方差误差率（即target和实际轨迹点之间的平均距离）使用 Q-learning。
</details></li>
</ul>
<hr>
<h2 id="Short-Run-Transit-Route-Planning-Decision-Support-System-Using-a-Deep-Learning-Based-Weighted-Graph"><a href="#Short-Run-Transit-Route-Planning-Decision-Support-System-Using-a-Deep-Learning-Based-Weighted-Graph" class="headerlink" title="Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph"></a>Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12828">http://arxiv.org/abs/2308.12828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadav Shalit, Michael Fire, Dima Kagan, Eran Ben-Elia</li>
<li>for: 提高公共交通服务的效率和可靠性，协助公共交通观察员快速地找到短程改善。</li>
<li>methods: 使用深度学习方法，利用多种数据来源（如GTFS和智能卡数据）提取特征，建立交通网络 graphs，并通过自我监督进行预测。</li>
<li>results: 透过评估在 Tel Aviv 的应用，能够降低更多于 9% 的路线时间，包括城市内部和郊区路线，实现更高效的公共交通服务。<details>
<summary>Abstract</summary>
Public transport routing plays a crucial role in transit network design, ensuring a satisfactory level of service for passengers. However, current routing solutions rely on traditional operational research heuristics, which can be time-consuming to implement and lack the ability to provide quick solutions. Here, we propose a novel deep learning-based methodology for a decision support system that enables public transport (PT) planners to identify short-term route improvements rapidly. By seamlessly adjusting specific sections of routes between two stops during specific times of the day, our method effectively reduces times and enhances PT services. Leveraging diverse data sources such as GTFS and smart card data, we extract features and model the transportation network as a directed graph. Using self-supervision, we train a deep learning model for predicting lateness values for road segments.   These lateness values are then utilized as edge weights in the transportation graph, enabling efficient path searching. Through evaluating the method on Tel Aviv, we are able to reduce times on more than 9\% of the routes. The improved routes included both intraurban and suburban routes showcasing a fact highlighting the model's versatility. The findings emphasize the potential of our data-driven decision support system to enhance public transport and city logistics, promoting greater efficiency and reliability in PT services.
</details>
<details>
<summary>摘要</summary>
公共交通路径规划在城市交通网络设计中发挥关键作用，为乘客提供满意的服务水平。然而，当前的路径解决方案通常基于传统的运筹学方法，可能需要很长时间来实施并且无法提供快速的解决方案。在这里，我们提出了一种基于深度学习的决策支持系统，可以帮助公共交通（PT）规划员在短时间内迅速地改善路径。通过在特定时间段和特定路段上轻松调整路径，我们的方法可以减少时间并提高PT服务质量。通过利用不同的数据源，such as GTFS和智能卡数据，我们提取特征并模型了交通网络为指向图。使用无监督学习，我们训练了深度学习模型，以预测路段延迟值。这些延迟值然后被用作交通图的边权重，使得 PATH 搜索更加高效。通过对特拉维夫进行评估，我们可以减少路径时间超过 9%。改进的路径包括了城市内部和郊区路径，这一结果表明模型的 universality。发现表明了我们数据驱动的决策支持系统的潜力，以提高公共交通和城市物流的效率和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Prediction-without-Preclusion-Recourse-Verification-with-Reachable-Sets"><a href="#Prediction-without-Preclusion-Recourse-Verification-with-Reachable-Sets" class="headerlink" title="Prediction without Preclusion: Recourse Verification with Reachable Sets"></a>Prediction without Preclusion: Recourse Verification with Reachable Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12820">http://arxiv.org/abs/2308.12820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avni Kothari, Bogdan Kulynych, Tsui-Wei Weng, Berk Ustun</li>
<li>for: 这篇论文是关于机器学习模型如何决定提供借款、面试或公共援助的。</li>
<li>methods: 这篇论文使用了一种正式的测试过程来检测模型是否可以提供“回退”（recourse），并开发了一种机制来确定模型是否可以在用户指定的行动可行性约束下提供回退。</li>
<li>results: 研究人员使用了这种方法来检测真实世界数据集中的模型是否可以提供回退，并发现了一些模型可能会不可逆地分配预测，从而永久地排除用户访问借款、面试或援助的机会。<details>
<summary>Abstract</summary>
Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar access, and we provide tools to design algorithms that account for actionability when developing models.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar access, and we provide tools to design algorithms that account for actionability when developing models.Translated into Simplified Chinese:机器学习模型经常用于决定谁会获得贷款、面试或公共援助。标准的建模技术会忽略人们的行动能力。因此，模型可能会分配 fixes 的预测，这意味着被拒绝的消费者可能会被永久排除出访问借贷、就业或援助的机会。在这项工作中，我们引入了一种正式的测试过程，用于检测模型是否可以提供回退（recourse），我们称之为回退验证。我们开发了一套可靠地确定给定模型是否可以通过用户指定的行动能力约束来提供回退。我们示例了我们的工具可以在实际数据集中确保回退和对抗强度，并使用它们来研究实际借贷数据集中回退的不可能性。我们的结果表明模型可能会不慎地分配 fixes 的预测，永久排除访问权限，我们提供了工具来设计考虑行动能力的算法。
</details></li>
</ul>
<hr>
<h2 id="Job-Shop-Scheduling-Benchmark-Environments-and-Instances-for-Learning-and-Non-learning-Methods"><a href="#Job-Shop-Scheduling-Benchmark-Environments-and-Instances-for-Learning-and-Non-learning-Methods" class="headerlink" title="Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods"></a>Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12794">http://arxiv.org/abs/2308.12794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-for-decision-making-tue/job_shop_scheduling_benchmark">https://github.com/ai-for-decision-making-tue/job_shop_scheduling_benchmark</a></li>
<li>paper_authors: Robbert Reijnen, Kjell van Straaten, Zaharah Bukhsh, Yingqian Zhang</li>
<li>for: 提供一个开源的GitHub存储库，包含各种机器调度问题的完整 benchmark，包括Job Shop Scheduling (JSP)、Flow Shop Scheduling (FSP)、Flexible Job Shop Scheduling (FJSP)、FAJSP with Assembly constraints (FAJSP)、FJSP with Sequence-Dependent Setup Times (FJSP-SDST)、在线 FJSP (with online job arrivals)。</li>
<li>methods: 提供一个中心化的平台，供研究者、实践者和爱好者一起解决机器调度问题。</li>
<li>results: 提供一个完整的 benchmark，可以帮助研究者和实践者更好地了解不同机器调度问题的特点和挑战。<details>
<summary>Abstract</summary>
We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling (FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our primary goal is to provide a centralized hub for researchers, practitioners, and enthusiasts interested in tackling machine scheduling challenges.
</details>
<details>
<summary>摘要</summary>
我们介绍一个开源的GitHub存储库，包含了广泛的机器安排问题的测试，包括作业shop scheduling (JSP)、流行shop scheduling (FSP)、可动作job shop scheduling (FJSP)、FJSP具有组装约束 (FAJSP)、FJSP具有时间相依的组装 (FJSP-SDST)，以及在线的FJSP。我们的主要目标是提供一个中央集中地 для研究人员、实践者和热爱者，来解决机器安排挑战。
</details></li>
</ul>
<hr>
<h2 id="Single-shot-Bayesian-approximation-for-neural-networks"><a href="#Single-shot-Bayesian-approximation-for-neural-networks" class="headerlink" title="Single-shot Bayesian approximation for neural networks"></a>Single-shot Bayesian approximation for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12785">http://arxiv.org/abs/2308.12785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaibrach/Moment-Propagation">https://github.com/kaibrach/Moment-Propagation</a></li>
<li>paper_authors: Kai Brach, Beate Sick, Oliver Dürr</li>
<li>for: 提高深度神经网络（NN）的预测性能和不确定性评估。</li>
<li>methods: 使用蒙те Carlo（MC）抽取和粒子抽取来提供不确定性度量，并同时提高预测性能。</li>
<li>results: 提出了一种单次MC抽取近似方法，可以快速地对常用的NN层（如卷积层、最大池化层、稠密层、softmax层和抽取层）进行预测，并且可以Analytically approximate MC抽取信号的期望值和方差。这种方法可以将标准抽取采用的NN转换成BNN，无需重新训练。在不同的 benchmark 数据集和一个模拟的玩具示例中进行了评估，并证明了这种单次MC抽取近似方法可以快速地提供预测分布的点估计和不确定度评估，而且可以在实时部署BNN时使用。此外，我们还证明了将部分时间用于将MP方法与深度ensemble技术相结合可以进一步改善不确定度评估。<details>
<summary>Abstract</summary>
Deep neural networks (NNs) are known for their high-prediction performances. However, NNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provide uncertainty measures and simultaneously increase the prediction performance. The only disadvantage of BNNs is their higher computation time during test time because they rely on a sampling approach. Here we present a single-shot MC dropout approximation that preserves the advantages of BNNs while being as fast as NNs. Our approach is based on moment propagation (MP) and allows to analytically approximate the expected value and the variance of the MC dropout signal for commonly used layers in NNs, i.e. convolution, max pooling, dense, softmax, and dropout layers. The MP approach can convert an NN into a BNN without re-training given the NN has been trained with standard dropout. We evaluate our approach on different benchmark datasets and a simulated toy example in a classification and regression setting. We demonstrate that our single-shot MC dropout approximation resembles the point estimate and the uncertainty estimate of the predictive distribution that is achieved with an MC approach, while being fast enough for real-time deployments of BNNs. We show that using part of the saved time to combine our MP approach with deep ensemble techniques does further improve the uncertainty measures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Intentionally-underestimated-Value-Function-at-Terminal-State-for-Temporal-difference-Learning-with-Mis-designed-Reward"><a href="#Intentionally-underestimated-Value-Function-at-Terminal-State-for-Temporal-difference-Learning-with-Mis-designed-Reward" class="headerlink" title="Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward"></a>Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12772">http://arxiv.org/abs/2308.12772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taisuke Kobayashi</li>
<li>for: 解决TD学习在终端时的问题，即在终端时强制设置值为零，导致不INTENTIONAL的过高估计或者下降估计，具体来说是在任务失败时导致高估计，Acquire wrong policy。</li>
<li>methods: 提出一种方法，INTENTIONALLY underestimate the value after termination to avoid learning failures due to unintentional overestimation。同时，根据终端状态的度适应性来调整度量下降的程度，以避免过度探索。</li>
<li>results: 通过实验和实际机器人实验，显示该方法可以稳定地获得多种任务和奖励设计的优化策略。<details>
<summary>Abstract</summary>
Robot control using reinforcement learning has become popular, but its learning process generally terminates halfway through an episode for safety and time-saving reasons. This study addresses the problem of the most popular exception handling that temporal-difference (TD) learning performs at such termination. That is, by forcibly assuming zero value after termination, unintentionally implicit underestimation or overestimation occurs, depending on the reward design in the normal states. When the episode is terminated due to task failure, the failure may be highly valued with the unintentional overestimation, and the wrong policy may be acquired. Although this problem can be avoided by paying attention to the reward design, it is essential in practical use of TD learning to review the exception handling at termination. This paper therefore proposes a method to intentionally underestimate the value after termination to avoid learning failures due to the unintentional overestimation. In addition, the degree of underestimation is adjusted according to the degree of stationarity at termination, thereby preventing excessive exploration due to the intentional underestimation. Simulations and real robot experiments showed that the proposed method can stably obtain the optimal policies for various tasks and reward designs. https://youtu.be/AxXr8uFOe7M
</details>
<details>
<summary>摘要</summary>
机器人控制使用反做学习已经广泛应用，但其学习过程通常在一个 episoden 中中止，以保证安全和时间效率。这个研究解决了反做学习在中止时的最常见问题，即TD学习在中止时对未完成任务的执行进行强制设置为零值，导致不INTENDED 的下降或上涨。当 episoden 中止由任务失败引起时，失败可能具有不INTENDED 的高估，并且可能获得错误的策略。虽然这个问题可以通过奖励设计的注意来避免，但在TD学习的实际应用中，需要 periodic 检查中止时的例外处理。这篇论文因此提出了一种将中止时强制设置为零值，以避免由不INTENDED 的高估导致的学习失败。此外，根据中止时的稳定性度进行调整，以避免由意外估计导致的过度探索。实验和实际机器人实验表明，提议的方法可以稳定地获得多种任务和奖励设计的优化策略。
</details></li>
</ul>
<hr>
<h2 id="On-the-Consistency-of-Average-Embeddings-for-Item-Recommendation"><a href="#On-the-Consistency-of-Average-Embeddings-for-Item-Recommendation" class="headerlink" title="On the Consistency of Average Embeddings for Item Recommendation"></a>On the Consistency of Average Embeddings for Item Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12767">http://arxiv.org/abs/2308.12767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deezer/consistency">https://github.com/deezer/consistency</a></li>
<li>paper_authors: Walid Bendada, Guillaume Salha-Galvan, Romain Hennequin, Thomas Bouabça, Tristan Cazenave</li>
<li>for: 这篇论文 investigate了权重平均法的合理性，它是一种常见的推荐系统方法。</li>
<li>methods: 作者提出了一个预期准确度分数，用于衡量权重平均的一致性。然后，他们分析了这个分数的数学表达，以及其在实际数据上的实验行为。</li>
<li>results: 研究发现，实际的权重平均不太一致于对应的理论设定，这提供了未来研究的可能性。<details>
<summary>Abstract</summary>
A prevalent practice in recommender systems consists of averaging item embeddings to represent users or higher-level concepts in the same embedding space. This paper investigates the relevance of such a practice. For this purpose, we propose an expected precision score, designed to measure the consistency of an average embedding relative to the items used for its construction. We subsequently analyze the mathematical expression of this score in a theoretical setting with specific assumptions, as well as its empirical behavior on real-world data from music streaming services. Our results emphasize that real-world averages are less consistent for recommendation, which paves the way for future research to better align real-world embeddings with assumptions from our theoretical setting.
</details>
<details>
<summary>摘要</summary>
一种常见的做法在推荐系统中是将item embedding平均化以表示用户或更高层次的概念在同一个嵌入空间中。这篇论文检查了这种做法的相关性。为此，我们提出了一个预期精度分数，用于衡量average embedding的一致性 relative to the items used for its construction。我们随后对这个分数的数学表达在具体的假设下进行了分析，以及其在实际数据上的实际行为。我们的结果表明，实际中的均值更不一致，这为未来的研究提供了更好地将实际嵌入与假设中的嵌入相一致的可能性。
</details></li>
</ul>
<hr>
<h2 id="IP-UNet-Intensity-Projection-UNet-Architecture-for-3D-Medical-Volume-Segmentation"><a href="#IP-UNet-Intensity-Projection-UNet-Architecture-for-3D-Medical-Volume-Segmentation" class="headerlink" title="IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation"></a>IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12761">http://arxiv.org/abs/2308.12761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nyothiri Aung, Tahar Kechadi, Liming Chen, Sahraoui Dhelim</li>
<li>For: The paper is written for proposing an end-to-end deep learning approach called IP-UNet for multi-class segmentation of 3D volumetric data, specifically for automatic breast calcification detection.* Methods: The paper uses a UNet-based model that operates on Intensity Projection (IP) of 3D volumetric data, which reduces the memory consumption and maintains the original image resolution. The model is compared with two other methods: 2D UNet model for slice-by-slice segmentation and 3D-UNet model for direct segmentation of 3D volumes.* Results: The experimental results show that IP-UNet achieves similar segmentation accuracy as 3D-UNet but with much better performance in terms of training time and memory consumption. Specifically, IP-UNet reduces the training time by 70% and memory consumption by 92%.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了提出一种终端深度学习方法，即IP-UNet，用于多类分割三维数据集，特别是自动乳腺细胞检测。* Methods: 该论文使用基于UNet的模型，将三维数据集转化为强度投影(IP)，从而降低内存使用量并保持原始图像分辨率。模型与两种其他方法进行比较：2D UNet模型进行 slice-by-slice分割，以及直接使用3D-UNet模型进行3D分割。* Results: 实验结果表明，IP-UNet可以与3D-UNet准确率相似，但性能更好，具体来说，它降低了训练时间70%，内存使用量92%。<details>
<summary>Abstract</summary>
CNNs have been widely applied for medical image analysis. However, limited memory capacity is one of the most common drawbacks of processing high-resolution 3D volumetric data. 3D volumes are usually cropped or downsized first before processing, which can result in a loss of resolution, increase class imbalance, and affect the performance of the segmentation algorithms. In this paper, we propose an end-to-end deep learning approach called IP-UNet. IP-UNet is a UNet-based model that performs multi-class segmentation on Intensity Projection (IP) of 3D volumetric data instead of the memory-consuming 3D volumes. IP-UNet uses limited memory capability for training without losing the original 3D image resolution. We compare the performance of three models in terms of segmentation accuracy and computational cost: 1) Slice-by-slice 2D segmentation of the CT scan images using a conventional 2D UNet model. 2) IP-UNet that operates on data obtained by merging the extracted Maximum Intensity Projection (MIP), Closest Vessel Projection (CVP), and Average Intensity Projection (AvgIP) representations of the source 3D volumes, then applying the UNet model on the output IP images. 3) 3D-UNet model directly reads the 3D volumes constructed from a series of CT scan images and outputs the 3D volume of the predicted segmentation. We test the performance of these methods on 3D volumetric images for automatic breast calcification detection. Experimental results show that IP-Unet can achieve similar segmentation accuracy with 3D-Unet but with much better performance. It reduces the training time by 70\% and memory consumption by 92\%.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗图像分析中广泛应用。然而，处理高分辨率三维数据的内存容量有限是最常见的缺点。通常会将三维数据进行裁剪或压缩，从而导致分辨率下降、类别不均衡加大和分割算法性能下降。在这篇论文中，我们提出了一种终端深度学习方法，即IP-UNet。IP-UNet是基于UNet模型的终端深度学习方法，对三维图像的强化投影（IP）进行多类分割，而不是耗费内存的三维数据。IP-UNet在训练中不失去原始三维图像的分辨率，并且具有有限的内存 capacidad。我们将三种模型进行比较，分别是：1）通过切割CT扫描图像的层次进行二维分割，使用传统的二维UNet模型进行分割。2）IP-UNet，它在提取的最大强化投影（MIP）、最近血管投影（CVP）和均衡强化投影（AvgIP）表示中提取数据，然后应用UNet模型对输出的IP图像进行分割。3）直接使用CT扫描图像序列构建的三维体volume，使用3D-UNet模型进行分割。我们对这些方法在三维体积图像自动乳腺病变检测中的性能进行测试。实验结果表明，IP-UNet可以与3D-UNet具有相同的分割精度，但是具有训练时间减少70%和内存消耗减少92%的优势。
</details></li>
</ul>
<hr>
<h2 id="Motion-In-Betweening-with-Phase-Manifolds"><a href="#Motion-In-Betweening-with-Phase-Manifolds" class="headerlink" title="Motion In-Betweening with Phase Manifolds"></a>Motion In-Betweening with Phase Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12751">http://arxiv.org/abs/2308.12751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pauzii/phasebetweener">https://github.com/pauzii/phasebetweener</a></li>
<li>paper_authors: Paul Starke, Sebastian Starke, Taku Komura, Frank Steinicke</li>
<li>for: 这种研究旨在开发一种基于数据驱动的人物动作 interpolating 系统，以达到目标pose的character。</li>
<li>methods: 这种方法使用Periodic Autoencoder学习的阶段变量，并使用权重的混合 neural network 模型，以生成在当前和目标状态之间的动作序列。在满足特定的制定pose或特定的终端器的约束下，还实现了学习控制方案。</li>
<li>results: 结果表明，使用阶段变量进行动作 interpolating 可以增强 interpolated 运动的精度，并且可以在长transition duration 下稳定学习过程。此外，这种方法还可以synthesize 更加复杂的运动，以及在给定的目标帧之间实现样式控制。与当前状态艺技相比，这种方法可以具有类似的运动质量和泛化能力，尤其是在存在长transition duration 时。<details>
<summary>Abstract</summary>
This paper introduces a novel data-driven motion in-betweening system to reach target poses of characters by making use of phases variables learned by a Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network model, in which the phases cluster movements in both space and time with different expert weights. Each generated set of weights then produces a sequence of poses in an autoregressive manner between the current and target state of the character. In addition, to satisfy poses which are manually modified by the animators or where certain end effectors serve as constraints to be reached by the animation, a learned bi-directional control scheme is implemented to satisfy such constraints. The results demonstrate that using phases for motion in-betweening tasks sharpen the interpolated movements, and furthermore stabilizes the learning process. Moreover, using phases for motion in-betweening tasks can also synthesize more challenging movements beyond locomotion behaviors. Additionally, style control is enabled between given target keyframes. Our proposed framework can compete with popular state-of-the-art methods for motion in-betweening in terms of motion quality and generalization, especially in the existence of long transition durations. Our framework contributes to faster prototyping workflows for creating animated character sequences, which is of enormous interest for the game and film industry.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Human-Comprehensible-Active-Learning-of-Genome-Scale-Metabolic-Networks"><a href="#Human-Comprehensible-Active-Learning-of-Genome-Scale-Metabolic-Networks" class="headerlink" title="Human Comprehensible Active Learning of Genome-Scale Metabolic Networks"></a>Human Comprehensible Active Learning of Genome-Scale Metabolic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12740">http://arxiv.org/abs/2308.12740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lun Ai, Shi-Shun Liang, Wang-Zhou Dai, Liam Hallett, Stephen H. Muggleton, Geoff S. Baldwin</li>
<li>for: 这项研究旨在提高生物工程的HOST Cell系统设计、建立和测试过程中的效率和成本。</li>
<li>methods: 该研究使用了一种新的机器学习框架ILP-iML1515，基于推理逻辑编程（ILP），实现了推理逻辑推理和从训练示例学习。</li>
<li>results: ILP-iML1515能够高效地进行大规模 simulations，并可以通过学习新的逻辑结构来更新模型，从而降低了学习基因功能的实验成本。<details>
<summary>Abstract</summary>
An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that reduce the experimental cost of learning gene functions in comparison to randomly selected experiments.
</details>
<details>
<summary>摘要</summary>
Important applications of synthetic biology include engineering host cell systems to produce useful products. However, as the scale of the host system increases, the design space grows exponentially, requiring a large number of validation trials with high experimental costs. To address this challenge, we need a machine learning approach that efficiently explores the hypothesis space and guides experimental design.We propose a novel machine learning framework called ILP-iML1515 based on inductive logic programming (ILP). This framework performs abductive logical reasoning and actively learns from training examples. Unlike numerical models, ILP-iML1515 uses comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials.The ILP-iML1515 framework offers two key advantages:1. High-throughput simulations: The framework allows for high-throughput simulations, enabling the rapid exploration of the design space.2. Experiment selection: The framework actively selects experiments that reduce the experimental cost of learning gene functions, compared to randomly selected experiments.In summary, ILP-iML1515 is a novel machine learning framework that efficiently explores the hypothesis space and guides experimental design for the design-build-test-learn (DBTL) cycle of host cell systems, reducing the experimental cost of learning gene functions.
</details></li>
</ul>
<hr>
<h2 id="Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion"><a href="#Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion" class="headerlink" title="Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion"></a>Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12734">http://arxiv.org/abs/2308.12734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan J. Bird, Ahmad Lotfi</li>
<li>for: 本研究旨在 Addressing the growing ethical concerns surrounding generative AI in the speech domain, particularly DeepFake Voice Conversion.</li>
<li>methods: 研究使用了 Retrieval-based Voice Conversion 技术，生成了 DEEP-VOICE 数据集，包含 eight well-known figures 的真实人类语音和他们之间的转换语音。将 speech 分类为真实语音和 AI-generated 语音，并通过 Statistical analysis of temporal audio features 进行分析，发现了两者之间存在显著不同的分布。</li>
<li>results: 研究人员使用 Hyperparameter optimisation 进行机器学习模型的训练，并实现了一个 Extreme Gradient Boosting 模型，可以在10-fold cross validation中达到平均分类精度为99.3%，并在约0.004毫秒内分类语音。所有数据生成于本研究都公开发布，供未来研究人员进行 AI speech detection 的研究。<details>
<summary>Abstract</summary>
There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learning models over 10-fold cross validation, it is found that the Extreme Gradient Boosting model can achieve an average classification accuracy of 99.3% and can classify speech in real-time, at around 0.004 milliseconds given one second of speech. All data generated for this study is released publicly for future research on AI speech detection.
</details>
<details>
<summary>摘要</summary>
“Currently, there are growing implications surrounding generative AI in the speech domain, which enables voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation. Therefore, there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address these emerging issues, the DEEP-VOICE dataset was generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. The problem is presented as a binary classification issue, with statistical analysis of temporal audio features through t-testing revealing significantly different distributions. Hyperparameter optimization is implemented for machine learning models to identify the source of speech. After training 208 individual machine learning models over 10-fold cross validation, it was found that the Extreme Gradient Boosting model can achieve an average classification accuracy of 99.3% and can classify speech in real-time, at around 0.004 milliseconds given one second of speech. All data generated for this study is released publicly for future research on AI speech detection.”Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Out-of-the-Box-Thinking-Improving-Customer-Lifetime-Value-Modelling-via-Expert-Routing-and-Game-Whale-Detection"><a href="#Out-of-the-Box-Thinking-Improving-Customer-Lifetime-Value-Modelling-via-Expert-Routing-and-Game-Whale-Detection" class="headerlink" title="Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection"></a>Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12729">http://arxiv.org/abs/2308.12729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Zhang, Xin Yan, Xuejiao Yang, Binfeng Jia, Shuangyang Wang</li>
<li>For: 这种研究旨在提高手机游戏发布者的用户生命周期预测（LTV）和游戏鲨鱼检测（Game Whale Detection）的准确性。* Methods: 该研究提出了一种名为ExpLTV的多任务框架，通过将游戏鲨鱼检测作为门控网络，可以融合共享信息和场景特定信息，以提高LTV预测的准确性。* Results: 经过广泛的实验 validate，ExpLTV在三个industrial dataset上显示出了明显的超越性。<details>
<summary>Abstract</summary>
Customer lifetime value (LTV) prediction is essential for mobile game publishers trying to optimize the advertising investment for each user acquisition based on the estimated worth. In mobile games, deploying microtransactions is a simple yet effective monetization strategy, which attracts a tiny group of game whales who splurge on in-game purchases. The presence of such game whales may impede the practicality of existing LTV prediction models, since game whales' purchase behaviours always exhibit varied distribution from general users. Consequently, identifying game whales can open up new opportunities to improve the accuracy of LTV prediction models. However, little attention has been paid to applying game whale detection in LTV prediction, and existing works are mainly specialized for the long-term LTV prediction with the assumption that the high-quality user features are available, which is not applicable in the UA stage. In this paper, we propose ExpLTV, a novel multi-task framework to perform LTV prediction and game whale detection in a unified way. In ExpLTV, we first innovatively design a deep neural network-based game whale detector that can not only infer the intrinsic order in accordance with monetary value, but also precisely identify high spenders (i.e., game whales) and low spenders. Then, by treating the game whale detector as a gating network to decide the different mixture patterns of LTV experts assembling, we can thoroughly leverage the shared information and scenario-specific information (i.e., game whales modelling and low spenders modelling). Finally, instead of separately designing a purchase rate estimator for two tasks, we design a shared estimator that can preserve the inner task relationships. The superiority of ExpLTV is further validated via extensive experiments on three industrial datasets.
</details>
<details>
<summary>摘要</summary>
顾客全生值预测（LTV）是移动游戏发布商必须优化每个用户获取的广告投资的关键，根据估计的值来进行优化。在移动游戏中，实施微交易是一种简单而有效的营收化渠道，吸引了一些游戏巨鲸，这些巨鲸在游戏中购买各种内购。巨鲸的购买行为可能会妨碍现有的LTV预测模型的实用性，因为巨鲸的购买行为总是与普通用户的购买行为存在差异。因此，识别巨鲸可以开启新的机会，以提高LTV预测模型的准确性。然而，目前对游戏巨鲸检测的应用在LTV预测中受到了少量的关注，现有的工作主要是长期LTV预测，假设高质量的用户特征可以获取，这不适用于UA阶段。在本文中，我们提出了ExpLTV，一种新的多任务框架，用于同时进行LTV预测和游戏巨鲸检测。在ExpLTV中，我们首先创新地设计了一个深度神经网络基于的游戏巨鲸检测器，可以不只是根据财务价值来INFER内在顺序，还可以精准地识别高支付者（即游戏巨鲸）和低支付者。然后，我们将游戏巨鲸检测器作为LTV预测器的闭合网络，以便全面利用共享信息和场景特定信息（即游戏巨鲸模型和低支付者模型）。最后，相反于分别设计两个任务的购买率估计器，我们设计了共享的估计器，以保持内任务之间的关系。ExpLTV的优势得到了EXTENSIVE的实验 validate，在三个industrial dataset上。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Reinforcement-Learning-based-Dynamic-Difficulty-Adjustment-in-a-Visual-Working-Memory-Game"><a href="#Continuous-Reinforcement-Learning-based-Dynamic-Difficulty-Adjustment-in-a-Visual-Working-Memory-Game" class="headerlink" title="Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game"></a>Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12726">http://arxiv.org/abs/2308.12726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoud Rahimi, Hadi Moradi, Abdol-hossein Vahabie, Hamed Kebriaei</li>
<li>for: 提高玩家的游戏体验 (enhance the player’s experience in video games)</li>
<li>methods: 使用可观察学习 (RL) 方法，并在非竞争性游戏中应用连续状态动作空间 (continuous state-action space)</li>
<li>results: 提出了一种基于RL的可观察学习改进游戏难度调整方法，该方法可以根据玩家的得分和上一次游戏难度来调整游戏难度，从而提高玩家的游戏体验和得分。通过对52名参与者进行 Within-subject experiment 进行评估，该方法与两种基于规则的难度调整方法进行比较，并表明该方法可以提供更好的游戏体验、更高的得分和更高的胜率，同时也可以避免在20个尝试中的得分下降。<details>
<summary>Abstract</summary>
Dynamic Difficulty Adjustment (DDA) is a viable approach to enhance a player's experience in video games. Recently, Reinforcement Learning (RL) methods have been employed for DDA in non-competitive games; nevertheless, they rely solely on discrete state-action space with a small search space. In this paper, we propose a continuous RL-based DDA methodology for a visual working memory (VWM) game to handle the complex search space for the difficulty of memorization. The proposed RL-based DDA tailors game difficulty based on the player's score and game difficulty in the last trial. We defined a continuous metric for the difficulty of memorization. Then, we consider the task difficulty and the vector of difficulty-score as the RL's action and state, respectively. We evaluated the proposed method through a within-subject experiment involving 52 subjects. The proposed approach was compared with two rule-based difficulty adjustment methods in terms of player's score and game experience measured by a questionnaire. The proposed RL-based approach resulted in a significantly better game experience in terms of competence, tension, and negative and positive affect. Players also achieved higher scores and win rates. Furthermore, the proposed RL-based DDA led to a significantly less decline in the score in a 20-trial session.
</details>
<details>
<summary>摘要</summary>
“智能困难调整”（DDA）是一种可以增强玩家体验的游戏技术。近期，人工智能学习（RL）方法已经应用于非竞争性游戏中的 DDA，但是它们仅将状态动作空间划分为简单的类别。在这篇论文中，我们提出了一个基于RL的绘谱记忆游戏中的连续RL-DDA方法。这个方法根据玩家的得分和上一次游戏的难度来调整游戏的困难度。我们定义了一个连续的记忆难度度量，然后将任务难度和难度得分作为RL的动作和状态。我们透过对52名参与者进行在内体验研究来评估这个方法。该方法与两种基于规则的困难调整方法进行比较，并且根据玩家的得分和游戏体验（Questionnaire）进行评估。结果显示，提案的RL-DDA方法可以提供更好的游戏体验，包括能力感、紧张感、正面和负面情感。玩家也取得了更高的得分和胜率。此外，RL-DDA还导致游戏20次的得分下降较少。”
</details></li>
</ul>
<hr>
<h2 id="Solving-Forward-and-Inverse-Problems-of-Contact-Mechanics-using-Physics-Informed-Neural-Networks"><a href="#Solving-Forward-and-Inverse-Problems-of-Contact-Mechanics-using-Physics-Informed-Neural-Networks" class="headerlink" title="Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks"></a>Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12716">http://arxiv.org/abs/2308.12716</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. Sahin, M. von Danwitz, A. Popp</li>
<li>for: 本文使用物理学 Informed Neural Networks (PINNs) 解决了小塑性弹性理论中的前向和反向问题。</li>
<li>methods: 本文使用了混合变量的 PINNs 形式，通过输出变换来强制执行 Dirichlet 和 Neumann 边界条件作为硬件约束。 具有 KKT 类型条件的不等约束被 incorporated 到网络训练中的损失函数中，以便在训练过程中作为软约束。</li>
<li>results: 本文表明了 PINNs 可以作为纯 PDE 解决器，数据增强的前向模型，参数透析的 inverse 解决方案，以及快速评估的代理模型。 此外，本文还证明了选择合适的 hyperparameter，如损失权重，以及组合 Adam 和 L-BFGS-B 优化器可以提高准确性和训练时间。<details>
<summary>Abstract</summary>
This paper explores the ability of physics-informed neural networks (PINNs) to solve forward and inverse problems of contact mechanics for small deformation elasticity. We deploy PINNs in a mixed-variable formulation enhanced by output transformation to enforce Dirichlet and Neumann boundary conditions as hard constraints. Inequality constraints of contact problems, namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft constraints by incorporating them into the loss function during network training. To formulate the loss function contribution of KKT constraints, existing approaches applied to elastoplasticity problems are investigated and we explore a nonlinear complementarity problem (NCP) function, namely Fischer-Burmeister, which possesses advantageous characteristics in terms of optimization. Based on the Hertzian contact problem, we show that PINNs can serve as pure partial differential equation (PDE) solver, as data-enhanced forward model, as inverse solver for parameter identification, and as fast-to-evaluate surrogate model. Furthermore, we demonstrate the importance of choosing proper hyperparameters, e.g. loss weights, and a combination of Adam and L-BFGS-B optimizers aiming for better results in terms of accuracy and training time.
</details>
<details>
<summary>摘要</summary>
To formulate the loss function contribution of KKT constraints, we investigate existing approaches applied to elastoplasticity problems and explore a nonlinear complementarity problem (NCP) function, namely Fischer-Burmeister, which has advantageous characteristics in terms of optimization.Based on the Hertzian contact problem, we show that PINNs can serve as pure partial differential equation (PDE) solvers, as data-enhanced forward models, as inverse solvers for parameter identification, and as fast-to-evaluate surrogate models. Furthermore, we demonstrate the importance of choosing proper hyperparameters, such as loss weights, and a combination of Adam and L-BFGS-B optimizers to achieve better results in terms of accuracy and training time.
</details></li>
</ul>
<hr>
<h2 id="Disentanglement-Learning-via-Topology"><a href="#Disentanglement-Learning-via-Topology" class="headerlink" title="Disentanglement Learning via Topology"></a>Disentanglement Learning via Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12696">http://arxiv.org/abs/2308.12696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Balabin, Daria Voronkova, Ilya Trofimov, Evgeny Burnaev, Serguei Barannikov</li>
<li>for: 本研究的目的是提出一种基于多尺度拓扑特征的分解表示学习方法，以实现数据的解释性和深度学习模型的稳定性。</li>
<li>methods: 本方法基于VAE的架构，通过添加多尺度拓扑损失项来实现分解表示。我们从数据集的拓扑特征角度分析数据集的特征，并且通过优化拓扑相似性来优化分解表示。我们是第一个提出了可导的拓扑损失的论文。</li>
<li>results: 我们的实验表明，提出的拓扑损失可以提高分解表示的评价指标，如MIG、FactorVAE分数、SAP分数和DCI分解分数，与现有的最佳Result比较。此外，我们还示出了如何使用提出的拓扑损失来找到已经训练的GAN中的分解方向。<details>
<summary>Abstract</summary>
We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised manner, permitting to apply it for problems without labeled factors of variation. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN.
</details>
<details>
<summary>摘要</summary>
我们提出了TopDis（拓扑分解）方法，通过添加多尺度拓扑损失项来学习分解表示。分解是深度学习模型的解释性和鲁棒性的重要属性，也是高级认知的一步。现状的方法是基于VAE（变量Autoencoder）的总 correlate损失。我们从数据集的拓扑性质来分析数据集的分解性。特别是，我们优化了数据集的拓扑相似性。到目前为止，我们的文章是第一篇提出了可导的拓扑损失的分解方法。我们的实验表明，我们的方法可以在不带标签因素的情况下提高分解分数，包括MIG、FactorVAE分数、SAP分数和DCI分解分数。我们的方法是无监督的，可以应用于无标签因素的问题。此外，我们还展示了如何使用我们的拓扑损失来找分解方向在训练过的GAN中。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Data-Analysis-Method-for-Big-Data-using-Multiple-Model-Linear-Regression"><a href="#An-Efficient-Data-Analysis-Method-for-Big-Data-using-Multiple-Model-Linear-Regression" class="headerlink" title="An Efficient Data Analysis Method for Big Data using Multiple-Model Linear Regression"></a>An Efficient Data Analysis Method for Big Data using Multiple-Model Linear Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12691">http://arxiv.org/abs/2308.12691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Bohan Lyu, Jianzhong Li</li>
<li>for: 这篇论文提出了一种新的大数据分析方法，使用新定义的多模型线性回归（MMLR）模型，可以将输入数据集分解成子集并建立本地线性回归模型。</li>
<li>methods: 该论文提出了一种新的approx算法，基于($\epsilon$, $\delta$)-估计器，用于构建MMLR模型。论文还提供了对MMLR算法的数学证明，证明其时间复杂度为输入数据集的线性。</li>
<li>results: 该论文通过实验表明，MMLR算法在许多情况下与现有回归方法相当，而且它的运行时间相对较短。<details>
<summary>Abstract</summary>
This paper introduces a new data analysis method for big data using a newly defined regression model named multiple model linear regression(MMLR), which separates input datasets into subsets and construct local linear regression models of them. The proposed data analysis method is shown to be more efficient and flexible than other regression based methods. This paper also proposes an approximate algorithm to construct MMLR models based on $(\epsilon,\delta)$-estimator, and gives mathematical proofs of the correctness and efficiency of MMLR algorithm, of which the time complexity is linear with respect to the size of input datasets. This paper also empirically implements the method on both synthetic and real-world datasets, the algorithm shows to have comparable performance to existing regression methods in many cases, while it takes almost the shortest time to provide a high prediction accuracy.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的大数据分析方法，使用新定义的多模型线性回归（MMLR）模型，将输入数据集分解成子集并建立当地线性回归模型。提议的数据分析方法比其他回归基于方法更高效和灵活。这篇论文还提出了一种近似算法来构建MMLR模型，基于($\epsilon$, $\delta$)-估计器，并提供了数学证明MMLR算法的正确性和效率。时间复杂度为输入数据集的线性。此外，论文还通过实验证明了方法在 sintetic 和实际数据上的表现，其性能与现有回归方法相当，而时间几乎最短提供高预测精度。Note: "MMLR" stands for "Multiple Model Linear Regression" in English.
</details></li>
</ul>
<hr>
<h2 id="Match-And-Deform-Time-Series-Domain-Adaptation-through-Optimal-Transport-and-Temporal-Alignment"><a href="#Match-And-Deform-Time-Series-Domain-Adaptation-through-Optimal-Transport-and-Temporal-Alignment" class="headerlink" title="Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment"></a>Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12686">http://arxiv.org/abs/2308.12686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rtavenar/MatchAndDeform">https://github.com/rtavenar/MatchAndDeform</a></li>
<li>paper_authors: François Painblanc, Laetitia Chapel, Nicolas Courty, Chloé Friguet, Charlotte Pelletier, Romain Tavenard</li>
<li>for: 这篇论文旨在适应没有标签的大量数据的情况，通过源领域中的标签来分类目标领域的数据。</li>
<li>methods: 这篇论文提出了匹配和变形（Match-And-Deform，MAD）方法，该方法在源和目标时间序中寻找匹配，同时允许时间偏移。该方法通过优化transport损失和时间扭曲来同时对时间序进行对齐。</li>
<li>results: 实验结果表明，MAD可以做到有效地对时间序进行对齐和时间偏移估计，并且可以在深度神经网络中学习新的时间序表示，同时将领域对齐和分类性能提高。<details>
<summary>Abstract</summary>
While large volumes of unlabeled data are usually available, associated labels are often scarce. The unsupervised domain adaptation problem aims at exploiting labels from a source domain to classify data from a related, yet different, target domain. When time series are at stake, new difficulties arise as temporal shifts may appear in addition to the standard feature distribution shift. In this paper, we introduce the Match-And-Deform (MAD) approach that aims at finding correspondences between the source and target time series while allowing temporal distortions. The associated optimization problem simultaneously aligns the series thanks to an optimal transport loss and the time stamps through dynamic time warping. When embedded into a deep neural network, MAD helps learning new representations of time series that both align the domains and maximize the discriminative power of the network. Empirical studies on benchmark datasets and remote sensing data demonstrate that MAD makes meaningful sample-to-sample pairing and time shift estimation, reaching similar or better classification performance than state-of-the-art deep time series domain adaptation strategies.
</details>
<details>
<summary>摘要</summary>
大量未标注数据通常可available,但相关的标签却罕见。不supervised domain adaptation问题 aimsto exploit source domain中的标签来类别target domain中的数据。当时序序列是问题时，新的困难出现，因为特征分布shift可能会出现，同时还需要解决时间推移问题。在这篇论文中，我们引入了Match-And-Deform（MAD）方法，该方法 aimsto在source和target时序序列之间找到匹配，并允许时间扭曲。相关的优化问题同时使用了最优运输损失和时间戳Dynamic Time Warping来协调时序序列。当 embed into a deep neural network时，MAD可以帮助学习新的时序序列表示，同时同时对域和网络的泛化能力做出贡献。empirical studies on benchmark datasets and remote sensing data表明，MAD可以实现meaningful sample-to-sample pairing和时间偏移估计，达到或更好的深度时序领域域 adaptation表现。
</details></li>
</ul>
<hr>
<h2 id="LR-XFL-Logical-Reasoning-based-Explainable-Federated-Learning"><a href="#LR-XFL-Logical-Reasoning-based-Explainable-Federated-Learning" class="headerlink" title="LR-XFL: Logical Reasoning-based Explainable Federated Learning"></a>LR-XFL: Logical Reasoning-based Explainable Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12681">http://arxiv.org/abs/2308.12681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanci Zhang, Han Yu</li>
<li>for: 提高 Federated Learning（FL）模型的透明度和解释性，保护数据隐私。</li>
<li>methods:  incorporating logic-based explanations into FL， clients create local logic rules based on their local data and send them to the FL server，server aggregates local model updates with weight values determined by the quality of clients’ local data as reflected by their uploaded logic rules.</li>
<li>results: LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in terms of classification accuracy, rule accuracy and rule fidelity, respectively，explicit rule evaluation and expression enable human experts to validate and correct the rules on the server side.<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients' local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in terms of classification accuracy, rule accuracy and rule fidelity, respectively. The explicit rule evaluation and expression under LR-XFL enable human experts to validate and correct the rules on the server side, hence improving the global FL model's robustness to errors. It has the potential to enhance the transparency of FL models for areas like healthcare and finance where both data privacy and explainability are important.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种emergingapproach дляtrain machine learning model collaboratively while preserving data privacy. 由于需要隐私保护，FL模型难以 achieve global transparency和 explainability. 为了解决这些limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach.在LR-XFL中, FL客户端创建本地逻辑规则 based on their local data,并将其发送到FL服务器 alongside model updates. FL服务器通过基于客户端数据的属性 derive proper logical connector,而无需访问原始数据。此外，服务器还将本地模型更新与客户端数据质量所决定的权重值相结合。结果显示，LR-XFL比最相关的基准方案高1.19%、5.81%和5.41% in terms of classification accuracy, rule accuracy和 rule fidelity, respectively. explicit rule evaluation and expression under LR-XFL enable human experts to validate and correct the rules on the server side, hence improving the global FL model's robustness to errors. it has the potential to enhance the transparency of FL models for areas like healthcare and finance where both data privacy and explainability are important.
</details></li>
</ul>
<hr>
<h2 id="Master-slave-Deep-Architecture-for-Top-K-Multi-armed-Bandits-with-Non-linear-Bandit-Feedback-and-Diversity-Constraints"><a href="#Master-slave-Deep-Architecture-for-Top-K-Multi-armed-Bandits-with-Non-linear-Bandit-Feedback-and-Diversity-Constraints" class="headerlink" title="Master-slave Deep Architecture for Top-K Multi-armed Bandits with Non-linear Bandit Feedback and Diversity Constraints"></a>Master-slave Deep Architecture for Top-K Multi-armed Bandits with Non-linear Bandit Feedback and Diversity Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12680">http://arxiv.org/abs/2308.12680</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huanghanchi/master-slave-algorithm-for-top-k-bandits">https://github.com/huanghanchi/master-slave-algorithm-for-top-k-bandits</a></li>
<li>paper_authors: Hanchi Huang, Li Shen, Deheng Ye, Wei Liu</li>
<li>for:  solving the top-$K$ combinatorial multi-armed bandits problem with non-linear bandit feedback and diversity constraints in recommendation tasks</li>
<li>methods:  introducing six slave models with distinguished merits to generate diversified samples balancing rewards and constraints, and using teacher learning based optimization and policy co-training technique to boost the performance of multiple slave models</li>
<li>results:  significantly surpassing existing state-of-the-art algorithms in both synthetic and real datasets for recommendation tasks.Here is the Chinese translation of the three key information points:</li>
<li>for: 解决 combinatorial multi-armed bandits problem 中的 top-$K$ 问题，具有非线性的飞行反馈和多样性约束，在推荐任务中。</li>
<li>methods: 引入 six 个具有独特优势的附属模型，以生成具有奖励和约束的多样化样本，并使用教师学习基于优化和策略共训技术来提高多个附属模型的性能。</li>
<li>results: 在 synthetic 和实际数据集中，与现有状态的算法相比，显著超越。<details>
<summary>Abstract</summary>
We propose a novel master-slave architecture to solve the top-$K$ combinatorial multi-armed bandits problem with non-linear bandit feedback and diversity constraints, which, to the best of our knowledge, is the first combinatorial bandits setting considering diversity constraints under bandit feedback. Specifically, to efficiently explore the combinatorial and constrained action space, we introduce six slave models with distinguished merits to generate diversified samples well balancing rewards and constraints as well as efficiency. Moreover, we propose teacher learning based optimization and the policy co-training technique to boost the performance of the multiple slave models. The master model then collects the elite samples provided by the slave models and selects the best sample estimated by a neural contextual UCB-based network to make a decision with a trade-off between exploration and exploitation. Thanks to the elaborate design of slave models, the co-training mechanism among slave models, and the novel interactions between the master and slave models, our approach significantly surpasses existing state-of-the-art algorithms in both synthetic and real datasets for recommendation tasks. The code is available at: \url{https://github.com/huanghanchi/Master-slave-Algorithm-for-Top-K-Bandits}.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的主奴隶架构，用于解决排名前-$K$  combinatorial多臂炮兵问题，具有非线性炮兵反馈和多样性约束。到目前为止，这是首次对 combinatorial bandits 设置中考虑多样性约束的研究。specifically，我们引入了六个奴隶模型，每个模型具有特殊优势，以生成多样化的样本，既保证奖励也保证约束。此外，我们提出了教师学习基于优化和策略共训技术，以提高多个奴隶模型的性能。master模型然后收集奴隶模型提供的精英样本，并通过神经网络基于上下文ual UCB 算法选择最佳样本，以实现 Explorer 和 exploiter 之间的平衡。由于奴隶模型的精心设计、奴隶模型之间的合作机制以及主奴隶模型与奴隶模型之间的新型互动，我们的方法在 synthetic 和实际数据集上对于推荐任务显著超越了现状的算法。代码可以在：\url{https://github.com/huanghanchi/Master-slave-Algorithm-for-Top-K-Bandits} 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Continual-Learning-Approach-for-Cross-Domain-White-Blood-Cell-Classification"><a href="#A-Continual-Learning-Approach-for-Cross-Domain-White-Blood-Cell-Classification" class="headerlink" title="A Continual Learning Approach for Cross-Domain White Blood Cell Classification"></a>A Continual Learning Approach for Cross-Domain White Blood Cell Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12679">http://arxiv.org/abs/2308.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Raheleh Salehi, Armin Gruber, Sayedali Shetab Boushehri, Pascal Giehr, Nassir Navab, Carsten Marr</li>
<li>for: 这篇论文旨在提出一种继续学习方法来实现白血球分类 tasks 中的持续学习，以应对在临床设定中不断改变的数据来源和疾病分类。</li>
<li>methods: 本文提出的方法是使用练习集选择法来选择 previous tasks 中最有代表性的数据，以解决继续学习时的忘却问题。</li>
<li>results: 本文的结果显示，该方法在三个不同的白血球分类 dataset 上均表现出色，并在 Cross-domain 环境中进行了持续学习。特别是在长期增量学习和跨领域学习 scenario 中，该方法均以最佳成绩超越了现有的 iCaRL 和 EWC 方法。<details>
<summary>Abstract</summary>
Accurate classification of white blood cells in peripheral blood is essential for diagnosing hematological diseases. Due to constantly evolving clinical settings, data sources, and disease classifications, it is necessary to update machine learning classification models regularly for practical real-world use. Such models significantly benefit from sequentially learning from incoming data streams without forgetting previously acquired knowledge. However, models can suffer from catastrophic forgetting, causing a drop in performance on previous tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual learning approach for class incremental and domain incremental scenarios in white blood cell classification. To choose representative samples from previous tasks, we employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model. We thoroughly evaluated our proposed approach on three white blood cell classification datasets that differ in color, resolution, and class composition, including scenarios where new domains or new classes are introduced to the model with every task. We also test a long class incremental experiment with both new domains and new classes. Our results demonstrate that our approach outperforms established baselines in continual learning, including existing iCaRL and EWC methods for classifying white blood cells in cross-domain environments.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将白血球分类在 péripheral 血液中的准确性是诊断血液疾病的关键。由于临床设置不断改变，数据源不断增加，疾病分类不断演化，因此需要定期更新机器学习分类模型以适应实际世界中的应用。这些模型受益于随着新数据流入而不断学习，而不会忘记之前学习的知识。然而，模型可能会出现慢速忘记，导致对先前任务的性能下降。在这种情况下，我们提出了一种基于熵的连续学习方法，适用于白血球分类的类增量和领域增量场景。我们使用模型预测结果来选择先前任务的表示样本。这里选择最有信心的样本和通过模型的不确定性测量得到的最具挑战性的样本。我们对三个不同的白血球分类数据集进行了严格的评估，包括分类任务中新增的领域和新类。我们还进行了长期类增量实验，其中模型需要处理新的领域和新类。我们的结果表明，我们的方法在跨领域环境中的 continual learning 中表现出色，比较了 EXISTS iCaRL 和 EWC 方法。
</details></li>
</ul>
<hr>
<h2 id="Masked-Feature-Modelling-Feature-Masking-for-the-Unsupervised-Pre-training-of-a-Graph-Attention-Network-Block-for-Bottom-up-Video-Event-Recognition"><a href="#Masked-Feature-Modelling-Feature-Masking-for-the-Unsupervised-Pre-training-of-a-Graph-Attention-Network-Block-for-Bottom-up-Video-Event-Recognition" class="headerlink" title="Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition"></a>Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12673">http://arxiv.org/abs/2308.12673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Daskalakis, Nikolaos Gkalelis, Vasileios Mezaris</li>
<li>for: 这 paper 是为了提高视频事件识别性能而设计的。</li>
<li>methods: 这 paper 使用了一种名为 Masked Feature Modelling (MFM) 的新方法，该方法利用一个预训练的视觉化トークниザー来重建视频中 объек 的遮盲特征，然后将这些特征与一个已有的底层超参 Video-Event Recognition 架构（ViGAT）结合使用，以提高模型的起点和总准确率。</li>
<li>results:  эксперименталь评估表明，MFM 可以有效地提高事件识别性能。<details>
<summary>Abstract</summary>
In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了一种新的无监督预训练方法，即Masked Feature Modelling（MFM），用于提高视频事件识别模型的起点和总体准确率。MFM使用预训练的视觉化 токен化器来重建视频中对象的遮盲特征，利用了MiniKinetics dataset。然后，我们将预训练的GAT块integrated到了state-of-the-art底层supervised视频事件识别架构ViGAT中，以提高模型的起点和总体准确率。实验评估在YLI-MED dataset上，表明MFM可以提高事件识别性能。
</details></li>
</ul>
<hr>
<h2 id="Optimal-data-pooling-for-shared-learning-in-maintenance-operations"><a href="#Optimal-data-pooling-for-shared-learning-in-maintenance-operations" class="headerlink" title="Optimal data pooling for shared learning in maintenance operations"></a>Optimal data pooling for shared learning in maintenance operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12670">http://arxiv.org/abs/2308.12670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Collin Drent, Melvin Drent, Geert-Jan van Houtum</li>
<li>for: 本研究探讨了维护操作中数据共享学习的好处。我们考虑了一个由波尔tz分布降解的系统集合，这些系统之间有一个未知的概率coupling。</li>
<li>methods: 我们提出了一种分解结果，将高维Markov决策过程（MDP）分解成两个维度的MDP。我们利用这种分解，证明数据共享可以导致成本减少，比不共享数据更便宜。</li>
<li>results: 我们的结果表明，共享数据可以减少成本，比不共享数据更便宜。这种减少的成本可以达到10%以上。<details>
<summary>Abstract</summary>
This paper addresses the benefits of pooling data for shared learning in maintenance operations. We consider a set of systems subject to Poisson degradation that are coupled through an a-priori unknown rate. Decision problems involving these systems are high-dimensional Markov decision processes (MDPs). We present a decomposition result that reduces such an MDP to two-dimensional MDPs, enabling structural analyses and computations. We leverage this decomposition to demonstrate that pooling data can lead to significant cost reductions compared to not pooling.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文研究了维护操作中数据集合的好处。我们考虑了一个系统集合，这些系统都是受到Poisson衰减的，并且通过一个未知的速率相互连接。决策问题 involving这些系统是高维Markov决策过程（MDP）。我们提出了一个分解结果，将这个MDP分解成两个维度的MDP，使得结构分析和计算变得可能。我们利用这个分解，示出了将数据集合起来可以比不集合更大的成本减少。
</details></li>
</ul>
<hr>
<h2 id="Geodesic-Mode-Connectivity"><a href="#Geodesic-Mode-Connectivity" class="headerlink" title="Geodesic Mode Connectivity"></a>Geodesic Mode Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12666">http://arxiv.org/abs/2308.12666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/char-tan/geodesic-mode-connectivity">https://github.com/char-tan/geodesic-mode-connectivity</a></li>
<li>paper_authors: Charlie Tan, Theodore Long, Sarah Zhao, Rudolf Laine</li>
<li>for: 研究模型连接性，即训练模型之间可以通过低损失连接。</li>
<li>methods: 使用信息几何学方法研究神经网络 Parametric 分布空间的弯曲结构，并提出使用最短路径（geodesics）来实现模型连接性。</li>
<li>results: 提出了一种算法来近似地odesics，并证明其可以实现模型连接性。<details>
<summary>Abstract</summary>
Mode connectivity is a phenomenon where trained models are connected by a path of low loss. We reframe this in the context of Information Geometry, where neural networks are studied as spaces of parameterized distributions with curved geometry. We hypothesize that shortest paths in these spaces, known as geodesics, correspond to mode-connecting paths in the loss landscape. We propose an algorithm to approximate geodesics and demonstrate that they achieve mode connectivity.
</details>
<details>
<summary>摘要</summary>
模式连接性是一种现象，训练过的模型被连接成一条低损的路径。我们将这种现象重新划分为信息几何学的视角，将神经网络看作参数化分布空间的弯曲geometry。我们假设低损路径在这些空间中对应于模式连接的路径，并提出了一种算法来近似低损路径。我们证明了这种算法可以实现模式连接。
</details></li>
</ul>
<hr>
<h2 id="Don’t-Look-into-the-Sun-Adversarial-Solarization-Attacks-on-Image-Classifiers"><a href="#Don’t-Look-into-the-Sun-Adversarial-Solarization-Attacks-on-Image-Classifiers" class="headerlink" title="Don’t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers"></a>Don’t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12661">http://arxiv.org/abs/2308.12661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paulgavrikov/adversarial_solarization">https://github.com/paulgavrikov/adversarial_solarization</a></li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 评估深度神经网络对于不同输入的鲁棒性是非常重要，特别在自动驾驶和安全系统中，以避免安全隐患和黑客digital alter输入。</li>
<li>methods: 我们引入了基于图像曝光的攻击方法，该方法简单易行，但它不会破坏自然图像的全局结构独立于强度。我们通过对多个ImageNet模型进行全面评估，示出了该攻击的准确率下降，但是它并不能保证完全免疫于准确下降。在其他设置下，该攻击可以简化为黑盒子攻击，并且可以使用模型独立的参数。</li>
<li>results: 我们的实验结果表明，对于不同的图像分类模型，该攻击可以导致准确率下降，尤其是当攻击不包括在训练增强中时。此外，我们发现了一些防御机制不能延伸到对我们的特定攻击的鲁棒性。<details>
<summary>Abstract</summary>
Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into the training augmentations. Interestingly, even then, no full immunity to accuracy deterioration is achieved. In other settings, the attack can often be simplified into a black-box attack with model-independent parameters. Defenses against other corruptions do not consistently extend to be effective against our specific attack.   Project website: https://github.com/paulgavrikov/adversarial_solarization
</details>
<details>
<summary>摘要</summary>
评估深度神经网络对于不同类型的输入的Robustness是非常重要，特别是在自动驾驶和安全系统中，因为这些系统可能会遭受到恶意的攻击。然而，设计有效的不同类型输入测试是一项具有挑战性的任务，因为需要涵盖所有可能的场景，同时保持准确的标签信息。现有的方法ologies often involve a compromise between variety and constraint levels for attacks, and sometimes even both.在图像分类模型的Robustness评估中，我们引入了基于图像折衣的攻击方法，该方法是概念简单且不会损害自然图像的全球结构，无论输入的折衣水平。通过对多个ImageNet模型进行全面的评估，我们示出了该攻击的能力可以导致准确性下降，只要它不包括在训练增强中。意外地，甚至在这种情况下，也没有完全免疫于准确性下降。在其他设置下，该攻击可以简化为黑obox攻击，并且模型独立的参数。防御其他损害的方法不一定能够对我们的特定攻击延伸出效果。项目网站：https://github.com/paulgavrikov/adversarial_solarization
</details></li>
</ul>
<hr>
<h2 id="APART-Diverse-Skill-Discovery-using-All-Pairs-with-Ascending-Reward-and-DropouT"><a href="#APART-Diverse-Skill-Discovery-using-All-Pairs-with-Ascending-Reward-and-DropouT" class="headerlink" title="APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT"></a>APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12649">http://arxiv.org/abs/2308.12649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadar Schreiber Galler, Tom Zahavy, Guillaume Desjardins, Alon Cohen</li>
<li>for: 本研究旨在在奖励环境中发现多样化技能，目标是在简单的格子世界环境中发现所有可能的技能，而先前的方法在这些环境中很难成功。</li>
<li>methods: 我们的初始解决方案是替换标准的一对一（所有对）推识器与一对一推识器，并将其与一种新的内在奖励函数和dropout regularization技术结合使用。这种结合的方法被称为APART：多样化技能发现使用所有对with ascending reward和dropout。我们证明了APART在格子世界中发现所有可能的技能，使用了非常少的样本，比先前的工作更高效。</li>
<li>results: 我们的实验结果表明，APART在格子世界中发现所有可能的技能，使用了非常少的样本，比先前的工作更高效。此外，我们还提出了一种更简单的算法，通过修改VIC、重新规定内在奖励和软max推识器温度来实现最大技能数。我们认为我们的发现可能 shed light on 抽象学习中成功的关键因素。<details>
<summary>Abstract</summary>
We study diverse skill discovery in reward-free environments, aiming to discover all possible skills in simple grid-world environments where prior methods have struggled to succeed. This problem is formulated as mutual training of skills using an intrinsic reward and a discriminator trained to predict a skill given its trajectory. Our initial solution replaces the standard one-vs-all (softmax) discriminator with a one-vs-one (all pairs) discriminator and combines it with a novel intrinsic reward function and a dropout regularization technique. The combined approach is named APART: Diverse Skill Discovery using All Pairs with Ascending Reward and Dropout. We demonstrate that APART discovers all the possible skills in grid worlds with remarkably fewer samples than previous works. Motivated by the empirical success of APART, we further investigate an even simpler algorithm that achieves maximum skills by altering VIC, rescaling its intrinsic reward, and tuning the temperature of its softmax discriminator. We believe our findings shed light on the crucial factors underlying success of skill discovery algorithms in reinforcement learning.
</details>
<details>
<summary>摘要</summary>
我们研究了不带奖励的环境中多样化技能发现，目的是发现所有可能的技能在简单的格子世界环境中。这个问题被формализова为用内在奖励和用来预测技能的抽象器进行相互训练。我们的初始解决方案是将标准的一对一（所有对）抽象器取代一个一对多（softmax）抽象器，并将其与一种新的内在奖励函数和抽象器 Dropout 技术相结合。这种结合方法被命名为 APART：多样化技能发现使用所有对的升降奖励和Dropout。我们示出了 APART 在格子世界中发现所有可能的技能，并且使用 remarkably  fewer samples  than previous works。受 APART 的实验成功的激发，我们进一步调查了一种更简单的算法，通过修改 VIC，增加其内在奖励，并调整其 softmax 抽象器的温度来实现最大技能数。我们认为我们的发现可以透视到抽象学习中成功技能发现算法的关键因素。
</details></li>
</ul>
<hr>
<h2 id="The-GENEA-Challenge-2023-A-large-scale-evaluation-of-gesture-generation-models-in-monadic-and-dyadic-settings"><a href="#The-GENEA-Challenge-2023-A-large-scale-evaluation-of-gesture-generation-models-in-monadic-and-dyadic-settings" class="headerlink" title="The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings"></a>The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12646">http://arxiv.org/abs/2308.12646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taras Kucherenko, Rajmund Nagy, Youngwoo Yoon, Jieyeon Woo, Teodor Nikolov, Mihail Tsakov, Gustav Eje Henter</li>
<li>for: 这个研究的目的是为了评估参与者们在 Speech-driven Gesture-generation 领域的系统，以及这些系统在实际对话中的表现。</li>
<li>methods: 这个研究使用了同一个语音和动作数据集，并采用了共同评估方式。参与者们需要基于语音和动作来生成全身动作。</li>
<li>results: 研究发现了一个很大的人类化范围，其中一些系统评分非常接近人类捕捉数据。然而，适应性问题仍然很大，大多数提交系统在一定范围内表现在随机上方。此外，与对话伙伴的效果也很微妙，最好的提交系统只能在随机上方表现。更多资料可以通过项目网站 <a target="_blank" rel="noopener" href="https://svito-zar.github.io/GENEAchallenge2023/">https://svito-zar.github.io/GENEAchallenge2023/</a> 获取。<details>
<summary>Abstract</summary>
This paper reports on the GENEA Challenge 2023, in which participating teams built speech-driven gesture-generation systems using the same speech and motion dataset, followed by a joint evaluation. This year's challenge provided data on both sides of a dyadic interaction, allowing teams to generate full-body motion for an agent given its speech (text and audio) and the speech and motion of the interlocutor. We evaluated 12 submissions and 2 baselines together with held-out motion-capture data in several large-scale user studies. The studies focused on three aspects: 1) the human-likeness of the motion, 2) the appropriateness of the motion for the agent's own speech whilst controlling for the human-likeness of the motion, and 3) the appropriateness of the motion for the behaviour of the interlocutor in the interaction, using a setup that controls for both the human-likeness of the motion and the agent's own speech. We found a large span in human-likeness between challenge submissions, with a few systems rated close to human mocap. Appropriateness seems far from being solved, with most submissions performing in a narrow range slightly above chance, far behind natural motion. The effect of the interlocutor is even more subtle, with submitted systems at best performing barely above chance. Interestingly, a dyadic system being highly appropriate for agent speech does not necessarily imply high appropriateness for the interlocutor. Additional material is available via the project website at https://svito-zar.github.io/GENEAchallenge2023/ .
</details>
<details>
<summary>摘要</summary>
We found a large range in human-likeness among challenge submissions, with a few systems rated close to human motion capture. However, appropriateness was not well-solved, with most submissions performing in a narrow range slightly above chance, far behind natural motion. The effect of the interlocutor was even more subtle, with submitted systems at best performing barely above chance. Interestingly, a dyadic system being highly appropriate for agent speech does not necessarily imply high appropriateness for the interlocutor. Additional information can be found on the project website at <https://svito-zar.github.io/GENEAchallenge2023/>.
</details></li>
</ul>
<hr>
<h2 id="Towards-Hierarchical-Regional-Transformer-based-Multiple-Instance-Learning"><a href="#Towards-Hierarchical-Regional-Transformer-based-Multiple-Instance-Learning" class="headerlink" title="Towards Hierarchical Regional Transformer-based Multiple Instance Learning"></a>Towards Hierarchical Regional Transformer-based Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12634">http://arxiv.org/abs/2308.12634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josef Cersovsky, Sadegh Mohammadi, Dagmar Kainmueller, Johannes Hoehne</li>
<li>for: 这个研究旨在提高大幅 histopathology 图像的分类效果，使用深度多实例学习模型。</li>
<li>methods: 该方法使用 Transformer 核心，替代传统的学习注意力机制，使用区域 Vision Transformer 自注意力机制。另外，该方法还利用区域补做来 derive 板块级别预测，并可以堆叠来进行不同距离水平的特征处理。</li>
<li>results: 该方法在两个 histopathology 数据集上显著提高了性能，特别是对于具有小、地方形态特征的数据集。这些结果预示了该方法在精细医学领域的潜在应用。<details>
<summary>Abstract</summary>
The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.
</details>
<details>
<summary>摘要</summary>
“ digitization pathology 和精度医学中， gigapixel  histopathology 图像的分类已成为一个关键任务。在这种工作中，我们提议使用 transformer 基于多实例学习模型，取代传统的学习注意力机制。我们提出一种基于区域的 Vision Transformer 自注意力机制，并将地域补丁信息融合以 derive 扫描级别预测。我们还介绍了一种用于在推理过程中专注高注意区域进行图像处理，以提高预测精度，特别是对于具有小、本地形态特征的数据集。我们的方法能够显著超越基线性能，在两个 histopathology 数据集上，并指向了未来研究的可能性。”Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-and-Explainable-Analysis-of-Machine-Learning-Model-for-Reconstruction-of-Sonic-Slowness-Logs"><a href="#Uncertainty-and-Explainable-Analysis-of-Machine-Learning-Model-for-Reconstruction-of-Sonic-Slowness-Logs" class="headerlink" title="Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs"></a>Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12625">http://arxiv.org/abs/2308.12625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hua Wang, Yuqiong Wu, Yushun Zhang, Fuqiang Lai, Zhou Feng, Bing Xie, Ailin Zhao<br>for:This paper aims to predict missing compressional wave slowness and shear wave slowness logs in horizontal or old wells using other logs in the same borehole.methods:The authors use the NGBoost algorithm to construct an Ensemble Learning model that can predict the results as well as their uncertainty. They also use the SHAP method to investigate the interpretability of the machine learning model.results:The NGBoost model performs well in the testing set and provides a probability distribution for the prediction results. The variance of the probability distribution can be used to justify the quality of the constructed log. The machine learning model captures the influence of the changing borehole caliper on slowness, which is consistent with the physical principle of borehole acoustics.Here’s the simplified Chinese text:for: 这篇论文目的是预测垂直或老井中缺失的压缩波慢速和剪切波慢速记录。methods: 作者使用NGBoost算法构建了一个ensemble学习模型，可以预测结果以及其不确定性。他们还使用SHAP方法来调查机器学习模型的可读性。results: NGBoost模型在测试集中表现良好，可以提供预测结果的概率分布。预测结果的方差可以用来评估构造的日志质量。机器学习模型捕捉了改变的孔隙矿物带产生的影响，这是physical principle of borehole acoustics的一部分。<details>
<summary>Abstract</summary>
Logs are valuable information for oil and gas fields as they help to determine the lithology of the formations surrounding the borehole and the location and reserves of subsurface oil and gas reservoirs. However, important logs are often missing in horizontal or old wells, which poses a challenge in field applications. In this paper, we utilize data from the 2020 machine learning competition of the SPWLA, which aims to predict the missing compressional wave slowness and shear wave slowness logs using other logs in the same borehole. We employ the NGBoost algorithm to construct an Ensemble Learning model that can predicate the results as well as their uncertainty. Furthermore, we combine the SHAP method to investigate the interpretability of the machine learning model. We compare the performance of the NGBosst model with four other commonly used Ensemble Learning methods, including Random Forest, GBDT, XGBoost, LightGBM. The results show that the NGBoost model performs well in the testing set and can provide a probability distribution for the prediction results. In addition, the variance of the probability distribution of the predicted log can be used to justify the quality of the constructed log. Using the SHAP explainable machine learning model, we calculate the importance of each input log to the predicted results as well as the coupling relationship among input logs. Our findings reveal that the NGBoost model tends to provide greater slowness prediction results when the neutron porosity and gamma ray are large, which is consistent with the cognition of petrophysical models. Furthermore, the machine learning model can capture the influence of the changing borehole caliper on slowness, where the influence of borehole caliper on slowness is complex and not easy to establish a direct relationship. These findings are in line with the physical principle of borehole acoustics.
</details>
<details>
<summary>摘要</summary>
Logs 是钻井油气田中的重要信息，它们可以帮助确定附近钻井的地层学特性和潜在的油气储量。然而，在水平或老井中，重要的 logs 经常缺失，这会对钻井实际应用带来挑战。在这篇文章中，我们使用2020年机器学习竞赛的SPWLA数据，以预测缺失的压缩波慢速度和剪切波慢速度 logs。我们使用NGBoost算法建立了一个 Ensemble Learning 模型，可以预测结果以及其不确定性。此外，我们使用 SHAP 方法来调查机器学习模型的可解释性。我们将 NGBoost 模型与四种常用的 Ensemble Learning 方法进行比较，包括Random Forest、GBDT、XGBoost 和 LightGBM。结果表明，NGBoost 模型在测试集中表现良好，可以提供预测结果的概率分布。此外，预测结果的方差可以用来评估构造的 logs 质量。使用 SHAP 可解释机器学习模型，我们计算输入 logs 对预测结果的重要性以及输入 logs 之间的相互作用。我们发现，NGBoost 模型在 neutron  pórosity 和γ射辐射大的情况下提供更高的慢速度预测结果，这与石油物理模型的认知一致。此外，机器学习模型可以捕捉随着钻井尺寸的变化而影响慢速度的复杂关系，这与物理原理相一致。
</details></li>
</ul>
<hr>
<h2 id="Try-with-Simpler-–-An-Evaluation-of-Improved-Principal-Component-Analysis-in-Log-based-Anomaly-Detection"><a href="#Try-with-Simpler-–-An-Evaluation-of-Improved-Principal-Component-Analysis-in-Log-based-Anomaly-Detection" class="headerlink" title="Try with Simpler – An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection"></a>Try with Simpler – An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12612">http://arxiv.org/abs/2308.12612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Junjie Chen, Zhihao Gong, Shutao Gao, Hongyu Zhang, Yue Kang, Huaan Li<br>for: 这个研究的目的是将传统机器学习和数据探勘技术与深度学习结合，以实现对log事件的独特检测。methods: 这个研究使用了七种log事件检测方法，包括四种深度学习方法、两种传统方法和优化的PCA技术。results: 结果显示，优化的PCA技术与高级的对照方法相比，具有更高的稳定性和资源效率，并且在训练数据和资源方面具有更好的适应力。<details>
<summary>Abstract</summary>
The rapid growth of deep learning (DL) has spurred interest in enhancing log-based anomaly detection. This approach aims to extract meaning from log events (log message templates) and develop advanced DL models for anomaly detection. However, these DL methods face challenges like heavy reliance on training data, labels, and computational resources due to model complexity. In contrast, traditional machine learning and data mining techniques are less data-dependent and more efficient but less effective than DL. To make log-based anomaly detection more practical, the goal is to enhance traditional techniques to match DL's effectiveness. Previous research in a different domain (linking questions on Stack Overflow) suggests that optimized traditional techniques can rival state-of-the-art DL methods. Drawing inspiration from this concept, we conducted an empirical study. We optimized the unsupervised PCA (Principal Component Analysis), a traditional technique, by incorporating lightweight semantic-based log representation. This addresses the issue of unseen log events in training data, enhancing log representation. Our study compared seven log-based anomaly detection methods, including four DL-based, two traditional, and the optimized PCA technique, using public and industrial datasets. Results indicate that the optimized unsupervised PCA technique achieves similar effectiveness to advanced supervised/semi-supervised DL methods while being more stable with limited training data and resource-efficient. This demonstrates the adaptability and strength of traditional techniques through small yet impactful adaptations.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）的快速发展促使异常检测方法的改进。这种方法目的在于从日志事件模板中提取意义并开发高级DL模型进行异常检测。然而，这些DL方法面临着大量训练数据、标签和计算资源的压力，尤其是模型的复杂性。相比之下，传统机器学习和数据挖掘技术更加不依赖于训练数据和计算资源，但效果较差。为了让日志异常检测更加实用，目标是提高传统技术，使其与DL的效果相匹配。前一项研究（在Stack Overflow上的问题连接）表明，优化传统技术可以与当前DL方法相比。以此为起点，我们进行了一项实验研究。我们对传统的无监督PCA（主成分分析）进行了优化，通过 integrate lightweight semantic-based日志表示。这解决了训练数据中未见日志事件的问题，提高日志表示。我们对公共和工业 dataset 进行了七种日志异常检测方法的比较，包括四种DL基于方法、两种传统方法和优化PCA技术。结果显示，优化无监督PCA技术与高级监督/半监督DL方法相比，在有限的训练数据和资源下具有更高的稳定性和效率。这表明传统技术通过小 yet 有力的改进，可以具备DL方法的灵活性和效果。
</details></li>
</ul>
<hr>
<h2 id="A-Greedy-Approach-for-Offering-to-Telecom-Subscribers"><a href="#A-Greedy-Approach-for-Offering-to-Telecom-Subscribers" class="headerlink" title="A Greedy Approach for Offering to Telecom Subscribers"></a>A Greedy Approach for Offering to Telecom Subscribers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12606">http://arxiv.org/abs/2308.12606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piyush Kanti Bhunre, Tanmay Sen, Arijit Sarkar</li>
<li>for: 这个论文是为了解决电信运营商面临的客户销售或落户预防问题，提出了一种新的组合算法来优化电信运营商的奖励策略，以最大化预期收入。</li>
<li>methods: 该论文提出了一种新的组合算法，使用了各种策略来选择目标客户和奖励，包括考虑多个目标函数，例如最大化收入和最小化落户率。</li>
<li>results: 该论文的实验结果表明，该算法可以准确地选择目标客户并提供合适的奖励，从而最大化预期收入。 当前的算法可以处理大规模的客户基数，并且具有高效和准确的特点。<details>
<summary>Abstract</summary>
Customer retention or churn prevention is a challenging task of a telecom operator. One of the effective approaches is to offer some attractive incentive or additional services or money to the subscribers for keeping them engaged and make sure they stay in the operator's network for longer time. Often, operators allocate certain amount of monetary budget to carry out the offer campaign. The difficult part of this campaign is the selection of a set of customers from a large subscriber-base and deciding the amount that should be offered to an individual so that operator's objective is achieved. There may be multiple objectives (e.g., maximizing revenue, minimizing number of churns) for selection of subscriber and selection of an offer to the selected subscriber. Apart from monetary benefit, offers may include additional data, SMS, hots-spot tethering, and many more. This problem is known as offer optimization. In this paper, we propose a novel combinatorial algorithm for solving offer optimization under heterogeneous offers by maximizing expected revenue under the scenario of subscriber churn, which is, in general, seen in telecom domain. The proposed algorithm is efficient and accurate even for a very large subscriber-base.
</details>
<details>
<summary>摘要</summary>
客户退订或防止落叶是电信运营商面临的挑战。一种有效的方法是向用户提供一些吸引力强的奖励或额外服务，以保持用户的兴趣和使他们尽快留在运营商的网络中。运营商通常将一定的财务预算用于实施这些奖励。选择一个从大量用户基础中选择用户并决定每个用户所需的奖励金额是这个campaign的困难之处。运营商可能有多个目标（例如，最大化收入、最小化落叶数），这会影响选择用户和向选择用户提供的奖励。此外，奖励可能包括额外的数据、短信、快速热点连接等。这个问题被称为奖励优化。在这篇论文中，我们提出了一种新的 combinatorial 算法，用于在不同的奖励下进行奖励优化，以 maximize 用户退订后的预期收入。提议的算法是高效和准确， même pour une base de souscripteurs très large。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Time-Frequency-Conformers-for-Music-Audio-Enhancement"><a href="#Exploiting-Time-Frequency-Conformers-for-Music-Audio-Enhancement" class="headerlink" title="Exploiting Time-Frequency Conformers for Music Audio Enhancement"></a>Exploiting Time-Frequency Conformers for Music Audio Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12599">http://arxiv.org/abs/2308.12599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunkee Chae, Junghyun Koo, Sungho Lee, Kyogu Lee</li>
<li>for: 提高网络视频平台上的音乐表演录音质量，改善听众体验。</li>
<li>methods: 基于Conformer架构，利用注意力机制进行音乐提升。</li>
<li>results: 实验结果显示，提出的模型在单音轨音乐提升任务中达到了状态元表现，并且可以进行通用音乐提升，处理多轨混合音频。<details>
<summary>Abstract</summary>
With the proliferation of video platforms on the internet, recording musical performances by mobile devices has become commonplace. However, these recordings often suffer from degradation such as noise and reverberation, which negatively impact the listening experience. Consequently, the necessity for music audio enhancement (referred to as music enhancement from this point onward), involving the transformation of degraded audio recordings into pristine high-quality music, has surged to augment the auditory experience. To address this issue, we propose a music enhancement system based on the Conformer architecture that has demonstrated outstanding performance in speech enhancement tasks. Our approach explores the attention mechanisms of the Conformer and examines their performance to discover the best approach for the music enhancement task. Our experimental results show that our proposed model achieves state-of-the-art performance on single-stem music enhancement. Furthermore, our system can perform general music enhancement with multi-track mixtures, which has not been examined in previous work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LORD-Leveraging-Open-Set-Recognition-with-Unknown-Data"><a href="#LORD-Leveraging-Open-Set-Recognition-with-Unknown-Data" class="headerlink" title="LORD: Leveraging Open-Set Recognition with Unknown Data"></a>LORD: Leveraging Open-Set Recognition with Unknown Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12584">http://arxiv.org/abs/2308.12584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Koch, Christian Riess, Thomas Köhler</li>
<li>for: 本研究旨在解决部署过程中遇到的完全未知数据问题，即 Classification 模型在推理过程中遇到的 откры集数据。</li>
<li>methods: 本研究提出了一种名为 LORD 的框架，即 Leverage Open-set Recognition by exploiting unknown Data。LORD 在类ifier 训练过程中直接模型开放空间，并提供了一种系统atic evaluation 方法来评估这些方法。研究人员还提出了三种模型无关的训练策略，这些策略可以应用于常见的类ifier 中。</li>
<li>results: 研究人员通过广泛的实验和分析表明，LORD 可以更好地识别未知数据。此外，研究人员还发现了一种名为 mixup 的数据生成技术，可以作为背景数据的替代品。并且通过对 mixup 的约束进行优化，可以进一步提高 OSR 性能。<details>
<summary>Abstract</summary>
Handling entirely unknown data is a challenge for any deployed classifier. Classification models are typically trained on a static pre-defined dataset and are kept in the dark for the open unassigned feature space. As a result, they struggle to deal with out-of-distribution data during inference. Addressing this task on the class-level is termed open-set recognition (OSR). However, most OSR methods are inherently limited, as they train closed-set classifiers and only adapt the downstream predictions to OSR. This work presents LORD, a framework to Leverage Open-set Recognition by exploiting unknown Data. LORD explicitly models open space during classifier training and provides a systematic evaluation for such approaches. We identify three model-agnostic training strategies that exploit background data and applied them to well-established classifiers. Due to LORD's extensive evaluation protocol, we consistently demonstrate improved recognition of unknown data. The benchmarks facilitate in-depth analysis across various requirement levels. To mitigate dependency on extensive and costly background datasets, we explore mixup as an off-the-shelf data generation technique. Our experiments highlight mixup's effectiveness as a substitute for background datasets. Lightweight constraints on mixup synthesis further improve OSR performance.
</details>
<details>
<summary>摘要</summary>
处理完全未知数据是任何部署 классифика器的挑战。分类模型通常是静态预定的数据集上训练的，因此在推理时遇到不同类型的数据时会陷入困难。为解决这个问题，我们称之为开放集 recognition（OSR）。然而，大多数 OSR 方法都有限制，因为它们只是在预定的类别上进行了适应。这项工作提出了一个框架，称之为 LORD，可以利用未知数据进行开放集认识。LORD 在分类器训练时显式地模型开放空间，并提供了系统的评估方法。我们认为有三种模型无关的训练策略可以利用背景数据，并应用到了成熟的分类器上。由于 LORD 的广泛的评估协议，我们在不同的需求水平上 consistently 表现出了未知数据的更好的识别。这些标准化的宽泛可以进行深入的分析。为了减少依赖于大量和昂贵的背景数据，我们探索了 mixup 作为一种可用的数据生成技术。我们的实验表明，mixup 是一种有效的替代方案。另外，对 mixup 的 sintesis 进行轻量级的限制可以进一步提高 OSR 性能。
</details></li>
</ul>
<hr>
<h2 id="Persistent-learning-signals-and-working-memory-without-continuous-attractors"><a href="#Persistent-learning-signals-and-working-memory-without-continuous-attractors" class="headerlink" title="Persistent learning signals and working memory without continuous attractors"></a>Persistent learning signals and working memory without continuous attractors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12585">http://arxiv.org/abs/2308.12585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Il Memming Park, Ábel Ságodi, Piotr Aleksander Sokół</li>
<li>for: 这篇论文探讨了神经动力系统中稳定吸引结构，如点吸引器和连续吸引器，如何支持有用的时间行为，需要工作记忆。</li>
<li>methods: 论文使用了 periodic和 quasi-periodic吸引器来支持学习，而不 LIKE continuous吸引器，这些吸引器可以学习无限长的时间关系。</li>
<li>results: 论文的理论有广泛的应用于人工学习系统的设计，以及对生物神经动力系统中的时间依赖学习和工作记忆的预测。同时，论文还提出了一种新的初始化方案，可以超过标准方法在需要学习时间动力学任务上表现出色。此外，论文还提出了一种Robust的回忆机制，可以维护和集成方向memory。<details>
<summary>Abstract</summary>
Neural dynamical systems with stable attractor structures, such as point attractors and continuous attractors, are hypothesized to underlie meaningful temporal behavior that requires working memory. However, working memory may not support useful learning signals necessary to adapt to changes in the temporal structure of the environment. We show that in addition to the continuous attractors that are widely implicated, periodic and quasi-periodic attractors can also support learning arbitrarily long temporal relationships. Unlike the continuous attractors that suffer from the fine-tuning problem, the less explored quasi-periodic attractors are uniquely qualified for learning to produce temporally structured behavior. Our theory has broad implications for the design of artificial learning systems and makes predictions about observable signatures of biological neural dynamics that can support temporal dependence learning and working memory. Based on our theory, we developed a new initialization scheme for artificial recurrent neural networks that outperforms standard methods for tasks that require learning temporal dynamics. Moreover, we propose a robust recurrent memory mechanism for integrating and maintaining head direction without a ring attractor.
</details>
<details>
<summary>摘要</summary>
神经动力系统 WITH stable attractor structure, such as point attractors and continuous attractors, are hypothesized to underlie meaningful temporal behavior that requires working memory. However, working memory may not support useful learning signals necessary to adapt to changes in the temporal structure of the environment. We show that in addition to the continuous attractors that are widely implicated, periodic and quasi-periodic attractors can also support learning arbitrarily long temporal relationships. Unlike the continuous attractors that suffer from the fine-tuning problem, the less explored quasi-periodic attractors are uniquely qualified for learning to produce temporally structured behavior. Our theory has broad implications for the design of artificial learning systems and makes predictions about observable signatures of biological neural dynamics that can support temporal dependence learning and working memory. Based on our theory, we developed a new initialization scheme for artificial recurrent neural networks that outperforms standard methods for tasks that require learning temporal dynamics. Moreover, we propose a robust recurrent memory mechanism for integrating and maintaining head direction without a ring attractor.Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other countries. The translation is written in Simplified Chinese, but the text remains the same in both Simplified and Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Huber-Loss-Minimization-Approach-to-Byzantine-Robust-Federated-Learning"><a href="#A-Huber-Loss-Minimization-Approach-to-Byzantine-Robust-Federated-Learning" class="headerlink" title="A Huber Loss Minimization Approach to Byzantine Robust Federated Learning"></a>A Huber Loss Minimization Approach to Byzantine Robust Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12581">http://arxiv.org/abs/2308.12581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Fei Yu, Zhiguo Wan</li>
<li>for: 防止 Federated Learning 系统受到攻击，我们提出一种基于ubyte loss函数的聚合器，并提供了全面的理论分析。</li>
<li>methods: 我们的方法基于ubyte loss函数进行聚合，具有优化 $\epsilon$ 的依赖性，不需要准确知道 $\epsilon$，并允许客户端数据大小不同。</li>
<li>results: 我们的方法在独立同分布（i.i.d）假设下有多个优势，包括优化 $\epsilon$ 的依赖性和不需要准确知道 $\epsilon$。此外，我们还扩展了分析至非i.i.d数据，包括客户端数据有轻度不同的情况。<details>
<summary>Abstract</summary>
Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
</details>
<details>
<summary>摘要</summary>
联合学习系统容易受到敌意攻击。为了解决这问题，我们提出了基于捷径函数损失的新的聚合器，并进行了全面的理论分析。在独立同分布（i.i.d）假设下，我们的方法有以下优点：首先，它对epsillon（攻击客户端的比率）具有优化的依赖度。其次，我们的方法不需要准确地知道epsillon。最后，它允许客户端有不同的数据大小。然后，我们扩展了我们的分析范围，包括非i.i.d数据，例如客户端的数据具有略微不同的分布。
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Convolutional-Networks-for-Fine-grained-ICU-Patient-Similarity-Analysis-and-Risk-Prediction"><a href="#Hypergraph-Convolutional-Networks-for-Fine-grained-ICU-Patient-Similarity-Analysis-and-Risk-Prediction" class="headerlink" title="Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction"></a>Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12575">http://arxiv.org/abs/2308.12575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Liu, Zhenhao Zhang, Shaowen Qin, Flora D. Salim, Antonio Jimeno Yepes, Jun Shen</li>
<li>for: 预测 critically ill 患者的死亡风险</li>
<li>methods: 使用 Hypergraph Convolutional Network capture 非对应关系，以计算细致的患者相似性</li>
<li>results: 在 eICU Collaborative Research Database 上，方法比前一代模型提高了死亡风险预测性能，并在几个实际案例中展现了良好的透明度和可靠性。Here’s a breakdown of each point:</li>
<li>for: The paper is written to predict the mortality risk of critically ill patients.</li>
<li>methods: The paper proposes using a Hypergraph Convolutional Network (HGCN) to capture non-pairwise relationships among diagnosis codes in a hypergraph, allowing for the calculation of fine-grained patient similarity and personalized mortality risk prediction.</li>
<li>results: The proposed method achieves superior performance over state-of-the-art models on mortality risk prediction, as demonstrated by evaluation on a publicly available dataset (eICU Collaborative Research Database). Additionally, the results of several case studies show that the method provides good transparency and robustness in decision-making.<details>
<summary>Abstract</summary>
The Intensive Care Unit (ICU) is one of the most important parts of a hospital, which admits critically ill patients and provides continuous monitoring and treatment. Various patient outcome prediction methods have been attempted to assist healthcare professionals in clinical decision-making. Existing methods focus on measuring the similarity between patients using deep neural networks to capture the hidden feature structures. However, the higher-order relationships are ignored, such as patient characteristics (e.g., diagnosis codes) and their causal effects on downstream clinical predictions.   In this paper, we propose a novel Hypergraph Convolutional Network that allows the representation of non-pairwise relationships among diagnosis codes in a hypergraph to capture the hidden feature structures so that fine-grained patient similarity can be calculated for personalized mortality risk prediction. Evaluation using a publicly available eICU Collaborative Research Database indicates that our method achieves superior performance over the state-of-the-art models on mortality risk prediction. Moreover, the results of several case studies demonstrated the effectiveness of constructing graph networks in providing good transparency and robustness in decision-making.
</details>
<details>
<summary>摘要</summary>
医院重症监护部（ICU）是医院中最重要的部分之一，患有严重疾病的患者可以在这里接受无间断监测和治疗。不同的患者结果预测方法已经被尝试以帮助医疗专业人员进行临床决策。现有方法主要是使用深度神经网络来捕捉患者特征结构的隐藏关系。然而，高阶关系（如诊断代码）和其对下游临床预测的影响却被忽略了。在本文中，我们提出了一种新的超graph卷积神经网络，允许表示诊断代码之间的非对比关系在超graph中表示隐藏特征结构，从而可以计算出细化的患者相似性，以进行个性化死亡风险预测。使用公共可用的eICU合作研究数据库，我们的方法比现有的模型在死亡风险预测中表现出色。此外，多个案例研究表明，建立图网络可以提供良好的透明度和稳定性在决策中。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Kernel-Imitation-Learning-for-Continuous-State-Environments"><a href="#Conditional-Kernel-Imitation-Learning-for-Continuous-State-Environments" class="headerlink" title="Conditional Kernel Imitation Learning for Continuous State Environments"></a>Conditional Kernel Imitation Learning for Continuous State Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12573">http://arxiv.org/abs/2308.12573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Agrawal, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar</li>
<li>for: 本研究旨在 solving the imitation learning problem in continuous state space environments without access to transition dynamics information, reward structure, or additional interactions with the environment.</li>
<li>methods: 我们的方法基于Markov balance equation，并使用conditional kernel density estimator来估算环境的转移动力学。</li>
<li>results: 我们的实验结果表明，我们的方法在 continuous state benchmark environments 上表现出了 consistently superior empirical performance compared to many state-of-the-art IL algorithms。<details>
<summary>Abstract</summary>
Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our approach is based on the Markov balance equation and introduces a novel conditional kernel density estimation-based imitation learning framework. It involves estimating the environment's transition dynamics using conditional kernel density estimators and seeks to satisfy the probabilistic balance equations for the environment. We establish that our estimators satisfy basic asymptotic consistency requirements. Through a series of numerical experiments on continuous state benchmark environments, we show consistently superior empirical performance over many state-of-the-art IL algorithms.
</details>
<details>
<summary>摘要</summary>
imitational learning（IL）是RL方法olo的重要分支，不同于大多数RL，它不假设有奖励反馈。奖励推断和形成是difficult和error-prone的方法，特别是当示例数据来自人类专家时。古典方法 such as behavioral cloning和inverse reinforcement learning是高度敏感于估计错误，这是特别突出的在连续状态空间问题上。在这篇论文中，我们考虑了基于观察行为的imitational learning在连续状态空间环境中的问题，没有访问过程动力学信息、奖励结构或任何其他与环境进行交互的数据。我们的方法基于Markov平衡方程，并提出了一种基于 conditional kernel density estimation的imitational learning框架。它通过估计环境的过程动力学使用conditional kernel density estimator，并寻求满足环境的 probabilistic balance equation。我们证明了我们的估计符合基本的极限consistency要求。通过对连续状态benchmark环境进行numerical experiment，我们展示了与许多现状征的IL算法相比，我们的方法具有consistent superior empirical performance。
</details></li>
</ul>
<hr>
<h2 id="Multivariate-Time-Series-Anomaly-Detection-with-Contaminated-Data-Application-to-Physiological-Signals"><a href="#Multivariate-Time-Series-Anomaly-Detection-with-Contaminated-Data-Application-to-Physiological-Signals" class="headerlink" title="Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals"></a>Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12563">http://arxiv.org/abs/2308.12563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thi Kieu Khanh Ho, Narges Armanfard</li>
<li>for: 本研究旨在 Addressing the challenge of training with noise in practical anomaly detection, and presenting a novel and practical end-to-end unsupervised time-series anomaly detection (TSAD) approach when the training data are contaminated with anomalies.</li>
<li>methods: 本方法包括 three modules: a Decontaminator to rectify the abnormalities present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data, and an Anomaly Scoring module to detect anomalies.</li>
<li>results: 经过广泛的实验 validate that our approach surpasses existing methodologies, thus establishing a new state-of-the-art performance in the field.<details>
<summary>Abstract</summary>
Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate of the pure normal data, and an Anomaly Scoring module to detect anomalies. Our extensive experiments conducted on three widely used physiological datasets conclusively demonstrate that our approach surpasses existing methodologies, thus establishing a new state-of-the-art performance in the field.
</details>
<details>
<summary>摘要</summary>
主流无监督异常检测算法经常在学术数据集上表现出色，然而在实际应用中，它们的性能受到干扰的限制，主要是因为干扰的训练数据。寻找训练数据中干扰的挑战是实际异常检测中的一个普遍存在的问题，然而这种问题在学术研究中得到了忽视。本研究在感知时序异常检测（TSAD）领域进行了先锋性的尝试，推出了一种新的实用无监督TSAD方法，称为TSAD-C。TSAD-C方法在干扰了异常数据的训练数据上进行了三个模块的组合：一个Rectifier模块用于修正训练数据中的异常（即噪声），一个Variable Dependency Modeling模块用于捕捉修正后的数据中的长期内部和间部变量相互关系，以及一个异常检测模块用于检测异常。我们对三个常用的生理数据集进行了广泛的实验，结果表明，我们的方法在现有方法之上升级了新的状态态�arte，这有力地证明了我们的方法在实际应用中的优势。
</details></li>
</ul>
<hr>
<h2 id="Variational-Information-Pursuit-with-Large-Language-and-Multimodal-Models-for-Interpretable-Predictions"><a href="#Variational-Information-Pursuit-with-Large-Language-and-Multimodal-Models-for-Interpretable-Predictions" class="headerlink" title="Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions"></a>Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12562">http://arxiv.org/abs/2308.12562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwan Ho Ryan Chan, Aditya Chattopadhyay, Benjamin David Haeffele, Rene Vidal</li>
<li>for: 这个论文的目的是提出一种基于变换信息抽取（V-IP）框架的可解释预测方法，以便在预测 задании中实现内在可解释性。</li>
<li>methods: 该方法使用了两个步骤，首先使用大语言模型（LLM）生成一个具有足够大容量的任务相关可解释概念集，然后使用大量多模态模型对每个数据样本进行semantic similarity的标注。</li>
<li>results:  comparative experiments show that the proposed FM+V-IP method can achieve better test performance than V-IP with human-annotated concepts, and can also achieve competitive test performance using fewer number of concepts&#x2F;queries compared to other interpretable-by-design frameworks such as CBMs.<details>
<summary>Abstract</summary>
Variational Information Pursuit (V-IP) is a framework for making interpretable predictions by design by sequentially selecting a short chain of task-relevant, user-defined and interpretable queries about the data that are most informative for the task. While this allows for built-in interpretability in predictive models, applying V-IP to any task requires data samples with dense concept-labeling by domain experts, limiting the application of V-IP to small-scale tasks where manual data annotation is feasible. In this work, we extend the V-IP framework with Foundational Models (FMs) to address this limitation. More specifically, we use a two-step process, by first leveraging Large Language Models (LLMs) to generate a sufficiently large candidate set of task-relevant interpretable concepts, then using Large Multimodal Models to annotate each data sample by semantic similarity with each concept in the generated concept set. While other interpretable-by-design frameworks such as Concept Bottleneck Models (CBMs) require an additional step of removing repetitive and non-discriminative concepts to have good interpretability and test performance, we mathematically and empirically justify that, with a sufficiently informative and task-relevant query (concept) set, the proposed FM+V-IP method does not require any type of concept filtering. In addition, we show that FM+V-IP with LLM generated concepts can achieve better test performance than V-IP with human annotated concepts, demonstrating the effectiveness of LLMs at generating efficient query sets. Finally, when compared to other interpretable-by-design frameworks such as CBMs, FM+V-IP can achieve competitive test performance using fewer number of concepts/queries in both cases with filtered or unfiltered concept sets.
</details>
<details>
<summary>摘要</summary>
各位：Variational Information Pursuit（V-IP）是一个框架，用于将预测结果变得更加解释性，通过逐步选择任务相关、用户定义且可解释性的问题，以实现预测模型内置的解释性。然而，实施V-IP需要具有对应的数据样本，且需要专家 manually 进行标签，因此仅能应用于小规模任务。在这个工作中，我们将V-IP框架与基础模型（FM）融合，以解决这个限制。具体来说，我们运用两步过程：首先，通过大语言模型（LLM）产生足够多的任务相关且可解释性的概念集，然后，使用大多媒体模型进行标签，将每个数据样本与每个概念进行semantic similarity标识。相比于其他可解释性设计框架，如概念瓶颈模型（CBM），我们不需要进行拒绝重复且无用数据的概念 filtering。此外，我们还证明了，只要概念集足够信息内容和任务相关，则FM+V-IP方法不需要进行任何型数据排除。最后，我们显示了FM+V-IP这个方法可以使用LLM生成的概念集，在test数据上取得更好的表现，并且在对照数据上也能够取得更好的表现。此外，相比于其他可解释性设计框架，FM+V-IP可以使用 fewer 数量的概念来取得相同或更好的表现。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-driven-Cross-Community-Energy-Interaction-Optimal-Scheduling"><a href="#Deep-Reinforcement-Learning-driven-Cross-Community-Energy-Interaction-Optimal-Scheduling" class="headerlink" title="Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling"></a>Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12554">http://arxiv.org/abs/2308.12554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Fanjin Bu, Zhen Yang, Bin Wang, Meng Han</li>
<li>for: 该论文 targets 多个社区和多种能源互相协同、共享资源，以实现多元社区综合能源系统的优化和调度。</li>
<li>methods: 该论文提出了一种基于多智能深度学习算法的综合调度模型，通过学习不同社区的负荷特点来做出决策。</li>
<li>results:  simulation results show that the proposed method effectively captures the load characteristics of different communities and utilizes their complementary features to coordinate reasonable energy interactions among them, leading to a reduction in wind curtailment rate from 16.3% to 0% and lowering the overall operating cost by 5445.6 Yuan.<details>
<summary>Abstract</summary>
In order to coordinate energy interactions among various communities and energy conversions among multi-energy subsystems within the multi-community integrated energy system under uncertain conditions, and achieve overall optimization and scheduling of the comprehensive energy system, this paper proposes a comprehensive scheduling model that utilizes a multi-agent deep reinforcement learning algorithm to learn load characteristics of different communities and make decisions based on this knowledge. In this model, the scheduling problem of the integrated energy system is transformed into a Markov decision process and solved using a data-driven deep reinforcement learning algorithm, which avoids the need for modeling complex energy coupling relationships between multi-communities and multi-energy subsystems. The simulation results show that the proposed method effectively captures the load characteristics of different communities and utilizes their complementary features to coordinate reasonable energy interactions among them. This leads to a reduction in wind curtailment rate from 16.3% to 0% and lowers the overall operating cost by 5445.6 Yuan, demonstrating significant economic and environmental benefits.
</details>
<details>
<summary>摘要</summary>
为了协调不同社区之间的能源互动和多种能源子系统内部的能源转换，并在不确定条件下实现整体能源系统的优化和调度，这篇论文提出了一种综合调度模型，利用多代理深度学习算法来学习不同社区的荷载特点，并根据这些知识来做出决策。在这个模型中，集成能源系统的调度问题被转化为了马尔可夫决策过程，并使用数据驱动的深度学习算法来解决，这有效避免了复杂的能源协同关系的建模。实验结果表明，提议的方法能够有效捕捉不同社区的荷载特点，并利用它们的共同特点来协调合理的能源互动。这导致风力废弃率从16.3%降至0%，并且全局运行成本下降5445.6元，表明了明显的经济和环保效益。
</details></li>
</ul>
<hr>
<h2 id="Don’t-blame-Dataset-Shift-Shortcut-Learning-due-to-Gradients-and-Cross-Entropy"><a href="#Don’t-blame-Dataset-Shift-Shortcut-Learning-due-to-Gradients-and-Cross-Entropy" class="headerlink" title="Don’t blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy"></a>Don’t blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12553">http://arxiv.org/abs/2308.12553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aahlad Puli, Lily Zhang, Yoav Wald, Rajesh Ranganath</li>
<li>for: 这篇论文旨在解释短cut学习的问题，即模型在训练分布下采用短cut来预测结果，但在测试分布下可能不适用。</li>
<li>methods: 这篇论文使用了一种名为default-ERM的方法，即通过梯度下降优化十字积分loss函数来训练模型。然而，研究发现，even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, default-ERM still exhibits shortcut learning。</li>
<li>results: 研究发现，default-ERM的 preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization。这种情况存在于感知任务中，并且可以通过开发一种偏见（induction bias）toward uniform margins来解决这个问题。这种偏见使得模型仅仅依赖于稳定的feature，而不是短cut。<details>
<summary>Abstract</summary>
Common explanations for shortcut learning assume that the shortcut improves prediction under the training distribution but not in the test distribution. Thus, models trained via the typical gradient-based optimization of cross-entropy, which we call default-ERM, utilize the shortcut. However, even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, like in perception tasks, default-ERM still exhibits shortcut learning. Why are such solutions preferred when the loss for default-ERM can be driven to zero using the stable feature alone? By studying a linear perception task, we show that default-ERM's preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization. This insight suggests that default-ERM's implicit inductive bias towards max-margin is unsuitable for perception tasks. Instead, we develop an inductive bias toward uniform margins and show that this bias guarantees dependence only on the perfect stable feature in the linear perception task. We develop loss functions that encourage uniform-margin solutions, called margin control (MARG-CTRL). MARG-CTRL mitigates shortcut learning on a variety of vision and language tasks, showing that better inductive biases can remove the need for expensive two-stage shortcut-mitigating methods in perception tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>传统的短cut learning解释假设，短cut提高训练分布下预测的准确率，但在测试分布下不提供任何帮助。因此，通过常见的梯度下降优化方法来训练模型，我们称之为默认ERM（default-ERM），会使用短cut。然而，即使稳定特征可以决定训练分布下的标签，而短cut不提供任何信息，如感知任务中，默认ERM仍然会展现短cut learning。为何这些解决方案被选择，即使损失函数可以通过稳定特征alone驱动到零？通过研究一个线性感知任务，我们显示了默认ERM的偏好是通过最大化margin来适应模型，这会导致模型更加依赖于短cut，而不是稳定特征，即使没有过参数。这一见解表明，默认ERM的隐式偏好向max-margin是不适合感知任务的。相反，我们开发了一种偏好向 uniform margins，并证明这种偏好使得模型只依赖于稳定特征。我们开发了一种损失函数，called MARG-CTRL，以便鼓励uniform-margin解决方案。MARG-CTRL可以 Mitigating短cut learning在视觉和语言任务中，证明更好的偏好可以消除过两个阶段的短cut-mitigating方法。
</details></li>
</ul>
<hr>
<h2 id="A-Co-training-Approach-for-Noisy-Time-Series-Learning"><a href="#A-Co-training-Approach-for-Noisy-Time-Series-Learning" class="headerlink" title="A Co-training Approach for Noisy Time Series Learning"></a>A Co-training Approach for Noisy Time Series Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12551">http://arxiv.org/abs/2308.12551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiqi Zhang, Jianfeng Zhang, Jia Li, Fugee Tsung</li>
<li>for: 本文关注 robust 时间序列表示学习，假设实际世界时间序列具有噪音和不同视图的信息对分析噪音输入时发挥重要作用。</li>
<li>methods: 我们创建了两个视图的输入时间序列通过两个不同的编码器，然后通过协同对比学习来学习这两个编码器。</li>
<li>results: 我们的TS-CoT方法在四个时间序列标准 benchmark 上的无监督和半监督设置下表现出色，特别是通过利用不同视图的补做信息来mitigate数据噪音和损害。此外，TS-CoT 学习的表示可以通过细化来转移到下游任务中。<details>
<summary>Abstract</summary>
In this work, we focus on robust time series representation learning. Our assumption is that real-world time series is noisy and complementary information from different views of the same time series plays an important role while analyzing noisy input. Based on this, we create two views for the input time series through two different encoders. We conduct co-training based contrastive learning iteratively to learn the encoders. Our experiments demonstrate that this co-training approach leads to a significant improvement in performance. Especially, by leveraging the complementary information from different views, our proposed TS-CoT method can mitigate the impact of data noise and corruption. Empirical evaluations on four time series benchmarks in unsupervised and semi-supervised settings reveal that TS-CoT outperforms existing methods. Furthermore, the representations learned by TS-CoT can transfer well to downstream tasks through fine-tuning.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们关注于鲁棒的时间序列表示学习。我们假设现实世界中的时间序列是噪音的，而不同视图中的相同时间序列信息在分析噪音输入时扮演着重要的角色。基于这个假设，我们创建了两个视图来对输入时间序列进行编码。我们通过训练彩色学习来学习这两个编码器。我们的实验表明，这种合作学习方法可以提高表达性。特别是通过不同视图之间的补充信息，我们的提案的TS-CoT方法可以减轻数据噪音的影响。我们在四个时间序列标准曲线上进行了empirical评估，并在无监督和半监督设置下证明TS-CoT方法在现有方法之上具有显著的提升。此外，TS-CoT方法学习的表示可以通过细化来转移到下游任务中。
</details></li>
</ul>
<hr>
<h2 id="CALM-A-Multi-task-Benchmark-for-Comprehensive-Assessment-of-Language-Model-Bias"><a href="#CALM-A-Multi-task-Benchmark-for-Comprehensive-Assessment-of-Language-Model-Bias" class="headerlink" title="CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias"></a>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12539">http://arxiv.org/abs/2308.12539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vipulgupta1011/calm">https://github.com/vipulgupta1011/calm</a></li>
<li>paper_authors: Vipul Gupta, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, Rebecca J. Passonneau</li>
<li>for: 这个论文的目的是为了量化和比较语言模型（LM）中的社会经济阶层偏见，以及这些偏见可能对人们造成伤害的可能性。</li>
<li>methods: 这篇论文使用了16个不同领域的数据集，包括Wikipedia和新闻文章，并将224个模板筛选成78,400个例子的 dataset。它们还比较了不同的数据集的多样性，并测试了小型变化的敏感性。</li>
<li>results: 研究发现，新的数据集（CALM）比之前的数据集更加多样和可靠，能够更好地评估语言模型的偏见。研究还发现，某些LM系列中的大型参数模型比小型参数模型更加偏见，而T0系列的模型最少偏见。此外，研究发现一些LM系列中的 gender 和种族偏见与模型大小成正相关。<details>
<summary>Abstract</summary>
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language models including six prominent families of LMs such as Llama-2. In two LM series, OPT and Bloom, we found that larger parameter models are more biased than lower parameter models. We found the T0 series of models to be the least biased. Furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. The code is available at https://github.com/vipulgupta1011/CALM.
</details>
<details>
<summary>摘要</summary>
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language models including six prominent families of LMs such as Llama-2. In two LM series, OPT and Bloom, we found that larger parameter models are more biased than lower parameter models. We found the T0 series of models to be the least biased. Furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. The code is available at https://github.com/vipulgupta1011/CALM.Here's the translation in Traditional Chinese:当语言模型（LM）的能力不断增强时，这些模型的社会经济特征偏见的评估也变得非常重要。现有的偏见评估数据集在手动设计的模板上有敏感性，因此不可靠。为了获得可靠性，我们引入了语言模型偏见全面评估（CALM）数据集，用于评估语言模型在三个任务上的偏见。我们将16个不同领域的数据集融合，包括Wikipedia和新闻文章，以筛选出224个模板，从而建立一个78,400个例子的数据集。我们与先前的数据集进行比较，包括语义相似性和模板长度的多样性，并测试小量的敏感性。我们发现，我们的数据集比先前的数据集更加多样和可靠，因此更好地捕捉了需要评估模型偏见的语言多样性。我们评估了20个大型语言模型，包括Llama-2等6个家族的模型。在两个LM系列中，OPT和Bloom中，我们发现大型模型比较偏见，而且模型大小增加时，男女和种族偏见之间存在贸易。相关的代码可以在https://github.com/vipulgupta1011/CALM上获取。
</details></li>
</ul>
<hr>
<h2 id="FedSoL-Bridging-Global-Alignment-and-Local-Generality-in-Federated-Learning"><a href="#FedSoL-Bridging-Global-Alignment-and-Local-Generality-in-Federated-Learning" class="headerlink" title="FedSoL: Bridging Global Alignment and Local Generality in Federated Learning"></a>FedSoL: Bridging Global Alignment and Local Generality in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12532">http://arxiv.org/abs/2308.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gihun Lee, Minchan Jeong, Sangmook Kim, Jaehoon Oh, Se-Young Yun</li>
<li>for: 这篇论文的目的是提出一种名为 Federated Stability on Learning (FedSoL) 的联合学习方法，以提高联合学习的稳定性和本地学习的通用性。</li>
<li>methods: 这篇论文使用了一种称为“偏好损失曲线”的概念，将本地学习变得更加稳定，并且不会干扰原本的本地目标。它还使用了一种名为“偏好损失”的概念，帮助实现本地学习的通用性。</li>
<li>results: 这篇论文的实验结果显示，FedSoL 可以在不同的设置下实现州际学习的稳定性和本地学习的通用性，并且在不同的资料分布下实现比较好的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (FedSoL), which combines both the concepts of global alignment and local generality. In FedSoL, the local learning seeks a parameter region robust against proximal perturbations. This strategy introduces an implicit proximal restriction effect in local learning while maintaining the original local objective for parameter update. Our experiments show that FedSoL consistently achieves state-of-the-art performance on various setups.
</details>
<details>
<summary>摘要</summary>
federa 学习（FL）通过将客户端上进行本地训练的模型集成而构建全球模型。 FL 允许在数据隐私的前提下学习模型，但它经常因客户端数据分布不均而表现较差。 多种前一代 FL 算法已经解决了这个问题，通过引入不同的距离约束来激励全球准确性。 然而，这些约束会限制本地学习，因为它们会干扰本地目标。 最近，一种新的方法出现了，以提高本地学习通用性。 通过在本地学习中获得一个平滑的损失曲线，这种方法可以减轻客户端之间的冲突。 然而，这种方法不能保证全球稳定性，因为本地学习没有考虑全球目标。 在这项研究中，我们提出了联邦稳定学习（FedSoL），它结合了全球准确性和本地通用性的两个概念。 在 FedSoL 中，本地学习寻找一个鲁棒对抗辐射的参数区域。 这种策略将在本地学习中引入隐形的距离约束效果，同时保持原始本地目标 для参数更新。 我们的实验表明，FedSoL 可靠地实现多种设置的状态前例性表现。
</details></li>
</ul>
<hr>
<h2 id="SieveNet-Selecting-Point-Based-Features-for-Mesh-Networks"><a href="#SieveNet-Selecting-Point-Based-Features-for-Mesh-Networks" class="headerlink" title="SieveNet: Selecting Point-Based Features for Mesh Networks"></a>SieveNet: Selecting Point-Based Features for Mesh Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12530">http://arxiv.org/abs/2308.12530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sievenet/sievenet.github.io">https://github.com/sievenet/sievenet.github.io</a></li>
<li>paper_authors: Shengchao Yuan, Yishun Dou, Rui Shi, Bingbing Ni, Zhong Zheng</li>
<li>for:  This paper aims to address the limitations of using remeshed proxies in mesh neural networks, and proposes a novel paradigm called SieveNet that combines the benefits of regular topology and exact geometry.</li>
<li>methods:  The proposed method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. It also eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer.</li>
<li>results:  The comprehensive experimental results on classification and segmentation tasks demonstrate the effectiveness and superiority of the proposed method, showing that it can retain the underlying geometry of the original mesh while maintaining the benefits of regular topology.<details>
<summary>Abstract</summary>
Meshes are widely used in 3D computer vision and graphics, but their irregular topology poses challenges in applying them to existing neural network architectures. Recent advances in mesh neural networks turn to remeshing and push the boundary of pioneer methods that solely take the raw meshes as input. Although the remeshing offers a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer. Comprehensive experimental results on classification and segmentation tasks well demonstrate the effectiveness and superiority of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>meshes 广泛应用于三维计算机视觉和图形处理中，但它们的不规则结构会使得现有神经网络架构中应用困难。最近的 mesh 神经网络技术转而使用重新拟合，将边缘推导法的限制 pushed 到最前面。虽然重新拟合提供了规则的结构，但从such 拟合代理中提取的特征可能难以准确保持下方geometry，这会限制后续神经网络的能力。为解决这个问题，我们提出了 Sievenet，一种新的思路，该思路考虑了结构化的 mesh 结构和原始 mesh 上的精度信息。此外，我们的方法不需要手动设计特征工程，可以利用现有的网络架构，如视Transformer。 comprehensive 实验结果表明，我们的方法在分类和 segmentation 任务中表现出色，superior 于 существу方法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The traditional Chinese writing system is also widely used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="UNISOUND-System-for-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#UNISOUND-System-for-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023"></a>UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12526">http://arxiv.org/abs/2308.12526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Zheng, Yajun Zhang, Chuanying Niu, Yibin Zhan, Yanhua Long, Dongxing Xu</li>
<li>for: 这份报告介绍了在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC 2023）的Track1和Track2上提交的UNISOUND系统。我们对Track 1和Track 2使用了同一个系统，它是基于VoxCeleb2-dev进行训练的。</li>
<li>methods: 我们采用了大规模ResNet和RepVGG架构来开发系统，并提出了一种具有稳定性的相似性分数权重因子（CMF）来改善系统的性能。</li>
<li>results: 我们的最终系统是六个模型的 fusions，在VoxSRC 2023的Track 1中获得了第一名，在Track 2中获得了第二名。我们的最终提交的minDCF为0.0855，EER为1.5880%。<details>
<summary>Abstract</summary>
This report describes the UNISOUND submission for Track1 and Track2 of VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC 2023). We submit the same system on Track 1 and Track 2, which is trained with only VoxCeleb2-dev. Large-scale ResNet and RepVGG architectures are developed for the challenge. We propose a consistency-aware score calibration method, which leverages the stability of audio voiceprints in similarity score by a Consistency Measure Factor (CMF). CMF brings a huge performance boost in this challenge. Our final system is a fusion of six models and achieves the first place in Track 1 and second place in Track 2 of VoxSRC 2023. The minDCF of our submission is 0.0855 and the EER is 1.5880%.
</details>
<details>
<summary>摘要</summary>
这份报告介绍了我们在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC 2023）的UNISOUND提交，包括Track1和Track2。我们使用了VoxCeleb2-dev进行训练，并开发了大规模ResNet和RepVGG架构。我们提出了一种具有稳定性的相似性分数准则，通过一个稳定度指数（CMF）来优化分数。CMF带来了很大的性能提升。我们的最终系统是六个模型的拟合，在Track 1中获得第一名，在Track 2中获得第二名。我们的最小dCF是0.0855，EER为1.5880%。
</details></li>
</ul>
<hr>
<h2 id="Not-Only-Rewards-But-Also-Constraints-Applications-on-Legged-Robot-Locomotion"><a href="#Not-Only-Rewards-But-Also-Constraints-Applications-on-Legged-Robot-Locomotion" class="headerlink" title="Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion"></a>Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12517">http://arxiv.org/abs/2308.12517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunho Kim, Hyunsik Oh, Jeonghyun Lee, Jinhyeok Choi, Gwanghyeon Ji, Moonkyu Jung, Donghoon Youm, Jemin Hwangbo</li>
<li>for: 这个论文的目的是提出一种基于奖励学习的控制器训练方法，以便用于复杂的机械系统。</li>
<li>methods: 该方法使用了一种基于神经网络的奖励学习算法，并具有两种约束类型和高效的政策优化算法。</li>
<li>results: 该方法可以在许多不同的四肢动物机器人上培养出高性能的控制器，并且可以避免大量的奖励工程化。<details>
<summary>Abstract</summary>
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attributes to traverse challenging terrains. Extensive simulation and real-world experiments demonstrate that performant controllers can be trained with significantly less reward engineering, by tuning only a single reward coefficient. Furthermore, a more straightforward and intuitive engineering process can be utilized, thanks to the interpretability and generalizability of constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
</details>
<details>
<summary>摘要</summary>
多个以前的研究已经表现出了复杂机器人系统中使用神经网络控制器和无模型奖励学习实现出色的控制性能。然而，这些独特的控制器通过大量的奖励工程来实现自然的运动风格和高任务性能，这是一个非常劳动 INTENSITY 和时间消耗的过程。在这种工作中，我们提出了一种基于奖励学习的控制器训练框架，用于训练神经网络控制器。为了让工程师明确表达他们的意图并快速地处理约束，我们建议了两种约束类型和一种高效的政策优化算法。该学习框架在许多脚趾机器人的不同形态和物理属性下训练步行控制器，并在实际实验和模拟中得到了证明。结果表明，可以通过较少的奖励工程，只需调整单个奖励系数，可以训练出高性能的控制器。此外，通过约束的解释和普适性，工程师可以使用更直观和直接的工程过程。相关视频可以在https://youtu.be/KAlm3yskhvM中找到。
</details></li>
</ul>
<hr>
<h2 id="Masked-Autoencoders-are-Efficient-Class-Incremental-Learners"><a href="#Masked-Autoencoders-are-Efficient-Class-Incremental-Learners" class="headerlink" title="Masked Autoencoders are Efficient Class Incremental Learners"></a>Masked Autoencoders are Efficient Class Incremental Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12510">http://arxiv.org/abs/2308.12510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scok30/mae-cil">https://github.com/scok30/mae-cil</a></li>
<li>paper_authors: Jiang-Tian Zhai, Xialei Liu, Andrew D. Bagdanov, Ke Li, Ming-Ming Cheng</li>
<li>for: Sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge.</li>
<li>methods: 使用Masked Autoencoders (MAEs) 为Class Incremental Learning (CIL) 提供有效的学习方法，并通过捆绑supervised loss来实现分类。</li>
<li>results: 实验表明，我们的方法在CIFAR-100、ImageNet-Subset和ImageNet-Full上比前一个状态的方法表现更好，并且可以更好地保持过去任务的知识。<details>
<summary>Abstract</summary>
Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. The code is available at https://github.com/scok30/MAE-CIL .
</details>
<details>
<summary>摘要</summary>
类增量学习（CIL）目标是逐渐学习新类，而不导致之前的知识忘记。我们提议使用Masked Autoencoders（MAEs）作为CIL的有效学习器。MAEs原本是用于通过无监督学习获得有用表示，并可以轻松地与分类损失结合使用。此外，MAEs可靠地重建原始输入图像从随机选择的块中，我们用此来更高效地存储过去任务的示例。我们还提出了双向MAE框架，以学习图像水平和表示水平的混合，该框架可生成更高质量的重建图像和更稳定的表示。我们的实验表明，我们的方法在CIFAR-100、ImageNet-Subset和ImageNet-Full上比现状态的更好。代码可以在https://github.com/scok30/MAE-CIL上获取。
</details></li>
</ul>
<hr>
<h2 id="False-Information-Bots-and-Malicious-Campaigns-Demystifying-Elements-of-Social-Media-Manipulations"><a href="#False-Information-Bots-and-Malicious-Campaigns-Demystifying-Elements-of-Social-Media-Manipulations" class="headerlink" title="False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations"></a>False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12497">http://arxiv.org/abs/2308.12497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Majid Akhtar, Rahat Masood, Muhammad Ikram, Salil S. Kanhere</li>
<li>for: 本研究旨在提供关于社交媒体欺诈（SMM）元素的全面分析，以满足现有研究中缺乏这些元素的涵盖。</li>
<li>methods: 本研究通过系统性的文献调查，挖掘社交媒体欺诈元素的相互关系，并提出了现有研究的共同点、缺点和有价值的发现。</li>
<li>results: 研究发现，社交媒体欺诈的元素包括假信息、机器人和恶意运动，这些元素之间存在着密切的关系。此外，研究还发现了现有研究中缺乏用户心理学、假信息检测和恶意软件的研究，这些方面的研究具有重要的意义。<details>
<summary>Abstract</summary>
The rapid spread of false information and persistent manipulation attacks on online social networks (OSNs), often for political, ideological, or financial gain, has affected the openness of OSNs. While researchers from various disciplines have investigated different manipulation-triggering elements of OSNs (such as understanding information diffusion on OSNs or detecting automated behavior of accounts), these works have not been consolidated to present a comprehensive overview of the interconnections among these elements. Notably, user psychology, the prevalence of bots, and their tactics in relation to false information detection have been overlooked in previous research. To address this research gap, this paper synthesizes insights from various disciplines to provide a comprehensive analysis of the manipulation landscape. By integrating the primary elements of social media manipulation (SMM), including false information, bots, and malicious campaigns, we extensively examine each SMM element. Through a systematic investigation of prior research, we identify commonalities, highlight existing gaps, and extract valuable insights in the field. Our findings underscore the urgent need for interdisciplinary research to effectively combat social media manipulations, and our systematization can guide future research efforts and assist OSN providers in ensuring the safety and integrity of their platforms.
</details>
<details>
<summary>摘要</summary>
随着社交媒体 manipulate（SMM）的迅速扩散和持续的欺诈攻击，在在线社交网络（OSN）上的开放性受到了影响。虽然不同领域的研究人员已经对OSN上的不同欺诈触发元素进行了研究（如了解信息传播在OSN上或检测帐户的自动化行为），但这些研究没有被集成起来，不能提供全面的报告。特别是用户心理学、bot的普遍性和他们在假信息检测方面的策略尚未被前期研究所涵盖。为了填补这个研究漏洞，本文通过将不同领域的知识和技术集成起来，进行了全面的欺诈领域的分析。我们通过系统性的调查和分析，找到了各种SMM元素之间的相互关联，并揭示了现有的研究漏洞和未解决的问题。我们的发现表明，需要跨学科研究，以有效地对抗社交媒体欺诈，而我们的系统化分析可以导引未来的研究努力，并帮助OSN提供者保持平台的安全和完整性。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Neural-Network-Scale-for-ECG-Classification"><a href="#Optimizing-Neural-Network-Scale-for-ECG-Classification" class="headerlink" title="Optimizing Neural Network Scale for ECG Classification"></a>Optimizing Neural Network Scale for ECG Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12492">http://arxiv.org/abs/2308.12492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeong Tak Lee, Yong-Yeon Jo, Joon-Myoung Kwon</li>
<li>for: 这个论文主要是为了分析电cardiogram（ECG）信号，使用卷积神经网络（CNN）模型。</li>
<li>methods: 这个论文使用了残差神经网络（ResNet）模型，并对其进行了优化。</li>
<li>results: 研究发现，采用更 shallow 的网络、更多的通道数和更小的卷积kernel size可以提高 ECG 分类的性能。<details>
<summary>Abstract</summary>
We study scaling convolutional neural networks (CNNs), specifically targeting Residual neural networks (ResNet), for analyzing electrocardiograms (ECGs). Although ECG signals are time-series data, CNN-based models have been shown to outperform other neural networks with different architectures in ECG analysis. However, most previous studies in ECG analysis have overlooked the importance of network scaling optimization, which significantly improves performance. We explored and demonstrated an efficient approach to scale ResNet by examining the effects of crucial parameters, including layer depth, the number of channels, and the convolution kernel size. Through extensive experiments, we found that a shallower network, a larger number of channels, and smaller kernel sizes result in better performance for ECG classifications. The optimal network scale might differ depending on the target task, but our findings provide insight into obtaining more efficient and accurate models with fewer computing resources or less time. In practice, we demonstrate that a narrower search space based on our findings leads to higher performance.
</details>
<details>
<summary>摘要</summary>
我们研究了归一化 convolutional neural networks (CNNs)，特指 Residual neural networks (ResNet)，用于分析电子心脏图像 (ECG)。虽然 ECG 信号是时间序列数据，但 CNN 基本结构已经在 ECG 分析中显示出比其他神经网络更高的性能。然而，大多数前一 Studies 在 ECG 分析中忽略了网络缩放优化的重要性，这对性能有着重要的提高作用。我们探索了和证明了一种有效的方法来缩放 ResNet，包括层数、通道数和滤波器大小等关键参数的影响。通过广泛的实验，我们发现了一个更浅的网络结构、更多的通道数和更小的滤波器大小会对 ECG 分类表现更好。但是，优化网络缩放的最佳方法可能因目标任务而异，但我们的发现可以帮助您更快地获得更高效和更准确的模型，使用更少的计算资源或时间。在实践中，我们示出了基于我们发现的更窄的搜索空间可以 достичь更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Fall-Detection-using-Knowledge-Distillation-Based-Long-short-term-memory-for-Offline-Embedded-and-Low-Power-Devices"><a href="#Fall-Detection-using-Knowledge-Distillation-Based-Long-short-term-memory-for-Offline-Embedded-and-Low-Power-Devices" class="headerlink" title="Fall Detection using Knowledge Distillation Based Long short-term memory for Offline Embedded and Low Power Devices"></a>Fall Detection using Knowledge Distillation Based Long short-term memory for Offline Embedded and Low Power Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12481">http://arxiv.org/abs/2308.12481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Zhou, Allison Chen, Celine Buer, Emily Chen, Kayleen Tang, Lauryn Gong, Zhiqi Liu, Jianbin Tang</li>
<li>for: 本研究旨在提出一种可靠、低功耗的滥落检测方法，以提高检测精度。</li>
<li>methods: 本研究使用知识储备学习（Knowledge Distillation）基于LSTM（Long Short-Term Memory）模型，以提高检测精度。它主要分析各种感知器的时序数据，并提供实时检测功能，以确保提前、可靠地识别滥落。</li>
<li>results: 研究表明，通过使用知识储备学习技术，可以提高检测精度，同时降低能耗。因此，该提议的解决方案可能成为未来关键领域中能效的滥落检测系统的开发之道。<details>
<summary>Abstract</summary>
This paper presents a cost-effective, low-power approach to unintentional fall detection using knowledge distillation-based LSTM (Long Short-Term Memory) models to significantly improve accuracy. With a primary focus on analyzing time-series data collected from various sensors, the solution offers real-time detection capabilities, ensuring prompt and reliable identification of falls. The authors investigate fall detection models that are based on different sensors, comparing their accuracy rates and performance. Furthermore, they employ the technique of knowledge distillation to enhance the models' precision, resulting in refined accurate configurations that consume lower power. As a result, this proposed solution presents a compelling avenue for the development of energy-efficient fall detection systems for future advancements in this critical domain.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Translation in Traditional Chinese:本研究提出了一种经济高效、低功耗的滥落检测方法，使用知识储 transmit 基于 LSTM (Long Short-Term Memory) 模型，以提高准确率。该解决方案主要是分析不同传感器收集的时间序列数据，并在实时检测中提供可靠的滥落标识。作者们比较不同传感器基于的滥落检测模型的准确率和性能，并使用知识储 transmit 技术进一步提高模型的精度，从而实现更加精准的配置，同时减少功耗。因此，该提出的解决方案为未来在这一重要领域的能量减少型滥落检测系统的开发提供了一条可行的道路。
</details></li>
</ul>
<hr>
<h2 id="Zero-delay-Consistent-Signal-Reconstruction-from-Streamed-Multivariate-Time-Series"><a href="#Zero-delay-Consistent-Signal-Reconstruction-from-Streamed-Multivariate-Time-Series" class="headerlink" title="Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series"></a>Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12459">http://arxiv.org/abs/2308.12459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emilio Ruiz-Moreno, Luis Miguel López-Ramos, Baltasar Beferull-Lozano</li>
<li>for: 这篇论文是为了提出一种可靠地从数据流中重建多变量时间序列的方法。</li>
<li>methods: 该方法使用了一种基于循环神经网络的方法，通过学习时空相关性来减少数据流中的信号拟合粗糙。</li>
<li>results: 对于测试数据，该方法可以实现与采样率相对的较低的错误率下降，并且保证了信号重建的一致性。<details>
<summary>Abstract</summary>
Digitalizing real-world analog signals typically involves sampling in time and discretizing in amplitude. Subsequent signal reconstructions inevitably incur an error that depends on the amplitude resolution and the temporal density of the acquired samples. From an implementation viewpoint, consistent signal reconstruction methods have proven a profitable error-rate decay as the sampling rate increases. Despite that, these results are obtained under offline settings. Therefore, a research gap exists regarding methods for consistent signal reconstruction from data streams. This paper presents a method that consistently reconstructs streamed multivariate time series of quantization intervals under a zero-delay response requirement. On the other hand, previous work has shown that the temporal dependencies within univariate time series can be exploited to reduce the roughness of zero-delay signal reconstructions. This work shows that the spatiotemporal dependencies within multivariate time series can also be exploited to achieve improved results. Specifically, the spatiotemporal dependencies of the multivariate time series are learned, with the assistance of a recurrent neural network, to reduce the roughness of the signal reconstruction on average while ensuring consistency. Our experiments show that our proposed method achieves a favorable error-rate decay with the sampling rate compared to a similar but non-consistent reconstruction.
</details>
<details>
<summary>摘要</summary>
通常情况下，将实际世界上的扩散信号数字化都需要采样时间和采样幅度。接下来的信号重建都会产生错误，这些错误取决于采样幅度和时间分布的获取频率。尽管在实现视图下，一些信号重建方法已经证明了随采样率增加的负责任逻辑下的错误率下降。然而，这些结果是在线上获取的。因此，一个研究漏洞存在，即如何在数据流中一致地重建流体时间序列。这篇论文提出了一种方法，可以在零延迟响应下一致地重建流体多变量时间序列。在前一些研究中，已经证明了在单变量时间序列中可以利用时间相关性来减少零延迟重建的粗糙性。这篇论文则表明，在多变量时间序列中也可以利用空间时间相关性来实现改进的结果。具体来说，通过一种循环神经网络的帮助，学习多变量时间序列中的空间时间相关性，以减少重建中的粗糙性，同时保证一致性。我们的实验表明，我们提出的方法可以与采样率相比，实现更好的错误率下降。
</details></li>
</ul>
<hr>
<h2 id="PFL-GAN-When-Client-Heterogeneity-Meets-Generative-Models-in-Personalized-Federated-Learning"><a href="#PFL-GAN-When-Client-Heterogeneity-Meets-Generative-Models-in-Personalized-Federated-Learning" class="headerlink" title="PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning"></a>PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12454">http://arxiv.org/abs/2308.12454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achintha Wijesinghe, Songyang Zhang, Zhi Ding</li>
<li>for: 这篇论文的目的是提出一种基于对各客户数据的聚合和共享的GAN模型，以应对客户数据不同的情况。</li>
<li>methods: 本论文使用的方法是基于GAN模型，并且提出了一种Weighted Collaborative Data Aggregation（WCDA）策略，以应对客户数据不同的情况。</li>
<li>results: 经过严谨的实验证明，PFL-GAN可以有效地应对客户数据不同的情况，并且可以提高模型的准确性和稳定性。<details>
<summary>Abstract</summary>
Recent advances of generative learning models are accompanied by the growing interest in federated learning (FL) based on generative adversarial network (GAN) models. In the context of FL, GAN can capture the underlying client data structure, and regenerate samples resembling the original data distribution without compromising the private raw data. Although most existing GAN-based FL works focus on training a global model, Personalized FL (PFL) sometimes can be more effective in view of client data heterogeneity in terms of distinct data sample distributions, feature spaces, and labels. To cope with client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in different scenarios. More specially, we first learn the similarity among clients and then develop an weighted collaborative data aggregation. The empirical results through the rigorous experimentation on several well-known datasets demonstrate the effectiveness of PFL-GAN.
</details>
<details>
<summary>摘要</summary>
Recent advances in generative learning models have led to growing interest in federated learning (FL) based on generative adversarial network (GAN) models. In the context of FL, GAN can capture the underlying client data structure and regenerate samples resembling the original data distribution without compromising the private raw data. Although most existing GAN-based FL works focus on training a global model, Personalized FL (PFL) can sometimes be more effective in view of client data heterogeneity in terms of distinct data sample distributions, feature spaces, and labels. To address client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses client heterogeneity in different scenarios. Specifically, we first learn the similarity among clients and then develop an weighted collaborative data aggregation. The empirical results from rigorous experimentation on several well-known datasets demonstrate the effectiveness of PFL-GAN.Here is the word-for-word translation of the text into Simplified Chinese:现代生成学模型的进步使得联合学习（FL）基于生成对抗网络（GAN）模型的兴趣不断增长。FL中的GAN可以捕捉客户端数据结构，并生成符合原始数据分布的样本，无需泄露私有原始数据。大多数现有的GAN基于FL工作集中在全球模型的训练上，而个性化联合学习（PFL）在客户端数据多样性方面可以更有效。为了Address客户端多样性在GAN基于FL中，我们提出了一种新的GAN共享和汇集策略，称为PFL-GAN。PFL-GAN在不同的场景下可以处理客户端多样性。具体来说，我们首先学习客户端之间的相似性，然后开发一种权重合作数据汇集。经验结果表明，PFL-GAN在多个知名数据集上得到了有效的result。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-medical-image-classifiers-with-synthetic-data-from-latent-diffusion-models"><a href="#Augmenting-medical-image-classifiers-with-synthetic-data-from-latent-diffusion-models" class="headerlink" title="Augmenting medical image classifiers with synthetic data from latent diffusion models"></a>Augmenting medical image classifiers with synthetic data from latent diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12453">http://arxiv.org/abs/2308.12453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke W. Sagers, James A. Diao, Luke Melas-Kyriazi, Matthew Groh, Pranav Rajpurkar, Adewole S. Adamson, Veronica Rotemberg, Roxana Daneshjou, Arjun K. Manrai</li>
<li>for: 这个论文的目的是提高医疗人工智能算法的性能，尤其是在数据有限的情况下。</li>
<li>methods: 该论文使用了潜在扩散模型来生成皮肤病图像，并在模型训练中添加这些数据以提高性能。</li>
<li>results: 研究发现，使用生成的皮肤病图像可以提高模型的性能，但是这种提高的效果随着真实图像的比例而减少。研究还生成了458,920张synthetic图像，并对其进行分析。<details>
<summary>Abstract</summary>
While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA), many studies have shown inconsistent generalization or latent bias, particularly for underrepresented populations. Some have proposed that generative AI could reduce the need for real data, but its utility in model development remains unclear. Skin disease serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone. Here we show that latent diffusion models can scalably generate images of skin disease and that augmenting model training with these data improves performance in data-limited settings. These performance gains saturate at synthetic-to-real image ratios above 10:1 and are substantially smaller than the gains obtained from adding real images. As part of our analysis, we generate and analyze a new dataset of 458,920 synthetic images produced using several generation strategies. Our results suggest that synthetic data could serve as a force-multiplier for model development, but the collection of diverse real-world data remains the most important step to improve medical AI algorithms.
</details>
<details>
<summary>摘要</summary>
美国食品和药品管理局（FDA）已批准或批准了数百种人工智能（AI）算法，但许多研究表明这些算法在不同人口群体中存在不一致的泛化或隐藏偏见。一些人提议使用生成AI可以减少实际数据的需求，但这些模型在模型开发中的用途仍然不清楚。皮肤病提供了一个有用的案例研究，因为皮肤病的表现iversity，特别是根据保护特征（skin tone）。我们显示了潜在扩散模型可以在数据有限情况下扫描性地生成皮肤病图像，并且在模型训练时添加这些数据可以提高性能。这些性能提升在真实图像与生成图像的比例大于10:1时达到饱和，与真实图像添加的提升相比较小。在分析中，我们生成了458,920个synthetic图像，并进行了分析。我们的结果表明，生成数据可以成为模型开发的力multiplier，但收集真实世界数据仍然是改进医疗AI算法的最重要步骤。
</details></li>
</ul>
<hr>
<h2 id="An-Intentional-Forgetting-Driven-Self-Healing-Method-For-Deep-Reinforcement-Learning-Systems"><a href="#An-Intentional-Forgetting-Driven-Self-Healing-Method-For-Deep-Reinforcement-Learning-Systems" class="headerlink" title="An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems"></a>An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12445">http://arxiv.org/abs/2308.12445</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahmedhajyahmed/drdrl">https://github.com/ahmedhajyahmed/drdrl</a></li>
<li>paper_authors: Ahmed Haj Yahmed, Rached Bouchoucha, Houssem Ben Braiek, Foutse Khomh</li>
<li>for: 本研究旨在解决深度强化学习（DRL）系统在大规模生产环境中遇到的环境风险问题。</li>
<li>methods: 本研究提出了一种名为“Dr. DRL”的自适应approach，它将在DRL系统中 интеGRATE一种新的忘记机制，以解决环境风险问题。</li>
<li>results:  compared withvanilla CL，Dr. DRL能够降低平均维护时间和精制化集数量，并在19.63%的风险环境中保持或提高获得的奖励。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which often occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment's conditions shifts. However, successive shifts of considerable magnitude may cause the production environment to drift from its original state. Recent studies have shown that these environmental drifts tend to drive CL into long, or even unsuccessful, healing cycles, which arise from inefficiencies such as catastrophic forgetting, warm-starting failure, and slow convergence. In this paper, we propose Dr. DRL, an effective self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL to overcome its main issues. Dr. DRL deliberately erases the DRL system's minor behaviors to systematically prioritize the adaptation of the key problem-solving skills. Using well-established DRL algorithms, Dr. DRL is compared with vanilla CL on various drifted environments. Dr. DRL is able to reduce, on average, the healing time and fine-tuning episodes by, respectively, 18.74% and 17.72%. Dr. DRL successfully helps agents to adapt to 19.63% of drifted environments left unsolved by vanilla CL while maintaining and even enhancing by up to 45% the obtained rewards for drifted environments that are resolved by both approaches.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）在大规模生产环境中应用越来越普遍，如Netflix和Facebook。就像大多数数据驱动系统一样，DRL系统可能会出现不жела的行为，这些行为经常在生产环境中逐渐发生变化。逐渐学习（CL）是DRLAgent的自适应方法，但是连续的环境变化可能会使生产环境偏离原始状态。现在的研究表明，这些环境变化可能会驱使CL进入长期或无法恢复的治疗循环，这些循环由于不可预测的干扰、热启动失败和慢 converges而起来。在这篇论文中，我们提出了Dr. DRL，一种有效的自适应方法，用于解决DRL系统中的主要问题。Dr. DRL通过在vanilla CL中添加一种新的忘记机制，系统地忘记DRL系统的次要行为，以优先级刻意适应关键问题的解决技能。使用了已知的DRL算法，Dr. DRL与vanilla CL进行比较，Dr. DRL能够在不同的逐渐变化环境中减少，平均退还时间和精度调整集数量为18.74%和17.72%。Dr. DRL成功地帮助代理人适应19.63%的逐渐变化环境，同时保持和提高达45%的得到的奖励。
</details></li>
</ul>
<hr>
<h2 id="TAI-GAN-Temporally-and-Anatomically-Informed-GAN-for-early-to-late-frame-conversion-in-dynamic-cardiac-PET-motion-correction"><a href="#TAI-GAN-Temporally-and-Anatomically-Informed-GAN-for-early-to-late-frame-conversion-in-dynamic-cardiac-PET-motion-correction" class="headerlink" title="TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction"></a>TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12443">http://arxiv.org/abs/2308.12443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gxq1998/tai-gan">https://github.com/gxq1998/tai-gan</a></li>
<li>paper_authors: Xueqi Guo, Luyao Shi, Xiongchao Chen, Bo Zhou, Qiong Liu, Huidong Xie, Yi-Hwa Liu, Richard Palyo, Edward J. Miller, Albert J. Sinusas, Bruce Spottiswoode, Chi Liu, Nicha C. Dvornek</li>
<li>for: 这个论文是为了解决快速追踪和动态心脏PET中的帧间运动偏移问题，特别是在早期帧中，传统的INTENSITY-based图像 регистраción技术无法应用。</li>
<li>methods: 该论文提出了一种使用生成方法来处理追踪分布的变化，以帮助现有的图像 регистраción方法。具体来说，我们提出了一种名为TAI-GAN的模型，通过一个all-to-one映射，将早期帧转换成参照帧中的图像。</li>
<li>results: 我们在一个临床 $^{82}$Rb PET数据集上验证了我们的TAI-GAN模型，并发现它可以生成高质量的转换图像，与参照帧的真实图像相似。另外，我们还发现，在使用TAI-GAN转换后，运动估计精度和临床血液流量（MBF）的量化也得到了改善。<details>
<summary>Abstract</summary>
The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) raise significant challenges for inter-frame motion correction, particularly for the early frames where conventional intensity-based image registration techniques are not applicable. Alternatively, a promising approach utilizes generative methods to handle the tracer distribution changes to assist existing registration methods. To improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform the early frames into the late reference frame using an all-to-one mapping. Specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as the anatomical information. We validated our proposed method on a clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames. After TAI-GAN conversion, motion estimation accuracy and clinical myocardial blood flow (MBF) quantification were improved compared to using the original frames. Our code is published at https://github.com/gxq1998/TAI-GAN.
</details>
<details>
<summary>摘要</summary>
《快速追踪器kinetics和动态心脏PET中的差异性框架分布降低了插入动作 corrections的极大挑战，特别是在早期帧中，传统的Intensity-based图像registration技术无法适用。 alternatively，我们提出了利用生成方法来处理追踪器分布变化，以帮助现有的registration方法。为了提高帧内registration和参数量化，我们提出了一种Temporally and Anatomically Informed Generative Adversarial Network（TAI-GAN），用于将早期帧转换成参照帧中的late frame。specifically，一个特征wise linear modulation层编码了由时间追踪器动力学信息生成的通道wise参数，而粗略的cardiac segmentation with local shifts serves as the anatomical information。我们在临床$^{82}$Rb PET数据集上验证了我们的提议方法，并发现了TAI-GAN可以生成高品质的转换帧，与参照帧相似。 после TAI-GAN转换，运动估计精度和临床血液流量（MBF）量化得到了改进。我们的代码已经在https://github.com/gxq1998/TAI-GAN上发布。》Note: Simplified Chinese is used here, which is a more casual and informal version of Chinese. If you prefer Traditional Chinese or a more formal version, please let me know and I can translate it accordingly.
</details></li>
</ul>
<hr>
<h2 id="BaDExpert-Extracting-Backdoor-Functionality-for-Accurate-Backdoor-Input-Detection"><a href="#BaDExpert-Extracting-Backdoor-Functionality-for-Accurate-Backdoor-Input-Detection" class="headerlink" title="BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection"></a>BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12439">http://arxiv.org/abs/2308.12439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal</li>
<li>for: 防止深度神经网络（DNN）中的后门攻击（backdoor attacks）。</li>
<li>methods: 基于反工程approach，直接提取backdoored模型中的后门功能，并将其转换为一个名为backdoor expert模型。</li>
<li>results: 能够高度准确地检测backdoor输入，并且可以在模型推理过程中过滤掉backdoor输入。<details>
<summary>Abstract</summary>
We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet) across various model architectures (ResNet, VGG, MobileNetV2 and Vision Transformer).
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的防御机制，用于对深度神经网络（DNN）中的后门攻击。在这种攻击中，敌对者会隐藏式地嵌入恶意行为（后门）到DNN中。我们的防御方式属于post-开发防御，可以独立于模型的生成方式操作。我们的防御方式基于一种新的反工程approach，可以直接将backdoored模型中的后门功能提取出来，并将其转换为一个名为backdoor expert模型。这种approach straightforward，只需要使用一些意外地标注的清洁样本进行微调，使得模型忘记了正常的功能，仍然保留后门功能，从而生成一个只能识别后门输入的模型。基于提取出的backdoor expert模型，我们证明了可以设计高精度的后门输入检测器，以避免在模型推断过程中发现后门输入。此外，我们还使用了一种ensemble策略和微调的auxiliary模型，我们的防御机制（BaDExpert）可以高效地抵御16种SOTA后门攻击，而且尽量减少了干扰量。我们的防御机制在多个 dataset（CIFAR10、GTSRB和ImageNet）和多种模型架构（ResNet、VGG、MobileNetV2和Vision Transformer）上进行了验证。
</details></li>
</ul>
<hr>
<h2 id="Deploying-Deep-Reinforcement-Learning-Systems-A-Taxonomy-of-Challenges"><a href="#Deploying-Deep-Reinforcement-Learning-Systems-A-Taxonomy-of-Challenges" class="headerlink" title="Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges"></a>Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12438">http://arxiv.org/abs/2308.12438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drldeploymentchallenges-icsme2023/replicationpackage">https://github.com/drldeploymentchallenges-icsme2023/replicationpackage</a></li>
<li>paper_authors: Ahmed Haj Yahmed, Altaf Allah Abbassi, Amin Nikanjam, Heng Li, Foutse Khomh</li>
<li>For: The paper aims to identify and understand the challenges that practitioners face when deploying deep reinforcement learning (DRL) systems, and to provide a taxonomy of these challenges.* Methods: The paper uses an empirical study on Stack Overflow (SO), a popular Q&amp;A forum for developers, to identify and analyze the challenges related to DRL deployment. The study involves categorizing relevant SO posts by deployment platforms, filtering and manually analyzing the posts, and investigating the prevalence and difficulty of the challenges.* Results: The study finds that the general interest in DRL deployment is growing, and that DRL deployment is more difficult than other DRL issues. The paper also identifies a taxonomy of 31 unique challenges in deploying DRL to different platforms, with RL environment-related challenges being the most popular and communication-related challenges being the most difficult among practitioners.Here are the three key points in Simplified Chinese text:* For: 本研究目标是找到和理解实践者在深度强化学习（DRL）系统部署中遇到的挑战，并提供一个DRL部署挑战的分类。* Methods: 本研究使用Stack Overflow（SO），一个最受欢迎的开发者问答论坛，来找到和分析DRL部署中遇到的挑战。研究包括对SO帖子的分类、筛选和手动分析，以及挑战的流行度和困难程度的调查。* Results: 研究发现DRL部署的一般兴趣在增长，而DRL部署比其他DRL问题更加困难。研究还发现了31个不同的DRL部署挑战，其中RL环境相关的挑战最多，而通信相关的挑战最Difficult。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in reinforcement learning, has shown significant potential in achieving human-level autonomy in a wide range of domains, including robotics, computer vision, and computer games. This potential justifies the enthusiasm and growing interest in DRL in both academia and industry. However, the community currently focuses mostly on the development phase of DRL systems, with little attention devoted to DRL deployment. In this paper, we propose an empirical study on Stack Overflow (SO), the most popular Q&A forum for developers, to uncover and understand the challenges practitioners faced when deploying DRL systems. Specifically, we categorized relevant SO posts by deployment platforms: server/cloud, mobile/embedded system, browser, and game engine. After filtering and manual analysis, we examined 357 SO posts about DRL deployment, investigated the current state, and identified the challenges related to deploying DRL systems. Then, we investigate the prevalence and difficulty of these challenges. Results show that the general interest in DRL deployment is growing, confirming the study's relevance and importance. Results also show that DRL deployment is more difficult than other DRL issues. Additionally, we built a taxonomy of 31 unique challenges in deploying DRL to different platforms. On all platforms, RL environment-related challenges are the most popular, and communication-related challenges are the most difficult among practitioners. We hope our study inspires future research and helps the community overcome the most common and difficult challenges practitioners face when deploying DRL systems.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL），利用深度学习（DL）在强化学习中，已经显示出可以达到人类水平自主的潜力，在Robotics、计算机视觉和计算机游戏等领域都有广泛的应用前景。这种潜力正在学术界和业界中受到广泛关注和探索。然而，社区目前主要关注DRL系统的开发阶段，却忽略了DRL系统的部署。在这篇论文中，我们通过Stack Overflow（SO），最受欢迎的开发人员问答论坛，进行了一项实证研究，探索和理解开发者在部署DRL系统时遇到的挑战。特别是，我们将相关的SO帖子分为不同的部署平台：服务器/云端、移动/嵌入式系统、浏览器和游戏引擎。经过筛选和手动分析，我们分析了357个SO帖子关于DRL部署，了解了当前状况，并识别了部署DRL系统时遇到的挑战。然后，我们研究了这些挑战的普遍性和Difficulty。结果显示，DRL部署的总兴趣正在增长，证明了这项研究的重要性和实用性。结果还表明DRL部署比其他DRL问题更加困难。此外，我们还建立了一个31个唯一挑战的分类标准，这些挑战分别发生在不同的平台上。在所有平台上，RL环境相关的挑战是最受欢迎的，而与通信相关的挑战则是开发者最难以解决的。我们希望这项研究能够激励未来的研究，并帮助社区解决最常见和最难以解决的开发者在部署DRL系统时遇到的挑战。
</details></li>
</ul>
<hr>
<h2 id="Evolution-of-ESG-focused-DLT-Research-An-NLP-Analysis-of-the-Literature"><a href="#Evolution-of-ESG-focused-DLT-Research-An-NLP-Analysis-of-the-Literature" class="headerlink" title="Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature"></a>Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12420">http://arxiv.org/abs/2308.12420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walter Hernandez, Kamil Tylinski, Alastair Moore, Niall Roche, Nikhil Vadgama, Horst Treiblmaier, Jiangbo Shangguan, Paolo Tasca, Jiahua Xu</li>
<li>for: 本研究旨在为分布LEDGER技术（DLT）的发展提供全面的认知，特别是关注环境、可持续性和治理（ESG）方面的多个组成部分。</li>
<li>methods: 本研究采用机器学习技术进行系统性的文献评估，首先选择107个种子论文，建立了63,083个参考文献的公共网络，然后缩放到24,539份文献进行分析。其中，对46份论文进行命名实体标注，并将DLT的ESG元素归类为12个顶级类别。使用变换器基于语言模型，进行Named Entity Recognition（NER）任务的精度调整。</li>
<li>results: 本研究提供了一种机器学习驱动的系统性文献评估方法，特别是在DLT领域中强调ESG方面的研究。此外，我们还提供了一个具有54,808个命名实体的NER数据集，用于DLT和ESG相关的探索。<details>
<summary>Abstract</summary>
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our contributions are a methodology to conduct a machine learning-driven systematic literature review in the DLT field, placing a special emphasis on ESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed of 54,808 named entities, designed for DLT and ESG-related explorations.
</details>
<details>
<summary>摘要</summary>
区块链技术(DLT)在快速演化，需要全面的报告其多种组件。然而，一篇强调环境、可持续发展和治理(ESG)元素的系统性文献评议仍然缺失。为了填补这个差距，我们选择了107个种子论文，建立了63,083个引用的公共网络和提高了其中的24,539篇文献进行分析。然后，我们将46篇论文中的命名实体标注为12个顶级类别，基于已确立的技术分类法，并将DLT的ESG元素突出。通过使用基于转换器的自然语言模型，我们精细了一个预训练的语言模型，以NER任务进行特정。我们使用我们精细的语言模型来缩减 corpus 到505个关键论文，以便通过命名实体和时间图分析来对DLT的演化进行文献评议。我们的贡献是一种机器学习驱动的系统性文献评议方法，特别是在DLT领域的ESG方面。此外，我们还提供了一个具有54,808个命名实体的NER数据集，用于DLT和ESG相关的探索。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-in-parameter-estimation-of-nonlinear-systems"><a href="#Machine-learning-in-parameter-estimation-of-nonlinear-systems" class="headerlink" title="Machine learning in parameter estimation of nonlinear systems"></a>Machine learning in parameter estimation of nonlinear systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12393">http://arxiv.org/abs/2308.12393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaushal Kumar</li>
<li>for: 用于估计复杂非线性系统中参数的精度是科学和工程领域的关键。</li>
<li>methods: 我们提出了一种使用神经网络和惊异损失函数来进行参数估计的新方法。这种方法可以利用深度学习来探索非线性方程中的参数。我们使用synthetic数据和预定的函数来验证我们的方法。通过训练神经网络，它可以根据噪声时间序列数据来细化惊异损失函数，以达到准确的参数。</li>
<li>results: 我们在振荡体、Van der Pol振荡器、Lotka-Volterra系统和lorenz系统中使用这种方法进行参数估计，并得到了高精度的结果。训练神经网络可以准确地估计参数，可以从相似的latent dynamics中看到。在比较真实和估计的轨迹时，我们可以视觉地证明我们的方法的精度和可靠性。这种方法可以在实际挑战中 navigates 噪声和不确定性，表明其适应性。<details>
<summary>Abstract</summary>
Accurately estimating parameters in complex nonlinear systems is crucial across scientific and engineering fields. We present a novel approach for parameter estimation using a neural network with the Huber loss function. This method taps into deep learning's abilities to uncover parameters governing intricate behaviors in nonlinear equations. We validate our approach using synthetic data and predefined functions that model system dynamics. By training the neural network with noisy time series data, it fine-tunes the Huber loss function to converge to accurate parameters. We apply our method to damped oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz systems under multiplicative noise. The trained neural network accurately estimates parameters, evident from closely matching latent dynamics. Comparing true and estimated trajectories visually reinforces our method's precision and robustness. Our study underscores the Huber loss-guided neural network as a versatile tool for parameter estimation, effectively uncovering complex relationships in nonlinear systems. The method navigates noise and uncertainty adeptly, showcasing its adaptability to real-world challenges.
</details>
<details>
<summary>摘要</summary>
估计复杂非线性系统中参数的精度是科学和工程领域中关键的。我们提出了一种使用神经网络和哈伯损失函数的新方法来估计参数。这种方法可以利用深度学习的能力来探索非线性方程中的参数。我们使用生成的数据和预定义的函数来验证我们的方法。通过训练神经网络，它可以根据噪声时间序列数据来细化哈伯损失函数，以达到准确的参数。我们在振荡体、范德洛 oscillators、洛兹几何和多个随机变量下进行了实验，并证明了神经网络可以准确地估计参数。 Comparing true and estimated trajectories visually reinforces our method's precision and robustness. 我们的研究表明，哈伯损失函数引导的神经网络是一种 versatile 的参数估计工具，可以有效地揭示非线性系统中复杂的关系。这种方法在噪声和不确定性中 Navigation 的能力，表明它适用于实际问题。
</details></li>
</ul>
<hr>
<h2 id="FOSA-Full-Information-Maximum-Likelihood-FIML-Optimized-Self-Attention-Imputation-for-Missing-Data"><a href="#FOSA-Full-Information-Maximum-Likelihood-FIML-Optimized-Self-Attention-Imputation-for-Missing-Data" class="headerlink" title="FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data"></a>FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12388">http://arxiv.org/abs/2308.12388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oudeng/fosa">https://github.com/oudeng/fosa</a></li>
<li>paper_authors: Ou Deng, Qun Jin</li>
<li>for: 这篇论文旨在提出一种新的数据填充方法，即 FIML Optimized Self-attention (FOSA) 框架，用于有效地 Addressing missing values 在复杂的数据集中。</li>
<li>methods: 该方法首先使用 FIML 估计初始化缺失值，然后使用自注意力神经网络进行精细调整。</li>
<li>results: 对于 simulate 和实际数据集的实验表明，FOSA 方法在精度、计算效率和数据结构适应性等方面表现出优于传统 FIML 技术，并且能够在 SEM 模型被误差报告的情况下仍然提供优秀的预测结果。<details>
<summary>Abstract</summary>
In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputation outcomes. Our empirical tests reveal that FOSA consistently delivers commendable predictions, even in the face of up to 40% random missingness, highlighting its robustness and potential for wide-scale applications in data imputation.
</details>
<details>
<summary>摘要</summary>
在数据补充中，有效地处理缺失值是关键，特别是在复杂的数据集中。这篇论文探讨了FIML优化自注意（FOSA）框架，这是一种将FIML估计的优点与自注意神经网络的能力相结合的创新方法。我们的方法开始于使用FIML初步估计缺失值，然后通过自注意机制来纠正这些估计。我们在实验中发现，FOSA在真实世界数据集和模拟数据集上具有明显的优势，包括精度、计算效率和对多种数据结构的适应性。尤其是在SEM可能是错误的情况下，FOSA的自注意结构可以干涉和优化投入结果，从而提高数据补充的准确性。我们的实验结果表明，FOSA可以在40%的随机缺失下提供优秀的预测结果，这说明它的稳定性和广泛应用的潜力。
</details></li>
</ul>
<hr>
<h2 id="Open-set-Face-Recognition-with-Neural-Ensemble-Maximal-Entropy-Loss-and-Feature-Augmentation"><a href="#Open-set-Face-Recognition-with-Neural-Ensemble-Maximal-Entropy-Loss-and-Feature-Augmentation" class="headerlink" title="Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation"></a>Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12371">http://arxiv.org/abs/2308.12371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Henrique Vareto, Manuel Günther, William Robson Schwartz</li>
<li>for: 针对无法全面知ledge的面部识别场景，旨在防止未注册人脸样本被误认为已经注册的身份。</li>
<li>methods: 该方法采用一种新的组合方法，结合紧凑型神经网络和边缘基于成本函数的margin化技术，以及新的混合特征增强技术，从外部数据库或生成于表示层次上 Synthetically obtain supplementary negative samples。</li>
<li>results: 对知名的LFW和IJB-C datasets进行实验，结果表明该方法能够提高关闭和开放集face识别率。<details>
<summary>Abstract</summary>
Open-set face recognition refers to a scenario in which biometric systems have incomplete knowledge of all existing subjects. Therefore, they are expected to prevent face samples of unregistered subjects from being identified as previously enrolled identities. This watchlist context adds an arduous requirement that calls for the dismissal of irrelevant faces by focusing mainly on subjects of interest. As a response, this work introduces a novel method that associates an ensemble of compact neural networks with a margin-based cost function that explores additional samples. Supplementary negative samples can be obtained from external databases or synthetically built at the representation level in training time with a new mix-up feature augmentation approach. Deep neural networks pre-trained on large face datasets serve as the preliminary feature extraction module. We carry out experiments on well-known LFW and IJB-C datasets where results show that the approach is able to boost closed and open-set identification rates.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SafeAR-Towards-Safer-Algorithmic-Recourse-by-Risk-Aware-Policies"><a href="#SafeAR-Towards-Safer-Algorithmic-Recourse-by-Risk-Aware-Policies" class="headerlink" title="SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies"></a>SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12367">http://arxiv.org/abs/2308.12367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Wu, Shubham Sharma, Sunandita Patra, Sriram Gopalakrishnan</li>
<li>For: This paper focuses on providing recourse for individuals adversely affected by machine learning (ML) models in critical domains such as finance and healthcare. The goal is to empower people to choose a recourse based on their risk tolerance.* Methods: The paper proposes a method called Safer Algorithmic Recourse (SafeAR) that considers risk uncertainties and variability in cost when computing and evaluating recourse. The method connects algorithmic recourse literature with risk-sensitive reinforcement learning and uses measures such as Value at Risk and Conditional Value at Risk to summarize risk concisely.* Results: The paper applies SafeAR to two real-world datasets and compares policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity). The results show that SafeAR can provide more risk-sensitive and personalized recourse recommendations compared to existing methods.<details>
<summary>Abstract</summary>
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The objective is to empower people to choose a recourse based on their risk tolerance. In this work, we discuss and show how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures ``Value at Risk'' and ``Conditional Value at Risk'' from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity).
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose Safer Algorithmic Recourse (SafeAR), which incorporates risk considerations when computing and evaluating recourse. Our objective is to empower individuals to choose a recourse based on their risk tolerance. We show that existing recourse desiderata can fail to capture the risk of higher costs and present a method to compute recourse policies that consider variability in cost. We also connect algorithmic recourse literature with risk-sensitive reinforcement learning and adopt measures like "Value at Risk" and "Conditional Value at Risk" from the financial literature to summarize risk concisely.We apply our method to two real-world datasets and compare policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity). Our results show that SafeAR can provide more risk-aware recourse recommendations, which can help individuals make more informed decisions and reduce the risk of higher costs.
</details></li>
</ul>
<hr>
<h2 id="Renormalizing-Diffusion-Models"><a href="#Renormalizing-Diffusion-Models" class="headerlink" title="Renormalizing Diffusion Models"></a>Renormalizing Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12355">http://arxiv.org/abs/2308.12355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Cotler, Semon Rezchikov</li>
<li>for: The paper is written for studying field theories using machine learning, specifically using diffusion models to learn inverse renormalization group flows.</li>
<li>methods: The paper uses diffusion models, which are a class of machine learning models that can generate samples from complex distributions, to learn the inverse process of a diffusion process that adds noise to the data until the distribution is pure noise. The paper also combines these observations with nonperturbative renormalization group schemes to build ML-based models for studying field theories.</li>
<li>results: The paper details how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory, and provides explicit prescriptions for comparing results derived from models associated with different renormalization group schemes of interest. The paper also applies some of these methods to numerically find RG flows of interacting statistical field theories.<details>
<summary>Abstract</summary>
We explain how to use diffusion models to learn inverse renormalization group flows of statistical and quantum field theories. Diffusion models are a class of machine learning models which have been used to generate samples from complex distributions, such as the distribution of natural images, by learning the inverse process to a diffusion process which adds noise to the data until the distribution of the data is pure noise. Nonperturbative renormalization group schemes can naturally be written as diffusion processes in the space of fields. We combine these observations in a concrete framework for building ML-based models for studying field theories, in which the models learn the inverse process to an explicitly-specified renormalization group scheme. We detail how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory. Because renormalization group schemes have a physical meaning, we provide explicit prescriptions for how to compare results derived from models associated to several different renormalization group schemes of interest. We also explain how to use diffusion models in a variational method to find ground states of quantum systems. We apply some of our methods to numerically find RG flows of interacting statistical field theories. From the perspective of machine learning, our work provides an interpretation of multiscale diffusion models, and gives physically-inspired suggestions for diffusion models which should have novel properties.
</details>
<details>
<summary>摘要</summary>
我们解释如何使用扩散模型来学习逆减杂化群流程的统计和量子场论。扩散模型是一种机器学习模型，可以生成复杂分布的样本，如自然图像的分布，通过学习逆 процесс，将资料中的噪声添加到样本中，直到样本的分布是纯粹的噪声。非perturbative renormalization group scheme可以自然地写作扩散程序在场论空间中。我们将这些观察结合在一个实际框架中，以建立基于机器学习的场论模型，这些模型学习逆 процесс，以跟踪一个明确地Specified renormalization group scheme。我们详细说明这些模型是一种适应桥（或平行摄氏）抽样器，用于确定点网场论。因为renormalization group scheme有物理意义，我们提供了明确的比较方法，以评估从各个renormalization group scheme中获得的结果之间的关联。我们还说明如何使用扩散模型在统计方法中进行测试，以找到量子系统的基点。从机器学习的角度来看，我们的工作提供了扩散模型的解释，并将提供物理灵感的扩散模型建议，这些建议应有新的性质。
</details></li>
</ul>
<hr>
<h2 id="Improving-Generative-Model-based-Unfolding-with-Schrodinger-Bridges"><a href="#Improving-Generative-Model-based-Unfolding-with-Schrodinger-Bridges" class="headerlink" title="Improving Generative Model-based Unfolding with Schrödinger Bridges"></a>Improving Generative Model-based Unfolding with Schrödinger Bridges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12351">http://arxiv.org/abs/2308.12351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sascha Diefenbacher, Guan-Horng Liu, Vinicius Mikuni, Benjamin Nachman, Weili Nie</li>
<li>for: 这篇论文旨在探讨基于机器学习的对折推算法，以实现不归一化和高维对折推算法的应用。</li>
<li>methods: 这篇论文提出了基于描述性模型和生成模型的两种主要方法，其中描述性模型可以学习一个小的修正，而生成模型可以更好地扩展到数据少的区域。</li>
<li>results: 作者提出了一种基于施罗丁格 bridges 和扩散模型的 unfolding 方法，称为 SBUnfold，该方法可以结合描述性和生成模型的优点，并且可以在一个 synthetic Z+jets 数据集上实现出色的性能。<details>
<summary>Abstract</summary>
Machine learning-based unfolding has enabled unbinned and high-dimensional differential cross section measurements. Two main approaches have emerged in this research area: one based on discriminative models and one based on generative models. The main advantage of discriminative models is that they learn a small correction to a starting simulation while generative models scale better to regions of phase space with little data. We propose to use Schroedinger Bridges and diffusion models to create SBUnfold, an unfolding approach that combines the strengths of both discriminative and generative models. The key feature of SBUnfold is that its generative model maps one set of events into another without having to go through a known probability density as is the case for normalizing flows and standard diffusion models. We show that SBUnfold achieves excellent performance compared to state of the art methods on a synthetic Z+jets dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="D4-Improving-LLM-Pretraining-via-Document-De-Duplication-and-Diversification"><a href="#D4-Improving-LLM-Pretraining-via-Document-De-Duplication-and-Diversification" class="headerlink" title="D4: Improving LLM Pretraining via Document De-Duplication and Diversification"></a>D4: Improving LLM Pretraining via Document De-Duplication and Diversification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12284">http://arxiv.org/abs/2308.12284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos</li>
<li>for: 这篇论文是为了研究大语言模型（LLM）的预训练和下游性能而写的。</li>
<li>methods: 这篇论文使用了预训练模型 embedding 进行精心的数据选择，以提高预训练和下游性能。</li>
<li>results: 这篇论文的结果表明，通过精心的数据选择可以提高 LLM 的预训练和下游性能，并且可以获得20%的效率提升和2%的下游准确率提升。此外， repeating 数据也可以提高下游性能，而不同的数据重复可以超过基eline训练。<details>
<summary>Abstract</summary>
Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.
</details>
<details>
<summary>摘要</summary>
在过去几年，一些大型语言模型（LLM）的训练已经减少了大量计算和数据，通常通过一次学习在大规模网络corpus中随机选择的单词进行训练。虽然训练在无限大规模网络上 führt zu consistent performance improvements，但这些改进的大小随着规模减少，并且有少量的工作研究了预训练和下游性能以外的数据选择的影响。在这里，我们表明了针对预训练模型embeddings进行精心的数据选择（在除掉了重复的数据之后）可以提高训练效率（20%的效率提高）和16种NLP任务的平均下游准确率（最高达2%），并且我们表明了重复数据智能的方法可以连续超过基eline训练（而重复随机数据的训练则比基eline训练差）。我们的结果表明，智能的数据选择可以在LLM预训练中提高性能，质疑了训练一次性地使用最多数据的常见做法，并且表明了可以继续改进我们的模型，超过随机抽样网络数据的限制。
</details></li>
</ul>
<hr>
<h2 id="Extended-Linear-Regression-A-Kalman-Filter-Approach-for-Minimizing-Loss-via-Area-Under-the-Curve"><a href="#Extended-Linear-Regression-A-Kalman-Filter-Approach-for-Minimizing-Loss-via-Area-Under-the-Curve" class="headerlink" title="Extended Linear Regression: A Kalman Filter Approach for Minimizing Loss via Area Under the Curve"></a>Extended Linear Regression: A Kalman Filter Approach for Minimizing Loss via Area Under the Curve</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12280">http://arxiv.org/abs/2308.12280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokulprasath R</li>
<li>for: 提高线性回归模型的精度，使其能够更好地预测数据。</li>
<li>methods: 将 kalman 筛引入到线性回归模型中，分析曲线面积来减少损失。</li>
<li>results: 提出一种基于杂志梯逐步进程的优化线性回归方程，使用梯逐进程来更新参数，并使用 kalman 筛来预测下一个融合参数。该方法可以避免不断更新参数的困境，并且可以处理部分数据集，不同于需要整个数据集来进行预测。但是，计算复杂性应该被考虑。<details>
<summary>Abstract</summary>
This research enhances linear regression models by integrating a Kalman filter and analysing curve areas to minimize loss. The goal is to develop an optimal linear regression equation using stochastic gradient descent (SGD) for weight updating. Our approach involves a stepwise process, starting with user-defined parameters. The linear regression model is trained using SGD, tracking weights and loss separately and zipping them finally. A Kalman filter is then trained based on weight and loss arrays to predict the next consolidated weights. Predictions result from multiplying input averages with weights, evaluated for loss to form a weight-versus-loss curve. The curve's equation is derived using the two-point formula, and area under the curve is calculated via integration. The linear regression equation with minimum area becomes the optimal curve for prediction. Benefits include avoiding constant weight updates via gradient descent and working with partial datasets, unlike methods needing the entire set. However, computational complexity should be considered. The Kalman filter's accuracy might diminish beyond a certain prediction range.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种基于 Kalman 筛和直线回归模型的优化方法，以降低损失。目标是通过随机梯度下降（SGD）来更新参数。我们的方法包括以下步骤：首先定义用户参数，然后使用 SGD 训练直线回归模型，并跟踪参数和损失的变化。接着，我们使用 Kalman 筛来预测下一个融合参数。预测结果是通过输入均值与参数进行乘法，并评估损失来形成一个参数对损失曲线。这个曲线的方程可以通过两点方程得出，而曲线下面积可以通过积分来计算。最佳曲线的方程是Minimum Area的曲线，它可以用于预测。这种方法的优点包括避免了不断地更新参数 via 梯度下降，同时可以处理部分数据集，而不是需要整个数据集。然而，计算复杂度应该被考虑。Kalman 筛的准确性可能在预测范围内逐渐减退。
</details></li>
</ul>
<hr>
<h2 id="On-Manifold-Projected-Gradient-Descent"><a href="#On-Manifold-Projected-Gradient-Descent" class="headerlink" title="On-Manifold Projected Gradient Descent"></a>On-Manifold Projected Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12279">http://arxiv.org/abs/2308.12279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JonasGrabbe/GradientDecentOnManifolds">https://github.com/JonasGrabbe/GradientDecentOnManifolds</a></li>
<li>paper_authors: Aaron Mahler, Tyrus Berry, Tom Stephens, Harbir Antil, Michael Merritt, Jeanie Schreiber, Ioannis Kevrekidis</li>
<li>for: This paper is written for researchers and practitioners in the field of machine learning, particularly those interested in adversarial training and the geometry of high-dimensional data.</li>
<li>methods: The paper uses conformally invariant diffusion maps (CIDM) to approximate class manifolds in diffusion coordinates, and the Nystr&quot;{o}m projection to project novel points onto class manifolds in this setting. The paper also employs the spectral exterior calculus (SEC) to determine geometric quantities such as tangent vectors of the manifold.</li>
<li>results: The paper provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, and demonstrates the effectiveness of the approach by generating novel, on-manifold data samples and implementing a projected gradient descent algorithm for on-manifold adversarial training. The paper also shows that the proposed method can obtain adversarial examples that reside on a class manifold, yet fool a classifier, and provides a human-understandable explanation of the manipulations within the data that lead to misclassifications.Here is the answer in Simplified Chinese:</li>
<li>for: 这篇论文是为了各种机器学习领域的研究者和实践者，特别是关注防御训练和高维数据的geometry。</li>
<li>methods: 这篇论文使用均匀扩散映射(CIDM)来 aproximate类 manifold在扩散坐标中，并使用尼斯特罗姆投影来在这种设定下 проек novel点 onto class manifold。论文还使用spectral exterior calculus (SEC)来 determinegeometric量 such as tangent vector of the manifold。</li>
<li>results: 这篇论文提供了可计算、直接、数学上正确的高维数据类 manifold的difficult geometryapproximation，并证明了该方法的有效性。论文还表明了该方法可以生成onto-manifold数据amples，并实现了 onto-manifold防御训练中的梯度下降算法。论文还显示了该方法可以获得onto-manifold上的骗例，却能让分类器进行误分类，并提供了人类可理解的数据中的扭曲 manipulate的解释。<details>
<summary>Abstract</summary>
This work provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, along with nonlinear projections from input space onto these class manifolds. The tools are applied to the setting of neural network image classifiers, where we generate novel, on-manifold data samples, and implement a projected gradient descent algorithm for on-manifold adversarial training. The susceptibility of neural networks (NNs) to adversarial attack highlights the brittle nature of NN decision boundaries in input space. Introducing adversarial examples during training has been shown to reduce the susceptibility of NNs to adversarial attack; however, it has also been shown to reduce the accuracy of the classifier if the examples are not valid examples for that class. Realistic "on-manifold" examples have been previously generated from class manifolds in the latent of an autoencoder. Our work explores these phenomena in a geometric and computational setting that is much closer to the raw, high-dimensional input space than can be provided by VAE or other black box dimensionality reductions. We employ conformally invariant diffusion maps (CIDM) to approximate class manifolds in diffusion coordinates, and develop the Nystr\"{o}m projection to project novel points onto class manifolds in this setting. On top of the manifold approximation, we leverage the spectral exterior calculus (SEC) to determine geometric quantities such as tangent vectors of the manifold. We use these tools to obtain adversarial examples that reside on a class manifold, yet fool a classifier. These misclassifications then become explainable in terms of human-understandable manipulations within the data, by expressing the on-manifold adversary in the semantic basis on the manifold.
</details>
<details>
<summary>摘要</summary>
Traditional adversarial training methods introduce adversarial examples into the training process to increase the robustness of neural networks (NNs) to attacks. However, these methods can also reduce the accuracy of the classifier if the examples are not valid examples for that class. Our work explores the use of realistic "on-manifold" examples generated from class manifolds in the latent space of an autoencoder to improve the robustness of NNs. We employ conformally invariant diffusion maps (CIDM) to approximate class manifolds in diffusion coordinates, and develop the Nystr\"{o}m projection to project novel points onto class manifolds in this setting. On top of the manifold approximation, we leverage the spectral exterior calculus (SEC) to determine geometric quantities such as tangent vectors of the manifold. We use these tools to obtain adversarial examples that reside on a class manifold, yet fool a classifier. These misclassifications can be explained in terms of human-understandable manipulations within the data, by expressing the on-manifold adversary in the semantic basis on the manifold. Our approach is more effective than traditional adversarial training methods because it uses realistic, on-manifold examples that are closer to the raw, high-dimensional input space than can be provided by VAE or other black box dimensionality reductions. This makes our approach more robust and more accurate than traditional methods. In summary, our work provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, along with nonlinear projections from input space onto these class manifolds. We use these tools to obtain adversarial examples that reside on a class manifold, yet fool a classifier, and to explain the misclassifications in terms of human-understandable manipulations within the data. Our approach is more effective and more robust than traditional adversarial training methods.
</details></li>
</ul>
<hr>
<h2 id="LCANets-Robust-Audio-Classification-using-Multi-layer-Neural-Networks-with-Lateral-Competition"><a href="#LCANets-Robust-Audio-Classification-using-Multi-layer-Neural-Networks-with-Lateral-Competition" class="headerlink" title="LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition"></a>LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12882">http://arxiv.org/abs/2308.12882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti</li>
<li>for: 实现音频分类任务中的敏捷识别和防御性能。</li>
<li>methods: 使用类神经网络（CNNs），并在第一层使用本地竞争算法（LCA）进行简短编码，以减少依赖标签数据。</li>
<li>results: 比较标准CNNs和LCANets，LCANets++ 具有更高的防御性和敏捷性，可以对噪声和攻击（如误导和快速梯度签名攻击）进行更好的识别和抵抗。<details>
<summary>Abstract</summary>
Audio classification aims at recognizing audio signals, including speech commands or sound events. However, current audio classifiers are susceptible to perturbations and adversarial attacks. In addition, real-world audio classification tasks often suffer from limited labeled data. To help bridge these gaps, previous work developed neuro-inspired convolutional neural networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA) in the first layer (i.e., LCANets) for computer vision. LCANets learn in a combination of supervised and unsupervised learning, reducing dependency on labeled samples. Motivated by the fact that auditory cortex is also sparse, we extend LCANets to audio recognition tasks and introduce LCANets++, which are CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that LCANets++ are more robust than standard CNNs and LCANets against perturbations, e.g., background noise, as well as black-box and white-box attacks, e.g., evasion and fast gradient sign (FGSM) attacks.
</details>
<details>
<summary>摘要</summary>
Audio 分类目标是识别音频信号，包括语音命令或声音事件。然而，现有的音频分类器受到干扰和敌意攻击的影响。此外，实际世界中的音频分类任务经常面临有限的标注数据的问题。为了bridging这些差距，过去的工作开发了基于神经元的启发式卷积神经网络（CNN），使用本地竞争算法（LCA）在第一层进行稀疏编码，称为LCANets。LCANets在超过supervised和unsupervised学习方法下学习，减少了依赖于标注样本。驱动了听觉皮层也是稀疏的事实，我们扩展LCANets到音频识别任务，并引入LCANets++，它是多层使用LCA进行稀疏编码的CNN。我们示示了LCANets++对干扰、背景噪音、黑盒和白盒攻击等方面更加强大，比如逃避和快速梯度签名（FGSM）攻击。
</details></li>
</ul>
<hr>
<h2 id="Language-Reward-Modulation-for-Pretraining-Reinforcement-Learning"><a href="#Language-Reward-Modulation-for-Pretraining-Reinforcement-Learning" class="headerlink" title="Language Reward Modulation for Pretraining Reinforcement Learning"></a>Language Reward Modulation for Pretraining Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12270">http://arxiv.org/abs/2308.12270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ademiadeniji/lamp">https://github.com/ademiadeniji/lamp</a></li>
<li>paper_authors: Ademi Adeniji, Amber Xie, Carmelo Sferrazza, Younggyo Seo, Stephen James, Pieter Abbeel</li>
<li>for: 本研究问题是否现代learned reward functions（LRFs）适用于直接替换任务奖励？相反，我们提议利用LRFs作为RL的预训练信号。</li>
<li>methods: 我们提出了一种名为LAMP的方法，它利用预先冻结的视觉语言模型（VLMs）作为RL的预训练资源，通过计算语言指令和机器人观察图像之间的对比性Alignment来生成噪音的探索奖励。</li>
<li>results: 我们的LAMP方法可以在RLBench上具有较高的效率，通过与标准的探索奖励结合LRFs来学习语言conditioned的预训练策略。<details>
<summary>Abstract</summary>
Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\textbf{LA}$nguage Reward $\textbf{M}$odulated $\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy. Our VLM pretraining approach, which is a departure from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FECoM-A-Step-towards-Fine-Grained-Energy-Measurement-for-Deep-Learning"><a href="#FECoM-A-Step-towards-Fine-Grained-Energy-Measurement-for-Deep-Learning" class="headerlink" title="FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning"></a>FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12264">http://arxiv.org/abs/2308.12264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabhsingh Rajput, Tim Widmayer, Ziyuan Shang, Maria Kechagia, Federica Sarro, Tushar Sharma</li>
<li>for:  This paper aims to provide a fine-grained energy consumption measurement framework for deep learning (DL) systems, which can help promote green development and energy awareness in the field.</li>
<li>methods: The paper introduces FECoM (Fine-grained Energy Consumption Meter), a framework that uses static instrumentation to measure energy consumption at a fine granularity (e.g., at method level) for DL systems. FECoM addresses various challenges, such as computational load and temperature stability, to provide accurate energy consumption measurements.</li>
<li>results: The paper assesses FECoM’s capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, TensorFlow. The results show that FECoM can effectively measure the energy consumption of TensorFlow APIs and provide insights into the impact of parameter size and execution time on energy consumption.<details>
<summary>Abstract</summary>
With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, namely TensorFlow. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow APIs' energy profiles. Furthermore, we elaborate on the considerations, issues, and challenges that one needs to consider while designing and implementing a fine-grained energy consumption measurement tool. We hope this work will facilitate further advances in DL energy measurement and the development of energy-aware practices for DL systems.
</details>
<details>
<summary>摘要</summary>
In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers with a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at a fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability.We assess FECoM's capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, namely TensorFlow. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow APIs' energy profiles. Furthermore, we elaborate on the considerations, issues, and challenges that one needs to consider while designing and implementing a fine-grained energy consumption measurement tool.We hope this work will facilitate further advances in DL energy measurement and the development of energy-aware practices for DL systems.
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Negative-User-Feedback-and-Measuring-Responsiveness-for-Sequential-Recommenders"><a href="#Learning-from-Negative-User-Feedback-and-Measuring-Responsiveness-for-Sequential-Recommenders" class="headerlink" title="Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders"></a>Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12256">http://arxiv.org/abs/2308.12256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueqi Wang, Yoni Halpern, Shuo Chang, Jingchen Feng, Elaine Ya Le, Longfei Li, Xujian Liang, Min-Cheng Huang, Shane Li, Alex Beutel, Yaping Zhang, Shuchao Bi</li>
<li>for:  This paper aims to improve the performance of sequential recommender systems by incorporating negative user feedback into the training objective.</li>
<li>methods: The authors use a “not-to-recommend” loss function to optimize for the log-likelihood of not recommending items with negative feedback in the retrieval stage.</li>
<li>results: The authors demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system, and also develop a counterfactual simulation framework to compare recommender responses between different user actions, showing improved responsiveness from the modeling change.<details>
<summary>Abstract</summary>
Sequential recommenders have been widely used in industry due to their strength in modeling user preferences. While these models excel at learning a user's positive interests, less attention has been paid to learning from negative user feedback. Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user. However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions. In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a "not-to-recommend" loss function that optimizes for the log-likelihood of not recommending items with negative feedback. We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system. Furthermore, we address a challenge in measuring recommender responsiveness to negative feedback by developing a counterfactual simulation framework to compare recommender responses between different user actions, showing improved responsiveness from the modeling change.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:Sequential recommender systems 已经广泛应用于实际场景，这是因为它们可以很好地模型用户喜好。然而，这些模型往往忽略了用户的负反馈，这是用户控制和期望的重要依据。在这种情况下，我们将负反馈 incorporated 到sequential retrieval模型的训练目标中，使用“不推荐”损失函数，以优化不推荐item的log-likelihood。我们通过实际实验表明了这种方法的效果，并且通过对不同用户行为进行对照分析，显示了改进的响应性。
</details></li>
</ul>
<hr>
<h2 id="How-Safe-Am-I-Given-What-I-See-Calibrated-Prediction-of-Safety-Chances-for-Image-Controlled-Autonomy"><a href="#How-Safe-Am-I-Given-What-I-See-Calibrated-Prediction-of-Safety-Chances-for-Image-Controlled-Autonomy" class="headerlink" title="How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy"></a>How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12252">http://arxiv.org/abs/2308.12252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maozj6/hsai-predictor">https://github.com/maozj6/hsai-predictor</a></li>
<li>paper_authors: Zhenjiang Mao, Carson Sobolewski, Ivan Ruchkin</li>
<li>for: 该论文主要用于提出一种可配置的家族式学习管道，以提高自动化系统的安全性验证。</li>
<li>methods: 该论文使用生成世界模型，并解决了在预测induced分布转移下学习安全知识的问题，以及在缺少安全标签的情况下实现安全预测。</li>
<li>results: 该论文通过对两个图像控制系统的评估，证明了该方法的可靠性和有效性。<details>
<summary>Abstract</summary>
End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a cartpole.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-to-Protect-Copyright-Data-in-Optimization-of-Large-Language-Models"><a href="#How-to-Protect-Copyright-Data-in-Optimization-of-Large-Language-Models" class="headerlink" title="How to Protect Copyright Data in Optimization of Large Language Models?"></a>How to Protect Copyright Data in Optimization of Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12247">http://arxiv.org/abs/2308.12247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy Chu, Zhao Song, Chiwun Yang</li>
<li>for: 研究大型自然语言模型（LLM）和生成AI的法律问题</li>
<li>methods: 使用softmax函数进行回归问题来训练和优化大型语言模型</li>
<li>results: 提出一种有效地进行回归问题的方法，以避免生成版权数据<details>
<summary>Abstract</summary>
Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.   In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和生成AI在计算机研究和应用中发挥了转变性的作用。但是，有争议是这些模型输出的数据是否受版权保护，这可能会发生如果模型训练数据是受版权保护的。LLMs是基于转换神经网络架构，该架构则依赖于一种数学计算 called Attention，它使用软饶函数。在这篇论文中，我们表明了大型语言模型训练和优化可以被看作为软饶 regression问题。然后，我们建立了一种有效地进行软饶 regression的方法，以避免生成版权数据。这种方法可以在训练大型语言模型时避免生成版权数据。
</details></li>
</ul>
<hr>
<h2 id="Multi-Objective-Optimization-for-Sparse-Deep-Neural-Network-Training"><a href="#Multi-Objective-Optimization-for-Sparse-Deep-Neural-Network-Training" class="headerlink" title="Multi-Objective Optimization for Sparse Deep Neural Network Training"></a>Multi-Objective Optimization for Sparse Deep Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12243">http://arxiv.org/abs/2308.12243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salomonhotegni/mdmtn">https://github.com/salomonhotegni/mdmtn</a></li>
<li>paper_authors: S. S. Hotegni, S. Peitz, M. Berkemeier</li>
<li>for: 这篇论文旨在提出一种多目标优化算法，用于训练深度学习网络（DNNs），以满足多个任务的需求。</li>
<li>methods: 这篇论文使用了一种 modificated Weighted Chebyshev scalarization 技术，将多个任务转换为一个单一目标函数，并使用 Augmented Lagrangian 方法来解决这个问题。</li>
<li>results: 这篇论文的实验结果显示，这种多目标优化算法可以在训练 Deep Multi-Task 网络时，自动将网络簇化为更简单的网络，而不会影响网络的性能。<details>
<summary>Abstract</summary>
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economical and also ecological) sustainability issue of DNN models, with a particular focus on Deep Multi-Task models, which are typically designed with a very large number of weights to perform equally well on multiple tasks. Through experiments conducted on two Machine Learning datasets, we demonstrate the possibility of adaptively sparsifying the model during training without significantly impacting its performance, if we are willing to apply task-specific adaptations to the network weights. Code is available at https://github.com/salomonhotegni/MDMTN.
</details>
<details>
<summary>摘要</summary>
不同的冲突优化标准在深度学习场景中自然出现。这些标准可以处理不同的主任务（例如在多任务学习设置中），也可以处理主任务和次任务，如损失最小化和稀疏化。通常的方法是简单地Weighting这些标准，这只有在几何设定下才能正确工作。在这篇论文中，我们提出了一种多目标优化算法，使用修改后的Weighted ChebyshevScalarization来训练深度神经网络（DNNs）与多个任务之间的关系。通过使用这种Scalarization技术，算法可以找到原始问题的所有优秀解决方案，同时减少问题的复杂度到一个序列单个目标问题。这些简化后的问题可以使用Augmented Lagrangian方法解决，这使得可以使用流行的优化技术，如Adam和Stochastic Gradient Descent，同时有效地处理约束。我们的工作旨在解决深度神经网络模型的（经济和生态）可持续性问题，尤其是深度多任务模型，这些模型通常具有大量的参数，以便在多个任务上 equally well 表现。通过在两个机器学习数据集上进行实验，我们表明了在训练过程中适应性减少模型的可能性，只要愿意对任务特定的网络参数进行修改。代码可以在https://github.com/salomonhotegni/MDMTN 上获取。
</details></li>
</ul>
<hr>
<h2 id="Critical-Learning-Periods-Emerge-Even-in-Deep-Linear-Networks"><a href="#Critical-Learning-Periods-Emerge-Even-in-Deep-Linear-Networks" class="headerlink" title="Critical Learning Periods Emerge Even in Deep Linear Networks"></a>Critical Learning Periods Emerge Even in Deep Linear Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12221">http://arxiv.org/abs/2308.12221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Kleinman, Alessandro Achille, Stefano Soatto</li>
<li>For: 这个论文研究了深度学习网络中的批处理期，以及这些期间对学习行为和学习表征的影响。* Methods: 该论文使用了深度线性网络模型，并通过分析和实验来研究批处理期的特点和影响因素。* Results: 研究发现，批处理期取决于网络的深度和数据分布结构，并且学习特征的形成与数据源之间的竞争有关。此外，研究还发现在多任务学习中，预训练过程的持续时间可以影响新任务的转移性能。<details>
<summary>Abstract</summary>
Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show analytically and in simulations that the learning of features is tied to competition between sources. Finally, we extend our analysis to multi-task learning to show that pre-training on certain tasks can damage the transfer performance on new tasks, and show how this depends on the relationship between tasks and the duration of the pre-training stage. To the best of our knowledge, our work provides the first analytically tractable model that sheds light into why critical learning periods emerge in biological and artificial networks.
</details>
<details>
<summary>摘要</summary>
“重要的学习时期是在发育初期的一段时间，在这段时间内，暂时的感知缺陷可能会导致永久性的行为和学习的表征。尽管生物和人工网络之间存在巨大的不同，但critical learning periods在两种系统中都有证据存在。这表明critical periods可能是学习的基本特征，而不是生物学的巧合。然而， precisely why critical periods emerge in deep networks is still an open question, and in particular, it is unclear whether the critical periods observed in both systems depend on specific architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and the structure of the data distribution. We also show analytically and in simulations that the learning of features is tied to competition between sources. Finally, we extend our analysis to multi-task learning to show that pre-training on certain tasks can damage the transfer performance on new tasks, and show how this depends on the relationship between tasks and the duration of the pre-training stage. To the best of our knowledge, our work provides the first analytically tractable model that sheds light on why critical learning periods emerge in biological and artificial networks.”Please note that the translation is in Simplified Chinese, and the sentence structure and wording may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Language-Models-Can-Perform-Many-Tasks-with-Scaling-and-Instruction-Finetuning"><a href="#Diffusion-Language-Models-Can-Perform-Many-Tasks-with-Scaling-and-Instruction-Finetuning" class="headerlink" title="Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning"></a>Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12219">http://arxiv.org/abs/2308.12219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yegcjs/diffusionllm">https://github.com/yegcjs/diffusionllm</a></li>
<li>paper_authors: Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu</li>
<li>for: 这篇论文目的是探讨扩展扩散概率模型的可能性，以解决通用语言任务。</li>
<li>methods: 该论文使用扩散语言模型，并通过数据、大小和任务的扩展来提高其语言学习能力。</li>
<li>results: 实验显示，扩展扩散语言模型可以在下游语言任务中提高表现，并且发现了在线上学习和受 instruktion 的适应能力。<details>
<summary>Abstract</summary>
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning
</details>
<details>
<summary>摘要</summary>
Recent years have seen a surge in the development of generative AI, fueled by the power of diffusion probabilistic models and the scalability of large language models. However, it remains to be seen whether diffusion language models can perform general language tasks as well as their autoregressive counterparts. This paper shows that scaling diffusion models in terms of data, size, and tasks can make them strong language learners. We build competent diffusion language models by first pretraining them on massive data using masked language modeling, and then adapting them to specific tasks through a process called diffusive adaptation. This involves task-specific finetuning and instruction finetuning, which enables the models to learn a wide range of language tasks with few examples. Our experiments show that scaling diffusion language models consistently improves performance across downstream language tasks, and we also discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that allow the models to tackle unseen tasks by following natural language instructions. This has promising applications in advanced and challenging abilities such as reasoning.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.LG_2023_08_24/" data-id="clltaagoi006or888h11q000a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/08/24/cs.CV_2023_08_24/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          cs.CV - 2023-08-24 21:00:00
        
      </div>
    </a>
  
  
    <a href="/2023/08/24/cs.SD_2023_08_24/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">cs.SD - 2023-08-24 123:00:00</div>
    </a>
  
</nav>

  
</article>


</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">92</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
